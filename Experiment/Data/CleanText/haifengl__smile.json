{
    "Xyclade": ":+1:  I would love to see this basic tutorial too!\n. A good example to start with would be a short tutorial how to classify males/females (example from wikipedia ) \n. Thank you for the quick feedback. The main reason I want to do a 50% training-testing split is because this is exactly how it is done in an example in R, and my goal is to work through that example using Smile instead.  I will experiment a bit more and use Bootstrap too to see how that works. Thanks again! \n. Thanks! I'll add a unit test and PR it to this issue later today, as I'm in a meeting right now.\n. PS. the reason I found this was by extracting a feature array for 1 category, then for a second one and combining the two feature arrays into 1 new one, causing duplicates to occur as some terms where in both categories. I should probably remove the intersecting features to make the algorithm predictions better, or does the implementation take these into account? \n. Oh maybe I explained it wrongly:\nMy suggestion was to keep the array for the labels the same, but to change the implementation in the constructor, such that next to the array you get a global lookup table which holds the unique values and their Colors / legends.  I'll work it out, such that you can see exactly what I mean, because explaining it is rather difficult.\nPs. It should not affect the performance, as the extra computation is only n where n is the amount of labels. At least, in the way I see the adjustment. \n. I will test with a lot of datapoints. This is the fix I'm suggesting. Since the lookup in the hashmap should be O(1) it should not affect the plot method in performance.\nIn a second commit I merged the two for loops as I missed the second one in the first commit.\nI'll let you know how the test goes.\n. For 10 million datapoints the time to plot was equal  (+/- 1 second on 1.04 minutes plotting time) for both methods.\n. I removed the negative check and renamed the table. However I cannot find the commit regarding the comment, could you point me out where I should update this comment? :+1: \n. Tnx for merging it :smile: \n. The Lasso model is not working well for the test data, but that is the actual goal this time. I'm writing an example of how it can go wrong, thank you for the heads up though! :+1: \n. It's because there is too few datapoints and no actual statistical relation within the data. \nAs John Tukey once said:\n\nThe data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.\n\nThis is the case in my example, to make people aware that Machine Learning cannot perform miracles on every single dataset.\n. yes, its 100 datapoints (with rank 1 to 100) with 27000 + features, this can never go well :)  But since it's an example for my blog rather than an actual dataset that I want to use for predictions, I still worked it out to show what goes wrong when you do these kind of things.\nIn the end result the trained LASSO model should predict a rank value for a new datapoint, but it predicts worse than just predicting the average (50).  This makes it a perfect example for what can go wrong if you have no clue what you are doing :+1: \n. Cool tnx! I'll read that and see if I can incorporate it if that's ok with you? \n. I noticed the same issue as @experquisite, but it seems fixed now with 1.0.2 via maven for me :+1: \n. Hi @gustavosiqueira94, in my blog about machine learning using Smile, I've made a code example on how to use the SVM. In this example there where 2 classes. Note that the code is in Scala, so if you have questions feel free to ask.  You can find the example here\n. @haifengl  no problem, I can't seem to find the SmileNLP 1.0.4 on Maven, do you have a download link for me?\n. I've updated it now and it works, thank you!\n. My bad, should've formatted it before committing, I'll do so next time :+1: \n. @ravikuril  an explanation in addition to the tutorials on the wiki: The library can be used for processing natural languages to a more structured set of data, by stemming, removing stopwords, finding bigrams etc. In machine learning you tend to use NLP in combination with another form of learning, to make something powerful. \nI suggest you look up some of the general notions of NLP (be careful that you search for Natural Language processing, and not for Neuro-linguistic programming) , and look at the examples from the tutorials. If you understand the examples, then getting the NLP part to work should be easy as it follows the same principles :+1: \n. The network is set up as [4,100] for the layers\nIn the end it ran on 125k points and only 1 came back with this weird value, so I'll investigate a bit further. Just wanted to make sure that it should not return classifications outside the scope of the training data\n. I see, I had expected the PMML to be rather complete by now since it was first proposed in 1999, and currently is in version 4.  \nI'll use the serialization of models instead, thank you for enabling this!\n. I'm still experiencing an indexOutOfbounds exception on predict with the latest version from maven (1.2.0). The code snippet on which it happens in my 1.2.0 version of smile differs from the repository, so I think the fix is not yet deployed in a new version to maven. \n. Ok, I will work around it.\n. ",
    "haifengl": "A basic tutorial is now available at https://github.com/haifengl/smile/wiki/Tutorial:-A-Gentle-Introduction-of-SmileMiner\nPlease let me if you have any questions and suggestions. I will keep adding more wiki pages. Thanks!\n. Thanks for quick response. Sorry for the long wait. Not productive during the holiday season :)\n. Fix was pulled from issue #4. Thanks!\n. The first parameter of CrossValidation is the number of samples in the data. The second is the number of rounds of cross validation. In your case, it should be 2. BTW, it is better to use a larger number for cross validation (e.g. 5 or 10). Otherwise, the estimated error rate may be very biased.\nBesides, Bootstrap may be a better chance for more accurate estimation (with a large number of rounds such as 100). But it takes longer time. Worth to try if your data is not too large. \n. Hi Mike,\nThanks for the bug report! I would like to do c and a. The \"training bag\" is actually not for training. It is the feature list, and thus should be a set of unique words. But for sanity check, we should check the duplicates in the constructor. I will do these right now. Can you please add a unit test (in the smile.feature.BagTest of test directory) with your test case? Thanks!\nHaifeng\n. Hi Mike,\nThe fix is in the master now. Thanks!\n. Yes, you better use Set class to get the union of the features rather than just concat in general. But the constructor now handles the duplicates. So it should be fine for you to use the feature list in your example.\nThe main problem is the documentation of Bag. It is not for training purpose. It just assumes that you have a set of unique features and uses them to calculate the double valued feature vectors of some text for you.\n. Sorry, this is a documentation problem. The labels should be in the range [0, k), where k is the number of classes. In your sample code, we should have\nval exampleLabels = Array(0, 1)\nWe could change Array to Map. However, there is a performance issue for large number of points. Some people tried to plot millions data points. Map will make the UI unusable.\n. The performance is not in the constructor but in plot. For each data point, we may need change color or legend, now it is done by a Map lookup instead of array index. In terms of complexity, it is still O(n) for HashMap. But the constant factor of n is much bigger now. In your experiments, please try millions data points to make sure if it is still ok. Maybe I am wrong. Thanks!\n. Cool. BTW, we don't need to check if id is negative now. Can you rename valueLookupTable to classLookupTable? Thanks!\nI had a commit updating the comment. Can you please update the comment again? The API may be confusing now. The user may not have clear idea of what is the color/legend of class 100.\n. Sorry, forgot to push to github. It is there now. Thanks!\n. Thank you very much!\n. Sure, you can remove them. At least the first one. But I am little worried about this case. This function ( iterative biconjugate gradient method) usually should converge pretty fast and thus not much print out. Please check if your LASSO model works well for test data. Thanks!\n. Interesting. It doesn't work well because LASSO is not a good fit for the problem? Or the parameter settings (e.g. regularization factor)?\n. Is your data size less than the dimensionality?\n. That is the true reason. It is known as small sample size problem. I have a paper (http://lectures.molgen.mpg.de/networkanalysis13/LDA_cancer_classif.pdf) to deal with it.\n. Try FLD in SMILE for your case. I don't remember if I implemented this algorithm there. Thanks!\n. The library has been in development for several years. At beginning, it is close source to myself. Now I open source it in Apache license. Need some work to update all source files.\n. Hi Diego,\nWould you like to create a build script or eclipse project of SmileMiner for Android? I already provide eclipse project files and settings for these projects. Since the library uses only java standard library, it should be easy to port them to Android (except SmilePlot, which uses Swing). Then you can use these libraries in your framework easily. I don't feel that it is a good strategy to copy files to another projects. It will be hard to synchronize our efforts. If you need helps, I definitely would like to work with you together. Thanks!\n. all copyrights header in source files have been updated. Thanks!\n. In supervised learning, we need a response variable for each sample to train the model. For classification, it is the class label. For regression, it is a real value. If a dataset contains such response variable, the method setResponseIndex is used to indicate which one (by column index, starting with 0).\nI will update the tutorial. Thanks!\n. The datasets were collected from Internet. They are open dataset so that I can use them for testing the library. However, it is not okay for me to publish them in the project as they belong to others.\n. There are a lot and collected from everywhere. But you can always go to UCI machine learning repository https://archive.ics.uci.edu/ml/datasets.html to find something interesting for testing.\n. The demo source code (SmileDemo) is imported. Please have a look. Thanks!\n. Good idea. Will figure out how to do it. Thanks!\n. Refactored pom.xml in preparation of maven central repository.\n. It is in the maven central repository now. Please try\n\ncom.github.haifengl\nsmile-core\n1.0.0\n\nThanks!\n. Smile-all is the parent pom to build all these modules. It is not really needed. Maybe I didn't do it right to publish to central repository. Any ideas? Thanks!\n. I will figure out how to fix it. In short term, maybe you can download the parent pom from github to your local ivy2 directory?\n. Can you please try again with version 1.0.1? Thanks! If maven cannot find this version, please just wait a couple of hours.\n. Great! Thanks for quick response!\n. Can you please try version 1.0.2 now? Thanks!\n. Great. Thanks guys!\n. Thanks for the bug report! Do you have a stack dump?\n. Thanks! Can you please change that line to ex.printStackTrace() and run it again?\n. Fixed. Can you try version 1.0.2? Thanks! If your machine is multi-core, it should be faster now.\n. @mschulkind Any updates? Thanks!\n. Do you mean that everything runs fine and you get all results, but the program doesn't exit? Or the program hangs in the creating the random forest? Thanks!\n. There is a thread pool, shared by many algorithms. If you are the master branch, you can use\nsmile.util.MulticoreExecutor.shutdown()\nat the end of your program to exit smoothly. Thanks!\n. What's your use case? Do you want it mt-safe for training or testing/prediction? Thanks!\n. I doubt how much we can gain by making training of neural net mt-safe. There are a lot of shared data (weights in the net). To make it mt-safe, locking/mutex will make it slow. Can you have a try with your data in single thread to see how long it takes? Thanks!\nBTW, Neural Net of smile is traditional back propagation algorithm. It is different deep learning and is suitable for different problems.\n. Any updates? Is it too slow for you? Thanks!\n. Thanks! If you have any questions about Smile's NeuralNetwork, please feel free open a ticket.\n. Given the sample size, the number of attributes, this looks very good to you. How do you feel? Especially compared to your experience with other packages? Thank you very much for reporting the results!\nBtw, what's neural net settings?\n. Can you please write a post on our wiki describing what you are doing? It doesn't have to be long. Simply describe your data and your problem, why you choose neural net, how you train and test the model (and some code snippets).  This will be extremely valuable for others. Thanks a lot in advance!\n. BTW, our SVM training algorithm is also an online algorithm. You may want to give it a try and compare it to neural net. Thanks!\n. How's going with 100k?\n. Thanks for updates! Hope everything is fine with you.\n. For sure, I can share the source code of SmileDemo. The problems is that it depends on some open datasets that I download from the web. I cannot put these data in Github as they belong to others. You will be able to browse the code but cannot really run it due to the data. Is it okay?\n. SmileDemo is imported. Please let me know if you have any questions. Thanks!\n. Thank you very much for the improvements! Have you run the unit test? The test uses USPS data, which you can easily find on internet.\nYou are very welcome to contribute to the project. If you can push your work on SimHash and MinHash, it will be great. BTW, I am not satisfied with my implementation of MPLSH. I will really appreciate if you improve it too. Thanks a lot!\n. Don't worry about it. I will pull your changes and do the tests. Thanks!\n. It is the right dataset, but the format is different. In the LSHTest, add the following to the second line\nparser.setDelimiter(\",\");\nThanks!\n. Sorry, I double checked and it is space delimited. I think that the problem is that there is extra white space at the end of each line. For now, you can just remove all the trailing space manually. It is easy to do in vim or an IDE.\nFor long term, we want to improve the parser. The problem is that it could introduce problems by trimming each line because sometime it is intended for missing values. \n. I tested the changes. On my machine, it is slightly faster in some cases, but slightly slower on other cases. It is hard to tell the difference. I wonder how you find that it is twice faster on toy data. Thanks!\nBut overall I like your changes, which is more concise. Thanks!\n. BTW, the recall rate matches previous values, which is good.\n. The problem is not abou space but missing values. Let's use _ as space.\n1__1\nMeans 1,?,1 where ? Stands for missing value. A trailing space may indicate a missing value.\n. I pull the merge because the code is cleaner. Thanks! Would you like to look into mplsh? The performance is not good so far. \nI will look into the parser again.\n. String[] s = line.split(delimiter, -1); //DelimitedTextParser.java line:250\nCan you replace -1 with 0 and then try your data? I am afraid that this change will break other cases. So please just change it in your local branch right now. Thanks!\n. Thank you very much for looking into mplsh!\n. I mean change -1 to 0 and try again. I just use ? and _ for an example, not about your data.\n. Sorry. You also need change line 362\n. I use XStream (http://xstream.codehaus.org) for serialization of models. Can you have a try? Thanks!\n. Previously, all models support Serializable interface. However, I remove them due to several concerns. For example, the exact format is hard to keep stable, class changes can easily make your serialized data unreadable, reading/writing the data in non-Java code is almost impossible. I am still looking for a simple and flexible solution. Currently, XStream seems good enough and your classes don't need implement any interfaces. In the future, I would like to support PMML for some models.\nI am very open to this question and would like to listen all kinds of suggestions! Thanks!\n. Can you please reopen this ticket? I would like to listen to different voice. Hope more people will see this ticket. Thanks!\n. Thanks!\n. Thanks! I will check out them.\n. Protostuff looks very good. But seems we need create schema for every model. A lof of work.\n. Sounds great! Will learn more about it. If we choose it, would you like to work together on it? Thanks!\n. With runtime schema, it seems that the usage of ProtoStuff is similar with XStream. I prefer not to modify the model object for specific serialization library. So it is better for the users to choose whatever they want. If we finally choose some serialization library, I still want to do it in a separate module and leave smile-core unchanged.\n. A second thought: do we really need backward/forward compatibility? If the model member variables change, it usually means a serious change to the algorithm. It is better to retrain the model rather than importing the old model.\n. Any comments (or complains) to my second thoughts? Thanks!\n. Do you mean this algorithm http://fogo.dbs.ifi.lmu.de/Publikationen/Papers/OPTICS.pdf? Thanks!\n. I will investigate it. Why would you like it? Would you like to join the effort? Thanks!\n. Thanks!\n. Hi Haluk,\nI have been too busy to work on this. Would you like to start this effort? Thank you very much in advance!\n. Great! Thanks a lot!\n. I will update the wiki for this. Thanks!\n. Do you mean searching the optimal hyper parameter by stepping through an interval? We don't have that.\n. With Java 8 or Scala, it should be quite easy to do things like that. Libsvm has a little tool in python for that purpose.\n. Thank you very much for the great work! In smile.sort (of SmileMath) package, we have a HeapSelect class. I guess that it does the same thing as your MaxHeap.\nI am not familiar with SimLSH. What test data did they use in the original paper?\nI totally agree that there are a lot of duplicated code between LSH and MPLSH. I was very lazy and simply copied LSH to MPLSH and then modify it. Please feel free to refactor it.\n. That is the case of hit < k. Note that we add k fake neighbors with 0 distance (line 506 - 512). The purpose of that piece code to remove these fake neighbors. But why starting with 1? I don't remember. Probably should be a more dynamic determined number to filter out all fake neighbors. Maybe it is a bug. Thanks!\n. Thank you very much for finding the bug! I am out of town without easy access to computer. I will check it asap.\n. This is actually not a bug. Note that thee peek methods returns the k-th smallest value seen so far. k is 10 in your test case. The values you insert are 0.0, 0.1 0.2 0.3 0.4 MAX_VALUE MAX_VALUE ....\nOf course, the peek method will return MAX_VALUE in this case.\n. Any updates?\n. Thanks for updates! I will debug line 534 ASAP. So busy in these days.\n. Line 534 is a bug. I fix it. Thanks!\n. Thanks! Are you sure to keep MaxHeap? BTW, I imported SmileNLP, which includes stuffs such as tokenizer. Do you still want to use Google library?\n. If you believe that murmur is very important for this class, I will implement it. It doesn't look complicated. All other algorithms don't depend on third party libraries. Very few applications use LSH but they will have include this google library in their distribution due to this dependency. Not a necessary. Thanks!\n. I import Apache Cassandra's murmur implementation to smile.hash.MurmurHash. It is Apache licensed and thus okay.\n. Can you assume that the input data is already tokenized? This also give the user flexibility to use their own tokenizer (e.g. for some specific language).\n. Thanks for refactors. If you can remove MaxHeap, I will pull in. Thanks!\n. Thanks! The MaxHeap file is still there although it is not in use now. Shall we remove it?\n. Thank you!\n. Fixed. Thanks!\n. can you show your code? Thanks!\n. This is great! Thanks! BTW, you can call MulticoreExecutor.shutdown to kill the thread pool.\n. There are no silly questions. I prefer that people ask questions here so that others can also see them. Just please give it a meaningful title. Thanks!\nFor binary attributes, please use NominalAttribute.\n. We don't have export to arff facility right now. Are you interested to implement one? Thanks!\n. Thank you!\n. ARFF is basically a better csv with header, data types, etc. We probably should support exporting to plain text delimited file, and csv.\n. We support this feature now in Scala api. \nwrite.arff(attributeData, filename). XStream web has tons of examples there.\n. From http://xstream.codehaus.org/javadoc/com/thoughtworks/xstream/XStream.html\nXStream xstream = new XStream();\n String xml = xstream.toXML(myObject); // serialize to XML\n Object myObject2 = xstream.fromXML(xml); // deserialize from XML\n. Check out the package smile.validation. We have all of them (and something else) there.\n. Ok. Let's create a class to calculate confusion matrix and then update these measure classes for multi-class.\n. ConfusionMatrix should be fine. You can make a pull request and we can start from there. Thanks!\n. SmileTestData contains various datasets that I download from internet. As I am not the owner of these datasets, I cannot include them in github.\n. The maven .pom files have no such dependency. Only for eclipse project files. BTW, SmileTestData is only for unit tests. You don't need them to compile the packages. Thanks!\n. Thanks for the great work! The ConfusionMatrix is merged into master.\n. Can you please share your code and samples for debugging? Thanks!\nIt should be fine for groups with different size. Please note that for input matrix, each row is a group. In your case, the input 2-d array should have two rows and each row is for male or female data, respectively. Do you input data with 2 columns?\n. If you do\nnew int[2][10]\nyes, it has the same length for all rows. but you can do\nnew int[2][]\nand then allocate each row separately with different size.\n. Hi SmileTestData contains the datasets that I download from internet. As I don't own them, I cannot publish it in github. They are only used for unit tests. You can remove the dependency in eclipse or create a dummy project there. Thanks!\n. 1. weka\n2. usps ocr\n3. uci machine learning repo\n4. chameleon\n5. many others\n. Are you predicting signal strength based on location? If so, it is doable with SVR.\nBut if you predict location based on signal strength, it is impossible. \n. In general, many locations have same signal strength. It is a one-to-many mapping (from signal strength to locations). It is not a function at all. Note that supervised learning is just to fit a function. \n. It is impossible for a mapping from low-dimensional to higher-dimension to be one-to-one mapping. In case of wlan localization, the intrinsic dimension is one if the signal is restricted to a very narrow area. That is what you want to fit. Then with some other information (e.g. your signal source location and direction), you map decouple it to (x, y). BTW, SVR has no advantage for this low dimensional problem.\n. As I said, you should fit with something like distance instead of (x, y). then you reconstruct (x, y) with additional information.\n. Unfortunately, we don't support it currently. Let me check if we can add this feature quickly. Thanks!\n. Sorry for the issue. I have published SmileNLP 1.0.4 with resource files included. It works with my test app. Please have a try. Thanks!\n. My test app uses sbt:\nlibraryDependencies += \"com.github.haifengl\" % \"smile-nlp\" % \"1.0.4\"\nCan you try it later? Maybe the central repo takes some time to publish it. Thanks!\n. The first phase of RBF networks is just k-means. If you need only that, why not just use k-means?\n. K-means inside rbf networks still needs the number of clusters. The trainer class has a method setNumCenters(int) to set it.\n. Yes, you can use predefined centroids. When creating a RBFNetwork instance, you can provide them in the constructor.\n. I don't understand your question. It is not valid english. sorry.\n. For many partitioning clustering algorithms (e.g. k-means), each cluster has just one centroid.\n. Thanks! Can you make a pull request?\n. No problem at all. Thank you very much for contribution! Really appreciate!\n. @ravikuril our wiki has some tutorials. here is one:\nhttps://github.com/haifengl/smile/wiki/Tutorial:-A-Gentle-Introduction-to-SmileMiner\n. These tests are about random number generators. After generating many random numbers, we calculate the mean and standard deviations and compare them to the true parameters. As you know, there are difference due to the nature of randomness. We use 0.1 as the difference boundary in general. But it may be bigger than that sometimes. If you run it again, the error may (or may not) disappear. I will relax the error boundary so that people feel more comfortable. Thanks!\n. Yes, please. Thank you very much! I recently updated some code with 1.7 features but forgot to update pm.xml.\n. Oh, please don't include the parent part to subproject pom.xml. This parent project is only for building purpose. Add it to subproject will create dependency problems in maven repo.\n. I already update all pom.xml files. Will push it to github soon.\n. it is in master now. I tested it. Thanks!\n. After plotting your graph, try\nPlot.getAxis(1).addLabels\nYour need to provide the labels and locations.\n. scaling and swap were for visualization purpose. these are not the reason. can you please post your code snippet? Thanks!\n. Do you have problems with it?\n. If you have no problems with it any more, I will close this. Thanks!\n. Thank you!\n. As you know, eigen vectors don't care about the sign. Mathematically, nothing is wrong about that. But for visualization, it maybe a problem to you. I don't have a general fix for that. You may have to check it case by case.\n. Multiple data formats are available, depending on problems and algorithms. What is your problem and what algorithm do you try to use?\n. For document classification, I suggest you to use Maximum Entropy classifier (MaxEnt class, http://haifengl.github.io/smile/doc/index.html). Mathematically, it is equivalent to logistic regression. And our implementation supports sparse data. It takes an integer array for features, of which each element is the index of non zero features. Checkout the unit test case for examples.\n. A common mistake in NLP is that use all words in the documents for the features. It is better to do a feature selection first and use this (much) smaller set of words as features (with the Bag class as the helper). BTW, tree based method (e.g. Random Forest) will be very slow if the number of features is too large.\n. Thank you very much! Very nice work.\n. I will merge the pull request. When you get time, it would be very nice to have more documents about the kernels. Thanks!\n. the data type is double. just in case of small random errors. it is most of a safe guard.\n. input data is parsed as double. And I found some data files encode data always as doubles. It better be careful here. Thanks for investigation!\n. what materials?\n. In the code, I list a couple of references in each algorithm's class documents. Check the java doc, which you can find the link in read me. For general introduction, the book \"the elements of statistical learning\" is pretty good.\n. Thanks for fix. Because these methods are static and I assume synch on method should have same effect (it actually syncs on the class object Math). I will research why it causes the problem.\nBTW, I usually create a Random object in parallel algorithms that need random numbers, which avoids the problem and also improves the performance. Can you share your use case that multiple threads learning/clustering? If it is important to you, I will use thread load variable to avoid sync. Thanks!\n. I make the random variable thread local. It should be more efficient and safe. Thanks!\n. Interestingly. I guess that you also use other machine learning tools. How do you feel SMILE? Anything you want us to improve? Compared to others, is it fast enough? Thanks!\n. Thanks! I am working on a shell for SMILE. It is embedded into scala and provides a much simpler api for interactive use.\n. You can use smile.data.parser.DelimitedTextParser\nparser = new DelimitedTextParser();\nparser.setDelimiter(\",\")\nThere are several other parameters to control the data input format. Please check Javadoc (http://haifengl.github.io/smile/doc/index.html) to make sure you use the right settings for your data. Thanks!\n. Yes, it means if the feature is selected. Because it is GA, we print out multiple solutions (and their fitness).\n. Thanks! As you may figure it out yourself, DBSCan use RNNSearch inside. When a Distance or Metric is supplied, we wrap it with LinearSearch or CoverTree. setIdenticalExcluded(true) is important for some uses including DBScan. But for your use case, it is different due to the meaning of JAR_WINKLER_DISTANCE. I will check if I should update  the semantical meaning of setIdenticalExcluded, for example it means exactly same object (not just distance 0). I have to double check before making this change. Thanks!\n. BTW, can you please contribute your JARO_WINKLER_DISTANCE implementation to SMILE? Thank you very much in advance! If you agree, please put it in package smile.math.distance. Thanks again!\n. Nearest neighbor search is used in many density estimation and clustering algorithms. In most situations, distance 0 means that two objects are the same in the feature space. Without excluding identical data points, we may get into troubles in some cases. It is why we do setIdenticalExcluded(true) by default. I think that it is reasonable and also more efficient. But in your use case, many objects have 0 distance even if they are different. Your approach with customized LinearSearch is correct and I am glad it works. The problem comes from the distance definition not DBSCan. I won't change the default behavior otherwise many other algorithms will not work correctly. Thanks!\n. Thanks for explanation. Value equality and reference equality are treated differently in almost all programming languages. I feel that our behavior is correct in this sense. In DBScan, we need to query every object's neighborhood. If we return the query object in the neighborhood, we will get into a self-reference loop. Although our code avoids getting into dead loop, it is still not a desired behavior to including the query object in the neighborhood. It is the purpose of setIdenticalExcluded.\nSo it is clear that Java pool string literal, which is the root cause. As you said, new String() is probably the right solution. With other data, I observe that the result is worse if I do setIdenticalExclude(false). Even though it may work for you, be careful with the clustering results.\nSince you use apache commons for Jar-Winkler distance, it is not necessary to include it here. Thanks!\n. Sure. Go ahead adding comments and make a pull request. Thanks!\n. Btw, not sure your use case. But usually BKTree and CoverTree are much faster than linear search. We do have a very efficient implementation of edit distance. If your data is large, maybe worth of a try.\n. Since your data is not big, it is okay with linear search. I don't know your application, which may need that string distance. Just suggest to try edit distance if it makes sense in your case. It really depends on the use case. Speed is only a secondary consideration for small data.\n. setIdenticalExclude is really an implementation feature, not at this interface level. Actually, different nearest neighbor search data structures and algorithms may have different behaviors. I will clarify this in each implementation. Thanks!\n. I add details comments on setIdenticalExcluded for all nearest neighbor search classes. Thanks!\n. If you strongly need it, we can start to work on it.\n. Sure. You are very welcome to contribute!! Thank you!\n. not yet. i am busy with other issues. are there great needs of this feature?\n. I am not sure if it is a good idea to compare NN and SVM by posteriori probabilities. With right choice of active functions in NN, it calculate posterior probabilities. But it is just an estimation in SVM with Platt's trick.\n. Ok, it is there. I just push PlattScaling to master. To use it, please call\nsvm.trainPlattScaling(x, y);\nafter training a SVM (i.e. after call finish()). Then use\nsvm.predict(x, prob)\nto get the posteriori probabilities.\n. sovled.\n. Thank you very much first of all!! Please check out our manifold learning algorithms and follow their API. For JBlas, what functions will you use? We have a lot of matrix algorithms in smile.math.matrix package. BTW, I am planing to include JBlas or ND4j into smile for native matrix computation. I hope that they will be in separate packages so that the most algorithm packages can be platform independent. Thanks!. added t-SNE to manifold module.. Yes, it is our next major effort.. Yes. It is useful to support several well-known trained models.. An interesting benchmark:\nhttps://blog.monkeylearn.com/sentiment-analysis-apis-benchmark/\n. The website is companioned with 1.1.0 because scala api is only available from 1.1.0. The version 1.1.0 is still in rapid development and the website is not really open to public yet. Sorry for confusion. As you can tell, the website is still in construction. I will remove the link in readme.md. \nBTW, I guess that you are using scala. If so, I strongly recommend you try the new scala api in the shell. In your clone, just type ./smile.sh, which will build and start the shell. Thanks!\n. What do you mean \"fixed partitions\"?\n. smile.validation.CrossValidation is for that. \n. atts(0).valueOf(\"abc\") will return the double value of a nominal string.\n. We are still polishing the scala api. We will publish it to maven so. Stay tuned. Thanks!\n. it is there now. Thanks!\n. data is the package of class attributes, dataset, parser, etc. It is not the directory of testing data. The testing data is in shell/src/universal/data.\n. If the class supports SoftClassifier interface, you can use predict(x, prob) to get the posteriori probabilities. The parameter prob is an array of size # of classes. On output, it will hold the probabilities.\n. Thanks. I don't have a concrete number how many projects are using it. Given the stars and forks, I guess a lot. BTW, we have a website under construction http://haifengl.github.io/smile/ on various features of smile.\nThanks for divulging it in advance!!\n. There are many algorithms printing out progress information. How about changing them to sl4j systematically?\n. I have updated all algorithms using SLF4J for progress, debug, and error messages. In your applications, if you don't include any SLF4J bindings (e.g. log4j, logback, etc.), these will be nop (nothing will be printed). If you include some bindings but don't want see these message, please adjust the log level above INFO. Thanks!\n. Good point. I will look into how to reduce the memory footage. I don't quite get the part of bag[N]. Can you please give more details? Thanks!\nBTW, how large is your data and RAM in the OOM example?\n. By default random forest train fully grown trees. Before we get a solution to reduce the memory usage, you can use maxNodes to control the tree size as a work around. Actually this also often has better accuracy for large data.\n. Sorry, I don't really get your bag example. What's the meaning of bag index and value? Thanks!\n. How about you try the bag idea and see the memory and speed effect? If it is good, please do a pull request. Thanks a lot!\n. Thanks! The accuracy is same? How much memory usage reduced?\n. Thanks! How's speed?\n. Cool. Can you please make a pull request so that we can get your changes? Thanks a lot!\n. Thanks Douglas! How's the result compared to other machine learning libraries in terms of speed, memory usage, accuracy, etc.?\n. Did scimitar-learn discretize numeric values first? Thanks\n. We check every possible split for numeric value (it is up to n possible splits). For large data, it is a big cost. To speed up, we could check only a small number of splits. Says at every 1% step. This should speed up 100 times. The error may be larger though.\n. The variable order is shared by all trees in the forest. It should not cause memory problems if I didn't make a bug. The matrix is large itself though. As large as the input data if all variables are numeric.\n. Btw, why do you treat all variables as numerical? It doesn't make sense to me.\n. Thanks! Can you also contribute your bag implementation back to smile? Thanks again!\n. Cool. Thank you!\n. Hi @myui, can you please make a pull request for your improvements on memory usage? Thank you very much!\n. @myui, how's your test result with checking less splits for numeric value? Thanks!\n. Thanks! I am also working on my bag implementation. Will let you know the performance.\n. What is your network settings?\n. Can you share the setup code for the network? You have only two layers? Thanks!\n. Do you find the reason?\n. It was published yesterday and it shows that staging was successful. But it may take a couple of days to fully visible. Let's wait a little bit. Thanks!\n. Meanwhile, you can download the prebuilt packages to try out the new shell and scala api. Thanks!\n. Finally it shows up in the central maven. Have a try. Thanks!\n. We will expose more API to scala in short terms. Other enhancements are listed in this issues panel. Thanks!\n. Try this\nval window = plot(data)\nwindow.canvas.save(new java.io.File(\"test.png\"))\nwindow.close\n. I see. Maybe we need to add new interface for your use case.\n. As a quick work around, you may pause a little bit before calling save, which make sure the window is properly initialized.\n. Read your first message again. Do you mean #2 approach works? Thanks!\n. What platform do you use?\n. Can you try approach 2 on windows? Swing has al it of issues on Linux and Mac. Thanks!\n. Have you tried pausing a little bit before save?\n. Any updates? Did you try pause a little bit.\n. The problem is that graphics projection depends on the PlotCanvas size. Because the canvas is not materialized, nothing works as expected. If you create a multithreaded program that shows the canvas, save it, and closes automatically, I guess that it may be not too bad for  your use cases.\n. Would you like to take the lead on fixing it? Thanks!\n. We use sbt now. Pom files will be deleted. Please install sbt plugin and import it again.\n. Following @chhh suggestion, I add headless support now. You need to do a git pull and build smile by yourself to use it for now.\nval toy = readTable(\"shell/src/universal/data/classification/toy/toy-train.txt\", Some((new NominalAttribute(\"class\"), 0)))\nval (x, y) = toy.unzipInt\nval canvas = ScatterPlot.plot(x, '.', java.awt.Color.BLACK)\nval headless = new Headless(canvas);\nheadless.pack();\nheadless.setVisible(true);\ncanvas.save(new java.io.File(\"zone.png\"))\n. It works for me. Please let me know if it works for you too. Thanks!\n. BTW, you show run JVM with option -Djava.awt.headless=true\n. Smile has scala api and shell now. It is not just Java libraries any longer. And we will add web services etc.\n. It is not supported. Are you aware of good algorithms for that case? Thanks!\n. Thanks! Would like like to implement GLRM in smile?\n. Thank you very much! Most algorithms are implemented in Java. If you are familiar with Java, it is easy to start. If you prefer scala, it is also very welcome. Don't worry about turnaround time.\n. Pom.xml files are deprecated. We are using sbt now. Sorry for confusion.\nI am on vacation. I will investigate the second problem when I am back.\n. I have removed pom.xml files. Please use sbt for building. Thanks! You will need to setup SBT PGP plugin. Please refer #78 for details. Thanks! \n. Sorry for misleading. You should use smile.sh script. Pkg.sh is for me to publish the project. I will update the webpage.\n. I am on vacation right now and cannot provide detailed info of pgp plugin setup. Sorry.\nCurrently the easiest way for you is probably to remove line 3 and 27 in the build.sbt file.\n. PGP settings for SBT is per user, not per project. So I cannot add it here. But it is very easy to setup it. You can follow the instruction at\nhttp://www.scala-sbt.org/sbt-pgp/\nI assume that you are using the latest sbt. So basically add the following to your ~/.sbt/0.13/plugins/gpg.sbt file:\naddSbtPlugin(\"com.jsuereth\" % \"sbt-pgp\" % \"1.0.0\")\n. I make this plugin dependency local now. Hopefully you don't need to configure it by yourself any more.\n. Please use our scala api write() to save any models. Later you can read it back by read(). Under the hood, we use XStream to serialize the models.  Please checkout the FAQ, which has a section on model serialization. \n. I see. So it is preferred to use Java serialization in spark? Originally we make all models support Serializable interface. But it also have many issues as we discuss in the readme. So we removed that support later. But if there is a need, we definitely can add it back.\n. How about scala pickling?\n. I have enabled all classifiers with Serializable\nhttps://github.com/haifengl/smile/commit/d627d3a6efab668ad498f370c5c781ec41a218e1\nTo use it, you need build by yourself. If everything is fine, we can release a new version.\n. I will release 1.2 to include this soon.\n. @stuz5000 and @quertenmont:  smile 1.2 is just released with these changes. please have a try. thanks!\n. yes, all models are serializable.. Thank you very much! This is really nice!\n. Cool. Thanks!\n. Have you checked out our website? We have a lot of examples there. If you start with the shell, it is much easier to use. You do need to know some scala.\nFor this example, it assumes the data file is packaged into the jar, which is not any more. Instead, you have to open it as a file. If you clone the project, the file is in shell/src/universal/data. The sample data are available in the download package too.\n. You don't have to use scala, although our scala api is much easier to use.\nI have update ReadME on the code example. You need to have the data files download first. They are included in the packages on releases pages.\n. Your particular error is that you call\nthis.getClass().getResourceAsStream()\nin a static function main. There is no this in a static context. First of all, the data file is not in the jar, so you should not use this.getClass().getResourceAsStream(). Instead, use FileInputStream and make sure the path pointing to the correct file.\n. Thank you very much! This is awesome!\n. Thank you! I fixed it. The fix is in the master now.\nBTW, please also check my new project unicorn. Hope it could be used in your organization.\n. Smile.data is pure Java. Check out its doc in java doc.\n. It is actually a bug with scaladoc. If a package has only the package object, it doesn't generate the doc. Btw, this package isn't really about the high level api of java smile.data. It is some implicit helper functions on divulge arrays.\n. Btw, I will add a scala api for matrix computation with runtime optimization. Some of this package object will move there.\n. We already have a lot of matrix computation stuffs in smile.math.matrix. I will add a nice scala api so that you can use them just like write math formula. \n. 120k x 1200 is a big matrix, which takes a lot of memory. How much ram did you allocate to jvm. Maybe it is GC's problem. Btw, did you try this data on R? Thanks!\n. Thanks! It is not a memory issue given your settings. Can you share your data for me to debug?\n. Our QR implementation is a standard algorithm. I am afraid that we may got trapped into numeric stability problems, for example NaN values. Scikit probably uses lapack for QR.\n. Can you send the link to my email haifeng.hli@gmail.com? Thanks!\n. Thanks Luke for sharing data. I ran OLS on this data, very quickly get the error that the matrix is deficient:\nsmile> ols(x,y)\njava.lang.RuntimeException: Matrix is rank deficient.\n  at smile.math.matrix.QRDecomposition.solve(QRDecomposition.java:272)\n  at smile.regression.OLS.(OLS.java:174)\n  at smile.regression.Operators$$anonfun$ols$1.apply(Operators.scala:78)\n  at smile.regression.Operators$$anonfun$ols$1.apply(Operators.scala:78)\n  at smile.util.package$time$.apply(package.scala:49)\n  at smile.regression.Operators$class.ols(Operators.scala:77)\n  at smile.regression.package$.ols(package.scala:30)\n  ... 42 elided\n. I think that this is the right response. Does scikit return a result without any warnings? \n. I tried this data on R:\n\nlm(y ~ x)\n\nCall:\nlm(formula = y ~ x)\nCoefficients:\n(Intercept)          xV2          xV3          xV4          xV5          xV6\n -1.687e-01    4.095e-01   -9.897e-02   -1.193e-01    1.357e-01    5.209e-01\n        xV7          xV8          xV9         xV10         xV11         xV12\n  4.587e-01    6.723e-02    1.514e-01    2.371e+01           NA           NA\n       xV13         xV14         xV15         xV16         xV17         xV18\n         NA           NA   -1.943e-02    6.006e+00    7.137e-02   -3.794e-02\n       xV19         xV20         xV21         xV22         xV23         xV24\n  1.277e-01   -1.528e-01   -5.212e-01           NA           NA           NA\n       xV25         xV26         xV27         xV28         xV29         xV30\n         NA   -1.252e-01    4.160e-01   -4.567e-01    1.170e+00    2.685e-01\n       xV31         xV32         xV33         xV34         xV35         xV36\n -9.798e-02    2.610e-01   -5.236e-01   -1.917e+00   -1.232e+01   -1.115e+03\n       xV37         xV38         xV39         xV40         xV41         xV42\n         NA           NA    1.788e+00    5.770e-01    2.120e-01    2.736e-02\n       xV43         xV44         xV45         xV46         xV47         xV48\n -6.194e-02   -3.168e-01   -2.081e+00   -1.096e-01    3.247e+00           NA\n       xV49         xV50         xV51         xV52         xV53         xV54\n         NA           NA    2.172e+00    1.996e-01    1.203e-01   -1.242e+00\n       xV55         xV56         xV57         xV58         xV59         xV60\n -1.187e+00   -2.550e+00   -1.243e+00    5.093e+01           NA           NA\n       xV61         xV62         xV63         xV64         xV65         xV66\n         NA           NA   -6.792e-02   -2.219e-02   -3.570e-01   -1.781e-01\n       xV67         xV68         xV69         xV70         xV71         xV72\n         NA           NA           NA           NA           NA           NA\n       xV73         xV74         xV75         xV76         xV77         xV78\n         NA   -3.220e-01   -6.726e-03    2.905e-01   -1.963e-03    1.769e+00\n       xV79         xV80         xV81         xV82         xV83         xV84\n         NA           NA           NA           NA           NA           NA\n       xV85         xV86         xV87         xV88         xV89         xV90\n         NA           NA           NA           NA    2.085e-01   -7.914e-01\n       xV91         xV92         xV93         xV94         xV95         xV96\n -8.538e-02   -1.785e+00           NA           NA           NA           NA\n       xV97         xV98         xV99        xV100        xV101        xV102\n         NA           NA           NA           NA           NA   -1.906e-01\n      xV103        xV104        xV105        xV106        xV107        xV108\n -3.237e-02   -7.539e-02   -3.532e-01    4.279e-01           NA           NA\n      xV109        xV110        xV111        xV112        xV113        xV114\n         NA           NA    5.013e-01    7.254e-01   -1.434e+00   -1.580e-01\n      xV115        xV116        xV117        xV118        xV119        xV120\n -4.744e-01   -6.724e-01   -7.261e-01   -8.577e-01    4.295e-01    1.070e+00\n      xV121        xV122        xV123        xV124        xV125        xV126\n  7.081e+01    6.526e+00   -5.940e-01    4.334e-01   -7.251e-02    8.535e-02\n      xV127        xV128        xV129        xV130        xV131        xV132\n  1.656e-01    4.407e-01    1.001e+00    1.412e+00    1.515e+00    1.643e+00\n      xV133        xV134        xV135        xV136        xV137        xV138\n  2.796e+00           NA    1.022e-01    2.268e-02    3.109e-01    4.767e-01\n      xV139        xV140        xV141        xV142        xV143        xV144\n  1.287e-01    6.654e-03   -8.654e-02   -1.037e+00    7.940e+00           NA\n      xV145        xV146        xV147        xV148        xV149        xV150\n         NA           NA   -7.921e-01   -7.592e-01   -8.487e-01   -5.844e-01\n      xV151        xV152        xV153        xV154        xV155        xV156\n -5.882e-01   -3.923e-01   -1.601e+00   -1.720e+00    3.941e+01           NA\n      xV157        xV158        xV159        xV160        xV161        xV162\n         NA   -4.680e-01   -7.047e-01   -5.855e-01    9.691e-02    2.125e+00\n      xV163        xV164        xV165        xV166        xV167        xV168\n  7.015e-01    7.910e-01    8.878e-01    1.374e+00    1.204e+00    2.363e+00\n      xV169        xV170        xV171        xV172        xV173        xV174\n -5.009e+00    2.151e+02    7.348e-01    4.341e-01    1.340e-01    1.373e-01\n      xV175        xV176        xV177        xV178        xV179        xV180\n -5.297e-01   -1.394e-01   -3.315e-01   -1.476e+00   -5.088e+00           NA\n      xV181\n         NA\n. Although R finish the fitting without any error or warnings, many coefficients are NA. The model is essentially useless.\n. A matrix could be of rank deficient because of collinear (or as simple as some column is zero) even if there are more rows than columns. For OLS, we always have more samples than dimensions (that is why we do least squares). But rand deficient is a common reason of poor fitting for OLS. The easy way to verify is to calculate the singular values (both R and Matlab can do it, smile supports that too). BTW, which version smile, java, and hardware do you use? I am using the master branch (but there is no changes to these basic methods for long time). Thanks!\n. 1.64e-13 is very close to zero, which does indicate rand deficient. you will never get exact 0 singular values on float computing.\nduplicate rows are not problems. For least squares, collinear is about columns.\nR also uses LAPACK and returns a model with NA parameters, which means the model cannot be applied. Did you observe similar thing with NumPy?\nSmile returns quickly on the data you emailed me. I don't why it is very slow for you. Can you provide more information about your system? Such as JVM, smile version, hardware, etc. Thanks!\nPS, smile is surely slower than LAPACK for this task. LAPACK is written in FORTRAN and highly optimized with years tuning.\n. what is the output of \"java -version\" on my machine? I am afraid that you are not using oracle jdk.\n. Well, that explains a lot. OpenJDK JVM misses many optimizations. After removing the zero columns, I can quickly fit the model on my laptop.\n. val data = readTable(\"data\", Some((new NumericAttribute(\"y\"), 0)))\nval (x, y) = data.unzipDouble\n// Print out the summary of data. Obivously many constant columns\ndata.summary\n// Column min and max\nval min = Math.colMin(x)\nval max = Math.colMax(x)\n// filter out constant columns\nval cols = (0 until min.length).filter { i => min(i) != max(i) }\n// Hopefully not rank deficient\nval x2 = x.col(cols: _*)\nval start = System.currentTimeMillis\nval model = ols(x2, y)\nval end = System.currentTimeMillis\n// eclipsed time\n(end - start) / 1000.0\n. The above is the script I used to fit your data. Here is the model summary:\nsmile> println(model)\nLinear Model:\nResiduals:\n           Min          1Q      Median          3Q         Max\n      -11.0241     -0.5573     -0.0010      0.5443     20.1205\nCoefficients:\n            Estimate        Std. Error        t value        Pr(>|t|)\nIntercept     1.4630            0.0575        25.4472          0.0000 \nVar 1       230.9277          108.2488         2.1333          0.0329 \nVar 2         0.0186            0.0236         0.7860          0.4318 \nVar 3         4.5291            0.2927        15.4722          0.0000 \nVar 4         0.0946            0.0106         8.9606          0.0000 \nVar 5         0.4202            0.0166        25.3531          0.0000 \nVar 6        -0.2238            0.0210       -10.6375          0.0000 \nVar 7         0.5990            0.0526        11.3951          0.0000 \nVar 8         0.0970            0.0246         3.9448          0.0001 \nVar 9        -0.0258            0.0189        -1.3646          0.1724 \nVar 10       -0.1041            0.0110        -9.4750          0.0000 \nVar 11       -0.1278            0.0116       -11.0109          0.0000 \nVar 12       -0.1323            0.0180        -7.3606          0.0000 \nVar 13        0.0134            0.0108         1.2426          0.2140 \nVar 14       -0.1563            0.0091       -17.2506          0.0000 \nVar 15       -0.0613            0.0090        -6.8372          0.0000 \nVar 16       -0.0436            0.0110        -3.9687          0.0001 \nVar 17       -0.0240            0.0110        -2.1884          0.0286 \nVar 18        0.0039            0.0103         0.3782          0.7053 \nVar 19        0.0407            0.0120         3.4028          0.0007 \nVar 20        0.1061            0.0134         7.9049          0.0000 \nVar 21        0.0768            0.0157         4.8871          0.0000 \nVar 22        0.1590            0.0231         6.8770          0.0000 \nVar 23        0.1866            0.0342         5.4571          0.0000 \nVar 24        0.2983            0.0479         6.2275          0.0000 \nVar 25        0.3623            0.0771         4.7010          0.0000 \nVar 26        0.3121            0.1118         2.7910          0.0053 \nVar 27        0.4906            0.2022         2.4264          0.0153 \nVar 28        1.3943            0.3153         4.4217          0.0000 \nVar 29        4.0306            5.4185         0.7439          0.4570 \nVar 30       -2.4711            0.4292        -5.7576          0.0000 \nVar 31        0.3044            0.1881         1.6183          0.1056 \nVar 32       -0.8267            0.6034        -1.3699          0.1707 \nVar 33        0.9129            0.1837         4.9684          0.0000 \nVar 34       -0.2590            0.1460        -1.7741          0.0760 .\nVar 35        1.9595            0.3186         6.1507          0.0000 \nVar 36        0.2317            0.1276         1.8167          0.0693 .\nVar 37        0.0137            0.1910         0.0719          0.9427 \nVar 38       -0.2433            0.1975        -1.2315          0.2181 \nVar 39        0.0493            0.2362         0.2087          0.8347 \nVar 40       -0.0459            0.1871        -0.2454          0.8061 \nVar 41        0.3964            0.1910         2.0753          0.0380 \nVar 42        0.0472            0.1351         0.3493          0.7269 \nVar 43        0.7308            0.1307         5.5927          0.0000 \nVar 44       -0.4392            0.1029        -4.2701          0.0000 \nVar 45        0.1889            0.1211         1.5592          0.1189 \nVar 46        0.4101            0.1855         2.2107          0.0271 *\nVar 47        0.6162            0.2025         3.0422          0.0023 \nVar 48        0.0153            0.1266         0.1209          0.9038 \nVar 49       -0.0529            0.1354        -0.3904          0.6962 \nVar 50        0.1421            0.1263         1.1256          0.2603 \nVar 51        0.4159            0.1354         3.0715          0.0021 \nVar 52        0.0498            0.1262         0.3945          0.6932 \nVar 53       -0.1372            0.1495        -0.9173          0.3590 \nVar 54        0.0119            0.1607         0.0739          0.9411 \nVar 55        0.2754            0.1737         1.5851          0.1129 \nVar 56       -0.1507            0.1439        -1.0473          0.2950 \nVar 57       -0.8649            0.3184        -2.7164          0.0066 \nVar 58        0.1269            0.2017         0.6294          0.5291 \nVar 59       -0.1398            0.2252        -0.6209          0.5347 \nVar 60       -0.0274            0.1282        -0.2136          0.8308 \nVar 61       -0.7757            0.2187        -3.5469          0.0004 \nVar 62       -0.4186            0.1377        -3.0409          0.0024 *\nVar 63       -0.2772            0.2431        -1.1403          0.2542 \nVar 64       -0.3090            0.2418        -1.2778          0.2013 \nVar 65       -0.5493            0.3094        -1.7756          0.0758 .\nVar 66       -0.4370            0.3173        -1.3775          0.1684 \nVar 67       18.4904           11.2558         1.6427          0.1004 \nVar 68        0.2496            0.0392         6.3617          0.0000 \nVar 69       -0.2817            0.0505        -5.5754          0.0000 \nVar 70        0.5768            0.0780         7.3919          0.0000 \nVar 71       48.0317            8.6565         5.5486          0.0000 \nVar 72        0.2642            0.0484         5.4636          0.0000 \nVar 73       -0.0553            0.0206        -2.6823          0.0073 \nVar 74       -0.0891            0.0182        -4.9116          0.0000 \nVar 75        0.1692            0.0276         6.1364          0.0000 \nVar 76       -0.0494            0.0144        -3.4413          0.0006 \nVar 77        0.0170            0.0115         1.4810          0.1386 \nVar 78       -0.1324            0.0105       -12.6250          0.0000 \nVar 79       -0.1510            0.0190        -7.9306          0.0000 \nVar 80        0.0788            0.0215         3.6670          0.0002 \nVar 81        0.2407            0.0223        10.7775          0.0000 \nVar 82       -0.0989            0.0141        -7.0159          0.0000 \nVar 83        0.1531            0.0173         8.8619          0.0000 \nVar 84        0.0939            0.0120         7.8522          0.0000 \nVar 85        0.0296            0.0117         2.5265          0.0115 *\nVar 86        0.0494            0.0099         4.9773          0.0000 \nVar 87        0.0654            0.0120         5.4343          0.0000 \nVar 88        0.1052            0.0139         7.5438          0.0000 \nVar 89        0.0952            0.0131         7.2738          0.0000 \nVar 90        0.0286            0.0146         1.9536          0.0508 .\nVar 91        0.1620            0.0204         7.9450          0.0000 \nVar 92        0.0601            0.0188         3.1878          0.0014 *\nVar 93        0.0622            0.0204         3.0470          0.0023 \nVar 94        0.1048            0.0324         3.2308          0.0012 \nVar 95       -0.0966            0.0551        -1.7539          0.0795 .\nVar 96       -0.0180            0.0821        -0.2188          0.8268 \nVar 97       -0.1836            0.1612        -1.1394          0.2545 \nVar 98       -0.3031            0.2209        -1.3721          0.1700 \nVar 99       -0.8797            0.3451        -2.5487          0.0108 \nVar 100      -9.1354            1.8460        -4.9487          0.0000 \nVar 101       1.5603            0.0298        52.3572          0.0000 \nVar 102       2.2920            0.1395        16.4259          0.0000 \nVar 103       0.1980            0.0187        10.5782          0.0000 \nVar 104       0.2153            0.0255         8.4461          0.0000 \nVar 105       0.4177            0.0172        24.2909          0.0000 \nVar 106       0.2372            0.0173        13.7261          0.0000 \nVar 107       0.1776            0.0138        12.8671          0.0000 \nVar 108       3.0448            0.1111        27.4046          0.0000 \nVar 109      -0.1091            0.0426        -2.5630          0.0104 \nVar 110      -0.5512            0.1100        -5.0130          0.0000 \nVar 111      -0.0864            0.0184        -4.6837          0.0000 \nVar 112       0.1986            0.0177        11.2308          0.0000 \nVar 113      -0.0755            0.0167        -4.5089          0.0000 \nVar 114       0.0537            0.0151         3.5641          0.0004 \nVar 115       0.0725            0.0121         5.9919          0.0000 \nVar 116      -0.0887            0.0124        -7.1465          0.0000 \nVar 117      -0.0458            0.0137        -3.3576          0.0008 \nVar 118      -0.0959            0.0131        -7.3153          0.0000 \nVar 119      -0.0368            0.0121        -3.0443          0.0023 \nVar 120      -0.0661            0.0124        -5.3215          0.0000 \nVar 121      -0.0171            0.0127        -1.3530          0.1760 \nVar 122       0.1453            0.0130        11.1819          0.0000 \nVar 123      -0.0517            0.0160        -3.2402          0.0012 \nVar 124      -0.1903            0.0194        -9.8177          0.0000 \nVar 125      -0.2861            0.0253       -11.2913          0.0000 \nVar 126      -0.3542            0.0373        -9.4951          0.0000 \nVar 127      -0.6925            0.0590       -11.7327          0.0000 \nVar 128      -0.6490            0.1162        -5.5866          0.0000 \nVar 129      -0.6553            0.1349        -4.8577          0.0000 \nVar 130       1.5980            1.7018         0.9390          0.3477 \nVar 131       2.4145            0.4818         5.0117          0.0000 \nVar 132       0.7843            0.0125        62.6378          0.0000 \nVar 133       3.0597            0.0649        47.1689          0.0000 \nVar 134      -0.0293            0.0148        -1.9760          0.0482 \nVar 135       0.1119            0.0122         9.1996          0.0000 \nVar 136       0.2394            0.0122        19.6213          0.0000 \nVar 137       0.1659            0.0605         2.7405          0.0061 *\nVar 138     441.4116         1872.3619         0.2358          0.8136 \nVar 139      -0.4209            0.0770        -5.4684          0.0000 \nVar 140       0.1779            0.0180         9.8676          0.0000 \nVar 141       0.1103            0.0267         4.1280          0.0000 \nVar 142      -0.0197            0.0248        -0.7966          0.4257 \nVar 143      -0.0316            0.0177        -1.7888          0.0736 .\nVar 144       0.2720            0.0144        18.9379          0.0000 \nVar 145       0.0618            0.0118         5.2620          0.0000 \nVar 146       0.0758            0.0092         8.2108          0.0000 \nVar 147       0.0217            0.0100         2.1643          0.0304 \nVar 148       0.0646            0.0104         6.2131          0.0000 \nVar 149       0.0454            0.0104         4.3473          0.0000 \nVar 150      -0.0998            0.0109        -9.1751          0.0000 \nVar 151      -1.0689            0.0534       -20.0273          0.0000 \nVar 152      -0.3062            0.0191       -16.0317          0.0000 \nVar 153      -0.2395            0.0163       -14.7179          0.0000 \nVar 154      -0.3579            0.0203       -17.6098          0.0000 \nVar 155      -0.3377            0.0204       -16.5471          0.0000 \nVar 156      -0.2483            0.0282        -8.8199          0.0000 \nVar 157      -0.3350            0.0346        -9.6873          0.0000 \nVar 158      -0.3729            0.0326       -11.4338          0.0000 \nVar 159      -0.6318            0.0375       -16.8518          0.0000 \nVar 160      -0.6944            0.0481       -14.4240          0.0000 \nVar 161      -0.8661            0.0575       -15.0715          0.0000 \nVar 162      -0.8338            0.0762       -10.9388          0.0000 \nVar 163      -0.5674            0.1115        -5.0886          0.0000 \nVar 164      -0.7079            0.1957        -3.6169          0.0003 \nVar 165      -1.0854            0.2257        -4.8101          0.0000 \nVar 166       1.3247            2.2573         0.5868          0.5573 \nVar 167      -1.3093            3.6342        -0.3603          0.7186 \nVar 168      -0.8052            0.2481        -3.2460          0.0012 *\nVar 169      -0.5214            0.3825        -1.3633          0.1728 \nVar 170       0.5448            0.6787         0.8027          0.4221 \nVar 171      -0.3114            0.1789        -1.7409          0.0817 .\nVar 172      -0.5382            0.5066        -1.0624          0.2881 \nVar 173      -0.2431            1.1207        -0.2169          0.8283 \nVar 174       1.5874            1.5090         1.0520          0.2928 \nVar 175       2.0706            0.6458         3.2061          0.0013 \nVar 176     -55.3814           34.2143        -1.6187          0.1055 \nVar 177       0.3192            0.2837         1.1251          0.2605 \nVar 178       0.2467            1.3413         0.1839          0.8541 \nVar 179      -0.4304            0.1881        -2.2877          0.0222 \nVar 180      -0.0326            0.5341        -0.0611          0.9513 \nVar 181      -0.8445            5.5658        -0.1517          0.8794 \nVar 182      -0.1187            0.5394        -0.2200          0.8259 \nVar 183       0.1852            0.2832         0.6540          0.5131 \nVar 184   -1159.0040          711.3195        -1.6294          0.1032 \nVar 185      -0.2962            0.7520        -0.3939          0.6936 \nVar 186      -0.5719            0.2249        -2.5427          0.0110 \nVar 187      -0.3098            0.1846        -1.6779          0.0934 .\nVar 188       0.0382            0.1086         0.3521          0.7248 \nVar 189      -0.2124            0.1156        -1.8367          0.0663 .\nVar 190      -0.4035            0.0978        -4.1248          0.0000 \nVar 191      -0.0707            0.1302        -0.5428          0.5872 \nVar 192      -0.1736            0.1471        -1.1799          0.2380 \nVar 193      -0.3062            0.1828        -1.6748          0.0940 .\nVar 194       0.9217            0.2142         4.3029          0.0000 \nVar 195       0.0610            0.1757         0.3474          0.7283 \nVar 196      -0.3460            0.1918        -1.8045          0.0712 .\nVar 197       0.1925            0.1679         1.1466          0.2516 \nVar 198      -0.0972            0.0989        -0.9831          0.3256 \nVar 199       0.1409            0.1180         1.1937          0.2326 \nVar 200      -0.0310            0.1227        -0.2530          0.8003 \nVar 201      -0.1010            0.1536        -0.6574          0.5109 \nVar 202      -0.0257            0.1462        -0.1756          0.8606 \nVar 203       0.0234            0.1887         0.1242          0.9012 \nVar 204      -0.1218            0.1353        -0.8998          0.3683 \nVar 205      -0.0364            0.2081        -0.1749          0.8612 \nVar 206       0.1307            0.0925         1.4128          0.1577 \nVar 207      -0.4160            0.1000        -4.1594          0.0000 \nVar 208       0.1155            0.0846         1.3664          0.1718 \nVar 209       0.1285            0.1026         1.2524          0.2104 \nVar 210       0.0831            0.1033         0.8049          0.4209 \nVar 211      -0.0335            0.1044        -0.3205          0.7486 \nVar 212       0.1520            0.1014         1.4996          0.1337 \nVar 213       0.1752            0.1267         1.3823          0.1669 \nVar 214       0.2044            0.1138         1.7970          0.0723 .\nVar 215       0.5428            0.1272         4.2668          0.0000 \nVar 216       0.3291            0.1247         2.6383          0.0083 \nVar 217       0.1690            0.1077         1.5693          0.1166 \nVar 218       0.7193            0.1146         6.2788          0.0000 \nVar 219       0.4295            0.1779         2.4145          0.0158 \nVar 220       0.4494            0.2797         1.6065          0.1082 \nVar 221      -6.9510            8.3400        -0.8335          0.4046 \nVar 222       1.6754            0.5784         2.8965          0.0038 \nVar 223       0.0543            0.1476         0.3678          0.7130 \nVar 224      -1.5624            0.3013        -5.1861          0.0000 \nVar 225     -31.9835           17.7456        -1.8023          0.0715 .\nVar 226       2.5167            0.3881         6.4839          0.0000 \nVar 227      -0.0323            0.1221        -0.2644          0.7915 \nVar 228       0.2823            0.2375         1.1883          0.2347 \nVar 229       0.2146            0.3054         0.7025          0.4823 \nVar 230      -0.1881            0.3898        -0.4826          0.6294 \nVar 231      -0.0879            0.1535        -0.5724          0.5671 \nVar 232      -0.0834            0.1734        -0.4809          0.6306 \nVar 233       0.0958            0.0929         1.0314          0.3023 \nVar 234      -0.4702            0.2552        -1.8422          0.0655 .\nVar 235       0.7133            0.1965         3.6300          0.0003 \nVar 236      -0.2968            0.2353        -1.2612          0.2072 \nVar 237       0.0221            0.1503         0.1474          0.8828 \nVar 238       0.2132            0.1862         1.1447          0.2523 \nVar 239      -0.0628            0.1356        -0.4628          0.6435 \nVar 240       0.4104            0.2727         1.5048          0.1324 \nVar 241      -0.2650            0.1352        -1.9600          0.0500 .\nVar 242      -0.3257            0.2213        -1.4722          0.1410 \nVar 243      -0.0563            0.1396        -0.4033          0.6868 \nVar 244      -1.1349            0.3723        -3.0481          0.0023 *\nVar 245      -0.4815            0.1319        -3.6508          0.0003 \nVar 246      10.8584            3.1042         3.4979          0.0005 \nVar 247      -6.0321            4.1645        -1.4484          0.1475 \nVar 248      -0.5189            0.2378        -2.1821          0.0291 \nVar 249       0.3834            0.4551         0.8425          0.3995 \nVar 250      -0.9750            6.4835        -0.1504          0.8805 \nVar 251      -0.6839            0.3298        -2.0740          0.0381 *\nVar 252       0.4169            0.1048         3.9784          0.0001 \nVar 253       0.2494            0.2587         0.9639          0.3351 \nVar 254       0.0720            0.2986         0.2412          0.8094 \nVar 255       0.6008            0.4818         1.2470          0.2124 \nVar 256      -0.5119            0.2466        -2.0755          0.0379 *\nVar 257      -0.0314            0.3465        -0.0907          0.9277 \nVar 258      -0.3122            0.1144        -2.7298          0.0063 \nVar 259       0.1446            0.3077         0.4699          0.6384 \nVar 260      -0.5283            0.2098        -2.5179          0.0118 \nVar 261      -0.1913            0.3501        -0.5466          0.5847 \nVar 262      -0.4490            0.2237        -2.0070          0.0448 \nVar 263       0.2557            0.2555         1.0010          0.3168 \nVar 264      -0.0154            0.2094        -0.0737          0.9412 \nVar 265       0.0024            0.2906         0.0082          0.9934 \nVar 266       0.0753            0.3338         0.2255          0.8216 \nVar 267       0.4456            0.4410         1.0102          0.3124 \nVar 268      -1.5315            0.1921        -7.9724          0.0000 \nVar 269      -0.2350            0.3179        -0.7392          0.4598 \nVar 270      -0.0970            0.1377        -0.7041          0.4814 \nVar 271      -0.1391            0.2757        -0.5044          0.6140 \nVar 272      -0.1408            0.1119        -1.2579          0.2084 \nVar 273       0.1948            0.2798         0.6960          0.4864 \nVar 274      -0.3549            0.3213        -1.1046          0.2694 \nVar 275      -0.1932            0.2331        -0.8286          0.4073 \nVar 276      -0.1007            0.1655        -0.6084          0.5429 \nVar 277      -0.0337            0.3520        -0.0957          0.9238 \nVar 278      -0.1240            0.1788        -0.6932          0.4882 \nVar 279       0.0295            0.2764         0.1069          0.9149 \nVar 280       0.2398            0.2116         1.1335          0.2570 \nVar 281      -1.4096            7.4958        -0.1880          0.8508 \nVar 282      -5.5626            0.8175        -6.8044          0.0000 \nVar 283       0.1344            0.0217         6.1974          0.0000 \nVar 284       0.5342            0.0217        24.5877          0.0000 \nVar 285      -0.1078            0.0124        -8.6590          0.0000 \nVar 286       0.0160            0.0134         1.1940          0.2325 \nVar 287       0.0126            0.0095         1.3290          0.1839 \nVar 288       0.0032            0.0242         0.1337          0.8936 \nVar 289       0.1678            0.0101        16.5911          0.0000 \nVar 290       0.2130            0.0085        24.9566          0.0000 \nVar 291       0.2400            0.0122        19.6594          0.0000 \nVar 292       0.1675            0.0093        18.0783          0.0000 \nVar 293       0.0631            0.0086         7.3184          0.0000 \nVar 294       0.0748            0.0081         9.2340          0.0000 \nVar 295       0.0455            0.0079         5.7226          0.0000 \nVar 296       0.0611            0.0081         7.5534          0.0000 \nVar 297      -0.0386            0.0099        -3.8817          0.0001 \nVar 298       0.0437            0.0090         4.8729          0.0000 \nVar 299       0.1022            0.0091        11.1746          0.0000 \nVar 300       0.0101            0.0089         1.1318          0.2577 \nVar 301      -0.0569            0.0112        -5.0813          0.0000 \nVar 302       0.0046            0.0118         0.3864          0.6992 \nVar 303      -0.0641            0.0101        -6.3203          0.0000 \nVar 304      -0.1655            0.0100       -16.5072          0.0000 \nVar 305      -0.1578            0.0133       -11.8268          0.0000 \nVar 306      -0.3374            0.0125       -26.9624          0.0000 \nVar 307      -0.4252            0.0127       -33.3584          0.0000 \nVar 308      -0.4710            0.0132       -35.6199          0.0000 \nVar 309      -0.5940            0.0187       -31.8095          0.0000 \nVar 310      -0.5811            0.0305       -19.0644          0.0000 \nVar 311      -0.6326            0.0505       -12.5367          0.0000 \nVar 312      -0.4535            0.1202        -3.7729          0.0002 \nVar 313       0.0838            0.3111         0.2695          0.7876 \nVar 314       8.4083            2.3818         3.5302          0.0004 \nVar 315      -3.4641            0.9119        -3.7989          0.0001 \nVar 316       0.1738            0.0133        13.0266          0.0000 \nVar 317      13.4024            0.3958        33.8603          0.0000 \nVar 318     -30.3177            6.3114        -4.8036          0.0000 \nVar 319      -2.9137            0.4620        -6.3070          0.0000 \nVar 320       0.5653            0.3122         1.8105          0.0702 .\nVar 321       0.7009            0.1918         3.6533          0.0003 \nVar 322      -0.3840            0.0474        -8.0992          0.0000 \nVar 323      -0.1336            0.0564        -2.3686          0.0179 \nVar 324      -0.0056            0.0114        -0.4933          0.6218 \nVar 325       0.1042            0.0119         8.7655          0.0000 \nVar 326       0.0068            0.0120         0.5636          0.5730 \nVar 327       0.0168            0.0143         1.1751          0.2400 \nVar 328      -0.1223            0.0142        -8.5883          0.0000 \nVar 329      -0.1172            0.0227        -5.1699          0.0000 \nVar 330      -0.2218            0.0212       -10.4490          0.0000 \nVar 331      -0.1912            0.0198        -9.6448          0.0000 \nVar 332      -0.1660            0.0149       -11.1360          0.0000 \nVar 333      -0.0948            0.0121        -7.8114          0.0000 \nVar 334      -0.0002            0.0108        -0.0140          0.9888 \nVar 335       0.0197            0.0108         1.8261          0.0678 .\nVar 336      -0.0290            0.0111        -2.6091          0.0091 \nVar 337      -0.0253            0.0133        -1.8945          0.0582 .\nVar 338      -0.0620            0.0108        -5.7192          0.0000 \nVar 339       0.0550            0.0119         4.6291          0.0000 \nVar 340      -0.0446            0.0154        -2.9043          0.0037 \nVar 341       0.1323            0.0123        10.7968          0.0000 \nVar 342       0.0748            0.0106         7.0233          0.0000 \nVar 343       0.0352            0.0110         3.1843          0.0015 \nVar 344       0.1858            0.0108        17.2711          0.0000 \nVar 345       0.1466            0.0110        13.2841          0.0000 \nVar 346       0.1405            0.0115        12.1886          0.0000 \nVar 347       0.0370            0.0118         3.1429          0.0017 *\nVar 348       0.0036            0.0109         0.3342          0.7382 \nVar 349       0.0574            0.0117         4.9017          0.0000 \nVar 350       0.1727            0.0114        15.1089          0.0000 \nVar 351       0.1770            0.0125        14.1615          0.0000 \nVar 352       0.2658            0.0133        20.0379          0.0000 \nVar 353       0.2439            0.0144        16.9922          0.0000 \nVar 354       0.5084            0.0201        25.3457          0.0000 \nVar 355       0.4896            0.0270        18.1030          0.0000 \nVar 356       0.9615            0.0472        20.3551          0.0000 \nVar 357       1.0738            0.0980        10.9564          0.0000 \nVar 358       1.3237            0.1371         9.6567          0.0000 \nVar 359       0.9284            0.1824         5.0901          0.0000 \nVar 360       1.4475            0.4339         3.3358          0.0009 \nVar 361      -1.1372            4.6823        -0.2429          0.8081 \nVar 362      -0.9246            0.3262        -2.8349          0.0046 *\nVar 363      -0.0125            0.0588        -0.2132          0.8312 \nVar 364       0.4502            0.0566         7.9586          0.0000 \nVar 365       0.1125            0.0280         4.0167          0.0001 \nVar 366       1.6832            0.1065        15.7997          0.0000 \nVar 367       0.9661            0.2034         4.7499          0.0000 \nVar 368       0.5704            0.0565        10.1036          0.0000 \nVar 369       0.6114            0.0260        23.4913          0.0000 \nVar 370      -0.0331            0.0181        -1.8305          0.0672 .\nVar 371       0.2017            0.0429         4.7063          0.0000 \nVar 372       0.0741            0.0238         3.1097          0.0019 \nVar 373      -0.2296            0.0254        -9.0563          0.0000 \nVar 374      -0.1441            0.0151        -9.5502          0.0000 \nVar 375      -0.0053            0.0310        -0.1710          0.8642 \nVar 376       0.4055            0.0250        16.2113          0.0000 \nVar 377       0.0734            0.0259         2.8368          0.0046 *\nVar 378      -0.1381            0.0229        -6.0212          0.0000 \nVar 379      -0.3788            0.0509        -7.4470          0.0000 \nVar 380      -0.0563            0.0386        -1.4600          0.1443 \nVar 381      -0.1117            0.0531        -2.1024          0.0355 \nVar 382      -0.5015            0.0510        -9.8254          0.0000 \nVar 383      -0.2141            0.0810        -2.6420          0.0082 \nVar 384      -0.3949            0.0713        -5.5393          0.0000 \nVar 385      -0.6335            0.1730        -3.6626          0.0002 \nVar 386      -0.9620            0.1368        -7.0318          0.0000 \nVar 387       4.9957            1.7360         2.8777          0.0040 *\nVar 388     -85.3715           12.1993        -6.9981          0.0000 \nVar 389       1.8593            0.1117        16.6436          0.0000 \nVar 390       0.0778            0.1300         0.5983          0.5496 \nVar 391       0.0025            0.0303         0.0817          0.9349 \nVar 392      -0.4441            0.0861        -5.1553          0.0000 \nVar 393      -0.9635            0.1690        -5.7009          0.0000 \nVar 394      -0.6194            0.5089        -1.2169          0.2236 \nVar 395      -8.7251            8.3045        -1.0507          0.2934 \nVar 396      -0.5815            0.0593        -9.7980          0.0000 \nVar 397      -0.0655            0.0144        -4.5522          0.0000 \nVar 398       0.0255            0.0260         0.9783          0.3279 \nVar 399      -0.0811            0.0134        -6.0440          0.0000 \nVar 400      -0.3023            0.0141       -21.4457          0.0000 \nVar 401      -0.3581            0.0106       -33.8395          0.0000 \nVar 402      -0.0780            0.0201        -3.8807          0.0001 \nVar 403      -0.0613            0.0158        -3.8773          0.0001 \nVar 404      -0.4015            0.0157       -25.6335          0.0000 \nVar 405      -0.4220            0.0156       -26.9746          0.0000 \nVar 406      -0.4725            0.0252       -18.7483          0.0000 \nVar 407      -0.2034            0.0237        -8.5890          0.0000 \nVar 408      -0.5261            0.0313       -16.8221          0.0000 \nVar 409      -0.3602            0.0274       -13.1529          0.0000 \nVar 410      -0.2947            0.0401        -7.3513          0.0000 \nVar 411      -0.8911            0.0469       -18.9813          0.0000 \nVar 412      -0.7269            0.0793        -9.1610          0.0000 \nVar 413      -0.0939            0.1005        -0.9345          0.3500 \nVar 414       0.2099            0.2139         0.9809          0.3266 \nVar 415      -0.8145            0.2826        -2.8826          0.0039 \nVar 416      25.6728           10.5823         2.4260          0.0153 \nVar 417      13.1196          116.2027         0.1129          0.9101 \nVar 418      -0.1497            0.0195        -7.6845          0.0000 \nVar 419      -0.7910            0.3179        -2.4880          0.0128 \nVar 420     -10.8770            0.9840       -11.0544          0.0000 \nVar 421       0.6308            0.2926         2.1556          0.0311 \nVar 422       0.5786            0.0730         7.9252          0.0000 \nVar 423       0.1884            0.0376         5.0133          0.0000 \nVar 424      -0.2013            0.0099       -20.3671          0.0000 \nVar 425      -0.0178            0.0120        -1.4814          0.1385 \nVar 426      -0.0623            0.0109        -5.7122          0.0000 \nVar 427      -0.0248            0.0115        -2.1611          0.0307 \nVar 428      -0.0797            0.0139        -5.7186          0.0000 \nVar 429      -0.0845            0.0190        -4.4405          0.0000 \nVar 430      -0.1532            0.0310        -4.9463          0.0000 \nVar 431      -0.2576            0.0124       -20.7293          0.0000 \nVar 432      -0.2043            0.0113       -18.1560          0.0000 \nVar 433      -0.0073            0.0091        -0.7986          0.4245 \nVar 434      -0.0482            0.0087        -5.5126          0.0000 \nVar 435       0.0243            0.0094         2.5912          0.0096 *\nVar 436       0.0935            0.0093        10.0308          0.0000 \nVar 437      -0.0366            0.0094        -3.8990          0.0001 \nVar 438       0.1714            0.0099        17.2532          0.0000 \nVar 439       0.1936            0.0088        21.8863          0.0000 \nVar 440       0.1085            0.0087        12.4691          0.0000 \nVar 441       0.1193            0.0090        13.2681          0.0000 \nVar 442       0.3107            0.0091        34.2000          0.0000 \nVar 443       0.2844            0.0096        29.6555          0.0000 \nVar 444       0.3692            0.0098        37.5630          0.0000 \nVar 445       0.2608            0.0129        20.2577          0.0000 \nVar 446       0.2734            0.0142        19.2596          0.0000 \nVar 447       0.4791            0.0127        37.7910          0.0000 \nVar 448       0.5991            0.0158        37.8357          0.0000 \nVar 449       0.6478            0.0230        28.1913          0.0000 \nVar 450       0.9558            0.0329        29.0725          0.0000 \nVar 451       1.4188            0.0627        22.6439          0.0000 \nVar 452       1.2997            0.1126        11.5443          0.0000 \nVar 453       1.0688            0.1939         5.5113          0.0000 \nVar 454       1.9206            0.2862         6.7116          0.0000 \nVar 455      -1.4669            2.4001        -0.6112          0.5411 \nVar 456       0.4735            0.1349         3.5106          0.0004 \nVar 457       0.5588            0.0172        32.5795          0.0000 \nVar 458       0.0609            0.0329         1.8522          0.0640 .\nVar 459       0.3946            0.1189         3.3179          0.0009 \nVar 460      -0.2073            0.0406        -5.1105          0.0000 \nVar 461       0.4840            0.0134        36.0418          0.0000 \nVar 462       0.2124            0.0151        14.0866          0.0000 \nVar 463      -0.3112            0.0123       -25.2636          0.0000 \nVar 464       0.2155            0.0204        10.5378          0.0000 \nVar 465      -0.1596            0.0143       -11.1842          0.0000 \nVar 466      -0.2368            0.0186       -12.6959          0.0000 \nVar 467      -0.1618            0.0195        -8.3161          0.0000 \nVar 468      -0.0550            0.0241        -2.2857          0.0223 \nVar 469      -0.1570            0.0237        -6.6252          0.0000 \nVar 470      -0.4621            0.0360       -12.8335          0.0000 \nVar 471      -0.3842            0.0385        -9.9672          0.0000 \nVar 472      -0.4520            0.0552        -8.1942          0.0000 \nVar 473      -0.1655            0.0517        -3.2006          0.0014 *\nVar 474      -0.3016            0.0460        -6.5618          0.0000 \nVar 475      -0.6393            0.0455       -14.0446          0.0000 \nVar 476      -0.2174            0.0861        -2.5255          0.0116 \nVar 477      -0.4818            0.0867        -5.5571          0.0000 \nVar 478      -0.5280            0.8545        -0.6180          0.5366\nSignificance codes:  0 '**' 0.001 '__' 0.01 '' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 0.9883 on 130350 degrees of freedom\nMultiple R-squared: 0.5695,    Adjusted R-squared: 0.5679\nF-statistic: 360.7469 on 478 and 130350 DF,  p-value: 0.000\n. For the first data you shared with me, the model can be fit in 30 seconds on my laptop. For the new data you just shared, it took about 10 minutes. \n. Both R and sklearn use lapack for linear regression. It is interesting that they show huge performance difference. Maybe they call different lapack routines? BTW, lapack doesn't use multithreading, but they do use SSE. If you can disable SSE on R and sklearn, the performance gap will be much smaller.\nFor this particular case, we are actually not comparing the performance among R, sklearn, and smile. In fact, we are comparing JVM and lapack/Fortran. No doubt who is the winner.\nBecause Java and JVM are not optimal platform for numeric computing such as linear regression, I won't expect that we can beat lapack on matrix computation. But I will check if we can find fast algorithms to improve smile. Or we will call lapack through JNI, which I tried to avoid for sake of java's cross-platform.\nFor second problem, I checked sklearn source code. I din't find they prune zero columns. I guess that lapack takes care of that.\nThanks for checking LASSO code, I will go through it and make improvements as you suggested.\nP.S., if your goal is to compare the performance of sklearn and smile, I suggest you try some other algorithms such as svm, logistic regression, decision trees and random forest, etc. These methods are not about matrix computation. So you are not comparing to lapack indeed.\n. @douglasArantes Thanks! ND4j looks very good. It seems baking all kinds of linear algebra backends into it. I am not sure how portable it is for deployment. With it, should I provide multiple builds separately for windows, linux, macOs, cuda, etc.?\nThe math and matrix computation parts of smile was created back to 2009 (maybe 2008, cannot remember). It is time to use all latest technologies to speed it up. Shall we create a branch 2.0 to use ND4j and keep old 1.x for pure java implantation? Any thoughts? Thanks!\n. @lukehutch I agree with you that the algorithms play much bigger impacts on speed. It is why I said it is the comparison to fortran/lapack (fortran for language, lapack for algorithms). QR in smile is a standard algorithm based on Household transformation. Lapack provides multiple algorithms (complete orthogonal factorization, singular value decomposition, or SVD divide and conquer). sklearn uses numpy for least squares, which in turn calls lapack's svd with divide and conquer, which is the best available algorithm today. SVD decomposition doesn't care matrix ranks. This is probably why R and sklearn don't need to remove zero columns.\nWith ND4j, I see the potentials to leverage highly optimized libraries such as MKL, OpenBlas, and even CUDA. I will check how we can incorporate ND4j into smile.\nBTW, what's speed comparison for lasso between sklearn and smile? I do use advanced algorithms for many methods such as lasso, svm, etc. Thanks!\n. Thanks @DiegoCatalano ! I will check them  out.\n. @lukehutch , I tried least squares with SVD (smile's pure java implementation), which handle rank deficient nicely. However, it is actually slower than QR for your data. I will add a control parameter to let people to chose which method in use.\n. we can handle rank deficient now with svd. In Java,\nOLS(x, y, true)\nto use SVD.\nIn scala,\nols(x, y, \"svd\").\nTo use QR, simply OLS(x,y) or OLS(x, y, false) in Java. ols(x,y) or ols(x, y, \"qr\") in scala.\n. I am working on reducing memory footage for decision trees on large datasets. After that, I will release 1.2. It is 1.2 instead of 1.1 because some interface changes in scala api. I hope to release in one or two weeks.\nThen I will work on 2.0 with ND4j.\n. @luke, for lasso, do they produce same results? Do then coverage in similar number of iterations? Thanks!\n. BTW, your data is extremely sparse. Do you use sparse matrix in sklearn?\n. On my laptop (unplugged, suppose slower), it took 50 seconds to fit your first data, 6 minutes to fit the second/larger data. Are you still using OpenJDK?\nThe error swinging you felt is not really a swing. The error such as\n[main] INFO smile.math.Math - BCG: the error after 149 iterations: 0.008335\nis of BCG, which is an inner iteration. This may make you feel the error is swinging.\nThe real LASSO iteration output looks like\n[main] INFO smile.regression.LASSO - LASSO: primal and dual objective function value after   3 iterations: 1.2731e+05   3076.6\nNote that 150 iterations reported on the console is actually of BCG. The real LASSO takes only 3 iterations to fit your bigger data.\n. To match your sklearn setting, I uses lambda = 1.0 now. On my laptop, our LASSO finishes in 414 seconds with a regular matrix. But if we use sparse matrix, it takes only 50 seconds, which is much closer to sklearn's time on your machine. Moreover, Our LASSO finishes with RMSE 0.98, which is much better than sklearn's 1.5.\n. @lukehutch i think the the problem is more with double[][] data structure for matrix. With a one-dimensional column-major matrix, I can fit your data with OLS in 55 seconds on my laptop by QR, which is close to R.\n. Sorry for missing the warning in the comment. I just update it. The javadoc will be updated in the next release. Thanks!\n. I added warning in the web site too\nhttp://haifengl.github.io/smile/index.html#regression\n. BTW, matrix decomposition operations LU, QR, and Cholesky in smile.math.matrix may change the input matrix if a control parameter \"overwrite\" is true.\n. I did quickly double check other classifier and regression algorithms. Will do more go through later. Thanks!\n. Wait a second, a second look shows that we don't modify data in place. Two lines below, we have\nif (n > p) {\n        center = Math.colMean(x);\n        X = new double[n][p];\n....\nAs shown, if we normalize the data, we allocate a new matrix. So it is not a bug.\n. By switching to SVD, we can solve this problem. QR requires full rank.\nBTW, the rank of a matrix <= min(rows, cols). In least squares, we always have more rows than columns. When we say rand deficient in LS, we always means column rank deficient. see details at http://www.netlib.org/lapack/lug/node27.html\n. I improve the code to handle constant columns in LASSo and ridge regression, which is in the master branch now. Before we release a new version, please git clone and build by yourself. Thanks!\n. For linear regression, we definitely should use SVD instead of QR to handle rank deficient. It seems a lot of work to improve smile.matrix.\nOriginally I didn't expect people will pay attention to linear regression in smile. I put a lot of efforts to implement advanced algorithms for other learning methods (the motivation of creating smile). I was wrong :) \n. @lukehutch it is a good practice. How's lasso performance comparison between smile and sklearn? Thanks!\n. I add another safe guard for qr method. When qr fails in OLS, it automatically falls back to SVD now.\n. The first column is constant, which has zero variance, which cause NaN. I will improve the code to handle this.\n. we handle bias automatically. you should not include this column. I will update the comments to reflect it. Thanks!\n. I improve the code to handle constant columns, which is in the master branch now. Before we release a new version, please git clone and build by yourself. Thanks!\nAlthough no need to include bias column, some data do have constant columns as shown in #86. We can handle it now :)\n. Are you using 1.1 or master?\n. Each iteration needs to call BCG, which is an iterative algorithm too. It is not a restart. I tried on both data you provided with master code. I have not observed this problem yet. Will keep trying.\n. Did you see my comments on SparseMatrix with Lasso in another thread? It is a lot faster.\n. Cannot replicate this issue with Oracle JDK and latest code. Will keep an eye on this if it happens. Thanks!\n. many added equals and hashCode are for internal classes. They never get chanced to call.\n. many hashCode implementation are copied from BitString and don't seem correct for those classes (missing other fields).\n. I think we should squash this request. Thanks!\n. compareTo is for sorting purpose, which is very important for these algorithms. But we never call equals and hasCode. The rule squid:S1210 actually doesn't apply here.\n. Thank you very much for helping!!\n. BTW, are these all problems found your tools? Thanks!\n. Cool! Thanks a lot!\n. PMML is most useful to exchange models with other systems, which seems not a strong need right now. Besides, PMML doesn't cover all of our algorithms. Also, our implementations may contain additional information, which will be lost when exposed to PMML.\n. Thanks! More importantly, we should use -XX:+UseStringDeduplication for JVM, which is very important for runtime. I don't think that we will have millions smile model in an application. So the duplicated message strings are not big issues. But when we parse data files, we usually get a lot of duplicated string tokens. So -XX:+UseStringDeduplication helps a lot in that use case.\nBTW, is it your lint tools or IDE reorganize the import statements? I see that it always move java system import to the bottom of imports and use .*, which I don't like.\nSometimes, it is dangerous to move java system import to the end. For example, we may have same class names (e.g. smile.math.Math). Can you please turn of the import reordering and .*? Thanks!\n. Thanks for updates. Can you please don't use things like\nimport java.awt.*\nThings can be weird in case of conflict name space such as java.awt.Event and com.mycompany.calendar.Event.\nAs you can see, I never use import wildcard in my code. Thanks!\n. @elkfrawy-df the conflict is a simple one line change (some typo fix). Can you please resolve it and I will merge the request. Thanks!\n. I think we should withdraw this one. These dead stores are not really errors and have no performance hit. They were used on purpose before to make javac happy. Otherwise, we will get a lot of warnings.\n. Without them, I will get a lot of warnings. Have you check build.sbt on javac options. I guess that your compiler settings are different.\n. I updated FontChooser by removing i. Please have a look.\n. Please remove the change to FontChooser. I will merge the rest changes. Thanks!\n. I understand these assignments are useless in Java because the language does it implicitly. However, I prefer to keep them in case that I may port the code to c/c++ and/or cuda. If so, I can simply copy the code without worries I miss initializations.\n. Many changes here are wrong. For example, for all validation and hash functions, we should be able to create instances and pass to other functions, which we do use in practice.\n. For model validation, we have several test functions in scala that take model validation objects. It is up to the users to decide which measure to use. \n. Similarly for all classes for sort, BIC, etc. We provide static functions so that people can call them directly without create an object. But it is also possible for user to decide which measure to use. For example, in a GUI/web application, the user can drag and drop measures to test model. In that case, we need to create objects.\n. I think that many classes are more complicated than adding a private constructor. There are more space to improve. For example, we can change the methods of MulticoreExecutor.java to not static. Then we can have multiple executors in runtime, which may be useful in a large system.\nInstead of merging the whole request, I will review each case, cherry pick some of your changes and manually update the code. Thanks!\n. I cherry pick your changes. Thank you very much!. Are all changes \"\\n\" to \"%n\"? Thanks!\n. My understand is \"\\n\" is okay but \"%n\" is more platform independent, right?\n. Thanks for this. But there are conflicts. Please fix them and I will merge it.\n. Thanks! But there conflicts with earlier merges. Please fix and I will merge. \n. Thank you!\n. Can you share the data with me for debugging? I won't share the data with others. Thanks!\n. Thanks! I will check it out.\n. How did you save the data? I cannot open it with read.xstream.\n. Hi, MaxEnt takes sparse data as input. Each sample is represented by a set of sparse\n binary features. The features are stored in an integer array, of which\n are the indices of nonzero features.\nYour data doesn't look like sparse array encoding. If your data are dense matrix, I suggest that you use logistic regression instead, which is same as MaxEnt mathematically.\n. Please check out http://haifengl.github.io/smile/api/java/index.html, which has detailed information on data format.\nOnly if your nominal data is binary and very sparse (e.g. bag of words in NLP), you should use Nominal2SparseBinary.\n. Linear and ridge regression are for over determined systems. BTW, LASSO can handle under determined systems (but the solutions are not unique).\n. Hi, you can do a headless plot:\nval canvas = ScatterPlot.plot(x, '.', java.awt.Color.BLACK)\nval headless = new Headless(canvas);\nheadless.pack();\nheadless.setVisible(true);\ncanvas.save(new java.io.File(\"zone.png\"))\nNote that you should run JVM with option -Djava.awt.headless=true\nBesides, this feature is not in release 1.1 but in master branch. You need to build the master branch by yourself for now.\n. smile 1.2 is just released with headless plot. please have a try. Thanks!\n. Many algorithms are multithreaded and each thread has their own random number generator, for example random forest. It is not right to initialize every thread with the same seed in this case. What're your suggestions for this?\n. It is much more complicated by passing a sequence of random numbers:\n1. it is not clear how many random numbers are needed for many algorithms. Also it breaks the encapsulation as the caller has to know the details of algorithms.\n2. Some algorithms need many kinds of random numbers (integers in a given range and/or real values). Passing plain random bits make life more complicated.\nA work around is to ensure only one thread when you want repeatable results. This should be fine as one want repeatable results for debugging/testing. For production, you can still use multithreading. To us only one threads, use\n-Dsmile.threads=1 \nfor java command line.\n. call smile.math.Math.setSeed before call an algorithm to set the seed\n. You have not gotten the point of multithreading. Each random number generator maintains an internal state so that it is not multithread-safe. If multithreads share a random number generator, we have to use locks, which significant reduce the performance.\nSo each thread has its own random number generator in our design. And each thread should use different seeds for its own random number generator. Otherwise, algorithms like random forest won't work properly. Now, the caller must know how many system threads are running and passes an array of seeds or generators by your suggestion. That interface/contract is not really nice. \n. By my experience, the only case that repeatable results matter is to test and/or verify an algorithm. I think that -Dsmile.threads=1 and smile.math.Math.setSeed should be sufficient for that purpose.\n. You probably should ask this question on gitter to people using smile as my answer is surely biased :)\nSmile is actually very mature and all algorithms are covered by many test cases. The Javadoc and Scaladoc documents are also very intensive. I am not sure what do you mean scikit-learn like documentation. But you can find more tutorial on http://haifengl.github.io/smile/\nBTW, we cover many more algorithms than scikit-learn and we are planning more (on deep learning). I don't understand why you feel a project is not mature if it is being actively developed. In my opinion, a project without development is a dead project.\n. Did you view the user guide on iPad? The menu doesn't go away after clicking on iPad although the content is updated. Sorry for the issue. Try on a computer.\n. Most algorithms have been implemented for several years. They are really mature.\nBtw, all major algorithms are in Java. Try java doc first.\n. Why do you want to change some variables and init to protected in multi variate gaussian?\n. What's the use case of partial decomposition?\n. Thanks! Can you please contribute your SDAR to smile?\n. Can you please share your code snippets and data with me privately? Thanks!\n. If the data is not too big, please email me at haifeng.hli@gmail.com. Thanks!\n. Looks like the problem is caused by duplicated samples in the data. I am working on enhancing CoverTree.\n. We fix the bug. Your data should run without problems with CoverTree. BTW, KNN is not a good method for your data. Many sample pairs have same distances. Given a sample, you may get a lot of data points (> 9) has same small distances. Different nearest neighbor data structures may return different set of 9 samples. The prediction may seem random.\n. v1.2.0 was released before this fix. We will release a new version soon. Thanks!. v1.2.1 is just released with the fix. Thanks!. Thank you very much! It is a bug with logistic regression. Now it is fixed and pushed to the master branch.\n. Thank you very much! We have fixed the bug as you suggest. It is pushed to master now. \n. It is by design. Stochastic algorithms work in this way.\n. So you figured out the issue of double[] y? \n. Cool. We have some examples in Scala because the Scala API is simple and easy to use. The Java API is more complete and complicated. We will continue improving the documents and examples. Thanks!\n. All core algorithms are in Java. So you should be fine to use them in Scala 2.10. \nThe smile-scala is only a wrapper of Java algorithms into Scala functions. They are useful for training in the shell. However, it is not necessary to use trained models in spark.\n. I am working on cross build for both 2.10 and 2.11.\n. We support 2.10 now in master branch. Please pull the latest version and do\nsbt ++2.10.6 scala/package\nto build 2.10 version of scala api.\nThanks!\n. Please see my previous comment. Thanks!\n. You need JDK8.\n. I am not sure what's wrong. I have no issues at all. Can you do \nsbt clean\nfirst?\n. The error message says that javac doesn't recognize source level 1.8. Can you double check your javac version? Thanks!\n. Maybe some machines have open jdk while others have oracle jdk? I strongly recommend oracle jdk.  Besides, smile could be slower on open jdk sometimes. Thanks!\n. Thanks @elbamos and @bruce-campbell-sensus !! Really confused about this weirdness. Glad it doesn't block you moving forward.\n. Interesting. Thanks!\n. Have you tried training random forest with smile? It is much faster than spark. \n. Have you ever tried smile?\n. For this special requirement, we can do it with your company's sponsorship. Thanks!\n. I am not sure if weighted data are plausible to mixture model mathematically. EM algorithm may not converge at all without carefully updated formulas.\n. It is not what I said. It is not about integers.\n. Suppose your mixture components are Gaussian distributions. The weights may have some impacts on the estimation of means. But it has little impacts on the estimation of covariance matrix. If it does, the impact may be not good as the covariance matrix may be closer to singular. I don't recommend to bin data and then do pdf estimation.\n. Regularization doesn't work for 1D data. Set the regularization parameter to 0, you should be fine.\n. Thanks! I change the default value 0.0 for 1D mixture models. It is in master now.\n. Can you do a \"git pull\"? This error was fixed before. Thanks!\n. We don't support this algorithm directly. But we do have multiple MDS algorithms. You can calculate the distance matrix after KPCA and then feed it to one of our MDS algorithm.\n. Thanks!\n. Thanks for the suggestion. I just added this feature and it is in master now. You can pass an InputStream of additional rules to constructors now.\n. v1.2.1 is just released to Maven Central with this update. Thanks!. Sorry, I misunderstood your request \"the default rules remain\". Let me change the behavior. You can git clone the repo and \"sbt package\" to build the jars.. Please test the new fix. If everything is okay, I will push a new release soon. Thanks!. version 1.2.2 is released. It may take a couple of days to be available on maven central repo. Thanks!. In old time, we can find the syntax info at http://www.comp.lancs.ac.uk/computing/research/stemming/Links/paice.htm\nUnfortunately, this page is gone now. Sorry. Multilabel classification is usually modeled as a group of binary classification. The new example is applied to all these classifiers and the ones with high posterior probabilities are reported. You can use any SoftClassifier for t his purpose.\n. Thanks!\n. Hi, smile.feature.Nominal2Binary is what you want. Also smile.feature.Nominal2SparseBinary provide the one-hot encoding in compact sparse format.\nAlthough some algorithms such as neural network needs the one-hot encoding for categorical data, many algorithms such as decision tree and random forest in our implementation don't need it. We can handle categorical data natively in smile. Thanks!. Note that we distinguish LDA and Fisher's linear discriminant (FLD class in smile). When you google LDA, most pages actually talk about FLD, which doesn't require the normal distribution assumption. In our LDA implementation, we assume that each class follows a normal distribution (with same variance matrix) and use Bayes decision theory to learn the decision boundary. The predict method actually use this idea to calculate the posteriori probability (we don't calculate posteriori probability in FLD though).  Some old text book on Pattern Recognition describes this approach. I need to check my bookshelf to find the reference :) Thanks!\n. Please check the book \"Pattern Classification\" by Duda, etc. Page 39 for details. . Are you using 1.2.0 or 1.2.1? v1.2.1 was just released and should fixed this problem. Thanks!. Thanks!. Python is dynamic type while Java is static type. NaiveBayes class is of Classifier, mostly to provide a compatible interface with other classifiers. It is easy to add additional learn and predict function that take a sparse format. Would you like to add them? Thanks!\n. I add the support of sparse input in NaiveBayes now. The fix is pushed to master. Please clone the repo and do \"sbt package\" to build the libraries locally. See http://haifengl.github.io/smile/index.html#quickstart for details. If everything is okay, I will push a new release soon. Thanks!. Thank you! Please have a try the new methods and report any encountered issues. Thanks!. version 1.2.2 is released. It may take a couple of days to be available on maven central repo. Thanks!. What's your exact confusion? Can you please elaborate? Thanks!. I got your confusion. See line 447. For CROSS_ENTROPY and LOGISTIC_SIGMOID combination, the last layer should have only one output neuron.. in shell or scala, use write.xstream to save any object/model. If you have to use Java serialization, we can add the support to clustering and other alorithms.. I make all clustering algorithms serializable now. The fix is pushed to master. Please clone the repo and do \"sbt package\" to build the libraries locally. See http://haifengl.github.io/smile/index.html#quickstart for details. If everything is okay, I will push a new release soon. Thanks!. version 1.2.2 is released. It may take a couple of days to be available on maven central repo. Thanks!\nHappy new year!!. in scala, use write.xstream to save any object/model.. Just use XStream. Three lines should be enough. If you have to use Java serialization, we can add the support to clustering and other alorithms.. more details on out of memory error? when and where? how big your data/model?. If your model is classification or regression, you can save and load them by \"write\" and \"read\" methods in the shell directly.. what is your model?. For SVM (or any classification and regression algorithms), you can serialize them by Java serialization. See examples here https://www.tutorialspoint.com/java/java_serialization.htm. The kernel function is not serializable. The fix is pushed to master. Please clone the repo and do \"sbt package\" to build the libraries locally. See http://haifengl.github.io/smile/index.html#quickstart for details. If everything is okay, I will push a new release soon. Thanks!. Of course. \"sbt package\" build everything (most modules are in Java, and only one in Scala)  into jar files. Just copy the jar files to your project.. you need slf4j library.. I tried your example yesterday and it works. Probably because it is an empty model (not trained). I updated the inner classes with Serializable and it works with a trained model. Please git pull and try again. Thanks!. It works for me. git pull only update the source, not build the jars. Did you \"sbt package\"?. I see. What's your input data type? Sparse array?. Just make smile.math.DoubleArrayList and related classes serializable. Please git pull and build again. Thanks!. version 1.2.2 is released. It may take a couple of days to be available on maven central repo. Thanks!. I am using Chrome on windows and has no problems. Are you on mobile devices? Thanks!. @asmaier I updated index.html as you suggested. We use https://code.jquery.com/jquery.min.js now. Hope it fixes the problem. Thanks!. Thanks!. Look good! Thank you very much!. I guess that you are using v1.2.1. This bug is fixed in v1.2.2. Please try the latest version. Thanks!. Currently no plan to support it. Would you like to implement it? I am more than happy to help. Thanks!. I wouldn't take the risk of license. But it seems not very complicated and we have many built in functions. It should be easy to implement it.. Yes, I like the changes. I had a comment for you to change. Did you see it? Basically, I would like some function name is more meaningful than \"default\".. I merged your commit and changed it a little bit (see the commit a72901d6d805af5d4fb83926aa6ded8e0f381d81). Thanks!. multinomial or multi-variate normal/Gaussian distribution? We don't have multinomial but have MultiVariateGaussian. BTW, smile.classification.NaiveBayes supports multinomial model. Is it what you are looking for?. So what you care is the random number generation?. If all you need is that multinomial random number generator, there is a function in smile.math.Math\npublic static int random(double[] prob)\nGiven a set of n probabilities, it will generate a random number in [0, n).. You are welcome!. KDTree is designed for multiple dimensional data. Your example is of only on dimension. If you have only dimension, I suggest that you sort and binary search your data. Much faster.. Your question is really about Scala/Java interoperation. Make sure your values are boxed integer (thus objects):\nval values = Array(0, 1, 2).map(Int.box(_))\nand your KDTree is of type Integer (not Int, which is int in Java)\nval tree = new KDTreeInteger. Thanks!. Hi @Mega4alik , what do you mean \"getting same result for each test\"? Thanks!. Do you have missing values in your input? And what's the range of Y?\n. Is your Y in range of [0, 1499]? How many samples do you have?. I guess that you are using one-hot encoding, which is what sklearn expects. If you don't specify the attribute type (as shown in your code), smile will assume they are numeric values. It is why it doesn't work. Instead, you should pass an array of NominalAttribute. See the details in smile.data.NominalAttribute.\nBesides, in case that you use one-hot encoding, please don't do it. Smile can handle categorical variables directly (and much more  efficient). Suppose you have only 10 attributes (each takes discrete value 0 to 149), just pass 10 attributes to us. You will see that we are way faster than sklearn. Thanks! . Suppose you are still using 1500 one-hot encoding features.\nNominalAttribute ars[] = new NominalAttribute[1500];\nString[] values = {\"0\", \"1\"};\nfor (int i=0;i<ars.length;i++) ars[i] = new NominalAttribute(\"atr\"+i, values);           \nmodel = new RandomForest(ars, X, Y, 500);. Can you provide some sample data? Both X and Y. Thanks!. Thanks for sharing the data! I don't have time to try it yet but it seems really really sparse. Very few ones and a large number of classes. Each class has only around three samples. Our random forest does the strata sampling, which is very good when the data is unbalanced. But given your samples per class ratio, each class may get only one sample during training, which is clearly not good for machine learning.. BTW, your have less samples than features, which is underdetermined. Machine learning is in general to solve the overdetermined system. For example, the basic least square method. For underdetermined system, the solution is not unique and it is not clear how to assess the model quality.\nI don't know how sklearn handles this situation. But what's the accuracy by cross validation or bootstrap?. Thanks! SVM is not that sensitive to high dimension as all it cares is the kernel matrix. However, no matter you use one vs one or one vs all strategy, you have only a few samples in one (or two) side(s). Basically they will all be the support vectors. And the classifier is essentially a template matching.\nFor Random Forest, there are many parameters you should carefully choose given your very special data. For example, mtry, nodeSize, maxNodes, subSamples. Currently you use the default values, which are probably not good in this case. Also there may be some logic in the code that doesn't consider the use cases like yours, such as only one or two samples per class. It may be the reason that the model always predicts the same class (very likely some NaN value is generated in the model). Is the predicted class always 0 (or the last class)?\nAnyway, I strongly suggest that you collect more data if possible. I wouldn't hold any faith on these accuracy numbers if I were you. Thanks!. Thanks! We did test our implementation against Matlab. I think that the difference is because of the data format. Mathematica assumes that the \"Periodic\" padding, that is how to extend data beyond boundaries. It is why if you rotate the 1 to back, we get the same results. Also, the input is only 4 elements, we use 1 level here.. You are right. Padding is not the issue here. But there are two other things that may cause the difference.\n\n\nIn the example of Wiki, H4 matrix is used, which means two stage transformation. As you already noticed, we are doing only level transformation with Haar. It means that we are using H2 matrix in that wiki page.\n\n\nIn fact, we are not using the exact same H2 matrix. Most wavelet documents use [1, 1] as coefficients in Haar because it is simple. However, its norm is 2, not 1. In our implementation, we use the coefficient 0.7071067811865475. Similarly, we normalize all other wavelet coefficients to norm 1. You can check these coefficients in smile.wavelet.DaubechiesWavelet, for example. They are different from what you will find in wiki (https://en.wikipedia.org/wiki/Daubechies_wavelet), where they explicitly states that the norm is 2. Some people normalize them to of norm sqrt(2), too. It is mainly a matter of scaling.\n\n\nI don't have Mathematica. Can you please do me a favor by calculating\nDiscreteWaveletTransform([1,2,3,4]) with one level refinement and then scale the result with 0.7071067811865475. I hope this will match our result.\nBTW, we have unit tests that all can transform a signal back and forth without errors. And the wavelet shrinkage also works fine. If our DWT is wrong, i cannot image that we can transform the signal back without errors. Thanks!. @bernacek any updates? can we match the results after scaling? Thanks a lot!. Most wavelet's coefficients are at least 4. It is why the generic wavelet.transform() method stops at that level. Haar is special with only 2 coefficients. I will add special handling to it in transform().\nCan you post the result from Mathematica with one level pass and scaling here? I want check if the sign difference is systematic or not. Thanks!\n. I checked [JWave] (https://github.com/cscheiblich/JWave/blob/master/src/jwave/transforms/wavelets/coiflet/Coiflet1.java). Our coiflet coefficients match theirs. Which doc/software do you refer about coif let coefficients? Thanks! . Can you confirm that this issue is only with Haar wavelet? After scaling, can other wavelets match Mathematica (let's leave coiflet alone now). This will help me to locate the issues. Thanks!. Some people call it Symmlet, for example http://www.colorado.edu/engineering/CAS/courses.d/ASEN5519.d/kaist.lecture.11.pdf\nNot sure which one is \"officially\" correct spelling :)\nFor sure, I will make  transform() working until the last pair.\nNot sure the coefficients (not matter coiflet and others). I found many different definitions. Don't know how they come up and which one is (absolutely) correct. Coiflet is almost symmetric. Maybe some people don't care if it is reversed?. In the book Numerical Recipes (http://cacs.usc.edu/education/cs653/NRC-c13.10-Wavelets.pdf), the DWT stops at last four samples (not 2). See the wt1 function there. Do I miss something here? Or Haar is special?. In this web page, the order of coiflet coefficients is same as in my code. Does the order really matter in coiflet?. Find the bug with HaarWavelet. It is fixed now and we match the results with Mathematica on [1,2,34] input. Thanks!. Matlab uses this function to calculate how many levels of wavelet transform.\nhttp://www.codeforge.com/read/200483/wmaxlev.m__html\nWe follow the same rule now.. all changes are pushed to repo. you can try it by clone this project and then run ./smile.sh in the repo. Thanks!. Please use smile.classification.Maxent (maximum entropy) classifier, which is essentially a logistic regression. But it takes sparse input.. Well, we assume double[][] be a dense matrix. In your case, it is 300000 x 40000. But for int[][], we assume it is a sparse binary matrix. The entry is the index of nonzero features.. exactly. Let me know if you have any other questions.. Yes. 0. How's it work? How's the speed and accuracy compared to StandfordNLP? Thanks!. Do you have full stack trace? Very likely, the optimization process is not stable on this data. Thanks!. Yes, p should be the number of features. In this case, p should be 40000. In the input x, the value should be in [0, 40000). It is interesting that the error is raised here. Thanks!. How many classes do you have?. The order should not be an issue mathematically. But it may become a problem if underflow happens. It is tricky and also depends on the order of samples. Your y takes values of 0 and 1, right?. Yes, we have several kernels for binary sparse data. Check out smile.math.kernel package.. When you train maxent, can you call trainer.setTolerance(0.01)? This will terminate the optimization early and should still give good accuracy.. Why do you want to set C = 0 here? Can you try C = 1 or 10? These numbers are not optimal but want to check if C = 0 causes this problem. Thanks!. I am confusing why the feature filtering causes the problem. Trying to find out any possible bug. For accuracy, you test it on the training data or separate test data?. I tried the small test dataset in smile shell without any problem\nsmile> val x = Array(Array(1,2,3),\n     | Array(1,2,3),\n     | Array(0,1),\n     | Array(0,2),\n     | Array(0,3)\n     | )\nx: Array[Array[Int]] = Array(Array(1, 2, 3), Array(1, 2, 3), Array(0, 1), Array(0, 2), Array(0, 3))\nsmile> \nsmile> val y = Array(1,1,0,0,0)\ny: Array[Int] = Array(1, 1, 0, 0, 0)\nsmile> maxent(x,y,4)\n[main] INFO smile.math.Math - L-BFGS: initial function value: 3.4657\n[main] INFO smile.math.Math - L-BFGS: the function value after   5 iterations: 1.8836\n[main] INFO smile.util.package$ - runtime: 17.362722 ms\nres0: smile.classification.Maxent = smile.classification.Maxent@44a6a68e\nAre you using OpenJDK? It is better to use Oracle JDK. People met weird numeric problems with other smile algorithms on OpenJDK before.. We are using the same data and setting, but I don't have issues. I am using mac. I guess that there is something wrong with environment. Will try on a windows box.. Try the binary sparse Gaussian kernel. If you choose the right kernel parameter, it should have better accuracy.. I tried the small test data on Windows 10 without problems:\nsmile> smile.classification.maxent(x,y,4)\n[main] INFO smile.math.Math - L-BFGS: initial function value: 3.4657\n[main] INFO smile.math.Math - L-BFGS: the function value after   5 iterations: 1.8836\n[main] INFO smile.util.package$ - runtime: 33.689975 ms\nres1: smile.classification.Maxent = smile.classification.Maxent@2c768ada\nsmile>\nMy java version is\nD:\\github\\smile>java -version\njava version \"1.8.0_112\"\nJava(TM) SE Runtime Environment (build 1.8.0_112-b15)\nJava HotSpot(TM) 64-Bit Server VM (build 25.112-b15, mixed mode)\nIt is essentially same as yours. I don't know what other environment things could cause the failure on your machine.\n. I use smile shell, which has dependency to logger too. I guess that it is not the root cause. But it does reminder that this may be caused by some race condition in multithreading. Can you please pass\n-Dsmile.threads=1\nto JVM. I hope that this will solve the problem. The training will be slower though. Thanks!. It is a known issue on Windows (because of SBT or Scala). You can use the shell as usual. You just have to import smile libraries by yourself.. So far, I cannot repeat the training problem on my machines (including Windows, Mac, and Linux). No clues right now why this error happens to you. Will keep working on this. Thanks!. Yes, I noticed that yesterday. Training finishes without problems but predictions are not okay. The possible reason is that we have only 5 samples here (and the model has 5 parameters) here. The model is not correctly trained. . Scala api is only a thin wrap of Java API (which provides the real algorithms). We should get the same result.. Can you train the large data set on Linux?. With Gaussian, you are training a non linear SVM, which is slower. But the main problem here is sigma. Your choice of sigma = 1 is way from optimal given your data. In general, your kernel value is close to zero with this setting. A good sigma should be close to typical distance between samples. I suggest that you calculate the distance of samples and plot the histogram. Then choose sigma close to the peak or median. Then fine tune it. Given your data (so many features), I guess that is value should be pretty big.\nThank you very much for all the feedbacks and issues! I look forward to more!!. Notice you use 1.2.1. Can you upgrade to latest version 1.2.2. Or even better clone the repo and build the latest version. Thanks!. Hi, the formula you post is not the error function, but the gradient of error function against to the weight. The g calculates this exactly (without x_i part). In the adjustWeights function, this gradient and x_i are used to update the weight.\nBut we do have a bug here after revisiting the code. The error should be 0.5 * (output[i] - out) ^2. That is that the lines\nif (errorFunction == ErrorFunction.LEAST_MEAN_SQUARES && activationFunction == ActivationFunction.LOGISTIC_SIGMOID) {\n                g *= out * (1.0 - out);\n            }\nshould be moved down below the error calculation and above \ngradient[i] = g;\nFortunately, this error is never used. So this bug should not caused any real problems. I fixed the bug. Thank you very much! . Sorry for inconvenience. We currently don't support space in quotes. A quick work around is to use different separator such as csv or tab. Or you can replace space with underscores in the quoted fields. Thanks!. Are you using 1.2.1? Please use 1.2.2, which fixed this issue.. Currently we don't expose the internal of decision trees. I can add a few methods for you to travel the decision tree. BTW, you want to export the model by PMML and then use the model in another package for prediction? Is there any difficult for you to use smile for prediction directly? Thanks!. How about we work together to add toPMML methods to DecisionTree and other algorithms? Create a fork and starts with the \"root\" member variable, you can generate the PMML easily. Thanks!. Have you run the test after your change?\nsbt core/\"test-only smile.classification.NeuralNetworkTest\"\nThe error is much bigger. Not sure which book/paper you are referring about the gradient computing, I wonder if the update rules match too. Thanks!. A possible confusion is that for cross-entropy, the objective is to maximize it. While for mean-squared-errors, the objective is to minimize it. Our code has to handle them uniformly, so we have to revert the sign of cross-entropy.. In this tutorial http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function, the cross entropy cost function is a little different. And the gradient also has different sign.\nMost importantly, you should run the unit tests. With your change, the error rate is way too high. Thanks!. You can always ask me questions here. This place is better than email as everyone can see the discussions! Thanks!. SimpleTokenizer is designed for natural language. I am not sure your example is what we target. Besides, if we changed the rules for your case, it won't work well for things like U.S.. Now we are clear your use case. I add a new rule to handle this case. It is in master branch now. Thanks!. For your case, cross entropy cost function and logistic activation function should be fine.. Thanks!  A community member is working on exporting decision tree to PMML. . @myui Thanks! We implement a dot() function to export the tree to graphviz. The algorithm is different from yours though.. smile.validation.CrossValidation. CrossValidation is not designed for split test. In fact, cross validation is better than split test and it is why I suggest it. If you have to do split test, do a smile.math.Math.permutate(n), where n is your sample size. Then use the returned index to split your data.  . You have two classes, right? And I guess that you are NOT using cross entropy as the error function, right? If so, your last layer should have two neurons. You have only one here. Thanks!. Correct.. I cannot run your code in smile shell. Which \"Random\" object do you refer? BTW, the double value 2.0 is used in the line you talked about. All calculation will be lifted to double. So there is no integer overflow. Thanks!. Thanks! I change pos and neg to double. It reports 0.50 now.. You are using JDK 7. Please use JDK8. . Just notice your issue title. We don't support JDK7 anymore as Oracle doesn't support it.. Thanks! I fixed it.. Neural network cannot handle categorical data directly. The typical approach is to use one hot encoding. If a categorical variable has three levels, you convert this variable to three binary variables.. You probably has 3 neurons in the second layer. The net variable to the next layer is the output of this layer.. Each neuron has 7 weights. Logically, there is the first layer of inputs. But programmatically you don't need it.. No. You don't need to store the first/input layer, which is only for people to draw the graph. The first layer in our NeuralNetwork is actually the second layer that you normally see in a text book. If you have 10 neurons in the second layer of logical graph, then you will see 10 neurons in our first layer. In each neuron of our first layer, it takes 7 inputs and also has 7 weights. The output of each neuron is a single value, which is part of that layer's output vector.. Can you post your code of training the neural network?. Can you print out trainingArray[0].length?. If it is 100, the weight should be of 100. You are confused with PMML and implementation.. To use categorical variable in smile, you should do the one hot encoding before feeding the data to neural network. We have classes to do this task, which separated from NeuralNetwork. PMML is just a specification. And most importantly, it is the output of training process. It doesn't mean that an implementation work in the same way.\nAre you trying to import an PMML model into smile or export from smile?. In general, random forest samples data by bootstrap (i.e. sample the data WITH replacement n times). Here is an extension the allows sampling without replacement. But without replacement, we have to specify a lower sample size. Otherwise, we will always get the same original samples.. Sampling WITHOUT replacement. Only subsample =1.0 means sampling with replacement.. It is just a convention. We keep the same practice for random forest and gradient boosting.. In fact, our sampling without replacement is more advanced as we consider the unbalanced distribution among classes. I think it is better than sampling a smaller size with replacement .. We are refactoring matrix library. This may be caused by an accidental commit. Please wait a little bit. Our refactor will finish soon. Thanks!. We just push the refactoring to master. Please do \"git pull\" and build again. Thanks!. Thank you very much! It is a bug. The fix is in master now.. Thanks! Updated it.\nclassWeight is the priors of the classes. The weights of each class is roughly the  ratio of samples in each class. For example, if  there are 400 positive samples and 100 negative samples, the classWeight should be [1, 4]  (assuming label 0 is of negative, label 1 is of positive). If the classWeight is null, we will assume that the data is balanced and assign the weight 1 to each class.. 1. For unbalanced data, firstly try to set a proper classWeight. If your ratio is 1: 10 for false : true, start with [1, 10] for classWeight and do some adjustment based on experiment results. Second, use the posterior probability and choose a cutoff by yourself. The default predict() just use 0.5, which may not be proper to you.\n\nThere is no good theory to find the classWeight for all data. The caller should do some experiments to find the good one for their use cases. If I change the default value to other values, some other people will complain that it is not  good for their data :). Hehe! Thanks!. It is an issue with cache. Clearing your browser cache will fix it. A workaround without clearing cache is to click the mean item (say User Guide -> Statistics). It will look good. Then click the title to go back to home page. . You are right. This is more about the base / path. Our javascript doesn't handle it well. I will fix it. Thanks!. Fixed. Thank you very much!. Yes, please. If you can provide your code snippet, it will be great. Thanks!. Thanks! I will look into it. BTW, here is the same code in Scala\n\nval x = read.table(\"/Users/lihb/Downloads/test.txt\", delimiter=\";\").unzip\nval d = proximity(x)\nval hac = hclust(d, \"single\")\nunique(hac.partition(97)).length\nunique(hac.partition(89)).length\nunique(hac.partition(8)).length\nAlso, you use a very large number of clusters to partition given the size of your data. I suggest to use a smaller number.. I think that it is a design problem with our partition algorithm. Our partition algorithm is simple and efficient for almost all cases except yours. You have  only 131 samples but try to partition them into 97 or 89 clusters. For these settings, only a few clusters contain (a very small number of) multiple instances while all other clusters have only one instance. In practice, I don't really think that these numbers are meaningful.\nTo keep our algorithm simple and efficient, I would not revise it to pursue the perfectness in these extreme cases. Thanks!. The fix is in master branch now. Thanks!. The declare of \"throw ParseException\" comes from the parent class Attribute. Note that what you describe is only true when the attribute is open. If not, we could throw an exception. I just update the code to throw ParseException instead of IllegalArgumentException. Thanks!. v1.3.0 was just released. So we will wait a little bit to cumulate enough fixes for next release. For now, you can clone and build the project by yourself. Thanks!. Yes, please. Thank you very much!\nWe actually had pom files before. But since we switched to IntelliJ, the life is much easier with sbt. Then we removed pom files as they became outdated. It is hard to keep tracking two building system. More importantly, we have the scala module now.. v1.3.1 is released with all the bug fixes. Thanks!. Thanks! Did you run unit tests on this change?. I manually pick your changes but I didn't change line 424 (instead I precompute radius * radius outside of for loop). I don't see any performance changes as I expected. A save of sqrt won't significantly change the speed. I guess that your speedup comes from the change at the line 424. Note that it is very dangerous that you pass radius * radius there but the \"raw\" radius is used in many places besides your changes at line 357. This change may cause wrong nearest neighbor search. I suggest that you run unit tests to test out your changes. Thanks!. Can you please update the pull request? In general, I prefer that the changes are in local scope so that we don't get too many side effects. I am not exactly clear what you try to change here. Thanks!. 1. smile.math.Math has multiple ways to normalize the data (normalize, standardize, unitize, etc.) Please check out the Javadoc (http://haifengl.github.io/smile/api/java/index.html) for their difference.\n\nSee http://haifengl.github.io/smile/manifold.html how to plot the results (and also the nearest neighbor graph if needed). . You probably try to do a headless plot on a server. If so, see the bottom of is page how to save headless plot:\n\nhttp://haifengl.github.io/smile/faq.html. Thank you very much! I will merge the PR once I understand the warning.. Thanks!. Can you share your data to debug? BTW, if the matrix is singular indeed, it is not the problem with the algorithm. Besides, do you have duplicated samples in this data? It may cause this problem. Thanks!. As this was caused by duplicated samples, I will close this ticket. Thanks!. Can you share your data? I will debug it. BTW, did you test this with 1.3.0? Have you tried 1.2.3? I made big changes to matrix computation. Want to make sure if the changes cause this. Thanks!. I will test it with 1.3.0 (latest version) anyway.. I guest that this is the same data as in ticket 174. Can you first check if you have duplicated samples in your data? Thanks!. Duplicated sample will cause the distance matrix singular, which cause the issue in ticket 174 for sure. You should remove duplicated samples. no work around for singular matrix.\nThis ticket might be cause by the duplicated samples. But I am not sure. Thanks!. Does \"d\" means the dimensionality of input data? If so, k = 5 is probably too big.. Also, normalization and standardization may not be good ideas for manifold learning. They are mostly for classification.. In general k should be less than d. The purpose of manifold learning is to find the intrinsic dimensions, which should be smaller.. The dimension of MNIST is 28 X 28 = 784. You are confused with the t-SNE plot.. Sorry, there were miscommunications. I was asking if d is the input dimension in your settings. You said yes. In our API, d is the output dimension.. Duplicates are more likely the issue.. Can you share the data? I will debug it. Sometimes if two samples are too close, the distance matrix might be singular or near singular, which will cause the problem. Thanks!. Thanks! Do you have the code snippet too?. For parsing and also the call to LLE. Thanks!. Thanks! What's your k and d? You already filtered the duplicates in the attached files, right?. Large k is not recommended in general. I know the meaning of k and d :) I was asking their values in your settings that cause the problem.. Can you serialize the parsed data (the data matrix) into a plain csv file or Java object file? I am afraid that I will load your data incorrectly, which is pretty complicated. BTW, IsoMap/LLE uses Euclidean distance, which seem not appropriate to your data, which is mix of numeric, nominal and string data.  . You better first convert nominal values to one-hot encoding.. Also don't include operation id and timestamp in the features. If you have to use timestamp, better convert it to things like day of month, day of weeks, etc. Feature engineering is very important in machine learning.. Thanks! Sounds good. I will try it tonight.. Got OOM error on a small machine last night. Will try it on a bigger machine.. I see the exception in IsoMap, which happens during the eigen value decomposition. I will try to figure out what's wrong. It may take some time.. Likely it is a numeric stability issue. It is also why you have fewer problems with smaller data. For long term, we should use something like blas and lapack, which are more numerical stable and also faster. I am looking into how to do it. But it will take a lot of time.. I add your changes to the master branch (I cannot merge yours directly as the line numbers don't match). I also add additional unit tests. Thanks!\nThe problem was probably because I compare distance to diff * diff in knn and nearest neighbor search, which are correct. I might just copy some logic from there. Glad you find it. Thanks!. Since you have millions of data, did you try LSH? It should be faster if you don't care it is an approximate algorithm.\nBTW, what do you mean \"anti-library version\"? Have you compare smile to other nearest neighbor search libraries? Is smile fast enough? Thanks!. If you download the binary package, the data are in the install direcotry data/weka. If you try to find them in the source code, they are in shell/src/universal/data/weka.. In your clone, run ./smile.sh to build and get into the smile shell. It also set the working directory to the right location so that you load data directly from the data directory such as read.arff(\"data/weka/cpu.arff\"). Cheers!. Although we have only one keyword extractor for now, I expect more keyword extraction algorithms in the future. Some of them will need setup (e.g. parameters). In general, we will have an interface KeywordExtractor and  all these algorithms will be implementations. So I would like to keep it non-static. A similar design can be found in tokenizer package. Thanks!. What's your OS?. Swing doesn't work well on mac. The image saving facility works well on windows. I will see if there is any work around.. When in the file dialogue, please try CMD + Shift + G to bring up a window to type the path. I find this on internet and have not tried it yet. Thanks!. I fixed it. Please try v1.3.1. Thanks!. This is surprising!! I will check it out. Thank you very much!!. I don't observe what you described:\nsmile> Math.isZero(0.0)\nres0: Boolean = true\nsmile> Math.isZero(java.lang.Double.MIN_VALUE)\nres1: Boolean = true\nsmile> Math.isZero(2*java.lang.Double.MIN_VALUE)\nres2: Boolean = false. But Double.MIN_VALUE is very small (2^-1074). A more practical epsilon value maybe Math.EPSILON (2.220446049250313E-16 on my machine). I will try this value and run unit tests.. I made a revise yesterday about isZero. The default epsilon is now smile.math.Math.EPSILON. It should be more predictable and small enough for practical purpose.\nThe problem with Math.MIN_VALUE is that it is so small (as small as a random error in FPU). It maybe the root cause.\nPlease clone the repo or git pull to test it out. Thanks!. Do it in the shell. It is easier. If you install the binary package, type bin/smile to run the shell. Then type demo in the shell.\nIf you clone the repo, type ./smile.sh to build and run shell.. Please check the document at\nhttp://haifengl.github.io/smile/classification.html#svm\nand\nhttp://haifengl.github.io/smile/api/java/index.html\nYou should also read http://haifengl.github.io/smile/shell.html about how to use the smile shell and http://haifengl.github.io/smile/data.html to load the data. Thanks!. You are welcome!. Thanks!. It is a bad data. All the first column is 0. The data is essentially one dimensional. Kmeans and alike won't work on one dimensional data.. Why don't you use some real data? Even if you have to use synthetic data, you have to at least follow some distributions. Kmeans and alike work well on Gaussian distribution.\nNote that no clustering algorithms work well on all kinds of data. I don't feel your generated data have practical meaning.. I am confused what you try to do. You split your data into 30k groups (how?) and then run Heman's in each group? Can you provide as much as possible information so that I can help?. Well very likely you didn't follow our building instructions. Are you using JDK8 and SBT?. Can you share your code and some input sample?. Read.csv doesn't guess the data type. You should provide an array of Attribute to tell it the data type of each column. . Don't know if you check out the user guide at http://haifengl.github.io/smile/\nFor your use case, check out this example:\nhttps://github.com/haifengl/smile/blob/master/benchmark/src/main/scala/smile/benchmark/Airline.scala#L48. For repeatable test results, set -Dsmile.threads=1 in command line and call smile.math.Math.setSeed before training the model.. It is our assumption. And it is consistent in all our classification algorithms. It is the caller's responsibility to make sure the input data follow the requirements. . What do you mean case-wise importance? Thanks!. Your setting is correct. The bias term is implicitly handled by smile.. Nearest neighbor is a lazy learning algorithm, which means no learning :)\nI guess what you want is an online algorithm to build a nearest neighbor search data structure. I am not aware of such algorithms. . Check out FAQ on haifengl.github.io. Basically use read(), read.xstream(), write(), and write.xstream() functions. First of all I don't recommend to save models to Mongodb as it has hard limit on document size (16mb), which is not sufficient for big models.\nTo serialize an object in memory with Java, please just google it. There are many examples online and it is very simple.. the tokenizer is rule based and is not perfect. We were mostly concerned with the cases like \"U.S.\"\n\"Boston.\" is not perfect but it seems a bigger issue if we split U.S. to U . S .. The data should be normalized first before feeding to neural network. Check our unit tests. We use iris there too.. Absolutely!! Thank you very much for contributions!!! Can you please provide more information on your system? Would love to learn more and decide how and which module we can integrate your work.. Cool. I will create a module and directory structure. You can import your code there.. Beyond strings as input and output, we can create a DSEL in scala, which is more useful in a programmatic way.. I add the 'symbolic' module. Please add your code there. Thanks a lot!. not part of maven. this is only for test purpose. Because the data is from microsoft, we keep their license there.. Thanks! I merged it. The first thing we should do is to add more comments so that others can understand the code and contribute.. I made some changes to unit tests. Since all of our tests are based on jUnit 4. I change your test to it from jUnit 5 to avoid conflicts.\nBesides, why do we have both CalcTest and CalculusTest? Can we combine them? Also the tests in CalculusTest fail.. Cool. Let's add the headers to explain each class and its algorithm in high level. Then we can start the rewrite part. I have a lot of experience in DSL. Will look into your code and see what I can do. Honestly functional languages like ocaml and scala are much better choice for this task. But since the whole project is mainly based on Java, Java is still a good start. We can use scala to add a nicer interface. Thanks again!. Sure thing. Hi Ernest, I am reviewing your code and plan to refactor it so that we can use it in practical apps (not just string in, string out). First of all, I notice that you put \"Copyright (c) 2017 Ernest DeFoy\" in all the source code. Can you please release the copyright as you contributed it to the smile project? Thanks!. Thanks! If you can add more comments in the code, it would be very helpful.. PCA/PPCA/KPCA are projection algorithms while IsoMap and LLE are manifold learning. They are different animals.\nThe reason that IsoMap and LLE provide getIndex is because they work on the largest connected component, which doesn't necessarily cover all the data points. The getIndex method let you know what original data points are used.\nA projection algorithm learns the mapping and don't store the the projected data points (actually they don't do this computation at all).. Yes, it is one-to-one mapping. There are examples in demo that do the color coding in visualization with PCA etc.. I have not experience in this.. Smile uses light weight component JPanel, Canvas, etc. It should not be a problem. Did you try to add a PlotCanvas or PlotPanel to SwingNode? I guess that you should add a PlotCanvas to SwingNode.. Does it work now?. Fixed. You need build by yourself to use the fix before a new version be released. Thanks!. I understand that you may face some issues when we trail empty string at the end for your file. But many .txt files have trail spaces without sense. This change will break the behavior. \nIn general, we don't suggest to use space as an indication of missing values. A better choice is to use symbols like '?', which we support and you can customize.\nIf you have strings for that column, quote the string (\"\") is a good practice.. The new matrix library will be ready soon. BTW, which smile version do you use? I couldn't find meaning lines in EigenValueDecomposition.java according to your stack trace.. Sorry, I made so many changes recently. It is why I couldn't find the location raising the exception. The function tql2 is a standard implementation, same as you can find in python, c, or other packages. It is interesting that the other packages don't have this issue. I guess that their preprocessing is different thus that the matrix feeded into eigen decomposition is different and doesn't trigger this issue.\nI have added some safe guard to avoid the out of bound index. Meanwhile, the BLAS/LAPACK based matrix decompositions are ready. All you need is to add smile-netlib to your dependency list. For now, you have to build the master by yourself. Also make sure that you have BLAS/LAPACK available on your PATH. See https://github.com/haifengl/smile/blob/master/netlib/src/main/java/smile/netlib/package-info.java for details. . Please use the master branch and build by yourself for now. The new features are not released to maven central repo yet. Also make sure you have BLAS/LAPACK/ARPACK installed on your machine. Check the link in previous comment for details. Thanks!. Sorry I forgot to commit some file. I will do it ASAP when I can access a computer.. I have pushed all commits to master. Please git pull and build again. Thanks!. The error shows that you have not use the latest netlib base matrix computation yet. Did you include smile-netlib as the dependency?. Please do the following in the smile shell:\nsmile> val card = read.csv(\"/Users/hli/Downloads/card.csv\")\nsmile> card.summary\nSeveral observations from the output. You have several columns (e.g. V25, V26, V37) of all zeros, which will cause NaN values when calculating standard deviations. I guess that this was the root cause of pca(x, cor=true) failure. It is okay for pca(x, cor=false) because it does SVD without computing correlation matrix. PPCA is fine too as it doesn't compute standard deviations.\nSecond, your variables have very different ranges (PCA with covariance matrix doesn't make sense in this case). Please use cor = true for PCA (after removing constant columns). For KPCA, what's your kernel (and hyper-parameters)? In general, KPCA won't generate good results for common kernels (Gaussian, polynomial etc.) in your case. \nIsoMap doesn't work well either for your data (it is essentially same as KPCA with a particular kernel).\nMaybe some software doesn't throw exceptions for your data, it doesn't mean that the results are meaningful. \nOn my (old) laptop (without power cord), pca can finish in 330 ms for your data with smile-netlib.. You cannot just directly add jar. Smile-Netlib depends on third party library. You should use sbt or maven to manage library dependencies . Did you remove the constant columns?. BTW, this time the error is from NLMatrix/LAPACK. Previously, the same error came from JMatrix (our pure java implementation). So I am pretty sure that it is not a bug in our code. The matrix is singular indeed here. . LLE firstly solves a linear system for each point with its neighbors (the LU part you see in the log). Since your data is very sparse (a lot of zeros), given a small number of neighbors, the matrix can easily have a column of all zeros. It is why the matrix is singular.  . Your data is so sparse, k = 10 is not a solution. LLE is not suitable here.. The error means that the matrix is too large (array length overflows so that Java thinks it is a negative number). I suggest that your target should be evaluate if an algorithm is suitable for your data, not if a software is suitable. It is always good to understand the algorithms first.. I have add initial support netlib.\nIs it possible something wrong with your data? Things like NaN, Inf values?. Maybe you trigger some bug in smile. But you get errors with PCA, KPCA, IsoMap, LLE, etc., which actually uses different matrix decomposition algorithms (eigen value, singular value, and LU). Also no one else has ever met similar issues. I really suspect that there is something wrong in your data or preprocessing steps.. Have you tried other ML software on your data?. Do you have very large values in your data? I have been very busy and don't have time to play with your data.\nI am working on integrating netlib blas/lapack. It is a big change to my matrix library design. Need one or two more weeks.. I will check it out. Thanks!. Thanks a lot for the contributions! I like your changes overall. Only two small suggestions:\n\n\nThe better way to set the seed is smile.math.Math.setSeed(). I have tried to avoid to use Random class directly in the code. But somehow missed the old usage in CrossValidation. I have fixed it and push the change to master. Please have a look. With this design, you will always get consistent results even with a random algorithm in a single threaded application (e.g. unit tests). In a multithreaded app, you should call Math.setSeed(), which is rarely needed in production.\n\n\nIn the future, please make a git pull request only for one feature. This request actually have 4 different things. It is faster to review and merge independent requests. Thanks again!. There is no need SoftClassifierTrainer as all ClassifierTrainer specializes the return type. BTW, we have more than two types of Classifier. Your design won't work for multiple types.. Smile uses multicores when possible. But Spark does that too. So there is some conflict. You can use the JVM flag -Dsmile.threads=1 to force smile to run in single thread.. Can you provide more information about the data, code snippets of calling smile, stack trace, etc? Thanks.. Can you please provide complete code for me to reproduce? Thanks!. Do you observe this on multiple datasets? Thanks!. Thanks!. Thanks! Your github README.md says\n\n\n\"com.github.fommil.netlib\" % \"all\" % \"1.1.2\" pomOnly()\nfor dependency. Can you please point me to more detailed information of the best practice to add netlib-java dependency? I am not clear \"only need to depend on the java and system variants. The \"reference\" versions are a waste of download space.\" Thanks again!\nWe surely will add your project link to all relevant pages in our documents. Thanks for the great work!. Thank guys! I will try it.. It works well on Mac. I will test it on Windows and Linux soon. Thanks!. @fommil Do you support MKL on windows? It is easy to install it on windows and it does provide excellent performance. Thanks!. Sure I did. Let me rephrase my question. I understand that you support MKL on Linux (with alternatives). On Windows, you look for libblas3.dll and liblapack3.dll. Is it possible that you support MKL out of box on Windows (suppose its dlls are in PATH)? Thanks!. I draw the conclusion too quickly. Check the log again on Mac, I found the following warnings:\nAug 18, 2017 10:34:51 PM com.github.fommil.netlib.LAPACK <clinit>\nWARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\nAug 18, 2017 10:34:51 PM com.github.fommil.netlib.LAPACK <clinit>\nWARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\nHere is my dependencies:\n```\nlibraryDependencies += \"net.sourceforge.f2j\" % \"arpack_combined_all\" % \"0.1\"\nlibraryDependencies ++= Seq(\n  \"com.github.fommil.netlib\" % \"netlib-native_system-linux-x86_64\" % \"1.1\",\n  \"com.github.fommil.netlib\" % \"netlib-native_system-win-x86_64\"   % \"1.1\",\n  \"com.github.fommil.netlib\" % \"netlib-native_system-osx-x86_64\"   % \"1.1\",\n  \"com.github.fommil.netlib\" % \"native_system-java\"                % \"1.1\",\n  \"com.github.fommil.netlib\" % \"core\"                              % \"1.1.2\",\n  \"com.github.fommil\"        % \"jniloader\"                         % \"1.1\"\n)\n```\nWhat do I miss? Thanks!. It seems a bug with SBT (or sbt-native-packager). Although we list libraries like netlib-native_system-osx-x86_64, SBT doesn't include it in the distribution package. I revert the dependency back to all.. I submit a ticket:\nhttps://github.com/sbt/sbt-native-packager/issues/1028\nHope it can be fixed in the next 0.13 release. Thanks!. Thanks for the report. I will debug it.. Case 1 is a bug, which I fixed now. Case 2 is not really a bug as we expect a sentence starting with capital. It is not perfect but a reasonable requirement for a rule-base system. Case 3 works because na is not an abbreviations in our dictionary.. BTW, after the fix, the first case will still return only one sentence (but the whole string), which is expected by the rules.. We implement all visualization by ourselves. We just use JXTable in swingx.. there is no difference between interpolate 4x4 and 6x6 (or any other data size). What exact problem do you have?. What do you mean \"there is no change\"? Did you check the content of yy after interpolation? And you should change\nyy[i][j] = bicubic.interpolate(i*0.03, j*0.03);\nto \nyy[i][j] = bicubic.interpolate(i*0.05, j*0.05);\n. 0.03 (or 0.05) is the step of interpolation here? Note that we interpolate for 100 steps, so (5 - 0) / 100 = 0.05 for your new data.. If y changes, the interpolation results should change of course. Bicubic interpolation is CPU intensive. If you have constraints on CPU resource, try other interpolation algorithms.. It really depends on your problem/data. But in general, bicubic gives best results on regular grids in terms of quality. Try the demo program (type demo in the shell). There are examples on a variety of interpolation algorithms and see which one may fit your problem.. Thanks for your effort! However, we don't use camel case here on purpose. We choose these names to be compatible (or similar) with R or Matlab. The usage of null for parameter is to be same as the Java API.\nIn the future, please communicate first for styling things. There is no right or wrong on styling. But there are reasons why I choose the current style. Thanks!. Do you mean implement decision tree from scratch with Scala? If so, it is not necessary. \nBTW, I don't really see a big difference between for (i <- 0 until 100) and (0 until 100) foreach. These small syntax difference won't cause the difference on code readability and performance..  Xgboost is a library in c++. What do you mean \"implement it\"?. My understanding is that it's main selling point is the high performance. We support gradient boosting too besides random forest and AdaBoost. And our performance is excellent too.. See an example at line 68 here:\nhttps://github.com/haifengl/smile/blob/master/demo/src/main/java/smile/demo/classification/ClassificationDemo.java\nI will update the documents on this. Thanks!. Our README actually have example on this use case in the tutorial part.. Thanks!. Thanks! The plugins are still with 0.13 series. . Can you give more code snippets on how you call CRF?. I am afraid that featureArr or labelArr may not be in the expected way. Can you show how you create these variables.. I cannot download your data right now. But can you check if your data meet our assumptions:\nthe features are in [0, 5] and class labels in 0 or 1 for your data.\nThanks!. This CRF implementation supposes working on sparse binary data (mostly for NLP).  See\nhttps://github.com/haifengl/smile/blob/master/core/src/test/java/smile/sequence/CRFTest.java\nhow it works. And\nhttps://github.com/haifengl/smile/blob/master/shell/src/universal/data/sequence/sparse.protein.11.train\nis a sample data.. Sorry, I don't know.. This is a little bit harder than it looks like. For a dynamic language Python, it is straightforward to pass the grid of parameters (as a dict) and the trainer will pick the hyperparameters by name. But it is impossible for a static language such as Java and Scala. Even with reflection, it is pretty much impossible as the reflection cares about the data types of parameters, not the parameter name.\nA word around is that the caller pass a grid of Trainer objects. However, if the caller already generates this grid, the GridSearchCV is just a loop on the grid. I don't see much values.. Yes.. @alanmarazzi I am not familiar with Clojure. It is easier indeed to implement this in a dynamic language. My worries is how easy Java/Scala programs call it. Besides, I am working on a new design of Classifier interface, which will make grid search possible in plain Java.. @alanmarazzi What's the link to your Clojure project? Would you like to contribute it to the Smile root repo? Thanks!. I prefer an explicit configuration. It is not really bad just passing in 1/x.length in constructor.. In the Scala api smile.validation, we have test(), test2(), test2soft(), cv(), loocv(), where I print out confusion matrix. It will be available in the next release.. These model validation methods are supposed to be used in the shell and scala is a great language for this purpose. I have no plan to support them in Java.. As a functional language, scala is way better than Java for scripting. And we make most of our scala API compatible with matlab and R. It is really easy to learn our scala API.\nBut I will look into the Java shell. The big question is that Java 9 will have built in shell. Is this one compatible with Java 9?. Well, it is a notebook, not a shell. It is very interesting though. Surely it is nice to enable smile in Jupyter + BeakerX although I still strongly suggest Scala over Java in a notebook.. Java can call scala methods directly. just google how to do it.. I respect math/statistical naming conventions more than Java's.. Sure thing.. I add \"getWeight(int)\" to return the weights of a layer. 0 for input layer.. Please do this way:\nsvm.learn(new double[][]{\n                {3, 0, 0, 0},\n                {1, 0, 1, 0},\n                {0, 2, 0, 0},\n                {0, 1, 0, 0},\n                {0, 0, 1, 0},\n                {0, 0, 0, 3}\n            },\n                new int[] {1, 1, 0, 0, 1, 0}\n        );\n        svm.finish();\nAlthough SVM support online learning, it is recommended that the initial model should be learned in a batch way. You can use online learning to update the model later.. I added some safe guard so that your code can run without problems. But it is still recommended to train the initial model with a set of data. There are some extra optimization.. Thanks! Your change is okay but a little bit too much overhead. It is only necessary in the finish() function. See my latest commit.. I don't see the delimiter \",\" in your data. That is the problem.. So, the parsing is okay but the output of NumericAttributeFeature is wrong?. For which column? And it is better to provide a file for us to debug. Thanks!. Position and level should NOT be normalized as they are nominal attributes. My question was which column generates all 0 output.. Haha, it is clear now. You better say that \"numericAttributeFeature.attributes() returns empty array\".\nAnd it is not a bug. First of all, you have only three columns and by your configuration, two of them are part of x as nominal attributes, and the third one is numeric as the response variable.\ndouble[][] x = usps.toArray(new double[usps.size()][]);\nCheck the x, it has only two columns. And you should check \nusps.attributes()\nwhich returns only two attributes too and they are nominal.\nAs I said before, NumericAttributeFeature is only for numeric attribute (by its name) and ignores the nominal attributes. Of course, att.length is zero.\n. smile.math.Math has several functions such as unitize, normalize, standardize.. It is a warning, not an error. When there is no logger binding, everything is fine. There is no log output. That is it.\nHow do you run this demo code? Your project misses a slf4j binding dependency.\nBtw, try \"demo\" in the shell. You will have a window for all demos and the logger is bonded too.. All class labels are translated into numbers first. The output is a number too. You can convert the number to a string by\ntrain.response().toString(y)\nHere train is the AttributeDataset returned by the parser. y is the predicted value.. If you want to use the library in your own project, please just add the library as a dependency. The project readme has instructions how to do it with maven or sbt.\nIf you want build smile in intellij, just install sbt plugin for intellij and then import the project into intellij (File -> Import), which is simple and straightforward.. You miss other modules. You should import the whole smile project into IntelliJ, not just some of its subdirectories. Btw, you can run the demo from the shell if you download the prepackaged binary.. Thanks! I fixed the bug.. Reading the python link. So this is not a generic mutual information function but a mutual information score for validation?. Since this is a metric to evaluate clustering, please change it from object to class and implements the interface ClusterMeasure. Scala is fine. Please ignore some of my previous comments. RandIndex is an example to follow. Instead of providing two functions, move normalization parameter to the constructor.\nThe reason that it is a class instead of single instance object is that we can pass a set of ClusterMeasure as generic measures. Thanks!. Thanks! I merged the pull request and also made some minor revisions. Can you please also implement the adjust mutual information score?. Our implementation works on the generic data type. Depending on the data type, it may or may not use advanced nearest neighbor search algorithm. And we give the user option of customized nearest neighbor data structure in the constructor. Check out smile.neighbor package for available data structures.. 30k is not big. With 6-8 columns, you can use KDTree for nearest neighbor search.. How do you call it?\n. There are two partition() methods. paritition(double) cut the tree by the hight. This requires the caller has some knowledge how trees are merged, which you can get some feeling by plotting the tree out.\nparition(int) simply cut the tree into n groups. \nThis is not a bug. As the exception indicate, it is an invalid parameter.. Btw, the height is based on the cluster distance, not the discrete values of layers. . The height doesn't mean which layer to cut the tree. The height is about the linkage when merging two sub trees. The value depends on the data and I cannot tell you the min and max. Plot the tree, we drew the tree merge point based on the height.. Why don't you specify an integer k, the number of clusters?. If your goal is partition, you can try several methods that automatically discover the number of clusters. For example, x-means, g-means, minimum entropy, dbscan, etc.. Only if your data is 2 dimensional, you can plot it like the demo.\nThere is no function to do it directly. Check out smile.demo.classification.ClassifictionDemo for sample code.. If you use Scala, there is a function for this purpose. For example,\nplot(x, y, knn(x, y, 3)). See https://github.com/haifengl/smile/blob/master/demo/src/main/java/smile/demo/classification/ClassificationDemo.java#L151\nfor java example.\nBut you should be able to call a scala function in Java too. add smile-scala_2.11 to your dependency and give it a try first.. recall, precision, f-measures should be applied only to binary classification. We may raise exceptions if we see values other than 0 or 1.. By definition, they are defined for binary cases. Anyone can create extension but no one with agree with each other.. I add safe guard to these measures. We accept only 0 or 1 now and raise exceptions in case that people misuse them.\nIn scala api, we have multiple functions test(), test2(), test2soft(), cv(), loocv(), boostrap() that train and test a classifier and reports meaningful measures and confusion matrix.\nThanks!. Thanks!. Hi,\nAfter merge the pull request, I decided to remove the epochs member variable. It can be part of the Trainer but should not be part of the network itself. Note that the learn() should go through the samples only once. The caller has the flexibility how many epochs to train the model. Thanks! . What you look for is called online learning. Not all algorithms support online learning. If a classification class implements OnlineClassifier, it has a method learn(double[]) that takes a new sample. But the two algorithms you mentioned don't support this feature.. They are batch learning algorithms. I don't think that you can make them online learning.. Interesting. Thanks.. See http://haifengl.github.io/smile/data.html how the data be handled.. There were matrix multiplication methods in Math.java but I removed them on purpose. They are very slow because of the design of Java. It is recommended to use smile.math.matrix.Matrix for this operation.. It maybe a good idea to separate online learning version out. Most time, people use the batch version and it is not necessary to calculate all extra work for online learning part.. Thank you!. I clean up the code a little bit.\n\nAlways use SVD instead of QR because it is more stable when the data is close to rank deficient, which is more likely in RLS as the initial data size may be small.\nremove duplicate w, b, and W\nkeep only the constructor with batch initialization. The other don't initialize gamma/V properly.. Thanks! But I already updated the copyright before. There are many conflicts between this pull request and the master branch.. why do we need updatedVx? It is same as Vx, right?. don't do matrix operation manually. ax() is much more efficient.. Thank you!. Do you add smile-netllib to dependency? If so, smile will use a native implement ion, which may be more numeric stable. It is a quick way for you to move forward. I will look into this problem anyway but it may take some time. Thanks!. I cannot reproduce the problem in the shell\n\n```\nsmile> val a: DenseMatrix = ones(100, 100) * 0.0071 \na: DenseMatrix =   0.0071    0.0071    0.0071    0.0071    0.0071    0.0071    0.0071  ...\n  0.0071    0.0071    0.0071    0.0071    0.0071    0.0071    0.0071  ...\n  0.0071    0.0071    0.0071    0.0071    0.0071    0.0071    0.0071  ...\n  0.0071    0.0071    0.0071    0.0071    0.0071    0.0071    0.0071  ...\n  0.0071    0.0071    0.0071    0.0071    0.0071    0.0071    0.0071  ...\n  0.0071    0.0071    0.0071    0.0071    0.0071    0.0071    0.0071  ...\n  0.0071    0.0071    0.0071    0.0071    0.0071    0.0071    0.0071  ...\n  ...\nsmile> a.setSymmetric(true) \nsmile> val e = eigen(a,2) \n[main] INFO smile.netlib.ARPACK - ARPACK: 6 iterations for Matrix of size 100\n[main] INFO smile.netlib.ARPACK - ARPACK computed 2 eigenvalues\ne: EVD = smile.math.matrix.EVD@113337a0\nsmile> e.getEigenValues \nres17: Array[Double] = Array(0.7100000000000003, 2.403532089570415E-19)\n```. BTW, how did you pin the issue to the line 360? Thanks!. I tried on 5000 x 5000 matrix too.. Thanks! the function startv tries to find initial vectors. If it fails, we will get fewer vectors. But the line 360 is the same logic that you can find in other implementation including native fortran code. Not sure why it happens in our java code.. I think that our eigen decomposition behavior for your matrix is correct. Your matrix is of same values and thus has only one eigen vector. Therefore, our Lanczos implementation converges to 1 eigen vector. The native implementation returns two but the second one is close to zero. Besides, the second returned vector could be random according to mathematical theory.\nBut I would enhance MDS in case that the user gives a too big k. Thanks!. I actually don't throw exceptions in this case. For MDS and manifold learning (major user of eigen(k)), I will log a warning when getting fewer eigen vectors and returns a fewer-dimensional results. These are still useful in practice as they are mostly for visualization. It is better than an invalid array index exception.. By default, the shell use only 4GB memory. You are creating three large matrices, which exceeds 4GB. You can change the JVM options in the conf/smile.ini. Or you can pass the option -J-Xmx16GB to command line.. Thanks for reporting this. But I can not reproduce it on Mac, Windows, and RHEL. I don't have a native Ubuntu 16.04. But I tried in Windows Linux Subsystem, which is actually Ubuntu 16.04. It doesn't have issue either there. Do you observe this issue every time? Or only for some matrix?\nThe error seems from the underlying BLAS/Lapack library. I suspect that there is some issue in your environment.. I just used the default version:\nsudo apt-get install libblas-dev liblapack-dev. Since it happens every time, I am more confident that it is an environment issue.. Do you have another ubuntu machine? First we want to make sure if this issue happens with all ubuntu machines or just your current one. Thanks!. Can you try oracle JDK?. I can NOT reproduce it. To debug, please clone and build smile on your local machine.. I use jave8 and scala 2.12.3/2.11.8.. Any updates? When you create your scala project, do you also include smile-netlib jar? Smile shell uses it to use native BLAS/LAPACK library. Without it, we will fall back to pure Java implementation by smile. Your segment error seems from BLAS/LAPACK or JNI. Wihout smile-netlib, I guess that we won't get this error.. $ ldd /tmp/jniloader3583089873415736808netlib-native_system-linux-x86_64.so\n        linux-vdso.so.1 =>  (0x00007fffcb527000)\n        libgfortran.so.3 => /usr/lib/x86_64-linux-gnu/libgfortran.so.3 (0x00007f531fc00000)\n        libblas.so.3 => /usr/lib/libblas.so.3 (0x00007f531f990000)\n        liblapack.so.3 => /usr/lib/liblapack.so.3 (0x00007f531f190000)\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f531edc0000)\n        libquadmath.so.0 => /usr/lib/x86_64-linux-gnu/libquadmath.so.0 (0x00007f531eb80000)\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f531e860000)\n        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f531e640000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007f5320400000). You have both libblas and libopenblas. Maybe it is the problem?. We actually use OpenBlas on Windows and have no issues. But I use a prebuit version, which I don't know if USE_OPENMP=1.\nIt is interesting that libblas is much slower than OpenBlas for this use case. How much slower? Thanks!. Something is wrong with your libblas. I used libblas on RHEL7 and Windows Linux Subsystem, and it is very fast this operation, definitely subseconds. With even pure Java implementation, it won't take 24 seconds for 2000 x 2000 matrix multiplication.. I know you are benchmarking. But for production, please don't use a.t %*% a, we have a.ata (and aat) for the same purpose, which should be faster in general.. Yes, it is nice to make it a base class.. Sure. Thanks. Several things to note:\n\nIt is very straightforward to support SGD in binary case. But for multiple class, it is quite different especially our implementation trains the model in one optimization.\nDon't touch the existing algorithm. Make the class support OnlineClassifier and just implement the additional method for model update.. Thanks!. I guess that you mean regularization. Check out LASSO, which has L1 regularization for linear regression.\n\nAdditional work is needed for logistic/maxent. It is a big work. It is not just an option. It is whole new algorithms :) . How do you have more classes with predictions?. We will enhance confusion matrix. But seriously why don\u2019t you include all classes in the test data? It doesn\u2019t sound a good way to test the model to me.. The enhancement is pushed to the master branch. Thanks!. Thanks!. There is ambiguous implicit conversion. Both our API and Scala's Predef convert Array implicitly. Let me think of a way to avoid the conflicts while maintaining the functionality.. I removed Traversable trait from our VectorExpression, which resolves this conflicts. However, you will have to call toArray first on our scala vector expression first to use map and other traversal functions.. Do you mean topic classification in NLP?.  No LDA right now. But we have unsupervised key word extraction, which may be useful to you.. Thanks!. thanks!. Please see the paper for details.\nPlatt, John (1999). \"Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods\". Advances in large margin classifiers. 10 (3): 61\u201374.\n. Several things to notice.\n\n\nOur design on tree size regularization is different from sklearn. I believe that sklearn use the max depth. It doesn't make much sense to set 49500 in our case to match their settings. The OOB accuracy = 99.9% with this setting strongly indicates overfitting. A much smaller maxNodes (try 250 - 1000 first) should be used.\n\n\nSimilarly, nodeSize = 1 is way too small for this big data set. It should be (much) larger.\n\n\nIt will be helpful to see your code, especially how you create featureAttributes.\n\n\nEach tree takes 4 hours to train with this setting, but the first tree takes 24 hours. There is 20-hour overhead, which indicates something wrong with JVM (especially GC). What's your JVM parameters?\n\n\nPlease use Oracle JDK instead of OpenJDK. Somehow OpenJDK is very slow for some numeric computations.\n\n\nIs your data balanced?\n\n\nSmile handles categorical variables directly but sklearn use one-hot encoding. Make sure NOT use one-hot for Smile.\n. OOB size doesn't look right.. Thanks for updates! Can you try -Xms100g? Set it same as the -Xmx so that we can avoid potential GC. What's bug with OOB?. Can you try subsample = 0.65? Your data is big, please increase maxNodes to 1000. Can you please share the settings of featureAttributes? Thanks!. Other systems requires one-hot encoding because they cannot handle categorical variables. Smile can handle categorical variables directly and it is faster to do so. So we strongly discourage the usage of one-hot encoding.. It is on purpose to jump over the following loop. I probably should just comment that part out. Thanks!. Thank you!. Will fix. Thanks!. Fixed. Thanks!. set -Dsmile.threads=1 for JVM and call smile.math.Math.setSeed before training the model. If you just want repeatable result, the first step should be sufficient.. Thanks!. Smile is built with Java 8. . No. We cannot import proprietary serialization of other systems.. You talk two different ideas here. First, It is not salable to build decision trees manually. And it is probably out of reach to manually build random forest of 500 trees (even just forgetting tricky things like sampling and correlation between these trees). We have no intents in this area.\n\n\nFor your second paragraph, you should look into PMML or PFA.. Okay . Thank you very much! We fixed the bug as you suggest.. Any theory support for this feature? Paper reference?. We need theoretical support that this is a legitimate operation.. I will add a combine function in scala package. This should not be in the core algorithm. Thanks.. Thanks!. You can check the book \"numerical recipe\" for details. BTW, you can call java library from matlab easily.. You are solving a regression problem, not classification. First of all, you should use SVR or RandomForest in smile.regression package. Second,\ndouble[] y = train.toArray(new double[train.size()]);. For cross validation, see smile.validation package. It is better to read the user guide first http://haifengl.github.io/smile/validation.html\nBesides, it is better to use scala api, which is similar to scikit. And you can do it interactively in the shell.\n. parser.setColumnNames(true)\nparser.setRowNames(true) // you better do this to handle the id column. With this setting, the id will be the id of row, not the data itself.\nWe have high quality javadoc at http://haifengl.github.io/smile/api/java/index.html. You should not have the id attribute now. The 3rd column amenities is troublesome. Thanks for interest in smile. However, we have very limited time and resources. We expect the users to solve the basic programming issues by studying tutorials, user guide and javadoc with themselves. We are happy to help on the core machine learning algorithms.. StringAttribute and NominalAttribute are different things. Many of your attributes are simply long strings, not nominal variables. SVM and RandomForest cannot handle string attributes. you should filter them out first.\nMost of your problems are related to format. You use \nparser.setDelimiter(\",\");\nbut you have comma \",\" in many your columns too, which breaks the format. Please understand what you are doing first. We don't have resources on debugging these kind of trivial things.. This is very interesting. I will try to figure out what's wrong ASAP. Thanks a lot! . I fixed the nextInt of UniversalGenerator. Thanks.. UniversalGenerator and MersenneTwister work in different ways. MersenneTwister generates random bits (and thus integer) and then we scale it to double value in [0, 1). On the other hand, UniversalGenerator generates real values directly in [0, 1) then we scale it to an integer. Since it generates values only in [0, 1), not all bits would be used by nature. The bug was caused by that we adopt MersenneTwister's nextInt to UniversalGenerator. Because they are not compatible algorithms, this cause the issues.\nEither way has its pros and cons. It is probably a good idea that only use UniversalGenerator for nextDouble while MersenneTwister for nextInt.\nBTW, you pull requests includes a lot of unnecessary white space changes, which is not desired. Thanks.. In smile.math.Random, we use UniversalGenerator for nextDouble and MersennaTwister for nextInt now.. The project was developed by myself for several years before open source. When I open sourced it, all major algorithms (> 95%) were there already. Smile has very high quality, proven by the small number of bug reports even with wide usage. We get ten thousands downloads every month. Many leading technology companies use smile based on private communication. Because of these two reasons, the number of additional contributions has been small.\nIn addition, to contribute to core machine learning library with general purpose language like Java/Scala,  the developers have to be very good at both mathematics and programming. Therefore, the contributor pool is very small by nature.. Feel free to open tickets or ask on gitter if you have any questions on using smile. Thanks.. We use one of most advanced algorithm for hclust. In theory, it should be very fast. I guess that the slowness is again because of JVM's memory management. The distance matrix double[][] is really not modern CPU friendly. I will change the underlying data structure to speed it up.. I made some optimization and it takes 9 seconds on my 4 year old laptop for a test data of 14470 samples. Try the master branch and see if it works better now on your data.\nBTW, R's hclust is implemented by C underlying. If they use the same algorithm as ours, it will be hard for Java to win in this case. Basically, the array index checking in Java is a performance killer.. It takes 8.8s on my laptop (unplugged).\n```\nsmile> val x = read.csv(\"trainData.csv\",header=true).unzip\nx: Array[Array[Double]] = Array(\n  Array(0.32919254658385094, 0.16779060123827824),\n  Array(0.3105590062111801, 0.19826924340214483),\n  Array(0.7018633540372671, 0.33629456734164326),\n  Array(0.515527950310559, 0.10448280534069074),\n  Array(0.37888198757763975, 0.14139610123006685),\n  Array(0.30434782608695654, 0.13457756482895666),\n  Array(0.14906832298136646, 0.13143054802844428),\n  Array(0.2484472049689441, 0.15725538256885255),\n  Array(0.2732919254658385, 0.15380146491271288),\n  Array(0.22981366459627328, 0.30400736562053504),\n  Array(0.12422360248447205, 0.2641061486919249),\n  Array(0.5652173913043478, 0.1875),\n  Array(0.3416149068322981, 0.19664749306137197),\n  Array(0.32298136645962733, 0.1931935754052323),\n  Array(0.2608695652173913, 0.16732152534857367),\n  Array(0.2670807453416149, 0.1839978404033437),\n  Array(0.36645962732919257, 0.2119586638419471),\n  Array(0.5527950310559007, 0.22162347473354027),\n  Array(0.4658385093167702, 0.20391560329112676),\n  Array(0.5093167701863354, 0.19729106107634956),\n  Array(0.6645962732919255, 0.18380589906554334),\n  Array(0.2919254658385093, 0.1718193164835526),\n  Array(0.6149068322981367, 0.14651898474322972),\n  Array(0.5962732919254659, 0.19937059663989753),\n...\nsmile> val d = pdist(x)\nd: Array[Array[Double]] = Array(\n  Array(0.0),\n  Array(0.03572333208105006, 0.0),\n  Array(0.40899525342055637, 0.4149338292742624, 0.0),\n  Array(0.1967962390479195, 0.2254066636314669, 0.29741818317645197, 0.0),\n  Array(0.05626464411118842, 0.08889647958905339, 0.3772298704672966, 0.1415440233447636, 0.0),\n  Array(0.04147729407875742, 0.06399381749830109, 0.4457671410773985, 0.2133137112699339, 0.07484539844086947, 0.0),\n  Array(0.1837574199803093, 0.17477600513025338, 0.5895350818816917, 0.3674490975607588, 0.23002963436432003, 0.1553113897930917, 0.0),\n  Array(0.08142973059589859, 0.07443126113406716, 0.4874845986490785, 0.2722445030474166, 0.13139539306520776, 0.060325474377111325, 0.10267952212656789, 0.0),\n  Array(0.0576244338498421, 0.05801912297738562, 0.46580811695578167, 0.24720562678578542, 0.10631629350608848, 0.03652433843095588, 0.1262218734469703, 0.02508365371599998, 0.0),\n  Array(\n    0.1686154473465351,\n    0.13304270247848707,\n    0.4731525892313369,\n    0.34848630276682285,\n    0.2205987040377702,\n    0.18509942902503287,\n    0.1905323283926425,\n    0.14793023138094952,\n    0.1563719021297947,\n    0.0\n  ),\n  Array(\n    0.22647064429305172,\n    0.19762434257988248,\n...\nsmile> val clusters = hclust(d, \"complete\")\n[main] INFO smile.util.package$ - runtime: 8880.695798 ms\nclusters: HierarchicalClustering = smile.clustering.HierarchicalClustering@7725470b\n```. I use oracle jvm with these options (they are default with smile shell)\n-Xmx4096M\n-Xms1024M\n-server\n-XX:+AggressiveOpts\n-XX:+UseG1GC\n-XX:+UseStringDeduplication\n-XX:-PrintGCDetails . Do you install jdk in cygwin? If not, it will pick up the one from windows. Things are more complicated then. \nIf you have a linux box, it is probably easier way for you to try this out. BTW, I am not sure if -server makes that difference.. You can build and run smile without cygwin. In the source dir, run\nsbt stage\nthe binary will be available in shell/target/universal/stage/bin.\nI am not sure either that Intellij introduces the noise. Something else probably.. I released v1.5.1, which includes this enhancement. Please make sure to use Oracle JDK. I know that OpenJDK has performance issues with some other algorithms in smile.. What's your current performance and settings (hardware & software)?. Thanks for reporting this. I change the logic to\nif (description != null && a.description != null) {\n    return description.equals(a.description);\n} else {\n    return description == a.description;\n}. Please see the new comments, which are unfortunately associated with old commits. click \"show outdated\" to see them. thanks.. Scala can use Java (distance) class directly. And it is not necessary to duplicate things like HammingDistance.. Thanks. Can you please add some comments/documents (including the reference) on all the classes? The library means to be used by others. Without documents, it is hard for others to leverage your work.. Please make the changes according to latest comments. I will then merge the PR. Thanks.. Thank you! I will handle the rest.. I refactor the code in a more functional way. Please have a look. If you can add some unit tests to make sure my changes be okay, it would be great. Thanks.. We always include intercept in logistic regression. It is part of weight vector.. In the log? Yes, it is negative log likelihood. The optimization method is out of logistic regression and it just prints out the objective function value. I don't think it says what it is.. in general likelihood < 1 and thus log likelihood is negative.. You misunderstand the regularization, which penalizes the weights. And in general it does make the loglikelihood smaller.. Is this only for regression? The repetitive always have the same response values? Thanks.. The weight will have big impact on the sampling strategy and approximate split point determination. What's your algorithm on these points?. Thanks. I am refactoring DecisionTree/RegressionTree for performance improvement. After that, I may merge this feature.. Thanks!. In smile.validation.Validation we have several Java helper functions for this purpose. \nWith Scala API, it is even more convenient. For example,\ncv(x, y, 10) { case (x, y) => lda(x, y) }\nSee http://haifengl.github.io/smile/validation.html for more example.\n. Sorry for the problem. It is fixed in the master branch now.. KMeans.centroids() returns the centroids of each cluster. PartitionClustering.getClusterLabel() returns the cluster labels of input data. KMeans is a subclass of PartitionClustering.. Yes, you are right.. Thanks. Since you take out the code to a new function, the diff doesn't show the exact changes. Can you please explain what are exactly your changes?. Thanks. Have you try your changes on large datasets, especially with big itemsets? I chose non-recursive implement ion with the consideration of performance and also avoid of stack overflow. If we can keep non-recursive implementation, it would be great.. What do you mean \"not obvious\"? On what data?. Thank you!. Thanks for PR!. So it is okay with netlib but fails with JMatrix? BTW, what's ru.msu.cmc.sp in your error trace?\nat ru.msu.cmc.sp.kriging.JMatrix.get(JMatrix.java:137)\n    at ru.msu.cmc.sp.kriging.LU.solve(LU.java:136)\n    at ru.msu.cmc.sp.kriging.LU.solve(LU.java:110)\n    at ru.msu.cmc.sp.kriging.KrigingInterpolation.<init>(Kriging.java:63)\n    at ru.msu.cmc.sp.kriging.KrigingInterpolation.<init>(Kriging.java:19)\n    at ru.msu.cmc.sp.kriging.KrigingInterpolation.test(KrigingTest.java:131). What's your environment? OS? JDK?. Yes, your approach (SVM with bag of words) is okay as the start point. Depending on your kernel, you may want to normalize the feature vector. Besides, the dimension is a little bit high (what do you mean \"increasing the dimensionality of the feature space to ~ 10k\"). It might be good to try some feature selection or reduction techniques.. Do you include smile in your project dependency?. You should use maven for dependency. If you cannot, I guess that you copy only smile-core to your lib directory, you should at least also include smile-math and smile-data. If you google about this kind of basic java package problems.. Please provide the details how you call DBScan. Thanks.. In smile shell, type demo. In the GUI, choose Clustering/DBScan and choose data Chameleon/t7.10k, which has 10k points. Just use the default parameters MinPts = 10 and Range = 10. DBScan can finish in 240 ms on my PC. Your settings on MinPts and Range may not be right.. BTW, DBScan is designed for spatial data (2 or 3 dimensional).. Please provide detailed information. What's exactly data? Is it the raw array or KDTree? You should pass KDTree. Lacking of information, it is really hard to help you. In the future, please follow our ticket template.\nMinPoints = 300 is absolutely too big. Range = 1000 seems very large although I have not see your sample data yet.\nAlso I suggest you read the DBScan paper first.. Please share some sample data. And pass in KDTree, not the raw array.. StringAttribute is not directly supported by any algorithm. It is only a step stone to generate Nominal or Numeric attributes.. Can you work on this? Thanks.. Document is updated.. Which paper do you refer to? \"Fast Kernel Classifiers with Online and Active Learning\"?. Your guess is right. Note the code at line 795\nif (y > 0) {\n                v.cmin = 0;\n                v.cmax = weight * Cp;\n            } else {\n                v.cmin = -weight * Cn;\n                v.cmax = 0;\n            }\nMathematically, it should be v.cmax == 0. But for numeric stability with FPU, I use 0 >= c.cmax instead of v.cmax == 0.. Array index is useless as a feature. Even if it finishes with normalization, the model is not trustworthy. . Thanks. I have added coverage on java api in this page. Will push to website soon. . Will do. . See PR #336 . We could make it public for two-class case. But first of all, it is not probability at all.. Mathematically it is not really a distance. If you really want the probability, we do compute it based on Platt scaling.. We will make it accessible.. add getRoot() method to DecisionTree and RegressionTree.. What's your input data and how do you call FPGrowth?. The input should be sparse. For example, if a transaction has items 1 and 12, then the input row should be just [1, 12]. I guess that your input is something like [0, 1, 0, 0, ..., 1, 0, ...].. Can you provide more code snippets of your program? What're bkTreePortName, e, and dist?. We can make it serializable. But for large dataset, it may be huge and takes time to serialize.. KNN is a lazy learning algorithm. Basically it doesn't learn :)\nYou really just want to serialize the nearest neighbor search data structure if you have to serialize a KNN model.. I have make all nearest neighbor search data structures, including CoverTree, serializable. The changes will be available in the next release.. check out smile.imputation package. We provide several ways to impute missing values. This is more generic and work for other classification algorithms.. This will be a big change. Typically decision trees are binary trees. What you suggest is something like three-value-logic in database domain. Also this logic is different from the standard way to handle missing values in CART. I am afraid that it will introduce confusions to other users. Lastly, this design will be highly unstable given the ratio of missing values.\nTherefore, I don't intent to implement it. I suggest that you discretize your numeric variable and encode missing values properly first. This work around is not perfect but should be sufficient in many cases.. We expect the item IDs are compact, that is in the range of [0, numItems). For any meaningful data, the header table size is about numItems. BTW, numItems is typically a small number. I don't see that such a change will change the footprint of program notably.. Thanks for you effort. But I prefer simple code than a change that has very minimal impact on the performance. Actually, I don't expect any benchmark difference for this change. Have you run the unit tests to check the run time?. Thanks. But with the original implementation, I get the following metrics:\nkosarak\n990002 transactions, 41270 items\nDone building FP-tree: 3.83 secs.\n219725 frequent item sets discovered: 2.64 secs.\nBTW, my machine is old. I don't know why it takes much more time on your machine.. BTW, in the future, please don't create both a ticket and PR for the same thing. We can exchange comments in the PR too. Thanks.. Your change is about building FT-Tree. I don't see there is real difference for that part. The time difference in the frequent item set mining should be caused by other reasons.. I have 4 threads too. Beside, it is single-threading for building FP-tree.\nIt is different between spawning an FP-Tree and create a new one. It is much simpler with spawning and I don't think that it gets involves your changes either.. Thanks. But as I explained before, it may not be a good idea to incorporate such a missing value handling feature. It is not standard and many mathematical results (e.g. split metrics) may not hold any more given your changes.. There are many research in this area but I am not very familiar. . Thanks for using smile. Due to limited time and resource, we are not able to debug other's code.. Thanks. Check the book Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schutze. Introduction to Information Retrieval, Chapter 13.. It should be a bug. Let me fix it. Thanks.. I fixed all of them. Thanks.. Thanks!. Since you care about it, can you fix and make a pull request? Thanks.. For functional programming, I try to minimize the usage of var. for loop is just a syntactic sugar in Scala, which is translated to map or foreach by compiler. Thanks.. KModesModel is actually not used any more. I will remove it.. Thanks!. What do you mean \"can not Show the clustered data\"?. DENCLUE.getClusterLabel() returns the cluster labels of data.. BTW, you can find Smile's javadoc at http://haifengl.github.io/smile/api/java/index.html.. I cannot repeat this error. You should not call DENCLUE.getClusterLabel(). Instead call vbbx.getClusterLabel() after the clustering.. Thanks!. Thanks. Why Spark for only 10000 samples? You can train it with smile directly on a single machine. Smile take simple double[][] as input for most algorithm. If you have to use spark, extract data from your RDD or DataFrame to double[][].. If you have to use Spark, I assume that the data is too big to be trained by smile. You can still use smile with Spark for prediction though.. @rayeaster no.. Check this paper https://ieeexplore.ieee.org/abstract/document/1498032/. LDA takes class labels when calculating C. PCA doesn't.. For more details, check the book Pattern Classification by Duda, Hart, & Stork.. What abnormal behavior do you observe? Thanks.. I think that we just change line 118 to\nif (score[i] <= 0.0)\nThe lines 116 - 121 are to keep old clusters without split.. I fixed the bug in X-Means and G-Means. The fix is available in master branch now.. Thanks. What about other datasets, e.g. unit tests? The demo is only for, well, \"demo\". They are not serious test cases.. There is no U.abmm(U.atbmm(B)). You missed the lines 291-295. We actually calculate inv(S_t) * S_b.. It is very expensive and instable to calculate inv(T) directly. In general, it is not a good idea to translate your math formula to code directly. Computational mathematics studies how to calculate them quickly and reliably. It is the gap you missed.. For all the data you tested, we should get the same results with the old implementation and new one because the data do not in the small sample size problem as discussed in the paper. The difference may be caused by the numerical stability. Iris has only 4 variables. So it is probably better that we use the old implementation that works on T (4x4 matrix, very small). For other data with high dimensions, we should use the fast algorithm/new implementation.\nOf course, the difference may also caused by some bug :) I will take some time to review both implementations. Thanks!. Yes, it is what I would like.. I will cherry pick your PR. I still believe that two algorithms should generate the same model for the test data in the unit tests. These data don't have the small sample size problem. So the two algorithms should be same mathematically.. Mathematically, inv(S_t) * S_b and inv(S_w) * S_b are same in sense of optimizing the FLD criterion when S_w is not singular.. Your logic is wrong. We don't have time to debug other's code given limited time and resource. To help you, I just list some mistakes in your code. Please don't open tickets about this again. Each\n line in the input data is an instance. But you are clustering the (incomplete) data for each word in each line. It makes no senses. The clustering should be called only once after you parse all the data.\nBesides, density based clustering is not suitable for high dimensional data. This data has 57 variables. It is better to try something else.. Thanks!. Thanks. The overall process is okay. In fact, our SVM implementation is incremental by natural. The SVM::finish() is to refine learned support vectors. However, there are several things that you may try to improve the results:\n\nMake sure your data is randomized. Note that you have 100 or 200 parts. So each part may not contain sufficient samples in each class.\nHow many sample do you have? If not too many, 100 or 200 parts are too many. Try smaller number of parts if so.. It is very common to generate millions association rules, which I didn't have memory issues if printing the results to a file. How much memory did you allocate to JVM? Try giving it more with -Xmx option.. High support and confidence is the way to go. Not only reduce the number of generated rules, they also give you most important ones.. The max depth 10 doesn't mean 1024 leaf nodes in smile. These are two different regularization approaches. In general, 1024 is too big for many small/medium data size. You have only 2208 samples, 1024 leaf nodes are way too many.. Some hyperparameter tuning is needed. But for your case, some quick trail-n-fail should be sufficient to get similar results.. @JoshCason please contact me at haifeng dot hli at gmail. I will give you some industry use cases. Thanks.. Add some testimonials to the project homepage. More will come.. See this page for some practical suggestions.. Not really. It is just a heuristics, not a golden rule. For these hyper-parameters, they should be chosen based on your data. No trail-n-error tests are needed.. Check the book \"numerical recipes\". I don't have some theoretical ways to calculate the hyperparameters. As suggested before, it is more a trial-n-error process. Interpolation is for low dimensional data (2 or 3 dimension). Some visual check can help you choose the hyperparameters.. We have Gaussian Mixture Model implementations. They are in the class of MultivariateGaussianMixture. We actually support more advanced mixture model in MultivariateExponentialFamilyMixture. Thanks.. Thanks for quickly implementing this. It is interesting that the error rate is higher after one more epoch. Shall we shuffle the samples first? May be the samples are ordered by their class labels. It is not good for SGD.. There is permutate method in smile.math.Math. I think that the unit test with SGD should be revised. Note that our batch learning reaches the global optimal. By online learning with the same data, we won't get better results (actually could be worse). I suggest that we train with only part of data (says 80% samples) with batch algorithm. And then update the model with online learning with the rest of samples.. Thanks!. Add a jupyter-beaker example page on the project website. Thanks.. We can add that function. Currently you work around with the function residuals. It is easy to calculate the fitted values from it.. Add getFittedValues() method.. Thanks. We have AttributeDataset that provide attribute name and other info. RandomForest, for example, takes it as input.. Thanks!. Unit test fails with this change. I fixed it. Please make sure to run unit tests first for PRs. Thanks.. Thanks. On the right, click the package smile and it will show the classes. It used to work. Not sure if a change in scaladoc format. Will investigate. Thanks.. K-modes supposes working on binary data therefore the input should be either 0 or 1. It is a new added algorithm. We will add more documents and examples. . Thanks. Although it is more flexible, we prefer a simpler API and compatibility with R. It is rare that people has header in other rows.. It is hard to find a real need for this. If the first row is not the header, it is unlikely the data either. Probably other metadata. It is hard to develop a general method for text/csv files with multi-row metadata. We have other dedicated methods for these cases, e.g. arff and a variety of gene expression data. I would like to keep the text/csv parser simple. Thanks.. Thanks.. See the project website how to train a model.\n\nval centers = new Array[Array[Double]](50)\n    val basis = gaussrbf(x, centers)\n    val classifier = rbfnet(x, y, new EuclideanDistance, basis, centers)\n    plot(x, y, classifier)\nYou can save a model as shown in FAQ.\n. Thanks for the bug report. The fix is in the master branch now.. 30k x 100k of course takes a lot of space. But your data should be highly sparse. I suggest that you use a sparse encoding of your data and create a customized kernel that recognizes your data format. In fact, some of our kernels take sparse encoding. Check out BinarySparse*Kernel classes in the package smile.math.kernel.. Yes. . They are not really machine learning algorithm. But if you want, you may implement it in the math module.. Maybe. Can it support the methods of Projection interface?. No. They are hyper-parameters.. Can you add documents/comments to all methods? Thanks.. ICA is good for cocktail party problem. Do the data in your demo/test make sense for it?. Can we rename the class name to ICA? The interface should reflect the purpose only. FastICA is only one way to do the computation. The user should not bother the implementation details. Thanks.. I don't have test data for cocktail problem. There should be some standard benchmark data for ICA. It is also a good opportunity for you to show the correctness of the implementation on the benchmark data.. Thanks. Do you use tab in the code? I observe it in your commits across multiple PR. Can you set tab -> space in your IDE and also set indent to 4?. sounds good. . merged. Thanks.. ElasticNet is essentially equivalent linear SVM. Have you compared the accuracy between them? Thanks.. Thanks a lot!. It is possible to get different decision trees with different order of features if two features have the same reduction on the criterion. This happens more likely if two features are highly correlated or the sample size is small.. In general, they should not. But maybe you have samples with same values. We sort samples for each numeric features first. If you shuffle the samples, the order may be different for samples with same values, then the results may be slightly different.. Probabilistic Outputs for Support Vector Machines and Comparisons to\nRegularized Likelihood Methods\nhttps://www.researchgate.net/profile/John_Platt/publication/2594015_Probabilistic_Outputs_for_Support_Vector_Machines_and_Comparisons_to_Regularized_Likelihood_Methods/links/004635154cff5262d6000000.pdf. in what case, you (or some method) calls gamma with negative values? I wouldn't call the behavior wrong. The caller may do some safe guard before calling gamma.. Thanks. The problem is more tricky than what you suggest. I will look into this.. The problem comes from Gamma.lgamma(-111.0). And python and R fails too with this value for lgamma. So first of all, it is not a bug of lgamma. The input alpha actually should be a positive value from LASSO's point of view. I guess that your sample size is smaller than dimensionality. Is it right? Our algorithm is not designed for this case.. gamma function part is for p-value calculation. If it is not important to you, try commenting that part out. . I add a check before p-value computation. We will skip it if df2 is not positive. It should be fine with your case.. Thanks. I cherry picked your fix.. We don't support such kind of artificial data. You basically have only two unique data points (1, 1) and (1, 0) but assign them random labels. The objective function may not make sense at all and thus line search fails.. See FAQ http://haifengl.github.io/smile/faq.html. Thanks.. BFGS issues may be resolved by tuning hyper-parameters. Besides, if you are familiar with Java, please set a breakpoint to check the objective function values in the loop. Just to ensure we don't get NaN during the iteration.. For sparse data (OneHot), you better use Maxent. Mathematically, it is same as LogisticRegression. But it is more efficient with sparse encoded data.. Significance test for Spearman, Pearson and Kendall are in smile.stat.hypothesis.CorTest. LASSO has the methods ftest() and pvalue() for goodness of fit test.. Why do we need this? First of all, this doesn't control the iterations of SVM. Second, for finish(), we need to reprocess all SVs to refine the model. If this part is too long to you, it means that there are too many SVs and you should tuning your kernel, hyper-parameters, regularization, etc.. If smo doesn't return, we better figure out why. Your revision doesn't solve the real problem. Can you share your data and settings that causes it never returns?. You actually can pass a parameter epsgr to finish() to reduce the iterations. Why don't you try that?. I add maxIter to finish(). If reprocess doesn't finish timely, it is more likely that your kernel and hyper-parameters are not suitable. I would rather to try other algorithms.. CrossValidation is only for generate the random splits (index). You can call the cv function (in Scala), which take a code block that you can do anything you want. BTW, you can call a scala function in Java too.. For now, you can use your own branch build. I will revise the interface in next release.. Use Maxent.. We have to make abstract class serializable to serialize child classes?. Thanks!. Cool. Thanks.. Thanks. Is it a real case of missing category?. I see. Will fix it.. The bug fix is in the master branch now. Note that the old implementation assume that the training data has all classes, which is reasonable.\nNow since you trigger this bug, some of your sub test sets miss some class at all. The trained model (and test results) may not be trustable.. NominalAttribute is not Strings. Internally they are integers. NumericAttribute are double values and thus can be used as response in regression, not labels in classification.. It is defined in parent interface and is available for all classification algorithms . Smile doesn't have normalization in LASSO. Normalization/standardization can be done before the training with algorithms in smile.feature.. Thanks. We shall move this into an utility function shared by all classifiers.. All Distributions are serializable now. Thanks.. Thanks. It is beyond the java array index range. It is a limitation of JVM.. Smile started with Java 6 long time ago. The cross validation API has many limitation today due to the strong type of Java. I will change the API to leverage lambda, which is more flexible to add a train method/interface.. if you look into the algorithm details, training is not a natural part of many missing value imputation algorithms. Cross validation is more meaning for supervised learning.. row-wise means is very typical for sensor and biological data. All other algorithms in smile.imputation are essentially of column-wise. The difference is how to find the local scope, which is in general much better than the average of whole column.\nNote that the whole column average doesn't make sense in machine learning. That means that we assume that all samples follow the same distribution for that attribute.. I mean that each class has its own distribution (e.g. different means in case of Gaussian distribution). Calculating the mean of the mixture of all classes doesn't make much sense. All other algorithms in smile calculates the mean in a small neighborhood of the sample. The difference is how to find the neighbors.. For reference, google some papers on microarray (gene expression). Each row is the expressions of a gene in different tissues/environments/patients/...\nRow means is more meaningful because it is about the same gene. The column mean over all genes has no meaning.. It is upgraded to 1.4.11.1 now. Thanks.\nBTW, only the scala module has this dependency. If you just use the core java smile libraries, you are fine. . Models are immutable. Why do you want tot set weights? I may add a method to get the weights though.. add -Dsmile.threads=1 to jvm options.. It is in ~/.smile.. lambda is the step size in diagonal Newton method. The above condition is for early stop as the step is so small that the search won't progress much in general. In your case, the objective function is probably very flat given your data. So we may want no early stop as long as you don't care the time. How many iterations it keeps going after lambda is smaller than 0.001?\nWe may remove the early stop behavior or add an additional parameter to enable/disable early stop.. We use a stochastic algorithm to train SVM. Thus, the model may be slightly different each time. You may run SVM.learn(X, y) multiple times (typically twice is sufficient), the difference should be very small.. I don't get your problem. Can you explain? Do you make the prediction during the training? Do you use SVM or SVR?. Can you share your training and testing code?. As said, we use a stochastic algorithm to train SVM. That is, a random number generator is used in the algorithm. In your test, when you do the first test, the random number generator uses a default seed so that you can repeat the results (this is specially designed for tests). But after the first run, the random number generator is different state and thus generate different sequence of numbers from the first run. Therefore, the trained model will be sightly different. Nothing is strange at all.\nBTW, other implementations don't use stochastic algorithms. But it is hard for them to handle large number of samples.. use smile.math.Math.setSeed() to set the seed before the training.. Set the workspace to be the home directory of smile.. - make sure add smile-netlib to your project.\n- you may compare apple to orange. Smile doesn't just fit a linear model but also do goodness-of-fit test, variable significance test, etc. A lot of extra computation are involved to ensure the fitting quality. It is beyond the computation of R squared.\n- although not critical, you better do Math.transpose(x) out of the loop.  It is interesting that you generate a 36 x 3000 array and then transpose it. Why?. @yelangchs Any updates of new running time? Is smile still slower? Thanks.. Thanks for updates.\n\n90% time are used for hypot and max, which are called by Beta function. We use the Beta function to calculate the p-value of goodness-of-fit and significance of variables.\nSo the fitting part (SVD in your case) is only about 10%, which means smile is faster than Apache in plain OLS fitting.\nIn your profiling screenshot, I can see that still the pure Java implementation is used. Is the screenshot was before smile-netlib added to the project? If it is after, it means that there is some configuration wrong so that smile didn't pick up the native library. With native matrix computation library, smile should be faster.\n\nSpecial function like Beta is computation intensive. I will see if we can leverage a native implementation to speed it up.. For the training data matrix, I mean why don't you generate a 3000x36 matrix directly. I don't see the benefits of creating 36x3000 and then transpose it. In fact, it may have negative effective depending on the random number generator. It is possible that you introduce unnecessary correlations in math.rand(3000) across all your stocks.. Smile uses QR for OLS by default. If it fails (e.g. collinear), smile will switch to SVD. The screenshot shows that SVD is used, which indicates the data generation part is problematic.\nAgain, JMatrix, a pure java implementation of matrix computation, is used as shown in the screenshot. Even though with this settings, we can infer that smile is much faster than Apache Commons if we disable the computation of p-values. With smile-netlib, smile should be even faster.\nI am not ask that the test code to be optimal. But bad settings often draw wrong conclusions. . @yelangchs please pay attention to my comments on p-value computation, which is the real difference. Apache Commons doesn't do it. You are comparing apple to oranges.. @yelangchs This is not about whose fault but to find out the root cause. We all want better performance. But in machine learning, the model quality matters more than performance in most cases. In the case of OLS, we always want to calculate the p-values to check the goodness-of-fit even though it is of high cost. Otherwise, we will blindly apply an ill-trained model as in this example where Apache Commons silently ignores the failure of QR due to the collinearity of variables.\nThanks for submitting this ticket.. What do these functions do?. You may use NominalAttribute to do that. . If you specify a NominalAttribute to read a file, it will automatically build the unique word list and map the strings to int directly. There is no directly function to convert StringAttribute to NominalAttribute currently. In 2.0, we will have an advanced data frame to support all kinds of mappings.. It is in math module. . t-SNE is not a predictive method. It is for visualization.. PCA is a projection but t-SNE is not.. t-SNE doesn't have an explicit mapping. It simply just wants to mimic the distance distribution.. It is supported in 2.0, which will be available in near future.. You can set the window size as a workaround.. What does the \"image resolution\" means here? Image size? PPI?\nBesides, this will be a low priority to us. Would you like to take on it? Thanks.. Thanks!. Yes, it should. Thanks.. The error code indicates that there was a convergence failure inside the SVD code. Your input data is probably problematic (e.g. singular matrix).. The beta function is used to calculate p-value. It fails very likely because the fitting is failed due to invalid data (e.g. collinearity) and produces NaN values.. A point p is a core point if at least minPts points are within distance \u03b5. The point p itself is not counted as neighbors.. It is on purpose. You need be consistent. We guess that the first field is a label (integer) or real-value response based on the first sample. You should always use integers in this case.. Missing value should be imputed first. Check out smile.imputation package.. I don't have a publication about the library. You can refer the project website http://haifengl.github.io/smile/\nThanks.. Smile can learn LLE directly.. It is better not using * for import. Can you please import only necessary classes?\n. We have tokenizer in SmileNLP.\n. Please discard this change\n. Will this be reused in other algorithms? If not, shall we move it into SNLSH?\n. Do you really want to keep it? HeapSelect doesn't meet your needs?\n. License?\n. Nice work. Shall we just change generate to the constructor. It is not necessary to create an object and then call this method. Let's do it in one step. Thanks!\n. Our code format is \nif () {\n}\nI will merge the request and revise it by myself. Please use the same format next time. Thank you very much!\n. In smile.math.Math, we should have an inner product function. We actually have three correlation coefficient functions (pearson, spearman, and kendall).\n. Can you add some comments about this kernel? The math formula, and suitable for what use cases. Thanks!\n. Can you add some comments about this kernel? The math formula, and suitable for what use cases. Is this for computer vision? Thanks!\n. If obj is null, shall we return false?\n. same question as above\n. For sake of consistency, shall we use instanceOf pattern as above/BiString?\n. same issue with null\n. same issue with null\n. same issue with null\n. instanceOf?\n. Let's keep it as is and use this.getClass() == obj.getClass(). Thanks!\n. I prefer java system package imports at the beginning. Thanks!\n. why call it fitnessLong?\n. why call it fitnessLong?\n. why call it fitnessLong?\n. why call it fitnessLong?\n. this doesn't make sense. all other fields are ignored. and I don't expect BitString to be used in a hash map.\n. Can you change back to original way?\nimport java.awt.Font;\n. This is actually to remove warning like \"uninitialized variable\". Maybe today javac doesn't report that warning any more?\n. same as above\n. Please keep these variables. I may miss some test on these results. I should add more tests, not remove them. Thanks!\n. same as above\n. same as above\n. Is this correct behavior? Note this constructor is used by newInstance(), which are used by QR and SVD.\n. Can you please rename the function getClusterLabel, which is used by other clustering algorithms?\n. We should check if y is null at the beginning and throw an IllegalStateException if it is. Note y is only created after the method partition is called.\n. can you rename \"default\" to something more meaningful?. What's the warning about this? My IDE never report warnings about it. And previously another community member scanned the whole code base with lint tools. He didn't report something like this. I am just wonder why and is it because of Java 8? Thanks!. I understand all these. I remember that in Java Generics spec, if the return type is the the same class, it is assumed to be of the same T. I was surprised if this changes (or my bad memory :) In fact, I used to use eclipse and it didn't report this warning (in old days).. The point is not about to rename the radius.. I add the following here:\ndouble radius2 = radius * radius;. Here I compare it to radius2.. Don't change anything here and below.. We has no assumption on abs(diff) < 1 and I don't see anywhere we do that check. It may be true in your specific data. But we cannot simply assume it in general. Thanks!. I will cherry pick your change and test locally first. Thanks!. @Override can be added here. Please add copyright header.. Do we really need this as FNR = 1 - sensitivity and we already have Sensitivity class?. Shall we add this to CrossValidation class with an optional parameter \"boolean stratified\" to control the behavior? The default value should be false so that we won't break existing code.. In general, the file name should not contain underscore _ and the class name is better same as the file name. It is just a matter of style. Better be consistent with the rest of library.\nMore importantly, why is this in smile.validation package? For fundamental math functions like this, I suggest too add it to smile.math.Math. Also the Java users can leverage it too.. Can you copy our copyright header here? Thanks!. Comments please. Why integer array? Are they counts or index of binary sparse array?. The implementation is overall imperative rather than functional. So why not we implement it in Java so that Java users can use it too? Again, for fundamental things, I prefer Java for bigger audience. For high algorithms, I am fine with Scala implementation as long as the interface can be called from Java too.\nSorry for miscommunication in earlier gitter chat.. Overall, please add enough comments.. can we a little bit consistent with the naming? nmi is not clear for a starter and not consistent with the naming convention of mutualInformation().\nIt is understandable to use a string in Python for the parameter \"normalization\". But for a strong type language, String is a bad idea. Even worse, without documentation, who knows what should I pass in for this parameter. . You don't need to define ClusterMeasure. It is already defined in Java and Scala can implement a Java interface.. Three class design is over-complicated. A single class is enough. The default normalization parameter in constructor can be None.. Please list all valid values of normalization in document.. case _ for String is bad idea in general. Exception should be thrown for invalid values. . please check our scala code for styling. We don't put { in the next line.. Detailed information about the measure should be given. What is its meaning, the formula, what do they try to detect, etc.. JMatrix should never be directly used. Instead, you should declare a matrix as DenseMatrix and use Matrix.newInstance() to create one. In this way, the system will automatically pick up a native implementation if it is available. If a native implementation is not available, it will fall back to JMatrix.. What exactly is the gamma matrix? Can you add comments how gamma is defined and used?. Why different value for gamma(i,i)?. If s is close to zero, this will be very large. It is not numerically stable.. It is not efficient to create W for each sample. Similarly for matrix X.. System.arraycopy() is more efficient.. please add space between operator and parameters. Please keep consistent with the style of existing code. For example, we would like to format it as\nprivate void updateGamma(JMatrix x) {\n        double v = 1 + gamma.xax(x.data());. OnlineRegression is a subclass of Regression. We don't both of them here.. Looks like that you don't really need DenseMatrix for X and W. If you want matrix-vector multiplication, A.ax(x) is more efficient than A.abmm(x). Just define X and W as double[]. Do I miss something here?. Why 0.01? It seems very large to me. Do you have a reference on this choice?. Why 1? 0 seems right to me for inverse matrix.. don't do transpose V.transpose(). just call sigmaInverse.abtmm(V). It is more efficient.. People may be confused with this method and the learning in constructor. Do we have to add this method?. Matrix-vector multiplication is much better than matrix-matrix multiplication in this case. In fact, you just need gamma.ax(x) and then calculate the norm. No need gamma.atx(x).. Gamma X X ^T Gamma = (Gamma X)(Gamma X) ^T because Gamma is symmetric.. just add \nV.ax(x1, Vx)\nhere. And remove updatedVx. . It should be\nsetSeed((int) (seed % UniversalGenerator.BIG_PRIME));\nOtherwise, we will lose useful digits in the seed. Thanks.. Distance classes should be in the package smile.math.distance. We have many distance classes already defined there, e.g. HammingDistance.. specialArg is not recommended. If a distance class need customization, it should be passed in the constructor. Please check our implementation in smile.math.distance for general guideline.. No underscore in class name. BinaryDistance is preferred.. Please move { up. Please see out scala code for general format guideline. In general, we follow the standard scala library formatting.. Please add documents and references to the algorithms. Actually, please add documents to all your classes. Thanks.. Why this trait?. Why define these special types in a general trait?. Do we need this class as we already implemented it in Java in the same package?. What's difference from MahanttanDistance? If different, can we leverage the MahanttanDistance class in Java in the computation to simplify the code?. Documents for this and the below distance classes?. Is this class necessary? I understand there are some utility functions in the class. But it seems better to use this as mixin than a base class.. How does k-modes compared to k-medoids? We implement CLARANS, which is an efficient algorithm for k-medoids.. This trait is not necessary as we already has the same interface in Java.. Please add comment what this is, how it is computed, and what use cases it is good for.. Please add comment what this is, how it is computed, and what use cases it is good for.. What's the difference from regular Mahanttan distance?. This method is better in a utility class/object rather than a base class.. Who uses this class?. Do you just create ShapeDifference and PatternDIfference by yourself? If not, any reference?. Is it necessary to add this variable? It is algorithm dependent if a attribute is used for monotonic regression. It is sufficient to define this in the algorithm API, rather than here.. This trait seems redundant to me. It doesn't really add new functionality or (real) constraints to Distance. Do we really need it?. I don't think that this is the correct way to calculate Hamming distance.. This class is not necessary. And what about a distance is both binary and metric? This design creates difficulty for that.. We have HammingDistance in the same  package. Not necessary to duplicate it.. The comment is not really helpful. A minimum description is needed.. See comments about PatternDifference.. Function name apply is more Scala style.. Our style is that the constructor should do the clustering computation.. Please reverts the changes to Attribute.java and NumericAttribute.java. no, no, no.. With missing values, can we really still use this formula?. Please don't make this change. I understand your intent from Java/Scala point of view. But I try to provide an interface here similar to R/Matlab.. Please remove this file.. Please add license header as in our other files.. Don't create the package kmodes. The algorithm should be just in the smile.clustering package.. I don't see the necessary to define this trait. If you have to define these types, just define them in the KModes class. In fact, I don't feel these types are necessary.. It should implements PartitionClustering<int[]>. KModesModel should be merged into this class. . Any book or papers on decision tree or CART should have it. . What do these two lines mean?. why */.gitignore? . This is slow and also not numerically stable.  D is a diagonal matrix, which may have zeros in the diagonal. But inverse() is a general method, suitable for dense matrix.. why remove this?. %n is a portal way for \\n. Please don't change this.. same above.. Can we separate this file change from the above? They are not related topics.. why U? Isn't it A?. The document is for the users, not the implementers. The users don't know and shouldn't concern the implementation details. Let's us A here. The old comment Q*R was not good either. Thanks.. setLearningRate() is better. THe learning rate in NeuralNetwork is called eta. Let's be consistent.. Why debug? I prefer see the messages to asses if the algorithm is converging. . Please put OnlineClassifier before Serializable.. use setLearningRate for the method name and use eta for the variable name. Thanks.. no need to create a new an array here. It introduces memory fragment and slows down the system when we update the model with a lot of samples. It is easy to combine this loop with the below one and just use a local variable.. same as above comment.. We typically use tol for convergence test parameter.. m1 is a column vector. Although you can treat it as a matrix, it is faster to just create a double[] and use matrix-vector multiplication instead of abmm. There are additional optimization in BLAS for matrix-vector multiplication.. wps is essentially a matrix (wps[i] is a row vector). DenseMatrix[] will make the reference of matrix element very slow. Please make it a simple DenseMatrix.. abtmm(whiteended) is faster.. Is copy necessary?. Is copy necessary? Copying a matrix is slow and create a lot of memory fragment. Strong discouraged if it is not necessary.. We should not support set* function in most core algorithms. These parameters are set in the constructor. Setting it to a new value in a trained model is meaningless and causes confusion. Please also remove set* methods in other PRs. Thanks.. This is not necessary. PCA and ICA are for different purposes. And I don't see you really test any real results here.. I would like to see real unit tests. That it, you should compare your results to the results from other packages (e.g. R) on a benchmark data. This test gives us no correctness test.. no need to test on iris. the data doesn't make sense for ICA.. please avoid creating unnecessary double[][] here, which is not performance and memory friendly. A simple projection.ax() does the job easily.. Move this to below of next line and change it to double[] mu = centered.colMeans. It is faster. In general, we take double[][] as input because we don't want force any special format for the caller. But we should always avoid double[][] inside the implementation for better performance. Smile's Matrix resolves many performance issues with double[][].. I guess that you follow PCA class for the meaning of n, m, and p. But they are not consistent with the paper. Extra caution is needed to ensure that we don't use the wrong index variable.. Math.cov() calculate the covariance matrix. It takes double[][] though. Let me add similar help functions for Matrix too.. In fact, you can do a centered.aat() here for the covariance matrix as it is already centered.. we need to safeguard v[i] is not too small or zero.. why abtmm and then transpose in the blow? Isn't it just white.abmm(X)?. Why D not E as in the wiki link you post? Keeping variable names consistent with your reference will make it easy to check the formula.. It is better to define an interface that has f (nonquadratic nonlinear function), g (first derivative) and g2 (second derivative). And provide implementations for log cosh and - exp(-u^2/2). This way the user can easily plug in their special functions without changing this code.. Make this interface and log cosh etc implementations inner classes of ICA.. hard to follow m and n as they are swapped compared the wiki page.. why normalization? w is already standard normal distributed.. check p <= n.. rename whitened to X. The expression will be shorter and easy to check correctness.. This part is too complicated compared to the simple formula in the reference. We can simply it greatly if leverage Matrix interface more.. atx is the right way. Are s1, s2 and s3 expected signals?. Not sure RMSE is the right metric to measure ICA.. Why so many lambdas? it is very confusing (at least to me). and I don't see real benefits or flexibility of this design. why not just lambda1 and lambda2? . let's just call it w.. let's just call it b. The beauty of math is short and simple.. To be consistent with LASSO, please rename it to coefficients.. no need of this and following Getter methods.. comments, please.. Do we need this?. c() should be called only once and the result can be reused.. just call it lasso.. comments, please. It is important for the caller to understand what's the return object. Also rename it to lasso(). All machine learning algorithm objects in Smile is immutable. So we don't use getXXX method name and just use xxx for getting values. It is simple and consistent with math convention. I prefer math convention when there is conflict between math naming convention vs java's.. And I believe that the logic is wrong here. correctedW should be sqrt(1 + lambda2) * W. Check the formula 11 in your reference.. Do we really need the method c()?. no need to call modifiedLasso here.. check lambda1 and lambda2 > 0. If they are zero, throw an exception and tell them to use LASSO or RidgeRegression, correspondingly.. no need as we safe guard this in the constructor.. System.arraycopy is much faster.. no need. Java automatically initialize array with 0.. see above. no need to repeat the computation c() * Math.sqrt(lambda2) in a loop. It is const.. precompute c() here before the loop. Again I don't think you need a method c().. where do you get the values of w?. Why do you modify this test? BTW, ElasticNet is equivalent a special formula of binary classification formula of SVM, not SVR.. No need to modify SVM test then.. Can we do this? Do you have a reference?. no sure about this. simple prediction without postiori probability should not bother this at all.. no need of initialization. JVM does it.. same as above. All public methods should have documents.. see above. It is faster if we split this loop over into two. for (i = 0; i < x.length) and (i=x.length; i < ret.length).. The equivalent SVM of ElasticNet is not this straightforward. Let's skip this for now.. You accidentally delete this test. Please get it back.. Please also add\n/**\n     * Returns the intercept.\n     */\n    public double intercept() {\n        return b;\n    }. ",
    "w4nderlust": "Very nice, thank you!\n\nIl giorno 11/gen/2015, alle ore 23:53, Haifeng Li notifications@github.com ha scritto:\nA basic tutorial is now available at https://github.com/haifengl/smile/wiki/Tutorial:-A-Gentle-Introduction-of-SmileMiner https://github.com/haifengl/smile/wiki/Tutorial:-A-Gentle-Introduction-of-SmileMiner\nPlease let me if you have any questions and suggestions. I will keep adding more wiki pages. Thanks!\n\u2014\nReply to this email directly or view it on GitHub https://github.com/haifengl/smile/issues/2#issuecomment-69516026.\n. \n",
    "cschneid": "Thank you for open sourcing it! Just want to get it by lawyers who might balk at the license text in the files.  :smile:\n. ",
    "DiegoCatalano": "Hello Haifeng,\nThe library is awesome, the code is nicely writted. Congratz !!\nI have a question about the license too. I'm development a framework in Java and Android in LGPL. I would to know if can you permit to port some classes into my project ?\n. Hello Haifeng Li,\nI hope that my contributions could improve your incredible project. I'll write more in this weekend.\nCheers !\n. ",
    "sebastian-alfers": "or could you link them?\n. ",
    "benmccann": "Thank you!!\n. So perhaps replace it with the following?\n\"com.github.fommil.netlib\" % \"netlib-native_system-linux-i686\" % \"1.1\"\n\"com.github.fommil.netlib\" % \"netlib-native_system-linux-x86_64\" % \"1.1\"\n\"com.github.fommil.netlib\" % \"netlib-native_system-win-x86_64\" % \"1.1\"\n\"com.github.fommil.netlib\" % \"netlib-native_system-win-i686\" % \"1.1\"\n\"com.github.fommil.netlib\" % \"netlib-native_system-osx-x86_64\" % \"1.1\"\n\"com.github.fommil.netlib\" % \"native_system-java\" % \"1.1\". Thanks for the clarification!. That's great. Thank you!\nWould it be possible to make it available in the Java API as well?. I actually wanted them to use in the shell in Java. There's a great shell for Java called BeakerX: https://github.com/twosigma/beakerx. We should wait to test BeakerX until 0.3 is released later this week. I will post an example after the release. I've been playing with BeakerX Java notebooks quite a bit. It's very fun\nHere's an example of a Groovy notebook using the tablesaw-smile library: https://github.com/twosigma/beakerx/blob/master/doc/groovy/Tablesaw.ipynb\nI would really love it if we were able to use a ConfusionMatrix in Java notebooks. Ok. Thought i would suggest it. Thanks. There's a method that returns loglikelihood which I was referring to: https://github.com/haifengl/smile/blob/205d98db4d8940885d499097987254594303ac0f/core/src/main/java/smile/classification/LogisticRegression.java#L880. Thank you. That's a super helpful pointer!\nWould it be possible to add something about smile.validation.Validation to the docs? Perhaps on http://haifengl.github.io/smile/validation.html?. I'm unlikely to have time unfortunately. @haifengl what do you think about this PR?. Thanks!!. @haifengl thanks for the helpful pointer! I've updated this PR so that OLS takes an AttributeDataset. Thank you! Sorry for the breakage. Ok. I understand better now the intended usage. Thank you\nI've updated Tablesaw, so that the next version has improved Smile support based on this feedback. There are a couple examples of using them together here: https://github.com/jtablesaw/tablesaw/blob/06db8082afe113a091a1891682c9258e773f5cb9/core/src/test/java/tech/tablesaw/conversion/SmileConverterTest.java. I was looking at Java's LogisticRegression. ",
    "experquisite": "Yes this is awesome, thanks!\n. Is there a link to find it on maven.org ? I can't seem to see it.\n. I found it at https://repo1.maven.org/maven2/com/github/haifengl/ , but it seems to be trying to pull in smile-all which doesn't exist?  (I'm doing this from within an sbt file for a scala project)\n. Regrettably I am not an expert.  In my build.sbt I have:\nresolvers += Resolver.sonatypeRepo(\"releases\")\nlibraryDependencies += \"com.github.haifengl\" % \"smile-core\" % \"1.0.0\"\nWhich, when I 'sbt compile', results in:\n[info] Resolving com.github.haifengl#smile-all;1.0.0 ...\n[warn] io problem while parsing ivy file: https://repo1.maven.org/maven2/com/github/haifengl/smile-core/1.0.0/smile-core-1.0.0.pom: Impossible to load parent for file:/home/xxx/.ivy2/cache/com.github.haifengl/smile-core/ivy-1.0.0.xml.original. Parent=com.github.haifengl#smile-all;1.0.0\n[info] Resolving com.github.haifengl#smile-all;1.0.0 ...\n[warn] io problem while parsing ivy file: https://oss.sonatype.org/content/repositories/releases/com/github/haifengl/smile-core/1.0.0/smile-core-1.0.0.pom: Impossible to load parent for file:/home/xxx/.ivy2/cache/com.github.haifengl/smile-core/ivy-1.0.0.xml.original. Parent=com.github.haifengl#smile-all;1.0.0\n[warn]  module not found: com.github.haifengl#smile-core;1.0.0\n[warn] ==== local: tried\n[warn]   /home/xxx/.ivy2/local/com.github.haifengl/smile-all/1.0.0/ivys/ivy.xml\n[warn] ==== public: tried\n[warn]   https://repo1.maven.org/maven2/com/github/haifengl/smile-all/1.0.0/smile-all-1.0.0.pom\n[warn] ==== sonatype-releases: tried\n[warn]   https://oss.sonatype.org/content/repositories/releases/com/github/haifengl/smile-all/1.0.0/smile-all-1.0.0.pom\n. 1.0.1 works perfectly, thanks!\n. Sorry to follow up on a closed ticket, but I might be having some trouble with resources.  I am getting a crash when creating a plot:\njava.lang.NullPointerException\nfrom the ImageIcon constructor here in PlotCanvas:\n```\nprivate class SaveAction extends AbstractAction {\npublic SaveAction() {\n    super(\"Save\", new ImageIcon(PlotCanvas.class.getResource(\"images/save16.png\")));\n}\n\n```\nI don't see that these images are in the maven jar file.  jar tf smile-plot-1.0.1.jar  didn't list them anyway.  Should they be?\n. ",
    "mschulkind": "That's it because the random forest implementation swallows it.\nThe interesting thing is that the forest still appears to work correctly\nafter that.\nOn Mar 6, 2015 9:07 PM, \"Haifeng Li\" notifications@github.com wrote:\n\nThanks for the bug report! Do you have a stack dump?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/issues/16#issuecomment-77667578.\n. I'm pretty sure the log line comes from here:\nhttps://github.com/haifengl/smile/blob/master/Smile/src/smile/regression/RandomForest.java#L335\n. Sorry for the delay. I was out of town.\n\nI just updated and I no longer get the exception. Thanks.\nAny idea why my example never exits though? It seems like it must be\nhanging on a background thread or something. If I simple comment out the\nline to create the random forest, it exits cleanly, but with it, it just\nhangs, although it does return from the call to new.\nOn Thu, Mar 12, 2015 at 9:17 AM, Haifeng Li notifications@github.com\nwrote:\n\n@mschulkind https://github.com/mschulkind Any updates? Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/issues/16#issuecomment-78477059.\n. Things run, but the program never exits. It hangs after main() returns.\n\nOn Thu, Mar 12, 2015 at 10:44 AM, Haifeng Li notifications@github.com\nwrote:\n\nDo you mean that everything runs fine and you get all results, but the\nprogram doesn't exit? Or the program hangs in the creating the random\nforest? Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/issues/16#issuecomment-78493410.\n. Thanks!\n\nOn Thu, Mar 12, 2015 at 11:27 AM, Haifeng Li notifications@github.com\nwrote:\n\nThere is a thread pool, shared by many algorithms. If you are the master\nbranch, you can use\nsmile.util.MulticoreExecutor.shutdown()\nat the end of your program to exit smoothly. Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/issues/16#issuecomment-78502646.\n. \n",
    "owlmsj": "My case: I am looking for a NeuralNet capable to train a corpus of 20GB data in a 8-thread server. Already tried @deeplearning4j and other java impl., but didn't like any of it.\nPredicting not mt-safe could be bypassed implementing a resource manager (such as priority queue or duplicating models).\nSince Smile was clean, source independent and intuitive (got running mt-thread SVM and Random Forest in a couple minutes), thought to ask about project priorities :)\n. Wow, it makes sense. I will give a try and and im closing this issue. Thanks for the answer!\n. Found some bugs in my code at \"processing features\" phase. After validating this, I am going to test and post some results here!\n. Hi, finally got my first results.\nTraining 10k instances (14k attributes) got 33 seconds to learn.\nTrain size:10000\nTraining in: 33.127 seconds\nTest size: 1000\nFinishing Cross Validation\nFinal precision: 0.91607128289355\nFinal recall: 0.908765520895934\nNext step: 100k\n. My experience has been awesome. Please, keep this great job, elegant and effective :)\nAnd sure, it would be great! I am going to test everything before and then I will write the post. \nAbout SVM, I tried it in first time. But got results on 0.7 / 0.9 in prec/recall. And it got time to train (like 250 seconds). But I will give a second try, right after implement a naive grid search.\nThanks!\n. Hi! Really sorry for the delay. I was \"stormed\" by my Master's thesis qualification (writing and evaluating a lot what already did). \nGetting back to the track tomorrow and hope to give good news (results and blog post) soon.\n. Yes. Okay!\n. I know. This is not an issue, just a question! Closing...\n. Okay, Thanks!\nI am going to open another questions in the next days.\n. Yes, for sure! I will keep in touch\n. Sure, but Precision, Recall and FMeasure implementation is only for one-class validation, right? It test 0 (false) and 1 (true), not the class-number. Or what am I doing wrong?\n. This issue is like this question : http://stats.stackexchange.com/questions/51296/how-to-calculate-precision-and-recall-for-multiclass-classification-using-confus\n. Hi, I did that in meantime. Any suggestions about this new class name?\n. Okay! Just a little con: Project 'Smile' is missing required Java project: 'SmileTestData'  . Where can I find this project?\n. Ah, no problem. I just remove the dependency and it was OK!\n. Any progress in this thread? Just to know...\n. Thanks! I was looking to compare the NN vs SVM results in an article.\n. I agree with you. The idea is not to benchmark each other probabilities, but to show use cases of classifiers that have probabilistic outputs (even if its only an estimation).\n. Wonderful! I am going to play with PlattScaling this weekend and try to share some results in an article!\n. \"Running example\" at 5-fold cross validation (partitions A,B,C,D and E = the entire dataset).\nfold 1) A,B,C,D = train and E = test\nfold 2) A,B,C,E = train and D = test\nfold 3) A,B,D,E = train and C = test\nfold 4) A,C,D,E = train and B = test\nfold 5) B,C,D,E = train and A = test\nProbably this is not the best explanation and it is kind of a trivial code to do. Just to know if this feature in smile is already coded or should I implement something look like and make a pull request...\n. My mistake (because my code was shuffling the data at the beginning). Sorry.\n. ",
    "andy-goryachev": "Yes, this should be fine.  It is much easier to evaluate or learn the code if one looks at a working example.\nThank you!\n. Thanks!\n. ",
    "amorroxic": "I too am a beginner and was sweating with a few books aside trying to grasp the concepts. \nThank you for sharing your incredible work Professor Haifeng Li!\nAdrian.\n. ",
    "kid1412z": "I have some trouble running the test, when loading the dataset, it throws\nexceptions:\njava.lang.NumberFormatException: empty String\nat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1020)\nat java.lang.Double.valueOf(Double.java:504)\nat smile.data.NumericAttribute.valueOf(NumericAttribute.java:62)\nat smile.data.parser.DelimitedTextParser.parse(DelimitedTextParser.java:304)\nat smile.neighbor.LSHTest.(LSHTest.java:46)\nat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nat\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n...\nMaybe the problem is:\nString[] s = line.split(delimiter, -1); //DelimitedTextParser.java line:250\nit produced String \"\" but didn't ignore.\nthe data come from\nhttp://statweb.stanford.edu/~tibs/ElemStatLearn/data.html and MD5\n(zip.train) = a2d376ecc766edab27f0c4621f180f99  MD5 (zip.test) =\nec27c60f11cf4a80de34679dd9108b4d\nAnd I'm following up this case.\nAbout the import, my IDE import-optimizer do this automatically, I will fix it.\n. Is it the right dataset? \n. I was confused, the data I have get is delimited by whitespace so I'd like to confirm it again, is it http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.train.gz and http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/zip.test.gz ?\n. I just run testToy(), and watch the printed time of NN, KNN and range and I was not rigorous without using the real test set.\nAbout the data parser, do we allow using maven lib like guava? it provide like Splitter.on(',').omitEmptyStrings().split(str), or we will do it all by ourselves?\n. OK, I willl check it out.\n. I did not change the unit test, \"String[] s = line.split(delimiter, -1);\"  is the origin one.   I still have problem running the test. And I can find _ or ? in my dataset.... :(\n. Still fail: java.text.ParseException: 258 columns, expected 257.\n. It works, thanks!\n. Protostuff(https://code.google.com/p/protostuff/) or Hessian(http://hessian.caucho.com/doc/hessian-serialization.html)?  I use them to compress the loaded Objects in my LSH of SimHash version. It reduce the memory of old generation of jvm. In my case, the serialized objects of Protostuff are smaller.\n. I think schema is used to keep compatible if the model has to be changed. \nOtherwise, runtime schema can be used and it doesn't need too much code as shown in the example code.\n. Of course  :)\n2015-03-25 21:52 GMT+08:00 Haifeng Li notifications@github.com:\n\nSounds great! Will learn more about it. If we choose it, would you like to\nwork together on it? Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/issues/20#issuecomment-86036429.\n. In LSH.java line:534\n\nn2[i] = neighbors[i + 1];\nIs it right that copy from index 1 not 0 ?\n. line 506 puts n fake neighbors with distance Double.MAX, if it's a max heap, the real neighbors are far from top.\nNeighbor<double[], E> neighbor = new Neighbor<double[], E>(null, null, 0, Double.MAX_VALUE);\nI pushed a test that indicate there's a bug exists in HeapSelect.\n. There's no specified data set in this paper, I decide to use MSRP for testing, and I am completing it.\n. Hi Haifeng,\n   I have completed the test, but had no time to do the refactor.\nrecall test result\uff1a\nSNLSH KNN recall is 0.6253140096618396\n    SNLSH Nearest recall is 0.6492753623188405\n    SNLSH range recall is 0.3713021310103753\n. I will try to change to the existing lib, I used the murmur hash in Google library, do we need to implement it ourselves?\n. module smile-nlp depends on smile-core, so I can't use nlp in core.\n. Hi Haifeng,\n   I completed the simhash lsh. Thanks.\n. Sorry, I forgot to push the change.\n. ",
    "nopper": "Fair enough. Thanks for the suggestion ;) Really appreciated. I am gonna close the issue.\n. ",
    "haluk": "Yes, that one.\n. Yes, I would like to.\n. I would like to start. I first need to finalize some other work. I will start the implementation in a month.\n\nHD\nFrom: Haifeng Li notifications@github.com<mailto:notifications@github.com>\nReply-To: haifengl/smile reply@reply.github.com<mailto:reply@reply.github.com>\nDate: Wednesday, May 13, 2015 at 1:06 PM\nTo: haifengl/smile smile@noreply.github.com<mailto:smile@noreply.github.com>\nCc: Haluk Dogan haluk.dogan@huskers.unl.edu<mailto:haluk.dogan@huskers.unl.edu>\nSubject: Re: [smile] clustering optics algorithm (#21)\nHi Haluk,\nI have been too busy to work on this. Would you like to start this effort? Thank you very much in advance!\n\nReply to this email directly or view it on GitHubhttps://github.com/haifengl/smile/issues/21#issuecomment-101761574.\n. ",
    "erdos": "I see that Attribute.valueOf must be used to convert string to double. Sorry for bothering you. Also, thank you for your wonderful library.\n. ",
    "gosiqueira": "Thank you for helping, now I'm able to train my dataset. The problem was that in your tutorial the examples showed how to train using only Multiclass One vs. One or One vs. All.\n. ",
    "guillaumecherel": "Oh right, thanks. I hadn't seen the shutdown method.\nGlad I could help!\n. ",
    "guilhermecgs": "I have just read serialization info in the main page...\nCan you provide examples for using XStream with random forest?\nis it just like this?\nXStream xstream = new XStream(new BinaryStreamDriver());\nRandomForest randomForest = (RandomForest) xstream.fromXML(is);\n. really thanks!\n. I think the name of the algorithm answers my own question... RANDOMforest\n. ",
    "lwhite1": "Sorry, I was under the impression that a 2D array in java needed to have\nthe same length for all rows.  Let me see if I can fix my code and I will\nupdate the issue.\nthanks\nOn Wed, May 13, 2015 at 7:36 PM, Haifeng Li notifications@github.com\nwrote:\n\nCan you please share your code and samples for debugging? Thanks!\nIt should be fine for groups with different size. Please note that for\ninput matrix, each row is a group. In your case, the input 2-d array should\nhave two rows and each row is for male or female data, respectively. Do you\ninput data with 2 columns?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/issues/33#issuecomment-101754685.\n. The problem was in my code. Sorry to trouble you. \n. sure. whats the best way to do that?\n\nOn Wed, Aug 31, 2016 at 8:37 PM, Haifeng Li notifications@github.com\nwrote:\n\nCan you please share your code snippets and data with me privately? Thanks!\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/114#issuecomment-243943368, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADRXghqOiU2e1e6VwPAwFBhhFytZbcPMks5qlh5TgaJpZM4JyE4G\n.\n. Thank you very much!\n\nOn Wed, Sep 21, 2016 at 9:00 AM, Haifeng Li notifications@github.com\nwrote:\n\nWe fix the bug. Your data should run without problems with CoverTree. BTW,\nKNN is not a good method for your data. Many sample pairs have same\ndistances. Given a sample, you may get a lot of data points (> 9) has same\nsmall distances. Different nearest neighbor data structures may return\ndifferent set of 9 samples. The prediction may seem random.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/114#issuecomment-248604826, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADRXgpEZaZo-jKcQzyKsGtTYvNKCn_Caks5qsSp0gaJpZM4JyE4G\n.\n. I tried to add a PlotCanvas. It could be I was doing something wrong at the\ntime.\n\nOn Sun, Jun 25, 2017 at 4:01 PM, Haifeng Li notifications@github.com\nwrote:\n\nSmile uses light weight component JPanel, Canvas, etc. It should not be a\nproblem. Did you try to add a PlotCanvas or PlotPanel to SwingNode? I guess\nthat you should add a PlotCanvas to SwingNode.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/197#issuecomment-310924640, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADRXgr1CDk1sFVqjGKLvA5I8yCcSORk7ks5sHryKgaJpZM4OEWRD\n.\n. I switched for my current project to a JavaScript plotting library.\n\nOn Thu, Jun 29, 2017 at 9:49 AM Haifeng Li notifications@github.com wrote:\n\nDoes it work now?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/197#issuecomment-311972224, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADRXgsLU_p62xXCHhAver6dE2MSuhHtQks5sI6t6gaJpZM4OEWRD\n.\n. Sorry for my ignorance. I assume then that it is doing standard gradient\nboosting, and its popularity is a result of good performance?\n\nOn Tue, Aug 22, 2017 at 10:52 AM Haifeng Li notifications@github.com\nwrote:\n\nXgboost is a library in c++. What do you mean \"implement it\"?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/212#issuecomment-324051344, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADRXgh_iZq44pI0aLGlBVd2sLlFTIaHBks5sausygaJpZM4O-u7b\n.\n. Sounds good. Thanks very much Haifeng. Closing. Thanks very much for adding this. \n\nDo you have any idea when we'll see a release in maven central with this change?. ",
    "ashrafs": "please  send us  link for download dataset used in regression part or name of dataset\nthanks\n. please In  my project indoor location  I use  SVR Regression\nI need train svr offline using x,y location and received signal strength wifi ( build fingerprint database) for training in my location and build model after that predict new location for  new signal strength will come online phase.\nin SVR Code published  Predict function in SVR takes an array of numbers as input and returns ONE numeric value as output. but I have two instances, for X and Y.\n public double predict(T x);{\n}\nmy question :\nI can train two dimensional x,y together or each coordinate is considered as one output of SVR and trained independently ?\nthanks\n. I need predict location based signsl strength. \nI Must train all diminsion for  all coordinate xy independent. \ntrain x\ntrain y\nfor solve this problem?\nor whats your suggested solution\nmany thanks\n. Let consider the location doesn't have same signal strength from all access point in same time must be different values .\nwe will consider it  fingerprint .\nI can train all diminsion for xy and use svr\nI read some paper use svr indoor wlan localization its doable? Because I have real\nValue data and I must use regression. \n. Yes I use orientation when record signal strength as measure value and I do 4 cluster for all orientation measured and I will train all cluster for all orientation (north south east wast) using svr.\nI must do train for x and another train for y for all 4 cluster stored? In this case it douable?\n Thanks alot for your support\n. Kmeans  need predefined number of cluster and its one from disadvantages. \nAffinity Propogation used to cluster all data after that I use rbf neural for train all cluster  alone. \nIi implement all one from components in graph above .\nMy Qz  how I can integrate this part?\nAny code integrate between cluster and train all  one from cluster with rbf nn will be helpful\n. I can use just one cluster algorithm\nI use affinity propagation for cluster data at first .\nand I can use same algorithm in rbf with predefined centroid to build gaussian \nthats correct?\n.  If I train all one from cluster in rbf neural network .\nin this case all cluster have one centroid?\n. I ask about \nafter use cluster algorithm for data \nEach one will have just one centroid in this case?\n. Thank you \n. ",
    "ravikuril": "What does it do I can't understand can u please give me brief introduction\nand how to use this module ......help needed\nOn 09/09/2015 5:30 pm, \"Xyclade\" notifications@github.com wrote:\n\nIncluded 2 unit tests to cover the bugfix\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/haifengl/smile/pull/40\nCommit Summary\n- Merge pull request #4 from haifengl/master\n- Fix for NullPointer exception on search without result #39\nFile Changes\n- M SmileNLP/src/main/java/smile/nlp/SimpleCorpus.java\n  https://github.com/haifengl/smile/pull/40/files#diff-0 (44)\n- M SmileNLP/src/test/java/smile/nlp/SimpleCorpusTest.java\n  https://github.com/haifengl/smile/pull/40/files#diff-1 (23)\nPatch Links:\n- https://github.com/haifengl/smile/pull/40.patch\n- https://github.com/haifengl/smile/pull/40.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/pull/40.\n. \n",
    "cranst0n": "Understood. If I don't add the <parent> reference, the build fails even after changing the source/target level to 1.7. The only other way I can get it to work without adding the <parent> reference is to add the maven-compiler-plugin entry to each submodule's POM.\n. Thanks!\n. Thanks you. That was exactly what I was looking for.\n. That also throws the same exception. I'm guessing because the window hasn't really opened 'fully' and the width/height of the plot canvas still thinks it's zero.\n. No it just results in a solid image of the color as if fillRect(0, 0, width, height) were called.\n. Linux 3.19.0-32-generic #37~14.04.1-Ubuntu SMP Thu Oct 22 09:41:40 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\njava version \"1.8.0_77\"\nJava(TM) SE Runtime Environment (build 1.8.0_77-b03)\nJava HotSpot(TM) 64-Bit Server VM (build 25.77-b03, mixed mode)\n. Would be happy to except I don't have access to a Window machine.\n. Ah. Sorry about that. I've haven't had time to try it out. I suspect it will work when I get a chance but it's not really a viable option in my specific use case since I was generating 100's of plots. Introducing even a small delay (i.e. a few seconds per plot) would be a show stopper.\n. ",
    "sechaparroc": "Ok, thanks for your quick response.\n. ",
    "lfomendes": "Hello haifengl,\n    I want to try some algorithms (naive, logistic, random forests) to analyze twitter posts, so my features will be \"contains word 'love'\".\n   There is the Bag option here -> https://github.com/haifengl/smile/blob/master/Smile/src/main/java/smile/feature/Bag.java But i'm afraid that the model and each tweet will contain a lot of features as 0. The double[][] will be too large to process.\nI have found the SparseDataset.java and BinarySparseDataset.java but I don't understand hot to use them with the classifiers.\nThanks\n. Humm.. I will definitely try that. \nI will compare the MaxEntropy with an implementation using feature vectorization using the hashing trick\nThank you very much\n. ",
    "myui": "Ah, sorry it was okey to be shared for serial execution. I'll close this PR.\n. Got it. \nBTW, int to double conversion does not  lose information. \nhttp://docs.oracle.com/javase/specs/jls/se8/html/jls-5.html#jls-5.1.2\nAnd, assignment of splitValue is int to double conversion.\nhttps://github.com/haifengl/smile/blob/master/Smile/src/main/java/smile/regression/RegressionTree.java#L516 \nI'm not sure about x assignment though.\nThanks.\n. Instead of samples[0]=1, samples[1]=2, samples[2]=0, use bag[0]=1, bag[1]=0, bag[2]=1.\nAs depth grows, bag.length can be reduced because a bag is divided into two bags on a node split.\n-Xmx is set to be restricted, 2GB. But, I guess the current implementation consumes more than 10 GB as it consumes 2N * nodes * 4 bytes. \nI'm using Kaggle rossmann dataset for evaluation. The number of training instances is about 1 million.\nhttps://www.kaggle.com/c/rossmann-store-sales/data\n. Replace sampling as follows:\n// Training samples draw with replacement.\n            final int[] bags = new int[N];\n            final BitSet sampled = new BitSet(N);\n            for (int i = 0; i < N; i++) {\n                int index = rnd1.nextInt(N);\n                bags[i] = index;\n                sampled.set(index);\n            }\n            ...\n            ...\n            // out-of-bag prediction\n            for (int i = sampled.nextClearBit(0); i < N; i = sampled.nextClearBit(i + 1)) {\n                 int p = tree.predict(_x[i]);\n                 ...\n            }\nThe original code is  https://github.com/haifengl/smile/blob/9e1082984a677a9a17e21afbc59ba589ef438502/core/src/main/java/smile/classification/RandomForest.java#L231\n. The depth of binary tree of N nodes is log_2(N+1)\u22121 and 5.658.. where N = 100. The maxNodes of smile is actually maxLeafNodes and thus N should be 127 for depth 6 (though DecisionTree is un-balanced and not accurate). Depth 6 is often too small where the number of features is large and I would like to target depth 10.\nThe number of nodes in a binary tree of depth D is 2^D\u22121 and the current implementation would consume 2N * nodes * 4 bytes \u2248 8 GiB where D = 10, nodes = 1023 (512 leaf nodes), and N = 1,000,000.\n. By using bag approach, it worked fine. It might be other better approaches though.\nYou can find my approach in \nhttps://github.com/myui/hivemall/blob/master/core/src/main/java/hivemall/smile/classification/DecisionTree.java#L632\n. I'm not performing all the unit tests of smile, but it was same results for Iris and Weather unit tests.\nIt can run depth=10 for rossmann dataset while the original smile implementation caused OOM at depth=5 with -Xmx2g. \n. I might consume 110% or so for construction as it involves some additional computation, e.g. in\nhttps://github.com/myui/hivemall/blob/master/core/src/main/java/hivemall/smile/classification/DecisionTree.java#L568\nBut, I think it can be negligible because memory is more valuable resource than CPU for constructing a decision tree (suppose building decision trees in parallel for RandomForest).\nWhen getting a competitive result to RF of scikit-learn for Kaggle rossmannn dataset, it requires depth 30. Logloss were 0.29751 for depth=10, 0.13112 for depth=30, and 0.12992 for an infinite depth.\nPartial overfitting + bunch of model ensemble sometime be a good solution. A fav cynical quote about kaggle: Overfitting? Nay, it's called winning. \n. I found that smile's RegressionTree construction is very slow when compared scikit-learn for rossmann dataset.\nI guess findBestSplit call for Attribute.Type.NUMERIC is the main factor though it's better for use a profiler for a detailed investigation.\nhttps://github.com/haifengl/smile/blob/master/core/src/main/java/smile/regression/RegressionTree.java#L492\n. No, treated all variables as Numeric attributes. Just converted/identified categorical variables to numbers as follows:\nstateholiday    store   promo   dayofweek       schoolholiday   promo2sinceweek competitionopensinceyear        assortment      promo2sinceyear competitiondistance     promointerval   promo2  storetype       competitionopensincemonth       year    month   day     target_sales\n0       660     0       2       0       0       0       0       0       1200    0       1       0       0       0       0       0       8.085178748074537\n0       669     0       2       0       1       1       0       1       17080   0       1       1       1       0       0       0       7.88570539124302\n0       760     0       2       0       0       2       0       0       560     0       0       0       2       0       0       0       8.696008608880904\n0       1018    0       3       0       0       1       1       0       140     0       0       2       3       0       0       1       8.931419805192975\n0       15      1       4       0       2       3       1       2       4110    0       1       1       4       0       0       2       8.725994381014571\n0       685     0       6       0       3       4       0       3       650     0       1       0       0       0       1       3       8.37816098272068\n0       686     0       6       0       0       5       0       0       20050   0       0       0       5       0       1       3       7.661997558901893\n0       687     0       6       0       0       0       1       0       2770    0       0       1       0       0       1       3       8.857799727175905\nUploaded the training data at https://dl.dropboxusercontent.com/u/13123103/rossmann_train.tsv.bz2\nTest code snippet:\n``` java\n    @Test\n    public void testRossmann() throws FileNotFoundException, IOException, ParseException {\n        DelimitedTextParser parser = new DelimitedTextParser();\n        parser.setDelimiter(\"\\t\");\n        parser.setResponseIndex(new NumericAttribute(\"sales\"), 17);\n        parser.setColumnNames(true);\n        AttributeDataset dataset = parser.parse(new File(\"/Users/myui/Downloads/rossmann_train.tsv\"));\n        double[][] features = new double[dataset.size()][];\n        dataset.toArray(features);\n        double[] label = new double[dataset.size()];\n        dataset.toArray(label);\n    int maxLeafs = Integer.MAX_VALUE;\n    StopWatch stopwatch = new StopWatch();\n    RegressionTree tree = new RegressionTree(null, features, label, maxLeafs);        \n    System.out.println(stopwatch);\n}\n\n``\n. As I guess,findBestSplit()andfindBestSplit(int n, double sum, int j)` consumes most of time when profiling the above test.\nhttps://gyazo.com/584f1cafe5e1d76057256ed9bfed55c6\nLittle surprised that findBestSplit() itself was consuming time while 52% of it was consumed by findBestSplit(int n, double sum, int j).\nFor memory consuming objects, I found that int[][] order is the dominator after applying my bag approach.\nhttps://gyazo.com/1e57183f244f0139d35024c6e22a598b\nFor rossmann dataset, int[17][N] is allocated for 17 explanatory variables where N is the number of training instances. This scheme should be revised.\n. @haifengl just to compare the accuracy of Smile to Scikit and to investigate CPU bottlenecks of Smile where treating all variables are quantitative.\nScikit cannot handle categorical variables (i.e., one-hot encoder is required for that) but accuracy and training speed was excellent. \nI'll compare when using proper attribute types in Smile. The test result will follow.\nThe variable order is shared by all trees in the forest. It should not cause memory problems\nAgreed. However, better to reduce memory usages for where the number of variables is large.\n. @haifengl still working on reducing CPU bottlenecks on node split. Will do after fixing the implementation!\n. We check every possible split for numeric value (it is up to n possible splits). For large data, it is a big cost. To speed up, we could check only a small number of splits. Says at every 1% step. This should speed up 100 times. The error may be larger though.\nSounds it's a good idea. I'll test that for where the number of training instances is large.\n. Not yet tested sampling scheme for splitting numeric value. Getting busy for other daily jobs :-(\nIssued a ticket to my own project.\n. @haifengl (cc: @L3Sota)\n\nWhy do you want to change some variables and init to protected in multi variate gaussian?\n\nReverted the change about multi-variate gaussian for the time being.\n\nWhat's the use case of partial decomposition?\n\nMy use case is computing the inverse of sigma in the multi variate gaussian as seen in \nhttps://github.com/L3Sota/hivemall/blob/36990d10e1ebae61604a3cfa2b808577568c72cd/core/src/main/java/hivemall/utils/math/MultivariateGaussianDistribution.java#L169\nhttps://github.com/L3Sota/hivemall/blob/36990d10e1ebae61604a3cfa2b808577568c72cd/core/src/main/java/hivemall/anomaly/ChangeFinderUDF.java#L469 (Autoregression-like algorithm named SDAR [1]).\n[1] http://ieeexplore.ieee.org/document/1599387/\nThe sigma of variance-covariance matrix is positive-semidefinite and symmetric and the diagonal entries might be zero [2].\n[2] https://en.wikipedia.org/wiki/Covariance_matrix#Properties\nOn the other hand, Cholesky decomposition requires  positive-definite and thus inverse of sigma cannot sometimes be resolved.\nhttps://en.wikipedia.org/wiki/Cholesky_decomposition\nThoughts?\n. @haifengl I'll close this ticket because I resolved my issue withou partial decomposition of CholeskyDecomposition, by using custom pdf function for Multivariate normal distribution.\nSolver fallbacks to least square solution using SVD if matrix is singular.\nMy SDAR need to be modified to apply it to Smile because it depends on apache.commons.math for matrix operations.\n. @kazjote \nYou can persist Smile's RandomForest model on Apache Hivemall.  It runs on both Spark 1.6/2.0 and Hive.\nFeel free to port our model serialization/deserialization scheme (particulary opCodegen and it's Evaluator ) to Smile while I made some modifications to Smile's implementation due to mismatch in the multi-threading policy, reducing memory usage, serialization, and etc. I think it is easy to share persistent models between Hivemall and Smile for opCodegen .\nBTW, Smile's DecisionForest is also serializable and thus you can persist it. https://github.com/haifengl/smile/pull/127\n. Please feel free to porting Graphviz export to Smile.\nIt can visualize a DecisionTree as seen in this link.\n. I'm recently revisiting RandomForest issue that we discussed in the past. I guess node-splitting efficiency or stopping criteria completely differs between Smile and Scikit. \nI'm not sure what makes Scikit's RF so fast and Smile's RF slow. It's not GC-related issue in my setting as far as seeing GC logs. \nThis paper is Scikit RF author's PhD thesis. FYI. ",
    "intohole": "ml and nlp materials , papers or books?\n2015-12-07 23:12 GMT+08:00 Haifeng Li notifications@github.com:\n\nwhat materials?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/issues/51#issuecomment-162551757.\n. thx !\n\n2015-12-08 11:48 GMT+08:00 Haifeng Li notifications@github.com:\n\nIn the code, I list a couple of references in each algorithm's class\ndocuments. Check the java doc, which you can find the link in read me. For\ngeneral introduction, the book \"the elements of statistical learning\" is\npretty good.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/issues/51#issuecomment-162750555.\n. \n",
    "jamespedwards42": "Nice, yeah, a ThreadLocal is way better.  \nOne use case is for parallelizing cross validation. And then I also have another couple of threads clustering using NeuralGas to select data sets similar to what I'm trying to predict.  Specifically I've been using the NeuralNetwork, KNN and NeuralGas classes that all were hitting that Random.\n. SMILE is amazing!!! Thank you so much for creating this, it is so nice to have such a clean interface into everything.  Sorry, no reference point on speed, but I think that issue usually needs to be solved horizontally, so the real difference maker for me is the ability to write clean/maintainable/reusable code against your framework/library. I'm a complete beginner when it comes to machine learning, just a traditional CS background.  I feel like this library will really help open doors for people to start learning about this field.\n. ",
    "cutdir": "Thanks, I have solved the problem of load csv file,and now I run the GAFeatureSelectionTest sample code, returns 0 and 1. What does this mean?Is 0 for abandon it while 1 means use it?\n. ",
    "badgersow": "Thank a lot you for fast and detailed reply.\nI think I was not clear last time with my explanation, please let me rephrase it.\nMy task is to cluster the set of strings into groups.\nIn my case, distance 0 is possible only with equal or identical (equal by reference in Java terms) strings.\nThe problem occurs when I have two identical Java strings in dataset.\nThey have distance 0 and they are reference-equal. I expect them to be in one cluster, but because of identicalExcluded = false, objects are labeled as noise.\nYou can check that with dataset {\"123\", \"123\"} strings are labeled as noise because they are the same Java objects (thanks, Java string pool), but with dataset {new String(\"123\"), new String(\"123\")} objects are clustered together because they have different references and distance 0.\nIn my opinion this example exposes inconsistent behavior because the algorithm depends on reference equality in Java language.\nAnyway, I am not a specialist at machine learning, I just want to be sure you understand me right because of my terrible English. Thanks again for reviewing this issue!\nAbout Jaro-Winkler distance implementation\nIn my case I used implementation provided by Apache commons library, please see code below:\njava\npublic static final Distance<String> JARO_WINKLER_DISTANCE = new Distance<String>() {\n    @Override\n    public double d(String from, String to) {\n        return 1.0 - StringUtils.getJaroWinklerDistance(from, to);\n    }\n};\nDo you think I should add dependency below in SMILE and add distance to package smile.math.distance?\nxml\n<dependency>\n    <groupId>org.apache.commons</groupId>\n    <artifactId>commons-lang3</artifactId>\n    <version>3.3.2</version>\n</dependency>\nMany thanks for your time.\n. Thanks for clear and detailed explanation.\nWith your permission I'd like to add couple of lines to documentation of RNNSearch class, explaining default behavior.\nIf you are OK with this, I'll open pull request.\nAlso, I will follow your advice and rewrite my code to use new String() approach\n. Thanks for advice, but according to this article, Jaro-Winkler is not a metric because it does not satisfy triangle equality. It seems I can use BKTree and CoverTree only with metric.\nDo we have better approach than LinearSearch when distance does not satisfy triangle inequality?\nBTW, my dataset should not be more than 10K objects.\n. Many thanks for help.\nI am closing the issue.\n. ",
    "ghost": "Do you have an estimate for this task? Thanks for this great job.\n. It would be great, because probabilistic outputs helps to evaluate the errors. Can I help?\n. ",
    "leo-xin": "hi,Li.\nI have an intention to implement t-sne in smile framework. Is there any requirement or advise? Can I import  other depandency like jblas?. oh,my God.Your productivity is amazing!\nthank for the effort.. oh,sorry. I have a mis-spelling in the issue. I mean multinomial distribution, not multi Gaussian.\nIt is a generalization of binomial distribution. I want to simulate a series of random trial from that distribution.. yes,  I have It according  to your instruction. That is just what I want, thank you so much.. I am not the author. By reading source, I think the reason is that you have not set paramter subsample Which by default is set to be 1.0 in smile.  In that way, there is no random in building trees.. I am sorry, after carefully reading. I think I have give the wrong hint. there are two ways of random in building forest. feature and data are both randomly selected. I have no Idea about your problem. Can you show your detailed code?. ",
    "zjuhasz": "Just wanted to check if this is still planned for a future release and ask if it's possible for you to say the priority of supporting deep neural networks in smile. Also what types of deep neural networks are you planning on supporting?. ",
    "rayeaster": "is this feature still in roadmap?. http://help.sentiment140.com/for-students\nSentiment140 was created by Alec Go, Richa Bhayani, and Lei Huang, who were Computer Science graduate students at Stanford University, allowing you to discover the sentiment of a brand, product, or topic on Twitter.. http://www.cs.cmu.edu/~wcohen/10-605/sgd-part2.pdf\n@haifengl How about let me give it a try for sgd on LR:). will follow your notes. Thanks.\nHere it is :)\nhttps://github.com/haifengl/smile/pull/332\n. @haifengl why you mention \u201cMake sure NOT use one-hot for Smile.\u201d sparse matrix not friendly to smile? . My pleasure. Thanks for taking time to review. . Thanks for the clarification. I will close this issue.\n\n. multiple demo files might have same issue. following is an illustration of the contour visualization in Java GUI\n\n. following is the training error for differnt model settings on Iris  data set (the first column is the feature indexes chosen for  demo training):\n\n. my pleasure:). @haifengl  just curious how to use smile with Spark for prediction if the model is not trained within smile? smile support model import from Spark?. @haifengl \nthat is great reference and Thanks. Thought we could add the reference in the code for clarity. \nBTW, I checked the paper and submit a PR here  you may want to have a look at. \nReally appreciate your time.. @haifengl \nanother minor issue is about the prediction formula \nf += ux[j] * ux[j] / eigen[j];\nf = ct[i] - 0.5 * f; \nin LDA and QDA which I failed to find reference in the paper. \nCould you please mind to point it out? \nBTW current LDA implementation is more like PCA style(i.e., unsupervised), not related to training data class at all.. @haifengl \nPR is updated with above code comments incorporated.\nTest Update summary: \nnew version is obviously better on USPS/Pendigits/Segmentation dataset but a bit worse on Iris dataset (maybe due to the iris dataset size). \nActually, what confuse me most in the old version is the equation U.abmm(U.atbmm(B)) which equlas to B essentially, meaning the mapping matrix is only based on B (between class scatter)\nold version:\nFLD-USPS error = 561,  error rate = 27.95%\nFLD-Iris error = 5, error rate = 3.33%\nFLD-Segmentation error = 320, error rate = 39.51%\nFLD-Pendigits error = 1515, error rate = 20.22%\nold version with eigenvector directly from B (without calculting U and above matrix multiplication):\nFLD-USPS error = 373,  error rate = 18.58%\nFLD-Iris error = 12, error rate = 8.00%\nFLD-Segmentation error = 239, error rate = 29.51%\nFLD-Pendigits error = 1201, error rate = 16.03%\nnew version:\nFLD-USPS error = 257,  error rate = 12.81%\nFLD-Iris error = 22, error rate = 14.67%\nFLD-Segmentation error = 84, error rate = 10.37%\nFLD-Pendigits error = 888, error rate = 11.85%. Sorry that I do not get why use dot multiplication with eigenvalue array s[] for only the first k rows in UB instead of all p rows.\nAnd if we are going to calculate inv(T) * B, why not calculate inv(T) from U and s[] first and then do the matrix multiplication with B which seems more clear and straight-forward?\nForgive me if above questions looks too simple for you:). That sounds great:) Thank you for taking time to review this. Maybe we could use a parmeter to switch between 2 implementations for different data size. Also a lot thanks for the comment about computational math.BTW any resource on that topic you recommend as reference?. Great. I could add the parameter.\n@haifengl \nnew code change generate following unit test results with explicit implementation chosen for different data sets (LOW_DIM or HIGH_DIM):\nFLD-USPS error = 257,  error rate = 12.81%\nFLD-Iris error = 5, error rate = 3.33%\nFLD-Segmentation error = 84, error rate = 10.37%\nFLD-Pendigits error = 888, error rate = 11.85%\nfor old implementation, if k > p, the element-wise multiplication between UB and s in for-loop would be broken due to matrix index-out-of-bounds. I change it to p as the outer loop end index.\nI failed to find any obvious bug in both old and new implementations and not sure why the test result differ obviously for those data sets. Maybe it is due to what @haifengl mentioned as \"numerical instability\".\nAny comment would be appreciated.. @haifengl \nadd a new classification test for Breast Cancer Wisconsin Diagnostic Data Set from https://www.kaggle.com/shravank/predicting-breast-cancer-using-pca-lda-in-r/data\ntest result shows:\nLOW_DIM setting: FLD-Breastcancer error = 64, error rate = 11.25% \nHIGH_DIM setting: FLD-Breastcancer error = 26, error rate = 4.57% \njust a note, the report in above kaggle link gave following test result using K-fold cross validation instead of LOOCV:\n\nBy applying the classification rule we have constructed a diagnostic system that predicts malignant tumors at 99.47% accuracy rate and predicts benign tumors at 93.06% accuracy rate using a 10-fold cross validation plan. @haifengl please let me know if you see any implementation mistake in this Pull Request or you spot any inappropriate coding style. Thanks. I am definitely glad to hear your input.. Thanks for taking time to review the code. Really appreciate. Let me know if any bug was found.. @haifengl maybe because the old implementation calculate inv(S_t) * S_b, not inv(S_w) * S_b?. so what could possibly be the cause for discrepancy there? Are all S_w in those test datasets singular?. add some refer for matrix decomposition used in linear equation solver: \nhttps://www.quora.com/Is-it-better-to-do-QR-Cholesky-or-SVD-for-solving-least-squares-estimate-and-why\nhttps://github.com/mikera/core.matrix/wiki/QR-Decomposition\nhttps://github.com/mikera/core.matrix/wiki/Cholesky-Decomposition\nhttps://github.com/mikera/core.matrix/wiki/Singular-Value-Decomposition\nhttps://github.com/mikera/core.matrix/wiki/Eigen-Decomposition. @shikharsharma23  did you compare the implementation? there should be some uncertainty or choice needed to make in decision tree building process even with same depth, especially with numeric features. . testimonials added:\nhttps://github.com/haifengl/smile/commit/314977c862906093af7148ccc12b7d449f964d2e. unit test result:\n\n```\nSegment\nSegment error rate = 5.93%\nSGD-Segment\nSGD-Segemnt one more epoch...\nSGD-Segment error rate = 6.17%\nIris binary\nLogistic Regression error = 3\nIris\nLogistic Regression error = 3\nIris binary\nSGD-Iris Binary one more epoch...\nSGD-Iris Binary Logistic Regression error = 3\nUSPS\nUSPS error rate = 9.37%\nSGD-USPS\nSGD-USPS one more epoch...\nSGD-USPS error rate = 9.37%\n```. \"Shall we shuffle the samples first?\"\nany utility class I could use for sample shuffle? @haifengl .  @haifengl  code changed according your input and unit test result remains the same. Thanks,. That is actually the method I took in the first place:) I check other unit tests using SGD and decide to be consistent with existing ones. I will go with what you suggest, thanks.. unit test results with 80% samples for batch algorithm, 20% training samples for sgd (shuffle enabled) :\nSGD-Segment error rate = 6.17% (same with all samples used for batch)\nSGD-Iris Binary Logistic Regression error = 8 (obviously more than 3, i.e., all samples used for batch)\nSGD-USPS error rate = 9.57% (a bit more than 9.37%, i.e., all samples used for batch)\nunit test results with optimal error percentage from parameter-tuning I tried : \nSGD-Segment error rate = 5.68% \uff0870% samples for batch algorithm\uff09\nSGD-Iris Binary Logistic Regression error = 3\uff0895% samples for batch algorithm\uff09\nSGD-USPS error rate = 9.57%\uff0880% samples for batch algorithm\uff09. my pleasure. This is great and attractive feature for pythoners :). There is a standard EM implementation in ExponentialFamilyMixture and MultivariateExponentialFamilyMixture. glad to help:) But why not add it to projection package in core module like PCA?. of course:)  here is the PR https://github.com/haifengl/smile/pull/349. \n\n. > Can you add documents/comments to all methods? Thanks.\nwill do. > Can we rename the class name to ICA? The interface should reflect the purpose only. FastICA is only one way to do the computation. The user should not bother the implementation details. Thanks.\ngot it. will do. > ICA is good for cocktail party problem. Do the data in your demo/test make sense for it?\nThe demo follow same setting with other projection mechanism like PCA.\nThe unit test indeed do not use specific data set from cocktail problem or mixed signals. If time permit, I would like to add a similar data set of that type. . @haifengl  modified according to your comments above. \nAdd a mixed signal test dataset from sklearn: \nhttp://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html\nDouble-check the implementation in this PR against algorithm reference below and other implementations like https://github.com/EdwardRaff/JSAT/blob/master/JSAT/src/jsat/datatransform/FastICA.java\nThe reference pseudo-code comes from Wiki:\n\n\nRecovered signals from this PR (ICA) is the 4th pictures from top while the 1st,2nd,3th pictures are noise-added observations, true signals and sklearn result (FastICA):\n\n. > Thanks. Do you use tab in the code? I observe it in your commits across multiple PR. Can you set tab -> space in your IDE and also set indent to 4?\n@haifengl fix the tab with space. I also fix all my open PRs using the space formatter. Thanks for remind. . @haifengl thanks for this thorough code review. I modify the PR with respect to your comments and add test correctness check. It now shows comparable (even better) result with sklearn package (updated signal plot is embed in comments above). also remove setxxxmethods in other PR\nsmile:\nrmse for signal1 is 0.556332788860372\nrmse for signal2 is 0.502187993412292\nrmse for signal3 is 0.5816896466939584\nsklearn:\nrmse for signal1 is 0.70226691359\nrmse for signal2 is 0.999406929533\nrmse for signal3 is 0.579000347791. @haifengl PR updated with respect to your current comments .. For least squares, matrix inverse is required.. PR: https://github.com/haifengl/smile/pull/352. This implementation allows reduced to pure LASSO (L1) but not allowed to reduced to pure RIDGE (L2) since it use a modified version of LASSO for existing solver in smile:\n\n. skip SVM comparison for this moment. PR is here : https://github.com/haifengl/smile/pull/356. @haifengl and would you mind to share the reference for static method of PlattScaling#multiclass() which looks quite interesting? Really appreciate. @haifengl \nReally appreciate for taking time for this issue. But I failed to find relevant reference for following multi-class probability calibration code in above paper. It does not mention calculation procedures nor give pseudo-code for multi-class. \nIt would be also helpful if we add more documentation about arguments and key variables/calculations around Qp, pQp,etc.  Is this implementation follows typical binary classification reduction, one-to-one  (all-pairs)?\n\n . Note that usually it is recommended to use separate data set other than traing data set for probability calibration, i.e., probability calibration should be done on new data not used for model fitting. Typically,  N-fold cross-validation could be used to split train samples and validation samples for the calibration. The probabilities predicted for the folds are then averaged. . add friendly toString to output pair-wise lines generated by Isotonic Regression for better illustration of the scaling effect. Also add straightforward assertion check at the end of scaling fitting to ensure we cover all sample predictions/scores in fitted lines.. quantile regression objective function with parameter q (with y hat as the prediction):\n\nwhen q > 0.5, smaller prediction get more attention in loss;\nwhen q < 0.5, bigger prediction get more attention in loss;\nwith q=0.5, it reduced to LAD regression (median regression):\n\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.470.9161&rep=rep1&type=pdf\nPR is https://github.com/haifengl/smile/pull/389. @haifengl I have several questions about the RegressionTreeimplementation as following:\n1) this code snippet seems not a typical squared error calculation and in which the split.output actually never get updated (means it is always zero)\n(trueCount * trueMean^2 + falseCount * falseMean^2) - n * split.output^2\n2) why do we use average instead of x[i][j] directly as split value? but we indeed already count the samples[i] as true child: split.splitValue = (x[i][j] + prevx) / 2 . since average will make the actual split value is lower than the value of x[i][j] if I understand correctly\n3) sorry would you please share the reference for monotonic constraint? It is a little hard to understand the trick behind the implementation: score *= 1 - Math.abs(monoRegForFeature). Looks to me that it always try to reduce the score and make split less likely to happen. Not sure why it help maintain monotonic constraint\nThanks!. > data is 5000000\nis your memory large enough to hold that data? Maybe try stochastic/online LR with a subset pre-training. do you try reduce the dataset, like start using 10% (500000) of the data set to see if LR could run?. It would be better to separate the change from MCC metric change and do not use code format for existing code:). which classifier? there seems exists some method already support that batch prediction. @OsamaFadel  please try to add -Dsmile.home=<pathToSmile>\\shell\\src\\universal to your demo/test running setting. maybe you could use some real dataset like classic boston house price instead of random fake data. that would be more persuasive for your statement.. https://github.com/sbt/sbteclipse/wiki/Using-sbteclipse#withsource\nhttps://github.com/sbt/sbteclipse/wiki/Using-sbteclipse#withjavadoc\nbasically, these two allows to download source and java doc for convenient reference in Eclipse, which I think is good to most veteran Java/Eclipse fans.. it seems to me that there is no need for a separate gitignore file under each sub folder/project so I add this line. If that is not true or you want more flexibility, of course we could remove this line. you are right. I remove the inverse invocation\n. you are right, this line is important to achieve reasonable test result. I checked the weka implementation and confirm this is canonical practice. sorry for any confuse.. No problem. I will separate the files. Got it.sorry for that change. Yes I will keep it as it was. Thanks for pointing out. For DenseMatrix.svd(), current javadoc says:  \nReturns the singular value decomposition. Note that the input matrix will hold U on output.\nand in QR.solve() javadoc, currently it use QR as specific param explanation instead of generic A\nthought we might keep consistent explanation behavior for QR and SVD solve methods.. Got it. updated.. so setLearningRate() or setEta()?. consider it done.. ok. I will revert this change.. Thanks and got it.. Got it and I will try to remove the new array there. Thanks. @haifengl rmse is used here as a quick and rough comparison taken point-by-point. Yes, for ICA, I think the most intuitive way is to check the visualization of unmixed-signals which vividly shows the power of ICA to distinguish mixed signals.. @haifengl  yes, the original signal data. I extract the data from the code here: http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py. will change.thanks. will change.thanks. will change.thanks. ok. Sure, I could remove that. Never mind, I remove that toy test. I use the so-called \"vanilla\" solution. But yes, your suggestion is a better estimate according to MLAPP book section 13.5.3.3. > No need to modify SVM test then.\nyou mean no need to modify SVR test, right? . Remove modification to SVR test and add test on a binary SVM too. @haifengl please advice how to simplify this part using Matrix. Thanks. done.Thanks. > why normalization? w is already standard normal distributed.\neach value in w is normal distributed, but w itself as a vector is not normalized. I borrow code from Math.randn. There is a quote in reference. But I assume existing multi-class implementation(reduced to one-to-one binary classification?) applies here too so simply reuse.\n\nOne way to deal with multiclass problems is to transform them to binary problems, calibrate the binary models, and recombine the predictions\n\nZadrozny, B., & Elkan, C. (2002). Transforming classifier\nscores into accurate multiclass probability estimates.\nKDD (pp. 694\u2013699).\nBTW, reference for platt does not mention multi-class calculation steps, that is what I am curious via question in #354: https://www.researchgate.net/profile/John_Platt/publication/2594015_Probabilistic_Outputs_for_Support_Vector_Machines_and_Comparisons_to_Regularized_Likelihood_Methods/links/004635154cff5262d6000000.pdf\n. This is a straightforward switch added on existing logic of platt scaling. And this method is only invoked for prediction when posterior probability is included as input argument.. ",
    "digital-thinking": "Just found this issue and I recently found this data set.\nhttps://www.kaggle.com/c/twitter-sentiment-analysis2. Thanks for your quick response. I am currently using SVM and the SparseLinearKernel, so it should be ok, right?. Ohh yes, sorry I updated the pr. It happens, that the training gets stuck in an infinite loop, when the tolerance is never achieved, see the smo call in the loop (line 866). I am working on model selection and I am tuning hyper parameters programmatically, so if a SVM gets stuck, i need some limit to make sure that the training stops at some point. . I will try to provide some data, but in general I can not determine how to set epsgr before finish() is invoked. It depends on the data, sometimes it\u00b4s necessary to set epsgr to 0.01 0.1 or even 0.2. If epsgr is to low, it will get stuck and never reach the criteria (infinite loop). When this happens I can adjust the epsgr param manually, but in grid search/model selection I have to restart the process.. Unfortunately maxIterdoes not help, when it is setable only in finish(). As far as I see there is no way to pass the param, when using CrossValidation.. In my case I am using static Validation.cv(), which accepts  (number of folds, the trainer, X, Y and measures). Even If I implement this on my own, I can not pass the maxIter param, because in the end finish() is called inside of the train() Method from the SVM and I don\u00b4t have any point, where I could call finish(int maxIter).. Would be nice, thank you!. I decided to do so after looking at the childs classes (should work for all current extended classes). Would be also an option to just make the Child classes Serializable.\nI found this article helpful . ",
    "douglasArantes": "Thanks!\n. Thank's.\nCongratulations for the work. Any number related to adoption of Smile? I watched the rise of stars here in github, I intend to soon make a series of posts to divulge this library.\n. Training a RandomForest with the dataset San Francisco Crime (Kaggle) was getting a OOM, plus a long run time, I decided to change the GC for G1, with the configuration:  -XX:+UseG1GC, and solved the problems.\n. Still I haven't tested this dataset with other libraries.\n. Very good, i also liked the introduction of SoftClassifier interface.\nDo you have any idea in mind about the next features to be introduced in the project?\n. But I refer to Scala API (high-level operators).\nhttps://github.com/haifengl/smile/blob/master/scala/src/main/scala/smile/data/package.scala\n. Ok, thanks!\n. Cool! You will use some library for this?\n. You could use a library like ND4J in Smile Core for matrix computation?\nND4J's Benchmarking\n. I believe that creating a branch 2.0 that use the ND4J and keep 1.x branch with pure Java will be a great choice.\nRegarding the distribution I do not know what would be the best approach, ND4J is used in the DeepLearning4J (DL4J Github), you may want to contact the staff of Skymind through Gitter.\nUseful information can be find in dependencies page and native GPUs.\n. I think static final String INVALID NUMBER OF TREES must have the private modifier instead of public.\n. That's it.\n. I had the same problem, but now it's back to normal.. ",
    "crockpotveggies": "@haifengl sounds good thanks for the update!\n. Perfect solves my issue. Thanks :)\n. Quickly realized that the Scala default apply() methods for read/write do not leverage XStream. I've confirmed that by using smile.write.xstream I can successfully read/write a model. However, the file size is a bit...big. 11MB is very hefty, is there a way we can just serialize the parameters?. Confirmed the bug is fixed in v1.2.2. Thanks for your help.. Glad it's solved!\nOn Tue, Mar 28, 2017 at 7:54 PM Haifeng Li notifications@github.com wrote:\n\nFixed. Thank you very much!\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/165#issuecomment-289966752, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABYNrwxk5TrJguwohbXYNkGvIoZ3-bxuks5rqcfagaJpZM4Msanj\n.\n. \n",
    "christianolms": "Dear haifengl,\nthank you very much for the fast reply. The solution with mapping the algorithm progress to SLF4J is a much better approach than my suggestion. I will even go further and think of adding SLF4J to my own project for logging. Thank you for the recommendation.\n. ",
    "chhh": "I think the correct solution for this is to use Graphics2D implementation\nthat supports printing not only to the screen but to images(files:\nvector/bitmap... etc) -- just call\nsmile.plot.PlotCanvas#paintComponent(Graphics g) providing the capable\nimplementation of graphics.I believe, there must be a constructor of\nPlotCanvas that doesn't call setVisible anywhere.\nFor example of non-standard Graphics2D implementation, see e.g. this one:\nhttps://github.com/freehep/freehep-vectorgraphics (\nhttp://freehep.github.io/freehep-vectorgraphics/)\nSpecifically check out this example of headless painting:\nhttps://github.com/freehep/freehep-vectorgraphics/blob/master/freehep-graphics2d-headless-example/src/main/java/org/freehep/graphics2d/example/HeadlessExample.java\nOn Thu, Apr 7, 2016 at 3:52 PM, Haifeng Li notifications@github.com wrote:\n\nThe problem is that graphics projection depends on the PlotCanvas size.\nBecause the canvas is not materialized, nothing works as expected. If you\ncreate a multithreaded program that shows the canvas, save it, and closes\nautomatically, I guess that it may be not too bad for your use cases.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/issues/74#issuecomment-207065087\n. @haifengl I'll think about it, so far I've checked out the code and tried to explore it a little bit. It seems like it should be doable.\n\nWhen I imported the code into IntelliJ IDEA there were a couple of problems, the most notable of which I guess was that SLF4J wasn't set as a dependency in pom.xml files.\n. Ok, thanks!\nSo the weights should only be allowed to be integers then (in which case formula updates are trivial)?\n. My dataset is 1D and the problem is the following. I have several million 1D data points for measurements. I do Kernel Density Estimation and plot it and see some nice structure. Here are examples. \nRed is KDE estimate, Blue is the sum of GMM components (GMM fitted using BIC criterion), other colors are separate components of the GMM.\nHere are three distinct peaks in KDE, but only one component for the GMM.\n\nHere are two peaks in KDE, but the GMM fits the main peak and the broad noise distribution (green)\n\nThe GMM is being fitted using the raw data. There might be some bias in the original data, e.g. imagine floating numbers only being stored in a text file with 2 digits after the dot, so a lot of points might fall into the same exact spot (this should not be happening, but it what it is). KDE fixes that and produces the expected results, however fitting the GMM doesn't seem to work that well. So I wanted to fit GMMs to KDE estimates (which are like smoothed histograms). Right now I'm just distributing a fixed number of points (e.g. 1000 according to the KDE distribution and fitting that with the GMM. But I need to do this many many times, so this gets slow if I use a lot of fake points. It can be fast though, if points were just weighted. That's what I meant by \"integer weighting\" in this case.\n. Thanks a lot for the suggestion, it worked!  \nI didn't find any simple way to set the regularization parameter to 0 other than to copy GaussianMixture class and change the way EM is called in the GaussianMixture(double[] data) there.\nSecondly, if you're saying that regularization doesn't work for 1D data, then maybe it would be a good idea to change the default behavior of GaussianMixture(double[] data) to use gmma = 0.0 or some other gamma, because right now it's 0.2 which is also the maximum allowed value.\n. Thanks @haifengl \n. ",
    "mohdbilal87": "Sorry for the typos in the initial comment.\nTwo methods come to my mind which I have explored recently:\n1) The generalized low rank method by (Madeleine Udell, Corinne Horn, Reza Zadeh, Stephen Boyd). \nLink: http://arxiv.org/abs/1410.0342.\nIt essentially is representation of data matrix in low dimension while simultaneously imputing the missing values, something similar to _softImpute _ method (by Hastie and Mazumder). However the latter works only for numeric data.\nThe GLRM on the other hand deals with numeric, Boolean, ordinal and categorical data types. This has several existing implementations as follows:\n    a) Implementation in h20: http://learn.h2o.ai/content/tutorials/glrm/glrm-tutorial.html\n    b) Implementation in python(by one of the co-author): https://github.com/cehorn/GLRM\n    c) Implementation in spark(by another co-author):: https://github.com/rezazadeh/spark/tree/glrm/examples/src/main/scala/org/apache/spark/examples/glrm\n    d) Implementation in julia(by the lead co-author): https://github.com/madeleineudell/LowRankModels.jl\n2) The second method is not imputation itself but rather numerical coding of categorical data. Perhaps after such conversion, traditional methods of numeric data imputation could be applied. I have not tried this method yet.\nLink: http://zeszyty-naukowe.wwsi.edu.pl/zeszyty/zeszyt12/Numerical_Coding_of_Nominal_Data.pdf\nThis one is quite recent. An arxiv print came out in early 2016.\n. I can definitely try. But I don't have experience in Scala. I work mostly in R and occasionally in python. If you don't have any problem with me having a slower pace than your usual turnaround time I surely will love to implement this in scala. \n. ",
    "andrewpalumbo": "@haifengl thank you for yor response.  The second problem turned out to be a configuration error on my end (I had not added the classes to a assembly fIle where they were required).  I've updated the thread to reflect this.  Thanks alot.  Have a good vacation.\n. Thank you very much for your answers @haifengl ,  I've followed your instructions on #78, and Smile is building without issue for me.   \nWe've begun (very basic so far) integration of smile-plot with Apache Mahout for use in plotting sampled data-points from Distributed Row Matrices, and it is going very well, the plots from this library are great!  Thank you very much. \n. ",
    "AndreiBarsan": "Thanks for the reply! I also tried using smile.sh (even on a fresh clone), and I still get basically the same error:\n$ ./smile.sh\n[info] Loading project definition from /Users/andrei/workspace/smile/project\n[info] Updating {file:/Users/andrei/workspace/smile/project/}smile-build...\n[info] Resolving org.fusesource.jansi#jansi;1.4 ...\n[info] Done updating.\n/Users/andrei/workspace/smile/build.sbt:3: error: object pgp is not a member of package com.typesafe.sbt\nimport com.typesafe.sbt.pgp.PgpKeys.{useGpg, publishSigned, publishLocalSigned}\n                        ^\n/Users/andrei/workspace/smile/build.sbt:27: error: not found: value useGpg\n  useGpg := true,\n  ^\nsbt.compiler.EvalException: Type error in expression\n[error] sbt.compiler.EvalException: Type error in expression\n[error] Use 'last' for the full log.\nProject loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore?\n. Oh, no rush. I was able to add Smile as a Maven dependency to my Scala project and everything is in order. Building the toolkit from scratch was just an add-on activity for me. Thank you for your replies!\n. ",
    "quertenmont": "yes, I found the faq right after posting (sorry).\nHowever, i am running with spark and it would be convenient not to save the model on disk but keeping simply keeping it in memory.  I switched to Kryo serializer in spark instead of default java serializer and that seems to work (it's just a bit less efficient).\n. yes...\nEventually adding a wrapper class would be an intermediate solution.\nIt might be enough to add a wrapper that implement serializable in the scala smile.io.package ?\n. Thanks!\n2016-05-19 21:30 GMT+02:00 Haifeng Li notifications@github.com:\n\nI have enabled all classifiers with Serializable\nd627d3a\nhttps://github.com/haifengl/smile/commit/d627d3a6efab668ad498f370c5c781ec41a218e1\nTo use it, you need build by yourself. If everything is fine, we can\nrelease a new version.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/haifengl/smile/issues/79#issuecomment-220426884\n\n\n\nLoic Quertenmont - European Organization for Nuclear Research (CERN) - CMS\nexperiment\nOffice CERN   : bldg.40, RB-08\nPhone CERN    : +41 2276 71548\nMobile BE     : +32 474 831531\nWWW           : http://www.fynu.ucl.ac.be/users/l.quertenmont/\n. ",
    "stuz5000": "I'm also interested this.\nHowever a new release with this hasn't been published yet. I was hoping that I could make my sbt depend on the active master here on github. However, this doesn't seem straightforward.\nUntil then, \n- is there a way to serialize directly to memory (smile.io.write saves to file -- which is no good for many applications, including spark), or\n- is there a way to get sbt to build to master, or\n- can we get a new release\n  ?\n. Wonderful. Thank you\nOn Monday, August 15, 2016, Haifeng Li notifications@github.com wrote:\n\n@stuz5000 https://github.com/stuz5000 and @quertenmont\nhttps://github.com/quertenmont: smile 1.2 is just released with these\nchanges. please have a try. thanks!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/79#issuecomment-239814293, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AHcErIyEGnaDk0ie56kO-jJm1NCfbSO4ks5qgHXxgaJpZM4IiR3S\n.\n. Random sequences need to be assigned to tasks/algorithms, not threads.\n\nSuppose we have, an algorithm that wants to start 20 tasks in 5 threads.\nAnd suppose that tasks(6) wants to start 8 jobs in 2 thread. Each job in\nthis tree needs to work with a random number generator that's determined by\nits position in the tree:\nWe'd like the functional dependence:\ngetRandomGenerator( tasks(6.3.4) )\nwhere job 6 started job 6.3, which started job 6.3.4.\nOne way to do this is:\ndef sometask(seed:Long):\n val  rnd = getRandomGenerator(seed)  # rnd is used for\n  for n in numSubTasks:\n      subtask( seed=rnd.generateInt() )\nsubtask could executed immediately or in a thread.\nIf this method doesn't have degenerate cases, then I think its sufficient\nif:\n- that every algorithm that requires a random number generator is required\n  to be provided a seed\n- every subalgorithm started that requires a random number generator must\n  be issued the seed for its own generator by the parent algorithm\nAlternatively we could provide a random number generator directly for all\nalgorithms (instead of the seed):\ndef sometask(rnd1:RandomGenerator):\n  for n in numSubTasks:\n      subtask( rnd2=RandomGenerator(rnd1.generateInt()) )\nThis is equivalent, but care needs to be taken that rnd1 isn't accidentally\npassed as rnd2 in the case where a subtask is executed in another thread.\n. If you want functional determinism ( a requirement for repeatable\nexperiments, and also deterministic unit tests) one of 3 things has to\nhappen:\n- the algorithm is passed a seed\n- the algorithm is passed an interface that generates the rand one\n  numbers,\n- the algorithm is made deterministic by internally using a fixed seed.\nI think all this means is that they should use either the supplied seed or\nrandom number interface to generate its numbers or initialize further\nrandom generators.\nSetting the number of threads to 1 doesn't lead to determinism if algorithm\nrandomly pick their own seeds.\nThreading isn't the major issue (although I can understand it makes it\ndifficult at the edges). It's not worth tackling the non determinism caused\nby multi threading until the basic single threaded versions and\nfunctionally determined by their parameters.\n. My point is, supplying the means to be deterministic is no bad thing for an\nML algorithm, or any algorithm where you want repeatable behavior.\nPreserving encapsulation isn't a good argument for behavior that is\nrequired to be externally sepecified. IF you want to be able to externally\ndetermine the pseudorandom sequence then it MUST be externally specified. I\ndon't see that it breaks encapsulation any more than others parameters to\nalgorithms.\nOn Monday, August 15, 2016, Stuart Reynolds stuart@stuartreynolds.net\nwrote:\n\nIf you want functional determinism ( a requirement for repeatable\nexperiments, and also deterministic unit tests) one of 3 things has to\nhappen:\n- the algorithm is passed a seed\n- the algorithm is passed an interface that generates the rand one\n  numbers,\n- the algorithm is made deterministic by internally using a fixed seed.\nI think all this means is that they should use either the supplied seed or\nrandom number interface to generate its numbers or initialize further\nrandom generators.\nSetting the number of threads to 1 doesn't lead to determinism if\nalgorithm randomly pick their own seeds.\nThreading isn't the major issue (although I can understand it makes it\ndifficult at the edges). It's not worth tackling the non determinism caused\nby multi threading until the basic single threaded versions and\nfunctionally determined by their parameters.\n. +1\n\nOn Mon, Jan 7, 2019 at 9:57 AM ngorelick notifications@github.com wrote:\n\nI would like to reopen this issue. Repeatability is pretty crucial for\nsome uses (including mine) that aren't just test & verification.\nYou can manage the multi-threading issue by using a global seed value\n(lets say 0) and for each task, increment the seed. In RandomForest.java,\nfor instance, the threading is done with TrainingTask()s built in a\nfor-loop. Passing a new seed to each TrainingTask of (userSeed + i) would\nfulfill the requirements for repeatability and still give each thread its\nown unique seed with which it could start its own random number generator.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/110#issuecomment-452023155, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AHcErAhd7PUMpNOk0PZHUuGspAOLkekpks5vA4qTgaJpZM4JiqoI\n.\n. \n",
    "purnendu23": "Hello,\nThis is helpful. Are all the models now serializable? I am using gbm. \nthanks,\npurnendu. ",
    "pacozaa": "Thank you. \nYes, I have checked it. Do you mean in User Guide tab? If not please give me some example link.\nCan I use this library in JAVA or I need to use it by Scala only?\nI updated my question again for more information.\nDo you mean I have to use some java's file reader?\nI don't think my error in the question is about file stuff as I search it on google.\nAny further question would be appreciate.\n. Thank you. I will try this and give a feedback as soon as possible.\n. ",
    "lukehutch": "This might be useful as another, faster least squares regression method:\nhttp://papers.nips.cc/paper/5105-new-subsampling-algorithms-for-fast-least-squares-regression.pdf\n. I allocated 30GB to the JVM, and I have 32GB of RAM. It's not a GC issue as far as I can tell, I stopped the code in the debugger, and it's running in QRDecomposition.java, which doesn't seem to allocate new objects in its loops, as far as I can see.\nI didn't try this on R, but I have successfully run linear regression on this dataset in SciKit-Learn (Python / C++), and it returns the result pretty quickly (I think it takes 10 or 20 seconds).\n. The QRDecomposition is O(n^2 . m), which gives ~120000 . 120000 . 1200 = 17.3T operations.\n. Sure, how should I send it to you? I can send you a file in LibSVM format.\n. Hi Haifeng, thanks for looking into this (sorry for the delayed response). I'm not seeing the \"java.lang.RuntimeException: Matrix is rank deficient.\" message with OLS. With Lasso I see:\n```\nJun 13, 2016 6:38:48 PM smile.regression.LASSO \nINFO: LASSO: primal and dual objective function value after   0 iterations: NaN NaN\nJun 13, 2016 6:39:21 PM smile.math.Math solve\nINFO: BCG: the error after  10 iterations: NaN\nJun 13, 2016 6:39:54 PM smile.math.Math solve\nINFO: BCG: the error after  20 iterations: NaN\nJun 13, 2016 6:40:23 PM smile.math.Math solve\nINFO: BCG: the error after  30 iterations: NaN\n```\nI don't understand how the data could be rank-deficient for OLS, given that there are many more data points than dimensions, and the data points are mostly unrelated to each other. What am I missing?\n. I performed an SVD in NumPy. The singular values are all non-zero, although the smallest 7 values are 1.64e-13, very close to zero -- which I assume is what you mean about the data being rank deficient?\nIt is possible for some of the columns (some of the instance features) to be colinear, but I didn't know that would cause a problem with regression. Do I need to remove exact-duplicate features from the feature vector before performing regression? Although I think only duplicate rows will create a problem with rank deficiency, so do I need to find exact-duplicate rows first and remove them? If so, isn't it a problem for SMILE that it can't automatically handle exact-duplicate training examples without the numerical methods failing?\nIncidentally, NumPy uses LAPACK for SVD, and is significantly faster than SMILE (though I still have never waited long enough for SMILE to complete, so I don't know how long it will take, I always kill it after waiting many minutes for it to complete, since I don't know if it is going to take minutes, hours or years, depending on how the implementation scales with array dimension).\n. Sorry, I forgot to answer the info on the system. I am using a 64-bit Java 8 JRE on a 12-core (hyperthreaded) Xeon E5-1650 v3 @ 3.50GHz. I am using the latest release of SMILE, 1.1.0.\nI looked at the data, and there are 7 columns that are all zero, which explains the rank deficiency. However, I don't understand how having linearly-dependent columns should stop regression from working (assuming that by columns, we're both talking about data dimensions, and by rows, we're talking about instances). In fact, I have no problems using either sklearn.linear_model.LinearRegression or sklearn.linear_model.Lasso to get reasonable regression results. Both methods take 30-35 seconds. With the same exact data, I am still getting NaN from SMILE using the corresponding methods. I haven't tried R.\nHere is the latest version of my data, which might be slightly different from the version I sent you previously (it may contain a few thousand more training examples). Again the first column is y (the regression target), and the remainder of the columns are the feature matrix X, with features in the columns and data instances in the rows.\nhttps://drive.google.com/file/d/0B3BBfApbFAuZUS05YlpNclJoUXc/view?usp=sharing\nAs far as SVD speed: np.linalg.svd takes 90 seconds to perform an SVD, using full_matrices=False, so NumPy SVD is 3x slower than SciKit-Learn regression. Yes, I understand that LAPACK should be faster, but based on these numbers alone, it is clear that SciKit-Learn is not performing SVD via NumPy to implement LinearRegression, since LinearRegression finishes 3x faster than SVD. I am timing SMILE's SVD on this data, and it has been running a while, so I need to leave it running overnight as I leave work, will report back. (Progress so far in the outermost loop indicates it may take 40-50 minutes to complete.)\n. I'm not, I'm using a Google-internal patched version of OpenJDK 8. The actual build number won't mean anything outside that context. Assume that it is (more or less) equivalent to OpenJDK.\nHow does the JRE version affect the issues I described?\n. Your linear regression results look correct, and the summary stats at the bottom are exactly the same as the output printed by R without removing the zero columns. (R correctly detects 478 degrees of freedom across 485 variables, 7 of which are zero for all rows.)\nOn my machine, this regression takes: (SMILE times below are using Oracle JDK 8_91, not OpenJDK):\n- 40 seconds in R (no need to remove zero columns)\n- 10 seconds in sklearn (no need to remove zero columns)\n- 13 minutes in SMILE OLS (after removing zero columns to avoid rank deficient exception) --10 minutes on your machine\n- For comparison, SMILE LASSO takes 9 minutes to converge (after removing zero columns to avoid NaN values, doesn't throw an exception)\n(PS: I don't think it has been true for quite some time that OpenJDK performs substantially differently than Oracle JDK, in fact I think the codebases have largely converged. To check this though, I ran the same SMILE SingularValueDecomposition on both OpenJDK and Oracle JDK, and the time taken was 44 minutes vs. 42 minutes respectively -- roughly within the expected margin of variation, depending on whatever else the machine was doing at the time.) \nSo, there are two problems here:\n(1) SMILE is roughly 78x slower at linear regression than sklearn, on my machine, on Oracle JDK 8.\n(2) SMILE cannot handle zero columns during either linear regression or lasso, but both sklearn and R can handle zero columns.\nProblem (1) could potentially be alleviated by at least multithreading the QR decomposition, or using some other method. Linear regression is such an important staple, as a baseline for machine learning, that this method really needs to be faster!\nProblem (2) is particularly bad for rare features: it might be the case that in a given feature vector, for a specific 5-fold cross validation division of the data, all the non-zero entries in the rare feature component end up in the test set, and the training set has all zeroes for that column. It could be the case that SMILE crashes only sometimes, on some random cross-validation divisions, and not on others. This is a serious problem, because rare features are quite commonly present in a model.\nFor OLS, the exception triggered in problem (2) is caused by the linear regression method (involving QR decomposition) requiring full rank. How can this be fixed to work with deficient rank?\nFor LASSO, the NaN values triggered in problem (2) is caused by division-by-zero when the variance of a column is zero -- see the following line:\nhttps://github.com/haifengl/smile/blob/129889d482379526d2ff0ecbb344b1dac9d2c11e/core/src/main/java/smile/regression/LASSO.java#L280\nLine 275 should be changed to:\ndouble s = Math.sqrt(scale[j] / n);\n            scale[j] = s == 0.0 ? 0.0 : 1.0 / s;    // (or compare Math.abs(s) to some EPSILON)\nthen line 280 should be changed to:\nX[i][j] *= scale[j];\nI didn't check the rest of LASSO to see if this is the only change needed, but it's never good practice to do floating point division without checking for zero first. (I spotted another case of possible unchecked division-by-zero in QRDecomposition.) (It's also faster in general to multiply by the reciprocal than to divide.)\n. Yes, I'm aware of the likely difference in runtime between Fortran code and Java code. However, a factor of 78 difference cannot be explained by a combination of SSE speedup (generally up to 4x), inefficiency in JIT-generated code compared to AOT-compiled code (up to 2x), Java-specific overheads (array bounds checks, reference handling etc., maybe a 20-50% overhead at most, assuming you're not allocating new memory, which you're not). There's still a factor of ~10x difference in performance between sklearn and SMILE, taking all of this into account (and sklearn also has overheads for the Python<->C++ and C++<->Fortran interface boundaries). You're right that the LAPACK code is not multithreaded either (although most numerical methods can be parallelized to some degree). So more likely the difference does not come down to just language/compilation/runtime efficiencies, but difference in numerical methods.\nThis is easy to test though -- there are several projects to bring BLAS/LAPACK to Java using f2j and other tools, e.g.:\n- http://icl.cs.utk.edu/f2j/\n- https://github.com/fommil/netlib-java  ;  http://jeshua.me/blog/NetlibJavaJNI\n- http://jblas.org/\nYou may find some other fast pure Java solutions here that would work:\n- http://math.nist.gov/javanumerics/\nIt would actually be helpful to have some benchmarks published on the SMILE site for the core numerical methods (SVD, QR decomposition, etc.) of SMILE vs. other more common Java alternatives.\nIf you do eventually want to try vectorizing the most crucial numerical methods, some options are:\n- JOCL -- a Java wrapper for OpenCL: https://jogamp.org/jocl/www/\n- Aparapi -- a wrapper for turning data-parallel operations into OpenCL: https://github.com/aparapi/aparapi  ;  http://developer.amd.com/tools-and-sdks/opencl-zone/aparapi/  (Note that Aparapi is likely going to be part of Java 9 at release.)\n- RootBeer for GPU programming from Java: https://github.com/pcpratts/rootbeer1\nHowever, I think a large speedup (at least 5-10x) could be obtained directly in Java by improving the numerical methods, or choosing different algorithms altogether.\nI will create a separate bug for the rank deficiency issue (#88), since this bug has turned more into a discussion of efficiency.\nEdit: It does look like ND4J is the best solution. You get portability across CPUs / GPUs (via BLAS) and clusters (via Spark) if you base all your computationally-intense operations on ND4J.\n. Great, thanks for your work on this. When are you thinking of doing the next release?\n. SMILE LASSO: 9 minutes\nsklearn Lasso: 38 seconds\nHowever, I don't know if I set the parameters the same way -- SMILE takes lambda (I set it to 1.0), whereas sklearn takes alpha (defaults to 1.0, not sure if it's even the same thing).\n. @haifengl re. shipping different versions for different platforms: What you would do is just depend on ND4J, then the user can depend on the appropriate backend for their architecture, and/or to add GPU or cluster support (ND4J will automatically pick the fastest backend using ServiceLoader and a priority system). See:\nhttp://nd4j.org/backend.html\nhttp://nd4j.org/dependencies.html\nYou can see the difference in backend speeds here: http://nd4j.org/benchmarking\n. I'm not using sparse matrices, unless sklearn detected the matrix is sparse by itself, and sparsified it.\nI can't see how to make sklearn print verbose output with the Lasso method, so I don't know how many iterations it is using, but it stopped with an RMSE of 1.5. SMILE stopped after 150 iterations with an RMSE of 0.72. However, SMILE's debug output (on stderr) show the error swinging a lot during covergeance, so presumably the parameters are not set right.\n. Thanks. I only looked at one regression class, you probably want to double-check all the other regression and classification classes to be safe.\n. Oops, my bad, I don't know how I missed that. Sorry for the false alarm. \n. Personally, I always use linear regression as a baseline measurement. Any regressor should do at least as well as linear regression. It's easy then to see them how mutch lift you get from a given regression and set of parameters.\n. (added comment to the other bug about speed of SMILE vs. sklearn with Lasso)\n. I'm still using 1.1.\nOn Jun 16, 2016 6:50 PM, \"Haifeng Li\" notifications@github.com wrote:\n\nAre you using 1.1 or master?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/91#issuecomment-226661201, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAxhKeW-H-qRj94arsW5CwARGcp4KZr7ks5qMf1IgaJpZM4I39tM\n.\n. \n",
    "alexeygrigorev": "Maybe there should be a flag, something similar to inplace=True in pandas? \n. +1 run onto this issue today as well\n. I also tried Ridge and it also outputs NaNs for all the values of alpha I tried.\n. Possibly related to #86 \n. @haifengl isn't it a common practice to have the constant in the first (or last) column to account for bias? \nBy a quick look at the code I see that both the predictors and the target are mean-normalized, so the constant term isn't really needed. Maybe it should be reflected in the java docs somewhere? \n. Thanks\n. ",
    "elkfrawy-df": "Please let me know if you want me to quash commits.\n. Yes, sorry about messy copy-paste code, I will fix the names. Generally implementation of equals and hashcode are working on the same fields. So I looked at what you already implemented on compareTo and did the same on equals and hashcode.\n. No problem then, we can close this pull request. \nThanks for your respond.\n. No, there still much to fix, I just started with these two and I needed that response to continue on the rest.\n. So should I make all static final String I added to be private? I think it would be better.\n. @haifengl Sorry about the import problem, I modified it no to do so anymore. I will fix all imports in my pull requests.\nAbout the string duplication, these fixes are not meant only for performance matters, it is most intended to improve maintainability. \n\nconstants can be referenced from many places, but only need to be updated in a single place.\n. @haifengl I will squash and resolve the conflicts\n. @haifengl Sorry, I forgot to resolve it, I will now.\n. @haifengl This is more for readability purposes, that there is no meaning for the initial value and it might confuse the developer.\n. That is fine, no problem.\n. @haifengl I can't see any good use of that. Why would you create an object of such utility class and pass it around? \n. Well if you have specific usage for such classes that is fine. But as a general practice, it is better for utility classes to be passed as class not as an object, specially there is no any polymorphism or inheritance there. \n. @haifengl Yes, it is only that replacement.\n. Yes exactly.\n. Yes, you are right, I will fix this.\n. Actually that rises a compile time exception \"Illegal generic type for instanceof\", after I searched the problem, this was one of the solutions, the other solution is to make the class \"DistanceNode\" static. Should I make it static?\n. This assignment for example is redundant and has no effect which reduces the readability.\n. \n",
    "justinsowhat": "I serialized the AttributeDataset with XStream and attached it here. Thanks!\n. Oh ok, that's why. Thanks! So if I were to use the MaxEnt model, my dataset should be a SparseDataset object instead AttributeDataset? And when should I use Nominal2SparseBinary?\n. Great, thank you!\n. ",
    "gkhanacer": "Thank you very much! \n. ",
    "ngorelick": "I would like to reopen this issue.  Repeatability is pretty crucial for some uses (including mine) that aren't just test & verification.\nYou can manage the multi-threading issue by using a user-supplied seed value and for each task, increment the seed.  In RandomForest.java, for instance, the threading is done with TrainingTask()s built in a for-loop.  Passing a new seed to each TrainingTask of (userSeed + i) would fulfill the requirements for repeatability and still give each thread its own unique seed with which it could start its own random number generator.\n. ",
    "sumanyu": "Cool! Will do.\nI clicked on User Guide and basically found none of the links were actually active, so I was judging the book by its cover. I haven't dug too deep into the scala docs yet.\nLet me rephrase. There is a difference between bleeding edge and leading edge. I couldn't quite tell whether the core is stable and new features are added or the core apis are rapidly changing. Hence, I asked! :)\n. ",
    "KevinCooper": "Haifeng,\nThe two proposed fixes have been made.  Thanks for your rapid response!\n. ",
    "jkirsch": "Thanks for looking into this \n. ",
    "rislam": "Yes figured that one. If you can provide some examples it will be really helpful to use such fast library.\n. ",
    "elbamos": "Do you have an alpha/beta of the cross build? \n\nOn Oct 4, 2016, at 12:47 PM, Haifeng Li notifications@github.com wrote:\nI am working on cross build for both 2.10 and 2.11.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Thanks, @haifengl \n\n[info] Compiling 141 Java sources to /vhome/amos.elberg/smile/math/target/classes...\n[error] javac: invalid source release: 1.8\n[error] (math/compile:compileIncremental) javac returned nonzero exit code\n[error] Total time: 6 s, completed Oct 4, 2016 8:22:52 PM\nAny suggestions?\n. I'm using Jdk 8.\n\nOn Oct 4, 2016, at 4:25 PM, Haifeng Li notifications@github.com wrote:\nYou need JDK8.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. That was the first thing I tried :( \nOn Oct 4, 2016, at 4:36 PM, Haifeng Li notifications@github.com wrote:\nI am not sure what's wrong. I have no issues at all. Can you do\nsbt clean\nfirst?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. I did double check the javac version, and it was definitely 1.8. That said, I've subsequently gotten it to work on some systems, but not on others.  It seems to be sbt/scala weirdness, but weirdness that's beyond your control, so I'm closing this issue.  Thanks for your help, and I'm really enjoying the software!\n. Haifeng it resolved when I added library dependencies for the scala library and scala compiler to the sbt file. I know that's not supposed to be necessary but that's what worked.\nOn Oct 14, 2016, at 9:24 AM, Haifeng Li notifications@github.com wrote:\nThanks @elbamos and @bruce-campbell-sensus !! Really confused about this weirdness. Glad it doesn't block you moving forward.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Fixed it-thanks!\nOn Oct 14, 2016, at 4:49 PM, Haifeng Li notifications@github.com wrote:\nCan you do a \"git pull\"? This error was fixed before. Thanks!\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "bruce-campbell-sensus": "I saw the same issue using Oracle.  I also wasn't sure what the problem was.  I got it building inside the IntelliJ IDE but not from the command line.  Same SDK and Scala version in and outside the IDE.\nI too am enjoying the software!\n. ",
    "kazjote": "Not really.\nIt is very convenient that we can load our data from multiple sources, join them, transform, prepare training sets and train model using one tool (Spark currently).\nIf we would switch model generation to Smile, we would have to persist training data outside of spark and run separate training process on separate machine (with Smile). It is more about convenience than money / time.\n. Thanks @myui . I will have a look at this.\n@haifengl we are still evaluating available options.\n. ",
    "mjunsilo": "Thanks a lot. I'll integrate when its available from a new Maven Central distribution.\n. Finally I found time to integrate the new rules feature but I was surprised to find that I can only add rules, not replace the default rules entirely with new rules. I tried to overwrite the existing rules just by applying the existing rules again in the constructor, but I got an array index out of bounds exception in line 83 of the stemmer. This was just a simple dry shot to see if my integration worked as intended, but I was surprised about the implementation. I thought specifying some input stream would completely override the existing rules, but that is not the case. I can see the benefits of just overwriting, if that is the intention with the addition approach, but it brings with it some peculiarities in edge cases, e.g. removing some rules requires specifying no op rules to replace them.\nSorry for not coming back earlier with this, I should have inspected the code, when it was committed the first time in master, but I simply didn't find the time. Its also not easy for me to test it, unless I have access to public snapshot distributions, do you have this somewhere?. The changes seem to work as expected. I have now included the new feature in the DKPro UIMA wrapper:\nhttps://github.com/mjunsilo/dkpro-core/commit/d9f8cf875b45b1eb306e3d1fa1b22b99cd7c7f81\nA DKPro issue has also been created for the change request:\nhttps://github.com/dkpro/dkpro-core/issues/1001\nI will make a pull request once you pushed the 1.2.2 release to Maven Central.. BTW if you have some links to how the rule syntax works then I'll include it. I think its probably clear to someone who knows how the stemmer works, so I don't think its crucial if you don't have it.. ",
    "caomiaoke": "Thanks for your warm explanation.. ",
    "lanastazia": "the new version 1.2.1 fixed this issue, thank you!. ",
    "beckgael": "It works like a charm for my needs, try to build new scala/spark algorithm on sparknotebook using your gorgeous building blocs, thank you for your reactivity.\nI will inform you if i encounter any new issue with it.\nMery Christmas :). I improve general design thanks to your advices. Delete BinaryDistance. Extends metric to Distance[Array[Int]]. Unfortunately i have no more usefull informations about some distances. So we can erase them knowing that user can implement its own metric for a specific usage.\nI'm waiting for new wise comments.\nHappy Easter . I made correction as you requested except for the last one with the implementation of PartitionClustering which cause me some troubles.. Here are listed implemented distance plus other possibilities.\nA survey of binary measurement.pdf\n. K-Modes is the binary equivalent for K-Means. The mean update for centroids is replace by the mode one which is a majority vote among element of each cluster. I don't know well enough Clarans, but here we select a mode and not a medoid.. Did you recommend that for already java existing version of hamming and manhattan distance, i call them from the scala version ?\nThank you.. Done. Harmonized. At origin it is because i have multiple clustering algorithms sharing this common properties. It could be more usefull when i will add the others algorithms.\nI removed it for the moment. . I don't know good use case, i just implemented it in order to test them. Did i need to remove them ?. I don't know good use case, i just implemented it in order to test them. Did i need to remove them ?. It's an utility to choose the distance but yes, user can make it by himself. A survey of binary measurement.pdf\nI found these measures ih that survey.. A survey of binary measurement.pdf\nBased on that survey difference between Manhattan and Mean Manhattan are as follow.\na,b,c,d are the resuts of the contingency matrix\nManhattan : b + c\nMean Manhantan : (b + c) / (a + b + c + d) . ",
    "Mega4alik": "Thanks for response, What about Java? . Constantly getting Out of memory error, even after increasing the heap size.\nCan you suggest some alternative?\nThanks in advance. Tried to print directly to file as well \nPrintWriter out = new PrintWriter(new File(g.Path+\"/temp/model.txt\"));      \n xstream.toXML(model, out);\n. ```\nException in thread \"main\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n    at java.util.Arrays.copyOfRange(Arrays.java:3664)\n    at java.lang.StringBuffer.toString(StringBuffer.java:671)\n    at com.thoughtworks.xstream.io.xml.XmlFriendlyReplacer.escapeName(XmlFriendlyReplacer.java:55)\n    at com.thoughtworks.xstream.io.xml.AbstractXmlWriter.escapeXmlName(AbstractXmlWriter.java:36)\n    at com.thoughtworks.xstream.io.xml.PrettyPrintWriter.startNode(PrettyPrintWriter.java:73)\n    at com.thoughtworks.xstream.io.xml.PrettyPrintWriter.startNode(PrettyPrintWriter.java:86)\n    at com.thoughtworks.xstream.io.WriterWrapper.startNode(WriterWrapper.java:22)\n    at com.thoughtworks.xstream.io.path.PathTrackingWriter.startNode(PathTrackingWriter.java:30)\n    at com.thoughtworks.xstream.io.ExtendedHierarchicalStreamWriterHelper.startNode(ExtendedHierarchicalStreamWriterHelper.java:6)\n    at com.thoughtworks.xstream.converters.collections.AbstractCollectionConverter.writeItem(AbstractCollectionConverter.java:51)\n    at com.thoughtworks.xstream.converters.collections.ArrayConverter.marshal(ArrayConverter.java:34)\n    at com.thoughtworks.xstream.core.AbstractReferenceMarshaller.convert(AbstractReferenceMarshaller.java:55)\n    at com.thoughtworks.xstream.core.TreeMarshaller.convertAnother(TreeMarshaller.java:50)\n    at com.thoughtworks.xstream.annotations.AnnotationReflectionConverter.marshallField(AnnotationReflectionConverter.java:46)\n    at com.thoughtworks.xstream.converters.reflection.AbstractReflectionConverter$2.writeField(AbstractReflectionConverter.java:112)\n    at com.thoughtworks.xstream.converters.reflection.AbstractReflectionConverter$2.visit(AbstractReflectionConverter.java:88)\n    at com.thoughtworks.xstream.converters.reflection.PureJavaReflectionProvider.visitSerializableFields(PureJavaReflectionProvider.java:117)\n    at com.thoughtworks.xstream.converters.reflection.AbstractReflectionConverter.doMarshal(AbstractReflectionConverter.java:73)\n    at com.thoughtworks.xstream.converters.reflection.AbstractReflectionConverter.marshal(AbstractReflectionConverter.java:43)\n    at com.thoughtworks.xstream.core.AbstractReferenceMarshaller.convert(AbstractReferenceMarshaller.java:55)\n    at com.thoughtworks.xstream.core.TreeMarshaller.convertAnother(TreeMarshaller.java:50)\n    at com.thoughtworks.xstream.converters.collections.AbstractCollectionConverter.writeItem(AbstractCollectionConverter.java:52)\n    at com.thoughtworks.xstream.converters.collections.CollectionConverter.marshal(CollectionConverter.java:44)\n    at com.thoughtworks.xstream.core.AbstractReferenceMarshaller.convert(AbstractReferenceMarshaller.java:55)\n    at com.thoughtworks.xstream.core.TreeMarshaller.convertAnother(TreeMarshaller.java:50)\n    at com.thoughtworks.xstream.annotations.AnnotationReflectionConverter.marshallField(AnnotationReflectionConverter.java:46)\n    at com.thoughtworks.xstream.converters.reflection.AbstractReflectionConverter$2.writeField(AbstractReflectionConverter.java:112)\n    at com.thoughtworks.xstream.converters.reflection.AbstractReflectionConverter$2.visit(AbstractReflectionConverter.java:88)\n    at com.thoughtworks.xstream.converters.reflection.PureJavaReflectionProvider.visitSerializableFields(PureJavaReflectionProvider.java:117)\n    at com.thoughtworks.xstream.converters.reflection.AbstractReflectionConverter.doMarshal(AbstractReflectionConverter.java:73)\n    at com.thoughtworks.xstream.converters.reflection.AbstractReflectionConverter.marshal(AbstractReflectionConverter.java:43)\n    at com.thoughtworks.xstream.core.AbstractReferenceMarshaller.convert(AbstractReferenceMarshaller.java:55)\n\n```. I need to load/save in Java . SVM . Exception in thread \"main\" java.io.NotSerializableException: smile.math.kernel.LinearKernel\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)\n    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n    at ml.Classifier.learn(Classifier.java:64)\n    at ml.Classifier.(Classifier.java:37)\n    at global.Function.(Function.java:45)\n    at global.Scripts.(Scripts.java:53)\n    at global.Scripts.main(Scripts.java:39). static SVM model;  \nmodel = new SVM(new LinearKernel(), 1.0, Math.max(Y)+1, SVM.Multiclass.ONE_VS_ONE);      . Thank you, can I build it for Java? . I added smile, smile-core, smile-data, smile-math, smile-ploat jar files into Java project and got this error))\nException in thread \"main\" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory\n    at smile.classification.SVM.(SVM.java:90)\n    at ml.Classifier.learn(Classifier.java:54)\n    at ml.Classifier.(Classifier.java:37)\n    at global.Function.(Function.java:45)\n    at global.Scripts.(Scripts.java:53)\n    at global.Scripts.main(Scripts.java:39)\nCaused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    ... 6 more. got it, but new error coming up \nException in thread \"main\" java.io.NotSerializableException: smile.classification.SVM$LASVM$SupportVector\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)\n    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n    at java.util.ArrayList.writeObject(ArrayList.java:762)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)\n    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n    at java.util.ArrayList.writeObject(ArrayList.java:762)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)\n    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n    at ml.Classifier.learn(Classifier.java:66)\n    at ml.Classifier.(Classifier.java:37)\n    at global.Function.(Function.java:45)\n    at global.Scripts.(Scripts.java:53)\n    at global.Scripts.main(Scripts.java:39). Thank you. Run \"git pull\"  and updated all smile libraries in Java project, but still getting the same error.\nHere's the code \nstatic SVM model;    \nmodel = new SVM(new LinearKernel(), 1.0, Math.max(Y)+1, SVM.Multiclass.ONE_VS_ONE);      \n        model.learn(X, Y);                           \n        FileOutputStream fileOut = new FileOutputStream(g.Path+\"/temp/model.txt\");\n        ObjectOutputStream out = new ObjectOutputStream(fileOut);\n        out.writeObject(model);\n        out.close();\n        fileOut.close(); \n        model.finish();  . Oh sorry, now did. But still not working))\n```\nException in thread \"main\" java.io.NotSerializableException: smile.math.DoubleArrayList\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)\n    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n    at java.util.ArrayList.writeObject(ArrayList.java:762)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)\n    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n    at java.util.ArrayList.writeObject(ArrayList.java:762)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)\n    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n    at ml.Classifier.learn(Classifier.java:66)\n    at ml.Classifier.(Classifier.java:37)\n    at global.Function.(Function.java:45)\n    at global.Scripts.(Scripts.java:53)\n    at global.Scripts.main(Scripts.java:39)\n```\nI tried  to initialize SVM without  and with  instead.  In this case the program runs without error, but gives wrong results in the end. . Actually arrays of integers, but I use double like in examples. Makes no difference for final result. \ndouble[][] X = gson.fromJson(g.getFileContent(g.Path+\"/temp/X.txt\"), double[][].class);\nint[] Y = gson.fromJson(g.getFileContent(g.Path+\"/temp/Y.txt\"), int[].class);                    . It worked! \nThank you and best of luck . Awesome! . @leo-xin Can you give an example? Thanks in advance. Sure,\nTraining Part\ndouble[][] X = gson.fromJson(g.getFileContent(g.Path+\"/temp/X.txt\"), double[][].class);\nint[] Y = gson.fromJson(g.getFileContent(g.Path+\"/temp/Y.txt\"), int[].class);            \nRandomForest model;\nmodel = new RandomForest(X, Y, 500);\nPrediction part\nint getPrediction(double[] doubles){\n int qIdx = model.predict(doubles);   \n return qIdx;\n}\n. @haifengl can you help? . @haifengl Thanks for response, \nIt means that the model always predict the same class . No, I don't\nThere're 1500 possible classes \nAs I mentioned before, python sklearn RandomForestClassifier works as expected for this dataset\n. Oh no, sorry\nThe size of X is [0,1499] binary and range of Y is [0,323] \nThere are about 1300 samples\n. * Size of X - 1500 fixed. Ok! \nI did not understand how to use NominalAttribute. Can you please give an example in code? \nI tried this, but getting java.lang.ArrayIndexOutOfBoundsException exception\nNominalAttribute ars[] = new NominalAttribute[10];\nfor (int i=0;i<ars.length;i++) ars[i] = new NominalAttribute(\"atr\"+i);             \nmodel = new RandomForest(ars, X, Y, 500);\nThanks). Thanks, it compiles. But results are still same\nActually X is not one-hot encoding vector.\nIt is vector of binary values which is built for marking existing features. \nexample: [0,0,1,0,1,0....]\n. Sure \nX\nY. Thank you for detailed answer\nTest set shows 84% accuracy using sklern Randomforest and 77% using smile SVM\n. ",
    "asmaier": "I still have the problem with Chrome on Mac OS X. But the webpage works now with Safari and on my iPhone 4S. It seems to me that Chrome thinks that there is \"active mixed content\" on the website.. According to https://www.whynopadlock.com the problem is that jquery doesn't use an https link. Instead the webpage includes jquery via the unsafe link http://code.jquery.com/jquery.min.js . . I can confirm that the problem is fixed. Thank you.. ",
    "jimbethancourt": "Possibly -- I've found a Java implementation at http://www.cnblogs.com/zhangchaoyang/articles/2182752.html but the author doesn't list a license, so I'm not sure if it would be appropriate to base an implementation on it.\nThoughts?\nThanks,\nJim. ",
    "deric": "In order to have the original Chamelenon as described in the paper, you'd need a hMETIS implementation that is available only as binary. Also the license states that it is for non-commercial usage only.. ",
    "jovilius": "Hi @haifengl does this PR make sense?. Thank you @haifengl . ",
    "the21st": "@haifengl thanks for your quick reply! My example was intended to be a minimal illustration of the problem.\nMy data are actually 3-dimensional, but I still need to store primitives as values, because they are indices of my data.\nSo this is a full example:\n```scala\nimport smile.neighbor.KDTree\nval keys = Array(\n    Array(5.0, 6.0, 7.0),\n    Array(9.0, 0.0, 9.5),\n    Array(5.0, 5.0, 3.5)\n)\nval values = Array(0, 1, 2)\nval tree = new KDTreeInt\nval result = tree.nearest(Array(6.3, 2.7, 5.9))\n```. ",
    "softignition": "Padding is never a problem for Haar wavelets.\nPlease visit the following page:\nhttps://en.wikipedia.org/wiki/Haar_wavelet#Example\nIt explains Haar transform of the vector {1,2,3,4} using matrix multiplication and it proofs that the result computed by your library is incorrect.. No, scaling won't help. The signs in output does not match.\nI think there are two problems with your implementations:\n1. Haar filter is misaligned with the input signal. Thats why rotating input gives correct result for the first level of DWT.\n2. For a 4-element vector should be 2 levels of DWT, not 1; 3 levels for 8-element vector, etc.\nAdditionally I think that coiflet wavelet filter coefficients are in reverse order.. In Mathematica DWT of the vector {1,2,3,4} with one level of refinement, scaled by 0.7071067811865475, gives:\n{1.5, 3.5, -0.5, -0.5}. But again - you are missing one level of decomposition here. You confused number of levels in DWT (algorithm passes - in case of full decomposition) with the minimum data length required to perform wavelet transform - these are not related to each other. The full decomposition stops at the deepest level when LAST PAIR of samples is convoluted with a filter. It's always last two samples regardless how long the filter is. But the longer the filter is, the largest minimal data length is required. Obviously you cannot transform signal shorter than a filter length.\nRegarding the coiflet coefficients, I may point you to a Wikipedia: https://en.wikipedia.org/wiki/Coiflet#Coiflet_coefficients\nMathematica also gives coefficients matching the ones mentioned in this Wikipedia article.\nOther wavelets probably would work fine, expect the missing last level of decomposition.\nAnd one more small thing: \"Symmlet\" wavelets are misspelled, it's \"Symlet\".\n. I'm not so sure either about the spelling, however both Mathematica and Matlab, and most academic papers refer it as \"symlet\".\nI'm sure that most people don't care if the coeffs are reversed, but you might wish to have it in pair with other software.. ",
    "hrzafer": "Thanks for the response. I've checked constructors for both. All I can see is they accept the data as a matrix. The only difference is the MaxEnt's data type being int[][] rather than double[][]. I want to be able to train the models with dense inputs pairs like {feature_index, feature_value}. . From the JavaDoc x - training samples. Each sample is represented by a set of sparse binary features. The features are stored in an integer array, of which are the indices of nonzero features.\nNow I got it. So the parameter p will be the actual number of features (size of feature space) and an element with n features will be represented with an array of n length.  Thanks.. Hi, I've one more related question. Do the indices of nonzero features have to be ordered in the array?. Does the index start with 0 or 1? For example {0, 3, 11} vs {1, 4, 12} ?. Thanks a lot.. Hi,\nI was going to open another issue regarding the accuracy and the following exception which I encounter occasionally. I can continue on this thread if you want.\njava.lang.IllegalArgumentException: Line Search: the search direction is not a descent direction, which may be caused by roundoff problem.. I'll send the stack trace shortly. But before that, I want to make sure that the problem is not arising from my code. There is a parameter int p for the MaxEnt.Trainer constructor. It supposed to be equal to the number of features right?. Does the number of features include the class label as well? When I try numOfFeats-1 or any value greater than numOfFeats, I get no error.. I have just 2 classes . I've prepared a small test set to see what is going on. My test set has 5 instances and 6 binary features as following:\n{1,1,1,1,1,0}\n{1,1,1,1,1,0}\n{1,0,0,0,0,1}\n{0,0,1,0,0,1}\n{0,0,0,0,1,1}\nThe first two instances belong to class-1 and the rest belong to class-2. The corresponding sparse int matrix for this dataset is this:\n{0,1,2,3,4}\n{0,1,2,3,4}\n{0,5}\n{2,5}\n{4,5}\nMaxEnt successfully converges and the following tests pass. (I've wrapped the MaxEnt class)\nassertEquals(d1.label(), classifier.classOf(d1));\nassertEquals(d2.label(), classifier.classOf(d2));\nassertEquals(d3.label(), classifier.classOf(d3));\nassertEquals(d4.label(), classifier.classOf(d4));\nassertEquals(d5.label(), classifier.classOf(d5));\nWhen I apply a feature occurrence threshold and eliminate features occurring in less than 3 instances, the data set becomes like this. 2nd, and 4th features of the initial data set is eliminated. \n{0,1,1,1}\n{0,1,1,1}\n{1,1,0,0}\n{1,0,1,0}\n{1,0,0,1}\nAnd the corresponding sparse int matrix is:\n{1,2,3}\n{1,2,3}\n{0,1}\n{0,2}\n{0,3}\nHere the 6th feature of the initial set becomes the first feature. This is because I'm using a hashmap internally which does not preserve order. However, theoretically, this should not be a problem. But MaxEnt fails to optimize tests fail.\nIf I preserve the order of features when applying the feature threshold, the dense and sparse matrices become like this and tests pass.\n{1,1,1,0}\n{1,1,1,0}\n{1,0,0,1}\n{0,1,0,1}\n{0,0,1,1}\n{0,1,2}\n{0,1,2}\n{0,3}\n{1,3}\n{2,3}\nThe order of features, in other words changing the order of columns in a dataset should not be a problem I suppose? I'll send more feedback shortly.. Yes the vector is {1,1,0,0,0}.. He is the all necessary data to reproduce the following exception.\ndataset as binary sparse matrix and y :  smile-test.txt\n[main] INFO smile.math.Math - L-BFGS: initial function value: 16473\n[main] INFO smile.math.Math - L-BFGS: the function value after  10 iterations: 746.75\n[main] INFO smile.math.Math - L-BFGS: the function value after  20 iterations: 156.15\n[main] INFO smile.math.Math - L-BFGS: the function value after  30 iterations: 27.074\n[main] INFO smile.math.Math - L-BFGS: the function value after  40 iterations: 3.2807\n[main] INFO smile.math.Math - L-BFGS: the function value after  50 iterations: 2.2741\n[main] INFO smile.math.Math - L-BFGS: the function value after  60 iterations: 1.6914\n[main] INFO smile.math.Math - L-BFGS: the function value after  70 iterations: 1.5584\n[main] INFO smile.math.Math - L-BFGS: the function value after  80 iterations: 1.5533\n[main] INFO smile.math.Math - L-BFGS: the function value after  90 iterations: 1.5434\n[main] ERROR smile.classification.Maxent - Failed to minimize binary objective function of Maximum Entropy Classifier\njava.lang.IllegalArgumentException: Line Search: the search direction is not a descent direction, which may be caused by roundoff problem.\n    at smile.math.Math.linesearch(Math.java:5011)\n    at smile.math.Math.min(Math.java:5197)\n    at smile.classification.Maxent.<init>(Maxent.java:246)\n    at smile.classification.Maxent$Trainer.train(Maxent.java:153)\n    at com.diligen.ai.classification.smile.SmileMaxentFactory.train(SmileMaxentFactory.java:47)\n    at com.diligen.parafix.Benchmark.test(Benchmark.java:101)\n    at com.diligen.parafix.Benchmark.main(Benchmark.java:32)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147). Can I train the SVM  implementation in Smile with a binary sparse matrix (int[][]) ? . Hi,\nSetting tolerance worked with this small data set. However, it didn't work for the larger one with more than 300.000 examples. When the data set grows, the feature space also grows. This is the output for 382876 examples with 39056 features. \n[main] INFO smile.math.Math - L-BFGS: initial function value: 2.6539e+05\nClassifier is trained in 138\n[main] INFO smile.math.Math - L-BFGS: the function value after   1 iterations: 2.6539e+05\nI apply k=5 threshold for features. If I don't apply this threshold, the feature space becomes 188433 but I don't get any error with smile. Also I don't get any error for both with StanfordNLP LogReg.\nThe accuracy is usually %1-%3 better with StanfordNLP with different dataset sizes and feature space.. For the following code,\nSVM<int[]> svm = new SVM<int[]>(new BinarySparseLinearKernel(), 0);\n        svm.learn(matrix, vector);// matrix is binary sparse\n        svm.finish();\nI got this error:\njava.lang.NullPointerException\n    at smile.classification.SVM$LASVM.smo(SVM.java:588)\n    at smile.classification.SVM$LASVM.reprocess(SVM.java:846)\n    at smile.classification.SVM$LASVM.learn(SVM.java:506)\n    at smile.classification.SVM$LASVM.learn(SVM.java:438)\n    at smile.classification.SVM.learn(SVM.java:1191)\n    at smile.classification.SVM.learn(SVM.java:1145)\nSince this SVM is an online classifier, I can also use it with dense vectors without having any memory issue. However, I couldn't find a code example for this.. I test it on a separate test set with 200.000 examples. I have written several unit-tests to find possible bugs but everything seems work fine. Besides I've converted the feature filtered data set to ARFF sparse format and tested with WEKA without any problems. \nFor the C, actually I don't know what it is, I just tried 0 :). Thanks for all the quick responses. As a conclusion, I think changing the order of a feature in the dataset should not affect the accuracy in any way.  This is the case I've seen with other libraries such as StanfordNLP and WEKA. I think the very small dataset I've provided above with 5 examples and 6 features is a good point to start to find out a possible bug.. Nop, I am using Oracle JDK on Windows 10.  \nRight now I am testing the SVM. It gives better accuracy but slower than MaxEnt. Do you think using any kernel other than BinarySparseLinearKernel or using dense double[] vectors gets it faster?\n. I've just tried the following:\n```\n  int[][] x = new int[][]{\n                {1, 2, 3},\n                {1, 2, 3},\n                {0, 1},\n                {0, 2},\n                {0, 3}};\n    int[] y = new int[]{1, 1, 0, 0, 0};\n\n    Maxent maxent = new Maxent.Trainer(4).train(x, y);\n\n```\nand  get this:\n[main] INFO smile.math.Math - L-BFGS: initial function value: 3.4657\n[main] ERROR smile.classification.Maxent - Failed to minimize binary objective function of Maximum Entropy Classifier\njava.lang.IllegalArgumentException: Line Search: the search direction is not a descent direction, which may be caused by roundoff problem.\n    at smile.math.Math.linesearch(Math.java:5011)\n    at smile.math.Math.min(Math.java:5197)\n    at smile.classification.Maxent.<init>(Maxent.java:246)\n    at smile.classification.Maxent$Trainer.train(Maxent.java:153)\n    at com.diligen.ai.TestMain.main(TestMain.java:19)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)\nMy JDK:\n```\njava version \"1.8.0_121\"\nJava(TM) SE Runtime Environment (build 1.8.0_121-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)\n````\nAnd I'm using smile-core 1.2.2.\n. I've created an empty maven project just added the smile dependency and tested it on both windows and ubuntu (Oracle JDK 1.8.1_121).  I did not see any error. However when I enabled the logging, I have seen the same error on both linux and windows. To enable logging I've added the following dependency. Please download the project from here and run on your environment. \n<dependency>\n        <groupId>org.slf4j</groupId>\n        <artifactId>slf4j-jdk14</artifactId>\n        <version>1.7.22</version>\n        </dependency>. I've just tried that option. It is still the same. Also, I get the following error when I try to use the shell on Windows. \n```\nC:\\Users\\harun\\Desktop\\smile-1.2.1\\bin>smile\n[ERROR] Terminal initialization failed; falling back to unsupported\njava.lang.NoClassDefFoundError: Could not initialize class scala.tools.fusesource_embedded.jansi.internal.Kernel32\n        at scala.tools.fusesource_embedded.jansi.internal.WindowsSupport.getConsoleMode(WindowsSupport.java:50)\n        at scala.tools.jline_embedded.WindowsTerminal.getConsoleMode(WindowsTerminal.java:204)\n        at scala.tools.jline_embedded.WindowsTerminal.init(WindowsTerminal.java:82)\n        at scala.tools.jline_embedded.TerminalFactory.create(TerminalFactory.java:101)\n        at scala.tools.jline_embedded.TerminalFactory.get(TerminalFactory.java:158)\n        at scala.tools.jline_embedded.console.ConsoleReader.(ConsoleReader.java:229)\n        at scala.tools.jline_embedded.console.ConsoleReader.(ConsoleReader.java:221)\n        at scala.tools.jline_embedded.console.ConsoleReader.(ConsoleReader.java:209)\n        at scala.tools.nsc.interpreter.jline_embedded.JLineConsoleReader.(JLineReader.scala:62)\n        at scala.tools.nsc.interpreter.jline_embedded.InteractiveReader.(JLineReader.scala:34)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n        at scala.tools.nsc.interpreter.ILoop$$anonfun$scala$tools$nsc$interpreter$ILoop$$instantiater$1$1.apply(ILoop.scala:858)\n        at scala.tools.nsc.interpreter.ILoop$$anonfun$scala$tools$nsc$interpreter$ILoop$$instantiater$1$1.apply(ILoop.scala:855)\n        at scala.tools.nsc.interpreter.ILoop.scala$tools$nsc$interpreter$ILoop$$mkReader$1(ILoop.scala:862)\n        at scala.tools.nsc.interpreter.ILoop$$anonfun$21$$anonfun$apply$9.apply(ILoop.scala:873)\n        at scala.tools.nsc.interpreter.ILoop$$anonfun$21$$anonfun$apply$9.apply(ILoop.scala:873)\n        at scala.util.Try$.apply(Try.scala:192)\n        at scala.tools.nsc.interpreter.ILoop$$anonfun$21.apply(ILoop.scala:873)\n        at scala.tools.nsc.interpreter.ILoop$$anonfun$21.apply(ILoop.scala:873)\n        at scala.collection.immutable.Stream$$anonfun$map$1.apply(Stream.scala:418)\n        at scala.collection.immutable.Stream$$anonfun$map$1.apply(Stream.scala:418)\n        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n        at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n        at scala.collection.immutable.Stream.collect(Stream.scala:435)\n        at scala.tools.nsc.interpreter.ILoop.chooseReader(ILoop.scala:875)\n        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1$$anonfun$apply$mcZ$sp$2.apply(ILoop.scala:914)\n        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:914)\n        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)\n        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)\n        at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)\n        at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909)\n        at smile.shell.Main$.runTarget$1(Main.scala:70)\n        at smile.shell.Main$.run$1(Main.scala:83)\n        at smile.shell.Main$.process(Main.scala:94)\n        at smile.shell.Main$.delayedEndpoint$smile$shell$Main$1(Main.scala:29)\n        at smile.shell.Main$delayedInit$body.apply(Main.scala:26)\n        at scala.Function0$class.apply$mcV$sp(Function0.scala:34)\n        at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)\n        at scala.App$$anonfun$main$1.apply(App.scala:76)\n        at scala.App$$anonfun$main$1.apply(App.scala:76)\n        at scala.collection.immutable.List.foreach(List.scala:381)\n        at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35)\n        at scala.App$class.main(App.scala:76)\n        at smile.shell.Main$.main(Main.scala:26)\n        at smile.shell.Main.main(Main.scala)\n                                                   ..::''''::..\n                                                 .;''        ``;.\n ....                                           ::    ::  ::    ::\n\n,;' .;:                ()  ..:                  ::     ::  ::     ::\n   ::.      ..:,:;.,:;.    .   ::   .::::.         :: .:' ::  :: :. ::\n    '''::,   ::  ::  ::::   ::  ;:   .::        ::  :          :  ::\n  ,:';  ::;  ::  ::  ::   ::   ::  ::,::''.         :: :.      .:' :::,,,,;;' ,;; ,;;, ;;, ,;;, ,;;, :,,,,:';..::::''..;'::,,,,::''\nWelcome to Smile Shell; enter 'help' for the list of commands.\n  Type \":quit\" to leave the Smile Shell\n  Version 1.2.1, Scala 2.11.8, SBT 0.13.11, Built at 2016-12-02 20:49:58.541\n===============================================================================\nsmile>\n```. I've tried the shell on Linux.  Here is the output:\nsmile> val me = maxent(x,y,4)\n[main] INFO smile.math.Math - L-BFGS: initial function value: 3.4657\n[main] INFO smile.math.Math - L-BFGS: the function value after   5 iterations: 1.8836\n[main] INFO smile.util.package$ - runtime: 7.839131 ms\nme: smile.classification.Maxent = smile.classification.Maxent@110e9982\nsmile> me.predict(Array(1,2,3))\nres5: Int = 0\nsmile>\nAs you may notice, the prediction is not correct. The model tends to predict everything as 0. \n. By the way if you could try to run the actual Java code, you may get the same result. . Scala api is only a thin wrap of Java API (which provides the real algorithms). We should get the same result. I know. But still...  :) . Hi,\nI've changed something on feature filtering code and I don't have the issue anymore. But I couldn't diagnose the problem. So I am still not sure. I'll take a deeper look tomorrow. My brain is exhausted for Today. Thanks for all your support.\nOn the other hand, I've tried the SVM with Linear and Gaussian kernels. The Gaussian kernel is terribly slow. I've given 1 for sigma. The Linear kernel is faster but again 4-5 times slower than MaxEnt. \nI'll keep giving feedback and creating new issues :) Thanks.. I think MS Word and PDF documents are considered as natural language. And ToC is not unusual in those documents.  Besides using ...... is quite common in forms to indicate blank fields. Actually, I already have a workaround for this. But for a wider range of use cases, this change could be nice.\nI don't think the rules for this case and the U.S. case have to conflict.\n. Thanks a lot. Not related but what is a typical good choice of parameters for the NeuralNetwork class. I've a big binary dataset and with two classes and I get around 94% accuracy with MaxEnt. Since a linear model is doing okay with this data. I keep hidden layers small (dimensionx3x2).  Also what would you advise for the error and activation function?. Thanks. I don't see the verison 1.2.2 neither on github releases page nor on the home page. I guess it is only on maven. I'll update the pom. Thanks again.. ",
    "aminaaslam": "Thanks. Thanks, it worked.. Not relevant to this question. I have to generate PMML from the decision tree model. I know your API doesnot provide PMML support. Can you give me an insight as how should i go about doing it(generating the PMML)?? . I would really appreciate if you can add methods to traverse the Decision Tree so i can export Decision Trees as PMML. I need PMML model so it can run on any system and is part of the requirement for my project. Then i will the predictions made by SMILE model and compare them with PMML predictions for verification. \nPlease let me know how long would it take for you to expose the traversal functionality.\nThanks,\n . ok cool i will do it. I will keep you posted.\nThanks for your prompt responses.. hi hai,\nThis is how i am trying to use Cross Validation for Split Validation \nsplitPct =0.6\n`int trainSize = (int) (data.size()(this.splitPct));\n         int valSize = (int) (data.size()(1-this.splitPct)); \n         double[][] x = data.toArray(new double[data.size()][]);\n         CrossValidation cv = new CrossValidation(trainSize, 2); \n         int error = 0;\n         int[] y = data.toArray(new int[data.size()]);\n     double[][] trainx = Math.slice(x, cv.train[0]);\n     int[] trainy = Math.slice(y, cv.train[0]);\n\n      model = new DecisionTree( data.attributes(),trainx, trainy, this.maxNodes, this.rule);\n      cv = new CrossValidation(valSize, 1); \n      double[][] testx = Math.slice(x, cv.test[0]);\n      int[] testy =   Math.slice(y,  cv.test[0]);\n      //model = new DecisionTree( data.attributes(),testx, testy, this.maxNodes, this.rule);\n      prediction = new int[testx.length];\n      for (int j = 0; j < testx.length; j++) {\n            prediction[j] = model.predict(testx[j]);\n            if (testy[j] != prediction[j]){\n                error++;\n            }\n        }`\n\nBut when i try to use  1 in the number of folds it doesnot give me anything in train array of CV. CrossValidation cv = new CrossValidation(trainSize, 2); \nso i am not able to train my model on the entire training data. Can you please guide me by looking at my code.. thanks hai.. Please dont worry about it. I was able to resolve it.. hi hai,\nI keep encountering this issue off and on. I am not sure whats going on? Can you please look into it and guide me. Here is my code.\ndouble[][] x = data.toArray(new double[data.size()][]);\n         CrossValidation cv = new CrossValidation(data.size(), this.numFolds); \n         int error = 0;\n         int[] y = data.toArray(new int[data.size()]);\n         NeuralNetwork.Trainer trainer = new NeuralNetwork.Trainer(this.errorFunction, this.activationFunction, x[0].length, this.units_layer_1,this.units_layer_2, 1);\n           for (int i = 0; i < this.numFolds; i++) {\n            double[][] trainx = Math.slice(x, cv.train[i]);\n            int[] trainy = Math.slice(y, cv.train[i]);\n            double[][] testx = Math.slice(x, cv.test[i]);\n            int[] testy =   Math.slice(y,  cv.test[i]);\n            model = trainer.train(trainx, trainy);\n            this.prediction = new int[testx.length];\n            this.confidence = new double[testx.length];\n            for (int j = 0; j < testx.length; j++) {\n                double[] probability = new double[1];\n                prediction[j] = model.predict(testx[j],probability);\n                this.confidence[j] = probability[0];\n                if ( prediction[j]!= testy[j]) {\n                    error++;\n                }\n            }\nThis is the exception that i keep getting.\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 1\n    at smile.classification.NeuralNetwork.learn(NeuralNetwork.java:908)\n    at smile.classification.NeuralNetwork.learn(NeuralNetwork.java:858)\n    at smile.classification.NeuralNetwork.learn(NeuralNetwork.java:925)\n    at smile.classification.NeuralNetwork$Trainer.train(NeuralNetwork.java:374)\n. Hi Hai,\nThanks for your help. Yes i have two classes and i get this problem when i use least.mean.square as error function. It means with least.mean.square output neurons =2 and if i am using cross.entropy then output neurons = 1\nerror function = least.mean.square\nNeuralNetwork.Trainer trainer = new NeuralNetwork.Trainer(this.errorFunction, this.activationFunction, x[0].length, this.units_layer_1,this.units_layer_2, 2);\nmodel = trainer.train(trainx, trainy);\nerror function = cross.entropy\nNeuralNetwork.Trainer trainer = new NeuralNetwork.Trainer(this.errorFunction, this.activationFunction, x[0].length, this.units_layer_1,this.units_layer_2, 1);\nmodel = trainer.train(trainx, trainy);\n. Thanks Sir!!!!!. Thanks hai and villu for your guidance. I havenot given enough thought to it. I may or may not publish it depending on how well it goes.. Thanks @vruusmann; thats how i am handling categorical features by having NormDiscrete element.\nBut i  am having trouble providing weights to Con element from Smile's NeuralNetwork.\n@haifengl I exposed Layer class in your implementation of Neural Network to get weights and outputs of a layer. But when i debug it; i come across this scenario. \ne.g. in my dataset i have 2 categorical features each has 3 categories and a Numeric attribute.  There are 7 NeuralInput but i get only 3(NeuralInputs) attributes from the net variable of type Layer[] which goes as input to the next layer;How do i work around other NeuralInput. According to one-hot-encoding i will have following NeuralInputs \n<NeuralInput id=\"0\">\n        <DerivedField optype=\"continuous\" dataType=\"double\">\n          <NormContinuous field=\"age of car\">\n            <LinearNorm orig=\"0.01\" norm=\"0\"/>\n            <LinearNorm orig=\"3.07897\" norm=\"0.5\"/>\n            <LinearNorm orig=\"11.44\" norm=\"1\"/>\n          </NormContinuous>\n        </DerivedField>\n   </NeuralInput>\n<NeuralInput id=\"1\">\n    <DerivedField name=\"derivedNI_MaritalWidowed\" optype=\"continuous\" dataType=\"double\">\n     <NormDiscrete field=\"Marital\" value=\"Widowed\"/>\n    </DerivedField>\n   </NeuralInput>\n   <NeuralInput id=\"2\">\n    <DerivedField name=\"derivedNI_OccupationClerical\" optype=\"continuous\" dataType=\"double\">\n     <NormDiscrete field=\"Occupation\" value=\"Clerical\"/>\n    </DerivedField>\n   </NeuralInput>\n   <NeuralInput id=\"3\">\n    <DerivedField name=\"derivedNI_OccupationExecutive\" optype=\"continuous\" dataType=\"double\">\n     <NormDiscrete field=\"Occupation\" value=\"Executive\"/>\n    </DerivedField>\n   </NeuralInput>\n<NeuralInput id=\"4\">\n    <DerivedField name=\"National_EmploymentPrivate\" optype=\"continuous\" dataType=\"double\">\n     <NormDiscrete field=\"Employment\" value=\"Private\"/>\n    </DerivedField>\n   </NeuralInput>\n   <NeuralInput id=\"5\">\n    <DerivedField name=\"Nationla_EmploymentPSFederal\" optype=\"continuous\" dataType=\"double\">\n     <NormDiscrete field=\"Employment\" value=\"PSFederal\"/>\n    </DerivedField>\n   </NeuralInput>\n   <NeuralInput id=\"6\">\n    <DerivedField name=\"National_EmploymentPSLocal\" optype=\"continuous\" dataType=\"double\">\n     <NormDiscrete field=\"Employment\" value=\"PSLocal\"/>\n    </DerivedField>\n   </NeuralInput>. @haifengl  this is my first layer or the input layer. but the net variable only give me 3 values in the output array where are ther should be 7 values in the output array to go into the next layer??. So are you saying if there are 3 features 2 categorical(with 3 categories in each feature) and one 1 Numeric feature even then input layer will have 3 neurons and 3 values in output(which will go as input to the neuron of the next layer) because there is nothing in the weights array. Then the weights in the next layer go as input to the following array?? . Hi hai, I am sorry i am bothering you. Your first layer doesnot have the same number of inputs as required by the PMML. I have in total 100 features and some of them are categorical so the total count comes up to 132(in terms of PMML 132 Neural Inputs). I only see 100 inputs in both the hidden layers. I am attaching pictures for your reference. This is from my debugger.\n\n\n. Thats how i am training the neural network. These are my input parameters.\n\"number.layers\": 2,\n    \"error.function\": \"least.mean.square\",\n     \"activation.function\": \"linear\",\n     \"hidden.layer1.size\":10,\n     \"hidden.layer2.size\":10,\n     \"validationMethod\": \"entire\"\ndouble[][] trainingArray = this.data.toArray(new double[data.size()][]);\nint[] y = data.toArray(new int[data.size()]);\n            NeuralNetwork.Trainer trainer = new NeuralNetwork.Trainer(this.errorFunction, this.activationFunction, trainingArray[0].length, this.units_layer_1,this.units_layer_2, this.output_layer);\n            model = trainer.train(trainingArray, y);. @haifengl, did u get a chance to look at my code. Is there something that i am missing which is causing the problem.. The length of trainingArray[0] is 100 bc i have hundred features. But some features are categorical and PMML treats every category of a feature as a separate input which  is the problem that  i am facing. I used Nominal2Binary class to do One hot encoding and i am using that as a data preprocessing step and feed the processed data to the Neural Network.\nBut my problem remains that same. If u look at my PMML, there are 131 inputs but i have only 9 connections in my hidden layer. I think i am not calculating the weights properly.\nI am trying to generate PMML from smile neural network output. \nI am using JPMML evualuator to verify my predictions. Problem was at my end and i am able to fix it. Thank you so much. As always this was very helpful. Hi Hai,\nI am trying to save the image generated by IsoMap in .png file but it keeps throwing this exception. Am i doing it correctly or i am missing something. Thanks,\n. hi Hai,\ni changed it to headless plot because i dont want to use the swing UI. I just need to export the image to a png\n`  PlotCanvas plot = ScatterPlot.plot(learner.coordinates, '.', Color.BLUE);\n       Headless headless = new Headless(plot);\n       headless.pack();\n       headless.setVisible(true);\n   plot.save(new java.io.File(\"headless.png\"));`\n\n. Please ignore this. There was a problem with my environment. HI Hai,\nI am using smile version 1.2.3. Do you think i should try 1.3.0??. ok I will test it with the latest version and let you know. until then i will keep the issue opne.\nThanks. hi hai,\nyou guessed it right. there may be duplicate samples in the data. Is duplicate data instances causing the problem? Does this mean i need to remove duplicate data instances from data?? or version 1.3.0 works around this problem.. Let me remove the duplicated samples and see what happens. I will revert back.\nThanks. Hi Hai,\nI ran some experiments on my data which didnt have duplicate samples and here are the results from my experiments.\nI believe ticket 174 are not linked \nManiFold Learning- LLE\n\nCards data.\nnone:works fine\nStandardize: Gives this error\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 402\n    at smile.manifold.LLE.(LLE.java:209)\n    at com.smile.dimensionality.reduction.LLELearner.learn(LLELearner.java:67)\n    at com.smile.dimensionality.reduction.ManifoldLearningFunction.execute(ManifoldLearningFunction.java:85)\n    at com.common.ModelingEngine.main(ModelingEngine.java:81)\nNormalize: Gives this error\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 402\n    at smile.manifold.LLE.(LLE.java:209)\n    at com.smile.dimensionality.reduction.LLELearner.learn(LLELearner.java:67)\n    at com.smile.dimensionality.reduction.ManifoldLearningFunction.execute(ManifoldLearningFunction.java:85)\n    at com.common.ModelingEngine.main(ModelingEngine.java:81)\nManiFold Learning- ISOMap\n\"method\" : \"isomap.learner\",\n  \"parameters\" : {\n    \"d\" : 2,\n    \"k\" : 5,\n    \"normMethod\" : \"normalize\",\n   }\n\"method\" : \"isomap.learner\",\n  \"parameters\" : {\n    \"d\" : 2,\n    \"k\" : 5,\n    \"normMethod\" : \"standardize\",\n}\n\"method\" : \"isomap.learner\",\n  \"parameters\" : {\n    \"d\" : 2,\n    \"k\" : 5,\n    \"normMethod\" : \"none\",\n    }. yes d means dimensions of data.\ni can have only two values in it 2 or 3.\nSo what would be a good range for k for these dimensions???. Hi Hai,\nReferring to your earlier comment k should be less than d. Then how do i run manifold learning on Mnist dataset and get these results.\nhttp://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py\nHere in the experiment k = 30 and number of dimensions =2 ??. This is one of the examples in the link\nn_neighbors = 30\nIsomap projection of the digits dataset\nprint(\"Computing Isomap embedding\")\nt0 = time()\nX_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\nprint(\"Done.\")\nplot_embedding(X_iso,\n           \"Isomap projection of the digits (time %.2fs)\" %\n\n           (time() - t0))\n\n. I am sorry for the miscommunication. This means i can set k greater the output dimensions of the data. So when i do that it gives me this error. Is it because of duplicate data samples? But when i set k <d(output of dimensions) this error disappears? can you please explain what is going on?. \nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 402\nat smile.manifold.LLE.(LLE.java:209)\nat com.smile.dimensionality.reduction.LLELearner.learn(LLELearner.java:67)\nat com.smile.dimensionality.reduction.ManifoldLearningFunction.execute(ManifoldLearningFunction.java:85). Hi Hai, \nSo i made sure there are no duplicates in my data but when i give these parameters it gives me this exception \nd = 2(dimensions of output data )\nk =3 \nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\nSLF4J: Defaulting to no-operation (NOP) logger implementation\nSLF4J: See\u00a0\u00a0for further details.\nException in thread \"main\" java.lang.RuntimeException: Matrix is singular.\n\u00a0\u00a0at smile.math.matrix.LUDecomposition.solve(LUDecomposition.java:254)\n\u00a0\u00a0at smile.manifold.LLE.(LLE.java:178)\n\u00a0\u00a0 \nI have raised a similar ticket https://github.com/haifengl/smile/issues/174\nWhat do you think is the cause of this issue ?. Here is the data that i am using and the data description file.\n. \niso-card.json.txt\nCardOperations-Training.csv.txt.gz\n. .json is the data descriptor \nthe other file is data. \nThanks,\n. For parsing the data ??\n. Hai, I am using univocity parser for parsing the data. \nimport com.univocity.parsers.csv.CsvParser;\nimport com.univocity.parsers.csv.CsvParserSettings;\nI dont know how to share the code with you because its so interdependent that i will have to share the entire project with you and thats a total waste of your time. This is the best i could do \ndouble data[][]; \n       IsoMap isomap = new IsoMap(data, d, k,true);\n        this.coordinates = isomap.getCoordinates();\n        this.graph = isomap.getNearestNeighborGraph();. k = k-neighbor\nd= dimensions of output data \nYes there are no duplicates in the data.\nHai, one more thing i actually ran Iso map on a data set with duplicate samples and it worked fine. Its just that there were only 100 records in there. \nIs size of data with higher number of k the cause of this exception?. Sorry , This is the value that i used \n \"d\" : 2,\n \"k\" : 3,. Here is the parsed data with no Strings and nominal features( i have done one-hot encoding). Let me know if this is what u wanted. Please find attached the .csv file and this data goes into the ISO-Map learner.\noutdata.csv.gz.zip\n. I am not using any String features and i am converting nominal features. Please let me know if this makes sense.. outdata.csv.gz\n. this is .gz file. You should be able to open this.\nTHanks!!! . Hi Hai,\nDid u get a chance to look at the file that i sent you.\nThanks!!!!. Hi Hai, \nI have a good news so i have a smaller data set which has 3000 records and ISOMap works on as big a value of k=50. I am not able to test this data set that i attached here because i am running into OOM even on my biggest machine and with k=3. \nHowever, LLE doesnot work even with k=3 on my smaller data set and throws the exception that i mentioned in ticket: 174 and here is the exception \nException in thread \"main\" java.lang.RuntimeException: Matrix is singular.\nat smile.math.matrix.LUDecomposition.solve(LUDecomposition.java:254)\nat smile.manifold.LLE.(LLE.java:178)\nI hope i am not confusing you any further.\nThanks!!!. Thanks that will be very helpful.\nAmina. Hi Hai, Did u get a chance to look at what is going one with Eigen Value Decomposition. Please let me know when you get a chance to look at this issue.\nThanks,\nAmina. Thanks hai for a very prompt reply. Is it safe to say the output points generated by PCA algorithms have one to one mapping with the original datapoints? The reason i am asking is this is because i am plotting the output points and i want to color code them based on their label or class(0 or 1) in the original dataset. If there is one to one mapping between original and output then i can use the same index to get the label of that data row. I hope i am making some sense?. Thanks i will check it out and will add my observations here. I was using version 2.0.0 of Smile. But for this change i will switch to Version 3 and work on that.\nThanks you once again.. Hi Hai,\nI am using CentOS 7 and installed following binaries using yum\nsudo yum install numpy scipy blas-devel lapack-devel atlas-devel \nand  i cloned smile's Master branch and when i do \nsbt package \nit gives me this error \n[error] /run/media/root/BackUpData/smile/scala/src/main/scala/smile/math/Operators.scala:233: value svd is not a member of object smile.math.matrix.Lanczos\n[error]   def svd(A: DenseMatrix, k: Int) = Lanczos.svd(A, k)\n[error]                                             ^\n[error] one error found\n[error] (scala/compile:compileIncremental) Compilation failed\n[error] Total time: 6 s, completed Jul 24, 2017 2:46:40 PM\nWhat am i missing ??\nThanks!!. Hi Hai,\nI was able to build smile 1.4.0 after your check in. So i tested following three algorithms using the new version but i am getting these errors \nPCA: \njava.lang.ArrayIndexOutOfBoundsException: 100\n        at smile.math.matrix.JMatrix.tql2(JMatrix.java:1749)\n    at smile.math.matrix.JMatrix.eigen(JMatrix.java:1368)\n    at smile.projection.PCA.(PCA.java:173)\n    at com.amina.jsat.evaluation.SmileCardDataTest.runPCAOnCard(SmileCardDataTest.java:90)\nOn KPCA i am getting this exception but i will try it on a bigger machine. I tried it with 6G RAM and even this its failing. \njava.lang.OutOfMemoryError: Java heap space\nat smile.math.matrix.JMatrix.<init>(JMatrix.java:80)\nat smile.math.matrix.Factory.matrix(Factory.java:105)\nat smile.math.matrix.Matrix.zeros(Matrix.java:100)\nat smile.projection.KPCA.<init>(KPCA.java:136)\nat com.amina.jsat.evaluation.SmileCardDataTest.runKPCAOnCard(SmileCardDataTest.java:67)\n\nPPCA completes in 1s and 149ms.\nI am also attaching performance results from JSAT's evaluation. I am using the same dataset for evaluating both libraries.\nCard Data Execution Time:\nIsoMap: 26 minutes with 12GB\nPCA:   823ms with 6G RAM\nKPCA:  17secs and 114ms with 6G RAM\ntSNE : 1min 55secs with 6G RAM\n    . I am attaching the data file for you\ncard.csv.gz\n. So i have added smile-netlib 1.4.0 jar on my classpath but i am getting this error. Is LAPACK not part of your jar. Do i need to add that to my classpath aswell. if yes then what version ?\njava.lang.NoClassDefFoundError: com/github/fommil/netlib/LAPACK\nat smile.netlib.NLMatrix.eigen(NLMatrix.java:435)\nat smile.projection.PPCA.<init>(PPCA.java:153)\nat com.amina.jsat.evaluation.SmileCardDataTest.runPPCAOnCard(SmileCardDataTest.java:99)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\nat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\nat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\nat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\nat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\nat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\nat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\nat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\nat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\nat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\nat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\nat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\nat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\nat org.junit.runners.ParentRunner.run(ParentRunner.java:363)\nat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\nat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)\nat com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)\nat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)\nat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)\n\nCaused by: java.lang.ClassNotFoundException: com.github.fommil.netlib.LAPACK\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335. Hi Hai,\nI was able to run PCA, PPCA algorithms on my data that i shared with you. Running into OOM issues with ISOMap and KPCA. Will try it on a bigger machine. \nBut when i ran LLE this is the exception that i am getting.  This i believe is the same exception that i was getting with older version of Smile.\njava.lang.RuntimeException: Matrix is singular.\nat smile.netlib.LU.solve(LU.java:115)\nat smile.netlib.LU.solve(LU.java:102)\nat smile.manifold.LLE.<init>(LLE.java:180)\nat com.amina.jsat.evaluation.SmileCardDataTest.runLLEOnCard(SmileCardDataTest.java:85)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\nat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\nat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\nat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\nat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\nat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\nat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\nat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\nat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\nat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\nat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\nat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\nat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\nat org.junit.runners.ParentRunner.run(ParentRunner.java:363)\nat org.junit.runner.JUnitCore.run(JUnitCore.java:137)\nat com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)\nat com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)\nat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)\nat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70). If i increase the number of neighbors would it help. In the above experiment i used k=5 ?\n\nI also try it on another dataset and see what it does.. Even with k =10 it gives the same problem on this dataset.. Hi Hai, Please dont think that i am trying to undermine your software all i am trying to do is evaluate it with different datasets that i have to make a determination whether to use it with my datasets or not.\n    kernel = new GaussianKernel(Math.sqrt(0.5));\n    KPCA kpca = new KPCA(data,kernel,2,0.001);\n\nI am getting this error \njava.lang.NegativeArraySizeException\n    at smile.math.matrix.JMatrix.(JMatrix.java:80)\n    at smile.math.matrix.Factory.matrix(Factory.java:105)\n    at smile.math.matrix.Matrix.zeros(Matrix.java:100)\n    at smile.projection.KPCA.(KPCA.java:136)\n    at com.amina.jsat.evaluation.SmilePaymentDataTest.runKPCA(SmilePaymentDataTest.java:72)\n. There is one comment that i forgot to add \nI have tried PPCA with all of my datasets and  it is working fine and not generating any errors. \nSize of datasetA:25000 instances.\nSize of datasetB: 4500 instances.\nSize of datasetC: 470000 instances.\nI made sure there no duplicates in these datasets.\nI am wondering why am i not seeing this issue(numeric instability) in PPCA?. Hi hai, there are no NaN or Inf values in my dataset. I am attaching one of the datasets that i am using for your reference \ncard.csv.gz\n. Hi hai, there are no NaN or Inf values in my dataset. I am attaching one of the datasets that i am using for your reference \ncard.csv.gz\n. Hi Hai, I have tried the same dataset on JSAT using ISOMAP and it works perfectly fine. I will be running KPCA and PCA with JSAT too and will update you. \nAnother observation that may or may not be correct is the size of data. I have a smaller data set with 4500 data instances and all of the above mentioned algorithms work fine with SMILE(ISOMAP,KPCA,PCA). . Hi Hai,  size of the dataset is 25000 records and it doesn't have large values. There are around 100 features in the dataset. \nPlease let me know when you are finished redesigning you matrix library.. ",
    "lazymahendrathapa": "I am referring the slides of Geoffrey Hinton.\nIn case of cross-entropy, cost function is given by (-1.0 * summation(t*log(y))). How can our objective be to maximize it? y always range from 0-1. In the case of y less than zero, log(y) gives negative value and -1.0 * log(y) gives the positive value. . Thanks for being cooperative. I will study again about ANN from basic and try to understand everything mathematically and geometrically. SMILE is a great library, thanks for making open source. This helps lots of people including me. Can you send your contact mail at lazymahendrathapa@gmail.com, so that I can contact you if I stuck somewhere in future? . ",
    "DataOmbudsman": "I used scala.util.Random. Here is the complete Scala code with imports:\n```\nobject AUCIssue extends App {\n  import scala.util.Random\n  import smile.validation.AUC\nval n = 100000\n  val trueLabels = Seq.fill(n)(Random.nextInt(2)).toArray\n  val probabilities = Seq.fill(n)(Random.nextDouble).toArray\nprintln(AUC.measure(trueLabels, probabilities)) // -1.89...\n}\n```\nMaybe (pos * neg) is lifted to double after the overflow? . ",
    "xuejianbest": "Thinks, I have solved it by using Java1.7 to recompile it.. ",
    "medke": "@xuejianbest Can you please tell me how you did it? or maybe send me the recompiled jar files ? I could not do it, because there things that can only be done in Java1.8 like default methods ..etc. @takun2s Thanks. . No unfortunately I am using Java.. ",
    "takun2s": "https://github.com/takun2s/smile_1.5.0_java7\nMaybe this can help you , i modified the code for java7/jdk1.7. ",
    "vruusmann": "@aminaaslam Nice to see that you're working on your own set of converter tools. Will you be publishing them when ready?\nRegarding the one-hot-encoding of NN input fields, then this is typically done using the NormDiscrete element:\nXML\n<NeuralInput id=\"ohe(x1=A)\" optype=\"continuous\" dataType=\"double\">\n  <NormDiscrete field=\"x1\" value=\"A\"/>\n</NeuralInput>\nFor example, here's a sample R/Rattle model:\nhttps://github.com/jpmml/jpmml-evaluator/blob/master/pmml-rattle/src/test/resources/pmml/NeuralNetworkAudit.pmml#L97-L126. ",
    "CreateRandom": "So, which sampling method would be used if I were to specify subsample = 0.8?. Sorry to keep bothering you about this, I just don't see the logic to it yet: Suppose I had a training set with n = 100 examples and would now want each tree in the RF to be trained on bagged subsets containing 80 examples each (i.e. I would set subsample to 0.8). Now, wouldn't it be possible to draw 80 examples with replacement? This is essentially what I'm trying to do and I would say what many users are trying to do when setting subsample to values smaller than 1.0. Especially since the authors of the paper mentioned above suggest that for sampling with replacement, subsample = 0.8 is usually a good option. Thanks again in advance!. Alright, I kept thinking I was missing something. Thanks for letting me know. Presumably, if I want sampling with replacement with smaller subset sizes, I will just make the necessary adjustments to the code on my own fork.. ",
    "DaveXster": "Dear Haifeng,\nIt works now!\nThank you,\nDavid. ",
    "javierfvargas": "Thank you. \nJust in case.  Bagging.java includes a very similar portion of code.\nAlso.  Could you elaborate a little on how classWeight works?  E.g, how I can achieve balanced class weight in unbalanced sets?. ",
    "nmaisonneuve": "I had to go to the code too to understand how to handle unbalanced data with random Forest .\n\n\ni have highly unbalanced data with a ratio of 1 / 10 for the classWeight , but the optimal results for the randomForest is to  put everything in false class...    any advice to be sure I train the classifier the right way?\n\n\nthe default classWeight is [1, 1] but this default assumption (of a balanced data) is really unlikely. From a end-user point of view, why is this classWeight parameters not more visible in the public training methods? trainer.train(dataset, prior)  ?\n\n\nshould not be computed automatically from the training test to set the default classWeight values?. \n\n",
    "PhlppKnst": "Hi,\nfirst, Thank you for creating and sharing this package. I forgot to mention this previously.\nI have here a code snippet from a unit Test.\nYou have to read a txt file and have to change the path to your file location.\nThen it should work. Let me know if you have problems or need help.\nThe test.txt with the test data.\nThank you!\nBest,\np\n` \n@Test\npublic void testCutTree() throws IOException {\nList<double[]> spectra = new ArrayList();\n\nBufferedReader br = null;\n//Reading the csv file\nbr = new BufferedReader(new FileReader(\"test.txt\"));\n\nString line = \"\";\n\nwhile ((line = br.readLine()) != null) {\n    String[] lineArr = line.split(\";\");\n\n    double[] vector = new double[lineArr.length];\n\n    for (int i = 0; i < lineArr.length; i++) {\n      vector[i] = Double.valueOf(lineArr[i]);\n    }\n\n    spectra.add(vector);\n}\n\nint n = spectra.size();\n\ndouble[][] proximity = new double[n][];\n\nfor (int i = 0; i < n; i++) {\n  proximity[i] = new double[i + 1];\n  for (int j = 0; j < i; j++) {\n    proximity[i][j] = Math.distance(spectra.get(i), spectra.get(j));\n  }\n}\n\n/*\nend file read\n */\n\nHierarchicalClustering hac = new HierarchicalClustering(new SingleLinkage(proximity));\n\nint[] backArray = hac.partition(97);\nList<Integer> backList = uniqueArray(backArray);\nSystem.out.println(backList.size());//94\n\nbackArray = hac.partition(89);\nbackList = uniqueArray(backArray);\nSystem.out.println(backList.size());//87\n\n}\nprivate static List uniqueArray(int[] arr) {\nList<Integer> uniList = new ArrayList();\n\nfor (int i = 0; i < arr.length; i++) {\n\n  int index = uniList.indexOf(arr[i]);\n\n  if (index == -1) {\n\n    uniList.add(arr[i]);\n  }\n\n}\n\nreturn uniList;\n\n}\n`. ",
    "stieglma": "wow, that was fast, thanks!. That's true, thanks for the fast fix! Do you already have a date in mind for the next release?. That's what I thought, we did clone the project already, and I also made some changes to clean up the code a little bit (many easy-to-fix rawtype/generics warnings coming up with eclipse, as well as some unused imports) if you are interested in that I can create a pull request with these fixes.\nExactly for the task of importing this project into eclipse was quite hard as there is no really good sbt-support. Would you mind creating a pom file such that it can be properly imported? This will make it for everyone who wants to contribute much easier to start.. At first, no this isn't a Java 8 feature, this is in place since Generics are, I think this is since 1.5.\nWhat happens there? Well Trainer itself is a class with a generic type parameter T (c.f. line 127 in the code, later on used for the distance field (Metric<T>)) and as such it should not refer to the \"raw type\" Trainer (without generic parameter) as you loose compile-time safety about the types used there.\nThink of e.g. Lists, when you have a list of strings you write List<String> and get only offered methods to add strings to this list, and of course you can also only retrieve strings from that List (no ints, doubles, other types). But an even better example is the method subList which returns a List of Strings again, if they did omit the generic type parameter here, only a list would be returned where you do not know of which type the elements are, you would get only Objects instead everywhere. (This is actually what happens at runtime due to type erasure, but this is another topic ;) )\nSo in your specific case, the reason is that you wouldn't know which type is inside the trainer instance afterwards besides by using casts, which are not necessary if generics are used consistently.. One further notice, in eclipse many of these warnings are generated directly when you write the code due to the incremental compiler, I am not sure how IntelliJ IDEA is working there but I have in mind that it doesn't compile instantly when you write code but only on certain actions. What I am quite sure of, is that IntelliJ does not by default run the so called \"code inspection\" all the time, you have to trigger it manually, and it has a vast amount of configuration options. They are quite powerful and should definitely find and report such things.. Sounds like a reasonable default, but I never heard of that, in fact you could also write something like this public <X> Trainer<X> setRBF(...) which could potentially make sense if you created a new trainer instance of different type there, so I think even if omitting the generic type parameter is valid you should add it such that this does not lead to problems in future development.\nI also just tried that:\n```\npublic class Test {\n        T tmp;\n    public Test setTmp(final T x) {\n        tmp = x;\n        return this;\n    }\n\n    public T getTmp() {\n        return tmp;\n    }\n\n    public Test<T> setTmp2(T y) {\n        tmp = y;\n        return this;\n    }\n}\n\n```\nNow it is perfectly fine to write something like String str = new Test<String>().setTmp2(\"abc\").getTmp();\nbut writing String str = new Test<String>().setTmp(\"abc\").getTmp(); does only work when you add a cast to String. ",
    "lifehackman": "Sorry for misleading you. The sqrt really doesn`t impact performance. \nWhat really matters is this line of code:\ndouble distance = Math.squaredDistance(q, keys[index[idx]]);\n    if (distance <= radiusSquared) { ... }\n...\nif (radiusSquared >= diff * diff) {\n    search(q, further, radiusSquared, neighbors);\n}\ncompared to Nearest Neighbour search:\ndouble distance = Math.squaredDistance(q, keys[index[idx]]);\nif (distance < neighbor.distance) {\n    ...\n    neighbor.distance = distance;\n}\n...\n// now look in further half\nif (neighbor.distance >= diff * diff) {\n     search(q, further, neighbor);\n}\nThe diff (distance from cutting plane) is squared, and we must check it with square of distance (or radius), not with radius, and this type of check is done in the KNN and Nearest search, so there the radius is squared\nI double checked the unit test, but it passed, as I understand (some time earlier I got assertion fails  because of not getting sqrt finally, and now i'm not getting them)\n\nSorry for the lack of my github knowledge.. oh no, I have ruined everything. Maybe it is better to create a new branch and a new pull request, there is just one line of code that is changed. I myself use the highly optimized anti-library version with all basic types and inline convolution functions, because I need to range search for millions of objects, that's why I found this)\nThank you for maintaining the good ml library with rich possibilities!. but when the abs(diff) > 1, the original check might be wrong:\n\n. ",
    "kliu20": "Oops. Just found how to include plot package on your README file. Sorry ;). ",
    "tarlen5": "Ahh, thanks. I installed from source by cloning the repo, so I see them now in my shell/src/universal/data/weka directory. Thanks again!. ",
    "lolaclinton": "Mac\nSent via the Samsung Galaxy S\u00ae 6, an AT&T 4G LTE smartphone\n-------- Original message --------From: Haifeng Li notifications@github.com Date: 5/2/17  6:40 PM  (GMT-05:00) To: haifengl/smile smile@noreply.github.com Cc: lolaclinton oferm@cluemail.com, Author author@noreply.github.com Subject: Re: [haifengl/smile] saving an image (#179) \nWhat's your OS?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/haifengl/smile\",\"title\":\"haifengl/smile\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/haifengl/smile\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@haifengl in #179: What's your OS?\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/haifengl/smile/issues/179#issuecomment-298781384\"}}}. It's kind of strange as it is bringing up the open file menu not a save file one.\u00a0\nSent via the Samsung Galaxy S\u00ae 6, an AT&T 4G LTE smartphone\n-------- Original message --------From: Haifeng Li notifications@github.com Date: 5/2/17  7:09 PM  (GMT-05:00) To: haifengl/smile smile@noreply.github.com Cc: lolaclinton oferm@cluemail.com, Author author@noreply.github.com Subject: Re: [haifengl/smile] saving an image (#179) \nSwing doesn't work well on mac. The image saving facility works well on windows. I will see if there is any work around.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/haifengl/smile\",\"title\":\"haifengl/smile\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/haifengl/smile\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@haifengl in #179: Swing doesn't work well on mac. The image saving facility works well on windows. I will see if there is any work around.\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"https://github.com/haifengl/smile/issues/179#issuecomment-298786353\"}}}. Doesn't seem to work.\nOn 5/2/2017 7:49 PM, Haifeng Li wrote:\n\nWhen in the file dialogue, please try CMD + Shift + G to bring up a \nwindow to type the path. I find this on internet and have not tried it \nyet. Thanks!\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub \nhttps://github.com/haifengl/smile/issues/179#issuecomment-298792558, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AHeiCkvk1v31ZpbBD3K0OcH14ZTPucXXks5r18EWgaJpZM4NOvDc.\n\n\n. ",
    "AlexDBlack": "Thanks for looking into this. So, the plot thickens...\nHere's a stand-alone method that (when run in my project) returns the correct values:\npublic static void main(String[] args){\n        System.out.println(\"-- Standalone Class ---\");\n        double d = 2.0;\n        System.out.println(\"2.0 * Double.MIN_VALUE: \" + (d * Double.MIN_VALUE));\n        System.out.println(\"Is zero: \" + smile.math.Math.isZero(0.0));\n    }\n-- Standalone Class ---\n2.0 * Double.MIN_VALUE: 1.0E-323\nIs zero: true\nYet, running the exact same code as part of my training pipeline gives a different result:\n```\n    public static RidgeRegression ridgeRegression(MultiDataSet trainData, double lambda){\n        System.out.println(\"-- My Code ---\");\n        double d = 2.0;\n        System.out.println(\"2.0 * Double.MIN_VALUE: \" + (d * Double.MIN_VALUE));\n        System.out.println(\"Is zero: \" + smile.math.Math.isZero(0.0));\n    Pair<double[][], double[]> pair = mdsToDoublesRegression(trainData);\n    return new RidgeRegression(pair.getFirst(), pair.getSecond(), lambda);\n}\n\n```\n-- My Code ---\n2.0 * Double.MIN_VALUE: 0.0\nIs zero: false\nI'm not doing anything special in my code - same machine, same environment, standard hardware (Intel i7), not messing with anything unusual like strictfp, reflection or anything. Suffice to say, I'm pretty confused by this result - but it is repeatable.\nAt any rate, 2.0*Double.MIN_NORMAL gives me a positive number (4.450147717014403E-308) while 2.0*Double.MIN_VALUE does not.. ",
    "sitinurdiana": "Mr. Haifeng \ncan you help to tell me run your program about Support vector machine, because I am newbie and I am very confused to my research.\nthank you. thank you Mr. you are very helpful. ",
    "luciferlu": "Do you mean input data should preprocess, and exclude all this kind of data set? I am not sure how to distinguish the data not fit GMeans. For example:\n```\n    val data = Array(0.1d, 0.0d) :: (0 to 30)\n      .map(_ => Array(0.0d, 0.1d))\n      .toList\n    val gMeans = new GMeans(data.toArray, 100)\n```\nI know this is also not a good data, But it is harder to distinguish than the first example. This code will also run into the dead loop.. Actually, both data examples are real data. We partition our data into 30k groups, and I found most groups run well, but some groups run into the deal loop. So I tracked into each group and found first data set. I tried to limit iterations in GMeans and passed the first data set, but got an ArrayIndexOutOfBoundsException exception in another data group, which only have 50 pairs of zero. I understand these data are not good enough for this algorithm, but I have to run it against all data groups and I cannot ensure all data have good quality. I believe this is the hardest part of data processing.. Anyway, this library is really great and helped me a lot. I will try to workaround the issue. Thanks for your help.. ",
    "yiming-chen": "This is the layout of my cvs:\ncount_x domain  perc_x  count_y\n0   007safe.com 0   1\n1   012.net.il  0.006111386 13\nI am trying to load the data with:\nval data = read.csv(\"data/domain_stat.csv\", header=true)\nThis will return error message (it would work if I delete the \"domain\" column which is string):\njava.lang.NumberFormatException: For input string: \"007safe.com\"\n  at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n  at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n  at java.lang.Double.parseDouble(Double.java:538)\n  at java.lang.Double.valueOf(Double.java:502)\n  at smile.data.NumericAttribute.valueOf(NumericAttribute.java:62)\n  at smile.data.parser.DelimitedTextParser.parse(DelimitedTextParser.java:372)\n  at smile.data.parser.DelimitedTextParser.parse(DelimitedTextParser.java:259)\n  at smile.data.parser.DelimitedTextParser.parse(DelimitedTextParser.java:216)\n  at smile.data.parser.DelimitedTextParser.parse(DelimitedTextParser.java:196)\n  at smile.read$.table(package.scala:195)\n  at smile.read$.csv(package.scala:200). Thanks. I am trying to find more tutorial/sample codes for Scala API on this. Do you know where I could find them other than the \"3rd party tutorial\" link?. Thanks!. ",
    "dopicar": "I came here as well after encountering this error. While the culprit is quite easy to understand, the official documentation offers little explanation of the need to provide the file's schema (where? how?). \nAutomatic parsing of CSV (or at least suggestion for user adjudication) could be very helpful to facilitate work. \nThanks. ",
    "smutchler": "For example, if you are predicting setosa/virginica/versicolor in classification and ultimately predict setosa (with some posterior probability) for case/row #1... what variables were important in making that prediction. \u00a0For case 1, Sepal.Length may be the strongest predictor and for case 2, Sepal.Width may be the strongest predictor. \u00a0\nIn the RandomForest package in R, this is turned on by localImp = TRUE.\nI guess you would look at Gini decrease for each variable (turned on) for all trees but just for a single row of data. \u00a0I'm just guessing.\n      From: Haifeng Li notifications@github.com\n To: haifengl/smile smile@noreply.github.com \nCc: smutchler smutchler@yahoo.com; Author author@noreply.github.com\n Sent: Monday, June 5, 2017 7:44 PM\n Subject: Re: [haifengl/smile] Case-wise variable importance (#188)\nWhat do you mean case-wise importance? Thanks!\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.  \n. If you look at my output, I normalize the data upfront and use it both for training and scoring. \u00a0Something else I'm doing is wrong. \u00a0I'll try look at the unit tests.\nOne final note... It would be great if I could use MLP for regression also.\nSmile is great by the way! Thanks for releasing it.\nOn Thursday, June 15, 2017, 9:53:06 AM MDT, Haifeng Li notifications@github.com wrote:\nThe data should be normalized first before feeding to neural network. Check our unit tests. We use iris there too.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n  . ",
    "skx300": "Thank you @haifengl . By the way, is there any demo for storing the trained model? . ",
    "lorinying": "Thanks for the useful information provided by @haifengl and @skx300. But I have another question. Is there any demo or api to store the trained model to buffer in memory or serialized to string and then put the data stream in memory  into mongodb dataset?. ",
    "defoye": "That's wonderful!  I asked because it is a project I have not completely finished(doesn't derive and integrate all expressions yet), but would be glad to develop into a finished product if it serves an awesome project like smile.\nIt is a relatively simple system, and I think my code is fairly understandable and adaptable.  Basically, it parses expressions, converts them into a tree with the shunting yard algorithm and applies differentiation rules at each level.  It can easily be expanded to include integration.\n. It reads a string and outputs a string.  It uses differentiation rules(product rule, quotient rule, chain rule)\n. Great.  I'll check out the rest of your project to get a feel for your style.  I can add \"headers\" like you have in some of your classes.  I can take care of all of them if you would like.. I started to rewrite that test class.  Yes, we can merge them.  The tests fail because I stopped developing the method for rewriting expressions.  I came up with a better way, a way in which we would have \"rewrite rules,\" and check the expression tree for these rewrite patterns.  This sounds like a good plan to me.  . we can also use the rewrite rules to expand expressions, should they need to be for computation or expression rewriting\n. always glad to learn & contribute!.  If you look at the system and decide that it doesn't fit with the rest of the project, I would be happy to change it, or even start over a different way(scala sounds interesting).  I am very interested in Smile. . Hi Haifeng, cool, sounds like a good plan.\nSure, I can remove the copyright, not a problem at all.\nI am very busy and will only be busier come this fall, but I'm still here!  . ",
    "jcrotinger": "Thanks!. ",
    "redsnow1992": "it is make sense. ",
    "SyCode7": "Hi, is this already added ? one-class svm\n. ",
    "pvillacorta-stratio": "Thanks so much. Tried this before my vacation :-) \nI noticed some change (it is actually faster!) but unfortunately it did not fix the problem. Is there anything else related to the parallel processing that can/should be disabled?\nThanks again. This is my stack trace (Smile version 1.3.1 but I am getting the same error with 1.4.0):\njava.lang.ArrayIndexOutOfBoundsException: 4122\n    at smile.classification.DecisionTree$TrainNode.findBestSplit(DecisionTree.java:599)\n    at smile.classification.DecisionTree$TrainNode.findBestSplit(DecisionTree.java:531)\n    at smile.classification.DecisionTree$TrainNode.split(DecisionTree.java:717)\n    at smile.classification.DecisionTree.(DecisionTree.java:987)\n    at smile.classification.DecisionTree.(DecisionTree.java:845)\n    at smile.classification.Operators$$anonfun$cart$1.apply(Operators.scala:585)\n    at smile.classification.Operators$$anonfun$cart$1.apply(Operators.scala:585)\n    at smile.util.package$time$.apply(package.scala:49)\n    at smile.classification.Operators$class.cart(Operators.scala:584)\n    at smile.classification.package$.cart(package.scala:130)\nI am calling it like this:\nsmile.classification.cart(fold1selectedFeatures, fold1labels, fold1data.length / 4, selectedAttributes)\nwhere selectedAttributes is an Array[Attribute] in which every element has been created like NumericAttribute(s\"v$attIndex\") with increasing attIndex numbers (no repeated). The most strange thing, as I said, is that running this outside of Spark does not yield this error. The dataset used (fold1selectedFeatures together with the corresponding element from fold1labels at the end of each line) is attached (gzipped csv,  522 examples, 973 features plus the label column) \nerror_fold_data.txt.gz\nThanks. Any suggestion?. ",
    "fommil": "Just look it the pom you're depending on and only pick the bits you need. You probably don't want the raspberry pi binaries either.. assuming they are the latest, that is the kind of thing that would be good. You might even want to drop 32 bit linux. Make sure you also have\n\"com.github.fommil.netlib\" % \"core\" % \"1.1.2\"\n\"com.github.fommil\" % \"jniloader\" % \"1.1\"\nthe JVM fallback logic and JNI loader, respectively.\n. Did you not watch the conference talk referenced from the README? I think you should as it is highly relevant for what you're doing here.. Hmm, I would expect that to work.. ",
    "hguhlich": "Thanks! I added Case 2 and 3 just to show that in these cases the first sentence is not deleted.. ",
    "dhhyuk": "double[] x1 = {0, 1, 2, 3};\n        double[] x2 = {0, 1, 2, 3};\n        double[][] y = {\n                {1, 2, 4, 1},\n                {6, 3, 5, 2},\n                {4, 2, 1, 5},\n                {5, 4, 2, 3}\n        };\n    PlotCanvas canvas = Heatmap.plot(y, Palette.jet(256));\n    canvas.setTitle(\"Original\");\n    add(canvas);\n\n    BicubicInterpolation bicubic = new BicubicInterpolation(x1, x2, y);\n    double[][] yy = new double[101][101];\n    for (int i = 0; i <= 100; i++)\n        for (int j = 0; j <= 100; j++)\n            yy[i][j] = bicubic.interpolate(i*0.03, j*0.03);\n\n    canvas = Heatmap.plot(yy, Palette.jet(256));\n    canvas.setTitle(\"Bicubic\");\n    add(canvas);\n\nIt's your demo code.\nTo change a 4x4 cell to a 6x6 cell.\ni changed \ndouble[] x1 = {0, 1, 2, 3, 4, 5};\ndouble[] x2 = {0, 1, 2, 3, 4, 5};\ndouble[][] y = {\n                {1, 2, 4, 1, 5, 6},\n                {6, 3, 5, 2, 2, 4},\n                {4, 2, 1, 5, 1, 1},\n                {5, 4, 2, 3, 5, 2},\n                {4, 2, 1, 5, 1, 1},\n                {1, 2, 4, 1, 5, 6}\n        };\nbut there is no change.. Oh, i changed 0.03 to 0.06, that's working.\nfor (int i = 0; i <= 100; i++)\n        for (int j = 0; j <= 100; j++)\n            yy[i][j] = bicubic.interpolate(i0.03, j0.03);. Thank you so much.. if changed y array value changed,\nshould make new interpolation instance?\nits using so much cpu... In your experience, is there a good algorithm for expressing pressure?. Thank you so much \ud83d\udc4d . ",
    "anuj1207": "Thanks, sir! I understand the camel casing but I was hoping you would use the functional style for scala API. And as for null, I was hoping you would want to work on creating Scala classes for Decision Tree and others in future.\nThank you. @haifengl So this issue is not needed?? Are we moving ahead on this one or should I close it?. ",
    "opaetzel": "I call it this way, the featureArr and labelArr are the linked json data as arrays:\nCRF classifier = trainer.train(featureArr, labelArr);. you could download the files linked above and use gson to deserialize them: \njava\ndouble[][][] featureArr = null;\ntry (InputStream is = Files.newInputStream(Paths.get(\"/path/to/features.json.gz\")); GZIPInputStream gis =\n                            new GZIPInputStream(is); Reader jsonReader = new InputStreamReader(gis)) {\n    featureArr = new Gson().fromJson(jsonReader, double[][][].class);\n }\nint[][] labelArr = null;\ntry (InputStream is = Files.newInputStream(Paths.get(\"/path/to/labels.json.gz\")); GZIPInputStream gis =\n                            new GZIPInputStream(is); Reader jsonReader = new InputStreamReader(gis)) {\n    labelArr = new Gson().fromJson(jsonReader, int[][].class);\n }\nI directly extract the variables from a larger dataset before, if you are interested in that, I could give you that code, too, but it's quite a lot.. the features are normalized in [-1,1]. That could be the problem, if it has to be in [0,5]. Would normalizing to [0,1] help?. Ah okay I am using it wrong then. I am feeding it this data (extract):\njson\n[\n    [\n        [\n            0.9502296043498872,\n            0.04901466986045237,\n            0.0,\n            0.034034610957892716,\n            1.0,\n            0.013876651982378854\n        ]\n    ],\n    [\n        [\n            0.4854410007187256,\n            0.4864551706736476,\n            0.0,\n            0.017287421438929636,\n            0.0,\n            0.007048458149779736\n        ],\n        [\n            0.24547584143671364,\n            0.24372600251989104,\n            0.08628030791852799,\n            0.043218553597324086,\n            0.0,\n            0.024669603524229075\n        ],\n        [\n            0.39752057179598427,\n            0.3935003660316032,\n            0.03328320483504048,\n            0.01998858103876239,\n            0.0,\n            0.02378854625550661\n        ],\n        [\n            0.4889477984535358,\n            0.48720279644824843,\n            0.0,\n            0.012425334159230675,\n            0.0,\n            0.021145374449339206\n        ],\n        [\n            0.23971467372952548,\n            0.23724657914001662,\n            0.037891648581430695,\n            0.033021676107955436,\n            0.0,\n            0.0026431718061674008\n        ],\nSo it is not sparse at all and the features are actual double values. But good to know how to use it correctly, for other projects. Do you know of a sequence tagging algorithm that can handle this data?. ",
    "JohnKoumarelas": "On a similar matter, is it safe to do this grid search (specifically I want to do it for SVMs) in parallel? Can we assume this for all/some classifiers? I am trying to use parallelStream (Java 8+) for this reason.. ",
    "alanmarazzi": "I am working to Clojure wrappers for SMILE (I just started) and this functionality might be somewhat easier to implement with Clojure. Let me know if this is interesting for you. I have yet to push it, I'm in a very early stage, I wanted to get a better feeling of smile before getting seriously to it.\nAnyway, it would be very cool to have Clojure wrappers here as well! If you want to have a better feeling how these things look like you can take a look at the wrappers I released for XGBoost https://gitlab.com/alanmarazzi/clj-boost\nI might be able to push something in a couple of weeks (a very partial coverage considering the size of smile), I'll let you know something here as soon as possible.. ",
    "mgiannini": "That makes sense. Thanks for the quick feedback.. I submitted a pull request that will fix this #225. I am using the SVM class behind another api which allows users to train models one instance at a time. So it is not practical in my use case to have all the training data available at once to initially learn the model.. Thanks for fixing!. ",
    "yeahia2508": "Its a csv file. if I delete delimiter then ParseException: 1column,eexcepted 3 occurring.  Only NumericAttributeFeature giving 0 attribute. AttributeDataset giving 3 attribute.. yes. NumericAttributeFeature is wrong. all time giving 0 attribute. I want to scale dataset to fit in SVR. I want to scale all column. Here is my  data file. Thanks.\nPosition_Salaries.csv.zip\n. numericAttributeFeature.attributes() giving 0, not column. \nthis line line causing problem.\nAttribute[] att = numericAttributeFeature.attributes(); //att.length = 0\n. my depedent data is numeric (Salary). How to scale this. it is double[] array. but NumericAttribute only receive double[][] data. please give just a little example. Thank you very much.. Thank you very much.. ",
    "Malkitti": "I clone  the library and  executing  the Java code one by one.. ",
    "andrewcz": "Hi!\ncheers for the reply.\nso i have tried to run SmileDemo.java and got the following in the intellij ide.\nWould you be able to advise next steps?\nMany thanks for your reply.\nBest,\nAndrew\nInformation:Using javac 1.8.0_91 to compile java sources\nInformation:java: Some messages have been simplified; recompile with -Xdiags:verbose to get full output\nInformation:java: Errors occurred while compiling module 'demo'\nInformation:9/17/17 9:49 PM - Compilation completed with 100 errors and 0 warnings in 5s 844ms\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/stat/distribution/GeometricDistributionDemo.java\nError:(33, 18) java: package smile.plot does not exist\nError:(34, 18) java: package smile.plot does not exist\nError:(35, 18) java: package smile.plot does not exist\nError:(36, 31) java: package smile.stat.distribution does not exist\nError:(46, 13) java: cannot find symbol\n  symbol:   class PlotCanvas\n  location: class smile.demo.stat.distribution.GeometricDistributionDemo\nError:(47, 13) java: cannot find symbol\n  symbol:   class PlotCanvas\n  location: class smile.demo.stat.distribution.GeometricDistributionDemo\nError:(48, 13) java: cannot find symbol\n  symbol:   class PlotCanvas\n  location: class smile.demo.stat.distribution.GeometricDistributionDemo\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/wavelet/CoifletWaveletDemo.java\nError:(25, 18) java: package smile.plot does not exist\nError:(26, 18) java: package smile.plot does not exist\nError:(27, 21) java: package smile.wavelet does not exist\nError:(28, 21) java: package smile.wavelet does not exist\nError:(29, 21) java: package smile.wavelet does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/wavelet/SymletWaveletDemo.java\nError:(25, 18) java: package smile.plot does not exist\nError:(26, 18) java: package smile.plot does not exist\nError:(27, 21) java: package smile.wavelet does not exist\nError:(28, 21) java: package smile.wavelet does not exist\nError:(29, 21) java: package smile.wavelet does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/classification/RDADemo.java\nError:(26, 28) java: package smile.classification does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/classification/ClassificationDemo.java\nError:(33, 18) java: package smile.data does not exist\nError:(34, 18) java: package smile.data does not exist\nError:(35, 25) java: package smile.data.parser does not exist\nError:(36, 18) java: package smile.plot does not exist\nError:(37, 18) java: package smile.plot does not exist\nError:(38, 18) java: package smile.plot does not exist\nError:(39, 18) java: package smile.plot does not exist\nError:(54, 12) java: cannot find symbol\n  symbol:   class AttributeDataset\n  location: class smile.demo.classification.ClassificationDemo\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/plot/GridDemo.java\nError:(24, 18) java: package smile.plot does not exist\nError:(25, 18) java: package smile.plot does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/classification/LDADemo.java\nError:(23, 28) java: package smile.classification does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/wavelet/HaarWaveletDemo.java\nError:(25, 18) java: package smile.plot does not exist\nError:(26, 18) java: package smile.plot does not exist\nError:(27, 21) java: package smile.wavelet does not exist\nError:(28, 21) java: package smile.wavelet does not exist\nError:(29, 21) java: package smile.wavelet does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/plot/LinePlotDemo.java\nError:(25, 18) java: package smile.plot does not exist\nError:(26, 18) java: package smile.plot does not exist\nError:(27, 18) java: package smile.plot does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/manifold/ManifoldDemo.java\nError:(32, 18) java: package smile.data does not exist\nError:(33, 25) java: package smile.data.parser does not exist\nError:(49, 12) java: cannot find symbol\n  symbol:   class AttributeDataset\n  location: class smile.demo.manifold.ManifoldDemo\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/classification/KNNDemo.java\nError:(26, 28) java: package smile.classification does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/plot/StaircasePlotDemo.java\nError:(24, 18) java: package smile.plot does not exist\nError:(25, 18) java: package smile.plot does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/projection/PCADemo.java\nError:(28, 18) java: package smile.plot does not exist\nError:(29, 18) java: package smile.plot does not exist\nError:(30, 24) java: package smile.projection does not exist\nError:(31, 18) java: package smile.math does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/projection/ProjectionDemo.java\nError:(31, 18) java: package smile.data does not exist\nError:(32, 18) java: package smile.data does not exist\nError:(33, 25) java: package smile.data.parser does not exist\nError:(51, 22) java: cannot find symbol\n  symbol:   class AttributeDataset\n  location: class smile.demo.projection.ProjectionDemo\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/stat/distribution/GaussianMixtureDemo.java\nError:(25, 18) java: package smile.math does not exist\nError:(26, 18) java: package smile.plot does not exist\nError:(27, 18) java: package smile.plot does not exist\nError:(28, 18) java: package smile.plot does not exist\nError:(29, 31) java: package smile.stat.distribution does not exist\nError:(30, 31) java: package smile.stat.distribution does not exist\nError:(31, 31) java: package smile.stat.distribution does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/plot/ScatterPlotDemo.java\nError:(25, 18) java: package smile.plot does not exist\nError:(26, 18) java: package smile.plot does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/interpolation/LaplaceInterpolationDemo.java\nError:(25, 27) java: package smile.interpolation does not exist\nError:(26, 27) java: package smile.interpolation does not exist\nError:(27, 18) java: package smile.plot does not exist\nError:(28, 18) java: package smile.plot does not exist\nError:(29, 18) java: package smile.plot does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/clustering/MECDemo.java\nError:(27, 18) java: package smile.plot does not exist\nError:(28, 18) java: package smile.plot does not exist\nError:(29, 24) java: package smile.clustering does not exist\nError:(30, 27) java: package smile.math.distance does not exist\nError:(31, 18) java: package smile.plot does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/clustering/ClusteringDemo.java\nError:(34, 18) java: package smile.data does not exist\nError:(35, 25) java: package smile.data.parser does not exist\nError:(36, 18) java: package smile.plot does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/stat/distribution/EmpiricalDistributionDemo.java\nError:(25, 18) java: package smile.plot does not exist\nError:(26, 18) java: package smile.plot does not exist\nError:(27, 18) java: package smile.plot does not exist\nError:(28, 31) java: package smile.stat.distribution does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/stat/distribution/MultivariateGaussianDistributionDemo.java\nError:(30, 18) java: package smile.plot does not exist\nError:(31, 18) java: package smile.math does not exist\nError:(32, 18) java: package smile.plot does not exist\nError:(33, 31) java: package smile.stat.distribution does not exist\nError:(42, 13) java: cannot find symbol\n  symbol:   class PlotCanvas\n  location: class smile.demo.stat.distribution.MultivariateGaussianDistributionDemo\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/stat/distribution/ChiSquareDistributionDemo.java\nError:(33, 18) java: package smile.plot does not exist\nError:(34, 18) java: package smile.plot does not exist\nError:(35, 18) java: package smile.plot does not exist\nError:(36, 18) java: package smile.plot does not exist\nError:(37, 18) java: package smile.plot does not exist\nError:(38, 31) java: package smile.stat.distribution does not exist\nError:(48, 13) java: cannot find symbol\n  symbol:   class PlotCanvas\n  location: class smile.demo.stat.distribution.ChiSquareDistributionDemo\nError:(49, 13) java: cannot find symbol\n  symbol:   class PlotCanvas\n  location: class smile.demo.stat.distribution.ChiSquareDistributionDemo\nError:(50, 13) java: cannot find symbol\n  symbol:   class PlotCanvas\n  location: class smile.demo.stat.distribution.ChiSquareDistributionDemo\nError:(51, 13) java: cannot find symbol\n  symbol:   class PlotCanvas\n  location: class smile.demo.stat.distribution.ChiSquareDistributionDemo\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/clustering/NeuralGasDemo.java\nError:(24, 16) java: package smile.vq does not exist\nError:(25, 18) java: package smile.plot does not exist\nError:(26, 18) java: package smile.plot does not exist\nError:(27, 18) java: package smile.plot does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/classification/NeuralNetworkDemo.java\nError:(26, 28) java: package smile.classification does not exist\nError:(27, 18) java: package smile.math does not exist\n/home/andrewcz/Desktop/Java/smile/demo/src/main/java/smile/demo/projection/PPCADemo.java\nError:(25, 18) java: package smile.plot does not exist\nError:(26, 18) java: package smile.plot does not exist. ",
    "harry-2016": "Does DBSCAN works well for large volume of data. say some 30k rows with 6-8 columns.\nHow long will it take to complete.? . nope getting out of memory for even 16gb ..\nany help is appreciated..\nthanks in advance\nOn Mon, 12 Mar 2018 at 7:40 PM, Haifeng Li notifications@github.com wrote:\n\n30k is not big. With 6-8 columns, you can use KDTree for nearest neighbor\nsearch.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/232#issuecomment-372322292, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AP4JG3m5QXhwM_KzOrl9ZzkbI-3U9rPVks5tdoG6gaJpZM4PjlZa\n.\n. For 20k how much i have to give as heap size. Because 10GB is not enough\nfor 20k rows.\nFor 10k rows its working perfectly fine.\n\nIs there any way to optimize it.?. Yeah i tried for Chameleon/t7.10k data. \nBut for my data(which will be around 40k and 5 dimension).\nIs there any way that i can use smile DBScan for my data.?. dbscan(data, 300, 1000)\nAlso i have tested with my dataset around 20k rows and reduced my dimension to 2, again getting more time to run the model.. \nAny insights on this.?\n. I have to reduce my dimensionality or by using any other clustering algorithms like kmeans, is the only solution.? \n. Data will be raw array.\nyeah i tried with  min ponits to 20 and epsilon from 100 to 1000.\n\n. okay thanks..\nAs of now will pass data as KDTree.\nWill share sample data soon.\n. \n",
    "asmirn1": "I see your point. However, a clustering result with only one cluster isn't invalid in my opinion, while the code is raising an exception in this case and I have to additionally process it.\nBut if it's an intentional behavior then it's fine.\n. Not sure, what do you mean by \"discrete values of layers.\" Also, if I use complete linkage, what would be the minimum and maximum valid h-parameter? . Well, I'm trying to cluster hundreds of small datasets, so plotting the tree for each one of them is not an option. Moreover, users that may use my program would have no idea about the hierarchical clustering. \nI understand that it is hard to find min and max since it depends on the linkage and the data. That's why I prefer any value from interval (0, +infinity) to be valid for h-parameter.\nBut, it's just my opinion. I'm catching the exception now, and it seems to work fine for now.. Because I don't know the number of clusters beforehand. I may have only one cluster or many clusters for different datasets.. Thank you for the suggestions! I might try them. I chose hierarchical clustering because I can approximately estimate the diameter of each cluster and this diameter stays the same for all datasets. That's why I use the hierarchical clustering with the complete linkage.. ",
    "vagmcs": "OK, but I think it would be very useful to have some extension for these measures in the multi-class case. Otherwise each user should write its own function for each measure using the confusion matrix.. ",
    "SonnyBao": "Thanks for your explanation. Is there any chance to support this feature for those algorithms in near future?\nThanks.. Thank you all.. @divyansh1329: I followed this issue here to save/load the trained model: https://github.com/haifengl/smile/issues/135\nBasically, when loading using Java's ObjectInputStream, you can directly cast the loaded object to the model type that you trained before.. ",
    "csalperwyck": "If you are interested in such tools, you can have a look at MOA:\nhttps://moa.cms.waikato.ac.nz/\nThere is an online tree such as Hoeffding Tree, but it needs lots of data to have good performances.. ",
    "divyansh1329": "How to Save/Load trained models as @SonnyBao  mentioned in the question ? I want to train the model once and use it for predicting multiple times later.. ",
    "SaiprasadKrishnamurthy": "Hi, I'm trying to serialize the RandomForest class and I'm able to do so successfully. I'm intending to use this instance to call predict multiple times. I'd like to understand how can I create the input values for double[] x. Should I also save the original training data set along with the model?\nAny pointers?. ",
    "serickso": "Haifeng, I fixed everything that you have requested. Does this look ready to be merged?. I think this will be easier to use if we place it in a separate class entirely, called RLS, short for recursive least squares. This has the following benefits:\n1. Users who actually need to on online model for linear regression won't get confused.\n2. Users who only want to use OLS won't get confused.\n3. Having separate classes will make maintenance easier for developers.. Ok, I have added the RLS and RLSTest.. It changes because of the update to V. First we update V, then use the updated V to update w. Instead of worrying about resetting the memory \n(note that memory is h_{t-1} in h_t = sigma(Wxh x Whh h_{t-1})), \nI think it might just be easier to use the batch mode for the RNN instead. Then implement a learn method that takes double[][] x and double[] y as input, and reset the h_{t-1} variable to 0s every time learn is called. \nMy reasoning is that this will make time series forecasting easier for users, so that if a user learns some sequence of points (X1, y1), (X2, y2), . . ., (Xn, yn), then they can predict Xn+1 through Xn+k.\n. I think it is better to convert RNN as an abstract base class. This would ensure proper behavior in RNNs that don't have issues with vanishing or exploding gradients, such as LSTM or GRU. To get started with a more comman RNN right away, I could just convert the existing RNN to an abstract base class and implement an Elman network.. I'm curious if you get an exception when you use the DecisionTree class?. @Ajk4 could you provide some references for your pull request?. If you want repeatable results, try calling smile.math.Math.setSeed in addition to using -Dsmile.threads=1 before training your model. See https://haifengl.github.io/smile/faq.html#seed for more details. . . let us know if this works or not so we can close this issue.. @ishaan007 try setting the parameter for the number of threads with System.setProperty(\"smile.threads\", 1) before you train your random forest.. I did it this way to get gammaXX^Tgamma, for the XX^T part. A more efficient way could be:\ndouble[] gammax=gamma.ax(x)\ndouble[] xtGamma = gamma.atx(x)\n// compute update to gamma\n. I used 0.01 because I was worried about 1/s^2 becoming very large, and potentially causing the elments in the gamma matrix becoming large and unstable (after repeated matrix multiplications on gamma).\nI'm going to have to look at this in more depth to find some more information about this potential issue issue.. Haifeng, I'm not seeing how to get gammaXX^Tgamma from gamma.ax(x) and the norm of gamma.ax(x). Could write the equation down for me?. ",
    "CBrophy": "No I didn't. I will try that, thanks!. I've confirmed that this issue does not happen with netlib, so pure-java problem only. Line 360: I used a debugger to figure out where the decision to short-circuit k was happening and traced it back to the zero-out logic.. I'll add that I'm on OpenJDK 1.8.0.151, Fedora 26 64. Until you suggested it, I did not have BLAS installed, so the implementation fell back to a JMatrix.. Yes, that makes sense. I figured that a better exception was probably the real solution. Thanks for looking into it.. Ah, ok. Thanks again for your help.. ",
    "sudarsun": "I am able to consistently reproduce the error every time.  The error is definitely not random.\nWhat is the recommended BLAS package version number?\nWhere should I look into to get more useful information for you to debug further?. Hmm.  When checked the version of blas and lapack, I could find that the version is 3.6.0-2ubuntu2\nsudar@Imhotep:~$ sudo apt install libblas-dev liblapack-dev\nReading package lists... Done\nBuilding dependency tree     \nReading state information... Done\nlibblas-dev is already the newest version (3.6.0-2ubuntu2).\nliblapack-dev is already the newest version (3.6.0-2ubuntu2).\n0 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\nIs there a place where I should check next to resolve the problem?\n. Yes Sir.  I have tried it already on a different machine.\nsudar@optimus:~/smile$ ./smile.sh \n[info] Loading project definition from /home/sudar/smile/project\n[info] Set current project to smile (in build file:/home/sudar/smile/)\n[info] Wrote /home/sudar/smile/shell/target/scala-2.12/smile-shell_2.12-1.5.0-RC2.pom\n[info] Main Scala API documentation to /home/sudar/smile/shell/target/scala-2.12/api...\n[info] Compiling 1 Scala source to /home/sudar/smile/shell/target/scala-2.12/classes...\nmodel contains 13 documentable templates\n[info] Packaging /home/sudar/smile/shell/target/scala-2.12/smile-shell_2.12-1.5.0-RC2.jar ...\n[info] Done packaging.\n[info] Main Scala API documentation successful.\n[info] Packaging /home/sudar/smile/shell/target/scala-2.12/smile-shell_2.12-1.5.0-RC2-javadoc.jar ...\n[info] Done packaging.\n[info] Wrote /home/sudar/smile/math/target/smile-math-1.5.0-RC2.pom\n[info] Wrote /home/sudar/smile/data/target/smile-data-1.5.0-RC2.pom\n[info] Wrote /home/sudar/smile/graph/target/smile-graph-1.5.0-RC2.pom\n[info] Wrote /home/sudar/smile/netlib/target/smile-netlib-1.5.0-RC2.pom\n[info] Wrote /home/sudar/smile/core/target/smile-core-1.5.0-RC2.pom\n[info] Wrote /home/sudar/smile/interpolation/target/smile-interpolation-1.5.0-RC2.pom\n[info] Wrote /home/sudar/smile/nlp/target/smile-nlp-1.5.0-RC2.pom\n[info] Wrote /home/sudar/smile/plot/target/smile-plot-1.5.0-RC2.pom\n[info] Wrote /home/sudar/smile/scala/target/scala-2.12/smile-scala_2.12-1.5.0-RC2.pom\n[info] Wrote /home/sudar/smile/benchmark/target/scala-2.12/smile-benchmark_2.12-1.5.0-RC2.pom\n[info] Wrote /home/sudar/smile/demo/target/smile-demo-1.5.0-RC2.pom\n[success] Total time: 8 s, completed 3 Nov, 2017 8:56:17 PM\nCompiling (synthetic)/ammonite/predef/interpBridge.sc\nCompiling (synthetic)/ammonite/predef/replBridge.sc\nCompiling (synthetic)/ammonite/predef/DefaultPredef.sc\nCompiling (synthetic)/ammonite/predef/CodePredef.sc\nWelcome to Smile Shell; enter 'help' for the list of commands.\nType \"exit\" to leave the Smile Shell\nVersion 1.5.0-RC2, Scala 2.12.3, SBT 0.13.16, Built at 2017-11-03 15:26:08.605\n===============================================================================\nsmile> val a = randn(100,100) \na: DenseMatrix =  -0.2015    0.0481    0.0432   -0.6100   -1.0286   -0.7054    0.1032  ...\n  0.9704    0.3677   -0.0011    1.5735   -0.7982    1.1519   -0.3758  ...\n  2.7269   -0.7253   -0.2067   -0.8212    1.8176   -0.7403   -0.1626  ...\n -0.1460   -1.6094    1.0685   -0.7500   -2.9544    0.9874   -0.7922  ...\n  1.8722    0.6541   -1.5475    1.7311    1.5635    1.2492   -0.6935  ...\n  0.4959    0.6218    1.8119    2.1485    0.0260   -0.2688    0.0886  ...\n  0.5539   -1.8473    0.8755   -0.2812    1.3386   -0.3521    0.3232  ...\n  ...\nsmile> inv(a) \nNov 03, 2017 8:56:47 PM com.github.fommil.jni.JniLoader liberalLoad\nINFO: successfully loaded /tmp/jniloader308925819341196954netlib-native_system-linux-x86_64.so\n./smile.sh: line 11: 29624 Segmentation fault      (core dumped) bin/smile\nsudar@optimus:~/smile$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.3 LTS\nRelease:    16.04\nCodename:   xenial\nsudar@optimus:~/smile$ lscpu\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                40\nOn-line CPU(s) list:   0-39\nThread(s) per core:    2\nCore(s) per socket:    10\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 79\nModel name:            Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz\nStepping:              1\nCPU MHz:               1200.117\nCPU max MHz:           3100.0000\nCPU min MHz:           1200.0000\nBogoMIPS:              4391.67\nVirtualization:        VT-x\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              25600K\nNUMA node0 CPU(s):     0-9,20-29\nNUMA node1 CPU(s):     10-19,30-39\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts\n. The error is reproducible in JDK 1.9 Oracle, JDK 1.8 Oracle & OpenJDK, JDK 1.7 Oracle.   Is there a way to debug the issue in some form?  You'd mentioned that you could reproduce this in mac, windows, and RHEL.  Is there are solution coming for those in the next versions?. Oh! I will do that then.\nBy the way, I am getting the bug if I create a Scala project using the scala_smile jar as well.  The error was originally happening only the \"smile\" shell.  While single stepping, the crash happens when LU decomposition is done on the matrix.  I am using scala 2.12.4 on JDK 1.8.  \nCan you please mention the environment where you tested it?  I shall change my environment to that and check if I am getting the errors.  If so, we can remove \"environment\" reason from the list of reasons.\n. Upon analysis, the LAPACK function dgetrf() is throwing SIGSEGV.   dgetrf() is the function called within LAPACK when inv() or lu() methods are called.  \nI tried plain qr(a:DenseMatrix), the LAPACK function dgeqrf() seems to work fine.\nLikewise for eig(a:DenseMatrix), the LAPACK function dgeev() seems to work fine.\nSo, I don't think we have a problem with LAPACK integration through smile-netlib & com.github.fommil.netlib.\nPlease also find the BLAS,LAPACK version from the following \"ldd\" dump.\nsudar@Imhotep:/projects/smile$ ldd  /tmp/jniloader5694017120184178737netlib-native_system-linux-x86_64.so\n        linux-vdso.so.1 =>  (0x00007ffdee175000)\n        libgfortran.so.3 => /usr/lib/x86_64-linux-gnu/libgfortran.so.3 (0x00007fdf59ecc000)\n        libblas.so.3 => /usr/lib/libblas.so.3 (0x00007fdf59c6b000)\n        liblapack.so.3 => /usr/lib/liblapack.so.3 (0x00007fdf59488000)\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fdf590be000)\n        libquadmath.so.0 => /usr/lib/x86_64-linux-gnu/libquadmath.so.0 (0x00007fdf58e7f000)\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fdf58b76000)\n        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fdf58960000)\n        libopenblas.so.0 => /usr/lib/libopenblas.so.0 (0x00007fdf568cc000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007fdf5a587000)\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fdf566af000)\n. Oh genius!\nAfter removing libopenblas, inv() works great!  Thanks very much Haifengl.  You made my day.\nsudar@Imhotep:~$ ldd /tmp/jniloader3856932465116552375netlib-native_system-linux-x86_64.so \n        linux-vdso.so.1 =>  (0x00007ffde2fcb000)\n        libgfortran.so.3 => /usr/lib/x86_64-linux-gnu/libgfortran.so.3 (0x00007f806feaf000)\n        libblas.so.3 => /usr/lib/libblas.so.3 (0x00007f806fc71000)\n        liblapack.so.3 => /usr/lib/liblapack.so.3 (0x00007f806f48d000)\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f806f0c3000)\n        libquadmath.so.0 => /usr/lib/x86_64-linux-gnu/libquadmath.so.0 (0x00007f806ee84000)\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f806eb7b000)\n        libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f806e965000)\n        libatlas.so.3 => /usr/lib/libatlas.so.3 (0x00007f806e3c7000)\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f806e1aa000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007f807056a000)\nsmile> val a = randn(5000,5000) \na: DenseMatrix =  -0.6220   -0.0783   -0.8023   -0.3890   -0.5245    0.2128   -0.5224  ...\n -1.6479   -1.5208   -0.8594    0.3664    0.9871    0.6149   -0.1476  ...\n  0.8378   -1.1239   -1.4638    0.3877   -0.2612    1.6644    0.1382  ...\n  0.3510    1.3740    0.0927    1.7777    0.3945   -0.9368   -0.7576  ...\n -2.2577   -0.3411    0.4708    0.9353    0.4971   -0.5266   -0.8725  ...\n -1.0600   -0.0399    0.7752   -0.0318   -0.5856    0.0688    0.2655  ...\n -0.6944   -1.8710    0.2559    1.6812   -0.7443   -2.3982   -0.4802  ...\n  ...\nsmile> inv(a) \nres3: DenseMatrix =   0.0045    0.0216    0.2812    0.2703    0.1335   -0.0393   -0.1738  ...\n  0.0162   -0.0113   -0.1276   -0.1123   -0.0600    0.0148    0.0721  ...\n -0.0246   -0.0067   -0.1294   -0.1178   -0.0681    0.0025    0.0673  ...\n -0.0030   -0.0208   -0.2531   -0.2495   -0.1152    0.0466    0.1596  ...\n -0.0198   -0.0255   -0.4126   -0.3651   -0.1945    0.0253    0.2417  ...\n  0.0144   -0.0246   -0.2498   -0.2304   -0.1156    0.0385    0.1543  ...\n -0.0215    0.0034    0.1684    0.1285    0.0888   -0.0090   -0.1044  ...\n  ...\nThanks very much.. The exact problem is the following:  The OpenBlas 0.2.18 package is broken at dgetrf() LAPACK function when the OPENBLAS_NUM_THREADS > 1.  When the thread count set to 1, openblas works fine for evaluating matrix inverse.   The same LAPACK function from libblas and libatlas are working fine, but painfully slower compared to libopenblas's single thread version.\n. The code that are used to benchmark\nval a = randn(2000,2000)\nval b = (a.t %*% a).toMatrix\nThis operation took ~500mS with openblas single thread mode \nsudar@Imhotep:/tmp $ update-alternatives --config libblas.so.3\nThere are 3 choices for the alternative libblas.so.3 (providing /usr/lib/libblas.so.3).\nSelection    Path                                    Priority   Status\n\n0            /usr/lib/openblas-base/libblas.so.3      40        auto mode\n  1            /usr/lib/atlas-base/atlas/libblas.so.3   35        manual mode\n  2            /usr/lib/libblas/libblas.so.3            10        manual mode\n  3            /usr/lib/openblas-base/libblas.so.3      40        manual mode\n\nPress  to keep the current choice[*], or type selection number: 2\nupdate-alternatives: using /usr/lib/libblas/libblas.so.3 to provide /usr/lib/libblas.so.3 (libblas.so.3) in manual mode\nsudar@Imhotep:/tmp $ update-alternatives --config liblapack.so.3\nThere are 3 choices for the alternative liblapack.so.3 (providing /usr/lib/liblapack.so.3).\nSelection    Path                                      Priority   Status\n\n0            /usr/lib/openblas-base/liblapack.so.3      40        auto mode\n  1            /usr/lib/atlas-base/atlas/liblapack.so.3   35        manual mode\n  2            /usr/lib/lapack/liblapack.so.3             10        manual mode\n  3            /usr/lib/openblas-base/liblapack.so.3      40        manual mode\n\nPress  to keep the current choice[*], or type selection number: 2\nupdate-alternatives: using /usr/lib/lapack/liblapack.so.3 to provide /usr/lib/liblapack.so.3 (liblapack.so.3) in manual mode\nAfter changing to libblas, the same operation took ~24986mS\nThe speed up is over 50 times!!\n. ",
    "avanco": "consider for example, i have a model which could map an instance in three possible values [0, 1, 2]\nwhile testing this model it is possible this scenario:\ntruth = {1, 0, 1, 0}\nprediction = {1, 2, 0, 1}\nySet is used to hold all possible labels, but only values from 'truth' are used\nthis way it is created a matrix 2x2, but should be 3x3. Exactly haifengl, no doubt a test must always include all classes considering a model construction process.\nBut sometimes we have new tiny data, and would be nice to get confusion matrix from it.\nIt would be nice this enhancement :)\nthanks, regards. ",
    "pscholze": "I mean unsupervised topic detection in NLP, commonly done with Latent Dirichlet Allocation etc.. ",
    "scarton": "Thanks - I had read the paper. . ",
    "gcoter": "Thank you for your quick response. I have run additional tests taking into account your comments:\n\nI decreased maxNodes to 250.\nI increased nodeSize to 1000.\nI used Oracle JDK to run the training.\nI specified the following parameters to the JVM: -Xms30g -Xmx100g.\n\nMy data is highly imbalanced (sorry, I forgot to mention it in my first message).\nI don't use one-hot encoding for the categorical variables.\nHere are the logs of the training I launched:\n[2017-12-13 02:05:11,570] [INFO] RfTrainer:79 main - Training...\n[2017-12-13 02:05:11,570] [INFO] RfTrainer:80 main - Train X: (20932649,37); Train Y: (20932649,)\n[2017-12-13 02:56:24,320] [INFO] RandomForest$TrainingTask:406 Thread-36 - Random forest tree OOB size: 20931746, accuracy: 99.80%\n[2017-12-13 02:56:26,790] [INFO] RandomForest$TrainingTask:406 Thread-22 - Random forest tree OOB size: 20931774, accuracy: 99.79%\n[2017-12-13 02:57:09,308] [INFO] RandomForest$TrainingTask:406 Thread-12 - Random forest tree OOB size: 20752722, accuracy: 99.79%\n[2017-12-13 02:58:14,551] [INFO] RandomForest$TrainingTask:406 Thread-30 - Random forest tree OOB size: 20932037, accuracy: 99.80%\n[2017-12-13 02:58:30,801] [INFO] RandomForest$TrainingTask:406 Thread-24 - Random forest tree OOB size: 20931068, accuracy: 99.79%\n[2017-12-13 02:58:45,694] [INFO] RandomForest$TrainingTask:406 Thread-23 - Random forest tree OOB size: 20931739, accuracy: 99.79%\n[2017-12-13 02:58:47,722] [INFO] RandomForest$TrainingTask:406 Thread-15 - Random forest tree OOB size: 20931174, accuracy: 99.80%\n[2017-12-13 02:58:49,272] [INFO] RandomForest$TrainingTask:406 Thread-2 - Random forest tree OOB size: 20931729, accuracy: 99.79%\n[2017-12-13 02:58:50,537] [INFO] RandomForest$TrainingTask:406 Thread-18 - Random forest tree OOB size: 20920984, accuracy: 99.80%\n[2017-12-13 02:58:50,669] [INFO] RandomForest$TrainingTask:406 Thread-39 - Random forest tree OOB size: 20931565, accuracy: 99.79%\n[2017-12-13 02:59:08,627] [INFO] RandomForest$TrainingTask:406 Thread-27 - Random forest tree OOB size: 20931615, accuracy: 99.79%\n[2017-12-13 02:59:11,264] [INFO] RandomForest$TrainingTask:406 Thread-4 - Random forest tree OOB size: 20931754, accuracy: 99.80%\n[2017-12-13 02:59:25,077] [INFO] RandomForest$TrainingTask:406 Thread-9 - Random forest tree OOB size: 20930787, accuracy: 99.80%\n[2017-12-13 02:59:28,505] [INFO] RandomForest$TrainingTask:406 Thread-1 - Random forest tree OOB size: 20931927, accuracy: 99.80%\n[2017-12-13 02:59:29,279] [INFO] RandomForest$TrainingTask:406 Thread-17 - Random forest tree OOB size: 20930816, accuracy: 99.80%\n[2017-12-13 02:59:32,660] [INFO] RandomForest$TrainingTask:406 Thread-25 - Random forest tree OOB size: 20931631, accuracy: 99.79%\n[2017-12-13 02:59:32,993] [INFO] RandomForest$TrainingTask:406 Thread-10 - Random forest tree OOB size: 20931688, accuracy: 99.80%\n[2017-12-13 02:59:35,608] [INFO] RandomForest$TrainingTask:406 Thread-21 - Random forest tree OOB size: 20676648, accuracy: 99.79%\n[2017-12-13 02:59:36,845] [INFO] RandomForest$TrainingTask:406 Thread-19 - Random forest tree OOB size: 20932063, accuracy: 99.80%\n[2017-12-13 02:59:38,103] [INFO] RandomForest$TrainingTask:406 Thread-29 - Random forest tree OOB size: 20924036, accuracy: 99.79%\n[2017-12-13 02:59:38,986] [INFO] RandomForest$TrainingTask:406 Thread-6 - Random forest tree OOB size: 13197558, accuracy: 99.70%\n[2017-12-13 02:59:41,609] [INFO] RandomForest$TrainingTask:406 Thread-8 - Random forest tree OOB size: 20931652, accuracy: 99.80%\n[2017-12-13 03:00:22,981] [INFO] RandomForest$TrainingTask:406 Thread-28 - Random forest tree OOB size: 20931917, accuracy: 99.79%\n[2017-12-13 03:00:23,049] [INFO] RandomForest$TrainingTask:406 Thread-32 - Random forest tree OOB size: 20931209, accuracy: 99.80%\n[2017-12-13 03:00:24,508] [INFO] RandomForest$TrainingTask:406 Thread-35 - Random forest tree OOB size: 20931689, accuracy: 99.79%\n[2017-12-13 03:00:25,100] [INFO] RandomForest$TrainingTask:406 Thread-34 - Random forest tree OOB size: 20931987, accuracy: 99.80%\n[2017-12-13 03:00:29,737] [INFO] RandomForest$TrainingTask:406 Thread-26 - Random forest tree OOB size: 20931991, accuracy: 99.79%\n[2017-12-13 03:00:30,098] [INFO] RandomForest$TrainingTask:406 Thread-38 - Random forest tree OOB size: 20931266, accuracy: 99.79%\n[2017-12-13 03:00:31,736] [INFO] RandomForest$TrainingTask:406 Thread-13 - Random forest tree OOB size: 20931750, accuracy: 99.80%\n[2017-12-13 03:00:36,797] [INFO] RandomForest$TrainingTask:406 Thread-7 - Random forest tree OOB size: 20931956, accuracy: 99.80%\n[2017-12-13 03:00:43,184] [INFO] RandomForest$TrainingTask:406 Thread-14 - Random forest tree OOB size: 20931775, accuracy: 99.80%\n[2017-12-13 03:00:45,851] [INFO] RandomForest$TrainingTask:406 Thread-31 - Random forest tree OOB size: 20931658, accuracy: 99.79%\n[2017-12-13 03:01:13,036] [INFO] RandomForest$TrainingTask:406 Thread-11 - Random forest tree OOB size: 20931848, accuracy: 99.80%\n[2017-12-13 03:17:57,166] [INFO] RandomForest$TrainingTask:406 Thread-37 - Random forest tree OOB size: 20931932, accuracy: 99.80%\n[2017-12-13 03:32:24,957] [INFO] RandomForest$TrainingTask:406 Thread-5 - Random forest tree OOB size: 20932037, accuracy: 99.80%\n[2017-12-13 03:33:16,369] [INFO] RandomForest$TrainingTask:406 Thread-40 - Random forest tree OOB size: 20931880, accuracy: 99.79%\n[2017-12-13 03:33:32,414] [INFO] RandomForest$TrainingTask:406 Thread-20 - Random forest tree OOB size: 20930869, accuracy: 99.79%\n[2017-12-13 03:33:49,880] [INFO] RandomForest$TrainingTask:406 Thread-16 - Random forest tree OOB size: 20931211, accuracy: 99.79%\n[2017-12-13 03:34:58,156] [INFO] RandomForest$TrainingTask:406 Thread-33 - Random forest tree OOB size: 20931491, accuracy: 99.79%\n[2017-12-13 03:36:06,779] [INFO] RandomForest$TrainingTask:406 Thread-36 - Random forest tree OOB size: 20931724, accuracy: 99.80%\n[2017-12-13 03:36:09,851] [INFO] RandomForest$TrainingTask:406 Thread-22 - Random forest tree OOB size: 20931817, accuracy: 99.79%\n[2017-12-13 03:36:38,154] [INFO] RandomForest$TrainingTask:406 Thread-30 - Random forest tree OOB size: 20891011, accuracy: 99.80%\n[2017-12-13 03:37:11,024] [INFO] RandomForest$TrainingTask:406 Thread-21 - Random forest tree OOB size: 20931913, accuracy: 99.80%\n[2017-12-13 03:37:33,413] [INFO] RandomForest$TrainingTask:406 Thread-23 - Random forest tree OOB size: 20932026, accuracy: 99.79%\n[2017-12-13 03:37:36,819] [INFO] RandomForest$TrainingTask:406 Thread-39 - Random forest tree OOB size: 20932090, accuracy: 99.79%\n[2017-12-13 03:37:49,770] [INFO] RandomForest$TrainingTask:406 Thread-29 - Random forest tree OOB size: 20931523, accuracy: 99.79%\n[2017-12-13 03:37:52,770] [INFO] RandomForest$TrainingTask:406 Thread-15 - Random forest tree OOB size: 20931877, accuracy: 99.80%\n[2017-12-13 03:37:54,667] [INFO] RandomForest$TrainingTask:406 Thread-38 - Random forest tree OOB size: 20919989, accuracy: 99.79%\n[2017-12-13 03:38:11,528] [INFO] RandomForest$TrainingTask:406 Thread-4 - Random forest tree OOB size: 20931545, accuracy: 99.80%\n[2017-12-13 03:38:12,919] [INFO] RandomForest$TrainingTask:406 Thread-27 - Random forest tree OOB size: 20931643, accuracy: 99.80%\n[2017-12-13 03:38:14,148] [INFO] RandomForest$TrainingTask:406 Thread-34 - Random forest tree OOB size: 20931961, accuracy: 99.79%\n[2017-12-13 03:38:14,614] [INFO] RandomForest$TrainingTask:406 Thread-8 - Random forest tree OOB size: 20931617, accuracy: 99.79%\n[2017-12-13 03:38:15,356] [INFO] RandomForest$TrainingTask:406 Thread-12 - Random forest tree OOB size: 20931504, accuracy: 99.80%\n[2017-12-13 03:38:17,931] [INFO] RandomForest$TrainingTask:406 Thread-1 - Random forest tree OOB size: 20931862, accuracy: 99.79%\n[2017-12-13 03:38:18,014] [INFO] RandomForest$TrainingTask:406 Thread-10 - Random forest tree OOB size: 20930980, accuracy: 99.80%\n[2017-12-13 03:38:18,247] [INFO] RandomForest$TrainingTask:406 Thread-24 - Random forest tree OOB size: 20929761, accuracy: 99.80%\n[2017-12-13 03:38:19,298] [INFO] RandomForest$TrainingTask:406 Thread-32 - Random forest tree OOB size: 20931493, accuracy: 99.79%\n[2017-12-13 03:38:20,273] [INFO] RandomForest$TrainingTask:406 Thread-25 - Random forest tree OOB size: 20838559, accuracy: 99.79%\n[2017-12-13 03:38:20,440] [INFO] RandomForest$TrainingTask:406 Thread-17 - Random forest tree OOB size: 20932044, accuracy: 99.80%\n[2017-12-13 03:38:20,813] [INFO] RandomForest$TrainingTask:406 Thread-13 - Random forest tree OOB size: 20931719, accuracy: 99.79%\n[2017-12-13 03:38:21,227] [INFO] RandomForest$TrainingTask:406 Thread-19 - Random forest tree OOB size: 13022455, accuracy: 99.69%\n[2017-12-13 03:38:21,286] [INFO] RandomForest$TrainingTask:406 Thread-9 - Random forest tree OOB size: 20931196, accuracy: 99.80%\n[2017-12-13 03:38:21,409] [INFO] RandomForest$TrainingTask:406 Thread-6 - Random forest tree OOB size: 20931839, accuracy: 99.80%\n[2017-12-13 03:38:25,175] [INFO] RandomForest$TrainingTask:406 Thread-18 - Random forest tree OOB size: 20931549, accuracy: 99.80%\n[2017-12-13 03:38:25,481] [INFO] RandomForest$TrainingTask:406 Thread-35 - Random forest tree OOB size: 20926773, accuracy: 99.79%\n[2017-12-13 03:38:29,268] [INFO] RandomForest$TrainingTask:406 Thread-28 - Random forest tree OOB size: 20931499, accuracy: 99.80%\n[2017-12-13 03:38:45,698] [INFO] RandomForest$TrainingTask:406 Thread-14 - Random forest tree OOB size: 20931772, accuracy: 99.80%\n[2017-12-13 03:38:46,259] [INFO] RandomForest$TrainingTask:406 Thread-7 - Random forest tree OOB size: 20932018, accuracy: 99.80%\n[2017-12-13 03:39:01,949] [INFO] RandomForest$TrainingTask:406 Thread-26 - Random forest tree OOB size: 20931925, accuracy: 99.79%\n[2017-12-13 03:39:06,222] [INFO] RandomForest$TrainingTask:406 Thread-31 - Random forest tree OOB size: 20931729, accuracy: 99.79%\n[2017-12-13 03:43:22,144] [INFO] RandomForest$TrainingTask:406 Thread-11 - Random forest tree OOB size: 20930744, accuracy: 99.79%\n[2017-12-13 03:44:09,096] [INFO] RandomForest$TrainingTask:406 Thread-3 - Random forest tree OOB size: 20931882, accuracy: 99.79%\n[2017-12-13 03:44:10,646] [INFO] RandomForest$TrainingTask:406 Thread-37 - Random forest tree OOB size: 20931969, accuracy: 99.80%\n[2017-12-13 03:44:11,364] [INFO] RandomForest$TrainingTask:406 Thread-40 - Random forest tree OOB size: 11355905, accuracy: 99.68%\n[2017-12-13 03:44:25,728] [INFO] RandomForest$TrainingTask:406 Thread-16 - Random forest tree OOB size: 20643535, accuracy: 99.79%\n[2017-12-13 03:44:27,311] [INFO] RandomForest$TrainingTask:406 Thread-2 - Random forest tree OOB size: 20932031, accuracy: 99.80%\n[2017-12-13 03:44:28,538] [INFO] RandomForest$TrainingTask:406 Thread-20 - Random forest tree OOB size: 20932053, accuracy: 99.80%\n[2017-12-13 03:44:48,111] [INFO] RandomForest$TrainingTask:406 Thread-33 - Random forest tree OOB size: 20931999, accuracy: 99.79%\n[2017-12-13 03:44:48,289] [INFO] RandomForest$TrainingTask:406 Thread-5 - Random forest tree OOB size: 20924933, accuracy: 99.79%\n[2017-12-13 03:44:49,834] [INFO] RandomForest$TrainingTask:406 Thread-21 - Random forest tree OOB size: 20931017, accuracy: 99.79%\n[2017-12-13 03:45:00,073] [INFO] RandomForest$TrainingTask:406 Thread-23 - Random forest tree OOB size: 20931748, accuracy: 99.79%\n[2017-12-13 03:45:01,601] [INFO] RandomForest$TrainingTask:406 Thread-39 - Random forest tree OOB size: 20931532, accuracy: 99.79%\n[2017-12-13 03:45:05,750] [INFO] RandomForest$TrainingTask:406 Thread-22 - Random forest tree OOB size: 20769349, accuracy: 99.79%\n[2017-12-13 03:45:05,789] [INFO] RandomForest$TrainingTask:406 Thread-30 - Random forest tree OOB size: 20931548, accuracy: 99.80%\n[2017-12-13 03:45:17,329] [INFO] RandomForest$TrainingTask:406 Thread-29 - Random forest tree OOB size: 20931850, accuracy: 99.80%\n[2017-12-13 03:45:17,768] [INFO] RandomForest$TrainingTask:406 Thread-36 - Random forest tree OOB size: 20931691, accuracy: 99.80%\n[2017-12-13 03:45:36,523] [INFO] RandomForest$TrainingTask:406 Thread-38 - Random forest tree OOB size: 20931475, accuracy: 99.79%\n[2017-12-13 03:45:56,367] [INFO] RandomForest$TrainingTask:406 Thread-15 - Random forest tree OOB size: 20931995, accuracy: 99.80%\n[2017-12-13 03:45:56,374] [ERROR] RandomForest:598 main - Failed to train random forest on multi-core\njava.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: Java heap space\n    at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n    at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n    at smile.util.MulticoreExecutor.run(MulticoreExecutor.java:107)\n    at smile.classification.RandomForest.<init>(RandomForest.java:596)\n    at smile.classification.RandomForest.<init>(RandomForest.java:504)\n    at smile.classification.RandomForest.<init>(RandomForest.java:484)\n    at ...\nCaused by: java.lang.OutOfMemoryError: Java heap space\n[2017-12-13 03:50:51,945] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931992, accuracy: 99.80%\n[2017-12-13 03:55:38,640] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20929719, accuracy: 99.80%\n[2017-12-13 04:00:36,094] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932045, accuracy: 99.80%\n[2017-12-13 04:05:33,509] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931796, accuracy: 99.79%\n[2017-12-13 04:10:23,509] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931685, accuracy: 99.79%\n[2017-12-13 04:14:44,601] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932051, accuracy: 99.79%\n[2017-12-13 04:19:00,367] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20930019, accuracy: 99.79%\n[2017-12-13 04:23:14,482] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931344, accuracy: 99.80%\n[2017-12-13 04:27:31,456] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20930712, accuracy: 99.79%\n[2017-12-13 04:31:32,907] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931931, accuracy: 99.80%\n[2017-12-13 04:35:43,279] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931796, accuracy: 99.80%\n[2017-12-13 04:39:58,819] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20930452, accuracy: 99.79%\n[2017-12-13 04:43:47,221] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931293, accuracy: 99.80%\n[2017-12-13 04:47:40,188] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931953, accuracy: 99.79%\n[2017-12-13 04:51:14,950] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931543, accuracy: 99.79%\n[2017-12-13 04:55:18,451] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20929265, accuracy: 99.79%\n[2017-12-13 04:59:18,437] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931664, accuracy: 99.79%\n[2017-12-13 05:03:01,696] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 13245673, accuracy: 99.70%\n[2017-12-13 05:07:02,509] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931855, accuracy: 99.80%\n[2017-12-13 05:10:57,371] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931877, accuracy: 99.79%\n[2017-12-13 05:14:54,110] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931783, accuracy: 99.80%\n[2017-12-13 05:18:47,889] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931659, accuracy: 99.80%\n[2017-12-13 05:22:54,895] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931544, accuracy: 99.80%\n[2017-12-13 05:26:46,867] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932007, accuracy: 99.80%\n[2017-12-13 05:30:39,919] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932085, accuracy: 99.80%\n[2017-12-13 05:34:51,118] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931892, accuracy: 99.80%\n[2017-12-13 05:38:38,057] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20930937, accuracy: 99.79%\n[2017-12-13 05:42:25,326] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932060, accuracy: 99.80%\n[2017-12-13 05:46:30,194] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931776, accuracy: 99.80%\n[2017-12-13 05:50:23,126] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20926644, accuracy: 99.80%\n[2017-12-13 05:54:27,529] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931936, accuracy: 99.80%\n[2017-12-13 05:58:22,689] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931405, accuracy: 99.80%\n[2017-12-13 06:02:20,160] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931976, accuracy: 99.79%\n[2017-12-13 06:06:29,162] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20688381, accuracy: 99.79%\n[2017-12-13 06:10:17,799] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931590, accuracy: 99.79%\n[2017-12-13 06:14:05,972] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932032, accuracy: 99.79%\n[2017-12-13 06:18:06,319] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20929822, accuracy: 99.80%\n[2017-12-13 06:21:59,865] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20930671, accuracy: 99.79%\n[2017-12-13 06:25:59,430] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931261, accuracy: 99.79%\n[2017-12-13 06:29:46,707] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931607, accuracy: 99.80%\n[2017-12-13 06:33:34,071] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931762, accuracy: 99.80%\n[2017-12-13 06:37:09,943] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931970, accuracy: 99.79%\n[2017-12-13 06:41:02,052] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931661, accuracy: 99.79%\n[2017-12-13 06:45:03,914] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931966, accuracy: 99.80%\n[2017-12-13 06:49:03,150] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931877, accuracy: 99.80%\n[2017-12-13 06:52:56,887] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931826, accuracy: 99.79%\n[2017-12-13 06:56:52,265] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932051, accuracy: 99.79%\n[2017-12-13 07:00:30,714] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931905, accuracy: 99.79%\n[2017-12-13 07:04:26,696] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932060, accuracy: 99.80%\n[2017-12-13 07:08:11,558] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931960, accuracy: 99.80%\n[2017-12-13 07:12:06,326] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931723, accuracy: 99.79%\n[2017-12-13 07:15:53,709] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 14483136, accuracy: 99.71%\n[2017-12-13 07:19:38,167] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20924930, accuracy: 99.79%\n[2017-12-13 07:23:27,187] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931698, accuracy: 99.80%\n[2017-12-13 07:27:15,029] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931706, accuracy: 99.79%\n[2017-12-13 07:31:10,426] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932084, accuracy: 99.80%\n[2017-12-13 07:35:01,165] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931967, accuracy: 99.80%\n[2017-12-13 07:38:57,912] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931714, accuracy: 99.80%\n[2017-12-13 07:42:46,087] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931775, accuracy: 99.79%\n[2017-12-13 07:46:35,845] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932070, accuracy: 99.79%\n[2017-12-13 07:50:30,579] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931763, accuracy: 99.80%\n[2017-12-13 07:54:23,764] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931871, accuracy: 99.79%\n[2017-12-13 07:58:22,733] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931911, accuracy: 99.80%\n[2017-12-13 08:02:23,484] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20930919, accuracy: 99.79%\n[2017-12-13 08:05:54,410] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931628, accuracy: 99.79%\n[2017-12-13 08:09:46,085] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931568, accuracy: 99.79%\n[2017-12-13 08:13:34,521] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20930725, accuracy: 99.79%\n[2017-12-13 08:17:38,075] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931773, accuracy: 99.80%\n[2017-12-13 08:21:45,785] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931433, accuracy: 99.80%\n[2017-12-13 08:25:34,675] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931630, accuracy: 99.79%\n[2017-12-13 08:29:25,727] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932017, accuracy: 99.80%\n[2017-12-13 08:33:17,502] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931774, accuracy: 99.79%\n[2017-12-13 08:36:53,871] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931777, accuracy: 99.79%\n[2017-12-13 08:40:44,582] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931005, accuracy: 99.79%\n[2017-12-13 08:44:37,504] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931887, accuracy: 99.80%\n[2017-12-13 08:48:34,297] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20930954, accuracy: 99.79%\n[2017-12-13 08:52:23,708] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931875, accuracy: 99.80%\n[2017-12-13 08:56:15,023] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932070, accuracy: 99.80%\n[2017-12-13 09:00:13,224] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932061, accuracy: 99.79%\n[2017-12-13 09:03:58,737] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931722, accuracy: 99.79%\n[2017-12-13 09:08:06,576] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931234, accuracy: 99.79%\n[2017-12-13 09:12:02,443] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931873, accuracy: 99.80%\n[2017-12-13 09:15:45,735] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931747, accuracy: 99.80%\n[2017-12-13 09:19:34,209] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931723, accuracy: 99.79%\n[2017-12-13 09:23:33,572] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932075, accuracy: 99.80%\n[2017-12-13 09:27:33,683] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931980, accuracy: 99.80%\n[2017-12-13 09:31:47,690] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931595, accuracy: 99.80%\n[2017-12-13 09:35:40,716] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932058, accuracy: 99.79%\n[2017-12-13 09:39:16,942] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931867, accuracy: 99.79%\n[2017-12-13 09:42:52,829] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932071, accuracy: 99.79%\n[2017-12-13 09:46:55,738] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931931, accuracy: 99.79%\n[2017-12-13 09:51:08,027] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932057, accuracy: 99.80%\n[2017-12-13 09:54:59,033] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20930830, accuracy: 99.79%\n[2017-12-13 09:59:01,939] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931493, accuracy: 99.80%\n[2017-12-13 10:03:01,082] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932067, accuracy: 99.80%\n[2017-12-13 10:06:41,614] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932026, accuracy: 99.80%\n[2017-12-13 10:10:34,110] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931895, accuracy: 99.79%\n[2017-12-13 10:14:16,967] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931139, accuracy: 99.79%\n[2017-12-13 10:18:14,780] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20932060, accuracy: 99.79%\n[2017-12-13 10:22:19,441] [INFO] RandomForest$TrainingTask:406 main - Random forest tree OOB size: 20931845, accuracy: 99.80%\n[2017-12-13 10:22:19,613] [INFO] RfTrainer:86 main - Training finished in 29828.041964647s\nAs you can see, the training did finish but:\n\nI had an OutOfMemory exception as in the previous execution, despite the very high -Xmx I specified. This exception was thrown after 88 trees were built. At this point, the training started again, growing one tree at a time (4 minutes per tree). I find it strange that an OutOfMemory exception occured when the trees were trained on several cores but did not occur when they were trained one at a time.\nIt took 50 minutes to train the first trees.\nThe OOB accuracy is still quite high (99.8%) and the OOB size doesn't look right indeed.\n\nRegarding the OOB size, I think I have found a bug in the TrainNode class. We can discuss this in a separate issue if you prefer.\nThank you very much for your help.. I managed to run a training with:\n\n-Xms150g -Xmx150g\nmaxNodes = 200\nnodeSize = 2000\n\nIt took 1 hour and a half and I didn't get an OutOfMemory exception this time. I measured the Average Precision on a separate test set and I got 0.2. With Scikit Learn, I got 0.4.\nI have the feeling that by setting maxNodes to 200, I impose too much restriction on the size of the trees whereas Scikit Learn doesn't restrict trees at all (by default, max_depth is None). So maybe the Random Forest constructed with Smile is underfitting with those parameters. However, if I increase maxNodes, it increases the training time and also it can cause OutOfMemory exceptions. What could I try in order to get results similar to Scikit Learn while keeping a reasonable training time?. Regarding the OOB size, I made a little change and noticed more reasonable OOB sizes in the logs, as for instance:\n[2017-12-14 17:15:49,612] [INFO] RandomForest$TrainingTask:406 Thread-24 - Random forest tree OOB size: 9004578, accuracy: 99.82%\nThe OOB size is calculated from an array called samples. I noticed that this array is passed to a TrainNode object at the beginning of the training. But this array seems to be modified while building a tree (here and here). So, at the end of the training, most instances are associated to a 0 value in samples. This causes the OOB evaluation step to take too many instances into account.\nTo avoid this, I added some code in the constructor of TrainNode to deep copy the array samples. This prevents the values of the original samples array from being overwritten.. ",
    "AntonYurchenko": "Thanks a lot! I have tried the method. It works.\nBut I need to call smile.math.Math.setSeed before each training of the model\nFor example:\n```scala\nsmile.math.Math.setSeed(777)\nval model1 = regression.randomForest(trainDs.x, trainDs.y)\nval trainRMSE1 = rmse(model1.predict(trainDs.x), trainDs.y)\nval model2 = regression.randomForest(trainDs.x, trainDs.y)\nval trainRMSE2 = rmse(model2.predict(trainDs.x), trainDs.y)\ntrainRMSE1 == trainRMSE2 // false\nTrue way is:scala\nsmile.math.Math.setSeed(777)\nval model1 = regression.randomForest(trainDs.x, trainDs.y)\nval trainRMSE1 = rmse(model1.predict(trainDs.x), trainDs.y)\nsmile.math.Math.setSeed(777)\nval model2 = regression.randomForest(trainDs.x, trainDs.y)\nval trainRMSE2 = rmse(model2.predict(trainDs.x), trainDs.y)\ntrainRMSE1 == trainRMSE2 // true\n```\n. ",
    "lfoppiano": "Hi, I have a couple of questions: \n1. the parameter -Dsmile.threads=1 should be used only for training or everytime we want to set the seed manually? \n2.  once the model is serialised, do we still need to set the seed / set the threads=1 after deserialise it?\nThank you in advance. More details in the FAQ: http://haifengl.github.io/smile/faq.html. ",
    "qianyiwei": "\u8c22\u8c22\uff01\uff01\uff01. ",
    "mpeychev": "Thank you for your quick response. However, I am not asking for support for other systems. Rather than that, I thought it would be useful to provide a way or a method for building and creating decision trees/random forests manually i.e. to have your own custom serialization format.\nIn this way one can use another tool (e.g. SparkML, scikit-learn) for training, than translate the model (on their own) so that it is compatible with your library, and eventually use it for prediction/serving.. ",
    "meyerjp3": "Thanks. I really appreciate all the work you do for this library. It is an excellent resource!. ",
    "Ajk4": "Hi  @haifengl  \nI'm not exactly sure what you meant.\nDo you want to check paper references regarding specific implementation details? \nOr do you want to check paper references whether it make sense to weight features in the first place? If the latter maybe I can drop some links to R implementation that have this feature.\nRBorist:\nhttps://cran.r-project.org/web/packages/Rborist/Rborist.pdf\nExample usage:\n```\nCauses first three predictors to be selected as splitting candidates\ntwice as often as the other two:\nrb <- Rborist(x, y, predWeight=c(2.0, 2.0, 2.0, 1.0, 1.0))\n```\nAnother alternative could be providing some optional function parameter so user can provide custom split scoring function. That way if you don't like having weighted features here user could provide in it's own scoring function.\nWhat do you think?. Also note, that data needed for proper error estimation is already lost, so I am doing an weighted average (in respect to forest size) of errors from both forest.. Hi @serickso \nI doubt that there would be a paper reference since it's a technical detail. Could you try to be more specific about what you expect?\nRandom forest is List of DecisionTree's. Merging two Lists of decision trees into one is something that might be (and in my case it is) very useful.\nWith current implementation it's not possible, since there are no constructor accepting list of decision trees.\nIf you don't like adding new 'merge' method what about adding public constructor accepting list of decision trees? \nThat way in users code we could do something like\n```\nRandomForest rf1 = (...); // from some spark executor\nRandomForest rf2 = (...); // from other spark executor\nList allTrees = Lists.concat(rf1.getTrees(), rf2.getTrees())\nRandomForest combined = new RandomForest(allTrees);\n```\nNow that I see it I like it more that adding merge function.. Some reference, in R there is combine function for that -> https://github.com/cran/randomForest/blob/master/man/combine.Rd. Hi @haifengl\n1) Underlying nextDouble() function is still probably bugged. Least significant bits in it are always zeros. Perhaps original algorithm was not designed to work with 64 double floating numbers, but with less bit numbers instead?\nI feel like such workaround will bite us again eventually. Could you consider merging PR I added that switches the implementation to MersenneTwister?\n2) Additionally we could consider adding java.random implementation of smile random interface just so users can switch to it if needed.\n. Okay! Thanks for clarification. TIL about some of those RNG algorithms.\n\nSince it generates values only in [0, 1), not all bits would be used by nature.\n\nThat is true. \nI should have clarified my comment - fraction part of double number has all zeros on least significant bits, while those bits could be randomized while it's still [0, 1) range.\nAnyways, this probably doesn't matter now that we have your fix for Integer values and we probably don't need randomness that far after decimal point.\n\nBTW, you pull requests includes a lot of unnecessary white space changes, which is not desired.\n\nOkay - I'll keep this in mind for future commits and try not to introduce unrelated white space changes.\nThose white spaces were automatically applied by my IDE (InteliJ). It's probably cleaner not to have orphan spaces in random places so we might have one commit that automatically cleans up all those orphan spaces :P \n@haifengl \nI extracted infinite-recursion fix to another PR -> https://github.com/haifengl/smile/pull/274. Hi @haifengl \nI pushed commit with your suggestion and moved monoreg param to the regression random forest algorithm API.\nCould you maybe review my commits and accept the changes if possible? Thanks!. I forgot to add monotonic regression to regression RandomForest as well (and not single RegressionTree only). \nRecent commit adds it. Good catch. I fixed it in new commit. Yeah, I see what you mean. I think I will change it that way. \nAttribute class is used for things other than regression trees too. And monotonic regression makes sense only for regression tree model. So it's probably better to have it in Algorithm API. Done. ",
    "Panagiota-Antonia": "Hi, thank you very much for your advice. I checked the book and I\u2019m not very sure about the meaning of the nt parameter still.Is it the number of past processes related to the old cummulative distrubution function? In our case nt parameter according to nd increases, like 2nd and 3nd for every update.\nI\u2019m sorry I don\u2019t seem to understand it very well. Can you please help me?  . ",
    "Nagendra080389": "Yes that solved some of it,\nI am yet curious how to predict as we do in Scikit. Currently in Java I have this, but the issue is how do I predict the log_price , what index should I pass:\nAs in Scikit we have something like KFold for cross Validation and fir method like this and we drop the log_price \n    train_x = data[data.dataset == \"train\"] \\\n    .select_dtypes(include=numerics) \\\n    .drop(\"log_price\", axis=1) \\\n    .fillna(0) \\\n    .values\n\ntest_x = data[data.dataset == \"test\"] \\\n    .select_dtypes(include=numerics) \\\n    .drop(\"log_price\", axis=1) \\\n    .fillna(0) \\\n    .values\n\ntrain_y = data[data.dataset == \"train\"].log_price.values\n\n    cv_groups = KFold(n_splits=3)\n\n    regr = RandomForestRegressor(n_estimators=500, oob_score=True, n_jobs=-1, random_state=50, max_features=\"auto\",\n                                 min_samples_leaf=leaf_size)\n\n    for train_index, test_index in cv_groups.split(train_x):\n        # Train the model using the training sets\n        regr.fit(train_x[train_index], train_y[train_index])\n\n        # Make predictions using the testing set\n        pred_rf = regr.predict(train_x[test_index])\n\n        # Calculate RMSE for current cross-validation split\n        rmse = str(np.sqrt(np.mean((train_y[test_index] - pred_rf) ** 2)))\n        print \"Accuracy :\", metrics.accuracy_score(train_y[test_index], pred_rf)\n        print(\"RMSE for current split: \" + rmse + \" for leafsize \", leaf_size)\n        # print \"AUC - ROC : \", roc_auc_score(train_y[train_index], regr.oob_prediction_)\n\n    # Create submission file\n    regr.fit(train_x, train_y)\n    final_prediction = regr.predict(test_x)\n\nIn Java I have written it like this, but how to predict the log_price.\n        AttributeDataset train = parser.parse(\"LogPrice Train\",\n                new FileInputStream(\"C:\\\\Users\\\\nagesingh\\\\IdeaProjects\\\\machineLearning\\\\src\\\\main\\\\resources\\\\train_new.csv\"));\n        AttributeDataset test = parser.parse(\"LogPrice Test\",\n                new FileInputStream(\"C:\\\\Users\\\\nagesingh\\\\IdeaProjects\\\\machineLearning\\\\src\\\\main\\\\resources\\\\test_new.csv\"));\n\n        double[][] trainx = train.toArray(new double[train.size()][]);\n        double[] trainy = train.toArray(new double[train.size()]);\n\n\n        double[][] testx = test.toArray(new double[test.size()][]);\n        double[] testy = test.toArray(new double[test.size()]);\n\n        smile.regression.RandomForest randomForest = new smile.regression.RandomForest(trainx, trainy, 500);\n\n        int error = 0;\n        for (int i = 0; i < testx.length; i++) {\n            if (randomForest.predict(testx[i]) != testy[i]) {\n                error++;\n            }\n        }\n\n        double[] accuracy = randomForest.test(testx, testy);\n        for (int i = 1; i <= accuracy.length; i++) {\n            System.out.format(\"%d trees accuracy = %.2f%%%n\", i, 100.0 * accuracy[i-1]);\n        }. Tried to get the RMSE:\n\n        RMSE rmse = new RMSE();\n        System.out.println(\"RMSE : \"+rmse.measure(trainy,randomForest.predict(trainx)));\n\nwith the training set and got this result: \nRMSE : 0.495494833227403. I have done the prediction like this: \nIs this correct:\n        smile.regression.RandomForest randomForest = new smile.regression.RandomForest(trainx, trainy, 500, 200, 2000, 6);\n\n        double[] predict = randomForest.predict(trainx);\n        RMSE rmse = new RMSE();\n        System.out.println(\"RMSE : \"+rmse.measure(trainy,predict));\n\n        double[] finalPredict = randomForest.predict(testx);\n\n        System.out.println(\"Test123\");. I am trying to parse the csv file here but it always throws this error:\n\n    DelimitedTextParser parser = new DelimitedTextParser();\n    parser.setDelimiter(\",\");\n    parser.setResponseIndex(new NumericAttribute(\"log_price\"), 1);\n\n    Attribute[] attributes = new Attribute[28];\n    attributes[0] = new NumericAttribute(\"id\");\n    //attributes[1] = new NumericAttribute(\"log_price\");\n    attributes[1] = new NominalAttribute(\"property_type\");\n    attributes[2] = new NominalAttribute(\"room_type\");\n    attributes[3] = new NominalAttribute(\"amenities\");\n    attributes[4] = new NumericAttribute(\"accommodates\");\n    attributes[5] = new NumericAttribute(\"bathrooms\");\n    attributes[6] = new NominalAttribute(\"bed_type\");\n    attributes[7] = new NominalAttribute(\"cancellation_policy\");\n    attributes[8] = new NominalAttribute(\"cleaning_fee\");\n    attributes[9] = new NominalAttribute(\"city\");\n    attributes[10] = new NominalAttribute(\"description\");\n    attributes[11] = new DateAttribute(\"first_review\");\n    attributes[12] = new NominalAttribute(\"host_has_profile_pic\");\n    attributes[13] = new NominalAttribute(\"host_identity_verified\");\n    attributes[14] = new NumericAttribute(\"host_response_rate\");\n    attributes[15] = new DateAttribute(\"host_since\");\n    attributes[16] = new NominalAttribute(\"instant_bookable\");\n    attributes[17] = new DateAttribute(\"last_review\");\n    attributes[18] = new NumericAttribute(\"latitude\");\n    attributes[19] = new NumericAttribute(\"longitude\");\n    attributes[20] = new NominalAttribute(\"name\");\n    attributes[21] = new NominalAttribute(\"neighbourhood\");\n    attributes[22] = new NumericAttribute(\"number_of_reviews\");\n    attributes[23] = new NumericAttribute(\"review_scores_rating\");\n    attributes[24] = new NominalAttribute(\"thumbnail_url\");\n    attributes[25] = new NumericAttribute(\"zipcode\");\n    attributes[26] = new NumericAttribute(\"bedrooms\");\n    attributes[27] = new NumericAttribute(\"beds\");\n\n    try {\n        AttributeDataset train = parser.parse(attributes,new File(\"C:\\\\Users\\\\nagesingh\\\\IdeaProjects\\\\machineLearning\\\\src\\\\main\\\\resources\\\\train.csv\"));\n\njava.lang.NumberFormatException: For input string: \"id\"\n    at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043)\n    at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110)\n    at java.lang.Double.parseDouble(Double.java:538)\n    at java.lang.Double.valueOf(Double.java:502)\n    at smile.data.NumericAttribute.valueOf(NumericAttribute.java:62)\n    at smile.data.parser.DelimitedTextParser.parse(DelimitedTextParser.java:351)\n    at smile.data.parser.DelimitedTextParser.parse(DelimitedTextParser.java:256)\n    at smile.data.parser.DelimitedTextParser.parse(DelimitedTextParser.java:245)\n    at LoadData.main(LoadData.java:136)\nCSV Data:\nid | log_price | property_type | room_type | amenities | accommodates | bathrooms | bed_type | cancellation_policy | cleaning_fee | city | description | first_review | host_has_profile_pic | host_identity_verified | host_response_rate | host_since | instant_bookable | last_review | latitude | longitude | name | neighbourhood | number_of_reviews | review_scores_rating | thumbnail_url | zipcode | bedrooms | beds\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\n6901257 | 5.010635 | Apartment | Entire home/apt | {\"Wireless Internet\",\"Air   conditioning\",Kitchen,Heating,\"Family/kid   friendly\",Essentials,\"Hair dryer\",Iron,\"translation   missing: en.hosting_amenity_50\"} | 3 | 1 | Real Bed | strict | TRUE | NYC | Beautiful, sunlit brownstone 1-bedroom in the loveliest neighborhood in   Brooklyn. Blocks from the promenade and Brooklyn Bridge Park, with their   stunning views of Manhattan, and from the great shopping and food. | ######## | t | t | \u00a0 | ######## | f | ######## | 40.69652 | -73.9916 | Beautiful brownstone 1-bedroom | Brooklyn Heights | 2 | 100 | https://a0.muscache.com/im/pictures/6d7cbbf7-c034-459c-bc82-6522c957627c.jpg?aki_policy=small | 11201 | 1 | 1\nI think its taking header too, should I remove the header from CSV, or is there a parameter which I can pass in the parser to skip header?. Ok so the above works, but all the String attributes declared as NominalAttributes returns 0, is this ok? I thought that it was like onehotEncoder where it will assign some numeric values to the string attributes ? But the debugger window shows all the string ones are 0.\nDebugger IMage . Yes I dont but why all the Strings in my Excel sheet coming up as 0 when declared as Nominal Attribute.\n    Attribute[] attributes = new Attribute[27];\n    //attributes[0] = new NumericAttribute(\"id\");\n    //attributes[1] = new NumericAttribute(\"log_price\");\n    attributes[0] = new NominalAttribute(\"property_type\");\n    attributes[1] = new NominalAttribute(\"room_type\");\n    attributes[2] = new NominalAttribute(\"amenities\");\n    attributes[3] = new NumericAttribute(\"accommodates\");\n    attributes[4] = new NumericAttribute(\"bathrooms\");\n    attributes[5] = new NominalAttribute(\"bed_type\");\n    attributes[6] = new NominalAttribute(\"cancellation_policy\");\n    attributes[7] = new NominalAttribute(\"cleaning_fee\");\n    attributes[8] = new NominalAttribute(\"city\");\n    attributes[9] = new NominalAttribute(\"description\");\n    attributes[10] = new NominalAttribute(\"first_review\");\n    attributes[11] = new NominalAttribute(\"host_has_profile_pic\");\n    attributes[12] = new NominalAttribute(\"host_identity_verified\");\n    attributes[13] = new NominalAttribute(\"host_response_rate\");\n    attributes[14] = new NominalAttribute(\"host_since\");\n    attributes[15] = new NominalAttribute(\"instant_bookable\");\n    attributes[16] = new NominalAttribute(\"last_review\");\n    attributes[17] = new NumericAttribute(\"latitude\");\n    attributes[18] = new NumericAttribute(\"longitude\");\n    attributes[19] = new NominalAttribute(\"name\");\n    attributes[20] = new NominalAttribute(\"neighbourhood\");\n    attributes[21] = new NumericAttribute(\"number_of_reviews\");\n    attributes[22] = new NumericAttribute(\"review_scores_rating\");\n    attributes[23] = new NominalAttribute(\"thumbnail_url\");\n    attributes[24] = new NominalAttribute(\"zipcode\");\n    attributes[25] = new NumericAttribute(\"bedrooms\");\n    attributes[26] = new NumericAttribute(\"beds\");. Sure, will see tutorials to find out how to resolve this one, thanks for this library though. But the reset string attributes which are a normal strings are converted to zero. Is that expected?\n\nI will replace the (\"\") in the amenities tab with some underscore or something.. ",
    "dariomx": "Thanks a lot by your prompt response Haifeng. While it may not depend entirely on me whether to use it officially within my org, your feedback will certainly help (along with actual experiments that I yet need to conduct).\nSaludos.. Thanks for the quick reply Haifeng.\nLooking forward to it, I am quite interested in this type of optimization. I would like to contribute in the future, but for now probably just watch and learn ;-?\n. Dear Haifeng,\nThanks a lot for your super-fast reply!\nI gave it a try right away, and detected an speedup of 10-12secs less with mentioned data. That is, without your optimization it takes around 30s, and with your optimization around 18-20s. \nMy laptop is not that old, so I was hoping to achieve something similar or superior than your test. In case you get a chance, please give the file below a try; is the very same data I am trying:\nhttps://drive.google.com/open?id=1sYs70zD8GksHcl909cePS-guRpRFmErf\nThanks.. Thanks Haifeng,\nI guess that difference could be the \"client\" JDK I am using (through Intellij). I set the same options that you shared, but I suspect the \"server\" feature is not really honored on my client JDK/JRE.\nI will give it a try with a Server JRE, either from command line as you did or same Intellij. So far no banana due a couple of issues:\n1) Intellij complains about invalid JDK when passing the Server JRE flavor, asked about it on an existing ticket at JetBrains:\nhttps://intellij-support.jetbrains.com/hc/en-us/community/posts/115000708044-Cannot-use-Server-JRE-The-selected-directory-is-not-a-valid-home-for-JDK\nis not totally clear yet if I can use this Server JRE or not from the IDE, awaiting reply.\n2) My smile shell does not resolve predefined library names, not even \"help\" command is available though I can perform basic Scala operations that do not involve the library. I am pasting a sample session below, it relates perhaps with the fact I am using Cygwin and not plain Linux shell nor Windows (I tried from Windows DOS prompt though, but the smile.bat tells error about command too long):\nDBAHENA-LAP+dbahena@DBAHENA-LAP /cygdrive/c/Users/dbahena/IdeaProjects/smile\n$ ./smile.sh\n[info] Loading settings from buildinfo.sbt,plugins.sbt ...\n[info] Loading project definition from C:\\Users\\dbahena\\IdeaProjects\\smile\\project\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Loading settings from build.sbt ...\n[info] Set current project to smile (in build file:/C:/Users/dbahena/IdeaProjects/smile/)\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\shell\\target\\scala-2.12\\smile-shell_2.12-1.5.1.pom\n[info] Main Scala API documentation to C:\\Users\\dbahena\\IdeaProjects\\smile\\shell\\target\\scala-2.12\\api...\n[info] Compiling 1 Scala source to C:\\Users\\dbahena\\IdeaProjects\\smile\\shell\\target\\scala-2.12\\classes ...\n[info] Done compiling.\n[info] Packaging C:\\Users\\dbahena\\IdeaProjects\\smile\\shell\\target\\scala-2.12\\smile-shell_2.12-1.5.1.jar ...\n[info] Done packaging.\nmodel contains 13 documentable templates\n[info] Main Scala API documentation successful.\n[info] Packaging C:\\Users\\dbahena\\IdeaProjects\\smile\\shell\\target\\scala-2.12\\smile-shell_2.12-1.5.1-javadoc.jar ...\n[info] Done packaging.\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\math\\target\\smile-math-1.5.1.pom\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\data\\target\\smile-data-1.5.1.pom\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\graph\\target\\smile-graph-1.5.1.pom\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\netlib\\target\\smile-netlib-1.5.1.pom\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\core\\target\\smile-core-1.5.1.pom\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\interpolation\\target\\smile-interpolation-1.5.1.pom\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\nlp\\target\\smile-nlp-1.5.1.pom\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\plot\\target\\smile-plot-1.5.1.pom\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\scala\\target\\scala-2.12\\smile-scala_2.12-1.5.1.pom\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\benchmark\\target\\scala-2.12\\smile-benchmark_2.12-1.5.1.pom\n[info] Wrote C:\\Users\\dbahena\\IdeaProjects\\smile\\demo\\target\\smile-demo-1.5.1.pom\n[success] Total time: 17 s, completed 20/02/2018 03:03:35 PM\n                                                   ..::''''::..\n                                                 .;''        ``;.\n ....                                           ::    ::  ::    ::\n\n,;' .;:                ()  ..:                  ::     ::  ::     ::\n   ::.      ..:,:;.,:;.    .   ::   .::::.         :: .:' ::  :: :. ::\n    '''::,   ::  ::  ::::   ::  ;:   .::        ::  :          :  ::\n  ,:';  ::;  ::  ::  ::   ::   ::  ::,::''.         :: :.      .:' :::,,,,;;' ,;; ,;;, ;;, ,;;, ,;;, :,,,,:';..::::''..;'::,,,,::''\nWelcome to Smile Shell; enter 'help' for the list of commands.\n  Type \":quit\" to leave the Smile Shell\n  Version 1.5.1, Scala 2.12.4, SBT 1.1.0, Built at 2018-02-20 21:03:19.193\n===============================================================================\nsmile> val x = read.csv(\"trainData.csv\",header=true)\n:11: error: not found: value read\n       val x = read.csv(\"trainData.csv\",header=true)\n               ^\n:11: error: not found: value header\n       val x = read.csv(\"trainData.csv\",header=true)\n                                        ^\nsmile> pdist\n:12: error: not found: value pdist\n       pdist\n       ^\nsmile> hclust\n:12: error: not found: value hclust\n       hclust\n       ^\nsmile> help\n:12: error: not found: value help\n       help\n       ^\nsmile> print(666)\n666smile>\nsmile> val x = 665\nx: Int = 665\nsmile> x + 1\nres4: Int = 666\nsmile>\nI will keep on trying, for me the practical thing would be to be able to use the Server JRE from my IDE (Intellij). Will keep you updated when I achieve that.\nThanks.\n. I am setting env vars to force cygwin pick the one I wanna, but perhaps have not touched system vars to enforce that. \nI agree that cygwin is becoming a pain, I wish I could freely use Ubuntu at work ;-?\nWill keep on trying, thanks for your continuous support.\nPS: Perhaps the -server option is not the culprit as you said, but so far is the best I could think of. The experiment will confirm.\n. I think you were right Haifeng. With help of Intellij I managed to use Server JRE, with all flags you mentioned; but still getting ~ 17secs for same data. I guess the IDE is bringing some extra noise, hence should try to test this in command line as you did.\nWill try to fix the cygwin + java thing and report outcome, thanks again.. Thanks a lot Heifeng.\nEven if I do not get yet same performance as you do, I can definitely see an speedup with the changes applied. This is great support, thanks again.\n. So far I see all this classes using MulticoreExecutor, so it does not apply to all algorithms; but to several:\n./core/src/main/java/smile/association/FPGrowth.java\n./core/src/main/java/smile/classification/DecisionTree.java\n./core/src/main/java/smile/classification/LogisticRegression.java\n./core/src/main/java/smile/classification/Maxent.java\n./core/src/main/java/smile/classification/RandomForest.java\n./core/src/main/java/smile/classification/SVM.java\n./core/src/main/java/smile/clustering/CLARANS.java\n./core/src/main/java/smile/clustering/DENCLUE.java\n./core/src/main/java/smile/clustering/DeterministicAnnealing.java\n./core/src/main/java/smile/clustering/KMeans.java\n./core/src/main/java/smile/clustering/MEC.java\n./core/src/main/java/smile/clustering/SIB.java\n./core/src/main/java/smile/gap/GeneticAlgorithm.java\n./core/src/main/java/smile/manifold/TSNE.java\n./core/src/main/java/smile/regression/RandomForest.java\n./core/src/main/java/smile/regression/RegressionTree.java\n./core/src/main/java/smile/regression/SVR.java\n./core/src/main/java/smile/sequence/CRF.java\n. By looking into the code, looks like property smile.threads can control the number of threads I do want to have. Hence, setting that to one should make the trick.\nPlease just confirm my understanding is correct.\n. Thanks.. ",
    "wmbuchanan": "Thank you!. ",
    "albertmolinermrf": "\nIs this only for regression?\n\nNo, it could be applied to classification too.\n\nThe repetitive always have the same response values?\n\nYes.\n\nThe weight will have big impact on the sampling strategy and approximate split point determination. What's your algorithm on these points?\n\nIt would be applied only by modifying the sampling probability of each data point. This can be easily achieved by:\n\nCreating an array of accumulated weights (if no weights array is passed, all weights are taken to be 1).\nGenerating a random number between 0 and the last accumulated value.\nPicking the data point at the position where the random number above would fall if it had to be inserted into the accumulation array.\n\nThis way, data points with higher weight would have higher probability during sampling.\nSplit point determination would not be affected by this approach. Therefore, loss function of each tree would not change, but loss function of the whole forest would be biased by the weights due to the bias introduced in the sampling.. I agree that lift (and sometimes also leverage, defined as the difference of supports instead of the quotient) is often a useful measure of a rule.\nAmong the multiple ways to provide it, probably the following would be the simplest and would not imply any impact at all in performance or resources for those users who are not interested in it. Its downside is that it is not handy when calling learn(double confidence, PrintStream out), only for learn(double confidence).\nIt requires only adding the following method to smile.association.ARM:\njava\n    public double getSupport(int[] itemset) {\n      return ((double) ttree.getSupport(itemset)) / fim.size();\n    }\nAs AssociationRule already offers the itemsets of antecedent and consequent, as well as the rule's support, given a rule, the new method could be used to obtain the missing supports and to calculate the resulting lift (or leverage).\nDo you think it could be added sometime?. ",
    "dcram": "Thx. ",
    "peter-toth": "There is an optimization in the code to handle the case when the conditional FP-tree is a simple sequence of nodes. In that case we need to add all combinations of the items in the tree as frequent itemset. The original code was wrong with the nested loops, I changed it to a correct recursive solution.. FPGrowth mining is a recursive algo by its nature. But this optimization part can be implemented without recursion using loops and some stack data structure. I'm still benchmarking the non-recursive form, but unfortunatelly it is not obvious which one performs better.. I benchmarked the single path mining on datasets where there there is 24 (or more) nodes in the \"single path\" in the FP-tree. In this case 16.8M frequent itemsets are added to the result. The recursive version you can find in this PR and my non-recursive version (wip version can be found here: https://github.com/peter-toth/smile/commit/079c37dee8b28dfb4d55bf2f6c05b92c7cd763e8) performed very similarly. I also expected the non-recursive version to be faster, but it is not.\nI'm will run some more benchmarks and share the results with you.\nThe advantage of the recursive version is the cleanness of the code.\n. On this branch you can find the JMH benchmarks: https://github.com/peter-toth/smile/tree/benchmark-fp-growth-fix\nIn a propper benchmark the non-recursive version proved somewhat faster so I changed the implementation to that in this PR.\n[info] Benchmark                                          Mode  Cnt   Score   Error  Units\n[info] FPGrowthBenchmark.measureFPGrowth_20_NonRecursive  avgt   15   0,020 \u2592 0,001   s/op\n[info] FPGrowthBenchmark.measureFPGrowth_20_Recursive     avgt   15   0,023 \u2592 0,001   s/op\n[info] FPGrowthBenchmark.measureFPGrowth_24_NonRecursive  avgt   15   0,330 \u2592 0,011   s/op\n[info] FPGrowthBenchmark.measureFPGrowth_24_Recursive     avgt   15   0,375 \u2592 0,014   s/op\n[info] FPGrowthBenchmark.measureFPGrowth_28_NonRecursive  avgt   15   5,233 \u2592 0,102   s/op\n[info] FPGrowthBenchmark.measureFPGrowth_28_Recursive     avgt   15   6,071 \u2592 0,118   s/op\n[info] FPGrowthBenchmark.measureFPGrowth_30_NonRecursive  avgt   15  21,539 \u2592 0,303   s/op\n[info] FPGrowthBenchmark.measureFPGrowth_30_Recursive     avgt   15  24,559 \u2592 0,512   s/op\nPlease see full logs here: https://github.com/peter-toth/smile/blob/benchmark-fp-growth-fix/benchmark/jmh.txt. ",
    "ViaSacra": "Thank you in advance, the project has successfully earned. I created pom.xml and added the following:\n<dependencies>\n        <dependency>\n            <groupId>com.github.haifengl</groupId>\n            <artifactId>smile-core</artifactId>\n            <version>1.5.1</version>\n        </dependency>\n        <dependency>\n            <groupId>com.github.haifengl</groupId>\n            <artifactId>smile-interpolation</artifactId>\n            <version>1.5.1</version>\n        </dependency>\n        <dependency>\n            <groupId>com.github.fommil.netlib</groupId>\n            <artifactId>all</artifactId>\n            <version>1.1.2</version>\n            <type>pom</type>\n        </dependency>\n        <dependency> \n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-classic</artifactId>\n            <version>1.0.13</version>\n        </dependency>\n        <dependency> \n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n            <version>1.8.0-beta1</version>\n        </dependency>\n        <dependency> \n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-jdk14</artifactId>\n            <version>1.8.0-beta1</version>\n        </dependency>\n   </dependencies>. This is a package.\nThe problem was in bad jar, so LAPACK produced incorrect calculations and the matrix was flying out of bounds. After creating Maven and connecting libs through it all works correctly.\nThe only warning that is present is:\nINFO  Factory - netlib module is not available in the classpath. Pure Java matrix library will be employed.\nCan this affect the correctness of the calculations and how can I fix it?. You have classes that implement these models (GaussianVariogram, ExponentialVariogram, SphericalVariogram).\nOnly these classes expect an already counted alpha.\nTell me please, calculate alpha from the points you need the same way as in the class PowerVariogram?\nExample code for the constructor:\n```\npublic PowerVariogram(double[][] x, double[] y, double beta, double nugget) {\n            if (beta < 1 || beta >= 2) {\n                throw new IllegalArgumentException(\"Invalid beta = \" + beta);\n            }\n        if (nugget < 0) {\n            throw new IllegalArgumentException(\"Invalid nugget effect = \" + nugget);\n        }\n\n        this.beta = beta;\n        this.nugget = nugget;\n\n        int n = x.length;\n        int dim = x[0].length;\n\n        double num = 0.0, denom = 0.0;\n        for (int i = 0; i < n; i++) {\n            for (int j = i + 1; j < n; j++) {\n                double rb = 0.0;\n                for (int k = 0; k < dim; k++) {\n                    rb += ((x[i][k] - x[j][k])*(x[i][k] - x[j][k]));\n                }\n\n                rb = Math.pow(rb, 0.5 * beta);\n                num += rb * 0.5 * ((y[i] - y[j] - nugget)*(y[i] - y[j] - nugget));\n                denom += rb * rb;\n            }\n        }\n\n        alpha = num / denom;\n    }\n\n```. That is, ideally, you need to calculate these three parameters individually for your points?\nBut in the constructors of your classes of variograms they are set by default. Only alpha is considered as presented above, but this is not entirely correct, as I understood.\nTell me please, can I read data using Kriging, using only the constructors and submitting the data by default and how much they will be reliable?. Dear @haifengl , please reply to my last message :)\n\n\nThat is, ideally, you need to calculate these three parameters individually for your points?\nBut in the constructors of your classes of variograms they are set by default. Only alpha is considered as presented above, but this is not entirely correct, as I understood?\nTell me please, can I read data using Kriging, using only the constructors and submitting the data by default and how much they will be reliable?. \n\n",
    "kalki123123": "I am pretty new to java as well as smile. Excuse me for silly question. . Its a java project. I have added smile/lib folder as library dependency to my project. \nnow I get same error for StringAttribute as well. \nPlease let me know, in a normal setting, how would you create dataset of custom objects X ? \nX is a separate class defining properties of custom objects.\n. Input data is an int [5000] [400] array. It is a transaction data, with each row corresponding to a unique user transaction. i would like to derive association relation between items (People bought this also bought ... ).\nI am calling ARM which in turn uses FPGrowth. . Sure, It works fine when I try to give more memory with -Xmx option. I am wondering if there is a way to restrict the number of association rules generated. This would actually save time writing to file and reading a rule at a time.  Currently, I restrict results by way of setting high support and confidence.\nThank you for the quick reply.. ",
    "ghostdogpr": "Hi,\nI ran into the same issue.\nFrom what I observed, the problem comes the StringAttribute. As soon as you include one in the list of Attributes, the predictions make no sense (majority rule as you observed).\nAs a workaround, I used a NominalAttribute and gave the whole list of strings in my training data as values. It works well, but I'm not sure this is intended (if it is, it would be nice that the documentation mention it).. ",
    "Pimenta14": "Ok, thanks for the info, I will use something like sliding window or I will study other possibilities. My data set is not very rich in terms of features. \nThey are, for example, simple sensors (like temperature value of a room) so I only have 2 columns - a timestamp and the returned value, aggregated per hour - That is why I'm using the indexes as features :)\n\nBut anyway, is it a normal behaviour or not without the normalization? \nEven if I use something different as features? \nOr this model is not the indicated one for my type of data set?\n\nI guess the point is my 3rd question.\nIf you can answer I would appreciate :) Thanks again.. ",
    "SpertsyanKM": "Is it a distance value to the dividing hyperplane according to the chosen kernel?. It would be very helpful for me at least. By the way, I can try to write it myself, but first of all I need public double predict(T x) method.. I've found now, that there is already implemented Platt scaling algorithm with probability method for SVM. It's my oversight, I haven't understood you correct.\nI think, the issue may be closed. Thanks once again for this helpful library!\nP.S.\nFor those, who are interested in the answer: to get probabilities from SVM classifier you should call \ntrainPlattScaling() method after finish()-ing SVM training and then you can call predict(T x, double[] prob) method instead of predict(T x) to get expected probabilities.. ",
    "scottmutchler": "I see that the instance variables have package friendly scoping.  I can work with that.  Thanks!. ",
    "ryujin88": "It shouldn't be a problem. \nDo you suggest any other way to serialize the KNN model? or should I simply discard the idea of serialization of the KNN model?. ",
    "monapasan": "@haifengl Thank you for the prompt answer!\nImputing the numerical values does not make sense In the case where a missing value actually has a mining, rather than missing at random.\nFor example, in case of nominal features, it's possible to encode a missing value as a new category.\nFor numerical features, it would be very helpful to have a similar way of doing that.\nFor decision tree, it would mean that when splitting on a numerical feature there would be an additional check on equality to the missing value(representation of missing value, e.g. Double.NaN).. @haifengl yeah, that sounds like a good workaround! What is in your opinion would the best way to create range buckets out of the numerical features? One way of doing it can be to choose amount of buckets you want to have and split the value by percentile so every bucket would have the same amount of observations.. Hey @haifengl !\nI have created a PR with the support of missing values.\nOn a split, missing values will be sent to the majority direction. Functionally it's the same as in rpart package with usesurrogate=2, but with a possible split on a missing value.\nPlease let me know if it's possible to merge it at all.\nWould be also happy to see your improvement suggestions!\n306 . @haifengl Thank you for taking a look at my changes!\nIn case there is not missing values in features, it should give exactly the same results. Regarding split metrics, I would love to hear your opinion why they can be corrupted. . This was needed for the application we use the library from as we need all of the information relating the tree. I can change it. \n. The question I also had in mind. I haven't found the formula in literature... Maybe you can give me a hint on where I can read about it? I guess this is not a standard way to compute gain, but more for the perfomance reasons? or am I wrong?\n. @haifengl any idea where I can find out about this formula? . ",
    "akshanshjain95": "Hello @haifengl \nYes, I ran all the test cases. For small data like the hardcoded one and the Pima file, there wasn't any significant improvement, but for a little larger data, i.e., Kosarak file, there was a significant improvement. I have attached the screenshots of the results of Kosarak test case generated from both the current and proposed implementation.\nCurrent implementation(Kosarak) -\n\nProposed implementation(Kosarak) -\n\nThis kind of improvement suggests that for actual data the algorithm would turn out to be much faster than the current version. Plus, the implementation hasn't changed a lot, and as far as I can tell, the implementation is still simple. I think keeping the code almost the same with very few changes and getting a good amount of improvement in terms of performance would be a good enhancement for the algorithm.\nPlease ping back with your thoughts on this, and any improvement that could be done.\nThank you!. Hello @haifengl \nCan you please tell me how many threads are spawning for you? My machine is spawning 4 threads, so maybe there is a difference between the number of threads between our machines.\nConsidering there is a difference in the number of threads, wouldn't it be better for the algorithm to be implemented while keeping in mind machines with low thread-pool, as in my case the performance difference is huge.\n\nBTW, in the future, please don't create both a ticket and PR for the same thing. We can exchange comments in the PR too. Thanks.\n\nI'm sorry about this. I will keep this in mind.\n\nYour change is about building FT-Tree. I don't see there is real difference for that part. The time difference in the frequent item set mining should be caused by other reasons.\n\nIn the FP-Growth algorithm, various conditional FP-Trees are being built when mining the frequent itemsets. In FPGrowth.java, in the grow() method at line 324, the else case spawns an FP-Tree at line\n385. So, a change in the building of FP-Tree would also matter on the performance of the mining algorithm.\nPlease ping back with your thoughts on this, and any improvement that could be done.\nThank you!. ",
    "tkorach": "Update: the problem resolved after aligning the feature arrays (the last dimension of the data variable) to the number of the attributes in the model. \nIt could be useful to 1) elaborate on the data format and/or 2) validate the data before commencing training. \n. ",
    "kno10": "I don't actually use Smile.\nAs this will break APIs when renaming the class, it is probably something you should decide how you want to handle a transition. E.g. by renaming the class to DBSCAN but adding a DBScan legacy subclass instead. Or just postpone this to the next major release.\nJust the documentation change is of course an easy s/DBScan/DBSCAN/ - except for class names.... ",
    "programer92": "What do you mean \"can not Show the clustered data\"?:\nI mean I want to show the result of the clustering in console e.g before clustering  we have these data 0,1,0,2,1,1,2,0,0 \nbut after clustering in I want to have output like below in console:\n0,0,0.0\n1,1,1\n2,2\nor\n0,0,0,0,1,1,1,2,2. What do you mean \"can not Show the clustered data\"?:\nI mean I want to show the result of the clustering in console e.g before clustering  we have these data 0,1,0,2,1,1,2,0,0 \nbut after clustering in I want to have out put like below in console:\n0,0,0.0\n1,1,1\n2,2\nor\n0,0,0,0,1,1,1,2,2\nbut I dont know. how? . I add DENCLUE.getClusterLabel() but  I get the below error\n[Exception in thread \"main\" java.lang.NullPointerException\n    at smile.clustering.DENCLUE.toString(DENCLUE.java:348)\n    at test.ReadStringFromFileLineByLine.main(ReadStringFromFileLineByLine.java:80)]\nit means this line \nsb.append(String.format(\"DENCLUE clusters of %d data points:%n\", y.length)); in DENCLUE.java has error.\nI cant solve it.\nWhat can I do?\n. I add this line after clustering \n System.out.println(vbbx.getClusterLabel());\nbut the result in output is as below:\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nnull\nWhats wrong on it?. ",
    "Ranganaths": "well its not about 10k samples.. basically i was looking for seemless integration with Spark DataSet like how we do with the R. . ",
    "rbouadjenek": "If the minimum number of clusters is set to kmin=10 (we modified a little bit the original algorithm), the algorithm may return a lower number of clusters. This is due to this sorting bug.. yes, you can also change line 118 to \nif (score[i] <= 0.0)\nthanks.. Thank you very much :)\nOn behalf of: @Suri-Xiang, @Nhrishti, @ssanner \nbest,\nreda. ",
    "shikharsharma23": "Yes. I couldnt find any difference. All parameters appear to be same . Okay. Makes sense. Could you advise on what to do in order to attain the same accuracy as h2o and scikit ?. ",
    "JoshCason": "My team at IBM is considering libraries to replace Apache Spark in our pipeline (it was a lot of trouble to maintain). Smile should have been a slam dunk, but proving its adoption in industry has been difficult to accomplish. Such a page would be great for that purpose.. Fantastic, thanks!. ",
    "yzhang672": "How to save the trained RBFNetwork or NeuralNetwork for future usage?  How to set parameters (distance, rbf, centers) for RBFNetwork?  Any example codes in Java?  Thank you very much!. Thank you very much for your reply!! I really appreciate it!\nHave a great rest of your summer!\nBest,\nYilin\n\nFrom: Haifeng Li notifications@github.com\nSent: Tuesday, August 14, 2018 7:37:27 AM\nTo: haifengl/smile\nCc: YILIN ZHANG; Author\nSubject: Re: [haifengl/smile] ## RBFNetwork (#344)\nSee the project websitehttp://haifengl.github.io/smile/classification.html how to train a model.\nval centers = new Array[Array[Double]](50)\nval basis = gaussrbf(x, centers)\nval classifier = rbfnet(x, y, new EuclideanDistance, basis, centers)\nplot(x, y, classifier)\n\nYou can save a model as shown in FAQhttp://haifengl.github.io/smile/faq.html.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/haifengl/smile/issues/344#issuecomment-412856812, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AKr83cUz4j4EzXgzRHRfAxB9TJscWbURks5uQsSHgaJpZM4V5wB_.\n. ",
    "betrw": "I would appreciate elastic-net. Required for my thesis.. ",
    "ishaan007": "The order of features is static, what I meant to convey was that the order of training examples is shuffled so features say F1,F2,F3 are still in same order just that rows/training examples R1,R2,R3.... are now in a shuffled order say R1,R3,R2. And I am getting different decision trees in those two cases. Is this the expected behaviour ?. Yes you are spot on, I am guessing the sorting is an internal mechanism . As far as I know, decision tree should not be affected by  the order of training examples when it tries to find the feature and associated splitValue with maximum information gain. It should be same in both the cases (irresepective of the order) because it is computed on a global set of combination of attribute and its value.. Hi serickso,\nThanks for your comment, Is there any way to set the number of threads from an API instead of jvm parameters? . ",
    "MZinchenko": "As I can see this problem occurs in regression algorithms with gamma (LASSO, RLS, ...).\nAll arguments for Beta.regularizedIncompleteBetaFunction is legal (by contract).\nF.e.\nalpha = -111.0\nbeta = 122.5\nx = 0.20359308593578362\n\u0421all regularizedIncompleteBetaFunction(-111.0, 122.5, 0.20359308593578362) and you see \"The gamma function is negative\".\nThat's very othen case.\nF.e. if we change alpha to -111.1 function works good (because fractional numbers proceed another way). \nI think increasing deviation in iterational or complex calculations like regularizedIncompleteBetaFunction always can lead fail border conditions without some epsilon. > I guess that your sample size is smaller than dimensionality\nYeah, this situation othen for my data (categorical features). \nFact count and dimensions are very close.\nI tried two approaches to solve problem:\n1) put data into regression (f.e. LASSO) as is, and then shrink model\n2) do some PCA projection, and then do regression\nBut first way more accurate at most, so I can not refuse it at now.. Ok. I think we can close this issue.\nI see that my use case is wrong.\nMay be some parameter assertion in LASSO, RLS and other regressions using regularizedIncompleteBetaFunction will be useful?\nOr may be add assertion in regularizedIncompleteBetaFunction for alpha? But assertion in this function is not obvious as for me (alpha can be calculated different way).. ",
    "mckeown12": "I am interested in the $q=.05$ and $q=.95$ cases, giving a sort of confidence interval for prediction.. ",
    "joyang1": "memory is enough, I use 80G memory.. yes,it's ok when I using 10% of the data set.\nI also have a question to ask you, How to use OneHot to train LR model?. ",
    "holysoros": "Alright.. ",
    "beachygreg": "I implemented it in scala for fun if you want want it \n```\nclass ConfusionMatrix(truth:Seq[Int], prediction: Seq[Int]) {\nprivate val labelsToIndex = (truth.toSet ++  prediction.toSet).toSeq.sorted.zipWithIndex.toMap\nprivate val matrixSize = labelsToIndex.toSeq.length\nprivate val positionsAndCounts:Map[Position, Int] = truth.zip(prediction).groupBy(tp => Position(labelsToIndex(tp.1), labelsToIndex(tp._2))).mapValues(.length)\nprivate val matrix = Array.tabulateInt((x, y) => positionsAndCounts.getOrElse(Position(x, y), 0))\noverride def toString(): String = {\n    \"ROW=truth and COL=predicted\\n\" +\n    matrix.map(row => {\n      row.mkString(\"\\t\")\n    }).mkString(\"\\n\")\n  }\n}\ncase class Position(truth:Int, prediction:Int)\n```. The code snippet is not the real case, but I did hit the case. We have 19 categories and it happens on some subsampling test sets that we get this exception.. Cool thanks, it was only in the test set that it did not have all the labels because it was a smaller dataset.. ",
    "inejc": "I went ahead and implemented the above as I needed it in my work.. Would you accept a PR that fixes the current implementations?. Can you please elaborate on the following:\nNote that the whole column average doesn't make sense in machine learning. That means that we assume that all samples follow the same distribution for that attribute.\nWe may be talking about two different things but I can't agree with your comment if I understand it correctly. If we don't assume that the distribution of inputs is fixed than learning itself isn't possible. We expect the future data to be distributed identically like the past. See this comment from StackExchange (point 2 to be more precise).\nI agree that the mean imputation technique where all missing data is replaced by a single value has its own problems though (e.g. reduces variance in the dataset).\nDo you have any references for using row-wise means in the above-mentioned domains?. ",
    "qingfengcss": "\nIt is beyond the java array index range. It is a limitation of JVM.\n\nThank you so much, I have changed the size of matrix,it works. ",
    "michellemay": "According to the change logs of xstream (http://x-stream.github.io/changes.html)\nAll known vulnerabilities has been patched in 1.4.11.1. That was quick!   thanks! . There have been a few comments about the test code not being optimal.\nIt\u2019s only test code meant to help reproduce a problem.  So please be\nindulgent, it\u2019s not production stuff where you codereview thoroughly every\nsuboptimal lines of code.\nBtw, I\u2019m not part of that jira but am interested in the results.\nOn Thu, Dec 13, 2018 at 11:53 Haifeng Li notifications@github.com wrote:\n\nFor the training data matrix, I mean why don't you generate a 3000x36\nmatrix directly. I don't see the benefits of creating 36x3000 and then\ntranspose it. In fact, it may have negative effective depending on the\nrandom number generator. It is possible that you introduce unnecessary\ncorrelations in math.rand(3000) across all your stocks.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/haifengl/smile/issues/390#issuecomment-447041189, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AFP7xYDtQeP4o5-P92JqYPf9Cirxl0FSks5u4oYHgaJpZM4ZDETT\n.\n. \n",
    "stathischaritos": "This is very specific to my use case. \nI have some historical values for weights that I wanted to use and compare with newly trained models. \nI found an alternative way to achieve this, by using the xml serialisation of the models.\ni.e. use write.xstream instead of write in scala \nso now I can see everything inside and create custom models as xml.. ",
    "manlioGit": "Using $HOME/.smile/predef.sc works.. thank you. ",
    "Algo90GT": "Thanks for the information, the results are now better with 2 times Training.\nBut I think my problem wasn't explained enough:\n\nThese Tests are made in a loop at runtime with the SVM for OHLC-Data Prediction.\nIf i run this test again, i get the same results (exactly). Using other Libs doesnt produce different\nresults. So my assumption is, that this has something to do with Smile at runtime. Its like a\ncache is used and the last test affects the new one (always in the same way), but i couldnt figure out whats the cause in the code.\n(Note: One Equity is produced with a new instanced and trained SVM at every moment (f.e. multiple hundred times))\nA solution would be very usefull.\nregards\n. I use the SVM.java in the classification package.\nIts very strange, i extracted the SVM and the used classes into a new project, removed all \"implements Serializable\" and now i get always different results via the stochastic algorithm (Isn't there a parameter to make the SVM predict uniform repeatable? The differences aren't really maintainable respectively to large (like in the image)).\nMy original problem:\nI tried it again with the Smile Project and there is the fault again.\nI think the problem is, that if a new instance of the SVM-class is created, the previous instance (and their training knowledge) affects the actual one. Here it is strange that all tests give the output of the image,\nat first always the blue curve and then the others.\nThe Test of one equity is always made with the exactly same Data Input at every point of time.\nThat means f.e. at date 2010.01.01 the SVM is trained always with the previous dates.\nBut i don't want to study all the code at the moment, rather i use a other lib for SVMs.. Here is a code snippet which explains my problem perfectly:\n` @Test\n    public void Test_Repeatable() {\n        int tests = 1;//1 Test no fault, if greater than 1 test failed\n        double[][] X = new double[][]{{1.0010988645066765, 0.9952449135892911, 1.0015117711697912},\n        {0.9976716338477913, 0.9969132517296433, 1.00189595529537},\n        {0.9918917116756685, 0.9898579715943189, 1.004407548176302},\n        {0.9951060139559271, 0.9929548112991919, 1.0034957043171948},\n        {0.9933795414319108, 0.9924405172064369, 1.0026008944374638},\n        {1.0078886878935842, 0.9985514736885055, 1.0104729132380343},\n        {0.99271284563378, 0.992645371982241, 1.002557251393331},\n        {1.0042072781153568, 0.9981444476163288, 1.0090126830064028},\n        {0.996405969745169, 0.9899895089512336, 1.0024637043554774},\n        {1.0093944869373837, 0.9987229475457496, 1.0127365603814855},\n        {1.0005922057645849, 0.9967630571276674, 1.0039166335794127},\n        {0.9984665568147425, 0.9963076302249723, 1.0030937888825369},\n        {1.0101510875202921, 1.0, 1.0117475093798205},\n        {0.9997266010509189, 0.9961124003094076, 1.001600384092182},\n        {0.9996131347425011, 0.9974186749197921, 1.0037285806714114},\n        {0.9908918026223602, 0.9889834184099022, 1.0019751109331732},\n        {1.0047946127946126, 0.9982895622895623, 1.00593265993266},\n        {1.003216899445085, 0.9973058467147415, 1.006319867034823},\n        {0.9937404804531972, 0.9892312214413596, 1.001997434732651},\n        {1.005808208127458, 0.9988975160498808, 1.0077509999663876},\n        {0.9974201137556059, 0.9920932501888129, 1.000240611152327},\n        {0.9958655257215228, 0.9917913598198791, 1.0008242144833919},\n        {1.0066009043440816, 0.9979073585616623, 1.0092654895838942},\n        {1.0004478699439159, 0.9952472308936677, 1.0019184876702072},\n        {1.0111115565533462, 0.9992783835792175, 1.0118799444088091},\n        {0.9924071712252275, 0.9885347624680328, 1.0005815221341863},\n        {0.9966439824741308, 0.9873816404533289, 1.0},\n        {1.0025321699159506, 0.999732752515467, 1.0077635394256852},\n        {1.004931590837904, 0.9977208053154553, 1.007457365064344},\n        {0.9976590425284994, 0.9968765128354764, 1.0020027454855331},\n        {1.000465301781441, 0.9998737038021803, 1.006461047593725},\n        {0.986419507009501, 0.9847917081921467, 1.0026642747990167},\n        {0.9985787991863456, 0.99391779936147, 1.0039537671924883},\n        {0.991696738727193, 0.9902600249569997, 1.0028127213247446},\n        {1.0014351398411143, 0.9977010556099685, 1.005407280444009},\n        {1.000441471117601, 0.997290046524264, 1.0024858219852617},\n        {0.9930889341479973, 0.9902308214528173, 1.0031296673455532},\n        {1.001804733323307, 0.9978329527898169, 1.003917091644905},\n        {0.9919342731974944, 0.9896960681287787, 1.0005868464509438},\n        {0.9995253295176246, 0.9978811810351944, 1.0037354503178229},\n        {0.9866891496610345, 0.9844867338862314, 1.000447365704257},\n        {0.999888393635647, 0.9947893778642727, 1.0053222285000802},\n        {0.9959189368307232, 0.9952980571348844, 1.0026997802504447},\n        {0.9978215186326702, 0.9958952087419446, 1.0039226674138415},\n        {1.0062197698825543, 0.9992488539758088, 1.0085153282929329},\n        {1.0021139289078034, 0.9992883803676703, 1.0058883036243764},\n        {1.0009607485484342, 0.9992829195616758, 1.003453125217561},\n        {0.9981846761629201, 0.9968005786779435, 1.0056059425773427},\n        {0.9989966205623106, 0.9945580601330872, 1.000675887537888},\n        {0.9996721791715201, 0.9978587022480138, 1.0071981084040704}};\n    double[] y = new double[]{1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0,\n        -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0,\n        -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0,\n        1.0, 1.0, -1.0, -1.0, -1.0};\n\n    for (int i = 0; i < tests; i++) {\n        int[] arr = new int[y.length];\n        for (int j = 0; j < arr.length; j++) {\n            arr[j] = (int) Math.max(0, y[j]);//just change input label -1 to 0\n        }\n        smile.classification.SVM<double[]> SVMS = new smile.classification.SVM<>(new LinearKernel(), 10.0);\n        SVMS.learn(X, arr);\n        SVMS.finish();\n\n        for (int k = 0; k < X.length; k++) {\n            int p = SVMS.predict(X[i]);\n            assertTrue(y[i] == p);\n        }\n    }\n}`\n\nIf int tests=1 there is no mistake. But if tests = 2 for example the weight vector isn't the same any more. If the JUnit-Test is started again, the weight vectors are always in the same order (i have created a function in the Lib for the weight vector).\nw(tests=1): [0.13172843168889337, 0.07509274485847683, -0.018891116784615747]\nw(tests=2): [0.14497829883409175, 0.05154082889292155, -0.031091794359431546]\nw(tests=3): [0.13172843168890402, 0.0750927448584715, -0.018891116784617523]\nw(tests=4): [0.25036854617134363, 0.09089504791907377, 0.05318142326254427]\n...\nThis is really strange. As i said i dont have this problem with other libs in the same test way.\nA solution would be very useful because Smiles seems to be a gettable lib. I use the SVM-class (beside other libs) for my bachelor thesis.\n. Ok thanks, that means i would have to change the random number generator. \nyes, in a mathematically viewpoint. I suppose with better inputs the tests are more equivalent.\ni was a little bit clouded that the results are always the same and not arbitrary random. ",
    "OsamaFadel": "\n@OsamaFadel please try to add -Dsmile.home=<pathToSmile>\\shell\\src\\universal to your demo/test running setting\n\nhow to include this path in netbeans, please.. > Set the workspace to be the home directory of smile.\nsorry for not responding early, i have tried to move the project directory to SMILE home directory, but the issue remains.\n. ",
    "yelangchs": "@haifengl yes, I have try again. I add smile-netlib, it improve the better. \n\nsmile netlike cost about 7s when run 1000 times.\napache cost about 4s when run 1000 times.\n\nbut smile ols is just QR, and the apache ols is the same. why the performance is different. so I try to trace the time.\n\nthe two method cost most of time.\n```\npublic static void main(String[] args) throws Exception {\n            double[][] x = new double[36][];\n            for (int i = 0; i < 36; i++) {\n                x[i] = Math.random(3000);\n            }\n            double[] y = Math.random(3000);\n            Stopwatch stopwatch = Stopwatch.createStarted();\n            double[][] datax = Math.transpose(x);\n            for (int i = 0; i < 10000; i++) {\n                OLS ols = new OLS(datax, y, true);\n                double[] coefs = ols.coefficients();\n            }\n            System.out.println(\"smile ols 100 times cost \" + stopwatch.elapsed(TimeUnit.SECONDS) + \"s\");\n        Stopwatch stopwatch1 = Stopwatch.createStarted();\n\n        for (int i = 0; i < 1000; i++) {\n            OLSMultipleLinearRegression regression = new OLSMultipleLinearRegression();\n            regression.setNoIntercept(false);\n\n            regression.newSampleData(y, datax);\n            double[] coefs2 = regression.estimateRegressionParameters();\n            regression.calculateAdjustedRSquared();\n            regression.calculateRSquared();\n            regression.estimateErrorVariance();\n        }\n        System.out.println(\"smile apache 100 times cost \" + stopwatch1.elapsed(TimeUnit.SECONDS) + \"s\");\n\n}\n\n```\n. > * make sure add smile-netlib to your project.\n\n\nyou may compare apple to orange. Smile doesn't just fit a linear model but also do goodness-of-fit test, variable significance test, etc. A lot of extra computation are involved to ensure the fitting quality. It is beyond the computation of R squared.\nalthough not critical, you better do Math.transpose(x) out of the loop.  It is interesting that you generate a 36 x 3000 array and then transpose it. Why?\n\n\nthis why the 3000 is the stock number, and the 36 is the 31 industry plus 5 risk.. @haifengl @michellemay ok, first, I agree with your point. second, I use the same random data. but the result is different. it is truth. and at last I will test use the real stock data to fit.. > @yelangchs please pay attention to my comments on p-value computation, which is the real difference. Apache Commons doesn't do it. You are comparing apple to oranges.\nyes, you are right , that is my fault.. > @yelangchs This is not about whose fault but to find out the root cause. We all want better performance. But in machine learning, the model quality matters more than performance in most cases. In the case of OLS, we always want to calculate the p-values to check the goodness-of-fit even though it is of high cost. Otherwise, we will blindly apply an ill-trained model as in this example where Apache Commons silently ignores the failure of QR due to the collinearity of variables.\n\nThanks for submitting this ticket.\n\nyes, I agree with you. your team do well.. ",
    "LukeWang163": "\nWhat do these functions do?\n\nIt can convert a column of strings to indices ranging from [0, classes-1], where 'classes' means how many different strings are there in that column. I guess I get your point. But is it possible that I change the attribute from StringAttribute to NominalAttribute?\n. > If you specify a NominalAttribute to read a file, it will automatically build the unique word list and map the strings to int directly. There is no directly function to convert StringAttribute to NominalAttribute currently. In 2.0, we will have an advanced data frame to support all kinds of mappings.\nThanks for the reply. What is in branch v2 seems great. I guess I`ll try for that.. ",
    "MartinSchmitzDo": "Well,\nt-SNE is a dimensionality reduction method which is primarily used to visualize. Yes. But there is no reason not to use it the same way than PCA, as dimensionality reduction in front of a learner.\nBut in my case it's for visualization. I want to project a second data set into the same 2D-Space to see how clusters move in this space.\n. Ehm, can you explain why? Why can't i use the same transformation into a lower dimensional vector space on a different data set?. ",
    "rumeshkrish": "\nYou can set the window size as a workaround.\n\nyes, you mean using zoom option I can set window and get full details for specific region.\nThe image I provided is just point plots. Let say, if I am computing the TSP for this points, all the points are connected in single loop. If I want to analyse more details where I am doing wrong, than I can zoom and find from window, if I want to find the cross join in the graph using some image analysis for large interested area then I want to save this graph image. This has only limited information. So if I can set resolution and see in depth that, I can do what ever the analysis.\n. > What does the \"image resolution\" means here? Image size? PPI?\n\nBesides, this will be a low priority to us. Would you like to take on it? Thanks.\n\nI mean PPI, if there is an option to define the window size and PPI, it will b very helpful. I will check the code and find the way to improve.  . ",
    "rahabwairimu05": "Sorted it out.. ",
    "calvinlfer": "Oh makes sense, thanks for clarifying !. ",
    "cfga": "https://github.com/haifengl/smile/pull/409 my attempt to fix it. "
}
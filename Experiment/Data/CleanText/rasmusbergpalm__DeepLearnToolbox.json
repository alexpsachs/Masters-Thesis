{
    "rasmusbergpalm": "This is the way to do it. I don't really like having the nn_ files lying around though (bloat), but i won't mind having the nn files be formatted your way. I'll delete the nn_ files on my end, and if you want to feel free to format the nn files and send a pull request again. \nAs long as your nn files does what they're supposed to and your commits are an improvement I'll merge :)\nRemember to pull my newest code so you don't make changes twice. (git pull upstream i believe)\nCheers\n. > Would that work for you? Some other way using Git?\nThe beauty of git (version control systems really) is that you can experiment and go crazy as much as you'll like. Destroy everything. When you a) finally make it work or b) realize it'll never work you can just revert to the last commit and everything is fine ('git stash' will destroy all your uncommitted changes). With git there's the added padding that you can create a branch, which is just like a copy of the entire repository but without any additional folders. just 'git branch crazy-stuff', then 'git checkout crazy-stuff' and you have a completely new repo to test your crazy stuff. If it work then merge it back into master and send a pull request.\nI'll be happy to have a skype meeting or similar if you need some explanation. I'm writing my thesis so i'm kinda busy but after 22:00 (GMT+1) i'm free.\n\nWhat do you think of the direction of the README.md so far?\n\nGreat stuff. I just added some myself. In my experience developers want examples up front. Code speaks. Also, see REFS.md.\n\nWould you really be ok with the changes? \n\nSure thing. If nothing else then a consistent style is better than not. (just remember to 'git pull upstream' before you send me a pull request, as I've made some changes to the NN files after you forked)\n. You too. It was nice talking to you\nLooks good. I'll merge. There's no reason for the NN/backup/ dir. If everything is fucked up you can always just revert to an earlier version which is the beauty of SCM.\n. Looking good. Having problems running the saeexamples?\nI'll commit some CNN and CSAE examples.\nActually, all the addpath shenanigans isn't necesary if you just 'addpath(genpath('DeepLearnToolbox'))' as described in the setup. But whatever makes the example run on the first try is a good thing, even if it's a bit redundant.\n. Something like a global setup file would be nice.. but it would still require a line executing it at the top of each example.\n. Hey. I havn't forgotten about this. I'll get back to you :)\nOn Thu, Mar 29, 2012 at 5:12 PM, carlcotner <\nreply@reply.github.com\n\nwrote:\nHi Rasmus,\nI probably didn't quite do this optimally, but I wanted to get it out\nanyway. Feedback desired. I know that the nnff2.m and nnbp2.m files need to\nbe folded back in, and I will do that once you've looked at everything.\nnninit.m is the proposed replacement for nnsetup.m. It has two advantages:\n(1) the struct nn doesn't have to be pre-defined; (2) nnsetup() is not\nidempotent: nnsetup(nn, x, y) != nnsetup(nnsetup(nn, x, y), x, y). (This\ncan come up like this at the command line: nn = nnsetup(nn, x, y); [buch of\nwork]; nn = nnsetup(nn, x, y);  % want to reinitialize; % nn is now\ndifferent!)\nnnexamples.m still needs to be cleaned up one way or another.\nI have the feeling that the optimization abstraction can still be made one\nlevel simpler, but I haven't figured that out yet. Sometimes it takes other\nexamples (or someone else's eyes) to see the whole thing. Passing in nn to\nnnJ.m seems to be the easiest way to let nnJ know about the shape of the\nneural net, even though it is also feels slightly redundant. nnunpackWb.m\nand nnrepackWb.m follow the technique from Ng's class.\nA small pretense of added option passing included in some places.\nThere are a few other small consistency issues that I will take care of\n(and don't quite remember at the moment).\nOh, also perhaps fmincg.m should be removed; it doesn't seem to help\ntraining.\nCarl\nYou can merge this Pull Request by running:\ngit pull https://github.com/carlcotner/DeepLearnToolbox experimental\nOr you can view, comment on it, or merge it online at:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/4\n-- Commit Summary --\n- Small changes to SAE/saeexamples, and started adding \"options\" to\n  NN/nntrain.\n- Minor reformatting.\n- Minor reformatting.\n- Minor reformatting.\n- Merge branch 'master' of github.com:carlcotner/DeepLearnToolbox\n- Merge branch 'master' of\n  https://github.com/rasmusbergpalm/DeepLearnToolbox\n- Lots of changes on experimental branch.\n- Just adding printf.m for to checkout master.\n- Trying to be able to use more advanced numerical methods.\n- First changes to abstract out optimization method. May need one more set\n  of changes?\n- Removed extraneous file.\n-- File Changes --\nA AE/aeeval.m (16)\nA AE/aeexamples.m (191)\nA AE/aeinit.m (19)\nA AE/aesetup.m (9)\nA AE/aetrain.m (9)\nM DBN/dbnexamples.m (2)\nA NN/fmincg.m (175)\nA NN/nnJ.m (47)\nM NN/nnapplygrads.m (9)\nM NN/nnbp.m (14)\nA NN/nnbp2.m (31)\nA NN/nneval.m (13)\nM NN/nnexamples.m (203)\nM NN/nnff.m (26)\nA NN/nnff2.m (26)\nA NN/nngraddescent.m (8)\nA NN/nninit.m (23)\nA NN/nnrepackWb.m (23)\nM NN/nnsetup.m (7)\nM NN/nntest.m (9)\nM NN/nntrain.m (187)\nA NN/nnunpackWb.m (12)\nM README.md (25)\nM SAE/saeexamples.m (10)\nA SAE/saeinit.m (10)\nM SAE/saesetup.m (5)\nM SAE/saetrain.m (21)\nM SPAE/max3d.m (10)\nM SPAE/nbmap.m (28)\nM SPAE/paesdlm.m (2)\nM SPAE/paetrain.m (4)\nM SPAE/spaeexamples.m (19)\nM SPAE/spaesetup.m (2)\nM SPAE/spaetrain.m (4)\nM util/allcomb.m (88)\nM util/printf.m (28)\nM util/rnd.m (4)\nM util/sigm.m (4)\nM util/softmax.m (10)\nM util/visualize.m (63)\n-- Patch Links --\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/4.patch\n https://github.com/rasmusbergpalm/DeepLearnToolbox/pull/4.diff\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/4\n. This looks very exciting. I'll take a more thorough look at it this week :)\nPull requests should be sent when something is stable and ready for prime-time. Stuff you just want me to look at you can just send me a link to your experimental branch, and i can grab the code from there and check it out.\nHere's an idea for a workflow:\nI'll look at implementing a (very basic) test-suite. When you've made something and it passes the tests then it should be sent as a pull request.\nChanges to the code that requires (extensive) changes to the tests should be made in a branch and discussed/approved before being sent as a pull request.\n\nCheers, Rasmus.\n. Can you be more specific?\n. Thanks! Will fix within a couple of days. Sorry about the trouble\n. If we have a batchsize of 100 and a network of size 784 - 80 -10 then \na{1} is 100x784\nW{1} is 80x784\nb{1} is 80x1\nand a{2} should be 100x80 which we can get by (100x784)x(784x80) i.e. a{1}*W{1}'\nWe need to add b to all samples so we flip it so it becomes 1x80 and replicate it 100 times in the first dimension so it becomes 100x80.\nDoes that answer your question?\nCheers, Rasmus.\n. You're welcome\n. AE's can be with or without tied weights, i.e. input weights = transposed output weights. My implementation does not have tied weights.\nDoes that answer your question?\n. :+1: @mohammadabdelaziz very nice. Thanks! You can add your explanation to cnnsetup and send a pull request if you'd like.\n. Yup :+1: If you like you can just do a pull request\n. Thanks! I must have forgot to commit that.\n. Glad you liked it! :) What school project? Can i see?\nAnd awesome that you are going to use it for your thesis. I'll hope for many pull-requests.\nAbout the bug: yea.. annoying that size(ones(2,2,1)) is [2,2]\nI'll be happy to accept a pull request where you put the samples as the first dimension i.e. [1,28,28]\nCheers.\n. Closing this for now. I'll happily accept a pull request that fixes this.\n. Damnit. yes. I'll fix now.\n. Hmm.. The parameters seems to add up for 250x250 input.\n250-5+1 = 244 is the size of a map after the first convolutional layer (type 'c')\n244/2 = 122 is the size of the map after subsampling (type 's')\n122-5+1 = 118\n118/2 = 59\nI'll look at it tonight.\n. Ah. I can't do math it seems (plus, minus, who counts?)\n250-5+1 = 246\n244/2 = 123\n122-5+1 = 119\n119/2 = 59.5 <-- no good.\nLayers must be of a natural number size. I'll add a check in cnnsetup so we can at least fail early and give a better error message.\n. You figured it out :+1: \n. It sounds like you want to do two things:\ntest your performance on a withheld test set. For that you should use \nmatlab\n[er, bad] = nntest(nn, test_x, test_y);\nmake predictions on unseen data (i.e. actually use the classifier)\nwhere you should use\nmatlab\nnn.testing = 1;\nnntest = nnff(nn, unseen_x, whatever_y);\n[~,label] = max(nntest.a{end},2);\ny can be whatever as it's not used for the predictions (it needs to be the size that the train_y was though). I should create a nnpredict method that wraps this nicer. If you're awesome you'll create that method and send a pull request.\n. Ah. Sorry. Try with\n[~,label] = max(nntest.a{end},[],2);\nWhat you are doing is you are taking the max output at the top level for\nall your input.\nSo if at the top you have two outputs nn.a{end}(1) and nn.a{end}(2) the max\noperation will find the one that is most activated by your input, i.e. the\nlabel that the classifier has predicted.\nOn Mon, Mar 4, 2013 at 4:31 PM, beamandrew notifications@github.com wrote:\n\nThanks for the quick reply and thanks again for this toolbox - it's\nallowed me to get my hands dirty with DBN.\nI don't understand the use of the max operator here, and when I run it as\nyou suggested I get an error. If nn.a{end} contains the top node's\nactivation level for each observation after the feedforward pass, why am I\ntaking the max of this component? When I run your suggestion I get this,\nwhich I sure is a simple Matlab error, but Matlab is not my native language\nso I'm not exactly sure what's going on.\n[~,label] = max(nntest.a{end},2);\nError using max\nMAX with two matrices to compare and two output arguments is not supported.\nSo if I replace that with\nlabel = max(nntest.a{end},2);\nit works but my labels are all 2. If I change it to\nmax(nntest.a{end},1), they are all 1, which is in line with my\nunderstanding of max().\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/16#issuecomment-14386219\n.\n. You are welcome.\n\nOn Mon, Mar 4, 2013 at 5:13 PM, beamandrew notifications@github.com wrote:\n\nOh ok, I see and I think I've found my issue. I'm doing two\nclass-classification, which now I'm guessing my response matrix should be a\nNx2 column of observations, right? I've been using 1 column, which means\nI've only been using 1 unit in the output layer, when I actually need two.\nThanks again.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/16#issuecomment-14388672\n.\n. Ah. Your data is not being normalized when you do the last nnff. If you use\nnntest it should be. Also make sure the loss function is dropping while\ntraining or make the learning rate smaller.\n\nOn 04/03/2013, at 18.07, beamandrew notifications@github.com wrote:\nI must be dense, but I can't seem to get this to work, even on the training\ndata used to build the NN. To simplify, I've switch from a DBN to NN for\nthe time being. Here is my setup\n\u00b4\u00b4\u00b4matlab\nnn = nnsetup([Nfea 100 2]); %Nfea is number of columns in train_x\nopts.numepochs = 10; % Number of full sweeps through data\nopts.batchsize = 100;\nnn.normalize_input = 1;\nnn.dropout = 0.5;\nnn.activation_function = 'sigm';\n%train nn\ntwocol = [~train_y train_y];\nnn = nntrain(nn, train_x,twocol, opts);\nnn.testing = 1;\nnntest = nnff(nn,train_x, twocol); %train_x is 24000 by 1025\nnn.testing = 0;\na = nntest.a{end};\na(1:10,:) % I can't even overfit to the training data at this point.\n\u00b4\u00b4\u00b4\u00b4\nThe first ten values of the top layer is:\nans =\n1.0000    0.0000\n0.9998    0.0003\n1.0000    0.0000\n1.0000    0.0000\n1.0000    0.0000\n1.0000    0.0000\n0.9999    0.0000\n1.0000    0.0000\n0.9985    0.0011\n1.0000    0.0000\nThe true values are:\ntwocol(1:10,:)\nans =\n1     0\n 1     0\n 1     0\n 0     1\n 0     1\n 1     0\n 0     1\n 1     0\n 0     1\n 1     0\nNo matter how I configure the NN, the story is always the same, I never get\nany activation for the unit representing the second label. As a sanity\ncheck, I dropped the data I'm using into some of the tree-based methods and\nR and I get decent classification results. Any idea what's going on?\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/16#issuecomment-14391848\n.\n. Glad you made it work. By the way check out 0fb07625655e15e0d22e6ddbaeb1f89bbedcac84. I added nnpredict to make stuff easier. It also does normalization 'the right way' now.\nAs for your theoretical question: It seems that nowadays, if you have enough labeled data you can just use dropout+maxout to get state of the art. If you want to beat state of the art I think maxout+marginalized dropout will be the winner in the coming months.\nHowever, if you don't have lots and lots of labeled data, pre-training seems to be more important.\nBy the way, what are you using the toolbox for? I'm curious :) If you don't feel like disclosing it in public you can send me a message on linkedin.\n. I think hinton proves it in the dropout paper for a single layer model, and\nthen does a little slight of hand; expands the model to many layers and\nshows that it works nicely in practice even though un-prooven. The maxout\npaper is the most thorough on dropout I've read so far. I think it expands\nhintons proofs to multiple layers under certain circumstances if i remember\ncorrectly.\nCheers, Rasmus.\nOn Mon, Mar 4, 2013 at 7:32 PM, beamandrew notifications@github.com wrote:\n\nThanks, that was my understanding as well. I was looking through the\ntoolbox for maxout, but didn't see it. It's pretty new so I wasn't\nsurprised.\nI'm using it for a contest on Kaggle to identify whale calls:\nhttp://www.kaggle.com/c/whale-detection-challenge\nI'm not very concerned about placing high, but instead I'm using it as an\nopportunity to apply a deep learning architecture. I'm a grad student\nmyself and part of my research involves machine learning (both applications\nand method development).\nWhile we've got this conversation going, I've read the dropout paper on\narxiv and watched Hinton's NIPS 2012 talk. He says that dropout is\nequivalent to the geometric mean of all possible models, but I haven't seem\na formal proof of this anywhere. Is there a paper floating around some\nwhere with this proof?\nThanks again,\nAndrew\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/16#issuecomment-14396462\n.\n. @gallamine or @summerstay Whoever sends a pullrequest first with a zscore using bsxfun get's it in the toolbox!\n. Awesome\n. Hey. Looks good! I made a new branch: master-experimental which contains your changes, plus some of mine own to make the examples work.\nI keep getting an error from test_nn_gradients_are_numerically_correct though..\n. I've verified this. It's pretty weird that octave and matlab give different results for this..\n. It has not been fixed yet. \n. For debugging here's a dump of the variables in the test after it completed succesfully. It should be enough to re-create the input and find out where the difference is exactly.\n\nhttps://dl.dropboxusercontent.com/u/1559520/cnn_test_snapshot.mat \n. I'll close this. We have the pull request open\n. Alright. I could not get the momentum normalization to giver better results on MNIST. Here's a couple of plots.\nThe normalized momentum is the red line. The NN's are identical except for the momentum normalization parameter.\nlr = 1, momentum=0.5, 10 epochs, 1000 minipatchsize\n\nlr = 1, momentum=0.9, 10 epochs, 1000 minipatchsize\n\nlr = 1, momentum=0.99, 30 epochs, 1000 minipatchsize\n\nIf I've misunderstood something about the usecases, feel free to make another pull request with a sample of the needed hyper parameters.\n. btw. I did use the learning rate stuff :)\n. Oh. I should remove it then. Or a delicious pull request from a fellow respected hacker might do that... hint ;)\n. Interesting!\nWhy do think this is a better choice?\n. Hmm.. yea it makes sense. Thanks :+1: \n. Fuck yea I'd like it :) :+1:\nYes you fork the project, add the function, and send a pull request!\n. Yes see #28 \n. That looks really good! Actually I think it should be merged with the main nntrain method and controlled with options. That'd also remove the need for the weird \"temp_opt.numepochs = 1\" hack. (I've done this myself too many times to count)\nIf you move this stuff to to nntrain so nntrain becomes [nn, L] = nntrain(train_x, train_y, val_x, val_y, opts) and then in opts have the save stuff. I think the L to pick up from somewhere is a bit too specific and also the fighandle. Just make a new figure.\nLast thing: I think the examples should be moved to a NN example, preferably a new one that shows of how to do cross-validation. They don't really have anything particular to do with SAE imo. \nI can't wait to get a new awesome nntrain :) :+1: \n. Awesome :) I mean the normal validation that you did\nOn 07/03/2013, at 22.31, skaae notifications@github.com wrote:\nNice. I will create a new version of nntrain and move the example to the nn\ntest, I put it in SAE because I was playing around with it.\nBy cross-validation do you mean normal validation as in the plot in my\nprevious post? Or do you mean \"real\" cross validation? In the latter case i\nthink its cleaner to handle cross validation by a separate function.\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/26#issuecomment-14587454\n.\n. Why will this break the SAE examples? \nThe way I see it, we can just check for the number of input arguments to nntrain and if there are only 4 inputs (nargin) (nn, train_x, train_y, opts) then we don't do any validation, and if there is 6 inputs we do validation. \nWith regards to the opts structure I think using defaults in nntrain is better than a specific nnsetupopts() function.\nYou can use 'isfield()' to check whether a struct has fields set.\nDoes that make sense? \n. There's no reason to use vargin, just use\nmatlab\nfunction [nn, L] = nntrain(nn, train_x, train_y, val_x, val_y, opts)\nassert(nargin()==4 || nargin()==6, 'blah blah');\nif(nargin()==4)\n%no validation\nend\nif(nargin()==6)\n%validation\nend\n. Awesome. I'll close this pull request, and then you can open a new one!\n. https://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/DBN/rbmtrain.m#L6 looks right to me. Maybe you have an old version.\n. I'm really busy atm.\nI'll look at this asap :) \n. hi @breznak.\nUnfortunately I never got around to looking at this gigantic pull request. Sorry.\nI've focused on getting an automated test suite to run, and it just went green for the first time. This will make it much easier to evaluate whether PR's should be accepted, as I can have confidence that they don't break anything.\nIf you're still interested in contributing I suggest making smaller pull requests, only introducing one change at a time, and if adding new functionality covering it with a unit test. That'll make it far easier for me to review and far more likely to be accepted.\n. Can you point me to a reference. If you're right and can prove it then I'll happily accept a PR to change it\n. ?\n. Alright. Send a PR and i'll accept it!\n. No, that's a bug. Pull request much appreciated :)\n. Fixed in #52 \n. It's a running exponential average.\nhttp://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average\n. It's a bug. Just remove the '1'.  I'll be happy to accept a pull request\n. They are not tied.\n. http://sachinashanbhag.blogspot.dk/2012/09/setting-up-random-number-generator-seed.html\n. I'd rather just always use imagesc then. With the right options they are almost identical\nimagesc(I)\naxis equal %maybe image, don't remember\ncolormap grey\n. Still interested in fixing this @gallamine?\n. Awesome\n. Nope. It modifies cae.\nWhy would you think its useless?\n. Aha! nicely spotted. I can't remember why that is out-commented.. I'm pretty sure its safe to bring it back in (and commenting out the next two lines obviously).\nYou can do a simple test where you plot MSE vs epochs and see whether it decreases faster with SDLM activated or not. If it does I'll happily accept a pull request that changes it.\n. A RBM uses a sigmoid function as its activation function (also on the input), as such it can only model data in [0;1].\nIf you can add a check that the data is in [0;1] and throw a meaningful error I'd gladly accept it as a pull request. That'd help others.\n. Any progress?\n. Hmm.. have you verified it numerically using caechecknumgrad.m?\n. have you verified it numerically?\n. Do you use matlab or octave? There's a known bug in octaves conv function\nthat causes imprecise results causing the gradient check to fail. See the\npull requests for a partial solution\nOn 26/07/2013, at 12.14, Lin Min notifications@github.com wrote:\n@rasmusbergpalm https://github.com/rasmusbergpalm:\nSorry for the late reply, I was busy with my project and did not check the\nnotifications for a long time.\nI've tried to check the gradients, but it fails for both the original code\nand modified code.\nFirst I thought it should be the code in caeff and caebp, flipall should\nalso be added to these files if we consider the convn function is flipping\nthe kernel.\nI guess I did not figure out how the data is stored in the cell array.\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/42#issuecomment-21609887\n.\n. What OS/program?\n. conv('valid') in octave has numerical instability. We're working on a workaround\n. How slow is the convn(x,k,'same')(ranges) compared to matlabs convn(x,k,'valid') ?\n. In order for me to move on with this in I need to know how slow convn(x,k,'same')(ranges) compared to matlabs convn(x,k,'valid') is, and the out of place files needs to be removed.\n. I'll close this and wait in anticipation of the new one :)\n. It's in the readme to add the toolbox to the path\n. Ok. It's in the readme that you have to add the toolbox to the path.\nOn 28/07/2013, at 23.02, RemonComputer notifications@github.com wrote:\nSolved:\nthe problem lies in that matlab has a function named nnsetup so it call it,\ninstead of nnsetup that lies in the NN Folder, so after adding this line\naddpath('D:\\Programs\\Deep Learning ToolBox\\GitHup Repository To\nModify\\trunk\\NN');\nit can call the desired function and problem solved\nso the final file is:\nfunction test_example_SAE\naddpath('D:/Programs/Deep Learning ToolBox/GitHup Repository To\nModify/trunk/data');\nload mnist_uint8;\ntrain_x = double(train_x)/255;\ntest_x = double(test_x)/255;\ntrain_y = double(train_y);\ntest_y = double(test_y);\n%% ex1 train a 100 hidden unit SDAE and use it to initialize a FFNN\n% Setup and train a stacked denoising autoencoder (SDAE)\naddpath('D:\\Programs\\Deep Learning ToolBox\\GitHup Repository To\nModify\\trunk\\SAE');\naddpath('D:\\Programs\\Deep Learning ToolBox\\GitHup Repository To\nModify\\trunk\\NN');\naddpath('D:\\Programs\\Deep Learning ToolBox\\GitHup Repository To\nModify\\trunk\\util');\nrng(0);\nsae = saesetup([784 100]);\nsae.ae{1}.activation_function = 'sigm';\nsae.ae{1}.learningRate = 1;\nsae.ae{1}.inputZeroMaskedFraction = 0.5;\nopts.numepochs = 1;\nopts.batchsize = 100;\nsae = saetrain(sae, train_x, opts);\nvisualize(sae.ae{1}.W{1}(:,2:end)')\n% Use the SDAE to initialize a FFNN\nnn = nnsetup([784 100 10]);\nnn.activation_function = 'sigm';\nnn.learningRate = 1;\nnn.W{1} = sae.ae{1}.W{1};\n% Train the FFNN\nopts.numepochs = 1;\nopts.batchsize = 100;\nnn = nntrain(nn, train_x, train_y, opts);\n[er, bad] = nntest(nn, test_x, test_y);\nassert(er < 0.16, 'Too big error');\n%% ex2 train a 100-100 hidden unit SDAE and use it to initialize a FFNN\n% Setup and train a stacked denoising autoencoder (SDAE)\nrng(0);\nsae = saesetup([784 100 100]);\nsae.ae{1}.activation_function = 'sigm';\nsae.ae{1}.learningRate = 1;\nsae.ae{1}.inputZeroMaskedFraction = 0.5;\nsae.ae{2}.activation_function = 'sigm';\nsae.ae{2}.learningRate = 1;\nsae.ae{2}.inputZeroMaskedFraction = 0.5;\nopts.numepochs = 1;\nopts.batchsize = 100;\nsae = saetrain(sae, train_x, opts);\nvisualize(sae.ae{1}.W{1}(:,2:end)')\n% Use the SDAE to initialize a FFNN\nnn = nnsetup([784 100 100 10]);\nnn.activation_function = 'sigm';\nnn.learningRate = 1;\n%add pretrained weights\nnn.W{1} = sae.ae{1}.W{1};\nnn.W{2} = sae.ae{2}.W{1};\n% Train the FFNN\nopts.numepochs = 1;\nopts.batchsize = 100;\nnn = nntrain(nn, train_x, train_y, opts);\n[er, bad] = nntest(nn, test_x, test_y);\nassert(er < 0.1, 'Too big error');\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/46#issuecomment-21689749\n.\n. Hmm... That's annoying.. There's a known bug in Octave's conv function which causes numerical instability, but SAE don't use that so it should work... \n. you probably need to addpath(genpath()) as noted in the readme\n. Yes that is possible, no they don't have to be images. Actually it's only the CNN/CAE stuff that expects it to be 2D.\nJust create your input like: \nN,M = size(train_x). N = number of examples, M being the size of the time window. \nAnd output would be N,1 = size(train_y)\nnn = nnsetup([M  1]); <-- since you're only predicting one output\n. monitor the MSE. Is it falling?\nTry a higher/lower learning rate, and way way more epochs.\n. Ah, wait I got it. It's because you need an output for each class, i.e. nnsetup([2 5 2]).\nTake a look at nnpredict, it'll return the column with the highest value, if there's only one column it's always 1.\nEach column of the output must be 0 or 1 depending on which class.\nAlternatively, you can keep your one column and just look at the output layer directly, i.e. nn.a{3}\n. yes\nnn = nnff(nn, x, y)\ndisp nn.a{3}\ny can be anything, just use the training y. It's not used.\nnn.a{3} is the activation of the last layer, i.e. the output layer\n. Cool. I'd love to accept a pull request where you add a regression example, so it's more obvious for other users.\n. DBN's are unsupervised. See the examples on how to unroll them to NN's\n. It's a bug, you should not L2 regularize the bias term in general. No need for the condition.\n. Thanks! \n. Clever... Can you please add a unit test (or example) to ensure that the calculations are correct?\n. Lots of good stuff, but too much at once! and too untested! Please read https://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/CONTRIBUTING.md \nand resubmit your pull requests if you are still interested.\nI'll not accept pull requests that use .mex or .cpp files, only pure .m.\nIt should be possible to do max-pooling very efficiently by calculating a set of indexes that will transform the stacks of images into stacks where each row is what you want to take the max across, and then simply using X*(X==max(X)), and then transforming back into stacks of images. See max3d for a 3d implementation. You would do the same for 2d.\n. There are many types of noise. The original DAE used zero masking. feel free to experiment with different noise regimes\n. Yea.. you're right. I wrote it during my thesis mostly. short variable names were sassy and smart and fitted nicely with my equations. In general I'm trying to make it more user friendly, for instance by using better variable names, but realistically it'll take a long time. I can only encourage you too read the basic papers on the methods, and correlate to my implementation. If you're really really nice you'd update the code as you go along and submit a pull request\n. use visualize(ff') and please submit a pull request to fix the error\n. please use camelCasedDescriptiveVariableName, so for instance it = iterations, and v_begin = inputActivations or visibleActivations.\nI know the current variables are not in that style, but I'm trying to change that. slowly.\n. does the logistic_cdf works in octave as well as matlab?\n. Awesome. Thanks!\nIf it works in octave it probably also does in matlab.\njust push to the branch that you made your changes to and they should be\nreflected in the pull request.\nOn Wed, Oct 16, 2013 at 10:02 AM, MaxG87 notifications@github.com wrote:\n\nI will use camel case variable names if you want me to. I don't know,\nwhether \"logistic_cdf\" works in Matlab as well. I thought so but can't test\ncurrently.\nHow do I submit the corrected version of my pull request? Shall I submit\njust another pull request?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/61#issuecomment-26399329\n.\n. Awesome! Thanks!\n. Hey. Matlab users have reported problems using the code since this pull request was merged so I've reverted all the commits.\n. Specifically the errors were:\n\nThe comments '#' in:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/DBN/dbnsetup.m\nAnd the do while / ++ operators in use here:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/DBN/rbmtrain.m\n. Every change to the toolbox is tested. All tests and examples pass on ubuntu with octave 3.6.4, except for the CNN's which required octave 3.8.0 or higher, which is not released yet. If you can reproduce the errors you've seen with the latests unmodified code, I'd love to hear more about it.\n. Hi @MaxG87. I've just gotten all the automatic tests to run (including a fix for octave with the help of @albertoandreottiATgmail. If you're still interested in contributing I'd advice you to split up your pull requests into small functional units. That way it'll be much easier for me to review and hence accept.\n. Please split your pull request into several small independent changes. It's really hard to understand the complete changeset in one go. Also, please describe your changes in more detail, and ensure that all tests pass, and that all new functionality is convered by at least one test\n. @sdemyanov Can I talk you into splitting this into PR into a couple of pull requests.\nI'm very interested in the bug you report in the backprop. What specifically is the bug and can you prove it?\n. Hmm.. good point. I followed hintons paper in REFS.md when implementing it. Both will probably work fine.\nGive it some experimentation and see which works best.\n. Hmm, actually that paper also says:\n\"For the last update of the hidden units, it is silly to use stochastic binary states because nothing\ndepends on which state is chosen. So use the probability itself to avoid unnecessary sampling noise.\"\nSo, nice catch! I'll be more than willing to accept a pull request that changes the default behavior to simply use sigm()\n. The build failed: https://travis-ci.org/rasmusbergpalm/DeepLearnToolbox/builds/14332866,\ndoes running runalltests.m work locally?\n. AWESOME :)\n. Hmm... well, I think the honest answer is that I simply wrote the code one method at a time, as I was reading up on them.\nCurrently the architecture of the toolbox is very much split into whole methods (NN, CNN, etc.) It could definitely be interesting to define each method using some kind of 'layer' interface and then be able to mix/match layers.\nMatlab is not a very efficient language for that architecture though as it is not object oriented. I would suggest looking at Theano or Torch7 if you want something like that.\n. Yes, true. Last time I looked though, octave and matlab were not really in agreement on how to do OOP. It might have become better though!\nThat does sound pretty cool, and I'd love to see a small sample implementation. Some of my concerns in no particular order:\nSpeed\nCode complexity (big switch cases / if elses)\nBig change!\nmixing and matching incompatible learning algos\nAble to create architectures that does not really make sense, i.e. conv on non spatial data\nSome of the benefits:\nMight make code more testable!\nIf done right could show the user which endpoints fits together\nIt's not a project I feel strongly for though, I kind of like the functional implementations of the current methods. I think people primarily use the toolbox for proof of concept stuff, and using it as a reference implementation when reading papers. I fear that might become harder if the methods were more entangled. I look forward to being proven wrong though,\n. I'll close this @rmanor. Feel free to open a pull request with your proposed changes or another issue if you have further questions or comments.\n. addpath(genpath('/Path/to/DeepLearnToolbox')) :)\n. Ah. Crap. My mistake. Ran it in Octave. Thanks. I'll happily accept a PR\nOn 24/11/2013, at 18.00, Krzysiaczek99 notifications@github.com wrote:\npath was set otherwise I would not be able to call the functions. It look\nyou have obvious bugs in this part of the code\nassert(all(x>=0) && all(x<=1), \"all data in x must be in [0:1]\");\nMATLAB is not using \" but ' for text !!!!\nit was a reason of this bug\ndbn = dbntrain(dbn, train_x, opts);\nError: File: rbmtrain.m Line: 3 Column:\n36\nDid you ever run it ???\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/68#issuecomment-29159788\n.\n. Fixed in 4ae73b19. Thanks for letting me know.\nI'll setup a check that there are no quote characters in the repository in the automatic build job to avoid similar mistakes in the future.\n. if there are M classes then there must be M output variables.\nSo if you have a binary class, then you must use:\nnn = nnsetup([size(xtrain,2) size(xtrain,2), 2]);\nand ytrain must be Nx2\n. trainy needs to have as many columns as there are output options (classes). So for a binary class problem trainy must have two columns. The sum of all rows must always be one, that is, either the first column must be a one and the second column a zero, meaning, it was class 1, or vice versa meaning it was class 2.\ne.g.\n[\n1, 0\n0, 1\n0, 1\n1, 0\n1, 0\n1, 0\n0, 1\n]\netc.\n. Paste some of your trainx, testx, trainy and testy here.\nOn Thu, Nov 28, 2013 at 10:23 PM, Krzysiaczek99 notifications@github.comwrote:\n\noutput is just column vector filled with 1\n1\n1\n1\n1\netc\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/69#issuecomment-29485823\n.\n. what is sum(ytrain1) and sum(ytest1) ?\n\nOn Fri, Nov 29, 2013 at 12:45 PM, Krzysiaczek99 notifications@github.comwrote:\n\nxtest 1440x170 just 2 rows\n0 0 0 0 0 1 1 1 1 1 0.218547300472350 0.225937358052214 0.216674597059053\n0.227945339431778 0.215153242184393 0.232003580347566 0.213872705753537\n0.236579612797980 0.212182452883033 0.243243330424282 0 0 0 0 0 0 0 0 0 0 0\n0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0.533333333333284 0.356432767281168 0.450007656795560\n0.502875538475751 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0.242677824271872 0.531250000000759 0.516135099553724\n0.478465434292333 1 1 1 1 1 1 1 1 1 1 0.222542452095340 0.220399647944716\n0.221510447370142 0.219182395265049 0.202468488630368 0.220306710191902\n0.218403786269454 0.214079443890809 0.2073282188657 62 0.197718920079906\n0.253968253968103 0.117647058823481 0.401889583041499 0.173353188821610\n0.188235294117570 0.108843537414918 0.325665569634448 0.258050048644223\n0.302642836771541 0.313105339847422 0.385941763206152 0.434534442686124 1\n0.482758620689655 0 0 0.800000000000000 0.233515692484883 0.225147613977348\n0.238281249999999 0.231226053639845 0.301481127568084 0.317084942084939\n0.326128722382317 0.336146272855128 0.328941176470580 0.332712022367191\nxtrain 20000x170 just 2 rows\n1 1 1 0 0 1 1 1 1 1 0.865834623192517 0.869917195396788 0.864766731642010\n0.870934784521438 0.862723005967957 0.871849972786718 0.859456053150873\n0.872814536708904 0.854528933794986 0.874123920184252 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0.510204081632611 0.421482225462971 0.562895808000664\n0.732766513453159 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0.214856230036509 0.364406779661790 0.490646746266447\n0.498307696731945 1 1 1 0 0 1 1 1 0 0 0.867405359090200 0.868207451836880\n0.869456998755480 0.867677871528134 0.863635661024698 0.867957405495072\n0.868747544950211 0.868254556980696 0.8677155207534 43 0.867750538758196\n0.292307692307540 0.471264367816036 0.391789468517534 0.690796336936732\n0.292307692307540 0.530612244897820 0.485168748022951 0.771373684052620\n0.304781823367239 0.335047570069438 0.414195404869440 0.455072540858243 0\n0.833147942157953 0.0869565217391304 0.542372881355932 1 0.864958249352145\n0.862452811925273 0.868807164634146 0.865613026819923 0.321070234113703\n0.315637065637064 0.312680115273764 0.331458040318794 0.317647058823521\n0.319664492078278\n1 1 1 0 0 1 1 1 1 1 0.866006185749419 0.870192975585101 0.864912014769438\n0.871236322867946 0.862841680014431 0.872176071257371 0.859547931897117\n0.873164661680746 0.854594032655948 0.874496728152717 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0.376623376623208 0.418341380915570 0.570430033749145\n0.728609744228968 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0.300536672633351 0.365572315883730 0.489469652649986\n0.496260150482426 1 1 1 0 0 1 1 1 0 0 0.866548527808070 0.868037943736349\n0.869401323115209 0.867687754848477 0.863737814651655 0.867686066862982\n0.868626223274084 0.868244179420492 0.867764199435538 0.867842944777933\n0.292307692307540 0.386666666666521 0.341300768855299 0.633025786902175\n0.292307692307540 0.530612244897820 0.417620942481458 0.750442389395922\n0.303355832303451 0.335004714151029 0.414016489988198 0.455072540858243 0\n0.833147942157953 0.0869565217391304 0.559322033898305 1 0.865246184854592\n0.862259219823830 0.868997713414633 0.865613026819923 0.319159101767797\n0.326254826254819 0.324687800192116 0.331926863572421 0.317176470588234\n0.311742777260012\nytest 1440x1\n0\n0\n0\nytest1 1440x2\n1 0\n1 0\n1 0\n1 0\n1 0\nytrain 20000x1\n1\n1\n1\n1\nytrain1 20000x2\n0 1\n0 1\n0 1\n0 1\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/69#issuecomment-29511603\n.\n. What is the sum column wise?\n. ok, that's really weird.\n\nI'm clueless. I'd have to play with the data myself. You can email it to me, but I don't have time to look at it for the foreseeable future though...\nI'd advice playing with some toy data sets until you figure out what is wrong. Simplify the problem.\nAs for the answer being 1440x1: it returns the maximum index\n. Yes it'll always be Nx1. See the code.\nYou can find my email in the readme\n. Cool!\nOn Fri, Nov 29, 2013 at 2:54 PM, Krzysiaczek99 notifications@github.comwrote:\n\nI think it works !!! The data what i was trying was very hard so it\nreturned all 1s which corresponds to class 0 for me, Than I tried another\ntest data set and it returns both classes !!!\nFor 1st data set a lot of algorithms returns class 0 (so 1 as an class\nindex) for all test instances so it is not surprise for me.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/69#issuecomment-29517290\n.\n. W.r.t. passing numerical gradient checks:\nFor mean pooling it's question of distributing the delta evenly, so no problem.\nFor max pooling it should actually just work without any changes, because the units that were not the max one will be zero, and as such backprop will only change the max one.\n\nW.r.t max3d: \nin short what it does is:\nreshape that 3d data into a 2d form where each column represents the volume over which i max pool.\nThen i take the max of each column, and multiply it with the original column, in effect setting all values that were not max to zero and the max value to the original value.\nThen i transform the data back to the 3d representation.\nThe tricky parts of max3d is to effeciently transform between the forms. Take a look at https://github.com/stevenwudi/DeepLearnToolbox/blob/master/CAE/scaesetup.m#L42 for the method that creates the indexes to get the 2d form given the 3d form\n. I'll close this @stevenwudi. Feel free to open a new issue if you have futher questions\n. Crap. Yes. Thanks! I'll use x(:)>=0 though\n. fixed\n. What do you mean I don't use it?\nSee \nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/CNN/cnnff.m#L17\nand \nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/CNN/cnnapplygrads.m#L8\n. Good point! Do you have working code for including the bias in the subsampling layers?\n. Ok. Good to know. Thanks!\n. Funky.. #76 implements non-square kernels. Did you guys coordinate?\nAnyway, same comments apply.\nmake scale a vector, don't be backwards compatible, fix examples to use new style and add assertions in cnnsetup to verify correct scale (i.e. positive integer vector) in order to fail fast. \nAdd a test that using non-square and square scale works as intended. (i.e. the outputmaps are the expected size, for a known input the computed output is correct, etc.)\nSorry about the extra feedback btw. I was really busy last time I reviewed.\n. Any updates on this?\n. What's the status?\n. Looks good. Some points: \nmake kernelsize a vector, don't be backwards compatible, fix examples to use new style and add assertions in cnnsetup to verify correct kernelsize (i.e. positive integer vector) in order to fail fast. \nAdd a test that using non-square and square kernels works as intended. (i.e. the outputmaps are the expected size, for a known input the computed output is correct, etc.)\n. Yes! These should definitely be implemented and tested separately.\n. What's the status on this one?\n. @albertoandreottiATgmail I decided to revert your last pull request and replace it with a check that we have octave 3.8 or above (not released yet I know). Even with your PR CNN's still didn't work, because of the other places convn('valid') was used, which is why it needed 7 epochs instead of 1 to get below 12% error (which took the test time to 45 minutes, which is also too much)\nThe alternative would be to replace all calls to convn('valid') with a substitute function, but that just gets ugly and slow really fast.\nIt's sad but I don't think there's really anything to do but to just wait for octave 3.8.0 to be released.\n. That the method can reach good results is not a guarantee of correct\nimplementation. I've found out the hard way.\nWell, the conv operation is a well defined operation, and the results\nshould be the same within machine precision. Basically we know that the\nconv operation is doing something wrong, so we should not allow people to\nuse a method that is provably wrong.\nHopefully octave 3.8.0 will be released shortly.\nOn Wed, Dec 4, 2013 at 3:07 PM, albertoandreottiATgmail \nnotifications@github.com wrote:\n\nHey Rasmus, where have you seen it fail? I used this very same\nimplementation to train a deeper network, with lots of data, and very good\nresults.\nTwo different implementations are supposed to lead to different behaviours\nregarding numerical stability. That's why you see different convergence\nspeed. Anyways it's your call ;-).\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/77#issuecomment-29806843\n.\n. The problem was not with convn_valid.\nThe problem was that we were not using it all the places, so we had places\nwhere we used convn(a,b,'valid') still which is provably wrong.\n\nOn Thu, Dec 5, 2013 at 4:29 PM, albertoandreottiATgmail \nnotifications@github.com wrote:\n\nI agree with you, good results for a certain number of inputs does not\nmean correctness.\nMy question is: have you performed some tests on the convn_valid()\nfunction that spots a problem with the implementation? Because if that's\nthe case I would like to know as I'm planning to keep using it until the\nnew Octave is released.\nBtw, I'm not using conv() for the implementation, I'm using conv2(). How\nbig is the error that you see and on which sitaution? do you have a test\ncase? I mean, implementing convolution from scratch should not be a problem\nfor us. We just need a few test cases to tests for numerical stability.\nThanks for keeping an eye on this,\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/77#issuecomment-29906923\n.\n. Thanks!\n. Try experimenting with the nn.learningRate.\n. Look at the examples. It's all there is.\n. Please merge this into your other pull request, and see the comments there.\nAlso I think happynears suggestion should work, so please try that.\n. NICE!\nI guess this also requires changes to the backprop derivatives?\nAlso, please add the new activation functions to tests/test_nn_gradients_are_numerically_correct and verify that the derivatives are correct.\n. The DBN implementation is based on binary input/output, so unless you implement something new you can't use it for anything else than binary input/output\n. thx!\n. I'm not doing a whole lot of active development on this toolbox these days. Most changes are because ppl make pull requests and prove them to improve something.\nSo, no, but I'd love you to make a PR showing it's awesome!\n. You're right. The loss function should reflect the L2 weight penalty.\nHowever, if you look at nnbp you'll see that we don't actually use nn.L for any of the derivatives, which is why the backprop is correct.\nI'd happily accept a pull request (with a test) where you add the various regularization penalties to the loss function.\nThanks!\n. Yea! Nicely spotted!\nI simply don't have a test case where I use L2 regularization.\nAgain, I'd be happy to accept a pull request if you're up for it :)\n. You can look at nn.a{max}. It contains the output activations, i.e. prediction probabilities. Is this good enough?\n. see https://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/tests/test_example_SAE.m on how to unfold your SAE into a NN. from there on it should be fairly easy\n. See http://cogprints.org/5869/1/cnn_tutorial.pdf which is the one i followed when implementing it.\n. In caeup.m we evaluate the 'up'-step i.e. encode inputs into a hidden layer.\nIn caedown.m we evaluate the 'down'-step, i.e. decode inputs from their hidden layer representation,\n. Awesome!\nOnly one small comment then I'll merge.\nI'll look at the ReLU PR now\n. If you're referring to the #85 and #86 then it's because there are unresolved issues with those PRs, see the comments. If you'd resolve the comments then I'd be happy to merge them. \n. Hmm, I'm pretty sure it's correct.\nThe sigm function is applied at the output layer.\n\nDoes cnnnumgradcheck work if you change it?\n. Ok. I think I got that formula from a paper by Yann Lecun. IMO, it's just one of the things you need to vary when playing with these nets.\nIf you can prove that your initialization is superior (theoretically or by experiments), I'll happily change the formula \n. The examples work in octave 3.8. I'm guessing you've changed something and have mismatching dimensions in your data somehow.\n. Use a linear output function see https://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/NN/nnff.m#L43\n. Use linear output neurons and update the derivatives backwards. Sorry\nthere's no easy way ;)\nDen 24/11/2014 04.12 skrev \"Li Yanghao\" notifications@github.com:\n\ncan you suggest how to modify the code inside CNN folder for regression?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/104#issuecomment-64148318\n.\n. Duplicate. Closing.\n. I've gotten weights like that. Use sparsity.\n. Cross validation\n\nOn 15/07/2014, at 14.58, mnaetr notifications@github.com wrote:\nOk. How can I correctly choose the values of the different hyperparameters\n(learningRate, sparsityTarget, nonSparsityPenalty and weightPenalty)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/106#issuecomment-49027423\n.\n. Hmm. No idea. That would totally saturate all the sigmoids, which would drive the delta to zero, so I cannot make any sense of it.\n. github markdown can't parse it.\n. Thanks a lot @pakozm :)\n. Thank you for the kind words @tambetm :)\nI'm glad people find it useful.\nGood point about MatConvNet.\n. Thank you for all the kind words :)\nI'm super grateful that I've been able to contribute, however little, with this toolbox.\nAs you might have guessed I'm not using this toolbox anymore, or Matlab for that matter, which is why I don't have the time or incentive to maintain it. FYI I'm using Lasagne instead. Although I think Keras is actually better nowadays.\nA new active maintainer would definitely be nice. As @oxinabox points out, anyone is free to fork the project and develop on it. I don't think appointing anyone in particular would be a good idea. I'd rather let the most popular fork win naturally. \n. Why does it require nn.it_no_momentum to be bigger than 0?\n. Can you provide a reference for this momentum normalization technique?\n. Ok. Read it. Looks good.\nI'm a bit reluctant to merge this in without verifying that this will have the desired effect.\nWould it be possible to create a test that verifies that using this technique does indeed results in faster convergence? Maybe throw in a couple of plots that shows the error rates going down (also for a validation set) faster?\n. This file looks out of place\n. this file looks out of place\n. The comments does not mention how fast the matlab implementation is...\n. shouldn't it be 'it' here?\n. How can this be the only place it needs changing? It's used in a lot of places\n. please calculate ones(xscale, yscale) / (xscale*yscale) in cnnsetup, put it in the net somewhere (net._meanFilter or similar), use that and remove the comment to replace with var\n. Please properly remove all references to nn.scaning_learningRate.\nYour new stuff depreceates it.\n. ",
    "carlcotner": "Hi Rasmus,\n\nI don't really like having the nn_* files lying around though (bloat) ...\n\nYes, I understand your feeling (and I agree with it too).\n\nAs long as your nn* files does what they're supposed to ...\n\nI was afraid of this, at least at the very beginning, and would like\nto be able to experiment for a (very) short time and ask you questions\nfirst. Would that work for you? Is there another way to it while not\ndisturbing the nn* files immediately, and then I can compare their\noutputs? Putting the nn_* files in a subdirectory? Some other way\nusing Git? On my end, a day or two would be enough time as long as I\nam able to get your feedback/answers on a couple things.\n\nand your commits are an improvement I'll merge :)\n\nI am completely happy to let you be the judge :-), and also to include\nanything you actually like without including what you don't like.\nTwo more questions. What do you think of the direction of the README.md so far?\nSecond, I did go through and reformat the nn_ files according to my\nstyle (not because it's better, just because I'm used to it and am\nusing the process to understand and become familiar with your code).\nWould you really be ok with the changes? (Please, I don't mind if the\nanswer is no - it's not important to the code, of course.) The\nformatting changes are almost all just adding spaces around Matlab\noperators.\nAfter this I just have a couple interface questions (and then only\nalgorithm questions that I don't fully understand yet).\nThanks,\nCarl\nOn Wed, Feb 29, 2012 at 4:02 AM, rasmusbergpalm\nreply@reply.github.com\nwrote:\n\nThis is the way to do it. I don't really like having the nn_* files lying around though (bloat), but i won't mind having the nn* files be formatted your way. I'll delete the nn_* files on my end, and if you want to feel free to format the nn* files and send a pull request again.\nAs long as your nn* files does what they're supposed to and your commits are an improvement I'll merge :)\nCheers\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/1#issuecomment-4234015\n. > Actually, all the addpath shenanigans isn't necesary if you just 'addpath(genpath('DeepLearnToolbox'))' as\ndescribed in the setup. But whatever makes the example run on the first try is a good thing, even if it's a bit\nredundant.\n\nYes - I didn't actually have trouble running it, I just wanted to get\nrid of the warning messages. Perhaps we should remove the individual\naddpath()s and  create a global 'setup file' that does\n'addpath(genpath('DeepLearnToolbox'))' (perhaps with the\nfileparts(mfilename('fullpath')) stuff, if that's necessary to make it\ncompletely full-proof) - what do you think?\n\nHaving problems running the saeexamples?\n\nOn my machine I run out of memory. :-(\nCarl\nOn Thu, Mar 8, 2012 at 6:02 AM, rasmusbergpalm\nreply@reply.github.com\nwrote:\n\nLooking good. Having problems running the saeexamples?\nI'll commit some CNN and SAE examples.\nActually, all the addpath shenanigans isn't necesary if you just 'addpath(genpath('DeepLearnToolbox'))' as described in the setup. But whatever makes the example run on the first try is a good thing, even if it's a bit redundant.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/3#issuecomment-4389426\n. That's fine, Rasmus. I hope you're celebrating finishing dissertation!\n\nCarl\nOn Sun, Apr 1, 2012 at 5:14 PM, rasmusbergpalm\nreply@reply.github.com\nwrote:\n\nHey. I havn't forgotten about this. I'll get back to you :)\nOn Thu, Mar 29, 2012 at 5:12 PM, carlcotner <\nreply@reply.github.com\n\nwrote:\nHi Rasmus,\nI probably didn't quite do this optimally, but I wanted to get it out\nanyway. Feedback desired. I know that the nnff2.m and nnbp2.m files need to\nbe folded back in, and I will do that once you've looked at everything.\nnninit.m is the proposed replacement for nnsetup.m. It has two advantages:\n(1) the struct nn doesn't have to be pre-defined; (2) nnsetup() is not\nidempotent: nnsetup(nn, x, y) != nnsetup(nnsetup(nn, x, y), x, y). (This\ncan come up like this at the command line: nn = nnsetup(nn, x, y); [buch of\nwork]; nn = nnsetup(nn, x, y); \u00a0% want to reinitialize; % nn is now\ndifferent!)\nnnexamples.m still needs to be cleaned up one way or another.\nI have the feeling that the optimization abstraction can still be made one\nlevel simpler, but I haven't figured that out yet. Sometimes it takes other\nexamples (or someone else's eyes) to see the whole thing. Passing in nn to\nnnJ.m seems to be the easiest way to let nnJ know about the shape of the\nneural net, even though it is also feels slightly redundant. nnunpackWb.m\nand nnrepackWb.m follow the technique from Ng's class.\nA small pretense of added option passing included in some places.\nThere are a few other small consistency issues that I will take care of\n(and don't quite remember at the moment).\nOh, also perhaps fmincg.m should be removed; it doesn't seem to help\ntraining.\nCarl\nYou can merge this Pull Request by running:\ngit pull https://github.com/carlcotner/DeepLearnToolbox experimental\nOr you can view, comment on it, or merge it online at:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/4\n-- Commit Summary --\n- Small changes to SAE/saeexamples, and started adding \"options\" to\n  NN/nntrain.\n- Minor reformatting.\n- Minor reformatting.\n- Minor reformatting.\n- Merge branch 'master' of github.com:carlcotner/DeepLearnToolbox\n- Merge branch 'master' of\n  https://github.com/rasmusbergpalm/DeepLearnToolbox\n- Lots of changes on experimental branch.\n- Just adding printf.m for to checkout master.\n- Trying to be able to use more advanced numerical methods.\n- First changes to abstract out optimization method. May need one more set\n  of changes?\n- Removed extraneous file.\n-- File Changes --\nA AE/aeeval.m (16)\nA AE/aeexamples.m (191)\nA AE/aeinit.m (19)\nA AE/aesetup.m (9)\nA AE/aetrain.m (9)\nM DBN/dbnexamples.m (2)\nA NN/fmincg.m (175)\nA NN/nnJ.m (47)\nM NN/nnapplygrads.m (9)\nM NN/nnbp.m (14)\nA NN/nnbp2.m (31)\nA NN/nneval.m (13)\nM NN/nnexamples.m (203)\nM NN/nnff.m (26)\nA NN/nnff2.m (26)\nA NN/nngraddescent.m (8)\nA NN/nninit.m (23)\nA NN/nnrepackWb.m (23)\nM NN/nnsetup.m (7)\nM NN/nntest.m (9)\nM NN/nntrain.m (187)\nA NN/nnunpackWb.m (12)\nM README.md (25)\nM SAE/saeexamples.m (10)\nA SAE/saeinit.m (10)\nM SAE/saesetup.m (5)\nM SAE/saetrain.m (21)\nM SPAE/max3d.m (10)\nM SPAE/nbmap.m (28)\nM SPAE/paesdlm.m (2)\nM SPAE/paetrain.m (4)\nM SPAE/spaeexamples.m (19)\nM SPAE/spaesetup.m (2)\nM SPAE/spaetrain.m (4)\nM util/allcomb.m (88)\nM util/printf.m (28)\nM util/rnd.m (4)\nM util/sigm.m (4)\nM util/softmax.m (10)\nM util/visualize.m (63)\n-- Patch Links --\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/4.patch\n\u00a0https://github.com/rasmusbergpalm/DeepLearnToolbox/pull/4.diff\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/4\n\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/4#issuecomment-4869552\n. Sounds good. As long as you don't mind, I really appreciate your\nfeedback on how this should work, because I have not worked on a\ncollaborative, at-a-distance project before. Just let me know when you\nhave tests for me to run against!\n\nThanks,\nCarl\nOn Mon, Apr 2, 2012 at 4:50 PM, rasmusbergpalm\nreply@reply.github.com\nwrote:\n\nThis looks very exciting. I'll take a more thorough look at it this week :)\nPull requests should be sent when something is stable and ready for\nprime-time. Stuff you just want me to look at you can just send me a link to\nyour experimental branch, and i can grab the code from there and check it\nout.\nHere's an idea for a workflow:\nI'll look at implementing a (very basic) test-suite. When you've made\nsomething and it passes the tests then it should be sent as a pull request.\nChanges to the code that requires (extensive) changes to the tests should\nbe made in a branch and discussed/approved before being sent as a pull\nrequest.\nCheers, Rasmus.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/4#issuecomment-4888193\n. \n",
    "squall1988": "thank you for providing such a library.\nthere are two litter problems\nfirst :\nError using rbmtrain (line 6)\nnumbatches not integer\nError in dbntrain (line 4)\n    dbn.rbm{1} = rbmtrain(dbn.rbm{1}, x, opts);\nError in dbnexamples (line 17)\ndbn = dbntrain(dbn, train_x, opts);\nI change \n         numbatches = m / opts.batchsize; to be numbatches=floor(m/opts.batchsize);\nin file rbmtrain.m line 4\nsecond \nin dbnexample.m\nline 20\n%%  ex2 train a 100-100-100 DBN and use its weights to initialize a NN\ndbn.sizes = [100 100 100];\nif we set every layer of the dbn to be 100, after we pre-train the dbn and put it into the FFNN, it will appear problem,\nbecause the final layer corresponding to the labels, and the label number is 10 so if we want the example run we have to change the final layer of the dbn to be 10 just like :\ndbn.sizes = [100 100 10];\n. ",
    "iN1k1": "Hi the error from the first example comes from:\nassert(rem(numbatches, 1) ~= 0, 'numbatches not integer');\nThe assert function fires if the condition (first argument) is not satisfied.\nFrom MATLAB help\n\n\nhelp assert\n assert Generate an error when a condition is violated.\n    assert(EXPRESSION) evaluates EXPRESSION and, if it is false, displays the\n    error message 'Assertion Failed'.\n\n\nObvisously if numbatches is an integer the reminder will be 0 so the condition is not satisfied.\n. ",
    "algorithmdog": "Thanks for your reply.I am new for matlab so that I don't ' means transposition,which makes me confused.Thank you again.\n. Oh,I understand.Thank you.~_~\n. ",
    "yx3zhang": "Would you mind quickly point out where needs to be changed if I were to use cross entropy errors instead? I've spent sometime and changed some things, but perhaps not all.\n. ",
    "ghost": "\n'onum' is the number of labels, that's why it is calculated using size(y, 1). If you have 20 labels so the output of the network will be 20 neurons.\n'fvnum' is the number of output neurons at the last layer, the layer just before the output layer.\n'ffb' is the biases of the output neurons.\n'ffW' is the weights between the last layer and the output neurons. Note that the last layer is fully connected to the output layer, that's why the size of the weights is (onum * fvnum)\n\nI hope that was clear enough.\n. i am also new to CNN so based on little knowledge i have about it all i can say is that yes you can modify the kernel size and scale size for tuning.\nOn Tuesday, May 5, 2015 9:01 AM, AbbyLu <notifications@github.com> wrote:\nHi \nI have just started to learn CNN. \nI have a couple of questions about the structure of CNN.\nIs it possible to change the kernel size from square to rectangle in convolution layer? \nCan the scale in sampling layer be changed in the same way, too?\nNow I am learning to use the code about CNN in DeepLearnToolbox which is created by rasmusbergpalm. %% ex1 Train a 6c-2s-12c-2s Convolutional neural network \n%will run 1 epoch in about 200 second and get around 11% error. \n%With 100 epochs you'll get around 1.2% errorrand('state',0)cnn.layers = {\nstruct('type', 'i') %input layer\nstruct('type', 'c', 'outputmaps', 6, 'kernelsize', 5) %convolution layer\nstruct('type', 's', 'scale', 2) %sub sampling layer\nstruct('type', 'c', 'outputmaps', 12, 'kernelsize', 5) %convolution layer\nstruct('type', 's', 'scale', 2) %subsampling layer\n};I don't know how to modify the code to set a rectangle kernel size and scale. (or it is impossible?)\nCould someone help me? Thank you very much!!\u2014\nReply to this email directly or view it on GitHub. \n. ",
    "breznak": "ok, given patch produces other error. \nSo, if you want to classify vector x (1x764) which you reshaped to x2 (28x28), just do \na3(:,:,1)=x2; a3(:,:,2)=zeros(28,28); \nc=cnnff(c,a3);\nc.o(:,1)\nc.o(:,1) is classification for vect x2 (c.o(:,2) is for fake zeros(28,28)).\nCheers, M\n. ok, thanks. No need to hurry, in the meantime im working on HTM CLA support. \nHave a nice spring ;)\n. ",
    "summerstay": "Perhaps I should unfold the DBN to a NN and use nnff?\n. I don't have the statistics toolbox, so I wrote my own zscore function.  Maybe you want to include it?\nfunction [x, mu, sigma] = zscore(x)\n        sigma=std(x);\n        mu=mean(x);\n        repmatmu=repmat(mu,[size(x,1),1]);\n        x=x-repmatmu;\n        repmatsigma=repmat(sigma,[size(x,1),1]);\n        x=x./repmatsigma;\nend\n. Yeah, it looks like bsxfun would be better.\n. I'm not using git, but here's my version, for what it's worth. Maybe someone can do the pull request.\nfunction [x, mu, sigma] = zscore(x)\n        sigma=max(std(x),eps);\n        mu=mean(x);\n        x=bsxfun(@minus,x,mu);\n        x=bsxfun(@rdivide,x,sigma);\nend\n. ",
    "beamandrew": "Could you give me a little guidance with this? So far I've trained the RBM layers and unrolled to a NN, as you did in the demo. For a two-class classification problem, I'm trying to make predictions on a withheld test set, but I don't think I'm doing it correctly:\nnn = nntrain(nn, Xtrain(traini,:), Y(traini,:), opts);\nnn.testing = 1;\nnntest = nnff(nn,Xtrain(testi,:), Y(testi,:));\npredictions = round(nntest.a{end});\nI'd like to predict for sets where I don't have the label information, but I'm not sure how to do it. If I just drop in a vector of all 1s or 0s as a placeholder in the code above, I get awful accuracy results, making me believe I'm not doing it correctly.\nAny pointers?\nThanks\n. Thanks for the quick reply and thanks again for this toolbox - it's allowed me to get my hands dirty with DBN.\nI don't understand the use of the max operator here, and when I run it as you suggested I get an error. If nn.a{end} contains the top node's activation level for each observation after the feedforward pass, why am I taking the max of this component? When I run your suggestion I get this, which I'm sure is a simple Matlab error, but Matlab is not my native language so I'm not exactly sure what's going on.\n\n\n[~,label] = max(nntest.a{end},2);\nError using max\nMAX with two matrices to compare and two output arguments is not supported.\n\n\nSo if I replace that with \n\n\nlabel = max(nntest.a{end},2);\n\n\nit works but my labels are all 2. If I change it to max(nntest.a{end},1), they are all 1, which is in line with my understanding of max(). \n. Oh ok, I see and I think I've found my issue. I'm doing two class-classification, which now I'm guessing my response matrix should be a Nx2 column of observations, right? I've been using 1 column, which means I've only been using 1 unit in the output layer, when I actually need two. Thanks again.\n. I must be dense, but I can't seem to get this to work, even on the training data used to build the NN. To simplify, I've switch from a DBN to NN for the time being. Here is my setup\nnn = nnsetup([Nfea 100 2]); %Nfea is number of columns in train_x\nopts.numepochs =  10;        %  Number of full sweeps through data\nopts.batchsize = 100;\nnn.normalize_input = 1;\nnn.dropout = 0.5;\nnn.activation_function = 'sigm';\n%train nn\ntwocol = [~train_y train_y];\nnn = nntrain(nn, train_x,twocol, opts);\nnn.testing = 1;\nnntest = nnff(nn,train_x, twocol); %train_x is 24000 by 1025\nnn.testing = 0;\na = nntest.a{end};\na(1:10,:) % I can't even overfit to the training data at this point.\nThe first ten values of the top layer is:\nans =\n1.0000    0.0000\n0.9998    0.0003\n1.0000    0.0000\n1.0000    0.0000\n1.0000    0.0000\n1.0000    0.0000\n0.9999    0.0000\n1.0000    0.0000\n0.9985    0.0011\n1.0000    0.0000\nThe true values are:\n\n\ntwocol(1:10,:)\n\n\nans =\n1     0\n 1     0\n 1     0\n 0     1\n 0     1\n 1     0\n 0     1\n 1     0\n 0     1\n 1     0\nNo matter how I configure the NN, the story is always the same, I never get any activation for the unit representing the second label. As a sanity check, I dropped the data I'm using into some of the tree-based methods in R and I get decent classification results. Any idea what's going on?\n. Thank a lot of all the help, it appears to be working now. I have a theoretical question for you as well if you don't mind. My understanding is that generative pre-training (i.e. the RBM layers of a DBN) were used before the paper on dropout was published, but now training deep neural nets is favored using dropout instead of using the DBNs. Do you think this is the case, or are DBNs still used for some applications?\n. Thanks, that was my understanding as well. I was looking through the toolbox for maxout, but didn't see it. It's pretty new so I wasn't surprised. \nI'm using it for a contest on Kaggle to identify whale calls:\nhttp://www.kaggle.com/c/whale-detection-challenge\nI'm not very concerned about placing high, but instead I'm using it as an opportunity to apply a deep learning architecture. I'm a grad student myself and part of my research involves machine learning (both applications and method development). \nWhile we've got this conversation going, I've read the dropout paper on arxiv and watched Hinton's NIPS 2012 talk. He says that dropout is equivalent to the geometric mean of all possible models, but I haven't seem a formal proof of this anywhere. Is there a paper floating around some where with this proof?\nThanks again,\nAndrew\n. ",
    "mohsenali": "can bsxfun be used instead of the repmat?\nOn Thu, Mar 7, 2013 at 10:13 AM, summerstay notifications@github.comwrote:\n\nI don't have the statistics toolbox, so I wrote my own zscore function.\nMaybe you want to include it?\nfunction [x, mu, sigma] = zscore(x)\nsigma=std(x);\nmu=mean(x);\nrepmatmu=repmat(mu,[size(x,1),1]);\nx=x-repmatmu;\nrepmatsigma=repmat(sigma,[size(x,1),1]);\nx=x./repmatsigma;\nend\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/16#issuecomment-14565640\n.\n. Because if standard deviation is zero it means even a small change (with respect to mean) should be calculated as large (mahalanobis distance). Since we cannot divide by zero we should replace it with some small number. \n. \n",
    "gallamine": "Make sure you error check that if std == 0 set it to 'eps'\n\u2014\nSent from Mailbox for iPhone\nOn Thu, Mar 7, 2013 at 11:16 AM, summerstay notifications@github.com\nwrote:\n\nYeah, it looks like bsxfun would be better.\nReply to this email directly or view it on GitHub:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/16#issuecomment-14569843\n. Lol. I'll give it a shot. My git skills are embarrassingly bad!\n\u2014\nSent from Mailbox for iPhone\n\nOn Wed, Mar 6, 2013 at 3:33 AM, Rasmus Berg Palm notifications@github.com\nwrote:\n\nOh. I should remove it then. Or a delicious pull request from a fellow respected hacker might do that... hint ;)\nReply to this email directly or view it on GitHub:\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/23#issuecomment-14488059\n. Well, that certainly looks great to me! Do you have it posted anywhere yet?\n. My comment was in reference to Hinton's paper on A Practical Guide to Training\nRestricted Boltzmann Machines, though I seem to recall a Bengio paper where he didn't do it, so I'm not sure whether it's necessary or not. \n\nSection 8 says:\n\nThe weights are typically initialized to small random values chosen from a zero-mean Gaussian with\na standard deviation of about 0:01. Using larger random values can speed the initial learning, but\nit may lead to a slightly worse \ffinal model. Care should be taken to ensure that the initial weight\nvalues do not allow typical visible vectors to drive the hidden unit probabilities very close to 1 or 0\nas this signi\fsignificantly slows the learning.\n. Yes, just got slammed at work. Will try to put in the fix soon.\n\nOn Mon, Jul 8, 2013 at 2:50 PM, Rasmus Berg Palm\nnotifications@github.comwrote:\n\nStill interested in fixing this @gallamine https://github.com/gallamine?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/37#issuecomment-20627355\n.\n. Ok, I think that should take care of it. Pretty sure I did the updated pull correctly ...\n. Come to think of it, I've had this problem to. I think you just diagnosed\nwhy I couldn't get the DBN to work for a lot of examples I tried! Now, to\nfind a solution ...\n\nOn Tue, Apr 30, 2013 at 12:25 PM, fritol notifications@github.com wrote:\n\nHi there\nI tried to figure out how to modify the code to be able to apply the DBN\non data with some negative values, no luck -_- thou.\nI tested the data with other machine learning algos (not from this\npackage) and there is always some more or less meaningful solution learned,\nso it seems the input space is not badly conditioned and that this toolbox\nis limited to positive data only(?)\nHow to go about it?\nAny feedback appreciated.\nfritol\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/40\n.\n. \n",
    "Noumansoomro": "Hi! Iam new in deep learning. I want to extract features for each image separately from all algorithms of  DeepLearnToolbox, can you guide me how to extract features of each image separately From DBN, CAE, CNN,SAE\n. ",
    "marcofraccaro": "Hi rasmus!\nThe error is due to the fact that in test_nn_gradients_are_numerically_correct you forgot to replace batch_y with y in nn = nnff(nn, batch_x, batch_y), and this gives problems when using softmax!\nIf you change it the test should work ;)\n. \nHere there is a really small example. I'm really busy right now, so I've just tried quickly without spending time in finding the optimal parameters for both the cases nn.normalize_momentum = 0  and nn.normalize_momentum = 1.\nOf course the optimal learning rates, dropout fraction, momentum term may differ using normalization of the momentum or not, for example when you normalize you can have a higher value of nn.momentum (I've used 0.8 with normalization and 0.5 without). Also, maybe for a dataset normalization works and for another one it is better not to normalize.\nWith my quick example with MNIST and no dropout there is a slight improvement with normalization both in training error and test error (the final test error is  0.0393 without normalization and 0.0381 with normalization).\nThe settings I have used are:\nnn = nnsetup([784 200 100 10]);\nnn.activation_function = 'tanh_opt';    %  Sigmoid activation function\nnn.normalize_input = 1;             %  Don't normalize inputs\nnn.learningRate = 2;                %  Sigm and non-normalized inputs require a lower learning rate\nopts.numepochs =  200;                %  Number of full sweeps through data\nopts.batchsize = 100;               %  Take a mean gradient step over this many samples\nnn.it_no_momentum = 5; \nnn.scaling_learningRate = 0.998;\nnn.normalize_momentum = 0;   ||    nn.normalize_momentum = 1;\nnn.momentum  = 0.5;               ||    nn.momentum  = 0.8; \nAnyway, if you want to include it, I suggest to put the default value to nn.normalize_momentum = 0, and let the user decide on what to do.\n. L. Chan. E\u000efficacy of di\u000bfferent learning algorithms of the backpropagation network. IEEE Region 10\nConference on Computer and Communication Systems, Hong Kong. September 1990.\n. Because at the very first iteration, as nn.vW is initialized as zeros, we have norm_vW=0, hence an infinite normalization constant. You could deal with this maybe with an internal variable to specify the first iteration, but I didn't want to add more internal variables\n. ",
    "mphielipp": "I'm still getting the same message.\n. ",
    "albertoandreottiATgmail": "Guys, I'm interested in fixing this. Do you know if this test would work under an old matlab(2004)? I'm planning to run both things in parallel and see where the error appears.\n. Hi Rasmus,\nthanks for the file. I was suspicious about the convolutions in Octave, but after doing some debugging it seems that the output is pretty similar for both Octave/Matlab. I replaced more of the random stuff for hard coded values, and the output is pretty similar in both systems.\nMore specifically, after replacing the rand() by ones() in cnnsetup, the test runs OK. The test_example_CNN TC will still give a huge error(~89%). I didn't have much time for looking into that, but will spent a couple of hours during the weekend.\nThanks again,\nAlberto.\n. Ey sorry for the delay i'll check it!\nEl 10/08/2013 05:11, \"Rasmus Berg Palm\" notifications@github.com escribi\u00f3:\n\nIn order for me to move on with this in I need to know how slow\nconvn(x,k,'same')(ranges) compared to matlabs convn(x,k,'valid') is, and\nthe out of place files needs to be removed.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/44#issuecomment-22436001\n.\n. Hi Rasmus,\n\nI didn't know that once the pull request was done you continue to see the\nchanges in the branch. I had thought that when I offer something for you to\npull I was offering a specific revision, and not the last state of the\nbranch.\nI put the other stuff I'm working on in another branch, and left the master\nbranch with the changes that are specific to the Octave fix. I believe that\nnow you will find the correct stuff. Should you need me to do the pull\nrequest again, please let me know. I'm a rookie with Git, but I've already\nlearnt cvs, svn, and hg! I will get used to it :-).\nRegarding the speed of the code in Matlab/Octave, I have timed the code for\nOctave, and there are some comments on when to use the convn with 'same'\nand when to use fft. I really have little access to Matlab, I borrowed a\ncomputer from my girlfriend in order to debug the problem. It's like the\ntiming info you already provide for Matlab should be enough. A fair\ncomparison would require us to use the same hardware, and considering this\nis a bug confirmed by the Octave guys, we should take this as a temporal\npatch until Octave fixes the convn() problem.\nThanks for the patience,\nAlberto.\nOn 10 August 2013 15:05, Alberto Andreotti albertoandreotti@gmail.comwrote:\n\nEy sorry for the delay i'll check it!\nEl 10/08/2013 05:11, \"Rasmus Berg Palm\" notifications@github.com\nescribi\u00f3:\nIn order for me to move on with this in I need to know how slow\n\nconvn(x,k,'same')(ranges) compared to matlabs convn(x,k,'valid') is, and\nthe out of place files needs to be removed.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/44#issuecomment-22436001\n.\n\n\n\nJos\u00e9 Pablo Alberto Andreotti.\nTel: 54 351 4730292\nM\u00f3vil: 54351156526363.\nMSN: albertoandreotti@gmail.com\nSkype: andreottialberto\n. Yes, sorry. I'm overloaded with lots of stuff.\ncheers,\nAlberto.\nOn 18 September 2013 04:40, Rasmus Berg Palm notifications@github.comwrote:\n\nI'll close this and wait in anticipation of the new one :)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/44#issuecomment-24646414\n.\n\n\nJos\u00e9 Pablo Alberto Andreotti.\nTel: 54 351 4730292\nM\u00f3vil: 54351156526363.\nMSN: albertoandreotti@gmail.com\nSkype: andreottialberto\n. I'll take a look. Cool CI platform by the way!\nOn 22 November 2013 07:30, Rasmus Berg Palm notifications@github.comwrote:\n\nThe build failed:\nhttps://travis-ci.org/rasmusbergpalm/DeepLearnToolbox/builds/14332866,\ndoes running runalltests.m work locally?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/66#issuecomment-29063078\n.\n\n\nJos\u00e9 Pablo Alberto Andreotti.\nTel: 54 351 4730292\nM\u00f3vil: 54351156526363.\nMSN: albertoandreotti@gmail.com\nSkype: andreottialberto\n. Hi Rasmus,\nthe problem is Octave 3.2.4, I download that version, and I get the same\nerror,\noctave-3.2.4.exe:2> test_cnn_gradients_are_numerically_correct\nerror: convn: first and second arguments must be matrices of the same\ndimensiona\nlity\nerror: called from:\nerror:   C:\\Octave\\3.2.4_gcc-4.4.0\\share\\octave\\3.2.4\\m\\polynomial\\convn.m\nat li\nne 48, column 5\nerror:   C:\\Users\\jpandreo\\Documents\\GitHub\\DeepLearnToolbox\\CNN\\cnnff.m at\nline\n 14, column 3\nerror:\nC:\\Users\\jpandreo\\Documents\\GitHub\\DeepLearnToolbox\\tests\\test_cnn_grad\nients_are_numerically_correct.m at line 13, column 5\nby the way this is the error in 3.4.3,\noctave:4> test_cnn_gradients_are_numerically_correct\nwarning: nested functions are coerced into subfunctions in file\nC:\\Users\\jpandre\no\\Documents\\GitHub\\DeepLearnToolbox\\CNN\\cnnbp.m\nerror: numerical gradient checking failed\nerror: called from:\nerror:\nC:\\Users\\jpandreo\\Documents\\GitHub\\DeepLearnToolbox\\CNN\\cnnnumgradcheck\n.m at line 63, column 33\nerror:\nC:\\Users\\jpandreo\\Documents\\GitHub\\DeepLearnToolbox\\tests\\test_cnn_grad\nients_are_numerically_correct.m at line 15, column 1\nwe know that Octave has problems with convn() so, you should switch the CI\nto 3.6.4. I'm currently running all the tests in 3.6.4, I'll let you know\nthe results.\nAlberto\nOn 22 November 2013 09:48, Alberto Andreotti albertoandreotti@gmail.comwrote:\n\nI'll take a look. Cool CI platform by the way!\nOn 22 November 2013 07:30, Rasmus Berg Palm notifications@github.comwrote:\n\nThe build failed:\nhttps://travis-ci.org/rasmusbergpalm/DeepLearnToolbox/builds/14332866,\ndoes running runalltests.m work locally?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/66#issuecomment-29063078\n.\n\n\nJos\u00e9 Pablo Alberto Andreotti.\nTel: 54 351 4730292\nM\u00f3vil: 54351156526363.\nMSN: albertoandreotti@gmail.com\nSkype: andreottialberto\n\n\nJos\u00e9 Pablo Alberto Andreotti.\nTel: 54 351 4730292\nM\u00f3vil: 54351156526363.\nMSN: albertoandreotti@gmail.com\nSkype: andreottialberto\n. Another useful info is that you need 6 epochs in Octave to obtain the same\nerror rate for the test case of the CNN while in Matlab you'll need fewer.\nWith one epoch in Octave you get arround 18% error which is above the\ntreshold you set in the assert in the test case.\nOn 22 November 2013 11:17, Alberto Andreotti albertoandreotti@gmail.comwrote:\n\nHi Rasmus,\nthe problem is Octave 3.2.4, I download that version, and I get the same\nerror,\noctave-3.2.4.exe:2> test_cnn_gradients_are_numerically_correct\nerror: convn: first and second arguments must be matrices of the same\ndimensiona\nlity\nerror: called from:\nerror:   C:\\Octave\\3.2.4_gcc-4.4.0\\share\\octave\\3.2.4\\m\\polynomial\\convn.m\nat li\nne 48, column 5\nerror:   C:\\Users\\jpandreo\\Documents\\GitHub\\DeepLearnToolbox\\CNN\\cnnff.m\nat line\n 14, column 3\nerror:\nC:\\Users\\jpandreo\\Documents\\GitHub\\DeepLearnToolbox\\tests\\test_cnn_grad\nients_are_numerically_correct.m at line 13, column 5\nby the way this is the error in 3.4.3,\noctave:4> test_cnn_gradients_are_numerically_correct\nwarning: nested functions are coerced into subfunctions in file\nC:\\Users\\jpandre\no\\Documents\\GitHub\\DeepLearnToolbox\\CNN\\cnnbp.m\nerror: numerical gradient checking failed\nerror: called from:\nerror:\nC:\\Users\\jpandreo\\Documents\\GitHub\\DeepLearnToolbox\\CNN\\cnnnumgradcheck\n.m at line 63, column 33\nerror:\nC:\\Users\\jpandreo\\Documents\\GitHub\\DeepLearnToolbox\\tests\\test_cnn_grad\nients_are_numerically_correct.m at line 15, column 1\nwe know that Octave has problems with convn() so, you should switch the CI\nto 3.6.4. I'm currently running all the tests in 3.6.4, I'll let you know\nthe results.\nAlberto\nOn 22 November 2013 09:48, Alberto Andreotti albertoandreotti@gmail.comwrote:\n\nI'll take a look. Cool CI platform by the way!\nOn 22 November 2013 07:30, Rasmus Berg Palm notifications@github.comwrote:\n\nThe build failed:\nhttps://travis-ci.org/rasmusbergpalm/DeepLearnToolbox/builds/14332866,\ndoes running runalltests.m work locally?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/66#issuecomment-29063078\n.\n\n\nJos\u00e9 Pablo Alberto Andreotti.\nTel: 54 351 4730292\nM\u00f3vil: 54351156526363.\nMSN: albertoandreotti@gmail.com\nSkype: andreottialberto\n\n\nJos\u00e9 Pablo Alberto Andreotti.\nTel: 54 351 4730292\nM\u00f3vil: 54351156526363.\nMSN: albertoandreotti@gmail.com\nSkype: andreottialberto\n\n\nJos\u00e9 Pablo Alberto Andreotti.\nTel: 54 351 4730292\nM\u00f3vil: 54351156526363.\nMSN: albertoandreotti@gmail.com\nSkype: andreottialberto\n. Hi guys,\nI believe this is already working in a branch of my clone of the repo. Let\nme check :-).\nOn 11 May 2014 10:01, Rasmus Berg Palm notifications@github.com wrote:\n\nAny updates on this?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/75#issuecomment-42769948\n.\n\n\nJos\u00e9 Pablo Alberto Andreotti.\nTel: 54 351 4730292\nM\u00f3vil: 54351156526363.\nMSN: albertoandreotti@gmail.com\nSkype: andreottialberto\n. Hi rmanor, rasmus,\nthis is no exactly what my pull request was about, I wanted to have non squared inputs(e.g., an spectrogram of 50x600). I'll work in a test case for checking gradients when non squared inputs are present.\nI have a test case on language detection that uses non squared inputs, but it requires huge data(arround 3GBs) and it takes many hours(~15) to complete. I'll try to create a toy test case with artificial inputs. So rmanor, please do not work in the same thing, focus on the non squared kernel.\nContact me if you want to talk, I'll be glad.\n. Hey Ran, are you talking about the CNN? are you sure it worked fine? Would you mind taking a look at my pull request to see if my changes make sense to you? Alberto.\n. Hey!, sorry for the confusion. You're right, the net would work as it is with non squared inputs. I needed this non squared average operation for reducing vectors(say 50x1) to a single variable, the only way to achieve this was with an average operation using a non squared kernel.\nSo, just to give some context, I wanted to convert each map to a variable, near the output layer, that was achieved by this change. I think you can introduce your changes withouth conflicting with mine. \nRamus: is it ok to add separate test cases for each of these changes?\n. I'll take a look during the weekend, if you could wait. Thanks, Alberto.\nOn 5 May 2014 17:26, Taygun Kekec notifications@github.com wrote:\n\nanyone progress on this issue?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/76#issuecomment-42234915\n.\n\n\nJos\u00e9 Pablo Alberto Andreotti.\nTel: 54 351 4730292\nM\u00f3vil: 54351156526363.\nMSN: albertoandreotti@gmail.com\nSkype: andreottialberto\n. I remember we had problems with the convn() function in Octave. My branch had a fix for this. Now Octave 3.8.1 is released, so I'll test everything under the new version and see what happens.\n. Hey Rasmus, where have you seen it fail? I used this very same implementation to train a deeper network, with lots of data, and very good results.\nTwo different implementations are supposed to lead to different behaviours regarding numerical stability. That's why you see different convergence speed. Anyways it's your call ;-).\n. I agree with you, good results for a certain number of inputs does not mean correctness. \nMy question is: have you performed some tests on the convn_valid() function that spots a problem with the implementation? Because if that's the case I would like to know as I'm planning to keep using it until the new Octave is released. \nBtw, I'm not using conv() for the implementation, I'm using conv2(). How big is the error that you see and on which sitaution? do you have a test case? I mean, implementing convolution from scratch should not be a problem for us. We just need a few test cases to tests for numerical stability.\nThanks for keeping an eye on this,\n. Yeah, I remember this discussion. My rationale was that in the other places where it was used, the error would not appear as one of the arguments was of the form MxNx1, and the implementation was only having trouble with rotations.\nAnyways it's too messy, let's wait for the Octave guys to solve this for use. Cheers.\n. Take a look a the times that are explained here to have a notion of the differences among implementations.\n. Yes it is, :-). Sorry, my bad.\n. Yes, please don't pay attention to it.\n. I moved this to the isOctave() function.\n. Make this very explicit here, so when the fix is available from the Octave guys the change will be obvious.\nAlso, this is the only place where the function needs to be replaced. Replacing it everywhere would make things much slower.\n. Lines 14 and 16 are interchanged. My mistake.\n. Hey!, this is a good question. The 'valid' option is the only one that is having problems in Octave. For example, in other places of cnnbp.m, convn() is used but not the 'valid' version. Additionally, in other places where the 'valid' version is used the second argument is somethig of dimensionality MxNx1. This is the reason why it is working in these cases. Because of the third dimension being '1'. The Octave implementation is having problems fliping the input, but this error is not present when the dimension is 1, as fliping would give the same result.\nThat's why you only need to replace it there. Hope I didnt mess things, do not hesitate to ask clarification.\n. ",
    "skaae": "no. But i can post it tonight/tomorrow. I'm new to github. Do i just fork this project and add my function to the fork?\n. Ok, after messing a bit around with git, i think i managed to send you a pull request.\nedit: I have 3 corrections - dont know if i should pull request again?\nThe first comment in test_example_SAE.m should read: %split: train-val-test: 50000-10000-10000\nDelete assert statement in nn_val\nThe assert in the first example does not pass after splitting of dataset. The error needs to be changed to 0.22 instead of 0.16\n. Nice. I will create a new version of nntrain and move the example to the nn test, I put it in SAE because I was playing around with it. \nBy cross-validation do you mean normal validation as in the plot in my previous post? Or do you mean \"real\" cross validation? In the latter case i think its cleaner to handle cross validation by a separate function. \n. There is a problem with  nntrain being redefined to\nmatlab\n[nn, L] = nntrain(train_x, train_y, val_x, val_y, opts)\nThis will breaks SAE examples when the Autoencoders are trained. \nI think you should include a simple training function  for the autoencoders. \nCould you include a function like nnsetup(...) for the opts struct to make sure that all struct fields are initialized? \nIt would be nice for documentation as well.\nI need to following fields:\nmatlab\nopts.saveAfter  = 10;    %interval between backups\nopts.backupFile = 'nn_backup.mat';    %path and filename to backup file\nopts.plot       = 1;    % [] = no plotting , all other = plotting\n. Allright So the function will be\nmatlab\n[nn, L] = nntrain(nn,train_x, train_y, vargin)\nwhere vargin is either opts or val_x, val_y, opts.\n. ok. Didn't know you could do that.\n. I rewrote the code to include the changes. \nCan you see the changes? It should include two new functions:\nNN/nneval.m, NN/nnupdatefigures.m plus updates in nntrain and the test_example_NN.\nI cant create a new pull request because: \"Oops! There's already a pull request  for skaae:master \"\nI haven't really figured github out yet :)\nThe code passes all tests, icluding SAE, CNN, DBN etc.\n. ",
    "aniketvartak": "is this feature available currently?\n. ok, got it, thanks!\n. how about this?!....early train stop with check on validation error?..\n. ",
    "user9": "renaming LearningRate to Alpha would be harder to understand for newbies.\n. ",
    "rt77789": "Current gradient updating doesn't involve ddok and ddik, and I found some commented statements contain ddok and ddik, in caeapplygrads.m file:\nmatlab\n%             cae.vik{i}{j} = cae.momentum * cae.vik{i}{j} + cae.alpha ./ (cae.sigma + cae.ddik{i}{j}) .* cae.dik{i}{j};\n%             cae.vok{i}{j} = cae.momentum * cae.vok{i}{j} + cae.alpha ./ (cae.sigma + cae.ddok{i}{j}) .* cae.dok{i}{j};\n            cae.vik{i}{j} = cae.alpha * cae.dik{i}{j};\n            cae.vok{i}{j} = cae.alpha * cae.dok{i}{j};\nI'm not familiar with \"stochastic diagonal levenberg-marquardt\", and I think it makes convergency fast.\n. ",
    "mavenlin": "@rasmusbergpalm:\nSorry for the late reply, I was busy with my project and did not check the notifications for a long time.\nI've tried to check the gradients, but it fails for both the original code and modified code. \nFirst I thought it should be the code in caeff and caebp, flipall should also be added to these files if we consider the convn function is flipping the kernel.\nI guess I did not figure out how the data is stored in the cell array.\n. ",
    "reedscot": "Red Hat Enterprise Linux Server release 5.9 (Tikanga)\nMatlab R2013a\nOn Mon, Jul 1, 2013 at 3:13 AM, Rasmus Berg Palm\nnotifications@github.comwrote:\n\nWhat OS/program?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/43#issuecomment-20266348\n.\n. \n",
    "zym1010": "I have this problem too, using MATLAB R2013a on Mac OS X 10.9.2\n. ",
    "RemonComputer": "HI mx70,\nI Think I had the same problem with test_example_sae\nthe problem lies in that matlab has a function named nnsetup so it call it, instead of nnsetup that lies in the NN Folder, so try this \nput that line of code before calling dbnunfoldronn\naddpath('D:\\Programs\\Deep Learning ToolBox\\GitHup Repository To Modify\\trunk\\NN');\nbut modify the path such that it points to your NN Folder\n. @rasmusbergpalm \nSolved:\nthe problem lies in that matlab has a function named nnsetup so it call it, instead of nnsetup that lies in the NN Folder, so after adding this line\naddpath('D:\\Programs\\Deep Learning ToolBox\\GitHup Repository To Modify\\trunk\\NN');\nit can call the desired function and problem solved\nso the final file is:\nfunction test_example_SAE\naddpath('D:/Programs/Deep Learning ToolBox/GitHup Repository To Modify/trunk/data');\nload mnist_uint8;\ntrain_x = double(train_x)/255;\ntest_x  = double(test_x)/255;\ntrain_y = double(train_y);\ntest_y  = double(test_y);\n%%  ex1 train a 100 hidden unit SDAE and use it to initialize a FFNN\n%  Setup and train a stacked denoising autoencoder (SDAE)\naddpath('D:\\Programs\\Deep Learning ToolBox\\GitHup Repository To Modify\\trunk\\SAE');\naddpath('D:\\Programs\\Deep Learning ToolBox\\GitHup Repository To Modify\\trunk\\NN');\naddpath('D:\\Programs\\Deep Learning ToolBox\\GitHup Repository To Modify\\trunk\\util');\nrng(0);\nsae = saesetup([784 100]);\nsae.ae{1}.activation_function       = 'sigm';\nsae.ae{1}.learningRate              = 1;\nsae.ae{1}.inputZeroMaskedFraction   = 0.5;\nopts.numepochs =   1;\nopts.batchsize = 100;\nsae = saetrain(sae, train_x, opts);\nvisualize(sae.ae{1}.W{1}(:,2:end)')\n% Use the SDAE to initialize a FFNN\nnn = nnsetup([784 100 10]);\nnn.activation_function              = 'sigm';\nnn.learningRate                     = 1;\nnn.W{1} = sae.ae{1}.W{1};\n% Train the FFNN\nopts.numepochs =   1;\nopts.batchsize = 100;\nnn = nntrain(nn, train_x, train_y, opts);\n[er, bad] = nntest(nn, test_x, test_y);\nassert(er < 0.16, 'Too big error');\n%% ex2 train a 100-100 hidden unit SDAE and use it to initialize a FFNN\n%  Setup and train a stacked denoising autoencoder (SDAE)\nrng(0);\nsae = saesetup([784 100 100]);\nsae.ae{1}.activation_function       = 'sigm';\nsae.ae{1}.learningRate              = 1;\nsae.ae{1}.inputZeroMaskedFraction   = 0.5;\nsae.ae{2}.activation_function       = 'sigm';\nsae.ae{2}.learningRate              = 1;\nsae.ae{2}.inputZeroMaskedFraction   = 0.5;\nopts.numepochs =   1;\nopts.batchsize = 100;\nsae = saetrain(sae, train_x, opts);\nvisualize(sae.ae{1}.W{1}(:,2:end)')\n% Use the SDAE to initialize a FFNN\nnn = nnsetup([784 100 100 10]);\nnn.activation_function              = 'sigm';\nnn.learningRate                     = 1;\n%add pretrained weights\nnn.W{1} = sae.ae{1}.W{1};\nnn.W{2} = sae.ae{2}.W{1};\n% Train the FFNN\nopts.numepochs =   1;\nopts.batchsize = 100;\nnn = nntrain(nn, train_x, train_y, opts);\n[er, bad] = nntest(nn, test_x, test_y);\nassert(er < 0.1, 'Too big error');\n. OK\n. ",
    "nederhrj": "OK. I think I found the error:\nI tried to add all paths from DeepLeanToolbox, while I was already in that directory. Skip the \"cd DeepLearnToolbox\" and it works much better.\nI now get an error about \"rng\" is undefined. Octave does not seem to support this.\n. ",
    "tiagotrigger": "Thanks for the reply, but I already did that and still didn't work.\nI just did the same thing on Windows and worked, the error is occuring on the Ubuntu version of Matlab that I was using.\nI'll just stick with Windows version since it is working.\n. ",
    "marcospaulo18": "Thanks very much.\nI was using the output as a row vector 1,N instead of a column vector N,1.\nNow I have a different doubt, how do I test the trained neural network for just one input and see the output found by my trained nn?\nI use the nnpredict function, but I don't know if that is the purpose of the function. It always returns 1 (maybe I trained wrong or the nnpredict function don't do what I think it would).\nHere is a simple XOR (exclusive or) that I use as input for training:\n\ntrain_in = [0.0, 0.0; 0.0, 1.0; 1.0, 0.0; 1.0, 1.0]; % XOR IN\ntrain_out = [0.0; 1.0; 1.0; 0.0]; % XOR OUT\nrng(0);\nnn = nnsetup([2 5 1]);\nopts.numepochs =  100;   %  Number of full sweeps through data\nopts.batchsize = 4;  %  Take a mean gradient step over this many samples\n[nn, L] = nntrain(nn, train_in, train_out, opts);\n[er, bad] = nntest(nn, train_in, train_out);\npredicted1 = nnpredict(nn, [0.0, 0.0]);\npredicted2 = nnpredict(nn, [0.0, 1.0]);\npredicted3 = nnpredict(nn, [1.0, 0.0]);\npredicted4 = nnpredict(nn, [1.0, 1.0]);\ndisp([predicted1 predicted2 predicted3 predicted4]);\n\nThe predicted1, predicted2, predicted3 and predicted4 always return 1.\n. Sorry, I think that I don't understand what you said.\nI don't known what \"nn.a{3}\" is supposed to do.\nThere is a function that return the output of the nn after training? i.e. I pass the input, and the function returns the output of the network for my input (something similar with pybrain activate method).\nI don't think that nn.predict do that because it returns just one value, and in the future (not the XOR example that I'm testing) if I what to use a nn with 5 neurons on output, the nn.predict would return 1 value when I nedded 5. (observation: I'm not familiar with matlab or your code, so I may getting it wrong).\n. Thanks, that was what I needed.\n. One more question, there is a way to do supervised training with DBN? Because dbnsetup take just train_x as input and not train_y.\nI'm trying to repeat the xor function using DBN instead of NN:\ntrain_in = [0.0, 0.0; 0.0, 1.0; 1.0, 0.0; 1.0, 1.0]; % XOR Input\ntrain_out = [0.0; 1.0; 1.0; 0.0]; % XOR Output\nrng(0);\nlayers = [2 15 1];\nnn = nnsetup(layers);\nopts.numepochs =  1000;   %  Number of full sweeps through data\nopts.batchsize = 4;  %  Take a mean gradient step over this many samples\n[nn, L] = nntrain(nn, train_in, train_out, opts);\n[er, bad] = nntest(nn, train_in, train_out);\ndisp(er)\n\n. ",
    "gafurquim": "Hi,\nI\u2019m studying your Toolbox and trying to use regression, too.\nIs there any example?\nOther point, using DBN I receive the message: \u201call data in x must be in [0:1]\u201d.\nIs there any problem using negative or different values (bigger than 1)? Or can I simply discard this verification?\nThanks for all help.\n. ",
    "magic2du": "It's possible to use the validation error for early stopping?\n. ",
    "ananda621": "The bug on L2 regularization on weights has been fixed. Could you review this and merge if everything's good?\n. ",
    "MaxG87": "I will use camel case variable names if you want me to. I don't know, whether \"logistic_cdf\" works in Matlab as well. I thought so but can't test currently.\nHow do I submit the corrected version of my pull request? Shall I submit just another pull request?\n. Your right, that's a copy and paste mistake.\n. ",
    "sdemyanov": "@rasmusbergpalm I don't remember the details already. Maybe for this current architecture (c - s - c - s) all the calculations were correct, but it your implementation s layer performs the job of c layer (computing the derivatives of activation function). It means for example that you cannot remove s layer and put two c layers together. Btw, I've just finished my CNN library on Matlab and C++, compiled to mex functions, here https://github.com/sdemyanov/ConvNet Both versions work identically, the speed up is about 2 times, You can copy my Matlab implementation to your toolbox, just give a link on it.\n. ",
    "justdark": "Thanks for your quick reply\uff01\n. It's the convolution layer,not subsampling layer,you setup b IN both layers.https://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/CNN/cnnsetup.m#L10\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/CNN/cnnsetup.m#L21 \n. I just try to implement it in python this afternoon according to your code and that paper,but still have some problem and found it much slower than MATLAB\n. I  implement it in python,but if I add the sigmoid function in subsampling layers as that paper said,It won't converge although I have passed the numeric_gradcheck,I notice most CNN don't use sigmoid in 's' layer,so I cancel it at last.\n. ",
    "rmanor": "Matlab does support objects and classes and even inheritance.\nI was actually thinking about adding an option to define an activation function per layer and then adding an activation function \"conv\" will do the convolution.\n. Why shouldn't the gradient be correct? It propagates it as it should do the source units that were subsampled.\nCan you share your code for max pooling back propagation?\n. Hi,\nMy change is about non squared kernels, not inputs. As far as I can tell\nthere is currently no problem with non squared inputs, I already used it\nwith non squared inputs.\n- Ran\nOn Wed, Dec 4, 2013 at 3:05 PM, albertoandreottiATgmail \nnotifications@github.com wrote:\n\nHi rmanor, rasmus,\nthis is no exactly what my pull request was about, I wanted to have non\nsquared inputs(e.g., an spectrogram of 50x600). I'll work in a test case\nfor checking gradients when non squared inputs are present.\nI have a test case on language detection that uses non squared inputs, but\nit requires huge data(arround 3GBs) and it takes many hours(~15) to\ncomplete. I'll try to create a toy test case with artificial inputs. So\nrmanor, please do not work in the same thing, focus on the non squared\nkernel.\nContact me if you want to talk, I'll be glad.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/76#issuecomment-29802786\n.\n. Yes, CNNs. Pretty sure, I will re-check.\nWhy? Where did you see problems?\n- Ran\n\nOn Wed, Dec 4, 2013 at 3:17 PM, albertoandreottiATgmail \nnotifications@github.com wrote:\n\nHey Ran, are you talking about the CNN? are you sure it worked fine?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rasmusbergpalm/DeepLearnToolbox/pull/76#issuecomment-29803498\n.\n. \n",
    "Krzysiaczek99": "path was set otherwise I would not be able to call the functions. It look you have obvious bugs in this part of the code\nassert(all(x>=0) && all(x<=1), \"all data in x must be in [0:1]\");\nMATLAB is not using \"  but '  for text  !!!! \nit was a reason of this bug\ndbn = dbntrain(dbn, train_x, opts);\nError: File: rbmtrain.m Line: 3 Column:\n36\nDid you ever run it ???\n. yes, I know this and I tried already as I had similar problem with another  algorithm so I created dummy column vector filled with 0 and merged with my class vector but it did not change the result. Anyway I will try again and let you  know.\nKrzysztof\n. So I made class binary but output still the same....Any idea ??\nKrzysztof\nytest1 = zeros(size(ytest,1),1);\nytest1 = [ytest1 ytest1];\nytrain1 = zeros(size(ytrain,1),1);\nytrain1 = [ytrain1 ytrain1];\nfor i = 1:size(ytrain,1)\n    if ytrain(i) == 1 %class 1\n        ytrain1(i,2)=1;\n    else              %class 0\n        ytrain1(i,1)=1;\n    end\nend\nfor i = 1:size(ytest,1)\n    if ytest(i) == 1 %class 1\n        ytest1(i,2)=1;\n    else             %class 0\n        ytest1(i,1)=1;\n    end\nend\n%  Setup and train a stacked denoising autoencoder (SDAE)\nrand('state',0)\nsae = saesetup([size(xtrain,2) size(xtrain,2)]);\nsae.ae{1}.activation_function       = 'tanh_opt';\nsae.ae{1}.learningRate              = 0.1;\nsae.ae{1}.inputZeroMaskedFraction   = 0.5;\nopts.numepochs =   epoch;\nopts.batchsize = 100;\nsae = saetrain(sae, xtrain, opts);\nvisualize(sae.ae{1}.W{1}(:,2:end)')\n% Use the SDAE to initialize a FFNN\nnn = nnsetup([size(xtrain,2) size(xtrain,2),2]);\nnn.activation_function              = 'tanh_opt';\nnn.learningRate                     = 0.1;\nnn.W{1} = sae.ae{1}.W{1};\n% Train the FFNN\nopts.numepochs =   epoch;\nopts.batchsize = 100;\nmodel = nntrain(nn, xtrain, ytrain1, opts);\nclasses = nnpredict(model, xtest);\n. output is just column vector filled with 1\n1\n1\n1\n1\netc\n. xtest 1440x170 just 2 rows\n0   0   0   0   0   1   1   1   1   1   0.218547300472350   0.225937358052214   0.216674597059053   0.227945339431778   0.215153242184393   0.232003580347566   0.213872705753537   0.236579612797980   0.212182452883033   0.243243330424282   0   0   0   0   0   0   0   0   0   0   0   0   1   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0.533333333333284   0.356432767281168   0.450007656795560   0.502875538475751   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0.242677824271872   0.531250000000759   0.516135099553724   0.478465434292333   1   1   1   1   1   1   1   1   1   1   0.222542452095340   0.220399647944716   0.221510447370142   0.219182395265049   0.202468488630368   0.220306710191902   0.218403786269454   0.214079443890809   0.207328218865762   0.197718920079906   0.253968253968103   0.117647058823481   0.401889583041499   0.173353188821610   0.188235294117570   0.108843537414918   0.325665569634448   0.258050048644223   0.302642836771541   0.313105339847422   0.385941763206152   0.434534442686124   1   0.482758620689655   0   0   0.800000000000000   0.233515692484883   0.225147613977348   0.238281249999999   0.231226053639845   0.301481127568084   0.317084942084939   0.326128722382317   0.336146272855128   0.328941176470580   0.332712022367191\nxtrain 20000x170 just 2 rows\n1   1   1   0   0   1   1   1   1   1   0.865834623192517   0.869917195396788   0.864766731642010   0.870934784521438   0.862723005967957   0.871849972786718   0.859456053150873   0.872814536708904   0.854528933794986   0.874123920184252   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0.510204081632611   0.421482225462971   0.562895808000664   0.732766513453159   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0.214856230036509   0.364406779661790   0.490646746266447   0.498307696731945   1   1   1   0   0   1   1   1   0   0   0.867405359090200   0.868207451836880   0.869456998755480   0.867677871528134   0.863635661024698   0.867957405495072   0.868747544950211   0.868254556980696   0.867715520753443   0.867750538758196   0.292307692307540   0.471264367816036   0.391789468517534   0.690796336936732   0.292307692307540   0.530612244897820   0.485168748022951   0.771373684052620   0.304781823367239   0.335047570069438   0.414195404869440   0.455072540858243   0   0.833147942157953   0.0869565217391304  0.542372881355932   1   0.864958249352145   0.862452811925273   0.868807164634146   0.865613026819923   0.321070234113703   0.315637065637064   0.312680115273764   0.331458040318794   0.317647058823521   0.319664492078278\n1   1   1   0   0   1   1   1   1   1   0.866006185749419   0.870192975585101   0.864912014769438   0.871236322867946   0.862841680014431   0.872176071257371   0.859547931897117   0.873164661680746   0.854594032655948   0.874496728152717   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0.376623376623208   0.418341380915570   0.570430033749145   0.728609744228968   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0.300536672633351   0.365572315883730   0.489469652649986   0.496260150482426   1   1   1   0   0   1   1   1   0   0   0.866548527808070   0.868037943736349   0.869401323115209   0.867687754848477   0.863737814651655   0.867686066862982   0.868626223274084   0.868244179420492   0.867764199435538   0.867842944777933   0.292307692307540   0.386666666666521   0.341300768855299   0.633025786902175   0.292307692307540   0.530612244897820   0.417620942481458   0.750442389395922   0.303355832303451   0.335004714151029   0.414016489988198   0.455072540858243   0   0.833147942157953   0.0869565217391304  0.559322033898305   1   0.865246184854592   0.862259219823830   0.868997713414633   0.865613026819923   0.319159101767797   0.326254826254819   0.324687800192116   0.331926863572421   0.317176470588234   0.311742777260012\nytest 1440x1\n0\n0\n0\nytest1 1440x2\n1   0\n1   0\n1   0\n1   0\n1   0\nytrain 20000x1\n1\n1\n1\n1\nytrain1 20000x2\n0   1\n0   1\n0   1\n0   1\n. always 1 row wise\nytest1 = zeros(size(ytest,1),1);\nytest1 = [ytest1 ytest1];\nytrain1 = zeros(size(ytrain,1),1);\nytrain1 = [ytrain1 ytrain1];\nfor i = 1:size(ytrain,1)\nif ytrain(i) == 1 %class 1\nytrain1(i,2)=1;\nelse %class 0\nytrain1(i,1)=1;\nend\nend\nfor i = 1:size(ytest,1)\nif ytest(i) == 1 %class 1\nytest1(i,2)=1;\nelse %class 0\nytest1(i,1)=1;\nend\nend\n. maybe i should mail you my xtrain ytrain xtest ytest ???\n. sum(ytrain1)\nans =\n16076        3924\nsum(ytest1)\nans =\n1062         378\nnote ytest/ytest1 is not used by this example\n. so sums are matching i.e. 20000 and 1440\n. why the answer is 1400x1 ?? I would expect 1440x2 as binary class ??\n. so is it correct 1440x1 or not ??\nWhat is your email address ??\n. I think it works !!! The data what i was trying was very hard so it returned all 1s which corresponds to class 0 for me, Than I tried another test data set and it returns both classes !!! \nFor 1st data set a lot of algorithms returns class 0 (so 1 as an class index) for all test instances so it is not surprise for me.\n. ",
    "stevenwudi": "HI rmanor, you can find the max pooling code here https://github.com/stevenwudi/DeepLearnToolbox/tree/master/CNN\nI need to reorganize my code base, but I think you should be able to sort it out.\nBtw, there is a max3d function which I found it a bit confusing, hope you can shine some light\n. ",
    "provemyself": "hi\uff0cstevenwudi .  Recently, I faced the same question. I wonder that if your question has been solved. Could you share the solution with me ? \n. ",
    "taygunk": "anyone has a progress on this issue?\n. for test_example_CNN of the library:\nAssuming you already runned the network on test data, it will contain activations of the last batch you fed into the network. The following code will visualize the average result of 4th filter of first conv layer over your batch:\nfor features of a given layer:\nfiltNo = 4\nfirstConvLayer = 2 \nlastActivations = cnn.layers{firstConvLayer}.a{filtNo};\nlastActivations = sum(lastActivations,3); % sum over batch instances\nimagesc(lastActivations)\nfor weights of a conv kernel\nimagesc( cnn.layers{firstConvLayer }.k{1}{filtNo} )\n. Using global variables is a bad practice. What you should do is to inspect the input shape, and come up with choosing correct batchsize and numberOfChannels from the input data. \n. I think initializing weights to 0 is theoretically wrong (Symmetry breaking). From the reference http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm \n\"If all the parameters start off at identical values, then all the hidden layer units will end up learning the same function of the input (more formally, W^{(1)}_{ij} will be the same for all values of i, so that a^{(2)}_1 = a^{(2)}_2 = a^{(2)}_3 = \\ldots for any input x). The random initialization serves the purpose of symmetry breaking. \"\n. Modifying params depends on your problem and input dims. Having a very\nlarge input will require to have bigger pooling units\nWell, normally in the end of conv layers you have 1x1 feature maps generally. Then you have a\nfully connected layer.\nHowever in DLT, I remember that after convolutions sizes were not 1x1, and\nprogramatically reduced to 1x1 with full connections, which means that you\ndo not have to worry about kernel sizes very precisely (otherwise, you have\nto pick conv and pooling sizes such that at the end of this operations you\nhave 1 dimension)\nBut as I said, one cant say what is the optimized kernel size for your application.\nOn Thu, Sep 25, 2014 at 3:30 PM, Clinuxyj notifications@github.com wrote:\n\nHii,\nI'm using the code about CNN,but I don't know that .\n%% ex1 Train a 6c-2s-12c-2s Convolutional neural network\n%will run 1 epoch in about 200 second and get around 11% error.\n%With 100 epochs you'll get around 1.2% error\nrand('state',0)\ncnn.layers = {\nstruct('type', 'i') %input layer\nstruct('type', 'c', 'outputmaps', 6, 'kernelsize', 5) %convolution layer\nstruct('type', 's', 'scale', 2) %sub sampling layer\nstruct('type', 'c', 'outputmaps', 12, 'kernelsize', 5) %convolution layer\nstruct('type', 's', 'scale', 2) %subsampling layer\n};\nActually, I want to know why the layer of CNN is 6c-2s-12c-2s and why\nkernelsize is 5. Could I modify these parameters? Is it a experienced\nmodel? what can I modify it according to?\nThanks\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/117.\n\n\nTaygun Keke\u00e7\nPhD Student,\nComputer Vision Laboratory\nTechnical University of Delft\nMekelweg 4, 2628CD\n+31645165967\n+905414677824\n. ",
    "russellfei": "Hi, about two month ago, I was testing the CNN code in this toolbox, and I wrote a script to display the kernels and feature maps for each layer (even with more layers, all will be fine) with input options controlled. This script might be helpful to you. However, I just have to spend some time to comment that script and figure out how to commit to github @_@.\n. ",
    "dinosg": "OK, I reduced the learning weight to 0.01, that helped and I get a fit. However using the dbn routines in addition to the nn training doesn't help any (the dbn routines do reduce error by a factor of 2 on the NIST handwriting recognition database). Any idea why dbn doesn't work as well with linear data or am I missing something?\n. you can probably do that if you want but then have to make sure that \nsize of the last batch is truncated so you don't run out of data and \ncause an error by exceeding an array index\nOn 11/1/14, 8:54 PM, Eta_li wrote:\n\nHi, I am confused by some part of nntrain.m\nIn nntrain.m:\nnumbatches = m / batchsize;\nassert(rem(numbatches, 1) == 0, 'numbatches must be a integer');\nThe size of batch(variable batchsize) is supposed to have this \nproperty \"mod(m,batchsize) == 0\". It seems not necessary.\nWhy not set numbatches=ceil(m / batchsize) ?\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/122.\n. \n",
    "Duanexiao": "Hello,may I ask you some questions?firstly,I know you have used the DBN+NN for regression prediction,Can you share me some big data (the feature and the number of dataset are both big enough for using DBN+NN)\uff1fI have downloaded the blogDatatrain dataset for UCI,but when I test DBN+NN for regression prediction on it,the outcome is so bad, and the outputs are all the same!\nSecondly,when I use the Matlab's house_dataset ,and split it into three parts,train_x,val_x,test_x.using the nn for regression prediction,it throws an error in nnsetup,that caused by the inner nn toolbox in Matlab.I change the nnsetup to nnsetups,but another problem occurs,none-existence field batchsize in \nbatchsize = opts.batchsize(in nntrain.m),why?\nThank you in advance!\n. @reactgary may I ask you,what's the version of your matlab?when I add the validation dataset,the nntrain throw an error, it says the batchsize is not existent in line batchsize = opts.batchsize.does this is also that reason?\n. have solved this problem?\n. Can you tell me how to solve it ? I have met the same problem you refered above.Thank you!\n. Thank you very much!\n. ",
    "sundw2014": "Maybe, it's 'learning rate', but not 'learning weight'. \nThat worked! Thank you.\n. ",
    "AK747": "Hello.I will ask a problem about convolutional neural network. There are  forty-five gray pictures (256*256),I use them  as traing sample, which is divided into three categoriesapple\uff08apple, camel , car \uff09, each category 15 pictures:  apple is set label 1, camel is label 2, car is label 3.Then  I also put them into 100,010,001.  I choose extra fifteen pictures  as traing sample.My mat documents types are the same with handwritten database. I  simply modify the main function data, while I haven't changed other functions. Regardless of the iteration number, the results are the same. Divided into three categories, the error rate is 2/3.\nAccordingly, I also test ohers, if divided into four  categories, the error rate is 3/4;Divided into five categories, the error rate is 4/5......\nI found the function of cnntest (~, h] = max (net.o) was wrong.I got the results are the same value, such as, all is 1 or 2 or  3 ...... Why  this was happen? Why not get the right h?\n. ",
    "michealja": "no use of softmax Regression? why?\n. ",
    "eric-haibin-lin": "Yes, I also agree on this. Currently it's doing a N binary class prediction. I think this is an important to fix. \n. ",
    "tambetm": "Binary class prediction has its merits as well - if final dataset, that you are going to apply your prediction on, has inputs that don't belong to any class (i.e. letters A, B, C), then using binary logistic units allows you to set threshold and not classify these at all. When using Softmax, all probability must be allocated between classes, even if their scores are very low.\nBut I find @JasperSnoek's comment interesting, that backpropagating through Softmax results in better model. Why would that be? Would it make sense to train using Softmax, but use binary logistic units at prediction time?\n. Actually no, because y uses one-of-N coding - it is boolean matrix with dimensions [num_samples]x[num_classes], where value is 1, when this is the correct class for this sample. The data returned by nnpredict() has dimensions [num_samples]x1, because it returns class number for each sample instead. max() is needed exactly to convert from one-of-N coding to class number coding.\nIf there is anything wrong with this, then maybe having nnpredict() returning class numbers is limiting, because it doesn't account for using the network for function approximation (final layer is linear). I would have nnpredict() returning the final layer activations and nnpredictclass() returning the class number with maximum value.\nYour comment about ignoring return variable is appropriate though.\n. I you mean DBN as implemented in DeepLearnToolbox, then it is used for unsupervised learning and there are no classes involved at all. Yes, its input is binary, but that doesn't mean 1-of-N coding.\n. Some people have had good experience with https://www.whetlab.com/. Even if it doesn't optimize all parameters automatically, at least it gives you insight which hyperparameters actually matter and which don't. There is nice API for Matlab.\n. Try smaller learning rate. Usually you try learning rate in powers of 10, i.e. 0.1, 0.01, 0.001 and so on. Pick the first one, that makes your loss to decrease. Choosing good hyperparameters for deep networks is still an art, you can find few rules of thumb in these articles:\n- Y. Bengio, \"Practical recommendations for gradient-based training of deep architectures\", http://arxiv.org/abs/1206.5533\n- Y. LeCun et al, \"Efficient BackProp\", http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n- I. Sutskever, \"A Brief Overview of Deep Learning \", http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html\n- T. M. Breuel, \"The Effects of Hyperparameters on SGD Training of Neural Networks\", http://arxiv.org/abs/1508.02788\n. I would start with some well-known architecture. CIFAR-10 examples are good start, if your images are not too big. Otherwise AlexNet, but AlexNet is way too big for DeepLearnToolbox to handle.\nFor example CIFAR-10 network in Caffe examples has worked well for me:\nhttps://github.com/BVLC/caffe/blob/master/examples/cifar10/cifar10_quick_train_test.prototxt\nHopefully you can figure out the layer parameters from all this prototxt cruft.\n. There is pull request https://github.com/rasmusbergpalm/DeepLearnToolbox/pull/131 for adding ReLU to regular feed-forward network (nnff() and nnbp()). Maybe you can borrow some ideas from there. Certainly you need to add ReLU support to backprop as well.\nMy experience with DeepLearnToolbox CNN code is, that it is unbearably slow and rather limited. For example it doesn't support fully-connected layers at all. You may have better luck with MatConvNet, which seems to be quite full-featured, but admittedly more complex.\n. You can achieve fully connected layer using convolutional layer with filter size equal to input. For example if last convolutional/subsampling layer outputs 10x10 matrix, then you should add another convolutional layer with kernelsize=10 and outputmaps=. Then each filter is applied to input only once and resulting nodes are connected to all input node - therefore it's fully connected layer.\n. Sorry, I hadn't tried it myself, I just assumed it would work. Did you use 1x1 size for the second fully connected layer? Anyway, you are on your own here. \n. I guess you figured out the first question already - for training you need to give targets to the network using 1-of-N coding, where for each sample there is 1 for correct class and 0 for all others. nnpredict() returns just the correct class number.\nRegarding cross-entropy, it is already implemented, you have to set nn.output = 'softmax'. In case you need to add new loss function, in addition to modifying nnff() you also need to implement gradient calculation in nnbp().\nHowever looking at the loss function of sigmoid, it appears to be wrong in nnff(). The correct implementation would be:\nnn.L = -sum(sum(y .* log(nn.a{n}) + (1 - y) .* log((1 - nn.a{n})))) / m;\nDerivative appears to be correct though, so this doesn't affect learning, only reported loss is somewhat different what it should be.\n. Bias term is simulated by having one of the inputs 1 all time. Then bias corresponds to weight of this item. These lines in the start of nnff.m implement it:\nx = [ones(m,1) x];\nnn.a{1} = x;\n. Look into nnpredict(), basically after nnff() the nn.a{end} is the probabilities (softmax layer activations). nnpredict() just picks the class with highest probability.\n. The mean calculation for dropout is in nnff.m:\n%dropout\n    if(nn.dropoutFraction > 0)\n        if(nn.testing)\n            nn.a{i} = nn.a{i}.*(1 - nn.dropoutFraction);\n        else\n            nn.dropOutMask{i} = (rand(size(nn.a{i}))>nn.dropoutFraction);\n            nn.a{i} = nn.a{i}.*nn.dropOutMask{i};\n        end\n    end\nBasically in testing phase you compensate for not dropping nodes by multiplying all activations by (1 - dropoutFraction), which gives the next layer roughly the same amount of input as with dropout.\n. You could have suggested MatConvNet as well, because people coming here are supposedly working in Matlab.\nI think the code still works, is cleanly written and valuable as a learning tool. I've used it in neural network practicals and I've seen people on internet refer to it as reference implementation for dropout. Normal feed-forward networks and RBMs worked well for me, convolutional networks were too slow.\nThanks for creating it!\n. ",
    "billderose-zz": "Were you thinking of a direct port of http://learning.eng.cam.ac.uk/carl/code/minimize/minimize.m? I'm interested in the idea.\n. ",
    "paulgribble": "that would be the idea, simply incorporating the use of minimize.m into the existing code (e.g. as an option for a training method)\nthen again mini batch gradient descent is probably better ...\n. ",
    "mawady": "Thanks @taygunk for your reply\nthe presented code is just quick modification for original modification ... i already organized my implementation using some coding standards! :+1: \nmy problem is not selecting batch size or number of channels but the way i modify the code to accept N-dimensional data (not technical aspect), reason of not using adaptive learning rate, and  status of cnnnumgradcheck.m (not called by any m-file!)\nI already met some former PhD students of hinton (Ruslan & Volod) last week and already discussed my conceptual ambiguity and got quite reasonable answers for selecting CNN parameters :)\n. ",
    "pliskowski": "Backprop is indeed correct, however the fact that nnapplygrads multiplies the weights by weightPenaltyL2 is a direct result of taking derivatives with respect to the regularised loss function. Makes me wonder how is it possible that your numerical gradient checking tests pass given this discrepancy.\n. ",
    "markerdmann": "That worked perfectly, thank you!\n. ",
    "lyttonhao": "can you suggest how to modify the code inside CNN folder for regression?\n. all right, thanks.\n. I have modified \"feedforward to output\" in cnnff.m and \"output delta\" in cnnbp.m\n\n\nbut it doesn't work. Is there anything else to modify? Thanks.\n. ",
    "Tgaaly": "I see that linear output neurons are now supported so looks like regression is now possible. \nsee https://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/NN/nnff.m#L43\n. I'm having the same problem with the DBN example. The 2nd layer weights learnt by the DBN are pretty much garbage. The 1st layer is fine. Unlike Autoencoders there doesnt seem to be many options to set with the DBN. Any information about how to obtain good 2nd layer weights would be greatly appreciated. \n. So the final test error of running test_example_DBN.m is 0.07. This is the 'er' output of the following line: [er, bad] = nntest(nn, test_x, test_y)\nWhile the first layer weights looks good, I still dont know why my second layer is messed up. They look like this\n\n. anyone able to run the test_example_DBN.m and get decent looking weights for the second layer?\n. The 100 dimensions comes from the batch size. After training you can evaluate the SAE on all the data to get the reduced dimensions. \n. use nnff(.) and return the error variable nn.e(end) for the test instances.\n. After checking the code - different layers have to have the same learning rate. I did a pull request to allow this - Pull Request #143 \n. ",
    "supercz": "You'd better normalize the data firstly,i suppose.\n. OK, but i don't get it.\n------------------ \u539f\u59cb\u90ae\u4ef6 ------------------\n\u53d1\u4ef6\u4eba: \"mnaetr\"notifications@github.com; \n\u53d1\u9001\u65f6\u95f4: 2014\u5e747\u67087\u65e5(\u661f\u671f\u4e00) \u665a\u4e0a9:11\n\u6536\u4ef6\u4eba: \"rasmusbergpalm/DeepLearnToolbox\"DeepLearnToolbox@noreply.github.com; \n\u6284\u9001: \"\u4f19\u9890\"905833099@qq.com; \n\u4e3b\u9898: Re: [DeepLearnToolbox] CNN data normalization (#105)\nI tried, but I get better results if the data are not normalized\n\u2014\nReply to this email directly or view it on GitHub.\n. I suppose that usually there is a normalization process, but your experiment seems to be good,which confuses me. In fact i'm also a new learner in this field.\n------------------ \u539f\u59cb\u90ae\u4ef6 ------------------\n\u53d1\u4ef6\u4eba: \"mnaetr\"notifications@github.com; \n\u53d1\u9001\u65f6\u95f4: 2014\u5e747\u67087\u65e5(\u661f\u671f\u4e00) \u665a\u4e0a9:19\n\u6536\u4ef6\u4eba: \"rasmusbergpalm/DeepLearnToolbox\"DeepLearnToolbox@noreply.github.com; \n\u6284\u9001: \"\u4f19\u9890\"905833099@qq.com; \n\u4e3b\u9898: Re: [DeepLearnToolbox] CNN data normalization (#105)\nDon't you understand what I've done or don't understand why this happens?\n\u2014\nReply to this email directly or view it on GitHub.\n. You are welcome.\n------------------ \u539f\u59cb\u90ae\u4ef6 ------------------\n\u53d1\u4ef6\u4eba: \"mnaetr\"notifications@github.com; \n\u53d1\u9001\u65f6\u95f4: 2014\u5e747\u67087\u65e5(\u661f\u671f\u4e00) \u665a\u4e0a9:31\n\u6536\u4ef6\u4eba: \"rasmusbergpalm/DeepLearnToolbox\"DeepLearnToolbox@noreply.github.com; \n\u6284\u9001: \"\u4f19\u9890\"905833099@qq.com; \n\u4e3b\u9898: Re: [DeepLearnToolbox] CNN data normalization (#105)\nAll the papers I've read so far suggest to normalize the data (between 0 and 1 or -1 and 1, in different possible ways) to speed up and improve the training. So my results have confused me too. I will explore the problem better. Thanks anyway\n\u2014\nReply to this email directly or view it on GitHub.\n. I haven't studied the code yet, just used some test examples.\n. ",
    "mnaetr": "I tried, but I get better results if the data are not normalized\n\n. Don't you understand what I've done or don't understand why this happens?\n. All the papers I've read so far suggest to normalize the data (between 0 and 1 or -1 and 1, in different possible ways) to speed up and improve the training. So my results have confused me too. I will explore the problem better. Thanks anyway\n. Ok. How can I correctly choose the values of the different hyperparameters (learningRate, sparsityTarget, nonSparsityPenalty and weightPenalty)?\n. ",
    "yu239": "First Make sure you didn't visualize your second layer with just weight matrix because that is not correct. By doing this, you ignored the nonlinear activation function. \nIf you want to optimize higher layer, you have to solve optimization problems that try to maximize the activation for each hidden unit each time, given the learned parameters.\n. Visualizing higher layers is more difficult and does not have an analytic solution like the first layer (which you can derive in several minutes). For a reference, please see the 'google cat paper' (google 'google cat paper') and look at how they visualize high layers in the experiment.\n. ",
    "titubeta": "How did you do it? Please let us know before closing the thread.\n. Training different layers in SAE should be done like this (I think similar for CAE):\nsae = saesetup([768 500 300]);\nwhere 768 are input neurons, 500 in hidden  layer 1 and 300 in hidden layer2\n. This should be done like this:\nsae = saesetup([768 500 300]);\nwhere 768 are input neurons, 500 in hidden  layer 1 and 300 in hidden layer2\n. ",
    "lyzhangjm": "There is no example how to test a trained CAE. Could you provide a  complete example like SAE ?\nThank you.\n. ",
    "datanewb": "I don't believe the derivative of the sigmoid is equivalent to the derivative of the softmax. Have you performed a gradient check using softmax output?\n. Thank you for the derivation, mabdollahi. I see now that is quite easy.\nThank you for the link pakozm as this has been confusing me.\n. \"How come the errors on MNIST in the examples are less than state of the art? I get ~0.07 error on the 2-layer DBN-NN in the example. State-of-the-art, as far as I am aware, is higher than this.\"\nAre you referring to the test error? Do you mean .07%? That would indeed be a new record! You might get a publication out of this.\nHere is a link to the state of the art publications. http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\nThe best error rate is .21% using Hinton's dropout method.\n. The bias is properly handled in nnff.m.\nWhat scores do you get for the shifted and not shifted targets? \nOne possibility: When the data is centered around zero nnsetup creates a model that will make guesses near zero that have a relatively low error already. Look at the learning curves (and/or compare the models predictions vs. a model that always guesses the average, zero for the centered data. \nAnother possibility: The learning rate might need to be initialized smaller when the output is shifted by 1000 to account for the larger initial squared errors.\nThere is less to learn if the model's initial guesses are in the same range as the targets, so more iterations may be required.\n. I made the following changes to speed up training on the shifted targets:\n1) Increase the batch size to 100. The errors have to back propagate through a hidden layer before they can effect the weights directly tied to the input bias. When you average a bunch of error derivatives at once, gradient decent can focus it's efforts on the input bias weights. When you look at one sample at a time, gradient decent takes you back and forth across a steep ravine, where instead you'd like to be traveling down the ravine--towards a model which has large weights tied to the bias. Also, it is quicker because MATLAB's BLAS implementation can work its magic on matrices more so than single row vectors.\nGradient decent can work even with batch size = 1, but it will take forever.\n2) Set momentum at 0. In general, this is not the best idea, but since every data point is just a linear transformation (every part of the data set is exactly the same), momentum slows down training because it averages old costs with new costs.\ninput_train = randn(100000,10);\noutput_train = 1000 + sum(input_train,2);\ninput_val = randn(10,10);\noutput_val = 1000 +sum(input_val,2);\nnhu= 32;\nnn = nnsetup([size(input_train,2) nhu size(output_train,2)]);\nnn.output='linear';\nnn.learningRate = 0.01;\nnn.mometum = 0;\nopts.numepochs = 100;\nopts.batchsize = 100;\nopts.plot = 0;\nnn=nntrain(nn,input_train,output_train,opts);\nx=input_val;\nnn.testing = 1;\nnn = nnff(nn, x, zeros(size(x,1), nn.size(end)));\nnn.testing = 0;\nprediction = nn.a{end};\nscore = mean(sqrt(mean((output_val-prediction).^2)))\nscore = 0.0541\n. I think so. Increase the number of epochs by a factor of 100-- When we increased the batch size, we decreased the total number of steps in gradient decent. I chose to create a larger data set, because I figured it would be a little faster.\n. My mistake, I did not cross validate. Looks like overfitting. Increasing the epochs will not help--it will hurt.\nSo, this is a general learning problem. A larger training set it the best solution if possible. Note, NNs are highly non-linear beasts. A training set of only 1000 samples is very small without a lot of regularization.\n. It has nothing to do with the backpropagation implementation--That is totally solid. In general, however, there are different implementations of nntrain  that would do a better job with a small dataset. I think this toolbox is a great starting point, but for production, it must be extended.\nYou have to iterate over a bunch of hyperparameters to get a NN to work. It's not quite plug an play.\n. ",
    "moabaom": "well I did do the sort of trivial hand calculation and ended up with the exact result as sigmoid's of \"an (1 - an)\" : \n\nalso when i tried to look it up for confirmation i got the same answer like in e.g. http://www.cs.bham.ac.uk/~jxb/INC/l7.pdf   page 13 \nbut above all even if all this is wrong, by no mean I can see the derivative to be \"1\" ...as it is in the code ..can it ?\nand no i didn't do the gradient check as the math of it didnt sound right in first place.\n. Oh thank you very much Pakozm ! you're a life saver. The article you referred to answered precisely my question and based on that seems like the code is just fine if you pair the CE cost function with softmax activations.\n. ",
    "pakozm": "I'm not a user of DeepLearnToolbox, but I'm interested ;-)\nThe derivation of cross-entropy loss and softmax activation is coupled in the way that you can achieve better numerical stability if you compute the delta at output layer taking into account both functions. In this way, several NN tools implement the cross-entropy loss derivative to be the combination of cross-entropy+softmax derivative, and therefore the delta computation at softmax layer is reduced to a linear transformation. You can see more in the following paper:\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.49.6403\n. Your welcome :-) I was also a bit confused about this a time ago ;-)\n. ",
    "janehiii": "maybe you forgot normalization\n. ",
    "wajihullahbaig": "You may not be able to use the images for the network. This network works only for a single image single object aspect. If you have multiple objects in the image, then probably you need to have a look at R-CNNs. RCNNs are not part of this repo.  One source is https://github.com/rbgirshick/rcnn. Ah, really I want to thankyou @rasmusbergpalm for the code. Your code has flung me into deep neural networks. So given the fact that I needed to learn the internals, i thought of translating your code to python. It is still a work in progress and I am sure some will find the translation useful. \nhttps://github.com/wajihullahbaig/DeepNeuralNetworks\nYou might like to read the readme at the end to see the accuracy tests. \nHave a good day, cheers!\n. @bgat your suggestion is truly wonderful. This  toolbox has to be a beginners must read source. Plus a new maintainer would be great!\n. No offence, but this toolbox gives the  basic insight where caffe, torch, tensorflow etc actually are able to hide. \n. @rasmusbergpalm  cool dude. Really some interesting things are happening in deep learning, definitely some improvements or new networks should be added. Lets hope we find a contributor.\n. In nnff.m, the sigmoid is already being calculated and stored in nn.a. Just need  to do 1-sigm(x) in back propagation.\n. Well its clear now :)\n. refer to https://github.com/rasmusbergpalm/DeepLearnToolbox/issues/167\n. It is too vague to understand, may be you need to show how you setup the neural network. Make sure the term  [net.layers{l}.kernelsize - 1 net.layers{l}.kernelsize - 1 0] has the same dimension as size(net.layers{l - 1}.a{1})\n. Did you add the path to the folders of DeepLearnToolBox?\nYOu have to tell matlab the path to the whole tool box. I am afraid I did not understand the question clearly. You get many kernels because you are using a whole batch of images and push them through the convolution layer. I don;t understand your second question but still may be you did not understand maps correctly in convolution? \nA division by 255 keeps the data in the range of [ 0 , 1] and this is normal practice in computer vision. \n. And for a better understanding on CNN filters,maps etc here is a good link\nhttp://cs231n.github.io/convolutional-networks/. Max pooling. If you are talking about the CNN, its sigm() found here https://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/util/sigm.m \nwhich is inside the feed forward module, https://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/CNN/cnnff.m\nFor simple neural network, the tests show softmax output layer. Check the 'tests' folder for a better understanding.. Those are the very basic things my friend. You need to go through the understanding the mnist dataset. \nYou can find the dataset under 'data' folder. The dataset is divided into training and testing sets.\nTraining sets have images and labels, testing sets have images only for which a label is predicted. \nHope this helps. If you are comfortable with python, You can find a python translation of the code here.\nhttps://github.com/wajihullahbaig/DeepNeuralNetworks. You should first understand the concepts of  convolutional networks. Its too lengthy to explain here. Once you grasp the ideas, you will note that it is easy to follow code or write your own. I suggest you read  about them from link/blogs/videos by andrej karpathy.. You can add any sorts of layers - Just make sure you dont break the code. A ReLU is a good idea. \nI am note sure why you are getting array dimension errors, possibly your data that is being convolved with a filter is some how mismatched in terms of dimensions. You should check if convn() function parameters accepted. . The best you can do is to use change the code to use GPU array. Its is a very length task though.\nMay be you can write parallel code but Parallelism in Matlab can be difficult and tricky.. You might need to make a better split on testing/training sets e.g. 80/20, 70/30 etc. Change in  learning rates and in the end, if truly mean -inf,+inf values actually existing in your dataset. This means Matlab is going to mess things up. Try  normalizing your dataset to [0,1] or [-1,1]. At the moment I dont really have a link to point you to. Sorry!. @allinjoe  If you want to test it on real data. You still need training and testing.\nSay you have 800 samples. You give 600 for training and 200 for testing nowl\na) Once you have trained the network, you need to save the neural network and then load it to test it on anther sample set.\nb) Say this other sample set is of 500 images that you want to test.\nc) Load the neural network (trained on 600 images), then just test push your new test samples (500 images)  and see visually if it does a correct prediction. If you have labeled the data, you can make a comparison against the labeled data and the NN predictions.\nHope this helps.. @allinjoe  I dont understand the excel file. \nNo one is going to do it for you except yourself. Now following the path to tests folder\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/tests/test_example_NN.m\nAnd see the line#20 -  [er, bad] = nntest(nn, test_x, test_y);\nFollow the function and see how the errors are calculated.  This is where labeled data is compared against predictions. . One way is to convert the RGB images to gray scale Or re-write the code to support RGB images. By the way RGB images dont make sense. So just convert them to grayscale using Matlab functions :)\n. When the code is tested, the initial state must be same for every test. Otherwise you will never know what happened during any parameter tuning. Therefore to ensure a fixed initial stated rand/rng are used. Dont vary your kernel size. Keep it consistent. And then see the outputs. Flawed, I dont know but implementation details may vary for random state generators. I went through all this mess when I translated the Matlab of this repo  to Python.. What happens is that for a random initial state with fixed seed, the training/testing batches are always selected the same. This keeps the testing consistent. If you change kernel size, you are then bound to have different accuracy. Plus, try changing depth of the networks, and you will end up with different accuracy. So many tuning parameters that once can fiddle with,. ",
    "zillou2": "The score is very good when I remove the bias in the target (I remove the \"1000+\") and very bad with it. I have checked nnff and indeed the bias is properly handed. But I do not understand why it does not work on such a simple example.\n. Wow, this is great insight thanks.\nCan you make it work with the initial data ?\ninput_train = randn(1000,10);\noutput_train = 1000 + sum(input_train,2);\ninput_val = randn(1000,10);\noutput_val = 1000 +sum(input_val,2);\n. I have tried to run\ninput_train = randn(1000,10);\noutput_train = 1000 + sum(input_train,2);\ninput_val = randn(1000,10);\noutput_val = 1000 +sum(input_val,2);\nnhu= 32;\nnn = nnsetup([size(input_train,2) nhu size(output_train,2)]);\nnn.output='linear';\nnn.learningRate = 0.01;\nnn.mometum = 0;\nopts.numepochs = 10000;\nopts.batchsize = 100;\nopts.plot = 0;\nnn=nntrain(nn,input_train,output_train,opts, input_val, output_val);\nx=input_val;\nnn.testing = 1;\nnn = nnff(nn, x, zeros(size(x,1), nn.size(end)));\nnn.testing = 0;\nprediction = nn.a{end};\nscore = mean(sqrt(mean((output_val-prediction).^2)))\nand the result is:\n...\nepoch 9998/10000. Took 0.0082383 seconds. Mini-batch mean squared error on training set is 0.082187; Full-batch train mse = 0.086150, val mse = 1.799848\nepoch 9999/10000. Took 0.0080685 seconds. Mini-batch mean squared error on training set is 0.080941; Full-batch train mse = 0.079493, val mse = 1.796827\nepoch 10000/10000. Took 0.010358 seconds. Mini-batch mean squared error on training set is 0.079385; Full-batch train mse = 0.083484, val mse = 1.796972\nscore =\n1.7970\nDo I have to increase the number of epochs again more ?\n(anyway thanks again for you help)\n. So you say it is not a problem of the toolbox, and you think that it would give the same kind of results with other implementations of the backpropagation algorithm.\n. ",
    "Neil-Aframe": "How do I trigger a Travis build for this PR (just noticed PRs above and below this one have been tested)? Probably the most useful part of the PR is the new test.\n(Sorry just posted this using my work account - my personal account is \"neilslater\")\n. ",
    "tjbf": "yes!! i also think so !!\n. ",
    "mrgloom": "something like this:\n```\nclear;\nn=900; % number of samples per class\nw=28; \nh=28; \npos_path= 'C:\\positives';\nim_list= dir([pos_path filesep '*.png']);\nif(n>length(im_list))\n    n= length(im_list);\nend\npos= zeros(h,w,n,'uint8');\nfor i=1:n\n    im= imresize(rgb2gray(imread([pos_path filesep im_list(i).name])),[h w]);\n    imshow(im,[]);\nend\nneg_path= 'C:\\negatives';\nim_list= dir([neg_path filesep '*.png']);\nif(n>length(im_list))\n    n= length(im_list);\nend\nneg= zeros(h,w,n,'uint8');\nfor i=1:n\n    im= imresize(imread([neg_path filesep im_list(i).name]),[h w]);\n    neg(:,:,i)= im;\nend\ndata = cat(3, neg, pos);\npos_y= [zeros(n,1)';ones(n,1)'];\nneg_y= [ones(n,1)';zeros(n,1)'];\ny= uint8([pos_y neg_y]);\n%test-train split\nidx= randperm(n*2);\nper= 0.9; % 90%\ntrain_x= data(:,:,idx(1:pern2));\ntrain_y= y(:,idx(1:pern2));\ntest_x= data(:,:,idx(pern2+1:n2));\ntest_y= y(:,idx(pern2+1:n2));\n% train_x= double(data(:,:,idx(1:0.8n2)))/255;\n% train_y= double(y(:,idx(1:0.8n2)));\n% test_x= double(data(:,:,idx(0.8n2+1:n2)))/255;\n% test_y= double(y(:,idx(0.8n2+1:n2)));\nsave data/mydata_uint8.mat train_x train_y test_x test_y\n```\n. ",
    "reactgary": "Hi Oliver,\nLooks like you are using the new version of matlab. The most recent version of matlab have a method to declare custom neural network, which appear to have the same name \"nnsetup\" as the custom neural network function in deepLearning. My quick solution is to change all nnsetup call to a new name in deepLearning. Not sure if there is a more elegant method...\nGary\n. ",
    "krisdestruction": "also the proper way of ignoring a preceeding return variable is:\n[~,desiredVar] = fun(x)\n. Sorry I just saw. Thanks for clarifying.\n. On a side note, does DBN always use one-of-N coding?\n. Thank you\n. ",
    "zxzhijia": "Have you successfully changed the code to use CIFAR-10?. ",
    "Lepsoy": "Yes, thank you for reminding me. I had completely forgotten to close the question.\n. There wasn't really an issue with the code for my part, but rather the way the layers were initialized. The initial hidden biases were set to too large values which lead to the sigmoids producing more or less the same outputs due to the fact that the sum of the activations was smaller than the bias by several magnitudes. Also, the initialization of the regression layer was done with the pseudo-inverse method, but this is a very unstable approach and due to the nature of my training set, it gave values of size (10^8, 10^10). Initializing this to small random values and setting a lower initial hidden bias solved the problem for me. \n. ",
    "mongoose54": "Same here. Is there a documentation for configuring the CNN? \n. Thanks for the information. However I was interested in how to set up the structure of CNN here: https://github.com/rasmusbergpalm/DeepLearnToolbox/blob/master/tests/test_example_CNN.m#L15-L21 \n. ",
    "micmarty": "I found this formula in Andrej Karpathy's CNN course and it worked for me:\n(it's really simple after a while of thinking)\nIt assumes square images, vertical stride equals horizontal stride and a square kernel_size!\n```python\nin_channels = 3 # nearly always, because image has 3 channels (3 matrices -> red, green, blue)\nout_channels = (image_width - kernel_size + 2*padding) / stride + 1\nif you don't know what these variables mean, google it -> these are the basics of CNN\nin_channelsandout_channelsare the parameters for one convolution layer, but each following layer'sin_channelsequals to number ofout_channels``` from the previous one.\n. ",
    "oxinabox": "This is really useful.\nIn my PR: https://github.com/rasmusbergpalm/DeepLearnToolbox/pull/128\nfor DBNs it is also supported.\nBecause DBNs are RBMs and each RBM has a learning rate (alpha) parameter (in my PR, before I believe it was specced for DBNs in opts to the training function).\n. I attended a Matlab workshop/seminar (run by mathworks), two weeks ago.\nThey recommend your package, (still).\nAll the best in future endeavors.\n. @bgat, no offense to rasmusbergpalm -- this is a great project that helped a lot of people (including me).\nbut your statement:\n\nI already I can see that DeepLearnToolbox is NOT some kind of weary, inferior, abandoned alternative to the projects you refer us to.\n\nis just wrong.\nIt is abandonned, has been for years; the deprecation just makes it official.\nNo PR has been merged/closed (except this deprecation notice) since merged on 11 May 2014.\nThe last issue to be closed with any comment from rasmusbergpalm was #113 in 23 July 2014\nI think the last issue with any even vaguely serious discussion, was #74 in 1 dec 2013.\nrasmusbergplam has done the honourable thing and made this abandonment, and deprecation official. That saves people (like a past me) spending hours on a PR that is not even going to be discussed for merging. \nIt is inferior -- technology has come a way since 2013.\nParticularly since there is not a single technique in this package, that is more recent than 2006ish.\nMore recent/maintained libraries have a variety of new activation functions,\nnew training schemes (Adelta, ADAM, AdaGrad, and/or integration with numerical optimisers for CGD, or LBFGS etc).\nThey are even starting to be able to use each others pretrained models (Which has become quiet the thing recently, with tranfer learning, and hugely deep networks taking months to train). Mocha for example can import Cafe networks.\nIt is even kinda weary. It has a bit of tech-debt where things don't actually work the way they look like they are working -- particularly with how options are processed, and then partially reprocessed.\nIt has no automated Gradient Check based tests; which means it is hard to be sure the backproperagtion is actually calculated correctly (it is a notoriously fiddly algorithm like that, particularly since the network will still work, but worse it it is wrong). This is a particular concern for any new contributor, since there code can't be checked; and you can't exactly demand they have gradient checks with nothing else in the library does.\nMore notably, it has a core architectural piece of uglyness, in that Training State, is mixed into the same Object as the model itself. Which is a nice enough way to implement momentum, but it make a lot of enhancement awkward. Not impossible, just awkward.\nNice thing about open source though, is that you are free to do what you want.\nSomeone could pick this up, fork it, and maintain it.\nAnd fix all the above things.\nI just don't think it is a constructive use of anyone's time.\n. ",
    "primetang": "For example, if 'cnn' is your trained model, and 'img' is your image, do like this:\nnet = cnnff(cnn,img);\n[~, h] = max(net.o);\nThen 'h' will be the  predict label.\n. ",
    "bobbyali": "I tried this with the following code:\n% after running test_example_CNN.m\nimg = squeeze(test_x(:,:,1));\nnet = cnnff(cnn,img);\nAnd got the following error:\nMatrix dimensions must agree.\nError in cnnff (line 11)\nz = zeros(size(net.layers{l - 1}.a{1}) - [net.layers{l}.kernelsize - 1 net.layers{l}.kernelsize - 1 0]);\nAny suggestions on how to fix this? \n. The problem is because cnnff is looking for a 3rd dimension, and the single image case yields 2 dimensions only. As far as I can see, you can't artificially add a singleton 3rd dimension, so I figured out a hacky fix which involves changing two lines in cnnff to handle single image cases:\nif size(x,3) == 1\n         z = zeros([size(net.layers{l - 1}.a{1}) 1] - [net.layers{l}.kernelsize - 1 net.layers{l}.kernelsize - 1 0]);\n    else\n         z = zeros(size(net.layers{l - 1}.a{1}) - [net.layers{l}.kernelsize - 1 net.layers{l}.kernelsize - 1 0]);\n    end\nand\nif size(x,3) == 1            \n        net.fv = [net.fv; reshape(net.layers{n}.a{j}, sa(1) * sa(2), 1)];\n    else\n        net.fv = [net.fv; reshape(net.layers{n}.a{j}, sa(1) * sa(2), sa(3))];\n    end\n. ",
    "Sehla": "Hello,\nI am new new with CNN. My project is to train the CNN with a image dataset and test the prediction with new image. I already download the caltech-101image dataset but how can i launch the training step?\nThanks \n. ",
    "Chuanhai": "Yeah, I can get one fully connected layer according to what you said. But the problem is when I use convolutional layers to get more fully connected layers, the program crashed.It seems like that your code does not provide two consecutive convolutional layers based on my test. Thanks.\n. Yes, I used 1x1 size for the second fully connected layer. But the program\ncrashed. Thanks.\n2015-07-03 6:09 GMT-05:00 tambetm notifications@github.com:\n\nSorry, I hadn't tried it myself, I just assumed it would work. Did you use\n1x1 size for the second fully connected layer? Anyway, you are on your own\nhere.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/146#issuecomment-118317083\n.\n. \n",
    "rccheung": "Have you tried adding in another sub-sampling layer before adding another 1x1 convolutional layer? I suppose if you have the 1x1 convolutional layer before this the sub-sampling layer won't do anything, i.e. it will simply pass all the convolutional maps unchanged to the next layer. \n. ",
    "shay86": "I built a project using (Merge, merge) layer once, and use share layer in another to combine two CNN's. The problem I can't find any documentation that explains how merge or share layer works like convolution or max-pooling layers.\nI will be grateful if anyone can direct me or suggest some paper to me ???. ",
    "JohanBook": "Thank you very much!\n. ",
    "champnaman": "Thanks a lot for your help.\n. ",
    "Dallas0608": "Did you solve it?. solved it?. ",
    "haoqi": "That's correct\n. ",
    "preksha12": "Convn function is also used in subsampling operation in cnnff function but in subsampling layer(s) we only perform pooling so convolution operation is not executed on this layer but still it seems that calling the function convn will perform convolution operation.\n. ",
    "JannieLee": "Hello,@Duanexiao recently I also try dbn+nn for a regression problem, but the fitted outputs are constant. Have you already found out the reason for this issue, please? \nIs there anything wrong? OR dbn+nn in this toobox are designed for some certain datasets?\n. ",
    "grunterGah": "the problem is that the output activation function is not 'linear' by default, but \"'sigm\". This will always return an output between 0 and 1 which is not what you want for regression problems, so set it to \"linear\". Having looked at the code though, there so many other implementation issues (drop out implementation is flat out wrong - there is no calculation of the mean net anywhere), so,  don't bother with this toolbox.\n. ",
    "stutys": "@grunterGah the pasted coded in the second part:\nnn.output = 'linear';\nso i feel  output activation function is not the key.\n. ",
    "senyeer": "So. I also encounter the problem. How can I solve it? The outputs are constant now. @tambetm Can you help me?\n. I have the same problems with you.Have you solved that?. ",
    "oakkas": "before load mnist_unit8:\nadd these: \naddpath(genpath('../data'))\naddpath(genpath('../CNN'))\n. ",
    "bgat": "I'm just starting with neural networks, but already I can see that DeepLearnToolbox is NOT some kind of weary, inferior, abandoned alternative to the projects you refer us to. The integration with Octave makes it a really, REALLY useful and one-of-a-kind toolbox both for learning AND using of neural networks.\nInstead of calling it a \"deprecation notice\", why not instead describe it as a call for a new maintainer? I really, really hope that DeepLearnToolbox doesn't someday end up bit-rotting in the big bucket at the end of the internet.\n. Fair enough. Like I said, I'm new to neural networks and so your insight is greatly appreciated. And reading it from your point of view, and thinking towards the future, I'm inclined to agree with you.\nI'm working with some pretty large (for me) datasets, however, and Octave makes that VERY convenient. Is there an Octave-friendly alternative?\nTorch, Theano, and TensorFlow all seem great, but they're all not-Octave. And since our team is using Octave in other parts of the project already, you can see why I'm keen to stick with one environment if I can. And that's even before I mention that we have no Python and very little C++ fu.\n. All the above notwithstanding, the simplicity of DeepLearnToolbox combined with the familiarity of Octave has definitely helped me come up to speed with neural network concepts pretty quickly.\nAs such, I still think there's a place for something like DeepLearnToolbox: doesn't try to do everything, but does what it does in a straightforward, clear, teachable, interactive, and relatively robust fashion. Very useful for beginners, even if we know that we'll have to abandon it later as our skills and needs grow.\n. ",
    "14353wangkai": "This error means you are using the default function in matlab, which is arisen by forgetting adding path of deeplearningtoobox. ",
    "GrifisJP": "Thank you.\nI see, so this is why every iteration in training, nnff is also called before nnbp to update nn.a.\nWith Bishop notation in his book,  nn.a is rewritten to --->  z = sigmoid( nn.a ) \n. ",
    "TYS11": "Thanks, solved it.\nI wonder why I get so many Kernels at second Convolution layer.\nI wanted to have a 8 filters(aka weights) at both of the Conv layers.\nSo for First conv layer I set outputmaps to 8, for second layer I set outputmaps to 8 as well, \nbut the filters it produced is 64 at second Conv layer, instead of 8?. by the way, your pooling layer uses max pooling or mean pooling?. thanks, much obliged. . One last thing, softmax classifier in the output layer?. ",
    "Addhi86": "Thanks Wajihullah baig,  Yes, you are right it was the basic query.\ni am more comfortable with MATLAB. \nsome other things which I want to know is:\n1)  struct('type', 'c', 'outputmaps', 6, 'kernelsize', 5) %convolution layer\nI have confusion in understanding what is meant by 'outputmaps'? 'kernelsize' 5 mean the filter size is 5x5.\n2)  struct('type', 's', 'scale', 2) %sub sampling layer\nThis sub sampling layer with the size 2x2? is it possible to add ReLu layer?\n3) What is opt.alpha?\n4) In the test_example_CNN if i add more convolution and sub sampling layer I got an error. what is the reason?\ncnn.layers = {\n    struct('type', 'i') %input layer\n    struct('type', 'c', 'outputmaps', 6, 'kernelsize', 5) %convolution layer\n    struct('type', 's', 'scale', 2) %sub sampling layer\n    struct('type', 'c', 'outputmaps', 12, 'kernelsize', 5) %convolution layer\n    struct('type', 's', 'scale', 2) %sub sampling layer\n    struct('type', 'c', 'outputmaps', 18, 'kernelsize', 5) %convolution layer\n    struct('type', 's', 'scale', 2) %subsampling layer\n};\nError:\nArray dimensions must match for binary array op.\nError in cnnbp (line 32)\n                     z = z + convn(net.layers{l + 1}.d{j}, rot180(net.layers{l + 1}.k{i}{j}), 'full');\nError in cnntrain (line 17)\n            net = cnnbp(net, batch_y);\nError in test_example_CNN (line 31)\ncnn = cnntrain(cnn, train_x, train_y, opts);\n. Thanks I will check that.. Is it too lengthy to give answer of each question? . ",
    "vgupta123": "Thanks for reply wajihullahbaig !!\nOne more thing the implementation of SDA (separated layer in SDA for Encoding and Decoding) is with untied weight. Can you recommend me to some Matlab source where tied weight is use.. ",
    "wangruinan": "how can I help you? please give me an email in detail to clarify your question.. ",
    "allinjoe": "Dear Sir,\nThanks for your Kind Consideration on my issue.\nI had 100 samples. I trained my NN.m with 80 samples and validated using\nthe remaining 20 samples. Now I need to give real data other than those 100\nsamples used for training and validation. I will give only the input values\nof samples and the system needs to find the output values based on the\ntraining provided. I felt that it was not possible using your toolbox. Help\nme in resolving this issue. Thank You.\nOn Mon, Jun 26, 2017 at 1:17 PM, wangruinan notifications@github.com\nwrote:\n\nhow can I help you? please give me an email in detail to clarify your\nquestion.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rasmusbergpalm/DeepLearnToolbox/issues/175#issuecomment-310986977,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AcR3ZhM-ZIHYcgZpt1ZbTfnXAAoYOjrYks5sH2IJgaJpZM4ODRnS\n.\n\n\n-- \nRegards,\nMr.D.Allin Joe\nAssistant Professor - I,\nDepartment of ECE,\nKumaraguru College of Technology,\nCoimbatore - 641049.\nallinjoe.d.ece@kct.ac.in\n. Sir,\nWhere can i find the output values of validation data before mse calculation? kindly help. TRAINING DATA.xlsx\nKindly reply this issue with the attached excel file not for images.. ",
    "lcayer": "Thank you for your answer\uff01\n\u57282017\u5e7406\u670829 12\u65f647\u5206, \"wajihullahbaig\"notifications@github.com\u5199\u9053:\n@allinjoe  If you want to test it on real data. You still need training and testing.\nSay you have 800 samples. You give 600 for training and 200 for testing nowl\na) Once you have trained the network, you need to save the neural network and then load it to rest it on anther sample set.\nb) Say this other sample set is of 500 images that you want to test.\nc) Load the neural network (trained on 600 images), then just test push your new test samples (500 images)  and see visually if it does a correct prediction. If you have labeled the data, you can make a comparison against the labeled data and the NN predictions.\nHope this helps.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.. ",
    "f123gh": "hye!\nI need some help regarding Matlab. \nFrom the point cloud 3D data I have to extract specific square area for which i know only 2 corner information i.e upper left x,y,z coordinate and bottom right x,y,z coordinates.\nFor this i've to apply some codes in MATLAB through which i get my area of interest(AOI) i.e specific square area which i mentioned above.\ncould you please check my code.Because there accour an error.\nM= dlmread('sample_data.txt');\nx= -7.836000;\ny= 20.534000;\nz= 4.541000;\nx1= -7.095000;\ny1= 20.013000;\nz1= 4.675000;\ndiscard = M(:,1) < x(1) | M(:,2) < y(2) | M(:,3) < z(3) | ...\n          M(:,1) > x1(1) | M(:,2) > y1(2) | M(:,3) > z1(3) ;\nY(discard,:) = [];\nThanks.\n. ",
    "Badarmaqsood4417": "I want to run this example  Stacked Auto-Encoders in my PC that is uploaded at this link https://github.com/rasmusbergpalm/DeepLearnToolbox \nkindly help me \nBadar_maqsood@hotmail.com. ",
    "ShirleyGuo": "Have you solved the problem?. ",
    "victor158128": "@wajihullahbaig \nThank you for the thorough explanation. I tried different both rand() and rng() with different seeds. Like you said, they give you different initial states. To ensure the initial state is consistent, use the same function and seed. All make sense now. \nI still don't understand how the error rate is affected by random number generator functions with the same kernel size. For example, when the kernel size is set to 7 or 15, with both rand('state', 0) and rng(0) produce error of 35%. When the kernel size is set to 11, rand('state', 0) produces 35% error. rng(0) produces 99% error.\n. ",
    "joey0111": "You can try to change the initial weight of the RBM and increase the number of training.. A random number between 0 and 1 is not enough. You can try multiplying all the initial weights by 0.01 or a smaller number. Also, have you normalized the input data?. ",
    "SJHMAX": "@joey0111 \nthanks for your help.\ni tried to set the initial weight to a random number between 0 and 1, and increased the number of training.The result is still the same as before.. @joey0111 \nthanks....but it si still not working...and All data has been normalized.\nThe data source is the open bearing fault data set of Case Western Reserve University, USA, which is the original time domain signal. The training samples include 10 types of faults, each of which has 600 training samples, a total of 6000 groups, and 128 data points per group (all normalized).\n. "
}
{
    "mbostock": "Thanks for the fix, and for signing the CLA!\nI added a simple test to go along with this. If you have a suggestion for a better testing mechanism for requests, let me know!\n. Looks good. Thank you!\n. Good catch on the test framework.\nI'm a fan of parsimonious APIs: if there's only one way to do something, then there's less to learn (and to implement, document, and test). If you want to post a single event, it seems reasonable to wrap the event in square brackets, sending \"[event]\" rather than \"event\". That's only two additional characters. Also, it allows the API to indicate that you can batch multiple events together for greater efficiency.\n. Nice, thank you!\n. Hooray!\n. Hi @mrkurt, would you mind signing the cla so I can merge this request? Thanks!\n. Merged into #40 for 0.2.0 staging.\n. Thanks, yes, the README is out-of-date from when we switched WebSocket implementations.\n. Retitled this pull request, as the latest release 0.0.13 upgrades the websocket dependency and thus adds support for Node 0.6.x.\n. I've added UDP support in 0.2.0; see commit 1918af1cecd5b53c824ebd6f98a4578aecd8f3f6. If you want to send a pull request with 0MQ support, that'd be great too!\n. Thanks for this contribution. However, the Cube front-end will be changing dramatically in 0.2.0 (for the better!), so there's no need for boards.\n. Thanks for this contribution. However, the Cube front-end will be changing dramatically in 0.2.0 (for the better!), so there's no need for new block types.\n. Thanks for this contribution. However, the Cube front-end will be changing dramatically in 0.2.0 (for the better!), so there's no need for new block types.\n. http://collectd.org/wiki/index.php/High_resolution_time_format\n. BTW, if you want to update this pull request to distinguish automatically between UNIX epoch seconds and Timestamps Of Unusual Size (T.O.U.S.'s) then have at it! :smiley:\n. Thanks for this!\n. I'll work on this today.\n. This has been folded in #40, the staging branch for 0.2.0. Feel free to pull and try it out, but note that it comes with a couple other changes.\n. I like it.\nI think it could be further improved by ensuring only one POST request at-a-time, which is to say, the next request should not be kicked off until the response is received from the server. If there's an error, or the status code is not 200, then you'd want to push the event back onto the queue for retry.\nTo reduce overhead, each POST should send an array of events, rather than just a single event. For example, you could send every queued event since the last POST. Though, you'd likely want to limit the number of events per POST to a reasonable amount (say, 500); you probably don't want to post tens of thousands of events in a single request.\nI'd also be tempted to use a setTimeout rather than process.nextTick when using POST, so that we don't DOS the collector. If you can post 500 events in a single request, then a timeout of 500 ms would mean 1,000 events per second at 2 requests per second which I think is more than reasonable.\n. Right on, that looks good. Sorry to ask the obvious, but have you had a chance to test it locally, too?\n. And, could I please ask you to sign the CLI? I know, it's annoying, but it's required. :hammer:\n. Looks good!\n. This sounds reasonable. I think if the plugin name is persisted in the collection name, we don't also need the plugin name in the event data, though.\n. I don't think this is the correct fix, though perhaps another fix is possible.\nAssume your Cube application is in the directory foo. The expected behavior is npm install cube to install Cube into foo/node_modules/cube, and then create a binary, such as foo/bin/evaluator.js:\n``` js\nvar options = require(\"./evaluator-options\"),\n    cube = require(\"cube\"),\n    server = cube.server(options);\nserver.register = function(db, endpoints) {\n  cube.evaluator.register(db, endpoints);\n};\nserver.start();\n```\nTo start the evaluator, you cd foo and node bin/evaluator. Any static files you want to serve should be in foo/static. However, with your change, Cube will serve from foo/node_modules/cube/static instead. The goal is to allow people to create their own custom dashboards (by copying over the example content and modifying it to suit their needs).\nFeel free to reopen this or open a related issue if you'd like different behavior.\n. Related #60.\nI would rather this be implemented as a wildcard operator, rather than adding a new query parameter.\n. Yes, I agree: type(*). You can already get type(prop.*) by saying type(prop).\n. Right, we want the event expression \"event\" to only return the event times\u2014it's an existence check.\n. Related #59 #86\n. Related #59\n. I think you could change the + to a * to fix that. I can't remember why I required two letters\u2026\n. Does this mean you can't send two events with the same type and time?\n. Cool, thanks for the clarification.\n. Fixed in 0.2.4. Thank you!\n. According to the API reference event times must be reported in ISO 8601 UTC format; this is supported by new Date in V8. Probably the correct fix is to report an error when the time is not in the correct format rather than extending it to arbitrary undocumented date representations.\n. Not aware of any gotchas. Everything seemed to work out of the box.\n. The current behavior is intentional. If no id was specified when the event was sent to Cube, then an id is generated automatically, and it is an instance of ObjectID. On the other hand, if an id is specified, the id is necessarily not an instance of ObjectID (because Cube\u2019s interface is strictly JSON; it\u2019s typically a number or a string), and so this id is returned.\nIf you want ids on your events, you have to specify an id when you send an event. Cube does not currently expose any auto-generated ids.\n. This change seems unrelated. Also, you've effectively changed it to /^[a-zA-Z0-9_]+$/, which means that event types can start with a number (which is bad).\n. ",
    "aperiodic": "That's a fair point on the API. I was just frustrated that the events I was sending\u2013which were valid JSON\u2013elicited 400 responses from the collector, and that I had to dig into the source to figure out the underlying cause.\nSince this is really just a documentation issue, I've added a paragraph on sending events to the end of the main wiki page, and added a commit that reverts the collector changes while keeping the new tests (one is modified to ensure that plain events are now rejected).\n. ",
    "mrkurt": "Done!\n. ",
    "tylerkovacs": "LGTM\n. ",
    "jedsmallwood": "Sounds good.  This is my first time using git so I think I should have just made a new branch or something to do this other stuff.\n. I'm going to clean this up a bit, just do the UDP and then do the cluster as a different pull.\n. I can switch the input to accept an array to be consistent.  \nI also have not had the opportunity to try this under production load, but I'm trying to use this patch to allow me to get production pointed at it.\nThat said, I notice that the other endpoints respond to a URI that matches '/1.0/event/put'.  Should this endpoint do the same?  I just can't figure out if it is \"normal\" for UDP URIs to specify a path.  The only example I have run across is the bittorrent protocol, http://en.wikipedia.org/wiki/URI_scheme.  I assume this was put in place to allow for versioning.  Thoughts?\n. If I am reading the collector code correctly, it appears that the websocket endpoint just reads individual events rather than an array of events.  Looking at the original random.js example, it also appears that only individual events are being sent.  The POST endpoint on the other hand appears to rip the array appart with the following in collector.js:\nfunction post(putter) {\n  return function(request, response) {\n    var content = \"\";\n    request.on(\"data\", function(chunk) {\n      content += chunk;\n    });\n    request.on(\"end\", function() {\n      try {\n        JSON.parse(content).forEach(putter);\nConsidering that you may need to worry about packet size with UDP, I'm not sure it would be wise to send a large array of events rather than just shooting the event off as necessary.  At this point I am inclined to leave the input as a single event, but I am still not sure about implementing a URI scheme.\n. I just realized that issue 12 for cube had a conversation about using UDP.  Scaling it up a little higher I'm seeing that it is dropping to many packats to be useful for me just like the other commenters.  I think I'm going to take a look at 0MQ.\n. Is special handling for replica sets really needed if you just front your mongo setup with mongos?  I know I have used cube out of the box with a mongo installation that I had setup with sharding and replica sets and it worked fine.  With this setup I had to have mongos running, so I just pointed cube at mongos and all was well.\n. ",
    "dougcole": "Switching the expected input to one event rather than an array seems like an odd choice that will lead to confusion, otherwise this looks great! I tested locally and it worked for me, but I didn't tested in anything like a production environment.\nLet me know if there is anything else I can do to help get this merged.\n. Ah, didn't realize the websockets api was different than the http api - in that case I agree that single events make sense.\nAs for a URI scheme for UDP I don't think that makes sense as a question. UDP is just a transport protocol (replacing the standard TCP). It doesn't support any sort of path component on it's own - theoretically you can use HTTP over UDP, but I don't think there would be any benefit.\nAll you're sending now is the json string to a particular port, there aren't any headers to dictate a path component and I think that's fine.\n. ",
    "LemonCake": "No it's not, but it just so happens that the system I'm working with doesn't have a mongos process fronting the replicaSet. We only have one replicaSet at the moment, so we didn't feel the need to setup mongos quite yet. It's inevitable at some point we would, but I thought it'd be nice to allow for that flexibility.\n. ",
    "jplock": "+1\n. ",
    "databus23": "+1\n. ",
    "rookus": "+1\n. ",
    "RandomEtc": "This looks pretty safe and has strong support, so I'll try to get it tidied up and merged in soon. Not sure why it won't auto-merge, so I might revise my statement when I check :)\nJust as a style thing I think I'd prefer:\njavascript\n\"mongo-hosts\": [\n  { \"host\": \"xxx\", \"port\": \"xxx\", \"options\": {} },\n  { \"host\": \"xxx\", \"port\": \"xxx\", \"options\": {} },\n  { \"host\": \"xxx\", \"port\": \"xxx\", \"options\": {} }\n]\nRather than an array. Maybe the properties should have \"mongo-\" prefixes, so that the set up code for a mongodb.Server could be reused. What do folks think?\n. So... I finally got a chance to look at this. Thanks again for the pull request!\nI dug around the mongodb driver, which we recently updated to use version 1.2.14. There's a mongodb.Db.connect(url,...) method which supports url configurations for replicasets, mongos, and more, as well as neatly encapsulates authentication and a couple of other optional steps.\nDocumentation of the connect function and connection string is good, so I defer to that for more explanation.\nCube 0.2.8 adds support for connection strings as \"mongo-url\", and addition options to Db.connect as \"mongo-options\" in Cube's config JSON support. This way we can defer to the mongo driver for handling all the various options and formats. I'll keep the old mongo config vars around for backwards compatibility, but you should prefer URLs from now on, they're much more convenient.\nI don't have a replica set to test with - if you're still using this setup can you let me know if \"mongo-url\" works for you with comma separated host:port for your replica set?\n. Just a quick note that I'm starting to look closely at these pull requests. This one seems like a good first one for me to work with, DRYs a few things up nicely.\nThings to do:\n- [x] check contributor agreement for @mrflip \n- [ ] rebase against master\n- [ ] confirm that tests pass\n- [ ] publish new version on npm\nSince you also rolled this change into #92 I'm not sure if rebasing against master is the right way to get back on track. I'm also not sure why it won't automatically merge as-is. I'll start investigating and see what happens, feel free to chime in :)\n. It's a small gesture, but closing in favor of #129 \n. It's a small gesture, but closing in favor of #129 \n. I like the idea of this but I don't know a lot about the node cluster module. Does it support udp and websockets and everything else?\nI might split this out into a separate bin file to keep the basic collector/emitter very simple and clear.\nI worry a bit about some of the timing logic in event.js, around setInterval (not in your code, which is quite clear). I'm still getting familiar with the small details of Cube so I'm not sure if there are weird race conditions or if there's any duplicated work if there's more than one collector or evaluator handling the same event type?\nAlso - have you run into CPU issues with only a single instance?\n. Thanks for the extra notes. Let's keep this pull request open for the time being - if anyone has time to look more thoroughly at the use of intervals and timeouts in Cube and how they interact with node's cluster module then please post here. If I start looking into it I'll post back with an update.\n. +1 for making authentication pluggable. It should be possible to run cube as a library and wire it into a server that uses a library like connect/express to allow pluggable middleware.\nI feel pretty uneasy merging this pull request as-is. \nThe UDP security isn't really very secure - anyone else on the network could see those messages and trivially extract the password. You'd need to look at a Message Authentication Code scheme (e.g. HMAC) to encrypt your messages and validate that only you could have sent them. But... if you're on Heroku, it's probably smarter to just disable UDP collection entirely since Heroku apps can't receive UDP messages anyway.\nThe HTTP security also seems suspicious. Sending a password in a query string over HTTP seems like a bad idea. I'd rather see this go over HTTPS... but before I get too far into critiquing again, what I'd really like to see is less HTTP server stuff in Cube and more in a pluggable library like Connect.\nI'll close this for now but feel free to re-open for further discussion if you attempt to wire Cube into Connect/Express and find that certain things aren't possible. Otherwise, there are many tried-and-true authentication schemes available as Connect middleware and that should be a safer path to happiness on Heroku etc.\n. I'm using node v0.8.16 here and it can parse '2013-02-18T00:45:36-01:00' using new Date(), so I'm going to close this in agreement with @mbostock.\n@george-vacariuc feel free to reopen this request for discussion, or file a separate issue if you've found a problem sending ISO8601 compliant dates.\n. Thanks so much for digging into this!\nI know it's only a small change so I don't want to hold it up too much but can you explain the check for the presence of the read method? Are there streams without .read()?\n. Just looking in the new docs, here: http://nodejs.org/api/stream.html#stream_compatibility it seems like .resume() or .read(0) might be more correct? There's also the readable event, but I'm not sure if we need to wait for that.\nRegardless I will test this fix and merge as soon as I can.\n. Testing this just now with node 0.10.0 on Mac OS 10.8.3, using node bin/collector.js and node bin/evaluator.js and then node examples/random-emitter/random-emitter.js, when I open http://localhost:1081 I get a flood of:\n(node) warning: Recursive process.nextTick detected. This will break in the next version of node. Please use setImmediate for recursive deferral.\nI get the same flood of messages if I run make test.\nIf I replace calls to process.nextTick with a shim to prefer setImmediate:\njavascript\nif (typeof setImmediate === 'function') {\n    module.exports = setImmediate;\n} else {\n    module.exports = process.nextTick;\n}\nI still get the same flood of messages. Many of our package dependencies use process.nextTick, so we'll need to take a look at them and see if any have been updated for node 0.10. Please update this pull request if you have time to check on it, I will most likely have to wait until the weekend.\n. Here are the packages we depend on and their current versions. I'll update this check-list when the package has been investigated/updated:\n- [x] mongodb: 1.0.1\n- [ ] node-static: 0.6.1\n- [ ] pegjs: 0.6.2\n- [ ] vows: 0.5.11\n- [ ] websocket: 1.0.3\n- [ ] websocket-server: 1.4.04\n. Do the tests pass for you on this pull request with node 0.10.0?\n. @seanmoon sorry I should have mentioned this earlier - could you complete our individual contributor license agreement so we can safely merge your contributions when we're ready?\n. Great - thanks! I'll pull from both and get this tested and merged as soon as I can. Thoughts welcome on read - I started testing but was thrown off by the nextTick warnings. The main thing I didn't understand was your existence check, but it seems harmless.\n. I got this in, as well as an update to the mongodb driver and a few tweaks to tests and comments. Published in cube@0.2.6.\nThanks so much for your help with this. I may follow up with 0.2.7 with the other updated dependencies shortly, but this seemed more important.\n. Rebased and merged in c921af3. Thanks again!\nI found I needed to jiggle the tests a little bit, renamed test.js to helpers.js so that the new version of vows didn't try to run it. I think. Couldn't find this behavior documented anywhere but I'm going with it :)\n. LGTM - thanks for getting to this! \nI can merge and push new npm and gh-pages later today. Anything else to be aware of?\n. Just finished a first pass review of this. Mainly notes to myself but feel free to clarify anything you can:\nThings to consider tidying up:\n- queue-async should be in package.json and not a submodule\n- https://secure.travis-ci.org/infochimps-labs/cube.png shouldn't be failing if we're keeping Travis stuff\n- do we need a new travis account for square/cube? https://secure.travis-ci.org/square/cube.png should be green\n- Jakefile and related horizon concept need docs in wiki\n- I think most (all?) of TODO.md is on the wiki - can we remove it?\n- config/cube.js refers to dashpot_development - what's \"dashpot\"?\n- config/cube \"import\" stuff seems overly complicated?\nNew features (to be documented where possible):\n- use strict :)\n- horizons concept\n- warmer\n- visualizer\n- authenticator\n- broker\n- metalogging\n- separate database for events\n- new examples (all seem good - could some of them be tests?)\n- new Db class \n- new Model class for events, metrics, measurements and invalidators\n- HTTP endpoint can now take regexp matches\n- metric deletion/refreshing now possible\n- tier binning (where's that used?)\nMy thoughts and bits I skimmed:\n- most substantive changes are to the events, metrics, measurements, invalidator models - need to look closer and understand the underlying Model class too - and the broker stuff I think\n- lib/cube/server.js needs a closer read too - quite heavily tied to new model of authentication, which would ideally be more optional\n- a lot of vars_with_underscores, but other cube things are camelCased\n- I think process.env.TZ might have been deprecated, we should look for another solution if so\n- I made some recent improvements to database.js to accept mongo-url in configs - looks like it's no longer used because of db.js, needs a closer look (and we should double-check other deleted require calls)\nNext steps: \n- I'd welcome help reviewing/understanding/testing the new model classes\n- I'd like to understand/fix the failing test so we're off to a clean start\n- make sure we don't miss the mongo-url thing and other recent bugfixes\n- perhaps we can do some small merges for parts of this first - I think there are open pull requests for the metalogging and authentication stuff for example - but I am OK to push through on this thread too\n. I'll take a look - thanks!\nAt first glance I don't like the use of cfg - I prefer seeing the options passed around explicitly. We use Cube as a library here and do a lot of custom config using raw JS objects. Not sure if this is compatible.\n.  think there's a balance to be struck between using libraries (cfg and per-environment settings are good) and keeping things separate. For example I don't think the test settings should be anywhere except in the tests.\nLet me take a look at it - I don't like just critiquing without offering an alternative way - I have picked Cube up again for a side-project and should have some time this week. I'd love to be running this version for real rather than speculating about what works.\n. I'm not in a rush to change the cfg stuff - thanks for keeping moving forward :+1: \n. I was hoping to minimize your effort! If you undid it for now, thank you. Definitely want to work on config at some point but I'd like to resist loading new features into this branch. I'd rather this be Cube 0.3.0 pretty much as-is, and then push for more cleanups for 0.4.x.\nAlso hoping to hear from @mrflip and @hustonhoburg - at least a little encouragement, guys? :)\n. Let me look at it. Unless you're suddenly struck with a good idea :)\n. Am attempting to rebase this against current master. Wish me luck :)\n. I went with a merge of square/cube master into Marsup/cube infochimps-merge, which was relatively easy. I also remove the (unused) broker and test, and tidied up usage of process.nextTick. This is pushed to square/cube infochimps-merge.\nI am having trouble running the tests without vows -i again, which I'd like to get fixed. Let's try to find the last time the tests passed without -i \u2013 I assume they passed after f1c9715 at least?\nI think we should remove the unused bins stuff that you pointed out. Is there anything else in here that seems half-done?\nBiggest sticking point for me now, apart from the tests and general cleanup, is that I am also not a huge fan of the code style of anything that uses Model here. I'm trying to figure out if I can quickly rewrite it in a js style more like the original Cube code - no new, way less this, etc. I'm willing to skip it and press on but if it's just a mechanical transformation I might do it in the name of consistency. (Otherwise consistency will be very hard to maintain after the merge).\n. Cool, let's keep this thread moving, I think it will be worthwhile. I'll try to get back to you more quickly with my take on anything. Would be good to publish a 0.3.0 release candidate to npm and let a few other people try it out.\nLet me know if you need a hand with the bug-fixing.\nI certainly don't dislike oop, or the specific oop approach taken, but I dislike having two competing systems in one code-base.  I'd prefer to keep Cube using mbostock-style (or Crockford-style) modules and classes if possible.\n. I'm OK with -i for tests too. I assume we can make that work with Travis.\nSome other TODOs:\n- [ ] add -i to Makefile and package.json definition for tests\n- [ ] remove bins stuff\n- [ ] backported bug-fixes from @Marsup\n- [ ] consolidate test_db with database.js\n- [ ] remove TODO.md (and double-check that all the useful stuff is already on the wiki)\n- [ ] documentation for new features I listed above\n- [ ] get Travis set up for square/cube\n- [ ] publish release-candidate for testing\nAnything else?\n. I didn't see your full-merge branch before, sorry. Are we up to date now?\nI left some comments on the individual commits. Definitely progress, but I would like to stay focused on merging existing infochimps functionality before adding too many more fixes. I'd like to create a 0.3.0 branch from this soon and we can work on extra bug fixes and style enhancements there.\n. As with cfg, don't worry about undoing it. But it may need more massaging to make sure people understand the changes. Thanks again for the contributions.\nNext for me is testing this in our staging environment at Square and seeing what that shakes out.\n. Regarding cfg my main goal would be to pass raw objects into Cube interfaces, so that people can use JSON or YAML or whatever they want for config. The cfg approach as it stands is hard to follow and relies on side-effects. \nA good compromise might be to limit require('cfg') to things in the bin directory. At that point \"configuration\" stops and \"instantiation\" begins. This aligns with the goal that I'd like to see a more modular cube (for example, perhaps visualizer and warmer could be separate npm modules) and people should be free to configure those modules however they want. \n. > What do you think ?\nLess is more. Fewer dependencies. Raw objects and no merge/extends/inherits/defaults please.\n. I hear that. I'm running this with personal data but I haven't thrown\nsquare data at it yet. How do you feel about merging into a 0.3 branch and\nrefining things there?\n. Thanks both for keeping the discussion going. I'm sorry I've been silent on most matters. I haven't had a chance to try this new branch on real data so I'm hesitant to express too strong of an opinion about it.\nI'm open to landing it as a 0.3-pre branch here so there's a clearer target for new contributions/optimizations/docs. What do you think?\n. Sounds like a plan. Thanks for your help triaging issues!\n. Hi @jeffhuys - this branch is stalled mainly because I ran out of time/bandwidth for the project. But also because we only have one person (@Marsup) who has run this code to date. I'd like to run it before I merge it but I haven't had time.\nIf you've followed our discussion above, and the related issue where I asked for community input into the merge, you can see I was very optimistic about bringing all the Infochimps changes into the Square cube repo, but the actual process of doing this was a lot more complex and time consuming than I imagined. We don't run Cube in an official/production capacity at Square any more, so it's more or less a volunteer side-project for me.\nMajor apologies to @Marsup for not using this integration work yet.\nNext step remains setting up a new branch here for this work, and getting a few more people to try it out for their use-case. I'll try to get that done soon, including trying it out at Square. Until then, please comment on this thread if you've run @Marsup's version and let us know how it goes.\n. @ticean my intention was to make a merge branch and update our readme to encourage people to try it out. Unfortunately Cube has become less and less of my day-job here at Square and since starting this merge, despite the heroic efforts of @Marsup (thank you!) I haven't carved out the time to make much progress.\nAlso since we started this project infochimps was acquired, so I suspect they haven't been able to give it the attention they wanted either.\nI still have this on a TODO list, and hope to get to it one day soon, though I realize we are very likely to be losing goodwill and attention by letting this branch linger. \nEnough excuses...\nFor using cube as a library, here's an example collector script that we have in our internal cube repo:\n```\n!/usr/bin/env node\nvar options = require(\"../config/collector\"),\n    cube = require(\"cube\"),\n    server = cube.server(options);\nserver.register = function(db, endpoints) {\n  cube.collector.register(db, endpoints);\n};\nserver.start();\n```\nThe require for cube is the stock one from npm. The ./config/collector.js file looks something like:\nmodule.exports = {\n  \"mongo-host\": \"127.0.0.1\",\n  \"mongo-port\": 27017,\n  \"mongo-database\": \"cube_development\",\n  \"http-port\": 1080\n};\nHope that gets you started. If you have a chance to checkout @Marsup's branch please do, any feedback on that will help others work out which version to use. Until then, now we have 3 versions...\nhttps://xkcd.com/927/\n. I am declaring Cube-maintenance-failure for myself. I have updated the README here to indicate that nobody at Square is actively developing or maintaining Cube. Since I have failed to make progress on this branch I encourage people to help @Marsup with his integration branch and fork if you have any new features or bug fixes. I will be closing all issues here in a moment.\n. Excellent, that's great news @zuk. I'm open to updating the status and the Cube homepage with more info in future if you, @Marsup and others want to publish a new version. Thanks for letting us know!\n. Sounds good. Thanks for the contribution. We're in the middle of a drawn-out - and delayed - merge of another fork of Cube, so apologies if this doesn't get any attention for a while. As that process moves along we'll revisit all pull requests and try to get them landed.\n. I know it's been a while, but can you clarify why this metalog is commented out?\n. The import here for node-gyp looks like a stray import - is it related to the metalog stuff or can it be safely removed?\n. Query strings over HTTP are really easy to steal if you're on the same network. If you're using this now (without https) it's probably not offering you any real security.\n. I missed this comment until just now, but it underscores what I said earlier. I'd rather put this back in Express :)\nI'd like Cube to support integrating into an Express server (it would also solve the static hosting stuff and remove the need for node-static). Let me know if you look into this, I should be able to help.\n. Yep :)  I have the same problem with the pluggable auth patch. I'm imagining rewriting the server stuff to be compatible withfunction(req,res,next) style middleware so that there is no need for an auth story inside Cube itself. This is similar to how learnboost/kue supports basic auth, for example.\n. ",
    "taylorphillips": "LGTM\n. ",
    "ChrisLundquist": "sweet, can we make [human] named boards?\n. Tested this out and I like it. Provided I have time I will build on this request and make name editable.\n. I would love to see this merged. It is needed for our collectd roll out. (otherwise all the data is stuck in 1970s )\nI deployed it and verified it works with collectd 5.0.1\n. @mbostock You are my hero. If I cook up a chef recipe for cube would that be something you're interested in? ( presuming mongo is setup )\n. Thank you for your quick action on this.\n. ",
    "trotter": "Thanks for the feedback. I'll try to find time tonight to make these changes.\n. I've updated this pull request with what I think are fixes to the requested changes. Let me know what you think.\n. Yup, I've tested it locally using the random emitter example. 2880\nevents were created in Mongo. As for the CLI, I just signed it. Let me\nknow if I need to do anything else.\n. So... everything seems to work now, but the code is quite a mess. @mbostock, you have time to talk about this sometime? I'm happy to come up to SF.\n. Thanks, @sbuss. I'll see about factoring those out. I hadn't actually considered just wrapping it within the server code itself.\n. ahh... makes total sense. Thanks.\n. ",
    "temsa": "yes, I agree, I can change that too, it's less than a line\n. @mbostock it would be useful for us, is there any issue with this pullrequest ?\n. @Marsup ain't it missing some code for managing data collection, e.g. through collectd ?\n. @Marsup just did not seen the id creation in the commit diff\n. ",
    "gravitezero": "The reason why I expected the stop parameter to be skiped was because I misread the documentation.\nI thought websocket polling could be possible with metrics too, so I implemented the polling feature into metrics as well.\nAs always, it might not be the best solution, but it seems to work well.\nLet me know if you have any comment about my patch, or if you intended to modify the design to implement later this feature.\n. Still needs work.\n. @mbostock\nThe code might be ready to be tested and merged to the main branch.\n. Some minor changes made the test pass successfully.\n. ",
    "edmund-huber": "We are using this, and it seems to work as advertised. Good job @gravitezero! When can we see this in master?\n. Why not just have a query like \"event\" return all data for all events called \"event\"?\n. Sorry -- this broke things! I've reopened the pull request with the version that works more correctly.\n. Sorry, my intent was to allow single-letter event names, but I didn't consider numbers.\n. I think for the sake of clarity, I'll just leave that alone in this pull request. :)\n. ",
    "sbuss": "I really like these namespace changes. Thanks for writing this, @trotter \nI have a style nit, though. The authentication checking pattern\n} catch (e) {\n    if (e.toString() == \"AuthenticationError: Invalid Credentials\") {\n        response.writeHead(401, headers);\n        response.end(JSON.stringify({error: e.toString()}));\n    } else {\n        response.writeHead(400, headers);\n        response.end(JSON.stringify({error: e.toString()}));\n    }\n    return;\n}\nand the namespace application pattern\n```\n// Namespace the type if necessary.\nif (namespaceFun) {\n  namespaceFun(typeWithoutNamespace, request, namespaceFunCallback);\n} else {\n  namespaceFunCallback(typeWithoutNamespace);\n}\nfunction namespaceFunCallback(type) {\n```\nare repeated several times and seem like they could be factored out.\nIs there a reason that the authentication doesn't just wrap endpoint? Maybe add an authenticated parameter to endpoint that does this authentication checking in one place.\n. Fixes #79\n. Sorry, I used the wrong git user for the commit in this pull request. I'm closing & resubmitting this.\n. I'm not much of a JS developer so it took me some time to wrap my head around all of the callbacks in the unit tests, but I managed to write some tests for events. The tests even exposed a problem in my pull request.\n. Is there anything I might change with this PR to help it get merged?\n. ",
    "mmmulani": "Yeah that would be nicer, I'll try hacking it out today.\nMy idea is to only allow \"type()\" rather than also allow \"type(prop.)\" since that can be queried by \"type(prop)\"\n. Sorry about the huge size, that's from me regenerating the grammar with pegjs.\nI had to reorder the grammar rules, looks like pegjs eagerly fails as soon as it starts matching a subrule in the grammar.\n. @edmund-huber there is definitely value in the knowledge that an event simply occurred (and overhead in sending down/calculating all those properties)\nhaving \"type\" return all data for all events called \"type\" also makes \"sum(type)\" confusing as it doesn't act on any of the properties.\n. @mbostock how does this diff look? seems to work for me..\n. ",
    "Marsup": "If you are referring to @temsa comment, the computation of ObjectID by the mongodb module is split like this :\n- timestamp : now taken from the event time\n- machine : randomized at the node module startup\n- pid : taken from the node pid\n- increment : incremented at the module level each time a new ObjectID is created (with a modulo to always fit on 24 bits)\nI think collisions are nearly impossible with this logic, even on multi-process/machine scenarios.\n. The collectd putter is reusing the event putter, so it shouldn't matter which source you are using as long as you let event handle it.\n. I have just seen a slight anomaly in this PR, on the line 196 of event.js.\nSince _id will now always be an ObjectID, you will never get any, but you still get your custom id in the data field.\nWhat do you prefer @mbostock, duplicating the id in both data and id, or simply removing the id field ?\n. We are also running horizontally scaled collectors and evaluators, but we did it before cluster made it to node. It seems to go well on our side too except for one thing : if you plug the collectors to a collectd, you must never send any \"derive\" event, since these events depend on the previous value to compute the actual value before inserting into mongo, you will of course have very unexpected values depending on the collector you reach :)\nI would add to the matter that scaling the server itself is one good thing, but I was thinking that it would be even better to scale the computations themselves. With the cluster mode you improve your responsiveness with many clients, but the computations will still take a good amount of time individually.\n. I'm resurrecting this PR just to tell I had the same need and implementing $nin was far easier and hopefully more optimized on mongo, hope this helps.\n. > Things to consider tidying up\n- Queue-async can indeed be added to dependencies, I don't know why they kept it as submodule, I tested it with success\n- Travis errors are probably because the tests doesn't check for errors on db.open, can't say for sure, anyway I don't see these errors on my env, maybe a mongodb problem on travis\n- You will need to add your repo to travis yes\n- Sure\n- Wiki is not ideal for this IMHO, I'd rather have tagged issues for each todo item and reference them from commits so that we can trace it\n- Don't know what dashpot is too, other than a branch on their repo :)\n- Agreed, it can be much better, I love cfg package to deal with configuration, what do you think ?\n\nNew features (to be documented where possible)\n\nI'm gonna need help to document since I'm a bit busy working on our own merge, but sure, user-facing concepts should be documented, and some bits of code here and there. I dug into the code but don't fully understand all the tiniest details.\n\nMy thoughts and bits I skimmed\n- As I understand it, roles and responsibilities are isolated in their respective models, so far I like it, it seems a bit DRYer\n- It kinda follows the middleware pattern you can find in some other modules, I think it's the right place to handle it, and the default (allow_all) doesn't add much to the connection handling process\n- Some jslint/jshint rules should take care of that\n- Just tested in the REPL, it still works on dates, where did you see it was deprecated ?\n- That should probably be backported, didn't see it in the merge, sorry\nNext steps\n- I like it so far, even though I don't understand everything, like the binning you mentioned, it seems to me that some properties are only useful for tests\n- That's critical, can't spot the bug so far but I will\n- Sure, noticed any other I might have missed ?\n- The 2 other PRs indeed overlap with this one, but I don't really see the benefit of it since they both don't induce many (if any) conflicts, it's not much help for this merge\n. @RandomEtc I have a bit more time to work on this again since I've finished merging our own branch, that might be the subject of another pull request once we're done here :)\n\nSo far I have done :\n- queue-async is now an actual dependency\n- cfg is now used to deal with configuration, options are not passed anymore all over the code\n- Backported the mongo-url setting\nI'm going to work on the tests passing since that remains an issue.\n. OK so that was maybe a false alarm. Rising the timeout of the metric-tests made them all pass, I knew the cpu was very busy but didn't think it would take over 20s to complete a test. Can you confirm everything's good on your side too ?\n. Actually we use it as well as a library. I missed some things in the cherry-picking process, like exporting the configuration. I'll complete it tomorrow.\nDoing this allows you to do the same thing in your application (tuning every parameter, even with your own per-environment settings, which is a great benefit IMHO). The reason I removed all the options passing is I found errors in the code, and requiring the configuration from a central place is much safer and makes less noise in the functions signatures.\n. Well that's just an habit, because I like having my settings in a single place, test being an environment among all the others, but that would be as easy to put the settings back in the test_helper.js if you prefer so.\nI also usually put it there because then I don't have to worry whether each test file loads the good settings.\nI'm pushing an update without node-gyp, and with the test config in the old place.\n. You're welcome :)\nJust so you know, I modified the git history and forced the push to avoid useless commits, so the above ones are new even if GitHub doesn't think so ;)\nI'm not sure I understand you, do you want to keep the old one or the one with cfg ?\n. Well I didn't revert to the old config file, I just moved the tests configuration. I had some bugs with the old way of dealing with options, though I can't remember exactly what those were, probably nothing I can't fix, but it felt right at that moment, bad judgment call maybe :)\nFor the record, I've been using the cfg version from my fork since 5 days (as a lib), nothing came up.\nDo you want some time to think about it or should I remove it altogether ?\n. Moving on to the next problem, I'd really like some hints from @mrflip or @hustonhoburg to understand why the events bins are stored (to_wire function) into MongoDB. I can't find any query containing this field, and it is computed at runtime when the events are fetched anyway.\n. @mrflip You eliminated the use of bisect in infochimps-labs/cube@7881f54b2e, do you think the problem that it fixed no longer exists in your implementation ?\n. It's fairly stable for a WIP, I've been letting a slightly modified version run on my laptop for at least a week, looks good, but of course it's not under a lot of pressure :)\nFor the bins, I see what you mean, but I'm not sure it would be a good thing. The timestamp is already indexed so the query should be lightning fast. If I'm not missing anything, it will cost you additional space and index, without any added value.\nI agree with you on the bisect, the $in doesn't care about order, so I don't see the point. The original commit containing the code was square/cube@234d6e6dd5a4b7ca7167d4a6e232688ef87c43f1, no indication whatsoever. Maybe @mbostock could help on this one ?\n. I'm happy that went well :)\nI'm not sure having the -i is such an issue. Sure the tests would be faster without it, but datasets of tests might interfere with one another which might create false-positives as well as false-negatives. Personally never liked vows on this point.  We could start at the commit you mentioned and git bisect until we find something.\nApart from the bins, I found some bugs that should probably be backported before any release is done, I'll try to make it happen this coming week.\nI like the OOP approach they took, it's easier to reason about when there's a single place for concerns. Is it the implementation that bothers you or the principle ?\n. OK I'll see what you come up with :)\nHere is a first batch of fixes, our branch have some major differences so I hope I didn't miss any.\nI'll detail a few that seems important :\n- c0b8972 : I don't understand how this bug could make it so far in the project, I guess nobody ever tried to send more than one event in the same request, you might want to backport it in the stable branch right now.\n- ad9a0d3 : I am working on authentication-enabled mongodbs and this was not working with separated databases, it should do now.\n- 137f278 : this allows a pyramid to be built even when the result is there's no value.\n- 44b16fd : group based queries would just hang if you asked for an interval where there's absolutely nothing, now we provide answers for each time slot with a null group.\n. I've cherry-picked all the bug-related code (which seems debatable :smile:) from the other branch yes.\nI think it's worth discussing those because they seem important for a proper release, even though it's hard to qualify what is considered a bug and what is not. I included everything that kept cube from working properly, or sometimes at all.\nIf you think some of them go beyond the goal, leave me the SHA and I'll rewrite the history.\n. I actually wondered a lot about cfg, we could probably make it work without, as long as we keep a central point of configuration, but I'm not comfortable exposing the raw object. Want me to try another solution ?\n. Actually my plan is to have a raw object for default settings, use lodash merge function on the call of the server to modify that shared object.\nPros:\n- Same API as before, but the configuration keys wouldn't be the same if we keep this tree-like organization\n- You can partially override the default options without repeating everything, server options would be mandatory\nCons:\n- Would be wiser to replace underscore with lodash to avoid duplicate dependency\n- Can't run 2 servers in the same node process without taking the risk to mix up their options\nWhat do you think ?\nAnother similar solution would be to keep underscore, flatten the whole options object, and use extend instead of merge. That seems a bit messy for readability and keys naming, but it's doable.\n. OK, I see where I'm going now :) Do we agree on the single point of configuration ?\n. @RandomEtc Tell me what you think of this solution.\nAs I've told you, multiple servers in a single node process will be complicated.\n. For the record I still experience partially unresponsive evaluator if I ask for data past horizon, I think the computation never gets done because start and end date are equal, so any further request on the same collection will stall.\nAnother thing I noticed is pretty poor performance on grouped queries, a mongodb distinct on several million un-indexed rows is supposed to take a while but still...\n. Have you given some thoughts on my new configuration proposal ?\nDo you want to complete your check-list up there before moving to 0.3.x or will it be done along the way ?\nI think it's not a perfect release and there are still known bugs, but at least we'd move on with tinier patches rather than this long running commit list, so why not, but maybe not publish to npm just yet.\n. @RandomEtc After running the service for a while, I was forced to remove a \"feature\" infochimps added. Cascading cache to the lowest tiers is a very bad idea. It sucks the MongoDB storage like hell, way too expensive for insignificant benefit considering a few seconds/minutes is not that long to re-compute, so I came back to the way things were in the current cube release. Might want to consider this before doing a release...\n. Hello Houston !\nFirst, reading it back, my previous comment seems more accusing that it was meant to be, so sorry for that.\nNow I can see why you would do that, in my case I keep everything, no capped collection at all, and I also have many events for any given time, our situation should be similar, so you might understand my pain seeing metrics grow horribly fast :)\nThe difference might be I'm on a sharded mongo, with many evaluators to answer queries at the same time, and we mostly stream metrics (which is a difference of our fork) so full time ranges are not queried that often.\nYour version definitely improved many things and I'm grateful for that, I'm just worried such a default setting would disappoint newcomers as it fills up several GB for only a few days/weeks of metrics. I think ideally this cascading aspect should be configurable, but that'll be the subject of another pull request ;)\nAnyway it's nice to see you're still following things here !\n. Well, this thread has lasted long enough I think :)\nYou have raised many concerns along the way so I would say do that branch and close this pull request, but let's not forget anything here, and maybe create a bunch of separate issues to track every doubt/task that needs to be dealt with before final release.\n. I'm confident as well, I had this branch running in production since September (with a few modifications since then as the commit history will tell you), but beware it doesn't only contain infochimps modifications, so not everything is documented as it should be.\nI'm also glad I came to reason for the config, imposing cfg in cube's core was not very clever, even though it's a very nice module and I still use it for my cube runners.\n. I'm not the one who conceived it so I'll try to describe it as best as I know it, I'm not using it either.\nIf you store your expressions in a specific collection, it will regularly take them and keep your metrics cache up-to-date even without a client asking for it.\nThe way to store expressions is inherited from the time cube had a kind of dashboard, there is no documentation about it AFAIK.\nAs for configuration, nope afraid not, you can still use cube as a module and do the slight variations programmatically.\n. - I have had similar issues with horizons during my tests and haven't found a way to make it work, but I don't use this feature and so it's highly unlikely I'll spend time on it, though I encourage you to give it a try.\n- I never ever had to create collections manually, you shouldn't have to either, I don't understand the meaning of this comment since there are no schema files anywhere in the project.\n- You seem to have troubles with your mongodb, never encountered this error/hang, this doesn't mean the code is right but you should check your mongo.\nBeware that full-merge is not the exact same thing as this pull request, I've piled up other modifications for my own needs.\n. Wouldn't it be easier to just fix the path with something like path.join(__dirname, \"..\", \"..\", \"static\") ?\n. That seems like a nice addition, but that wouldn't work very well with multiple operations. I like the idea but I'm not sure it's the best way to express constraints, though I have nothing better to offer yet :)\n. A meaningless example would be sum(test.gt(i, 42)) + min(test2.lt(i, 24)) but you get the idea.\n. It's partially done in the merge, I'll complete it when I can :)\n. Backported it in #129.\n. Dupe of #137 ?\n. Oh I missed that. I recreated a commit in the merge we're doing (#129) with your name, it was harder to cherry-pick, hope you don't mind.\n. The readme is already pretty explicit about it.... ",
    "sahil107": "Marsup:objectid-timestamps. ",
    "amolk": "Issue is valid but the fix isn't.\n. ",
    "nategood": "Cube is broken in the current version of Node (0.8.0+).  This will fix it.\n. Okay.  I must have misinterpreted the purpose of that.  We were hoping to get that auto generated Mongo ObjectId from Cube.\n. Yep, that would totally work.  I guess at first I was concerned about making assumptions about where cube was installed or how the evaluator server was being started but, yeah that should alway be safe. \n. ",
    "godsflaw": "I think it is wise to dig and see if there are any race conditions.  We did run into problems with inserts and especially queries against a single server, which may have caused me to make my change a little hastily.  At quick inspection, it looked like everything was contained well within a single server instance.  That is, it looks like one could get parallelism simply by running more collectors and evaluators, which made me think it was ideal for cluster.\nWe've been running this code for a few months and it's handling 250 largish documents a second with more than 10 indexes in one of the event collections.  It produced the speedup we needed, and appears to run well.\nIt is worth noting that, for some of my stats where I present a percentage, very rarely I will get values back (and cache them) that are over 100%.  This throws the cubism graphs scaling off.  This bug, however, could exist in a number of places and is likely unrelated.  Other than that, all my other limiting factors are related to MongoDB, and there are no other observable bugs.\n. ",
    "cwarden": "Also see @mrflip's pluggable authentication in #92.  It makes it easy to add new authenticators.\n. ",
    "ghost": "I only saw it after implementing ours :)\nWe mostly just wanted to feel comfortable having our backends talk with Cube without fearing someone will spam/query it, and we wanted to make the patch as compact as possible :)\nIf you're going to pull his request, we can port the simple password thing to his \"authenticator\" interface (we didn't bother with that since our use-case was strictly internal server-to-server stuff).\nDo note that UDP remains unsecured, which is a bit iffy.\n. Two things:\n- What do you want to do about UDP  ? We'd prefer not to worry about getting our cube instance spammed :p\n- There isn't any basic auth stuff in your implementation, so you just get kicked out and don't get a chance to authenticate :(\n. ",
    "mrflip": "+1 for putting this in the pluggable interface, but of course I think so :)\nOn Mon, Feb 18, 2013 at 4:41 PM, talkasa notifications@github.com wrote:\n\nI only saw it after implementing ours :)\nWe mostly just wanted to feel comfortable having our backends talk with\nCube without fearing someone will spam/query it, and we wanted to make the\npatch as compact as possible :)\nIf you're going to pull his request, we can port the simple password thing\nto his \"authenticator\" interface (we didn't bother with that since our\nuse-case was strictly internal server-to-server stuff).\nDo note that UDP remains unsecured, which is a bit iffy.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/square/cube/pull/108#issuecomment-13747193.\n. \n",
    "seanmoon": "Hm, I didn't see those messages on my Mountain Lion setup, I'll try it out again. I didn't load new data in with the random emitter, but the old data I had in mongo (some random, some other data from developing my cube setup) loaded up okay. There is a newer version of websocket and node-static, but the breakage seemed related to node proper so I backed out of my updates to those for this pull request. \n. Will check it out on v0.10.0 in an hour or two \n. I am seeing the same error on v0.10.0 that you are seeing, but it works on v0.10.1. I'll try to fix it for v0.10.0 now. Going to see if I can get that going without updating the dependencies.\n. https://github.com/square/cube/pull/119\nThe problem dependency was mongodb, and the tests were hanging in metric-test.js.\nCan confirm the tests are passing on v0.10.0. However, the server is still hanging on the HTTP request without this pull request as well. However, you might be right that there's a better way to fix it than the way I am calling read, but I haven't had a chance to check out the alternatives. Can take a look soon.\nI put the dependency updates in a separate pull request incase you wanted to merge it separately, but I can squash them together if that's easier for you.\n. Without the existence check it was failing under node v0.8.21, if you're okay dropping support for older node versions you can safely remove it. Thanks!\n. Glad I could help! You've been a pleasure to work with, thank you as well! If there's anything else outstanding about this I can help with let me know.\n. ",
    "hustonhoburg": "Thanks so much for taking a stab at this. We've had less time than we'd anticipated to pursue this project.\nKeep in mind, our modifications were written as a work in progress that likely could use a little cleaning up for a final public release.\nIf I recall correctly, storing \"bins\" was part of a possible optimization that never really came to fruition. The intent was to reduce the runtime computations and allow for fetching events by tier and time instead of using range selectors. I think we may have experimented with looping over each bin and fetching only its events as opposed to fetching whole time ranges for calculations.\nI can't remember why we stopped using bisect. It looks like the original intent was to maintain queued invalidations tiers/times in sorted order. At first glance, I don't see a reason to do this though. The code is used by an \"$in\" mongo query, so I'm not sure why the order would matter unless it's an optimization for mongo. If that's not the case, it seems like an unnecessary calculation.\n. I understand your feedback and that's definitely a valid concern, but want to clarify a little. First, as far as speed, a response time of seconds to minutes per query, given 20 to 30 queries on a page, wasn't acceptable for our UI requirements.  So, stored metrics did offer some speed improvements. Although it was an added bonus, our intent was not to cache calculations for speed, but to store data. Hopefully I can offer some insights on why we did it that way.\nFor our use, event data vastly outsized metric data, so we purposefully capped the events collection to make event records fall out.  To preserve the data contained in those events, we saved the metrics at the lowest tier. With the lowest tier, we could build back up a higher tier metric, like 5 minute or 1 hour, using those low tier 10 second metrics. We stored our permanent data in metrics with ephemeral events, as opposed to the previous situation of permanent events with ephemeral metric caches.\nSo assuming that one has large event record data sizes with many events per 10 second tier, storing only the metrics should use much less data. In our use case, it meant we were able to roll up thousands of multiple kB events into a handful of small, sub kB sized metrics per query.  We also had a separate \"cleaner\" cron job to remove metrics older than a day, or so, to keep the data size down. We wrote our version with the intent of optimizing for high throughput while keeping a small, unsharded mongo. Storing metrics actually ended up being significantly more storage efficient for us.\nI can see how for other data shapes / use cases, storing all metrics may not make sense. We definitely cut a couple corners to fulfill our use case because not everyone wants to predefine queries, drop events, and handle dense data. We lost some of the flexibility offered by cube in order to meet our needs. It sounds like our version didn't fit your use case. I'm glad you were able to change it to better meet your needs.\nI hope that cleared up our intentions. If not, I'm happy to clarify further. \n. ",
    "simonlopez": "when is it planned to be merged?\n. ",
    "jeffhuys": "This merge is (in my opinion) very important, why hasn't it been merged yet?\n. ",
    "consense": "Hi, \nrunning https://github.com/Marsup/cube/tree/full-merge in a testing/dev environment for the last weeks and so far no problems. Amount of data is relatively low though - ~100,000 events.\nJust as a sidenote the revert to plain js object for the config in that branch made my live a lot easier.\nThanks everyone for the effort in this.\n. ",
    "aganov": "I'm going to test this branch with 150K events/day @Marsup can you tell me what is the job of the \"warmer\" and is there any way to use only one config, instead of three, which are almost the same?\n. ",
    "ticean": "Hello. I've been using the current Cube version for some time. Great work and thanks for open sourcing this project. \nQuestion 1: What's the level of confidence and timeline that the InfoChimps branch will be merged? I need to add some additional features in our project (authentication). Since this contains a pluggable authentication system, it makes sense for me to go ahead and use what's here if it will be mainlined. Looks like there's been a lot of energy put into this branch, but it's long-running and hard for me to judge what's going on from the outside looking in. :)\nQuestion 2: How can I override the configuration when using Cube as a library now? Looks like this line will always include the configuration file from Cube. Maybe I'm missing some cfg functionality that handles this? I'm trying to override with env vars according to cfgs readme, but they don't seem to register. A simple example would really help.\nThanks.\n. Hi @RandomEtc. Thanks for the help and the quick reply! You helped me realize that I was testing with the wrong branch.  This PR is based on Marsup:infochimps-merge but I'd mistakenly branched square:infochimps-merge for testing. So my bad there. :grin:\nNow using @Marsup's branch, I'm able to override the config like I need to (that wasn't possible in square:infochimps-merge)\nI had some problems with the horizon feature not returning results when the request is \"past_horizon\". I see a metalogging output, but the server doesn't return a response and hangs. Don't think I'm interested in this feature anyway, so I was able to work around by removing the horizon configuration. This disables the feature. Some docs about horizons would be helpful, but you've already mentioned this a few times in the thread so I know you know that. :)\nThings are otherwise working well now that I'm using this branch. I'll keep testing and let you know if anything else comes up.\n. Ok, after more hands-on time with this code I found some issues.\n1. Collections aren't created automatically now. This comment informs me that I have to manually create them. Cube's flexibility to create collections as it gets events is a really good feature. Really sad to see this feature's getting dropped.\n2. More evaluator hangs. I've hit cases where metalogging logs an error and subsequent requests aren't handled. I think it's because callbacks aren't called on error? Like here. It would be better for the process to crash than hang.\n. ",
    "zuk": "Hey guys, can someone explain the status of this merge for those of us wanting to start using cube with all of the infochimps work merged in? What would be the best place to start? Clone this branch and go from there? Use the Marsup fork? GitHub is kind of useless right now in situations where the repo network gets complicated like this one :(\n. For the record, I'm up and running with the @Marsup branch. Working great so far.\n. ",
    "jkassemi": "I considered adding additional parameters to the request - so the event and metric getters would then filter based on those parameters. This turned out much cleaner - got some samples of multiple operations that you use that could get my head working through some possibilities?\n. ",
    "splbio": "Almost, this request includes 28d5571 which fixes where if udp port isn't specified then a random port isn't created.\nAlso I didn't see the pending pull of #137 when I wrote this sorry for the dup code!\n. Do not mind at all.  Enjoy and thank you!\n. ",
    "ThisIsMissEm": "The package node-websocket-server should be unpublished for the same reason that websocket-server was (I'm the original author of websocket-server). The project has been marked as deprecated in NPM for the last 6 months, and the project on Github was pretty much abandoned 3 years ago. \nI would suggest this project switch to the ws module, which provides support for the later versions of the websocket protocol (hixie-draft-76, HyBi drafts 07-12, HyBi drafts 13-17). The ws module features an interface fairly similar to websocket-server.\n. @paco3346 why not just pull request this project to switch it to 'ws'?\n. ",
    "loginwashere": "Thanks for the explanation\n. @miksago at the moment I don't have time to replace websocket-server with ws (and I don't think I would have time for this in near future) so I use cube from this pull request with websocket-server replaced with node-websocket-server\n. ",
    "mtotheikle": "@loginwashere any luck solving this to get Cube installed? Did you up up just using the node-websocket-server module?\n. ",
    "paco3346": "So, is it possible to least get something published? I realize using node-websocket-server isn't the best option but I think that's better than getting a 404.\n. ",
    "q2dg": "This project seems pretty dead, isn't?. Ooh, yes, sorry. I don't know how I could missed it.. "
}
{
    "benbjohnson": "@peterbourgon Thanks for finding that. You're really digging into the code! It's checked in with this commit (2efa6b4107fecf5305ffe37fe301bebfba787cb9) along with a whole bunch of other changes. I have raftd almost working. Leader election is in place and log replication is almost there. I'll send you an e-mail when it's working.\n. @cespare Yes, log compaction is in there. @xiangli-cmu implemented it and he can speak to the specifics. I'll close out this issue.\n. Done.\n. @jessta thanks!\n. @xiangli-cmu Looks good. We'll also need to update the leader field when accepting an AppendEntries request. That way we can keep track of which Peer is the current leader.\n. @philips Thanks! It's a small but very important detail. :)\n. Fixed in 4ca46e7714d41d93f2064df4bf39ee1b512ed0b0.\n/cc: @xiangli-cmu\n. @xiangli-cmu Do you have any unit tests?\n. @xiangli-cmu Ok, cool. I went through it and it's a good start. I'll add line notes.\n. @xiangli-cmu Is this implementation based off the Raft Log Compaction paper? I didn't see any start and end log entries.\n. @xiangli-cmu I went through the commits tonight and it looks good. I'll try to get it all merged in tomorrow.\n. @xiangli-cmu I'm on a flight back from Ottawa and I'm going through the PR right now. It should be merged and pushed in within about 3 hours (when I get Internet service again). Ill take a look at doing the Apply in the Do() as well.\n. @xiangli-cmu I merged in the pull request but there's some clean up that still needs to be done -- mostly with naming, API & panics. I wanted to get it pushed in so it wouldn't get stale. There's other clean up that I need to do on go-raft and I'll do that after I get back from vacation. Feel free to send PRs in the meantime and I'll do my best to push them in when I have internet access available.\n. I didn't realize that maps could use len(). Thanks. That's so much easier. :)\n. @xiangli-cmu Thanks for the pull request! Send me smaller, bite-sized ones in the future (on feature branches) and I'll get them merged in a lot faster.\n. @xiangli-cmu If you can remove the commented out code and imports then I'll merge the pull request.\n. @xiangli-cmu Thanks for the pull request. I think the Timer implementation looks good. I did a minor refactor but just to clean up some duplicate code and move some variables into the mutex.\nI'm thinking about it again, though, and do we need a STOPPED, READY & RUNNING states? It seems like we could combine the STOPPED and READY states so there's just READY and RUNNING. What do you think?\n. @xiangli-cmu Yeah, the committed index vs last stored entry makes sense to me.\n. @xiangli-cmu I added you as a collaborator on the repo so I can assign issues to you. Still send over pull requests so I can review the code before it goes in.\n. I think we should probably have a way to tweak how much of the log to keep around to prevent slow followers from having to grab a snapshot. I think incremental snapshots would be a pain to implement. Maintaining extra log entries is a lot easier.\n. @xiangli-cmu If the server that the client connects to fails then I think the response for the command is just lost. The client will have to determine the state of the system and retry the command if necessary. Is there a particular use case you're thinking of?\n. Yeah, I like the idea. It shouldn't be too hard to implement either.\n. @xiangli-cmu Thanks for the pull request. I'm going to be back working on go-raft more later this week.\n. @xiangli-cmu We probably need to switch the locks to use sync.RWMutex. That should fix the problem.\n. @xiangli-cmu I like that idea. I think it might make more sense to just have a single server goroutine with a for loop that can switch between functions depending on state. Kicking off a bunch of goroutines whenever the state changes tends to cause hard-to-catch race conditions.\n. @xiangli-cmu That's basically what I was thinking too. Looks good. Just a couple notes:\n1. Do we need separate goroutines for sendAndCollectVotes and commitCenter? It seems like those could be combined into the candidate and leader functions without spawning off something asynchronous.\n2. Can you rename the functions to be verbs? e.g. follower() -> follow(), candidate() -> promote(), leader() -> lead()?\n3. Change the constants to be initial-case: LEADER -> Leader. Go doesn't typically use all-caps.\n. @xiangli-cmu I think the structure looks great. Go for it.\n. @xiangli-cmu You don't need to close the pull request. You can just submit new commits on your same branch on GitHub and it'll automatically add them to the pull request.\n. @xiangli-cmu There are a couple minor naming issues but I'm going to try to clean up the API this weekend. Thanks for the refactor. It's looking a lot cleaner.\n. @xiangli-cmu Make sure you do a pull after this is merged. Most of the method names have changed to limit them to internal use only (with the exception of anything in Server.go). The server is now the only interface to outside the library. I'm going to pull out locks on the log next.\n. @xiangli-cmu I'll change the Server to use RWLock. Having locks on the Log seems like overkill since we're typically in a lock on the Server whenever it's accessed anyway.\n. @xiangli-cmu After taking a look at this, I think I will keep the locks on the log. However, I'm going to try to refactor all state changes and commands into the Server event loop. Making Server essentially single threaded would cut down on the weird behavior.\n. @xiangli-cmu The heartbeats will still run from the peers which will be independent of the server and the votes will still get collected within the event loop as they are now. I suppose it's not technically single threaded. I'm just making the operations more sequential.\n. @xiangli-cmu I'll merge the pull request in after I get mine in. I have some fairly massive changes to server.go.\n. @xiangli-cmu I'll be online for a couple more hours if you have any questions. I'll be on later tonight as well.\n. @xiangli-cmu Also, I think we can get rid of the timers and use time.After() and afterBetween() instead. Still need to look into it further though.\n. @xiangli-cmu No, I haven't added the step down for candidate via AER but I'll do that.\n. @xiangli-cmu By the way, I think with this change to an event channel it might be pretty easy to move to a UDP based approach.\n. @xiangli-cmu Let me know when the pull request looks good to you and I'll merge it in. Also, I'm starting a new repository called go-raft-runner that will be used for long running tests of go-raft. I'll add you as a committer once I get something basic in there.\n. @xiangli-cmu I added an issue to make sure we get it fixed: https://github.com/benbjohnson/go-raft/issues/53\nThanks for the help with reviewing the pull request!\n. @xiangli-cmu looks good. thanks!\n. @xiangli-cmu if you hit CTRL-\\ when the nodes get stuck then you can see the stack trace dumped out. I'll look later tonight on my end.\n. @xiangli-cmu I was getting issues sporadically with one of the promotion tests so I'm sure it's an issue that I introduced. I'll take a look in a little while too if you don't get to it first.\n. @xiangli-cmu I understand the issue. This will work for now but I think there needs to be a better way to start/stop peers. We should really be creating a new stopChan each time we start a heartbeat and pass it into the heartbeat function. Then each time the heartbeat is stopped (or restarted), the previous stopChan should be closed and a new one created.\n. @xiangli-cmu I think you're making the timer reset too complicated and duplicating the setState(Candidate). Instead of calculating the time differences, create a timeoutChan:\ntimeoutChan := afterBetween(s.ElectionTimeout(), s.ElectionTimeout()*2)\nChange the select to:\ncase <-timeoutChan:\n    s.setState(Candidate)\nAnd then reset the timeoutChan if update is true:\nif update {\n    timeoutChan = afterBetween(s.ElectionTimeout(), s.ElectionTimeout()*2)\n}\nThat way you can take out the whole else clause (since the state change to Candidate will be handled when the timeoutChan fires).\nP.S. I'm not sure if you're intending it or not but \"Please try to fully understand this\" comes off as condescending.\n. @xiangli-cmu No problem. I check everything that comes through pretty carefully unless it's either trivial or it's a temporary fix (and I'm planning on refactoring it later).\n. @xiangli-cmu Looks good. Thanks!\n. @xiangli-cmu What do you think of removing Initialize() and just having a Start()? It seems like we could combine the StartFollower() and StartLeader() into a single Start() that starts as a Leader if IsLogEmpty() == true.\n. @xiangli-cmu Good point. I see what you're saying. I'll keep it as-is for now.\n. @xiangli-cmu No, I wanted to make something that's reusable for simple projects. More complex projects might need to have their own transporter but for transporting AE & RV it does a pretty good job.\n. @xiangli-cmu Good point. I copied over a implementation from another project that used PATCH and client.Patch() doesn't exist. That's fixed now.\n. @peterbourgon What's the meaning of :dango:?\n. Cool! I wasn't sure if it was an emoji inside joke or if you were telling me to shove a :dango: up my ***. :flushed:\nJust checking. :)\nBy the way, great job on your HTTP transporter. I wrote one at first and it sucked so I started over and based it off yours.\n. @xiangli-cmu I'll give it a try. I'll convert it to a Go benchmark test (http://golang.org/pkg/testing/). Also, can you use underscore naming for files instead of camel casing?\n. @xiangli-cmu I'd definitely like to look into moving away from JSON. I used that at the beginning because it was easy to debug.\nThere's two options for encoding. We might want to support both.\n1. Change to a different encoding scheme. msgpack looks like it's about 3x faster than JSON in this benchmark.\n2. Allow the application to perform the encoding and simple exchange []byte values. I'd like to make it optional on a per-command basis since most people will only need to have a couple of high frequency commands be high performance.\n. @xiangli-cmu I'm trying out waffle.io to help manage the issues.\n. @ongardie Should a server wait until receiving an AppendEntries RPC (to receive the commitIndex) from the current leader before applying commands in the log? Or should the committed index be saved to disk?\n. @ongardie I understand that it's not needed for safety. I guess my biggest concern was if the entire cluster goes down and then reboots then every node is waiting for an AE. I suppose it could wait for an election timeout before trying to replay the log. That way you wouldn't get delayed by replaying first if it's not needed.\n. @xiangli-cmu Sounds good.\n. @xiangli-cmu I'm not stoked about the Index stuff but I'll leave it in for now. I don't want to do premature optimization.\n. @xiangli-cmu I was thinking that it actually wrapped functionality instead of having steps in between. I'm worried that this will give errant timings since there's not an explicit stop. If there's a pause after one of those steps then there's no way to indicate that the work is done. It might make more sense just to use the built-in profiler.\n. @xiangli-cmu Can you use feature branches? It's easier to revert changes that aren't included like this if you can just dump the branch.\n. @xiangli-cmu Is this for the purpose of testing? I'd like to change the API so we just have one Server.Start() method. For testing we can call out to a Server.startAs(role) but I'd like to make the external API as simple as possible.\n. @xiangli-cmu Nodes should know if they're brand new by the existence of a snapshot or log. We should be able to automatically set synced based on that.\n. @xiangli-cmu Honestly, I'd like to make the API as simple and clean as possible. It's a lot easier to add to an API later on than to take away.\n. @xiangli-cmu My thought was just to have this API:\ngo\nServer.Start()\nServer.startAs(role string)\nAnd then Start() would just be a call to startAs(Follower). As for syncing, I'd rather have the tests replicate a real life scenario. Can you have the test do this:\n1. Start up a cluster\n2. Run some commands\n3. Add a new node.\n4. Verify the follower catches up.\nThe new follower should see that it doesn't have a log and wait for a sync.\nWhere is this sync coming from in the Raft paper?\n. @xiangli-cmu Can you move MaxLogEntriesPerRequest to the Server and have it pass a maxEntryCount argument into the GetEntriesAfter()? The log shouldn't have any knowledge of requests.\n. @xiangli-cmu This works for now but I'm going to change it when I refactor the log to limit by bytes instead of messages. Also, when I do the log refactor, I'm going to store the log entries in memory as []byte variables so that we don't have to serialize for every peer. We'll just send the bytes. It'll also make limiting the message size easy.\n. @xiangli-cmu I agree that keeping messages small is a good thing but we don't always have control over message size. If a database is saving a record then that record could be 100b or it could be 100kb. 200 100kb messages is 20mb which is too big to be sending 4 peers for an AE.\nThe overhead per message could vary quite a bit. I could see adding a weight to commands to adjust the overhead of an individual command. But that's an optimization to do down the line if it becomes an issue.\nI won't add the bytes MTU right away. Let's see how the message MTU works for now.\n. @xiangli-cmu We may want to expand out the encoding benchmarks to see how it fares depending on AE size. Right now it's still encoding 2000 entries at a time.\n. @philips I don't have the exact numbers but I believe they were ~12MB/s for request encoding and ~6MB/s for request decoding.\n. @philips I referenced these benchmarks for gob and msgpack. They're not much faster than JSON. The biggest hurdle is having to use the reflection API which is very slow. I'm not a huge fan of binary protocols but it's definitely the fastest.\n. Talked with @xiangli-cmu over IM. He's going to try to swap out the binary encoding for protobufs to see if it's as fast/faster while being more maintainable in the long run.\n. @xiangli-cmu This looks good to me except for the minor changes I listed. We need to figure out a flexible strategy for snapshotting though. I'm going to be going through 200 entries in a fraction of a second so I'm going to constantly have followers needing snapshots. That's something we can worry about later though.\n. @xiangli-cmu If you want to write out the commitIndex then can you add it to a separate file? Maybe a commit file that lives in the same directory as the log. I see it as only a temporary fix as we really need to save the cluster configuration whenever it's updated. That should live inside Raft.\nAlso, I'm concerned that doing random access inside the log is going to kill performance on spinning disks. I'd rather keep the log append only. And if we're going to provide an option to save the commitIndex going forward then I'd rather it be isolated as its own file.\n. @xiangli-cmu Seek changes the pointer in memory but moving to a position in the file and writing causes the disk heads to have to move more (instead of simply appending to the end of a file). Also, it doubles our IOPS as we have to write two pages of data instead of just one. If you're running on a system like Amazon EC2 with network attached storage then IOPS can be really expensive.\nWe need to save the cluster configuration because that'll let the server know who to request votes from on startup. Otherwise you risk having a cluster of servers waiting to receive AEs.\nThe cluster configuration is going to have to live within Raft. It's basically just going to save a list of named nodes and connection strings (e.g. udp://mynode1). The application itself will have to determine how to parse the connection string.\n. @xiangli-cmu The configuration needs to be separate from the snapshot as we'll need to save the configuration immediately after it's committed.\n. @xiangli-cmu We're going to need to move basic configuration inside the library. The library needs to know too much about the configuration for it not to be in there.\nAs far as recovery, we shouldn't be recovering commands until we know that there's not a new snapshot. We should load up the configuration first and probably only recover the node becomes leader or it is sent an AE from a leader.\nWhile I am concerned about performance, I think that the bigger concern is that we need to move cluster configuration storage into the library and this is a stopgap solution so I'd prefer to keep it isolated as much as possible.\n. @xiangli-cmu Just some minor tweaks but otherwise looks good. If you can check-in the changes then I'll merge it.\n. @xiangli-cmu The first needs to be a self join since it'll need to be replicated to peers when other nodes join. If node A starts and adds B, then A fails and B becomes leader, but then B never knows about A since there wasn't a log entry.\n. @xiangli-cmu yes, that's correct.\n. @xiangli-cmu Looks good. Thanks!\n. @xiangli-cmu Can you change the LeaveCommand to be an interface with a DefaultLeaveCommand for consistency?\n. @xiangli-cmu This may need a mutex since the logLevel will be accessed and mutated in different goroutines. I'm going to get go-raft setup on Travis CI sometime soon and I'll be more concerned about it then. Not too concerned about it right now.\nJust fix the code mentioned in the line note above and go ahead and merge it.\n. @xiangli-cmu This won't work if you have more than one Raft server running in a process. I'm using a tiered Raft system so I actually have two Raft servers running in each process.\n. @xiangli-cmu Also, do you have benchmark numbers showing a performance improvement?\n. @xiangli-cmu I was looking for more of a Go-style benchmark that we could keep in the code so we could see how performance changes over time and so we can test whether changes like this make much of a difference. Moving the buffer to the Log is a messier implementation but I'm willing to do it if it means a significant performance boost. Can you convert this test to a Go-style benchmark?\n. @xiangli-cmu Can you send from a feature branch next time?\n. @matttproud Thanks for the PR. I've been meaning to set up Travis but I've been pushing it off until we move out some of the longer tests. If it's working for you, though, I'll go ahead and set it up on my account.\n. Looks like it's running now: https://travis-ci.org/benbjohnson/go-raft/builds/9779016\n. @philips It's just extraneous code. Response bodies need to be closed but request bodies don't.\n. @xiangli-cmu That makes sense to me. I could see how the channel would start to block after there's more than 256 events waiting.\n. @xiangli-cmu No time this weekend to look over this. Checking it out now. Do you know what the build error is from? https://travis-ci.org/benbjohnson/go-raft/builds/9796572\n. @traviscline Thanks for submitting the bug. The race detector was passing for a while but we must have gotten lazy. :)\nI'll fix the issues and change Travis CI to use the race detector on every build.\n. @erikstmartin I still want to address those races but I haven't had the time so far. If you want to send some PRs that'd be great. Otherwise I'll try to fix them in the near future.\n. :+1: \n. @philips Good call on the unnesting. Looks better. Thanks.\n. @philips Looks good.\n. @philips I think moving the library to a goraft organization is a good idea. It's grown up enough to have it's own org. Add me as an admin and I'll transfer it over.\n. @philips It's switched over: https://github.com/goraft/raft\n. @xiangli-cmu It seems like we could have the node continue to run but just prevent it from becoming a candidate or voting if it's not part of the cluster configuration. That way it can pick up changes when it re-joins. Would that work?\n. @xiangli-cmu We should prevent a node from promoting or voting if it's not a part of the cluster though. That's core Raft functionality.\n. @xiangli-cmu Just those two changes. Otherwise it looks good. Go ahead and merge after you fix those.\n. @xiangli-cmu Can you add a test case for this? After that we can merge in the changes.\n. @mattn What's the error on Windows? It actually looks like os.File.Sync() should be a cross-compatible way to sync the file. Although it also uses syscall.Fsync() internally which is generated on a per-OS basis.\n. Looks good to me.\n. @xiangli-cmu Sorry for the delay in looking at the PR. I'm trying to wrap up my Raft presentation that I'm giving at Strange Loop tomorrow morning.\nEverything looks good. I added one question to clarify. Also, why did you expose all the protobuf encoding/decoding API?\n. @xiangli-cmu Makes sense about the transport layer. Merging...\n. @xiangli-cmu By the way, here are my slides from Strange Loop today:\nhttps://speakerdeck.com/benbjohnson/raft-the-understandable-distributed-consensus-protocol\nThe video will be released in the next couple of months.\n. :+1: \n. @xiangli-cmu That'd be great. It's awesome how etcd has taken off lately.\n. @xiangli-cmu Thanks. I was adding the issue so I wouldn't forget about it. :)\n. @jvshahid Hmmm... the uncommitted logs shouldn't be getting lost just because leadership is lost. They should only get removed if another leader has a different log history. That's a bug that should be fixed.\nRemoving the timeout should make it hang until either a) the command is committed by that leader, or b) another leader causes the log entry to be removed.\nAs for retries, I think that should live on the client. If a command fails for some reason, the client is probably the only one with the knowledge to check the current state of the system and optional resubmit the command.\n. @grncdr Thanks! Good documentation is half the battle. :)\n. The leader doesn't try to re-elect if followers are down. It'll wait until another another node tries to become a candidate.\nAre you seeing the writes actually commit against the leader?\n. @xiangli-cmu Can you change this to use sort.Reverse() instead? I think making uint64Slice sort in reverse order is going to bite us down the road.\n. :+1: \n. After thorough testing, I approve this PR. :)\n. @dgryski Thanks! There's some internal clean up that needs to be done in goraft. Much appreciated. :)\n. @xiangli-cmu Sorry I didn't get to this sooner. Two points of feedback:\n1. I don't think we should expose setTerm() on an interface just for tests. Can you manually call s.(*server).mutex.Lock() in the tests instead and remove the setTerm() function? It's also strange because the initial-lowercase name will cause the function to not be exportable. I'm actually not sure how that would even work in Go.\n2. I'm still seeing a data race for event#returnValue. Can you wrap this in a getter/setter function with a lock? Here's the race condition report: https://gist.github.com/benbjohnson/763b7e55fa8f59244ba5\n. @philips I prefer that too actually. I started using the Zen Mode for editing right on GitHub and it's been nice but it does one long line. Also, I read that GFM adds line breaks for multiple lines but I guess .md is just straight markdown (ref).\n. @kellabyte Good point. The overhead is more related to message processing time and not necessarily latency. I'll make it more descriptive.\n. @ongardie Good point. I updated the README to be mainly around node failure tolerance. I wanted to keep it simple so I left out cost, latency, & election conflicts. Let me know what you think.\n. :+1: \n/cc: @bketelsen @erikstmartin\n. @jvshahid I just merged it: https://github.com/goraft/raft/pull/136\nLet me know if you have any other issues.\n. @xiangli-cmu Sorry for the delay in getting to this. Why is this in the Raft library? Sync isn't a part of the Raft protocol. Can you move this to etcd?\n. Sync is a strange concept in Raft since Raft is meant to deterministically change a state machine. A sync shouldn't be necessary in most applications. Even if there is a sync in another application, the details will probably be different.\nAdding it may not add CPU/network overhead but it complicates the library. Can you move it over to etcd and we'll integrate it back into Raft if we see a lot of people trying to do something similar? The etcd PeerServer already knows about Raft. I don't think it's too bad to have it manage the sync based on the leader state.\n. @xiangli-cmu I'm ok if you want to include some kind of callback from the Server for state changes. Maybe something like:\ntype StateChangeHandler interface {\n    HandleStateChange(*Server)\n}\nAnd then add a function to Server it:\ntype Server interface {\n    ...\n    SetStateChangeHandler(StateChangeHandler)\n}\nJust make sure it's called synchronously on state change. No channels or goroutines.\n. @densone I just added a fix to the fix-restart branch. Can you try it out?\nsh\n$ cd $GOPATH/src/github.com/goraft/raft\n$ git fetch\n$ git checkout fix-restart\n. Awesome. It seems like problem is that when the server restarts, it's trying to reuse the commit channel to notify clients of entry commits. If the channel has already been used then it'll default to that panic. Setting the commit to nil lets it meet the conditional that wraps the select and ensures that a commit notification only gets sent once.\n. Fixes #130.\n. @JensRantil Thanks for fixing that.\n. @JensRantil Some of the panics are for unrecoverable errors. We should use errors instead most of the time. The ones in command.go are there because they shouldn't fail but if they do then it'll typically happen at initialization. As Xiang said, feel free to send a PR. Otherwise we can try to tackle it in the near future.\n. @xiangli-cmu Why the change to time.Tick? Also, please let me take a look at PRs before you merge.\n. @xiangli-cmu What's the context for this change?\n. @xiangli-cmu The biggest thing that worries me is making Peer.sendAppendEntriesRequest() into an async call. If a peer is hanging for some reason and we're sending a large AE every 50ms then it's going to go downhill really fast.\n. @xiangli-cmu Otherwise it looks fine to me.\n. @klobucar Thanks! Good idea.\n. Thanks @Scooletz! :+1:\n. I'm merging this since it's low risk. Comments and change suggestions still welcome though.\n. @xiangli-cmu +1. let's make sure we test this change against etcd as well and get it merge in there soon.\n. @xiangli-cmu I thought about making it configurable but I didn't want to add any more methods to the already large Server API. I'm going to leave it for now and we can always add it in later if people need it.\n. @xiangli-cmu So what's the extra code the application needs to add in? Also, have you tested this code against etcd?\n. Works for me.\n. :+1: looks good to me. I'd be curious to hear what your use case is. Sounds cool.\nIs it a project we can list of the README?\n. I can honestly say I never thought go-raft would be used in a security competition. :)\nCan't wait!\n\n. @xiangli-cmu lgtm.\n. :+1: \n. @xiangli-cmu I don't think I quite understand the PR comment. Do you want to remove the Sync() on the log when the leader is committing a command?\n. @xiangli-cmu Technically I think it's supposed to fsync before it sends to the followers. Although you might be right. If the entry hasn't been committed by the leader then it's not externally visible so it might be ok if it gets lost. It'll be committed by the next leader.\nYou still need to fsync the followers after each append so this might just end up making the followers a lot slower.\nI'm ok with adding configurable options for fsync. Some people may want to trade off a little safety for a hefty performance boost. It should be strict by default though.\n. @xiangli-cmu lgtm. Can you clean up the writeFileSynced() and go ahead and merge.\n. @xiangli-cmu That looks good to me. Can we remove the Proto prefix from all the type names? It seems redundant since they're already in the protobuf package.\n. :+1: \n. @xiangli-cmu lgtm. How was it committing the entry for a single node before? It seems like the setCommitIndex() was only on AE req/resp.\n. @jvshahid thanks!\n:+1: :+1: :+1:\n. I also need to add the race detector to the CI but I'll get that in there after improving snapshot coverage.\n. :+1: \n. @philips @xiangli-cmu I removed Travis builds from Raft because they're constantly failing. My guess is that the servers are overloaded which causes pauses which fails the liveness tests. The Drone builds are running fine:\nhttps://drone.io/github.com/goraft/raft/8\n. Also, Coveralls coverage should update soon. Not sure why it takes so long to calculate the coverage:\nhttps://coveralls.io/builds/455791\n. @philips I agree that the naming is confusion and adding the lock will let us remove that. I wanted to limit the scope of this PR to adding test coverage so we could know if those changes will break anything. Test coverage got a +5% bump. woohoo!\n. @ptsolmyr Thanks for catching that. It looks like @xiangli-cmu got a fix in.\n. lgtm.\nhttps://drone.io/github.com/goraft/raft/11\n. @xiangli-cmu I think Travis is just slow. :)\n. @philips I'd rather not expose the channel directly. If we're interested in checking if the queue is backed up then I'd rather make an API for that. Maybe another threshold event for it?\n. @philips @mreiferson The idea of sendAsync() is to allow the heartbeat to continue even if the server gets backed up by something like a long disk write. That's why there's a buffer on Server.c.\nHowever, I agree that a buffered channel isn't the right approach. A better approach would be to eliminate the buffered channel and do processing of the peer AE responses through a goroutine.\nWhat do you think of that?\n. @mreiferson That's a good point -- a go-raft instance indefinitely hung on a disk write would hang the cluster. I think people will just need to bump up the heartbeat interval if they're running on something like EBS where 99% percentile latency can be higher than the heartbeat interval.\n. Sorry for the delay in responding. I've been out sick for a couple days. I'm :+1: and :ok_hand: (and any other related emoji) on removing the buffer.\nUnrelated: do you guys think it would make sense to maintain the heartbeat in the Server instead of in the Peer? We could kick off goroutines on regular intervals within the server event loop so we wouldn't have peers acting quite so non-deterministically. Just throwing that idea out there.\n. @xiangli-cmu It looks like the build is failing now:\nhttps://drone.io/github.com/goraft/raft/15\nCan you fix that?\n. :+1: Thanks @philips.\n. @philips I agree that \"interval\" makes more sense than \"timeout\". I feel like I used timeout originally because it was in the original Raft paper. I can't find a copy of the original anymore though.\nThe biggest issue that I have is that it breaks the API and therefore other user's integration. go-raft isn't at v1.0.0 yet so the API isn't set in stone but I'd like to hear from other users before merging.\n@bketelsen @erikstmartin @pauldix @jvshahid What do you guys think? Is it a problem for you if we make this aesthetic change?\n. Thanks everybody for chiming in.\n@philips What's the API change for the event channel?\n. :+1: \n. @mreiferson I think it makes sense to go through and clean up a lot of this. If we remove the buffered channel (#160) and move heartbeating initiation to the server then we could essentially have a completely single threaded implementation. That would make everything a lot easier to reason about and we could probably drop many of the locks.\nI agree with @xiangli-cmu that a lot of the quirkiness comes from the rewrite from the non-event loop design. But it's time to get it cleaned up now.\nI've also been thinking about splitting off the individual states into their own types and have the Server essentially be a wrapper that only holds one state at a time and just handles the handoff between states. It's essentially just a state machine. I haven't quite got the whole design figured out yet though (and I'm not entirely sure it's the best plan either). :)\n. lgtm. Did you test this against etcd?\n. @xiangli-cmu lgtm\n. @xiangli-cmu What static data?\n. @xiangli-cmu Why are you moving those to a separate package?\n. @xiangli-cmu They're all functionality related to the same thing. go-raft is meant to be a relatively small, self contained library. Splitting off into additional packages is overkill.\n. @xiangli-cmu I don't think there's any benefit to separating them off into their own package except for aesthetics. I'm fine if you want to consolidate some of the files in the root package:\nappend_entries_request.go + append_entries_response.go \u2192 append_entries.go\nEverything in the root package is related. It doesn't make sense to split it off.\n. @xiangli-cmu I'm not opposed to aesthetics but I am against making two packages for something that is in the same functional group. \n. @xiangli-cmu lgtm.\n. @philips :+1: \n. @chrislusf lgtm. If you can squash the commits into one then I can merge it. Also, Github doesn't notify us when there's a new commit on a PR so just add a comment if you make changes. Thanks!\n. @xiangli-cmu Is there something that requires this change?\n. @xiangli-cmu lgtm. The candidateLoop() seems simpler now. Thanks!\n. @xiangli-cmu Can you set up Drone to point to your local xiangli-cmu/raft repo so it'll do a build for these PRs?\n. @xiangli-cmu lgtm as long as the test suite is :green_apple:.\n. @xiangli-cmu Can you fix the spelling of \"updateCurrentTerm\" in the assert message. Otherwise lgtm.\n. @philips I haven't used chan chan before but it looks like a good way to do a req/resp. I'm good with it. lgtm.\n. :+1:\n. @xiangli-cmu I like that naming better. The previous naming always confused me.\n. @xiangli-cmu lgtm\n. Thanks for catching that. I added a fix to https://github.com/goraft/raft/pull/192. Look ok?\n. @xiangli-cmu lgtm\n. @xiangli-cmu Is this related to a bug? Does it pass the race detector? We need a lock around Server.peers since that can be read by the application.\n. @xiangli-cmu If you call Server.Peers() while updateCurrentTerm() is messing with the peers then you'll get a race error. We need the lock around there.\n. @xiangli-cmu Sure. Go for it. Check it against the etcd functional tests with -race set on the tests and on the build.\n. @xiangli-cmu Does the Init() function need to be exported? Are you going to call it directly or are you still calling Start()?\n. @unihorn Is there something you want to do differently with a New/Uninitialized state? The Server state machine is starting to get unwieldily and I don't want to add states for the sake of adding states.\n. @unihorn I'd like to stray from the Raft protocol as little as possible. I don't see the point in splitting off an additional state.\n. Fix the missing error check and then lgtm.\n. @unihorn Can you explain more why this is needed? Why isn't the commit index getting flushed on every update?\n. @xiangli-cmu Ok, I remember that conversation now. A manual flush is fine with me.\n. @xiangli-cmu What happens if the directory exists but it's not writable? I don't quite understand why we're only checking for one type of error with Mkdir().\n. @xiangli-cmu How is err != nil && os.IsNotExist(err) checking for all types except for existence? That should only be true where err == ErrNotExist. Basically, only when the directory doesn't exist.\nDid you mean err != nil && !os.IsNotExist(err)?\nhttp://golang.org/pkg/os/#IsNotExist\n. @xiangli-cmu The only time ErrNotExists will occur with Mkdir() is if the parent directory doesn't exist. In this case, the parent directory is s.path, which should exist. \nhttp://play.golang.org/p/Kezcz9swTX\n. No worries. You just need a bang for all errors besides \"not exists\": err != nil && !os.IsNotExist(err)\n. :+1: \n. @xiangli-cmu Where are you seeing the data races? From go test -race? Or from -race on etcd?\n. @tsenart Thanks for the PR. Looks good to me. I like good errors. :)\nCan you combine the error check from f.Write() into a single if/else block? e.g.:\ngo\nn, err := f.Write(data)\nif err == nil && n < len(data) {\n    return io.ErrShortWrite\n} else if err != nil {\n    return err\n}\n. @tsenart Well, I think chaining all of it is overkill. :) It's hard to see the Sync() embedded in there. Can you move the Sync() and error check back into its own if statement?\nAlso, the io.ErrShortWrite is now getting set but not returned. Instead of:\nerr = io.ErrShortWrite\ncan you do:\nreturn io.ErrShortWrite\n. @tsenart Thanks!\n. @unihorn Can you add error messages to the http.Error() calls? The WaitGroup makes me a little nervous since it could hang indefinitely on shutdown (or at least as long as an HTTP timeout). Although most stops are going to be because of a signal (SIGINT/SIGTERM) so I'm ok with this.\n. No CGO dependencies inside go-raft. Too many people depend on the library already. btrfs support seems like a niche feature to support.\nJust expose the log file's file descriptor and let the application support it.\n. @xiangli-cmu The JoinCommand and LeaveCommand interface don't actually work well as marker interfaces. They're currently both the same interface:\nhttps://github.com/goraft/raft/blob/047192edb174541d21f2f93f24e587b2f723889e/commands.go#L20\nNot sure if that bites you in this situation but it's something to be aware of.\n. lgtm\n. @watsonjiang There are not currently plans to make logging pluggable or change the logging infrastructure. Adding glog would add an extra dependency to go-raft that not everyone wants.\n. @otoolep @swsnider Can you guys confirm that this PR from @hanlz works and I'll merge it in.\nhttps://github.com/goraft/raft/pull/231\n. :+1: Thanks for the fix! :)\n. :+1: \n. @mre Yes, the original devs on the project have either built newer implementations or are not using the library anymore. @philips is adding a shield to the README to notify users that the library is unmaintained and new maintainers are welcome.\nhttps://github.com/goraft/raft/pull/241\n. @mre I'm working with InfluxDB on a high performance version of raft to stream all data through. After that's finished, InfluxDB won't have a go-raft dependency.\n. @philips I would suggest just adding an \"unmaintained\" shield and a call for maintainers at the top of the README.\n\n. lgtm, Brandon. Thanks for writing it up.\n. @chrislusf as @philips mentioned, this repo is currently unmaintained. You can submit a PR for the change or you can fork this repo and maintain changes in your fork.\n. This repo is no longer maintained and etcd uses a different raft implementation.\n. @peterbourgon Thanks for finding that. You're really digging into the code! It's checked in with this commit (2efa6b4107fecf5305ffe37fe301bebfba787cb9) along with a whole bunch of other changes. I have raftd almost working. Leader election is in place and log replication is almost there. I'll send you an e-mail when it's working.\n. @cespare Yes, log compaction is in there. @xiangli-cmu implemented it and he can speak to the specifics. I'll close out this issue.\n. Done.\n. @jessta thanks!\n. @xiangli-cmu Looks good. We'll also need to update the leader field when accepting an AppendEntries request. That way we can keep track of which Peer is the current leader.\n. @philips Thanks! It's a small but very important detail. :)\n. Fixed in 4ca46e7714d41d93f2064df4bf39ee1b512ed0b0.\n/cc: @xiangli-cmu\n. @xiangli-cmu Do you have any unit tests?\n. @xiangli-cmu Ok, cool. I went through it and it's a good start. I'll add line notes.\n. @xiangli-cmu Is this implementation based off the Raft Log Compaction paper? I didn't see any start and end log entries.\n. @xiangli-cmu I went through the commits tonight and it looks good. I'll try to get it all merged in tomorrow.\n. @xiangli-cmu I'm on a flight back from Ottawa and I'm going through the PR right now. It should be merged and pushed in within about 3 hours (when I get Internet service again). Ill take a look at doing the Apply in the Do() as well.\n. @xiangli-cmu I merged in the pull request but there's some clean up that still needs to be done -- mostly with naming, API & panics. I wanted to get it pushed in so it wouldn't get stale. There's other clean up that I need to do on go-raft and I'll do that after I get back from vacation. Feel free to send PRs in the meantime and I'll do my best to push them in when I have internet access available.\n. I didn't realize that maps could use len(). Thanks. That's so much easier. :)\n. @xiangli-cmu Thanks for the pull request! Send me smaller, bite-sized ones in the future (on feature branches) and I'll get them merged in a lot faster.\n. @xiangli-cmu If you can remove the commented out code and imports then I'll merge the pull request.\n. @xiangli-cmu Thanks for the pull request. I think the Timer implementation looks good. I did a minor refactor but just to clean up some duplicate code and move some variables into the mutex.\nI'm thinking about it again, though, and do we need a STOPPED, READY & RUNNING states? It seems like we could combine the STOPPED and READY states so there's just READY and RUNNING. What do you think?\n. @xiangli-cmu Yeah, the committed index vs last stored entry makes sense to me.\n. @xiangli-cmu I added you as a collaborator on the repo so I can assign issues to you. Still send over pull requests so I can review the code before it goes in.\n. I think we should probably have a way to tweak how much of the log to keep around to prevent slow followers from having to grab a snapshot. I think incremental snapshots would be a pain to implement. Maintaining extra log entries is a lot easier.\n. @xiangli-cmu If the server that the client connects to fails then I think the response for the command is just lost. The client will have to determine the state of the system and retry the command if necessary. Is there a particular use case you're thinking of?\n. Yeah, I like the idea. It shouldn't be too hard to implement either.\n. @xiangli-cmu Thanks for the pull request. I'm going to be back working on go-raft more later this week.\n. @xiangli-cmu We probably need to switch the locks to use sync.RWMutex. That should fix the problem.\n. @xiangli-cmu I like that idea. I think it might make more sense to just have a single server goroutine with a for loop that can switch between functions depending on state. Kicking off a bunch of goroutines whenever the state changes tends to cause hard-to-catch race conditions.\n. @xiangli-cmu That's basically what I was thinking too. Looks good. Just a couple notes:\n1. Do we need separate goroutines for sendAndCollectVotes and commitCenter? It seems like those could be combined into the candidate and leader functions without spawning off something asynchronous.\n2. Can you rename the functions to be verbs? e.g. follower() -> follow(), candidate() -> promote(), leader() -> lead()?\n3. Change the constants to be initial-case: LEADER -> Leader. Go doesn't typically use all-caps.\n. @xiangli-cmu I think the structure looks great. Go for it.\n. @xiangli-cmu You don't need to close the pull request. You can just submit new commits on your same branch on GitHub and it'll automatically add them to the pull request.\n. @xiangli-cmu There are a couple minor naming issues but I'm going to try to clean up the API this weekend. Thanks for the refactor. It's looking a lot cleaner.\n. @xiangli-cmu Make sure you do a pull after this is merged. Most of the method names have changed to limit them to internal use only (with the exception of anything in Server.go). The server is now the only interface to outside the library. I'm going to pull out locks on the log next.\n. @xiangli-cmu I'll change the Server to use RWLock. Having locks on the Log seems like overkill since we're typically in a lock on the Server whenever it's accessed anyway.\n. @xiangli-cmu After taking a look at this, I think I will keep the locks on the log. However, I'm going to try to refactor all state changes and commands into the Server event loop. Making Server essentially single threaded would cut down on the weird behavior.\n. @xiangli-cmu The heartbeats will still run from the peers which will be independent of the server and the votes will still get collected within the event loop as they are now. I suppose it's not technically single threaded. I'm just making the operations more sequential.\n. @xiangli-cmu I'll merge the pull request in after I get mine in. I have some fairly massive changes to server.go.\n. @xiangli-cmu I'll be online for a couple more hours if you have any questions. I'll be on later tonight as well.\n. @xiangli-cmu Also, I think we can get rid of the timers and use time.After() and afterBetween() instead. Still need to look into it further though.\n. @xiangli-cmu No, I haven't added the step down for candidate via AER but I'll do that.\n. @xiangli-cmu By the way, I think with this change to an event channel it might be pretty easy to move to a UDP based approach.\n. @xiangli-cmu Let me know when the pull request looks good to you and I'll merge it in. Also, I'm starting a new repository called go-raft-runner that will be used for long running tests of go-raft. I'll add you as a committer once I get something basic in there.\n. @xiangli-cmu I added an issue to make sure we get it fixed: https://github.com/benbjohnson/go-raft/issues/53\nThanks for the help with reviewing the pull request!\n. @xiangli-cmu looks good. thanks!\n. @xiangli-cmu if you hit CTRL-\\ when the nodes get stuck then you can see the stack trace dumped out. I'll look later tonight on my end.\n. @xiangli-cmu I was getting issues sporadically with one of the promotion tests so I'm sure it's an issue that I introduced. I'll take a look in a little while too if you don't get to it first.\n. @xiangli-cmu I understand the issue. This will work for now but I think there needs to be a better way to start/stop peers. We should really be creating a new stopChan each time we start a heartbeat and pass it into the heartbeat function. Then each time the heartbeat is stopped (or restarted), the previous stopChan should be closed and a new one created.\n. @xiangli-cmu I think you're making the timer reset too complicated and duplicating the setState(Candidate). Instead of calculating the time differences, create a timeoutChan:\ntimeoutChan := afterBetween(s.ElectionTimeout(), s.ElectionTimeout()*2)\nChange the select to:\ncase <-timeoutChan:\n    s.setState(Candidate)\nAnd then reset the timeoutChan if update is true:\nif update {\n    timeoutChan = afterBetween(s.ElectionTimeout(), s.ElectionTimeout()*2)\n}\nThat way you can take out the whole else clause (since the state change to Candidate will be handled when the timeoutChan fires).\nP.S. I'm not sure if you're intending it or not but \"Please try to fully understand this\" comes off as condescending.\n. @xiangli-cmu No problem. I check everything that comes through pretty carefully unless it's either trivial or it's a temporary fix (and I'm planning on refactoring it later).\n. @xiangli-cmu Looks good. Thanks!\n. @xiangli-cmu What do you think of removing Initialize() and just having a Start()? It seems like we could combine the StartFollower() and StartLeader() into a single Start() that starts as a Leader if IsLogEmpty() == true.\n. @xiangli-cmu Good point. I see what you're saying. I'll keep it as-is for now.\n. @xiangli-cmu No, I wanted to make something that's reusable for simple projects. More complex projects might need to have their own transporter but for transporting AE & RV it does a pretty good job.\n. @xiangli-cmu Good point. I copied over a implementation from another project that used PATCH and client.Patch() doesn't exist. That's fixed now.\n. @peterbourgon What's the meaning of :dango:?\n. Cool! I wasn't sure if it was an emoji inside joke or if you were telling me to shove a :dango: up my ***. :flushed:\nJust checking. :)\nBy the way, great job on your HTTP transporter. I wrote one at first and it sucked so I started over and based it off yours.\n. @xiangli-cmu I'll give it a try. I'll convert it to a Go benchmark test (http://golang.org/pkg/testing/). Also, can you use underscore naming for files instead of camel casing?\n. @xiangli-cmu I'd definitely like to look into moving away from JSON. I used that at the beginning because it was easy to debug.\nThere's two options for encoding. We might want to support both.\n1. Change to a different encoding scheme. msgpack looks like it's about 3x faster than JSON in this benchmark.\n2. Allow the application to perform the encoding and simple exchange []byte values. I'd like to make it optional on a per-command basis since most people will only need to have a couple of high frequency commands be high performance.\n. @xiangli-cmu I'm trying out waffle.io to help manage the issues.\n. @ongardie Should a server wait until receiving an AppendEntries RPC (to receive the commitIndex) from the current leader before applying commands in the log? Or should the committed index be saved to disk?\n. @ongardie I understand that it's not needed for safety. I guess my biggest concern was if the entire cluster goes down and then reboots then every node is waiting for an AE. I suppose it could wait for an election timeout before trying to replay the log. That way you wouldn't get delayed by replaying first if it's not needed.\n. @xiangli-cmu Sounds good.\n. @xiangli-cmu I'm not stoked about the Index stuff but I'll leave it in for now. I don't want to do premature optimization.\n. @xiangli-cmu I was thinking that it actually wrapped functionality instead of having steps in between. I'm worried that this will give errant timings since there's not an explicit stop. If there's a pause after one of those steps then there's no way to indicate that the work is done. It might make more sense just to use the built-in profiler.\n. @xiangli-cmu Can you use feature branches? It's easier to revert changes that aren't included like this if you can just dump the branch.\n. @xiangli-cmu Is this for the purpose of testing? I'd like to change the API so we just have one Server.Start() method. For testing we can call out to a Server.startAs(role) but I'd like to make the external API as simple as possible.\n. @xiangli-cmu Nodes should know if they're brand new by the existence of a snapshot or log. We should be able to automatically set synced based on that.\n. @xiangli-cmu Honestly, I'd like to make the API as simple and clean as possible. It's a lot easier to add to an API later on than to take away.\n. @xiangli-cmu My thought was just to have this API:\ngo\nServer.Start()\nServer.startAs(role string)\nAnd then Start() would just be a call to startAs(Follower). As for syncing, I'd rather have the tests replicate a real life scenario. Can you have the test do this:\n1. Start up a cluster\n2. Run some commands\n3. Add a new node.\n4. Verify the follower catches up.\nThe new follower should see that it doesn't have a log and wait for a sync.\nWhere is this sync coming from in the Raft paper?\n. @xiangli-cmu Can you move MaxLogEntriesPerRequest to the Server and have it pass a maxEntryCount argument into the GetEntriesAfter()? The log shouldn't have any knowledge of requests.\n. @xiangli-cmu This works for now but I'm going to change it when I refactor the log to limit by bytes instead of messages. Also, when I do the log refactor, I'm going to store the log entries in memory as []byte variables so that we don't have to serialize for every peer. We'll just send the bytes. It'll also make limiting the message size easy.\n. @xiangli-cmu I agree that keeping messages small is a good thing but we don't always have control over message size. If a database is saving a record then that record could be 100b or it could be 100kb. 200 100kb messages is 20mb which is too big to be sending 4 peers for an AE.\nThe overhead per message could vary quite a bit. I could see adding a weight to commands to adjust the overhead of an individual command. But that's an optimization to do down the line if it becomes an issue.\nI won't add the bytes MTU right away. Let's see how the message MTU works for now.\n. @xiangli-cmu We may want to expand out the encoding benchmarks to see how it fares depending on AE size. Right now it's still encoding 2000 entries at a time.\n. @philips I don't have the exact numbers but I believe they were ~12MB/s for request encoding and ~6MB/s for request decoding.\n. @philips I referenced these benchmarks for gob and msgpack. They're not much faster than JSON. The biggest hurdle is having to use the reflection API which is very slow. I'm not a huge fan of binary protocols but it's definitely the fastest.\n. Talked with @xiangli-cmu over IM. He's going to try to swap out the binary encoding for protobufs to see if it's as fast/faster while being more maintainable in the long run.\n. @xiangli-cmu This looks good to me except for the minor changes I listed. We need to figure out a flexible strategy for snapshotting though. I'm going to be going through 200 entries in a fraction of a second so I'm going to constantly have followers needing snapshots. That's something we can worry about later though.\n. @xiangli-cmu If you want to write out the commitIndex then can you add it to a separate file? Maybe a commit file that lives in the same directory as the log. I see it as only a temporary fix as we really need to save the cluster configuration whenever it's updated. That should live inside Raft.\nAlso, I'm concerned that doing random access inside the log is going to kill performance on spinning disks. I'd rather keep the log append only. And if we're going to provide an option to save the commitIndex going forward then I'd rather it be isolated as its own file.\n. @xiangli-cmu Seek changes the pointer in memory but moving to a position in the file and writing causes the disk heads to have to move more (instead of simply appending to the end of a file). Also, it doubles our IOPS as we have to write two pages of data instead of just one. If you're running on a system like Amazon EC2 with network attached storage then IOPS can be really expensive.\nWe need to save the cluster configuration because that'll let the server know who to request votes from on startup. Otherwise you risk having a cluster of servers waiting to receive AEs.\nThe cluster configuration is going to have to live within Raft. It's basically just going to save a list of named nodes and connection strings (e.g. udp://mynode1). The application itself will have to determine how to parse the connection string.\n. @xiangli-cmu The configuration needs to be separate from the snapshot as we'll need to save the configuration immediately after it's committed.\n. @xiangli-cmu We're going to need to move basic configuration inside the library. The library needs to know too much about the configuration for it not to be in there.\nAs far as recovery, we shouldn't be recovering commands until we know that there's not a new snapshot. We should load up the configuration first and probably only recover the node becomes leader or it is sent an AE from a leader.\nWhile I am concerned about performance, I think that the bigger concern is that we need to move cluster configuration storage into the library and this is a stopgap solution so I'd prefer to keep it isolated as much as possible.\n. @xiangli-cmu Just some minor tweaks but otherwise looks good. If you can check-in the changes then I'll merge it.\n. @xiangli-cmu The first needs to be a self join since it'll need to be replicated to peers when other nodes join. If node A starts and adds B, then A fails and B becomes leader, but then B never knows about A since there wasn't a log entry.\n. @xiangli-cmu yes, that's correct.\n. @xiangli-cmu Looks good. Thanks!\n. @xiangli-cmu Can you change the LeaveCommand to be an interface with a DefaultLeaveCommand for consistency?\n. @xiangli-cmu This may need a mutex since the logLevel will be accessed and mutated in different goroutines. I'm going to get go-raft setup on Travis CI sometime soon and I'll be more concerned about it then. Not too concerned about it right now.\nJust fix the code mentioned in the line note above and go ahead and merge it.\n. @xiangli-cmu This won't work if you have more than one Raft server running in a process. I'm using a tiered Raft system so I actually have two Raft servers running in each process.\n. @xiangli-cmu Also, do you have benchmark numbers showing a performance improvement?\n. @xiangli-cmu I was looking for more of a Go-style benchmark that we could keep in the code so we could see how performance changes over time and so we can test whether changes like this make much of a difference. Moving the buffer to the Log is a messier implementation but I'm willing to do it if it means a significant performance boost. Can you convert this test to a Go-style benchmark?\n. @xiangli-cmu Can you send from a feature branch next time?\n. @matttproud Thanks for the PR. I've been meaning to set up Travis but I've been pushing it off until we move out some of the longer tests. If it's working for you, though, I'll go ahead and set it up on my account.\n. Looks like it's running now: https://travis-ci.org/benbjohnson/go-raft/builds/9779016\n. @philips It's just extraneous code. Response bodies need to be closed but request bodies don't.\n. @xiangli-cmu That makes sense to me. I could see how the channel would start to block after there's more than 256 events waiting.\n. @xiangli-cmu No time this weekend to look over this. Checking it out now. Do you know what the build error is from? https://travis-ci.org/benbjohnson/go-raft/builds/9796572\n. @traviscline Thanks for submitting the bug. The race detector was passing for a while but we must have gotten lazy. :)\nI'll fix the issues and change Travis CI to use the race detector on every build.\n. @erikstmartin I still want to address those races but I haven't had the time so far. If you want to send some PRs that'd be great. Otherwise I'll try to fix them in the near future.\n. :+1: \n. @philips Good call on the unnesting. Looks better. Thanks.\n. @philips Looks good.\n. @philips I think moving the library to a goraft organization is a good idea. It's grown up enough to have it's own org. Add me as an admin and I'll transfer it over.\n. @philips It's switched over: https://github.com/goraft/raft\n. @xiangli-cmu It seems like we could have the node continue to run but just prevent it from becoming a candidate or voting if it's not part of the cluster configuration. That way it can pick up changes when it re-joins. Would that work?\n. @xiangli-cmu We should prevent a node from promoting or voting if it's not a part of the cluster though. That's core Raft functionality.\n. @xiangli-cmu Just those two changes. Otherwise it looks good. Go ahead and merge after you fix those.\n. @xiangli-cmu Can you add a test case for this? After that we can merge in the changes.\n. @mattn What's the error on Windows? It actually looks like os.File.Sync() should be a cross-compatible way to sync the file. Although it also uses syscall.Fsync() internally which is generated on a per-OS basis.\n. Looks good to me.\n. @xiangli-cmu Sorry for the delay in looking at the PR. I'm trying to wrap up my Raft presentation that I'm giving at Strange Loop tomorrow morning.\nEverything looks good. I added one question to clarify. Also, why did you expose all the protobuf encoding/decoding API?\n. @xiangli-cmu Makes sense about the transport layer. Merging...\n. @xiangli-cmu By the way, here are my slides from Strange Loop today:\nhttps://speakerdeck.com/benbjohnson/raft-the-understandable-distributed-consensus-protocol\nThe video will be released in the next couple of months.\n. :+1: \n. @xiangli-cmu That'd be great. It's awesome how etcd has taken off lately.\n. @xiangli-cmu Thanks. I was adding the issue so I wouldn't forget about it. :)\n. @jvshahid Hmmm... the uncommitted logs shouldn't be getting lost just because leadership is lost. They should only get removed if another leader has a different log history. That's a bug that should be fixed.\nRemoving the timeout should make it hang until either a) the command is committed by that leader, or b) another leader causes the log entry to be removed.\nAs for retries, I think that should live on the client. If a command fails for some reason, the client is probably the only one with the knowledge to check the current state of the system and optional resubmit the command.\n. @grncdr Thanks! Good documentation is half the battle. :)\n. The leader doesn't try to re-elect if followers are down. It'll wait until another another node tries to become a candidate.\nAre you seeing the writes actually commit against the leader?\n. @xiangli-cmu Can you change this to use sort.Reverse() instead? I think making uint64Slice sort in reverse order is going to bite us down the road.\n. :+1: \n. After thorough testing, I approve this PR. :)\n. @dgryski Thanks! There's some internal clean up that needs to be done in goraft. Much appreciated. :)\n. @xiangli-cmu Sorry I didn't get to this sooner. Two points of feedback:\n1. I don't think we should expose setTerm() on an interface just for tests. Can you manually call s.(*server).mutex.Lock() in the tests instead and remove the setTerm() function? It's also strange because the initial-lowercase name will cause the function to not be exportable. I'm actually not sure how that would even work in Go.\n2. I'm still seeing a data race for event#returnValue. Can you wrap this in a getter/setter function with a lock? Here's the race condition report: https://gist.github.com/benbjohnson/763b7e55fa8f59244ba5\n. @philips I prefer that too actually. I started using the Zen Mode for editing right on GitHub and it's been nice but it does one long line. Also, I read that GFM adds line breaks for multiple lines but I guess .md is just straight markdown (ref).\n. @kellabyte Good point. The overhead is more related to message processing time and not necessarily latency. I'll make it more descriptive.\n. @ongardie Good point. I updated the README to be mainly around node failure tolerance. I wanted to keep it simple so I left out cost, latency, & election conflicts. Let me know what you think.\n. :+1: \n/cc: @bketelsen @erikstmartin\n. @jvshahid I just merged it: https://github.com/goraft/raft/pull/136\nLet me know if you have any other issues.\n. @xiangli-cmu Sorry for the delay in getting to this. Why is this in the Raft library? Sync isn't a part of the Raft protocol. Can you move this to etcd?\n. Sync is a strange concept in Raft since Raft is meant to deterministically change a state machine. A sync shouldn't be necessary in most applications. Even if there is a sync in another application, the details will probably be different.\nAdding it may not add CPU/network overhead but it complicates the library. Can you move it over to etcd and we'll integrate it back into Raft if we see a lot of people trying to do something similar? The etcd PeerServer already knows about Raft. I don't think it's too bad to have it manage the sync based on the leader state.\n. @xiangli-cmu I'm ok if you want to include some kind of callback from the Server for state changes. Maybe something like:\ntype StateChangeHandler interface {\n    HandleStateChange(*Server)\n}\nAnd then add a function to Server it:\ntype Server interface {\n    ...\n    SetStateChangeHandler(StateChangeHandler)\n}\nJust make sure it's called synchronously on state change. No channels or goroutines.\n. @densone I just added a fix to the fix-restart branch. Can you try it out?\nsh\n$ cd $GOPATH/src/github.com/goraft/raft\n$ git fetch\n$ git checkout fix-restart\n. Awesome. It seems like problem is that when the server restarts, it's trying to reuse the commit channel to notify clients of entry commits. If the channel has already been used then it'll default to that panic. Setting the commit to nil lets it meet the conditional that wraps the select and ensures that a commit notification only gets sent once.\n. Fixes #130.\n. @JensRantil Thanks for fixing that.\n. @JensRantil Some of the panics are for unrecoverable errors. We should use errors instead most of the time. The ones in command.go are there because they shouldn't fail but if they do then it'll typically happen at initialization. As Xiang said, feel free to send a PR. Otherwise we can try to tackle it in the near future.\n. @xiangli-cmu Why the change to time.Tick? Also, please let me take a look at PRs before you merge.\n. @xiangli-cmu What's the context for this change?\n. @xiangli-cmu The biggest thing that worries me is making Peer.sendAppendEntriesRequest() into an async call. If a peer is hanging for some reason and we're sending a large AE every 50ms then it's going to go downhill really fast.\n. @xiangli-cmu Otherwise it looks fine to me.\n. @klobucar Thanks! Good idea.\n. Thanks @Scooletz! :+1:\n. I'm merging this since it's low risk. Comments and change suggestions still welcome though.\n. @xiangli-cmu +1. let's make sure we test this change against etcd as well and get it merge in there soon.\n. @xiangli-cmu I thought about making it configurable but I didn't want to add any more methods to the already large Server API. I'm going to leave it for now and we can always add it in later if people need it.\n. @xiangli-cmu So what's the extra code the application needs to add in? Also, have you tested this code against etcd?\n. Works for me.\n. :+1: looks good to me. I'd be curious to hear what your use case is. Sounds cool.\nIs it a project we can list of the README?\n. I can honestly say I never thought go-raft would be used in a security competition. :)\nCan't wait!\n\n. @xiangli-cmu lgtm.\n. :+1: \n. @xiangli-cmu I don't think I quite understand the PR comment. Do you want to remove the Sync() on the log when the leader is committing a command?\n. @xiangli-cmu Technically I think it's supposed to fsync before it sends to the followers. Although you might be right. If the entry hasn't been committed by the leader then it's not externally visible so it might be ok if it gets lost. It'll be committed by the next leader.\nYou still need to fsync the followers after each append so this might just end up making the followers a lot slower.\nI'm ok with adding configurable options for fsync. Some people may want to trade off a little safety for a hefty performance boost. It should be strict by default though.\n. @xiangli-cmu lgtm. Can you clean up the writeFileSynced() and go ahead and merge.\n. @xiangli-cmu That looks good to me. Can we remove the Proto prefix from all the type names? It seems redundant since they're already in the protobuf package.\n. :+1: \n. @xiangli-cmu lgtm. How was it committing the entry for a single node before? It seems like the setCommitIndex() was only on AE req/resp.\n. @jvshahid thanks!\n:+1: :+1: :+1:\n. I also need to add the race detector to the CI but I'll get that in there after improving snapshot coverage.\n. :+1: \n. @philips @xiangli-cmu I removed Travis builds from Raft because they're constantly failing. My guess is that the servers are overloaded which causes pauses which fails the liveness tests. The Drone builds are running fine:\nhttps://drone.io/github.com/goraft/raft/8\n. Also, Coveralls coverage should update soon. Not sure why it takes so long to calculate the coverage:\nhttps://coveralls.io/builds/455791\n. @philips I agree that the naming is confusion and adding the lock will let us remove that. I wanted to limit the scope of this PR to adding test coverage so we could know if those changes will break anything. Test coverage got a +5% bump. woohoo!\n. @ptsolmyr Thanks for catching that. It looks like @xiangli-cmu got a fix in.\n. lgtm.\nhttps://drone.io/github.com/goraft/raft/11\n. @xiangli-cmu I think Travis is just slow. :)\n. @philips I'd rather not expose the channel directly. If we're interested in checking if the queue is backed up then I'd rather make an API for that. Maybe another threshold event for it?\n. @philips @mreiferson The idea of sendAsync() is to allow the heartbeat to continue even if the server gets backed up by something like a long disk write. That's why there's a buffer on Server.c.\nHowever, I agree that a buffered channel isn't the right approach. A better approach would be to eliminate the buffered channel and do processing of the peer AE responses through a goroutine.\nWhat do you think of that?\n. @mreiferson That's a good point -- a go-raft instance indefinitely hung on a disk write would hang the cluster. I think people will just need to bump up the heartbeat interval if they're running on something like EBS where 99% percentile latency can be higher than the heartbeat interval.\n. Sorry for the delay in responding. I've been out sick for a couple days. I'm :+1: and :ok_hand: (and any other related emoji) on removing the buffer.\nUnrelated: do you guys think it would make sense to maintain the heartbeat in the Server instead of in the Peer? We could kick off goroutines on regular intervals within the server event loop so we wouldn't have peers acting quite so non-deterministically. Just throwing that idea out there.\n. @xiangli-cmu It looks like the build is failing now:\nhttps://drone.io/github.com/goraft/raft/15\nCan you fix that?\n. :+1: Thanks @philips.\n. @philips I agree that \"interval\" makes more sense than \"timeout\". I feel like I used timeout originally because it was in the original Raft paper. I can't find a copy of the original anymore though.\nThe biggest issue that I have is that it breaks the API and therefore other user's integration. go-raft isn't at v1.0.0 yet so the API isn't set in stone but I'd like to hear from other users before merging.\n@bketelsen @erikstmartin @pauldix @jvshahid What do you guys think? Is it a problem for you if we make this aesthetic change?\n. Thanks everybody for chiming in.\n@philips What's the API change for the event channel?\n. :+1: \n. @mreiferson I think it makes sense to go through and clean up a lot of this. If we remove the buffered channel (#160) and move heartbeating initiation to the server then we could essentially have a completely single threaded implementation. That would make everything a lot easier to reason about and we could probably drop many of the locks.\nI agree with @xiangli-cmu that a lot of the quirkiness comes from the rewrite from the non-event loop design. But it's time to get it cleaned up now.\nI've also been thinking about splitting off the individual states into their own types and have the Server essentially be a wrapper that only holds one state at a time and just handles the handoff between states. It's essentially just a state machine. I haven't quite got the whole design figured out yet though (and I'm not entirely sure it's the best plan either). :)\n. lgtm. Did you test this against etcd?\n. @xiangli-cmu lgtm\n. @xiangli-cmu What static data?\n. @xiangli-cmu Why are you moving those to a separate package?\n. @xiangli-cmu They're all functionality related to the same thing. go-raft is meant to be a relatively small, self contained library. Splitting off into additional packages is overkill.\n. @xiangli-cmu I don't think there's any benefit to separating them off into their own package except for aesthetics. I'm fine if you want to consolidate some of the files in the root package:\nappend_entries_request.go + append_entries_response.go \u2192 append_entries.go\nEverything in the root package is related. It doesn't make sense to split it off.\n. @xiangli-cmu I'm not opposed to aesthetics but I am against making two packages for something that is in the same functional group. \n. @xiangli-cmu lgtm.\n. @philips :+1: \n. @chrislusf lgtm. If you can squash the commits into one then I can merge it. Also, Github doesn't notify us when there's a new commit on a PR so just add a comment if you make changes. Thanks!\n. @xiangli-cmu Is there something that requires this change?\n. @xiangli-cmu lgtm. The candidateLoop() seems simpler now. Thanks!\n. @xiangli-cmu Can you set up Drone to point to your local xiangli-cmu/raft repo so it'll do a build for these PRs?\n. @xiangli-cmu lgtm as long as the test suite is :green_apple:.\n. @xiangli-cmu Can you fix the spelling of \"updateCurrentTerm\" in the assert message. Otherwise lgtm.\n. @philips I haven't used chan chan before but it looks like a good way to do a req/resp. I'm good with it. lgtm.\n. :+1:\n. @xiangli-cmu I like that naming better. The previous naming always confused me.\n. @xiangli-cmu lgtm\n. Thanks for catching that. I added a fix to https://github.com/goraft/raft/pull/192. Look ok?\n. @xiangli-cmu lgtm\n. @xiangli-cmu Is this related to a bug? Does it pass the race detector? We need a lock around Server.peers since that can be read by the application.\n. @xiangli-cmu If you call Server.Peers() while updateCurrentTerm() is messing with the peers then you'll get a race error. We need the lock around there.\n. @xiangli-cmu Sure. Go for it. Check it against the etcd functional tests with -race set on the tests and on the build.\n. @xiangli-cmu Does the Init() function need to be exported? Are you going to call it directly or are you still calling Start()?\n. @unihorn Is there something you want to do differently with a New/Uninitialized state? The Server state machine is starting to get unwieldily and I don't want to add states for the sake of adding states.\n. @unihorn I'd like to stray from the Raft protocol as little as possible. I don't see the point in splitting off an additional state.\n. Fix the missing error check and then lgtm.\n. @unihorn Can you explain more why this is needed? Why isn't the commit index getting flushed on every update?\n. @xiangli-cmu Ok, I remember that conversation now. A manual flush is fine with me.\n. @xiangli-cmu What happens if the directory exists but it's not writable? I don't quite understand why we're only checking for one type of error with Mkdir().\n. @xiangli-cmu How is err != nil && os.IsNotExist(err) checking for all types except for existence? That should only be true where err == ErrNotExist. Basically, only when the directory doesn't exist.\nDid you mean err != nil && !os.IsNotExist(err)?\nhttp://golang.org/pkg/os/#IsNotExist\n. @xiangli-cmu The only time ErrNotExists will occur with Mkdir() is if the parent directory doesn't exist. In this case, the parent directory is s.path, which should exist. \nhttp://play.golang.org/p/Kezcz9swTX\n. No worries. You just need a bang for all errors besides \"not exists\": err != nil && !os.IsNotExist(err)\n. :+1: \n. @xiangli-cmu Where are you seeing the data races? From go test -race? Or from -race on etcd?\n. @tsenart Thanks for the PR. Looks good to me. I like good errors. :)\nCan you combine the error check from f.Write() into a single if/else block? e.g.:\ngo\nn, err := f.Write(data)\nif err == nil && n < len(data) {\n    return io.ErrShortWrite\n} else if err != nil {\n    return err\n}\n. @tsenart Well, I think chaining all of it is overkill. :) It's hard to see the Sync() embedded in there. Can you move the Sync() and error check back into its own if statement?\nAlso, the io.ErrShortWrite is now getting set but not returned. Instead of:\nerr = io.ErrShortWrite\ncan you do:\nreturn io.ErrShortWrite\n. @tsenart Thanks!\n. @unihorn Can you add error messages to the http.Error() calls? The WaitGroup makes me a little nervous since it could hang indefinitely on shutdown (or at least as long as an HTTP timeout). Although most stops are going to be because of a signal (SIGINT/SIGTERM) so I'm ok with this.\n. No CGO dependencies inside go-raft. Too many people depend on the library already. btrfs support seems like a niche feature to support.\nJust expose the log file's file descriptor and let the application support it.\n. @xiangli-cmu The JoinCommand and LeaveCommand interface don't actually work well as marker interfaces. They're currently both the same interface:\nhttps://github.com/goraft/raft/blob/047192edb174541d21f2f93f24e587b2f723889e/commands.go#L20\nNot sure if that bites you in this situation but it's something to be aware of.\n. lgtm\n. @watsonjiang There are not currently plans to make logging pluggable or change the logging infrastructure. Adding glog would add an extra dependency to go-raft that not everyone wants.\n. @otoolep @swsnider Can you guys confirm that this PR from @hanlz works and I'll merge it in.\nhttps://github.com/goraft/raft/pull/231\n. :+1: Thanks for the fix! :)\n. :+1: \n. @mre Yes, the original devs on the project have either built newer implementations or are not using the library anymore. @philips is adding a shield to the README to notify users that the library is unmaintained and new maintainers are welcome.\nhttps://github.com/goraft/raft/pull/241\n. @mre I'm working with InfluxDB on a high performance version of raft to stream all data through. After that's finished, InfluxDB won't have a go-raft dependency.\n. @philips I would suggest just adding an \"unmaintained\" shield and a call for maintainers at the top of the README.\n\n. lgtm, Brandon. Thanks for writing it up.\n. @chrislusf as @philips mentioned, this repo is currently unmaintained. You can submit a PR for the change or you can fork this repo and maintain changes in your fork.\n. This repo is no longer maintained and etcd uses a different raft implementation.\n. ",
    "cespare": "What's the status here? I see \"Log Compaction\" listed as a feature in the readme, and I see some log compaction stuff in the code...is this working?\n. @benbjohnson Thanks!\n. What's the status here? I see \"Log Compaction\" listed as a feature in the readme, and I see some log compaction stuff in the code...is this working?\n. @benbjohnson Thanks!\n. ",
    "ongardie": "You shouldn't actually need to persist anything for peers.\n. @tsenart , theoretically, sure: Raft will only elect a leader with probability approaching 1. The election timeout should be randomly chosen to prevent split votes with reasonable probability, then randomly re-chosen in case a split vote does occur. See http://ramcloud.stanford.edu/raft.pdf for details on the solution (comes up in the main text and again in the evaluation), and see the FLP impossibility result for why any consensus algorithm for asynchronous systems must have some kind of wart to deal with this.\n. To make sure everyone's clear, persisting commitIndex isn't needed for safety, since a new leader can always figure this out again with help from a quorum, and then tells everyone else through AppendEntries RPCs. I assume the question is when/whether it's beneficial to persist the commitIndex to disk. You can certainly do it, and as xiangli points out, you can do so asynchronously or periodically. But here are a couple reasons why you may not want to:\n- It's extra code.\n- If you're booting a server, hopefully it's in the minority of your cluster that's not needed for availability.\n- If you're booting a server, it's probably already experienced significant downtime. If you delay applying log commands even by a few minutes, it wouldn't necessarily add significantly more downtime.\n- If you need to read the log from a magnetic disk, you're lucky if you can get 100MB/s. Applying the commands should be much faster (a few hundred MB/s probably), so Amdahl's law says you can't expect much gain by overlapping the two. A good SSD might change this, though.\nSo I guess the most super-optimized implementations would do this, but I probably wouldn't bother.\n. Looks pretty good, but in cluster size, I don't think heartbeat chattiness is the biggest concern. Even if your heartbeats were 500 bytes, you had 101 servers, and your heartbeats went out every 50ms, that's still less than 1% of a gigabit link out from the leader.\nI think the main reason to make a cluster larger is to tolerate more server failures before a human has to get involved (your Concurrent Node Failures). For this reason, I can't imagine needing a cluster bigger than 9 servers, allowing 4 of them to fail independently before any are replaced.\nI think the main reasons to keep a cluster small are (a) cost, (b) latency/bandwidth for new entries coming in to the leader, since the leader has to replicate each one out to a majority of the cluster, and (c) if you really do run with a ton of servers, you're more likely that candidates will interfere with each other during elections, so you might need to increase your election timeouts (baseline and range).\n. shipit\n. I haven't thought much about this, but I think @xiangli-cmu is right that until the leader externalizes entries (by telling followers or clients that they're committed), it's not required to retain them. As far as Raft is concerned, it's ok to lose uncommitted entries.\nOTOH, I think @philips's comment takes it too far -- I don't think you can get rid of all fsyncs safely. If an entry /is/ committed, then you have to maintain the invariant that a majority of the servers stores the entry. Otherwise, there's no way to guarantee that they'll be available when it comes to electing a new leader.\n. If indeed the backoff is desirable, have you considered placing a limit on the timeout? The concern I have is that a server could be down for arbitrary amounts of time, sending its timeout through the roof. Then, when it came back, it'd be ignored for an unnecessary period of time.\n. That's because I moved the RAMCloud wiki from self-hosted to Atlassian OnDemand, which renumbered all of the attachments. The best URL to use is http://ramcloud.stanford.edu/raft.pdf or https://ramcloud.stanford.edu/raft.pdf for now.\n. I added (fixed, really) a redirect on ramcloud.stanford.edu to make the old URL work, but please update still.\n. You shouldn't actually need to persist anything for peers.\n. @tsenart , theoretically, sure: Raft will only elect a leader with probability approaching 1. The election timeout should be randomly chosen to prevent split votes with reasonable probability, then randomly re-chosen in case a split vote does occur. See http://ramcloud.stanford.edu/raft.pdf for details on the solution (comes up in the main text and again in the evaluation), and see the FLP impossibility result for why any consensus algorithm for asynchronous systems must have some kind of wart to deal with this.\n. To make sure everyone's clear, persisting commitIndex isn't needed for safety, since a new leader can always figure this out again with help from a quorum, and then tells everyone else through AppendEntries RPCs. I assume the question is when/whether it's beneficial to persist the commitIndex to disk. You can certainly do it, and as xiangli points out, you can do so asynchronously or periodically. But here are a couple reasons why you may not want to:\n- It's extra code.\n- If you're booting a server, hopefully it's in the minority of your cluster that's not needed for availability.\n- If you're booting a server, it's probably already experienced significant downtime. If you delay applying log commands even by a few minutes, it wouldn't necessarily add significantly more downtime.\n- If you need to read the log from a magnetic disk, you're lucky if you can get 100MB/s. Applying the commands should be much faster (a few hundred MB/s probably), so Amdahl's law says you can't expect much gain by overlapping the two. A good SSD might change this, though.\nSo I guess the most super-optimized implementations would do this, but I probably wouldn't bother.\n. Looks pretty good, but in cluster size, I don't think heartbeat chattiness is the biggest concern. Even if your heartbeats were 500 bytes, you had 101 servers, and your heartbeats went out every 50ms, that's still less than 1% of a gigabit link out from the leader.\nI think the main reason to make a cluster larger is to tolerate more server failures before a human has to get involved (your Concurrent Node Failures). For this reason, I can't imagine needing a cluster bigger than 9 servers, allowing 4 of them to fail independently before any are replaced.\nI think the main reasons to keep a cluster small are (a) cost, (b) latency/bandwidth for new entries coming in to the leader, since the leader has to replicate each one out to a majority of the cluster, and (c) if you really do run with a ton of servers, you're more likely that candidates will interfere with each other during elections, so you might need to increase your election timeouts (baseline and range).\n. shipit\n. I haven't thought much about this, but I think @xiangli-cmu is right that until the leader externalizes entries (by telling followers or clients that they're committed), it's not required to retain them. As far as Raft is concerned, it's ok to lose uncommitted entries.\nOTOH, I think @philips's comment takes it too far -- I don't think you can get rid of all fsyncs safely. If an entry /is/ committed, then you have to maintain the invariant that a majority of the servers stores the entry. Otherwise, there's no way to guarantee that they'll be available when it comes to electing a new leader.\n. If indeed the backoff is desirable, have you considered placing a limit on the timeout? The concern I have is that a server could be down for arbitrary amounts of time, sending its timeout through the roof. Then, when it came back, it'd be ignored for an unnecessary period of time.\n. That's because I moved the RAMCloud wiki from self-hosted to Atlassian OnDemand, which renumbered all of the attachments. The best URL to use is http://ramcloud.stanford.edu/raft.pdf or https://ramcloud.stanford.edu/raft.pdf for now.\n. I added (fixed, really) a redirect on ramcloud.stanford.edu to make the old URL work, but please update still.\n. ",
    "xiang90": "solved\n. Should this be closed?\n. Not yet. But I tested the raftd to see how it works.\nI will write unit test today.\nI just want you to take a look. \nI have not finished all the code yet. : )\n. Yes. This is the 3rd approach, which store the snapshot externally. \n. you may read section 5.3(page 8) of the paper. I have not finished it yet (need to handle cluster conf).\n. do you have time to merge it today? i think i may need to change the server.do interface.\ni think to apply the real command in the server.do func is better and should return the result of the command to the application.\n. OK. In the following days, I probably will work on the application level stuffs. I will find ways to test go-raft and read through all the codes again. \n. I am doing it. Hopefully, I will finish it before Friday.\n. Rewrite the Timer. \nhttps://github.com/xiangli-cmu/timer\nWhen start the timer, the execution will be blocked until (1) timeout (2) timer has been stopped (3) timer has been fired\nMake sure that no matter how start() and stop() interleaves, the timer will be stopped.\nIt is very important for our raft lib. If the leader step down, we want to stop all the timer.\nRewrite the heartbeat and election function use the new timer\nEach of these function will be create in a go routines, and the func() will be blocked at the timer()\nAfter the time returns (true for time out and fired, false for stopped), we do the right stuffs.\nI think this implementation will be more clear than the current one.\n. done!\n. Done. \nHow do you think about the new timer implementation? \n. I think that keep three state may be good for further debugging purpose. \nAnd we do not need to merge the two state now. \nThere is a minor issue about commit, which I am going to work on. \n[ a log entry may only be considered committed if the entry is stored on a majority of the servers;\n(here -->) in addition, at least one entry from the leader\u2019s current term must also be stored on a majority of the\nservers]\nAlso you can close the timer issue.\n. Done!\n. Yeah. Let me clean up the snapshot codes and think a way to decide how many previous log we should keep. \n. We keep the recent NumberOfLogEntriesAfterSnapshot entries to alleviate this problem\n. I add a committedIndex() func in server.go\nAs we will pass the server pointer to the command, the application can get the current index and then return to the client.\n. For 1, probably we can do them in one select.\nFor 2 and 3, I will do it when I am going to write the codes.\nWhat do you think about this structure?\n. I will close this and send another one.\n. Probably, we need to write the conf to the disk in a separate file.\nCan you do that when you are going to clean up the API?\nThanks.\n. This can be solved by application. Add leader to itself.\n. This is caused by not discovering new leader.\n. We changed the server to a \"single\" threaded processes. Locking is not a big problem for now.\n. Try to use rwlock to separate write and read\n. make sure two thing: 1. try not to lock disk operation 2. try not to lock blocking channel sending\nOthers are ok for now. Lock will not be the bottleneck of the raft algo.\n. The server cannot be single threaded since you need to send votes and send heartbeat.\n. Yeah. Try to make as sequential as you can.\n. There is a big bug in server.go \nI have fixed it and update the pull request\n. I am going to read through all the code now.\n. Do you change the AER so that when the candidate discover a new leader it will also step down? You can see my last pull request\n. I re-write the setCurrentTerm func in the pull request.\nYou can read it.\n. Thanks for your refactoring and it is a great help. I was going to do that, but I think use lock to make the request reach the server in sequence is find for now.\n. The only thing I am concerning now is discover new leader in AE when the server is a candidate and the timeout.\nBut you can leave it for now and merge the pull request. \n. No problem. I will do it after a while.\n. Seems there is a problem with election after our new refactoring.\nI am trying to look into it.\n. Could you open the debug level and change multi-node test to loop for 2000000 times.\nAfter you run it for a while, some node will get stuck.\n. My previous version can run multi-node test for many hours until it eat up all my memory. You can do the multi-node test as a base. And try to do other test when the current version will not panic or stuck at that test.\n. I am also going to look into the problem\n. That is why I wrote the timer. That timer can make sure when you stop it, it will never restart or get stuck.\nThere is still some other issues, I am looking into them.\n. The problem here is the heartbeat func has returned, but the leader still try to send the stop signal to it. That will cause the leader blocking forever.\nThe problem you address can be solved by creating new channel or try to receive the previous signal before reuse it.\n. Sorry. I am afraid that you just merge it, since I think it is quite important. \nI will refactor that. \n. I am writing a application. For me, I would like to control if the server start as a follower or a leader.\nThe new node will have a cluster address and it will issue the join command to the cluster first then start as a follower.\nWhen having the log file, the node seems should start as a follower. If no one in the cluster send the node AppendEntriesRequest, it will promote itself automatically.\n. Do you add it for testing purpose? \nI implemented my transportation layer with http and https.\n. Look good to me.\n. Json package in GO is slow. The speed of it is comparable with network speed. 100kb of our data will require 100ms to encode and decode, which means we have to limit package size within 100kb when setting election timeout at 150ms.\n. @peterbourgon Do you mind to unify the API of your implementation and ours? Then we can use the same test case to do the test. \nMaybe we can come together one day and design that.\n. @peterbourgon I have built a key-value store on top of raft. So I am able to test it via my system. You can check at https://github.com/coreos/etcd\n. @benbjohnson Can you create a IRC channel for go-raft?\n. Yes. Sorry.\n. I just noticed the problem when I was doing tests. The speed is comparable with network speed, thus I think we should improve. Suppose that we set the election timeout to 150ms. If any node miss more than 5000 entries, it will never catch up with the faster machines (if client keep on issuing new commands).\nZookeeper can process 10K write operation/Second. \nFor current json implementation, it will need more than 400ms to decode and encode 10K operations.\nThe leader even needs to do more work (it need to encode multiple times).\n. fixed.\n. I wrote this in a hurry.\nThe main idea is when the server becomes leader, it will send out a NOP command.\nThe sendAppendEntriesRequest() in peer.go will check if the peer successfully append a log entry from the current term. If it is, it will set the append to true.\nThe server will check if most of the peers have been sycned by the current term log entry. If it is, it will begin to commit commands.\n. I will do all the changes tomorrow morning.\n. Yes. I need to add it each steps. I am just adding it to the go-raft and trying it a little bit.\nSo let us use the built-in one.\n. yea. thanks.\n. @benbjohnson \nNot for testing. Brand New nodes should not promote it self that until they get synced with the cluster once.\n. Yes we could, I just want to let the application have more option.\nBut I can change it.\n. @benbjohnson How about add a func() called startSyncedFollower(), which the follower will not to be synced once before promote?\n. The raft paper does not say it explicitly. But it is meaningless to let a follower promote itself if it is certain that it cannot be elected. \n. Also it is important if we decide to use AE to get the commitIndex. Reply the log requires time. We should not let a follower to promote only because it is replying log that cannot receive heartbeat. \n. No problem. I was testing my scripts. It cannot run without this limitation. Thus I did a quick fix. :)\n. @benbjohnson We need to think carefully to limit it by bytes or messages. \nThe process time is decided by networking + encoding/decoding + disk operation + state machines apply command.\nCurrently, we encoding/decoding every message and do disk operation pre message and apply each message. Also each message should not be very large (decide by the nature of distributed algo). \nThe number of message can decide the time of processing.\nWhen we can write a bunch of command to disk at one time and reduce encoding/decoding time for each command, it is meaningful to change it to limit by bytes.\n. wow. milestone. \n. um... seems that we have a getEntry func, but has some issue.\nI will fix that\n. @philips  https://github.com/benbjohnson/go-raft/pull/66\n. @philips \nFor the leader it needs to encoding and decoding multiple times for different peers in one round of flushing. \nThe encoding speed will be even slower for the leader, thus given the speed of json I think we should change to a faster encoding method.\n. BenchmarkAppendEntriesRequestEncoding       1000       1830835 ns/op      49.17 MB/s\nBenchmarkAppendEntriesRequestDecoding       1000       2187454 ns/op      41.15 MB/s\nPrevious binary:\nBenchmarkAppendEntriesRequestEncoding       2000       1391891 ns/op      96.31 MB/s\nBenchmarkAppendEntriesRequestDecoding       1000       1818073 ns/op      73.73 MB/s\nSince protobuf compress the data, we should compare the time/op.\nProtobuf is slightly slower than binary, but I think it is acceptable.\n. Also in face, due to the compress we can get better network performance.\n. Add a spell checking plug-in into my editor. \n:)\n. I am thinking to let client application to trigger the snapshot process, since it should know the state machine better than the library.\n. @benbjohnson \nCould you double check L220 in log.go? \nI do not think we need this when recovery. Is that right?\nThanks\n. hrm... we can separate the file. I think the seek func only change the pointer value in mem. \nWhat is useful is to write entries in one disk operation rather than a lot. I am going to change that.\nWhy you want to save the configuration?\nI have save that in the snapshot.\n. Store commitIndex at the head of the log file. When join or remove a peer, we update the commitIndex.\nWe need to keep that since when we restart we must recover the cluster configuration.\nI do not keep the configuration in a separate file. Since recording the commitIndex will actually help us to recover some commands that we will certain recover too.\nThat is why i choose to save the commitindex rather than conf.\n. We only seek to the head when join/remove. \nIf we keep it in another file, we still need to seek to the position.\n. \"we shouldn't be recovering commands until we know that there's not a new snapshot.\"\nI agree with you at this point, thus we should store the configuration.\nI will refactor the codes later.\n. @benbjohnson\n1. move configuration to conf file\n2. delete position in log_entry\n3. recover/create conf file when init\n4. append conf file when add peer\n5. rewrite conf file when delete peer\n. A quick fix. Could you please help me check if there is any logic/func error?  I will fix it later.\n. Done.\n. Only allow the first log entry to be a self-join.\nThe first log entry should be a join, but not necessarily a self-join i think. \n. A is the first node. It sends join-A, which is a self-join. Then becomes leader. \nA 's log entries will be join-A and nop.\nB tries to join the cluster of A. It sends join-B. Then A will commit join-B.\nA 's log entries will be join-A , nop and join-B.\nThen A send B appendEntries request. B will accept them. \nB 's log entries also will be join-A , nop and join-B.\nCorrect me, if I do not understand it right.\n. @philips  Yes. \nThis function will be called each time raft write an entry to disk. \nThus, it will be called very frequent. \nIt will be called by only one thread. \nThus, we do not need to have a local buffer to avoid race. \nWe create a global var for reuse to reduce GC time.\n. @benbjohnson \nOk. I will do\n1. Move the two global vars to a raft server unique struct\n2. Try benchmark\n. @benbjohnson\nDid this solve your problem?\nI embed a simple speed test into httptest. I just want you to try it, then you can remove it.\n. @benbjohnson  yes. sorry\n. @benbjohnson \nBenchmarkSpeed for before and after\neach op is for 20,000 nop commands\nI did it 5 (each benchmark also include 5 times, total 25 times) times and got the average time.\nBefore:\nBenchmarkSpeed         5     873075029 ns/op\nAfter\nBenchmarkSpeed         5     782130436 ns/op\nAbout 12% improvement.\n. BenchmarkAppendEntriesRequestEncoding       1000       1862026 ns/op      48.34 MB/s\nBenchmarkAppendEntriesRequestDecoding       1000       2304243 ns/op      39.07 MB/s\nBenchmarkAppendEntriesResponseEncoding   1000000          1456 ns/op       5.49 MB/s\nBenchmarkAppendEntriesResponseDecoding    500000          3287 ns/op       2.43 MB/s\nBenchmarkSpeed         5     424838309 ns/op\nmy result.\n. @matttproud  Thanks for the pr. I merged and fixed it. \nCould you please make sure you can successfully  go install/build and go test before you send the pr next?\nThanks.\n. @matttproud  the problem is that we change the default test heartbeat timeout in the previous pr, but have not changed it in the test. \n. @matttproud  I will leave this pr for Ben. He talked to me he was going to add Travis.\n. @matttproud  I fixed all the tests. If you have time, you can try it again on your environment. You can try bench also. But the last benchmark may sometimes fail due to leader change.\n. @benbjohnson Can you double check if this is correct? I got a deadlock when I try to stress the server. Now it works fine for me.\nThanks.\n. @benbjohnson\nI test snapshot with etcd. And now it \"works\".\nI am going to write snapshot tests for raft very soon,\n. @benbjohnson  yes. It seems like that testing server is slow. We have a timer test, then it fails.\n. @benbjohnson @traviscline \n Most the race is caused by debugln, and some other minor issues. We will fix it as thing is getting stable.\n. @benbjohnson I will take care about this very soon.\n. @Mistobaan  Solid simplification. LGTM. \n. @philips lgtm.\n. @benbjohnson  I am trying to make it work in application layer. I dont want raft be complicated either. :)\nLet us see what happens.\n. @benbjohnson I will think about this more tomorrow.\n. @philips In etcd we have internal store system, so we do not need anything store here. But for other application, like connection url maybe need to store here.\n. @benbjohnson do not merge it now. I will write test cases. \n. @benbjohnson Add connectiongString.  I change the API of Join Command to Name, ConnectionString. Thus the join command in fact has already included the connectionString information. We do not need to record peer info in the conf file. But it seems not bad effect also. `\n. @mattn Is this the only different syscall between windows and POSIX in golang? If there will be more, we probably need to have a better filename.\n. lgtm\n. I made the following changes:\nFirst: Raft server need to record its own connection string. And when doing snapshot, it need to add itself to the peer list also. Since the snapshot may send to other machine, and these machine need to figure out cluster configuration from the snapshot. \nSecond: Since we record the conf and snapshot, they all include the commit index information. When restart we need to pick up the larger one to be the true commit index.\n. @benbjohnson Since I think some people do need to implement their own transportation layer.\n. @benbjohnson Nice slides! Looking forward to the video!\n. @benbjohnson lgtm.\n. @benbjohnson In transportation layer, we still need to deal with http timeout. Or there will be problem when network is partitioned. I am working on that in etcd's transportation.  After that I will send pr to update the default one in go-raft.\n. @benbjohnson Actually, raft use a  false negative approach about the command commits. But I agree we should make this a application decision. I can make the change.\n. @benbjohnson I am going to work on this. I will remove the timeout related stuff and let the application make its own decision.\n. @benbjohnson BTW, can you review https://github.com/goraft/raft/pull/136/files? The only actual changes is making asnycSend a real one.\n. @grncdr It is also very convenient to accept your pr on github. So send as many as you can. :)\n. @benbjohnson  @kellabyte  There seems to be a problem. I will fix it soon.\n. @benbjohnson @kellabyte   We need to sort the commitIndex of peers in descending order. Then we can pick the first majority one as the current CommitedIndex of the whole cluster.\nI will send a pr.\n. @benbjohnson  will do that.\n. @dgryski  lgtm. Thanks a lot.\n. @benbjohnson This should fix #100.\n. @benbjohnson Maybe we can merge this? \n. @benbjohnson For the second, I cannot understand it. I think it is already synced by the channel... \nI will merge this. \n. @ongardie For the cluster size, in my reply I was comparing 8 and 9 rather than 5 and 9. \nAlso, if having a lot of nodes, we probably need to cache the serialization result, which we have not done yet.\n. @jvshahid This will not totally solve the problem. At the end of leader loop, we stop the peer heartbeat, but we cannot guarantee that we actually stop the flushing process. The peer might be already in the flushing process and will stop when it finish the process. If you want to do gracefully shutdown, probably you need to add a two way handshake among the server and followers. \n. @jvshahid No. It is because the peer may be inside the flush() function. It will stop after that function returns and go through the select again.\n. @jvshahid  I think I made a comment about why we use a buffered channel. \n. @jvshahid Oh. https://github.com/goraft/raft/blob/master/peer.go#L90 Here it is. I need to spend some time to refresh my mind about this. I am sure we have a problem here before. I will see if it is still case now.\n. @jvshahid I think the problem is the p.send if I remember correctly. Actually I have done a performance improvement of raft and remove the extra blocking stuff. I have not got a time to clean it up and push it to the current master. Probably I can wait for me. Or you can try to play it a little bit. :)\n. @jvshahid by the way, are you using raft in your project?\n. @jvshahid Cool. I will try to fix it during the weekend. \n. @jvshahid That should work. But in fact I have cleaned all these several days ago, but have not got a time to send a pull request. Please allow me a few days to clean my branch and try to keep your commit. \n. @benbjohnson The http test fails randomly. Can you take a look? \n. @jvshahid Can you send a pull request to add your db project to the projects list of goraft? Thanks.\n. @jvshahid Yes.\n. @jvshahid do you have a gtalk that i can contact? \n. @jvshahid I fixed this in another pr https://github.com/goraft/raft/commit/59cb7259c23e0bce91d0cab4fd229fff00555996\n. @benbjohnson I want to add this into raft library for the following reason:\n1. I can see other application want to have a time sync mechanism.\n2. I do not want to etcd know much about raft internal. \n. @benbjohnson Also this add no overhead to raft, if people do not want to use sync.\n. @benbjohnson OK I will have a try. Currently I can see we need to add a mechanism to push the status change to etcd in raft. I do not want the sync based on a polling model, since most of the nodes will be followers. \n. @benbjohnson I will use a polling model first. It all the ttl approach works well, i will make the change at raft side.\n. @davew2 Thanks. \n. @JensRantil I think we use panic as assert. Feel free to send pr to clean up.\n. @JensRantil Raft is used to maintain the state of several distributed stateMachine to be consistent. If the command is stateless and directly apply to filesystem or sth, you can just use nil. \n. Time.after will create an object every time you call it, which cause extra malloc and gc. \n. And we might want heartbeat every HEARTBEAT, rather than HEARTBEAT add process time.\n. @benbjohnson \n1. Gracefully stop the server\n2. Avoid unnecessary gc time\n3. Clarify debug info \nI have done this sometime ago while I was testing perf of raft.\n. @benbjohnson Right. I need to fix this. \n. @benbjohnson Hmm. I look at the codes again. \nPeer.sendAppendEntriesRequest() is actually not an async call. \nI just made sending response to server an async call. It is an internally call and will not cause the problem you were worry about.\n. @baruch fsnyc on snapshot is ok. But it will slow down the speed when applied on the log entry. When a log entry is committed, it should be persistent on the page cache of the majority of nodes. The only problem is that if all of theses machines die the committed log entries will be lost.\nActually I have not began to measure performance and its affect, so I cannot make a decision. \nI will review it all later, but want to give a fast response to thank you.\n. @baruch We are fixing the issue in another pr #150 \n. @benbjohnson This is reasonable. LGTM.\n. @benbjohnson I think the precent should be configurable. Besides, lgtm. \n. @benbjohnson Without timeout application will hang at a command, it the current leader cannot reach a majority of peers. Application needs to deal with this if it does not want to be blocked. \nI think a better approach is creating a new kind of event. And if the leader cannot reach the majority of peers for a given period of time, the leader will notify all the listeners.\n. @jbcrail Thanks.\n. @baruch @benbjohnson \nJust a spelling fix. Merged. \n. @benbjohnson \n1. The util.go and using rename to replace the log is cherry-picked from @baruch's pull request.\n2. Actually we only do a fsync after the follower append its entries. We do not fsync when the leader append its entries. I do not think the leader needs to do that. I think the leader only needs to do this before commit entries, so it can fsync entries in a batch. \n. @benbjohnson \nIt should be safe since we only need to make sure at the committing point, the log entries are safely stored on the majority of the nodes in the cluster. \nThe thing is that \n1. We send entries in a batch to follower. So when there are high traffic, the follower will fsync appendEntires to disk in a batch. Will not hurt performance too much.\n2. If we fsync before it sends to follower, we still are going to fsync multiple times. \nMy hope is to do heartbeat in a single go-routine , and broadcast in multiple go-routines. \nIn that way, we can \n1. Simplify the current approach.\n2. Find a way to reuse the serialized results, since most followers should be at the same pace.\n3. We can do fsync for leader each heartbeat.\n. @philips \nAs @ongardie suggests, if we disable fsync, then we are totally rely on the page cache of OS. Both power failure on leader and the the whole cluster might cause us to lose committed data. \n. @benbjohnson Will do.\n. @benbjohnson There are still some cleaning up stuff to work on. But you can review this pr to get a rough idea what I am doing.\n. @benbjohnson Will do.\n. @philips No. I did not change any format.\n. @benbjohnson Can we merge this? I have some more cleaning up work to do. But it is better to do in another pull request I think. Thanks.\n. @philips This is not a optimization just for one node cluster. One node is just a special case, which requires more work (In the if statement). \nThe gain is 1/n of command process time for a n nodes cluster. \n. @benbjohnson We send a ae resp to itself before when it is a single node. So I remove the ae resp send and add a if statement to achieve the same result.\n. @bcwaldon lgtm \n. @ptsolmyr Are you trying to annotate go-raft in Chinese? I am Chinese. So you can contact me if you need any information via emial or QQ.  I am very happy to help.\n. @benbjohnson Drone is fast...\n. @mreiferson Our raft server is serving in one for loop and collecting events via a buffered channel from\n1. HTTP triggered go-routines.\n2. Peer go-routines when the server is a leader.\nNo matter if the channel is buffered or not, the HTTP triggered go-routines cannot process until the server finish processes its request. Buffer will not help to reduce the latency here I think.\nThe reason to have a buffered is that our peer go-routine does not require the response from server. So we do not block it from doing another round of heartbeat. \nI can see we are able to remove the buffer completely after some refactoring.\n/cc @philips @benbjohnson \n. @mreiferson It is not feasible to make heartbeat strictly synchronous. We cannot afford let the server idle waiting for the reply from a peer. \n. @mreiferson I am trying to make heartbeat totally isolation for the state of the server. The responsibility of the heartbeat routine should be just sending out read-only data to peers and get back reply to the server routine via channel. It should not change any state of other stuff directly. \n. @mreiferson Cool. Thanks.\n. @nemothekid You can use the event.Value and event.PrevValut instead of calling the server.X to inspect the changed variables for now.\nYour comment are reasonable. We can work around it.\n. @benbjohnson sure. \n. @philips I think interval is better.\n. @benbjohnson @philips \nHere is my simple bench tool:\nhttps://github.com/xiangli-cmu/raft-bench\n. @philips The root problem is that the heartbeat can still be running after the server steps-down to a follower/candidate and increases its term. The best approach is to stop all the heartbeat before increase its term. \nI do not do it now since there are some more changes I want to make together. \nWe have separate go-routine for each peer's heartbeat. I want to avoid theses go-routines and try to make most of the peer synchronized. Thus, we might find a way to reuse the encoding results, which reduce the CPU usage by a factor of number of followers. And this can also make the codes simpler. \n. @philips More docs in the commit message.\n. @zenazn Fixing\n. @zenazn \nI think dispatcher is holding its own lock.\n@benbjohnson \nAll the other read locks are holding for debugging, shall we just drop them? We added theses locks to let go -race happy.\n. @mreiferson The locks come from our old codebase, when the server is not in a event loop. We largely refactor the codebase to make server in a event loop. I am going to clean up all locks soon. \n. @benbjohnson  I have finished making things in a single thread. I will send you a pull request.\nBut before this, I think I need to send you guys a design doc to clear all these sort of thing. \n. @zenazn @mathiasbynens I think this is fixed by https://github.com/goraft/raft/pull/167\nWe stop all the heartbeats go-routines before leader steps-down and truncates its log.\n. @zenazn Yes. Go-raft needs some cleaning up work. \n. @zenazn One more thing, how did you find out the problem? Are you trying to audit the codes against the raft paper?\n. @zenazn CTF is interesting! I found this bug several days ago when I was doing a benchmark. \n. @philips If a peer routine is inside https://github.com/goraft/raft/blob/master/peer.go#L146, it will try to grab the server lock at https://github.com/goraft/raft/blob/master/peer.go#L193.\nWhen the server routine wants to stop it, the server grabs the server lock first. If this is the case, the sever cannot stop this peer routine. Since the server is holding the server lock waiting for the peer routine to finish, but the peer routine is waiting for the server lock. Neither of them can make a progress.\n. @philips I am cleaning all static data into a data package. So I am not going to merge this now.\n. @benbjohnson Snapshot, Request and Reply. \n. @benbjohnson Personally, I think there are too many files in the first level folder. And some of the files do no depend on each other. So it is cleaner to make them into separate pkgs. \n. @benbjohnson @philips \nHow do you think about this. After this, I also want to move log into a separate pkg if possible. \n. @mreiferson Not good at naming. But it will also include reply_types in this pkg. \n. @mreiferson How about message? \n. @benbjohnson I can combine some related files into one file, although I still want to separating them into a pkg since I am aesthetic. \n. @benbjohnson They are in the same functional group so they are all in raft package. I can separate them into different package since message related codes do not depend on raft server codes.  If they are very related, it is not even feasible to separate them, or there will be circular dependencies. \nAnyway, I can understand that you want to keep raft coherent. So I close this pull request.\n. @benbjohnson @philips \nI have a simple design picture for this @ https://github.com/xiangli-cmu/goraft-doc/blob/master/raft.png.\n. @benbjohnson @philips Closing this one. Working on a clean solution.\n. @benbjohnson \n1. Remove unnecessary nested loop in candidate loop. \n2. Move processVoteResponse to a function.\n. /cc @philips @unihorn\n. @benbjohnson \nfor change No.1. I do not like meaningless codes\nfor change No.2. I want to write unit tests for every process function.\n. @benbjohnson @philips Anything I can do to get this merged? \n. @benbjohnson I have set up Drone for my branch:\nhttps://drone.io/github.com/xiangli-cmu/raft/3\nI will clean up this pull request soon.\n. @benbjohnson Add a _assert function.\n. @benbjohnson closed this since \n. @philips see #183 \n. @philips This section of effective go is good: http://golang.org/doc/effective_go.html#chan_of_chan\n. @philips @benbjohnson I will separate this into two pull request. In this pull request, I change the code path a little bit along with the naming stuff.\n. @unihorn Can you provide more details? In which case, there will be two goroutine calling this function?\n. @unihorn This is a awesome bug catch, but I have to close this. This pull request cannot totally solve the problem.\n I have found the root cause of the problem.\nHere is why there is a deadlock:\nWhen the leader call removePeer it is holding the log lock, since it entry the removePeer via setCommitIndex. The leader will send a stop signal and wait for receiving. \nIf the peer is actually in function: flush(), it is also need to acquire the log lock at func p.server.log.getEntriesAfter.\nSo a deadlock happens. \n. @unihorn Can you create an issue for this problem. So we will remember to solve it.\n. closed via #195 \n. separate https://github.com/goraft/raft/pull/184 into two commits. This is the first one.\n. @philips Refined the comment.\n. lgtm\n. @macb \n1. We can do back-off probing with limited growth (seconds)\n2. We can do actively requesting if one node loss connection with the leader and is approaching election timeout or just restart. \nWe are re-writing the heartbeat function. I think we can just leave this pull request here for now.\n. @unihorn Can you test this against etcd? Thanks.\n. @benbjohnson Our addPeer and removePeer are not safe if the leader dies before all the nodes commit the command.\nBut I am not worrying about this right now. \n. @unihorn Fix the race in remove test?\n. @philips I was trying to refactor the heartbeat and log lock. But the result was not that great. I am planning to do it more carefully next week. \n. @swinghu Hm... I cannot understand this pull request. Did you send pull request to the wrong repo?\n. /cc @benbjohnson @unihorn\n. @benbjohnson \nIt is a cleanup. I tried test -race and these changes did not introduce more races and not fix any either.  \nI think the peer lock is not related to this pull request. This pull request does not change that behavior. \n. can i merge this one? i will try to fix data race after this.\n. @benbjohnson We need to add this to fix etcd bootstrap problems. We want to be able to do some bootstrap work (send join requests, test the liveness of the previous member) based on its previous state before actually start raft server.\n/cc @unihorn\n. @benbjohnson I want to call Init first. Then raft will load all the existing logs. So we are able to examine the state of the statemachine before start the raft server.\n. I do not quite follow your comments. Can you try to elaborate that?\n. @unihorn Reasonable. \n. /cc @benbjohnson @unihorn  updated.\n. Talked about this over phone. closed this one. \n. lgtm.\n. @unihorn Oh. You need to add a function into interface also. \n. @benbjohnson We can update the commitIndex to disk every update. We have talked about this some months ago and you decided not to do that due to disk operation overhead. \n. /cc @benbjohnson https://drone.io/github.com/xiangli-cmu/raft/24\nIntroduced a bug in last pr :(\n. we are checking for all types except previous existence. pervious existence is not an error we should report.\n. @benbjohnson \nIsNotExist returns a boolean indicating whether the error is known to report that a file or directory does not exist. It is satisfied by ErrNotExist as well as some syscall errors.\nIt says \"It is satisfied by ErrNotExist as well as some syscall errors\".\nDid I misunderstand this sentence? \n. hm... i misunderstood this error sorry\n@xiangli-cmu https://github.com/xiangli-cmu The only time\nErrNotExistswill occur with\nMkdir() is if the parent directory doesn't exist. In this case, the parent\ndirectory is s.path, which should exist.\nhttp://play.golang.org/p/Kezcz9swTX\n\nReply to this email directly or view it on\nGitHubhttps://github.com/goraft/raft/pull/203#issuecomment-38364784\n.\n. /cc @unihorn @philips \n. @benbjohnson go test -race. \nI will test it against etcd and fix the race found there in another pull request.\n. @unihorn Can you test this changes on etcd? Thanks.\n. @unihorn Hm... I suspect that the log does not replicate to node 3 successfully. So the command cannot be committed in time, which cause the timeout. \nBut I need a deep analysis to see what happens. \n. @jvshahid Thanks for reporting this. It is a bug need to be fixed. \n. @jvshahid It will be great if you can test this patch. Thanks!\n. it is a quick fix. to test the snapshot we need to have a testing state\nmachine. i can do that later.\nOn Mar 25, 2014 12:21 PM, \"John Shahid\" notifications@github.com wrote:\n\nI think you should add a test to make sure the bug is fixed and to protect\nfrom future regressions. Looking at the patch, looks like it will solve the\nproblem, but again, i think a test case is important even if the bug is\nfixed.\n\nReply to this email directly or view it on GitHubhttps://github.com/goraft/raft/pull/208#issuecomment-38586229\n.\n. @jvshahid That will not work. https://github.com/goraft/raft/blob/master/server.go#L1124\nI can work around it of course. But I do not like to do that sort of heck. \n\nWe can test the snapshot and recovery process within raft if we add a testing state machine. \n. @jvshahid I was not aware that @benbjohnson have made a mockStatemachine before. I added a test for the regression. \n. @unihorn Can you take a look at this and test it against etcd? Thanks!\n. @philips I think I messed up the commit message. will fix.\n. @philips fixed\n. There are a lot of duplicate codes. But I cannot think of a better way to do it either. \nThe transportation layer should set a timeout for both read and write request. So stop won't be blocked for a long time. \n. rebase is needed. \n. @unihorn Can you setup drone on your local branch? Like https://drone.io/github.com/xiangli-cmu/raft? \n. @unihorn Thanks.\n. the bug is introduced here https://github.com/goraft/raft/commit/01e8793c9a031a13f8fcc67784545d76d96e94f9#diff-34c6b408d72845d076d47126c29948d1R906\n. @unihorn We can do that, but I would rather make that happen in another refactor pull request. \n. @unihorn Updated. How about this?\n. @unihorn Only candidate should step-down to follower. Both follower and  snapshotting should be as same as it is.\n. @unihorn I agree with @benbjohnson . I hope this can be done in etcd. We should expose API in go-raft to support that.\n. @philips @unihorn I will add a API to expose the fd in go-raft. \n. /cc @unihorn\n. @unihorn No it is not. See the comments at https://github.com/xiangli-cmu/raft/blob/fix_channel_race/server.go#L445\n. /cc @philips \n. @benbjohnson @philips I merge this since this one has no risk.\n. /cc @unihorn I further clarify the problem for you in the commit message. \n. /cc @unihorn\n. we need to add a test for this in goraft.\n. @benbjohnson Yes. I know that. This will just invoke a committed index re-calculation. So it will not hurt us.\n. lgtm\n. solved\n. Should this be closed?\n. Not yet. But I tested the raftd to see how it works.\nI will write unit test today.\nI just want you to take a look. \nI have not finished all the code yet. : )\n. Yes. This is the 3rd approach, which store the snapshot externally. \n. you may read section 5.3(page 8) of the paper. I have not finished it yet (need to handle cluster conf).\n. do you have time to merge it today? i think i may need to change the server.do interface.\ni think to apply the real command in the server.do func is better and should return the result of the command to the application.\n. OK. In the following days, I probably will work on the application level stuffs. I will find ways to test go-raft and read through all the codes again. \n. I am doing it. Hopefully, I will finish it before Friday.\n. Rewrite the Timer. \nhttps://github.com/xiangli-cmu/timer\nWhen start the timer, the execution will be blocked until (1) timeout (2) timer has been stopped (3) timer has been fired\nMake sure that no matter how start() and stop() interleaves, the timer will be stopped.\nIt is very important for our raft lib. If the leader step down, we want to stop all the timer.\nRewrite the heartbeat and election function use the new timer\nEach of these function will be create in a go routines, and the func() will be blocked at the timer()\nAfter the time returns (true for time out and fired, false for stopped), we do the right stuffs.\nI think this implementation will be more clear than the current one.\n. done!\n. Done. \nHow do you think about the new timer implementation? \n. I think that keep three state may be good for further debugging purpose. \nAnd we do not need to merge the two state now. \nThere is a minor issue about commit, which I am going to work on. \n[ a log entry may only be considered committed if the entry is stored on a majority of the servers;\n(here -->) in addition, at least one entry from the leader\u2019s current term must also be stored on a majority of the\nservers]\nAlso you can close the timer issue.\n. Done!\n. Yeah. Let me clean up the snapshot codes and think a way to decide how many previous log we should keep. \n. We keep the recent NumberOfLogEntriesAfterSnapshot entries to alleviate this problem\n. I add a committedIndex() func in server.go\nAs we will pass the server pointer to the command, the application can get the current index and then return to the client.\n. For 1, probably we can do them in one select.\nFor 2 and 3, I will do it when I am going to write the codes.\nWhat do you think about this structure?\n. I will close this and send another one.\n. Probably, we need to write the conf to the disk in a separate file.\nCan you do that when you are going to clean up the API?\nThanks.\n. This can be solved by application. Add leader to itself.\n. This is caused by not discovering new leader.\n. We changed the server to a \"single\" threaded processes. Locking is not a big problem for now.\n. Try to use rwlock to separate write and read\n. make sure two thing: 1. try not to lock disk operation 2. try not to lock blocking channel sending\nOthers are ok for now. Lock will not be the bottleneck of the raft algo.\n. The server cannot be single threaded since you need to send votes and send heartbeat.\n. Yeah. Try to make as sequential as you can.\n. There is a big bug in server.go \nI have fixed it and update the pull request\n. I am going to read through all the code now.\n. Do you change the AER so that when the candidate discover a new leader it will also step down? You can see my last pull request\n. I re-write the setCurrentTerm func in the pull request.\nYou can read it.\n. Thanks for your refactoring and it is a great help. I was going to do that, but I think use lock to make the request reach the server in sequence is find for now.\n. The only thing I am concerning now is discover new leader in AE when the server is a candidate and the timeout.\nBut you can leave it for now and merge the pull request. \n. No problem. I will do it after a while.\n. Seems there is a problem with election after our new refactoring.\nI am trying to look into it.\n. Could you open the debug level and change multi-node test to loop for 2000000 times.\nAfter you run it for a while, some node will get stuck.\n. My previous version can run multi-node test for many hours until it eat up all my memory. You can do the multi-node test as a base. And try to do other test when the current version will not panic or stuck at that test.\n. I am also going to look into the problem\n. That is why I wrote the timer. That timer can make sure when you stop it, it will never restart or get stuck.\nThere is still some other issues, I am looking into them.\n. The problem here is the heartbeat func has returned, but the leader still try to send the stop signal to it. That will cause the leader blocking forever.\nThe problem you address can be solved by creating new channel or try to receive the previous signal before reuse it.\n. Sorry. I am afraid that you just merge it, since I think it is quite important. \nI will refactor that. \n. I am writing a application. For me, I would like to control if the server start as a follower or a leader.\nThe new node will have a cluster address and it will issue the join command to the cluster first then start as a follower.\nWhen having the log file, the node seems should start as a follower. If no one in the cluster send the node AppendEntriesRequest, it will promote itself automatically.\n. Do you add it for testing purpose? \nI implemented my transportation layer with http and https.\n. Look good to me.\n. Json package in GO is slow. The speed of it is comparable with network speed. 100kb of our data will require 100ms to encode and decode, which means we have to limit package size within 100kb when setting election timeout at 150ms.\n. @peterbourgon Do you mind to unify the API of your implementation and ours? Then we can use the same test case to do the test. \nMaybe we can come together one day and design that.\n. @peterbourgon I have built a key-value store on top of raft. So I am able to test it via my system. You can check at https://github.com/coreos/etcd\n. @benbjohnson Can you create a IRC channel for go-raft?\n. Yes. Sorry.\n. I just noticed the problem when I was doing tests. The speed is comparable with network speed, thus I think we should improve. Suppose that we set the election timeout to 150ms. If any node miss more than 5000 entries, it will never catch up with the faster machines (if client keep on issuing new commands).\nZookeeper can process 10K write operation/Second. \nFor current json implementation, it will need more than 400ms to decode and encode 10K operations.\nThe leader even needs to do more work (it need to encode multiple times).\n. fixed.\n. I wrote this in a hurry.\nThe main idea is when the server becomes leader, it will send out a NOP command.\nThe sendAppendEntriesRequest() in peer.go will check if the peer successfully append a log entry from the current term. If it is, it will set the append to true.\nThe server will check if most of the peers have been sycned by the current term log entry. If it is, it will begin to commit commands.\n. I will do all the changes tomorrow morning.\n. Yes. I need to add it each steps. I am just adding it to the go-raft and trying it a little bit.\nSo let us use the built-in one.\n. yea. thanks.\n. @benbjohnson \nNot for testing. Brand New nodes should not promote it self that until they get synced with the cluster once.\n. Yes we could, I just want to let the application have more option.\nBut I can change it.\n. @benbjohnson How about add a func() called startSyncedFollower(), which the follower will not to be synced once before promote?\n. The raft paper does not say it explicitly. But it is meaningless to let a follower promote itself if it is certain that it cannot be elected. \n. Also it is important if we decide to use AE to get the commitIndex. Reply the log requires time. We should not let a follower to promote only because it is replying log that cannot receive heartbeat. \n. No problem. I was testing my scripts. It cannot run without this limitation. Thus I did a quick fix. :)\n. @benbjohnson We need to think carefully to limit it by bytes or messages. \nThe process time is decided by networking + encoding/decoding + disk operation + state machines apply command.\nCurrently, we encoding/decoding every message and do disk operation pre message and apply each message. Also each message should not be very large (decide by the nature of distributed algo). \nThe number of message can decide the time of processing.\nWhen we can write a bunch of command to disk at one time and reduce encoding/decoding time for each command, it is meaningful to change it to limit by bytes.\n. wow. milestone. \n. um... seems that we have a getEntry func, but has some issue.\nI will fix that\n. @philips  https://github.com/benbjohnson/go-raft/pull/66\n. @philips \nFor the leader it needs to encoding and decoding multiple times for different peers in one round of flushing. \nThe encoding speed will be even slower for the leader, thus given the speed of json I think we should change to a faster encoding method.\n. BenchmarkAppendEntriesRequestEncoding       1000       1830835 ns/op      49.17 MB/s\nBenchmarkAppendEntriesRequestDecoding       1000       2187454 ns/op      41.15 MB/s\nPrevious binary:\nBenchmarkAppendEntriesRequestEncoding       2000       1391891 ns/op      96.31 MB/s\nBenchmarkAppendEntriesRequestDecoding       1000       1818073 ns/op      73.73 MB/s\nSince protobuf compress the data, we should compare the time/op.\nProtobuf is slightly slower than binary, but I think it is acceptable.\n. Also in face, due to the compress we can get better network performance.\n. Add a spell checking plug-in into my editor. \n:)\n. I am thinking to let client application to trigger the snapshot process, since it should know the state machine better than the library.\n. @benbjohnson \nCould you double check L220 in log.go? \nI do not think we need this when recovery. Is that right?\nThanks\n. hrm... we can separate the file. I think the seek func only change the pointer value in mem. \nWhat is useful is to write entries in one disk operation rather than a lot. I am going to change that.\nWhy you want to save the configuration?\nI have save that in the snapshot.\n. Store commitIndex at the head of the log file. When join or remove a peer, we update the commitIndex.\nWe need to keep that since when we restart we must recover the cluster configuration.\nI do not keep the configuration in a separate file. Since recording the commitIndex will actually help us to recover some commands that we will certain recover too.\nThat is why i choose to save the commitindex rather than conf.\n. We only seek to the head when join/remove. \nIf we keep it in another file, we still need to seek to the position.\n. \"we shouldn't be recovering commands until we know that there's not a new snapshot.\"\nI agree with you at this point, thus we should store the configuration.\nI will refactor the codes later.\n. @benbjohnson\n1. move configuration to conf file\n2. delete position in log_entry\n3. recover/create conf file when init\n4. append conf file when add peer\n5. rewrite conf file when delete peer\n. A quick fix. Could you please help me check if there is any logic/func error?  I will fix it later.\n. Done.\n. Only allow the first log entry to be a self-join.\nThe first log entry should be a join, but not necessarily a self-join i think. \n. A is the first node. It sends join-A, which is a self-join. Then becomes leader. \nA 's log entries will be join-A and nop.\nB tries to join the cluster of A. It sends join-B. Then A will commit join-B.\nA 's log entries will be join-A , nop and join-B.\nThen A send B appendEntries request. B will accept them. \nB 's log entries also will be join-A , nop and join-B.\nCorrect me, if I do not understand it right.\n. @philips  Yes. \nThis function will be called each time raft write an entry to disk. \nThus, it will be called very frequent. \nIt will be called by only one thread. \nThus, we do not need to have a local buffer to avoid race. \nWe create a global var for reuse to reduce GC time.\n. @benbjohnson \nOk. I will do\n1. Move the two global vars to a raft server unique struct\n2. Try benchmark\n. @benbjohnson\nDid this solve your problem?\nI embed a simple speed test into httptest. I just want you to try it, then you can remove it.\n. @benbjohnson  yes. sorry\n. @benbjohnson \nBenchmarkSpeed for before and after\neach op is for 20,000 nop commands\nI did it 5 (each benchmark also include 5 times, total 25 times) times and got the average time.\nBefore:\nBenchmarkSpeed         5     873075029 ns/op\nAfter\nBenchmarkSpeed         5     782130436 ns/op\nAbout 12% improvement.\n. BenchmarkAppendEntriesRequestEncoding       1000       1862026 ns/op      48.34 MB/s\nBenchmarkAppendEntriesRequestDecoding       1000       2304243 ns/op      39.07 MB/s\nBenchmarkAppendEntriesResponseEncoding   1000000          1456 ns/op       5.49 MB/s\nBenchmarkAppendEntriesResponseDecoding    500000          3287 ns/op       2.43 MB/s\nBenchmarkSpeed         5     424838309 ns/op\nmy result.\n. @matttproud  Thanks for the pr. I merged and fixed it. \nCould you please make sure you can successfully  go install/build and go test before you send the pr next?\nThanks.\n. @matttproud  the problem is that we change the default test heartbeat timeout in the previous pr, but have not changed it in the test. \n. @matttproud  I will leave this pr for Ben. He talked to me he was going to add Travis.\n. @matttproud  I fixed all the tests. If you have time, you can try it again on your environment. You can try bench also. But the last benchmark may sometimes fail due to leader change.\n. @benbjohnson Can you double check if this is correct? I got a deadlock when I try to stress the server. Now it works fine for me.\nThanks.\n. @benbjohnson\nI test snapshot with etcd. And now it \"works\".\nI am going to write snapshot tests for raft very soon,\n. @benbjohnson  yes. It seems like that testing server is slow. We have a timer test, then it fails.\n. @benbjohnson @traviscline \n Most the race is caused by debugln, and some other minor issues. We will fix it as thing is getting stable.\n. @benbjohnson I will take care about this very soon.\n. @Mistobaan  Solid simplification. LGTM. \n. @philips lgtm.\n. @benbjohnson  I am trying to make it work in application layer. I dont want raft be complicated either. :)\nLet us see what happens.\n. @benbjohnson I will think about this more tomorrow.\n. @philips In etcd we have internal store system, so we do not need anything store here. But for other application, like connection url maybe need to store here.\n. @benbjohnson do not merge it now. I will write test cases. \n. @benbjohnson Add connectiongString.  I change the API of Join Command to Name, ConnectionString. Thus the join command in fact has already included the connectionString information. We do not need to record peer info in the conf file. But it seems not bad effect also. `\n. @mattn Is this the only different syscall between windows and POSIX in golang? If there will be more, we probably need to have a better filename.\n. lgtm\n. I made the following changes:\nFirst: Raft server need to record its own connection string. And when doing snapshot, it need to add itself to the peer list also. Since the snapshot may send to other machine, and these machine need to figure out cluster configuration from the snapshot. \nSecond: Since we record the conf and snapshot, they all include the commit index information. When restart we need to pick up the larger one to be the true commit index.\n. @benbjohnson Since I think some people do need to implement their own transportation layer.\n. @benbjohnson Nice slides! Looking forward to the video!\n. @benbjohnson lgtm.\n. @benbjohnson In transportation layer, we still need to deal with http timeout. Or there will be problem when network is partitioned. I am working on that in etcd's transportation.  After that I will send pr to update the default one in go-raft.\n. @benbjohnson Actually, raft use a  false negative approach about the command commits. But I agree we should make this a application decision. I can make the change.\n. @benbjohnson I am going to work on this. I will remove the timeout related stuff and let the application make its own decision.\n. @benbjohnson BTW, can you review https://github.com/goraft/raft/pull/136/files? The only actual changes is making asnycSend a real one.\n. @grncdr It is also very convenient to accept your pr on github. So send as many as you can. :)\n. @benbjohnson  @kellabyte  There seems to be a problem. I will fix it soon.\n. @benbjohnson @kellabyte   We need to sort the commitIndex of peers in descending order. Then we can pick the first majority one as the current CommitedIndex of the whole cluster.\nI will send a pr.\n. @benbjohnson  will do that.\n. @dgryski  lgtm. Thanks a lot.\n. @benbjohnson This should fix #100.\n. @benbjohnson Maybe we can merge this? \n. @benbjohnson For the second, I cannot understand it. I think it is already synced by the channel... \nI will merge this. \n. @ongardie For the cluster size, in my reply I was comparing 8 and 9 rather than 5 and 9. \nAlso, if having a lot of nodes, we probably need to cache the serialization result, which we have not done yet.\n. @jvshahid This will not totally solve the problem. At the end of leader loop, we stop the peer heartbeat, but we cannot guarantee that we actually stop the flushing process. The peer might be already in the flushing process and will stop when it finish the process. If you want to do gracefully shutdown, probably you need to add a two way handshake among the server and followers. \n. @jvshahid No. It is because the peer may be inside the flush() function. It will stop after that function returns and go through the select again.\n. @jvshahid  I think I made a comment about why we use a buffered channel. \n. @jvshahid Oh. https://github.com/goraft/raft/blob/master/peer.go#L90 Here it is. I need to spend some time to refresh my mind about this. I am sure we have a problem here before. I will see if it is still case now.\n. @jvshahid I think the problem is the p.send if I remember correctly. Actually I have done a performance improvement of raft and remove the extra blocking stuff. I have not got a time to clean it up and push it to the current master. Probably I can wait for me. Or you can try to play it a little bit. :)\n. @jvshahid by the way, are you using raft in your project?\n. @jvshahid Cool. I will try to fix it during the weekend. \n. @jvshahid That should work. But in fact I have cleaned all these several days ago, but have not got a time to send a pull request. Please allow me a few days to clean my branch and try to keep your commit. \n. @benbjohnson The http test fails randomly. Can you take a look? \n. @jvshahid Can you send a pull request to add your db project to the projects list of goraft? Thanks.\n. @jvshahid Yes.\n. @jvshahid do you have a gtalk that i can contact? \n. @jvshahid I fixed this in another pr https://github.com/goraft/raft/commit/59cb7259c23e0bce91d0cab4fd229fff00555996\n. @benbjohnson I want to add this into raft library for the following reason:\n1. I can see other application want to have a time sync mechanism.\n2. I do not want to etcd know much about raft internal. \n. @benbjohnson Also this add no overhead to raft, if people do not want to use sync.\n. @benbjohnson OK I will have a try. Currently I can see we need to add a mechanism to push the status change to etcd in raft. I do not want the sync based on a polling model, since most of the nodes will be followers. \n. @benbjohnson I will use a polling model first. It all the ttl approach works well, i will make the change at raft side.\n. @davew2 Thanks. \n. @JensRantil I think we use panic as assert. Feel free to send pr to clean up.\n. @JensRantil Raft is used to maintain the state of several distributed stateMachine to be consistent. If the command is stateless and directly apply to filesystem or sth, you can just use nil. \n. Time.after will create an object every time you call it, which cause extra malloc and gc. \n. And we might want heartbeat every HEARTBEAT, rather than HEARTBEAT add process time.\n. @benbjohnson \n1. Gracefully stop the server\n2. Avoid unnecessary gc time\n3. Clarify debug info \nI have done this sometime ago while I was testing perf of raft.\n. @benbjohnson Right. I need to fix this. \n. @benbjohnson Hmm. I look at the codes again. \nPeer.sendAppendEntriesRequest() is actually not an async call. \nI just made sending response to server an async call. It is an internally call and will not cause the problem you were worry about.\n. @baruch fsnyc on snapshot is ok. But it will slow down the speed when applied on the log entry. When a log entry is committed, it should be persistent on the page cache of the majority of nodes. The only problem is that if all of theses machines die the committed log entries will be lost.\nActually I have not began to measure performance and its affect, so I cannot make a decision. \nI will review it all later, but want to give a fast response to thank you.\n. @baruch We are fixing the issue in another pr #150 \n. @benbjohnson This is reasonable. LGTM.\n. @benbjohnson I think the precent should be configurable. Besides, lgtm. \n. @benbjohnson Without timeout application will hang at a command, it the current leader cannot reach a majority of peers. Application needs to deal with this if it does not want to be blocked. \nI think a better approach is creating a new kind of event. And if the leader cannot reach the majority of peers for a given period of time, the leader will notify all the listeners.\n. @jbcrail Thanks.\n. @baruch @benbjohnson \nJust a spelling fix. Merged. \n. @benbjohnson \n1. The util.go and using rename to replace the log is cherry-picked from @baruch's pull request.\n2. Actually we only do a fsync after the follower append its entries. We do not fsync when the leader append its entries. I do not think the leader needs to do that. I think the leader only needs to do this before commit entries, so it can fsync entries in a batch. \n. @benbjohnson \nIt should be safe since we only need to make sure at the committing point, the log entries are safely stored on the majority of the nodes in the cluster. \nThe thing is that \n1. We send entries in a batch to follower. So when there are high traffic, the follower will fsync appendEntires to disk in a batch. Will not hurt performance too much.\n2. If we fsync before it sends to follower, we still are going to fsync multiple times. \nMy hope is to do heartbeat in a single go-routine , and broadcast in multiple go-routines. \nIn that way, we can \n1. Simplify the current approach.\n2. Find a way to reuse the serialized results, since most followers should be at the same pace.\n3. We can do fsync for leader each heartbeat.\n. @philips \nAs @ongardie suggests, if we disable fsync, then we are totally rely on the page cache of OS. Both power failure on leader and the the whole cluster might cause us to lose committed data. \n. @benbjohnson Will do.\n. @benbjohnson There are still some cleaning up stuff to work on. But you can review this pr to get a rough idea what I am doing.\n. @benbjohnson Will do.\n. @philips No. I did not change any format.\n. @benbjohnson Can we merge this? I have some more cleaning up work to do. But it is better to do in another pull request I think. Thanks.\n. @philips This is not a optimization just for one node cluster. One node is just a special case, which requires more work (In the if statement). \nThe gain is 1/n of command process time for a n nodes cluster. \n. @benbjohnson We send a ae resp to itself before when it is a single node. So I remove the ae resp send and add a if statement to achieve the same result.\n. @bcwaldon lgtm \n. @ptsolmyr Are you trying to annotate go-raft in Chinese? I am Chinese. So you can contact me if you need any information via emial or QQ.  I am very happy to help.\n. @benbjohnson Drone is fast...\n. @mreiferson Our raft server is serving in one for loop and collecting events via a buffered channel from\n1. HTTP triggered go-routines.\n2. Peer go-routines when the server is a leader.\nNo matter if the channel is buffered or not, the HTTP triggered go-routines cannot process until the server finish processes its request. Buffer will not help to reduce the latency here I think.\nThe reason to have a buffered is that our peer go-routine does not require the response from server. So we do not block it from doing another round of heartbeat. \nI can see we are able to remove the buffer completely after some refactoring.\n/cc @philips @benbjohnson \n. @mreiferson It is not feasible to make heartbeat strictly synchronous. We cannot afford let the server idle waiting for the reply from a peer. \n. @mreiferson I am trying to make heartbeat totally isolation for the state of the server. The responsibility of the heartbeat routine should be just sending out read-only data to peers and get back reply to the server routine via channel. It should not change any state of other stuff directly. \n. @mreiferson Cool. Thanks.\n. @nemothekid You can use the event.Value and event.PrevValut instead of calling the server.X to inspect the changed variables for now.\nYour comment are reasonable. We can work around it.\n. @benbjohnson sure. \n. @philips I think interval is better.\n. @benbjohnson @philips \nHere is my simple bench tool:\nhttps://github.com/xiangli-cmu/raft-bench\n. @philips The root problem is that the heartbeat can still be running after the server steps-down to a follower/candidate and increases its term. The best approach is to stop all the heartbeat before increase its term. \nI do not do it now since there are some more changes I want to make together. \nWe have separate go-routine for each peer's heartbeat. I want to avoid theses go-routines and try to make most of the peer synchronized. Thus, we might find a way to reuse the encoding results, which reduce the CPU usage by a factor of number of followers. And this can also make the codes simpler. \n. @philips More docs in the commit message.\n. @zenazn Fixing\n. @zenazn \nI think dispatcher is holding its own lock.\n@benbjohnson \nAll the other read locks are holding for debugging, shall we just drop them? We added theses locks to let go -race happy.\n. @mreiferson The locks come from our old codebase, when the server is not in a event loop. We largely refactor the codebase to make server in a event loop. I am going to clean up all locks soon. \n. @benbjohnson  I have finished making things in a single thread. I will send you a pull request.\nBut before this, I think I need to send you guys a design doc to clear all these sort of thing. \n. @zenazn @mathiasbynens I think this is fixed by https://github.com/goraft/raft/pull/167\nWe stop all the heartbeats go-routines before leader steps-down and truncates its log.\n. @zenazn Yes. Go-raft needs some cleaning up work. \n. @zenazn One more thing, how did you find out the problem? Are you trying to audit the codes against the raft paper?\n. @zenazn CTF is interesting! I found this bug several days ago when I was doing a benchmark. \n. @philips If a peer routine is inside https://github.com/goraft/raft/blob/master/peer.go#L146, it will try to grab the server lock at https://github.com/goraft/raft/blob/master/peer.go#L193.\nWhen the server routine wants to stop it, the server grabs the server lock first. If this is the case, the sever cannot stop this peer routine. Since the server is holding the server lock waiting for the peer routine to finish, but the peer routine is waiting for the server lock. Neither of them can make a progress.\n. @philips I am cleaning all static data into a data package. So I am not going to merge this now.\n. @benbjohnson Snapshot, Request and Reply. \n. @benbjohnson Personally, I think there are too many files in the first level folder. And some of the files do no depend on each other. So it is cleaner to make them into separate pkgs. \n. @benbjohnson @philips \nHow do you think about this. After this, I also want to move log into a separate pkg if possible. \n. @mreiferson Not good at naming. But it will also include reply_types in this pkg. \n. @mreiferson How about message? \n. @benbjohnson I can combine some related files into one file, although I still want to separating them into a pkg since I am aesthetic. \n. @benbjohnson They are in the same functional group so they are all in raft package. I can separate them into different package since message related codes do not depend on raft server codes.  If they are very related, it is not even feasible to separate them, or there will be circular dependencies. \nAnyway, I can understand that you want to keep raft coherent. So I close this pull request.\n. @benbjohnson @philips \nI have a simple design picture for this @ https://github.com/xiangli-cmu/goraft-doc/blob/master/raft.png.\n. @benbjohnson @philips Closing this one. Working on a clean solution.\n. @benbjohnson \n1. Remove unnecessary nested loop in candidate loop. \n2. Move processVoteResponse to a function.\n. /cc @philips @unihorn\n. @benbjohnson \nfor change No.1. I do not like meaningless codes\nfor change No.2. I want to write unit tests for every process function.\n. @benbjohnson @philips Anything I can do to get this merged? \n. @benbjohnson I have set up Drone for my branch:\nhttps://drone.io/github.com/xiangli-cmu/raft/3\nI will clean up this pull request soon.\n. @benbjohnson Add a _assert function.\n. @benbjohnson closed this since \n. @philips see #183 \n. @philips This section of effective go is good: http://golang.org/doc/effective_go.html#chan_of_chan\n. @philips @benbjohnson I will separate this into two pull request. In this pull request, I change the code path a little bit along with the naming stuff.\n. @unihorn Can you provide more details? In which case, there will be two goroutine calling this function?\n. @unihorn This is a awesome bug catch, but I have to close this. This pull request cannot totally solve the problem.\n I have found the root cause of the problem.\nHere is why there is a deadlock:\nWhen the leader call removePeer it is holding the log lock, since it entry the removePeer via setCommitIndex. The leader will send a stop signal and wait for receiving. \nIf the peer is actually in function: flush(), it is also need to acquire the log lock at func p.server.log.getEntriesAfter.\nSo a deadlock happens. \n. @unihorn Can you create an issue for this problem. So we will remember to solve it.\n. closed via #195 \n. separate https://github.com/goraft/raft/pull/184 into two commits. This is the first one.\n. @philips Refined the comment.\n. lgtm\n. @macb \n1. We can do back-off probing with limited growth (seconds)\n2. We can do actively requesting if one node loss connection with the leader and is approaching election timeout or just restart. \nWe are re-writing the heartbeat function. I think we can just leave this pull request here for now.\n. @unihorn Can you test this against etcd? Thanks.\n. @benbjohnson Our addPeer and removePeer are not safe if the leader dies before all the nodes commit the command.\nBut I am not worrying about this right now. \n. @unihorn Fix the race in remove test?\n. @philips I was trying to refactor the heartbeat and log lock. But the result was not that great. I am planning to do it more carefully next week. \n. @swinghu Hm... I cannot understand this pull request. Did you send pull request to the wrong repo?\n. /cc @benbjohnson @unihorn\n. @benbjohnson \nIt is a cleanup. I tried test -race and these changes did not introduce more races and not fix any either.  \nI think the peer lock is not related to this pull request. This pull request does not change that behavior. \n. can i merge this one? i will try to fix data race after this.\n. @benbjohnson We need to add this to fix etcd bootstrap problems. We want to be able to do some bootstrap work (send join requests, test the liveness of the previous member) based on its previous state before actually start raft server.\n/cc @unihorn\n. @benbjohnson I want to call Init first. Then raft will load all the existing logs. So we are able to examine the state of the statemachine before start the raft server.\n. I do not quite follow your comments. Can you try to elaborate that?\n. @unihorn Reasonable. \n. /cc @benbjohnson @unihorn  updated.\n. Talked about this over phone. closed this one. \n. lgtm.\n. @unihorn Oh. You need to add a function into interface also. \n. @benbjohnson We can update the commitIndex to disk every update. We have talked about this some months ago and you decided not to do that due to disk operation overhead. \n. /cc @benbjohnson https://drone.io/github.com/xiangli-cmu/raft/24\nIntroduced a bug in last pr :(\n. we are checking for all types except previous existence. pervious existence is not an error we should report.\n. @benbjohnson \nIsNotExist returns a boolean indicating whether the error is known to report that a file or directory does not exist. It is satisfied by ErrNotExist as well as some syscall errors.\nIt says \"It is satisfied by ErrNotExist as well as some syscall errors\".\nDid I misunderstand this sentence? \n. hm... i misunderstood this error sorry\n@xiangli-cmu https://github.com/xiangli-cmu The only time\nErrNotExistswill occur with\nMkdir() is if the parent directory doesn't exist. In this case, the parent\ndirectory is s.path, which should exist.\nhttp://play.golang.org/p/Kezcz9swTX\n\nReply to this email directly or view it on\nGitHubhttps://github.com/goraft/raft/pull/203#issuecomment-38364784\n.\n. /cc @unihorn @philips \n. @benbjohnson go test -race. \nI will test it against etcd and fix the race found there in another pull request.\n. @unihorn Can you test this changes on etcd? Thanks.\n. @unihorn Hm... I suspect that the log does not replicate to node 3 successfully. So the command cannot be committed in time, which cause the timeout. \nBut I need a deep analysis to see what happens. \n. @jvshahid Thanks for reporting this. It is a bug need to be fixed. \n. @jvshahid It will be great if you can test this patch. Thanks!\n. it is a quick fix. to test the snapshot we need to have a testing state\nmachine. i can do that later.\nOn Mar 25, 2014 12:21 PM, \"John Shahid\" notifications@github.com wrote:\n\nI think you should add a test to make sure the bug is fixed and to protect\nfrom future regressions. Looking at the patch, looks like it will solve the\nproblem, but again, i think a test case is important even if the bug is\nfixed.\n\nReply to this email directly or view it on GitHubhttps://github.com/goraft/raft/pull/208#issuecomment-38586229\n.\n. @jvshahid That will not work. https://github.com/goraft/raft/blob/master/server.go#L1124\nI can work around it of course. But I do not like to do that sort of heck. \n\nWe can test the snapshot and recovery process within raft if we add a testing state machine. \n. @jvshahid I was not aware that @benbjohnson have made a mockStatemachine before. I added a test for the regression. \n. @unihorn Can you take a look at this and test it against etcd? Thanks!\n. @philips I think I messed up the commit message. will fix.\n. @philips fixed\n. There are a lot of duplicate codes. But I cannot think of a better way to do it either. \nThe transportation layer should set a timeout for both read and write request. So stop won't be blocked for a long time. \n. rebase is needed. \n. @unihorn Can you setup drone on your local branch? Like https://drone.io/github.com/xiangli-cmu/raft? \n. @unihorn Thanks.\n. the bug is introduced here https://github.com/goraft/raft/commit/01e8793c9a031a13f8fcc67784545d76d96e94f9#diff-34c6b408d72845d076d47126c29948d1R906\n. @unihorn We can do that, but I would rather make that happen in another refactor pull request. \n. @unihorn Updated. How about this?\n. @unihorn Only candidate should step-down to follower. Both follower and  snapshotting should be as same as it is.\n. @unihorn I agree with @benbjohnson . I hope this can be done in etcd. We should expose API in go-raft to support that.\n. @philips @unihorn I will add a API to expose the fd in go-raft. \n. /cc @unihorn\n. @unihorn No it is not. See the comments at https://github.com/xiangli-cmu/raft/blob/fix_channel_race/server.go#L445\n. /cc @philips \n. @benbjohnson @philips I merge this since this one has no risk.\n. /cc @unihorn I further clarify the problem for you in the commit message. \n. /cc @unihorn\n. we need to add a test for this in goraft.\n. @benbjohnson Yes. I know that. This will just invoke a committed index re-calculation. So it will not hurt us.\n. lgtm\n. ",
    "tsenart": "Isn't it in theory possible for the cluster to always come to a split vote? Raft guarantees at most one leader per term, but makes no such guarantees about at least one leader per term.\n. @benbjohnson\n. Done. I ended up chaining all of it :)\n. Ah, nice catch. Thanks. On not chaining all the conditionals, I agree, but thought you really liked chained conditionals :) I rebased all commits into one too.\n. I am not sure that was clear in my last comment, so: it's ready.\n. Isn't it in theory possible for the cluster to always come to a split vote? Raft guarantees at most one leader per term, but makes no such guarantees about at least one leader per term.\n. @benbjohnson\n. Done. I ended up chaining all of it :)\n. Ah, nice catch. Thanks. On not chaining all the conditionals, I agree, but thought you really liked chained conditionals :) I rebased all commits into one too.\n. I am not sure that was clear in my last comment, so: it's ready.\n. ",
    "philips": "@xiangli-cmu Perhaps it should just be two or three compactions with a tunable for people who need it?\n. What were the benchmark numbers from the JSON format?\n. Were tools like protobufs/gobs/etc explored? It seems like a large leap from a self describing JSON format to a hand coded binary protocol.\nFor example I could imagine a time in the not so distant future where having a wireshark dissector for this protocol will be nice. Or where adding a single field in a v2 protocol without breaking old versions will be necessary.\n. LGTM. The global protobuf log is to reduce object churn and garbage collection, right?\n. @xiangli-cmu Why are you doing this? It looks fine but I don't know why it needs to happen.\n. @benbjohnson Great, you are the owner and I added Xiang since I think he currently has merge rights on this repo.\n. What is an example of the info that will go in here?\n. LGTM :)\n. This fixes #118 reported by @kellabyte\n. @benbjohnson Good idea. It was super confusing reading: Less returns i > j \n. lgtm\n. @benbjohnson One style thing I have started doing in markdown files is one sentence per line. It is compatible with all of the editors and makes reviewing in git and github easier. Just a suggestion; no strong feelings either way.\n. lgtm, super helpful.\n. @benbjohnson Another thing to consider is even vs odd number of nodes. e.g. https://github.com/coreos/etcd/issues/149#issuecomment-23603009\n. I really like the possibilities this opens up! Don't have time to give it a proper review though.\n. Ack on the application handling log back off.\nOn Dec 29, 2013 12:02 PM, \"Ben Johnson\" notifications@github.com wrote:\n\nOverview\nThis pull request adds two new event types:\n-\nHeartbeatTimeoutEventType - Dispatched when a heartbeat times out.\n   This requires that the transporter returns a nil for timed out\n   requests.\n   -\nElectionTimeoutThresholdEventType - Dispatched when an AppendEntriesrequest comes within a given threshold of the election timeout. This\n   threshold is currently set to 80% of the election timeout.\nThis will give Raft users better insight into whether their heartbeat and\nelection timeouts are too low.\n@philips https://github.com/philips: We can add exponential backoff of\nnotification of these events at the application level. I'd rather dispatch\nall events at the go-raft library level and not filter any out.\n/cc: @xiangli-cmu https://github.com/xiangli-cmu\nYou can merge this Pull Request by running\ngit pull https://github.com/goraft/raft election-heartbeat-events\nOr view, comment on, or merge it at:\nhttps://github.com/goraft/raft/pull/144\nCommit Summary\n- Add election threshold and heartbeat timeout events.\n- Merge branch 'master' of https://github.com/goraft/raft\nFile Changes\n- M event.go https://github.com/goraft/raft/pull/144/files#diff-0(3)\n- M http_transporter.gohttps://github.com/goraft/raft/pull/144/files#diff-1(8)\n- M peer.go https://github.com/goraft/raft/pull/144/files#diff-2(1)\n- M server.go https://github.com/goraft/raft/pull/144/files#diff-3(13)\nPatch Links:\n- https://github.com/goraft/raft/pull/144.patch\n- https://github.com/goraft/raft/pull/144.diff\n. @benbjohnson +1 on making fsync something we can disable by default.\n\nPerhaps I am being naive but fsync only helps protect against data loss in a full cluster down, right? In the normal case where the current leader has a power failure but a majority of the cluster stays up then it doesn't matter if goraft is fsync'ing because the cluster will have moved on and the correctness of the leader's log is irrelevant once it rejoins.\nJust trying to wrap my head around why we are fsyncing the log at all.\n. Does this change the protocol or log format?\n. @xiangli-cmu Is there any reason for optimizing the single node case? Seems like there isn't much to gain from 4aa1042.\n. lgtm, merging.\n. Nice work :)\n. The tests look good. My problems are with the code as it was before this PR. If you want to merge this as-is it looks good to me. The other stuff can be cleaned up in a subsequent PR.\n. /cc @bcwaldon\n. @benbjohnson Yea, I guess a thresh hold event would work.\n. @benbjohnson I agree I think we should measure then consider getting rid of the sendAsync codepath. I don't know off of the top of my head why we decided to let there be an essentially infinite event backlog via sendAsync.\n. @benbjohnson Yea, exactly. :(\n. @benbjohnson How should we make this decision? It seems like we need to do a few things:\n- [ ] Make the channel size adjustable at construction\n- [ ] Benchmark how quickly goraft can drain a queue\n- [ ] Make a reasonable default\nMy hunches on the second two:\n- Based on my current observations this will have everything to do with the network, not go-raft\n- Based on say a 50ms worst case latency and 1ms average completion we get ~50 (round it to 64?)\n/cc @xiangli-cmu \n. @bcwaldon fixed.\n. go fmt'd sorry.\n. /cc @benbjohnson What do you think about this change?\n. Looks like we have quorum. ;)\nThe only other thing to consider is if we will need to break the API again for the event channel stuff.\n. @xiangli-cmu Can you please make the commit message more helpful? It is better to have information in the git history rather than on github.\n. This makes sense to me logically. I don't see what the hack is in it though.\n. @benbjohnson testing against etcd now.\n. @xiangli-cmu @benbjohnson I am having a hard time seeing why we drop the lock in 38087d2. We send false into the stopChan which means we bail without a flush(). https://github.com/goraft/raft/blob/master/peer.go#L127\nHaving a hard time seeing the deadlock.\n. 6c994074c0e2ba89ff182803b62586a60919332b looks fine to me.\n. I tested these changes with etcd master and it seems to work fine I just want to cleanup the commit message of 38087d2 since it doesn't explain what locking problem this is solving.\n. @xiangli-cmu Thank you. I see the race now. I found another quick cleanup I want to do and will pull in these two changes and cleanup the commit messages of 38087d2, send up a new PR and then merge it into etcd. Thanks!\n. @xiangli-cmu That is fine, I just ran it in my editor while reviewing code. Just do it in your PR.\n. /cc @bcwaldon this was part of the cleanup we were talking about a few days ago.\n. lgtm, this is likely to break other outstanding PRs though, right?\n. /cc @benbjohnson @xiangli-cmu \n. Merged via aed3d62. Thanks @chrislusf \n. Lgtm\n. lgtm ;)\n. agreed, much better. thanks xiang.\n. assert commit lgtm. builds pass too https://drone.io/github.com/xiangli-cmu/raft/4\n. @xiangli-cmu closed this since what?\n. I like this and it lgtm. @benbjohnson what do you think of chan chan? I haven't seen this before but it makes sense in this event loop case.\n. lgtm, besides the two nips. The naming is much better.\n. Thanks Xiang\n. lgtm.\n. thanks!\n. +1 thanks.\n. @macb In coreos/etcd#595 I was more meaning that the logging should backoff exponentially. The backoff on this side, if we add any, should be capped at a second or two.\n. Just hit this in the proxy pr, reading now.\n. lgtm! this seems like a valuable feature.\n. lgtm too. We need to document/refactor these locks.\n. @xiangli-cmu Sounds good. Thanks for taking it on.\n. dang, lgtm.\n. Lgtm\n. lgtm, merge at will.\n. lgtm once the commits are cleaned up a bit.\n. lgtm\n. Ick. We can do better by just making the ioctl syscall from Go, can't we?\n. @unihorn The constant is a constant and a kernel interface. Copying it into Go is fine. See https://github.com/coreos/go-namespaces/blob/master/namespace/linux_x86_64.go as an example.\n. @unihorn The syscall package is essentially just huge amounts of copy and paste from C header files. ;)\n. All we have to do is have the log directory set with NOCOW. This will be inherited by the file that goraft writes. Nothing should be necessary in goraft at all.\n. lgtm\n. yea, we need to make sure this gets a test first.\n. @xiangli-cmu this lgtm. I am OK merging it but @benbjohnson 's second opinion might help.\n. @xiangli-cmu What is the status on this one?\n. ping @xiangli-cmu ?\n. @xiangli-cmu @benbjohnson Can we merge this?\n. This makes sense to me. I wish the commit had a description like:\n\"If we need to truncate the file because of a decoding error then seek back to the last known good entry\"\n. lgtm\n. lgtm. @xiangli-cmu @unihorn ?\n. This LGTM. However, does it also fix thos \"empty\" messages as reported in coreos/etcd#913?\nAny objections @benbjohnson @xiangli-cmu ?\n. Fixed. Thank you.\n. This is a somewhat dangerous bug if people have been using rsync or something to backup their snapshot directory. But, in the common case there is only ever one snapshot file on disk and this is managed by go-raft.\n. @otoolep was ffe3da0 to fix 235? \n. @mre The etcd team's library is at http://godoc.org/github.com/coreos/etcd/raft and it is also used by http://godoc.org/github.com/cockroachdb/cockroach/multiraft if you are looking at options. \n. The etcd developers (@xiangli-cmu and @unihorn) have done quite a bit of maintenance on this library until now. However, as our new raft library has matured and the  0.4 branch of etcd goes into maintenance mode the goraft project needs new maintainers. Particularly since we have a few issues piling up: #239, #238, #235, #233 and #240.\nDoes any have a suggestion on who should take over maintenance?\n/cc @benbjohnson @xiangli-cmu @unihorn \n. @benbjohnson this seems reasonable. I will do that.\n. This was discussed in #241. @benbjohnson If you want to clarify the language we can do that in a separate PR. Just getting a lot of questions recently :)\n. lgtm, this project is currently unmaintained however: https://github.com/goraft/raft#overview\n. This is a recent change in location. We would gladly accept a patch to clean this up.\n. Also, note, that this package is currently unmaintained: https://github.com/goraft/raft#overview\n. FYI, go-raft is no longer maintained. https://github.com/goraft/raft/pull/244\nIf you need a raft implementation I would suggest the etcd/raft library: https://godoc.org/github.com/coreos/etcd/raft\nThere are other libraries too, I just don't have the links handy.\n. @joeshaw links added. Thanks.\n. @better0332 there are known issues with go-raft. I would not suggest using go-raft for production use. See an alternative here: https://godoc.org/github.com/coreos/etcd/raft\n. @xiangli-cmu Perhaps it should just be two or three compactions with a tunable for people who need it?\n. What were the benchmark numbers from the JSON format?\n. Were tools like protobufs/gobs/etc explored? It seems like a large leap from a self describing JSON format to a hand coded binary protocol.\nFor example I could imagine a time in the not so distant future where having a wireshark dissector for this protocol will be nice. Or where adding a single field in a v2 protocol without breaking old versions will be necessary.\n. LGTM. The global protobuf log is to reduce object churn and garbage collection, right?\n. @xiangli-cmu Why are you doing this? It looks fine but I don't know why it needs to happen.\n. @benbjohnson Great, you are the owner and I added Xiang since I think he currently has merge rights on this repo.\n. What is an example of the info that will go in here?\n. LGTM :)\n. This fixes #118 reported by @kellabyte\n. @benbjohnson Good idea. It was super confusing reading: Less returns i > j \n. lgtm\n. @benbjohnson One style thing I have started doing in markdown files is one sentence per line. It is compatible with all of the editors and makes reviewing in git and github easier. Just a suggestion; no strong feelings either way.\n. lgtm, super helpful.\n. @benbjohnson Another thing to consider is even vs odd number of nodes. e.g. https://github.com/coreos/etcd/issues/149#issuecomment-23603009\n. I really like the possibilities this opens up! Don't have time to give it a proper review though.\n. Ack on the application handling log back off.\nOn Dec 29, 2013 12:02 PM, \"Ben Johnson\" notifications@github.com wrote:\n\nOverview\nThis pull request adds two new event types:\n-\nHeartbeatTimeoutEventType - Dispatched when a heartbeat times out.\n   This requires that the transporter returns a nil for timed out\n   requests.\n   -\nElectionTimeoutThresholdEventType - Dispatched when an AppendEntriesrequest comes within a given threshold of the election timeout. This\n   threshold is currently set to 80% of the election timeout.\nThis will give Raft users better insight into whether their heartbeat and\nelection timeouts are too low.\n@philips https://github.com/philips: We can add exponential backoff of\nnotification of these events at the application level. I'd rather dispatch\nall events at the go-raft library level and not filter any out.\n/cc: @xiangli-cmu https://github.com/xiangli-cmu\nYou can merge this Pull Request by running\ngit pull https://github.com/goraft/raft election-heartbeat-events\nOr view, comment on, or merge it at:\nhttps://github.com/goraft/raft/pull/144\nCommit Summary\n- Add election threshold and heartbeat timeout events.\n- Merge branch 'master' of https://github.com/goraft/raft\nFile Changes\n- M event.go https://github.com/goraft/raft/pull/144/files#diff-0(3)\n- M http_transporter.gohttps://github.com/goraft/raft/pull/144/files#diff-1(8)\n- M peer.go https://github.com/goraft/raft/pull/144/files#diff-2(1)\n- M server.go https://github.com/goraft/raft/pull/144/files#diff-3(13)\nPatch Links:\n- https://github.com/goraft/raft/pull/144.patch\n- https://github.com/goraft/raft/pull/144.diff\n. @benbjohnson +1 on making fsync something we can disable by default.\n\nPerhaps I am being naive but fsync only helps protect against data loss in a full cluster down, right? In the normal case where the current leader has a power failure but a majority of the cluster stays up then it doesn't matter if goraft is fsync'ing because the cluster will have moved on and the correctness of the leader's log is irrelevant once it rejoins.\nJust trying to wrap my head around why we are fsyncing the log at all.\n. Does this change the protocol or log format?\n. @xiangli-cmu Is there any reason for optimizing the single node case? Seems like there isn't much to gain from 4aa1042.\n. lgtm, merging.\n. Nice work :)\n. The tests look good. My problems are with the code as it was before this PR. If you want to merge this as-is it looks good to me. The other stuff can be cleaned up in a subsequent PR.\n. /cc @bcwaldon\n. @benbjohnson Yea, I guess a thresh hold event would work.\n. @benbjohnson I agree I think we should measure then consider getting rid of the sendAsync codepath. I don't know off of the top of my head why we decided to let there be an essentially infinite event backlog via sendAsync.\n. @benbjohnson Yea, exactly. :(\n. @benbjohnson How should we make this decision? It seems like we need to do a few things:\n- [ ] Make the channel size adjustable at construction\n- [ ] Benchmark how quickly goraft can drain a queue\n- [ ] Make a reasonable default\nMy hunches on the second two:\n- Based on my current observations this will have everything to do with the network, not go-raft\n- Based on say a 50ms worst case latency and 1ms average completion we get ~50 (round it to 64?)\n/cc @xiangli-cmu \n. @bcwaldon fixed.\n. go fmt'd sorry.\n. /cc @benbjohnson What do you think about this change?\n. Looks like we have quorum. ;)\nThe only other thing to consider is if we will need to break the API again for the event channel stuff.\n. @xiangli-cmu Can you please make the commit message more helpful? It is better to have information in the git history rather than on github.\n. This makes sense to me logically. I don't see what the hack is in it though.\n. @benbjohnson testing against etcd now.\n. @xiangli-cmu @benbjohnson I am having a hard time seeing why we drop the lock in 38087d2. We send false into the stopChan which means we bail without a flush(). https://github.com/goraft/raft/blob/master/peer.go#L127\nHaving a hard time seeing the deadlock.\n. 6c994074c0e2ba89ff182803b62586a60919332b looks fine to me.\n. I tested these changes with etcd master and it seems to work fine I just want to cleanup the commit message of 38087d2 since it doesn't explain what locking problem this is solving.\n. @xiangli-cmu Thank you. I see the race now. I found another quick cleanup I want to do and will pull in these two changes and cleanup the commit messages of 38087d2, send up a new PR and then merge it into etcd. Thanks!\n. @xiangli-cmu That is fine, I just ran it in my editor while reviewing code. Just do it in your PR.\n. /cc @bcwaldon this was part of the cleanup we were talking about a few days ago.\n. lgtm, this is likely to break other outstanding PRs though, right?\n. /cc @benbjohnson @xiangli-cmu \n. Merged via aed3d62. Thanks @chrislusf \n. Lgtm\n. lgtm ;)\n. agreed, much better. thanks xiang.\n. assert commit lgtm. builds pass too https://drone.io/github.com/xiangli-cmu/raft/4\n. @xiangli-cmu closed this since what?\n. I like this and it lgtm. @benbjohnson what do you think of chan chan? I haven't seen this before but it makes sense in this event loop case.\n. lgtm, besides the two nips. The naming is much better.\n. Thanks Xiang\n. lgtm.\n. thanks!\n. +1 thanks.\n. @macb In coreos/etcd#595 I was more meaning that the logging should backoff exponentially. The backoff on this side, if we add any, should be capped at a second or two.\n. Just hit this in the proxy pr, reading now.\n. lgtm! this seems like a valuable feature.\n. lgtm too. We need to document/refactor these locks.\n. @xiangli-cmu Sounds good. Thanks for taking it on.\n. dang, lgtm.\n. Lgtm\n. lgtm, merge at will.\n. lgtm once the commits are cleaned up a bit.\n. lgtm\n. Ick. We can do better by just making the ioctl syscall from Go, can't we?\n. @unihorn The constant is a constant and a kernel interface. Copying it into Go is fine. See https://github.com/coreos/go-namespaces/blob/master/namespace/linux_x86_64.go as an example.\n. @unihorn The syscall package is essentially just huge amounts of copy and paste from C header files. ;)\n. All we have to do is have the log directory set with NOCOW. This will be inherited by the file that goraft writes. Nothing should be necessary in goraft at all.\n. lgtm\n. yea, we need to make sure this gets a test first.\n. @xiangli-cmu this lgtm. I am OK merging it but @benbjohnson 's second opinion might help.\n. @xiangli-cmu What is the status on this one?\n. ping @xiangli-cmu ?\n. @xiangli-cmu @benbjohnson Can we merge this?\n. This makes sense to me. I wish the commit had a description like:\n\"If we need to truncate the file because of a decoding error then seek back to the last known good entry\"\n. lgtm\n. lgtm. @xiangli-cmu @unihorn ?\n. This LGTM. However, does it also fix thos \"empty\" messages as reported in coreos/etcd#913?\nAny objections @benbjohnson @xiangli-cmu ?\n. Fixed. Thank you.\n. This is a somewhat dangerous bug if people have been using rsync or something to backup their snapshot directory. But, in the common case there is only ever one snapshot file on disk and this is managed by go-raft.\n. @otoolep was ffe3da0 to fix 235? \n. @mre The etcd team's library is at http://godoc.org/github.com/coreos/etcd/raft and it is also used by http://godoc.org/github.com/cockroachdb/cockroach/multiraft if you are looking at options. \n. The etcd developers (@xiangli-cmu and @unihorn) have done quite a bit of maintenance on this library until now. However, as our new raft library has matured and the  0.4 branch of etcd goes into maintenance mode the goraft project needs new maintainers. Particularly since we have a few issues piling up: #239, #238, #235, #233 and #240.\nDoes any have a suggestion on who should take over maintenance?\n/cc @benbjohnson @xiangli-cmu @unihorn \n. @benbjohnson this seems reasonable. I will do that.\n. This was discussed in #241. @benbjohnson If you want to clarify the language we can do that in a separate PR. Just getting a lot of questions recently :)\n. lgtm, this project is currently unmaintained however: https://github.com/goraft/raft#overview\n. This is a recent change in location. We would gladly accept a patch to clean this up.\n. Also, note, that this package is currently unmaintained: https://github.com/goraft/raft#overview\n. FYI, go-raft is no longer maintained. https://github.com/goraft/raft/pull/244\nIf you need a raft implementation I would suggest the etcd/raft library: https://godoc.org/github.com/coreos/etcd/raft\nThere are other libraries too, I just don't have the links handy.\n. @joeshaw links added. Thanks.\n. @better0332 there are known issues with go-raft. I would not suggest using go-raft for production use. See an alternative here: https://godoc.org/github.com/coreos/etcd/raft\n. ",
    "peterbourgon": ":dango:\n. I have no idea!! Vaguely positive? :ok_hand: \n. @xiangli-cmu Pure curiosity, but what is \"your data\"\u2014the commands you send to be replicated? Can you provide some sample data?\n. @xiangli-cmu I have strong opinions about APIs :) But it's possible! Could we chat in a better venue, perhaps? IRC or email?\n. :dango:\n. I have no idea!! Vaguely positive? :ok_hand: \n. @xiangli-cmu Pure curiosity, but what is \"your data\"\u2014the commands you send to be replicated? Can you provide some sample data?\n. @xiangli-cmu I have strong opinions about APIs :) But it's possible! Could we chat in a better venue, perhaps? IRC or email?\n. ",
    "matttproud": "@xiangli-cmu Of course!  My apologies about this!  It was part of a faulty rebase against Ben's repository.  :-(   I had rerun the tests, but it appears that something was errant in my development environment.\nAnyway, if you're interested in merging https://github.com/benbjohnson/go-raft/pull/91, this should help catch all future instances of this!\n. @xiangli-cmu, do you want to fix that in this pull request or do that separately?\n. Cool!  If @benbjohnson is interested in using Travis, I will de-register the individual configuration for go-raft from my Travis account and let him set that up himself after he merges this.  I think this would be a huge benefit.\n. @benbjohnson, if you give https://travis-ci.org/matttproud/go-raft/builds a peek, it is working.  :-)  The failures are because of spurious build environment setup iterations.\nI will go ahead and disable my setup for now.\n. @xiangli-cmu Of course!  My apologies about this!  It was part of a faulty rebase against Ben's repository.  :-(   I had rerun the tests, but it appears that something was errant in my development environment.\nAnyway, if you're interested in merging https://github.com/benbjohnson/go-raft/pull/91, this should help catch all future instances of this!\n. @xiangli-cmu, do you want to fix that in this pull request or do that separately?\n. Cool!  If @benbjohnson is interested in using Travis, I will de-register the individual configuration for go-raft from my Travis account and let him set that up himself after he merges this.  I think this would be a huge benefit.\n. @benbjohnson, if you give https://travis-ci.org/matttproud/go-raft/builds a peek, it is working.  :-)  The failures are because of spurious build environment setup iterations.\nI will go ahead and disable my setup for now.\n. ",
    "erikstmartin": "Looks like these races still exist. Is this something that's looking to be addressed? Should I look into some of these and submit pull-requests?\n. Awesome! Thanks Xiang, I was going to start peeking at it today :+1: :)\n. +1\n. Looks like these races still exist. Is this something that's looking to be addressed? Should I look into some of these and submit pull-requests?\n. Awesome! Thanks Xiang, I was going to start peeking at it today :+1: :)\n. +1\n. ",
    "mattn": "@benbjohnson you are rigth. I pushed --force.\n. File.Fd() is syscall.Handle not int on windows. So it should be cast to them. But I modified to use os.File.Sync() in last comit.\n. @benbjohnson you are rigth. I pushed --force.\n. File.Fd() is syscall.Handle not int on windows. So it should be cast to them. But I modified to use os.File.Sync() in last comit.\n. ",
    "jvshahid": "Well I don't think waiting indefinitely is an option. We run into this case when a leader lose leadership while there are uncommited log entries in its logs. The next truncate operation will get rid of the log entry and it will never commit causing the command to timeout, if there were no timeout then the command will hang indefinitely, unless we added some sort of timeout+retry logic.\n. I actually wanted to comment on this issue, because i don't know what's the right way to handle this case and it's not apparent from the paper what's the standard behavior. Although the paper say that the state machine should keep track of commands that were processed in case the client reissued a command. So maybe in this case the retry logic should live in the client. What do you guys think ?\n. @xiangli-cmu is that because the stopChan https://github.com/goraft/raft/blob/master/peer.go#L82 is buffered channel ? Also why is it buffered, the comment in stopHeartbeat() isn't very clear.\n. @xiangli-cmu yeah, but if stopChan wasn't buffered, the write to stopChan in stopHeartbeat will block until the flush call returns and the select is executed. Does that make sense ?\n. @xiangli-cmu yeah i saw the comment, but i can't think of a scenario where the leader will block. stopChan is read in the heartbeat loop, unless you call stopHeartbeat twice or the flush is hanging (both of which can be avoided) stopChan will be read in the next iteration.\n. @xiangli-cmu i actually just modified the channel to not be buffered and now the tests are hanging. i'll take a deeper look later. but if you can elaborate that will be very useful.\n. yes\n. @xiangli-cmu just added a commit to make the stopChan unbuffered. can you take a look and let me know what you think. Thanks.\n. @xiangli-cmu awesome. please close this pull request when the commits are merged into master. Thanks.\n. @xiangli-cmu i saw your commits, so i rebased my branch on the new master and forced push. It should be the same set of changes.\n. @xiangli-cmu do you mean the http_transporter_test.go ?\n. cool, i'll take a look.\nOn Mon, Nov 11, 2013 at 4:43 PM, Xiang Li notifications@github.com wrote:\n\n@jvshahid https://github.com/jvshahid Yes.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/goraft/raft/pull/126#issuecomment-28242373\n.\n. yes, it's my email jvshahid@gmail.com\n\nOn Mon, Nov 11, 2013 at 4:49 PM, Xiang Li notifications@github.com wrote:\n\n@jvshahid https://github.com/jvshahid do you have a gtalk that i can\ncontact?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/goraft/raft/pull/126#issuecomment-28242946\n.\n. I rebased my commits on top of the current master and made sure that all the tests are passing. can you guys take a look and merge it in if you think it's ok. Thanks.\n. Do you know when will that pr be merged ? This issue is causing our test suite to fail.\n. Thanks :+1: \n. I'm not sure you got a notification for my comments since the diff is outdated. FYI, i pushed a change to use url.Parse and u.Path and changed all references to Sprintf to use the new method.\n. Fine with me too\n. I think you should add a test to make sure the bug is fixed and to protect from future regressions. Looking at the patch, looks like it will solve the problem, but again, i think a test case is important even if the bug is fixed.\n. I don't understand what do you mean by state machine. You can just start couple of raft servers let them join each other, force a compaction on all nodes, restart and make sure the cluster is up. Am I missing something ?\n. I don't think this pr will get merged in with all the imports changed to reference otoolep instead of goraft. Also, why does this pr touch the protobuf files ?\n. ping, can someone confirm this bug ?\n. Well I don't think waiting indefinitely is an option. We run into this case when a leader lose leadership while there are uncommited log entries in its logs. The next truncate operation will get rid of the log entry and it will never commit causing the command to timeout, if there were no timeout then the command will hang indefinitely, unless we added some sort of timeout+retry logic.\n. I actually wanted to comment on this issue, because i don't know what's the right way to handle this case and it's not apparent from the paper what's the standard behavior. Although the paper say that the state machine should keep track of commands that were processed in case the client reissued a command. So maybe in this case the retry logic should live in the client. What do you guys think ?\n. @xiangli-cmu is that because the stopChan https://github.com/goraft/raft/blob/master/peer.go#L82 is buffered channel ? Also why is it buffered, the comment in stopHeartbeat() isn't very clear.\n. @xiangli-cmu yeah, but if stopChan wasn't buffered, the write to stopChan in stopHeartbeat will block until the flush call returns and the select is executed. Does that make sense ?\n. @xiangli-cmu yeah i saw the comment, but i can't think of a scenario where the leader will block. stopChan is read in the heartbeat loop, unless you call stopHeartbeat twice or the flush is hanging (both of which can be avoided) stopChan will be read in the next iteration.\n. @xiangli-cmu i actually just modified the channel to not be buffered and now the tests are hanging. i'll take a deeper look later. but if you can elaborate that will be very useful.\n. yes\n. @xiangli-cmu just added a commit to make the stopChan unbuffered. can you take a look and let me know what you think. Thanks.\n. @xiangli-cmu awesome. please close this pull request when the commits are merged into master. Thanks.\n. @xiangli-cmu i saw your commits, so i rebased my branch on the new master and forced push. It should be the same set of changes.\n. @xiangli-cmu do you mean the http_transporter_test.go ?\n. cool, i'll take a look.\n\nOn Mon, Nov 11, 2013 at 4:43 PM, Xiang Li notifications@github.com wrote:\n\n@jvshahid https://github.com/jvshahid Yes.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/goraft/raft/pull/126#issuecomment-28242373\n.\n. yes, it's my email jvshahid@gmail.com\n\nOn Mon, Nov 11, 2013 at 4:49 PM, Xiang Li notifications@github.com wrote:\n\n@jvshahid https://github.com/jvshahid do you have a gtalk that i can\ncontact?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/goraft/raft/pull/126#issuecomment-28242946\n.\n. I rebased my commits on top of the current master and made sure that all the tests are passing. can you guys take a look and merge it in if you think it's ok. Thanks.\n. Do you know when will that pr be merged ? This issue is causing our test suite to fail.\n. Thanks :+1: \n. I'm not sure you got a notification for my comments since the diff is outdated. FYI, i pushed a change to use url.Parse and u.Path and changed all references to Sprintf to use the new method.\n. Fine with me too\n. I think you should add a test to make sure the bug is fixed and to protect from future regressions. Looking at the patch, looks like it will solve the problem, but again, i think a test case is important even if the bug is fixed.\n. I don't understand what do you mean by state machine. You can just start couple of raft servers let them join each other, force a compaction on all nodes, restart and make sure the cluster is up. Am I missing something ?\n. I don't think this pr will get merged in with all the imports changed to reference otoolep instead of goraft. Also, why does this pr touch the protobuf files ?\n. ping, can someone confirm this bug ?\n. \n",
    "kellabyte": "\"we've found the maximum effective cluster size to be around 9 nodes. We\ntypically suggest a 5 node cluster for performance reasons though.\"\nMight be worthwhile giving some context as to what the setup configuration\nwas. Are these 9 and 5 node clusters on the same LAN or across WAN? What\nwas the ping like between them?\nI know there are a lot of variables at play but I think its good to give\nsome idea of the environment the suggestion is coming from.\nOn Fri, Oct 25, 2013 at 12:51 PM, Ben Johnson notifications@github.comwrote:\n\nPer an e-mail conversation with @pvo https://github.com/pvo, I added a\nsection to the documentation called Raft in Practice.\n@pvo https://github.com/pvo Can you tell me if this helps to answer\nyour question? Is there anything I'm missing?\n@ongardie https://github.com/ongardie @kellabytehttps://github.com/kellabyte\n@xiangli-cmu https://github.com/xiangli-cmu @philipshttps://github.com/philipsCan you guys give me a technical review? It's not very long.\nHere's the pretty printed versionhttps://github.com/goraft/raft/blob/docs/README.md#raft-in-practice\n.\nYou can merge this Pull Request by running\ngit pull https://github.com/goraft/raft docs\nOr view, comment on, or merge it at:\nhttps://github.com/goraft/raft/pull/124\nCommit Summary\n- Add 'Raft in Practice' to README.\nFile Changes\n- M README.md https://github.com/goraft/raft/pull/124/files#diff-0(27)\nPatch Links:\n- https://github.com/goraft/raft/pull/124.patch\n- https://github.com/goraft/raft/pull/124.diff\n. \"we've found the maximum effective cluster size to be around 9 nodes. We\ntypically suggest a 5 node cluster for performance reasons though.\"\n\nMight be worthwhile giving some context as to what the setup configuration\nwas. Are these 9 and 5 node clusters on the same LAN or across WAN? What\nwas the ping like between them?\nI know there are a lot of variables at play but I think its good to give\nsome idea of the environment the suggestion is coming from.\nOn Fri, Oct 25, 2013 at 12:51 PM, Ben Johnson notifications@github.comwrote:\n\nPer an e-mail conversation with @pvo https://github.com/pvo, I added a\nsection to the documentation called Raft in Practice.\n@pvo https://github.com/pvo Can you tell me if this helps to answer\nyour question? Is there anything I'm missing?\n@ongardie https://github.com/ongardie @kellabytehttps://github.com/kellabyte\n@xiangli-cmu https://github.com/xiangli-cmu @philipshttps://github.com/philipsCan you guys give me a technical review? It's not very long.\nHere's the pretty printed versionhttps://github.com/goraft/raft/blob/docs/README.md#raft-in-practice\n.\nYou can merge this Pull Request by running\ngit pull https://github.com/goraft/raft docs\nOr view, comment on, or merge it at:\nhttps://github.com/goraft/raft/pull/124\nCommit Summary\n- Add 'Raft in Practice' to README.\nFile Changes\n- M README.md https://github.com/goraft/raft/pull/124/files#diff-0(27)\nPatch Links:\n- https://github.com/goraft/raft/pull/124.patch\n- https://github.com/goraft/raft/pull/124.diff\n. \n",
    "pvo": "Thanks all. Super helpful. \n. Thanks all. Super helpful. \n. ",
    "densone": "Works .  Ben, what does  entry.commit = nil do?\n. Works .  Ben, what does  entry.commit = nil do?\n. ",
    "JensRantil": "Ok! Thanks for quick response!\n. Ok! Thanks for quick response!\n. ",
    "baruch": "The issue I'm trying to fix here is the possibility for data loss and consistency loss if the power is lost in mid-operation or after the operation but before the filesystem flushed the data to the disk. The filesystem may delay the actual write by 5 (ext3) to 30 (xfs) seconds and there is a real chance for losing consistency in that case.\nThis will however slow performance.\nAnother thing found is that there may be a resource leak if there is a write error when switching files, the temporary file is not closed nor deleted.\n. I generally agree but there are some fine points that need to be thought about, especially the change of the flushCommitIndex.\nFeel free to take only the parts you think are really necessary and ignore those that you're not sure about.\n. The issue I'm trying to fix here is the possibility for data loss and consistency loss if the power is lost in mid-operation or after the operation but before the filesystem flushed the data to the disk. The filesystem may delay the actual write by 5 (ext3) to 30 (xfs) seconds and there is a real chance for losing consistency in that case.\nThis will however slow performance.\nAnother thing found is that there may be a resource leak if there is a write error when switching files, the temporary file is not closed nor deleted.\n. I generally agree but there are some fine points that need to be thought about, especially the change of the flushCommitIndex.\nFeel free to take only the parts you think are really necessary and ignore those that you're not sure about.\n. ",
    "gdb": "Awesome. I'm working on Stripe CTF3 (https://stripe-ctf.com/)\u00a0\u2014 I guess this is a hint that there will be a level involving Raft :). It should be public in about a month; I'll send you a link once we have something public.\n. Hah. CTF3 is going to be a bit different \u2014 it'll be more of a systems engineering contest. Thanks for the great Raft implementation, BTW.\n. Awesome. I'm working on Stripe CTF3 (https://stripe-ctf.com/)\u00a0\u2014 I guess this is a hint that there will be a level involving Raft :). It should be public in about a month; I'll send you a link once we have something public.\n. Hah. CTF3 is going to be a bit different \u2014 it'll be more of a systems engineering contest. Thanks for the great Raft implementation, BTW.\n. ",
    "mreiferson": "this is pretty nice :+1:\n. Hmmm, curious as to the thinking behind the buffer of 256?\nI would argue that it might be prudent to reduce the size (or eliminate) this buffer entirely... This would propagate back-pressure up the chain (all the way to clients sending commands, I assume).\nBut, it's likely that I don't know enough about the codebase to understand how it's used :smile:\n. Well, it's not really possible to discriminate between a failed or unresponsive server (for example, blocked on disk IO).   Given that dilemma, would it not be \"correct\" for a heartbeat to fail due to resource contention?  (i.e. wouldn't we want that unresponsive peer demoted, for example, if it was the leader?)\nMy general feeling is that I would want back-pressure as a result of any kind of resource contention propagated to peers (and clients) so that they can make decisions about how to proceed.\nI think I need to better understand the semantics of sendAsync(), though.  I'll poke around later this evening.\n. hah, right :smile:\nThe benefit of removing this hidden internal buffer (and others?) is that when disk IO degrades the behavior will be well defined and unsurprising.\n. Is the intended purpose of the buffer to be able to batch work to reduce round-trip overhead of more/smaller AE requests to peers?\nIf not, and it is simply a workaround for potential resource contention, the other option is to:\n- [ ] remove the buffer\nEnd-users can still work around this potential issue by adjusting timeouts, right?\n. Agreed, I'm in favor of refactoring in order to remove the buffer.  I'm assuming this is related to your https://github.com/goraft/raft/pull/167#issuecomment-33226692, which I'm also :+1: on.\n. @benbjohnson I'm not sure.  I think, to start off with the safest and simplest approach would be for heartbeats to not be asynchronous at all and therefore can be trusted as a truer reflection of a peer's liveness.  (Perhaps we should move this discussion to #173.)\n. (edit: meant \"heartbeats\" not \"goroutines\" :smile:)\n. right, \"async\" and \"sync\" are not really the right words - a better way to phrase what I'm thinking is \"not in complete isolation\"\n. @xiangli-cmu I'll try to find some time later to detail what I'm thinking on #173 - I suspect, based on your pull requests and comments on other issues, that we're interested in achieving the same end result :smile:\n. It might be beneficial to take a first pass that focusses entirely on correctness (and simplicity) of the code in terms of memory safety, even if the cost is significant in terms of performance.\nIn many cases this may mean less granular locking, less work performed in dueling goroutines, etc.   I think that's fine to start off with.\n. :+1: to state machine\n. What about request_types?  data always feels like a catchall.\n(p.s naming is hard :frowning:)\n. right, you could argue that it's fine for a pkg called request_types to also include the responses...\n. this is pretty nice :+1:\n. Hmmm, curious as to the thinking behind the buffer of 256?\nI would argue that it might be prudent to reduce the size (or eliminate) this buffer entirely... This would propagate back-pressure up the chain (all the way to clients sending commands, I assume).\nBut, it's likely that I don't know enough about the codebase to understand how it's used :smile:\n. Well, it's not really possible to discriminate between a failed or unresponsive server (for example, blocked on disk IO).   Given that dilemma, would it not be \"correct\" for a heartbeat to fail due to resource contention?  (i.e. wouldn't we want that unresponsive peer demoted, for example, if it was the leader?)\nMy general feeling is that I would want back-pressure as a result of any kind of resource contention propagated to peers (and clients) so that they can make decisions about how to proceed.\nI think I need to better understand the semantics of sendAsync(), though.  I'll poke around later this evening.\n. hah, right :smile:\nThe benefit of removing this hidden internal buffer (and others?) is that when disk IO degrades the behavior will be well defined and unsurprising.\n. Is the intended purpose of the buffer to be able to batch work to reduce round-trip overhead of more/smaller AE requests to peers?\nIf not, and it is simply a workaround for potential resource contention, the other option is to:\n- [ ] remove the buffer\nEnd-users can still work around this potential issue by adjusting timeouts, right?\n. Agreed, I'm in favor of refactoring in order to remove the buffer.  I'm assuming this is related to your https://github.com/goraft/raft/pull/167#issuecomment-33226692, which I'm also :+1: on.\n. @benbjohnson I'm not sure.  I think, to start off with the safest and simplest approach would be for heartbeats to not be asynchronous at all and therefore can be trusted as a truer reflection of a peer's liveness.  (Perhaps we should move this discussion to #173.)\n. (edit: meant \"heartbeats\" not \"goroutines\" :smile:)\n. right, \"async\" and \"sync\" are not really the right words - a better way to phrase what I'm thinking is \"not in complete isolation\"\n. @xiangli-cmu I'll try to find some time later to detail what I'm thinking on #173 - I suspect, based on your pull requests and comments on other issues, that we're interested in achieving the same end result :smile:\n. It might be beneficial to take a first pass that focusses entirely on correctness (and simplicity) of the code in terms of memory safety, even if the cost is significant in terms of performance.\nIn many cases this may mean less granular locking, less work performed in dueling goroutines, etc.   I think that's fine to start off with.\n. :+1: to state machine\n. What about request_types?  data always feels like a catchall.\n(p.s naming is hard :frowning:)\n. right, you could argue that it's fine for a pkg called request_types to also include the responses...\n. ",
    "bcwaldon": "@xiangli-cmu thanks!\n. +1\n. lgtm - there were a few unrelated fmt changes added to the PR, but I'm cool with it\n. @otoolep You actually need to pad both fields, as you need to be able to sort by the last index in the event that the term appears more than once. \n. ...and we need to deal with snapshots that are already out there.\n. Ah, I didn't realize there was a PR.\nhttps://github.com/goraft/raft/pull/237\n. Handling backwards-compatibility could look like so:\n```\nfunc NewSnapshotFileNames(names []string) ([]SnapshotFileName, error) {\n    s := make([]SnapshotFileName, 0)\n    for _, n := range names {\n        trimmed := strings.TrimSuffix(n, \".ss\")\n        if trimmed == n {\n            return nil, fmt.Errorf(\"file %q does not have .ss extension\", n)\n        }\n    parts := strings.SplitN(trimmed, \"_\", 2)\n    if len(parts) != 2 {\n        return nil, fmt.Errorf(\"unrecognized file name format %q\", n)\n    }\n\n    fn := SnapshotFileName{FileName: n}\n\n    var err error\n    fn.Term, err = strconv.ParseUint(parts[0], 10, 64)\n    if err != nil {\n        return nil, fmt.Errorf(\"unable to parse term from filename %q: %v\", err)\n    }\n\n    fn.Index, err = strconv.ParseUint(parts[1], 10, 64)\n    if err != nil {\n        return nil, fmt.Errorf(\"unable to parse index from filename %q: %v\", err)\n    }\n\n    s = append(s, fn)\n}\n\nsortable := SnapshotFileNames(s)\nsort.Sort(&sortable)\nreturn s, nil\n\n}\ntype SnapshotFileNames []SnapshotFileName\ntype SnapshotFileName struct {\n    FileName string\n    Term     uint64\n    Index    uint64\n}\nfunc (n SnapshotFileNames) Less(i, j int) bool {\n    iTerm, iIndex := (n)[i].Term, (n)[i].Index\n    jTerm, jIndex := (n)[j].Term, (*n)[j].Index\n    return iTerm < jTerm || (iTerm == jTerm && iIndex < jIndex)\n}\nfunc (n SnapshotFileNames) Swap(i, j int) {\n    (n)[i], (n)[j] = (n)[j], (*n)[i]\n}\nfunc (n SnapshotFileNames) Len() int {\n    return len([]SnapshotFileName(n))\n}\n```\n. And if we've already established one filename format out there, I question why we want to change that if we still need to deal with backwards-compatibility.\n. @xiangli-cmu thanks!\n. +1\n. lgtm - there were a few unrelated fmt changes added to the PR, but I'm cool with it\n. @otoolep You actually need to pad both fields, as you need to be able to sort by the last index in the event that the term appears more than once. \n. ...and we need to deal with snapshots that are already out there.\n. Ah, I didn't realize there was a PR.\nhttps://github.com/goraft/raft/pull/237\n. Handling backwards-compatibility could look like so:\n```\nfunc NewSnapshotFileNames(names []string) ([]SnapshotFileName, error) {\n    s := make([]SnapshotFileName, 0)\n    for _, n := range names {\n        trimmed := strings.TrimSuffix(n, \".ss\")\n        if trimmed == n {\n            return nil, fmt.Errorf(\"file %q does not have .ss extension\", n)\n        }\n    parts := strings.SplitN(trimmed, \"_\", 2)\n    if len(parts) != 2 {\n        return nil, fmt.Errorf(\"unrecognized file name format %q\", n)\n    }\n\n    fn := SnapshotFileName{FileName: n}\n\n    var err error\n    fn.Term, err = strconv.ParseUint(parts[0], 10, 64)\n    if err != nil {\n        return nil, fmt.Errorf(\"unable to parse term from filename %q: %v\", err)\n    }\n\n    fn.Index, err = strconv.ParseUint(parts[1], 10, 64)\n    if err != nil {\n        return nil, fmt.Errorf(\"unable to parse index from filename %q: %v\", err)\n    }\n\n    s = append(s, fn)\n}\n\nsortable := SnapshotFileNames(s)\nsort.Sort(&sortable)\nreturn s, nil\n\n}\ntype SnapshotFileNames []SnapshotFileName\ntype SnapshotFileName struct {\n    FileName string\n    Term     uint64\n    Index    uint64\n}\nfunc (n SnapshotFileNames) Less(i, j int) bool {\n    iTerm, iIndex := (n)[i].Term, (n)[i].Index\n    jTerm, jIndex := (n)[j].Term, (*n)[j].Index\n    return iTerm < jTerm || (iTerm == jTerm && iIndex < jIndex)\n}\nfunc (n SnapshotFileNames) Swap(i, j int) {\n    (n)[i], (n)[j] = (n)[j], (*n)[i]\n}\nfunc (n SnapshotFileNames) Len() int {\n    return len([]SnapshotFileName(n))\n}\n```\n. And if we've already established one filename format out there, I question why we want to change that if we still need to deal with backwards-compatibility.\n. ",
    "wangsikuan": "@xiangli-cmu Yes, I have noticed that you are Chinese, next time when I need some help on learning go-raft, I will contact you, thanks! And golang and raft are cool, thanks for your effort to make goraft better.\n. @xiangli-cmu Yes, I have noticed that you are Chinese, next time when I need some help on learning go-raft, I will contact you, thanks! And golang and raft are cool, thanks for your effort to make goraft better.\n. ",
    "nemosupremo": "Actually the more I look at it, the more I'm seeing that there are problems with my code. Maybe its because the documentation is still nascent but if you try to Do(command) anything as a result of event callback, you can end up in a deadlock. Its actually obvious once you read the code and it may be my fault for not being properly tuned with the library. \nThis may be a bad idea, but what if event callbacks ran in their own goroutine (or pass events over channels)? It seems that currently, that care has to be taken when working on the raft goroutine from outside the libary, else you might end up in a deadlock.\n. Actually the more I look at it, the more I'm seeing that there are problems with my code. Maybe its because the documentation is still nascent but if you try to Do(command) anything as a result of event callback, you can end up in a deadlock. Its actually obvious once you read the code and it may be my fault for not being properly tuned with the library. \nThis may be a bad idea, but what if event callbacks ran in their own goroutine (or pass events over channels)? It seems that currently, that care has to be taken when working on the raft goroutine from outside the libary, else you might end up in a deadlock.\n. ",
    "bketelsen": "Fine with me!\nSent from my iPhone\n\nOn Jan 23, 2014, at 4:36 PM, Ben Johnson notifications@github.com wrote:\n@philips I agree that \"interval\" makes more sense than \"timeout\". I feel like I used timeout originally because it was in the original Raft paper. I can't find a copy of the original anymore though.\nThe biggest issue that I have is that it breaks the API and therefore other user's integration. go-raft isn't at v1.0.0 yet so the API isn't set in stone but I'd like to hear from other users before merging.\n@bketelsen @erikstmartin @pauldix @jvshahid What do you guys think? Is it a problem for you if we make this aesthetic change?\n\u2014\nReply to this email directly or view it on GitHub.\n. Fine with me!\n\nSent from my iPhone\n\nOn Jan 23, 2014, at 4:36 PM, Ben Johnson notifications@github.com wrote:\n@philips I agree that \"interval\" makes more sense than \"timeout\". I feel like I used timeout originally because it was in the original Raft paper. I can't find a copy of the original anymore though.\nThe biggest issue that I have is that it breaks the API and therefore other user's integration. go-raft isn't at v1.0.0 yet so the API isn't set in stone but I'd like to hear from other users before merging.\n@bketelsen @erikstmartin @pauldix @jvshahid What do you guys think? Is it a problem for you if we make this aesthetic change?\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "zenazn": "This commit introduces a deadlock over the server lock:\n- WLOG Node1 is the leader. It has a goroutine attempting to flush() data to Node2.\n- Node1 receives a response to a request from Node3. It discovers that an election has occurred, and that its term is no longer valid. It steps down.\n- As part of the step down, holding the server lock, it attempts to synchronously shoot down the heartbeat goroutines (https://github.com/goraft/raft/blob/b545b8d44e8d1c24a590e50b11fa832089f9f12c/server.go#L492-L506)\n- Meanwhile, the goroutine calling flush() to Node2, knowing nothing of the above, attempts to send an AppendEntries request. It calls p.server.Transporter(), which attempts to get a read lock on the server. (there are many other calls, like ones to DispatchEvent() and Term(), which similarly grab read locks)\n- Neither the flush() goroutine nor the setCurrentTerm() goroutine can proceed.\n. We discovered this bug in the process of writing the reference solution to Level 4 of Stripe's CTF (https://stripe-ctf.com). The network simulator I wrote for the event, Octopus (https://github.com/stripe-ctf/octopus), turns out to be very good at reproducing this particular bug :)\nThe patch in #167 introduces a deadlock, which I'll comment on there.\n. This commit introduces a deadlock over the server lock:\n- WLOG Node1 is the leader. It has a goroutine attempting to flush() data to Node2.\n- Node1 receives a response to a request from Node3. It discovers that an election has occurred, and that its term is no longer valid. It steps down.\n- As part of the step down, holding the server lock, it attempts to synchronously shoot down the heartbeat goroutines (https://github.com/goraft/raft/blob/b545b8d44e8d1c24a590e50b11fa832089f9f12c/server.go#L492-L506)\n- Meanwhile, the goroutine calling flush() to Node2, knowing nothing of the above, attempts to send an AppendEntries request. It calls p.server.Transporter(), which attempts to get a read lock on the server. (there are many other calls, like ones to DispatchEvent() and Term(), which similarly grab read locks)\n- Neither the flush() goroutine nor the setCurrentTerm() goroutine can proceed.\n. We discovered this bug in the process of writing the reference solution to Level 4 of Stripe's CTF (https://stripe-ctf.com). The network simulator I wrote for the event, Octopus (https://github.com/stripe-ctf/octopus), turns out to be very good at reproducing this particular bug :)\nThe patch in #167 introduces a deadlock, which I'll comment on there.\n. ",
    "mathiasbynens": "Permanent link to those lines of code: https://github.com/goraft/raft/blob/0a20921dcb19e24e19794ce70a43bc9aadff3732/server.go#L904-920\n. Permanent link to those lines of code: https://github.com/goraft/raft/blob/0a20921dcb19e24e19794ce70a43bc9aadff3732/server.go#L904-920\n. ",
    "ayende": "Yes, it does\n[image: RavenDB Conference] http://conference.ravendb.net\nOren Eini\nCEO\nMobile: + 972-52-548-6969\nOffice:  + 972-4-674-7811\nFax:      + 972-153-4622-7811\nOn Tue, Feb 25, 2014 at 3:58 PM, Ben Johnson notifications@github.comwrote:\n\nThanks for catching that. I added a fix to #192https://github.com/goraft/raft/pull/192.\nLook ok?\n\nReply to this email directly or view it on GitHubhttps://github.com/goraft/raft/issues/191#issuecomment-36009278\n.\n. Looks fine, yes.\n\n[image: RavenDB Conference] http://conference.ravendb.net\nOren Eini\nCEO\nMobile: + 972-52-548-6969\nOffice:  + 972-4-674-7811\nFax:      + 972-153-4622-7811\nOn Tue, Feb 25, 2014 at 3:57 PM, Ben Johnson notifications@github.comwrote:\n\nPer @ayende https://github.com/ayende, a log entry can be partially\nread which could cause corruption or data loss. This changes the read to\nensure that the expected number of bytes are read.\nFix: #191 https://github.com/goraft/raft/issues/191\n/cc @xiangli-cmu https://github.com/xiangli-cmu\nYou can merge this Pull Request by running\ngit pull https://github.com/benbjohnson/raft read-full\nOr view, comment on, or merge it at:\nhttps://github.com/goraft/raft/pull/192\nCommit Summary\n- Change LogEntry.Decode() to use io.ReadFull().\nFile Changes\n- M log_entry.gohttps://github.com/goraft/raft/pull/192/files#diff-0(2)\nPatch Links:\n- https://github.com/goraft/raft/pull/192.patch\n- https://github.com/goraft/raft/pull/192.diff\n\nReply to this email directly or view it on GitHubhttps://github.com/goraft/raft/pull/192\n.\n. Yes, it does\n\n[image: RavenDB Conference] http://conference.ravendb.net\nOren Eini\nCEO\nMobile: + 972-52-548-6969\nOffice:  + 972-4-674-7811\nFax:      + 972-153-4622-7811\nOn Tue, Feb 25, 2014 at 3:58 PM, Ben Johnson notifications@github.comwrote:\n\nThanks for catching that. I added a fix to #192https://github.com/goraft/raft/pull/192.\nLook ok?\n\nReply to this email directly or view it on GitHubhttps://github.com/goraft/raft/issues/191#issuecomment-36009278\n.\n. Looks fine, yes.\n\n[image: RavenDB Conference] http://conference.ravendb.net\nOren Eini\nCEO\nMobile: + 972-52-548-6969\nOffice:  + 972-4-674-7811\nFax:      + 972-153-4622-7811\nOn Tue, Feb 25, 2014 at 3:57 PM, Ben Johnson notifications@github.comwrote:\n\nPer @ayende https://github.com/ayende, a log entry can be partially\nread which could cause corruption or data loss. This changes the read to\nensure that the expected number of bytes are read.\nFix: #191 https://github.com/goraft/raft/issues/191\n/cc @xiangli-cmu https://github.com/xiangli-cmu\nYou can merge this Pull Request by running\ngit pull https://github.com/benbjohnson/raft read-full\nOr view, comment on, or merge it at:\nhttps://github.com/goraft/raft/pull/192\nCommit Summary\n- Change LogEntry.Decode() to use io.ReadFull().\nFile Changes\n- M log_entry.gohttps://github.com/goraft/raft/pull/192/files#diff-0(2)\nPatch Links:\n- https://github.com/goraft/raft/pull/192.patch\n- https://github.com/goraft/raft/pull/192.diff\n\nReply to this email directly or view it on GitHubhttps://github.com/goraft/raft/pull/192\n.\n. \n",
    "macb": "I was thinking about that but wasn't sure what the arbitrary limit should be\nOn Thu, Feb 27, 2014 at 7:45 PM, Diego Ongaro notifications@github.com\nwrote:\n\nIf indeed the backoff is desirable, have you considered placing a limit on the timeout? The concern I have is that a server could be down for arbitrary amounts of time, sending its timeout through the roof. Then, when it came back, it'd be ignored for an unnecessary period of time.\nReply to this email directly or view it on GitHub:\nhttps://github.com/goraft/raft/pull/193#issuecomment-36319475\n. @philips understandable. I had originally looked into logging backoff for failed heartbeats but didn't see a neat way to approach that. @xiangli-cmu had mentioned heartbeat probing back-off as well and it seemed like it'd kill two birds with one stone.\n\nA limit definitely makes sense, but I didn't want to do much else without getting feedback from more involved devs.\n. I was thinking about that but wasn't sure what the arbitrary limit should be\nOn Thu, Feb 27, 2014 at 7:45 PM, Diego Ongaro notifications@github.com\nwrote:\n\nIf indeed the backoff is desirable, have you considered placing a limit on the timeout? The concern I have is that a server could be down for arbitrary amounts of time, sending its timeout through the roof. Then, when it came back, it'd be ignored for an unnecessary period of time.\nReply to this email directly or view it on GitHub:\nhttps://github.com/goraft/raft/pull/193#issuecomment-36319475\n. @philips understandable. I had originally looked into logging backoff for failed heartbeats but didn't see a neat way to approach that. @xiangli-cmu had mentioned heartbeat probing back-off as well and it seemed like it'd kill two birds with one stone.\n\nA limit definitely makes sense, but I didn't want to do much else without getting feedback from more involved devs.\n. ",
    "yichengq": "@xiangli-cmu I cherry-pick it into etcd, and etcd passes all its tests.\n. @xiangli-cmu Yes, I think so. It is great!! :)\n. Excited to see the patch!\nThe state of raft changes in this way now:\nStopped -> Initialized -> Stopped -> Follower/Candidate/Leader\nIt is kind of weird to see 2 Stopped which contain different meanings.\n. When NewServer, the state of server is Stopped.\nAnd it needs to Init first, which makes its state become Initialized.\nAnd it starts running now, becomes Follower/Candidate/Leader.\nWhen it is stopped on some events, it becomes Stopped.\nSo there exist two types of Stopped, but they have different meanings.\nI think we should add another stage such as New/Uninitialized.\n. @benbjohnson I think the most important thing is to keep the logic of raft clear.\nBut maybe we could integrate Init into NewServer. Ideas? @xiangli-cmu \n. lgtm to me then. Could you also review this? @benbjohnson \nI really want to bump it into etcd and fix the test error.\n. coreos/etcd#639\n. coreos/etcd#643\n. @xiangli-cmu Silly me. Thanks for reminder!\n@benbjohnson Could you review this also?\n. @benbjohnson It is the stuff about conf file.\nRaft will write latest commit index into conf file only when AddPeer and RemovePeer.\nFor now, I want to update commit index without using AddPeer and RemovePeer. It could make raft to replay the log that far when restart.\nIt is used now because etcd may update peer ip through new join command, and it needs to replay the log that far to get the latest peer ip.\n. lgtm\n. These changes look good to me.\n@xiangli-cmu I have tried to bump it into etcd, and it works well.\n. I find it out!\nBecause node 3 joins the cluster and cluster size is 2, node 1 has to get the vote from node 3 to add node2,4,5.\nIt is the problem of etcd. Sorry for confusion.\n. @xiangli-cmu I have tested it in etcd, and it could pass the tests.\n. For raft algorithm, here is the mailing list: raft-dev@googlegroups.com\nYou could also open issues here. :)\nI think etcd is a good example for this:\nhttps://github.com/coreos/etcd/blob/master/server/peer_server.go#L282\nhttps://github.com/coreos/etcd/blob/master/server/peer_server.go#L292\nhttps://github.com/coreos/etcd/blob/master/server/peer_server.go#L305\n. @benbjohnson Check it!\nI understand the concerns totally.\nOne thing that I think is that each goroutine of ours should not hang a long time. Because the goroutine always needs to receive new requests from other peers, it is a great hurt if the goroutine is stuck on something for a long time.\nAnd we could fix them step by step, considering it will be used mainly for etcd testing only now.\nSo I think it is totally ok for this PR.\n. @xiangli-cmu @benbjohnson Updated it! Does it look better to you now, xiang?\n@bmizerany Could you give some advices on the new struct that manages goroutines?\n. @bmizerany It is also used to wait for all goroutines to finish in Stop. This is the functionality I want to add.\n. @bmizerany Based on our offline discussion, it is mainly for sensible Stop function, and etcd testing works well using etcd instance(and get rid of process).\n. I revert the goroutine manager now.\n@xiangli-cmu @bmizerany \n. Rebased.\nI just cannot see the merge status for this repo. I am gonna set the drone.\n. @xiangli-cmu Refreshed.\n. @xiangli-cmu updated.\n. I think in general, we should check whether or not s.state == state and return it at very first time. But it seems that the function also takes charge of reporting leader change, so I am unsure about it.\n. But if it doesn't impact the leader change report, we could make do the s.state == state check and return it if false at the start of this function. I think that could make logic easier.\n. @xiangli-cmu Could it be in Snapshotting state when processAppendEntriesRequest ? If so, the if condition should consider about this also.\n. @xiangli-cmu lgtm then!\n. @philips But that will use some fixed number. I am unsure about that.\n. @philips For constant as BTRFS_SUPER_MAGIC, we need to rewrite them in our own code. It may be dangerous to do that. But that could make the code much simpler.\n. exactly. I would close this issue then.\n. @xiangli-cmu We may don't need that API.\nLet me try to implement it in etcd first.\n. Would it be better if we move the code to initialize stopped in Init?\nI have concerns about it because it changed the boot behavior a little.\nRaft will reject incoming requests if it has not been started.\nBut I like this method better.\nI think this is more reasonable.\nIf we take this way, I think we should info users that they'd better to ensure start raft before sending requests.\n. lgtm then.\n. If it make the detector happy, lgtm then.\n. I am a little confused of this PR.\n1. I don't think all instances have to have the same commitIndex and execute them in the same batch way. So it may result in different state machines.\n2. And I think raft should know nothing about JoinCommand. But this is a long-term issue.\n3. Etcd may think the join command succeeds then, because it has been commited. This could confuse the nodes by letting them think that they have been joined.\n. @xiangli-cmu Sorry, I misunderstand the code: it doesn't skip the commits, but delay the execution of them to the next commit time.\nIt should work.\n. @xiangli-cmu @philips\n. @xiangli-cmu Based on our discussion, I make the above changes.\n. refreshed.\n. I guess it might want to filter out io.ErrNotExist.\n@xiangli-cmu \n. @wankai It may be a new raft machine, and doesn't have that file yet.\n. I think @xiang90 is working on https://github.com/coreos/etcd/tree/master/raft, and @benbjohnson is working on https://github.com/influxdb/influxdb/tree/master/raft\n. @xiangli-cmu I cherry-pick it into etcd, and etcd passes all its tests.\n. @xiangli-cmu Yes, I think so. It is great!! :)\n. Excited to see the patch!\nThe state of raft changes in this way now:\nStopped -> Initialized -> Stopped -> Follower/Candidate/Leader\nIt is kind of weird to see 2 Stopped which contain different meanings.\n. When NewServer, the state of server is Stopped.\nAnd it needs to Init first, which makes its state become Initialized.\nAnd it starts running now, becomes Follower/Candidate/Leader.\nWhen it is stopped on some events, it becomes Stopped.\nSo there exist two types of Stopped, but they have different meanings.\nI think we should add another stage such as New/Uninitialized.\n. @benbjohnson I think the most important thing is to keep the logic of raft clear.\nBut maybe we could integrate Init into NewServer. Ideas? @xiangli-cmu \n. lgtm to me then. Could you also review this? @benbjohnson \nI really want to bump it into etcd and fix the test error.\n. coreos/etcd#639\n. coreos/etcd#643\n. @xiangli-cmu Silly me. Thanks for reminder!\n@benbjohnson Could you review this also?\n. @benbjohnson It is the stuff about conf file.\nRaft will write latest commit index into conf file only when AddPeer and RemovePeer.\nFor now, I want to update commit index without using AddPeer and RemovePeer. It could make raft to replay the log that far when restart.\nIt is used now because etcd may update peer ip through new join command, and it needs to replay the log that far to get the latest peer ip.\n. lgtm\n. These changes look good to me.\n@xiangli-cmu I have tried to bump it into etcd, and it works well.\n. I find it out!\nBecause node 3 joins the cluster and cluster size is 2, node 1 has to get the vote from node 3 to add node2,4,5.\nIt is the problem of etcd. Sorry for confusion.\n. @xiangli-cmu I have tested it in etcd, and it could pass the tests.\n. For raft algorithm, here is the mailing list: raft-dev@googlegroups.com\nYou could also open issues here. :)\nI think etcd is a good example for this:\nhttps://github.com/coreos/etcd/blob/master/server/peer_server.go#L282\nhttps://github.com/coreos/etcd/blob/master/server/peer_server.go#L292\nhttps://github.com/coreos/etcd/blob/master/server/peer_server.go#L305\n. @benbjohnson Check it!\nI understand the concerns totally.\nOne thing that I think is that each goroutine of ours should not hang a long time. Because the goroutine always needs to receive new requests from other peers, it is a great hurt if the goroutine is stuck on something for a long time.\nAnd we could fix them step by step, considering it will be used mainly for etcd testing only now.\nSo I think it is totally ok for this PR.\n. @xiangli-cmu @benbjohnson Updated it! Does it look better to you now, xiang?\n@bmizerany Could you give some advices on the new struct that manages goroutines?\n. @bmizerany It is also used to wait for all goroutines to finish in Stop. This is the functionality I want to add.\n. @bmizerany Based on our offline discussion, it is mainly for sensible Stop function, and etcd testing works well using etcd instance(and get rid of process).\n. I revert the goroutine manager now.\n@xiangli-cmu @bmizerany \n. Rebased.\nI just cannot see the merge status for this repo. I am gonna set the drone.\n. @xiangli-cmu Refreshed.\n. @xiangli-cmu updated.\n. I think in general, we should check whether or not s.state == state and return it at very first time. But it seems that the function also takes charge of reporting leader change, so I am unsure about it.\n. But if it doesn't impact the leader change report, we could make do the s.state == state check and return it if false at the start of this function. I think that could make logic easier.\n. @xiangli-cmu Could it be in Snapshotting state when processAppendEntriesRequest ? If so, the if condition should consider about this also.\n. @xiangli-cmu lgtm then!\n. @philips But that will use some fixed number. I am unsure about that.\n. @philips For constant as BTRFS_SUPER_MAGIC, we need to rewrite them in our own code. It may be dangerous to do that. But that could make the code much simpler.\n. exactly. I would close this issue then.\n. @xiangli-cmu We may don't need that API.\nLet me try to implement it in etcd first.\n. Would it be better if we move the code to initialize stopped in Init?\nI have concerns about it because it changed the boot behavior a little.\nRaft will reject incoming requests if it has not been started.\nBut I like this method better.\nI think this is more reasonable.\nIf we take this way, I think we should info users that they'd better to ensure start raft before sending requests.\n. lgtm then.\n. If it make the detector happy, lgtm then.\n. I am a little confused of this PR.\n1. I don't think all instances have to have the same commitIndex and execute them in the same batch way. So it may result in different state machines.\n2. And I think raft should know nothing about JoinCommand. But this is a long-term issue.\n3. Etcd may think the join command succeeds then, because it has been commited. This could confuse the nodes by letting them think that they have been joined.\n. @xiangli-cmu Sorry, I misunderstand the code: it doesn't skip the commits, but delay the execution of them to the next commit time.\nIt should work.\n. @xiangli-cmu @philips\n. @xiangli-cmu Based on our discussion, I make the above changes.\n. refreshed.\n. I guess it might want to filter out io.ErrNotExist.\n@xiangli-cmu \n. @wankai It may be a new raft machine, and doesn't have that file yet.\n. I think @xiang90 is working on https://github.com/coreos/etcd/tree/master/raft, and @benbjohnson is working on https://github.com/influxdb/influxdb/tree/master/raft\n. ",
    "swinghu": "refresh the repo\n. refresh the repo\n. ",
    "bmizerany": "This seems like overkill. It seems you can avoid all of this by passing a stop chan int to all goroutines you who want to shutdown after a close(stop).\n. @unihorn Will you please explain why you need to wait for them to stop?\n. This seems like overkill. It seems you can avoid all of this by passing a stop chan int to all goroutines you who want to shutdown after a close(stop).\n. @unihorn Will you please explain why you need to wait for them to stop?\n. ",
    "MartyMacGyver": "I stumbled upon bug 711 a little while ago myself and did a bit of digging. In short, this really ought to be fixed in Go itself, but failing that, the easy way is to simply do a create/delete with error checking (the extra space isn't a concern since a new compacted logfile is created before the old one is remove in the rename process, and a mutex already guards the compaction operation.)\nIf atomicity of the rename itself is a concern then it's more complicated... but other similar bugfixes suggest the proper way to handle this in Windows (e.g., https://bugs.launchpad.net/juju-core/+bug/1240927), particularly the use of MoveFileEx (http://msdn.microsoft.com/en-us/library/windows/desktop/aa365240(v=vs.85).aspx)\nThis was previously raised as a bug against Go itself (https://code.google.com/p/go/issues/detail?id=3366) but it was rejected on the grounds that atomicity may not guaranteed in Windows. There's a much more thorough discussion of this problem as it affects the Python core code at http://bugs.python.org/issue8828, including a successful resolution.\n. Update: Since there will be no further movement on correcting os.Rename() in Go, I've opened a request for an os.Replace() function (https://code.google.com/p/go/issues/detail?id=8914).\n. I stumbled upon bug 711 a little while ago myself and did a bit of digging. In short, this really ought to be fixed in Go itself, but failing that, the easy way is to simply do a create/delete with error checking (the extra space isn't a concern since a new compacted logfile is created before the old one is remove in the rename process, and a mutex already guards the compaction operation.)\nIf atomicity of the rename itself is a concern then it's more complicated... but other similar bugfixes suggest the proper way to handle this in Windows (e.g., https://bugs.launchpad.net/juju-core/+bug/1240927), particularly the use of MoveFileEx (http://msdn.microsoft.com/en-us/library/windows/desktop/aa365240(v=vs.85).aspx)\nThis was previously raised as a bug against Go itself (https://code.google.com/p/go/issues/detail?id=3366) but it was rejected on the grounds that atomicity may not guaranteed in Windows. There's a much more thorough discussion of this problem as it affects the Python core code at http://bugs.python.org/issue8828, including a successful resolution.\n. Update: Since there will be no further movement on correcting os.Rename() in Go, I've opened a request for an os.Replace() function (https://code.google.com/p/go/issues/detail?id=8914).\n. ",
    "wankai": "@unihorn , config is as follow. if config file doesn't exist, peers will be nil, then this raft server will have no idea of others.\ngo\ntype Config struct {\n    CommitIndex uint64 `json:\"commitIndex\"`\n    // TODO decide what we need to store in peer struct\n    Peers []*Peer `json:\"peers\"`\n}\n. @unihorn , config is as follow. if config file doesn't exist, peers will be nil, then this raft server will have no idea of others.\ngo\ntype Config struct {\n    CommitIndex uint64 `json:\"commitIndex\"`\n    // TODO decide what we need to store in peer struct\n    Peers []*Peer `json:\"peers\"`\n}\n. ",
    "CodingAgainstChaos": "@philips using the coreos/etcd#913 example, the new logs should look like this:\n[etcd] Jul 30 20:50:13.509 INFO      | app02: leader changed from 'app03' to ' '.\n[etcd] Jul 30 20:50:13.509 INFO      | app02: leader changed from ' ' to 'app01'.\nI think the empty is correct because it signals that the leader changed to no leader so we should expect a state change to follow (e.g. follower is promoted to candidate or new leader is assigned).\n. @philips using the coreos/etcd#913 example, the new logs should look like this:\n[etcd] Jul 30 20:50:13.509 INFO      | app02: leader changed from 'app03' to ' '.\n[etcd] Jul 30 20:50:13.509 INFO      | app02: leader changed from ' ' to 'app01'.\nI think the empty is correct because it signals that the leader changed to no leader so we should expect a state change to follow (e.g. follower is promoted to candidate or new leader is assigned).\n. ",
    "otoolep": "Specifically, this is the build error:\n```\n~/r/raft (master)$ go build\ngithub.com/goraft/raft/protobuf\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:113: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:130: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:147: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:164: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:181: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:204: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_responses.pb.go:97: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_responses.pb.go:114: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_responses.pb.go:131: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_responses.pb.go:148: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_responses.pb.go:148: too many errors\n~/r/raft (master)$\n```\n. @swsnider -- that's exactly what I've done too. :-)\nI wish there were clear instructions on how to re-generate these files.\n. I've forked the repo to here and dropped in the latest copies from influxb. My fork now builds.\n. Thank you @hanlz.\n. Yes, if I merge @hanlz master into the raft source, I can then build.\n~/coding/check/src/github.com/goraft/raft (master)$ git remote add hanlz https://github.com/hanlz/raft.git\n~/coding/check/src/github.com/goraft/raft (master)$ git pull hanlz master\nremote: Counting objects: 12, done.\nremote: Compressing objects: 100% (12/12), done.\nremote: Total 12 (delta 6), reused 0 (delta 0)\nUnpacking objects: 100% (12/12), done.\nFrom https://github.com/hanlz/raft\n * branch            master     -> FETCH_HEAD\nUpdating 73f9c44..ec41f00\nFast-forward\n protobuf/append_entries_request.pb.go     | 633 +++----------------------------------------------------------------\n protobuf/append_entries_responses.pb.go   | 503 +-----------------------------------------------------\n protobuf/log_entry.pb.go                  | 505 +-----------------------------------------------------\n protobuf/request_vote_request.pb.go       | 505 +-----------------------------------------------------\n protobuf/request_vote_responses.pb.go     | 411 +-------------------------------------------\n protobuf/snapshot_recovery_request.pb.go  | 823 +---------------------------------------------------------------------------------------\n protobuf/snapshot_recovery_response.pb.go | 457 +------------------------------------------------\n protobuf/snapshot_request.pb.go           | 459 +------------------------------------------------\n protobuf/snapshot_response.pb.go          | 365 +--------------------------------------\n 9 files changed, 74 insertions(+), 4587 deletions(-)\n~/coding/check/src/github.com/goraft/raft (master)$ go build\nprotobuf/append_entries_request.pb.go:24:8: cannot find package \"code.google.com/p/goprotobuf/proto\" in any of:\n        /home/philip/coding/go/src/pkg/code.google.com/p/goprotobuf/proto (from $GOROOT)\n        /home/philip/coding/check/src/code.google.com/p/goprotobuf/proto (from $GOPATH)\n~/coding/check/src/github.com/goraft/raft (master)$ go get\n~/coding/check/src/github.com/goraft/raft (master)$ go build\n~/coding/check/src/github.com/goraft/raft (master)$\nSo it looks good to me.\n. raftd was great inspiration, and this meets a need I may have soon.\n. Wow. Something as simple as this should fix it.\nhttps://github.com/otoolep/raft/commit/ffe3da06a7a2a55c809724510392733b3f10a859\n. Check the PR -- it pads both fields.\n. Oops, I didn't mean to open this. This was just me mucking with my own repo. :-)\n. It wasn't to fix it per-se, it was just to suggest a fix. Obviously I'd need to generate a proper PR, but my fork of the Raft repo is not in suitable shape for that right now.\n. OK, what about this trivial change to address issue 235? This will not fix busted systems, but systems going forward should be better.\n. I ran the unit test suite with this change in place. It passed.\n. I guess to be fully functional, the patch would need to pad last index too.\n. http://play.golang.org/p/B5dVjbT37o\n. This is, of course, not backwards compatible with snapshots that already exist.\n. Cute. :-)\n. I don't know for sure either, but it's not clear to me why you think this is wrong behaviour. Couldn't CurrentIndex be interpreted as the next place a log entry will be written, and that the recovery case you outline is an exceptional case where CurrentIndex is quite a bit ahead of where the next log message will be written? But once recovery is complete, everything is OK?\nDoes this behaviour prevent you from building what you need to build?\n. Hang on -- why are you worrying if a particular commit has been applied in the past? Shouldn't a recovery commence with restoring the data source under that node from snapshot, and then applying any outstanding log entries? Is it not possible to wipe out the data source, and then apply a snapshot (or simply replay the entire log?\nI know this doesn't directly address your original question, but I am not sure I follow why you have issue in the first place. Perhaps you can't snap-and-restore your external data source? I do exactly this with rqlite and it works fine. I am interested in learning more about your application, to see if I can help.\n. OK, I think I follow you. In general, if a snapshot of your system does not contain the complete state, I think I see how you could encounter problems. I need to think about it a bit more to be sure.\nBut it's not 100% clear to me from the source if there is a bug. It may just be the way that variable is meant to work.\n. Yeah, @jrallison I think enhancing the information passed to Apply() might be something worth trying.\n. The original ticket is here: https://github.com/otoolep/rqlite/issues/42\n. Specifically, this is the build error:\n```\n~/r/raft (master)$ go build\ngithub.com/goraft/raft/protobuf\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:113: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:130: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:147: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:164: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:181: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_request.pb.go:204: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_responses.pb.go:97: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_responses.pb.go:114: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_responses.pb.go:131: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_responses.pb.go:148: undefined: proto.ErrWrongType\nsrc/github.com/goraft/raft/protobuf/append_entries_responses.pb.go:148: too many errors\n~/r/raft (master)$\n```\n. @swsnider -- that's exactly what I've done too. :-)\nI wish there were clear instructions on how to re-generate these files.\n. I've forked the repo to here and dropped in the latest copies from influxb. My fork now builds.\n. Thank you @hanlz.\n. Yes, if I merge @hanlz master into the raft source, I can then build.\n~/coding/check/src/github.com/goraft/raft (master)$ git remote add hanlz https://github.com/hanlz/raft.git\n~/coding/check/src/github.com/goraft/raft (master)$ git pull hanlz master\nremote: Counting objects: 12, done.\nremote: Compressing objects: 100% (12/12), done.\nremote: Total 12 (delta 6), reused 0 (delta 0)\nUnpacking objects: 100% (12/12), done.\nFrom https://github.com/hanlz/raft\n * branch            master     -> FETCH_HEAD\nUpdating 73f9c44..ec41f00\nFast-forward\n protobuf/append_entries_request.pb.go     | 633 +++----------------------------------------------------------------\n protobuf/append_entries_responses.pb.go   | 503 +-----------------------------------------------------\n protobuf/log_entry.pb.go                  | 505 +-----------------------------------------------------\n protobuf/request_vote_request.pb.go       | 505 +-----------------------------------------------------\n protobuf/request_vote_responses.pb.go     | 411 +-------------------------------------------\n protobuf/snapshot_recovery_request.pb.go  | 823 +---------------------------------------------------------------------------------------\n protobuf/snapshot_recovery_response.pb.go | 457 +------------------------------------------------\n protobuf/snapshot_request.pb.go           | 459 +------------------------------------------------\n protobuf/snapshot_response.pb.go          | 365 +--------------------------------------\n 9 files changed, 74 insertions(+), 4587 deletions(-)\n~/coding/check/src/github.com/goraft/raft (master)$ go build\nprotobuf/append_entries_request.pb.go:24:8: cannot find package \"code.google.com/p/goprotobuf/proto\" in any of:\n        /home/philip/coding/go/src/pkg/code.google.com/p/goprotobuf/proto (from $GOROOT)\n        /home/philip/coding/check/src/code.google.com/p/goprotobuf/proto (from $GOPATH)\n~/coding/check/src/github.com/goraft/raft (master)$ go get\n~/coding/check/src/github.com/goraft/raft (master)$ go build\n~/coding/check/src/github.com/goraft/raft (master)$\nSo it looks good to me.\n. raftd was great inspiration, and this meets a need I may have soon.\n. Wow. Something as simple as this should fix it.\nhttps://github.com/otoolep/raft/commit/ffe3da06a7a2a55c809724510392733b3f10a859\n. Check the PR -- it pads both fields.\n. Oops, I didn't mean to open this. This was just me mucking with my own repo. :-)\n. It wasn't to fix it per-se, it was just to suggest a fix. Obviously I'd need to generate a proper PR, but my fork of the Raft repo is not in suitable shape for that right now.\n. OK, what about this trivial change to address issue 235? This will not fix busted systems, but systems going forward should be better.\n. I ran the unit test suite with this change in place. It passed.\n. I guess to be fully functional, the patch would need to pad last index too.\n. http://play.golang.org/p/B5dVjbT37o\n. This is, of course, not backwards compatible with snapshots that already exist.\n. Cute. :-)\n. I don't know for sure either, but it's not clear to me why you think this is wrong behaviour. Couldn't CurrentIndex be interpreted as the next place a log entry will be written, and that the recovery case you outline is an exceptional case where CurrentIndex is quite a bit ahead of where the next log message will be written? But once recovery is complete, everything is OK?\nDoes this behaviour prevent you from building what you need to build?\n. Hang on -- why are you worrying if a particular commit has been applied in the past? Shouldn't a recovery commence with restoring the data source under that node from snapshot, and then applying any outstanding log entries? Is it not possible to wipe out the data source, and then apply a snapshot (or simply replay the entire log?\nI know this doesn't directly address your original question, but I am not sure I follow why you have issue in the first place. Perhaps you can't snap-and-restore your external data source? I do exactly this with rqlite and it works fine. I am interested in learning more about your application, to see if I can help.\n. OK, I think I follow you. In general, if a snapshot of your system does not contain the complete state, I think I see how you could encounter problems. I need to think about it a bit more to be sure.\nBut it's not 100% clear to me from the source if there is a bug. It may just be the way that variable is meant to work.\n. Yeah, @jrallison I think enhancing the information passed to Apply() might be something worth trying.\n. The original ticket is here: https://github.com/otoolep/rqlite/issues/42\n. ",
    "swsnider": "I'm experiencing this issue myself, couldn't figure out how to regenerate the protocol buffers successfully, so I started used the raft library frozen in _vendors by github.com/influxdb/influxdb.\n. I'm experiencing this issue myself, couldn't figure out how to regenerate the protocol buffers successfully, so I started used the raft library frozen in _vendors by github.com/influxdb/influxdb.\n. ",
    "siddontang": "I don't re-generate files but replace all proto.ErrWrongType with customized ErrWrongType error. It's not grace but simple. :)\n. I don't re-generate files but replace all proto.ErrWrongType with customized ErrWrongType error. It's not grace but simple. :)\n. ",
    "hanlz": "@otoolep -- there's some instructions can fix this issue:\ncd into 'raft/protobuf' directory, run command below:\nprotoc --go_out=. -I. -I/usr/include -I$GOPATH *.proto\nThere might be some extra installion depending your system, on my machine I also installed gogoprotobuf and libprotoc-dev.\nWishes to help people suffering with this issue.\n. You're welcome, but there might be a little difference of package reference, I'm using goprotobuf but not gogoprotobuf to generate the pb files.\n. You're welcome. :+1: \n. @otoolep -- there's some instructions can fix this issue:\ncd into 'raft/protobuf' directory, run command below:\nprotoc --go_out=. -I. -I/usr/include -I$GOPATH *.proto\nThere might be some extra installion depending your system, on my machine I also installed gogoprotobuf and libprotoc-dev.\nWishes to help people suffering with this issue.\n. You're welcome, but there might be a little difference of package reference, I'm using goprotobuf but not gogoprotobuf to generate the pb files.\n. You're welcome. :+1: \n. ",
    "mre": "+1 Can we help with this?\n. So as far as I can see, this library is currently unmaintained, right?\n. @benbjohnson, thanks for the info. @jvshahid: Is team around InfluxDB planning to maintain this or are you looking for alternatives?\n. @benbjohnson Very nice. I could help you with testing if you like.\n. +1 Can we help with this?\n. So as far as I can see, this library is currently unmaintained, right?\n. @benbjohnson, thanks for the info. @jvshahid: Is team around InfluxDB planning to maintain this or are you looking for alternatives?\n. @benbjohnson Very nice. I could help you with testing if you like.\n. ",
    "jrallison": "Note that using the CommitIndex value isn't usable either.  As a new node added to the cluster will receive the following information as it receives committed entries from the existing node.\nTerm: 0 CurrentIndex: 3 CommitIndex: 5\nTerm: 0 CurrentIndex: 4 CommitIndex: 5\nTerm: 0 CurrentIndex: 5 CommitIndex: 5\nWhich seems to make sense. The change in this pull request unifies the context information, so that it's consistent in all cases.\n. Hey @otoolep, it is as I cannot safely know which commit I'm currently applying against my state machine. I'm using raft to manage consensus for some external data rather than just an in-memory state machine, and knowing whether or not I've already processed a given commit in the past is important for correctness.\nAt any given time, I'm applying a single commit to the state machine on a given node. With the current implementation, a specific entry can be applied (or re-applied in the case of a node restart, etc) with totally different CurrentIndex and CommitIndex values.\nCommitIndex is the one that seems to be the next place a new log entry will be written from the perspective of a given node, if I understand correctly.\nAt the very least, this makes it difficult to program against, or use in any significant fashion, as there doesn't seem to be any firm definition of what to expect from CurrentIndex.\n. We're managing consensus of insert ordering for a time-based stream of data that grows without bound, and doesn't fit in memory (think terabytes of data).  We're using raft to manage the \"tail\" of the stream, so that each node can be confident that it is writing a consistent ordering of the data.\nWe do use snap-and-restore to essentially truncate the raft log at certain points. Since our data is immutable once written, at regular intervals we record the current index, take a snapshot which contains the current index applied, etc), and archive the block of data written since the last snapshot.\nIn the case of a new node added to the cluster, or a node being down for multiple snapshots, the nodes have ways to restore missing archived blocks outside of raft (think something like https://github.com/golang/groupcache).\nThe issue becomes, since we're marking the current index for these snapshots and we have no safe way of knowing which index we've just applied, we end up with the potential for duplicated data when a node is recovering, hence causing inconsistency between nodes of the underlying data.\nIf I'm thinking about this wrong, or using raft incorrectly, I'd love to hear your thoughts.\nIn our testing, simply being able to keep track of the last index we've applied to our underlying data is enough to prevent this duplication.\n. Thanks for the feedback.  Perhaps another approach would be to add a new context field with the... index of the current command being applied.\nWould love to get a better understanding of the intended meaning of the CurrentIndex/CommitIndex information.\n. Note that using the CommitIndex value isn't usable either.  As a new node added to the cluster will receive the following information as it receives committed entries from the existing node.\nTerm: 0 CurrentIndex: 3 CommitIndex: 5\nTerm: 0 CurrentIndex: 4 CommitIndex: 5\nTerm: 0 CurrentIndex: 5 CommitIndex: 5\nWhich seems to make sense. The change in this pull request unifies the context information, so that it's consistent in all cases.\n. Hey @otoolep, it is as I cannot safely know which commit I'm currently applying against my state machine. I'm using raft to manage consensus for some external data rather than just an in-memory state machine, and knowing whether or not I've already processed a given commit in the past is important for correctness.\nAt any given time, I'm applying a single commit to the state machine on a given node. With the current implementation, a specific entry can be applied (or re-applied in the case of a node restart, etc) with totally different CurrentIndex and CommitIndex values.\nCommitIndex is the one that seems to be the next place a new log entry will be written from the perspective of a given node, if I understand correctly.\nAt the very least, this makes it difficult to program against, or use in any significant fashion, as there doesn't seem to be any firm definition of what to expect from CurrentIndex.\n. We're managing consensus of insert ordering for a time-based stream of data that grows without bound, and doesn't fit in memory (think terabytes of data).  We're using raft to manage the \"tail\" of the stream, so that each node can be confident that it is writing a consistent ordering of the data.\nWe do use snap-and-restore to essentially truncate the raft log at certain points. Since our data is immutable once written, at regular intervals we record the current index, take a snapshot which contains the current index applied, etc), and archive the block of data written since the last snapshot.\nIn the case of a new node added to the cluster, or a node being down for multiple snapshots, the nodes have ways to restore missing archived blocks outside of raft (think something like https://github.com/golang/groupcache).\nThe issue becomes, since we're marking the current index for these snapshots and we have no safe way of knowing which index we've just applied, we end up with the potential for duplicated data when a node is recovering, hence causing inconsistency between nodes of the underlying data.\nIf I'm thinking about this wrong, or using raft incorrectly, I'd love to hear your thoughts.\nIn our testing, simply being able to keep track of the last index we've applied to our underlying data is enough to prevent this duplication.\n. Thanks for the feedback.  Perhaps another approach would be to add a new context field with the... index of the current command being applied.\nWould love to get a better understanding of the intended meaning of the CurrentIndex/CommitIndex information.\n. ",
    "smuir": "You're not restarting the clients with the correct command line arguments. raftd interprets the first non-option argument as the data directory, and ignores subsequent ones silently. In your invocation, raftd -v -p 4003 localhost:4001 ~/node.3 you forgot the -join argument preceding localhost:4001 and thus a directory called 'localhost:4001' is actually used for the configuration. You can confirm this by reading the log files for clients 2 and 3 and seeing that happen, for example:\nJasons-MacBook-Pro:ceptor jasona$ raftd -trace -p 4002 localhost:4001 ~/node.2\nRaft trace debugging enabled.\n2014/10/25 22:04:01 Initializing Raft Server: localhost:4001\n[raft]22:04:01.045543 [40536ee Term:0] readConf.open  localhost:4001/conf\n[raft]22:04:01.045557 log.open.open  localhost:4001/log\n[raft]22:04:01.045606 log.open.create  localhost:4001/log\n[raft]22:04:01.045616 [40536ee Term:0] start as a new raft server\nYou thus have two clients sharing that configuration directory, and all kinds of weird things will happen.\n. You're not restarting the clients with the correct command line arguments. raftd interprets the first non-option argument as the data directory, and ignores subsequent ones silently. In your invocation, raftd -v -p 4003 localhost:4001 ~/node.3 you forgot the -join argument preceding localhost:4001 and thus a directory called 'localhost:4001' is actually used for the configuration. You can confirm this by reading the log files for clients 2 and 3 and seeing that happen, for example:\nJasons-MacBook-Pro:ceptor jasona$ raftd -trace -p 4002 localhost:4001 ~/node.2\nRaft trace debugging enabled.\n2014/10/25 22:04:01 Initializing Raft Server: localhost:4001\n[raft]22:04:01.045543 [40536ee Term:0] readConf.open  localhost:4001/conf\n[raft]22:04:01.045557 log.open.open  localhost:4001/log\n[raft]22:04:01.045606 log.open.create  localhost:4001/log\n[raft]22:04:01.045616 [40536ee Term:0] start as a new raft server\nYou thus have two clients sharing that configuration directory, and all kinds of weird things will happen.\n. ",
    "glycerine": "Makes sense, thanks. The -join flag was omitted because raftd objects to it, but I didn't realize the address following it was the argument to join. Thanks again. - Jason\n. Makes sense, thanks. The -join flag was omitted because raftd objects to it, but I didn't realize the address following it was the argument to join. Thanks again. - Jason\n. ",
    "chrislusf": "hey,we need this patch ASAP. The code.google.com git system is down.\n. hey,we need this patch ASAP. The code.google.com git system is down.\n. ",
    "ghost": "some project like seaweedfs use goraft is not safe\uff1f\n. some project like seaweedfs use goraft is not safe\uff1f\n. "
}
{
    "asciimoo": "\\o/\n. Nice features!\nBut click tracking conflicts with the \"tracking free\" principle. It should be optional or maybe a new branch\n. lxml and gevent based on libraries implemented in C, so you need to install the required header files or just simply install the compiled packages: apt-get install python-gevent\n. \\o/ great =)\n. it seems to be a networking issue. Can you reproduce the bug somehow?\n. Probably you use it behind a reverse proxy and it can cause that localhost appears in your opensearch.xml\nA proper uwsgi config could fix this issue without changing the code, but your solution also works. =)\nhere's an example uwsgi setup with nginx\ncommand to start searx:\n/path/to/your/uwsgi --socket 127.0.0.1:3032 --chdir /path/to/searx/ --module searx.webapp --callable app\nnginx site config:\nserver {\n  listen 80;\n  server_name searx.yourdomain.tld;\n  location / {\n        include        uwsgi_params;\n        uwsgi_pass     127.0.0.1:3032;\n  }\n}\n. Sure, but searx based on flask and it supports plenty of deployment methods, not only uwsgi.\nHere is a detailed description about the flask apache/mod_wsgi relationship.\n. added @ 141b04c6dde9df0e6b7f2e2b30ec8131429c8932\n.  hostname replaced with base_url in settings.py ( 32512856b47f1cf13f141bedc116e61511814934 ) to handle custom protocol settings (http/https) too\nThis mod fixes https://searx.galex-713.eu/opensearch.xml\n. works like a charm, thank you =)\n. I can't see clearly how could we display dbpedia entities in searx.\nCreating a custom wikipedia engine which embeds dbpedia (or just duplicates the results with dbpedia urls) could be a solution.\n. I have no problem with javascript if the page fully functional without it.\nhtml5 compatibility also cool, but why do you want to remove the search category remembering?\nFor example, most of the time i use general + it categories.\nMaybe it could be optional somehow or with a cookie management extension like cookie monster you can forbid searx cookies\n. I think we can't assume that every searx user wants to use only the general category from the browsers url bar/search input.\nA personalization page could solve this problem, where you can set your default categories\n. new engines are always useful =)\nthanks\n. awesome! =)\ntext_content_from_html function in engines/dailymotion.py should replace searx.utils.html_to_text - if it handles html entity and character references as well\n. Well, it isn't important, it was the simplest solution.\n..But yes, we should use a static user agent or maybe a funciton, which generates random valid user agents.\n. quick fix @ a65070a72d1deecbcb5bd9a2905120d326a00527\n. Amazing! Perfectly works without errors and complications =)\nThank you\n. Sure, i joined #searx @ freenode\n. oh, yes! definitely needed a better and generalized config handler\n. partially done: 3afdd1d9941527e23cd7c05d2c15dd24a32de834\n. Favico support is a nice feature, but featured results seems to be a bit redundant, because you can set a weight - which multiplies the score of a result - to all the engines (1.0 is the default weight). A good example is the currency converter engine, where the weight is 100\n. use -[search_engine_name] <search terms> syntax to search with a specific engine e.g.: -youtube python\n. yap, good idea\nthere is already an engine list: https://searx.0x2a.tk/engines in a very experimental state\n. !bang functionality updated\nusing ! instead of - as prefix\nworks with engines, engine shortcuts (see: https://0x2a.tk/preferences ) and categories as well\n. and yes, a proper documentation would be necessary\n. what is the difference between lxquick and startpage?\n. I see, cool\nAnd why does this pull request contains the codes of #26 again?\n. great, thanks =)\n. https://pypi.python.org/pypi/searx/0.1 \\o/\n. \\o/\nthanks =)\n. Could you reproduce the error somehow?\nit works for me: https://searx.0x2a.tk/?q=oe5tpo\n. Thank you, fixed: ced6a94\nThere was the same bug with the download buttons, also fixed: 84fc0c0\n. You can create linkable search urls manually (eg.: https://searx.0x2a.tk/?q=asdf), but yes, a copyable url of search results would be useful - and a documentation =)\n. As a first solution, a copyable and linkable search url added: 6ff57df\n. finally, fixed\n. I agree @ThomasWaldmann, switching between different nodes while searching is a good approach, but there are open questions as @dalf mentioned.\nDifferent configs/engines/code versions could cause bugs too.\n. damn clicktrackers..\nthanks, fixed ( 46277888340ceaa76233c57be23fb21591188e26 )\n. I created a wiki page with instructions and the installation page updated - thanks @dalf\n. great! thank you =)\n. Fantastic! CSS was the most unclear/messy piece of searx so far.\nThanks =)\n. fixed: 13a27b9\n. I guess, your python version cause the problem. Currently searx runs only with python 2.x\n. thanks, transifex updated: https://www.transifex.com/projects/p/searx/\n. Thanks =)\n. Yes, it would be nice! The key issue is the latency i think - if it is too slow, it's nearly useless. Maybe rotating multiple autocompletition sources (google,wikipedia,etc) while typing could be a working solution.\n. dateutil isn't a default python package, probably your searxs virtualenv doesn't contains it. Dependencies are collected in requirements.txt.\n. First:\nNowadays yahoo changes its \"api\" often, need to fix the engine.\nSecond:\ncategory_none means that you don't want to search in any of the categories - (just with yahoo engine in this case), but i don't know, why doesn't provide results. I'll check it.\n. the url problem solved: faed14b2c691746ba6cf98d164a5e6b1ca3ee4c9\n. I agree with @pointhi, those are private changes and differs from the pull requests topic.\nThe new engines are nearly identical as i see, so a single engine is enough and you can set the necessary parameters (url) via settings.yml. A good example is the xpath or mediawiki engine.\n. great, thanks, i'll start to code the backend part.\n@dalf mentioned to use dbpedia in searx ( #11 ), maybe this is the suitable use case of it\n. Good idea, for a first solution i planned to implement \"offline engine\" functionality, which can be a widget with a custom result template.\n. @pointhi, agree, the only thing that holds this issue is an other approach i'm thinking on. Kind of a plugin system, where plugins can attach themselves to specific events like search start/end and manipulate the actual state of the performed search (eg. reorder/add/filter results, rewrite engine/search settings, ..).\n. It is optionally available by web server configuration: https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security#Implementation\n. unfortunately there are dependencies which have no py3 support yet.\nsee https://caniusepython3.com/check/c2d59224-f74f-4d07-9e00-2fd5174729ca for more details\n. @jleclanche  Gevent has been purged from searx, so there is no py3 blocking dependency - only searx\n. @josch great! There is a py3 branch where the search functionality works, but there can be bugs in other parts of the code.\nThe robot testing is completely missing, since it is a py2 only dependency.. @josch onice!\nI haven't found this - looks super promising. I'll rebase the py3 branch to the top of the current master (there were bigger changes in the code since the last update of py3 branch) and experiment with this lib. Thanks!. Thanks, good ideas. Maybe https forcing could be more generic somehow. EFF's https everywhere ruleset (https://www.eff.org/https-everywhere/rulesets) is an up-to-date and huge collection of https capable sites.\n. searx supports non-root base urls if you set base_url in searx/settings.yml to http://domaine.com/searx/, and your nginx is configured properly.\n. Sorry, you're right, it doesn't work =\\\n. This configuration works for me: http://flask.pocoo.org/docs/deploying/uwsgi/#configuring-nginx\n. wow, it would be definitely a great feature!\n. yes, it would be useful. EFF has a huge list of HTTPS supporting sites: https://www.eff.org/https-everywhere/rulesets , it is a good base imho, but still thinking on the proper integration to searx\n. Basic functionality implemented: 96c8b20a045f62205a3b9a03113086f0fcfbc579\n. Searx has partial localization support. Some of the engines (wikipedia, google, yahoo, bing, duckduckgo) supports localization and searx detects the browsers locale, also you can set it explicitly on the preferences page.\nQuick and dirty solution: Setup an instance with only these engines.\n. Good idea.\nI'm using the Italian translation: http://searx.0x2a.tk/?locale=it\n(https://github.com/asciimoo/searx/tree/master/searx/translations/it/LC_MESSAGES)\n. Interesting feature, but seems impossible with searx. Searx only a meta search engine - aggregates results from other search engines -, so if the engine doesn't support, searx neither\n. Search will be more nondeterministic (e.g. if all results filtered)\n. There are cases when all the results on the first page comes from the top n sites, the response is an empty resultset in these cases, but it doesn't mean that there are no results for the given search term on the next  pages.\n. It could radically increase the response time.\nAverage response time is around 1.5 sec for one page, so n * 1.5 will be the whole response time, where n is the number of the page with the first result.\n. thanks, fixed (111a86d355fe2fcbc714a478cb399ef47b79ec35)\n. Oh wow! beautiful! =)\n. it's implemented in oscar theme, thanks to @Cqoicebordel \n. Niiiice! This is ultra useful (also the engine-overview wiki page)! thanks a lot!\n. awesome again!! =]\n. Thanks the information.\nWhy lxml isn't enough to parse rss?\n. Let's see how lxml works. If it won't, we can still use some abstraction\n. Seems, google api is still fully functional, so closing this issue\n. yes, good idea\n. True, it would be useful\n. Did you measure how it affects the performance?\n. Probably some of the sites from EFF's list will never appear as a search result.\nAnother solution could be the limitation of the number of included rules to the top n most visited site. \n. Great, thanks!\n. closing, see #308\n. Fixed by @pointhi, thx again!\n. The pull request contains more changes than the topic explains and it breaks travis. Could you fix these?\n(btw, the infobox/widget idea is also cool!)\n. So heavy pull request, need some time to go over it, but it looks really nice!\n. Great refactor, thanks!\n. you can comment it out in your settings.yml\n. fixed ( 4da7958 )\n. Is resizing and the whole PIL/flask-images(/gunicorn? why?) dependency chain necessary?\nWhat about just a simple image proxy function (optionally with caching)? Then, the image parsing/manipulation can be avoided and can't be a security risk.\n. Okay, i start experimenting with it. A c-based lib which depends on libjpeg and friends is a big difference imho\n. Cache and image resize are different things. It's not too hard to add caching to the current solution and resize isn't necessary if we can grab the thumbnail pictures from the engines\n. @Cqoicebordel i agree, image resizing would be a useful feature, i just don't know how could it be securely implemented\n@pointhi yap, good idea, a token could prevent abuse. Requires user session, but it can be handled nonpersistently .\n@dalf technically whole searx is an open proxy - a specific one =)\n. Could you give me an example search term?\n. quick fix: b0bb94f\n. It was missing from setup.py, hopefully fixed in 7be9759\n. It's because Urllib3/gevent became incompatible with the latest python2.7.8 version. There is an open urllib3 issue about it: https://github.com/shazow/urllib3/issues/482 .\n. Downgrading python to the previous (<=2.7.7) version solves the problem\n. fixed: d959cb1\n. fixed by @pointhi\n. Thanks the fast response!\n. Great, thanks!\n. wow, really impressive!\n. No, it seems to be a bug. Can you reproduce it on http://searx.tk/ too?\n. Maybe, but that chrome bug has been fixed imho\n. \\o/\nthanks\n. Thanks, removed\n. thanks!\n. wow! thanks!\n. After all the errors will be corrected, we should add make pep8 check to the travis job\n. Thanks!\n. Thanks!\n. Thanks, before merge i want to test that it affects searxs global locale settings or not. Side effects can cause problems.\n. Seems, it works, thanks!\n. Cool thanks!\nAbout the  questions:\n- content can be the torrent description (if available)\n- categories: the request function gets the searx category name as a parameter property\n- icons: maybe it could work with the image caching/proxy solution by @pointhi\n. If the category selection is inclusive, then simply include all categories, otherwise a small DSL can be defined (eg.: a possible query syntax for the asdf search term in movies category:  cat:movies asdf or category:movies asdf)\n. About the favicon: add the icon to the themes and extend the list @ https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L72\n. Great, thanks!\nAlso good news that it can be filtered by categories.\nSome kind of documentation of these \"hidden features\" would be useful.\n(DSL: https://en.wikipedia.org/wiki/Domain-specific_language)\n. love you guys =)\nNeeds more debugging, but ssllabs gives A+ to KATs https ( https://www.ssllabs.com/ssltest/analyze.html?d=kickass.so&latest ).\nMaybe it's not supported by older openssl versions.\n. OpenSSL 1.0.1j 15 Oct 2014 with nginx\n. It breaks some tests, could you fix it?\n. thanks the information\n. Every time when a search performed it scans directories if i understand 0bfc793 correctly. Is it really necessary?\n. @josch I'm planning a release on the weekend.\nBTW, anyone can create a new release PR anytime. =). API keys are problematic because it isn't trivial to obtain (another dependency) and used for tracking\n. thanks - also for the good solution (support of both api/nonapi versions)!\n. Agree with @Cqoicebordel , it's a good temporary solution, thanks!\n. cool, thanks!\n. I agree, there is a lack of proper upgrade/development guide and the existing install guide is a bit messy.\nin short, there are multiple searx setups for different purposes:\n- Pip install is the easiest to install/maintain but only gives a single threaded instance - in this case the upgrade process is the standard pip upgrade process (pip install --upgrade searx)\n- Using git and the makefile is an other option which is more flexible and developer friendly - upgrade it with git and run a make command if there were dependency changes. See the readme for more details and advantages.\n- Manual setup (described on the install howto wiki page) which contains only searxs source and dependencies - upgrade with git and manually update dependencies if required.\nFor me the most comfortable way to develop searx is working directly on the git repo and running python searx/webapp.py with debug: True in settings.yml\n. done: https://asciimoo.github.io/searx/dev/install/installation.html , https://github.com/asciimoo/searx/blob/master/manage.sh\n. Yap, sounds useful, prefix should be - or ?\n. Thanks!\n. At last, searx has a code search engine! =)\nThanks!\n. The feature is great, but somehow we need to unify the client side components which are used in all themes. Maintain the same code in different places isn't the good way.\n. another aproach is moving all the themes to a themes subfolder in static/ and modifying the url_for_theme function in webapp.py to figure out the correct url path (static/ or static/themes/<theme>/) to a given file name\n. @Cqoicebordel i like this meta result template idea.\n@pointhi, the same what i proposed to static content can be done with the templates too. First check if the template exists in the themes directory and if not falling back to a default template\n. @pointhi, i'm afraid we need to implement an own solution, which reads the filesystem only at boot time\n. Okay, it seems, i successfully rewrote the static files and template handling and restructured the static content to be able to handle common static content and result templates, so now this feature could be simplified to use a single template file (eg. templates/result_templets/code.html) and css (eg. static/css/pygments.css)\n. Subtitle engine is a good idea.\nIs there any way to specify the subtitle language?\n. Thanks!\n. I agree, a better stat/status page would be useful, but it's a hard question that the displayed data is sensitive or not. E.g. you can estimate the usage of the searx instances from the query/time information\n. @dalf very cool stat enhancements.\nI'm not sure, that searx is fully thread safe, we need to check it. With twisted it's guaranteed.\n. @dalf it's very impressive and contains a lot of useful info about how to optimize/tweak engines. Way better than the current one for these purposes.\nAnd the question is also good - who is the target of the stats view?\nThis detailed view is mostly useful for developers/administrators and leaks some info as you mentioned, but the engine response times and result numbers/scores can help the users to configure their engine settings. So maybe a hybrid solution could work, where there is a simple stat page for users and a detailed (maybe only in debug mode?) for devs/admins.\n. Thanks the corrections\n. great, thanks!\n. New engines and pep8 \\o/\nThanks!\nThe bug sounds strange, never noticed the same - some test would be good..\n. Thanks\n. It's strange, because requests automatically handles HTTP redirects: http://requests.readthedocs.org/en/latest/user/quickstart/#redirection-and-history - requires deeper debugging.\nAnyway, thanks the fixes!\n. That's a user friendly feature, but i'm not sure that embedding (more) third party content by default is a good idea - maybe it would fit better to searxs philosophy if it could be enabled by the users, like the autocompletition.\n. ah, okay, i got it (the iframe data-src attribute hack), cool, thanks!\n. wow, super useful, especially the autocompletition! Thanks!\n@Cqoicebordel yes, the code around query parsing is really messy in whole searx, needs a refactor\n. @Cqoicebordel currently it shouldn't, mainly because it doesn't fully functional without javascript. With javascript it's way more usable than the current default theme, but using noscript you can't even select categories.\n@dimqua yap, both other themes requires improvements/fixes..\nI'll finish soon with the static content refactor and then all the 3rd party client side libs will be available for all themes\n. @pointhi it would be great\njavascript independence is always a good goal especially in the case of searx.\n@Cqoicebordel agree!\n. Okay, experimentally the default theme changed to oscar.\n@pointhi what do you think about removing the javascript warning? And maybe a less harsh logo?\n. i don't know, a lighter gray color?\nSome minor issues:\n- The category selection is buggy in console based browsers (tested with links2 and w3m)\n- Long suggestion overflows the suggestion box\n. another nice color scheme: http://flatuicolors.com/\n. done ^^^^\n. Thanks!\n. Thanks!\nThe build failed because of a pypi timeout again, of course..\n. Awesome, thanks!\n. Nice fix, thanks, we need to check that is it affects more engines\n. Awesome, thanks!\n. @julpec  currently searx doesn't support it, but can be easily implemented, i'll add an option to settings today/tomorrow\n. done ^^^\n. Awesome! Thanks both the mods and the review!\n. @Cqoicebordel nice catch! thanks!\n. @julpec thanks, wiki updated\n. Thanks!\n. @Cqoicebordel it is weird indeed. I can't imagine whats the motivation behind it\n. Nice improvements, thanks - the template corrections too!\n. could you please split it to two pull requests? Js-warning deactivation is great, but the theme topic is questionable, requires deeper review\n. @pointhi thanks!\n@Cqoicebordel yap, a sophisticated cookie handler could give bigger flexibility to themes\n. interesting, could you check it again on the fixed version?\n. I have no idea and i can't reproduce it (tried on an ARM based cubieboard).\n. great, thanks!\n. Awesome!\nMaybe it would be useful to enable the inner autocompletition by default and keep the remote sources optionally selectable\n. That's strange, i have no clue.. Maybe wrong pythonpath?\n. @pointhi is the same happen if you start the instance with the python2 searx/webapp.py command?\n. What about handling the template assignment of the request.cookies inside the render() function instead of passing as a parameter?\n. Great, thanks! =]\n. Oh wow! Impressive speed improvement!\nAs far as i know, requests(lib) session object also persists cookies through the (HTTP) requests, which would be good to avoid. @dalf could you solve this issue?\n. You're right about the cookies, thanks!\nConnection closing shouldn't be a problem, searx handles engine/request exceptions.\n. Sounds reasonable. I like the first solution if it's enough to fetch the PREF cookie value only once, in the initialization process.\n. \\o/ <3\n. Great, thanks!\n. Respect! I'll take a look at the code\n. Travis fail caused by pypi timeout, it happens often.. I restarted the job, but there are multiple errors in the tests too.\n. @Cqoicebordel could you show a reproducable example?\n. seems, it's fixed, thanks @pointhi =)\n. ah, right, it isn't..\n. Collect stats related improvements/ideas to #162\n. Thanks!\n. =))\nNice fix, thanks - and sry that i didn't spot it!\n. Uh-oh, very-very cool! Also the tests and the engine tweaks!\nQuick and dirty solution to the import is doing the same way as searx,  but we should rename these engines to fit to the python standards.\nAt first, I don't have any idea what can cause the coverage error.\n. Supercool!! Thank you sir!\n. @sanpii thanks the detailed report, it'll be fixed soon\n. i'm not sure, that this issue really requires a database.\n. Good idea, engines with language support could get a score multiplier if the search language is specified.\n. Thanks!\nAgree about the tests!\n. Great, thanks!\n. Seems the response is gzipped - no clue why\n. =))\n<3\nSeems, the currency convert try/except block isn't necessary.\n. very-very cool, thanks again!\n. cool, thanks!\n. True, thanks!\n. @GreenLunar thanks the translation, i'll update the repo.\n@Cqoicebordel constant list of RTL languages in __init__.py or webapp.py? so we can check in the render function that RTL is required and pass a boolean to the templates\n. @GreenLunar could you check that these issues are solved?\n. @GreenLunar thanks for the testing!\n. @Cqoicebordel thanks! The code still has redundant parts, but it's a good start.\n. also interesting: https://blog.openstreetmap.org/2015/02/16/routing-on-openstreetmap-org/\n. Great thanks!\n. Ah, good idea! Thanks!\n. @pointhi agree, the current \"solution\" is a dirty hack. I'll fix it as you suggested\n@Cqoicebordel that would be awesome\n. @pointhi http://creativecommons.org/licenses/by-sa/4.0/ fits best, also we need to extend the license file. Could you update the wikimedia info?\n. @pointhi <3\nImpressive article (even with my basic german knowledge =) )! Just a small note: https://searx.me/ isn't an official instance, it's just a demo (and my public) instance. Calling it official feels like it's better than other instances what may force users to use searx.me instead of run their own instance. One of the best features of searx is that it is not centralized.\n. @Cqoicebordel there is (yet =]) no technical problem with it, rather philosophical what i mentioned\n. @Cqoicebordel, totally agree on the todos.\n. Thanks! Could you add the hebrew translations too?\n. \\o/ thanks! Now @GreenLunar can start the testing of #214\n. What would be the optimal maximum size? 1024 chars?\n. Thanks!\n. @dalf, you're right, language handling is a bit messy. The reason to set language_support to True by default is only historical - there wasn't enough engine with language support.\n. \\o/ Thanks!\n. closing, fixed @ 3d5a4b091d00f0d67f4d15b0340d450ea204447e\n. this is the very first design of searx =)\n. Yap, a minimal, css free theme would be useful for text based browsers too.\n. @dalf, it's a good idea, we should create a response callback wrapper which checks if it is a redirect, i'll play with it.\nAnd thanks the fix, could you update the tests too?\n. Great, thanks!\n. Thanks! Could you contribute translations on https://www.transifex.com/projects/p/searx/ in the future?\n. Thanks!\n. Currently there is no HTTP(S) proxy support. As a workaround you can experiment with external transparent SOCKS proxy wrappers like tsocks.\n. implemented: #297\n. Clever, thanks!\n. @Cqoicebordel i don't want to depend (more) on third party services, so i'd leave the .po files in the repo. Alternatively, a contribution guide could clarify these kind of questions.\n. Keeping the po files in the repo doesn't mean that we can't define a policy to edit translations only on transifex. It just makes easier to create local modifications.\n. update-translations.sh won't create missing po files (nor fill it with the translations stored in the corresponding .mo) afaik, only updates existing ones.\n. I just can't see the benefit of it. Let's say that we wont pull translation requests from now, isn't it enough?\n. In this case you also have to tell to the contributors that transifex is the place where the translation happens. Anyway, what about removing .po files if there will be another incident around the topic?\n. @hovancik first of all, thank you the translations.\nI'll update the repo with the new transifex translations.\nIf you want to experiment with the translations in your local instance, see http://asciimoo.github.io/searx/dev/translation.html for more information.. Thanks!\n. Very cool ideas! Tests would be good to reveal bugs and measure response time/performance differences\n. +1, dalf?\n. Thanks - closes #244\n. +1\nA landing page would be super useful with links and an overview of searx. All the other functionalities are good to have but we already have working solutions.\nand +lot for styling (also lack of competence here)\n. started on https://asciimoo.github.io/searx/ (https://github.com/asciimoo/searx/tree/gh-pages)\n. it's on my list, i have to test it\n. Sorry for the big delay,\nthanks!\n. thanks!\n. \\o/ thanks!\n. Before i pull this, can anyone verify the existence of the bug? I can't reproduce it and not sure that it's good to disable ssl checking if it's a one-off issue.\n. Thanks!\nDiscuss IP tracking on #263\n. Same as @dalf, i cannot reproduce it.\n@pointhi what is the version of the requests lib?\n. hmm.. strange..\n(note: grequests is not part of searx anymore)\n. Maybe python2-requests package contains an older version of requests. Could you check it?\n( command: python2 -c 'import requests; print requests.__version__')\n. The only case when this feature could be reasonable is when a search result link published (eg. tweeted) and huge number of identical requests comes within a short time period.\n@dalf it's a too big drawback of ETag, that it only works with GET.\n. closing, see #284\n. Thanks! You're right, we have to do that. Searx works with the current latest dependencies imho.\n. Currently the order of categories: \"general\" first and alphabetical order after. The bug is in preferences which ignores this (https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L569)\n. What about defining custom compare methods in engines (optionally) and if it exists, the result merge uses that and if not, the default used?\nThis opens up a question: how to handle the merge of two results which comes from different engines when both have custom comparison method defined?\n. Seems, the connection unexpectedly ended. Can be a client side abort or any network related issue\n. This is the default behavior: https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L116\n. As i understand, the bug is exploitable only if a manually crafted image is referenced in a <script> tag. How could this provide surface to a successful attack through searx?\n. For the record, GET request leaks a lot more information than POST. Not just on server and client side, but the referrer also passes the search query to the target site. Thats why searx doesn't support GET by default.\n. Imho the existing is okay. Thanks!\n. Thanks!\n. The theme is very similar to oscar and since category selection implemented as a plugin this theme doesn't provide significant extra functionality but bunch of duplicated code.\n@ devs what do you think?\n. I can't reproduce, is it occurs on every search?\n. duplication of #236\n. @dalf yap, good questions. my plan is to rewrite both as plugins\n. I can't see any empty categories and files isn't in the category list.\n. If it isn't represented in any category it won't be listed on the preferences page. Can be accomplished by setting categories: \"\" to the engines in settings.yml\n. It works for me, tested with the string \"south\".\n. First of all, do not trust something you red on the internet =)\nSecond, do not trust searx.me\nSearx me is maintained by me with my best effort to protect users privacy. It also uses multiple IPs to spread outgoing requests.\nDNS rotation: SSL certs are another question\nlet's continue the discussion on #39 \n. see #162\n. Very-very cool, thanks!\nThe new settings format proposal also (https://github.com/dalf/searx/wiki/settings.yml)\n. Great, thanks!\n. \\o/ thanks!\n. Thanks!\n. <3\nthanks!\n. @dalf i really like this idea. It would be nice to add these fields too:\n - session_required (wolfram alpha, soundcloud, etc)\n - multiple_requests (wikidata)\n - special_search_syntax or query_example (e.g. currency convert has the format [Amount] [currency1] in [currency2]: 2 eur in huf). Thanks!\nAs you said, we still need a proper solution.\n. Thanks!\n. Thanks!\n. Thanks, also the pep8 fixes!\nThe problem is in the tear down function of robot tests, i have a fix for it\n. Thanks!\n. Thanks!\n. +lot for the new structure! \n@dalf where/how could we use the dns section?\n. handy feature, thanks!\n. Merged from transifex, thanks!\n. @pointhi yes, i forgot to update the setup.py, sry\n. closing, see #679\n. According to wikipedia, the official site is the .se again and it redirects to .la, but the engine works again..\n+1 to default disable without domain modifications\n. @Cqoicebordel that would be awesome, thanks!\n. Thanks!\n. Do you have any information about these \"new\" piratebay sites/domains?\nWho runs it and where?\nAre there any differences in the content comparing to thepiratebay.se?\n. .am is officially listed on https://en.wikipedia.org/wiki/Thepiratebay so merging as a tmp fix.\n. Great idea! The search syntax could be handled the same way as in the currency convert engine - fetching results from this engine only if the search query format is {lang1} {lang2} .+\n. This can be implemented as a new engine like the currency convert engine. Engines can parse the search query on their own way. Search syntax could be !dc en-de words to translate\n. Thanks!\n. Totally agree with disabling both by default, they are quite slow and unstable, but the 6sec default timeout isn't too much? It can cause three times slower responses in general category if one of these enabled. Personally i don't really like longer responses than 3sec.\nAnother alternative is a preferences option to set the timeout parameter individually (for each engine?).\n. You can enable them from the preferences too and there is no sign or warning about the significant response time increase which can be confusing - and i know, timeout and no results are confusing too.. =)\n. Overall i think it would be better at first.\ndevs?\n. @Cqoicebordel yes, a small notification or a box on the right side with the engine timeouts would be useful too!\n. Nice solution @dalf! Thanks!\n. ah, sry, i merged my local branch, but it's in the master: aac8d3a7bfdd77a5369e52a4ece99b20669a4625\nclosing\n. yap, currently the bang has higher priority than the category selection which has higher than the cookie settings which has higher than the instances default config. This is one kind of logic, maybe not the best - or most intuitive, but if you know the bang syntax, why would you click on categories? =]\n. Ah, yes, it's time to release! We have plenty of new features since 0.7.0. I'll focus on fixing the remaining bugs, when it's done we can release the 0.8.0.\n. @Cqoicebordel wow, sounds nice!\n2-3 weeks are okay.\n@pointhi build id is also a good idea, if it's automatically generated =)\nWhat can be the build id if not the last git commit id? Maybe a git hook which counts the commits of the local master and stores it to a file\n. I agree, i've finished the changelog - we can release tonight if there is no blocking issue\n. mkay\n. it would be good to fix #419 before\n. Very-very cool! +1 reason for the stats refactor, i'm not sure, that the numbers are always correct - because of timeouts/errors/etc\n. I've tested on some IPs, seems it works, thanks!\n. I agree, safe search should be turned off by default (or at least a config option to set default value per instance). It's another filter.. Nice to have optionally, but it's the first thing what i turn off\n. Great! What about implementing all the Qwant engines in a single file to handle different categories like the piratebay engine or swisscows (#348)?\n. What's the problem with defining more than one engines/categories in one file if the code is nearly identical? Don't repeat yourself. If there are redundant parts in the code, reduce them instead of writing new ones.\nWith the current solution an API change requires to modify the code in 4 different places, imho it's not the right approach.\n. ah, yes, so pretty and much simpler. Just a minor note: maybe a dict which maps categories to keywords would be better than setting search_url_keyword in the config.\nThe actual search category is available from the request() function as params['category'] and as resp.search_params['category'] from the response() function in each engine.\n. =]\nThanks!\n. cool! Thanks!\n. Clever solution, thanks!\nA note would be good about this feature in the engine overview wikipage - or maybe a standalone configuration page\n. Thanks!\n. i'll take a look on it\n. =))\nThanks again!\n. Thanks!\n. Thanks!\n. fixed: #362 \n. Could you add some tests, to demonstrate the usage/efficiency/performance?\n. Sorry for the long-long delay, I reviewed the code at last.\nThe functionality is ok, but it doesn't fit to the current plugin system. Implements custom variables/functionality in templates and webapp. Two possible solutions:\n- Improve the plugin system and templating\n- Rewrite it as a standard preferences option which is really easy since the user settings refactor\n. I understand the purpose of the modifications, it is just out of a plugins scope. Yes, we can extend the template system to handle these cases, but i think it's easier and less effort to rewrite the pr as a standard preferences option like the autocompleter.\n. cool, thanks!\n. nice fix, thanks!\n. that's strange, result['result']['parsed_url'].query shouldn't be empty. I've tested with https://aa.bb/cc?dd=ee#ff and it contained the dd=ee string.\n. yap, perfect, thanks!\nStoring the full and the parsed url is a bit redundant, i agree, but can be handy sometimes (e.g. result merge)\n. ah, yes, furl would be an elegant solution!\n. With a small fix (6ef7c3276c308ba12b80e22068de422247fdb4b9) searx now can be used behind a reverse proxy. Example apache configuration: http://ubuntuguide.org/wiki/Apache2_reverse_proxies\n. @autrui could you verify the fix?\n. fixed: #370\n. Very cool! Useful info about the preferences, wouldn't be better as small tooltips or descriptions near the features?\n. Thanks!\n. Good idea, +1 to the merge\n. great, thanks, two small notes:\n- The name self can be misleading in python it could be changed to self_info or client_info or something similar.\n- Description lacks of info about the usage/keywords - and the reference to DDG can be confusing\n. cool, thanks!\n. Thanks!\n. +1 to all\n. Thanks!\n. thanks!\n. It works for me with !youtube as expected even if i disable youtube engine in the preferences\n. Thanks, the first note fixed. I'm not sure about the second one, could you please point out which section of the AGPLv3+ describes it?\n. closing, see #634\n. Thanks!\n. Thanks, it's fixed\n. @GreenLunar thanks! Could you please add it to the oscar and courgette theme as well?\n. @gugod thanks!\n. ohi, i've extended the settings file with the missing options, check out the latest version\n. @magaretha42 universally not yet, but searx can be extended easily with the required hook to have access to the desired state. The hard part is the per user customization.\n. The problem with these info chunks is that you can't be sure about the correctness of them..\n. Still, it can be replaced with a static page\n. nice fixes, thanks!\n. nice, thanks!\n. hmmz, @icaroperseo what is your phones/browsers default language?\n. got it =)\nhttps://searx.me/?locale=es\n. fixed: d5931874ac8a7762c3a961f866e65a19671f9d54 m(\n. +lot\nThese topics really deserve huge refactors. The highest priority is search.py and the user settings/preferences. @kdani3 has nice improvements on user settings rethinking (https://github.com/kdani3/searx/tree/user_settings) and i plan to start the search logic modifications after the release.\nAny other ideas are highly appreciated.\n. yap! #240 will be the next one after user settings =)\n. great, thanks!\n. @austin1029 seems it is an old version of searx, this bug has been fixed, please update your instance\n. Seems an engine doesn't handle unicode urls. Could you reproduce the error/investigate which engine causes it?\n(we should extend our tests with unicode data)\n. hopefully reproduced with the search term \ud55c\uad6d\uc5b4 \uc704\ud0a4\ubc31\uacfc\uc5d0 \uc624\uc2e0 \uac83\uc744 \ud658\uc601\ud569\ub2c8\ub2e4, @pointhi could you verify the fix?\n. onice =)\n@Cqoicebordel  where did you found it?\nI rewrote the engine with the sdk_key and it works fine\n. cool, there's only one issue: the results are watermarked by 500px, i don't know why - yet.\n. there are two types of urls:\n- https://drscdn.500px.org/photo/71951633/m%3D2048_k%3D1_a%3D1/2cbfd85f1131b508cc4d654c8f869da6 (watermarked, comes from searx using the sdk_key)\n- https://drscdn.500px.org/photo/71951633/m%3D2048/4969ea5fb32b23b9cb13fc3ba5938835 (comes from 500px.com)\nseems, there is connection between the k=1_a=1 part and the hash\n. https://github.com/asciimoo/searx/commit/a58944892851d1de8e2b060318b925bf7347508f\n. @xinomilo isn't your /tmp mounted with noexec flag?\n. if you use pip, export TMPDIR=/your/tmp can help\n. Really-really cool, especially the test coverage! =)\nI have some code specific note, check the annotated diff.\nAnother small issue: instead of the %s style formatting please use the .format() function.\n. closing - see #534 \n. @cy8aer thanks the notes. I fix the shortcut. Can you help with the localization part? I couldn't find related search parameters.\n. @cy8aer thanks! =)\n. seems it throws captcha\n. Thanks!\n. @Cqoicebordel <3\n@moritan is it ok if i close this?\n. Thanks!\n. @Cqoicebordel thanks, I agree on the error manager.\n. fixed by @a01200356 , thanks\n. @GreenLunar it works for me: https://searx.me/?q=%21wp%20%3Ahu%20k%C3%A1poszta&categories=none - it is hungarian\n. +1, semantic annotation of the search data would be a huge improvement. I don't have too much xp in these questions, but schema.org looks a good way. What are the next steps?\n. First of all, thanks the detailed description. It's definitely bug, but it requires some further investigation, since it can occur in other parts of the code.\n. @pointhi, great, thanks! =)\n. Thanks!\n. @mpancorbo this inconsistence should be eliminated.\nany ideas?\n. @cy8aer what is your python/certifi version?\n. Try to update certifi: pip install --force certifi==2015.11.20.1\n. @virse what is the version of your requests package?\n. Thanks! =)\n. @GreenLunar could you update the tests too? (https://github.com/asciimoo/searx/blob/master/searx/tests/test_webapp.py#L147)\n. i don't know, I never used the webui for file editing\n. okay, thanks\n. great, thanks!\n. Thanks!\n. It works for me with Firefox 42.0 if i click on \"Subscribe now\" button.\nTested url: https://searx.me/?q=python&format=rss\n. fixed in d8f8bdc951f40fa56f1ff61306a78a28958dd41b\n@privacytoolsIO could you update privatesearch.io?\n. @THS-on it works for me: https://searx.me/?q=%21go%20south\n. @loganmarchione could you verify @misnyo's fix?\n. @loganmarchione i've started to refactor/simplify it on https://github.com/asciimoo/searx/tree/install-refactor - reviews are welcome.\n. @kujiu try the latest version\n. thanks\n. Nice, thanks!\n. Yes, ddg is really slow.\nThe default timeout is 2 seconds, ddg often exceeds it, but you can set a higher timeout value in settings.yml if you have an own instance\n. no problem, force push allowed on non-master branches =)\n..and I'll check it, thanks\n. wow! Very cool, thanks! =)\n. \\o/ thanks =)\n. @Wonderfall seems good to me, i like alpine, but i'm not a big docker expert.\n@glogiotatidis, you wrote the current docker file, what do you think?\n. You are awesome, thanks =)\n. yap, thanks\n. Thanks!\n. @GRMrGecko it works for me\n. @a01200356 thanks. There were some pep8 checking issues, could you rebase your pr to the current master?\n. @a01200356 thank you!\n. thanks\n. @teresaejunior thanks the report\n. Thanks =)\n. @Glandos I understand your point, but I don't want to allow >= in requirements.txt. The main reason behind it is that if a dependency has backward incompatible changes, then it breaks searx.\nYou can still experiment with other dependency versions, probably searx will work fine with it.\n. thanks!\n. great, thanks =)\n. @pointhi i agree, it's not the first time this bug occurs, but first we should fix these unescaping issues\n. \\o/ thanks =)\n. Unfortunately i don't know any ultimate solution. You can contact with the instance administrators, perhaps they can ensure you.\nStill, the easiest way to avoid this problem is an own instance.\n. @NIXOYE https://www.torproject.org/docs/tor-hidden-service.html.en\n. Very-very interesting idea, curl is much more flexible than requests (and probably also faster). i'll experiment with it =)\n. @dalf here's the current state: https://github.com/asciimoo/searx/tree/pycurl\nMost of the original functionality works.\nKnown issues:\n- google engine doesn't work (?)\n- HTTP compression not implemented.\n- Not tested with all engines/encoding\n. @dalf, that's what i really like in this solution that we don't have to depend on the not too reliable python threading =)\nResult parsing could be threaded, but i can't see the advantage of it, measurements needed\n. @dalf, after 34afcf2 the search with pycurl is still 100-200ms slower in average. I don't really know why, any ideas? It reuses connections, shares dns and parses results in threads just like in the original code\n. if i set pycurl.VERBOSE, it tells, that there is connection reuse:\n```\n Hostname was NOT found in DNS cache\n   Trying 208.80.154.224...\n Connected to en.wikipedia.org (208.80.154.224) port 443 (#0)\n successfully set certificate verify locations:\n   CAfile: none\n  CApath: /etc/ssl/certs\n SSL connection using TLSv1.2 / ECDHE-ECDSA-AES128-GCM-SHA256\n Server certificate:\n        subject: C=US; ST=California; L=San Francisco; O=Wikimedia Foundation, Inc.; CN=.wikipedia.org\n        start date: 2015-12-10 23:22:05 GMT\n        expire date: 2016-12-10 22:46:04 GMT\n        subjectAltName: en.wikipedia.org matched\n        issuer: C=BE; O=GlobalSign nv-sa; CN=GlobalSign Organization Validation CA - SHA256 - G2\n        SSL certificate verify ok.\n\nGET /w/api.php?action=query&list=search&srsearch=south&srprop=timestamp&format=json&sroffset=0&srlimit=1 HTTP/1.1\nHost: en.wikipedia.org\nAccept: /\nConnection: keep-alive\nUser-Agent: Mozilla/5.0 (X11; Linux x86; rv:35.0) Gecko/20100101 Firefox/35.0\n\n< HTTP/1.1 200 OK\n Server nginx/1.9.4 is not blacklisted\n< Server: nginx/1.9.4\n< Date: Sun, 07 Feb 2016 18:13:04 GMT\n< Content-Type: application/json; charset=utf-8\n< Transfer-Encoding: chunked\n< Connection: keep-alive\n< X-Powered-By: HHVM/3.6.5\n< X-Content-Type-Options: nosniff\n< Cache-control: private, must-revalidate, max-age=0\n< X-Frame-Options: SAMEORIGIN\n< Vary: Accept-Encoding,X-Forwarded-Proto,Cookie\n< Backend-Timing: D=200372 t=1454868784611699\n< X-Varnish: 2605412932, 1114822312\n< Via: 1.1 varnish, 1.1 varnish\n< Age: 0\n< X-Cache: cp1066 miss+chfp(0), cp1055 frontend miss+chfp(0)\n< Strict-Transport-Security: max-age=31536000; includeSubDomains; preload\n< Set-Cookie: WMF-Last-Access=07-Feb-2016;Path=/;HttpOnly;Expires=Thu, 10 Mar 2016 12:00:00 GMT\n< X-Analytics: https=1;nocookies=1\n< X-Client-IP: 84.1.214.170\n< Set-Cookie: GeoIP=HU:05:Budapest:47.50:19.08:v4; Path=/; Domain=.wikipedia.org\n<\n Connection #0 to host en.wikipedia.org left intact\nINFO:werkzeug:127.0.0.1 - - [07/Feb/2016 19:13:25] \"POST / HTTP/1.1\" 200 -\n-===========================END OF FIRST REQUEST====================================-\n Found bundle for host en.wikipedia.org: 0x38f4c01be50\n Re-using existing connection! (#0) with host en.wikipedia.org <<<<<<<<\n* Connected to en.wikipedia.org (208.80.154.224) port 443 (#0)\n\nGET /w/api.php?action=query&list=search&srsearch=south&srprop=timestamp&format=json&sroffset=0&srlimit=1 HTTP/1.1\nHost: en.wikipedia.org\nAccept: /\nConnection: keep-alive\nUser-Agent: Mozilla/5.0 (X11; Linux x86; rv:34.0) Gecko/20100101 Firefox/34.0\n\n< HTTP/1.1 200 OK\n Server nginx/1.9.4 is not blacklisted\n< Server: nginx/1.9.4\n< Date: Sun, 07 Feb 2016 18:13:30 GMT\n< Content-Type: application/json; charset=utf-8\n< Transfer-Encoding: chunked\n< Connection: keep-alive\n< X-Powered-By: HHVM/3.6.5\n< X-Content-Type-Options: nosniff\n< Cache-control: private, must-revalidate, max-age=0\n< X-Frame-Options: SAMEORIGIN\n< Vary: Accept-Encoding,X-Forwarded-Proto,Cookie\n< Backend-Timing: D=221809 t=1454868810267092\n< X-Varnish: 1785515536, 1114948799\n< Via: 1.1 varnish, 1.1 varnish\n< Age: 0\n< X-Cache: cp1065 miss+chfp(0), cp1055 frontend pass+chfp(0)\n< Strict-Transport-Security: max-age=31536000; includeSubDomains; preload\n< Set-Cookie: WMF-Last-Access=07-Feb-2016;Path=/;HttpOnly;Expires=Thu, 10 Mar 2016 12:00:00 GMT\n< X-Analytics: https=1;nocookies=1\n< X-Client-IP: 84.1.214.170\n< Set-Cookie: GeoIP=HU:05:Budapest:47.50:19.08:v4; Path=/; Domain=.wikipedia.org\n<\n Connection #0 to host en.wikipedia.org left intact\nINFO:werkzeug:127.0.0.1 - - [07/Feb/2016 19:13:50] \"POST / HTTP/1.1\" 200 -\n``\n. I just found this: https://curl.haxx.se/libcurl/c/CURLMOPT_PIPELINING.html , maybe helps\n. thanks =)\n. great, thanks\n. @jibe-b thanks the suggestion. It would be a useful engine and seems, it isn't hard to integrate it to searx. Could you work on it? If you can't, i'll do it =)\n. @pietsch first of all, thanks. Searx tries to avoid tracking, so we prefer accessing the remote content without any kind of authentication, so the https://www.base-search.net/ web form would be the best for us if it's acceptable to you.\n. @pietsch sounds reasonable. Do you have a preferred user-agent string orsearxis okay?\n. @jibe-b if you add it to the settings.yml and restart your searx, it'll appear\n. @jibe-b turn on debug mode in settings.yml to get more detailed error messages\n. @jibe-b nice! please open a pull request, then I can add review notes to it\n. @pietsch thank you so much the collaboration and your help to make searx a more useful tool\n. thanks\n. neat, thanks\n. great, thanks\n. Cool, nice job!\nAbout the plugin translations: it's easy to fix, just wrap the plugin string variables with_()` in the template. Could you do it in this pr as well?\n. great, thanks =)\n. thanks\n. I like it, +1\n. +1, @ldidry  i like it\nIt would be good to find method(s) to defend searx in these cases.\n@dalf Limiting requests can solve the problem, but e.g. google doesn't have a hard limit. It means when the traffic isn't suspicious (random different searches/useragents) it serves way more responses without blocking than if you just iterate over pages of a single search term. It makes hard to specify good limits and can be totally remote service specific.\nAn other solution can be the improvement of the engine suspend mechanism. If an engine have the ability to request suspend for a specific time, it could help.\nDifferent suspend time for different errors would be also useful.\nWe should also consider to mix these ideas: a loose request limit AND optional captcha solving capability AND dynamic suspension of engines\n. @jibe-b there was an error in the travis configuration, but from now it shows coding style issues properly. Please check the output (https://travis-ci.org/asciimoo/searx/builds/116728852#L683) and fix the issues.\n. @jibe-b i have a small note on the code, please fix it and if you squash your commits it's ready to pull =)\nthanks\n. @jibe-b sry, i forgot to send m(, see above\n. @jibe-b, @kvch, thanks your hard work =)\n. Great, thanks\n. Hi,\nfirst of all, thanks the detailed description. Arch wiki is definitely a resource what should be the part of the IT category.\n\nWill you be interested in adding its support to searx?\n\nDefinitely\n\n\nThere is no single base URL for all possible search requests, in fact, there are six of them. Obviously, I am not an expert in searx's internals, so I am not sure if it will cause any breakage. Currently I decided to set the base_url variable to https://wiki.archlinux.org/, as it is the most used domain.\n\n\nbase_url is just a convention, it isn't a required engine attribute.\n\n\nI believe I can only return one search URL from the request method, so when a user tries to make a search in all languages, they only get results for 20 of them (see the reason in the previous paragraph).\n\n\nYes, an engine currently can't spawn more than one request. I think it's totally acceptable if the engine supports \"only\" that 20 languages by default, maybe an option to set a custom url (that's how the base_url concept can be useful - base_url (and any engine attribute) can be overwritten from the config or the engine can be duplicated with different base_url)\n\nAny comments or suggestions?\n\nGreat job.\nOne little thing: the contents of the results are formatted with their wiki syntax and seems it doesn't contain much information, perhaps we should leave it blank.\n\nI will be happy to continue working on the project (gentoo wiki, etc.) if this pull request is of any help.\n\nVery cool, it's always good to see new engines =)\n. @ukwt great, thanks\n. @ukwt thanks, i'll review/test it\n. @ukwt, data transfer between searches is not implemented yet\n. @ukwt awesome work, thank you. Could you remove the custom user-agent from reddit engine? If it's done, i'll merge the pr.\nAlso, feel free to add yourself to AUTHORS.rst\n. thank you again, we will see\n. seems it's an encoding issue again - probably the url contains utf characters\n. cool, thanks\n. @pointhi thanks, added to the list\n. added\n. \\o/ thanks\n. @jibe-b great, thanks!\nLets Finish the remaining issues in the next release.\n. I agree that the error message is ugly, but it's a good practice to raise an exception inside an engine if something went wrong. Searx has an engine suspend mechanism which checks engine errors and suspends misbehaving engines for a while, what can be useful in this case.\nA possible solution can be a dedicated exception type where no traceback displayed.\nAlso, i'm not sure if google news engine is working at all, but it needs more investigation\n. @gbonnefille nice\nPlease remove ubuntu-fr from the settings, because it's not a general service - or comment it out and it can still demonstrate the usage of the engine.\n. great, thanks\n. great, thanks =)\n. @metal3d thanks, if you consider it as a public instance, feel free to add to the wiki\n. thanks\n. The documentation should be updated\n. thanks @kvch\n. great, thanks\n. thanks\n. Yes, the problem exists, but there is no good solution. Another possibility is throwing captcha after a specified amount of requests per minute.\n. I've created a filtering middleware and using it on searx.me.\nhttps://github.com/asciimoo/filtron\nit can effectively work even with a basic ruleset:\njson\n[\n    {\n        \"name\": \"search request\",\n        \"interval\": 60,\n        \"limit\": 60,\n        \"filters\": [\"Param:q\", \"Path=^(/|/search)$\"],\n        \"actions\": [\n            {\"name\": \"log\",\n             \"params\": {\"destination\": \"stderr\"}},\n            {\"name\": \"block\",\n             \"params\": {\"message\": \"Rate limit exceeded\"}}\n        ],\n        \"subrules\": [\n            {\n                \"name\": \"rss/json limit\",\n                \"interval\": 60,\n                \"limit\": 5,\n                \"filters\": [\"Param:format=(rss|csv|json)\"],\n                \"actions\": [\n                    {\"name\": \"log\",\n                     \"params\": {\"destination\": \"stderr\"}},\n                    {\"name\": \"block\",\n                     \"params\": {\"message\": \"Rate limit exceeded\"}}\n                ]\n            },\n            {\n                \"name\": \"useragent limit\",\n                \"interval\": 60,\n                \"limit\": 15,\n                \"aggregations\": [\"Header:User-Agent\"],\n                \"actions\": [\n                    {\"name\": \"log\",\n                     \"params\": {\"destination\": \"stderr\"}},\n                    {\"name\": \"block\",\n                     \"params\": {\"message\": \"Rate limit exceeded\"}}\n                ]\n            }\n        ]\n    }\n]\n. @cy8aer what is your uwsgi version (uwsgi --version)? I'm using the same setup on searx.me (except nginx instead of apache) with uwsgi 2.0.13.1, and it works well for me.\nThe above error message happens if a new request sent after connection keep-alive ended. I'm not sure how is it possible, because filtron disables http keep-alives: https://github.com/asciimoo/filtron/blob/master/proxy/proxy.go#L15\n. @cy8aer yap, you have to use reverse proxy both between apache-filtron and filtron-uwsgi/searx.\nuwsgi part: http://uwsgi-docs.readthedocs.io/en/latest/HTTP.html#the-uwsgi-http-https-router - see \"Embedded mode\".\n. @cy8aer 429 means filtron blocks the requests because configured  rate limit is exceeded. Increase limits in filtron config to avoid this message. So, probably the keep-alive issue exists between apache and filtron.\n. thanks\n. thanks\n. neat, thanks\n. try this:\n``` python\nsearch_url = base_url + '?action=search&{query}&search_more=feeds'\ndo search-request\ndef request(query, params):\n    params['url'] = search_url.format(query=urlencode({'query': query}))\n    return params\n```\n. try with these xpaths:\n- name : sparks news feeds\n    engine : xpath\n    paging : True\n    search_url : http://sparks.leola.me/?action=search&{query}&search_more=feeds\n    results_xpath : //div[@class=\"feed_item_inner\"]\n    url_xpath : .//h3[@class=\"title\"]//a/@href\n    title_xpath : .//h3//a//text()\n    content_xpath : .//p//text()\n    categories : news\n    shortcut : sn\nit works for me\n. sorry for the delay and thanks, it's a useful feature to finetue searx\n. thanks\n. sorry for the delayed answer, it's a nice improvement, thanks\n. @pointhi is there any better solution than reverting?\n. to not introduce new bugs in the release, reverting and leaving this issue to the next release\n. This is a great feature. It would be good not to limit this to users who use javascript.\nIMHO the basic functionality (popup the info) could be javascript free.\n. @dalf it could work similar as the image proxy. If we don't want to stream the whole video through searx by default we can create an endpoint which accepts the video page url (+hash) and queries the metadata and redirects the client to the direct video url.\n. @dalf so my proposed solution is partially done =] just the webapp needed to be modified to respond with a HTTP redirect to the video download url: https://github.com/asciimoo/searx/pull/552/files#diff-02503eba222466e11a3f7ae36debdc5cR710 - line 710\n. Yap but it can be rewritten to return with a default format. If it isn't enough, a preferences option could be added where everybody can set their own preferred formats.\n. thanks!\n. thanks!\n. thanks!\n. closing, see #634\n. Compelling!\nMy 2 cents: the results are too loose to me. I prefer compact result pages where i can see 8-10 results without scrolling. Currently only the \"default\" (we should rename it) theme compact enough.\nAs i see it doesn't modify the structure of the theme and i don't prefer a new theme with lots of duplicated template code. Maybe a better solution would be a selectable CSS styling for oscar - like in courgette theme.\n. closing - f496dc353d336ffe11dde161963d72366f0eee16\n. @rototom i don't know what can cause the constant blocking. Do you use scripts to fetch large amount of results or do you have custom settings?\n. @NIXOYE is this happens on searx.me?\nTry to reset your preferences.\n. @a01200356 great, the only problem is that it isn't compatible with the existing non-tor configuration. It would be cool to provide both to the users. Could be a separated settings_tor.yml for the tor config.\n. @LuccoJ added: fbbb307\n. thanks, package rereleased\n. @xinomilo good idea, maybe it is a better platform to discuss administration questions than github issues\n. @xinomilo great, thanks. I gladly help you with the administration\n. @fredhampton engines have weight 1.0 by default which affects result scoring. You can change it in settings.yml if you add weight: n to an engine.\nCurrency convert engine is a good example which has custom weight.\n. great, thanks\n. > I don't know if there is way to add a hash of the source code of each engine. \nsure, we can store the sha hash of the engine files\n\nSome instances follow the master branch, some others follow the release I guess. That's why the Searx version number is not enough in my opinion. \n\nyap, it would be good to add at least the git HEAD id to the version string (e.g.: 0.9.0+5dd2340).\n. The only question is how to handle these crashes. It happens in the engine initialization phase, so we can exit/retry/skip.\n. @PwnArt1st I'm hosting searx.me. I don't have any special setup (except some extra source IPs) and it can easily serve >10k searches/day.\nSince last week I'm also using filtron to protect searx.\n. great job! thanks\n. Result merger partially handles it: https://github.com/asciimoo/searx/blob/master/searx/results.py#L186\n. ah yes, sry\nThen maybe infobox merge function could handle these deduplications.\n. seems solved - thanks to @a01200356 \n. thanks, fixed\n. thanks the detailed report\n. @jibe-b searx.me does not support http:// anymore, only https://. Delete it from your search engines and add https://searx.me/ and it will work. \n. thanks\n. fixed: ea19e19\n. @blyxxyz thanks\n. IMHO it's better to have one favicon for all themes, because its main purpose is to help recognize your search tabs and it's confusing if it changes on theme switch.\n. great, thanks\n. settings.yml aleady contains urbandictionary, it is just commented out. Please delete one of them.\nAlso, it would be good to disable it by default (disabled: true).\n. okay, thanks\n. thanks\n. thanks\n. Nice\nplease fix pep8 errors: https://travis-ci.org/asciimoo/searx/builds/143576646#L713\n. Thanks\n. \\o/\nthanks\n. cool, thanks\n. thanks\n. thanks\n. thanks\n. thanks\n. @stepshal  we follow pep8 code style recommendations, so this pr is invalid, closing\n. thanks\n. duplication of #39 \n. nice, thanks\n. @davidar please use lxml instead of adding beautifulsoup as a new dependency\n. great, thanks\n. thanks, fixed\n. thanks =)\n. thanks\n. Wouldn't be better to create a specific engine which rewrites the results by default instead of introducing a new plugin which also checks all other engine's results?\n. > The plugin isn't specific to this engine, as any engine could potentially return DOI links.\nThat's true but the chance that it appears on an average search is very low imho. Another solution is setting the plugin disabled by default.\n. great, thanks\n. thanks\n. very-very awesome, thanks! =)\n. thanks\n. ohi,\nI'm hosting searx.me. I do not store/collect logs. But I can say anything, it cannot be validated. That's why searx is free software, you don't have to trust somebody you don't know or a system you can't control.\n\n\"Using Searx.me from Finland IP\" ===|!== \"Using searx.me from Japan\"\n\nThe only information forwarded to search services among the search query is the browser's Accept-Language HTTP header, nothing else.\n. thanks =)\n. thanks\n. @dalf do you have any trusted information about these mirror sites?\n. thanks\n. thanks\n. @logouthere  sorry, I was wrong, searx doesn't send this option to the services yet. It is just a planned feature\n. @davidar currently only oscar theme supports client side plugins.\n. closing - see #658\n. @ibrokemypie could you send a screenshot or example query where the problem occurs?\n. @dalf thanks!\n. already fixed, see:   89784ae. awesome, thanks\n. thanks\n. great, thanks\n. thanks\n. cool, thanks =]\n. neat, thanks\n. thanks. thanks\n. thanks\n. thanks\n. thanks\n. Thanks.\nI've started to reimplement the target=\"_blank\" feature to work without js and it will fix this issue too.\n. nice.\nPlease set it disabled by default in settings.yml if it cannot use https.\n. @pydo, thanks.\nPlease specify the default categories in the engine (categories = ['files', 'music', 'videos']) or in settings.yml (categories : files.music,videos)\n. thanks!\n. awesome! thanks =]\n. Thanks!\n. @Hypolite port and bind_address in settings.yml is used only if you run it without uwsgi (python searx/webapp.py). \n. I think this is fixed, @gpkvt could you verify it?. @Yuyu0 it would be good to call index_schedules() periodically if a timeout exceeded (e.g. from the request() function). Wolframalpha engine's session handling is similar.\n. thanks =]\n. The best way to preserve your custom settings is hosting your own instance. You can set your preferred defaults in settings.yml and no cookies/custom URLs are needed.\n. The feature is implemented (38d6ba4066a474c1b13e7ccb6f9ea92b43702a4a). You can check it out on the bottom of https://searx.me/preferences .. thanks\n. @rieje, searx.me doesn't use proxies to reach search services. Proxies are one more entity where the data possibly can be intercepted, so while it can increase privacy it can also create new attack surface.\n. thanks\n. The first beta version is available on searx.me and can be activated from settings.yml in other instances.\nThe proxy is a standalone app, available here: https://github.com/asciimoo/morty.\nI'll document the setup if it found to be usable after some tests.\n. @GinoHerelam, you can finetune timeout values in settings.yml for each engine.\n. @Athemis thank you, very-very cool.\nalso, @pydo @kvch thanks the useful reviews =]\n. There are service independent networking issues. Working on the solution\n. thanks!\n. thanks!\n. @Pofilo we can release a new version (0.13.2), but as I see these fixes don't solve the problem permanently for everybody.. This solution raises privacy questions. Session cookies can be used to build a user profile. It would be good to find a solution which doesn't increase the sent information about the instance.. @dalf awesome! This part was a very week point of searx's code - tightly coupled and messy. I'll review it in the following days, thanks! =]\n. great, thanks\n. thanks\n. can be handy on debugging, thanks\n. \\o/\nthanks\n. onice! could you fix the pep8 errors?\n. NamedTemporaryFile has delete parameter which deletes the file on close, maybe it works: https://docs.python.org/2/library/tempfile.html#tempfile.NamedTemporaryFile\n. great, thanks\n. thanks\n. \\o/ thanks\n. It would be good to rewrite the menu to js-free, then we could remove the js required message.\n. @a01200356 awesome!\nI'm thinking on if there is a way of automatic gathering of the engines supported languages, or at least a method to validate our language list against the search service, because now it is relatively hard to detect if a search service's language settings changed.\nNot sure it is the scope of this pull request, but it would be good to find a way to test search services in that manner - e.g. in https://github.com/dalf/searx-stats2 ? =]\n. @a01200356 supercool! i've started the review, looks very promising, thanks!. @a01200356, works well and the code is clean and understandable.\nPreferences is the only part which is still a bit confusing to me.\nIf search language is the default, then all the language support checkboxes are checked in the engines tab, but if e.g. Brezhoneg is selected as language, only dailymotion is checked however it still searches in all engines. Probably a more informative solution would be to display the list of supported languages for all engines as a dropdown or just keep the previous behavior: check the checkbox if the engine supports any language customization.. Pulling and creating the preferences mod, thank you again the awesome pr. thanks, fixed: 142cd87\n. Much cleaner and more understandable! thanks. very-very cool, thanks\n. thanks\n. seems, bing paging doesn't work\n. It is probably a timeout issue. Btw, it would be good to display a more informative error in these cases.\n. @der-domi sorry for the late answer, could you verify the fix?. It works for me: https://searx.me/?q=f%C3%BCr&lang=de_DE\n. great, thanks\n. It works well on other instances.\nSeems startpage throws captcha, you can check it if you search on: https://searx.me:3000/?mortyurl=https%3A%2F%2Fwww.startpage.com%2F&mortyhash=d83b2a41b2bb7e93f00ddf160acd2bc3025503c5e1441089ed04f085fc9c246b (proxified view)\nSolving the captcha doesn't help, because startpage inserts a mandatory session identifier attribute to the search form which can be used to track searches. It displays captcha again if the session id is not valid or missing.\n. Seems it is an actively used and useful feature, so keeping it. Thank you for your responses\n. @negroblack chrome cannot send POST on the URL which is set to the default search URL. Is searx the default search engine in your case? If so, try to rewrite your default search URL from /?q=%s to /search?q=%s\n. @arkgit it works for me with the latest chromium. Do you have any custom browser settings/plugins?. thanks\n. thanks. @cy8aer how can i reproduce the issue?. @cy8aer thanks, checking. I rewrote the result escaping, that can cause problem in some engines. qwant fixed in c3dcebb, could you verify?. thanks. great, thank you. nice! thanks. In the future it would be good to support custom time ranges.. @boyter great, thanks - both the project and the clarification =]. Both added, thanks. Thanks for the great source.\nI think disabling it by default would be better (disabled : True), because it isn't a general purpose video search service. Awesome, thanks. It works for me: https://searx.me/?q=%21images%20south. Awesome! thanks. agree, thanks. @dalf answerers are implemented for exactly this purpose: https://github.com/asciimoo/searx/tree/master/searx/answerers (not documented yet)\nI think it is a cleaner interface for offline calculations. What are the benefits of the offline engines compared to the answerers?. > Implement an engine that look up results into anything that is not HTTP : news group, a local database.\nThis is how answerers should work =]\n\nIt use thread as the others engines with timeout (I think about Sympy).\n\nAnswerers can be threaded too, but it still does not solve the main problem, that the thread cannot be killed on the given timeout and eats all the cpu times after a while.\n\nwhat do you think this commit : 12b077c\n\nCool! Agree with the changes and the small refactor makes the code more modular and understandable to me.. @dalf thanks the explanation.\n\nanswerers doesn't mix with other engines\n\nI implemented this way for privacy reasons. I thought it is a good idea to stop if there is an answer already from the answerers - so data leakage to external resources can be prevented.\nAs a solution, we can add a flag to the answerers which could indicate if the search can be stopped or not.\n(If no results provided by the answerers, the search will continue in the current code too, which can allow (pre)computation that can be used by the engines)\n\nThe purpose here is more about an usual/normal engine but doing a request that is not a HTTP request : querying a SQL server / NoSQL server for example. Example : a list of addresses that can used in the map category. In this case, I think the user still want to results from photon and OSM.\n\nThis is more like a scheduled/periodical event than a user triggered if i understand correctly.\n\nI know that a thread can't be killed, but process can be (not sure if SymPy haven't issue about that) : it's a first stone to implement this. Only an idea.\n\nYou are right, it could be a child process terminated on search timeout.\n\nI know this PR doesn't solve the following problem, but when there is more than one HTTP request to get the results : why searx blocks the engine if there is an HTTP error on the first request, but not on the second one (done in the response function of the engine without the right user agent, without time-out).\n\nI agree, it would be good to parallelize those requests.. > Use case : in the map category, the user searches for \"Riviere Du Loup\", and results comes from from OSM, Photon, and airport_offline engine.\nAh, okay. Then, i think we are talking about the same thing with a different terminology (offline engine vs answerer).  My goal with answerers was exactly the same: create offline result providers which can replace online ones.\nIt is true, sometimes it's not enough to only replace the results, answerers should handle this too.\n\nthis is most probably out of the scope of searx and / or useless (just a little experiment)\n\nI don't think so. It can strengthen privacy and give more useful features and another level of customization.\nThat's why i created a dedicated module for it, instead of polluting the engines with another slightly different use case.\nI admit, answerers are still very basic and they need to be more polished, but in long term if the number and the size of the answerers/offline-engines grows,  the separation (at least in code level) has advantages.\nWith the current design of answerers (one folder for each) it is possible, to include external answerers as submodules and if one of the answerer's code grows, it has it's own directory. It would be harder to implement these to engines.. Thank you. @dalf how can cause problem the mentioned search duration check?. ah, i see, great, thanks.\nGlobal+read timeout sounds reasonable to me.. Great, thanks, now the arrows are visible for noscript users too =]. thanks. I don't think extract text does anything else, and it works fine with unicode characters (tested with !gos \u6ca1\u6709\u90a3\u4e2a\u6587\u4ef6\u6216\u76ee\u5f55).\nThanks!. @ky0ncheng dependencies has been updated by @dalf, could you try to build again?. @ky0ncheng what is your operating system/hardware spec?. Meh, I totally forgot to pull this PR. @dalf could you please rebase it to the latest master?\nSorry for the inconvenience.. great, thanks. great, thanks. It is really nice that we have a pure css modal window, but it has other bugs too. E.g. the middle click (open in new tab) doesn't work which is a big regression.\nAs I saw it isn't possible to create css only modal window which also can open the given image on middle click.\nPossible solutions:\n - revert the changes\n - user can enable/disable the current modal window functionality (question: what would be the good default? on or off?)\n - a css guru fixes the missing features somehow\ndevs what do you think?. It would be good to fix it instead of remove. I figured out that the request requires an apiGuard parameter which can be obtained here: https://hulbee.com/generateApiGuardToken. As I see you can do multiple requests with one apiGuard token, but not sure if it has time or any other limits.\nUPDATE:\nToken expires after 1-2hrs. thanks. @dalf nice! thanks. thanks. nice, thanks. @dalf IE 8 support isn't mandatory. Strong HTTPS settings are also incompatible with IE 8, so it is implicitly unsupported. =]\nIf there is no reason to stay in the current version it would be more fail-proof to use the same version.. @courgette do you have any information about the license of searx/static/themes/courgette/img/bg-body-index.jpg?. done in #816. @dalf very cool!\nIt would be good to wait for the py3 branch merge with this, because debian packaging will use that code. I've finished with the rebase of the py3 branch to the current master, but there still can be bugs both in py2/py3 side.. @dalf could you rebase and resolve the conflicts?. Great, thank you.. thanks. Thanks. thanks. Onice! Very-very useful PR. It drastically reduces template code duplication and makes the about page translation easier. However travis fails - seems both opensearch.xml and opensearch_response_rss.xml are missing. Could you fix it?. Useful feature! Could you add it to the json output too? I think it would be useful to get this information in that form too.. Great, thanks. Thank you. Legacy is just the name of the theme - I admit it isn't the best.\nWe can rename it.. First of all, thank you the new engine. It looks like a fast and well functioning torrent search.\nSome thoughts:\n - It would be good to parse seeders/leechers and file size (see torrentz engine as an example)\n - I'm not sure if it is good to set it active by default in all categories. There are already torrent engines in video/music results and I'm not sure it is good if torrent results dominates these categories by default.. Great, thanks!. Nice, thanks. It would be a useful feature indeed.\nThis part of the original code is a bit messy and hard to understand. It would be good to refactor.\nAnd the PR does not cover all the custom query syntax types, just the engine shortcuts. Allowing position independent engine/category name and language setting would be useful too, i think.\nSo, I'd keep this PR open while the above issues are not resolved.. awesome, thanks. great, thanks. thanks. Nice - both the UI mod and the code simplification, thanks. fixed, thanks the report. thanks!. duplicate of #695. @eemantsal searx uses the Accept-Language HTTP header for the locale settings. How could be the user-agent used for this purpose?\nIP based language selection is not always accurate and can cause problems if you are not in your home country/behind a proxy. I don't think that it generally improves searx.. > What I meant was that web browsers have a method to communicate the language setting to websites\nThis is the Accept-Language HTTP header: https://www.w3.org/International/questions/qa-lang-priorities.en#background and searx uses it for the locale settings. Search language does not depend on this value currently. Allowing this behavior in search language selection could be a configurable option for an instance in the future.\n\ndo you really think that the majority of computer and mobiles users in all the world (who I suppose are your target; if you are just centering in the US and its satellites then I should probably stop talking), are using a proxy?\n\nThis software is built and defaulted to support privacy minded users, even if some of the solutions are not the most comfortable for regular users with fancy smart phones and tablets.\nProxy/VPN/Tor usage is very popular in this field, I think.\nFor more details, see our contribution guide: https://asciimoo.github.io/searx/dev/contribution_guide.html\n\nare using a proxy?\n\nOf  course, and I recommend it to you too. =]. I feel a bit overkill to introduce a new dependency for this feature.\nWouldn't be enough to add the ability to use environment variable for secret_key which overwrites the value defined in settings.yml? You can store it in a file and source it before starting searx.. @Profpatsch I still can't see the benefit of removing the secret key from settings.yml and disallowing the option of setting the secret key there.\nIf you place the key to another file, searx needs read permission for that file too. What extra safety is provided by this solution compared to the optional redefinition of settings.yml's key with an environment variable (which is ~5 lines of straight-forward code and backward compatible)?. great, thanks. great, thanks. Great! @kiney, @Wonderfall thank you for your work.. Thanks the fix.. > There are still plenty of different things to do, but I think it could be the time to merge, so more people could test it, or even contribute, @asciimoo what do you think ?\nI've tested it in the previous days and I have to agree. I really like it, especially the image result layout and the js-free engine preferences view =). Also, it is much faster and clean than oscar with all the bootstrap boilerplates.\nThank you for this great work!\n(One minor thing: Travis fails because a small pep8 issue, could you please fix it before squashing the commits? EDIT: it is already fixed, I refreshed my tabs.. =) ). Great, thanks. There are service independent network errors. Please use another public instance while it is down or setup your own searx: https://asciimoo.github.io/searx/user/own-instance.html\nThanks. @neggs I'd suggest to put static files to the searx/static/ folder. Templates are for python generated content.. @neggs try to restart searx after adding a static file, or serve static files directly from nginx with the following additions to the server { .. } block:\nlocation /static/ {\n           alias /path/to/your/searx/static/;\n           autoindex off;\n    }. @OJFord they provide different results.. Thank you. I think this is not a searx specific bug. Pyparsing is not a direct dependency of searx. The root cause is that pip and setuptools are outdated.\npip install --upgrade pip && pip install --upgrade setuptools solves the problem.. > probably, but searx could make this smoother for people who are in this situation, no?\nAgree, manage.sh could check pip and setuptools version before package updates.. thank you. @anarcat searx uses the instance_name string from settings.yml for this purpose. It is \"searx\" by default, but you can modify it. E.g. firefox displays Add \"searx.me\" on searx.me.\nI don't think that $(hostname -f) would be a better default. It can be confusing on systems with no proper domain configuration (my hostname is often just a single character).. >  I would argue that having \"a single character\" is still better than \"searx\" as a default, because then it's unlikely to conflict with others (unless the same letter is chosen of course). Besides, as you said it's only the default...\n\n(That's a rather strange convention, actually - i haven't seen many hosts with single-character hostnames...)\n\nIt was just an example. What if the hostname isn't related to searx or there are multiple services under the same hostname? Or, the default hostname of an AWS server is the server's internal IP, in this case displaying hostname would be an automatic info leak.\nFQDN is not a unique identifier for services, so i still don't agree that it would be a better default.\n\nWell, if not \"hostname -f\", something unique should be chosen, because there are lots of conflicting instances out there.\n\nThis makes sense. > well, it would more unique than a static default, so i think it would be better...\nMore unique doesn't mean that the whole solution is better. It would solve (more or less) your exact problem, but it would also introduce new ones.. Great, thanks.. The network latency of the foreign search requests is much bigger than the performance penalty of flask. While we can't prove that flask significantly degrades our performance, i have to agree with @Nikola-K.. @kvch thank you the review. Seems, there isn't any other note on the code, so i'm merging it.. Thank you =). @Apply55gx please modify translations on transifex: https://www.transifex.com/asciimoo/searx/dashboard/\n(see https://asciimoo.github.io/searx/dev/contribution_guide.html#translation ). @cy8aer thanks.. As I see, it tries to fetch the latest FF versions on every startup (why only if debug mode is turned off?), which makes the startup time longer. It would be good to cache the versions or fetch it parallel. What do you think?. >  when in production, I don't think it's an issue if startup time is a little bit longer.\nWell, it matters for me e.g. on searx.me if a webapp restart takes more time after every update. And, the current init time is far from the optimal, so I wouldn't increase it.\nAlso, I'm thinking on adding an optional init() function to the engines, which could be parallelized, so engines with heavy initialization (like wolframalpha or soundcloud) wouldn't affect the startup time of searx.\n\nThe other solution about parallel version could be simpler even it's more hackish. update_firefox_version() can create a thread to do the real work.\n\nI can't see why would be it hackish, this is the best solution for me =). >Just to be sure, Is it the fetch at the start of the searx that block this PR ?\nA static tmp file could solve the issue :\n    a file is create in a defined location\n    the file is updated once a week\n    when an instance is (re)loaded, the file is only loaded\nOnly issue : define this static file (or database).\nYes, it would be good to avoid hitting mozilla on every app reload.\nThe main reason why this PR is still open that I have doubts if the complexity of this feature isn't bigger than the original problem itself. It's not that hard to update the user-agent version strings occasionally (before releases) by hand and I can't see why would be beneficial to keep it more up-to-date.. @cy8aer this is strange, I can't reproduce. What is your environment?. Hmm, I'm using the following command: uwsgi --plugin python -w searx.webapp --http-socket 127.0.0.1:4000 --master --processes 8 -H env --enable-threads --lazy-apps. Without --lazy-apps option I can reproduce the mentioned issue (still no idea why). Could you try the --lazy-apps flag (http://uwsgi-docs.readthedocs.io/en/latest/Options.html#lazy-apps) ?. > I have the issue about the logo, should the one from oscar/pointhi be used or the one from oscar/logicodev ?\nWell, this is a good question. There is no strict rule about which logo to use. The \"official\" logo is the magnifier with horns, but I think all the themes uses different visuals. So, feel free to use anything else if you want. =). Thanks.. Thank you.. First of all, thank you for updating the translations.\n\nSince it's intended for devs, I suggest to remove the use of Transifex client from your translation page and just add my previous commands, so that you won't loose other people like me either. \ud83d\ude03\n\nIn long term the currently documented method is the most comfortable way to sync translations with transifex imo. But, we can document that the use of the transifex client is fully optional, only synchronization of the repo and transifex is important, not the way you do it.. @ShalokShalom preferences coming from GET HTTP parameters have bigger priority than preferences saved in cookies, so cookies will be overwritten by GET params. You have to rewrite the search URL to the new one genearated on preferences page if you save the preferences.. @ShalokShalom what would be the desired functioning?. This would require storing user's preferences data on server side what raises privacy concerns.. > You seriously dont get, what i mean.\nSeems, it is true. Could you describe your problem a bit more elaborately?\n\nThere is a save settings button on the bottom of the page. It works sometimes, and sometimes not.\n\nIt works for me every time I push the button.\n\nAh yeah, and the function so create URLs, who save the settings, is gone.\n\nIt is on the bottom of this page: https://searx.me/preferences. I agree, this is the good time to experiment. We could do the same with #1034.. Morty can listen on HTTP only. If you want to use it with HTTPS, put it behind the nginx and use that address in your searx config.\nExample nginx snippet:\nserver {\n    listen 3000;\n    server_name xy.com;\n    [SSL config]\n    location / {\n         # morty's HTTP address\n          proxy_pass http://127.0.0.1:4006/;\n    }\n}\nWith the above configuration morty_url should be https://xy.com:3000/ in searx settings.yml.. > In /Users/imac/ there is a empty text file named go but no directory.\n\nAny idea what I can do?\n\nRemove that file and create a folder with the same name (in console: rm /Users/imac/go && mkdir /Users/imac/go). > In order for the proxy to work in the search results how do I configure Searx/Morty to do that? \nI've created a documentation page which explains the setup: https://asciimoo.github.io/searx/admin/morty.html\n\nI've purchased a book on docker & command line which I'm intending to read.\n\n:+1:. As I see, both servers use HTTP/2, this is the only difference compared to e.g. searx.me, but I don't know if it is related to the bug.. @Angristan do you receive valid response if you perform the request directly to searx without nginx?. Uwsgi shouldn't cause the problem. You can test it through uwsgi with the following command: uwsgi --plugin python -w searx.webapp --http-socket 127.0.0.1:4000 --master --processes 2 -H env --enable-threads --lazy-apps. Then run curl localhost:4000/opensearch.xml -I.. -H env section specifies the path to the virtualenv which is ./env in the example. Change it if your venv has different path.. @Angristan thanks for the info. @vedranmiletic does the above solve your problem?. > So lets say I make my key \"key123\", in the settings.yml I place http://127.0.0.1:3000/ (or do I place my searx url:3000?) and add my new key \"key123\"\nIn this case your settings.yml have to contain these lines:\nresult_proxy:\n    url : http://127.0.0.1:3000/\n    key : key123\nand execute the following command (in shell) to start morty:\n\"$GOPATH/bin/morty\" -key \"key123\" -listen \"127.0.0.1:3000\". I think there are at least two errors. First, you have a space before the config entry result_proxy in settings.yml, which causes config error for me if I restart searx with it. So, probably the second error is that you edit a wrong file or you don't restart searx after settings has been changed.. Probably, line 1247 explains it in travis job 1755.1: The command \"./manage.sh styles\" exited with 127. (./manage.sh: 83: ./manage.sh: lessc: not found) - related PR: #995. Great.\nNow it has version issues: WebDriverException: Message: Can't load the profile. Possible firefox version mismatch. You must use GeckoDriver instead for Firefox 48+. Profile Dir: /tmp/tmppRU3uN If you specified a log_file in the FirefoxBinary constructor, check it for details.\nAnd, there is an other error: ./manage.sh: 83: ./manage.sh: lessc: not found when running ./manage.sh styles. Could you investigate it too?. No clue why. This is the purpose of the json and xpath engines. settings.yml contains some exaples about the usage of these engines.. duplication of #978 . @aurora-potter the main problem is that I cannot ensure that other instances don't leak data.\nPersonally I don't want to implement privacy degrading features for my instance, however anybody can create and host a wrapper you mentioned. A good example is https://searxes.danwin1210.me/ which randomly selects a searx instance on every search.. @aurora-potter they don't know the original IP, that's true, but the search data is accessible, also the response can be tampered. It is +1 uncontrollable component in the chain.. @piamancini thank you, added. Commit 8dfc02a can be a good base if you plan to support rst format in the future.. @NIXOYE restart searx after changing template files.. @webworker01 try to turn on debug messages in settings.yml.. > What is the process to merge in asciimoo/searx ?\n\nMake a huge PR from time to time ? as they arise ?\n\nIt's totally up to us. If a maintainer receives or writes some code which adds value to searx, then she/he can submit it to the main repo.\n\nSo I should move #1025 to my fork ?\n\nYes, that would be the proper way in the future, but I think, it's not necessary to put any extra effort to change previous contributions to fit to this model, so it's ok here.. Filtron is just a middleware between nginx and searx. \n\nI think the reverse proxy nginx to filtron is easy and well documented - any chance to get some info on how to debug this? I am simply not sure if it is working or not.\n\nYou can debug the whole chain with curl. First try to get searx directly (e.g. curl 127.0.0.1:8888), then try through filtron (e.g. curl 127.0.0.1:4004) then with nginx (curl 127.0.0.1)\n\nHow is filtron supposed to be running? Should it be the same user or a separate user? Startup via systemd? CRON? Other solutions?\n\nIt's your decision.\n\nHow does the reverse proxy filtron -> uswgi work? Seems this was discussed in #543 by user @cy8aer but I do not understand the solutions there. From the description of filtron, I would assume that filtron directly writes on port 8888 and the only requirement is that uwsgi listens on 8888 - but how would I get this done?\n\nFiltron cannot use wsgi protocol so you have to add the following line to your uwsgi.ini if you use searx with uwsgi:\nhttp = 127.0.0.1:8888\n. @scroom awesome, thanks for the great article. We already have a small documentation section about filtron and I'd appreciate it if you could extend it with details from your article: https://asciimoo.github.io/searx/admin/filtron.html. @scroom https://asciimoo.github.io/searx/dev/contribution_guide.html#documentation. @jibe-b, I can search on core.ac.uk without api key, which is the most privacy aware method of fetching data.\nDemonstration: curl -X POST -d '{\"basicQuery\":{\"count\":10,\"searchCriteria\": \"[SEARCH_QUERY]\",\"offset\":0,\"sortByDate\":false},\"facetMap\":{}}' https://core.ac.uk/search/api/search. @07416 see #869. @07416 i disabled yahoo in commit e2afc1c. @m7268900062 feel free to add it to https://github.com/asciimoo/searx/wiki/Searx-instances. Great, thanks! Both the PR and the review =). Seems it doesn't conflict with the master, so i'm pulling it. Thank you @Apply55gx @kvch @woorst. @jugi1 this would be definitely a useful addition. As an alternative solution there are dark themes available on userstyles.org: https://userstyles.org/styles/browse/searx. @jugi1 https://www.transifex.com/asciimoo/searx/language/sr/. @jibe-b i've added some error handling to the json engine, it should work now.. @jibe-b can you test it now (7a9b18e9)? Seems there are results with no url.. Hi, thanks for the translation. I've updated it in the repo, you can check it out: https://searx.me/. I think this issue is fixed in 9ab8536479f30960e79ed165369e97c635ed1be7. Could you verify it?. > Please standardize the 'Search URL of the currently saved preferences' to something shorter.\nStandardization isn't trivial, because everyone can host any version of searx. E.g. searx.me is always on the latest master, some others only install releases, etc.\nAlso, it is hard to compress the saved settings if we don't want to store user settings on server - and we don't want it. =]\nThere would be another option, to store only the changed settings compared to the server's default in url, but in this case changing the server's default config would affect the users with saved preferences, which is worse than using a long url imo.\n\nAlso why does this return an error ? \n\nIt was a bug, fixed in 128eb76. > We can image that there is a default configuration, not the one in settings.yml, but an absolute one. In this configuration, all engines are disabled, the image proxy is disabled, and so on.\nThis configuration could be used to create the settings URL : each instances would know how to infer the missing values. This would allow the same settings URL across searx instances.\nThis also requires every searx instances to be updated to the latest version which implements the mentioned solution. This is also true for the current implementation, it would work if all the instances were in the latest master. (Btw. i'm preparing the 0.13.0 release - perhaps it pushes maintainers to upgrade to the latest codebase).\n\nA second thing, if we decide to change the setting URL format, a bit field could be use to encode all boolean values (image proxy, include or not this engine).\n\nGood idea, we can optimize a lot on the size of the url with this.. Good solution indeed! Don't you want to create a PR with these changes?. Sorry, I misconfigured something after the upgrade to 0.13.0. It should work now.\nBtw, I don't recommend to use searx.me, it is still very crowded and has errors because of the massive amount of requests. Check out other public instances: https://github.com/asciimoo/searx/wiki/Searx-instances or host your own =). This is strange.. It doesn't throw any error and it works on my local instance.... Seems like google is throwing CAPTCHAs. I still don't know why and how to solve it. Any ideas?\nPerhaps it is related to 9ab8536 as @dalf mentioned.. I reverted 9ab8536 on searx.me and google engine works again. So it's probably related to 9ab8536.\nAny ideas how to fix the language setting without getting CAPTCHAs?. Hopefully 6fdb6640 and 6eb95038 will solve both problems. I've deployed it to searx.me.. @muppeth upgrade to the latest master and wait few minutes/hours before restarting searx. The captcha page says  that \"[...] The block will expire shortly after those requests stop.\".. Seems, the problem is solved. Feel free to reopen if you notice the opposite.. @muppeth interesting.. It works fine on searx.me since 0.13.1. Is your instance public?. @muppeth perhaps some scripts abuse it and the blocking is valid. Do you use firewall or any other defense mechanism to protect your instance? I'm using filtron on searx.me.. @Pofilo it is solved in v0.13.1. It works well on searx.me. Have you tried to suspend google engine for 1-2 hours?. This is strange.. Are you sure that nobody abuses your instance?. @dalf If @Pofilo is using fr-FR, the code is identical on that call path I think, that's why I don't understand the bug.. @dalf could you fix this?\n@josch  I'm preparing a new release (0.13.1) with other fixes, so perhaps it would be good to wait for 0.13.1 with packaging.\n. @Artur96 Awesome, thanks. We will update the repo soon.. Then we have three options:\n - Do another request to fetch these information\n - Exclude it from the infobox\n - Create a link from it which points to wikidata\nThe third option is the preferred solution to me. It doesn't require another request and doesn't hide information.. @gitbugged do you have \"doi_resolvers\" section in your settings.yml (https://github.com/asciimoo/searx/blob/master/searx/settings.yml#L741) ?. lol, that's quite a funny bug =]\nSeems like the problem is in the json engine which can return float result title. Openaire engine triggers the exception. Working on the fix.. fixed in 0969e50. @joshu9h 123456 is interpreted as an integer in yaml, put quotes around it: key: \"123456\". Seems like the problem is in nginx and in filtron also.\nYou have to increase the proxy header buffer size in nginx:\nhttp {\n  proxy_buffer_size   128k;\n  proxy_buffers   4 256k;\n  proxy_busy_buffers_size   256k;\n}\nfiltron patch is in progress.. filtron fixed in https://github.com/asciimoo/filtron/commit/d200b21c4284f21d2cea42f8d6d13b1fa9d8601c. I've added it to /etc/nginx/nginx.conf's http section and it solved the problem for me.. @scarejar update to the latest master. Everybody who has google errors, could you post what environment do you use?\n - Where is the instance being hosted (big datacenter/aws/digitalocean/pc/etc...)?\n - Is it a public instance?\n - Is there any other service running on the same node?. Seems like you don't have python development headers installed. Run apk add python-dev to fix the issue (https://pkgs.alpinelinux.org/package/v3.3/main/x86/python-dev).. It isn't the same, there are other missing headers (now libxml2-dev and probably libxslt-dev too). See the installation guide for all dependencies: http://asciimoo.github.io/searx/dev/install/installation.html#basic-installation. Do you have permissions to write to /var/run/uwsgi/? Try to change the socket path:\nsocket = /tmp/searx.sock. @SunilMohanAdapa thanks for the detailed explanation, you are right.. > Can we have a patch release with this fix included?\nSure, I'm planning to fix some engines and we can release at the end of this week.. probably related to #1141. @As4fN1v what is your firefox version and which searx instance do you use?\n  . Findx could be a dedicated engine to be able to extract image, video and map results as well.. http://asciimoo.github.io/searx/dev/search_api.html. It works for me on searx.me and on searx.kvch.me with both firefox and chromium.. > But things have changed and now the Occitan is part of it\nCould you provide links?\nAs I see it still doesn't work:\n[...]\nbabel.core.UnknownLocaleError: unknown locale 'oc'\n$ python3 -c 'import babel; print(babel.__version__)'\n2.5.3. This can be a useful addition, but I wouldn't change the api. It could be !!.. @thiswillbeyourgithub yes. @thiswillbeyourgithub it is documented here: http://asciimoo.github.io/searx/user/search_syntax.html . Also, the shortcuts are listed on the preferences page under the engines tab: https://searx.me/preferences .. Seems the api has changed. Could you print the result variable before the above assignment and post the output here?. > Or is there some way to get it to log more?\nYes, if you add it manually to youtube_api.py. Please place a print(result) line right above line 54. Then restart your searx and perform a search with this engine.. As I see your code has been modified. Youtube sends videoId and the codebase of searx also contains result['id']['videoId'] (https://github.com/asciimoo/searx/blob/master/searx/engines/youtube_api.py#L54) but in your code result['id']['videoid'] is all lower case and that causes the error.. @ShellCode33 the code is open, feel free to fork it, we appreciate pull requests. If it is really a 10 second fix and you use searx every day, perhaps it worth this little time. You can find the related documentation here: https://asciimoo.github.io/searx/dev/quickstart.html#how-to-compile-styles-and-javascript .. @usernameisntallowed it works well, thanks for the contribution!. @trankmichael searx/settings.yml is part of the repo, so i don't think adding it to the projects gitignore would be a good solution, but I agree on searx-ve, it could be included to searx's gitignore.. > searx/settings.yml could be replaced by searx/settings.example.yml and then we can add searx/settings.yml to the .gitignore.\nThis can be done without modifications. Copy settings.yml to e.g. prod_settings.yml and start searx with SEARX_SETTINGS_PATH=\"searx/prod_settings.yml\" python searx/webapp.py.. added in #1162, thanks @trankmichael.. Good fix, thank you. Could you update the generated css files? Instructions can be found here: https://asciimoo.github.io/searx/dev/quickstart.html#how-to-compile-styles-and-javascript. @IceGiant don't invest that many effort to creating the minified css, I have a working environment for this, so I'll do it. Thank you again.. done: a173c5b4e2a543d7091e260767978db57f7a0370. Remove the following section:\n```\nsearx start\n\n    Options FollowSymLinks Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n\n```\nIt overwrites ProxyPass \"/searx/\" \"http://127.0.0.1:4004/\". Also, you can test filtron on the local machine with curl http://127.0.0.1:4004/. If it returns correct response, then the error is in the webserver config, otherwise filtron/searx isn't configured properly.. @MarcAbonce What if we combine the two solutions and include all the languages which are supported in at least 10 engines or by both google, wp, bing?. related: https://github.com/mcclymont/nasearch-ruby/issues/4. @GitHubGeek set your custom url (e.g. https://foobar.example.com:1443/) as base_url in settings.yml: https://github.com/asciimoo/searx/blob/master/searx/settings.yml#L14. . I guess searx is behind a webserver which automatically compresses opensearch.xml - probably because of the .xml ending.\nThe solution should be the same as in #532 .. As I see from the number of up votes, this is a desired mod, so I'm going to implement it soon. Thanks for the contribution and for all the feedback.. @thiswillbeyourgithub it can be achieved using the ? operator (e.g. ?fa test). You can find the search syntax documentation here: https://asciimoo.github.io/searx/user/search_syntax.html. @jibe-b thanks for the info. I think it is worth a try.. Application sent.. These are fake news, you get similar results if you search for searx virus/searx malware. As I see, seeks.ru is a default searx installation without additional javascripts.. The above log doesn't contain errors as I see. Try to restart/reload apache too after the uwsgi restart. Maybe the error is between apache and uwsgi.. https://github.com/asciimoo/searx/blob/master/searx/settings.yml#L21. You have to change the following line if you want to change the default style of the oscar theme: https://github.com/asciimoo/searx/blob/master/searx/templates/oscar/base.html#L19 . E.g. replace logicodev.min.css with logicodev-dark.min.css.. > For example, pygments is missing.\nit is in the requirements file: https://github.com/asciimoo/searx/blob/master/requirements.txt#L6\n\nAnd selenium is listed but is not used.\n\nIt's only a development dependency: https://github.com/asciimoo/searx/blob/master/requirements-dev.txt#L11\n\nPlease also avoid using == in requirements list, because it makes it very difficult to install as a package. Please use >= instead, expect dependencies be backwards-compatible.\n\nUse virtualenv: https://asciimoo.github.io/searx/dev/install/installation.html#basic-installation. Do you use the debian packaged version of searx?. This is interesting.. I cannot reproduce it. What is your uwsgi version/config? Do you have uwsgi logs?. Try to add enable-threads = true to your uwsgi config.. It is mentioned in the setup guide: https://asciimoo.github.io/searx/dev/install/installation.html#uwsgi .. @scroom it works for me on searx.me . Which instance do you use and what is your search query?. @lonnieOST here's an example plugin which saves results asynchronously:\n```python\nimport threading\nname = 'Save results'\ndescription = 'Save results'\ndefault_on = True\ndef save_results(results):\n    # TBD..\n    print(results)\ndef post_search(request, search):\n    results = search.result_container.get_ordered_results()\n    threading.Thread(target=save_results, args=(results,)).start()\n    return True\n. @dalf I'm not sure if this is really a bug in our plugin architecture. Plugin handling now blocks the execution which makes the plugins able to change variables which affect the search. And, the current behavior allows a plugin to be async as the above example shows.. @lonnieOST you can find the search API documentation here: https://asciimoo.github.io/searx/dev/search_api.html as @dalf mentioned. It contains all the available URL parameters.. @riesnerd What is the purpose of this request?. @Drose221193 how could searx become a verified publisher and what are the benefits?. @lonnieOST it doesn't return suggestions for me. The screenshot shows an infobox which is presented in the json too:\ncurl -q \"http://localhost:8888/?q=solar%20panels&categories=general&format=json&lang=en&pageno=1\" | python -m json.tool\n[...]\n    \"infoboxes\": [      \n        {                                              \n            \"content\": \"Solar panels absorb sunlight as a source of energy to generate electricity or heat.\",\n            \"engine\": \"wikipedia\",                                                                                                                                         \n            \"id\": \"https://en.wikipedia.org/wiki/Solar_panel\",\n            \"img_src\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Photovoltaik_Dachanlage_Hannover_-Schwarze_Heide-1_MW.jpg/300px-Photovoltaik_Dachanlage_Hannover-Schwarze_Heide-_1_MW.jpg\",\n            \"infobox\": \"Solar panel\",                                                                                                                                                                                           \n            \"urls\": [                                                                                             \n                {                      \n                    \"title\": \"Wikipedia\",                                                 \n                    \"url\": \"https://en.wikipedia.org/wiki/Solar_panel\"                                                                                                      \n                }          \n            ]            \n        }                       \n    ]\n[...]\n```. @lonnieOST I don't get any suggestions from the web ui (http://localhost:8888/?q=solar%20panels&categories=general) so the JSON response is correct.. @lonnieOST there is no \"suggestions\" box for me:\n\n. @lonnieOST you have non-default engines enabled on your screenshot. Enable these engines when you create JSON request and it will return suggestions. The issue is closed, because searx works as expected. If you specify yahoo engine in the json request too, it returns suggestions: http://localhost:8888/?q=!yh%20solar%20panels&categories=general&format=json .. @virtadpt thanks for the report, hopefully it is fixed in  b9d4c05 .. Isn't this related to https://github.com/asciimoo/searx/issues/1210 ?. fixed in #1252. @cloo browsers use the ShortName attribute in opensearch.xml to identify search engines. Change the instance_name attribute in settings.yml and restart your searx and reload your browser, then you should see your instance.. Thanks for the report!. This issue is recently fixed in  b8543bcf3a4cc176645473165fcc855ae3a8210c and we don't have a new release since that.\nI maintain searx.me and it is updated to the current master, but I have to emphasize that it isn't an \"official\" instance, it is just a public instance like the others listed on https://github.com/asciimoo/searx/wiki/Searx-instances.. > If searx.me is not an official instance, so why it receives instant updates and is the only instance to have the footer 'Support the development and maintenance of searx'?\nIt receives instant updates, because I update it instantly =)\n\nis the only instance to have the footer 'Support the development and maintenance of searx'?\n\nWhy would this mean that searx.me is an official instance?. This is actually implemented but the duration of the ban is only 5s currently. Every engine has a ban counter which is increased by the ban duration on every error.\nPerhaps, we should make the fail ban duration configurable.. Searx has two search endpoints (/ and /search) because chrome/chromium does not allow to send POST requests to the default search URL. Perhaps this is a similar issue. Try to add https://yourinstance.com/search manually instead of https://yourinstance.com/ as default search engine. Does this solve the problem?. I agree with @LuccoJ searx can be used as a frontend for any kind of search backend. Good example is #1257 or the yacy engine.. @aurora-potter all of these are HTTP server related security issues and not application specific. Searx can be used as a local service (or over tor/vpn) where these are not really useful, so the above list is a good recommendation for public instances and searx admins should consider using most of them, but I don't think that searx should enforce HTTP protocol related security hardenings which affect only the public instances.\n\nSince, unofficial searx instances can be closed source, advise developers not to send the web server type and version, protecting them from being exploited by known vulnerabilities for that web server.\n\nThere is no official searx instance and all the instances have to publish the source code if it is modified according to the AGPL license.\n\nThe web server is vulnerable to CVE-2016-1247.\n\nWhich server?. @aurora-potter I agree, we should create a documentation section about our privacy/security recommendations.. See #1292. @lonnieOST, the number of results depends on the services you use. E.g. wikipedia returns 1 result but google images 50 on a single search. Because the various result numbers the only way to achieve the mentioned functionality is storing a \"session data\" about your previous searches. Searx doesn't persist data on server side, so if you really need this feature, you have to develop this as a small layer between searx and your application.. I have no idea about the why, but it caused by #1273 . This is another good example how fragile the google engine is. I reverted the engine's code as a temporary solution on searx.me.. @Jul10l1r4 this is not a bug. Public instances often limit non-human output formats like JSON/RSS (E.g. on searx.me only two JSON/RSS requests are allowed per minute to reduce bot traffic). Try to use an own instance if you'd like to search with scripts.. Please fix the URL in the test too, because travis fails.. It's the soundcloud engine. It performs an initial request on startup to gather a token, and soundcloud is hosted on aws.. If it bothers you, you can remove that engine from settings.yml. It performs just a single front page download, so I don't think it leaks much information.. yes, and there is another engine which performs a HTTP request on startup: wolframalpha, perhaps you want to disable it too.. Well, then this is a bug in logicodev dark, I'll fix it. Thanks.. It works for me: https://searx.me/?q=test&disabled_engines=google__general. @Pofilo I added you as a collaborator, so from now you can close issues. Thank you for your helpful work on searx. =] <3. Fixed, thanks!. @infosisio could you provide a better fix for this issue?. https://asciimoo.github.io/searx/dev/search_api.html. @Pofilo  it would be the futureproof solution, but there are still unresolved questions.. I agree with @arthurmougin . Update to the latest master, google images is fixed on the master branch. Also, I'll release a new version (0.15.0) soon.. I've removed 500px, google/bing works fine on the latest master for me.. Please fix the formatting: https://travis-ci.org/asciimoo/searx/jobs/400304833#L706. @GambaJo you can specify the language in the search query, too: https://asciimoo.github.io/searx/user/search_syntax.html .. @gr01d thanks the answer.. It works for me. curl https://searx.eu/search\\?q\\=water\\&pageno\\=1\\&format\\=json produces the desired results.. Wget gives me the same output (wget https://searx.eu/search\\?q\\=water\\&pageno\\=1\\&format\\=json -O -). What is that \"CORS proxy\"?. Thanks for the kind notification, it's fixed =). Also, you can run your own instance, if you're not satisfied with the existing ones.. Searx forwards the search query to the remote engines, so if a remote engine like google supports advanced syntax, then searx supports it, too. Searx also has an own advanced syntax, where you can specify engines/categories/language: https://asciimoo.github.io/searx/user/search_syntax.html. Seems you have network errors (RemoteDisconnected('Remote end closed connection without response'). Check your proxy or your proxy config.. You have multiple options:\n - Domain redirect to the main host\n - HTTP redirect to the main host\n - Webserver configuration which forwards the actual hostname to searx (similar to https://asciimoo.github.io/searx/dev/install/installation.html#from-subdirectory-url-searx)\nProbably setting the base_url in settings.yml to / also handles the domain differences, but it isn't tested.. Qwant is fixed (thx @dalf) and ddg works well for me on my local instance and on searx.me too.. Well, if I use dictionary, I only activate that engine from the search query, like !dc en-hu apple, so its' category doesn't really matter for me. What would be the benefit of an own category?. >  is that having dictionary result coming up in a general search doesn't look right.\nTrue, that's why I deactivate dict engines by default and activate only from queries when I need it.\nBtw, category creation is quite easy and fully dynamic, so if you change the category of any engine in settings.yml to any custom one, that engine will be displayed on your instance under the new category.\n\nand I can look up on more than a dictionary that I choose.\n\nYou can activate multiple engines from the search query without grouping them. E.g. query !wp !go !bi x will search for x in google, bing and wikipedia. Startpage stores your personal settings on their servers. Searx doesn't persist any data, that's why saving user preferences is a bit more problematic.. I agree with @dalf.. Do you mean getting the search results in RSS format? If so, it's implemented, just use format=rss search parameter: https://yoursearx.tld/?q=!wp%20searx&format=rss. Currently it cannot be configured, however you can edit the https://github.com/asciimoo/searx/blob/master/searx/data/engines_languages.json file \"manually\", which contains all the displayed languages.. What is required to be able to search with KDE Krunner?\nSearx has opensearch support, and you can use the URL https://searx.me/search?q={searchTerms} as a base search URL.. @beetleb check out https://asciimoo.github.io/searx/user/own-instance.html. Thanks for the question, no problem at all - I'm working on private projects and I don't have much time to code hobby projects.. @pointhi could you fix this docstring?\n. this could be a list/tuple, which contains the regexps.\n. ^^^ replaces e.g. http://wkey.com/ to http://\nmaybe checking the query params from result['parsed_url'] would be a safer solution\n. is the above regex necessary? it matches only the ? string\n. Should simply return with the dict object (line 97). With this minor modification we can remove the cookies parameter to avoid side effects. The function name could be get_as_cookies or something like this.\n. This can be retrieved from self._settings\n. please delete this debug print\n. this nested exception handling should be rewritten to something like this:\npython\nfor date_format in range [\"formatstr1\", \"formatstr2\", ..]:\n    try:\n        publishedDate = datetime.strptime(date, date_format)\n        break\n    except:\n        pass\n. Do we really want to set the current date if we cannot parse the original one?\n. it appends \"...\" even if the content is shorter than 300 chars\n. what is the purpose of the above line?\n. It would be good to behave similar to the original sites query syntax, even if it's just a subset of the available options. \n. seems these two lines are the same as 87-88\n. User-agent appears in their logs, so it can reduce anonymity. I think the original random user-agent would be better here.\n. Compiled regex would be more efficient\n. thanks the fix,\nthis could be also http_regex.sub('https:', infobox_id)\n. please remove this whitespace change\n. it would be better to disable it by default\n. please use searx.utils.html_to_text or cgi.escape to escape the content\n. i'm not sure if this really belongs here\n. why is this necessary?\n. I'd leave it False by default. Extra files need to be downloaded even for users who cannot use this feature (eg js blocking).\n. why?\n. ?\n. ?\n. ?\n. ?\n. ?\n. very cool, it's the same result selection logic as the xpath engine!\n. It would be good to disable it by default, since it has no https support\n. Why advanced_search has removed? It indicates that the advanced search panel should be open.\n. It isn't necessary to select request data here. Below in this function all GET arguments are merged with POST arguments - see L366 (original code). From that point it is enough to use request.form it contains all the client arguments. Maybe the name request.form is confusing after the merge and request.request_data would be better, but merging arguments allows more flexible usage than selecting only GET or POST data exclusively.\n. :+1: \n. Yes, using request.form is the good practice from that point, it contains all the available arguments after GET/POST merge.\n. Please don't enable debug mode.. ah, it isn't part of any category, so it will be disabled by default. What is the purpose of this media query?. this could be ':' + res.get('template', ''). \ud83d\udc4d . You can pass request.args to get_doi_resolver() as a parameter from this function, this way you don't need the from flask import request line.. What is the benefit of this rename? What does oa mean?. Seems this function is identical to searx/plugins/oa_doi_rewrite.py:get_doi_resolver(). You could import that function to avoid code duplication.. Add engine: json_engine to specify the engine you want to use. Please remove this debug print.. ",
    "okhin": "Not needed anymore due to the xpath engine\n. Well, I don't track click, I track only links. I haz no idea who's clicking and I do not care. And it's optional (need to activate a local search engine).\nBut if we want collarborations between different instances of private searx node I do not see how you can do that without knowing what are the correct suggestions.\n. Mmm, ok, maybe a longer answer, because I think there's something either you or I are missing, and I want to sort it out.\nThe issue abouttracking, is that you build up data on an identity. That's what every public services online is doing either, Google, DDG - even if they say they're not we can't blindly trust them, or any webservers with default logs (between the Referer and the IP you have enough info to buil up identities from the log files in apache or nginx).\nThe issue here is to track. Ti be able that this computer who did this search also did that one. Tracking implies the creation of a link between event, what is called an identity.\nI suppose that you're fully aware of that, and that's why I do not want tracking in the search engines I used either. that's why I ranned my personnal seeks node.\nSeeks node learn from results. They can improve themselves and provide answers to questions from other known seeks node. And then can provide search results without even asking to other search engine, being it's own crowd powered crawler.\nWhat a seeks node does, and it can perfectly be a private one, not accessible out of your own computer, just like searx does, is saving links visited by things after a search. Because it's how seeks can tell a link is relevant. It does not save links between search (ie: does not build an identity). If you're the only user of your node, then your node will know a lot about you. But you're controlling the database and decides to share it or not. And this is the point here.\nI do not think we should have only one (or two) big searx node. We should have a crowd of personal searx node, sharing results across a distributed network to provide crowd wise search results. And if you're really paranoid about privacy, you can perfectly run searx on 127.0.0.1, it works, it can learn from your habits, and no one besides you will have access to the data and then on your identity.\nThat's why I do not think recording and learning is tracking. Tracking comes from centralisation (and identity building), not from learning.\nBut yes, I'm working on a way to disable it. A commit will probably comes soon.\n. So now, there's a link instead of a simple display of the url. And it avoid the recording of the link.\n. Mmm, for me, the perfect privacy aware stuff is the one that is contained to only me, and not sharing ressources with people (or only on a controlled way), so everyone should have it's own search engine node, and those node could record anything since only me will use it.\nFor the data saved, let's just get the sqlite schema (well, the scritp used to create the database in fact): searx/schema.sql\n1 CREATE TABLE IF NOT EXISTS snippets\u22c5\n  2     (id INTEGER PRIMARY KEY ASC AUTOINCREMENT,\u22c5\n  3     url TEXT,\u22c5\n  4     content TEXT,\n  5     title TEXT);\n  6 \n  7 CREATE TABLE IF NOT EXISTS results\u22c5\n  8     (id INTEGER PRIMARY KEY ASC AUTOINCREMENT,\n  9     keyword TEXT,\u22c5\n 10     snippet INTEGER REFERENCES snippets (id),\u22c5\n 11     score INTEGER,\u22c5\n 12     UNIQUE (keyword, snippet));\nSo, I just save the content returned by a search result. And I associate it with words (and a score, which is not used yet).\nAssuming, you have no logs at all, an admin can only know what sites have been visited for which search terms. He can't know who did it (except from other data gathered on teh server like logs, but we assumed there's is none).\nFor the title thing, well it's just that I can have tenth of search opened at once. I need a way to look for the one I'm looking for. Title is usually a good one (since it can be searched in the adress bar of browser for instance). If you have a better idea :/\n. ",
    "stef": "You write about seeks and seeks nodes, the misunderstanding comes from the different expectations we have from seeks^Wsearx. For me the main point of searx is to be a totally stateless meta search-engine, without state there is no profiling, no private data - also no logs! This is how we i hosted seeks, i even patched it to disable tracking users. As such searx is a nearly perfect successor to seeks for me and my users.\nInstead of blindly reimplementing seeks, i think searx could improve on the ideas of seeks and apply some privacy-by-design principles also in all future features. To better understand the issue i have a few questions: \nIf an admin chooses to participate in the p2p sharing of this data:\n- What kind of data is available over public interfaces?\n- What kind of data is stored on the searx node - in case of a host security compromise?\nthx for all insights,\ns\nbtw i found a04ce10cefe221751072acc1b337aaab404f678d this commit results in logging all your queries in your browser history if you're not using private browsing. Not showing the query in the title is a as privacy-conscious decision as using POST requests per default.\n. <3 all you guys!\n. the biggest problem with this is, the very different settings of the various engines, imagine landing on a french searx instance that only does french searches. or consider the different patchlevels between installs.\n. because there was this example in the issues here, where a french instance operator was complaining that the results are english, and the recommendation was, to only allow engines that do localized results. so i guess, such a setting exists in the world somewhere, and i'd be dumbfounded if i'd get results from such a searx instance if it would be included in the round-robbin. also consider my searx instance, i know a lot of people who hate my inclusion of uncyclopedia among the default search engines. so i opt out also of this round-robin?\n. i think you misunderstand, this is about server settings. not client settings.\n. well, i expressed myself wrongly then. the problem is that the french engine only serves other engines that support french results. so the set of queried search-engines is lower, due to the french preference of the server.\n. centralization around searx.me is a decision by the people to use searx.me instead of running their own or using one of the many other servers: http://stats.searx.oe5tpo.com/\nsince searx nodes are run by many other operators the user cannot know which one is modified in what way or in which country it exist or many other factors and if that is acceptable.\nnoone however forbids anyone to create a round robin dns for many searx servers and operate this on their own and have other people use it and become more popular than searx.me. searx.me actually encourages this by having regular scheduled downtimes. but . sounds great, looks nice, is this searx? never saw that theme. is the way you do this documented? are your sources somewhere available?. unless this is possible without js, it would violate the basic searx premise of not requiring javascript and preserving privacy.\n. i think you might be mistaken with the premise of searx, it is a privacy respecting engine, if you disable the privacy respect, why not use any of the other kraakens instead? why all this work in the first place? so the default with searx is no js, because that is the privacy respecting way of doing things. maybe it's not explicit, but then it should be documented and made explicit.\n. javascript is one of the tools that enable a lot of privacy violations that are not possible without javascript. so not providing js is a conscious choice to protect the users. i'm hesitating to open an issue recommending, that 1/ searx includes a warning.js, that shows a warning to the user, that enabling js removes some of the privacy promises of searx, and recommending to use it without js instead and curse the browser vendors for having to click 1 more than without js. I think a similar warning could also be emitted when the user uses a get request. i mean perhaps, the problem is that the privacy vs convenience dichotomy is not explained and sold well enough. such a solution would 1/ educate the users and 2/ provide an escape for the js-apologists to include js in searx.\n. js is a huge security liability, by not using it you are not exposing your users. that is a responsible thing to do. the awareness of this is limited, and i am sure this also greatly affects what you call \"trust the searx instance\", of which statement i can only question the consciousness of this trust.\n. the travis build fails, because .git is not relative to the webapp.py, where is it?\n. the problem seems to be that many searx deployments are not in git repos. so this feature can only be activated on a few nodes. also the patch needs some cleaning up.\n. @jleclanche what benefit comes from porting to a new language?\n. python3 is a new language, in comparison to py2. and if you port to a new language, then there should be some reason for that. i mean, why not then do it properly and go for erlang instead?\nbut as far as i know, there's no plan to port to new languages.\n. excellent, but in this regard you can consider that py2 and py3 are different languages, and porting searx i think brings absolutely no benefit for the users. if searx would at least be a lib^Wmodule then maybe it'd make sense to provide it to other users. sorry if my consideration of this issue was incomprehensible to you. it was definitely not meant as fud. and if searx would be ported to another language, i think it'd make much more sense to chose erlang, as it provides much better support for all this network shoveling stuff, and being transient.\n. so the developer benefit by having to support 2 languages now?\nwhat other benefits?\n. if it's not a different language, why do you need porting, and this issue about it?\n. but, i think the main question is still, what's the benefit, if it is a different or the same language is irrelevant. what is, how much effort is this now, how much effort is this later, and what is the benefit of spending all this effort?\n. i guess we only disagree about epsilon, where epsilon is the distance between two languages, anything closer is \"the same\", anything beyond epsilon is a \"different language\". in this context, i hope you can see, that not everyone has the same epsilon threshold, and thus whether two things are different or the same lie in the discretion of the observer.\nso searx has been initially written in py2, i'd say a prudent choice, someone else might have made a different one. but searx is py2, so what would be the reaction if someone came, with even more radical epsilon thresholds, say, equating every script language as \"the same\", i'm sure we both have met this kind of person ;) if we'd be polite, i guess i'd ask, what would be the benefits of that, and what the cost. so i see the argument, that porting can be done 90% automatically, having seen asciimoo dig deeply into much of the code, i'm not sure if this is a bit optimistic. but let's say, it's done. what then? can someone come and commit py2 code, and be happy it'll work with py3? or vice versa? how much effort is it to support both? or does this porting mean in the end, that it's basically py3, but it might run also on py2? as for the benefits, compiling c with a c++ compiler is no fun, if it's a serious project, as is various compilers and interpreters. having to support suddenly py3 additionally brings no benefit but more work for the developer. and i'm not sure that our developer resources are so plentiful as to spend all this for 1 or 2 admins, who for some weird reason have no py2. in total this does not sound like a beneficial issue, irrespective of where our epsilon thresholds are.\n. ok so you say the change is minimal. but you fail to show any benefit either. so why do this? i think besides epsilon we also disagree on the vision of our future, not very uncommon i say. ;) and if py2 really dies, i guess erlang is still a more prudent choice for this kind of transient network proxying stuff. but let's focus on the benefits. \n. thanks for making that so clear. and sorry for the guy if my questions were inconvenient for him.\n. unless you can do an auto-completer without javascript and are ok with extra roundtrip times through the searx server, an auto-completer would greatly harm the prime directive of searx: preserve privacy.\ni think by reducing the privacy defaults in searx just to get more users, we do not honor the original idea of searx and become not better than what searx tries to fix.\n. i applaud the sane defaults of searx! yet again, why use a privacy respecting search engine if the only benefit of it you disable?\n. yes, but why does the user want to use the wrong tool? i mean they use google and want privacy, and they use searx and they don't want it, they want js instead. why can't those users who do not care use google, and those who care use searx?\n. i guess one way of preserving the privacy of searx users, is by limiting the ways they can shoot themselves in their foot. the not trusting searx, becomes muddled, when you consider other issues, like round-robbin dns searx instances, where you have a bunch of instances being served. would you still trust that js? also lot's of users, have not the privilege to trust their wires. many governments have their CA certs in the browser stores, there are other ways to strip or intercept https even in the enterprise setting, i heard of lenovo and other commercial enterprises doing this as crapware. \nin the case of the autocompleter, there is this other issue what i reflected on earlier, where the request will be routed through searx to preserve privacy, doing that increases latency quite significantly, your auto-completing will be slow.\n. well, but if you know that searx needs no js by default, then you'll be much happier using it than others where you have to enable noscript. finally search-engine that doesn't need no js, why erode that? i don't think it's the goal of searx to serve even users who want to shoot themselves in the foot, there's enough engines for that already. if you disable the privacy stuff, why use searx in the first place?\nwhy not telling the user how to use searx properly? i mean there's even a popup saying that you have to click after selecting a category, that is very much the same. and i think privacy aspects of searx also mean that searx should not allow or at least warn the user when searx \"respond to his queries as he wants to\" but violates the promises of searx in the mean time. \n. so we've run out of arguments it seems.\n. https://trisquel.info/en/forum/searx-metasearch-engine#comment-69404\n\n... and they managed to make it all js free!\n\nthere's other people being excited about not tainting stuff with js.\n. > it's only for people who would like to use javascript, knowing what's they doing.\nstatistically speaking the number of users conscious about their js usage is negligible in contrast to the number us users using it out of convenience.\n. <3 guys! (re: \"The main functionality of searx should always work without javascript.\")\n. what you call \"too high tech\", others call backdoor or increased attack surface.\nbut to not only sarcasm around, i think individual themes can solve this however they like, actually i'd really like a simple theme like okhins original, couldn't imagine it with anything else but checkboxes.\n. sorry for going off-topic: when i last checked palemoon a few months ago i found out, that backporting of security patches is not very diligent.\n. > Where are you hosting those instances?\nits only one instance with mulitple (virtual) ethernet interfaces. they're a kind donation of a trusted friend.\n. indeed copying cookies is what you want to do to use your settings on other domains. maybe it makes sense to open an issue at the browser vendors tracker to make cookie/local storage handling more user friendly as cookies seem to be entirely in the interest of the tracking industry currently thus also the hidden nature of the management of those.. does a user ever have to agree to terms of services when 1st time using google or any other search engine?. i think you are asking the wrong people. first of all this question is something that could only concern people running searx instances. if do run your own instance you should ask your local lawyer specializing in internet related cases. and then the answer depends on a lot of things like:\n\nthe level of fascism[1] in your jurisdiction vs respect of human rights,\nthe local level of respect for rule of law and US interests,\nanti-competition/anti-trust aspects,\nthe non-commercial nature of your instance,\nthe enforcability of terms of use that have never been actively accepted by anyone involved,\nthe affinity of google for being in the press with headlines like: \"US kraaken bullies small, non-profit privacy-enhancing free software project\",\nthe size and material of your balls ;)\netc\n\nso someone running a searx instance in st petersburg distributing malware with every result and doing some blackhat seo as a side-business could probably get away with all this, but some rich freedom-hating misogynist  running an instance with lots of advertising in San Francisco could probably probably have a few lawyers up his neck.\nDecentralization of searx instances helps a lot here i believe, the magic is to not become too big to be a target for google, but at the same time to be big enough to have a reasonable anonymity set to hide among.\n[1] fascism: authoritarian corporatism. yeah, these statements need to be created by the people running searx instances, there is no generic statement that fits all. for those who do not log ip addresses at all, there's not much to state, besides no logging happening and the cookies also do not contain anything. so there is not much to do. but for those who log there is. you cannot make a statement for your hoster, that is up to the hoster to make such a statement, if they indeed log ip addresses - in which case you might want to look for a more privacy respecting hoster anyway.. ",
    "Zeriuno": "If I may, I really appreciate your idea, @okhin. I really like searx, but I find it somehow constraining the choice to build just a meta search engine: if I have a relevant result, that don't show up in the search results, I can't provided it. And the use of searx don't build up anything: it will ever relay upon other search engines to function, even if one day (suppose) it will become the most used search engine in the world (one can dream!).\nAt the same time I understand @asciimoo's position and @stef's point of view. Searx starts with different goals than seeks.\nSo, @okhin, why don't you fork searx and build your version? It would be useful, it could build a search engine that searx could rely upon\u2026 I really hope you could find the time to go this way, (which, as far as I know, was your project before searx, with phoeneeks).\n. <3\n. Indeed, I think it could prove useful to automatically rotate between multiple searx instances, in order to allow a better load balance, and not ending up with http://searx.0x2a.tk/ dealing with most of the traffic.\nAt the same time one could be reluctant to share his own personal searx instance (or it could be hosted on a low-performance machine), so this could be an opt-in feature.\nFrom the user point of view, it could be useful to know where the searx instance is located, or not to use searx instances located in some countries.\n. Actually, I don't think that's how it works at the moment. Right now users are complaining that it doesn't work that way.\nAnyway preferences are set in the client, not in the server.\n. @gitbugged @stef I actually think this issue can now be closed, since Searxes exists: https://searxes.danwin1210.me/\n\nIf you can't decide which searx instance to use, try \"Searxes\". This will use other Searx randomly.\n\nhttps://github.com/asciimoo/searx/wiki/Searx-instances. It isn't mine, I just discovered a few weeks ago re reading the searx wiki!. @stef it looks like you don't really know searx. The main instance uses javascript: https://searx.me/\nAnyway, if you can think you can eliminate trust from the equation of using a computer tool (software or hardware), you are mistaken.\n. Why wouldn't you automatically match the user preference to the browser language?\n(And also, why aren't you using the italian localization I provided in transifex :-) )\n. Obviously it would still be possible to ask (through the syntax) other languages.\nOh, sorry: I guess the searx instance I checked upon today -which caused my question- wasn't up to date :-)\n. Wouldn't it be possible, when aggregatin, to filter the results based on a top sites list?\n. I don't get it: no, it won't, since it's a feature you requests and you may turn off. It would be just like the option Include The Pirate Bay. And it would be the same for everybody everywhere (=deterministic, as I mean it).\n. In these cases searx would directly serve the second page. What's the problem?\n. I get that. Still, if it's a feature, and one of the few ways to get what you're looking from? If you're cutting out the 100 top sites in the world I guess you are not really an unpatient user.\nAlso, wouldn't it be possible to include https://millionshort.com/ among the search engines? It would help on that.\n. ",
    "ibrokemypie": "bit of a necro, but why not just keep it a fork? of course many people want the privacy, but other people want the personalization, should be up to the individual.\n. many of the themes use jscript, any work on this?\n. Has any work been done on this? Another benefit would be easier installation on distros that have replaced the default python install from their package manager with 3\n. would be amazing to have this implemented\n. is the do not merge still relevant?\n. personally i think extending the scope of plugins to useful functionality like this would be much better than just adding them as features. independently developed plugins could add to the growth without adding bloat, plugins being independant to each instances choice.\n. any updaye? looks like an awesome idea\n. +1 for plugin\n. any work on any of this?\n. +1\n. bump?\n. +1\n. Problem with hidden services is that all users need to be running tor to connect, which has its own problems and overhead, and that discourages users from using your instance. If few users are using an instance, then it is not hard for google etc to identify the users based off of search history.\n. any progress on this?\n. google has issues with tor and appears to throw a captcha every second request, something to consider\n. AS searx is acting as a proxy for the results, shouldn't the requests all have the same user agent? Making the requests seem as normal as possible could help reduce the issue, if as you are saying, normal searches have a higher limit, is true.\n. @dalf afaict, duckduckgo just times out any connections when it blocks you\n. is the settings.yml being replaced by a new one? or is it simply being ignored?\n. Does it work with any other instance? I believe it is an issue in implementation on browser development end. Chrome allows you to set custom search engines and I believe there are extensions that allow that on Firefox as well, although Internet Explorer is locked to the presets afaik, although why you'd use IE is beyond me.\n. how would the download directly work?\n. Perhaps making this clearer, at the very least making it \"Supports SafeSearch\" rather than simply Safesearch which is incredibly misleading and potentially frustrating for users, would be a good idea.\n. seems to be overly shortening the result previews under general, makes it impossible to use it effectively\n. @logico-dev Version 9.1 (11601.5.17.1)\n. in my fork i was able to fix all the issues i had, and then split it off as oscar-neue, leaving the original oscar as is. https://github.com/PwnArt1st/searx/compare/master@%7B2hours%7D...master\ncould be a stopgap to add the theme to instances before the other issues mentioned have been dealt with\n. close now?\n. has any workaround been made yet?\n. +1\n. sorry, made a mistake, the problematic theme is actually chameleon. had problems with both firefox and safari so far\nSent from my iPhone\n\nOn 26 May 2016, at 11:59 PM, No\u00e9mi V\u00e1nyi notifications@github.com wrote:\nWorks for me, too. Which instance is the problematic one? Which version does it run on? Could you provide further information on how to reproduce the bug?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. @kvch sorry, forgot that I pulled it into mine, sorry for the inconvenience \n. +1\n\nSent from my iPhone\n\nOn 27 May 2016, at 3:19 PM, NIXOYE notifications@github.com wrote:\ntitle\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. > On 7 Jun 2016, at 7:52 PM, Alexandre Flament notifications@github.com wrote:\nCategories are sorted by alphabetic order except for the general which is the first one.\nThis order can't be modified.\nSee : https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L268 https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L268\nRelated to #258 https://github.com/asciimoo/searx/issues/258\nIt should be configured by settings.yml IHMO.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/asciimoo/searx/issues/573#issuecomment-224233338, or mute the thread https://github.com/notifications/unsubscribe/AHdxhqWIF2tTYahXQWIJ9WqTvt-5uop6ks5qJT9YgaJpZM4IpOJb.\n\ni agree, settings.yml OR by users for their own settings.\n. have you run ./manage.sh update_packages and ./manage.sh update_dev_packages from the searx root?\n. Yes that would be it, searx will not run on python3, i suggest completely remvoing everything python related from your system then running pacman -S python2 python2-pip python2-openssl python2-setuptools and then running the update scripts again. you may need to ln -s /usr/bin/python2 /usr/bin/python\n. > On 30 May 2016, at 6:52 PM, gregoirefavre notifications@github.com wrote:\n\nThat's not so easy as lots of other packages requires python2 or python3.\nThanks for your help.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub https://github.com/asciimoo/searx/issues/574#issuecomment-222445165, or mute the thread https://github.com/notifications/unsubscribe/AHdxhtfuoAGPURXJd408lEVriZNgNwfAks5qGqVYgaJpZM4IpSo2.\n\nOther packages? If you are running searx on a platform that is currently running other things that are preventing you of meeting searx\u2019s requirements then the problem is not with searx.\n. May 31 04:33:29 searxpi python[1918]:     search_results = engine.response(response)\nMay 31 04:33:29 searxpi python[1918]:   File \"/root/searx/searx/engines/gigablast.py\", line 77, in response\nMay 31 04:33:29 searxpi python[1918]:     response_json = loads(resp.text)\nMay 31 04:33:29 searxpi python[1918]:   File \"/usr/lib/python2.7/json/__init__.py\", line 339, in loads\nMay 31 04:33:29 searxpi python[1918]:     return _default_decoder.decode(s)\nMay 31 04:33:29 searxpi python[1918]:   File \"/usr/lib/python2.7/json/decoder.py\", line 364, in decode\nMay 31 04:33:29 searxpi python[1918]:     obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nMay 31 04:33:29 searxpi python[1918]:   File \"/usr/lib/python2.7/json/decoder.py\", line 382, in raw_decode\nMay 31 04:33:29 searxpi python[1918]:     raise ValueError(\"No JSON object could be decoded\")\nMay 31 04:33:29 searxpi python[1918]: ValueError: No JSON object could be decoded\n+1\n. #520 not necassarily an issue with searx, appears as if the instance is just being blocked by ddg\n. +1 for autistici, they provide my primary mail service \nSent from my iPhone\n\nOn 13 Jun 2016, at 7:30 PM, xinomilo notifications@github.com wrote:\ni'd suggest a privacy aware mailing list provider, like riseup.net/autistici.org or any of the others listed here : https://help.riseup.net/en/security/resources/radical-servers.\nwould like to read more suggestions on this. preferably not about commercial providers :)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. My application was accepted purely because I agreed with their values, and I asked politely. My searx instance and the minor contributions to the project I have made through the issues are the sole extent of my actions in line with their values, and at the time of application, I had nothing to say for myself except please. I really do not think they will have issue with hosting a mailing list for probably the best self hosted and privacy oriented meta-search engine due to the TLD. They are not religious extremists, just a few people who have their values and want to provide a service to those who share them.\nOn 23 Jun 2016, at 10:45 PM, No\u00e9mi V\u00e1nyi notifications@github.com> wrote:\nI think we should request a mailing list from autistici. First of all it worth a try and nothing happens if they do not provide a mailing list. There are many other providers.\nAlso, I don't think it is really a problem to have the docs under io TLD. I mean it is not the problem if I think along the logic of @a01200356 https://github.com/a01200356. So Github has the io TLD, not us. So the bigger problem is that the source code of searx is hosted on Github. By hosting the code here we indirectly \"support militarism\", because we use the service which has an io TLD. So I think if the io TLD is a problem for autistici, no project can get a mailing list from them, which hosts its code on Github.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub https://github.com/asciimoo/searx/issues/578#issuecomment-228038538, or mute the thread https://github.com/notifications/unsubscribe/AHdxhvxB0gj7EFRzWGDBma6AxbyVADeNks5qOn_cgaJpZM4Iu1LY.\n. what do you mean by extra source ips?\n. i have had numerous pages of results with a number being 2...\n. \n. whats the chance of this actually happening?\n. Searx is hosted by users on their own servers. Searx is not protected at all, just some instances have not been blocked yet. I host my own and every few days google comes back online before blocking me again.\n. searx.me is one instance run by one person as searx.space is run by me,  they aren't connected or shared at all\n. how about bitcq?\n. it is very nice to use,  fast,  extensive and includes inline magnet and torrent links,  however the uptime in the past has been a bit iffy\n. torrentz.eu has also gone down it seems, \"Torrentz will always love you. Farewell.\"\n. Aug 14 20:28:31 searx python[16133]: return self.run_wsgi()\nAug 14 20:28:31 searx python[16133]: File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 193, in run_wsgi\nAug 14 20:28:31 searx python[16133]: execute(self.server.app)\nAug 14 20:28:31 searx python[16133]: File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 184, in execute\nAug 14 20:28:31 searx python[16133]: write(data)\nAug 14 20:28:31 searx python[16133]: File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 152, in write\nAug 14 20:28:31 searx python[16133]: self.send_header(key, value)\nAug 14 20:28:31 searx python[16133]: File \"/usr/lib64/python2.7/BaseHTTPServer.py\", line 401, in send_header\nAug 14 20:28:31 searx python[16133]: self.wfile.write(\"%s: %s\\r\\n\" % (keyword, value))\nAug 14 20:28:31 searx python[16133]: IOError: [Errno 32] Broken pipe\n\n+1 crashes very frequently\n. I have had it enabled for a while, only started occurring after pulling the latest changes\n[edit] @dalf removed autcomplete, still crashes \nAug 25 19:14:54 searx python[3711]: return self.run_wsgi()\nAug 25 19:14:54 searx python[3711]: File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 193, in run_wsgi\nAug 25 19:14:54 searx python[3711]: execute(self.server.app)\nAug 25 19:14:54 searx python[3711]: File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 184, in execute\nAug 25 19:14:54 searx python[3711]: write(data)\nAug 25 19:14:54 searx python[3711]: File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 152, in write\nAug 25 19:14:54 searx python[3711]: self.send_header(key, value)\nAug 25 19:14:54 searx python[3711]: File \"/usr/lib64/python2.7/BaseHTTPServer.py\", line 401, in send_header\nAug 25 19:14:54 searx python[3711]: self.wfile.write(\"%s: %s\\r\\n\" % (keyword, value))\nAug 25 19:14:54 searx python[3711]: IOError: [Errno 32] Broken pipe\n. \n. scrot?\n. Very nice, and very obvious, I like.\n\nOn 8 Aug 2016, at 9:57 PM, No\u00e9mi V\u00e1nyi notifications@github.com wrote:\nEngines\n https://cloud.githubusercontent.com/assets/3876218/17478975/ddc957b0-5d6f-11e6-80d2-d8f4b44ad4ef.png\nPlugins\n https://cloud.githubusercontent.com/assets/3876218/17478974/ddc5a9d0-5d6f-11e6-9b59-a52040d1116a.png\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub https://github.com/asciimoo/searx/pull/654#issuecomment-238214879, or mute the thread https://github.com/notifications/unsubscribe-auth/AHdxhnIS7yhDo7FVsBNRNV5WehKaNQjEks5qdxmugaJpZM4Jel7L.\n. #662 and #651?\n. \n",
    "HLFH": "After \nbash\napt-get install python-pip\napt-get install python-lxml\nThen :\n``` bash\nroot@cubietruck:/var/www/searx# pip install -r requirements.txt\nRequirement already satisfied (use --upgrade to upgrade): flask in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 1))\nRequirement already satisfied (use --upgrade to upgrade): grequests in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 2))\nRequirement already satisfied (use --upgrade to upgrade): lxml in /usr/lib/python2.7/dist-packages (from -r requirements.txt (line 3))\nDownloading/unpacking Werkzeug>=0.7 (from flask->-r requirements.txt (line 1))\n  Downloading Werkzeug-0.9.4.tar.gz (1.1MB): 1.1MB downloaded\n  Running setup.py egg_info for package Werkzeug\nwarning: no files found matching '*' under directory 'werkzeug/debug/templates'\nwarning: no files found matching '*' under directory 'tests'\nwarning: no previously-included files matching '*.pyc' found under directory 'docs'\nwarning: no previously-included files matching '*.pyo' found under directory 'docs'\nwarning: no previously-included files matching '*.pyc' found under directory 'tests'\nwarning: no previously-included files matching '*.pyo' found under directory 'tests'\nwarning: no previously-included files matching '*.pyc' found under directory 'examples'\nwarning: no previously-included files matching '*.pyo' found under directory 'examples'\nno previously-included directories found matching 'docs/_build'\n\nDownloading/unpacking Jinja2>=2.4 (from flask->-r requirements.txt (line 1))\n  Downloading Jinja2-2.7.1.tar.gz (377kB): 377kB downloaded\n  Running setup.py egg_info for package Jinja2\nwarning: no files found matching '*' under directory 'custom_fixers'\nwarning: no previously-included files matching '*' found under directory 'docs/_build'\nwarning: no previously-included files matching '*.pyc' found under directory 'jinja2'\nwarning: no previously-included files matching '*.pyc' found under directory 'docs'\nwarning: no previously-included files matching '*.pyo' found under directory 'jinja2'\nwarning: no previously-included files matching '*.pyo' found under directory 'docs'\n\nDownloading/unpacking itsdangerous>=0.21 (from flask->-r requirements.txt (line 1))\n  Downloading itsdangerous-0.23.tar.gz (46kB): 46kB downloaded\n  Running setup.py egg_info for package itsdangerous\nwarning: no previously-included files matching '*' found under directory 'docs/_build'\n\nDownloading/unpacking gevent (from grequests->-r requirements.txt (line 2))\n  Downloading gevent-1.0.tar.gz (1.4MB): 1.4MB downloaded\n  Running setup.py egg_info for package gevent\nRequirement already satisfied (use --upgrade to upgrade): requests>=1.0.0 in /usr/lib/python2.7/dist-packages (from grequests->-r requirements.txt (line 2))\nDownloading/unpacking markupsafe (from Jinja2>=2.4->flask->-r requirements.txt (line 1))\n  Downloading MarkupSafe-0.18.tar.gz\n  Running setup.py egg_info for package markupsafe\nDownloading/unpacking greenlet (from gevent->grequests->-r requirements.txt (line 2))\n  Downloading greenlet-0.4.1.zip (75kB): 75kB downloaded\n  Running setup.py egg_info for package greenlet\nInstalling collected packages: Werkzeug, Jinja2, itsdangerous, gevent, markupsafe, greenlet\n  Running setup.py install for Werkzeug\nwarning: no files found matching '*' under directory 'werkzeug/debug/templates'\nwarning: no files found matching '*' under directory 'tests'\nwarning: no previously-included files matching '*.pyc' found under directory 'docs'\nwarning: no previously-included files matching '*.pyo' found under directory 'docs'\nwarning: no previously-included files matching '*.pyc' found under directory 'tests'\nwarning: no previously-included files matching '*.pyo' found under directory 'tests'\nwarning: no previously-included files matching '*.pyc' found under directory 'examples'\nwarning: no previously-included files matching '*.pyo' found under directory 'examples'\nno previously-included directories found matching 'docs/_build'\n\nRunning setup.py install for Jinja2\nwarning: no files found matching '*' under directory 'custom_fixers'\nwarning: no previously-included files matching '*' found under directory 'docs/_build'\nwarning: no previously-included files matching '*.pyc' found under directory 'jinja2'\nwarning: no previously-included files matching '*.pyc' found under directory 'docs'\nwarning: no previously-included files matching '*.pyo' found under directory 'jinja2'\nwarning: no previously-included files matching '*.pyo' found under directory 'docs'\n\nRunning setup.py install for itsdangerous\nwarning: no previously-included files matching '*' found under directory 'docs/_build'\n\nRunning setup.py install for gevent\n    Running '/bin/sh /tmp/pip_build_root/gevent/libev/configure > configure-output.txt' in /tmp/pip_build_root/gevent/build/temp.linux-armv7l-2.7/libev\n    building 'gevent.core' extension\n    arm-linux-gnueabihf-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DLIBEV_EMBED=1 -DEV_COMMON= -DEV_CHECK_ENABLE=0 -DEV_CLEANUP_ENABLE=0 -DEV_EMBED_ENABLE=0 -DEV_PERIODIC_ENABLE=0 -Ibuild/temp.linux-armv7l-2.7/libev -Ilibev -I/usr/include/python2.7 -c gevent/gevent.core.c -o build/temp.linux-armv7l-2.7/gevent/gevent.core.o\n    gevent/gevent.core.c:9:22: fatal error: pyconfig.h: Aucun fichier ou dossier de ce type\n     #include \"pyconfig.h\"\n                          ^\n    compilation terminated.\n    error: command 'arm-linux-gnueabihf-gcc' failed with exit status 1\n    Complete output from command /usr/bin/python -c \"import setuptools;file='/tmp/pip_build_root/gevent/setup.py';exec(compile(open(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /tmp/pip-bdOLXr-record/install-record.txt --single-version-externally-managed:\n    running install\nrunning build\nrunning build_py\ncreating build\ncreating build/lib.linux-armv7l-2.7\ncreating build/lib.linux-armv7l-2.7/gevent\ncopying gevent/event.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/os.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/fileobject.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/subprocess.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/timeout.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/hub.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/win32util.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/server.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/ssl.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/backdoor.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/_threading.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/resolver_ares.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/select.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/greenlet.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/baseserver.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/wsgi.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/pywsgi.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/coros.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/threading.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/socket.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/monkey.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/resolver_thread.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/pool.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/lock.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/local.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/init.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/util.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/thread.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/threadpool.py -> build/lib.linux-armv7l-2.7/gevent\ncopying gevent/queue.py -> build/lib.linux-armv7l-2.7/gevent\nrunning build_ext\nRunning '/bin/sh /tmp/pip_build_root/gevent/libev/configure > configure-output.txt' in /tmp/pip_build_root/gevent/build/temp.linux-armv7l-2.7/libev\nbuilding 'gevent.core' extension\ncreating build/temp.linux-armv7l-2.7/gevent\narm-linux-gnueabihf-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DLIBEV_EMBED=1 -DEV_COMMON= -DEV_CHECK_ENABLE=0 -DEV_CLEANUP_ENABLE=0 -DEV_EMBED_ENABLE=0 -DEV_PERIODIC_ENABLE=0 -Ibuild/temp.linux-armv7l-2.7/libev -Ilibev -I/usr/include/python2.7 -c gevent/gevent.core.c -o build/temp.linux-armv7l-2.7/gevent/gevent.core.o\ngevent/gevent.core.c:9:22: fatal error: pyconfig.h: Aucun fichier ou dossier de ce type\n#include \"pyconfig.h\"\n                  ^\n\ncompilation terminated.\nerror: command 'arm-linux-gnueabihf-gcc' failed with exit status 1\n\nCleaning up...\nCommand /usr/bin/python -c \"import setuptools;file='/tmp/pip_build_root/gevent/setup.py';exec(compile(open(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /tmp/pip-bdOLXr-record/install-record.txt --single-version-externally-managed failed with error code 1 in /tmp/pip_build_root/gevent\nStoring complete log in /root/.pip/pip.log\n```\n. Thanks ! I have now this warning :\n``` bash\nroot@cubietruck:/var/www/searx# pip install -r requirements.txt \nRequirement already satisfied (use --upgrade to upgrade): flask in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 1))\nRequirement already satisfied (use --upgrade to upgrade): grequests in /usr/local/lib/python2.7/dist-packages (from -r requirements.txt (line 2))\nRequirement already satisfied (use --upgrade to upgrade): lxml in /usr/lib/python2.7/dist-packages (from -r requirements.txt (line 3))\nRequirement already satisfied (use --upgrade to upgrade): Werkzeug>=0.7 in /usr/local/lib/python2.7/dist-packages (from flask->-r requirements.txt (line 1))\nRequirement already satisfied (use --upgrade to upgrade): Jinja2>=2.4 in /usr/local/lib/python2.7/dist-packages (from flask->-r requirements.txt (line 1))\nRequirement already satisfied (use --upgrade to upgrade): itsdangerous>=0.21 in /usr/local/lib/python2.7/dist-packages (from flask->-r requirements.txt (line 1))\nRequirement already satisfied (use --upgrade to upgrade): gevent in /usr/lib/python2.7/dist-packages (from grequests->-r requirements.txt (line 2))\nRequirement already satisfied (use --upgrade to upgrade): requests>=1.0.0 in /usr/lib/python2.7/dist-packages (from grequests->-r requirements.txt (line 2))\nDownloading/unpacking markupsafe (from Jinja2>=2.4->flask->-r requirements.txt (line 1))\n  Downloading MarkupSafe-0.18.tar.gz\n  Running setup.py egg_info for package markupsafe\nInstalling collected packages: markupsafe\n  Running setup.py install for markupsafe\nbuilding 'markupsafe._speedups' extension\narm-linux-gnueabihf-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/python2.7 -c markupsafe/_speedups.c -o build/temp.linux-armv7l-2.7/markupsafe/_speedups.o\nmarkupsafe/_speedups.c:12:20: fatal error: Python.h: Aucun fichier ou dossier de ce type\n #include <Python.h>\n                    ^\ncompilation terminated.\n==========================================================================\nWARNING: The C extension could not be compiled, speedups are not enabled.\nFailure information, if any, is above.\nRetrying the build without the C extension now.\n\n\n==========================================================================\nWARNING: The C extension could not be compiled, speedups are not enabled.\nPlain-Python installation succeeded.\n==========================================================================\n\nSuccessfully installed markupsafe\nCleaning up...\n```\n. @asciimoo Thanks. So install' is working now.\nbash\ngit clone git@github.com:asciimoo/searx.git && cd searx\napt-get install python-pip\napt-get install python-lxml\napt-get install python-gevent\napt-get install python-dev\npip install -r requirements.txt\n. @asciimoo Related to this finally. http://jesiah.net/post/59711878434/gevent-problems-gevent-dns-dnserror\n. @asciimoo My gevent version on Lubuntu 13.10 server is outdated = 0.13.7-4\napt-get autoremove --purge python-gevent  \ncheck this after : https://github.com/surfly/gevent\nbash\npip install cython git+git://github.com/surfly/gevent.git#egg=gevent\nSolved ! Searx is very Magic.\n. @asciimoo \nThis is what I did on an Intel NUC running ArchLinux x86_64 :\n``` bash\ncd /root\nssh-keygen -t rsa -C \"myemail@mywebmail.com\"\ncat /root/.ssh/id_rsa.pub\nAdd SSH key : https://github.com/settings/ssh\nyaourt -S python python2 python2-pip python-pip libxslt python-virtualenv python2-virtualenv uwsgi uwsgi-plugin-python uwsgi-plugin-python2\ncd /var/www\nsudo git clone https://github.com/asciimoo/searx.git\nsudo chown http:http -R /var/www/searx\nsudo -u http -i\ncd searx\nvirtualenv2 searx-ve\n. ./searx-ve/bin/activate\npip install -r requirements.txt\nsed -i -e \"s/ultrasecretkey/openssl rand -hex 16/g\" searx/settings.yml\nsed -i -e \"s/debug : True/debug : False/g\" searx/settings.yml\nexit\nexit\nmkdir -p /etc/uwsgi/vassals\nvim /etc/uwsgi/vassals/searx.ini\n[uwsgi]\nWho will run the code\nuid = http\ngid = http\nNumber of workers\nworkers = 4\nsocket = /run/uwsgi/searx.sock\nThe right granted on the created socket\nchmod-socket = 666\nPlugin to use and interpretor config\nsingle-interpreter = true\nmaster = true\nplugin = python2\nApplication base folder\nbase = /var/www/searx\nModule to import\nmodule = searx.webapp\nVirtualenv and python path\nvirtualenv = /var/www/searx/searx-ve/\npythonpath = /var/www/searx/\nchdir = /var/www/searx/searx/\nThe variable holding flask application\ncallable = app\n:wq!\nvim /etc/uwsgi/emperor.ini\n[uwsgi]\nemperor = /etc/uwsgi/vassals\nuid = http\ngid = http\nlogto = /tmp/uwsgi.log\n:wq!\nvim /etc/systemd/system/uwsgi.service\n[Unit]\nDescription=uWSGI in Emperor mode\nAfter=syslog.target\n[Service]\nExecStart = /usr/bin/uwsgi --ini /etc/uwsgi/emperor.ini\nExecStop = /usr/bin/kill -INT /usr/bin/cat /run/uwsgi.pid\nExecReload = /usr/bin/kill -TERM /usr/bin/cat /run/uwsgi.pid\nRestart = always\nType = notify\nNotifyAccess = main\nPIDFile = /run/uwsgi.pid\n[Install]\nWantedBy=multi-user.target\n:wq!\nvim /etc/tmpfiles.d/emperor.uwsgi.conf\nd /run/uwsgi 0755 http http -\n:wq!\nsystemctl enable uwsgi\nsystemctl start uwsgi\nvim /etc/nginx/conf.d/searx.conf\nupstream searx {\n      server unix:///run/uwsgi/searx.sock;\n}\nserver {\n    listen 80;\n    server_name searx.gitnote.eu searx.surfnote.net;\n    charset utf-8;\n    location /static  {\n        alias /var/www/searx/searx/static;\n    }\n    location / {\n        uwsgi_pass searx;\n        include /etc/nginx/uwsgi_params;\n    }\n}\n:wq!\nsystemctl restart nginx\ncp /usr/lib/systemd/system/uwsgi\\@.socket /etc/systemd/system/uwsgi.socket\nvim /etc/systemd/system/uwsgi.socket\n[Unit]\nDescription=Socket for uWSGI Cgit\n[Socket]\nChange this to your uwsgi application port or unix socket location\nListenStream=/run/uwsgi/searx.sock\n[Install]\nWantedBy=sockets.target\n:wq!\nsystemctl disable uwsgi.service\nsystemctl enable uwsgi.socket\n```\n. @Cqoicebordel Hi and thanks for translation.\n@dalf What I would like to add as @Cqoicebordel explained is a \"text input and a search button, which would redirect to a chosen page\": the dedidated results page of the searx instance. And where? For example here: https://gitnote.eu in the header, before the \"webapps boxes\". This call-to-action shortcut would improve the use of Searx and reduce the direct use of Google Search. In fact, a lot of open source users love their personal data, so they want to protect them with ownCloud, Cozy, ...and also with dedicated operating systems like YunoHost, arkOS. In this trend, these people use a lot of open source webapps in one single domain. \n\nFor this Yunohost example, I believe we need to add the search bar - maybe in a lighter version - on this homepage/web portal. \n\nIf it could be done with an iFrame or using the JSON API, I would love to know how to do it.\nBecause search engines for now are the key to the web surfing & productivity, they need to be more present than an subdomain (for example: https://searx.gitnote.eu) with just a \"box area\" in the homepage to reminder you that you have Searx on your server too. If you provide some code to implement directly the searx search bar inside the webapps portal, a lot of open source users would really appreciate that. It would be a shortcut that would redirect from the second-level domain (here: gitnote.eu) to the third-level domain (here with the running searx instance: searx.gitnote.eu) on the dedicated searx results page, according to the user input.\nThanks in advance,\n. @privacytoolsIO Btw, will https://github.com/privacytoolsIO/privacytools.io be an open source repo? I would like to add Sailfish OS in here: https://www.privacytools.io/#mobile_os, AsusWRT Merlin in \"worth mentioning\": https://www.privacytools.io/#firmware, ownCloud Desktop Syncing Client in \"worth mentioning\": https://www.privacytools.io/#sync, I believe you should switch Pydio and ownCloud here: https://www.privacytools.io/#mycloud, here you should place Firefox in \"worth mentioning\", and replace Firefox's nice place by its fork Pale Moon, that will never support HTML5 DRM (Encrypted Media Extensions) and Ads: http://www.ghacks.net/2014/05/21/palemoon-author-confirms-browser-will-ship-eme-ads-australis/ Also in VPN tools, you should add HideMyAss because it's a famous one that supports also BitCoin and is located for the most part outside of the US: https://www.privacytools.io/#vpn, in DNS, you should add the DNS server PowerDNS in \"worth mentioning\": https://www.privacytools.io/#dns, add Movim in \"worth mentioning\": https://www.privacytools.io/#social, and here, I believe it would be great to warn a little more about France, because with the recent law \"Projet de loi relatif au renseignement\", the intelligence agencies DGSI & DGSE could use Deep Packet Inspection (DPI) technologies to control the Internet, and IMSI-catchers to control phones: https://www.privacytools.io/#ukusa\nAnd thanks your search implementation clearly helps. I'll close this issue, when I'm done with the implementation on my side.\n. @pointhi If some crazy search engines still use it, it's to help you to unrank your searx website... The best SEO practice is not to use meta keywords because it's now considered as an old spam/black hat technique. A lot of SEO bloggers agree with that. Instead of using the obsolete meta keywords, you have to optimize your content and set up smart keywords in your posts/page to drive action. Matt Cutts from Google agreed with that. The meta keywords were useful when AltaVista was leading the search engine market, and it's closed since 2013... Even the well-known SEO French community agreed that you should not use meta keywords. http://www.webrankinfo.com/dossiers/debutants/meta-keywords ; and it's an article for beginners... the ABC of SEO, so. The triumvirate Google, Bing and Yahoo ignore the meta keywords.\n@asciimoo Hi, what do you think of this small PR? Could it be merged?\n. @Cqoicebordel Well... Let's take old figures (2012) from the product qSearch of ComScore. \n\nIn fact, the opponents to the US supremacy Baidu and Yandex still use meta keywords. At the end of 2012, they represented 11% of global market share, according to ComScore. The US triumvirate Google, Bing, and Yahoo represented 72.6% of market share. Should we keep the meta keywords because we should care of these 11% that will probably stop using the meta keywords or should we use the best practices used by those 72.6% (and probably more...). If you disagree, we should vote.\n. Ok. Incroyable ce d\u00e9saccord entre parisiens :'( \nI believe the main arguments have been said. Thanks for the disagreement, it always helps to find new relevant informations. I removed searx. I like bleeding-edge (here = Microdata, Open Graph, Twitter Cards), so - sadly or not - I'll keep Google as my main search tool. Have a nice day. Maybe I'll come back when searx 0.9 will be released.\n. There is also now the MySearch project on the open forge CodingTeam: https://codingteam.net/project/mysearch \nSo I suggest we also add P2P communication with MySearch.\n. ",
    "domenkozar": "@HLFH which version of gevent did that get you? 1.0 doesn't seem to work with grequests\n. ",
    "ghost": "Oh yes it\u2019s because currently I\u2019m using apache with the same reverse proxy conf for a lot of services, and I chose to spend more time on make YaCy work again after upgrade, improve and install others services rather than switch to another webserver ^^\nCould be possible to have a such setting (properly integrated) in the future or Searx is intended to be used in external mode only with uwsgi?\n. I don't think that a centralized DNS solution is a good idea unless it involved icann AND opennic and leaves the option open for people serving only from hidden-service or plain IPs or anything which is yet to come.\nIn terms of privacy/protection, opennic is the better solution if something like this should ever be considered by someone.\n. So you will be constantly checking github.com. I don't think this is a good idea to display which security holes have been patched and which ones have not been (you don't put it this way, but you can easily get this info by looking at the commits and compare), and not every instance runs in git. I'm currently working on getting searx packaged for Guix (a system comparable to Nix) and we remove every version control possibility of packaged software there (ie: a software in the install process is received from a git source, the .git dir in the source is removed).\nIf this gets merged this should be optional, not enabled in the default config.\nI see this was old, sorry for bumping old threads.\n. I think @Glandos' warning message would be a good idea.\n. It will be useful if I can set HTTP(S) proxy in searx's config file.\n.  searx/searx/templates/default/result_templates/code.html\n{{ result.title|safe }}\n--->\n{{ result.title|safe }}\n. I agree that hosting the translations on hosted.weblate.org would be better than on Transifex. However, even if this isn't changed, you can easily use free software like poEdit to translate locally, not needing any web services, free or proprietary, at all.\n. That's just another proprietary web 'service' so it doesn't change the situation. Unless they've hidden their licensing and code somewhere, but projects that do that are not worth looking at anyway because they have no interest in freedom.\n. If searx is not P2P, please add SOCKS proxy support.\nSOCKS proxy like Tor and I2P will hide original IP address.\n. > I think searx has much more privacy as Yacy\nI don't think so.\n1. Searx server owner can log user's IP address and search query.\n   searx.me can log user's IP address -> webserver's log\n   hack a code to log user's search query -> and nobody knows it\n\nYacy doesn't log search query. Results are shared by P2P protocol.\n2. \"and most searx-instance should have multible users, there is a mixing...\"\n   What if I run private searx website and use it? No anonymity right?\nYacy covers it. When you search, yacy mix a query into P2P.\n. Change the preferences to GET instead of POST\nand I click save... no GET params.\n\nI don't want to use cookie, so where can I find a list of params?\n. Let me explain what I wanna do with searx.\nI want to use searx as a \"backend\" server.\nUser <---> My WebApp ---> searx:localhost <----> Search Engine servers\nHow can I send a query with URL without cookie? I have no idea.\nWhy I can't send a query like this:\nhttp://searx.server/?q=AAAAAAAAAAA!&lang=en&img_proxy=on&return=json\nq=AAAAAAAAAAA!\n&lang=en\n&img_proxy=on\n&return=json\n. I think there should be (well, there already is, but still adding this comment) an addition to the installation instructions providing this so they can check up if it works in their country or if they want to ignore this law, keeping the logging settings to each persons choice.\nI personally have to use a combination of anonymized IPs log in nginx + /dev/null for @searx location + haven't figured out yet how to get rid of logging the actual keywords of the proxyimage search (the normal queries aren't logged in content)\n```\n        map $remote_addr $ip_anonym1 {\n            default 0.0.0;\n            \"~(?P(\\d+).(\\d+).(\\d+)).\\d+\" $ip;\n            \"~(?P[^:]+:[^:]+):\" $ip;\n        }\n    map $remote_addr $ip_anonym2 {  \n        default .0;  \n        \"~(?P<ip>(\\d+)\\.(\\d+)\\.(\\d+))\\.\\d+\" .0;  \n        \"~(?P<ip>[^:]+:[^:]+):\" ::;  \n    }\n\n    map $ip_anonym1$ip_anonym2 $ip_anonymized {\n        default 0.0.0.0;\n        \"~(?P<ip>.*)\" $ip;\n    }\n    log_format anonymized '$ip_anonymized - $remote_user [$time_local] '\n    '\"$request\" $status $body_bytes_sent '\n    '\"$http_referer\" \"$http_user_agent\"';\n\n    # log format for searx\n    # regex: (searx/search)+([\\w+?\\.\\w+])+([a-zA-Z0-9\\~\\!\\@\\#\\$\\%\\^\\&amp;\\*\\(\\)_\\-\\=\\+\\\\\\/\\?\\.\\:\\;\\'\\,]*)?\n    #\n\n```\nthe regex would get the searx/search part but not the proxie,\nso at the moment it's access_log /dev/null which just logs the anoymized IPs of the proxy searches.\n. I doubt you'd look up a search engine on another search engine, though.\n. https://en.wikipedia.org/wiki/Aidan_Baker\n\"Born   1974/1975[1] \"\noh, I see.\n. I can confirm this on several occasions.\n. Thanks,\nyes i am aware of the technical how-to and the points of failure.\nThe script in question is just an elvi for surfraw.\nIt's just the same conditions for accessing those instances from the open web or tor exit nodes directly with web browsers.\n. My comment might have been too unspecific:\nI am aware of the things you mentioned, but something which points out\n- this server does keep logs\n- this server doesn't keep logs\n- this server anonymizes IPs\nand so forth, like in the TIER2 server list of the opennic project might help get additional transparency.\n. Before we end up in a long discussion which leads to \"trust no one but yourself\"...\nWhat would be more effective in my opinion, would be:\n- thinking about a validation process which could provide at least a basic level of trust\n  or\n- do it like opennic (see: http://wiki.opennicproject.org/Tier2) and trust the statements of people running the instances. Eventually with a discussion on individual instances if it turns out the admin does indeed do thing X and wrote down that they do not do thing X.\n. xinomilo, how is providing a lookup on IP related to logs and other things?\nfor example: connecting to the .onion of my searx instance and asking for \"ip\" gives 127.0.0.1 (like it is intended to with tor), \"what is my ip\" gives something else (in onion and in clearnet) and simply \"ip\" in the clearnet address gives your real ip. that's no proof of $instance log policy.\n*keep in mind this was tested on a system I normally don't use/have not configured yet: so no dnscrypt, no local dns server running, no webrtc leakage fix in the clearnet browser.\nasciimoo, what about if we come up with an undefined (because I have no specific idea right now) way of providing verification or at least the possibility for instance admins to state further details in an overview, an overview which is located in a public place and not the instances server?\n. Okay, this doesn't sound like what I had in mind, but wouldn't it be possible to\nset server-ip and :: / 127.0.0.1 and similar IPs, then let an external script check the IP and see which value gets returned (the way searching for \"IP\"/\"what is my ip\" does now) and maybe provide a way to display the IP, so this could be a way to display this part?\nI still think a list like wiki.opennicproject.org/Tier2 would be okay, giving a bit more details then the current list.\nPossible fields: instance name, URL(s), certificate check link, State, Country, Anon Logs(yes,no,other), Log Comment, Other Comment\nIf this is a format some of you are generally okay with, I will create a wiki page for this tomorrow.\nsidenote:\nThere are too many factors to rule out to say \"this is 100% pseudo anonymous!\", for example VPS, consider all VPS compromised, but telling this to people who just want to search things is too much.\nI personally am thinking of ways to separate website services + everything else, and DNS servers I run.\n. Weird, i tried this approach earlier and it did not work.\nI copied oscar to oscar-$myname and did some changes without it appearing when I restarted searx.\n. The new menu entry does not appear in the webinterface after restart.\nSo what I did so far was copying '/searx/templates/oscar' to '/searx/templates/libertad' and 'searx/static/themes/oscar' the same thing, edited all references to the files which pointed to oscar to point to libertad/ to play around with theming.\n. Is there any further information I could post to debug this situation?\n. ls -al /usr/local/searx/searx/templates\ntotal 28\ndrwxr-sr-x 7 searx searx 4096 Sep  7 19:48 .\ndrwxr-sr-x 9 searx searx 4096 Sep  7 20:03 ..\ndrwxr-sr-x 3 searx searx 4096 Sep  1 19:02 courgette\ndrwxr-sr-x 3 searx searx 4096 Sep  1 19:02 default\ndrwxr-sr-x 4 searx searx 4096 Sep  7 20:36 libertad\ndrwxr-sr-x 4 searx searx 4096 Sep  1 19:02 oscar\ndrwxr-sr-x 3 searx searx 4096 Jun 17 22:41 pix-art\nlooks like this at the moment, so no special chars, pix-art has \"-\" as well, but i'm trying to temporarily remove one of the themes now.\n. So this is weird. I move the pix-art outside of /searx/ root, and I can stil access it after restart of searx.\ngit status:\n        deleted:    searx/static/themes/pix-art/css/style.css\n        deleted:    searx/static/themes/pix-art/img/favicon.png\n        deleted:    searx/static/themes/pix-art/img/preference-icon-pixel.png\n        deleted:    searx/static/themes/pix-art/img/search-icon-pixel.png\n        deleted:    searx/static/themes/pix-art/img/searx-pixel-small.png\netc\n. Cleared the cache of the webbrowser, still getting funky results under 'settings > theme'\n. i am not sure.\nI branched searx recently. Maybe I dive into that and see how to fix things. Thanks for your help anyway.\n. I ended up with the need to nuke the searx instance, even after hard reset,\nbut now I am left with: https://ptpb.pw/v7D3 and I wonder why, because it worked before.\n. this was run directly as the user searx runs as, in the python virtualenv.\nrestarting involved stop nginx, stop uwsgi, start nginx start uwsgi\nI could paste the settings.yml (minus the secret_key?)\n. well, minus the local modifications, it looks similar to this: https://github.com/krosos/searx/blob/libertad/searx/settings.yml\nand hasn't changed much locally, could be reset though tomorrow:  https://ptpb.pw/x1Dp\n. thanks for the tip with the virtualenv, i think it was just too late yesterday for my brain to work okay. It is solved.\n. The keywords after q= weren't that long, I doubt the url in its whole reached 2000 chars.\n. Can you show a single example of GnuPG or Tor being mentioned in a negative context when you search for them? I can't see it.\n. But what is your point? The very first thing you write in your suggestion is a completely hypothetical example which has nothing to do with real life. searx has no infoboxes displaying information straight from antivirus websites and I'm quite sure this will never be the case. Right now, there are infoboxes with information from DuckDuckGo, and what do you know, searching for 'Cryptolocker' gets you a nice infobox with a short description of Cryptolocker. Searching 'GnuPG' gets you an infobox with a short description of GnuPG. So, what's the problem?\nYour suggestion itself doesn't make any sense either. You don't want infoboxes written by antivirus companies, but you don't have a problem with them as long as said companies pay for them and are 'worthy', but then just after you write that you doubt those even exist. If you think that is the case, why even make this suggestion in the first place?\n\"Nice photography collection, but it would be nice if you could get some pictures of unicorns. I doubt they exist, but I figure it might be a good idea for you to take some pictures of them.\"\n. Thank you so much. Build process just started and since 5minutes no errors. Will report if it works in ... well, when it's done. ;)\n. user@cubietruck:/$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES\nb631461262f8        searx               \"/usr/bin/tini -- /us\"   12 minutes ago      Up 9 minutes        0.0.0.0:8888->8888/tcp   searx-noproxy\nuser@cubietruck:/$\n:) :) :)\nSo building process ended just fine.\nWell, another problem was totally not related to searx but. I didn't know that Docker reconfigures your iptables settings so after editing my standard settings and applying iptables-restore I couldn't run docker-searx anymore. https://fralef.me/docker-and-iptables.html was very helpful (for others with the same problem:\nAppend\n-A FORWARD -i docker0 -o eth0 -j ACCEPT\n-A FORWARD -i eth0 -o docker0 -j ACCEPT\nto your iptables settings and \n-A INPUT -p tcp -m tcp --dport 8888 -s 0.0.0.0/0 -j ACCEPT\nfor the case docker-searx is listening to port 8888 - and don't forget to forward the port in your router!).\nWell, anyway searx only works for me starting it via\ndocker run -d --name searx-noproxy -p 8888:8888 -e IMAGE_PROXY=True searx\n at the moment. http://my.ddns.net:8888 works just fine.\nUsing\ndocker run -d --name searx -e IMAGE_PROXY=True -e BASE_URL=https://my.ddns.net searx\nand trying https://my.ddns.net:8888 gives me\n\nUnable to connect\nIceweasel can't establish a connection to the server at my.ddns.net:8888\n\nBefore trying some more hours let me guess: I need to run another docker container for Nginx and link it to the searx container?\n. True, but other search engines are available.\n. @asciimoo thank you for your reply. You are definitely right, I tested the search for a bit and most of the time the content field is of no real use, too little text for too much page links and markup. I think results would be cleaner without it.\n. Oops, PEP8 checks failed, my bad. Fixed.\n. @asciimoo sorry for the delay, I was a bit busy lately. Removing custom UA is a definitely good idea from a privacy standpoint, although this violates the API usage terms and every popular searx server will probably get banned pretty soon. I think I'll try and contact them to discuss this issue, they take privacy matters pretty seriously, maybe we'll find a solution.\nFor now let's see how it works with a random user-agent.\nThanks for your work.\n. While debugging this I've found a similar problem. Steps to reproduce:\n1. Go to the settings page, set your search language to fr_FR.\n2. Search for \"yahoo lent\" in the news category.\nI am able to reproduce this bug in Firefox 45.0.1 and Midori 0.5.11, both on https://searx.me/ and my local installation.\nStacktrace:\n\n/home/neko/searx/searx/results.py:32: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n  return unquote(path_a) == unquote(path_b)\nERROR:main:Exception on / [POST]\nTraceback (most recent call last):\n  File \"/home/neko/searx/env/lib/python2.7/site-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/home/neko/searx/env/lib/python2.7/site-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/home/neko/searx/env/lib/python2.7/site-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/home/neko/searx/env/lib/python2.7/site-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/home/neko/searx/env/lib/python2.7/site-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functionsrule.endpoint\n  File \"searx/webapp.py\", line 461, in index\n    favicons=global_favicons[themes.index(get_current_theme_name())]\n  File \"searx/webapp.py\", line 344, in render\n    '{}/{}'.format(kwargs['theme'], template_name), **kwargs)\n  File \"/home/neko/searx/env/lib/python2.7/site-packages/flask/templating.py\", line 128, in render_template\n    context, ctx.app)\n  File \"/home/neko/searx/env/lib/python2.7/site-packages/flask/templating.py\", line 110, in _render\n    rv = template.render(context)\n  File \"/home/neko/searx/env/lib/python2.7/site-packages/jinja2/environment.py\", line 989, in render\n    return self.environment.handle_exception(exc_info, True)\n  File \"/home/neko/searx/env/lib/python2.7/site-packages/jinja2/environment.py\", line 754, in handle_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/home/neko/searx/searx/templates/oscar/results.html\", line 1, in top-level template code\n    {% extends \"oscar/base.html\" %}\n  File \"/home/neko/searx/searx/templates/oscar/base.html\", line 70, in top-level template code\n    {% block content %}\n  File \"/home/neko/searx/searx/templates/oscar/results.html\", line 22, in block \"content\"\n    {% include get_result_template('oscar', result['template']) %}\n  File \"/home/neko/searx/searx/templates/oscar/result_templates/videos.html\", line 3, in top-level template code\n    {{ result_header(result, favicons) }}\n  File \"/home/neko/searx/searx/templates/oscar/macros.html\", line 14, in template\n    {% if result.engine~\".png\" in favicons %}{{ draw_favicon(result.engine) }} {% endif %}{{ result.title|safe }}\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 54: ordinal not in range(128)\n. Thanks for the information, this is not the best solution then.\n. That's an interesting idea. I am working on it here: https://github.com/ukwt/searx/tree/media-links\n\nExample: https://i.imgur.com/uhejj1r.png\nOnly Oscar theme is supported at the moment.\nI won't make pull request yet because it needs further testing and tinkering with the styles (notice how the download button looks).\nThere is one serious problem with this approach though: it works very slowly, extracting links for each result takes a few seconds, so the whole result page typically loads in 20-30 seconds (although this depends on the site, for example YouTube works much faster than Vimeo for me, so the whole page processing time becomes... bearable.)\nThis is unlikely to change, because most of that time is spent inside youtube-dl's code, and I am not sure there is any way to speed it up besides implementing a caching system.\n. Yup, that was not a good idea. Here is a second attempt: https://github.com/ukwt/searx/tree/video-links\nUsually, results are sorted by the video quality in descending order, from best to worst. That's the reverse order of how youtube-dl returns them.\nA few examples:\nDownload error:\nhttp://i.imgur.com/OCAsTc8.png\nLinks were not found:\nhttp://i.imgur.com/wGsl2O5.png\nYouTube:\n1. http://i.imgur.com/Ac5YQwJ.png\n2. http://i.imgur.com/X72bN69.png\n3. http://i.imgur.com/RvAYDBH.png\nVimeo:\n1. http://i.imgur.com/C9sotk9.png\n2. http://i.imgur.com/SFf0Icq.png\nDailymotion:\n1. http://i.imgur.com/sTTFyun.png\n2. http://i.imgur.com/tQ4G1QU.png\nAny suggestions?\nI think that table contains too much information, but condensing it into a small informative string could be problematic because field formats vary greatly across all supported engines (extractors, as youtube-dl calls them). Some extractors return resolution information, some do not; some return useful human-readable information in the \"format\" field, and some don't; some return not only video and/or audio tracks, but playlists also; the list goes on and on.\nAlso, I am not sure about formats columns (\"audio\" & \"video\"). Should I remove them?\n. I was wondering what's the best way to prevent abusing the service. Thank you @pointhi, fixed.\n. @GreenLunar sure. I added settings for all file extensions supported by youtube-dl.\npreferences page: http://i.imgur.com/BngbL7p.png\nwithout filtering: http://i.imgur.com/1D2dCab.png\nwith 3gp filter: http://i.imgur.com/93r6LlD.png\nwith webm and mp4 filters: http://i.imgur.com/PU5d6Rg.png\nAlso I think it would be good to add an option to show all results without filtering when nothing of interest was found. I'll work on it in a little while.\n. OK, it's done.\nDisable all formats:\nhttp://i.imgur.com/ua2Ab6b.png\nSearch for something and click \"download video\":\nhttp://i.imgur.com/RUegMzh.png\nClick the link to view all results:\nhttp://i.imgur.com/SLnVJHw.png\n. Makes sense. I updated my previous comment. Thank you @pointhi for valuable input.\n. Thanks @GreenLunar for the suggestion, that's how I should have done it in the first place. Fixed.\nHere are the all possible scenarios:\nMobile view:\n320x480:\nhttp://i.imgur.com/WfUluOD.png\nhttp://i.imgur.com/rRP7F89.png\n360\u00d7640:\nhttp://i.imgur.com/DtYmkAO.png\nhttp://i.imgur.com/qSv8esI.png\n768\u00d71024:\nhttp://i.imgur.com/qhLbIbC.png\nhttp://i.imgur.com/AR6et9G.png\nNothing was found:\nhttp://i.imgur.com/hSbFxZi.png\nNothing of interest was found:\nhttp://i.imgur.com/EW55vAC.png\nhttp://i.imgur.com/65XFCSm.png\nSome results were filtered out:\nhttp://i.imgur.com/dNWGtiV.png\nhttp://i.imgur.com/uPyeMnv.png\nhttp://i.imgur.com/RgWULMr.png\nAll extracted links are in some of the preferred formats:\nhttp://i.imgur.com/7uQ4KIP.png\nConnection error:\nhttp://i.imgur.com/jqUzC3I.png\nAny other ideas or suggestions? Table sorting wouldn't hurt, I guess.\n. Thanks for your help everyone. I updated my previous comment.\n. OK, finished. For example, here is a table sorted by the resolution column: http://i.imgur.com/Nhh5yKA.png\nAny other ideas or suggestions?\n. @GreenLunar, done. Here is an example:\nhttps://dl.dropboxusercontent.com/u/18321926/cast2.mkv\nThe implementation is rather na\u00efve and inefficient though. It works fast for me, but could be better anyway. I'll have to optimize and prettify it a bit before making a pull request.\n. > Is this plugin ready to be released?\nYeah, I think. There is one problem with some results though:\nhttp://i.imgur.com/b0MqhX5.png\nTake a look at table headers and notice how some icons are wrapped to the second line.\nI've spent a few hours experimenting and haven't found a solution that would both keep icons on one line with text and keep them to the right of the cell, at least without major layout changes and changing the code. Many, many stackoverflow answers (among others) suggest using white-space: nowrap, which works by itself (that's how I 'fixed' this), but fails to work when icons are pulled to the side by float: right.\nDoes this look good enough?\nhttp://i.imgur.com/vDLNRsO.png\nIf not, it would be great if someone more experienced with web design could suggest a solution.\n. Phew. I believe I've finally found a solution.\nhttp://i.imgur.com/DDwbfGT.png\n\nI suggest not to include filter and sort buttons, for now.\n\nNah, it was almost ready, with the exception of those stubborn icons. But that seems to be fixed now.\nI could probably have not wasted all that time re-implementing a (slower) wheel, but every third-party library I have tried either does not have all the functionality, or adds as many JS code as searx has already, if not more. I think that's too much for auxiliary functionality of an auxiliary plugin.\n\nIf you need someone to perform QA experiments, I am willing to assist.\n\nSure, more testing on more systems and locales and whatnot is always good.\nDownloading the code is easy:\ngit clone https://github.com/ukwt/searx.git\ngit checkout video-links\n. @GreenLunar, it's a combination of two things: vokoscreen (https://github.com/vkohaupt/vokoscreen) and key-mon (https://code.google.com/archive/p/key-mon/).\nvokoscreen can show keys too, but in a form not suitable to demonstrate vim-like navigation: https://i.imgur.com/2qFmTaq.png\n. @asciimoo, this is a good idea and I'll certainly add this functionality, but I had to abandon my old account and no longer have access to this pull request (nor to its code), so it'll have to wait until the code is merged into master. Sorry.\n. Looks interesting.. @asciimoo removed.\n. @cy8aer your global IP is banned by google, then. Scroogle. > https://github.com/disconnectme/search/issues/53\n. (if any of you have Twitter, please tweet this URL to https://twitter.com/disconnectme to caught their attention, ty)\n. Also, can I expect same result on these conditions, or not? (if \"yes\", user can assume this searx is safe from Geo tracking)\n\"Using Searx.me from Finland IP\" ===|!== \"Using searx.me from Japan\"\n. > The only information forwarded to search services among the search query is\n\nthe browser's Accept-Language HTTP header, nothing else.\n\nI think this is acceptable, but some people may think this should be removed(return English result\nby default, unless user define their language from options).\n\nbecause no user/personal data is forwarded to search engines and your queries mix in with others queries\n\n@kvch, the statement \"no user/personal data\" is wrong here.\nBrowser data is user/personal identifiable info.\nIt'll be nice if you just make \"English\" as default and not use any browser data at all.\nUser who wish to see results in other languages should use searx like this:\n/?q=What&language=russian\n@asciimoo, ty for the reply and you can close this issue. But I think this need some discussion(send Accept-Language HTTP header to service engine).\n. Seems this is off-topic, sorry XD\n. Yep sorry for misinformation. Unlike @a01200356, I just don't have time to analyze source code at this moment.\n\nsend this option to the services 'yet'\nIt is just a planned feature\n\nOk, but I hope searx never send User information(yes, anything) to upstream service like Google at all.\n\n/master/searx/search.py\nLine 266: user_agent = request.headers.get('User-Agent', '') (disabled by # mark)\nLine 267: user_agent = gen_useragent()\n1. user_agent, which sent to upstream engine, must be randomized.\n\nhttps://github.com/dillbyrne/random-agent-spoofer/blob/master/data/json/useragents.json\nTake a look at this for good UserAgent collection. When gen_useragent() called, it will return 1 randomized\nstring, for example \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.75 Safari/537.36\".\nUpstream engine can't identify the request is searx because it is NOT static(same UA everytime).\n1. \"Accept\" can be randomize as well.\n2. Is \"Accept-Language\" already 'normalized'?\n   I mean, if I access searx from UK computer(Accept-Language= en-GB, en | en-GB), will searx send ONLY \"en\"?\n   If searx send \"en-GB\" this is a problem(expecting \"en\" for all English).\n. except search query, of couse.\n. Same here - can confirm all findings with similar config:\n- / not working\n- /search gives search page, results post back to / which gives error again\n- about and prefs working\n- direct access works (http://localhost:8888/)\n. Working for me with mod_uwsgi_proxy - thanks!\n. If you use clearnet service(searx.me), you are 100% NOT anonymous.\nUse http://ulrn6sryqaifefld.onion/ instead of clearnet.\n. Oh, strange. Maybe this only applies to the Dutch translation, then. All strings that are untranslated in the interface are unreviewed on Transifex, and vice versa.\n. This remains an issue.. @neomodern I didn't know that Ixquick search is still available at https://www.ixquick.eu/. Thanks for the heads up!\n. @asciimoo your clearnet domain is now online, but I still can't connect to your hidden service(onion).\n. I know that. I'm trying to say that I think it's redundant to provide the option to search from both StartPage and Ixquick when their results won't differ. Instead of just forwarding Ixquick queries to StartPage, searx should fetch results from https://www.ixquick.eu/ when Ixquick is turned on.\n. @asciimoo @dalf Would this change fetch results from https://www.ixquick.eu? I don't see a engine here for Ixquick.\n. I wonder how searx.me(and other searx admin) handle google captcha.\nSure they process 1K+/day request to google without getting \"scroogled\".\n. Thanks but I'm aware of this like you can see in my example case. I was referring to a workaround within searx's code itself.\n. I'm unfortunately in the dark myself here. I just know that when I search on google.de it omits the official video but if I search from the same IP on google.com it shows it. I don't know about a way to do the same on YouTube. In the past you could set the region in the footer to Worldwide which displayed all videos but that option was removed recently and USA doesn't seem to work.\n. They just came to an agreement which should resolve the issue for most YouTube results.\nhttps://en.wikipedia.org/wiki/Blocking_of_YouTube_videos_in_Germany#Agreement\nPlease feel free to close if this solves the problem sufficiently in your opinion. It does for me since I now get the results I was looking for.\nThanks a bunch for the awesome engine by the way. :)\n. That big box is gone, nice work! Now I can use searX with small screen mobile phone.\n. expected result:\ncombine same FQDN \nLine 6: http://www.whymicrosoft.com/\nLine 11: https://en.wikipedia.org/wiki/Criticism_of_Google\n...Youtube Videos(*1) section here...\nLine 16: https://www.youtube.com/watch?v=zAfQwDizpRo\nLine 21: https://www.youtube.com/watch?v=A8yZ5O96TtM\nLine 36: https://www.youtube.com/watch?v=R_6bSCW_UI0\nLine 61: https://www.youtube.com/watch?v=PtqeyxD0TtU\nLine 66: https://www.youtube.com/watch?v=VQkk9q3dE2A\nLine 26: http://www.cleaningforareason.org/\nLine 31: https://en.wikipedia.org/wiki/Google_and_privacy_issues\nLine 41: http://www.users.waitrose.com/~cresby/\nLine 46: https://en.wikipedia.org/wiki/Scroogled\nLine 51: https://tvshowtracker.net/\nLine 56: http://www.theverge.com/2016/8/19/1255[...]55052/google-shutting-down-chrome-apps\nLine 71: http://www.liveleak.com/view?i=dad_1265787025\nLine 76: http://marketingland.com/microsoft-scroogled-pawn-stars-chromebooks-66522\nLine 81: http://www.businessinsider.com/microso[...]ft-shuts-down-scroogled-website-2015-1\nLine 86: http://www.nogginsedge.com/technology/[...]1/danny-ori-microsoft-scroogle-google/\nLine 91: https://www.pinterest.com/pin/489133209501965157/\nLine 96: http://www.costaricaaa.com/12-principles-of-aa/\nLine 101: http://blog.ineedhits.com/search-news/[...]google-holiday-shopping-024512177.html\n1st link+desctiption,\n2nd,\n3rd youtubes [image1] [img2] [img3...5]\n4th link,\n5th link,...\n. How exactly does it not work? It's returning results for me (on searx.at at least).\n\n. @IDKwhattoputhere I get the following error on https://searx.at when I searched for english and community with only the yacy search engine enabled.\n\nSorry! we didn't find any results. Please use another query or search in more categories. \n. Previously worked but it's currently not working for me either.\n. @asciimoo Reddit has greater Max time than yacy does over here but it works just fine for me. Yacy, on the other hand, doesn't.. I also think it's quite useful from time to time. Maybe if a future release adds more entries it could become too cluttered but currently it's fine.\n. For the browser search bar this works:\nhttps://searx.me/?q=%s%20site%3Amypersonalsite.com\n%s being the search term.\nMaybe there's a way to use this on a website?. etymonline should be lowered than Google/Bing/Yahoo results!. IIRC startpage/ixquick already support this.. Sorry for this @kvch. Next time, if the issue is duplicate you can close and link to other issues.. Hmm, I don't see color difference on http://ulrn6sryqaifefld.onion/ , but No.3 is definitely resolved.\nThanks!. Google Image Search can serve images directly from google servers.. also, \"social media\" can become \"SNS\" (shorter text).. No, the \"information technology\" is \"IT\", not \"it\".\nWho will use this \"IT\" category? For what?\n\nSNS = Social Network Services. Facebook, Twitter, and so on.\n. @dalf I guess pyopenssl, pyasn1, ndg-httpsclient are installed properly otherwise searx would not work at all, no? And for the second question. All the machine is doing is starting uwsgi service which is:\n```\n[uwsgi]\nWho will run the code\nuid = searx\ngid = searx\ndisable logging for privacy\ndisable-logging = true\nNumber of workers (usually CPU count)\nworkers = 4\nThe right granted on the created socket\nchmod-socket = 666\nPlugin to use and interpretor config\nsingle-interpreter = true\nmaster = true\nplugin = python\nModule to import\nmodule = searx.webapp\nVirtualenv and python path\nvirtualenv = /usr/local/searx/searx-ve/\npythonpath = /usr/local/searx/\nchdir = /usr/local/searx/searx/\n```\nthanks and cheers. @dalf Thanks for your answer. Problem is gone with searx 0.11 or one of the ubuntu updates. Now searx starts fine. Maybe you are right and it was an ubuntu problem.\ncheers. I'll use my own instance, thanks.. Now I can search when searx.me goes offline again! Good.\nSo I tried searching some keywords and I found that some servers are doing filter bubble.\nproof 1:\nserver name / results\nsearx.itunix.eu 0\nsearch.mailaender.coffee    2\nsearch.homecomputing.fr 1\nproof 2:\nframabee.org    15\nsearch.deblan.org   0\nsearx.laquadrature.net  16\nsearch.matrix.ac    15\nsearch.alecpap.com  20. Search \"fox pet\" on searxes several times and you'll get:\nA:\n     How to Care for a Pet Fox (with Pictures) - wikiHow\n    How to Care for a Pet Fox. Domesticated foxes can make great pets. They're often described as giant kittens, or a mix between cats and puppies. Because foxes are not ...\n    http://www.wikihow.com/Care-for-a-Pet-Fox\nCan I Have A Pet Fox? | Popular Science\nJan 23, 2013 \u00b7 Foxes do not make good pets, it's only legal to own them in a handful of states, and domesticated ones can only be imported from a facility in Siberia.\nhttp://www.popsci.com/science/article/2012-10/fyi-domesticated-foxes\n\nFennec fox - Wikipedia\nThe fennec fox or fennec (Vulpes zerda) is a small nocturnal fox found in the Sahara of North Africa. Its most distinctive feature is its unusually large ears, which ...\nhttps://en.wikipedia.org/wiki/Fennec_fox\n\nB:\n     How to Care for a Pet Fox (with Pictures) - \u2026\n    How to Care for a Pet Fox. Domesticated foxes can make great pets. They're often described as giant kittens, or a mix between cats and puppies. Because foxes are not ...\n    http://www.wikihow.com/Care-for-a-Pet-Fox\nCan I Have A Pet Fox? | Popular Science\n23/01/2013 \u00b7 Do a YouTube search for pretty much any smallish animal you can think of and there'll be several videos of a \"tame\" or \"pet\" version. Any feline, any canid ...\nhttp://www.popsci.com/science/article/2012-10/fyi-domesticated-foxes\n\nMeet Juniper, The Pet Fox Who\u2019s Basically An \u2026\nWe first met Juniper on Instagram, where we saw her frolicking on her mom's bed, and instantly fell in love. Who was this cute fox? So we decided to reach out\nhttp://www.boredpanda.com/juniper-pet-fox-happiest/\n\nC:\n     Can I Have A Pet Fox? | Popular Science\n    Jan 23, 2013 \u00b7 Foxes do not make good pets, it's only legal to own them in a handful of states, and domesticated ones can only be imported from a facility in Siberia.\n    http://www.popsci.com/science/article/2012-10/fyi-domesticated-foxes\nFennec fox - Wikipedia\nThe fennec fox or fennec (Vulpes zerda) is a small nocturnal fox found in the Sahara of North Africa. Its most distinctive feature is its unusually large ears, which ...\nhttps://en.wikipedia.org/wiki/Fennec_fox\n\nAidra Fox : Penthouse Pet October 2014\nPenthouse Magazine has for almost 50 years found the sexiest models to pose in risque nude photos just for you. Penthouse showcases every single Penthouse Pet that ...\nhttp://www.penthousestars.com/aidra-fox-penthouse-pet-october-2014/\n\nHmm. Results are different.. As a conclusion, user shouldn't use one searx website.\nInstead, they should install searxes and use all searx engine randomly. I think.. \"Ecosia can't plant more tree.\" Oh.... Updating via:\ncd /usr/local/searx\nsudo -u searx -i\n. ./searx-ve/bin/activate\ngit stash\ngit pull origin master\ngit stash apply\n./manage.sh update_packages\nsudo service uwsgi restart\nsolves the problem. Sorry for the noise.. @dalf  Is there a solution to this problem?. @kvch But it links to Github and not a page hosted on searx.me... (which was my proposal). As a side note, it may also be worthwhile to promote HTTPS Everywhere so that searx users get its security benefits. Indeed, many searx instances don't have HSTS.. @janit Are you the operator of searx.fi? If so, you can see that it throws a 401, can you take a look at it?. Nope, it still doesn't seem to work. Tried on Firefox 57.0 and Chromium 62.0.3202.94 with a clean history and cache, and with no extensions.\nCan others confirm?\nEDIT: same problem with searx.info. This isn't an issue specific to searx.me anymore.. Got the same problem after upgrading to 13.1. As soon as i switch back to my 12.0 backup everything works flawlessly.. I just tried again. Now i have results from google with version 13.1. Strange. But good enough for me. \ncheers\nt.. And problem occurs again.  searx - 0.13.1 -> google (unexpected crash: CAPTCHA required). OK, i see. I thought 12.0 worked without problems. I have to re-check. Maybe 12.0 just misses the error message?. Hm. Switched back to 12.0. There is no error message regarding google. But the results contain no results from google.. Yeah. As discussed in https://github.com/asciimoo/searx/issues/1089#issuecomment-351942306 it doesn't seem to be the fault of searx.. Switching to another VM with another IP solves the problem for now. Lets see for how long.. Over the last few days I have been noticing this with my searx as well.. Still no solution to this problem?. Yes @Dominion0815 , unfortunately... how can I add startpage and duckduckgo to my default search? At this time I have just bing which isn\u2019t the best. And how can I remove google form the default search until it got fixed?\n. Great, thank you @Dominion0815! \nI am left with one stupid question. To apply those settings I just have to run sudo /etc/init.d/uwsgi restart or sudo service uwsgi restart, is that correct? Mine is running on uwsgi and Apache. \nBecause if I am doing that my searx isn't starting anymore and so far the only solution for me was to reinstall it.... ```\n$ sudo apk add python-dev\n(1/1) Installing python2-dev (2.7.14-r2)\nExecuting busybox-1.27.2-r7.trigger\nOK: 1537 MiB in 385 packages\ngenesisnet-n2:~/stuff/searx$ sudo ./manage.sh update_packages\nsh: ./manage.sh: unknown operand\nRequirement already up-to-date: pip in /usr/lib/python2.7/site-packages\nRequirement already up-to-date: setuptools in /usr/lib/python2.7/site-packages\nRequirement already satisfied: certifi==2017.11.5 in /usr/lib/python2.7/site-packages (from -r /home/babs/stuff/searx/requirements.txt (line 1))\nRequirement already satisfied: flask==0.12.2 in /usr/lib/python2.7/site-packages (from -r /home/babs/stuff/searx/requirements.txt (line 2))\nRequirement already satisfied: flask-babel==0.11.2 in /usr/lib/python2.7/site-packages (from -r /home/babs/stuff/searx/requirements.txt (line 3))\nCollecting lxml==4.1.1 (from -r /home/babs/stuff/searx/requirements.txt (line 4))\n  Using cached lxml-4.1.1.tar.gz\nCollecting idna==2.5 (from -r /home/babs/stuff/searx/requirements.txt (line 5))\n  Using cached idna-2.5-py2.py3-none-any.whl\nCollecting pygments==2.1.3 (from -r /home/babs/stuff/searx/requirements.txt (line 6))\n  Using cached Pygments-2.1.3-py2.py3-none-any.whl\nCollecting pyopenssl==17.4.0 (from -r /home/babs/stuff/searx/requirements.txt (line 7))\n  Using cached pyOpenSSL-17.4.0-py2.py3-none-any.whl\nCollecting python-dateutil==2.6.1 (from -r /home/babs/stuff/searx/requirements.txt (line 8))\n  Using cached python_dateutil-2.6.1-py2.py3-none-any.whl\nCollecting pyyaml==3.12 (from -r /home/babs/stuff/searx/requirements.txt (line 9))\n  Using cached PyYAML-3.12.tar.gz\nCollecting requests[socks]==2.18.4 (from -r /home/babs/stuff/searx/requirements.txt (line 10))\n  Using cached requests-2.18.4-py2.py3-none-any.whl\nRequirement already satisfied: click>=2.0 in /usr/lib/python2.7/site-packages (from flask==0.12.2->-r /home/babs/stuff/searx/requirements.txt (line 2))\nRequirement already satisfied: Jinja2>=2.4 in /usr/lib/python2.7/site-packages (from flask==0.12.2->-r /home/babs/stuff/searx/requirements.txt (line 2))\nRequirement already satisfied: Werkzeug>=0.7 in /usr/lib/python2.7/site-packages (from flask==0.12.2->-r /home/babs/stuff/searx/requirements.txt (line 2))\nRequirement already satisfied: itsdangerous>=0.21 in /usr/lib/python2.7/site-packages (from flask==0.12.2->-r /home/babs/stuff/searx/requirements.txt (line 2))\nRequirement already satisfied: Babel>=2.3 in /usr/lib/python2.7/site-packages (from flask-babel==0.11.2->-r /home/babs/stuff/searx/requirements.txt (line 3))\nCollecting cryptography>=1.9 (from pyopenssl==17.4.0->-r /home/babs/stuff/searx/requirements.txt (line 7))\n  Using cached cryptography-2.1.4.tar.gz\nRequirement already satisfied: six>=1.5.2 in /usr/lib/python2.7/site-packages (from pyopenssl==17.4.0->-r /home/babs/stuff/searx/requirements.txt (line 7))\nCollecting chardet<3.1.0,>=3.0.2 (from requests[socks]==2.18.4->-r /home/babs/stuff/searx/requirements.txt (line 10))\n  Using cached chardet-3.0.4-py2.py3-none-any.whl\nCollecting urllib3<1.23,>=1.21.1 (from requests[socks]==2.18.4->-r /home/babs/stuff/searx/requirements.txt (line 10))\n  Using cached urllib3-1.22-py2.py3-none-any.whl\nCollecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\" (from requests[socks]==2.18.4->-r /home/babs/stuff/searx/requirements.txt (line 10))\n  Using cached PySocks-1.6.7.tar.gz\nRequirement already satisfied: MarkupSafe>=0.23 in /usr/lib/python2.7/site-packages (from Jinja2>=2.4->flask==0.12.2->-r /home/babs/stuff/searx/requirements.txt (line 2))\nRequirement already satisfied: pytz>=0a in /usr/lib/python2.7/site-packages (from Babel>=2.3->flask-babel==0.11.2->-r /home/babs/stuff/searx/requirements.txt (line 3))\nCollecting asn1crypto>=0.21.0 (from cryptography>=1.9->pyopenssl==17.4.0->-r /home/babs/stuff/searx/requirements.txt (line 7))\n  Using cached asn1crypto-0.24.0-py2.py3-none-any.whl\nCollecting cffi>=1.7 (from cryptography>=1.9->pyopenssl==17.4.0->-r /home/babs/stuff/searx/requirements.txt (line 7))\n  Using cached cffi-1.11.2.tar.gz\nCollecting enum34 (from cryptography>=1.9->pyopenssl==17.4.0->-r /home/babs/stuff/searx/requirements.txt (line 7))\n  Using cached enum34-1.1.6-py2-none-any.whl\nCollecting ipaddress (from cryptography>=1.9->pyopenssl==17.4.0->-r /home/babs/stuff/searx/requirements.txt (line 7))\n  Using cached ipaddress-1.0.19.tar.gz\nCollecting pycparser (from cffi>=1.7->cryptography>=1.9->pyopenssl==17.4.0->-r /home/babs/stuff/searx/requirements.txt (line 7))\n  Using cached pycparser-2.18.tar.gz\nInstalling collected packages: lxml, idna, pygments, asn1crypto, pycparser, cffi, enum34, ipaddress, cryptography, pyopenssl, python-dateutil, pyyaml, chardet, urllib3, PySocks, requests\n  Running setup.py install for lxml ... error\n    Complete output from command /usr/bin/python2 -u -c \"import setuptools, tokenize;file='/tmp/pip-build-Nz6JLr/lxml/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-KsoL-record/install-record.txt --single-version-externally-managed --compile:\n    Building lxml version 4.1.1.\n    Building without Cython.\n    ERROR: /bin/sh: xslt-config: not found\n** make sure the development packages of libxml2 and libxslt are installed **\n\nUsing build configuration of libxslt\nrunning install\nrunning build\nrunning build_py\ncreating build\ncreating build/lib.linux-x86_64-2.7\ncreating build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/cssselect.py -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/pyclasslookup.py -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/doctestcompare.py -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/builder.py -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/sax.py -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/_elementpath.py -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/ElementInclude.py -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/__init__.py -> build/lib.linux-x86_64-2.7/lxml\ncreating build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/__init__.py -> build/lib.linux-x86_64-2.7/lxml/includes\ncreating build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/soupparser.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/formfill.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/_html5builder.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/diff.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/html5parser.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/defs.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/usedoctest.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/builder.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/clean.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/_setmixin.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/ElementSoup.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/_diffcommand.py -> build/lib.linux-x86_64-2.7/lxml/html\ncopying src/lxml/html/__init__.py -> build/lib.linux-x86_64-2.7/lxml/html\ncreating build/lib.linux-x86_64-2.7/lxml/isoschematron\ncopying src/lxml/isoschematron/__init__.py -> build/lib.linux-x86_64-2.7/lxml/isoschematron\ncopying src/lxml/etree.h -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/etree_api.h -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/lxml.etree.h -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/lxml.etree_api.h -> build/lib.linux-x86_64-2.7/lxml\ncopying src/lxml/includes/__init__.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/dtdvalid.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/config.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/uri.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/htmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xmlparser.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/schematron.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/relaxng.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/tree.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xslt.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/c14n.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xpath.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xmlschema.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xinclude.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/etreepublic.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/xmlerror.pxd -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/etree_defs.h -> build/lib.linux-x86_64-2.7/lxml/includes\ncopying src/lxml/includes/lxml-version.h -> build/lib.linux-x86_64-2.7/lxml/includes\ncreating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources\ncreating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\ncopying src/lxml/isoschematron/resources/rng/iso-schematron.rng -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/rng\ncreating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\ncopying src/lxml/isoschematron/resources/xsl/XSD2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\ncopying src/lxml/isoschematron/resources/xsl/RNG2Schtrn.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl\ncreating build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_svrl_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_message.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_dsdl_include.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_abstract_expand.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/iso_schematron_skeleton_for_xslt1.xsl -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\ncopying src/lxml/isoschematron/resources/xsl/iso-schematron-xslt1/readme.txt -> build/lib.linux-x86_64-2.7/lxml/isoschematron/resources/xsl/iso-schematron-xslt1\nrunning build_ext\nbuilding 'lxml.etree' extension\ncreating build/temp.linux-x86_64-2.7\ncreating build/temp.linux-x86_64-2.7/src\ncreating build/temp.linux-x86_64-2.7/src/lxml\ngcc -fno-strict-aliasing -Os -fomit-frame-pointer -g -DNDEBUG -Os -fomit-frame-pointer -g -DTHREAD_STACK_SIZE=0x100000 -fPIC -DCYTHON_CLINE_IN_TRACEBACK=1 -Isrc -Isrc/lxml/includes -I/usr/include/python2.7 -c src/lxml/etree.c -o build/temp.linux-x86_64-2.7/src/lxml/etree.o -w\nIn file included from src/lxml/etree.c:619:0:\nsrc/lxml/includes/etree_defs.h:14:31: fatal error: libxml/xmlversion.h: No such file or directory\n #include \"libxml/xmlversion.h\"\n                               ^\ncompilation terminated.\nCompile failed: command 'gcc' failed with exit status 1\ncreating tmp\ncc -I/usr/include/libxml2 -c /tmp/xmlXPathInityaxhjZ.c -o tmp/xmlXPathInityaxhjZ.o\n/tmp/xmlXPathInityaxhjZ.c:2:1: warning: return type defaults to 'int' [-Wimplicit-int]\n main (int argc, char **argv) {\n ^~~~\ncc tmp/xmlXPathInityaxhjZ.o -lxml2 -o a.out\nerror: command 'gcc' failed with exit status 1\n\n----------------------------------------\n\nCommand \"/usr/bin/python2 -u -c \"import setuptools, tokenize;file='/tmp/pip-build-Nz6JLr/lxml/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-KsoL-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-Nz6JLr/lxml/\n```\nStill same it appears.. Well, same as in result (exit status 1) :P\nHad to install SSL and possibly one other thing, finished with sucess now.  Thank you!. Still getting [emperor-tyrant] invalid permissions for vassal searx.ini in the log.\nIt also did the same thing when I had it set to socket = 127.0.0.1:8888.. The issue was the file permissions on searx.ini, solution:\nchown searx:searx searx.ini\nMy working config/ini:\n```\n[uwsgi]\nWho will run the code\nuid = searx\ngid = searx\ndisable logging for privacy\ndisable-logging = true\nNumber of workers (usually CPU count)\nworkers = 4\nThe right granted on the created socket\nchmod-socket = 666\nsocket = /tmp/searx.sock\nPlugin to use and interpretor config\nsingle-interpreter = true\nmaster = true\nplugin = python\nlazy-apps = true\nenable-threads = true\nModule to import\nmodule = searx.webapp\nVirtualenv and python path\nvirtualenv = /usr/local/searx/searx-ve/\npythonpath = /usr/local/searx/\nchdir = /usr/local/searx/searx/\n```. Hm.  Seems to have fixed itself (I no longer have the issue).. @asciimoo I tried the stable version, beta, nightly. All of them have this issue. I use searx.me. @apropostech Just added it to firefox from here: https://searx.me/about. Great, good to know. Thank you @asciimoo! \nI would have another question. Is it somehow possible to change the style of theme in the settings.yml?. Isn't that just the Theme? I wanted to change the style as well. In my case I would like to use oscar and style pointhi. Am I able to adjust that in that line as well?. That worked, thanks a lot @asciimoo!. also maybe a little information in the corner of image with its size?. I confirm this issue on https://searx.me.\nI tried on the following instances:\nhttps://searx.laquadrature.net/\nhttps://seeks.hsbp.org/\nhttps://searx.netzspielplatz.de/\nhttps://searx.potato.hu/\nhttps://searx.org/\nand they all work fine. This seems therefore to be an issue specific to searx.me.\nTested using Firefox Quantum 59.0.2 (64-bit) on Arch Linux x86_64, with kernel Linux 4.16.4-1-ARCH.. The thing which bothers me is that why sound-cloud transmitting packets even though I didn't carried out any searches at all.. Should I remove this line\n- name : soundcloud\n    engine : soundcloud\n    shortcut : sc\nI try to rebuild the docker without sound cloud and test once more. I will second this.. Can some one explain how to use proxies or make the whole process simple in searx?. ",
    "dalf": "Here, a attempt to include result from dbpedia : \nhttps://github.com/dalf/searx/commit/9a8c24fe6260d4f0ced3c00f99952986e4fb099c\n(example here: https://m.al-f.net/?q=paris&pageno=1&category_general )\nIt include result from dbpedia :\n- a image\n- the link to the official web site (or first referenced web site)\n- a link to show a map if there is geolocation information (weather could be added from this)\nNote : a request to dbpedia is sent after the wikipedia has return the result, so it might slow down the global response time. May the content could added with a ajax request on the right side. Other solution : have a local copy of dbpedia. \n. More or less equivalent with the duckduckgo_definition and wikidata engines\n. About javascript : that was my idea.\nAbout categories : It doesn't remove the remembering, it's only remove the use of the cookies when there is query.\nTwo cases : \n- the user comes from the index page : the categories are already selected with the form. No need to use the cookies.\n- the user comes from the search feature of a browser : in most of case he want to use the general category. \nThis second case is the reason of this commit : if you search an image, then use the browser (opensearch), it will use the image category.\nSo it doesn't remove feature, only fix a \"bug\" (it depends of the point of view).\n. oups, right ! \n. The /about page should be updated : it is too technical compare to what a normal user expect.\nI think it should embedded some documentation about the syntax and gives some links to https://asciimoo.github.io/searx/\n. Google, for example, seems to adjust the result according to the geolocalisation of the request IP.\n. For information : \nhttps://caniusepython3.com/check/518bd345-24fc-414b-983d-373cbca99fae\n. I don't know what's block this pull request (except the merge conflicts of course, but the idea)\nAnother solution \n1. declare the remote branch to this repository\n2. fetch\n3. compare :  https://major.io/2012/03/15/compare-commits-between-two-git-branches/ \nThis shell script could be rewrite using python and a Git interface ( https://git.wiki.kernel.org/index.php/Interfaces,_frontends,_and_tools#Python )\n#!/bin/sh\n    if [ \"$1\" = \"\" -o \"$2\" = \"\" ]; then\n        echo \"$0 usage: branch1 branch2\"\n        exit\n    fi\n    BRANCH1=`mktemp`\n    BRANCH2=`mktemp`\n    git log --pretty=format:'%H' $1 | sort > $BRANCH1\n    git log --pretty=format:'%H' $2 | sort > $BRANCH2\n    echo \"**********************************************************************\"\n    echo \"Commits not in \u2018$1\u2032 branch.\"\n    echo \"**********************************************************************\"\n    for i in `diff $BRANCH1 $BRANCH2 | grep '^>' | sed -e 's/^> //'`\n    do\n        git --no-pager log -1 --oneline $i\n    done\n    echo\n    echo \"**********************************************************************\"\n    echo \"Commits not in \u2018$2\u2032 branch.\"\n    echo \"**********************************************************************\"\n    for i in `diff $BRANCH1 $BRANCH2 | grep '^<' | sed -e 's/^< //'`\n    do\n        git --no-pager log -1 --oneline $i\n    done\n    echo\n    rm -f $BRANCH1 $BRANCH2\n. The Makefile could create a copy of .git/refs/heads/master ?\n. May be it was hidden with the infoxes pull request, but if you search for \"2+10/4\" you get the answer from duckduckgo_definition engine. The result is displayed just above \"Suggestions\".\nDuckduckgo provides some open source code about widget : https://duck.co/duckduckhack/determine_your_instant_answer_type\nThere is different types : \n- Spice : use external ressource (JSON API). Based on Javascript. https://github.com/duckduckgo/zeroclickinfo-spice \n- Goodie : answer from code, static data. Based on Perl. https://github.com/duckduckgo/zeroclickinfo-goodies\n- Fathead : https://github.com/duckduckgo/zeroclickinfo-fathead\n- Longtail : Based on python, https://github.com/duckduckgo/zeroclickinfo-longtail \nExample on DuckduckGo, search for : \n- hex blue\n- ascii table\n- password strong\n- london weather\nThere is a way to check if a host is up also. \n. I know that the actual implementation of answer is really rudimentary\nAt first, I didn't know that DuckDuckGo could return html code.\nAbout a searx implementation of instant widgets : I agree, except it divides the effort.\nLook at : https://github.com/duckduckgo/zeroclickinfo-goodies/tree/master/lib/DDG/Goodie\nIf we have to re-implement one third, it's a big challenge to write without bug, and to maintain.\nSpice could be rewrote more easily : \nhttps://github.com/duckduckgo/zeroclickinfo-spice/blob/master/lib/DDG/Spice/DNS.pm\nhttps://github.com/duckduckgo/zeroclickinfo-spice/tree/master/share/spice/dns\nBy the way, in Python, there the weboob (web out of browser) which can help (or not)\nFor example for the weather : http://weboob.org/applications/wetboobs\nAnd there is XBMC plugins, for example for the lyrics : https://github.com/ronie/script.cu.lrclyrics/tree/master/resources/lib/culrcscrapers\nLet's try :-) \n. done. see https://asciimoo.github.io/searx/blog/python3.html. stale ?\n. About google_news, there is a rss output : \nhttps://news.google.com/news/feeds?ned=us&q=test&output=rss\nned = region\nq = query\nsources :\nhttp://www.makeuseof.com/tag/5-interesting-ways-google-news-rss-feeds/\nhttp://stackoverflow.com/questions/20738454/is-there-a-way-to-get-bing-and-google-news-search-as-rss\nQuestion : which rss parser to include ?\nhttps://github.com/jmoiron/speedparser ?\n. Actually, speedparser is based on lxml.\nLook here : \nhttps://github.com/jmoiron/speedparser/blob/master/speedparser/speedparser.py\nThere is a bunch of hack / fall back for all kind of rss format.\nFor bing_news or for google_news we can use lxml directly, but we will rewrite some code for another engine, and in the end we will rewrite something more or less equivalent to speedparser.\n. First version of google engine : \nhttps://gist.github.com/dalf/33b06240e02a7ba6ca87#file-google-py\n. About google_image engine, there is an issue : there is no mean to get the image URL, only the thumb URL, or the context URL. \nTwo problems : \n- the thumb URL can be seen as a privacy issue since it's a hosted on google (but there is the same issue with video engines)\n- searx can't deduplicated URL since some will reference the thumb image, and some other will reference the real size image.\n. I'm using an debian wheezy with a virtual environnement and there is no issue.\n. About bing_news there is a rss output format: \nhttps://www.bing.com/news/search?q=searx&setmkt=en-US&first=1&format=RSS\nmay be more reliable.\n. Also https://duck.co/duckduckhack/ddh-intro\n. Some other notes : \n- https://pypi.python.org/pypi/duckduckgo2/0.2\n- http://api.duckduckgo.com/?q=1%2B1&format=json&pretty=1&no_redirect=1&kad=en_ES (Answer)\n. In my fork, I have added:\n- infoboxes from duckduckgo definition and wikidata\n- answers from duckduckgo\nExamples :\n- https://al-f.net/?q=berlin&category_general=on\n- https://al-f.net/?q=1%2B1&category_general=on \n- https://al-f.net/?q=python&category_general=on\nIssues : \n- too much suggestions\n- duckduckgo_definition only works correctly with english search\n- displaying information from wikidata is not easy since there is many references.\n. Some others ideas : https://github.com/asciimoo/searx/wiki/Infoboxes-brainstorming\n. Not a clean way : \n- http://stackoverflow.com/questions/14665064/using-python-requests-with-existing-socket-connection\n- https://github.com/kennethreitz/requests/blob/c879873b3e279f9d3531fe4b8b4a3ec1643bef65/requests/packages/urllib3/connection.py\n. Replace import requests with import searx.poolrequests ( source : https://gist.github.com/dalf/245d54a7e15c8ad84de8 ). \nIt's seem like a hack but we can't use directly urllib3 or HTTPAdapter() since session.py provides some features\nFor information, session.py source code : https://github.com/kennethreitz/requests/blob/53d02381e22436b6d0757eb305eb1a960f82d361/requests/sessions.py \n. I have fixed the tests and adjust the title of the pull request.\n. Side note : take care not to create a open proxy.\n. With the SPDY protocol, domain sharding is not necessary.\nNginx documentation : http://nginx.org/en/docs/http/ngx_http_spdy_module.html\n. The initial issue was fetching image and minimized them. \nThere current implementation only fetch the image.\nShould we consider the issue closed ?\n. not tested with IE\n. When I wrote this issue, I coudn't type ( in my searx intance and https://searx.tk/ but I was working anywhere else.\nNow I can't reproduce this bug. \nI close the issue...\n. I don't know how to do that : \n- the Debian are released are slow\n- searx is evolving fast\nIf we rely on the python packages from Debian we will have to\n- probably maintain two branches for compatibility issues. I think we will have more issue than the is_redirect one since requests version of the stable Debian is old.\n- have some automatic tests to check if the package is working correctly (install, upgrade, etc...).\nBut the other (ugly) way means that searx package will embedded all the dependencies. It's the exact oposite of Debian spirit.\nI've never created a debian package including python code, so may be this is dumb question.\n. I agree about a PPA but what about the dependencies ?\nNew version of the dependencies as .deb in the PPA ?\n. Why not a package for each case :\n- searx dependencies : see Installation wiki page\n- searx-uwsgi depends on searx, uwsgi and uwsgi-plugin-python \n- searx-uwsgi-nginx depends on searx-uwsgi and nginx\n- searx-uwsgi-apache depends on searx-uwsgi and apache2\nAbout searx package, we have to try, because according to https://wiki.debian.org/Python/LibraryStyleGuide#Build-Depends : \n\n... This is a huge no-no, and pybuild internally sets the http_proxy and https_proxy environment variables (to 127.0.0.1:9) to prevent this from happening. \nit will not work, at least with setup.py.\n\nOne solution : the package can contains everything, source code and the virtualenv.  \nIn this case, we can split the searx package into : \n- searx : only the searx source code, and depend on searx-env\n- searx-env : the virtualenv (48Mo for mine, may be there is some extra things) \nEDIT : there is lxml which is compiled, and then it's start to be complicated...\nEDIT2 : see http://dh-virtualenv.readthedocs.org/en/latest/info.html\n. > Installing the following dependencies from Debian Stretch seems to make searx work perfectly: nginx uwsgi uwsgi-plugin-python3 python3-requests python3-certifi python3-yaml python3-pygments python3-werkzeug python3-flask python3-flask-babel python3-lxml python3-dateutil python3-openssl python3-ndg-httpsclient python3-pyasn1\n I think python3-pyasn1-modules is missing (0.0.8 in requirements.txt, 0.0.7 in unstable)\n python3-certifi is based on version 2016.2.28 in unstable (instead of 2016.9.26). I don't know how this package is maintained ?  \n\nI only change the secret in the settings.yaml do I have to change anything else to distribute the result? In particular, do I have to change the port, bind_address or base_url settings when running searx behind uwsgi? It seems to run fine with the defaults.\n the base_url has to changed if searx is not served at the root of the HTTP server.\n bind_address and port can be anything as long the HTTP server (apache, nginx, h2o, haproxy...) uses this (bind_address, port) tuple.\n* other settings are ok, see :\nhttps://asciimoo.github.io/searx/dev/install/installation.html#installation\nand https://github.com/asciimoo/searx/wiki/Searx-with-haproxy\n\nIf there is an apache / nginx configuration, it would be good to add some HTTP headers to increase the security. See https://github.com/asciimoo/searx/wiki/Searx-with-haproxy#security \nStill about HTTP front-end, It would be good to encourage / force SSL, I don't know if it is possible ? \n\nsearx seems to work fine out of a directory that it cannot write to. Does searx really not write any logs or caches to anywhere? This would make the \"upgrade problem\" really easy as there is no data to either convert or regenerate once a new version gets installed.\nsearx doesn't write any logs except the one recorded from the logging module. The not merged PR #739 creates a temporary file into /tmp.\n\nAbout maintenance : if an engine changed its API, the searx has to be changed otherwise searx can't use anymore this engine. Can the changes be backported to the debian package even into the stable version ? If the changes can't be backported, the searx stable version will be less and less usable.. Great ! Thank you.\nHum one question : https://browse.dgit.debian.org/searx.git/ seems to be empty ?. @josch : One note the python3-searx debian package in sid : python3-ndg-httpsclient and python3-pyasn1 are declared necessary. From the PIP point of view, these two packages used to be dependencies of requests, but the version 2.18.1 doesn't need them anymore.. https://packages.debian.org/source/sid/searx\nsee #1091. @pointhi : did you found a solution ? a workaround ?\n. Some ideas : \n- Record and count the number of category combination. Purpose : adapt or not the UI / UX to the most common use cases.\n- Record the server and browser response time (using the Navigation Timing API). Purpose : find the bottleneck in the response time (engines, searx code, javascript).\n- Create a time out counter in addition to the error counter. (Note :  the current implementation handles the Read timed out exception has any other exception).\n. I have created a metrology branch with more detail statistics : fa4a795982ef6d25e204661c4a3423efeeb1de75\nThis allow to see that the callback takes sometimes far more time than the HTTP request.\nIt doesn't solve the issue #199 \n. About the issue #199 , if there is one process and many threads it seems to solve the problem. I don't know how it's scale.\nuwgi configuration :\n```\nNumber of workers\nworkers = 16\nprocesses = 1\nthreads = 16\n```\nYes 16 threads because of the image proxy.\n. About thread safety :  http://flask.pocoo.org/docs/0.10/quickstart/ look at the \"Accessing Request Data\" section.\nAbout global variables, it requires some changes : http://flask.pocoo.org/docs/0.10/appcontext/\nOr may be, in some cases, we can implement thread safety by yourself (statistics for example ?) \nOr we can think about Twisted.\n. I'm working on a new version of the stats page : \n\nIt is based on jqplot, I don't know if this is good choice. \n. Commit in https://github.com/dalf/searx/tree/metrology but still unfinished.\n. I've updated the metrology branch. Here an example : \nhttps://al-f.net/stats\nNew things : \n- errors count are displayed as a percentage of the total request count for an engine.\n- in the \"Page loads\" section, you can click on an engine to display the different quartiles.\n- new section \"Search page\", which display the average and the quartiles for the main search page.\nThese charts and stats can help to reduce response time as we can see\n- which engine is slow, how often.\n- how slow is the rendering code.\nBut obviously it reduces privacy : below 100 requests (per engine or globally), you can guess how many requests have been sent.\nSo, it's bring me to some questions : why stats ? for who ?\nFrom user point of view, the score and the page loads section can help to choose the right balance between the response time and useful responses.\nIn this case, the response time and the score should be displayed in the \"preference\" page, where the user can disable a search engine.\nFrom the developer point of view, I think all these details can be helpful.\nMay be, the stats page should be display to the administrator and no one else. \nBut the administrator is not necessarily a developer... \n. https://github.com/asciimoo/searx/blob/master/searx/settings.yml#L18\ncan be closed ?\n. May it is related to #77 ?\n. I still have an issue with \"!flick test\", the fourth image. \n. I found another weird case.\nHow to reproduce : \n- search for !500px big bang theory\n- click on each image\n- in case all images are displayed correctly, refresh the page\nI can reproduce the bug on PC and on searx.me\n. I can't be sure if I have tested with two browsers.\nNow I can't reproduce the case with Chrome and FF (before and after clearing the cache).\nMay be I didn't apply http://cdn.meme.am/instances/500x/57076520.jpg\n. I think that apache (or nginx) give a gzip version of the file, but firefox for some reason doesn't decompress it. If I remember correctly @pointhi already had a similar issue.\nPerhaps try something like that (for apache) :\n<FilesMatch opensearch\\.xml$>\nSetEnv no-gzip 1\n</FilesMatch>\nIt's a workaround, but if it can help.\n. Some links : \n- Language detection : https://code.google.com/p/language-detection/ ( slides : http://fr.slideshare.net/shuyo/language-detection-library-for-java )\n- Chronium Language Detector 2 : https://code.google.com/p/cld2/ (with Python bindings : https://code.google.com/p/chromium-compact-language-detector/ )\n- http://fr.slideshare.net/shuyo/short-text-language-detection-with-infinitygram-12949447\n. Google, to pick one, is more than a thing that is feed with privacy. \nIt's provide useful feature as a counter part.\nNow \"people\" knows that privacy matter but no everyone are willing to pay the price for it.\nMay be, this is something that should be explain.\nAbout searx : what privacy doesn't it provide ?\nIn my opinion, searx is proxy to different search engines. So it can \n- avoid tracking cookie / user.\n- avoid tracking of the IP.\n- avoid tracking of the geographic location.\n- avoid ads\nWhat does the auto-completer do : it's a proxy to different auto-completer. Same feature : no cookie, no IP,  no geographic location. \nHow does it hurt ? The search engine knows more ? but about who ?\nFrom the user point of view : with the auto-completer there is no need to type everything, only the first letters, no need to remember the right spelling.\nMoreover why disabling javascript is good ? because javascript can do evil stuff ? so either \n- you don't trust searx javascript code. Example : someone can modify the searx code to do nasty stuff in javascript.\n- you don't trust the \"wire\". Example : some company / country install proxy with appropriate certificates to intercept https traffic. This kind of appliance can modify the pages on the fly. \nIt's all about trust : why do you trust a searx instance ?\nWorth case scenario : someone says \"Hey look I have setup an searx with bunch of bandwidth and IP\", and log everything. You will never know. \nIn fact, how searx scale ?\n- if everyone has it own instance. I can't find the article, but I remember that Google is able to determine an user with 7 searches, even it is not logged. So which privacy doesn't it provide ?\n- at the opposite, if everyone is using the same instance, this unique instance must a bunch of IP to avoid captcha. But in this case, how can we trust this instance ?\nOf course, it's not black or white. A searx instance with few users will do the job. \n. > when you consider other issues, like round-robbin dns searx instances, where you have a bunch of instances being served. would you still trust that js?\nEven if there is no javascript in the searx github code, every searx administrator can add javascript, you will never know except if you look at source code of every pages.\nIn fact, it's not a searx issue, it's browser issue. noscript extension will do the job.\n. The new version of typeahead.js can handle multiple datasets : here an example\nI'm not sure that two datasets ( = two requests ?) is right to do that, but at least it shown one way to show the responses : one dataset for general result for example, another one for map. \nWith this UI, user could choose multiple autocompleters. None by default as now : as said before, it's only for people who would like to use javascript, knowing what's they doing.\nAbout the new version of typeahead.js : \nhttps://github.com/twitter/typeahead.js/blob/master/CHANGELOG.md#0110-april-25-2015\n\nThere are bunch of API changes with this release so don't expect backwards compatibility with previous versions. \n...\nBeware that since this release is pretty much a rewrite, there are bound to be some bugs. \n. @stef your point was understood and taking it into consideration : \n- searx can be use without javascript (look at #171 : one blocking point was javascript, since then oscar was modified to be use without javascript)\n- searx doesn't enable autocompleter even if javascript is enabled\n\nThe missing point is a warning about privacy and javascript but that's ANOTHER issue, feel free to create one issue about it, or even better to create a warning / documentation about javascript, and create a pull request.\nIt's not an order, just if you have time it would be great as we are lacking some arms / time. After you can argue on how we spend the time : security, fancy unsecure feature, etc...  but look at the lastest commit : some was about https connection. In my opinion it's far more dangerous to spread requests in clear text from the searx instance than using javascript.\nOnce again, that's not the issue here. Here it's about how to improve an existing one, even it's for people who want to use javascript (yes the warning / documentation part is still missing, read the previous paragraph, and should we stop thinking about it ? I said thinking).\nI saw a lot of question like \"which privacy searx provides ?\", once again it's another issue. Feel free to help.\nThis issue is about how to improve autocompleter.\nMy life:  I wish that this feature could be improved because I use it on my own instance on my own server, and shared it with some friends. BTW, one of my friend has the noscript extension.\nEDIT : I feel like your request is to have an searx without javascript, to force every user to use searx without javascript, and that we defend this opinion of rejecting every features that could use javascript. I'm not the only one here, but I think that's not the searx spirit even we have taken the point into consideration (look at the default settings, look at the missing documentation I spoke about).\n. As on the requests lib implementation, a new Session is created for each request but the two HTTPAdapter remain the same : there is no cookie persistence.\nThe code I posted yesterday was reusing Session and clearing cookie, but I think it's too much complicated.\nSeveral notes : \n- the 200ms is on my internet connection, may be on a server / datacenter it's different. \n- closed connection are detected by urllib3, but is there some exception, some special case ? \n. Seems strange since it was the case for the google engine : #193\nThere is an allow_redirects parameter for the Requests\n. I agree.\n(more or less related to #109 ) \n. DB-API is only for relational database :\nhttps://wiki.python.org/moin/DbApiModuleComparison\nOther framework are may be too much ?\nhttp://docs.python-guide.org/en/latest/scenarios/db/\n. I don't know how peewee handle multiprocess : https://github.com/coleifer/peewee/issues/263 \n. And as long there is no cache on the Peewee layer.\n. I agree.\nIs a web site database a bad or unrealistic idea ? It could describe for each web site :\n- language (example : wikipedia) see #206\n- can the result be merged with another website (with an id for example)\n- rewrite url (to https, or to replace fr-fr.facebook.com to www.facebook.com )\n- how to rewrite to the mobile url ( en.m.wikipedia.org )\n- favicon ? see #108\nI know it can be a very huge database, and this is not the scope of this issue.\n. I close the pull request in the current state : it's work on my ADSL connection, but not on a server.\nThe requests are redirected to the captcha test.\nI guess that google use the IP address.\n. I don't know Typescript, but why not ECMAScript 6 ? Since it will be the next version of (Java|ECMA)script.\nThe feature list : https://github.com/lukehoban/es6features#readme\nAn article about the classes : http://www.2ality.com/2015/02/es6-classes-final.html\nAnd there is a way to compile ECMAScript 6 to version 5 which is supported by the current browsers : https://github.com/babel/babel\n. I say \"ECMAScript 6\" because it will be supported natively one day or another, and at this point more and more people will know it.\nBut yes type checking is not supported except with this kind of hack :  http://www.nczonline.net/blog/2014/04/29/creating-type-safe-properties-with-ecmascript-6-proxies/ (and I don't know how this transpile to ECMAScript 5)\nBTW, I just found that there is a bunch of transpiler : http://kangax.github.io/compat-table/es6/\n@asciimoo what do you think ?\n. I'm the author of the deleted articled on the english version of wikipedia.\nI saw the french translation, I just translated the content, then it was quickly deleted : I didn't saw the need for secondary sources.\nAlso, I created the wikidata page (this one is not deleted) : \nhttps://www.wikidata.org/wiki/Q17639196\n. Open parenthesis\nSearching for \"searx\" on twitter, I found those references in russian : \n- http://hostink.ru/news/searx_0-7/\n- http://www.nixp.ru/news/13172.html\n- http://novostey.com/soft/news654053.html\nClose parenthesis\n(I know, it's not useful for the german version) \n. > they ask why all results are in English, and not in there own language\nIs it the issue #77 ( value based on the \"Accept Languages\" HTTP header) ?\nIn my opinion the value \"Automatic\" in \"Search language\" preference is misleading since it's really mean no language preference or worse English language. In fact in most of the engine, there is something like\npython\n    if params['language'] == 'all':\n        language = 'en'\nBTW the internal value is \"all\" which is different from user value \"Automatic\".\nMoreover, even it is not the case for now, what to do if an engine has \"language_support = False\" ? If the users doesn't set \"Search language\" to \"all\", the engine can't use ( https://github.com/asciimoo/searx/blob/master/searx/search.py#L450 and https://github.com/asciimoo/searx/blob/master/searx/engines/init.py#L71 ). What is the use case of this feature ?\n. What you say, it's that the common use case is a search with one category, at least for images.\nThat's why, I suggest that searx records the categories choices and frequency for searches (frequency in percentage to preserve privacy).\nMay be, it could help (but not decide) to choose the right category / sub category ; and UI / UX.\nAbout the design : why not ask help on twitter ? or feedback (but not before there is a forum)\n. A brainstorrming / cut & paste from other search engines.\n\n\nSeems to uses the space more efficiently, no need to click on \"Advanced settings\" \nPreferences / About could be with a menu on the right (can be implemented with only CSS)\nNumber of result could be on the left column just before the first result\nThe choice of color is ... \nThe button on infoboxes and suggestion becomes link  \n\nNo HTML sorry : it's a messy edit of the templates, HTML using the Inspector and Gimp.. Done. Sorry for the messy commits.\n. According to http://docs.python-requests.org/en/latest/user/advanced/#proxies\nHTTP(S)_PROXY are supported. \nI miss something ?\n. How do you start Searx ?\nUsing a virtualenv and starting searx in debug mode with python webapp.py the HTTP(S)_PROXY variables are used.\nI guess it's an issue with uswsgi / supervisord\n. About async requests : \n*http://tavendo.com/blog/post/going-asynchronous-from-flask-to-twisted-klein/ \nThis is exactly the \"problem\" of searx\nAbout uwsgi : \n- http://uwsgi-docs.readthedocs.org/en/latest/Async.html#running-uwsgi-in-async-mode\n. +1\n. Note : one thing that is missing is the current implement : reuse of threads (to avoid the cost of creating thread at each request which is time consuming). \nBut the way it is implement in the PR is way too complicated.\n. Why not both ?\n- Choose a category\n- Not mandatory : choose a sub category\n- Not mandatory : choose engines inside the selected sub category\nOr : \n- Choose a category\n- Not mandatory : choose engines inside the selected category, sorted by sub category \nAnother solution : show results with different tabs, one for each sub category.\n. I've started to list all existing engines with the result types :\nhttps://github.com/asciimoo/searx/wiki/existing-engines\nIt's unfinished, but look at the videos category.\n. Looking at : \nhttp://google.com/safebrowsing/diagnostic?site=btdigg.org/\nThere is two networks, one from cloudflare.\n. Side note about the stats / idea for later :\n- record the resolve IP for each host ( btdigg.org, etc...) \n- record the certificates (if possible ?) \n- make them public through a REST API.\nThis would allow the external searx stats to find potential  spoofing by comparing the results (or find \"fun\" facts about IP resolvers)\n. I can't reproduce the bug.\nDoes it solve with :\npython\nif hasattr(response, 'is_redirect') and response.is_redirect is True:\n?\n. One idea : use the browser cache with the help of the ETag HTTP header.\nIdea of implementation : \n- Create a hash of all parameters (query, browser language, selected engine based on query and cookie values) and add a ETag HTTP header.\n- Store this ETag and an expiration date\n- When there is a new request, check If-None-Match HTTP header : it's the previous ETag value stored by the browser. If the two ETag match and are not expired, then searx can response with the HTTP status 304\n- Important part : disable POST, and only use GET\nImproved idea : \n- the ETag is a hash of all parameters, an expiration date can be added. For example next day at midnight. When the result expired, the ETag changes.\n- In this case\n  -  there is no need to store a map (ETag -> expiration date).\n  -  All request are cached between 1 seconds (request at 23:59) and 24 hours (request at 00:00)\nI've never used it, but I guess a reverse proxy can also use this ETag header.\n. Sorry I don't saw that. Do you think it should be configured ? How ?\n. At least from the presentation layer point of view, this issue #211 will have the problem ( ? )\nAn idea : create group which can combine different results. A group is composed of \n- a common part.\n- some specific part for each result in the group.\nFor a map result : \n- the global part can show a map which show all the results of the group\n- for each result, there is a the normal link (but without \"show map\", etc..., only a link)\nImplementation : \n- a result can be a group, which contains other results\n- a \"group\" template show the common part and iterate over the results inside.\nThe same idea can be done for 'normal' text results.\nOf course, the main problem remains : how to combine the results.\n. Filter ?\nEach engine produces results, and the results are filtered.\nA filter can be used : \n- to apply https rules\n- to merge infoboxes\n- to merge 'normal' results and change the order according the scores.\n- to group results (this issue and the #211 )\n- to merge an image and normal result because they have the same target URL\n- add suggestions using spell checker (I don't know for this one)\nThe settings.yml can define which filters are activated and the order.\nSo when a new result is received from an engine, it goes through all filters.\nIdeally, a filter can receive a result one at a time, and emit result one at time. \nOf course, an merge operation will buffer all the results ; and there is performance / memory usage to take into account.\n. sh\npip install --upgrade -r requirements.txt\nshould help.\n. I've updated the page : https://github.com/asciimoo/searx/wiki/Installation#how-to-update\nnot tested.\nMore over, since we are talking about privacy : \nhttps://thomas-leister.de/open-source/linux/server/searx-ip-adressen-im-uwsgi-log-anonymisieren/\nwhich means to add to nginx configuration (TODO : apache ) :\nuwsgi_param REMOTE_ADDR 127.0.0.1;\nand uwsgi configuration : \ndisable-logging = true\nAnother point, why not create script that update a searx instance ?\n. First try.\npackage requests_certificates.py to import :\n``` python\nmonkey patch of the monkey patch to records certificates\nimportant : import requests which guarantie that pyopenssl hook has been done\nimport requests\nimport requests.packages.urllib3.contrib.pyopenssl\nimport requests.packages.urllib3.connection as  connection\ncertificates = {}\norig_connection_ssl_wrap_socket = connection.ssl_wrap_socket\ndef ssl_wrap_socket(*args, kwargs):\n    s = orig_connection_ssl_wrap_socket(*args, kwargs)\n    if type(s) is requests.packages.urllib3.contrib.pyopenssl.WrappedSocket:\n        print s.connection.get_cipher_version()\n        print s.connection.get_cipher_name()\n        certi = s.connection.get_peer_certificate()\n        print certi.get_issuer()\n        print certi.get_subject()\n        print certi.digest(\"sha256\")\n        certificates[kwargs['server_hostname']] = s.getpeercert(binary_form=False)\n        print certificates[kwargs['server_hostname']]\n    return s\nconnection.ssl_wrap_socket = ssl_wrap_socket\n```\nSee : \n- https://github.com/pyca/pyopenssl/blob/master/OpenSSL/SSL.py#L1676\n- https://github.com/kennethreitz/requests/blob/3880cf1255c70e7a13a491cd07d282d41d871649/requests/packages/urllib3/contrib/pyopenssl.py#L252\nHow to check : \n- something based on RFC 7469 ?\n- http://www.bortzmeyer.org/7469.html (French article that RFC) \n. I've committed a connectioninfo branch on my repo but there is a lot of missing part :\n- no tests with a proxy\n- the output format is wrong (we don't which IP return which certificate). Output : https://al-f.net/connectioninfo\n- I'm not sure that certi.digest(\"sha256\") is the right thing to do.\nWhat to do if the certificate changes for one host ? Issue a warning ? \n. See \n- http://www.certificate-transparency.org/\n- https://github.com/google/certificate-transparency\n(from this French article : http://linuxfr.org/users/spack/journaux/http-pousse-vers-la-sortie ) \nand : https://lwn.net/Articles/595790/\n. implemented: #299\n. Does the browser cut the connection ? How does it appear in the network tab of the browser ?\nIf I remember correctly, there is the same error when I use the autocompleter and type \"too\" fast : the browser cuts the first connection for the first request, and send another one with more characters.\nBUT there is no freeze in the case I describe.\n. If I understand correctly, the idea is the create a real gif (but empty) and add javascript content after.\n-  <img src=\"img.gif\"> display an empty image.\n- <script src=\"img.gif\"></script> load the script ( the javascript engine skip the garbage at the begining, the garbage is the gif).\nIf I still understand correctly, <img src=\"img.gif\"> won't harm since this is a valid gif : the browser will sniff a real gif (at least as describe in the blog entry).\nThe real issue in my opinion is a png that is a html document : \n\nThe worst instance related to mime sniffing is an old IE bug. As I understand it their sniffer tried some image formats and then HTML; then when they added PNG sniffing it was added to the sniff list after HTML, either by mistake or to maintain compatibility with pages that were currently being sniffed as HTML. The result of this is that even valid PNG images can be sniffed as HTML, converting a user-uploadable image into a Javascript (XSS) vector. \n\nfrom https://htaccess.wordpress.com/2009/09/22/x-content-type-options-nosniff-header/\nThe searx image proxy could do quick check with magics numbers : the would avoid wrong MIME type.\nnosniff test (doesn't include the script as the png / gif test) : \nhttps://philip.html5.org/tests/ie8/cases/content-type-nosniff.html\nfirefox doesn't support the nosniff header : \nhttps://bugzilla.mozilla.org/show_bug.cgi?id=471020\nchrom* implementation with comments about FF / Opera / IE implementations : \nhttp://src.chromium.org/viewvc/chrome/trunk/src/net/base/mime_sniffer.cc?revision=HEAD&view=markup\nOther issues : https://msdn.microsoft.com/en-us/library/dd565635%28v=vs.85%29.aspx (link from the htaccess blog). Could be a problem since searx includes content from other website.\nSide note (not really related to this issue) : \nhttps://media.ccc.de/browse/congress/2014/31c3_-5930-en-saal_6-201412291400-funky_file_formats-_ange_albertini.html#video\nEdit : another link about png interpreted as html : http://landofthefreeish.com/security/mime-sniffing-in-internet-explorer-enables-cross-site-scripting-attacks/\n. +1 for widget / plugin system\nEDIT : In my opinion, the fix should be in duckduckgo_definitions.py not in webapp.py\nIf I activate the zealot mode about privacy : searx shouldn't answer THIS question because, it's mean that the browser IP value will be spread over all the code.\nIf we talk about privacy we should even advice this in nginx configuration : \nuwsgi_param REMOTE_ADDR 127.0.0.1;\n. All these queries give the IP address : \n- ip\n- what is my ip\n- what is my ip address\nI suggest that\n- duckduckgo_definitions.py detect the answser \"Your IP address is ...\"\n- and either ignore this answer, or replace it with the right one.\n. Note : that's point of the rel=\"noreferrer\" : hide the referrer to the target site.\nOf course, I would have been too simple if all browser support this feature.\n. I think this is a duplicate of #271 : @ldidry can propose a pull request about this feature.\nIf you are in hurry, you can hack your instance, see : \nhttps://github.com/asciimoo/searx/blob/master/searx/webapp.py#L646\nReplace 'post' by 'get'.\n. @xavierle :  can you close this issue since there is another one about the same subject ?\n. Theme inheritance ? Say somewhere that frama theme is based on oscar, and frama theme could use the oscar resources.\n. > If searx is not P2P, please add SOCKS proxy support.\nLook at #236 : searx doesn't support directly an outgoing proxy right now, but you can use tsocks.\n. About image_proxy parameter.\nLooking at https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L235\nsearx reads directly the value of the cookie.\nAbout image_proxy and json format : \nhttps://github.com/asciimoo/searx/blob/master/searx/webapp.py#L230\nis called by the HTML templates, but for the JSON output format : \nhttps://github.com/asciimoo/searx/blob/master/searx/webapp.py#L391  \nMay be, image_proxy could be a plugin to filter URL ?\nThe same could be apply to HTTPS rewrite code.\n. Related to #39 \nIt's not easy to solve : \n- search engines use the ip (of the searx instance) to \"adjust\" the result\n- not all searx instances are updated\n- some times, searx are customized. \nBut what you suggest can be done (either DNS entries, or redirection).\nAnother ( funky / strange ) solution : implement a searx engine for searx (as there is google engine, yahoo engine etc...). The searx engine chooses a random instance, and send the request. The searx instance list could be downloaded. It (can) makes sense if the searx instances is installed on the user computer. \n. Some news :\nI have an improved version, which display stats by categories (which help to see which engine is slow in one category).\nBut it's still buggy, and it doesn't handle plugins. \nSadly, for now I don't have time to work on it...\n. done : https://asciimoo.github.io/searx/blog/python3.html. There is already some searx bangs as describe here https://github.com/asciimoo/searx/wiki/Query-language\nAbout the redirections as you describe, duckduckgo exposes its API.\nexample\n(replace the 0 by 1 with no_redirect parameter to have a redirection)\nThis API is used by the duckduckgo_definitions engine. \nIf this is implemented, I don't know : \n- how to not conflict with existing searx bangs\n- how searx should behave : direct redirection, display a link to the web site (like an answer)\n. I don't know how people upgrade theirs searx instance. One git pull will be not enough, they have to run pip install -r requirements.txt (or equivalent), if they don't they will see new \"bugs\". \n. One way to solve this privacy issue is to disable HTTP requests.\nCan be implemented with a specific requests.adapters.HTTPAdapter that throws an exception when the get_connection method is called and use it in https://github.com/asciimoo/searx/blob/master/searx/poolrequests.py\nThis behavior could be enable with an user setting.\n. To bring back this issue from the dead, one idea could : \n```python\nto import\nresults can be harmful, filter are not implemented\nHARMFUL_CONTENT='harmful'\nresults can be harmful, there is support for content filtering\nSAFESEARCH_SUPPORT='support'\nresults are for sure harmless\nHARMLESS_CONTENT='harmless'\nfor each engine\ndescription .. describes the engine\ndescription={\n  'categories': ['general'],\n  'language_support': True,\n  # should be safesearch_support ?\n  'safesearch': SAFESEARCH_HARMFUL,\n  # should be time_range_support ?\n  'time_range': True,\n  # should be paging_support ?\n  'paging': True,\n  # 'language': 'en',\n  # informative\n  'use_api': False,\n  # forbid HTTP connection for this engine\n  'allow_http': False,\n  #how the user can visit the engine without using searx\n  'url': 'https://www.bing.com/',\n  'name': 'Bing',\n}\n. *multiple_requestscan be automatically updated\n*special_search_syntaxandquery_example``` : :+1:   related to #992 (regex or a method ?)\nHow about settings.yml  ?\nyaml\nbing:\n   description.use_api: True\n?\n. One complex solution : \n- merge #215\n- but detect if there is a redirection to /sorry..., in this case the engine uses google.com eventually with the PREF cookie (it can be done one time at the start)\nAs I said, a complex solution... \n. Dependencies has been updated yesterday. Could you try again ? (git clone, make minimal). And if it still doesn't work, could try (make clean, make). Really not sure if it helps.\n. Broken pipe : it's like the TCP connection was closed. Do you have a firewall ? \nWhat does iptables -L say  ?\n. @g4jc  : did you gave up ? found a solution ?\n. Ok ! I think I get it.\nEverything is working, except those errors ? \nTry to disable the autocomplete feature, and see if the logs stay clean.\nExplanation : when you type of the search input field, the autocomplete makes request. Read: javascript is making AJAX request.\nWhen you click on the search button, the page is unloaded and the pending AJAX request is cancelled. From the searx point of view the TCP connection was broken.\nThere is nothing wrong with this those errors, except that the logs are not very clear about what is happening.\n. How did you configure searx ? I mean do you use Apache ou Ngnix ?\nCan you try this :\n- stop apache / ngnix / uwsgi etc...\n- edit searx/settings.yml : set debug to True\n- run bin/searx-run\n- send a request to localhost:8888 even using curl or wget.\n. From a fresh Debian Jessie installation, I installed privoxy and tor on the same machine as you described. But, I don't have any timeout (ifconfig.me shows that tor is correctly configured). The response time on searx.me is bellow 13 seconds (sometimes, 2 sec, 4 sec, 12 etc...)\nHow long does it take to have a time out ? 60 seconds ? The number could help to find the layer that is involved.\nIn Debian Jessie, the privoxy configuration doesn't set a timeout.  As I understand the documentation, there is no timeout. Does it help if remove privoxy ?\nAbout Nginx, there is a default 60 seconds timeout\nI can't find anything about Flask (the Python framework used by Searx)\n. :-)\n. done\n. I didn't know that. Let's go with /dev/null\n. done https://asciimoo.github.io/searx/dev/install/installation.html ?\n. DNS use case : people who would like to use an outgoing proxy (TOR or something else), doesn't want to use system DNS ( ? ), or at least to make sure that DNS requests go through the proxy. \nI don't know if requests allows this or not, or if we have to monkey patch.\n. done ?\n. What I mean: the flicknoapi_engine.py \"browses\" the HTML page intended to the browser. Now this HTML page uses a REST API with api key, and doesn't contains reference to the thumbnail etc... (not sure, I made a quick look).\nDisplay a this search with chrome dev tools / firebug, you will see a request to /rest..... with the results.\n. +1\n. @privacytoolsIO : great :-) \nIf you can give a list of website and affiliate parameters / patterns, it would be awesome.\nThe implementation will be something that look like the https_rewrite plugin, line 225\n. 404 error page can be the search page with a clear message.\n- it's avoid one click to go elsewhere most of the time ( the purpose of searx is to search something )\n- it's simple to implement\nJust an idea.\nIf we display one dedicated error page, we should provide links to elsewhere. \nIt's more or less \"free\" with the oscar theme, but not with the others themes.\n. In Google Chrome, I have looked on the network tab of the developper console for three urls : \n- /robots.txt\n- /opensearch.xml\n- /\nOn searx.me and al-f.net, there is a reponse header for /robots.txt, the content is not compressed according to response header.\nOn searx.oe5tpo.com there is no response header, it look like the robots.txt is sent without HTTP headers.\nThere is same behaviour (no header) with /opensearch.xml, but it works on all servers (except I remember there was a similar bug with this URL few months ago).\nAbout the /, it is different, there is a header for everyone, and the content is compressed.\nIf I compare : \n- https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L725\n- https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L702\nThere is no http status code, perhaps something like that should help : \npython\n    return Response(response=\"\"\"User-agent: *\nAllow: /\nAllow: /about\nDisallow: /stats\nDisallow: /preferences\n\"\"\", status=200, mimetype='text/plain')\nIt's only a guess.\nEDIT : see http://flask.pocoo.org/docs/0.10/api/#response-objects , perhaps try make_response\n. @pointhi > if possible, can you share your apache configuration ? \n. In fact, it's a whole configuration that should be analyse (an which module are enabled) : could be http://httpd.apache.org/docs/2.2/mod/mod_mime.html#removetype for instance\n. How do you want to include searx ? with a iframe as you descibe it ? another solution could use the JSON API (even if there is some missing information like the info boxes or the right side).\nI think the IFrame solution is possible but \n- I don't know why the search result replace the portal page (this is a simple HTML form without target or something else).\n- the theme has to be modify to be nicely include into a portal (source code of the oscar theme : https://github.com/asciimoo/searx/tree/master/searx/templates/oscar ). Perhaps create a new theme ?\nCan you post some code to test what you descibe ?\n. Note : the Search Bar, and Context Search is native as soon as you choose searx as your default search engine. Address Bar Search works without the suggestions.\nAbout \"Instant Answers Everywhere\", is it something everyone want ? \nDon't get me wrong, I'm not saying that we should do an FF / Chrome extension.\nI just saw this extension : https://addons.mozilla.org/en-Us/firefox/addon/searx-me/\nThe duckduckgo source code : https://github.com/duckduckgo/firefox-zeroclickinfo\n(I was thinking about an Android version too : https://github.com/duckduckgo/android)\n. Seems doable : \nhttps://api.qwant.com/api/search/web?count=10&offset=0&f=&locale=fr_fr&q=qwant\nthe response is in JSON format.\nReplace web by knowledge, social, news, suggest, smart_top (no count or offset for this one).\n. Done\n. [noise]\nThe \"new\" meta are : \n- microdata\n- open graph (used by FB and Google)\n[/noise]\n. There is no issue, only a timeout : on searx.me, the average response time is more than 5 seconds. \nI suggest to disable the engine by default, and set a default timeout to 6 seconds. \nIf someone want to use it, it works, and by default the speed is guaranty ( = startpage engine disabled).\n. Fixed in commit 484d0974f78df54a5173b14f1f1e3bc089a640fc\n. I can't access .se web site. Same for http://downforeveryoneorjustme.com/thepiratebay.se\n. I'm not sure if the API v2 is still working (it is deprecated since last year).\nAnd the v3 API requires a key.\nI suggest the same implementation than the flickr engine : \n- an engine based on scraping ( see https://www.youtube.com/results?search_query=test without javascript)\n- an engine based on the API v3 if we have time to write it.\n. Good point.\nTwo solutions :\n- make the request, in case of failure, switch to another domain. The user doesn't have result for one request.\n- It requires to test the different domains, and choose the right one in background.\nThe second solution can be done at the load time, but if the searx instance has a big uptime, the test has to done again, and there is nothing if the searx core to do that easily ( yes a timer inside the engine can be a solution, but if each engine starts to do the same it will be a mess ? )\n. The thepiratebay.am is registered to Fredrik Neij (co-founder of TPB).\nThe registar is http://www.1abcdomain.com/\nThe resolved IP is  104.18.48.8 which belong to cloudflare.\nNot so much information...\n. 6 sec is based on the average response time I saw on searx.me. It's average so there is slower time!  \nIf the engine is disabled by default, I think the use case is to enable it using a bang, and in this case, 6 sec is long but it is better than no result. I forgot the other use case : enable the engine.\nSome option about the time out are perhaps a solution, but this PR is quick & dirty fix that resolved the global response time, and try to make those two engines usable. Purpose : release a 0.8.0 version without referenced bugs.\n. Ok, so let's switch back to 3 seconds ?\n. Another idea : why set a time out when there is only one engine ? If the engine take time to respond, then searx also, but since there is nothing else to show, it's not issue.\nSo why not change the time out behaviour : \n- if there is only one engine : set the time out to 60 seconds (yes, because there is a connection pool, so if the user send a lot of requests they will have an end )\n- if there is more than one engine : normal time out behaviour.\n. see #343\n. done with commit 94cb3a7f11d252dc8dabd6bce4d7f4d67a1ececd\n. Use case : \n- first search using a bang --> searx doesn't display the excepting result.\n- second search using a category --> it is \"natural\" to click on the category button.\n. I found this one : https://github.com/webworka/Tagedit\nOne stackoverflow : http://stackoverflow.com/questions/519107/jquery-autocomplete-tagging-plug-in-like-stackoverflows-input-tags\n. @pointhi +1 for the build id (if automatically generated).\n@Cqoicebordel : yes no hurry, like I said, it's only to discuss about it. Anyway, there will be other release (I hope), so no stress. Also, nice idea about the FAQ :)\n. A little experiment : https://gist.github.com/dalf/9e9a6d519350180c4b69\n. Close ?\n(and / or new release since the 0.8.1, the google and bing image have been fixed ?)\n. More notes : \n- the \"language support\" is not shown : it is always True\n- the result count is not shown : wikidata and duckduckgo_definition has zero results but add infoboxes : it can be misleading for the users.\n- the \"Allow / Deny\" button is on left part (on ltr languages) for small screen : there is a horizontal scrolling, but the most important informations are show : allow / deny and engine names. A better solution could be : https://css-tricks.com/responsive-data-tables/\n. Perhaps what we miss here it is inheritance using Class and Object.\nIt could help to have common code at one place.\nFor now, we have two options :\n- With one python module per category it is easy to understand the query and the schema of the result.\n- With one python module for all categories, the copy / paste is avoided, but we have to carefully read the code to understand the schema of the result for each category.\n. I forgot : my comment was not about why we should block the merge of this PR. Only a brainstorming that should be another \"issue\". @asciimoo : merge ?\n. Thank's !\nLine 161 there is a print\n. Would it be ok without the map links ?\nIn this case, there is no merging of results issue.\n. Question (already implemented, but easy to remove) : use the geo URI, nothing is display, it's only a link.\n. I agree\n. No problem, next branches will be created on my repo, sorry for the annoyance\n. Try to start searx with debug: True in settings.yml and see the logs.\nWithout error logs it is difficult to understand what is happening.\n(duckduckgo definition doesn't seem to be block)\n. privatesearch.io seems to work again, can the issue be closed ?\n. One way, it's to have an Class like in this project :  https://github.com/gruns/furl/\n. Thank's !\n. H\u00e9 cool, but is it more a help / documentation than a FAQ ?\nMore over, there is only a version for oscar theme, not for the other themes. How does it behave with the others themes ?\nAnyway, thank's !\n. With the title attribute ? \n- http://www.w3schools.com/tags/tryit.asp?filename=tryhtml5_global_title\n- http://www.w3schools.com/tags/att_global_title.asp\n. Thank's. I don't know if the code souldn't be merged into self_ip.py (and renamed self_ip.py to self.py or something like that) ?\nFrom the user point of view, these features are about displaying information about the browser.\n@asciimoo what do you think ?\n. For now, searx doesn't parse the request (except the bangs and language selection). In others words, if google allows a such thing, it is possible to have the same result in searx.\nBut, regarding the !site: it is possible to something since searx can filter the result according this filter. A naive implementation would possibly give zero result on the first page but some on the next page. \n. +1\n. This option : \n\nIt's about quick, but also to make it obvious : since it's part of the search parameters, IMHO it should be display.\n. Off topic : we could even replace part of preferences page by \"Save as default preference\" button on the main page. This button could save the language / region / defaults categories.\n. implemented with search_on_category_select plugin : \nhttps://github.com/asciimoo/searx/blob/master/searx/static/plugins/js/search_on_category_select.js. I can't reproduce the bug...\nBut when I wrote this issue, searx.me, my instance, and a local instance had the problem.\nClosing for now...\n. Technical note : \n- a way in searx to specify the date criteria (from the user point of view and from the code point of view)\n- when a date criteria is asked, searx must not use engines that don't support this criteria\n- implements for different search engine this criteria\n. Whaou, nice !\nSome notes : \n- perhaps add sympy in requirements.txt\n- chrome has removed the mathml support, one workaround : https://github.com/mathjax/MathJax\n- (10*1.1) eur in usd works in google (I know that's suppose to include in sympy in an engine) \nWe need a way to have engine without HTTP request (perhaps sympy parsing is done after all requests and slow down the global response time).\n. Another thing : cos(4*x)+sin(2*x)=1 has more than one results, and the global width is too large (the results are hidden by the suggestions).\nPerhaps add a line break between the results ?\n. One more thing : there is no timeout\na equation like cos(x)^2+sin(2*x+pi)=tan(1) can easily create a DOS.\n. The date of birth comes from wikidata and there is a bug when the wikidata date contains only a year.\n. For information, it's wikidata not wikipedia : \nhttps://www.wikidata.org/wiki/Q403873\nThe lines of code that should be fixed (starting from, including the get_time function) : \nhttps://github.com/asciimoo/searx/blob/master/searx/engines/wikidata.py#L167 \n. See Pull Request #6 : that's suppose to record the links that are clicked by the user.\nFor now searx is stateless, with the seeks protocol support this won't be the case any more.\nAnother thing : the seeks protocol has to be review to make sure it is compatible with searx spirit. \nOne way to make everyone happy : implement everything as a plugin (but this required to change / improve the plugin system).\n. Idea : have two servers, one stateless, another one statefull.\nHow to do that ? \nAdd autonomous server / part : \n- which does the p2p communication\n- which can behave as a search engine\n- which can record user choice for a request (not the web part, only an API)\nImprove the plugin system to be able to add some public URL : this would allow to record the URL the user has clicked. Internally, the choice would be recorded using the API on the p2p server.\nThe protocol between the web side and the p2p server has to be choosen (HTTP or protocol buffer, or ?)\nSide effect : technically, we could have many web instance for one p2p server. Not sure this is the purpose.\nStart of the out of topic thoughts : the more time I spend time thinking about searx, the more I believe that we should have one process and different threads instead of multiple process which are independant.\nExamples of use : \n- sharing the statistics : it can be done with Redis / an ORM, but easier if it is in memory. The project is not storing gigabytes of statistics, only few numbers.\n- sharing HTTP(s) connection to other server.\n- In my opinion, we should have something that dispatch all the external requests taking them as one flow. For example : the PR that does asynchronous parsing of result create multiple thread, and this happen for each process, in fact this doesn't scale at all. Either we use a framework that doesn't need one thread per HTTP requests, either we have one unique pool of threads for the whole searx server.\nThe oposite idea is to have multiple independant searx communicating each other, even on one physical server. In this case the p2p / could is native. But I think it is overcomplicated for simple needs. \nI don't know enough python, but we should give a try to Twisted ?\nEnd of out of topic \n. Nice.\nThe searx/tests/engines/test_btdigg.py has to change.\nExample, line 26 (same line 104 and 370) : \nresponse = mock.Mock(text='<html></html>') \nshould be changed to \nresponse = mock.Mock(content=bytearray('<html></html>'))\n(not tested)\nAlso, according to travis, there is an issue with the selenium tests, I really don't know why.\n. Can you use Apache or Nginx as a reverse proxy ?\nThis would solve the issue.\n. Easy part, make sure that \n- searx log (uwsgi) are disabled even in case of error \n- apache or nginx as reverse proxy : anonymize IP.\nFor this part you have to rely / trust what the admins say or skills (they can think that everything is ok and forget something).\nNote : even this is unlikely to happen, the server can be configured to anonymize IP, and a mistake can \"restore\" real IP (upgrade or something like that).\nAnother point : internet provider can log IP (according to the laws of country). It can be your ISP or the server ISP.\nSide note : make sure that all requests from the searx instance are using https connection. Otherwise, someone can eavesdrop the outgoing requests. Same : you have to trust that admins have updated their searx instances.\n. the project page doesn't exist anymore\n. +1\n. one step for #265 (split core and ui)\n. The data seems to come from the wikidata engine. \n. close ?\n. I'm not sure about the score stats.\nAlso I saw a small issue with request time measure per engine : it's include the call to the request fonction (since it's based on params['started'] ). \nPerhaps the per engine measure should also include the time out / error requests, after all if one engine has 20% of timeout each time the user has too wait for that.\nBut here there is an issue : how to get the params['started'] in the function wrapper (the one with the try / catch ).\nAfter bring back the graph is not too difficult, nearly done actually except I would like to double check the consistency.\n@pointhi : I don't know this projects why not. The only : measure response time mustn't be slow.\n. In hurry, I mess up : I pushed the metrology2 branch to asciimoo/searx instead of dalf/searx and added my settings.yml...\nBy the way, there are the graphs in /stats2\nWarning : all measures have to be checked, double checked, there is always tricky parts...\n. Thank's !\nyoooooyoooooyooooo :-)\n. COPY requirements.txt ./requirements.txt is working whatever the used FS, so why not.\n@asciimoo  what do you think : merge ?\n. I think its a duplicate of #400 \n. Whaa,  nice!!! \nI've added some comments to the code.  My big concern is about the fact that there is one thread :-)\nI hadn't  think about it before, but with pycurl and your PR :\n- all requests are done in one thread : nice\n- all callbacks  are also done on one thread : less nice  since some of them take time \n. ## micro benchmark requests vs pycurl (libraries)\nwith an local http server\nI've forked and updated an existing repository to micro-benchmark pycurl and requests  ( [edit] the tests in this message have done with this version : https://github.com/dalf/python-client-benchmarks/tree/26b5c69857653db1d3487ab5ca2398139113555e )\nOne result for 10000 requests for each tests :\n- pycurl, saving in a new buffer, connection reuse : 15.6638448238 seconds\n- requests, connection reuse : 38.2421951294 seconds\n- requests, no connection reuse : 44.2702360153 seconds\n- pycurl, saving in a new buffer, no connection reuse : 66.603479147 seconds\n- pycurl, saving in the same buffer, connection reuse : 21.8963730335 seconds\n- pycurl, no saving, connection reuse : 18.0446221828 seconds\nNotes : \n- using the same buffer is slower than using creating a new one. Not sure if it is a real thing or a benchmark issue\n- \"pycurl connection reuse\" means reuse the Curl instance.\n- pycurl is slower then requests when the connection is not reused.\n- it's only an HTTP \"benchmark\", not an HTTPS benchmark.\nhttps !\na more realistic test must do HTTPS connections since most of engines used HTTPS connections. \nResults of an ugly HTTPS test on amazon.com with 100 requests : the first 2 tests are both around 30 seconds (a little more for requests). \nOne nice way : docker container with nginx + ssl serving a static file. An self signed certificate can be created.\nbenchmark of the pycurl branch\nUse ab  -c 1 -n 100 http://127.0.0.1:8888/?q=grouik\nI give the results but they are not relevants, may be some search engines have given captcha or errors with 200 requests in a few minutes... \n@asciimoo  : may be from searx.me host, you could do the test since there is a set of different IP.\nwith the pycurl branch\nPercentage of the requests served within a certain time (ms)\n  50%   1652\n  66%   1734\n  75%   1778\n  80%   1813\n  90%   1892\n  95%   1997\n  98%   2259\n  99%   2259\n 100%   2259 (longest request)\nwith the master branch\nPercentage of the requests served within a certain time (ms)\n  50%   1263\n  66%   1292\n  75%   1308\n  80%   1320\n  90%   1417\n  95%   2001\n  98%   2045\n  99%   2045\n 100%   2045 (longest request)\n. A possible architecture evolution : \n- everything in one process (uwsgi would be here only to recreate the process)\n- a thread pool to run CPU task (typically one thread per CPU core / hyperthread)\n- a task queue to submit task to the thread pool \n- a flask interface with multiple threads to parse user requests, do the network requests, as the results arrive, use the thread pool to parse them.  \nBut there are still a flaws :\n- some engine do secondary network requests, that's mean that the thread pool dedicate to CPU tasks will still do network requests.\n- in the future, some engine won't need to send network requests (calculator for example)\n. I have create a NGINX docker image to create some more realistic tests.\nSo to sum up for 10000 requests (time express in second) and response of 20480 bytes : \n|  | HTTP (Flask), total time | HTTP (NGINX), total time | HTTPS (NGINX), total time | HTTPS (NGINX), cpu time |\n| --- | --- | --- | --- | --- |\n| pycurl, saving in a new buffer, connection reuse | 15.7 | 4.1 | 12.8 | 4.9 |\n| requests, connection reuse | 38.2 | 23.18 | 30.3 | 27.0 |\n| requests, no connection reuse | 44.3 | 39.7 | 122.8 | 63.4 |\n| pycurl, saving in a new buffer, no connection reuse | 66.7 | 56.4 | 130.8 | 32.3 |\n| pycurl, saving in the same buffer, connection reuse | 21.9 | 4.5 | 13.7 | 4.8 |\n| pycurl, no saving, connection reuse | 18.0 | 4.0 | 13.0 | 4.5 |\nNote : The \"no connection reuse\" speed tests are limited by my laptop CPU (i3-2367M, 1.4Ghz)\nWith the HTTPS protocol, for each request, when the connection is reused, the pycurl overhead time is 1.2ms, and the requests overhead time is 3ms : requests is twice as slow or same thing, requests uses 2ms more than pycurl (compare to the network time it's nothing).  \nFor a full HTTPS connection without reusing an existing one, requests seems to be a little faster but it uses twice more CPU (difference of less than 4ms CPU time per request). \n. Another point about pycurl vs requests : certificates trust store. \nrequests uses certifi, but is considering the use of the system trust stores by default in 3.0.0\npycurl don't use certifi by default.The Curl documentation explains how to use the Firefox certificates. There is stackoverflow thread about pycurl and SSL\nHonestly I don't know what is the best solution, perhaps the one which use the Firefox store ?\n. It seems that the connections are no reused.\nI've made some tests with this version : \n``` python\ndef show_debug(debug_type, debug_msg):\n    if debug_type not in [3, 5, 6]:\n        logger.debug(\"debug(%d): %s\" % (debug_type, debug_msg.strip()))\ndef get_new_connection(source_address=None):\n    global curl_share\n# pycurl initialization                                                                                                                           \nh = pycurl.Curl()\nh.setopt(pycurl.VERBOSE, 1)\nh.setopt(pycurl.DEBUGFUNCTION, show_debug)\nh.setopt(pycurl.SHARE, curl_share)\nh.setopt(pycurl.ACCEPT_ENCODING, 'deflate, gzip')\n\n# Follow redirect and send referer                                                                                                                \nh.setopt(h.FOLLOWLOCATION, True)\nh.setopt(h.AUTOREFERER, True)\nh.setopt(h.MAXREDIRS, 5)\n\n# certifi                                                                                                                                         \nh.setopt(h.CAINFO, certifi.where())\n\n# consistently use ipv4                                                                                                                           \nh.setopt(h.IPRESOLVE, pycurl.IPRESOLVE_V4)\n\nif source_address:\n    h.setopt(h.INTERFACE, source_address)\n\nreturn h\n\n```\nOn second try : \n2016-02-08 00:41:11.142 debug(0): Hostname was found in DNS cache\n2016-02-08 00:41:11.143 debug(0): Trying 198.35.26.96...\n2016-02-08 00:41:11.357 debug(0): Connected to fr.wikipedia.org (198.35.26.96) port 443 (#1)\nThere is about 200ms lattency which isn't normal is the connection is reused.\n. The comment about the commit 34afcf254193498e3c0d1411d84e895efb1ebc4b doesn't completely solved the issue about slower response time with pycurl.\nUsing the response time of the metrology2 branch, I see that slower engine in pycurl are also the engines that have an important response time in the response function.\nSo the issue seems to be related to parsing of the result.\nSide note (but it seems to have no effect in response time...) : the wikidata engine in response function sends a HTTP request using requests. In the case of the pycurl branch, the HTTPS connection can't be reused.\n[Edit] I forget, I have try cProfile and pycallgraph : it's not easy to use that since \n- it's involved network, so multiple measure must be done\n- pycurl is C module which isn't catched by cProfile\nSecond note : DNS request are synchronised, see https://curl.haxx.se/dev/internals.html#multi\n. The +200ms exists for google engine even if it is the only engine called. As least the difference seems to be consistent.\nFrom that, I have added logs to the send_requests function, and I noticed few points :\n- the self._curl_multi_handler.select(...) function returns many times for nothing (no result to parse)\n- the delay seems to exists when CURL get the HTTP response even with multiple tries. So this is the opposite of my first hypothesis, still I have to investigate. Perhaps give a try with wireshark. \n(Is there a simple way to have a colored and millisecond timestamp logs in the console ? I end up with a messy code wrapping webapp.py... and I have still have duplicate logs) \n. After adding debug a little everywhere in curladapter.py, it seems that there is about  200ms delay when the response is get the network.\n```\n6410 (0): Found bundle for host www.google.co.th: 0x7fe40c00a430\n6411 (0): Re-using existing connection! (#0) with host www.google.co.th\n6411 (0): Connected to www.google.co.th (61.91.17.168) port 443 (#0)\n6412 (2): GET /search?q=test&start=0&gbv=1&gws_rd=ssl HTTP/1.1\nHost: www.google.co.th\nAccept-Encoding: deflate, gzip\nConnection: keep-alive\nAccept-Language: th,th-th\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:36.0) Gecko/20100101 Firefox/36.0\n6412 (9): --select exits for nothing0\n6512 (9): --select exits for nothing0\n6613 (9): --select exits for nothing0\n6693 (1): HTTP/1.1 200 OK\n6694 (1): Date: Sun, 21 Feb 2016 09:37:56 GMT\n6694 (1): Expires: -1\n6694 (1): Cache-Control: private, max-age=0\n6694 (1): Content-Type: text/html; charset=UTF-8\n6694 (1): P3P: CP=\"This is not a P3P policy! See https://www.google.com/support/accounts/answer/151657?hl=en for more info.\"\n6694 (1): Content-Encoding: gzip\n6695 (0): Server gws is not blacklisted\n6695 (1): Server: gws\n6695 (1): X-XSS-Protection: 1; mode=block\n6695 (1): X-Frame-Options: SAMEORIGIN\n6695 (1): Set-Cookie: NID=76=C3EdCg9TWUmZuZKaVzD2Vu4ge9zIsICyCY6bZsPYHv06YrDa6FNdsTheeTuhfanuQcNxduuQnxqNONh8RYWlsVD-JYZB9HpJLYPIFQE1PYbAVZUGzdDWy1PLObpb38Vq; expires=Mon, 22-Aug-2016 09:37:56 GMT; path=/; domain=.google.co.th; HttpOnly\n6695 (1): Transfer-Encoding: chunked\n6695 (1): \n6696 (9): --select exits for nothing, StringIO buffer size= 835\n6748 (9): --select exits for nothing, StringIO buffer size= 5936\n6748 (9): --select exits for nothing, StringIO buffer size= 14092\n6748 (9): --select exits for nothing, StringIO buffer size= 19233\n6749 (9): --select exits for nothing, StringIO buffer size= 19233\n6783 (9): --select exits for nothing, StringIO buffer size= 24899\n6783 (9): --select exits for nothing, StringIO buffer size= 27720\n6784 (9): --select exits for nothing, StringIO buffer size= 30827\n6784 (9): --select exits for nothing, StringIO buffer size= 34205\n6784 (9): --select exits for nothing, StringIO buffer size= 34205\n6785 (9): --select exits for nothing, StringIO buffer size= 37639\n6785 (9): --select exits for nothing, StringIO buffer size= 41419\n6786 (9): --select exits for nothing, StringIO buffer size= 43731\n6786 (9): --select exits for nothing, StringIO buffer size= 48713\n6786 (9): --select exits for nothing, StringIO buffer size= 48713\n6786 (9): --select exits for nothing, StringIO buffer size= 54158\n6787 (9): --select exits for nothing, StringIO buffer size= 61806\n6787 (9): --select exits for nothing, StringIO buffer size= 69468\n6787 (9): --select exits for nothing, StringIO buffer size= 70845\n6787 (0): Connection #0 to host www.google.co.th left intact\n6787 (9): finish call (create thread)\n6788 (9): finish called (thread started)\n6788 (9): self.response = self._extract_response()\n6788 (9): return self.callback(self.response)\n```\nThe first number is a timestamp in millisecond\n. with this version https://gist.github.com/dalf/d179276d73de9dd9bf1d : \n- the response time for one engine is the same than master branch\n- the response time for three or more engines still have a problem\nI've used https://mitmproxy.org/ to get the response time (far more easier to understand than wireshark).\n[edit] the pycurl / requests benchmark has updated : https://github.com/svanoort/python-client-benchmarks \n[edit] with pycurl, searx can moved to python 3 \n. Is it related to the fact that alpine contains x86 binary, and the Cubietruck has an ARM cpu ?\nIs there an official Alpine docker image for ARM ?\nI found this one : https://hub.docker.com/r/dduportal/rpi-alpine/ which seems to be build for Raspberry Pi.\n@saljut7 : if you want to try this image, replace this first line of Dockerfile by\nFROM dduportal/rpi-alpine\nReally not sure if it will work or not...\n[Edit] I found this project : https://github.com/multiarch/alpine\n. Related to docker.\nReopen if you need.. Closed. See PR #751 \n. Just as a reminder, some engines make HTTP request, so the exit Tor node can see the request in clear (if the engine is used directly or in a category).\nThe engines that make HTTP request at first sight : \n- faroo\n- filecrop\n- general-file\n- nyaa\n- subtitleseeker\n. The last version of requests adds the support of SOCK5 proxy (version 2.10.0) : \nhttp://docs.python-requests.org/en/master/user/advanced/#socks\nrequirements.txt needs to be modified to use the right requests version and to add requests[socks]\nA hack about HTTP requests: send HTTP requests to a dummy / non existant proxy.\n. I agree with this idea.\nIt doesn't solve everything but it's a start, when there is an exception with the HTTP request (time out for example), the engine is suspended for a time : \nhttps://github.com/asciimoo/searx/blob/master/searx/search.py#L48\nI don't know how duckduckgo blocks the results ? is there a redirection to a specific page ? a time out ?\nAbout google, an exception is triggered when google redirect to sorry.google.com : \nhttps://github.com/asciimoo/searx/blob/master/searx/engines/google.py#L194\nThe exception just crash the engine thread nothing more. Different way to extend : \n- throw a specific exception\n- provide an API to the engine to suspend requests\nAnother solution : limit the number of requests per hour and per IPs to prevent blocking.\n. It's working for me :\nsh\ndocker build -t dalf/searx .\ndocker run -d --name searx -p 80:8888 dalf/searx\nAbout your logs : \nsed: searx.setting.yml: No such file or directory : was the Docker image built without error ?\nsocket.error: [Errno 13] Permission denied : may be related to be previous error, missing settings.yml\nIf I do docker run -t -i dalf/searx ls searx, the results is \nsh\n__init__.py         plugins             search.py           testing.py\nautocomplete.py     poolrequests.py     settings.yml        translations\ndata                preferences.py      settings_robot.yml  utils.py\nengines             query.py            static              version.py\nlanguages.py        results.py          templates           webapp.py\nI can see settings.yml, what about your Docker image ?\nYou can try docker run -t -i dalf/searx cat searx/settings.yml to make sure your modifications have been taken into account.\n. For some reason there is no HTTP header in the response, and the content seems to be keep gziped with firefox (Chrome or curl seems to handle that).\nSeems to be related to https://github.com/asciimoo/searx/issues/209\nTry this in the apache configuration : \n<FilesMatch \\.xml$>\n  SetEnv no-gzip 1\n  </FilesMatch>\n. I confirm : resolved.\nhttps://github.com/asciimoo/searx/blob/master/searx/translations/fr/LC_MESSAGES/messages.po#L679\n. Sorry I wake up late: the duckduckgo definition engine can produce results in other languages, at least French (using the kl argument if I remember correctly).\nBut there is an problem : the current implementation uses an id which is the the English wikipedia link (see also https://github.com/asciimoo/searx/blob/master/searx/engines/wikidata.py#L191 ). So if the duckduckgo definition engine returns no English results, how to merge the infoboxes ?\n. @asciimoo : How do you want to make it javascript free ? It's require to download the metainformation for each videos which will slow down the response time a lot each, even with parallel downloads.\nPerhaps you ask for /video_links to returns HTML instead of JSON ? But that's Javascript that will load the content anyway (except if IFrame are used ?)\nOr perhaps I misunderstood your comment.\nNote : the PR can't be merged because of oscar.min.css, searx.min.js files.\nAnother PR can be created from https://patch-diff.githubusercontent.com/raw/asciimoo/searx/pull/552.diff\n. In the current implementation, the video is not downloaded from searx, youtube-dl gets the URLs nothing more  ( https://github.com/asciimoo/searx/pull/552/files#diff-36c9ae084b973041809e31fa6cc693d1R82 ).\nProxy the download could be great, but is flask the best framework to do that ? One download will block the one process for a very long time. Twisted ? Go ? I think it's related to the resize image proxy.\n. I think it's return a list of URLs / videos : one URL for each different format.\n. I think the format can be different each time / search. Sometimes to get the HD version, sometimes to get only the music, sometimes to get the low quality version. \nWouldn't be easier to have a global option in advanced settings, with a default setting from the cookies / preferences? But even in this case how to deal with videos that are not encoded in the preferred format? \n. +1\n. Actually this is only an information that's why the boxes are read only.\nWhen the engine support safe search, the box is checked otherwise the box is left unchecked.\n. Very nice, should it be another theme or update the current one ? @asciimoo , @pointhi  ?\nSome comments : \n- there are many modified mod bits from 100644 to 100755\n- the use of fonts.googleapis.com and maxcdn.bootstrapcdn.com create a privacy issue. Note : fonts.googleapis.com serves different content according to the user-agent, see http://stackoverflow.com/questions/25011533/google-font-api-uses-browser-detection-how-to-get-all-font-variations-for-font\n- the PR seems to be based on an old version of the code. For instance, the commit 8d335dbdaedd6113242e785e8fabac86128d069a modifies searx/templates/oscar/infobox.html to add the bdi HTML element, but this PR removes that element. Same the commit d27f7a1b071b4796e4716951a8e8e62a11dc99c3 seems to be cancelled by this PR. The error described by @a01200356 may be related to this.\n. Thanks a lot !\nI've created a wiki page about the properties that can extracted : https://github.com/asciimoo/searx/wiki/wikidata\n. PR has been merged\n. According to this line : \nGET /sorry/IndexRedirect....\ngoogle is sending a captcha.\n. Confirm with google image.\n. Categories are sorted by alphabetic order except for the general which is the first one.\nThis order can't be modified.\nSee : https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L268\nRelated to https://github.com/asciimoo/searx/issues/258\nIt should be configured by settings.yml IHMO.\n. It's require Python 3 and Chrome Browser !\nFor Python 3, searx will most probably need to switch to this version one day or another, but Chrome is another beast.\nFrom the documentation :\n\nI am pretty sure that it must be possible to handle 20 such browser sessions in a parallel way without stressing resources too much.  \n\nIt will require far memory and CPU ressource than the current implementation, and at the same time :\n\nAlone the dynamic nature of Javascript makes it impossible to scrape undetected.\n\nBut for sure it will solve the issues you have referenced in many more cases than the current implementation.\nAbout proxies, this bring this idea for searx : to be able to use different proxies as it is possible to use different IPs.\nThe documentation / code about google dorks is interesting. This file too.\nThe requests are prepared here : \nhttps://github.com/l2inc/GoogleScraper/blob/master/GoogleScraper/http_mode.py\nand\nhttps://github.com/l2inc/GoogleScraper/blob/master/GoogleScraper/selenium_mode.py\nThe response are parsed here : \nhttps://github.com/l2inc/GoogleScraper/blob/master/GoogleScraper/parsing.py\n. The results are merged using this implementation : \nhttps://github.com/asciimoo/searx/blob/master/searx/results.py#L145 \nThen the results are scored using this implementation : \nhttps://github.com/asciimoo/searx/blob/master/searx/results.py#L196\nI don't think there is documentation for now.\n. great! it can be used by stats.searx.oe5tpo.com or by the version I've started to write in Python : https://github.com/dalf/searx-stats2\n(help is welcome) \nI don't know if there is way to add a hash of the source code of each engine. \nSome instances follow the master branch, some others follow the release I guess. That's why the Searx version number is not enough in my opinion. \n. The problem occurs at this line : \nhttps://github.com/asciimoo/searx/blob/master/searx/engines/soundcloud.py#L61\nThe call is from this line : \nhttps://github.com/asciimoo/searx/blob/master/searx/engines/init.py#L53\nBasically a try / catch should be used each time a engine module is loaded.\n. https://github.com/asciimoo/searx/blob/master/searx/settings.yml#L32\nThe requests are sent to the engines using a pool of IP instead of always the same one. For a pool of two IPs : the first IP for the first outgoing request (let's say google), the second IP for the second outgoing (duckduckgo for example) request, the first IP for the third outgoing request (wikipedia for example) etc... \nSince many engine blocks the request based on the IP, it's help.\nsee https://github.com/asciimoo/searx/blob/master/searx/poolrequests.py#L46\n[EDIT] clarification\n. When there is more than one official website, there is a bug : there is one button \"official website\" with the contacted URLs.\nExample : World of warcraft.\nA different point : The links to YouTube, Twitter etc are not implemented, is it because these websites may / doesn't not respected privacy ? ( P2002,  P2013,   P2037,  P2003, ...)\n. :-) \nFew more notes / requests : \n- the JSON format is not used so %7Cwikitext can be removed from url_detail\n- the P18, P154 properties give a reference to an image.\nExample, the wikidata page about Linux contains :\nhttps://commons.wikimedia.org/wiki/File:Tux.svg\nthe image link (png or jpg) is : \nhttps://commons.wikimedia.org/wiki/Special:FilePath/Tux.svg?width=200\nwidth can be 200, 300, 500...\n. The more I spend time to understand / test the wikidata properties, the more I find...\n@asciimoo may be merged and update after ?\nNotes on current PR : \n- P2046 (area) may contains a list (for Paris for example). I guess it can be the case for other properties. Not a big deal I guess.\n- for the image perhaps get the logo and fall back to the image ( ['P154', 'P18', 'P242', 'P41', 'P2716', 'P2910'] ). For Ubuntu it seems more relevant for example. BTW the wikipedia engine replaces the image, and perhaps there is other example where the current implementation is better...\nNew things that could be nice (another PR ?) :\n- the property P402 gives a number, which can be used to link directly to OSM this way : https://www.openstreetmap.org/relation/{number}. When this property is set, it can replace the OSM link where the zoom is most of time not set correctly.\n- always about geocoordinate : P1332, P1333, P1334 and P1335 can be used to define the OSM link.\n- the P242 (locator map image) or P15 (route map) give a link to a map. Useful for to have all the stops of a metro line (see example for P15 and example for P242 ). A link to this image could be nice, and second picture that the user can click would be great.\n. But, here, the URLs are not merged using this code : there are inside one infobox.\n. Is it still valid ?. Great ! In the result page, would it be more clear if the categories and time range are displayed / expanded ?\n[EDIT] Suggestion : Time range as a combo box which takes far less space. So the safe search, the search language / locale could all fit in the screen (to have a live example : duckduckgo does that).\n. Yes, on the result page I think the categories / time range should be always displayed. I think the UX / UI should give a quick answer without click to the question \"there are some results, but what was the request ?\". By request I mean everything (text, time range, category, ... and language but that's another story)\nIt's not that the user will forgot what he has just entered, but he can adjust right away, leave the page and come back after, etc...\n. done. See https://asciimoo.github.io/searx/blog/python3.html. I haven't checked in details, but I think that in the current implementation if the safe search setting is \"moderate\" or \"strict\", the search is still done through all engines even engines that doesn't support safe search.\nSo, that's mean that if the safe search is not implemented for an engine and that engine can provide unsafe contents, the \"safe search\" doesn't work from the user point of view. \nSee : https://github.com/asciimoo/searx/blob/master/searx/search.py#L295 \nthere is no \"skip the engine if it doesn't support safe search and if the user ask for a safe search\"\nA way to improve that : \n- skip the engine that doesn't support safe-search as it is done in the time range PR : https://github.com/asciimoo/searx/pull/634/files#diff-35de57dede17c7660869ae806bbc99b6R283\n- Some search engines will not provide unsafe content for sure (photon, osm, wolframalpha engine are in this category). The implementation can say that \"safe search\" is supported, or make a distinction with a special value (False, True, \"SAFE\" or \"NA\" for example).\n. :-)\nI don't know how to keep the xpath engine to add information about magnet, upload, download.\nThat's really help to choose the right torrent. \n. What do you mean to support TOR ?\nSearx is a proxy between your computer and different search engines.\n- TOR can be used from Searx to the search engines (actually not possible now, but the implementation is possible, see https://github.com/asciimoo/searx/issues/519 )\n- TOR can be used from your computer to the searx instance (hidden service / .onion). In this case have a look to https://github.com/asciimoo/searx/wiki/Searx-instances . For example you can access searx.me using this link http://ulrn6sryqaifefld.onion/\n. After a quick overview, all websites are mirror of KAT just before everything stops.\nThe register link goes to ads.\nI wouldn't trust these mirrors.\n. Do you auto complete feature enabled? If you type and quickly press enter, the response from the auto complete engine doesn't reach tbe browser (the browser closes the connection). In this scenario, you see this kind of error. \n. https://searx.me/?q=Go+vs+python&category_general=on&time_range=\nThe layout has an problem with this link :\nhttps://www.reddit.com/r/golang/comments/2aup1g/why_are_people_ditching_python_for_go/\nThe pretty url isn't cut.\n. - [ ] https://github.com/asciimoo/searx/issues/92 ? Google news engine doesn't work.\n- [x] https://github.com/asciimoo/searx/issues/571 ? a quick fix is to disable paging support of the google image engine.\n- [ ] https://github.com/asciimoo/searx/issues/636 ? The current implementation of the safe search doesn't make sense : safe search is activated for the engines that support it, but other unsafe engines are used too.\n. Another point : does the current requests version support socks proxy (I'm thinking about Tor support) ?\nShould \nrequests = 2.10.0\nbe replace by\nrequests[socks] = 2.10.0 \n(not sure about the syntax, pip freeze doesn't show socks dependencie)\nPerhaps, Tor support requires more work (enable or not the proxy for each engine for example ?), but this is the first step.\n. for requests I would suggest to require at the version 2.10.0 (last version 2.11.1), to support socks proxy (require for Tor).\n. It seems that the NixOS philosophy is to manage all dependencies with its own package manager  : \nhttps://nixos.org/wiki/Python\nBut I see project like this one : \nhttps://github.com/ktosiek/pip2nix\nI guess @Profpatsch can say more about it ?\n. I think a lot people installs searx dependencies in a virtualenv. This is the documented way to install it.\nBut certifi could be would without a specified version, to make sure to install the last one, since it's involved security.\n. Good point, the answer should been given only on the first page.\nSo I guess ctx['search'].pageno should be tested here https://github.com/asciimoo/searx/blob/master/searx/plugins/self_info.py#L32\n. The ddd engine was activated. With !wikidata !wikipedia !ddd bubalus (language set to fr_FR) I can reproduce the bug :\n\nThe first infobox (from ddd) has a link to https://en.wikipedia.org/wiki/Bubalus \nThe second infobox has links to : \n- https://fr.wikipedia.org/wiki/Bubalus\n- https://en.wikipedia.org/wiki/Bubalus\n- https://www.wikidata.org/wiki/Q83521?uselang=fr\n. why not use gettext ?\nif there is no language cookie, the error is in English as now, otherwise it is translated.\n. Python seems to support punycode natively : https://docs.python.org/2/library/codecs.html\n. For information :\nhttps://news.ycombinator.com/item?id=14119713\nhttps://www.wordfence.com/blog/2017/04/chrome-firefox-unicode-phishing/. About the Oscar theme : I think the \"Search on category select\" plugin should be disable in your use case.\n. Can you copy / paste your Apache configuration? \n. The simple theme is far lighter.\nReopen the issue, if you want to replace the default theme.\n. Google throws a captcha (That's the meaning of the of the RuntimeWarning sorry.google.com). \nExcept changing your IP, there is no way to avoid the captcha if Google considers that your IP is abusing the service (or perhaps change the Google engine implementation, ) \n. Interesting note ! Thanks.\n. I agree with the easy to deploy and hardened.\nAssuming that HTTPS / TLS is used between the browser and the searx,  from the privacy point of view, what is the difference between cookies and URL ?\n- Both are sent / receive on each request. Side note : with HTTP2, the cookies are not sent each time but a reference to them.\n- The problem about the URL are the log (proxy, http server, uswgi) and the browser history \nAbout logs :\n- For the browser history : I don't know what can be done ? Throw a warning when the user sends requests using the GET method without using the privacy mode of the browser ?\n- For the logs : it is directly related to the easy to install and hardened.\nFor now, searx installation documentation requires : \n- to install python 2\n- to create a virtualenv\n- to install / configure uwsgi : log requests at least when there are errors\n- to install / configure apache or nginx : log requests by default\nThe HTTPS configuration is not covered, even if many other websites provide documentations.\nA simpler stack (may be experimental) : \n- configure / install nghttp2\n- python 3 and use python API of nghttp2. The server could behave a HTTP server (to be integrated with an existing nginx or apache) or an HTTPS server. In the latter case, the code could take care of the security, create the SSL certificate if require using letsencrypt. I suggest nghttp2, but I haven't investigate other implementation. See wsgi.py too.\n- virtualenv\nAnother idea can be a docker image with everything inside (can be related to the previous idea) : \n- python 2\n- virtualenv\n- uswgi \n- hardened apache / nginx / nghttpx  (some scripts can take care of the HTTPS certificate).\nAnother idea : provides some deb/rpm/snap packages ? or provides some scripts to ease the installation.\nAnother idea : rewrite searx in Go and provide (the GIL python problem would disapear). To deploy : just start the statically program and done. \nBTW I think there is another privacy issue related to URL : \n1. A user configures searx to use POST and the searx image proxy. \n2. The same user searchs for an image : for each image, the URL will identify the displayed image.\n. BTW, perhaps this PR gives a clean solution to this bug #532 \n. Why is there a problem with clearnet service?\n. Thank you. Could you adjust the test : \nhttps://github.com/asciimoo/searx/blob/master/tests/unit/engines/test_kickass.py#L17\nYou can run the tests : \nhttps://asciimoo.github.io/searx/dev/quickstart.html#how-to-run-tests\n. Could you add all changes in this pull request ?\n. As far as I know, you must set the preferences for each browser you use.\nIt can be implemented by replacing : https://github.com/asciimoo/searx/blob/master/searx/preferences.py#L226 with\npython\n 'method': EnumStringSetting(settings['search']['method'] or 'POST', choices=('GET', 'POST')),\non settings.yml : \nyml\nsearch:\n    safe_search : 0 # whatever you want\n    autocomplete : \"\" # whatever you want\n    method : \"GET\"\n. For testing purpose, you can create issues and commit comments on your own fork.\n. Example : https://searx.me/?q=%21google+xml+header+syntax&category_general=on&time_range=\nThe title to the link \nhttp://stackoverflow.com/questions/13743250/meaning-of-xml-version-1-0-encoding-utf-8\nshould be\nxml declaration - Meaning of - <?xml version=\"1.0\" encoding=\"utf-8\nit is\nxml declaration - Meaning of -\nin searx\n(the <?xml version=\"1.0\" encoding=\"utf-8 is not in the HTML source).\n. Technical details : \n extract_text is used in google.py\n Inside this function, xpath_results.text_content() returns ... <?xml ... instead the original content ... &lt;?xml ...\n* Then html_to_text sanitizes the content, and removes the thing that look like an xml tag\nAbout xpath_results.text_content() it seems there are an option not to decode entities for the XML parser but not for the HTML parser : http://lxml.de/parsing.html#parsers\nPerhaps extract_text should uses etree.tostring(html, method=\"text\")\n. (I can't reproduce the bug)\n. A solution about hard coded headers in webapp is settings.yml. But it would be messy to have header inside settings.yml and inside the HTTP server.\nI agree it should be in the web server, so it is related to the installation.\n. There is request_timeout :\nhttps://github.com/asciimoo/searx/blob/master/searx/settings.yml#L22\n. https://support.ixquick.com/index.php?/Knowledgebase/Article/View/103/19/what-is-the-difference-between-ixquickcom-and-ixquickeu\n. If you can, use a browser using the same IP than your server : google will throw some captchas at first, and will usually stop after few requests.\nOne way to do it : install Firefox on the server, then use ssh with X forwarding or VNC.\nNot sure, if you need to check if your IP is blacklisted ( how to check : http://www.dnsbl.info/dnsbl-database-check.php not sure either that this is right service to  use) \n. @steckerhalter : just in case, are you sure that the X-Forwarded-For header is set when the requests are forwarded to Filtron?. There is a lot more to do / more ideas : \n- get_search_query_from_webapp is HTTP related. So may be this function should move to another file (webapp.py ? webapp_support.py ?)\n- .encode('utf-8') is called on q HTTP parameter in autocompleter but not in the normal search (actually it's done at different location to have both values : encoded and not)\n- apply the PR #518 ( ? )\n- should the engine.request functions use the SearchQuery instance instead of copying the value to request_params ? If the lang parameter is specified, the Search.search method can create a copy of the Search_Query instance with a specific value. It's a lot of work to modified all the engines and tests, but the API to write an engine would be more or less self documented / more clear.\nNote(s) :\n-  request is needed only by the self_info plugin.\n. In the current implementation the temporary file is not deleted.\nMore over, I think this file should be in the safe place or in memory (I've try to use tempfile.TemporaryFile and tempfile.SpooledTemporaryFile without success)\nAnd I'm not sure if it is more efficient or less (at least it's avoid switch between kernel space, user space).\n. Thank's it works.\nI've difficulties to know if it is really faster. At least the compiled module are loaded only once.\nWithout this PR, template are loaded each there are used (for example : result.html)\nOne drawback is the behavior which is different when the debug mode is disabled. \n. With \nflask==0.12\nJinja2==2.9.5\nthe templates are not loaded on each request. \nMay be once per worker or something similar. \nI think this PR can be closed.. Dumb test (there is an exception catching problem in some cases) : \n```python\nfrom concurrent import futures\nexecutor = futures.ThreadPoolExecutor(max_workers=40)\ndef search_multiple_requests(requests, result_container, start_time, timeout_limit):\n    before = time()\n    engines = dict()\n    for engine_name, query, request_params in requests:\n        f= executor.submit(search_one_request_safe, engine_name, query, request_params, result_container, start_time, timeout_limit)\n        engines[f] = engine_name\nafter = time()\nlogger.warning('submit time: {0}'.format(after - before))\n\nremaining_time = max(0.0, timeout_limit - (time() - start_time))\n_, not_done = futures.wait(engines, timeout=remaining_time)\n\nfor f in not_done:\n    logger.warning('engine timeout: {0}'.format(th._engine_name))\n    f.cancel()\n\n```\nthis version is faster than the original version between 0 and 40ms (most of the time between 10 and 20ms).. To limit the number of the results on the first page is quite simple.\nBut it's more complicated than it seems on the second page: some engines give 30 results, some other 20 others, some others 10, some maybe it is undefined.\n. You need to install morty and then configure settings.yml.\nMorty is written in GoLang.. What if the user changes the selected language in the General tab, and then switches to the Engine tab ?. I know it may be not a need, or just out of the scope of searx.\nSome purposes :\n Implement an engine that look up results into anything that is not HTTP : news group, a local database.\n It use thread as the others engines with timeout (I think about Sympy).\n@asciimoo, whatever, what do you think this commit : https://github.com/asciimoo/searx/pull/797/commits/12b077c8fbe49f329d4b4c792fbaef7e56315cf6. Sorry for my dumb answer.\nanswerers doesn't mix with other engines : when an answerer responds to a query, all other engines are ignored. I think the purpose of answerers is respond to specific requests (math, get the color from hex code, duckduckgo has a bunch of that).\nThe purpose here is more about an usual/normal engine but doing a request that is not a HTTP request  : querying a SQL server / NoSQL server for example. Example : a list of addresses that can used in the map category. In this case, I think the user still want to results from photon and OSM.\nI know this PR doesn't solve the following problem, but when there is more than one HTTP request to get the results : why searx blocks the engine if there is an HTTP error on the first request, but not on the second one (done in the response function of the engine without the right user agent, without time-out). \nExample : https://github.com/asciimoo/searx/blob/master/searx/engines/wikidata.py#L83\nActually, in this case, it would super cool to use again the  search_multiple_requests  function. But I agree the current implementation is fine in nearly of the cases (I mean, preparing the HTTP request, sending the request, parsing the result).\nI know that a thread can't be killed, but process can be (not sure if SymPy haven't issue about that) : it's a first stone to implement this. Only an idea.\nBut honestly it was more a try than anything else.\nI hope it's more clear.. > This is more like a scheduled/periodical event than a user triggered if i understand correctly.\nUse case : in the map category, the user searches for \"Riviere Du Loup\", and results comes from from OSM, Photon, and airport_offline engine.\nThe airport_offline engine queries an database (the administrator has to setup the database on the searx instance or not far).\nThe data are loaded from https://github.com/jpatokal/openflights/tree/master/data\n(see airports.dat file).\nSo, doing this, searx becomes an search engine without the meta since it starts to host data.\nIn the general category, the engine can suggest \"Riviere Du Loup\" when the user searches for \"CYRI\" (the airport code). Actually it's what wikipedia engine does when the english language is selected, not in other languages.\nSide note : If the user searches for \"weather CYRI\", an answer can be provided, but the user may be interested in the usual results.\nI admit that :\n this is most probably out of the scope of searx and / or useless (just a little experiment)\n the use is most probably to implement infoboxes or answers in an asynchronous way (I understand now the privacy choice for the answers).. I just open the PR #798 with only the first commit of this PR. \n@asciimoo, feel free to close, leave it in stand by for the future / as note / reference. . Not sure if I understand correctly your question. \nWithout this commit : \n if an engine get a timeout with Requests, it will be suspended.\n if an engine get a timeout with searx (the if search_duration > timeout_limit + timeout_overhead check), the engine will not be suspended\nWith this commit, in both case the engine will be suspended. \nActually the if search_duration > timeout_limit + timeout_overhead check could be part of poolrequests.py. The purpose is to have a global timeout, not a timeout for the connect, and another one for the read.. Note that from flask_babel import gettext is still used here and there.. Not sure if I answer the questions : \nAbout mootools-autocompleter-1.1.2-min.js I don't have a 404 error : https://github.com/angelsk/mootools-autocompleter/tree/master/Source\nAbout bootstrap, the css is built from https://github.com/asciimoo/searx/tree/master/searx/static/less/bootstrap\nusing https://github.com/asciimoo/searx/blob/master/manage.sh#L91\nThe source is split between the less and js directories : https://github.com/asciimoo/searx/blob/master/searx/static/js/bootstrap.min.js \nSee the original archive :  http://blog.getbootstrap.com/2014/06/26/bootstrap-3-2-0-released/\nIs a link enough or should the content be added to searx ?. > For what it's worth, the oscar theme seems to work fine with jquery 3.1.1, bootstrap 3.3.7 and requirejs 2.3.2 which are the versions from Debian unstable.\nThe difference I see, is that jquery 3.1.1 doesn't support IE 8 and bellow. In this case, /searx/static/js/html5shiv.min.js and /searx/static/js/respond.min.js can be removed (and the oscar theme changed to remove the references to them).\nFor IE 8 and bellow, searx can disable javascript (may be not a bad idea for these browser versions).\n@asciimoo do you think it is a problem ?. Note : 192.168.99.100 is a private IP : https://en.wikipedia.org/wiki/Private_network\nSo this IP can't be access from public Internet.\n. If you use xampp, you should install mod_wsgi.\nI think, but can't confirm that sudo apt-get install libapache2-mod-uwsgi installs mod_wsgi for apache, the one included in embedded Linux (The Ubuntu included in Windows).\nSo either : \n you choose to use xampp, and you have to adapt the installation steps for Windows (python for windows, wsgi for windows, etc...). See https://code.google.com/archive/p/modwsgi/wikis/InstallationOnWindows.wiki and http://enkoding.blogspot.fr/2013/01/setup-python-for-web-in-7-steps-on.html\n or you use the embedded Linux. Install apache using apt install apache2. The point I don't know is how to turn apache2 into a service. You can still start it by hand using service start apache2\n. It's a different topic but : \n there is no specific integration with Django. Searx just uses the HTTP protocol to produce HTML, JSON, RSS, CSV output.\n ampps uses Python 3.5, for now searx is written in python 2. There a port python 3 under development in the py3 branch. mod_wsgi is included according to the documentation, so it should works as long as you can install the searx dependencies.. Thank you for reporting issues.\nJust notice that I do this project on my spare time, I think it's the case for all other contributors. There are more 70 engines now, most of them scrape pages, that's mean the API is not clearly defined : at any time an engine can starts to fail. Maintaining all these engines and the core of searx starts to be a huge thing. Feel free to contribute by writing code (python is not that hard, and it can be funny actually).\n. Can you try : \nsh\n./manage.sh update_packages\nAccording to the link you give : \n\nOlder versions of Python 2 are built with an ssl module that lacks SNI support and can lag behind security updates. For these reasons it\u2019s recommended to use pyOpenSSL.\nIf you want to install the packages manually, you will need pyOpenSSL, cryptography, idna, and certifi.\n\n./manage.sh update_packages will make sure that these packages are installed (except idna) as specified in https://github.com/asciimoo/searx/blob/master/requirements.txt\nThe urllib3.contrib.pyopenssl.inject_into_urllib3() is not done but it has been working since a long time without it.. Actually urllib3.contrib.pyopenssl.inject_into_urllib3() is done inside requests package\nSo HTTPS should works fine, try to run  ./manage.sh update_packages\nif it's still not working, can you post here the result of pip freeze. * How do you start searx ? using uwsgi ?\n* If you start searx directly with python searx/webapp.py does it work ?\nIn both case, have you this warning in the log : \nThe pyopenssl, ndg-httpsclient, pyasn1 packages have to be installed. \nSome HTTPS connections will fail. for reference, more or less related to #687 . The simple was merged into the master branch, it can be use on searx.me.\nOn first load (compression enabled) : \n for the first page : 23.3kb\n for a result page : 29,7kb\nWith js,css in cache (compression enabled) : \n for the first page :  3.3kb (the size can be cut down by reducing the number of available languages).\n for a result page without image : 13,6kb\nNot everything is implemented, but most of it the oscar features (RTL languages are not supported for now).. Can you check if pyopenssl, pyasn1, ndg-httpsclient are installed ? \nIf you use a virtualenv, can you check that the the virtualenv is setup correctly on the boot ?. without pyopenssl, pyasn1, ndg-httpsclient, searx seems to work but some engines may not work because the HTTPS connection can't be made (there are optional dependencies of the requests module).\n\nI have to login to the VM after the VM is started and have to restart the uwsgi service. If I don't do that searx is not starting. \n\nTo clarify : it's mean the error doesn't appear after restarting uswgi ? If yes, it's an Linux distribution problem.  \ngetrandom() initialization failed is the real error, it's an urllib3 error.. uwsgi demonizes searx.\napache (or nginx) is the frontend which allow to setup HTTPS connection (uwsgi is not a frontend).\nSo you need to follow : \n Basic installation\n Configuration\n Check\n uwsgi\n* Web server / with apache\n. Not sure but I guess that <Location /usr/local/searx> should be replaced by <Location /> or <Location /searx> .. I can't double check right now. But uwsgi is used so the searx install directory is not important except for the static files. So /searx. I'm not sure to understand. With \npython\nip_info = get('http://ip-api.com/json', proxies=outgoing_proxies).json()\nip_info contains the IP of the searx instance. Of course if the searx instance is run locally, you get your IP address.. Tor network allows to have hidden servers : the IPs of the servers are hidden. \nWhy display the instance IP in this case ?. > However, this could still be used if request.remote_addr could be passed as a parameter in the GET request, so that the user's IP info would be found.\nhttps://github.com/asciimoo/searx/blob/master/searx/plugins/self_info.py#L39 ?. @conmarap sorry the late answer. There are two issues : \n since the code sends a HTTP request, it should be converted to an engine similar to the currency engine.\n could you merge all the commits ?. Yep, got it for the https : good.\nThere are 15 commits : only one should be enough.. It seems there is bug : the link the json output has pageno=2 instead on pageno=1 or nothing.  When the pageno parameter asks the first page, number_of_results contains the right value.\nNotes : \n if you refresh the page, you may notice that the number may change\n this value is the max of values return by the different engines\n* if this number is bellow the number of result, then the value becomes 0\n. I've updated my instance with the simple branch : https://al-f.net/searx/ so everyone can give a try and add bugs reports, reviews, comments. \nAbout network usage\nA result page without images and without autocompleter downloads 37,6Kb. With autocompleted activated, 67.9kb are downloaded (using mootools, http://autocomplete-js.com/ can be used but seems buggy).\nMore over, the full glyphicons-halflings font is downloaded (17.7kb): a subset can be created to reduce the network usage (may be 10kb less ?).\nThe magnify can be replace by an icons ( glyphicon-search ) : 2.3kb.\nAbout plugins\nThey are heavily tied to jquery (and the oscar templates). In simple theme, a quick implementation is to load jquery when a plugin add some js files. A more long term is to implement plugin with native javascript (or to transpile from ES6 / ES7). Another third way is to load minimal jquery version, or use a compatible version like zepto.. Some updates : \n without image, the network usage is 22.1kb for results of the general category. \n the vim keyboard has been included and modify to use vanilla JS (the help doesn't work for now)\n the browser focus follows the vim selection\n images size are set in a way that each row is always full (the last commit fixes a bug when there are different block of images) \n* the autocompleter uses POST when the main search uses POST too. \nWhat can improved : \n search, the input has the focus, go to another page, go back to the searx page : the autocomplete choices are shown. I don't think the user expects that behaviour.\n the \"send search on category click\" is missing.\n still the preferences page.\n use the full page width when the user search for images only.\n which logo to used ?\n the customize ion font could be included in the css which should reduce the response time.\n* refactor the less sources : for now, I think the RTL modification won't be easy.. There are still plenty of different things to do, but I think it could be the time to merge, so more people could test it, or even contribute, @asciimoo what do you think ? \nIt is more than usable except with RTL languages.. see for remaining work on the simple theme : https://github.com/asciimoo/searx/issues/1025. side note : search404.io oscar template has been modified to include roboto font from google : \nhtml\n<link href=\"https://fonts.googleapis.com/css?family=Roboto\" rel=\"stylesheet\">\nas long\nhtml\n<meta name=\"referrer\" content=\"no-referrer\">\nis used by the browser there is no problem otherwise GET requests are leaked to google.. Great :-) \nBut there are two additional requests, one to download cloudflare.min.js and second one to download Rocket Loader. \nDisabling Rocket Loader won't solved the issue :  CloudFlare is used anyway.\nAbout your issue, I think that @pointhi should be able to fix that ?. @neggs : the search requests are going through CloudFlare (I guess CloudFlare provides the SSL termination), so the privacy is not increased.\nSo : \n if CloudFlare provides Roboto, you can use it (or download Roboto font localy, see https://www.fontsquirrel.com/fonts/roboto for instance)\n Rocket Loader can be use\n. About Cloudfare and MITM : \nhttps://info.ssl.com/the-real-cost-of-a-cloudflare-free-ssl-certificate/\nhttps://www.reddit.com/r/privacy/comments/41cb4k/be_careful_with_cloudflare/\n(may be not the best articles, but they give the idea)\nAbout Cloudfare about TOR users : https://en.wikipedia.org/wiki/Talk:Cloudflare#Tor\nIn my opinion the best option for privacy is no intermediate. If correctly configured, the static resources (js, css, fonts) are loaded only once by each browser.. about HSTS there is a preload list : https://hstspreload.org/\n. This error :\nKeyword 'Capture Page Screenshot' could not be run on failure: No browser is open\n...\nAssertionError: Setup failed:\nWebDriverException: Message: Missing 'type' parameter\nseems to appear in all new builds, see for example https://travis-ci.org/asciimoo/searx/builds/206807742 (A couple of fixes with search languages PR)  \nNote : the commits are a messy (one is empty actually), sorry.. ping ?. While it seems technically doable, I don't see the point : \n the results are identical to bing engine \n according to wikipedia Ecosia earns money with Ads. Using searx as a proxy won't allow them to do it.\nIn the end : \n Ecosia can't plant more tree.\n searx users are disappointed because the results are identical to the Bing engine. . I see\nNo route to host\nin the log, can you try to traceroute soundcloud.com ? . Does it happen with shorter searching phrases ? \nOne technical reason to that (may be not the real one) : \n Searx sends the user request to different search engine, and wait a global timeout : if the search engine didn't response before the timeout, the response is ignored.\n Searx keeps some HTTP connection to different search engine after a user request.\n the HTTP connections expire after a time.\n Opening the HTTP connection takes time (HTTPS connection actually).\nSo on the first request may be the HTTP connections are not opened, but the second time they are. The difference is enough important to create this phenomena : \n no result on the first user request : all search engines timeout (including everything : preparing the HTTP request for the search engine, opening the connection, send the HTTP request, parse the HTTP response) \n results on the second calls because the connection time is 0 \nIt can mean that the network connection is slower than before between Searx and the different searx engines.\nNow, from the user point of view, nothing can be done : the timeout should be increased, but it's Searx administrator parameter. \nWhen I look at https://searx.me/stats it seems there is issues, but it's just the average response time which can hide a lot of things. Are you using another instance ?. I can reproduce the bug with a Debian Jessie not a Debian Wheezy (using a docker image).\nInstalling the python-pyparsing debian package doesn't help. . Fixed with PR #972. related to #695 . are talking about : \ngit clone https://github.com/asciimoo/searx.git\ncd searx\ndocker build -t whatever/searx .`\nfrom https://asciimoo.github.io/searx/dev/install/installation.html#docker ?\nWhat do you want to do ? customize the installation ? . I forgot the question mark at the end of the subject and the first sentence :-)\n. Just a note : it's not possible to do anonymous search, Mastodon requires to be log in to interact with it. \nIt's not a deal breaker : some engine requires an API key).. @mangeurdenuage, that's precisely the purpose of searx : search information and keep the privacy.\nOtherwise for the same reason searx shouldn't implements an engine for youtube or twitter for example.\nThe different is that mastodon is not so huge, so implementing an engine in searx will give more credit to this tool.\nSo should searx gives access to information what ever the TOS, or should a selection be made or should a warning be displayed as in F-Droid ? I don't know.\nI've made a quick look around of the UI : I can't find a search in all the post ?. The way to implement the search would be : \n searx login with a dedicate user.\n searx send the search using this user (or set of user).\nso there is NO tracking of any kind.\nexcept : \n it's against the TOS\n and .... there is no search.\n. I close the issue since there is no search.. gnusocial search can be implements with the json engine : \nhttps://instance.hostname/api/search.json?q=%s&rpp=15\naccoding to http://skilledtests.com/wiki/Twitter-compatible_API#Search. I've never used libre.fm.\nIs the artist search the most relevant ?\nIf I search for \"pink floyd\" the result are somewhat messy.\nI see an API but not for search : https://git.gnu.io/foocorp/librefm/wikis/Librefm_API_methods\nBut it's doable.. Tank to drive through the wall : https://github.com/Anorov/cloudflare-scrape (not sure this is the right way to do.....). In the preferences, choose GET for the Method : \nhttps://searx.me/preferences. on settings.yml : \nyml\nsearch:\n    ...\n    method : \"GET\"\nsolved in https://github.com/asciimoo/searx/issues/703 \n. @mangeurdenuage, what do you mean not necessarily encrypted? \nIf HTTPS is used, everything is encrypted except the domain name and the TCP port/IP couples. Of course there is the problem with compression, weak protocol, etc.. \nMay be I miss a point? \nIMO, the two issues about URL are :\n the browser history. \n logs : form (when POST is used) are not logged by default on server, URL are logged by default. \nBut note that the title of page contains the query. So the browser may store that query in the history (whatever the method GET or POST). \n. Really fast answer about CLI : have a look to https://github.com/asciimoo/searx/blob/master/utils/standalone_searx.py. For reference : https://github.com/asciimoo/searx/pull/6. Whaou a lot of ideas. \nQuick note : There are a reddit and stackoverflow engines. \nDisclaimer, I'm not the main author, so my opinion may not reflect the author idea. @asciimoo  are you here :-)  ?\nBasically you are talking about of a search engine by itself (crawler, local import of wikidata/wikipedia), and this is out of topic for searx. The purpose of searx is to be meta-search engine : \n searx provides a search engine where the user privacy is preserved.\n searx must simple to install and resource friendly. Hidden top secret purpose : to have a lot of searx instance everywhere on Internet (on Raspberry Pi, on small VPS, etc...).\nSo searx :\n can't provides result by itself (to heavy to host a huge database). Okay there two very small databases : https://github.com/asciimoo/searx/tree/master/searx/data\n uses the power of existing search engine and \"only\" aggregate the results.\nThe initial purpose was to replace the dead project seeks. Some differences in the philosophy : \n it is written in python instead of C to be easily hacked.\n there is no peer to peer sharing of the search results. Why : IHMO because seeks considers other seeks instance as friendly, searx considers other instance as the hostile to the privacy.\nBut in the end there is balance to find : \n if only one person use a searx instance, big search engines will find out who the person is (if they have the appropriate algorithm, which I'm sure they have). But most probably this user is also the administrator of the searx instance, so he knows that there is no log (or only him can see them).\n if a lot of people uses one searx instance, how to be sure about the privacy of the log ? Yes it is written that there is no log, but how to sure ?\nThat's why a group of friend who use searx is may be the most advisable use case.\nSome noise to the user search is added by army of robots using to searx, that's why filtron exists.\nThere is another use case for searx : to be able to use search engine that are blocked in the region of the user. So morty helps in that way too, even if not perfect.\nAs you may see there are few active contributors., and there are a lot of opened issues / PR.\nI guess you know yacy, but may be not common search project ?\nI've thought to different ideas too : \n create a huge database of wikidata / wikipedia / airplane flight  / music / whatever titles. Just to make the search more easy / faster. A common use case is search english terms with searx configures to search in another language. The wikipedia won't return any results. Same if there is a misspell.  It could be linked to searx with a dedicated searx engine but I don't think it could be included inside searx. (using for example solr or riak).\n a network of searx instances could be used to send one user search through different searx instance : https://github.com/asciimoo/searx/issues/792\n a p2p network of searx instances to know to provide a list of all instance (and nothing more). It could replace http://stats.searx.oe5tpo.com/ (or https://github.com/dalf/searx-stats2 ). A simple way would be to send a ping to all instance not seen for a time. The response to a ping would the list of known instances. It could be combine to https://github.com/asciimoo/searx/issues/263 to make sure that there is no MITM between a searx instance and search engines.\n. There is contributed PR : https://github.com/google/open-location-code/pull/31. I haven't checked the details, but quick answer : \n the search language\n* the IP of the instance\nSearch engines in addition to the search language uses the GeoIP to give customize results.\nSo even, if the real IP is hidden, different the searx instances can provides different results.\nThere is nothing that can be done about that in a reasonable way. \nA hammer solution could be : \n have different IP in different region of the world.\n searx send the same request to the same engine from different IP\nA more practical way : \n* configure a searx instance to proxy requests to other searx instances and merge the results, may be more than one instance. Of course the response time will be affected ( see https://github.com/asciimoo/searx/blob/master/searx/engines/searx_engine.py ).. More or less related to this PR and / or outgoing http(s) connections.\nCreate a per engine HTTP configuration :\n- forbidden HTTP or not (could be set to true by default)\n- the cipher suite to use.\n- the proxy to use. The global proxy configuration remains : it would easy to send some requests through TOR for only some engines.\nIn addition, the set of IP address could be per engine instead of per process : for the engine, searx would never use the IP twice in row (now it is not guarantee).. > why only if debug mode is turned off?\nbecause it makes the startup time longer :-) \nwhen in production, I don't think it's an issue if startup time is a little bit longer.\nAt first I was thinking to store the versions in a file, but I don't know where ?\nIn the data directory, I think it will make trouble with the deb package.\nIn the /tmp may be ?\nThe other solution about parallel version could be simpler even it's more hackish. update_firefox_version() can create a thread to do the real work.. > I can't see why would be it hackish, this is the best solution for me =)\nSo thread it will be. About the hackish : it's my wish to remove the one HTTP request = one thread.\nMoreover if there are 4 process (with uwsgi), the firefox versions will be fetched 4 times.\n+1 for the init() method.. Done.\nI've added a 2 seconds timeout on the URL fetch and updated standalone_searx.py.\ntime_to_update check is not thread safe so once a week, schedule_update_firefox_version() can be called multiple time in a row. The update_lock prevents multiple URL fetches at this moment.. Just to be sure, Is it the fetch at the start of the searx that block this PR ?\nA static tmp file could solve the issue : \n- a file is create in a defined location\n- the file is updated once a week\n- when an instance is (re)loaded, the file is only loaded\nOnly issue :  define this static file (or database).. I agree it may be too much code.\nAccording to http://caniuse.com/usage-table  version 40 to 47 represents 0.37% of user requests.\nThe purpose of this PR is to merge with the 4.08% which is a little better (last two versions of Firefox).\nUsing the last versions of Chrome would be even better.\nBut anyway, I guess search engines can easily find out that searx is not a real browser. . Another implementation : \n create a utils/fetch_firefox_version.py which create searx/data/firefox_versions.json\n searx loads that file\nSo : \n The start time of searx is not impacted\n The versions can be updated whenever the administrator decided (requires to reload searx)\n. I have updated the branch.\nThe useragents.json file is located in searx/data/useragents.json\nBecause this file can be updated, it would be interresting to have this file in another location when searx is packaged.\nSome possible locations : \n the same directory than settings.yml\n a directory referenced by settings.yml ( relative path \"searx/data/useragents.json\" by default ). The design has changed a lot since this pull request has been opened.\nIn the current implementation : \n the searx/data/useragents.json contains the user agents to be used when requests are sent to the different search engines.\n this file can be updated by the searx administrator either manually, either automatically using the script utils/fetch_firefox_version.py. This script is never called automatically : someone has to call it.\nSo, in the current implementation, I don't think there is privacy breach ?. Moreover, it is possible to get the last Firefox versions using wikidata the same way searx answers user requests.\nIt is less realiable : anyone can modify the wikidata page to highlight searx searches (if searx administrator don't check the results from the script).\n. http://marisa-trie.readthedocs.io/en/latest/tutorial.html. Is there any errors in /var/log/uwsgi ? . Related to these two commits : 7388067 and 68cbf04\nWhen in DEBUG mode, the engines are initialized twice when started using\nsh\npython searx/webapp.py\nSee:\nhttps://stackoverflow.com/questions/25504149/why-does-running-the-flask-dev-server-run-itself-twice\nI haven't tried with uwsgi and debug mode.\nWhich version of uwsgi do you use ?. I'm (slowly) working on it with a new theme : \n see PR #858\n You can try here : https://al-f.net/searx/\nRelated to #687 #828. to make it similar to other search engines : \n\nThe screenshot is just a quick hack, it needs some works to make it really work.\nI have the issue about the logo, should the one from oscar/pointhi be used or the one from oscar/logicodev ?. I'm not sure to understand : \nhttps://github.com/asciimoo/searx/blob/master/searx/static/themes/simple/package.json\nhttps://github.com/asciimoo/searx/blob/master/searx/static/themes/simple/gruntfile.js\nhttps://github.com/asciimoo/searx/tree/master/searx/static/themes/simple/js\nThere are dependencies to grunt-cli and some npm packages. \nSame as the oscar theme.. To deploy searx, there is no need to use npm.\nTo modify the oscar and simple themes, you need npm. . the simple theme PR was merged.. Seems like a duplicate of #695. Backup solution in case, the RSS :\nhttps://news.google.com/news/rss/search/section/q/test/test?hl=fr&ned=fr. fixed : 3182ba7069b2aa2fba1a1b7879bd5f234a4d2868. done : #1075. \nSo searx.me uses https://github.com/asciimoo/filtron otherwise google and all will ban all searx.me ips. . https://github.com/asciimoo/searx/blob/243d3e4298c93014f37ca6f1f957a60cb09f4ae1/searx/search.py#L182\nTimeout in the search_multiple_requests function are not displayed.\nimplementation idea:\n- call add_unresponsive_engine method.\n- the add_unresponsive_engine checks if the engine was already added.. Thank you for your PR. \nI've added some comments about the code. \nMore over could you please fix these code style errors :\nhttps://travis-ci.org/asciimoo/searx/jobs/253845047\nYou can run locally these tests using ./manage.sh tests\nRead https://asciimoo.github.io/searx/dev/quickstart.html#devquickstart for more information. . The previous commit c0993ece546716e485478a5b86e30c41e81f5154 avoids --upgrade, so I don't know if this is the right place to do an upgrade of pip and setuptools. What about this one? . About if search_duration > timeout + timeout_overhead\nI don't know if it is relevant to keep it ?\nWithout it, it means the parsed results will be ignored later, inside search_multiple_requests\nIn addition, this code in search.py\npython\nuser_agent = gen_useragent()\n...\nrequest_params['headers']['User-Agent'] = user_agent\ncould be changed to have a default user agent on all HTTP requests. The default headers could be set at the beginning of search_one_request_safe (or perhaps search_one_request) using poolrequests.set_headers_for_thread. Since a new version has been released, what about merging this one, and take time to test it in master branch ? . It seems there is no uwsgi at all ?. Look at https://asciimoo.github.io/searx/dev/install/installation.html#docker\nsh\ndocker run -d --name searx -p $PORT:8888 searx\nwill run the images you have built. \nReplace $PORT by the number you want too.\nYou may want to instantiate different instances and run nginx as a front-end using docker compose. But you can start without, especially if searx runs on your laptop.. You have to install https://github.com/asciimoo/morty\nThen you can configure searx to use it in settings.yaml : see result_proxy section. . The key is a secret shared between morty and searx.\nThe purpose is to avoid abuse of the morty proxy : only searx can create valid links to morty.\nFor example one link from searx.me :\nhttps://searx.me:3000/?mortyurl=https%3A%2F%2Fasciimoo.github.io%2Fsearx%2F&mortyhash=e32499a7c4f6241f9dfa39cd577859bff01d137e13d9c493e9bb75d1b997d681\nThe mortyhash is created using the mortyurl and the shared key.\nMorty on searx.me:3000 will check that the mortyhash matches the mortyurl and the shared key. \nTo sum up : \n define a key (whatever you want) on settings.yml\n when you start morty add the  -key parameter and the shared key.. That's normal : morty expects a mortyhash parameter (given by searx).\nIt seems you built your own docker image. Have you built it again after you modified the settings.yml ?. Another more simple implementation : just define a regex for an engine.\nIf search.py see a defined regex, it checks if the query matches this regex.\ndictzone and translated do extended check of the languages in is_accepted, with the regex version it won't be possible.\n. I don't know why the travis build doesn't work.. also I'm not sure there is not encoding problem (between the python -> json -> python conversions ). The status on travis is build:failing but everything seems alright except : \n\n[!] Grunt build : oscar theme\n...\nRunning \"less:bootstrap\" (less) task\nDestination not written because no source files were found.. It seems that the coverage test never finish.. Is there issue ?\n\nHave a look to the source code: \nhttps://github.com/asciimoo/searx/blob/master/searx/engines/google.py#L30\nhttps://github.com/asciimoo/searx/blob/master/searx/engines/google.py#L185\nThe other domains are used since they give different results.. Sorry my answer was not clear : this is exactly what is happening. \nFor example :\n Google.be is used if the selected language is fr-BE\n Google.fr is used if the selected language is fr-FR\n[EDIT] may be the list could be updated according to : https://fr.wikipedia.org/wiki/Liste_des_domaines_de_Google (each domain must checked, last time there were some mistakes in the English version. English version that doesn't exist anymore).. Could you provide the names ?. What do you want to customize ? Can you elaborate ?\n. see #792. * Go \nhttps://www.transifex.com/asciimoo/searx/language/fa_IR/\n log in with your github account\n ask to join the team using the the upper right corner button.. Where it should be ? Application category ?\n. The travis build shows some errors : \n/home/travis/build/asciimoo/searx/tests/unit/engines/test_genius.py:98:121: E501 line too long (125 > 120 characters)\n/home/travis/build/asciimoo/searx/tests/unit/engines/test_genius.py:107:121: E501 line too long (127 > 120 characters)\n/home/travis/build/asciimoo/searx/tests/unit/engines/test_genius.py:167:121: E501 line too long (122 > 120 characters)\nyou can run these tests on your machine using : \nsh\n./manage tests\nsee https://asciimoo.github.io/searx/dev/quickstart.html. What about the site:only-this-website.com ?. Would it be good it could be set as a preference ?\n[EDIT] Searx appends automaticaly site:... to each request (the value would be from the preferences).\nEach engines needs to define if they can support site:..... Technically, it is simple : \nhttps://github.com/asciimoo/searx/blob/master/searx/templates/oscar/results.html#L11\nreplace by\n{% block title %}{% if method == 'GET' %}{{ q|e }} -{% endif %}{% endblock %}\nWarning : not tested, may be there is a syntax error, but that's the idea.. Do you know which engine gives harmful results ?\nSome technical details,  an engine can return : \n potential harmful result (we just don't know if it will be the case, no way to filter) \n harmful result is safe mode = 0\n harmless result is safe mode = 1\n harmless result what ever the request\nThe current implementation has an issue : \n if an engine (the implementation in searx) supports content filtering, it will be filtered according to the user request. Good.\n if an engine (the implementation in searx) doesn't support content filtering, it won't filtered. But the engine (the one which gives the result) can gives harmful results even if the searx user asks for filtered results. Not good.\nIn my opinion, for each engine, the implementation in searx must describe : \n potential harmful results but no implementation in searx\n potential harmful results with implementation in searx\n* harmless results.\n. Thank you. \nThe simple theme uses ionicons : A subset font is made to only pick the used icons up using grunt-webfont\nThe oscar theme uses the Glyphicons : https://marcoceppi.github.io/bootstrap-glyphicons/. is that the link you are looking for : https://asciimoo.github.io/searx/dev/install/installation.html#installation  ?. It can be any value you want, it's a kind of password. \nSearx can be use to proxy images. Have a look to the source of this page : \nhttps://searx.me/?q=%21goi+searx&language=all&time_range=&safesearch=0&theme=simple&image_proxy=True\nThe images are not fetched from google or whatever, but only from searx : the purpose is to protected your privacy.\nYou can see that there is a hash parameter for each images : the value is calculated using the secret key. Try to change it : you won't get the image. Without this hash, anyone would be able to fetch images from anywhere.\nSo to sum up, just do \nsh\nsed -i -e \"s/ultrasecretkey/`openssl rand -hex 16`/g\" searx/settings.yml\nand it will be fine.\n. @kvch I have followed the \"recipe\" inside .travis.yml : \nbash\nnpm install less@2.7 less-plugin-clean-css grunt-cli\nexport PATH=`pwd`/node_modules/.bin:$PATH\n./manage.sh install_geckodriver\n./manage.sh npm_packages\n./manage.sh update_dev_packages\nthen\nbash\n./manage.sh styles\n./manage.sh grunt_build\nor for the simple theme : \nbash\ncd .../searx/searx/static/themes/simple\ngrunt\n[EDIT] see npx. Try google without javascript : there are far less information than the javascript version. \nSome informations are not easy to parse, or just impossible (links to google url without external meaning, I think about the maps for example when you search for \"cinema new york\"). May be it has changed since the last time I've checked.\n. Technical informations :\n https://github.com/requests/requests/issues/1691\n https://stackoverflow.com/questions/33046733/force-requests-to-use-ipv4-ipv6. What is the process to merge in asciimoo/searx ?\nMake a huge PR from time to time ? as they arise ?\nSo I should move #1025 to my fork ?. Docker is one way to install searx among others : \n git clone\n debian package\n arch package\n yunohost\nSo is your request to remove the docker image ?. https://www.docker.com/components-licenses. :+1: \nIs ipv6 still used in case of redirect ?. Strangely coverall is working again in the different PR.. Technical note here.\nFor now, searx parses the query to :\n find a language\n find the category and / or a engine list.\nonce done, the query is sent to the search engines without anymore parsing.\nSo :\n  if you search !google site:github.com searx you will get the expected result from google since google supports this syntax.\n if you search site:github.com searx you will get the result from different search engines, some may not support site:github.com, and the results will be messy.\nBasically there are ways to implements this requests: \n Simple : each engine tells which special search is supported (searx doesn't modify the query). Searx uses some regex to match that.\n More advanced : parse the query, each engine must explicitly translate the query.\nSecond case :\n make a better parser for searx (parse the AND, OR, quote, etc..). More over the parser must be resilient : searching !google intext:\"powered by searx\" must still work with google.\n the code for each engine must provide which syntax is supported (for example, is site:something supported by bing engine)\n* the code for each engine must translate the searx query (perhaps a search engine doesn't support with the site:something syntax, perhaps it is supported with an additional parameter).\nDoable, but a huge task.\n. This project seems great, but there is no big data in searx : it's \"only\" a proxy to some other search engine.. IMO, it would make more sense to add this engine to the code category since it indexes pypi and CRAN projects and authors.\nActually the thing that would be awesome :\n implement the PyPI search\n for each packages display some additional links  to depsy and may be some other websites (security analysis of the package ?) . see commit 46fb0d860e35a45658969c4e2ac306a1072bc331. We can image that there is a default configuration, not the one in settings.yml, but an absolute one. In this configuration, all engines are disabled, the image proxy is disabled, and so on.\nThis configuration could be used to create the settings URL : each instances would know how to infer the missing values.  This would allow the same settings URL across searx instances.\nA second thing, if we decide to change the setting URL format, a bit field could be use to encode all boolean values (image proxy, include or not this engine).\nOf course, the question is \"what does this bit represents\" needs to be resolved : it could be done with a static mapping. For example, each engine would have a bit position. Each new version of searx adds  new bit positions, but never modified the old bit mapping. The mapping would be in an yaml file for example.\nIn this way, the settings URL could be way much more shorter, and compatible across searx instances, without any storage.. I confirm. May be related to 9ab8536479f30960e79ed1 ?. Technical note on search.py : We could imagine that an engine can throw a SearxThrottleException which gives how many minutes / hours to wait before to use this engine again. Then search.py can makes this request effective. It would avoid endless captcha (even if search.py already do that but I guess it doesn't wait enough because this is not the same use case) . The old commit 52e615de is nearly the same, but if you have time, you can give a try :\nhttps://raw.githubusercontent.com/asciimoo/searx/52e615dede8538c36f569d2cf07835427a9a0db6/searx/engines/google.py. Some details. \nThe original is written in typescript :\nhttps://github.com/autocompletejs/autocomplete.js/blob/2.6/src/autocomplete.ts\nThe current searx master contains a modified   Javascript version.\nThe font is subset of ionicons as you can see gruntfile.js. See http://ionicons.com\nThere is one exception : magnet.svg. Yes autocomplete.less is also part of the autocomplete.js project.\nAbout autocomplete.js : I've made contributions to the project, but the author didn't gave answer in time, that's why I ended up in this solution.\nI will make a full review of the copyrights of the simple theme files tonight (Europe time) or tomorrow night.. package.json describe the imported NPM packages : \n- https://www.npmjs.com/package/grunt-webfont(MIT license : https://github.com/sapegin/grunt-webfont/blob/master/License.md )\n- https://www.npmjs.com/package/ionicons-npm (MIT license : https://github.com/ionic-team/ionicons/blob/master/LICENSE )\n- https://www.npmjs.com/package/jslint ( BSD 3-clause license : https://github.com/reid/node-jslint/blob/master/LICENSE )\n- https://www.npmjs.com/package/less-plugin-clean-css ( Apache License 2.0 license : https://github.com/less/less-plugin-clean-css/blob/master/LICENSE ) \n- grunt and related tools ( main tool : https://github.com/gruntjs/grunt/blob/master/LICENSE )\nThese packages are not in the searx code base tree with the exception of ionicons-npm : it's a webfont, a subset is maded using grunt-webfont (see https://github.com/asciimoo/searx/blob/master/searx/static/themes/simple/gruntfile.js#L79 ) \nThe results is under the fonts directory (all the files here are the result of grunt-webfont, there is also ./less/ion.less file produced by grunt-webfont)\nOf course there is an exception : madnet.svg found : https://thenounproject.com/term/magnet/5022/ the website says \" Creative Commons CCBY\".\nWhat should be done ? remove it ? modify the files to add a reference to the website ?\nAbout autocomplete.js, my list of contributions : \n- https://github.com/autocompletejs/autocomplete.js/pull/47\n- https://github.com/autocompletejs/autocomplete.js/issues/46\n- https://github.com/autocompletejs/autocomplete.js/issues/40\n- https://github.com/autocompletejs/autocomplete.js/issues/39\n- and opened one : remaining https://github.com/autocompletejs/autocomplete.js/pull/48\nHonestly I need time to compare the version in searx and in autocomple.js .\nThere is still a lot of work to do for the simple theme but it is more than usable now ( https://github.com/asciimoo/searx/issues/1025 )\nThe file list : \n./.jshintignore\n./img/favicon.png               <-- copy/paste from oscar theme\n./img/logo_searx_a.png          <-- copy/paste from oscar theme\n./img/searx.png                 <-- copy/paste from oscar theme\n./img/loader.gif                <-- copy/paste from oscar theme\n./img/searx_logo.svg            <-- copy/paste from oscar theme\n./package.json          <-- describe the NPM packages to build the simple theme.\n./gruntfile.js                  <-- describe how to build the content of css, fonts, and searx.js, searx.min.js\n./magnet.svg            <-- see above\n./leaflet/leaflet.js        <-- there is the copyright inside (the version could be updated BTW...)\n./leaflet/images                <-- either copy from oscar theme or from the leaflet archive\n./leaflet/images/marker-icon-orange.png \n./leaflet/images/marker-icon-2x.png\n./leaflet/images/marker-icon-2x-green.png\n./leaflet/images/marker-icon-2x-red.png\n./leaflet/images/marker-icon-red.png\n./leaflet/images/marker-icon-green.png\n./leaflet/images/marker-icon-2x-orange.png\n./leaflet/images/layers.png\n./leaflet/images/marker-shadow.png\n./leaflet/images/marker-icon.png\n./leaflet/images/layers-2x.png\n./leaflet/leaflet.css           <-- from the leaftlet project\n./css/searx.css                 <-- compiled by gruntfile.js from ./less/searx-rtl.less and the referenced files (all under ./less)\n./css/searx-rtl.css             <-- compiled by gruntfile.js from ./less/searx.less and the referenced files (all under ./less )\n./css/searx.min.css     <-- minified version searx.css\n./css/searx-rtl.min.css         <-- minified version searx.min.css\n./fonts/ion.ttf\n./fonts/ion.woff\n./fonts/ion.css\n./fonts/ion.svg\n./fonts/ion.woff2\n./fonts/ion.eot\n./fonts/ion.html\n./less/style-rtl.less\n./less/ion.less                         <-- build with grunt-webfont\n./less/autocomplete.less                <-- from the autocomple.js project\n./less/toolkit.less\n./less/index.less\n./less/code.less                        <-- copy/paste from oscar theme\n./less/definitions.less\n./less/normalize.less                   <-- MIT license\n./less/style.less\n./less/search.less\n./less/mixins.less\n./less/preferences.less\n./less/stats.less\n./js/searx.js                           <-- concatenation of the files inside searx_src build with grunt\n./js/searx.min.js.map                   <-- built with grunt too\n./js/searx_src\n./js/searx_src/searx_search.js\n./js/searx_src/autocomplete.js          <-- see above\n./js/searx_src/searx_mapresult.js\n./js/searx_src/searx_imageresult.js     <-- from http://trinhtrunganh.com/2014/09/12/google-image-layout/\n./js/searx_src/00_searx_toolkit.js\n./js/searx_src/searx_results.js\n./js/searx_src/searx_keyboard.js    <-- heavily modified version of https://github.com/asciimoo/searx/blob/master/searx/static/plugins/js/vim_hotkeys.js\n./js/searx.min.js                       <-- minified version searx.js\n. Note : searx doesn't need grunt-webfont except at the build time to create the fonts which are commited into the git tree. \nI haven't found a easy way to replace the Created by FontForge 20120731 at Fri May 26 22:22:55 2017\n By alexandre,,,\nby something more appropriate in the SVG. I can give a try to \nhttps://github.com/bdukes/grunt-xmlpoke\nBut the problem still exist with the other font formats : the metadata doesn't contains a reference to the ionicons original font.\n. As I can see the python3-searx package (in sid) contains references to the fonts (ion.ttf, ion.eot, etc..) in searx.min*.css\nIt will create  404 errors on the HTTP server\nWorse it will make the theme not usable (the preference icon is displayed only using this font for instance).\nPossible workaround : use the unmodified ionicons downloaded directly from https://github.com/ionic-team/ionicons (the dependency to grunt-webfont is removed)\nQuick workaround : remove the simple theme.. Q581714 is the wikidata id : animated series\nGetting the name from the id requires an additional HTTP request to wikidata.. @Pofilo it's on the TODO list see : #1025 \nFeel free to make a PR.. may be Qwant/Instant-Answers .... Which theme do you use? Which instance?`. I think this is a problem with virtualenv : when you install pyopensll I guess it's inside a virtualenv. When searx starts as a service, it may use another virtualenv or not at all.\nYou may wish to try the searx package in sid : \nhttps://packages.debian.org/sid/web/searx\n. I can reproduce the bug :\nTraceback (most recent call last):\n  File \"/home/alexandre/code/searx/ve/lib/python2.7/site-packages/flask/app.py\", line 1997, in __call__\n    return self.wsgi_app(environ, start_response)\n  File \"/home/alexandre/code/searx/searx/searx/webapp.py\", line 908, in __call__\n    return self.app(environ, start_response)\n  File \"/home/alexandre/code/searx/ve/lib/python2.7/site-packages/werkzeug/contrib/fixers.py\", line 152, in __call__\n    return self.app(environ, start_response)\n  File \"/home/alexandre/code/searx/ve/lib/python2.7/site-packages/flask/app.py\", line 1985, in wsgi_app\n    response = self.handle_exception(e)\n  File \"/home/alexandre/code/searx/ve/lib/python2.7/site-packages/flask/app.py\", line 1540, in handle_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/home/alexandre/code/searx/ve/lib/python2.7/site-packages/flask/app.py\", line 1982, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/home/alexandre/code/searx/ve/lib/python2.7/site-packages/flask/app.py\", line 1614, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/home/alexandre/code/searx/ve/lib/python2.7/site-packages/flask/app.py\", line 1517, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/home/alexandre/code/searx/ve/lib/python2.7/site-packages/flask/app.py\", line 1612, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/home/alexandre/code/searx/ve/lib/python2.7/site-packages/flask/app.py\", line 1598, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/home/alexandre/code/searx/searx/searx/webapp.py\", line 512, in index\n    result['title'] = highlight_content(escape(result['title'] or u''), search_query.query)\n  File \"/usr/lib/python2.7/cgi.py\", line 1043, in escape\n    s = s.replace(\"&\", \"&amp;\") # Must be done first!\nAttributeError: 'float' object has no attribute 'replace'. Perhaps add some safeguard like in results.py :\n```python\n            if 'title' in result and not isinstance(result['title'], basestring):\n                continue\n        if 'content' in result and not isinstance(result['content'], basestring):\n            continue\n\n        if 'url' in result and not isinstance(result['url'], basestring):\n    continue\n\n```\n?. If the wiki is not enough, I see two solutions to this : \n- Write a small tool where anyone with a github account can add/remove/update a link to searx instances. This tool would be able to ping/check each searx instance. It would provide an API to get the list of searx instances (alive). Each instance could use this API to randomly dispatch search requests.\n- Write a small peer to peer network: searx instances sends messages to each other. It can be a simple broadcast to all know instances every day. The message would be the list of searx instances. To avoid too many messages, the message wouldn't be sent to instances which have been seen recently.. @irule2day , @prolibre , @SinaCutie , @scroom , @Dominion0815 \nCould you check if you are IP(s) are blacklisted : \n http://www.ipvoid.com/ip-blacklist-check/\n https://whatismyipaddress.com/blacklist-check\n?\n. \n@Dominion0815 @scroom , another idea: could you try to use your server IP with a real browser (with Javascript) for a time?\nmost probably using an ssh sock proxy?\nhttps://www.digitalocean.com/community/tutorials/how-to-route-web-traffic-securely-without-a-vpn-using-a-socks-tunnel\n. related to #729. I've to admit it's a little experiment.\nI've a local uncommitted / unpushed change that add autocompletion to a searx command.\nI will post this version here for information. \nAbout the node_modules/.bin path, I've found a command which give the installation path, but I don't remember which one.\n[EDIT] npm root. I suggest to \n1/ replace /bin/sh by /bin/bash\n2/ replace\nsh\nexport PATH=\"$BASE_DIR/node_modules/.bin\":$PATH\nby \nsh\nexport PATH=\"$(npm bin)\":$PATH\nsee https://docs.npmjs.com/cli/bin\n. The unfinished not fully tested version :  https://gist.github.com/dalf/0afe985fd7129cf7aa731eecbf681284\nAs I said it's an experiment.. The safe way to detect sourced file seems to be : \nsh\nsourced=0\nif [ -n \"$ZSH_EVAL_CONTEXT\" ]; then \n  case $ZSH_EVAL_CONTEXT in *:file) sourced=1;; esac\nelif [ -n \"$KSH_VERSION\" ]; then\n  [ \"$(cd $(dirname -- $0) && pwd -P)/$(basename -- $0)\" != \"$(cd $(dirname -- ${.sh.file}) && pwd -P)/$(basename -- ${.sh.file})\" ] && sourced=1\nelif [ -n \"$BASH_VERSION\" ]; then\n  [ \"$0\" != \"$BASH_SOURCE\" ] && sourced=1\nelse # All other shells: examine $0 for known shell binary filenames\n  # Detects `sh` and `dash`; add additional shell filenames as needed.\n  case ${0##*/} in sh|dash) sourced=1;; esac\nfi\n(according to https://stackoverflow.com/questions/2683279/how-to-detect-if-a-script-is-being-sourced ). About readlink on mac os :\nhttps://stackoverflow.com/questions/1055671/how-can-i-get-the-behavior-of-gnus-readlink-f-on-a-mac. Technical detail: \nthe English version is here :\nhttps://github.com/asciimoo/searx/blob/master/searx/templates/common/about.html\nIs Transiflex still a good solution for a long text?\nWould it be better to use Markdown and then convert it to HTML?. There is https://github.com/asciimoo/searx/blob/master/utils/standalone_searx.py\nwhich could be improved.. related to https://github.com/asciimoo/searx/pull/1127\nLet's remove the sourced feature.. The error is related to /home/lonnie/searx/builds/searx-0.14-20180302a/searx/plugins/outstep_sender.py\nThere is no outstep_sender.py in the searx source code.\n If possible (license and all), can you post the content of this file? \n Where and how did you downloaded searx ?\n. Yes, there is a global timeout on each request: if a plugin is slow, there will be more timeout or no result at all.\nThe solution is my opinion is to process the results asynchronously : \n- either with a dedicated thread, and Queue. \n- either using Celery\nBe careful, in production there will be more than one thread processing the searx requests (see uwsgi configuration).\n. @asciimoo right now, all plugins adds new features.\nAdd UX/UI using javascript / css :\n vim\n infinite scroll\n search on category select\n open results in new tab (actually dead code?) \nModify URL :\n OA DOI rewrite\n Tracker remover\n* HTTPS rewrite\nThe call in search.py\nplugins.call(self.ordered_plugin_list, 'on_result', self.request, self, result)\nCould be done in ResultContainer to speed up the process. \n[EDIT] : in search.py the duplicated results are already merged. In ResultContainer they won't.\nOne way to avoid duplicate processing by plugins is to add a cache in ResultContainer : \n```python\nclass ResultContainer(object):\n  def init(self):\n    ...\n    self.processedUrl = dict()\ndef extend(self, engine_name, results):\n    for result in list(results):\n        result['engine'] = engine_name\n        if 'url' in result:\n            result['url'] = self.processUrl(result['url'])\n        # process other URLs (images, etc...) ?\n        ...\n\n'''\n   should be thread safe?\n   is not, the same URL may be processed multiple time\n   (should not be a problem)\n'''\ndef processUrl(url):\n    if url in self.processedUrl\n        # self.ordered_plugin_list and self.request, self.search must be initialized.\n        self.processedUrl['url'] = plugins.call(self.ordered_plugin_list, 'on_result', self.request, self.search, url)\n    return self.processedUrl['url']\n\n```. Some context : \n some time ago, @asciimoo asked me to be the maintainer, unfortunatly I don't have some much time right now.\n at that moment, the idea was to move the project to searx/searx to clearly state someone else to take care of the project.  (Actually https://github.com/searx was used by someone else, but without activity since a very long time, I've asked github support if it was possible to do something)\nCurrent situation : \n all links are targeting asciimoo/searx\n searx/searx is just a mirror I updated from time to time.\nWhat may be possible to do : \n move asciimoo/searx to searx/searx \n people going to asciimoo/searx will be redirect to searx/searx\nIf there is a consensus about that solution, I will the github support (actually I don't know if it is possible to create a redirect from repository to another, anyway a big message in README.md can do the job)\nIf @asciimoo agree to, we could also move filtron and morty to searx/filtron and searx/morty.\n. As a server (pick the instance you want or install yours), there is an API: https://asciimoo.github.io/searx/dev/search_api.html\nIf you want to use it as a standalone program (command line interface rather than HTTP interface) : https://github.com/asciimoo/searx/blob/master/utils/standalone_searx.py\n(JSON output, can be easily hack to the output you want).\n. The requests are sent directly to the different search engines without any additional parsing.\n. One example: \n directly in google\n the same request in searx\nI agree it's require to craft a specific request for each search engine.\nSearx UX could be improved : http://advangle.com/. Thank you for your report !\nI've some difficulties to reproduce the examples you give. \nDid you used the Image category or the General category ? In both cases, I have some results in English.\nWhich instance have you used ? or better, can you provide some links?\nAbout google.cn: it just shows a redirect page to www.google.com.hk (the Hong Kong version).  Google is not allowed in mainland China.\n. sz\u00edvesen\n(I've searched :-) ). The problem is solved if you use the simple theme.\nThe implementation is here :\nhttps://github.com/asciimoo/searx/blob/master/searx/static/themes/simple/js/searx_src/searx_imageresult.js. Technical note to implement this feature in the oscar.\nThis file (searx_imageresult.js) could be moved to searx/static/js and be used by simple and oscar theme without modification (I haven't tried)\nIn simple theme : \njavascript\n    searx.image_thumbnail_layout = new searx.ImageLayout('#urls', '#urls .result-images', 'img.image_thumbnail', 200);\n    searx.image_thumbnail_layout.watch();\nFor oscar theme : \njavascript\n    searx.image_thumbnail_layout = new searx.ImageLayout('#main_results', '#main_results .result-images', 'img.image-thumbnail', 200);\n    searx.image_thumbnail_layout.watch();\n. I don't know if piratebay wikidata page is updated. If yes something similar to\nhttps://gitlab.com/Flockademic/whereisscihub\nCould be used. . https://github.com/dalf/searx-stats2 is not deployed because it is in an unstable state. \nThere is https://stats.searx.xyz/ too but I don't know where the source is.\nSide note : I was thinking about a peer to peer protocol embedded into searx to share the instance list, see https://github.com/asciimoo/searx/issues/1113#issuecomment-352703816. Look at fltron : \nhttps://github.com/asciimoo/searx/issues/543#issuecomment-225305326\nStep by step installation process written in German (not tested, maybe outdated) : \nhttps://scroom.de/eine-eigene-searx-instanz-unter-ubuntu-16-04-aufsetzen/. Some notes : \n In this implementation an administrator can't define its own engine. I think that \"search.engines_yaml\" and \"search.include_engines_yaml\" should be added in settings.yml. The first one to replace engines.yml path, the second to add another engines.yml.\n May be doi.yml, locales.yml, and engines.yml should be moved to the same directory than currencies.json and engines_languages.json ?. Add two new settings : \n search.engines_path : path of engines.yml\n search.include_engines_path : additional engine definitions (doesn't replace the content of engines.yml)\ndoi.yml, engines.yml, locales.yml are loaded directly into doi, engines, locales (not more prefix). Avoid to have settings about engines inside doi.yml for example. \nNote that the DOI settings have changed.\nSome bug fixes on oa_doi_rewrite.\n. I don't think that searx stores personal data, actually that's the whole point of searx: Stateless.\nSo there is no right to be forgotten, no access to personal records, etc...\n[EDIT] Actually we have to ensure that the IPs are not logged.. Thank you for contribution, but the documentation is built using Sphinx.\nSee : \n https://asciimoo.github.io/searx/dev/contribution_guide.html#documentation\n the gh-pages branch, file searx/_sources/dev/install/installation.txt on branch\n. For reference : https://github.com/asciimoo/searx/pull/6. There is no configuration about that, but you can hack the code :\nhttps://github.com/asciimoo/searx/blob/master/searx/webapp.py#L518\nchange 1024 to the value you need to.\nOf course, if one search engine provides a description shorter than 1024 characters, searx won't help.\n. Actually \"22 USD in SEK\" works ! (based on duckduckgo). recipe cookies for example will work in google and with this PR.\nage of barack obama won't be catched by this PR despite the google answer.\nActually a parser must be written for each answer type : recipe, age of..., actor of..., weather... \nSome links outside of the searx scope that may or may not provide a standalone solution aside of searx : \n https://github.com/ayoungprogrammer/nlquery\n https://github.com/facebookresearch/DrQA\n* https://docs.duckduckhack.com/. Side note : there is #933 too. . I'm not sure it will solve anything, but can you try this : \n- replace doi_resolvers = settings['default_doi_resolver']\n- by doi_resolver = settings['default_doi_resolver']\nin searx/plugins/ao_doi_rewrite.py (and keep return doi_resolver ). The comments in settings.yml are misleading, the syntax is : \nyaml\noutgoing: # communication with search engines                                                                                                                                                                     \n    request_timeout : 2.0 # seconds                                                                                                                                                                               \n    useragent_suffix : \"\"                                                                   \n    pool_connections : 100                                                                                                                                         \n    pool_maxsize : 10                                                                                                                                                                                                                                      \n    proxies :\n      http : http://127.0.0.1:8123\n      https: http://127.0.0.1:8123\nproxies has outgoing parent.\nI have check that : \n the port 8123 is used, using wireshark.\n If I stop polipo, searx doesn't work anymore.. As long you don't call manage.sh styles or manage.sh grunt_build you can comment this line.\nI will open a new PR to fix this issue.. The port 80 is not allowed to be used because only root can use ports <1024, and searx runs with a dedicated user inside the docker instance. \nYou should keep using port 8888 inside the docker instance, and map the port 80 from the host to the port 8888 of the docker instance when you start the instance.\nBut the most usual use case is to have different searx docker instance with nginx /apache as a reverse proxy. . Thank you for contribution.\nTravis shows an error :\n/home/travis/build/asciimoo/searx/searx/plugins/oa_doi_rewrite.py:33:23: E222 multiple spaces after operator\n. - duckduckgo seems ok\n- qwant : the API seems to have changed.\nhttps://github.com/asciimoo/searx/blob/master/searx/engines/qwant.py#L31\nshould be replaced by\npython\nurl = 'https://api.qwant.com/api/search/{keyword}?count=10&offset={offset}&f=&{query}&t={keyword}&uiv=4'. Using searx, the Ecosia ads won't be shown which defeat the purpose of Ecosia : \nhttps://en.wikipedia.org/wiki/Ecosia#Search_engine\nsee #880 . It seems okay. \nJust a few notes : \n the wikidata engine sends two requests for each language. The first request contains only the query text not the request language --> the exactly same request is sent many times. \n about infoboxes : count(infoboxes) = count(languages)2\n match_language (in utils.py) modification can be skipped, and replace by \npython\ncurrent_language=match_language(search_query.lang[0],\n(in webapp.py). It makes more sense to me : the choice of which languages is displayed belongs to webapp.py.. What Search Encrypt is describing is the connection between the browser and Search Encrypt servers.\n[EDIT] Forward Secrecy add an additional layer of protection on HTTPS : if someone records the encrypted packets between your browser and Search Encrypt, cracking one connection doesn't help to crack the other connections. Forward secrecy doesn't prevent Search Encrypt to log all requests.\nTo compare, let's see the usual Searx setup :\nBrowser ---[ HTTPS ] --->{ Reverse Proxy ---> Searx }---[ HTTPS ] ---> duckduckgo / google / etc...\nReverse Proxy can be Apache, Nginx, haproxy, nghttp2, etc...\nThe forward secrecy configuration is up to administrator of the Searx instance (the reverse proxy configuration).\nYou can see the Qualys SSL Lab score for each Searx instance here : https://stats.searx.xyz/\nExample with searx.xyz :\nhttps://www.ssllabs.com/ssltest/analyze.html?d=searx.xyz\nThere is Forward Secrecy for all ciphers (In Cipher Suites section, there is \"FS\" for each cipher).\nOf course doing so means searx.xyz is not available to old browsers.\nAbout www.searchencrypt.com : \nhttps://www.ssllabs.com/ssltest/analyze.html?d=www.searchencrypt.com\nSome cipher doesn't support Forward Secrecy.\nBasically if you use a mordern Firefox / Chrome / Safari you will use Forward Secrecy, but you are not sure with old browser.\nSo either : \n you are sure to use Forward Secrecy, if it is not possible you won't be able to access the web site (searx.xyz, searx.me, and some other Searx instances)\n you are most probably using Forward Secrecy if you are using a modern browser ( www.searchencrypt.com, google.com)\nAnother point is the encrypted search queries of SearchEncrypt : \nthe search queries are encrypted using AES and a random key (see https://www.searchencrypt.com/js/search ) \nThe AES parameters are stored in two differents cookies (EncKey and EncToken).\nSo the request is in clear on the wire as any other search engine ( eyedropper can't them since there is HTTPS ).\nSo why encrypting the search queries ?\n no browser history if the cookies are changed : require some tests, I don't know when the keys change, the cookies doesn't seems to expire.\n Search Encrypt server logs won't contains the query : most on the time logs contains the URL not the cookies. Not really helpful since Search Encrypt has the query anyway.\nSearx solves this in different way : it uses the POST HTTP method. \nThe URL remains the same, but the query is sent using the HTTP body of the request.\nThere is glitch about Searx : despite using the POST HTTP method, the HTML title contains the search query, that's mean the browser will contains the different search queries.\nSee https://github.com/asciimoo/searx/issues/1011 (issue solved with the simple theme).\nSome other technical notes about SearchEncrypt :\n web server host by Amazon, running using Microsoft-IIS.\n HTTPS certificate issued by Amazon.\n* The EncKey / EncToken cookies can be used as a user identifier (once again it depends when there are changed).\nMoreover see the privacy notice of SearchEncrypt, section \"What information we collect and how we collect it.\" : \nhttps://www.searchencrypt.com/legal/privacy\nAbout Searx, if you are worried about Forward Secrecy from Searx to the different engines, see https://github.com/asciimoo/searx/pull/931 for some technical details.\nTo sum up it's about who you trust, what is the threat model / what are you afraid of ?\nSide note : I don't know how SearchEncrypt provides results ? does it use another search engine ?. The fact the URL does not change is on purpose : your browser history won't contain the different queries you have made.\nBut you can revert to the usual behaviour : go to the preferences, and change the Method parameter to GET.. This is related to searx.org configuration.\nIf you look here : https://searx.org/opensearch.xml\nyou will see that the URL (template) is not configured correctly.\nMore easy to see, look at the \"Search URL\", it starts with\"False?\" :\nhttps://searx.org/?q=test&advanced_search=on&category_general=on&time_range=&language=en-US\nMost probably related to settings.yml :\n```yaml\n...\n  server:\n       ...\n       base_url : False\n````. The problem is that the URLs provide by baidu.com are not easy to deal with. For example search for \"searx\" in two different browser instance, the first result is searx.me, but two different links are provided : \n http://www.baidu.com/link?url=uioT3Vt5loJ97l-AkmY-SVaYi3rt-tCqIxuEFXsiPCy&wd=&eqid=a71336530002b326000000065be186c7\n http://www.baidu.com/link?url=oCWdPCvh3CHfRdYo1claY4albRnqM5gkR6eWCt0OJIC&wd=&eqid=91f491930002aacc000000065be1873f\nI don't know if this extensions works : \nhttps://chrome.google.com/webstore/detail/%E7%A7%BB%E9%99%A4%E7%99%BE%E5%BA%A6%E6%90%9C%E7%B4%A2%E7%BB%93%E6%9E%9C%E8%B7%B3%E8%BD%AC/polbnpmkonocdfocgiojhfjbdfjdiaic. I've looked at the source of the extension : it doesn't seems to work.\nAnyway sending a GET request to each result is not the solution : it will slow down the search time a lot, and most problably ban the searx instance from baidu. . Have a look to the theme simple.\n. Most probably related to https://github.com/flask-admin/flask-admin/issues/403. Oups sorry... \nthat's not the comment but the code which is wrong : 1x.com doesn't respond on https request (there is a https://secure.1x.com with an invalid certificate)\n. Perhaps, just in case .split('?', 1) or useresult['result']['parsed_url'].query\n. Why not move this line to run.sh ? This would create a secret key  each time the docker image is runed.\n. More for information than anything else : actually google.cn contains only a link to google.com.hk which is not accessible from main land China. So comment could be something like : \"empty web site\".\n. It has been removed to webapp.py only since it is part of the UI not of the search by itself. See webapp.py line 421. Sorry for the late answer.\n. So I basically search / replace request.request.request by request.form ? And remove request.request_data initialization. \n. There is an extra space I guess. HTTP request without TLS : the IP is in clear between the searx instance and ip-api.com server(s). Unfortunately ip-api.com doesn't seem to support HTTPS requests.\n . Seeing\n http://dev.maxmind.com/geoip/legacy/geolite/  (the new format has far less information)\n https://github.com/appliedsec/pygeoip/blob/master/docs/getting-started.rst\nI would say that searx can handle that internally ? \nIt would make sure that the IP is not spread. But when to download the database, when to update ?\nOtherwise there is https://ipinfo.io/ (a comparaison tool : https://www.iplocation.net/ hoping it is fair).. I think a \"s\" in missing :-p. Why a specific behavior for Tor ?. There is no more link to the stats page with this commit.. ",
    "matejc": "Any progress? As we discussed on #searx channel we agreed that new section in engines.cfg named [server], and rename the file to settings.cfg or something... maybe even ~/.searx.cfg\nIdeally I would use something like yaml parser for the config, but that is just me.. currently engines.cfg is parsed by code which you wrote without tests (sorry if I am too direct).\nDo please tell if you need any help.\n. Nice work! Looks very good!\n. have to find other solution...\n. there is no other solution for now, I guess it's a Chromium bug\n. failed.. one moment please..\n. this should do it\n. NixOS has a Nix package manager at its core, with witch you can package virtually anything, pip2nix and pypi2nix are here to generate Nix code for packaging python packages and its dependencies. I packaged the Searx into Nix, but then there where no tools like pip2nix and pypi2nix, or they where not reliable at the time. With Nix you can package any package with whatever dependencies you want, for example you can have multiple Searx versions with different versions of dependencies without interference. Why I didn't packaged it the right way after those tools became available, is partially because I am using Docker management software on my servers and partially because I am lazy.\n. To make this more clear, you do not need to adjust dependencies of any package for Nix, the Searx has to be properly packaged with Nix.\n. Maybe this helps, I have full url BASE_URL=\"https://s.matejc.com/\", opensearch.xml is generated using that variable.. ",
    "romainbe": "Yep, I was non convinced either by what I did. I removed it.\n. Startpage returns google result, but not exquick :\nhttps://support.startpage.com/index.php?/Knowledgebase/Article/View/24/22/i-wont-use-startpage-because-it-returns-google-results-is-there-an-alternative\n. I don't know. I probably screwed at some point with git. Sorry about that. I'll fix this.\n. ",
    "TheRadialActive": "Maybe we could add \"shortcuts\" to these engines, like \"-yt\" for YouTube. So I don't have to look up, how the search engine is named and just use it more user friendly. We could also add a page with a list of possible search engines.\n. (I'm sorry, that so many things I demanded are already in searx. I didn't find any other documentation about it. So maybe the wiki should be used for an introduction to the features of searx.)\n. ",
    "Fironet": "Oh, the shortcuts are activated prefixing !. Couldn't find this mentioned anywhere on the site and had been trying to activate them in various combinations. I rarely use DDG so it didn't occur to me.\n. @kvch Thanks for the link. Problem I found is it's not mentioned on the searx.me site. Neither the About page links to it, nor the Preferences. For one of the main features it's not clear where the user would discover how to use it from looking at the site.\nSomething as simple as prefixing the shortcuts with ! or putting an (i) circle tooltip beside the Shortcuts column, or perhaps in small grey text below the Preferences would be enough to explain it briefly.\n. > I am sorry for your inconvenience. Are images not loading at all or just taking too long? When I tested it is sometimes takes 10-20 seconds depending on the size of the picture.\nIt sometimes loads, sometimes just blank. When you say 'depending on the size of the image' is it now loading the full image in the modal window? As previously it only loaded the larger thumbnail IIRC. This may explain it. I may have just not waited so long as I'd been used to the quicker loading previously.\n\nA close button is being born right now. :)\n\nAppreciated! Though it looks like some kind of reversion is being suggested anyway.. > How did you get this picture? AFAIK there is no picture apart from the searx logo in the index page.\nI'm using the Logicodev theme, which is the default I believe? It's one of only two themes available. The background displays on all SearX pages.. > Do you use anything like userstyles to change its background on you side?\nOh my goodness. For some reason I had forgotten completely about a dark theme userstyle I had installed months ago, which I began to assume was the default for whatever reason (I'd normally remember such things).\nClosing as not a issue.. ",
    "kvch": "This functionality is documented: https://asciimoo.github.io/searx/user/search_syntax.html\n. Closing, see f6e9c074bbe8b4237ee361befa8dcb2c6d31a11a. The last Google service which depended on it was rewritten.. @josch The next release will contain Python3 support. However, I don't know when the next release is coming out. Now @asciimoo releases searx, so the release date depends on him.. @dalf Looks cool. However, it is very similar to DDG. But I am not sure if it is a problem. :). I've just started to work in realizing @dalf's mock.. Closing as it has been inactive. Feel free to reopen if needed.. Documentation is available on the topic: \n https://asciimoo.github.io/searx/dev/search_api.html\n https://asciimoo.github.io/searx/user/search_syntax.html. Closing, as it has been inactive. Feel free to reopen if needed.. Closing as 500px engine rewrite was merged a few months ago.. Closing, see 2f7752b\n. @jibe-b The failure is not caused by older code. The output you pasted here belongs to the manage.sh styles command. It is just a warning, so that command does not fail.\nThe PEP8 check still fails because of your code. There is only one remaining error to fix. It can be found in the build log above the first line you pasted. Please, check again and fix it.\n. I've found a bug while testing the engine locally. When I search for author:Kate, the engine crashes. The reason is quite funny. The error message is the following:\nValueError: year=1847 is before 1900; the datetime strftime() methods require year >= 1900\nSo it means that something has to be done about dates before 1900. I would say that for now, when ValueError is raised publishDate must remain None and the loop should be exited.\n. @jibe-b It is a great idea to add the exception handling to webapp.py. You should put it there. :) It could save us from a few engine crashes. :)\n. It should take None.\n. @jibe-b That should suffice. I also tested the engine. I had no problem with it apart from that ValueError.\nI think @asciimoo going to review your code soon, too. In the mean time could you fixup your patches into 1-2 commits? 42 commits is a lot for an engine. :)\n. @jibe-b  No, you should not make a new pull request. You should rebase your branch and force push it to github.\n. @jibe-b And of course it is going to be less next time. :) You are welcome to add engines in the future. :)\n. I am refactoring the user settings. I think it is going to be ready in a few days. :)\n. Are you sure pulled master properly? The trace shows that the error comes from the line which was deleted in the commit you referenced.\n. Hey! Turn on debug logging as described in the documentation.\nhttps://asciimoo.github.io/searx/dev/quickstart.html#tips-for-debugging-development\n. You have to import urlencode in order to use it.\n. Is it the whole stack trace?\n. What is the content of resp.text? Is it JSON? Is it HTML?\n. Then you should use dom = html.fromstring(resp.text). The function loads is used in case of JSON response.\n. Are you sure that your xpath is right?\n. I tested your XPATH expressions using Firebug. None of those worked. However, I have found a few examples which works for me. You could try these. :)\nTitle: //h3[@class=\"title\"]/a/text()\nURL: //h3[@class=\"title\"]/a/@href\nPubdate: //span[@class=\"feed_item_date\"]/text()\nContent: //div[@class=\"block_content\"]/p\n. If the param search_more is set to all, JS is utilized while retriveving results. In this case XPATH engine is not enough. An independent engine is needed to get the results. :)\n. Closing, as it is inactive. Feel free to reopen if needed.. Nice. :)\nPlease add the new options and its short description to settings.yml. Also please fix ypur code, because the PEP8 check fails.\n. Oh, sorry I haven't noticed you pushed the fixes. :) I think it can be merged. @asciimoo what do you think?\n. I  don't see why preferred format would change. I mean I would always download specific quality of videos and audios. It would only change if I changed the computer I use. Could anyone provide a reasonable usecase when someone would want to download a different format as set in his/her preferences? If there is, I agree with @dalf on the advanced setting option.\nMaybe a warning should be shown when the video is not in the preferred format? And/or users should be asked if the available format should be downloaded?\n. Hey! You can remove btdigg from videos and music by blocking them on the preferences page and blocking them in their categories. Or if you are the admin of the instance and you want to change the global default value, you should simply add the line catergories: files to the config of btdigg.\n. IMHO plugin management the way you describe is unnecessary for searx. I don't think that 5 plugins require such a complex management. I would say that this feature request right now is irrelevant. However, it can be revisited if and when searx has 20-30 plugins.\nATM creating and adding new plugins to the existing plugin \"store\" is easy and does not require black magic. But creating a documentation page for describing plugin creation might be a good idea. (But I am not sure if it is necessary based on the simplicity of creations of plugins.)\n. Invalid and abandoned.\n. Works for me, too. Which instance is the problematic one? Which version does it run on? Could you provide further information on how to reproduce the bug?\n. How and where have you found chameleon theme?\nThis theme is my work, but it is not finished yet. It is in my fork of searx. I haven't created a PR so it is not released. I don't understand why you opened an issue here, when the theme resides at my repo.\n. No problem. Then this issue can be closed because it is invalid.\n. https://en.wikipedia.org/wiki/.io#Controversy\n. I think we should request a mailing list from autistici. First of all it worth a try and nothing happens if they do not provide a mailing list. There are many other providers.\nAlso, I don't think it is really a problem to have the docs under io TLD. I mean it is not the problem if I think along the logic of @a01200356. So Github has the io TLD, not us. So the bigger problem is that the source code of searx is hosted on Github. By hosting the code here we indirectly \"support militarism\", because we use the service which has an io TLD. So I think if the io TLD is a problem for autistici, no project can get a mailing list from them, which hosts its code on Github.\n. Possible duplicate of #284, so I think it can be closed. What do you think?\n. Closing, as the question is answered. Feel free to reopen if needed.. Closing, as it is inactive.. Right now number of results is only an approximation just like on Google or Bing. It is fetched from a few pages (bing, yahoo) and the biggest number gets displayed. In case of common  keywords like internet, it is enough because you will not go through all the 9 million results. However, in case of an uncommon search query when lots of pages provide a few results this can be much smaller than the actual number than displayed. The number of results could be accumulated while the pages are scraped, but I am not sure that it is worth the effort. I agree that it can be disturbing for some, but I don't see what we gain by displaying the exact number. But I am open to reasonable counter-arguments. (Also implementing it would be a tedious task. In order to accumulate all of the number of results ~70 pages has to be scraped or at least check if number of results is displayed.)\nThe number changing behaviour however is a problem. Unfortunately, I was not able to reproduce it.\n. No, the other main focus is hackability, just like stated in the description and also in the How to contribute page of the docs.\n. @jan-kleks I would not say that these features would make searx outstanding. But I agree on that these requests are good and could be valuable features. :)\nAFAIK #537 is in progress. Unfortunately, after the first review phase with @asciimoo the author of the PR has disappeared. :( I am not sure about the other two, but it would be nice to have those features, too.\n. Scrips are in gettext and files were uploaded to Transifex.\n. I agree. Note, so it won't get forgotten, it has to be updated in the docs, too.\n. Oh, cool. :)\n. :+1: \n. :+1: \n. Based on what convention is this the accepted place of these operators? AFAIK PEP8 (https://www.python.org/dev/peps/pep-0008/#should-a-line-break-before-or-after-a-binary-operator) states present form is the accepted version.\n. Just to clear things up, is your problem is that a few search engines (Google, DDG) block your IP and does not return any results?\nIf so, I don't think that making searx.me the central instance for others is the way. I don't like the idea of centralizing searches. If searx.me would be the central instance it would run out of IPs after a while, and would not be able to serve requests just like other instances. So it is not a real solution for the problem.\nAnother type of search could be implemented where the request of the user would go through more instances without centralization, but still I don't think that is the way either.\n. Apart from the question I added inline, I think it is cool. :)\n. Closing, see #654\n. On searx.me it works.\n. Closing, as the question is answered and the user who opened is has diappeared. Feel free to reopen if needed.. @dalf What do you mean by displayed/expanded? Should the categories be shown always without clicking on advanced search?\nWhen I created the UI my main goal was to be user-friendly, not compact. I'va read at a few pages that using \"radiobuttons\" are more user-friendly that using dropdowns. However, nearly all of the other search services use dropdowns to select time range. Those websites have (I suppose) UX teams with great knowledge, so it is wiser to follow their methods. Also, for users it would be much easier to adopt to searx if selecting the time range is similar to the widely-used alternatives. In a few days I'll push the redesign version with dropdowns. :) Thanks for your suggestion. :)\n. I think pd means payload data and pd_name payload data name. So basically the code snippet you pasted here sets the categories and the engines based on the data in the request.\n. @GreenLunar I don't understand your reply. Could you elaborate?\n. I totally agree with @pointhi. Safesearch works right like it should. I think this issue is invalid.\n. Because you have written that \n\nSafe search does not always work.\n\nAs I have written before, it works perfectly. It is not searx's fault that a few search engines \"safe results\" are not safe enough. Searx only proxies the results from other services which implement safe search.\nFurthermore, as @pointhi pointed out\n\na JS implementation wouldn't be able to be used by all users\n\nAnd also the performance and the sanity your initial proposal was not sufficient.\n. TODO (so nothing will be forgotten in conenction with this PR)\n- [ ] refactor XPATH engine so it can support torrent results\n- [ ] let digbt use the refactored XPATH engine\n. I did not refactor the XPATH engine, because I did not want to pollute it with torrent related codes. So I moved digbt into a seperate engine. I did a small refactor: calculation of sizes of torrents were moved to utils to avoid code duplication.\n. @logouthere In case of searx, there is no filter bubble if you use a public instance, because no user/personal data is forwarded to search engines and your queries mix in with others queries. So results cannot be tailored to anyone. In case of private instance there might be filtering, because from your IP you could be profiled (I suppose, but I might be wrong). Also based on ONLY your queries the results cannot be that filtered, because the detailed profiling requires a lot more (like location, interests, personal history, education, friends...).\nBut if I was wrong about profiling, searx still gets results from a wide range of engines and not all of the engines creating profiles. Furthermore, you can search directly using services like wikipedia, github, etc without the filtering/profiling of big search companies.\n. @logouthere Yes, I know that. :) Sorry for not expression myself properly. I was building on @asciimoo's comment and he already told you about the browser data. :)\n. :+1: \n. Hey! Thanks for the docs contrib! However, could you please follow the steps in it (https://asciimoo.github.io/searx/dev/contribution_guide.html#documentation) and commit the other required files along with the html versions?\n. Engines\n\nPlugins\n\n. @PwnArt1st \\o/\n. I don't think that it is the responsibility of searx devs to guarantee that the project runs on a distro but the package maintainers. Thus, I would not merge this PR because it is packaging related. If someone wants to package it he/she has to collect all the dependencies and make sure it installs correctly.\nCurrently, only the documented installations are supported. I don't think that we should consider supporting more methods soon. Present ones are sufficient for most of the users as @dalf pointed out.\n. :+1: for disabling answers except in the first page\nBTW why is the answer shown in every page? Google, Yahoo and Bing show answers only on the first page of the results. Following their practice would make it mush easier to fix this problem.\n. I checked the URL you provided, but there is only one infobox containing two different links to two differen pages. What do I miss?\n. Ah, I see.\n. @dalf sorry, i misunderstood the original issue. :) i fixed the files, and amended the commit. thanks for the review.\n. \\o/ I prefer your solution. So after your fix is merged, this can be closed.\n. It is not implemented see https://github.com/asciimoo/searx/issues/870#issuecomment-286585153. There is a guide on search engies in the docs: https://asciimoo.github.io/searx/dev/engine_overview.html\nHowever, a new engine guide is planned to be written. So if something is not clear in this guide, you could help us identify the problems of the docs. Thus, the new page could be more informative. :)\n. Closing, as it has been inactive. Feel free to reopen if needed.. Closing, as it has been answered and inactive. Feel free to reopen if needed.. What do you mean by not working? Is the problem with the UI, settings.yml or both?. Which languages are not pulled?\nTo me your issue is weird because for example Hungarian translation is 100%, but only 2 words are reviewed. But all of the Hungarian translations are pulled. Based on Hungarian translation, I would say that Transifex is used the way you described.\n. It is already available on searx.me. You just have to click on \"proxied\", instead of the big result url.. AFAIK these results are coming from DuckDuckGo (from definitions). So it is a bug in DDG, not in searx.\n. Closing as old and should have been addressed in Transifex.. @Athemis could you please squash your commits into a single commit? After that it can be merged. :)\nThanks for this cool engine. :)\n. @sixpointzero Yes.. @gszathmari What do you mean? What is GOOGLE_ABUSE_EXEMPTION?. @zwnk have you tried the latest master? there were a few changes since the last release which fixed  known google issues.. It is not implemented see: https://github.com/asciimoo/searx/issues/870#issuecomment-286585153. Could  you please check if gigablast works now?. Unfortunately, it is not possible. However, you could turn on the plugin \"infinite scroll\" so you don't have to care about the number of the results.\n. Closing, as the question is answered and it has been inactive. Feel free to reopen if needed.. No, the intention is not to make users leak more data. This message is shown to tell the user that using javascript more features are accessible (just like on every other pages). Users have the right to know that if JS is enabled, the page looks and acts different.\nI agree with you, it is the users' decision to enable javascript or not.\nBut, it is worth considering to remove on mobiles devices and/or make the warning less prominent on the page.\n. All in all, it is a cool PR. I don't insist on modifying the code based on my notes, because most of it are just nitpicking.\nWhy are supported languages shown only when the user selected a language? To me it is counter-intuitive. I would have expected the checkboxes of engines that have language support to be checked in case of default search language.. AFAIK plugins cannot be configured using settings.yml. A possible workaround is to change default_on to True in infinite_scroll.py but I don't think that is the right solution. Best would be to load plugins like engines are loaded.\n. Well, it work for me. Have you tried restarting searx after modifying the file?\n. It seems that cookie setting fails when autocomplete and/or image proxy is default enabled.\n. Please, update the translations on Transifex: https://www.transifex.com/asciimoo/searx/ :)\n. Cool, thanks. :)\n. Closing as string is updated.. @NIXOYE It is already handled in opensearch.xml. If the user agent is Chrome, it uses /search as the default search url. So POST can work at /. If searx is added to Chrome as it supposed to be done, it should work without any problems.. According to the requests requirements (https://github.com/kennethreitz/requests/blob/master/requirements.txt), version of PySocks must be 1.5.6.. I think that in the first case your searx is unreachable because the containers ports are not published. In the second command -p is specified, so it is made public. Thus, it is reachable from the host.\nAt least that is what I gathered from https://docs.docker.com/engine/reference/builder/#expose.\nSo if you run searx using \nsudo docker run -d --name searx -p 8888:8888 -e IMAGE_PROXY=True -e BASE_URL=https://domain.tld searx\nyour containers 8888 port is published at port 8888 at your host.. Oh, sorry. I pushed it to your repo unintentionally. :(. @VoatSearch Looks promising. However, I am not sure if I had found its source. Is https://github.com/fuerve/voat-search where it resides?. @VoatSearch No, I was just curious if it was open source or not.. Huge :+1: for CCC. :) However, could you squash your commits into a single one? Five commits for an XPATH engine is a lot.. Engines can be used either by using their shortcut or the full name of the engine. For example you can search the new engine both using !c3tv and !ccc-tv.\nI don't think that searx needs that feature right now. Maybe later, when too many engines are introduced.. The name of your instance can be configured in settings.yml using the instance_name variable. Unfortunately, the image is not configurable yet using the config file. However, you could edit templates/oscar/index.html (if you use oscar :)) and change img/searx_logo.png to the path of the image you would like to use. Make sure that the image is under searx dir.\nLittle tutorial on how to build styles of searx after editing less files: https://asciimoo.github.io/searx/dev/quickstart.html#how-to-build-the-source-of-the-oscar-theme\n. Well, it is not configurable yet either. I guess it could be a useful feature. :)\nIn the mean time you could  remove {% if advanced_search %} and {% endif %} from templates/oscar/advanced.html. But these changes might conflict with future patches of searx.. There was already a discussion on the topic in #695. I am sorry for your inconvenience. Are images not loading at all or just taking too long? When I tested it is sometimes takes 10-20 seconds depending on the size of the picture.\nA close button is being born right now. :). As we want to release soon, I vote for reverting the changes. The other options raise lots of questions and answering those questions might take time. Unless the release can be delayed, I see no other way.. Closing, as these UI changes were reverted.. Closing as duplicate of #430, #757, #720.. Just being curious... Why is the legacy theme not sufficient?. \"it\" is short for information technology. :)\nCould you tell me how social media can become SNS? I don't see any connection between the two. . Closing, as the original user who opened it has disappeared. Feel free to reopen if needed.. I have multiple problems with this PR:\n\n\nTravis fails.\n\n\nYou submitted multiple things in a single PR: fixing dependency versions and adding a new feature.\n\n\nYou overcomplicated it. Publishing secret keys could be avoidable by setting it as an environment variable. This solution requires less change in the current behaviour then yours. The result would be the same. Security would not be a problem as you install/run searx as a user. There is no need to change settings.yml, adding a new dependency, generating a secret key. adding new error classes and so on.\n\n\nIntroduction of a new dependency. Not only lengthens the installation of searx, but makes the code harder to maintain, because if the API changes, it should be followed up.\n\n\nSo I would not merge this PR at all, as there is a simpler solutions to this problem.. > That is not a correct.\n\nThere is no change in the practical behaviour, as the config variable is superfluous in any case, since the user cannot influence much by setting it manually. \n\nGenerating a secret key automatically is a huge change compared to the current behaviour, because right now it is done manually by the user. In the solution described above secret key is still set manually by the user, so it requires less change. Your reason is partly true, the line in the settings.yml would become unnecessary, but the user would be able to set it.\nYour solution fails in case of load balanced searx instances. All of the running searx server would have different secret key. So an URL of a proxified image generated by an instance behind LB, could only be served by the instance which generated it. Thus, if you sent the URL to your friend who is served by a different running server, could not open the image, as that server had a different secret key. While having the secret key set in an environment variable or in settings.yml would enable all of the instances to use a single secret key.. No, you should skip the uwsgi section and read the section apache.. @fredhampton My bad. @dalf is completely right. I somehow thought you were referring to nginx. :). I know that I am late to the game. This is a cool feature, however it mustn't be an answerer. Answerers are are offline and don't make any requests to external services. @conmarap could you please implement it as an engine or plugin?. Now that I fixed the bug described by @dalf, I still wouldn't close this issue, as @kkaiser's question is not answered fully yet. But let me answer it. When rss or rss or csv or json button s clicked on, a new search request is made, so it is possible that different engines respond in that case. Thus, the number of results may be different.. Closing, as it is have been inactive. Feel free to reopen if needed.. Yes. There is no redirection, because of security reasons described in #870. . Closing, as the question is answered and issue is inactive. Feel free to reopen if needed.. Is the source available somewhere?. I run searx on a Digital Ocean instance with 512 MB RAM, 1 core, 20 GB SSD disk using nginx and tmux. So far I had no trouble, searx runs flawlessly.. As I can see on the engine page, qwant engine does not support these languages. I wonder if the language support refactor missed qwant.. Yes, but that does not mean necessarily that it is supported by searx. However, in this case it seems to me that these languages are supported by searx, but somehow does not work. . My bad, I checked the wrong part of the code. So the current implementation of qwant engine does not support searching when selected languages are English or French. This could be an enhancement of qwant engine.. Isn't this what searx engines would do in searx? I mean if you add multiple instances to a searx engine. it is possible to search using multiple instances.. > Searx don't make a connection to other searx servers.\n@login2github  What do you mean? If you define an engine which is a type of searx engine, you can set multiple searx instances, like it is written in settings.yml: https://github.com/asciimoo/searx/blob/master/searx/settings.yml#L468\nSearx can make a connection to other searx servers, if you configure it to do it.. Closing, as #873 was merged.. Closing as it is marked solved.. @pietsch Why is turning off either Bing or Yahoo using settings.yml or cookies not an option for you?. Which engine should be disabled by default? Bing or Yahoo?. Redirect is not enabled in these cases, because of security reasons. Users must validate the result and decide if it can be trusted or not. It would be quite a pain if searx redirected to a malicious site which could have been prevented by the user if he/she checked the result. Although, I see that Wikipedia engine can be trusted service, so enabling redirect might be reasonable for this specific engine.. AFAIK quotes are passed to search engines. Thus, if an engine supports this \"advanced\" search, searx supports it, too.. I tried to run again the Travis build to see if it was an environment problem, but the robot tests are still failing. Could you please take a look at it?. It is how searx works, because it is a metasearch engine. Not all of the search engines answer every time for the same query.. Closing, as the question is answered. Feel free to reopen if needed.. The process of updating the documentation can be found here: https://asciimoo.github.io/searx/dev/contribution_guide.html#documentation It would be great if you updated the docs. :). Closing, because the locking takes away the point of loading engines paralelly.. Yes, according to strings in messages.po files. For example \"images\" is translated to Bilden in German and Fotos in German_Germany. Or \"allow\" is translated to Erlauben in German but Zulassen in German_Germany. I don't know what is the difference, because I don't speak German. But I hope it helps.. Which German translation should be kept?. @Siggi0904 I think you can edit translations without joining the maintainers of searx on Transifex.. @Siggi0904 I've just added you as a reviewer. Sorry for the delay. Also, thank you for in advance for taking care of German translation.. I added @Siggi0904 to de, so I would get rif of de_DE and keep de.. Done by @asciimoo . @jodo13 Do you mean Bing or Bing Images? Both of them are seem to work on searx.me.. @jodo13 Thank. :) So far I haven't been able to reproduce it so far nowhere. So yes, it is possible that it is a Bing problem.. Thanks for the improvements. However, translations are maintained on https://www.transifex.com/asciimoo/searx/. Please, correct mistakes there. :). No, review of translation happens on transifex. But translation files are not updated immediately in this repo, after changes are made on transifex. It usually happens before a release or when lots of things are changed in the translations.\nBut if you want to get it merged now, please add the corresponding generated .mo to your PR. If you need help with that, there is a tutorial on pulling and generating bins from translation files: https://asciimoo.github.io/searx/dev/translation.html.. Thank you for updating your PR.\nI will update the documentation based on your feedback.\nI agree with you, transifex can be frustrating to use. However, we are not planing on leaving it, because it makes localization of searx accessible to people who don't know git or pybabel. We don't want to lose those translators. :). I looked into it yesterday and I tried to use the existing mediawiki engine, just like in free software directory. However, I got timeouts every time I tried to query it. @jibe-b have you tried this approach? Or does it have completely different layout from existing mediawikis?. It is possible to retrieve results using GET in the formats you mention. You need to specify format=csv or format=json, etc. Apparently, it is missing from the search API documentation. Thanks for pointing this out. :). Some of the instances limit the amount of possible requests in these formats. It's up the admin of each searx. Personally, I use filtron to limit these requests.. I added the error list to the alert box shown when there is no results.\n\n. @dalf You are right. I will fix it. Thanks for the review. :). How did you get this picture? AFAIK there is no picture apart from the searx logo in the index page.. On my instance https://searx.kvch.me it works. I haven't modified the themes at all. And on searx.me it is working, too. Do you use anything like userstyles to change its background on you side?. Unfortunately, Interlingua is not supported by pybabel, which transforms translations into binary files to be read by babel, so it cannot be added to searx.. Closing because of inactivity. Feel free to reopen.. Please, request it on Transifex, so someone who speaks Persian translate it.. @ahangarha @aurora-potter I can add both of you on Transifex as a reviewer of Persian. Just give me your Transifex user names.. @aurora-potter I added you as a reviewer. Thanks for your help in advance. :). Have you accepted the invitation? I tried adding you again, but Transifex doesn't allow me as you are already added.\nWhat is the language code? I might added you for a different Persian language.. Done. Let me know if it's not working.. Persian language has been released in v0.15.0.. Can you give us an example? I tried different search terms, but to me none of the results seemed inappropriate. Do you want to retrieve \"family-friendly\" content?. I am not sure if it is the fault of the engine or not, but the first few results shows the engine twice.\n\n. Links of the navbar are in searx/templates/oscar/navbar.html.. @dalf I started working on adding a clear button to the search input. I created a css style, but I hasn't been able to compile it, because of dependencies. How did you install clean-css? I tried using npm, but still no success.. Thanks! I was able to compile the styles.. It was added by #1075 . Does it comment everytime someone opens a pr?. All in all I like the idea. It will be a great addition to the existing science engines. After my notes are address, I will be glad to merge it.. What do you mean by updating them?\nDo you mean that templates_path and static_path have to be a list, so users can specify multiple custom themes? If yes, then I like the idea. However, I am not sure that it would be a really useful feature. Simply copying themes into the same directory does not seem like a big effort. So I don't think that the developer effort would worth it in the end.. I like your idea, but I have a few concerns regarding the authentication and API keys.\nDoes CORE worth it to leak that you are using a searx instance for searching?\nHow would the API keys be distributed? Would we have one for all of the instances or a key for every instance? If one is available for all of the instances, the key would be leaked, so their authentication would be useless. However, if every instance has one key, the instance is leaked to CORE. Also it raises more questions on distributing the keys.. @jibe-b Are you planning to finish your PRs? All engines seem valuable, so if not, I can take over.. Why isn't it implemented as a JSON engine like Crossref or Hoogle? If it can be done, please add it as a JSON engine to settings.yml.. Great engine! After the py3 compatibility issue is fixed, it can be merged.. Do you want to annotate the results or just showing your existing annotations?. To clear things up does displaying annotation require authentication? I suspect not.\nHow do you imagine the UI?. Are you sure you saved your preferences before saving the search URL?. Param q holds you search term and %s is a placeholder which is replaced by your term. So if you search for \"cars\", it will pass {your-other-search-params}&q=cars to searx.. I tried reproducing your issue with a local YaCy instance. But everything works as expected. Which instance are you using?. @michaelschefczyk thanks for sharing the URL of your instance.. Could you please rebase your branch?. Serbian language has been added. Feel free to translate.. Closing as dark version of oscar is available. Feel free to reopen if needed.. We don't want to have a list of searx instances in git. See in https://github.com/asciimoo/searx/issues/1113#issuecomment-351151611\nI think it is an acceptable compromise between the two requirements.. The problem must be around this area:\n```jinja\n{% for result in results %}\n\n    {% set index = loop.index %}\n    {% if result.template %}\n        {% include get_result_template('oscar', result['template']) %}\n    {% else %}\n        {% include 'oscar/result_templates/default.html' %}\n    {% endif %}\n\n{% endfor %}\n```. Yes, sorry about that. It was just a note for myself, because I thought that I would took another look on a Windows machine in a few days. Unfortunately, I had no time so far.. Does it happen in case of video or torrent results, too?. I tested your example using Google and it returns \n\nNo results found for \"Error invalid CA certificate getting chain\".\n\nBelow this message it shows the results for the query without quotes.\nThe correct query is sent to search services including \". But if there are no results, these services usually return results for the unquoted query. That is why you see those words scattered about in the results.. In engines which parse published date, all of them are shown in the same format under the title of the result. See: !news mama luigi\nYour example is part of the content of a result. In those cases where no date is parsed, only passed from the source service, it is possible to have differences. The format could be controlled if those engines parsed the dates of the results. . Preferences are stored in cookies if you save them. After the save a search URL is generated of those preferences. So if you want to, you can save that URL in your browser and search using that URL.. Have you clicked on the search button? Time range filters only work after you push that button.. I am able to reproduce it. I just did not know that searx had this feature. :D . But according to git history I am the person I added time range search to the script which enables search on selecting something from a search drop down.. @knyfe I put those two options under server without http in nginx and that solved the issue. Also, updated my filtron to handle bigger headers.\n```conf\nserver {\n    ssl on;\n/* my config */\n\nlocation / {\n        proxy_set_header        Host    $http_host;\n        /* more config */\n}\nproxy_buffer_size   128k;\nproxy_buffers   4 256k;\nproxy_busy_buffers_size   256k;\n\n}\n```. I am not a big fan of having the list of instances in the repo. This list tend to change a lot and some of it is not even up to date. Also, it has minimal connection to the source code.\nAdditionally, I would not want to add more overhead to adding a new instance by opening and merging PRs. Not only it is unnecessary, but it might lead to the false notion that those searx instances are somehow validated or trusted by the maintainers of searx. Which is not true, because there is no way we can guarantee anything regarding public instances (apart from instances run by us).\nI am not sure you know about it, but there is an existing searx aggregator available: https://searxes.danwin1210.me/\nAs a sidenote, I don't see how parsing that wiki page is not convenient. It's true, it's not in JSON format. But it has a predictable and uniform layout which can be scraped easily.. Thanks for your understanding.\nOriginal version of http://stats.searx.oe5tpo.com/: https://github.com/pointhi/searx_stats\nThere is an updated version with similar functionality: https://github.com/dalf/searx-stats2\nLooking forward to see your tool!. @prolibre From the wiki, you can delete anytime anything. I am not sure how to delete instance from searx-stats. I guess it has its own database, so only @pointhi can delete those instances. . Closing as translation fixes are coming from Transifex.. On client side it is not possible. \nIn case of local/own instances, in settings.yml weight can be assigned to each engine. Each result has a score which is multiplied by the weight of the engine. The default weight is 1.0. If it is changed to a different value for engines, order of the results might change as you described. But it is not guaranteed that results from engines would be in this order, because if google is set to 2.0 and bing is set to 1.0, and a result from bing has a score of 3.0 and the result from google is 1.0, the result from google would be after the result from bing. . Just curiosity: which character causes the problem?. Does it mean that you agree with my proposed changes?\nWhich searx command do you mean?\nHaving a command to find the installation path would be nice.. I rebased and updated the PR with changing echo to printf as suggested by https://github.com/asciimoo/searx/issues/1206#issue-298593155.. Does it show up for other search querys, too? I haven't been able to reproduce the issue with several instances and searching for a few times.. Thanks for your contribution! I added Chinese (Taiwan) to languages in Transifex.\nCould you please submit your translation there? https://www.transifex.com/asciimoo/searx/language/zh_TW/\nAfter it is finished, one of searx maintainers will pull it from Transifex in the future.. @RedSoxFan04 What does not work when you try it? If you mean redirecting to the page by typeing !!{enginename}, it's because this feature is not implemented.. Yes, indeed. Unfortunately, it's not maintained. If you feel like it, you can update it. :). GitHub is the main platform for development and reporting bugs. If you have any feedback for the project, you should open an issue.\nThere is a mailing list for instance admins: searx-instances@autistici.org. I meant  that this platforms is intended for the development of searx including sharing ideas.\nThe mailing list is for admins who run searx instances. The list is not for development details, but for operational topics.. Google images is working for me. Could you provide more details on how to reproduce the issue? E. g. query, instance name, etc.. Fix was released in the latest version.. Translation can be done from your browser. However, I don't think the text is translatable now.. I have added Filipino to the language list. You can start translating here: https://www.transifex.com/asciimoo/searx/language/fil/\nThanks for translating searx.. Cebuano is not supported by Transifex. Or at least I haven't been able to add it to the languages.\nTagalog is available: https://www.transifex.com/asciimoo/searx/language/tl/\nIs it enough?. Filipino is added as is from Transifex in the new release.. It means that Google refuses to send results until you resolve the CAPTCHA it sends you.\nSometimes it disappeares without any action.. If you are adding only a search engine, you need to edit files under engines and add it in settings.yml.\nThere is no need to edit anything else.\nShortcuts can be whatever letters you come up with. The only requirement is it has to be unique. It is advised to be short and easy to remember.\nIs there anything I can help you with?. There is laconic documentation on plugins: https://asciimoo.github.io/searx/dev/plugins.html. The templating language used by searx is Jinja2. It dumping its variables doesn't seem straightforward.\nYou could try adding print(kwargs) to this line: https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L390\nIt would print all variables passed to render a page.. #1127 is merged, the issues should be fixed on master. What problems did you face during update?. Thanks for the report. :) Please, correct the wiki page.. @Yetangitu What are the dependencies of the webui? I am trying to start it, but it fails with ImportError: No module named recoll. However, I have installed python-recoll using my package manager.. Can you still reproduce the issue? I tried multiple queries, but all work as expected.\nIt's possible that \"Last year\" is not working on searx, because the engine does not support \"last year\". For example, DuckDuckGo.. s\u0259\u02d0ks. I accepted your language request on Transifex. Do you mind submitting the translation there?. No, currently there is no way to do that.. Flask provides a development server which is used by searx in run: https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L878\nHow the dev server works: http://flask.pocoo.org/docs/0.12/server/#in-code. Yes. Does it crash again?. Have you pulled the latest changes?. Yes, I see now. It works locally, but on https://searx.kvch.me it's deployed and I get the same message.. What was your query? I tried reproducing the error with multiple queries on the instances you mentioned and also on mine and locally. But I wasn't successful.. I tried again today, even from behind a proxy, but it works every time both a local instance and https://searx.kvch.me What is the error message you get for qwant?. Could you please fix the link in the wiki page?. Could you share an example query which demonstrates this addition? So far I haven't been able to see anything besides regular results from Google.. Resolved by @MarcAbonce's PR and @dalf's PRs.. I cannot reproduce this with my local yacy instance. What is the error you see?\nI had to wait quite long to get results. I set engine timeout to 25.0, so I could get the same results as yacy shows. But for single word searches the default 3.0 is enough.. It's not a crash. This error is coming from filtron, because the rate limits have been reached.\nIn time these errors disappear if no one is abusing the service.\nAlso, the web interface works for me without problems. What are you trying to do which returns this problem?. Closing because of inactivity. Feel free to reopen it.. No, they are not removed from the engines.\nArchive.is seems to work for me with the following queries: \"asciimoo.github.io\" and \"fsf.org\".\nEtymonline doesn't work for me either.. Thanks for the contribution. We use British English, so the \"r\" is silent. See: https://github.com/asciimoo/searx/issues/1264\nThe page on Wikipedia has to be fixed. . Thanks for your contribution! Could you please address the issues detected by PEP8?. subtitleseeker.com is seems to be gone. Is there any viable alternative to that site?. Do you have an exact error which shows up when running with Python 3?\nIn v0.12.0 we added support for Python 3 and stayed compatible with Python 2.. Thank you for figuring it out!. @tmikaeld do you mind opening a PR with that change to branch gh-pages? :). @slackman76 what was your query? have you experienced this problem with the same query on a different instance?. Which engine returns these results?. In the long run I would like to support more result templates in XPATH and JSON engines, so such services which return images in JSON responses, can be added to settings.yml without introducing a new Python engine. But right now there is no support for that, so I am taking in the PR as is now.. Do you mind adding \"Closes #1403\" to the description of the PR?. I will try to pick up the missing PRs and create a new release this weekend. Sorry for keeping you waiting.. Thank you for your contribution. I have tested the branch locally. Result thumbnails are not loaded.\n\nDo you mind checking this problem?\n. Sorry for the delay. I tested your branch and I get the following error message when I use the example search query you mentioned.\nERROR:searx.search:engine google : exception : global name 'language_aliases' is not defined\nTraceback (most recent call last):\n  File \"/home/n/p/searx/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/home/n/p/searx/searx/search.py\", line 73, in search_one_request\n    engine.request(query, request_params)\n  File \"/home/n/p/searx/searx/engines/google.py\", line 169, in request\n    language = match_language(params['language'], supported_languages, language_aliases)\nNameError: global name 'language_aliases' is not defined. Thank you!. Thank you for your contribution and sorry for the delay.\nI tested the engine using multiple simple terms, but the following error happens:\nERROR:searx.search:engine gitea : exception : No JSON object could be decoded\nTraceback (most recent call last):\n  File \"/home/n/p/searx/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/home/n/p/searx/searx/search.py\", line 87, in search_one_request\n    return engine.response(response)\n  File \"/home/n/p/searx/searx/engines/gitea.py\", line 33, in response\n    search_res = loads(resp.text)\n  File \"/usr/lib/python2.7/json/__init__.py\", line 339, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python2.7/json/decoder.py\", line 364, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python2.7/json/decoder.py\", line 382, in raw_decode\n    raise ValueError(\"No JSON object could be decoded\")\nValueError: No JSON object could be decoded. #1475 . I am all for dropping Python 2 support. Our main intention was to keep as many users happy as possible.. It is used in two places:\n1. Flask, the web framework we use, requires a secret key for session handling. See more: http://flask.pocoo.org/docs/1.0/quickstart/#sessions\n2. When image proxy is enabled, the instance downloads images for you from the search service. To authenticate the request (to make sure that the request is coming from the UI of the instance) a HMAC is generated using the secret key.. Closing due to lack of response.. Thank you for the contribution!\nNext time please add a keyword \"closes\" or whatever you like from this list: https://help.github.com/articles/closing-issues-using-keywords/\nThis way Github closes the referenced issue when the PR is merged.. Thank you!. Could you please share an example search query which leads to this exception? I have failed to come up with one.. Only @asciimoo can release a pip package.. @asciimoo has just released the package: https://pypi.org/project/searx/. I've tested the engine. The error does not occur anymore. However, no result is returned.. I retract what I've just written. Now it works.. @DD8372812 I've just merged the PR which adds support for setting proxy configuration in Docker images: https://github.com/asciimoo/searx/pull/1485. Thank you!. I'm sorry, I am not sure I follow you. What is the issue you see? Or is it an enhancement request?\nRight now searx notifies the user when no engine has found anything. Do you want to get notified if nothing is returned by engine?. @Freeland3r Feel free to open a PR to the branch gh-pages if anything is missing from the installation docs.. @micressor The issue you are experiencing is a pip problem according to GH.\nhttps://gist.github.com/dmulter/38330962002d28533d7dd7c1a70ee4f5\nI am looking into it right now.. 500px engine was removed from searx a few months ago: https://github.com/asciimoo/searx/issues/1338#issuecomment-414126635\nTheir API is no longer accessible: https://support.500px.com/hc/en-us/articles/360002435653-API-\nI was not able to reproduce the issues you have seen with Bing images and Google images. In case of Google I get a different exception. Could you please share an example search query which lead to this issue? Also, what's the version of the instance you are using?. Closing as master is buildable again after a fix has been released.. Thank you!. I have tested the branch with the package updates. Unfortunately, jinja update does not help.\nIt seems like the UI cannot show the translated version of \"Change how forms are submited, <a href=\"http://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods\" rel=\"external\">learn more about request methods</a>\". I am still looking into the issue.. The translation seems to be incorrect. \nThe following URL\nhttps://zh.wikipedia.org/wiki/%E8%B6%85%E6%96%87%E6%9C%AC%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE#%E8%AF%B7%E6%B1%82%E6%96%B9%E6%B3%95\nshould be\nhttps://zh.wikipedia.org/wiki/\u8d85\u6587\u672c\u4f20\u8f93\u534f\u8bae#\u8bf7\u6c42\u65b9\u6cd5\nin the translation.. I updated it on Transifex.. Great contribution! However, do as I see the responses are in JSON format. Do you mind refactoring it as a JSON engine? Here is an example: https://github.com/asciimoo/searx/blob/master/searx/settings.yml#L424. Thank you!. Could you please add the engine to settings.yml and disable it by default?. Values of parse must be the keys of the results dict. In this case it is url, title, publishedDate, content.\n. This format of None-checking does not comply with PEP8. It should be if publishedDate is not None:. For further information on that, please check programming recommendations section of PEP8.\n. publishedDate must be initialized before checking if it is none. If an exception is thrown from the try block, it remains uninitialized, so the None-checking fails and the engine craches.\n. In this loop you are checking the value of only the name key in all of the ifs. Using elif would be much better because this code would be skipped if one of the conditions were true. The same applies to the next if below this one. In the last two cases using 'elifis the appropiate choice. :)\n. Please, indent the comments just like the code.\n. Could you write new shortcuts for the advanced search? These words could cause problems. For example if I want to searcch for the termcat titlethe query would be transformet tocat dctitle`. That is not what I wanted to search for.\nI checked the shortcuts at BASE. It seems to me that it has completelydifferent advanced search syntax. For instance if you want to search for something that is titled French, you would enter tit:French. To not to confuse experienced BASE users, I suppose the shortcut keys should be the same as at BASE.\n. Furthermore, you should move this definition out of this fuction. This dict is defined everytime someone makes a request. If you move it outside where the base_url is found, it would be only initialized at startup. This change would spare us time. :)\n. @jibe-b what do you mean by this going further than you proposed to add to searx? I think it is enough to change the keys in the dict to tit:, aut: and so on like described in the Limit search area section of BASE help. Extra API params could be parameterized using keywords you/we come up with. Or maybe we do not need all of the params provided by the BASE API.\nWhat do you think @asciimoo?\n. This indentation is still wrong.\n. It seems to me that only this line could throw the ValueError that should be caught. The lines above this should be put inside an else clause of the try statement.\n. I tried your PR with a locale that is only one long. It did not work as you had written here. If DDG requires both country and language why did you include the else? Is it possible that there will be locales which match the format DDG requires without any transformation?\n. Why is searx_useragent is called here? If you set it, you leak that information that a searx instance made the request. If the point was to hide the user agent of the user, it is unnecessary, because searx generates a random user agent when a request is sent.\nThis function was introduced to authenticate for BASE engine. Doest it use the same auth method as BASE does?\n. Why not simply 'q': query,?\n. skip_result is unnecessary in this function. If you want to skip the result use continue to skip this cycle of the loop.\n. Without skip_result:\nif result['status'] in pdb_unpublished_codes:\n    continue\n. hide_obsolete can be moved out from this if.\nif hide_obsolete:\n   continue\nif result['status'] == 'OBS':\n    # process result...\nelse:\n    # process result...\nThis way else: skip_result = True is not needed anymore.\n. This if is not required either, because when continue is used, processing never gets here if the result has to be skipped.\n. Could you make this condition less complex? For example you could create lists like invalid_codes = ['sl-SL', 'wt-WT', 'jw'] and use those in the condition, instead of enumerating all invalid codes.. It could be extracted into a function named filter_dialect. Thus, the comment becomes unnecessary. (But I don't insist.). IMHO using the with context is much cleaner.. These concatenations are uncessary until line 146. It is sufficient to initialize file_content with a multiline string containing these lines.. Why don't you import urlencode from searx.url_utils just like in other engines?. Why is this list here if it is not used anywhere?. Is it required beacuse of py2/py3 compatibility? If yes, please move it to url_utils, as it is where all proper URL functions are imported based on Python version.. Could you make the optional stuff configurable? So instead of commenting lines out, there could be a boolean which is False by default and can be configured from here or from settings.yml. This way the code can be kept cleaner, and those admins who don't like touching code can set it from config file. Also, if something changes in these lines and an admin who enabled this part has to deal with conflicts, too.. Could you remove these lines? It makes reading test code harder.. Could you please use format function to concatenate strings?. Please use gettext, so this line can be translated.. Where does oa_first option is used in the engine?. How about using etree.fromstring from lxml? In autocomplete.py that function is used without any compatibility checking.. Please use format to concatenate.. Please remove unnecessary comments.. Please disable this engine by default as it's done in case of library reference.. I would urlencode variable dir, too. If someone has weird directory names, it shouldn't break the query.\nTo me it seems like this URL is incorrect. If the URL encoded query is substituted into this format string, the following is returned: https://my.recoll.me/json?query=q=mytestquery&page=1&after=&dir=&hightlight=. The part query=q=mytestquery doesn't seem right.\nAlso, as a note, I would urlencode everything. It's up to you if you change it or not.\npython\nsearch_url = base_url + 'json?{query}&highlight=0'\nsearch_url.format(query=urlencode({\n    'q': query,\n    'page': params['pageno'],\n    'after': search_after,\n    'dir': search_dir,\n})). Could you comment out these two engines? Engines which require further settings by users should be commented out, so searx can start up using the default config.. My vim shows unnecessary whitespaces here. Could you please remove those?. Why not have a shorter shortcut? \"du\" is not yet taken.. Engines which require action (installation, setting base_url) before being usable are usually commented out. See yacy as an example. Could you please comment it out?. Why is this needed? AFAIK originalData is not used anywhere.. Users can set search_url from their settings.yml, too.. This shortcut is already taken by Geektimes engine.. Do you mind removing the debug logs?. Please delete the lines. In the future it would be nice to have a debug logger where you can select which component you would like to see debug logs from. But in the meantime I would prefer the engines does not have debug logging.. Please do not add engine specific variables to general, more abstract classes. Why is this needed? If you need to query some random value on startup in order to perform a search, you should take a look at the engine Soundcloud: https://github.com/asciimoo/searx/blob/master/searx/engines/soundcloud.py#L67\nIf not, please share the idea behind this architecture.. I am not a huge Docker expert, but I think ENV HTTP_PROXY_URL and ENV HTTPS_PROXY_URL lines are missing from the file.. Yes. You are right, this is teh only solution to persist the data temporarily. However, I am still not happy with adding engine specific attributes to a common resource among those.\nIt would be nice to \"generalize\" the functionality, so all engines can add their own attributes. What do you think about maybe adding a new attribute to the ResultContainer named engine_attributes which is a dict containing the key value pairs by engines? Then it would be persisted in the HTML as you have implemented it.. Thank you.. ",
    "pointhi": "I have used https://searx.netzspielplatz.de/, and on this site the error occure.\n. probably a configuration failure, not it work\n. I have seen that in your \"readme\":\nQueries are made using a POST request on every browser (except chrome*). Therefore they show up in neither our logs\nThat is a true objection. I have the following idea: Adding a button to export the current querry string, and adding an option, to show the current querry after every search request in the url-bar if desired. That is secure and user friendly.\n. In the last commit, I have added a placeholder with the text \"Search for...\". It would be fine adding it in transifex.\nThe location of the .less files at the moment is also not the best. I don't know enough about the file-organisation, but moving it to a other location would be a good idea. Probably a /less directory in /static, but the files are only required to generate the css,...\n. I found out that I have add the following strings in the en translation-file, and not in the template:\n- \"{minutes} minute(s) ago\"\n- \"{hours} hour(s), {minutes} minute(s) ago\"\n. I would make an initial implementation for the front part of autocompleter (at beginning, with direct connection to google), to test how a auto completer could work bandwidth and user friendly.\n. I have programmed a first version of the front part, and a minimum backend-part:\nhttps://github.com/pointhi/searx/commits/autocompleter\npointhi/searx@SHA: b5c695c6f525eb692d3e476817b49d2661ba2d25\npointhi/searx@SHA: ecd293c8a90f0adef5f3e07f8c99c94a3beae63a\npointhi/searx@SHA: 8abf4ab993cd4a81dc97f85fb8a30a6c875221f7\npointhi/searx@SHA: e181fd8f56fbc29f01aa6efa0d2de78c5d533166\npointhi/searx@SHA: 360543dec4b652950c67a7f1cc4027ee1b920a30\npointhi/searx@SHA: cc7f3cb61798463036a886ae5f0ccd06aca5e625\nI have used mootools and mootools-autocompleter to do this:\nhttp://digitarald.de/project/autocompleter/1-1/showcase/request-json/\ncurrently, searx return the wrong data because only the initial part of the backend is written, which return \"random\" data.\nExample\nThe Post-Request for an autocompleter call:\nformat          json\nq               abcd\nAnd an example json-answere:\njson\n[\"abcd\",\"abcde\",\"abcdef\",\"abcdefg\",\"abcdabc\"]\nFor the Backend, I need help because I'm not so good in Python.\n. I have created a Pull request #58 \n. I have added the package python-dateutil in requirements.txt, installing the requirements with pip install -r requirements.txt, cleaning the project with make cleane and build it with make.\nThe error still occour. I'm a complete beginner in python, so i don't know what can help.\n. I have added it to setup.py, now it work\n. I would note, that you have added edits, which look like private changes. For example, edit port and secret key in settings.yml and deleting the following html-code:\nhtml\n<div class=\"title\"><h1>searx</h1></div>\n. Ok, I have a few ideas about the implementation of the widget system:\n1. The widgets are similar to the engines, but seperated from them inside config, api,...\n2. If a search starts, every widget is processed in an seperated thread (if a widget request external data, it has not to wait for success)\n3. there is an api, which provide single access to every widget over the web. As example, an translation widget can request new translations without starting a new search request.\n4. If an widget is using web-sources, it has to do the calling itself (like search.py do)\n5. a widget can print normal results, infobox results or special widget results (special templates)\n6. there is an template.py or similar in every template main-directory, which say what special widgets can be displayed (and other template relevant informations like autor, description,...), because I think we cannot handle the different widgets with a few standard-templates like default.html, images.html.\n7. Widgets can be enabled/disabled in the preferences\n8. Widgets can be forced like the bang! functionality\nThis are a few ideas, which have to be improved and discuss before starting the implementation.\n. Ok, I doesn't know about that because I have used my oscar template all the time, and doesn't notice that ddd support that.\nI have added the answer functionality into my oscar template, but I think it is very rudimentary.\nI'm dreaming of intelligent widgets, like the calculator in google. But for that, every widget must have the possibility to display specific html, js for its requirement.\nProbably, there can be a fallback to an simple version with an simple default template, for templates with doesn't support the advanced widgets. But it should be possible to add interactive widgets, or special stylings.\nAdditional:\nsearch querys like qrcode asdf return gabage in the current implementation. My idea was same as it is implemented in duckduckgo. But I think it is probably better implementing the instant answers by ourselves.\nPRO:\n- we are independent of duckduckgo\n- we can adjust the widgets for our needs, including additional information or using metasearch to provide better answers\n- no confusing answers because searx cannot handle the results from ddd correctly\nCON:\n- implementation effort\n- code overhead?\nI have written down some widget ideas: https://github.com/pointhi/searx/wiki/widgets-ideas, and most of them are not a big problem.\n. ok, using the plugins is an idea. There is a project named PyPerl, but it is not maintained yet: https://wiki.python.org/moin/PyPerl.\nAs example, I have using https everywere rules inside searx, which is only working because of a few hacks, which are rewrite the js-regex to a python-regex. It work somehow, but is not very elegant. And I'm sure some regex not working with that hack yet.\nIf using the perl modules is like the current https code, I recommend rewriting. If we can implement it without problems, using it directly would be an idea. Probably making a python widget which does that stuff.\n. @asciimoo it would be greate if offline engines support is working soon.\nMy implementation ideas:\n- probably we can create an option offline=true and if this option is chosen only one function is calld\n- or we define url=None and calling dectecting offline-functionality in this way\nFurthermore, I think we should think about an master template for widegets. It would't be handable if we create a new .html-file for every wideget which is showing things a little bit different. Nothing which can handle all cases, but most.\n. I have activated it in apache2, but HSTS was not in the header-data of the website. With normal php-files it work correctly\n. I tried it, but it doesn't worked for me. I'm using uwsgi with the version: 1.2.3-debian. Probably it is not supported in this version.\n. Sorry, but the configuration:\nHeader always set Strict-Transport-Security \"max-age=15768000\"\nas well as\nHeader always add Strict-Transport-Security \"max-age=15768000\"\ndoesn't work.\nExplanation:\nhttps://stackoverflow.com/questions/25781833/can-mod-headers-change-headers-generated-by-uwsgi\n. Possible solution, add response header inside searx:\nhttps://stackoverflow.com/questions/25860304/how-do-i-set-response-headers-in-flask\n. my instance is already forced to https using apache. I want to use HSTS mainly to secure my instance again downgrade attacks.\n. I have made the initial code. It can display a big osm-map, show a menue to start routing (not working yet),...:\nhttps://github.com/pointhi/searx/commit/a9b326afcbafc7849b705a44822a8e9d77d0670d\nhttps://github.com/pointhi/searx/tree/map\nFurthermore, I would implement little maps to show geo-results (with links to the big map).\nThe only big problem for me is, where can I place the link to the map and return to normal search.\n. On which system do you work?\nI have installed searx on my laptop and on a v-server with 4 cores, and on both it work very well. I have used searx in connection with apache, not nginx, but this should make no difference.\nYou must understand that searx is a metasearchengine, so if the instance has a slow internet-connection it would work very slow. (First, the connections to the search-sites, and then the connection to the client)\nI don't know how much perfomance searx need, but I think it is very low because you only call different search-sites, refactor the results and send it back to the client. A raspberry pi should suffice, but this is only a presumption. In my case searx doesn't need much cpu-load.\npointhi\n. I find direct playing or streaming through a proxy is the best. writing videos to harddisk and streame back from it, is not very good (require harddisk space, lower streaming speed, delay at beginning of stream, full video download for only 1 view or only a part of the video)\n. I have made an inital implementation of the embedded video-player:\nhttps://github.com/pointhi/searx/tree/video_player\nThere are some little bugs, so I hope anyone can help me:\nflask (jinja2) escape my code wrong:\nthis is how I write it in the template:\njs\ndata-setup='{ \"techOrder\": [\"youtube\"], \"src\": \"{{ result.url }}\" }'\nthis is how I get it:\njs\ndata-setup=\"{ &quot;techOrder&quot;: [&quot;youtube&quot;], &quot;src&quot;: &quot;http://my_result_url&quot; }\"\nthis is how I would get it:\njs\ndata-setup='{ \"techOrder\": [\"youtube\"], \"src\": \"http://my_result_url\" }'\nI have test different ideas how to fix that, but without success.\nOther Bugs\n\nThe Browser slows down during loading the video-player (can be probably fixed if I the video-player is loaded last or loading after clicking on the thumbnail)\ndisplaying results from dailymotion break some video-results (probably connected with the template-bug)\nvimeo-player is displayed instead of default-video.js-player (bug, or the vimeo-plugin does it by default)\nloading of required js-libraries is not very elegant in my current commit\n. I have developed a little site which is doing that:\nhttp://stats.searx.oe5tpo.com/\n\ncode can be found on github:\nhttps://github.com/pointhi/searx_stats\n. Parsing all https rewrite rules at startup require a few seconds (4seconds on my laptop)\nusing https_rewrite rules to parse search results, result in a delay of 1-10ms per result.\nI think the best is deactivating https rewrite for default.\nFurthermore, I have worked on algorithm to minimize the querry, but the performance go worser due it. But I found out that the performance were improved if we reverse the url and the regex (be care about special regex words), the performance could be improved by 10% or more, because many rules have a wildcard on the beginning of rule, which is difficult for the regex parser.\nA other idea to improve performance is building a single tree of characters of the urls including a link to its special rules, but this would be more complicated.\n. this is also a good idea, 100+ regex rules are easier to handle as 10.000 rules\n. I have rebased the complete code, fix a bad bug and added the rules of a few websites. Now, the code should be ready.\n. I would recommend the following improvements:\n- ~~wikidata can show only 1. result (the 2., 3., ... result is mostly gabage)~~\n- if the result doesn't contain text and data, but a wikipedia link, using normal result template\n- language support\n- ~~probably check if search query match wikidata-name approximately~~\n. ~~sorry, I have imported some commits from @dalf, I hope I can fix it~~\nOk, fixed that problem with a few git hacks :)\n. I hope this template would be merged in the future :). Because of that, can I have a list of merge-blocker, to fix the problems.\nthx, pointhi\n. that is clear.\nI have added some extra-features to this template, but they are going in another pull request later.\na in development feature is as example osm-map-support: https://searx.oe5tpo.com/?q=otelo%2C%20austria&pageno=1&category_map\nthx, pointhi\n. I know, but I would make it for users accessible, but only for the users which force it, because yacy slow down searx.\n. yes\n- it is minimizing the images to a defined size\n- caching the results for 1hour or so\nmy problem is:\n- it is working well on my localhost with bin/searx-run\n- my apache2 server with uwsgi is creating 500 Errors for most of the images, but a few are working (I don't know why)\n. See: #137\nhttps://github.com/mikeboers/Flask-Images\nWhat is your working dir for the images ?\n\\tmp by default\nDoes this folder is owned by you, by apache2, by uwsgi or by searx user ?\nchmod 777 normaly, so should no problem\nDoes searx 'get' the images ?\nPartly, some images are processed. But I haven't found a pattern which is showing the error\nWhat do the servers respond (what http code) ?\n500 Error if the image delivery doesn't work\nSo the important information is to know exactly when the 500 error is issued\nThats a good question. I have searched through all logs, but doesn't found anything which is causing that errors.\n. It's the distant server that respond 500 ? Not searx ?\nI think not, because with bin/searx-run every is working fine for me\n. I have implemented it in this way because it was the simplest way to do it.\nIf someone would implement this feature directly in searx, it would be great. But the features of flask-images and of a self written solution wouldn't differ much.\nthx\n. - if we hash the url +  secret_key, we can add a protection to prevent misuse of the proxy. (adding a key property to the url)\n- probably we should support subdomains (like a.searx.me, b.searx.me,...) to improve image-loading-time. because most web-browsers are loading a maximum of 2 images per domain at the same time (https://stackoverflow.com/questions/985431/max-parallel-http-connections-in-a-browser). The problem is, that every subdomain require a certificate, or there is a wildcard-cert for all used searx-domains.\ngood work @asciimoo :+1: \n. There is a bug in the 3. commit. now results without text are displayed as normal result, what is good, but all url's are displayed as 1. result, so this result is not pointing to a valide website.\n. linux, result url: https://www.kernel.org/,%20http://www.gnu.org/,%20http://www.linux.org/\n. It look like the https_rules directory is not available. As Quick fix add it manually, or deactivate https rewrite.\n. I cannot reproduce that bug.\nProbably the keyboard is broken? Otherwise, can you give us a few informations about your Operating System and your Browser.\n. I have reproduced the error:\nTraceback (most recent call last):\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1836, in __call__\n    return self.wsgi_app(environ, start_response)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1820, in wsgi_app\n    response = self.make_response(self.handle_exception(e))\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1403, in handle_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/searx/webapp.py\", line 206, in index\n    search.answers, search.infoboxes = search.search(request)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/searx/search.py\", line 495, in search\n    results = score_results(results)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/searx/search.py\", line 136, in score_results\n    res['content'].strip().replace('\\n', ''))\nAttributeError: 'NoneType' object has no attribute 'strip'\nI'm searching for the bug, and a fix should be available in the next hours.\nthx, pointhi\n. probably he is using chrome?\n. One other idea to do that:\nAdding a line, similar to the following into https://github.com/asciimoo/searx/blob/master/searx/static/oscar/js/scripts.js\njs\n$(document).ready(function(){\n    $('#autofocus').focus();\n});\nI have tested this codeexample not yet, but I can add that functionality in the next days.\n. I have broken my searx site after updating with that pull request.\nFollowing error in uwsgi:\n* 29 https-rules loaded\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/webapp.py\", line 36, in <module>\n    from flask.ext.images import Images\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/exthook.py\", line 87, in load_module\n    raise ImportError('No module named %s' % fullname)\nImportError: No module named flask.ext.images\nSat Dec  6 19:19:16 2014 - unable to load app 0 (mountpoint='') (callable not found or import error)\nSat Dec  6 19:19:16 2014 - *** no app loaded. going in full dynamic mode ***\nIt look like that the new python package is not found, but I have no idea why.\nAfter updating searx, I am always running make production and a few other commands, and the new package was installed during that. But it look like uwsgi or so doesn't find it\n. ok, I haven't updated the virtual-env enviroment and now it is starting\nbut the caching enviroment is not working correctly on my server, so I have deactivated It by default:\nThe problem is, that my server cause 500-Errors for most images, only a few are cached correctly. I'm not sure if this is an error in searx, in my apache configuration, or something external (blocking requests because of to many queries?).\nUsing bin/searx_run on my local computer doesn't cause that problems, and the logs doesn't return usefull stuff.\n. now, this feature is mostyl implemented by @asciimoo. Thanks\n. I think I haven't understood your plane fully. Does the key \"answers\" mean all results or only something like the mediawiki results. I think all results, but I want make it clear :).\nIn addition, we need something like structured data (key:value), like wikidata, and a result can be provided by more than one engine, so the engine part have to be a liste or array.\nI found a few things in the code part, which I would improve :)\n. But I think that thing would be massiv, there are much tags, which can change on every object (like public-date).\nBut I have to say, it is an idea for ultimative results merging :)\n. I think the kickass engine is broken currently. (I don't get any results)\n. If I make an kickass request over localhost, I get the following error:\nSSLError: hostname 'kickass.so' doesn't match either of '*.kickass.to', 'kickass.to'\nIf I rewrite the url to http://, I get the following error:\n[E] Error with engine \"kickass\":\n    None\nengine timeout\nI have set the timeout to 10 seconds!!\nkickass.so is working in my browser, so no blocking.\nthis example query on searx.me is also not working:\nhttps://searx.me/?q=%21kickass%20linux&pageno=1&category_none\n. Ok, If I change the url to https://kickass.to the same error occour:\nSSLError: hostname 'kickass.so' doesn't match either of '*.kickass.to', 'kickass.to'\nFurthermore, my Browser is redirect me back to https://kickass.so if I open that url in my web browser. The same if using http://.\nI have try to deactivate ssl verification, but I have not managed it to be working.\nhttp://docs.python-requests.org/en/latest/user/advanced/#ssl-cert-verification\n@Cqoicebordel forgetting 'please' is no problem for me :)\n. thx for that information. I doesn't found that little pice of code.\nNow it is working (^ ^).\n. mine is also OpenSSL 1.0.1f 6 Jan 2014, coming from the ubuntu standard repositories (14.04)\n. .deb Packages are only one of the different packaging system. Beside Debian also Ubuntu and many other distributions use it. We can publish .deb packages with every new version of searx, and use a project ppa to delivere that new version. (Probably with up-to-date dependencies)\n. it's an idea. Otherwise we could probably use pip to get the packages up-to-date. But I have not the experience if that would be a good idea.\n. This Buttons are based on Bootstrap, and only \"visual improvements\" of normal checkboxes.\nSo, if you want to change the value, you have to change the value of the checkbox to \"checked\", or remove the \"checked\" attribute from it. Your script is currently only changing the visual style of the button.\nProbably helpful:\nhttp://getbootstrap.com/javascript/#buttons\n. The bug didn't occour a second time, but I though it would be a good idea to document this failure.\n. probably, I could write a special result.html for code-results, including a syntaxhighlighter, using pygments or a similar package.\n. I have started with a new Template-System like described in #138, but there is only a pice coded yet.\nThe idea was a generic template class for template handling, and every template can customise methodes from this class if required. Adding a function, which rewrite results to a general result-page (if required) would be easily done with this system, and every template can chose what results should not be rewritten.\nProbably that would be a good idea to solve that duplication problem, while every template can customise things if desired. But coding that subsystem isn't trival at all.\n. @asciimoo oh, I have thought you mean the code.html in every template :), jep, that is a good idea (probably already discussed on irc)\n@Cqoicebordel that is a template issue. If we know, that not every torrent-result return a magnet-link, we have to add an if in the torrent.html, which is figuring that out.\n. happy coding :)\n. please note that there are many possible combinations of data, which should be displayed nicely.\nI think, allowing thumbnails for default results, like in faroo would be an easy step in that direction.\nBut note, that a big all in one system, must have an intelligent, clear structure. Otherwise it wouldn't be maintainable. After all, I'm also a fan of intelligent, all in one products :).\n. @asciimoo, is there a efficient way in jinja2 to check if a template-file is available, or does we have to program a own solution.\n. ok, I have merged the master to make that pull-request up-to-date\nIn future I could improve ordering of static-files. It shouldn't be a big problem do it.\n. It look good to me. I'm not sure, but probably we should display Errors in a procentual way (at least the scaling)\nOtherwise, engines which always thrown an error, but are less used would look better as engines which are allways used, but only throw partly errors.\n. the fuzzy mechanism of the translation generation has generated some new gabage translations. Probably changing the line:\nshell\npybabel update -i messages.pot -d $SEARX_DIR'/translations/' -l $f\nhttps://github.com/asciimoo/searx/blob/master/utils/update-translations.sh#L12\nto\nshell\npybabel update -N -i messages.pot -d $SEARX_DIR'/translations/' -l $f\nfor deactivating the fuzzy mechanism. (this change is not tested by me yet)\nhttp://babel.edgewall.org/wiki/Documentation/cmdline.html#update\n. @asciimoo the truth with js-compalibity is: you can select categories, but it isn't visualised in the theme without js yet. Probably I can add a little css-hack to fix that.\n. I think removing the javascript-warning except in the preferences wouldn't be a problem. Also, the warning should appear if the screensize is to small, because the menue on top doesn't work wihout it.\nBecause of the logo, I'm not sure. I think the magnifying glass in the logo should remain. What's your idea for a logo? The same as in the other themes, or somthing new?\nIn the next time, I would add a feature to change the look of the oscar-theme, using a few themes found on http://bootswatch.com/. Probably we could make searx more colourful with it :).\n. Due to refactoring I think we should also improve the oscar template. I'm not a designer, but I know some basic stuff. But color choosing, main graphic elements are not my strong side so I hope we can produce something greate together (I would implement the client side after doing the planing)\nOn of the parts are the used colors. Probably we could choose some main colors which we can use for all things regarding to searx (something like a Corporate Design)\nMy first ideas for the used colors scheme (please add your ideas)\n\n\nhttp://paletton.com/#uid=73L1W0kgOos5bCdaKtrnljkumek\n\n\nhttp://paletton.com/#uid=73R210kg7os6AsCb9qqm6mqrDkz\n\n\nhttp://paletton.com/#uid=73R1M0kleqtbzEKgVuIpcmGtdhZ\n\n(it's only my first idea, but probably using a admin-interface-like style would look good: http://wrapbootstrap.com/preview/WB0696K5S (but heavily modified to display search results))\nI started brainstroming on my wiki, what has to be changed in the new version:\nhttps://github.com/pointhi/searx/wiki/oscar-template-v2-ideas\n. yes, I can write frontend templates, but not design them proper (I'm always using some other websites/templates to get an idea how things can be aranged, colored,...).\nI don't know if it is good or bad, but I think a tilebased template could be a modern looking and helpful design. I could write tiles for different applications, which are fully scalabel (from phone to big desktop) and visual seperated. Important informations, like widgets, wikipedia results and helpful infoboxes are colorised, normal results more discreetly designed.\nBut the design should also be able to work with more complex datamodels in the future (#449), so it has to be designed carefully to provide results in a homogenous way.\n. I would recommend checking if template_src != null, and showing img_src if necessary.\n. could you look please what files are requested by the browser, and if they are located in the searx/static/... directory?\nthx, pointhi\n. it look like http://searx.linuxandi.net/static/css/bootstrap.min.css isn't found. So check if the file searx/static/css/bootstrap.min.css is available in the filesystem.\n. you forgott some lines in the changeset: 52, 57, 62\n:-)\n. the line from sympy.parsing.sympy_parser import parse_expr is working as well in the python2 and the python2.7 shell.\nInside the python3.4 shell the error ImportError: No module named 'sympy' is thrown, but thats probably because make minimal make a python 2.7 installation.\nFurhermore, there is one more curiosity:\nif I'm adding the same line from sympy.parsing.sympy_parser import parse_expr additionaly to webapp.py, no ImportError is thrown. But If I'm calling the methode parse_expr inside the engine, the following error is thrown:\nTraceback (most recent call last):\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/searx/search.py\", line 42, in search_request_wrapper\n    return fn(url, **kwargs)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/requests-2.2.0-py2.7.egg/requests/api.py\", line 55, in get\n    return request('get', url, **kwargs)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/requests-2.2.0-py2.7.egg/requests/api.py\", line 44, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/requests-2.2.0-py2.7.egg/requests/sessions.py\", line 383, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/requests-2.2.0-py2.7.egg/requests/sessions.py\", line 491, in send\n    r = dispatch_hook('response', hooks, r, **kwargs)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/requests-2.2.0-py2.7.egg/requests/hooks.py\", line 41, in dispatch_hook\n    _hook_data = hook(hook_data, **kwargs)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/searx/search.py\", line 102, in process_callback\n    search_results = callback(response)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/searx/engines/sympy.py\", line 37, in response\n    expr = parse_expr(query)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/sympy-0.7.6-py2.7.egg/sympy/parsing/sympy_parser.py\", line 820, in parse_expr\n    return eval_expr(code, local_dict, global_dict)\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/eggs/sympy-0.7.6-py2.7.egg/sympy/parsing/sympy_parser.py\", line 733, in eval_expr\n    code, global_dict, local_dict)  # take local objects in preference\n  File \"<string>\", line 1, in <module>\nNameError: name 'Integer' is not defined\nThe same if I call this function inside webapp.py\nThe package-index of https://pypi.python.org/pypi/sympy tell me that it is compatible with python 2.7 as well with python 3.4.\n. It seems sympy is able to be included into a searx-plugin. Now I can start with the implementation of a math-plugin.\n. ok, I have add the possiblility to change levenshtein distance, and add a first implementation to use a wordlist.\nI have implemented the wordlist like described by faroo, so it is really fast. The problem is the memory consumption, because  we have no database yet (around 1GB for /usr/share/dict/words(around 3 Million words which have at least 1. correct spelling correction) inside every thread).\nI add a warning to this option becausse of that. (My computer was going frozen when I was loading the german wordlist, because of the big RAM consumption)\na dynamic wordlist would be more pratical for most use (using auto-suggestion results to fill it with new words), but does also require a database to work with multible threads correctly.\n. except spell suggestion we can also use it to detect the language of an result, if we are storing the language of every word. So we could implement #206 without knowing the language of the specified url.\nFor that we could load for example the top 500 or 1000 words of a desired language into the index, and then process language-detection from it. I would run tests how much word-language-checks/spelling-checks the current implementation can handle per second.\n. trigrammes are looking good. (I wasn't knowing about that, textprocessing is new for me, but very interesting :))\nI found a library which could do the required work: https://pypi.python.org/pypi/langdetect/1.0.1. On my computer it processed between 50-100 senteces/second. This isn't fast enought if we want process every result.\nI have also tested my current implementation of the spellchecker. I can process about 2000 Words/Second with the current implementation. There is much optimisation potential to make it faster for cases like correct words or low edit distance (currently every possiblity is calculated even if we could abort the calculation). I don't know if an external database like MySQL with hash index is improving or decreasing the lookup-speed. But without an database it wouldn't be practical because of the RAM-requirements.\nI'm not sure, but if I'm understanding trigrammes correctly, we could using hasmaps like inside the spellchecker to speedup detection(if not done yet)\n. I think It's clear that enhanced textprocessing operations are optional for an admin (and by default deactivated), because of the big cpu/storage requirements.\nBut If we implement text processing, it should be clear that speed is an important part, and then we have the known memory vs cpu problem. But I think we could implement greate features with text-processing:\n- In my opinion, the basic part/algorithm of the spell-suggestion is the best combination of speed to memory we could easily achieve. (There are some improvements possible, but working only with deletions and hashmaps is a good/basic-algorithm).\n- Language detection with trigrammes are interresting, but to slow to parse every result. A combination of a domainname-parser (.de domains are mostly german,...) with fast trigrammes for unknow cases can be a good combination. Probably we can also use language-detection for the search-query, to select a search-language (Automatic Language Modus)\n- Question Answering is also an interesting part. But I haven't figure out yet, how I could implement a fast/good parser for a specific subset of questions (Geodata, Math, Weather).\n- Autocompletion is a part where I don't know if we could achieve them without an very big database. The easiest idea would be using the spell-checker. But then, the autocompletion would be really random :). Probably using a Radix tree, build from older (autocompletion-) results.\n@Cqoicebordel thats greate that you have experience with text processing. \n. In my opinion, not js cause the privacy issues, programmers who use it to harm privacy are the problem. As before said, even there is no js, no one can fully trust an searx instance which is not controlled by himselfe, because every admin can add own privacy harming code on server-side.\nI think, we should always serving basic search-requests without needing js. I have implemented that feature for oscar soon, but I'm also developing new client-side code (only for the oscar-theme), to add new features to searx like a big map, which is also supporting routing and reverse geolocation.\nI don't force anyone to use it. If one doesn't like it, he can always switch to themes like default or courgett, or simply disable js, or more simple, not using that new features. (Because without user interaction, no external requests are done).\nI think it is a good choise, to have mostly js free themes like default, and themes like oscar which bundle more features, but also require more bandwidth for js-related stuff. In searx, the user choise what he want. In my opition, most potential users are on the side of modern features as for using a js-free site.\nWhy should we not create a search-site which has similar features like other big search-engines, but without privacy harming? Only because js can be used in an evil way?\n. yes, or we add something like a yellow warning-sign to the autocompletion configuration, which is showing that text while hovering.\n. probably \"^.*[0-9]+\\s*minute(s|).*$\"?\n. I think there is only the possibility of adding a dictionary, because otherwise we couldn't differ between minutes and hours, if the number is to low (<24)\n. one other (stupid) idea:\nwe know that hours can only be displayed between 0-23, so numbers above that are automatic minutes. If we parse all dateobjects, from a site, it should be possible to find the exact phrase, in which minutes/hours are displayed, as long one value is above 23. :)\n. I think we can fix that if we are using a database instead local variables to store the stats.\nFurthermore, a restarted searx-engine can build on the previous data like score.\n. I think we should add a database which can handle all of this database-things inside searx. With that, we could also fix #199.\nProbably something high-level which can use SQLite as well as MySQL and PostgreSQL, to be capable for later application which could require more speed/efficient, and to be more flexible for admin-side.\n. I think the peewee framework would be a good choise.\nLightweight, and support of SQLite, MySQL and Postgres by default.\n. I think that's more an issue for the underlying database, because that has to handle multible queries. Peewee is what I'm know only a high-level interface to the database. \nSo multible Peewee processes shouldn't interference, as long as the database is handling the queries correctly. MySQL and Postges shouldn't be problematic because there are standalone.\nSQLite otherwise could create problems, if we spawn multible instances of SQLite.\n. Ok, I have found out that SQLite3 is designed to work with multiple processes. So It shouldn't be a problem for SQLite as long as we are using SQLite3.\nIt look like Peewee is capable for caching. If we can turn it off or it doesn't affect our querys, I would prefere Peewee instead of only SQLite, because we are more flexible with it.\n. I have created a wiki-site for mindstorming: database-structure\nVarious tables are clear, how to build them. But other are very tricky, to be simple as well as flexibel.\n. I'm sure solving this issue don't require a database.\nProbably we should move this discussion to a new issue, because adding a database is at least necessary for #199, and could also be used to improve the speed of https-rewriting, caching images on server-side, adding an admin-panel,...\n. ok, the bug is fixed. It is created if a special unicode character is included inside the url, like the german \u00fc.\nI think we should add tests with unicode-characters, to find such Bugs.\n. A question: has anyone a problem if i'm rewriting the js code inside oscar to typescript? Advantage would be clearer code and include possibility, disatvantage: It has to be compiled, and it was created by microsoft :)(is opensource)\n. Typescript has many similarities to EmacScript6. Code style is similar to java, and readiblity is much higher as for vanilla js. Furthermore it has a grate typecheck sxstem.\nI would only rewrite searx-specific code, we can still use parts written in vanilla js.\nFor me, I need a client language in which I can use OOP and includes in an easy way. Without function inside function inside prototype,...\n. Babel looks good to me, but there switching to ECMAScript 6 only makes sense if there is something like include support like in typescript. otherwise we couldn't handle bigger code-parts on client side.\nCurrently, we copy together all js-files and minimize them. But writing a good map with routing-capability,... requires much more code, and also creating dependencies between the files.\nHere is a little example of which i have programmed a little time ago in typescript:\n``` typescript\n/// \nmodule engine {\n    export module baseType {\n    export class Point implements impl.iPoint {\n\n        constructor (private x: number\n                    ,private y: number) { }\n\n        // get x-axis of Point\n        getX() {\n            return this.x;\n        }\n\n        // get y-axis of Point\n        getY() {\n            return this.y;\n        }\n    }\n}\n\n}\n```\nand that is the created EMACScript5 to provide that functions:\n``` javascript\n/// \nvar engine;\n(function (engine) {\n    (function (baseType) {\n        var Point = (function () {\n            function Point(x, y) {\n                this.x = x;\n                this.y = y;\n            }\n            // get x-axis of Point\n            Point.prototype.getX = function () {\n                return this.x;\n            };\n        // get y-axis of Point\n        Point.prototype.getY = function () {\n            return this.y;\n        };\n        return Point;\n    })();\n    baseType.Point = Point;\n})(engine.baseType || (engine.baseType = {}));\nvar baseType = engine.baseType;\n\n})(engine || (engine = {}));\n```\nTypeScript is very similar to EMACScript6, but as long as no one can provide a similar or better way to solve dependencies, typescript would be the better choise. In typscript you can use /// <reference path=\"../impl/iPoint.ts\"/> to solve dependencies, and I haven't found anything similar for vanilla-js.\n. I think you dosn't understand me right. I don't want to create a routing engine on js-basis. There are great projects out there like OSRM which I want to use, but there is much more client-side programming required to get the routing-parameters, display it on map, display extra information,.... In addition there are some ideas which also require client-side programming, so the js-part in oscar would growing including more dependencies between the files.\nI would not write that stuff in one file, because no one could maintenance it properly. Furthermore, Typescript or EMACScript6 are much more pleasant to programm as vanilla-js, especially for programmers which are not web-developers (like me).\nThere is the javascript-part which is used on OpenStreetMap, to get a feeling how big the client-side codbase could grow for a map: https://github.com/openstreetmap/openstreetmap-website/tree/master/app/assets/javascripts\n. Ok, I have started reimplementing the client-side script part in typescript. Mainly, because of the dependency-solver, typecheck and much better readable syntax.\nThe code is developed in a new branche:\npointhi:big_map_typescript\nI have not only copied old code, but also improved it to be more flexibel, and also added post-request-support to the autocompleter. Map isn't supported yet, because I have to write a nice object, which can handle the little map as well as the big map with routing,... in a clear way.\n. thats a good idea. Probably someone create this article inside his local namespace, and we could improve that version before publishing it into the wikipedia. (and have a safe-copy if it is deleted again)\nProbably, searx is simply unknown for most people, so they think it is irrelevant.\n. @asciimoo I have uploaded the searx-logo to wikimedia-commons, but I don't know what licencing the logo has, so please update that: https://commons.wikimedia.org/wiki/File:Searx_logo.svg\nFurthermore, we should uploade a more recitfied logo without the useless whitespace.\nEDIT\nI have set AGPL as licence. If thats not the right licence, please change. (Because searx is licenced by AGPL, and there is no other licence information)\n. I have deployed the german translation of the wikipedia entry regarding to searx: https://de.wikipedia.org/wiki/Searx. Last time I have done it (Oct 14) it was deleted because of no relevance criteria. I think I have fixed that.\n. Ok, the german wiki entry was deleted. Because I had the suspicion that this happens, I saved the document in my own user-space: (https://de.wikipedia.org/wiki/Benutzer:Pointhi/searx).\nIt looks like, the main problems are:\n- the article is written from an innnen view. Probably, because most references pointing to GitHub, and the other one to a german installation instruction. I have not found any reference to searx, where the main visitors are not something like opensource programmers.\n- we don't have much references yet, specifically inside bigger news portals like the german IT-Portal heise.de. I have written a short letter to heise.de, so they know about searx, and probably take it into account if they writing something about search-engines.\n. @Cqoicebordel I'm also agree with this todos.\nThe idea with an official site is very good. Probably something based on a python CMS including blog, forum and custom apps like my stats-site.\n. Probably, I can add an option in the oscar-template to allow search only in one category.\n. probably we could add a Check mark if the engine is selected.\n. I'm not sure, but I think tx push -s -t overwrite the translations on transifex, so tx -a couldn't fetch new strings (and probably overwrite new translations).\nIf we change it to tx push -s we would not overwrite translations, and updating only the source-file.\n. If https://github.com/asciimoo/searx/commit/b6c3cb0bdd020a459d0ef5c21d1303ed0148cc0c has implemented all of this pull request it would be a good idea to close it.\n. I think the most important things are:\n- forum (for bigger discussions, to provide platform for users/admins without developing reference)\n- blog (to introduce new features officially)\n- searx-instances overview\n- tutorial (first start)\n- multilanguage!\nOther ideas:\n- roadmap?\n- creating a searx-style, which is used in oscar as well as on the official website. But I'm not a graphic designer.\n. In the oscar template it is already implemented. Look at #84 and #168 for more informations.\n. On my server, this bug is existing:\nERROR:searx.search:engine crash: btdigg\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/search.py\", line 41, in search_request_wrapper\n    return fn(url, **kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 78, in get\n    return request('get', url, **kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 71, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 461, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 573, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/adapters.py\", line 431, in send\n    raise SSLError(e, request=request)\nSSLError: hostname 'btdigg.org' doesn't match either of 'ssl2000.cloudflare.com', 'cloudflare.com', '*.cloudflare.com'\n. that's a cool idea. And it can also improve the security, if we are using the provided informations well.\n. yes, that line solve this issue.\n. I think 2.2.0\nedit: and there is also grequests 0.2.0\n. The bug is occouring on my server, and on my local computer for some branches.\n. I have merged the current master into a branche, and now I get also the following error:\nTraceback (most recent call last):\n  File \"bin/searx-run\", line 25, in <module>\n    import searx.webapp\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/searx/webapp.py\", line 40, in <module>\n    from searx.poolrequests import get as http_get\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/searx/poolrequests.py\", line 48, in <module>\n    http_adapters = cycle((HTTPAdapterWithConnParams(pool_connections=100), ))\n  File \"/home/thomas/Dokumente/Projekte/searx/searx_quick_fix/searx/searx/poolrequests.py\", line 14, in __init__\n    self.max_retries = requests.adapters.Retry(0, read=False)\nAttributeError: 'module' object has no attribute 'Retry'\nrunning\nmake clean\nmake minimal\ndoesn't solve this issue.\nThe same if I'm running that line with the python shell.\nEDIT:\nthe error is also occouring, if I'm downloading the master branche and install it with make minimal.\n. #257\n. The dependencies are wrong. You have to change them from python2-grequests to python2-requests, probably it is working afterwords.\n. yep, that could be a possible routing syntax.\nCurrently I'm working to develop the required backbone to search for routes. But it could require some time until searx support routing and other great map features.\n. The current status of implementation can be viewed on https://searx.oe5tpo.com.\n. I think yes.\nThe whole PR is running on my instance flawlessly: https://searx.oe5tpo.com, but there are still some open points and improvements to do.\n. I have not so much experience with pip and python, but could we implement a plugin-system in that way easily?\nBecause if #60 would be implemented, and we add functionalities like calculator or qr-code creator, much new dependencies are required. Using plugins which can be installed later and maintaining there own dependencies would help to let the searx-core slim, and easier to maintain.\n. I think not: http://munin.server1.oe5tpo.com/oe5tpo.com/server1.oe5tpo.com/df.html\n. I think we should add Widget/Plugin support, to support such features #60.\n. There are some searx-instances which are also available as hidden service.\nBeside of that, because searx is between the client and the different search-site-provider, and most searx-instance should have multible users, there is a mixing with the search-queries of other people. I think searx has much more privacy as Yacy, because it is not possible to insert a P2P Node and then getting search-request of other servers.\nhttps://en.wikipedia.org/wiki/Metasearch_engine\n. #148\n. Already implemented in a basic way: #112\nPlease look at: Preferences->Image proxy\n. It's possible, but it is not a documented feature of searx yet.\nYou have to pass the cookies as get arguments to searx:\nhttps://www.searx.me/?q=AAAAAAAAAAA!&locale=en&image_proxy=1&pageno=1&category_images\n. I think that switches would be great, but probably we have to add nojs support. The main functionality of searx should always work without javascript.\n. probably we can improve that idea.\nWe could add an information node to every search-engine, which can contain some informations like:\n- is this engine using https\n- is this an open source engine (like yacy or nominatim)\n- are we using the api\n- short description about the engine\n- website\n- ...\nThemes like oscar could display that informations using icons, or hover texts beside the allow/block button.\nBased on that informations we could also create a plugin which automatically deactivate all non-https engines.\n. I changed it on my server, but it doesn't helped.\n. Yes, this is my apache configuration:\n```\n\n    ServerName searx.oe5tpo.com\n    ServerAlias searx.oe5tpo.com searx.server1.oe5tpo.com\nErrorLog ${APACHE_LOG_DIR}/searx.oe5tpo.com_error.log\nCustomLog ${APACHE_LOG_DIR}/searx.oe5tpo.com_access.log combined\n\n<Location />\n    Options FollowSymLinks -Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n</Location>\n\n<IfModule mod_rewrite.c>\n    RewriteEngine On\n    RewriteCond %{HTTPS} off\n    RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} [R=301,L]\n</IfModule>\n\n<IfModule mod_evasive20.c>\n    DOSHashTableSize 3097\n    DOSPageCount 20\n    DOSSiteCount 500\n    DOSPageInterval 1\n    DOSSiteInterval 1\n    DOSBlockingPeriod 60\n    #DOSEmailNotify <someone@somewhere.com>\n</IfModule>\n\n# HSTS (mod_headers is required) (15768000 seconds = 6 months)\nHeader always add Strict-Transport-Security \"max-age=15768000\"\n\n\n\n    ServerName searx.oe5tpo.com\n    ServerAlias searx.oe5tpo.com searx.server1.oe5tpo.com    \nErrorLog ${APACHE_LOG_DIR}/searx.oe5tpo.com_error.log\nCustomLog ${APACHE_LOG_DIR}/searx.oe5tpo.com_access.log combined\n\n<Location />\n    Options FollowSymLinks -Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n</Location>\n\n<IfModule mod_evasive20.c>\n    DOSHashTableSize 3097\n    DOSPageCount 20\n    DOSSiteCount 500\n    DOSPageInterval 1\n    DOSSiteInterval 1\n    DOSBlockingPeriod 60\n    #DOSEmailNotify <someone@somewhere.com>\n</IfModule>\n\nSSLEngine on\nSSLCompression off\nSSLProtocol +All -SSLv2 -SSLv3\nSSLHonorCipherOrder On\n\nSSLCipherSuite ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA\n\nSSLCertificateFile /etc/apache2/ssl/searx.crt\nSSLCertificateKeyFile /etc/apache2/ssl/searx.key\nSSLCertificateChainFile /etc/apache2/ssl/sub.class1.server.ca.pem\n\n# HSTS (mod_headers is required) (15768000 seconds = 6 months)\nHeader always add Strict-Transport-Security \"max-age=15768000\"\n\n\n``\n. Thanks for that hint.$ a2dismod deflate` solved this issue, but I don't know why it is not working with searx, but working correctly with other php-applications running on the same webserver.\n. I think we have to add that directory into https://github.com/asciimoo/searx/blob/master/setup.py#L76\n. I think we should keep the keywords. Probably google and co. didn't use it, but there is a chance that other search-engines are using them.\n. I propose a generic implementation of the translation interface, because dict.cc isn't the only possible translator.\nFor example, http://wadoku.de is a very good german<->japanese translator, and the google translator can handle more cases than dict.cc. A generic approach like for normal search engines would be the best way.\nProbably done by a plugin, which parse the query, and returning some tags which are detected and used by the translation-engines. And the plugin is merging them together afterwards. This would be a clear way to allow special engines while keeping the core-code clean from specific algorithms.\n. It's a good idea, but I have no idea how to implement it in a proper way.\n. I would propose also to use the build number inside the version string in the future, to label some changes, which are improving searx noticeable. (Language update, new engines, things like the plugin system,...).\n. I would propose build number, because the git id is not very meaningful to most people. Furthermore, people who install searx from a fork (like me, because I added some dev-patches) returning another git id than default searx sites which are building from the official repository.\n2 weeks are a good deadline. Until then I can probably port the basic spell checker (#190) as plugin (based on suggestions).\n. Ok, about 3 months ago we thought about version 0.8.0. Probably we can do it in some weeks. There are many new features/changes, and I think there is no release blocker which has to be fixed before 0.8.0.\nI updated the translations on transifex so we can merge them in a few days hopefully.\n. I think it's an js issue, because we disabled this \"feature\" in the old autocompletion-js-library. Probably I can fix this issue while completing #259, but it would need some time until this patch is ready.\n. youtube_api is not working for me:\nERROR:searx.search:engine crash: youtube\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/search.py\", line 41, in search_request_wrapper\n    return fn(url, **kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 80, in get\n    return request('get', url, **kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 73, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 465, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 579, in send\n    r = dispatch_hook('response', hooks, r, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/hooks.py\", line 41, in dispatch_hook\n    _hook_data = hook(hook_data, **kwargs)\n  File \"/usr/local/searx/searx/search.py\", line 107, in process_callback\n    search_results = callback(response)\n  File \"/usr/local/searx/searx/engines/youtube_api.py\", line 56, in response\n    videoid = result['id']['videoId']\nKeyError: 'videoId'\nFurthermore, the normal youtube api is also not returning results.\n. I have added it to the wiki: https://github.com/asciimoo/searx/wiki/possible-search-engines. Probably I'm implementing it soon, because the search-quality is very good.\nthx\n. probaly using encode like inside the image_proxy? (https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L260)\n. There is a Licence issue, because it is not allowed to display google-map-results on other maps than from google. Furthermore, merging of results is very hard.\n. I'm not sure, but probably (what I know from google maps).\nWhat I know sure is, that coordinates which are comming from google's geolocation are not allowed to be imported into the osm-database. Anyway, the google geolocator is not very accurate.\n. I think this shouldn't be a problem.\n. hm, the thing why I changed some implementation in the webapp and template was because I added a new field in the template specific for such data. I'm not sure how to achive this in another way.\nAt least, we could extend the current template system to be able to add custom variables easily, but nevertheless it's somehow required to change the template to parse this new datasets.\n. yes, but I don't know how to do it properly (From what I found out, there is no clear way in python to handle timeouts of functions): http://eli.thegreenplace.net/2011/08/22/how-not-to-set-a-timeout-on-a-computation-in-python/\n. ok, I found out using signals would be working for sympy, but we cannot use them in threads: ValueError: signal only works in main thread.\nI tried to implement https://stackoverflow.com/questions/323972/is-there-any-way-to-kill-a-thread-in-python, but I didn't got it working. Any other idea?\n. Yes, theroretical using DNS should be fine.\n. Try some sites listed on: http://stats.searx.oe5tpo.com. Probably we can figure out the problem. For me, https://searx.me is working correctly.\n. ok, I got the error message in the uwsgi-log:\nERROR:searx.webapp:Exception on / [GET]\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/usr/local/searx/searx/webapp.py\", line 373, in index\n    'index.html',\n  File \"/usr/local/searx/searx/webapp.py\", line 343, in render\n    '{}/{}'.format(kwargs['theme'], template_name), **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/templating.py\", line 128, in render_template\n    context, ctx.app)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/templating.py\", line 110, in _render\n    rv = template.render(context)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/jinja2/environment.py\", line 969, in render\n    return self.environment.handle_exception(exc_info, True)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/jinja2/environment.py\", line 742, in handle_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/searx/searx/templates/oscar/index.html\", line 1, in top-level template code\n    {% extends \"oscar/base.html\" %}\n  File \"/usr/local/searx/searx/templates/oscar/base.html\", line 59, in top-level template code\n    {% block site_alert_warning_nojs %}\n  File \"/usr/local/searx/searx/templates/oscar/base.html\", line 62, in block \"site_alert_warning_nojs\"\n    {% include 'oscar/messages/js_disabled.html' %}\n  File \"/usr/local/searx/searx/templates/oscar/messages/js_disabled.html\", line 3, in top-level template code\n    {{ _('Please enable JavaScript to use full functionality of this site.') }}\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/jinja2/ext.py\", line 132, in _gettext_alias\n    return __context.call(__context.resolve('gettext'), *args, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/jinja2/ext.py\", line 141, in gettext\n    return rv % variables\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/markupsafe/__init__.py\", line 101, in __mod__\n    return self.__class__(text_type.__mod__(self, arg))\nValueError: incomplete format\n. @kdani3 approach to handle different settings (using two classes, and using hasattr to find the correct methode) could also be used to customize our (merge/duplicate check/ranking) code.\nFor example, adding duplicate_check_map and merge_map methodes to customize map merging.\nAnother idea would be implementing basic support of asyncron parsing (#240) while refactoring. (thread safety)\n. Please post your uwsgi log, and probably your apache2 log.\nYou can find the uwsgi log there: /var/log/uwsgi/app/searx.log\n. sorry, but this bug is not easy to reproduce. The same query is sometime throwing an error, sometime not.\nIt would be easier, to convert all url's to utf8 before doing further operations.\n. seems fixed.\nNow we have to update/add translations and then we should be ready for release.\n. It's very simple. You have to copy the theme folders which are contained in /searx/templates and searx/static/themes. Afterwards, searx should automatically detect the new theme.\n. have you changed the used theme in the web interface? does the new menue entry appear in the web interface?\n. - try to delete/rename a theme (is it disapearing in the menue?)\n- use a theme name without special characters (like -)\n. probably you are working on the wrong searx project.\n. What I found out the search now is using the api. Without API-Key no search\n. +1\nI have done a rough review of the code, and the basic code/structure looks good to me. But it need some rework and bugfixes:\n- Please add some comments to the most important functions/objects, add copyright header.\n- Loading default values doesn't work (for example default-theme)\n- some Tests doesn't work\n- it would be better to use Accept-Language or another language source by default instead of \"locale\": \"en\".\n- blocked_engines not initialized using settings.yml, the same with disabled_plugins.\n- disabling plugins is not working\n- probably other things I haven't spotted.\n. I would add at least class descriptions.\nFurthermore, methodes which are using functions like hasattr(self, \"verify_%s\" % setting) are not easy to understand, especially for python beginners (I required some time to understand the behaviour of this code parts). Same with methodes like override_from_form which are doing much stuff.\nMost of the code is self explanatory and doesn't need comments (which is greate :+1:).\n. how is the current status of this pull request? It would be cool to finish this in some time and merge it.\n. I agree with @Cqoicebordel, extending the current json format would be much better.\n. I'm not able to reproduce it. Can you please give me some informations:\n- what instance are you using?\n- what theme are you using?\n- what engines are you using?\n- what language are you using?\nIt look like an XSS vulnerability (search-engine results are not filtered correctly)\n. I think we have to solve this types of bug when refactoring (#415).\nOccasionally, we are finding XSS vulnerabilities which are  causing bugs in the web frontend (redirection, form fields in the results). We should fix that type of seccurity issue once and for all, by doing reasonable input checking which should also be documented in the wiki.\nI would propose to return all results from the engine unescaped (because every new engine could forgott escaping), and doing escaping after removing duplicates. The same with datatypes (converting all results into defined data types like int, float and utf8).\n. yep, I think this is something which could be started at the earliest after refactoring. I also think we have to design this idea very precisely before starting to code, to have a powerful datamodel which could on the other side also be handled by searx without too much calculations. (Using only a subset of schema.org, defining trees which are valid and easy to handle (for example: how to represent multible wikipedia results, wikidata, the official website and a geoposition in an easy parsable way, where the template can figure out what is important and what is not so important)).\n. yes, but I think my idea goes further, because it allowes merging of results with mutlible urls -> harder to display and parse.\nI think after core refactoring is done I'm starting to improve the js and theme part of oscar, to allow more different result types and trying to build a unified result template (for the current data model, but expandable for the future).\nProbably I can create some mockups to show possible arrangements of complex results.\n. ok, i have rewritten the wiki page and improved the schema and the data model a bit.\n@asciimoo: For the next steps, I think rewrite the engines to the new data model would be the first and easiest task, because it's renaming the keys and adding sub-nodes.\nThe template and merging code would also need some rework to work with the new keys and nodes, but this would be not so hard, and could already improve searx (for example, we would now be able to store height and width of every image and of it's thumbnail)\nWith the biggest task (handling of schema, data-model, merging) I would start programming soon, to test my current ideas described on my wiki page. I already have some ideas how to work effeciently with such data, and how to work with the fuzziness of some datasets (For example, a developer in Wikidata could be either a person or a organisation).\n\nOne Idea I had would be adding the possibility of listeners, which are triggered when specific datasets are appear. This would allow us to search in general databases like wikidata, and extending the dataset dynamicaly when data is found.\nIn other words, searx would be able to improve semantic datasets on the fly by looking into additional data sources.\nOne (very special) example would be a search for NOAA-15 (a weather satellite). When searx notices it found a satellite, it can improve the result by querying additional databases for satellites like the new database of satnogs: https://db.satnogs.org. Also searching in local databases could be triggered by such an event. Theroretical, searx would be capabel of working with thousands of different data sources, without calling all of them for every query.\n. ixquick and startpage using the same engine, and also the same layout (I think it's the same company)\n. look here: https://github.com/asciimoo/searx/issues/253#issuecomment-76009918\n. Hy, it's very likely, that you have an too old version of requests. Looking at versions.cfg, searx is currently using version 2.7.0.\nTry to run pip install --upgrade -r requirements.txt in the virtual enviroment (if you are using it) which should update your dependencies.\n. uwsgi should write a logfile into /var/log/uwsgi/app/searx.log.\nIf there are no errors shown, look at the apache logfile /var/log/apache2/error.log (or similar logfiles).\nYou should get a hint why you get a 500 Error in those logfiles. Normaly those bugs are caused by a misconfiguration of apache or uwsgi.\n. Hy, try to use this instructions for installation: http://asciimoo.github.io/searx/dev/install/installation.html\nYou have to note, we updated the dependencies, and also changed the logical structure of settings.yml. Probably this is causing the problems.\n. have you created a virtualenv when installing searx?\n. when you look on the install instruction, there is the command virtualenv searx-ve. When you installed searx in another way, the update instructions won't work\n. I also don't see what you want to achieve with this proposal.\nFor me, payed content wouldn't work, because of the low userrange as well as most user's and admins wouldn't accept it (like me).\nWhen you want to display more/better infoboxes inside searx I would suggest improving the database on wikidata, and improving the wikidata parser inside searx.\n. To share data between uwsgi threads we could propably using SharedArea?\n. output:\ncertifi==2015.11.20.1\n(1, 0, 1, 5, 15)\n. More specific urls:\nhttps://searx.me/?q=!yandex%20svg%20src%20xss&categories=none\nhttps://searx.me/?q=!swisscows%20svg%20src%20xss&categories=none\nSeems, the yandex engine and swisscows engine have insufficient input validation, but swisscows cause currently \"only\" template bugs.\n@ky0p Thanks for your report. We already had some of those bugs, and they were currently always solved in the engine. #441 Now it should be clear to move thos validation code into a central place. \n@asciimoo As solution, I would propose unescaping all results (if already escaped) in the engine, and then doing the escaping the collected results in the core at a specific position. (Not in the template!) This should fix all those engine based XSS Bugs\n. please look at #486\n. Also, we have to fix the plugin translation handling. Because we generate translations, but don't display them in the template.\n. if there are no outstanding issues or reserves left, I would suggest to merge #189.\n. #318 seems also to be releated\n. Localisation is handled on transifex: https://www.transifex.com/asciimoo/searx/\nWhen you are interested, you can fix this error by yourself.\n. I would suggest to simply load the download urls when needed (clicking on the download video button causes an ajax call to searx, which then search for the video urls)\n. I would suggest adding some protection to this feature, so It can't be abused to easily\nSimply hash the video-url with the secrete_key, and use it as additional request argument to get the download links of the result.\nLike already done for the image proxy:\nhttps://github.com/asciimoo/searx/blob/master/searx/webapp.py#L252\nhttps://github.com/asciimoo/searx/blob/master/searx/webapp.py#L587\n. I would display nothing foundonly when really nothing was found.\n. I would change in Some results were filtered out the text color of Click here to display ... results to mark that this is something to interact. (I think the css class text-info should be sufficient, normaly the color changes and a underline is displayed, when you hover such active elements on the oscar theme).\nAlso check how the table look like on small screens (mobile phone). (You can simply make the window smaller)\nThe rest looks fine for me.\n. The bots are causing more and more problems now, which results in regular blocks of many engines.\nI think the only way to detect such abuse automatically would be creating lots of metrics for each user (I collected about 30 metrics which could probably be used for bot detection), and use a self learning/responding system to do real time tracking of abuse.\nI'm aware this is not pretty helpful for privacy, but this is the best idea I have.\n. the problem is, the abuse (in my case) seems to be done by a botnet (Mostly single ip's), but follows other pattern (currently, the requests are always done with format=rss, at least by one of those bots, and mostly coming from russia).\nMy Idea would be, bot's normaly create some specific pattern, which should occour in some metrics. (Like an abnormal count of format=rss combined with russian search queries as GET-Request and always the same server string, but different ip's).\nUsing algorithms, which look for gatherings in all metrics, and cross checking those gatherings with other metrics should be able to create rules which can be used to detect bot behaviour. (most Bot's should normaly be found in the same gathering groups of different metrics).\nWhen I have time, I would do some tests how good this work on real live data. The Apache logs should give me more than enough metrics for first tests.\n. splitting '{minutes} minute(s) ago' was a very bad idea, because in German (and likely also other languages, you are not able to create meaningfull translations). For example (translated to english), It would be something like ago 5 hours, 10 minutes or 5 hours, ago 10 minutes.\n. There is a plugin to force websites to https.\nProbably we could improve it (to allow faster matching of a big list of urls), and using this core to create a new plugin which does matching on block lists.\n. ?\nsafe search is implemented on the engines. When the engine doesn't support safesearch or the engine has other ideas how safe search should work than you it can happen to show results which aren't \"save\".\nEven then, what can JavaScript do which python can't do?\n. that comment doesn't make sense at all:\n- a JS implementation wouldn't be able to be used by all users\n- a JS implementation increases bandwith requirements of the server, especially problematic for low bandwith users\n- a JS implementation would remove results not until the latest state of processing, instead of deleting them right after retrieving the results from the engine when done in python.\n- how would you even detect suspect results using JS? Downloading a blocklist every time?\n- that feature doesn't need client side code at all\n. Yep, it doesn't make sense to show broken tiles.\n. did you try to set: https://github.com/asciimoo/searx/blob/master/searx/settings.yml#L14 ?. This message is shown because small screen + js disabled results in a not working menue -> user cannot go to the settings page without editing the url.\nWhen this gets fixed, removing the message would be the logical way. Probably adding some icons when no js is enabled, or some other workaround.\n. I also think it's quite useful on a regular basis. Like when a site is offline / no longer reachable\n. I updated the entries in the database. Fixed stats should be visible tomorrow.. please note this site is testing the connectivit on my instance, which wasn't update a long time now. Many engines likely have fixes, as well as some engines are probably blocked. Dont trust the results to much :-). well, that's my site :)\nRepo can be found here: https://github.com/pointhi/searx_stats\nI did not update the list quite some time and left old instances intentionally in there (probably they reappear, and to count the complete number of instances made). I will remove it later today, like you requested.. there is a PR: https://github.com/asciimoo/searx/pull/384 someone has to actually fix it ^^. I think the comment is wrong, because this engine would use HTTPS after merging that pull request.\n. ",
    "petrus-lt": "Searx almost works but there are still some issues with this:\n- searx.png and search-icon.png url.\n- save link on preferences page goes back to root instead of parent\n- The 'Install' link to add searx in Firefox links to /opensearch.xml. \n. ",
    "ThomasWaldmann": "[geolocation] Which may make sense for Google or high-traffic sites, but is rather contra-indicated maybe for a search site that tries to improve privacy and anonymity.\n. ",
    "Cqoicebordel": "Why would a french searx instance only provide french results, or do french searches ?\nThe version could be a problem (I doubt it), but this kind of portal could limit itself to the last version or n && n-1 as detected by http://stats.searx.oe5tpo.com/\n. You do what you want.\nAnd remember that the language of the results can be choosen in the prefs, even if the default is french, and that this preference will work across all nodes.\n. No I understood correctly : the server cannot serve only french results. It can only default to french results.\nSo, it's in the client side to pick a prefered language, and bypass the default.\n. The premise of searx never was to be without javascript. Some themes work without it, some are better with JS. It's an user choice.\nAnd implementing it will in no way destroy your non-JS experience of searx.\n. Deactivate JS, and searx will work flawlessly. It doesn't hurt you if a JS function is implemented, it doesn't hurt your privacy. And as the code is public, even with JS activated, I can prove that your privacy is respected.\nSo no, the project wasn't around JS-less, but around privacy protecting.\n. If they trust the searx instance, why not trust its JS ?\n. py2 and py3 aren't different languages. As such, it isn't porting to go from py2 to py2+py3, it's only evolving toward the future.\nIt's like compiling a C program with a C++ compiler : it works almost out of the box, and allow greater possibilities for the future.\nSo the benefits aren't for the users, but for the developers.\nThere are also benefits to the admins that can deploy searx without installing another environnement for another version of python than they used to.\nSo it's win win win.\nAs for erlang, as it's not a port, there is no point of it here.\n. NO ! It's the same language !\n. Because there are some cleanup tasks to do, and imports to organize.\nThe effort now is like solving a bug, so a bit. Effort in the future, nil. Benefits : I have already listed a few above.\n. If the code stay the same, there are no epsilon, like in this case. Whatever the words you use, IT IS THE SAME LANGUAGE !\nI already did the conversion to py3 of searx. So I know the change is minimal.\nI also already responded to all of your questions above, but I will again as you don't seem to read my answers.\n\nCan someone commit py2 code, and be happy it'll work with py3 ?\n\nIt's the SAME CODE ! So yes. The only thing that change is the name of the import.\n\nor vice versa?\n\nObviously, yes.\n\nhow much effort is it to support both?\n\nNone\n\nor does this porting mean in the end, that it's basically py3, but it might run also on py2?\n\nYes, but it's not \"might\", it's \"will\".\n\nhaving to support suddenly py3 additionally brings no benefit but more work for the developer.\n\nNo, it's the SAME LANGUAGE, and so, the same work. The only differences, is that now, the py3 lib are at hands reach.\nAlso \"no benefit\" ?! Please read again, and again, and again, what I posted above. And once more.\n\ni'm not sure that our developer resources are so plentiful as to spend all this for 1 or 2 admins, who for some weird reason have no py2\n\nSure, and we shouldn't use IPV6 for the three of four sites that use it...\nPy3 is the future. Py2 is dead and will disappear sooner or later. We _have_ to move to Py3.\nUnless you want the project to be dead when py2 will be ?\n. Please don't. He is only one guy that is trolling. And he doesn't matter. I only like to show people they are wrong ;)\nI rather like you to stay, for tomorrow, when he will be gone :)\n. Benefits : Future proof, py3 libs. As stated again and again above.\n. You're welcome.\nBut what I can gather from @pointhi implementation, is that it's wildly different from mine, and I think, way better : I use players from the sites, when he is providing a unified searx player.\nAnyway, I think we should keep in mind his way, to one day upgrade mine.\n. I don't think it's the right way of doing things. Every site can have an icon.\nMaybe we could use the sites favicons directly ?\n. It would also allow a less tracked activity on searx.\nWhen you say minimize, is that producing thumbnails for those ? \n. It smells like a rights issue : \n- What is your working dir for the images ?\n- Does this folder is owned by you, by apache2, by uwsgi or by searx user ?\n- If you are not sure, try to put a chmod 777 on the working dir and test it.\nIf it's not that, I would look on the HTTP queries side : \n- Does searx 'get' the images ?\n- What do the servers respond (what http code) ?\n- Is there a transfer between the distant server and searx ?\nSo the important information is to know exactly when the 500 error is issued : before the get, before the download, before the write on disk, before the transform, before the new write on disk or before the use by searx.\n(BTW, is there a write on disk before the transform ? Because it could be an issue with the size of memory used to transform a big image into a smaller one : it must first convert it to raw, and in ram, it takes a lot of place)\n. It's the distant server that respond 500 ? Not searx ?\nIf that is the case, it could be because the distant server doesn't allow curl/wget/request to grab images. It could only be an issue of using the right user agent.\nI will try to go in details into your code, and test some things, but beyond what I already talked about, I have no idea...\n. I think resizing is not only necessary, but also essential : it's a pain to load a searx image result page with each picture weighting something like 2Mo.\nAnd in the side of dependency, I rather have a python dependency than an OS dependant lib...\nI don't think it would be too hard to build it from scratch (or at least with PIL)\nhttp://united-coders.com/christian-harms/image-resizing-tips-every-coder-should-know/\nEDIT : Wait, that's already what you did. My bad.\n. Sorry, just saw your commits.\nI don't think it's the right way to go : For that cache to be useful, it would need to have two users using the same query, or a query that return the same images, in a short time span. Because if it's the same user, the browser cache would suffice.\nBut, if the cache would also minimize the weight of the pictures (-> ~100ko) and cache that instead (or too), that would be very useful to the user, even if there is only one user of this searx instance.\n. Agree, cache and resize are different things.\nBut (and I'm really sorry if I sound harsh) caching without resizing is useless : there is no use case for it. Alone, it will never be useful for the user.\nIt could instead be worse for the user (I didn't look and grasp the meaning of all your code so I could be wrong. If it's the case, ignore this point) :\n1. The user makes an image search.\n2. Searx return the url of the images from the distant server, while caching the images (it's this step I'm not sure at all)\n3. The user makes the same search, and gets the cached urls of the images, breaking the browser caching.\nFor the \"getting the thumbnails from the engines\", I would gladly look at it tomorrow (that's something I would love to do). But there are some issues :\n- Not all engines provide thumbnails\n- Some engines provide thumbnails that are too small to be used in searx\n- Some provide multiple sizes. Which ones to use ? (that the simplest case)\nSo, in some cases, resizing would still be good.\n. I don't think so : the proxy part of the issue was closed using #281.\nBut let's rename this thread to reflect that it's a discussion about resizing.\n. I have the same issue.\nIs there a way to circumvent it ? Because in the meantime, we can't use searx...\n. Confirm\u00e9.\n(Youhou !!!)\n. I don't think it's the session cookie : the result page seems to have changed. \nAnyway there is an API to search Flickr. Yes, you need an API key, but the API has less chances of changing over time.\nThe API don't seem to be difficult. I'll write the new engine. I'll just add it to my todo list ;o)\nhttps://secure.flickr.com/services/api/flickr.photos.search.html\n. It seems it killed the \"view the map\" in the results of a map search.\nOr it may be because my local server serve only http and not https. Is that possible ?\n. Ok, let's have the discussion here.\nI'm thinking of the meta template, and of course, we need an object to send to it, with core blocks defined and accessible, that makes sense.\nWe also need that object to be stable across time.\nThis object will be build by searx itself, not by the engines, but using the engines data : we need to adapt some data for the locale/language (MB/Mo, DD-MM-YY/MM-DD-YYY...). \nSo I created a wiki page on my version of searx to build the perfect object : https://github.com/Cqoicebordel/searx/wiki/Format-of-answer\nBut I need your help so that it will be really perfect before I start coding.\nAlmost everything can be present with 0-N iterations.\nNote @pointhi , that I took your \"advice\" and separated the snippets of code from the same file.\nAnyway. If you have any ideas of core objects to add, or improvements, edit away !\n. Yes, it's all the answers, but we can infer from it the content of an unique answer ;)\nIt's only a visual representation of the object, it won't be a JSON.\nMy aim right now is to handle every possible result but only the results. No infobox, no definition.\nFor the engine part, you are right. We can't use it now, because we don't detect double result, but we will ;)\nTo answer your question : yes, we must fill it with as much information as possible. The aim it to allow a theme creator access to anything that could be useful in the final webpage.\n. Ok. I'm visibly not good at git or github. I thought that the lasts commits were sound, and after more testing I understood more how it worked. But now, I don't know how to go back without re-committing.\nSo, I would like to \"delete\" every \"Correct alt\" commits. Is there a way ?\n. Ok, I will create a new pull request on the correct branch\n. Yes this one could be trouble. The other way around is to delete the setlocale, and directly use the locale of the server.\nBut as I said in the comment, it's only a first step, because there is clearly something that must be done for the infobox : for example, I don't get why I did put searx in french, but the info are in english (I didn't look into it tho).\nIt was only a way to have a correct view of the date, because it bugged me ;)\n. For the content, there is no description readily available from the search result page. I don't think crawling the the result links would be a solution for getting the description\nFor the categories, yes, I got that. But my point was that I found a way to choose a category on kickass, but only one. And when we select Video on searx, it could mean either Movies, or TV Shows, or Anime or XXX or... on kickass, but I didn't found how to group those categories in the search result.\nI'm not confident about my english, so don't hesitate to tell me if I'm not clear enough ;)\nFor icons, I'll look on @pointhi 's work\n. I am silly. For content, I can add the size, the number of files, the age...\nFor the icons I looked at what @pointhi did, and I think there was again a misunderstanding : I was speaking of an icon not for each result, but for Kickass engine.\n. I think this one is good to go.\nIcons and content are done.\nFor the DSL (what is that ?), and for the category syntax, you can do it already : if you \"category:movie\" it will search only in the movie category. It's in Kickass Torrent's syntax, not in searx : it's included in the query.\n. Yes. It should be in the sources, and not (only) in the wiki. But I have no idea where to put it.\nFor KAT, the doc is \"simple\" : we can use the same syntax as in the official site.\n. Stupid question, but are you sure it's activated (ie, in settings.yml, and in the settings page) ?\nIf yes, do you have any trace from the console (or /var/log/uwsgi/app/searx.log) ?\nFinal idea, is Kickass Torrent blocked in your country ?\n. Ok, I have two theories now :\nFirst, it's weird that I can't reproduce it on localhost. But it may be because I didn't configure SSL on my local Apache.\nSecond, it may be because of the SSL of Kickass, and the blocking may come from my country and not yours : it's possible that kickass redirect to its .so domain only here, and use .to everywhere else.\nAnyway, can you test using 'https://kickass.to/' as url ? You shouldn't have a timeout issue because it has https, and no SSL error issue, because it's the right domain.\nIf it works, we can use this url for now, but we absolutely have to understand why it work for me and why nobody else.\n. (sorry, I forgot to say 'please'. Sometimes, I forget we all don't know each other ;) )\n. Ok, so https://kickass.so is the url to use.\nI had a lot of issues on a precedent projet with SSL and requests. It happened to be a OpenSSL issue... :/\nI think I solved it by using GnuTLS instead, but I don't really remember.\nAnyway, I think (but I'm far from sure) that you can put the verify=False in search.py L483, to have something like that :\npython\nrequest_args = dict(\n                headers=request_params['headers'],\n                hooks=dict(response=callback),\n                cookies=request_params['cookies'],\n                timeout=engine.timeout,\n                verify=False\n            )\nConcerning my previous issue : http://stackoverflow.com/questions/8619706/running-curl-with-openssl-0-9-8-against-openssl-1-0-0-server-causes-handshake-er (/!\\ I don't know if relevant)\nAnyway, seems like we'll need @asciimoo help ;)\n. :yellow_heart: ;)\nWhat I remember about the \"OpenSSL bug\", it's that using v0.9.8 to contact v1.0.0 didn't work, except if you specified what protocol to use.\nWhat is your SSL versions ? openssl version \nMine is OpenSSL 1.0.1f 6 Jan 2014. If yours are below 1.0 it may be that.\nAlso, are you using Apache or Nginx ?\n. Ok, so no OpenSSL issue.\nI have installed nginx too, and no error on my side, still. (I forgot, but I'm under Apache). So, that's not it.\n(Oh, and Kubuntu 14.10 on my side)\nI must confess I have no idea why it works on my side.\nAny idea ?\n. I'm looking into that right now. I will take a bit of time, because I didn't look at all at thos tests. I may need a bit of help, but I'll ask if necessary.\n. Done.\nBut I still unable to run the tests on my computer :/\nI have an error \"/usr/bin/env: node: Aucun fichier ou dossier de ce type\"\n(no file or directory of this type)\n. Yes, I looked through the Makefile too, and installed both (in reality, I installed grunt-cli which installed nodejs)\nI think it's a user/virtualenv issue. I'm not yet familiar with virtual env, and I should dig into that, but it's kind of low on my priority list right now.\nThanks anyway !\n. Probably not, but I don't see how to do it properly without an big change in the code : \nWe must be aware of what icons are available at every request, and every request could use a different theme.\nNow that I'm thinking about it, you're right, I shouldn't have done that : \nI could build an 2D array containing every icons for each theme, and pass the right 1D array at request time. Hum...\nBut it won't allow hot changes.\nSo, don't pull that, I'll make the changes\n(sorry for all that admin work for you ;) )\n. It could also be done in a PPA, so we wouldn't be tied to Debian release schedule.\n. A deb is a bunch of folders to copy, and a script to execute. So if we choosed to create a deb file to install searx inside apache as described in the wiki, it would be relatively easy (-> copy paste everything from the wiki in the script, including the pip commands).\nThe issue here is that we can install searx a bunch of different ways : apache, nginx, xxx.xx/ or xxx.xx/searx, using a virtual-env, using a webserver or directly executing python searx/webapp.py, using supervisorctl...\nSo what is difficult is to build a script that can do everything, and ask the user. Or maybe, make a choice for the user. Or maybe...\nSo also, what is difficult, is to take a decision...\nTo answer the other question, the dependencies are already taken care of by doing the apt-get install in the wiki. For the python packages, they are not dependencies, but are installed with pip during the installation.\nSo .deb -> depend from pip (we need it to install python packages), and others\nscript.sh in deb -> install requests etc. using pip.\n. Yes, I know, but I thought the positive of having Flickr was worth it.\nBesides, it's an API key per searx instance, not per user, and using an API is more respectful of the engine.\nAnyway, I can try to have it behave differently with or without API key.\n. Sorry for the double post, but I again looked at it, and I definitely can do that.\nGive me a bit of time, I'll try to tackle it tomorrow, but no promise...\n. Great !\nBut I still don't get why it did work on my computer. And it's a bit unsafe to not verify SSL.\nAnyway, it's not critical to verify SSL, so if it puts KAT in a working condition, that's good to me :)\n. Be careful, you still have a print in your engine\n. Thanks ! It was very helpful !\nIndeed, I looked in the wrong API...\nBut my troubles are not over...\nHere is the new version of my code : \njavascript\n$(\".btn-sm\").dblclick(function() {\n    var btnClass = 'btn-' + $(this).data('btn-class'); // primary\n    if($(this).hasClass('btn-default')) {\n        $(\".btn-sm > input\").attr('checked', 'checked');\n        $(\".btn-sm > input\").attr('checked', true);\n        $(\".btn-sm > input\").checked = true;\n        $(\".btn-sm\").addClass(btnClass);\n        $(\".btn-sm\").addClass('active');\n        $(\".btn-sm\").removeClass('btn-default');\n    } else {\n        $(\".btn-sm > input\").attr('checked', '');\n        $(\".btn-sm > input\").removeAttr('checked');\n        $(\".btn-sm > input\").checked = false;\n        $(\".btn-sm\").removeClass(btnClass);\n        $(\".btn-sm\").removeClass('active');\n        $(\".btn-sm\").addClass('btn-default');\n    }\n})\nFrom the point of view of the debugger (chrome tools, or dragonfly from Opera), it works. The classes of the labels are right, and the checked of the input too.\nWhen I launch the search, it works in Chrome, but in Opera et Firefox, both have every categories activated, except the one I double clicked on.\nThe other way around doesn't seem to have any issue : an empty search will activate 'general' and that's all.\nIt seems I'm missing something...\n. So, it seems that jquery changed its API in route. we mustn't use attr any more, but prop.\nOh and the node error I was getting when trying to do the make tests (/usr/bin/env: node: Aucun fichier ou dossier de ce type)was because NodeJS changed its name. To solve it, I had to do :\nsudo ln -s /usr/bin/nodejs /usr/bin/node\n. :)\n500px was easy but both searchcode were tricky.\nI'm not that happy of the presentation of both, and I don't like putting html tags in the engines.\nBut anyway, they both work, they are both good enough, and they are out there so we could build upon them.\nBTW, two things : If you want me to build an engine in particular, don't hesitate to ask. I like doing this stuff :)\nAnd, you know that both my Flickr and my Detect favicons pull requests are good to go, right ? ;)\n. My thought exactly. There are a few other issues with what is done right now : the way you have an horizontal scrollbar when the lines are too long (I much rather have a soft wrap), or the way the title, file, repos are displayed.\nIn the doc's one, the issues are similar, there were inconsistancy in the results, so I had to return everything in hope something was useful. But it creates a bit of a clutter when everything is filled...\nBTW, searchcode doesn't return the type of code in the code mode. But you could infer it from the file extension.\n. Whaou ! It's really awesome what you did here !\nCongratz !\nI saw two things that could be mistakes : \n- It seems like you minified (lessified ?)  searx/static/default/css/style.css. If it's the case, it will be harder to edit it.\n- You put links around the repo's name, but I saw repos of this kind : git://pkgs.fedoraproject.org/mingw-libsoup (!scc soup), so the links won't work.\nAnyway, wahou again ! And I learned a lot reading your code !\n. I have an issue too with the templates : I'm working on an engine (I would like to keep its name for myself for now...) which can provide a wide variety of results : photo, videos, text, code... Those are not an issue. But one is a set of photos, each with its caption, but they need to be together.\nAnyway, it brings the issue of the template system, that doesn't allow a wide variety of format handling. From the engines point of view, we should be allowed to return any information that is available, in any formats that make sense. But from the theme creators point of view, it shouldn't be dependant on the already available formats, or the future ones.\nAn example of that is the torrent format : I looked to create another engine, because one is never enough. But almost none of them provide a magnet link in their results. But the templates need those. So how to resolve this ?\nI don't have a solution. The best approch I could come up with was to create a meta result template, with everything inside, like \"if contains code then display it; if contain magnet link then display it; if...\"\nEDIT : I like that macro approch\n. @pointhi Not necessarily : we'll always stumble upon some engines which will return data that doesn't fit the current template schema. \nSo another way of doing it would be create a template that can take anything in, and produce a correct output.\nWe could code it by defining basic bricks : photo(with or without thumbnail)/text/video/map/link/code, and allow the engine to tell \n\"This result contains 3 images, a video and some code\" and the template builds a result answer, based on this output. \nBut it doesn't have to display everything. It will be based on the theme. A simple theme won't display images for exemple, but will keep the text. A fancy one (Oscar for ex), will display the thumbnails, and open a viewer when clicked on it, with the text and the code.\n. I'll try ;)\nBut it's a bit big for myself, so it will take time...\n. It could be done, as the language can be found in the URL. I'll work on an option for that, it's a good idea.\n. Sorry for this mess of commits. I'm still battling with git (I so hate it). I mixed stashed and staged :/\nAnyway, the change only change the result URL, not search in itself : it's a movie search, not really a subtitle search (the result brings you to the subtitles search)\n. Whaou !\n. Following my last commit, I looked a bit through all the engines, and I saw a big bug : using ''.join(DOM) for text will return all the nodes in what it seems polish notation (or reverse PN). When there is no tree, that's not an issue, the text is OK. But when there is html in the dom tree (so, a subtree with text in multiple nodes), the text is not in good order. lxml has html.tostring(..., method='text') to do this correctly.\nBut wait... Why searx.me doesn't have the bug ? If you run a twitter search (!tw), the content of the results should show issue with words order, but it doesn't. And again, it does on my apache installation, or python searx/webapp.py execution (on master, of course). Weird.\nAnyway, there are some shenanigans here. I'll explore that tomorrow, and correct it if I can.\n(and now, Twitter doesn't work at all on searx.me. Weird)\nEDIT : Ok, I tested every instances of searx in the wild, and didn't found this bug again. I don't get it. What is different with my computer ?\nThat's not the first type that my computer is having a different behaviour than everyone else, and that pisses me off.\n. After some tests, I still have this bug, and I don't know what causes it.\nI thought it was the \"tail\" issue in lxml as described in http://lxml.de/tutorial.html at \"Elements contain text\", and actually, I still do, but I don't understand why nobody else has it.\nAnyway, there are three options here : \n- Fuck it : I'm the only one to have it, so it's not important.\n- Let's do something, just in case : let's change every join to tostring(..., method='text'), it should be cleaner, and we never know.\n- It's really important, let's investigate and find what the hell is going on.\nYou know that I'm a newbie in almost every tech used in this project, so it's possible that I fucked up somewhere. \nBut I can't imagine where...\n. Yes I agree, but I got a resp.text equal to None, and resp.response equal to 301. And in browser, https was indeed redirected to http.\nAnyway, I have no more clues on my side.\n. Ok, I think It's ok here.\nOf course, some things should be taken care of over at Transifex, but I leave that to the admin ;)\n. Good catch.\nI had to remove by hand the fuzzy lines, but beyond that, I recreated the translations, and everything went fine.\n. I agree that it was questionnable.\nBut the way I did it, nothing is included until you click on \"show xxx\". No request is send to youtube/dailymotion/etc. Nothing. No service can know that something is included until you click on \"show xxx\".\nSo the way I see it, it's exactly the same thing as clicking on the result link, but without leaving searx. \nEven better : youtube uses the \"nocookie\" domain name www.youtube-nocookie.com, so it's even less tracking that clicking on the link.\nThe difference with autocomplete is that autocomplete needs more requests, and so, more tracking. That's not the case here.\nBut an easy way to put it as an option, is to modify the theme.\n. That seems nice to me !\nA few comments though (and feel free to hate me for those ;) ) :\n- I feel like there is a lot of similitude between the parse query in autocomplete, and the one in query. Is there a way to refactor this ?\n- For the Kickass engine, nice again ! To be honest, I was lazy on that one... But what I stumbled upon when looking to implement other torrent engines is that almost none of them provide magnet links. So it could be good to make it optionnal too.\n- You use two  in torrent result, with both the content class. I haven't tested it, but doesn't it make it take a lot of place ? (read the next one before testing anything on your side)\n- One of the pull request I wanted to send was adding the magnet link to the tool bar (archive/show media/show map etc.). Since you are in there you could add it yourself, and add the torrent one too.\nThat's all. I stop bothering you now ;)\n(PS : Nice for the autoescape too !)\n. @asciimoo It could also be useful to do a JS-free template, to allow those without JS activated to use a nice version of searx.\nBut graceful degradation should indeed be implemented. If @pointhi, you know how to do it, it would be perfect :)\n. I like the third color palette.\nBut I think the big issue here is that none of us are designer.\nWe can improve things, but I think a really good theme should be designed from the ground up, with UX and mobile in mind.\nA call for a open source designer to give us some help should maybe be posted.\n. Yep.\nAnd beyond that, the most important part is UX. And that is really really hard for a dev to do it right.\n. The Travis error is not mine, it's weird. Is there a way the run it again ?\n. I think it affected only startpage : it's the only one using POST as far as I can tell.\n. What I can gather from the code (and my personnal experience), is that the default locale of the searx instance is the user's browser locale.\nSo, when I install searx and use it in a new browser (no cookie), it's already in french...\nIt may be also possible to alter the order of the proposed locale language by changing it at the end of settings.yml, but I'm probably mistaken (haven't checked this)\nAlso, as stated in #77 you can directly provide a 'french url' by adding ?locale=fr at the end of the searx url like http://127.0.0.1:8888/?locale=fr\nAu final, normalement, \u00e7a devrait fonctionner tout seul ;)\n. Can you copy paste the content of /etc/uwsgi/apps-available/searx.ini here too please ?\nEDIT : Oh, and does it answer query ?\n. I agree. It seems you have the folders less and themes on your hard drive but not the folders css fonts and js.\nYou should check your folders.\nMaybe copy us a tree from the folder of searx.\nEDIT : I think I found it. Give me a few minutes to test (if you want to test yourself and fiddle, try to repeat the step python setup.py install, but modify the package data in setup.py to have the lines 'static/*/*.*', and 'static/*.*',)\n. Confirmed. I'll propose a correction in a few minutes and then, we'll wait for @asciimoo to pull it.\nIn the meantime, you could copy the folders yourself : \nsudo cp -R /usr/local/searx/searx/static/* /usr/local/searx/searx-ve/lib/python2.7/site-packages/searx-0.6.0-py2.7.egg/searx/static/\n. :)\n. Seriously ?! It was only that !\nSo, if we use the wrong url, flickr sends the images, but with the wrong mime-type I suppose. Kind of weird.\n. Did you try with another browser ? I can't reproduce it on chrome and opera 12.\nIt may be a cache issue locally.\n. I have an issue too with the theming : \nWe need to be able to propose different skin/color scheme within a theme, without having to modify the app side of searx (webapp.py).\nI was planning to do a color picker for courgette today, to change the theme, but I stumbled upon the issue of cookies.\nWe need a way to create and access cookies of local (theme) preference inside the theme. Maybe by extending a searx cookie class ?\n. Argh ! Didn't saw them !\n. I thinked more of a python version issue, but honestly, no clue either...\n. I toyed with the idea of only allowing in the \"cookies\" var, the cookies whose names began with the theme name, for privacy issue.\nBut, in the end, it's all searx. It's searx writing the cookie, and reading it, so there is no privacy issue.\n. Of course ! Much better !\n. Nice work !\nI would suggest three tiny things : \n- Add an option to allow to choose the levenshtein distance from settings.yml. Something like spell_suggestion is false or is the value of distance should be fine.\n- Like said in the blog post, you should break the calcul of levenshtein distance when the max distance threshold is reached. Could means better perfs.\n- As a static list of word, you could use the list that most linux have, /usr/share/dict/words\n. There is a much better way to detect language. You need to store the statistical prevalence of trigrammes of letters per languages (it works also with less letters than 3, but it's less exact). This way, you could have a score, that is independant of words and new requests.\nBecause, for example in Google, if my memory is correct, they have daily around 75% of their requests they never saw before. So with content new, and never saw before also.\nSo the word approach is less than perfect, because we have only a tiny content to detect those words, the words are fairly new, but the statisticall approach allow new words.\nI can give more details, I already succefully coded this kind of detection a long time ago.\n. :+1: Exactly that !\n. Yes, with trigrammes we could use hashmaps and be around O(1), because there is no n.\nBut I'm not sure using a DB with everything in it would be a good idea, even with an hash index, because the lookup time is not in O(1) but depends of n and would likely be O(log(n)).\nI was recommending using the system list of words, because mine is relatively small (~100K lines). \nBut you are right, having the words+the possible errors loaded or saved in base would be too much. We can't possibly ask a searx admin to have a few GB to spare for us (disk or RAM). Per languages.\nOr maybe with an option. But even...\nI have no idea how to do this properly anymore.\nFor the trigrammes, if you are interested in the basis, there is that science paper I used. It's not easy to understand (like any scientific paper...), but the math formula is the basis : \nhttp://swiss.knife.free.fr/StatisticalLID-II.PDF\nAnd if you need help in text processing, don't hesitate to ask, I have done a lot of those (but not in python ;) )\n. For the spell checker, I had a thought a few seconds ago : Hunspell.\nI don't know how it works, but it's really fast, and the dictionnaries files are very light. And it's Open Source. \nIt should be speedy enough for suggestions, and even auto complete.\nIt could be a good way to go.\n. I think it would be difficult to implement it in a way it was unobtrusive for the user : I tend to chose the category of the search after I typed my query, so the autocompleter would use the wrong engines to autocomplete my query.\nBut I like the idea anyway. Maybe we could do it the other way around : in the suggestion list, we could provide icons to indicate to which category this suggestion belong.\n. Autocompleter isn't activated by default, so your comment is moot.\n. Because it's a choice of the user.\n. Because what is wrong for you isn't wrong for them. I know it's hard to hear, but everybody isn't like you.\n. I as said earlier, this trade-off is a choice by the user.\nWe are not more intelligent than the user, and we are not here to teach him.\nWe are here to respond to his queries as he wants to. No more, no less.\n. I as said earlier, this trade-off is a choice by the user.\nWe are not more intelligent than the user, and we are not here to teach him.\nWe are here to respond to his queries as he wants to. No more, no less.\n(Oh wait, d\u00e9j\u00e0 vu)\n. Agreed, but I would present it differently : if we have to describe in details the risks of each preferences, I'm afraid it would overcharge the preferences' page.\nI think a better way of doing it would be to add a FAQ in the about page, and link to it in the preferences with something like this :\n\"Note : activating the auto-complete might undermine your privacy. Learn more here\"\n. Nicely said dalf !\nstef, I will shortly propose a change to add more information to the users.\n. Please do :)\nTravis failed, but not from the code.\nSo, make flake8 doesn't work locally, because it doesn't seem to work with 3.4. But I haven't changed my default python which is 2.7.\nAnyway it was a first try, to allow fiddling. You can put it in a branch, or use the branch in my repo to try things.\n. I didn't saw that you restarted it. It gave me a few clues to continue working on it.\nSo, much better, but there is still the Flake8 that is blocking.\n. It's weird that there was no error when running searx with python 2.7 or 3.4 around cStringIO, but the UT screamed at it.\nI think we need a lot more unit tests...\n. Yes, I know, and it is set at True (it is set in poolrequest). But in some cases, it doesn't follow.\n. Works : http://searx.me/?q=%21ka+22+jump+street&category_general=on\nDoesn't work : http://searx.me/?q=%21ka+22+Jump+street&category_general=on\nThe only difference is the capitalized J.\nKickass tolower() it to put it in URL. And so, does a redirection in the second case, returning a 301 that is not followed.\n. Uh ? Where ?\n. This issue wasn't Kickass specific, so I don't think it should be closed until there is actually the wrapper.\n(Unless you plan to do the wrapper shortly. I just don't want us to forget about this issue)\n. It won't work in russian or every other languages...\n. I agree with you.\nI just hoped there was a python librairy I didn't know about that could do that ;)\n. I like that !\nBut wait, there are days too...\nBut it may be not so stupid.\nOr maybe, there is a way to have the bing interface in english while the results are in any other language...\n. OMG ! I'm so sorry I let that go !\n. I let you do the smart thing. There is lot to do before it's blocking ;)\n. Since I cherry picked the changes in 500px and flickr-noapi, Travis won't run.\nYou could merge the already commited changes, so the next ones would be verified by Travis.\nAnd there are a lot of tiny fixes in the engines that could already benefit searx ;)\nBTW, since the pirate bay came back, I reactivated it in the settings.\n. Same as #181\n. :+1: for both.\nWe need more granularity of config in the user side of things, and also, in the admin side of it.\nHaving to modify a config file and restart the server isn't a good thing. We should have an admin interface.\n. I think something like SQLite would suffice, even for future use.\n. @pointhi > For SQLite, only if we write in those different instance. And still, if we allow a little bit of wait on the write requests, it shouldn't be an issue. SQLite is very very good at this :)\nThe advantages of SQLite are zeroconf, and it is serverless. And you don't need a lib to use it in python.\nEDIT : Just so you know, the kind of code I'm using with SQLite in another project : \npython\nimport sqlite3 as lite\ncon = lite.connect('base.sqlite')\ncon.text_factory = str\nsql = \"INSERT INTO feed VALUES(?,?)\"\nparams = (\"Text1\", \"Text2\")\nwith con:\n    cur = con.cursor()\n    try:\n        cur.execute(sql, params);\n    except lite.IntegrityError:\n        pass\nIt's that easy...\n. Sound good to me.\n(even if flexibility mean also dependency with peewee)\n(the only issue I ran into was using PHP, who set a default busy timeout of 0, so when SQLite was busy, I got an error. I set it to a value of 2 sec, and was good to go)\n. Exactly. It's not for this issue specifically, but a small base is needed to do some things properly, like actualising the conf, the icons, the themes... without restarting the server.\nAnd we can add an admin interface, for the user and the server admin, that has more granularity than now, like allowing an engine for a category but not another.\n. Since you know how 1x works, it should be easier for you to do the unit test for it.\nIf not, I'll have to study the details of the engine and the 1x.com page before being able to do it...\n. Awesome :)\n. If the issue is around gzip (I haven't checked), you could disable it in apache for xml files : \n<IfModule mod_deflate.c>\n    AddOutputFilterByType DEFLATE text/xml\n</IfModule>\n. I thought so, but wasn't sure. The other float() had also a try/except so I thought it was mandatory.\nAnyway, I will delete it tomorrow. Thanks !\n. I think I'm done here for now.\nWhat is left is either engines that don't work anymore or is out of my depth, I think. Maybe I will try to tackle them, but not right now. If anyone want to do it, you are welcome to.\nSo, @asciimoo you can merge :)\n. The database is discussed right now, but not for this kind of issues : https://github.com/asciimoo/searx/wiki/database-structure\nWe are thinking of a database for the parameters, the favicons, the rewrites, the languages...\nBut having a database containing every website (or almost) is clearly not in the scope of this issue.\nSomething more automated would be appreciated ;)\n. Of course, you are right. But as you can imagine, it's harder for us to know if our changes are the rights ones.\nDo you think adding only <html dir=\"rtl\"> would suffice, or each result, <p>, <span> etc. need to be RTL as well ?\n. The issue is, should we RTL the results, even if we don't know the languages of the results, or defining RTL for the page will automagically take care of that.\nNote that searx hasn't yet an interface in a RTL language (you can ask and provide traduction on transifex.org, if you want ;) ), so we can't test the interface as is.\nBut I tried something with a search in arabic. The page would look like this : \nhttp://cqoicebordel.free.fr/searx-rtl.png\nPersonnaly, I think it's not that bad, but I don't know if the result texts are in the right order or not...\n(PS : If you prefer a search in a different language, don't hesitate to ask. Just provide me a search term in the right script :) )\n. Cool for the hebrew translation !\nI will look at how we will implement it rapidly.\nAlso, keep in mind that the fact that I could provide such an image doesn't mean it will be here in searx as fast. There are some technicall issues to resolve first.\n@asciimoo How do you want to store the fact that an interface language is RTL ? In settings.yml ? Or maybe there is an other way, I have no idea.\n. I took everything you said as being in Oscar. If otherwise, try to specify it.\n- I knew about \"About\" but I didn't know how to do it properly. It will be fixed in the future, but not right now (anyone has an idea how to do it ?)\n- Label and description switched places in preferences is corrected, waiting to be merged.\n- If I understood properly, you want the text in the table to be aligned to the left. It's waiting to be merged. If it's not that, try to be a bit more... international in your way of speaking english ;)\n- I thought about that too, but here, it's an UX question. Does an RTL user has an habit to have the buttons to the left ?\n- For the suggestions, I don't know how to do it exactly. If @pointhi know how to, he is welcome to fix that, but if not, it will take a bit of time, while I try to find the solution in bootstrap.\nDon't forget to test the others themes too ;)\n. Sorry, I have a tendancy to respond to everything in order, so, it's clear for me but not always for everybody.\n\nI thought about that too, but here, it's an UX question. Does an RTL user has an habit to have the buttons to the left ?\n\nI was talking about the preference page and the list of engines, especially, the activate or ignore button.\n\nYou can use an </html> tag, or &rlm: or &rle: to change direction. I would prefer an </html> tag over a Unicode Control Character.\n\nNo, it's an issue of CSS, not tags or Unicode chars.\n\nJust did. /preferences#tab_engine of courgette theme can be set to be aligned from right-to-left (columns are properly placed, but everything is aligned from left-to-right).\n\nAgain, it's not really clear. It's aligned to the left. Would you prefer the same columns order, but everything aligned to the right ?\n. Let me some time for that.\n. Yes I know. But I don't know how to handle it, given the fact that the changes are globally around order of elements. \nAnd as stated, it's a first pass... ;)\n. For the alignements, it's mainly to align the boxes to the right, not the text itself. The same for the images.\nThe search field is in the index page, the page you have when you land for the first time in searx.\nThanks for the &lrm;, it was that :)\nI'm sorry, but I can't provide a public searx node. So we'll have to wait until the code is merged, and we will see from there.\nMy aim is to provide as soon as possible an RTL enabled interface here. Perfection will come later ;)\n. I think I'm done here.\n@asciimoo if you are happy with the code, you can merge.\n@GreenLunar for further comments or changes, tell us in the issue you opened.\nOnce this code is merged and your translation in hebrew too, you will be able to test it at searx.me.\n. Yes, thanks :)\nYour approach is much better than mine. I tried to fit everything into a macro, but there were too many variables, so I went to the simplest form. Your way is the good way.\nBut it still weird to have to delete this branch without merges ;)\n. It's your work now ;)\nGo to transifex, there is an update of the traduction to do ;)\n. Really really nice !\nI would like to add to your ideas list, to be able to go to the big map from a standard map result.\nAlso, you could ask for the position of the user to the browser to center the map (could be very helpful on a smartphone)\nBeyond that, it's really really great already !\n. The big obstacle for this rewrite would be for me to find someone who know typescript. I don't personnally. And I don't see it as a future language, so I'm not seeing myself investing in it. But I could be wrong.\nBut in the meantime, if you are the only one who can understand and modify the code I think it would be dangerous.\nAnother argument is that we have a lot of languages already. Another one would increase the barrier to participate.\nAlso, Oscar is already using Bootstrap and JQuery.\nFinally, I'm a big defensor of Vanilla JS http://vanilla-js.com/\nAnyway, I'm not opposing it. I just don't see the advantages and what is blocking you right now with regular JS. (Also, if TS when compiled give regular JS, why not write directly the regular JS part ?)\n. What is bothering me is that it seems to me you want to recreate the wheel with a technologie that is very very young, and client side.\nWhy not do the routing system in python, with ajax calls ?\nRouting can be CPU intensive, especially if you are using A*. And JS is not the language for it. At all.\n. It would be great, but I think it won't be possible to implement :\nEvery page of results of searx is independant from the others, and stateless.\nWhat is necessary to implement this feature is to log every results of openstreetmap for a time (privacy issues), and transmit it to the other pages. It is also necessary to implement sessions (to not leak results  to another user), and store pages and pages of results.\nAnd it would be problematic to link to a result page of searx that's not the first, and also problematic to go back in the paging of searx.\nSo, I don't think it would be feasible, and I think that it's nominatim that is a bad API.\n. Yes, it would indeed be easier for everyone.\nIn the future days, I will look into using a web hook to hook transifex and github to automagically update themselves with new translations or new strings to translate. It would simplify a lot the workflow I think.\n. I think it should be done the other way around : in the page about search engines of Wikipedia, we must first add Searx to the list, then, we could create the page itself.\n. So you know, searx is in the french wikipedia https://fr.wikipedia.org/wiki/Searx . Maybe everyone of you could create a page in his/her language ?\nWhen there are a few translations, they wouldn't delete it in english, I hope.\nI also added searx in the external links of https://en.wikipedia.org/wiki/Metasearch_engine . I hope it'll stick.\nEDIT : I reviewed the guideline by wikipedia. We must have secondary sources to prove that searx is know outside of our circle. And it shouldn't be written by us, but an user or someone else. So first, we have to be known a bit more...\n. That is a question I wanted to ask you @asciimoo : is it ok to give the searx.me url to everyone who want to test/use searx ? I'm asking if the size of the server is ok.\nAnd I think we feel that searx.me is the official one, is because it's yours, the creator/maintener of searx, and it is the instance always the most up to date :)\n. So the question are :\n- Is searx good enough for public use ?\n- Is searx good enough for non geek users ?\n- How to raise awareness of searx ? And should we yet ?\nI think we need to work on boring stuff like documentation and creating installation process that are easier (.deb/.rpm ?) to ease the installation and use of searx. Then, and only then, we should advertise the existence of searx.\nFor me, the todo list is this :\n- Streamline the workflow\n- Ameliorate substantialy the docs (readability, non-geek-proof, explain possible choices etc.)\n- Create a website to present the project, maybe with a forum (we need a better way to communicate and exchange ideas than creating github issues). Having Github as the face of the project is not great for non-tech users.\n- Ameliorate the UX and the UI still. Espacially the UI, I think (and it's only my opinion), that we lack something in searx, maybe a bit of eye candy. For example, the index page of courgette is better than oscar, but oscar is better for the results as UX. Anyway, this is my two cents. We need somebody that has experience in this domain.\nAnd then we could raise awareness, become really public, because we will have something that is not hacky anymore but a stable piece of software.\nMaybe for the v1.0 ? ;)\n. You have already the open issue #214 :)\n. I asked for this kind of help on FB, and for now, they think that it's a bit weird to have to click again on the magnifier glass after clicking on the images category (and also to have to deactivate General).\nI think that what is missing is the checkbox euphemism. Everybody know how a checkbox works, but the categories don't have that little square box with a tick in it. Maybe just by acknowledging that they are checkboxes, it would simplify the user's understanding of their mechanics.\n. No, I'm saying that they expected the search on images to be launched at the precise moment they clicked on Images.\n. I don't think it will solve the issue : it's for the first time users, and obviously, for them, an option won't change anything.\nI think it's only solvable in an UI way. We must show that the categories buttons are switches and not tabs.\nIt could be done by changing the color from blue and white to green and red, or using an image of a switch, or... I don't know :)\n. That is a choice of the theme maker.\nSome want a fully loaded theme with everything in it (oscar), some prefer something more simple (courgette/default).\n. I strongly disagree.\nA theme should be build as the theme maker want it. I could want to build a theme that return only the URL as result and nothing else (no text, no icon, nothing). Or maybe, to the contrary, a map of the favicons of the result sites, without a single word.\nMy point is, a theme is built with a big eye view (une vue d'ensemble), and shouldn't be subject to other themes bells and whistles, or functions and eyes candy. Even RTL.\nI did RTL because I wanted to do it, and did it on all themes because I could and wanted to. But a theme maker can build a theme only for RTL, because some things are better designed from the ground up to the target instead of modifying the existing. And it would serve no purpose to try to adapt this theme to LTR.\nAnyway, I strongly disagree ;)\n. No, I disagree on enforcing the good practice you propose, and calling it a good practice.\nIf someone want to add that switch he is welcome to. But it's the theme creator choice, and no one else.\n. This one will be tricky to implement :\nThe interface language is saved in a cookie, but the purpose of a RSS link is to be given elsewhere, where there are no cookies.\nWe could pass it as an URL option, like http://searx.me/?q=seeks&format=rss&category_general=1&pageno=1&uilang=he but it would be now a privacy issue. If you use this link in a web based RSS reader, it would know that you want the RSS in Hebrew. That's not a big deal, but could be in a few cases.\nAnyway, I think it's a low priority bug, because it influence only the title of the feed.\n. You can add :hebrew or :he in the request string to choose the language of the results.\nNote that it only works on engines that provide the possibility of choosing the language of the results.\n. http://searx.me/?q=%3Ahe+%21sp+france\nIt works only on engines that supports it. Startpage is a good exemple of that.\n. @dalf great catch !\n@ascimoo indeed, a wrapper would be a good idea, because we had the same issue in another engine (but I don't remember wich. Vimeo ?)\n. Ok. It's a mess : \nThere are differents translations on transifex, for the same language. Look at Russian and Russian (Russia). It created two folders too in the searx tree, ru and ru_RU.\nSo @asciimoo can you delete the languages on transifiex ru_RU, el_GR, and pt_BR ? It will simplify a bit the scripting part.\nAnd when adding a language, we should _only_ use the languages with a two or three chars code, not the full unicode ones.\nFor now, I can fully automatically update transifex daily from the code. I'm left with the other way around (transifex -> code).\n. There is the script. Be careful, it doesn't work propelly yet, because of the languages to be deleted in Transifex, and because it doesn't create a git branch before commiting (should it ?)\nAnyway, have a look :\n``` bash\nGet latest updates from upstream\ngit fetch --progress --prune upstream\nAnd push it to origin\ngit push --porcelain --progress --recurse-submodules=check origin refs/remotes/upstream/:refs/heads/\nFast forward the local git repository\ngit merge --no-commit --ff origin/master\nFind every translatable string in the updated code\n./utils/update-translations.sh\nAnd compile the translations (Transifex uses the compiled files)\npybabel compile -d searx/translations/\nPush to Transifex the new strings to be translated, and the new translations from the code\ntx push -s -t\nGet from Transifex the latest updated translations\ntx pull -a\nUncomment this line to debug the script. This one is a life savior.\nset -xv\nThis whole block serve only one purpose : dismiss translations files that has only one change :\nthe date at the beginning of the file (because of pybabel)\nfind -path './searx/translations//LC_MESSAGES/messages.po' | while read d; do \n    git diff --stat \"$d\" | grep changed | while read -a arr\n    do\n        insertion=\"${arr[3]}\"\n        deletion=\"${arr[5]}\"\n        if [ $insertion == \"1\" ]\n        then\n            if [ $deletion == \"1\" ]\n            then\n                files=echo $d | head -c -3\n                git checkout -- $files\n            fi\n        fi\n    done\ndone\nCompile the new translations including those from Transifex\npybabel compile -d searx/translations/\nAdd the folder of translations to the commit\ngit add './searx/translations/'\nAnd commit it\ngit commit -m \"Translations update\\nFrom Transifex (with script)\"\nAnd finally, push it to origin\ngit push\n```\nIt works globally. I couldn't test the case there isn't any change in a tranlation file, because right now, all of them have some...\nYou have to have configured Tx first, and I use a different folder for the git repository, so there are no other changes in it. Also, I used SSH to connect to github with git, and using a private key, so Github doesn't ask for my credentials every time.\nWhen the supperfluous languages and the branch issue will be dealt with, I will commit it in utils.\nAnd when that is done, I will be able to have a cronjob running it daily (or whenever).\nI'm also fiddling with something to do the pull request automatically.\nA web server doing this would be better, and web hooks could be used, but I don't have a web facing server, and it's a start.\nAny comments are welcome, of course :)\n. Good point. But I thought that pushing to Transifex would allow to keep the old translations. But upon rereading the doc http://docs.transifex.com/developer/client/push I can say for sure that Transifex docs are not very clear ;)\nMy point was to be able to handle a translation coming from Github and from Transifex simultaneously.\nBut streamlining meaning streamlining, I would propose to delete every .po files in the repository, so the translators would/should/could only use Tx, resolving this issue. But it would also reduce the choices for the contributors. So I let @asciimoo decide.\n@dimqua is there a reason you proposed your translations fixes using code (pull request) and not Transifex ?\n. In that case, we should close searx on Transifex completly.\nKeeping both means two things : \n- It's a mess to do things properly\n- There could be conflict in translations.\nFor streamlining sake, we must only have one access point, at least for \"tourists\" translators.\n. For local modifications, the po can be generated using update-translations.sh as described in the wiki. Searx only uses the mo.\n. Oh. True.\nIn that case, would you agree to remove the po files if update-translations.sh would create the po files ?\n(I'll take care of doing it of course)\n. There is no benefit in the code front. The benefit is elsewhere :\nThe core of it is in the streamlining : it's easier for everyone to contribute, maintain, deploy. You don't have to have a bunch of wiki pages to explain how to do this kind of stuff, there is only one option.\nIt's like the installation : For now, there are a bunch of options on how to deploy searx. That's great. But, there aren't enough doc for it, scripts for it, and everyone is a bit confused when it comes to maintaining it, or solving others issues.\nSo in the case of the translations, it's only to avoid confusion, difficulty of maintainability, and overall, allowing us to streamline the experience by providing tools (scripts) that do everything \"unimportant\" around searx.\n. Hum, I don't understand your question. Can you reformulate it ?\n. I faced the same problem but came up with another way to go past it :\nA drop down list of engines which would appear on hovering over a category. We could either activate deactivate each sub engine in a category at search time, or pick the only engine we want to use.\n. As you may think, I agree with all of you ;)\nFor the website, I think there shouldn't be the tech doc, at least not at first glance. I think the site should be for everyone.\nAlso, Github works pretty well for issues.\nBut it could be used for \"tourists\" to post their issue, so they can receive help.\nSo, for me :\n1. Index page should be a brief description of searx, with links to an instance of it, and to a tutorial.\n2. Blog : so there is a place for an user to have the latest news.\n3. Forums : To discuss issues and projects. Also to create a bit of a community.\n4. Documentation : Tutorial + User manual + Administrator manual + Dev manual + live infos (your tool @pointhi ) + Github link\nOf course, the site must be internationalized (even if we'll start in english). There should be a way to use Transifex here too (I'm against using hundreds of tools to do the same thing).\nPersonnally, I'm against the roadmap idea, and I don't like Trac or Redmine for the same reasons : we have no idea of what we will code tomorrow. And nobody is giving tasks. We code on what we want, when we want it, and that's what I like about this project. Maybe we need a precise todo list, and call each task so we move forward and not everywhere/nowhere, but for now, it seems to work.\nFor the tool to use, I have no idea. I didn't know Jekyll, but I like it very much.\nI think that we mustn't use a wiki, but treat the site as code : we (the devs) have access to it, to modify it, but not the users/tourists. So for me, whatever we use, we must put it on Github. But I can totally understand if you disagree :)\nFinally, for the searx-style I'm all for it, but I'm also not a graphic designer (technically, I'm not even a webdev...)\n. The base_url works only if you run searx directly through python.  \nAre you using uwsgi + apache or nginx ? In that case, the issue is from the web server configuration.\nIn Nginx, the conf is described in the wiki, but for Apache, you should have something like that in the conf :  \napache\n<Location /searx>\n    Options FollowSymLinks Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n</Location>\nEDIT : Sorry, found the issue, that was not the conf, but the JS. I will commit shortly the changes, but if you want to do it yourself, change in the JS files \"/autocompleter...\" by \"./autocompleter...\"\n. Have you tried with the last master, from 2 hours ago ?\nIt may also be a cache issue. Have you tried to restart the servers, and empty the browser cache ?\nBecause, right now, I have no idea.\nIs it a public instance ? It would be helpful if it was, so I could debug it myself.\n. It's not exactly public, it ask me for a login and a password...\n. Ok, I did go to https://github.com/abeudin/searx_ynh to check if the source code was correct. And it seems that you did something very wrong :o)\nYou updated the source in the wrong place : \nI know it's tricky, but searx at is root has another searx folder. And it seems that you mixed both...\nIndeed, for example https://github.com/abeudin/searx_ynh/blob/master/sources/static/themes/oscar/js/searx.min.js is right and updated, because it contains \"./autocompleter?q=%QUERY\", but searx doesn't look at this file, but look at https://github.com/abeudin/searx_ynh/blob/master/sources/searx/static/themes/oscar/js/searx.min.js which contains \"/autocompleter?q=%QUERY\", and that was wrong. Note the \"searx\" in the path.\nSo if you use that repo, it is indeed normal that it doesn't work ;)\nEt si t'as besoin de plus d'aide, je peux aussi le faire en fran\u00e7ais ;)\n. And with the login/pass you gave me, I confirm that it doesn't have the corrected files.\n. You are welcome :)\nI'm happy searx is in YunoHost :)\n. Can you give us the answer ? In case it happen to someone else :)\n(or in case someone google it and land here...)\n. You don't want to merge this ?\nI'm just curious about why, and why the other request isn't merged either...\n. I hope you didn't take my question as pressure on you :)\n. Just so you know, it works now in Chrom(e|ium) (in 41+).\nAnd this pull request is almost two months old. Just sayin'...\n. Did you merge correctly the wrapper that handled redirect issue in the engines ?\n. I think there would be some issues there : \n- First, it would be a privacy issue : if you do a search and searx response time is low, you know somebody else on the server have already made the same request.\n- Secondly, if it should work in a cross users way, the chance that two persons would make the exact same search in a short time period is almost nil.\nSo for me the only use case would be when you do a search, click on a link, and then go back to the result page (with back button). In some cases, the search is launched again, and yes, this shouldn't happen.\nAs searx doesn't save anything, except stats in a macro way, and doesn't plan to save anything else, I'll recommend you to dig in uWSGI and/or nginx and/or Apache, and probably more on the two latter ones, because they are those serving the static (rendered) pages.\nAnd please bring us anything you will find, we are very interested :)\n. The URL stay the same because in your prefs, you are using 'POST' as a way to send the data to the server. If you set it at 'GET', the url will change.\n. I think it raises privacy issues, but I don't really know which.\nMaybe something could be implemented to allow that kind of setting in settings.yml. But I'll let someone else first answer the privacy issues...\nPas de soucis ;)\n. Maybe some should be merged in the infobox ?\n. Maybe, custom compare methods per category instead of per engine ?\n. In the virtual-env environement of course.\n. Good catch !\nYou can commit the '//' fix if you want, but look around for the others embedded to use the same trick.\n. It may be because your harddrive is full.\n. I personnally don't care.\nI'm commited to open source software, but when it comes to handling data that are not sensitive at all, I rather want to use the right tool than limiting myself to OSS.\n. Changing platform will take time for the devs away of developping searx, when changing platform add zero advantage.\nAnd beyond that, the teams of translators are already registered at transifex. So It would be a loss to move away from them.\nI said I didn't care, but now, I think it's a bad idea to move away from transifex.\nAnd beyond that, you can use a fully open source stack to contribute to the translations.\n. There is no ads, and nobody here is donating money to Transifex.\nWhen and if a paid platform will be considered, of course an open source one will be used.\nOh, and Transifex isn't a bad word, you can use it.\nFor migrating translators, they choose what they do. And some of them may not be on github, and some of them are not devs. And so, some of them will be lost when changing platform.\nSearx's code and translations are still open source, no matter what. Even if transifex is closed source, and even if GitHub is closed source, and even if the routers at your FAI use closed source firmware.\nBut nobody forces you to use, or to translate, or to contribute. You are welcome to do it, but nobody forces you.\n. > Translators can even login to Weblate with GitHub and other social network accounts, such as F and G.\nMy point was that a lot a them won't even know that we switched platform.\nOh, and BTW, you prefer using an open source platform but recommand using closed source credentials ? That's weird ;)\n\nI use DD-WRT. The translation platform in question was available as Free Software.\n\nWe are moving away from the subject, but I was talking about your ISP, not you. I'm sorry, FAI is a french acronym. And I'm sure your ISP use closed source Cisco firmwares ;)\n. I'm not familiar with the vulnerability, but maybe in the img tag, the image can close this tag and open a new script tag with malicious code inside ?\nBut if it's not the case (and I think it's not), preventing against this vulnerability will break a lots of images results.\nAlso, if it's only in script tags, we are not vulnerable at all, as searx doesn't allow to enter html code.\n. +1 to both.\nWe should work on widget/plugin as soon as possible.\n. The issue is, to have a valid certificate you have to pay big money to trusted companies.\nOf course, nobody in the searx list would pay.\nSo they have created certificates for their servers, that allow the most privacy possible. You just have to trust them, as you trust the companies that take big money when you buy on amazon.\n. Yes, so to resume, some instances appear untrusted because you don't trust their root cert.\nThere is nothing to do here.\nAnd there is no cert in TOR.\n(So, you can close this issue if you feel your questions have been answered, if not, don't hesitate to ask specific questions :) )\n. I guess it's inherent to the way the search is done through POST.\nMaybe by setting it to GET in the prefs, would solve this.\nI'll nonetheless look into this.\n. 1/ The point of having searx is that anyone can install an instance instead of using an existing one. It allow full trust this way.\n2/ Sure, there is less anonymity if you are the only user of an instance. But it's more than using the search engines directly : they don't know if you are alone or not, and there is no cookies from them. You could have build an instance for a university, and they can't say.\nBy the way, true anonymity can't really exist on Internet. Even using a VPN+TOR+... there are always ways of determining who you are. If you don't believe me, look at that : https://panopticlick.eff.org/\n. Change the preferences to GET instead of POST\n. Hum, sorry, I didn't fully understand your question the first time, I thought you were asking about search parameters.\nThere is no way to have the preferences in the URL instead of a cookie right now. I could be an evolution.\nBut keep in mind that the cookies used by searx are exactly what is displayed in the preferences panel, nothing more. So, there are no way of tracking an user with those.\nFurthermore, the cookies are only used by searx, and no one else. If you trust searx to do your searches, you can trust it to store the cookies ;)\n. Agreed, but I'll use the inverse order : Use in URL, if not, use cookies.\n. Maybe it's too high tech, and we should go back to the origin : what do you think about a simple checkbox ?\n. Agreed. That's the trouble with this kind of widget. I hate the iOS checkboxes for the same reason : I never know if it's on or off.\n. We are exploring the possibility of using a small database. If and when it's done, it will be possible to store the stats data into it, and be restart proof.\nBut nothing for right now, sorry...\n. That's a way of doing it, but I fear it's not a clean way : we could import the right packages directly into the source without doing version detection, by using the future namespace. It allows importing future package from their namespaces, without losing compatibility with Python 2.\n. Why not use the same result templates we are already using ?\n. It depends on the engines you choose. But yes, most of the search engines use word by word search and if you need to search a phrase, you have to put quotes.\n. I like that idea a lot !\n. If it's that, the logs should be fixed ;)\n. I'm not sure that it's a real drawback : it allow a faster pace of reading by grouping the same formatted entries together.\nI know it's a bit (a lot) hackish to use the video template for that. That's why I was working on a way to rebuild from the ground up the template system, to have it be able to adapt itself to every attachment we throw to it : images, videos, torrents infos...\nI did put it on the backburner for a lot of reasons, but I'm still working on it.\nAnyway, I don't think there is an issue for the user here (there are a lot for the devs, but that's another storie) : for readability purpose, it's cleaner and easier to have the different styles of entries grouped together.\n. YES for the logs !!!\nIf you know how to do it, you're welcome to modify this page :)\nJust, allow the user to choose (ie. don't add it hidden in the installation process, but instead in a sub chapter)\nFor the legal part, is there a legal issue ?\nI know there might be in France, because ISP have to log for a year the connections, but I'm not sure about websites. And I don't know about others countries.\n. Yes, normally, the flickr_engine.py use the REST API, with a key you had to get from Flickr.\nBut searx.me uses the flicknoapi_engine.py, which uses the HTML part of the site.\nSo, unless you want to get an API KEY from Flickr, we have to manage all with the changes on the HTML part.\n. Ah yes, I understand what you mean.\nBut on my side, there is no request to the rest api.  The page still contains the api response directly in it, like it was when I wrote flickrnoapi_engine. Sure the regex may need to be changed (I don't have the time to look deeply into it right now), but globally, not much have changed.\nI confirmed it with Chromium, Firefox and Opera 12.\nSo maybe it come from the user agent, or from the beta search in Flickr. But if you do a wget on the page, you'll get the \"blob\" with all the stuff that the noapi engine parse. You'll see it around line 2644.\nEDIT : Oh and I forgot, try to load the page without JS, it might help.\n. Awesome idea !\n. @privacytoolsIO If you can provide me a list of trackers arguments in URL, or anything similar, it would be welcome to enhance what I already commited #365 :o)\n. Nice !\nThis list could be very useful to remove unwanted links !\nBut I was talking more about the arguments in the URLs, like ?utm_origin=XXX etc.\nIf you know about others than 'utm' it would be very useful. If you don't, don't worry :)\n. I'm not sure, but I think it's the other way around : you have to add in the result header that it's gzipped so the browser can open it properly.\nBut I don't reallty know...\n. If it's read/written only once, it's not important if it's a file or a DB.\nIf and when the DB we already talked about come to life, we can move it to the DB, but meanwhile, the file is good.\n. For multiple reasons, the translations must be done on Transifex.\nBut thanks for noticing and reporting this error, I corrected it directly on Transifex, and it soon will be integrated in searx.\nDon't hesitate to report anything else ! :)\n. Can you check what is installed and in what directory you are ?\nIf you can give us the result of the commands tree and pwd it would help a lot.\nCan you also try to run it when exiting the virtual env ?\nJust run it with python searx/webapps.py, but not from \"/usr/local/, but from a path that looks like \"/usr/local/searx/searx-ve/lib/python2.7/site-packages/searx-0.7.0-py2.7.egg/\"\n. Again the same thing ?!\nThere is a big chance that you are right.\nWe have to be careful about that thing, it's not new.\n. I think what he wants is that searx provide a small page to be included in a iframe, with only the text input and a search button, which would redirect to a choosen page.\nThat could be really easy to implement.\n. @HLFH I think you are misquoting the article you linked.\n1. The article said that most search engines don't look at them, but not all. Yandex, for example, may look at them. Yandex represent 42% of searches in Russia. That's not small.\n2. The article say you must delete them only so your competitors don't know what keywords you are targeting.\n3. The article say also that keeping them won't hurt at all\nSo why deleting them ? Even if they are not used by most search engines crawlers, some might, and it doesn't hurt to keep them.\n@plsng Why not ? \"Privacy respecting search engine\". Bam. :)\n. A vote is not the way of searx. It's the consensus, based on arguments.\nYou have shown that the keywords are not used by at least 73% of the market. Fine. But why oh why should we remove them ?\nThey are not hurting, they are not harming searx. In fact, they are useful for 11% of the market, that reprensent ~330,000,000 potential users (of the 3 billions Internet users).\nLet's make a pros and cons list :\nKeeping them : \n- Pros : Might be helpful for the 11% search market share\n- Cons : We are not in the advance guard of technology\nRemoving them :\n- Pros : We are in the advance guard of technology\n- Cons : Might hurt on 11% search market\nFor me, the decision is clear. But if you have other arguments, I'm ready to listen :)\n. C'est dommage de partir juste parce que tu n'es pas d'accord sur un point :)\nThe point of this tool is indeed I think, not being bleeding edge, but stable enough to accomodate the most people possible (including the long tail), while respecting privacy. So we have to think of everybody, in Russia, in Africa using cellphones, in the silicon valley.\nAnyway, you are welcome to come back any time. And maybe not wait for the 0.9, but instead the 0.8 ;)\n(oh and btw, just so you know, Yahoo still uses keywords in its own homepage (granted, not Google, Bing, Yandex or Baidu do the same ;) ))\n. And https://thepiratebay.mn/ ?\nI can't agree with disabling this engine. Look at my comment in the PR\n. I'm waaaaaaayyyyyyy behind on searx dev, but I like writting engines, so I'm OK to do it, probably within the next week.\n. If this engine is deleted, so must be kickass, for the same reasons.\nBut I don't think it's the solution.\nMaybe, in the case of those search engines, a fallback possibility should be implemented -> Rotating domain name based on history of the local searx instance (already tried and failed or succeded this domain name)\n. The new pirate bay is set to change frequently of domain name, following seizure by authorities.\nThey have changed their logo to become an hydra (don't know the right word -> mythical creature that sprouts several heads when one is cut). It's the official TPB.\nSo their extension are .mn, .gd, .la, .am, .gs and .vg. And I think .am was already seized, but I didn't follow that that closely.\nThe solution could be a mix of both your solutions @dalf :\n- Each request, test the next one on the list that the last test was positive and don't display any result if negative\n- In background retest regularly every negatives' ones.\nBut what should be done also, is a way to configure that easily for the admins : this should go into the conf, with a rotator between it and the engines, so this solution isn't only applicable to TPB, but also to others in the future.\n. Nice !!!\nAnd good idea for the tag !\n. I think the 6sec + disabled was a good quick and dirty fix.\nBut yes, we need more :\n- Being able to set the timeout in the prefs (with a bit of info about latency)\n- Having a tiny note indicating what search engines timeouted on the result page\nAnd yes @asciimoo you are right too. We can't distribute a version which has this kind of issue (-> no warning of the big latency if enabled)\n. That could work too, but I'm afraid that the user will refresh the page sooner that the timeout, and so, resending the request, and reseting the timeout to 0 each time, keeping himself away from the results.\n. Good point.\nFor me, in this case, searx should return no result : \"There is no result using DDG in Files\". That's what feel logic to me.\nBut the logic could also dictate that the bang has a high priority, and every other options don't matter. So the current behaviour may be correct also.\nIn any case, that's a good point. We should ask \"common\" people their opinions.\n. There is indeed a problem here. But I can't find a simple way (and logic one) of solving it.\nOr maybe...\nCould we transform every entity known by searx in the keywords (like bang ! and ?) by a tag like item ? I know I'm not clear, but look at this image : http://i.stack.imgur.com/61w8h.png\nIf we can replace \"!ddg\" by a button containing \"!ddg x\" (x being the close button), we are not totally solving the problem, but it's easier to remove a bang afterward : just click on the x !\n(It may be a totally stupid or impraticable idea, but I'm just brainstorming here)\n. Me neither.\nBut maybe we can find out ;)\n. We are in no hurry :)\nI'm planning of tackling the youtube issues next week, and wanted to do the dict.cc and qwant engines in the same time.\nAlso, one big request I had when I asked \"not geeks\" to test searx, is to have more info on the prefs. So I was planning of doing a FAQ page, including links from the prefs.\nSo please, and if you agree, let's wait 2 weeks so I have the chance to do what I planned :)\nAnd if I don't succed, it was only two weeks ;)\nAlso, I vigorously agree with @pointhi. \nBut, build number, or Git id ?\n. I'll put it there so we don't forget : we must update the project on transifex before the release, there are a lot of strings not translated in the current version.\n. I wouldn't recommend it.\nYeah, there is no blocking issue, but we are still in the summer holiday time, which means that we can't have a bit of press with this release.\nLet's wait one or two weeks to be in september instead.\n. So, tuesday ?\n. Indeed :)\n. Yeah !!!\nNow, let's flood our social medias ! :D\n. I think that is an OS issue here, but I may be wrong.\n. Added a few Qwant engines : \n- Web\n- Images\n- News\n- Social (seems to provide only results from Twitter, but search efficiently)\nA few more possibilities are available from the Qwant 'API' : \n- Knowledge graph (with keyword 'knowledge')\n- smart_top\n- shopping\n- videos (but this one provide only videos from youtube, so I skipped it)\nSolve #326 \n(and sorry for putting it in the same branch as youtube, it was an error on my part)\n. The difference is that for each type, in qwant, it's a different source of data, like bing or google.\nIt allow a bigger granularity than TPB for ex. by allowing disabling the images or news sources.\nI don't think it would be a good idea to put them in a single file.\n. Yes , there is a bit of code redundancy here. But if we look at all the engines, there is a lot a code redundancy.\nIn this case, I think I used the right approach, because if there was only one python module, it must have done four (4) requests by itself (because it's 4 different urls), and that's not the pattern we used so far : \nUntil now, we did a module per request (url).\nSo, I'm not sure why suddendly it's an issue blocking this merge ?\n. Ok, I understand.\nBut in this case, since it's using 4 different URLs, each API can change independantly.\nIf you want, I'll put them all in a single file, but I strongly think that's the wrong approach here.\n. Nevermind, I understood, and I'll propose another way tonight.\n. Sorry it took me so long to understand. I tought for a moment you wanted me to perform four requests simultaneously.\nTell me if this way is ok.\n. Agreed, it's much better. With a dict too.\nAnd thanks for the pointers. They weren't the right ones, but thanks anyway ;)\n. Ah fu* yes... It happens to everyone ;)\n. I'll do a new page describing the settings.\n. Confirmed\nI'll look into it tomorrow.\n. Ok, I give up.\nIt's clearly an unicode issue. But python 2.7 is a real pain to handle this.\nAnd so I've tested lots of solutions, but found nothing usable.\nIf anybody know how to have a string with non-ascii chars encoded as \\uXXXX, that could be a good approach to a solution.\n. Hmmm, didn't try that, but could help a lot.\nBut there would still be an issue, because not all money chars are defined by their htmlentities. I mean, it would take a bit of work to check every data in currencies.json, and add the correctly encoded money symbols.\nOr it could be easy, by replacing every \\u by something like #xx;\nAnyway, I can't work on it this we, and my brain is fucked up by all this encode/decode/unicode/unidecode/str stuff to do it properly, so I passed the torch on this one ;)\n. Finally found it, but lost a bit of hair in the process.\n. Sorry for this one, it seems that having olds cookies can mess with the blocked/disabled engines. Reseted the settings and everything worked as it should.\nClosing it.\n. I have a comment off topic :\nI don't know how it's usual done, or if I'm something wrong in my repo, but anytime you create a new branch in this repo, it's created also in every forked repo.\nSo my request is that, when you want a new branch, to request a pull a little while after, could you create it in your own forked repo ?\nSince this repo is the \"master\" one, I think it should be as clean as possible, having branches only when differents versions of searx must be developped alongside, and not for \"tiny\" commits (I don't use tiny as a perjorative word here, it's just opposed to a massive overhaul of the codebase).\nAnyway, that's my 2 cents, and I may be wrong :)\n. Don't worry and thanks a lot :)\n. That's weird. Have you a lot of traffic on you site ?\nAnyway, first, check the logs. Maybe it's something silly.\nSecondly, try to perform a search on Google for ex. from the IP used by the server.\nThird, try to change the user agent in settings.yml.\nIf nothing of those works/helps, maybe your hosting company propose a dynamic IP, that can change regularly, and thus bypassing the block.\nAnyway, I think that's really weird. I can imagine one or two engines blocking you, but all of them ?! That's really really weird.\nEDIT : Oh... Maybe it's your host that blocked your requests, because it thought that you were sending spams/DoS...\n. For the log file, it depends on the way you installed it. If you use apache, it may be in the logs of apache, or nginx if you use nginx. Maybe try to start manually a different instance to test it, with the command python searx/webapp.py\nAlso, to test if you are blocked, you can try launching a browser from the server and doing manually a search on the problematic search engines. Even a text browser would work.\nI already had a captcha on the search page of google, because I worked in an entreprise that had only a few external IPs, so google saw us as spam/DoS.\nThe search are performed differently depending on the engines. For some, it's scraping the webpage, for others, using public api, and some others, private apis that needs api keys.\nIt's hard to give you a list, as it depends also on your configuration, but you can check in the source code, in the \"engine\" folder each engine, they all have a header describing how they collect their data.\nOnly engines using APIs could be considered as stable, and even then, a ratio could be applied when they want...\nAlso, they could block you for a day or two, and then unblock you.\n. Hum... I agree, it needs retooling.\nBut :\n- result['parsed_url'] has always empty arguments\n- I found those regex in a JS to remove tracker, I forgot they only applied to the arguments parts. I'll retool them.\n- That explain the '?' string : after removing all arguments, if there is only a '?', we can also remove it. The order of the subs is important.\nAgreed also on the list for the regexes.\n(It pisses me off when I produce bad code like that)\n. @asciimoo ok, tell me if this is better.\n. For the split('?', 1), I thought about it, but it's more useful to test the lenght after, and doing nothing if the length is not 2. It eliminates the \"no query\" and the \"hmmm there is something bad in this URL\" type or results.\nAs for result['result']['parsed_url'].query, when I tested it, it always had empty arguments.\nSo I think I used the most usable way of doing things. But, as always, I may be wrong.\n. How can you try with a link you provide ?\nI'll test again. Maybe I messed up, but I don't think I did.\nAnyway, and off topic, isn't it redundant to have the url and the parsed_url in the same var ? It could be tricky too, if you change one but not the other.\n. Yeah. I messed up.\nI looked at params, and not at query...\nIs it good this time ?\n. Ah finally ! :)\nFor the redundancy, for me, the issue is not so much that it's redundant, it's that inevitably, in the future, a bug will arise from that, because one value would have been changed but not the other.\nAnd yes, it's handy, but it's pretty easy to infer one from the other, at any time : it's just a urlunparse/urlparse away...\n. Yeah, you are right, it's not formally a FAQ, but I used the user point of view, and tried to answer the questions \"What is X ?\" each time.\nI had some return from non-geek trying searx saying that they didn't understand what autocomplete meant, for example, and that's what I tried to answer.\nAlso, I think common users are more used to the form of a FAQ that give quick answers than documentation.\nAs for only a version for Oscar, I looked to include this in other themes, but Oscar is the only one providing a beginning of info on items, where I could add a link \"More info...\".\nBeyond that, what I think is more important, is providing info to the default theme (not the theme named \"default\", but oscar, in our case), so the first time an user uses searx, he has access to it. If he changes something in the settings, I think that this particular user is not a new comer anymore.\nAnyway, that was my point of view, but if you want me to change anything, don't hesitate to tell me :)\n. Hum... Yes !\nBut I don't know how to do it without JS.\nI'll look into it.\n. Ok, I (finally) changed it to icons+hover text (title indeed), so, no JS.\nBUT I made the icon myself, and it's... I don't have a word for it. Abysmal ? Horrendous ? Despicable ? You get the point.\nSo, I need your help on that one to build an icon that fits gracefully in the UI.\nAnd it might also be useful to do icons for the others themes, so I can integrate this FAQ to them.\n. I wasn't asked, but I think it's indeed a good idea to merge the two.\nAlso, @ldidry, the regex should be compiled outside the function, in the class : in the function, at each request the regex is compiled, in the class, it's compiled only once.\n. Those settings are stored in cookies. So if you use an extension or a setting in Fx that deletes automatically thoses cookies when you close the browser, that's the cause.\nAlso if you use private browsing.\n. I'm not familiar enough with DDG. What does it means ?\nWe can switch language in the settings pretty quickly, isn't that enough ?\n. So, +1 indeed.\nIt could be done with a few other button to increase specificity of the search, not related to the settings.\n. I would recommand that you try to do it the hard way : \nCreate a new thread containing the solve(), and if it's still alive 10s later (or whatever the time is set in pref), you kill it (if it's still alive, it didn't return the solution and killed itself).\nIt's not classy, but should work.\nLook at https://docs.python.org/release/2.7/library/multiprocessing.html and the answer with 25 votes in your link.\n.  3 . The others are error 404 or 403.\n. The issue is not that. Stateless means that searx doesn't remember anything : you can kill searx and relaunch it, and you will find the same results. But, for connecting it with a seeks node, it needs a long term memory. That will change a lot of the way searx is working right now.\n. Maybe this is stupid, but couldn't we include a PHP page that lists the options of the server, so we can know how it is configured ? Like a phpinfo() page, but only with what we need ?\n. I think it would be dangerous to do that on the whole searx plateform, but a plugin allowing the user to blacklist some URL could be feasible.\n. Shouldn't it be done at DNS level ?\nIf the DNS return an AAAA response, it has ipv6, if only A, only ipv4. Which means that if we limit ourselves to the domain names (and not their IP adresses), we should be fine.\nOr am I wrong ?\n. It doesn't fallback when the language is not present :/\n. Nice ! (and quick !)\n. Also, setting the debug variable in the settings of searx to true could help finding the issue.\n. And rename the folders to you name theme. It's the folder name that searx detect and is used as name.\nAlso, modifying a theme doesn't need a restart of searx, but adding one does, so don't forget to restart it after the copy :)\n. It looks like an issue with your settings.yml file.\nAs for the \"restarting searx\" how do you do it ? Do you restart just apache ? Or uwsgi too ?\nOr do you run searx by running it directly using the command line ?\n. Ok, I think the whole thing is an issue with virtualenv. If you followed the wiki to install searx, virtualenv copied the whole repository in its virtualenv folder. Which means that every modification you made at the root of the repository (something like /usr/local/searx) weren't visible in your instance of searx, because it didn't look in that folder, but looked in the virtualenv folder (something like /usr/local/searx/searx-ve/lib/python2.7/site-packages/searx-0.7.0-py2.7.egg/searx/)\nIf you didn't make too much change, I recommand that you scrape the whole thing, to start from fresh.\nAnd before deploying using nginx, uwsgi, virtualenv etc., if you have some devs to do, you should try to just run in the searx folder python searx/webapp.py. It will run an instance of searx following the settings (using the port number in the settings). With that, you can start, kill, restart searx with ease with that. And it will give you some more info if you activate the debug mode in the settings.\n. Ok I'll do it, probably tonight.\nBut using the API will mean having an API key. So first, I'll fix the non-API engine, and next, provide a new APIable one.\n. Yep. But it means that we can use 500px own key to request the API.\nIn that case, sdk_key=b68e60cff4c929bedea36ca978830c5caca790c3 :D\nSo I'll build an engine that is based on the API, but can work without a key, using 500px key.\nBut it would be useful to look at it, to know if that key get revoked a lot, or change every day, or just, is stable.\n. Oh, too fast !\nYou didn't commit it ?\nI found it in a cookie. I wasn't logged or anything, and in a new profile of the browser, so, nothing personnal to my account.\n. Hum, weird. No idea. \n. What requests do you do to get those ?\nJust to be sure we are talking about the same thing...\n. Hum, I have no idea.\nI tried with every cookie I had, but nothing. There is a callback present in one cookie, that could be a clue. But I won't look into it until tomorrow.\nIn the mean time, having the engine would be good, even with watermark, they still can go to the website to get the full image.\nAlso, from https://github.com/500px/api-documentation/blob/master/basics/formats_and_terms.md#image-urls-and-image-sizes : \n\"You'll find the URLs to the image(s) for a photo in the images field in the returned JSON for a photo. The images provided with our standard API access will be watermarked with the 500px logo and attribution. For non-watermarked images please contact sales@500px.com\"\nI think it could be possible to get image without watermark, but it's not that bad.\nAlso, why '&exclude_nude=true'\\ ? :rolleye:\n. You can add them in https://github.com/asciimoo/searx/wiki/possible-search-engines if they are relevant.\n. You can do that already, no ?\nAs for other engines than google, it depends on those engines.\n. Oh, didn't understand that.\nThat could be a good idea indeed.\n. I didn't have the time to review the code, so my comment won't be that useful. But please, when you commit, always write a comment. It's easier for everyone if you do.\n. My answer to #407 could work here too.\nA locally (on the user computer) stored list of notes on previous result could help order/hide some results.\n. Hum, I don't understand.\nDo you want us to implement a way to restrict results to the localization of the user ?\nIf so, it's partially done, but is set in the settings, by choosing your search language.\nBut doing more, as in, finding out where the users are, shouldn't be our goal.\n. Ok, but what are the advantage of your solution against using searx with format=json option ?\nBecause this option will indeed bypass the template engine, not using as much ressources as the classic IHM way.\nJust asking to better understand what you did :)\n. I agree on what you want to do, but it seems to me that it's not the right way to do it :\nFrom what I can gather from your code, you are duplicating the central engine of searx to provide the right JSON response.\nFrom my POV, the right way to do it would have been to :\n1.  Extend the current format=json to add in it what is missing\n2.  Add the JSON settings directly into the main engine, so that if present, the JSON settings will replace the defaults and/or the cookies' ones.\nWhich would have limited the duplication of code, while giving so much more possibilities.\nWith this solution, you would also still be able to update the engine easily, since your UI is elsewhere, and it would be also possible to serve it using Apache/Nginx (or even none of this as the engine can be started with python searx/webapp.py)\nBut that's only my opinion. Anyone else ?\n. You'll see, python isn't that hard ;) (I'm from Java and C++ myself)\nOk, so just to be clear, this PR won't/shouldn't be merged.\nWhat I'll recommand is that you try just to do the point 1 and do a PR just on that. No new entry point, just using what is already in place, enhancing it by adding what is missing.\nWe will better be able to guide/help you on smaller code than on the whole thing you already commited.\nI don't think we need multiple entrypoint to separate api or not. format=json should (must ?) suffice.\nAnd why care about enabled entrypoint if they are not visible ? If you wish to embed searx in an app, you still can, even if the UI entry points are enabled, they won't be used, and won't use ressources.\nAs for the HTML error page, I'll look into it.\nIn any case, start small, and build on that :)\n. Ok, I can reproduce in Fx, O12 and Chr.\nThere is an answer from Stackoverflow that contains the text <meta http-equiv=\"refresh\" content=\"0;url=http://example.com\">. It seems that we don't escape rightly the answer texts.\n. Agreed\n. This probably won't help, but can you check that pyasn1-modules and certifi are also installed ? Just in case.\nThe other (unlikely) possibility is that pip doesn't use the same python version than... well, python.\nFinally, I would like you to look directly in the right folder to see if the package are rightfully here. If you followed the guide in the wiki for the installation, the path should look like /home/tristank/venv/searx-ve/lib/python2.7/site-packages/. There should be a folder \"searx\" containing our code, and several folders for each of the requirements and their dependencies, present in the file requirement.txt in our code. Be also sure that you are indeed executing the command to run searx from /home/tristank/venv/searx-ve/lib/python2.7/site-packages/searx.\nThose are the low hanging fruits. For the rest, we will look after you check everything listed above ;)\n. The last one should be executed inside the virtualenv : you installed everything inside the virtualenv, like it was another machine, so now, you should run it inside it, so it can see the package installed inside it. I have no idea if pointing to it directly like you did would achieve that.\nAlso, you didn't answer the question about the version of pip and python. Execute python --version and pip --version to achieve that.\nFor example, on my machine, python use the version 2.7.9, but pip install packages for python 3.4. I have to use pip2.7 to install the package for the right version of python.\nIf it's not any of that, I will pass the torch to someone else...\n. The fact it's a meta search engine means that it's a difficult task : we have lots of results from lots of search engine. Even if we could get each number of results from each engine, adding them won't work, because of course, there are duplicates between them (duplicates that we filter before showing the results).\nBut maybe we could list the number of results for each engine used.\nBut that's a big \"maybe\", because it depends of lots of factors (does the engine provide such number, is it available through the api we use, how to display it...)\n. @nettlebay No, keep this bug open, it's not resolved yet :)\nWhile it's open, it's on our todo list, if it's closed, we'll probably forget it...\n. I have not looked at this idea recently, but I was partway through implementing a dynamic template a while back.\nAnd the issue is that it's very difficult. Not impossible, but demanding a lot of calculations and recursive algorithms to go deep in the trees of possible layouts.\nI'm not saying that what you are proposing is wrong or unhelpful. On the contrary, I think it would help us do a better work on the backend.\nBut I also think that it will be the easy part, and that building the corresponding layout won't be that easy.\nBut we should do it :)\n. We had started something similar a while back : https://github.com/Cqoicebordel/searx/wiki/Format-of-answer\nAnd for the full data on building an adaptable theme : https://github.com/Cqoicebordel/searx/wiki/Meta-Theme\n. Yes yes, I'm agreeing with you. I think indeed building upon schema.org is a good idea. I just wanted to point out that some of the work already done can be reused (notably the bin packing).\n. It could be mostly done, but there is the issue of the images. The logo contains the text searx and it's not that easy to change the text on an image. So some work should be done on your side too.\nBeyond that, I don't know if it must be done on our side : the project is named searx, so it would be normal that the name appeared on it. If you want to change its name, you are free to do so as it's an open source project, but I don't think (and it's only my own opinion) that we should help you change its name.\nFor me (again, just for me), it's like we gave you a car, and you asked us to also change the logo of the brand on it. It would be weird to ask Mercedes, or Audi (or any other) to do that.\nAgain, just my own opinion, and it's open for debate.\n. (there is no commit for ixquick. Is that wanted ?)\n. Sorry, jumped the gun a bit too quickly ;)\n. I love that idea, especially f-droid, for their Open sources softwares.\n. From my POV, it has enough differences to be another theme altogether. Could be the default one though.\n. > My 2 cents: the results are too loose to me. I prefer compact result pages where i can see 8-10 results without scrolling. Currently only the \"default\" (we should rename it) theme compact enough.\nPix-art is really compact too ;)\nAgreed on the renaming of default, since it's no longer the default.\n\nAs i see it doesn't modify the structure of the theme and i don't prefer a new theme with lots of duplicated template code. Maybe a better solution would be a selectable CSS styling for oscar - like in courgette theme.\n\nAgreed. Seems to be the best option.\n. Works for me in searx.me, in Chromium, Opera 12 and Firefox.\n. You should delete this print\n. ",
    "gitbugged": "Was about to open a duplicate when I found this ticket. Searx is very centralized around Searx.me, and it's overloading the server. I see two potential options.\n1) A round robin DNS that lands on a random, verified working, searx node.\n2) Implement some form of simple federation. e.g. If a search times out, ask the user to retry the search on another randomly chosen searx node. We already have a list to pull from.. @asciimoo, for some reason it was missing. Pulling latest git fixed the issue - thank you!. ",
    "vincib": "sorry, didn't see ubuntu install instructions ... closing\n. ",
    "blindly": "I agree @pointhi. Seems like a few changes that shouldn't have been included did.\n@asciimoo: I will modify those as suggested. Thanks.\n. ",
    "evaryont": "It'd be great to be able to integrate DDG's open source instant answers with searx more directly. Unfortunately, it's unlikely to directly embed them in the source code or server process -- the maintenance burden would be too high. I think something that'd be easier to maintain would be a secondary process written in perl with a lightweight API (HTTP on loopback, or something else) exposing the \"goodies\". (Unfortunately, not every instant answer is available on Github, some of them remain closed source due to licensing agreements with whomever.). ",
    "posativ": "Can you post a minimal, failing configuration? It's either an Apache quirks or mis-configuration. Nginx works fine with HSTS and uWSGI.\n. ",
    "xinomilo": "i can confirm that Nginx works fine with HSTS and uWSGI.\n. i dont use apache, but can you try these instructions : https://raymii.org/s/tutorials/HTTP_Strict_Transport_Security_for_Apache_NGINX_and_Lighttpd.html , on the apache searx config file? \n. just found this, dont know if it helps.: \nhttps://uwsgi-docs.readthedocs.org/en/latest/Snippets.html#force-https\n. i dont think that affects you in the scenario provided in the link. shouldn't be  different\n. well in any case i'll close this issue, since it doesn't seem people mind using transifex (or/and github). personally, i already quit transifex, so no more translations there for me. probably github in near future, as soon as there is a tested nice free alternative (and also not sexist one)\n. there are free certs, from startssl that work in 99% of browsers. no money asked there.\nto open cacert ssl sites, you need to download+import their root cert first, since its not in the browsers.\ncomodo offers some very cheap positive ssl certs, anyone can use. \na few are self signed from what i see, which in theory are better than the commercial certs (nsa associates...). \nproject \"lets encypt\" will make it much easier (and free), for everyone to encrypt their site traffic.\ntor HS instances are not https. tor is encrypted network, doesnt need https.\nalthough some companies start to sell .onion certs as well.\n. i saw that for nginx, access_log is set to off.  i think  its probably better to use access_log /dev/null; & error_log /dev/null;, cause otherwise access_log goes to default access and error log, of nginx.conf main config.  could edit wiki myself, would like to hear more opinions on this.\n. +1\n. have you noticed the Self Informations plugin? you can check which searx instances provide your real ip and which ones dont. just search for \"ip\" in any searx instance. could be a way to check, although maybe not be 100% reliable and certainly doesnt prove anything about server logs.\n. as i said it doesnt say anything about server logs. thats something only server admins know about. but if someone anonymizes ips in web server, this is visible on searx (not 100% reliable as stated). eg, my instane displays 127.0.0.1 ip for every guest, other instances i checked provide server ip for every guest (= replacing real ips with server ip) and others provide real ips. so its something to check... and if you're a server admin there are a lot of ways to implement this. \n. I'm ok with the wiki page. i'd suggest adding a contact field(s) such as contact_email (+ gpg_key), like in tor relays. \nAnon logs could be specified by the instance admin like : 1) no logs (/dev/null) 2) anon_ip logs 3) full detail logs. with comments as you suggest to clarify special setups.\nand maybe hidden services should be stated as different instances. not along with the public ones as it is now.\n. yes it is. damn forgot all about it. can i change tmp dir during installation? \n. it did, thx very much for the help. \n. works fine for me. have you tried clearing the cache from your browser? \n. works fine in debian sid with chromium. \nmaybe clearing the cache, or trying a new profile in chrome to check? \n. i'd suggest a privacy aware mailing list provider, like riseup.net/autistici.org or any of the others listed here : https://help.riseup.net/en/security/resources/radical-servers.\nwould like to read more suggestions on this. preferably not about commercial providers :) \n. i would also go for autistici.org, so if noone has objections about it,  i'll be requesting a mailing list from them in the next few days.\n. how so? care to explain that? how is a .io domain helping militarism?\nAnd, i've read both autistici manifesto (https://www.autistici.org/en/who/manifesto.html) and policy (https://www.autistici.org/en/who/policy.html) and i still think searx-instances mailing list should be hosted there.\n. my personal belief is that any public searx instance operator is consistent with those purposes/goals. \nsearch is a big piece in the whole \"privacy bazaar\", and i think searx is doing its best to keep us out of it. maybe its not as \"big\" as the tor project (for example), but still i find it equally important.\njust my 2cents :) \n. autistici notified me today that the list is ready.\nunfortunately i have very limited time as i'm in the process of moving to another city tomorrow. \nso, in the next days (when i get some free time ), i'll post details and probably update the instances wiki page to include the new mailing list. \nany volunteer(s) for helping out with list administration/moderation would be most welcome :) \n. @asciimoo i'm sending you an email with the list details. :)\ni'm still with very limited time, but i'll try to edit the wiki and make the list details public, soon.\n. just wrote something quick in the searx-instances page in the wiki. \nif that's enough/ok, i guess we can close this issue.\n. i think its time to close this, issue tracker doesnt need to be full :) \n. my instance doesnt have extra ips. just one.\nduring searx 0.8.1, i noticed google was blocking it, but after upgrading to 0.9.0 everything seems to work fine again. maybe its just less users (=less hits from that ip), dont know for sure, since i dont like access logs :) \n. and you can always host your own searx instance if you dont trust some instance's operator.\n. just came uppon this while moving from nginx to an apache2 server....\nwhat worked for me, was just adding \nRewriteEngine on\njust before \n<Location / >\nEverything else was exactly according to instructions. no extra ports or proxies. apache2+uwsgi just like with nginx. \nif someone can doublecheck/confirm this, then an update on docs would solve this for others as well, as it seems a common issue for apache users.. but because i needed some web headers, without me messing searx installation (https://github.com/asciimoo/searx/issues/715#issuecomment-257976755) , i went to reverse proxying solution with apache proxy_uwsgi module according to this : https://github.com/asciimoo/searx/issues/686#issuecomment-245639501. i can confirm this with debian jessie, nginx 1.6.2, python 2.7.9, uwsgi 2.0.7. \nafter reverting back to 0.12 release patch, searx starts normally. \nwith latest patches, uwsgi doesn't log any error, and nginx justs timeouts : \nconnect() to unix:/run/uwsgi/app/searx/socket failed (11: Resource temporarily unavailable) while connecting to upstream, client: xxx.xxx.xxx.xxx, server: searx_instance, request: \"GET / HTTP/1.1\", upstream: \"uwsgi://unix:/run/uwsgi/app/searx/socket:\", host: \"searx_instance\"\n. startpage, bing, reddit and others, aren't accessible to ipv6. no AAAA records.  in ipv6 vm, most engines produce same error.\nwikidata is working here in a test ipv6 instance, so don't know why the exception there. what does uwsgi say? . usually you get \"[Errno 101] Network is unreachable\" for engines without AAAA / ipv6.\nthis error \"[Errno -9] Address family for hostname not supported\" could indicate something else wrong with ipv6. \nlocal setttings maybe(?) is everything else working ok in that host with ipv6? dns lookups? . using nginx and the latest update, no such error here. try restarting uwsgi and maybe check for stuck processes. uwsgi log might also have more info.. python-babel package replaces it. python-pybabel was a transiotional package for some time, it's now removed in newer ubuntu & debian versions. same error in debian stretch. maybe the installation guide needs to change in https://asciimoo.github.io/searx/dev/install/installation.html#installation. remove folder /usr/local/searx, and start a new installation. yes, startpage is working with 0.15.0, tested in 2 instances with 2 different search languages.\nin your instance, maybe you should check out the errors first to figure out the problem... if you run apache/nginx + uwsgi setup, you can look up the uwsgi log for any errors regarding startpage or other engines.\nas far as i recall, there was never an \"all\" dropdown selection. just a \"default\" or lang \"en\", \"de\" etc.. . did you install using docker or classic installation? \nhttps://asciimoo.github.io/searx/dev/install/installation.html\nfor docker i'm not sure (don't use it), better let someone else to respond. \nin classic installation if you stopped at\nCheck\n$ python searx/webapp.py\nyou have to complete next steps too. \nstop that command from running (Ctrl+C) and go on with uwsgi and web server installation instructions.. \n. you could use https://github.com/matomo-org/matomo-log-analytics for importing raw web server logs to matomo. requires a matomo instance also (mysql/php). if you do go with matomo, make sure to turn on anonymization settings, for better user privacy.\ngenerally speaking, i wouldn't use a public instance that logs queries/users.\njust my 2c :) . i'm having similar issues in image search (google images only), since 0.15.0 upgrade. changed themes/languages, no difference.  (instance: search.stinpriza.org )\nerror message in each image search (line number is different in each refresh/search, but always same error message) : \ngoogle images (unexpected crash: line 476: htmlParseEntityRef: expecting ';' (line 476))\nuwsgi log:\n\nERROR:searx.search:engine google images : exception : line 476: htmlParseEntityRef: expecting ';' (line 476)\nTraceback (most recent call last):\n  File \"/path/to/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/path/to/searx/search.py\", line 87, in search_one_request\n    return engine.response(response)\n  File \"/path/to/searx/engines/google_images.py\", line 68, in response\n    dom = html.fromstring(resp.text)\n  File \"/path/to/searx-ve/local/lib/python2.7/site-packages/lxml/html/init.py\", line 876, in fromstring\n    doc = document_fromstring(html, parser=parser, base_url=base_url, kw)\n  File \"/path/to/searx-ve/local/lib/python2.7/site-packages/lxml/html/init.py\", line 762, in document_fromstring\n    value = etree.fromstring(html, parser, kw)\n  File \"src/lxml/etree.pyx\", line 3213, in lxml.etree.fromstring\n  File \"src/lxml/parser.pxi\", line 1877, in lxml.etree._parseMemoryDocument\n  File \"src/lxml/parser.pxi\", line 1758, in lxml.etree._parseDoc\n  File \"src/lxml/parser.pxi\", line 1068, in lxml.etree._BaseParser._parseUnicodeDoc\n  File \"src/lxml/parser.pxi\", line 601, in lxml.etree._ParserContext._handleParseResultDoc\n  File \"src/lxml/parser.pxi\", line 711, in lxml.etree._handleParseResult\n  File \"src/lxml/parser.pxi\", line 649, in lxml.etree._raiseParseError\nXMLSyntaxError: line 476: htmlParseEntityRef: expecting ';' (line 476)\n\n\nanother error, not so frequent (line number is also sometimes different): \ngoogle images (unexpected crash: line 26: Tag footer invalid (line 26))\nuwsgi log : \n\nERROR:searx.search:engine google images : exception : line 26: Tag footer invalid (line 26)\nTraceback (most recent call last):\n  File \"/path/to/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/path/to/searx/search.py\", line 87, in search_one_request\n    return engine.response(response)\n  File \"/path/to/searx/engines/google_images.py\", line 68, in response\n    dom = html.fromstring(resp.text)\n  File \"/path/to/searx-ve/local/lib/python2.7/site-packages/lxml/html/init.py\", line 876, in fromstring\n    doc = document_fromstring(html, parser=parser, base_url=base_url, kw)\n  File \"/path/to/searx-ve/local/lib/python2.7/site-packages/lxml/html/init.py\", line 762, in document_fromstring\n    value = etree.fromstring(html, parser, kw)\n  File \"src/lxml/etree.pyx\", line 3213, in lxml.etree.fromstring\n  File \"src/lxml/parser.pxi\", line 1877, in lxml.etree._parseMemoryDocument\n  File \"src/lxml/parser.pxi\", line 1758, in lxml.etree._parseDoc\n  File \"src/lxml/parser.pxi\", line 1068, in lxml.etree._BaseParser._parseUnicodeDoc\n  File \"src/lxml/parser.pxi\", line 601, in lxml.etree._ParserContext._handleParseResultDoc\n  File \"src/lxml/parser.pxi\", line 711, in lxml.etree._handleParseResult\n  File \"src/lxml/parser.pxi\", line 649, in lxml.etree._raiseParseError\nXMLSyntaxError: line 26: Tag footer invalid (line 26)\n. \n",
    "wiewo": "had the same problem with apache+uWSGI. Adding the following line to searx.ini solved it:\nroute-if = equal:${HTTPS};on addheader:Strict-Transport-Security: max-age=31536000\n. possible, I'm on debian jessie with uwsgi version 2.0.7. Maybe it's also possible to force apache to add its header data? \n. ",
    "jleclanche": "Gevent seems to be \"almost there\": https://github.com/surfly/gevent/issues/38. \n. @stef huh?\n. @stef Ive ported hundreds of apps to Python 3, please don't spread random FUD. 2to3 does 90% of the work and it's very easy to be compatible with both 2 and 3.\n. @stef Python 3 is not a different language, and if you really believe that, I suggest you port this just so you understand how easy it is for your own benefit.\n. I'm unsubscribing from this issue. If @stef is the sort of mindset people have to deal with in this project, I want nothing to do with it.\n. ",
    "josch": "Hi, I'm interested in packaging searx for Debian but as Debian is trying to get rid of Python2 (it will not be maintained past 2020 and is already on the way out of the standard installation) I would not like to add yet another package which is python2-only.\n@Cqoicebordel you say you already did the conversion to py3. Is your patch public somewhere?. Excellent! The py3 branch works great. Thank you!\nIsn't this a py3 fork of the robot framework (which also happens to be compatible with python 2.7): https://pypi.python.org/pypi/robotframework-python3. Hi, I'm a Debian developer who is currently looking into packaging searx for Debian. I found this issue and wonder:\n\nis this particular issue about just create a .deb package that you then distribute to users of Debian and its derivative or\nis this issue about creating a package that complies with the Debian policies and guidelines so that it can be distributed by Debian and thus also its derivatives?\n\nIf your goal is the former, then issues like:\n\nDebian doesn't allow downloading of anything during package build\nDebian doesn't allow embedded code copies\nDebian releases only every other year and expects certain versions to be supported during that time even if there are new releases by upstream\nDebian might contain versions of libraries that are too old or too new\nUpgrades must work even across multiple versions\n\nwill be irrelevant for your use case as you can easily create a .deb package that ignores all these issues and instead bundles up everything you need into one single directory structure that is then unpacked on the host system.\nI thought this is what you want but then several of the former posts bring up the several packaging policies from the Debian wiki which should be irrevelant for this use case.\nSince I'm currently working on a package that complies with Debian policies such that it can be uploaded to Debian proper I can give some of my own experiences (and questions) here:\n\nSince Debian is trying to throw Python2 out I'm using the py3 branch of searx\nInstalling the following dependencies from Debian Stretch seems to make searx work perfectly: nginx  uwsgi uwsgi-plugin-python3 python3-requests python3-certifi python3-yaml python3-pygments python3-werkzeug python3-flask python3-flask-babel python3-lxml python3-dateutil python3-openssl python3-ndg-httpsclient python3-pyasn1\nI only change the secret in the settings.yaml do I have to change anything else to distribute the result? In particular, do I have to change the port, bind_address or base_url settings when running searx behind uwsgi? It seems to run fine with the defaults.\nsearx seems to work fine out of a directory that it cannot write to. Does searx really not write any logs or caches to anywhere? This would make the \"upgrade problem\" really easy as there is no data to either convert or regenerate once a new version gets installed.\n\nThanks!\ncheers, josch. Hi,\nQuoting Alexandre Flament (2017-01-06 14:25:53)\n\n\nI think python3-pyasn1-modules is missing (0.0.8 in requirements.txt, 0.0.7 in unstable)\n\n\nI do not have that package installed at all but searx seems to work fine for\nme. I cannot see any \"from pyasn1 import modules\" anywhere in the code. Which\nparts are supposed to break without the python3-pyasn1-modules package?\n\n\npython3-certifi is based on version 2016.2.28 in unstable (instead of\n2016.9.26). I don't know how this package is maintained ?\n\n\nI'm not familiar with the package but again there seem to be no problems\ndespite the older version. If you tell me how to reproduce a problem introduced\ndue to the outdated version, then I can upload a newer version of\npython3-certifi into unstable.\n\n\nthe base_url has to changed if searx is not served at the root of the HTTP server.\nbind_address and port can be anything as long the HTTP server (apache, nginx, h2o, haproxy...) uses this (bind_address, port) tuple.\nother settings are ok, see :\nhttps://asciimoo.github.io/searx/dev/install/installation.html#installation\nand https://github.com/asciimoo/searx/wiki/Searx-with-haproxy\n\nIf there is an apache / nginx configuration, it would be good to add some HTTP headers to increase the security. See https://github.com/asciimoo/searx/wiki/Searx-with-haproxy#security \nStill about HTTP front-end, It would be good to encourage / force SSL, I don't know if it is possible ?\n\nI talked with some fellow Debian Developers about this topic yesterday. It\nseems that because consumers of the package will have very heterogeneous\nrequirements regarding their setup, the preferred way of shipping the service\nis to not enable it by default after installation but to instead provide enough\ntools that help the user to set everything up correctly. Another way to handle\nthis would be to ask the user on installation time about the setup they want to\nuse. The disadvantage about this approach is that one then has to cover all\nthe important use cases which easily lets the complexity of such a setup\nskyrocket, especially taking upgrades into account. Thus, to keep it simple, my\nplan was to provide default configuration files with helpful comments for a\nuwsgi/nginx setup. After installing the package, the user then just has to copy\nthese files into the right location and restart uwsgi. Does this sound like a\ngood solution to you?\n\nAbout maintenance : if an engine changed its API, the searx has to be changed\notherwise searx can't use anymore this engine. Can the changes be backported\nto the debian package even into the stable version ? If the changes can't be\nbackported, the searx stable version will be less and less usable.\n\nWe have a number of packages with such characteristics in Debian already. Think\nof software like youtube-dl and the like. There are two ways to deal with this.\nOne is to upload the fixed version to the stable-updates distribution:\nhttps://wiki.debian.org/StableUpdates With that the fixed package will be made\navailable easily to users of Debian stable.\nAnd then there is the possibility of using stable-backports which are usually\nused to not just contain the same package version as in stable but with fixes\napplied but to contain completely new package versions.\nThanks!\ncheers, josch\n. I packaged the current tip of the py3 branch (commit 94b99def). The only missing things are:\n\nfilling README.Debian with Debian-specific instructions how to get things running\nadjusting debian/copyright\nwriting an ITP bug and closing that bug in debian/changelog\n\nBut technically, the package already works and is nearly Debian policy compliant (minus the three items mentioned above). If you know what you are doing, you can get the package here:\nhttps://mentors.debian.net/package/searx\nThe package comes with seven Debian-specific patches and deletes 170 files from the original source. Maybe we can reduce these numbers but I'll open a separate issue about that at a later point.. I now handled the last three remaining bits and updated the version at mentors.debian.net with these changes.\nI plan to upload the package to Debian unstable once you make a release that includes the py3 branch.\nMy packaging is still using environment variables for the custom static and template directories (see issue #815) but as I see from pull request #816 you will probably end up putting these settings into the config file.\nThanks!\ncheers, josch. The Vcs-Git and Vcs-Browser headers contain the urls where the packaging repository can be accessed. I plan to use dgit (as can be seen from the url) so that repository will only become available once I do the first upload. I put the header in nevertheless already because it's something that is easily forgotten (there exists no lintian warning about it, for example).. Now that issue #63 is fixed I guess the next release will be including python 3 support? If somebody could give me a heads-up in this issue once the searx release with python3 support happens, I can upload the searx Debian package to the Debian NEW queue.. I uploaded a package to the Debian NEW queue:\nhttps://ftp-master.debian.org/new/searx_0.12.0+dfsg1-1.html\nIt will be added to Debian unstable once it has been reviewed by the FTP master team.. Just as a data point, I run searx behind a reverse proxy myself (pound) and that works fine. So maybe the problem is not with searx but with your reverse proxy setup.. Thanks, your explanations help to make the package compliant with Debian.\nAbout your question what you should or should not include, lets take the bootstrap code as an example. It seems to be licensed under MIT. Specifically it says:\nCopyright (c) 2011-2017 Twitter, Inc.\nCopyright (c) 2011-2017 The Bootstrap Authors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nI'm not a lawyer, but as far as my knowledge of the topic goes, the above means that if you want to distribute a derivative work of the bootstrap source code (and the minified code you distribute is such a derivative work) then you have to include above copyright notice and permission notice in your distribution to comply with the license. Not doing so violates the license that the bootstrap authors are releasing their software under.\nSpecifically your searx/static/js/bootstrap.min.js only says:\n/*!\n * Bootstrap v3.2.0 (http://getbootstrap.com)\n * Copyright 2011-2014 Twitter, Inc.\n * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)\n */\n\nand I cannot find anywhere that you included the aforementioned copyright notice and permission notice.\nSo if you want to be correct, read the licenses of all the code that you get from other people, read under which conditions the copyright holders allow you to distribute their work or derivatives thereof and make sure that you comply with these conditions.\nThanks!\ncheers, josch. It seems that at least for the default oscar theme (I didn't test the others yet) I do not need the content of the directories static/css, static/fonts, static/js and static/less at all. The content of static/plugins seems to be non-minified javascript written by you and not others so it's unproblematic. The content of the other directories can be replaced by symlinks to the versions of bootstrap, requirejs and jquery as shipped by Debian. Thus I can simply delete the versions that searx ships and therefore need not to worry about their copyright compliance.\nFor what it's worth, the oscar theme seems to work fine with jquery 3.1.1, bootstrap 3.3.7 and requirejs 2.3.2 which are the versions from Debian unstable.. Just to be clear: I'm not proposing searx to upgrade the version of its embedded javascript libraries. From the Debian perspective it's perfectly fine for you to continue shipping the version that you deem is best for your users.\nThings only will become troublesome for me if searx happens to not be compatible with the javascript library versions that we ship in Debian. But I'll let you know once that happens. :). Another thing to note is, that not only are there other distributions than Debian (so unless you want to make your software Debian-centric, there is no such thing as the \"same version\") but even in Debian there will be \"random\" bumps of the versions of the mentioned libraries over the course of a release cycle.\nSo if you do update the versions you ship, then don't do it for Debian but because you find it a good choice from a distribution-independent point of view.\nMaybe another important data point is also that it is already too late to get searx into the next Debian release which will happen in Q2 this year. Thus, we'll spend more than a year of having searx in unstable and (from the Debian side) will be able to get a feeling how well searx can work together with libraries from Debian (and derivatives).. And back to the original topic of copyright compliance:\nThere are a bunch of *.ico files in the themes/img/icons directories. What's the copyright on those? It seems those are plain copies of the icons of the respective services and thus you do not have the copyright to distribute these, no?. Same goes for searx/static/themes/courgette/img/bg-body-index.jpg. I cannot find the copyright information for that file.. Let me propose the following patch:\n```patch\n--- a/searx/utils.py\n+++ b/searx/utils.py\n@@ -181,15 +181,24 @@ class UnicodeWriter:\n def get_themes(root):\n     \"\"\"Returns available themes list.\"\"\"\n\nstatic_path = os.path.join(root, 'static')\ntemplates_path = os.path.join(root, 'templates')\nif 'SEARX_STATIC_PATH' in os.environ:\nstatic_path = os.environ['SEARX_STATIC_PATH']\nelse:\nstatic_path = os.path.join(root, 'static')\nif 'SEARX_TEMPLATES_PATH' in os.environ:\ntemplates_path = os.environ['SEARX_TEMPLATES_PATH']\nelse:\n\ntemplates_path = os.path.join(root, 'templates')\nthemes = os.listdir(os.path.join(static_path, 'themes'))\n return static_path, templates_path, themes\n\n\ndef get_static_files(base_path):\n-    base_path = os.path.join(base_path, 'static')\n+    if 'SEARX_STATIC_PATH' in os.environ:\n+        base_path = os.environ['SEARX_STATIC_PATH']\n+    else:\n+        base_path = os.path.join(base_path, 'static')\n     static_files = set()\n     base_path_length = len(base_path) + 1\n     for directory, _, files in os.walk(base_path):\n@@ -200,7 +209,10 @@ def get_static_files(base_path):\ndef get_result_templates(base_path):\n-    base_path = os.path.join(base_path, 'templates')\n+    if 'SEARX_TEMPLATES_PATH' in os.environ:\n+        base_path = os.environ['SEARX_TEMPLATES_PATH']\n+    else:\n+        base_path = os.path.join(base_path, 'templates')\n     result_templates = set()\n     base_path_length = len(base_path) + 1\n     for directory, _, files in os.walk(base_path):\n```\nThis introduces the environment variables SEARX_STATIC_PATH and SEARX_TEMPLATES_PATH which set the path to the static files and templates, respectively. Or would you rather have this be implemented as a configuration option in settings.yml, maybe under the general section?\nI also might've found a bug. webapp.py calls searx.utils.get_themes with the configuration value for themes path if it is non-empty. But get_themes will append static and themes to the path that it is given and thus, the argument to the function is not so much the theme path but the searx path. This might also hint to some refactoring being required. The get_themes function being responsible for resolving the static and templates path is suboptimal and should be moved to a separate function.. Because the name of the legacy theme suggests that it is just still there for \"legacy\" reasons and that it might be dropped at some point in favor of themes with more \"bling\" like oscar. On the other hand it might also just be poorly named and instead the plan is to keep it around exactly to provide a theme for low bandwith users. Could you clarify?. I am able to reproduce this.\nIt also failes on the Debian CI infrastructure since April with the same problem:\nhttps://ci.debian.net/packages/s/searx/unstable/amd64/\nI would've suspected that since uwsgi fails, this is due to a newer uwsgi version, but the version didn't change between the first failing run and the last successful run.\nThis has also been reported to Debian as https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=900068 and according to the bug reporter, fails in Ubuntu as well.. @csolisr This is not yet a solution, merely a workaround (which might point to the solution) but does running the following fix your problem?\nsed -i 's/autoload = true/#autoload = true/' /usr/share/uwsgi/conf/default.ini. Can we also discuss the privacy breach that this pull request introduces?\n\nThe searx slogan is \"Privacy-respecting metasearch engine\" and the website says \"Search without being tracked\".\nBut this pull request introduces a regular ping to mozilla.org and thus regularly informs mozilla about the existance of the server running searx.\nShould this pull request be accepted I'd ask to not enable this feature by default but let users who want it enable it explicitly.\nIf you decide to enable this feature by default, I'd ask you to add a list to the README or some such where you list all privacy breaches of this form. I think it's important to at least inform the users of these kind of unintended communication that searx performs with remote machines. It will also make my life as the Debian maintainer of searx easier because this feature will not be enabled by default in Debian. I think users installing a software calling itself \"privacy-respecting\" should have to worry about things like this being enabled by default.. @dalf cool! Thanks for the update! :smile: . Thanks for the quick response and the heads-up!\nI'd also appreciate clarification about the copyright of other material from the simple theme. For example searx/static/themes/simple/magnet.svg contains the information that it was created by Dmitry Baranovskiy but I cannot find any information about its license anywhere.\nAnd then there is the directory searx/static/themes/simple/fonts/ which contains files that claim to be generated using grunt-webfont (but from what?) and an svg that claims being created by a person named alexandre but again without any copyright information.\nThanks!\ncheers, josch. Hi,\nokay... this lets me assume that ./searx/static/themes/simple/less/autocomplete.less is also from the autocomplete.js project?\nDo you know of any other files from the simple theme for which you do not have the copyright so that we can correctly attribute the original authors together with the correct license?\nAlso, you say that autocomplete.js is a modified version. Do you plan to regularly update autocomplete.js and apply your patches on top? It might make more sense to suggest your changes to the autocomplete maintainers so that you do not need to carry a patched version of the code. Are your changes truly necessary for the functionality of the simple theme or are they purely cosmetic?\nWhat is the copyright situation of magnet.svg if it doesn't belong to ionicons?\nThanks!\ncheers, josch. Thanks for your work! There is of course no rush. In the very worst case, if you don't find time, then we can also just release 0.13 without the simple theme.\nCould you point me to the issue that you filed with the autcomplete.js maintainers? Edit: you are probably talking about https://github.com/autocompletejs/autocomplete.js/issues/43 It seems the autocomplete.js maintainer is waiting for you to allow enabling and disabling the component according to user preference.\nWhich tool can one use to convert a typescript file into javascript? Then I can verify the path from the autocomplete.ts to the autocomplete.js myself. Edit: found the answer to my question. It's the tsc tool from the node-typescript package.. Hi,\nthanks for your detailed analysis. The licenses of grunt related tools or jslint and clean-css are irrelevant though. This is only about the copyright of the content that you distribute.\ngrunt-webfont is not packaged for Debian so we cannot bundle the svgs to the searx-specific ion.woff. But this is a Debian-specific problem and nothing you should worry about. We have to solve this in Debian.\nThe CC-BY 3.0 liense of magnet.svg is fine for Debian. And even if it were not, this would not be a problem that you have to care about but something that we in Debian have to find a solution for. :)\nThough I would really recommend that you put information about individual file licensing somewhere in your git. For example the file ./searx/static/themes/simple/less/autocomplete.less doesn't contain any information that it comes from the autocomplete.js project. The autocomplete.js author distributes their software under the MIT license which only has one condition:\nThe above copyright notice and this permission notice shall be included in all copies or substantial     portions of the Software.\n\nIf you are not doing that, then you are violating their license terms.\nThanks a lot for your detailed analysis!. As far as I'm concerned this bugreport is fixed. I just needed to know the copyright so that I can add the appropriate copyright notices or remove the respective material. In case of the embedded ionicons I just removed them from the Debian package because we unfortunately do not yet have grunt-webfonts in Debian so there is no way to build the webfont from source. I will reintroduce the ionicons font once grunt-webfonts is in Debian.\nThis is btw also why you might want to ship your own Debian packages (if you care about that) because then you don't have to abide by the strict rules of the Debian distribution and can instead ship whatever you see fit to distribute to your users. :). Downloading the unmodified ionicons is not a solution because that one will not include magnet.svg.\nThis is a Debian specific problem and thus I suggest that we continue this discussion to find a good solution in the Debian bugtracker:   https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=882847\nYou can reply to the bug by sending an email to 882847@bugs.debian.org.. @dalf: Another thing that you might be interested in relates to the magnet.svg. The website where you have it from says: https://thenounproject.zendesk.com/hc/en-us/articles/200509928-How-do-I-give-creators-credit-in-my-work-\n\nIf you choose to give creator credit to download an icon for free, then you need to follow these requirements: \nThe format should be \u201cIcon name\u201d by Creator Name, from the Noun Project. The icon name must be hyperlinked to the original source. If you can\u2019t hyperlink in the medium you\u2019re using the icon, then type out thenounproject.com as the source. This attribution must be listed next to the icon, below the icon, or somewhere on the project you create to properly reference the icon. \n\nI believe such a credit notice is not yet included anywhere in the simple theme.. The bug was also reported for the Debian searx package: https://bugs.debian.org/886093\nOur CI system as well as my own live instance does not have any Python 2 packages installed. Only Python 3 packages are installed on both systems. Thus, it is unlikely that this problem occurs because of Python 3. See the Debian bts link for more details of my analysis.. Okay. But I still don't see how your patch is the proper fix for this issue. What you do is to just treat the input as pure binary. But what we want is to read the file in utf-8 encoding. So would it not be better to \"fix\" this properly by doing:\nwith codecs.open(\"settings.yml\", \"r\", \"utf-8\")) as f:\n    yaml.load(f)\n\nBut I still don't think that the current behaviour of searx even classifies as a bug. If the environment sets LC_ALL=C then this is to tell all applications that are run in this environment to apply this locale setting. By forcing searx to ignore this and use binary or utf8 instead, you essentially take away from the user the ability to run searx in the locale of their choosing. You effectively say that searx is limited to utf8 only.\nI don't see yet, why the fix has to happen in searx and why the proper solution to this problem is not for you to set your environment into the state that you want it. It seems obvious to me that you want to run applications with utf8 support, so why did you configure your OS with the C locale? To me it sounds like you want to fix a problem with the configuration of your OS in the application that you are running. You will be surprised when you find other applications that actually respect your locale settings. Do you also want to submit patches to them so that they also ignore your locale setting?\nI suggest you just adjust your system configuration to use the locale that you prefer. For you, that might be C.utf8.\nDisclaimer: I'm just the Debian maintainer of the searx package. The opinion of the searx developers on this topic might differ from mine. If searx developers think it's a good idea to ignore the system's locale setting and force utf8 then so be it.. But even with the explanation by @SunilMohanAdapa, is the correct solution to this problem not opening the file with codecs and then forcing utf-8 instead of reading it with b as suggested by this pull request?. I was unaware that yaml would automagically detect the right codec! That's wonderful! In that case I take back everything I said. Thank you for taking the time to cure my ignorance on the matter! :). @JosephKiranBabu depending on the reply from @asciimoo I'll make a new release of the Debian package. This is because there is no point of me backporting the patch if there is a new release around the corner.. Version 0.14.0 was just packaged and uploaded to unstable.. I also don't think that the check is correct. The script currently uses:\nif [ $_ != $0 ]; then\n\nbut this is problematic when the script is called with bash ./manage.sh. The proper way to do this (also described in that stackoverflow post) is to do:\nif [[ $0 != \"$BASH_SOURCE\" ]]; then\n\nIs there really enough utility in this functionality for such a fragile solution?. In the Debian package we just patch searx to not install tests. In fact we don't install lots of things. Our full diff against setup.py reads:\ndiff\ndiff --git a/setup.py b/setup.py\nindex 6466d1f..1b0ae30 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -51,23 +51,9 @@ setup(\n     },\n     package_data={\n         'searx': [\n-            'settings.yml',\n-            '../README.rst',\n-            '../requirements.txt',\n-            '../requirements-dev.txt',\n             'data/*',\n             'plugins/*/*',\n-            'static/*.*',\n-            'static/*/*.*',\n-            'static/*/*/*.*',\n-            'static/*/*/*/*.*',\n-            'static/*/*/*/*/*.*',\n-            'templates/*/*.*',\n-            'templates/*/*/*.*',\n-            'tests/*',\n-            'tests/*/*',\n-            'tests/*/*/*',\n-            'translations/*/*/*'\n+            'translations/*/*/*.mo'\n         ],\n     },. We patch them out in Debian because:\n\n\nsettings.yml is a configuration file and thus belongs into /etc/searx. We also ship /usr/share/doc/searx/examples/settings.yml.gz\n\n\ntemplates and static would be installed into /usr/lib/python3/dist-packages/searx if they were not commented out in setup.py. But those are static files which are not Python scripts and thus they belong to /usr/share/searx instead.. I'm a bit further. The timeouts did not happen on my local machine because there I used searx-run. Only when using nginx and uwsgi does the timeout problem occur with version 0.14.0.. Okay, I successfully bisected the commit range between 0.13.1 (good) and 0.14.0 (bad) and it seems that the commit that introduces this regression with uwsgi is 2f69eaeb2fb76a14d9c3f2a67a9e404b066049ba. I can also confirm that reverting commit 2f69eaeb2fb76a14d9c3f2a67a9e404b066049ba on top of 0.14.0 fixes the regression. So the culprit is indeed this commit.. I created fresh containers of Debian Stretch (stable) and Debian Buster (testing) and was able to reproduce my observations. It's important to note, that while the Debian packages exhibit the same issue, I am not using the Debian packages but a git clone to make sure that this is no packaging issue.\n\n\nWhich operating system are you using?\nI see several ways for you to reproduce this. Either you follow the steps that I took, which would be:\n$ sudo debootstrap stretch debian-stretch\n$ sudo chroot debian-stretch\nand then inside the chroot:\n$ apt install nginx uwsgi uwsgi-plugin-python3 git\n$ cd /var/www\n$ git clone https://github.com/asciimoo/searx.git\n$ chown -R www-data:www-data searx\nOr, I can also share a tarball of the resulting chroot environment with you.\nOne way to test the result is using systemd-nspawn:\nsudo systemd-nspawn -bD ~/debian-stretch\n\nIf you want to setup the container manually, I attached the relevant config files for searx, nginx and uwsgi.\nsearx settings.yml\nuwsgi.ini\nnginx config\nThe issue remains the same:\n\nthe front page loads fine\nsearch queries create timeouts for all search providers\nreverting commit 2f69eae magically fixes the problem\nsearx-run is unaffected and works either way\n\nSo maybe this is an uwsgi problem but I tried versions 2.0.14 (from Debian Stretch) as well as 2.0.15 (from Debian Buster).. If I do that, then not even the front page loads. Here is the relevant part from my uwsgi log after starting it with enable-threads = true and trying to access the front page:\nFri Feb 23 21:21:37 2018 - *** Starting uWSGI 2.0.15-debian (64bit) on [Fri Feb 23 21:21:37 2018] ***\nFri Feb 23 21:21:37 2018 - compiled with version: 7.3.0 on 10 February 2018 00:06:24\nFri Feb 23 21:21:37 2018 - os: Linux-4.13.0-1-amd64 #1 SMP Debian 4.13.4-2 (2017-10-15)\nFri Feb 23 21:21:37 2018 - nodename: hoothoot\nFri Feb 23 21:21:37 2018 - machine: x86_64\nFri Feb 23 21:21:37 2018 - clock source: unix\nFri Feb 23 21:21:37 2018 - pcre jit disabled\nFri Feb 23 21:21:37 2018 - detected number of CPU cores: 4\nFri Feb 23 21:21:37 2018 - current working directory: /\nFri Feb 23 21:21:37 2018 - writing pidfile to /run/uwsgi/app/searx/pid\nFri Feb 23 21:21:37 2018 - detected binary path: /usr/bin/uwsgi-core\nFri Feb 23 21:21:37 2018 - dropping root privileges as early as possible\nFri Feb 23 21:21:37 2018 - setgid() to 33\nFri Feb 23 21:21:37 2018 - setuid() to 33\nFri Feb 23 21:21:37 2018 - chdir() to /var/www/searx\nFri Feb 23 21:21:37 2018 - your processes number limit is 46775\nFri Feb 23 21:21:37 2018 - your memory page size is 4096 bytes\nFri Feb 23 21:21:37 2018 - detected max file descriptor number: 1024\nFri Feb 23 21:21:37 2018 - lock engine: pthread robust mutexes\nFri Feb 23 21:21:37 2018 - thunder lock: disabled (you can enable it with --thunder-lock)\nFri Feb 23 21:21:37 2018 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nFri Feb 23 21:21:37 2018 - dropping root privileges after socket binding\nFri Feb 23 21:21:37 2018 - Python version: 3.6.4+ (default, Feb 12 2018, 08:25:03)  [GCC 7.3.0]\nFri Feb 23 21:21:37 2018 - Python main interpreter initialized at 0x55aa9aeb6050\nFri Feb 23 21:21:37 2018 - dropping root privileges after plugin initialization\nFri Feb 23 21:21:37 2018 - python threads support enabled\nFri Feb 23 21:21:37 2018 - your server socket listen backlog is limited to 100 connections\nFri Feb 23 21:21:37 2018 - your mercy for graceful operations on workers is 60 seconds\nFri Feb 23 21:21:37 2018 - mapped 145536 bytes (142 KB) for 1 cores\nFri Feb 23 21:21:37 2018 - *** Operational MODE: single process ***\nFri Feb 23 21:21:37 2018 - added /var/www/searx/ to pythonpath.\nFri Feb 23 21:21:38 2018 - WSGI app 0 (mountpoint='') ready in 1 seconds on interpreter 0x55aa9aeb6050 pid: 140 (default app)\nFri Feb 23 21:21:38 2018 - dropping root privileges after application loading\nFri Feb 23 21:21:38 2018 - spawned uWSGI master process (pid: 140)\nFri Feb 23 21:21:38 2018 - spawned uWSGI worker 1 (pid: 153, cores: 1). Got it!\nSo, this was a uwsgi problem after all. But the necessary option was not enable-threads = true but lazy-apps = true. After inserting the latter searx works fine again. Even with enable-threads = false.. If searx requires the lazy-apps = true option when used with uwsgi, maybe it could document that somewhere? Maybe in form of an example uwsgi ini file?. Funny, I thought I had copied that when I included the uwsgi config. Seems I missed that.. Are the sources of those docs hosted somewhere? Then I could check whether any of these instructions change over time and adapt the Debian package accordingly.. From my log:\nWARNING:searx.search:engine timeout: duckduckgo\n[pid: 198|app: 0|req: 1/1] 10.0.0.1 () {50 vars in 1377 bytes} [Tue Aug  7 07:29:25 2018] GET /search?q=test => generated 49608 bytes in 6568 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nERROR:searx.search:engine duckduckgo : HTTP requests timeout(search duration : 18.096888065338135 s, timeout: 6.0 s) : ConnectTimeout\nBut I have another funny observation. When I run on the same host that searx is installed:\n$ curl --verbose --connect-timeout 30 'https://duckduckgo.com/search?q=test'\n*   Trying 79.125.105.113...\n* TCP_NODELAY set\n* After 14986ms connect time, move on!\n* connect to 79.125.105.113 port 443 failed: Connection timed out\n*   Trying 176.34.155.23...\n* TCP_NODELAY set\n* After 7377ms connect time, move on!\n* connect to 176.34.155.23 port 443 failed: Connection timed out\n*   Trying 46.51.179.90...\n* TCP_NODELAY set\n* After 3688ms connect time, move on!\n* connect to 46.51.179.90 port 443 failed: Connection timed out\n* Failed to connect to duckduckgo.com port 443: Connection timed out\n* Closing connection 0\ncurl: (7) Failed to connect to duckduckgo.com port 443: Connection timed out\nBut when I run the same command from my home network then everything is fine.\nMaybe duckduckgo added my host to a blacklist?. ",
    "beudbeud": "can you give me your ngixn configuration with non-root url please\n. ok with base_url in searx/settings.yml or not?\nbecause i try this \nnginx:\nlocation = /searx { rewrite ^ /searx/; }\nlocation /searx { try_files $uri @searx; }\nlocation @searx {\n  uwsgi_param SCRIPT_NAME /searx;\n  include uwsgi_params;\n  uwsgi_modifier1 30;\n  uwsgi_pass unix:/run/uwsgi/app/searx/socket;\n}\nuwsgi:\n```\n[uwsgi]\nWho will run the code\nuid = searx\ngid = searx\nNumber of workers\nworkers = 4\nThe right granted on the created socket\nchmod-socket = 666\nPlugin to use and interpretor config\nsingle-interpreter = true\nmaster = true\nplugin = python\nApplication base folder\nbase = /opt/searx\nModule to import\nmodule = searx.webapp\nVirtualenv and python path\nvirtualenv = /opt/searx/\npythonpath = /opt/searx/\nchdir = /opt/searx/searx/\nThe variable holding flask application\ncallable = app\n```\n. ok it' works for me. \nI had a documentation for sub url.\nThanks\n. i use nginx + uswgi, i try with master but is don't works\n\n. my nginx conf\nbash\nlocation = /searx { rewrite ^ /searx/; }\nlocation /searx {\n        if ($scheme = http) {\n                rewrite ^ https://$server_name$request_uri? permanent;\n        }\n        try_files $uri @searx; }\nlocation @searx {\n        uwsgi_param SCRIPT_NAME /searx;\n        include uwsgi_params;\n        uwsgi_modifier1 30;\n        uwsgi_pass unix:///run/uwsgi/app/searx/socket;\n}\n. Yes i try the last master\nit's a public instance.\nyou can try on https://beudibox.fr/searx/\n. i create account for you user : searxtest\npassword searx1234\n. thanks a lot\ni fix the bug the autocompletion works now\n. is a problem with my nginx 1.6 config on debian\n. it's a bug in ssowat the web SSO of yunohost ;)\n. thanks\n. ",
    "tetraf": "I've installed it on Debian 7 in an Openvz container (4 vCPU, 1G RAM) on a dedicated server (~100Mbps link). The 6 seconds measure come from searx logs (POST / => generated 31074 bytes in 6200 msecs).\nI understand it's a meta search engine and it doesn't bother me to wait even several tens of seconds if the result is displayed progressively (although I guess it would require rethinking the UI) but now 6 seconds without anything happening is relatively long.\n. Hum, even lowering it to 0.3 seconds it take nearly 6 seconds (POST / => generated 4822 bytes in 5697 msecs) with a lot of timeouts now.\n. ",
    "Reventl0v": "You can set a timeout in your settings.yml:\nrequest_timeout : 1 # seconds\nNote that certain search engine are really slow, and since search wait for them to answer, it's easy to get a waiting time of 5 or 6 seconds. Since i personnaly use seax as a google proxy, it's not a problem for me, but i'd love to see a \"live\" interface.\n. I get the same error when using https://aur.archlinux.org/packages/searx-git/\n. I still have the same error, when i build from the AUR (last git version, so). Can anyone confirm ?\n. Well, nope :|\n[Hyperion]/home/reventlov/searx-git# LANG=C pacman -Qi searx-git                                                                                                                                               2015-03-02 20:14:18\nName           : searx-git\nVersion        : r1231.v0.7.076g9981725-1\nDescription    : A privacy-respecting, hackable metasearch engine\nArchitecture   : any\nURL            : https://searx.0x2a.tk/\nLicenses       : AGPL\nGroups         : None\nProvides       : None\nDepends On     : python2-flask  python2-flask-babel  python2-requests  python2-lxml  python2-yaml  python2-dateutil\nOptional Deps  : None\nRequired By    : None\nOptional For   : None\nConflicts With : None\nReplaces       : None\nInstalled Size :   3.85 MiB\nPackager       : Unknown Packager\nBuild Date     : Mon Mar 2 20:12:25 2015\nInstall Date   : Mon Mar 2 20:12:54 2015\nInstall Reason : Explicitly installed\nInstall Script : Yes\nValidated By   : None\n[Hyperion]/home/reventlov/searx-git# searx-run                                                                                                                                                                 2015-03-02 20:15:07\nTraceback (most recent call last):\n  File \"/usr/bin/searx-run\", line 9, in <module>\n    load_entry_point('searx==0.7.0', 'console_scripts', 'searx-run')()\n  File \"/usr/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 521, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2632, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2312, in load\n    return self.resolve()\n  File \"/usr/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2318, in resolve\n    module = __import__(self.module_name, fromlist=['__name__'], level=0)\n  File \"/usr/lib/python2.7/site-packages/searx/webapp.py\", line 39, in <module>\n    from searx.poolrequests import get as http_get\n  File \"/usr/lib/python2.7/site-packages/searx/poolrequests.py\", line 48, in <module>\n    http_adapters = cycle((HTTPAdapterWithConnParams(pool_connections=100), ))\n  File \"/usr/lib/python2.7/site-packages/searx/poolrequests.py\", line 14, in __init__\n    self.max_retries = requests.adapters.Retry(0, read=False)\nAttributeError: 'module' object has no attribute 'Retry'\n. python2 -c 'import requests; print requests.__version__'\n2.2.1\nThank you for the hint.\n. ",
    "virtadpt": "Another possibility is automatically using a video stream downloader (like youtube-dl) to write the file locally and then spawn a media player (VLC?) to play it back.\n. As far as I know, the answer is \"Yes.\"  Here's the documentation:\nhttps://asciimoo.github.io/searx/dev/search_api.html. Those are the docs I've been referencing... the Memento Protocol would be a nice addition, but I was referring to the Archive itself and not the Wayback Machine.. It could be done by setting up a round-robin DNS entry for searx.me (one A record, multiple IP addresses) with a relatively short lifetime (five minutes).  The problems would then be the browser noticing that every instance had a different SSL certificate (or none at all) (which is a serious problem with the SKS keyserver network's HTTPS interface...), not knowing which of the instances were configured for privacy (i.e., reconfigured logging), keeping them all on the same version of searx...\nThe organizational overhead would be pretty serious, but it's not an insurmountable problem, either.\n. Will this functionality be accessible from the search API?\n. It's absolutely possible to disable logging in both Apache and Nginx.  Here's how I do it:\n- https://superuser.com/questions/916726/disable-apache-logging-completely\n- https://linuxconfig.org/how-to-disable-nginx-logging-on-linux-system\nI agree with packaging .deb files for Debian and its derivatives.  Ubuntu is probably the most popular distro at VPS providers these days, and the proliferation of RaspberryPis implies that Raspbian packages would be useful, also.  Arch Linux (my preferred distro of choice) has two packages already in the AUR, so continuing to play nice with the folks who maintain them is advisable.\nI'm not much of a fan of Docker but I'm in the minority of that category right now, so it would be advisable to include Docker configuration files.  At the very least, it would mean that more private installations of Searx might start popping up.\nOne thing that would make administration of Searx easier, though - if at all possible, please move the comments in searx/settings.yml off of the ends of the lines to above them.  It makes it tricky to automate the configuration process as-is.\n. :+1: \nI was going to implement something like this in my bot that uses Searx as its search front-end, anyway.\n. \ud83d\udc4d for this suggestion.  I've had problems loading the Oscar theme on my phone and tablet (multiple browsers), and a lightweight, no bells and whistles theme would be a really handy thing to have available.. It looks as if this would make life much more difficult for those of us who use Searx for its API functionality only.. I wrote a number of interactive bots that use Searx's API to issue search requests.  Having the secret key in a configuration file means it's static, which means that I can put it into the configuration files of my bots.  Making it dynamic and cached makes that more difficult to handle.\nWhile this would be merely annoying to me, there are other people out there who are doing much the same thing in more constrained environments, and they won't be able to read the cached file on disk to get the secret key.  This breaks things completely for them.. I'm running an instance on a Digital Ocean VPS (4GB RAM/60GB disk space/4 CPUs) (I think) behind Nginx from a GNU Screen session.  I've no doubt that I could run it on the 512 MB RAM version without any trouble.. Yes, I'm running a copy in a VM at DO (Ubuntu v16.04 LTS).  I basically followed the installation instructions and started it inside of a GNU Screen session, and it works a treat.  I didn't do anything special to the VM, I just used the CIS Benchmarks to harden it before I did anything else on the machine.. The virtualenv instructions from here: https://asciimoo.github.io/searx/dev/install/installation.html#basic-installation. @neggs My VPS @ DO spec is pretty high because I use that VM for quite a few things, not just Searx.  4 CPUs, 4GB RAM, 60GB disk space.  I use it internally only (listens on 127.0.0.1) because I use Searx for its search API ALONE (https://github.com/virtadpt/exocortex-halo/tree/master/web_search_bot. \ud83d\udc4d \nI need to play around with this site a bit to figure out how searches work.. \ud83d\udc4d . Scholarpedia may have just turned off the API.  It's implemented as a module with a toggle on it.. @lonnieOST It's pretty straightforward to do that.  You can make Searx return search results as JSON documents by specifying the format.  I do it like this: http://localhost:8888/?q=this%20search%20term&categories=general&format=json&lang=en&time_range=day\nYou can also have Searx return CSV or RSS for searches.\nAs for algorithmically interacting with Searx, here's some code I wrote that does that: https://github.com/virtadpt/exocortex-halo/blob/master/web_search_bot/web_search_bot.py#L419. I did a little tinkering, and I think this is how you'd make the API call:\nhttp://localhost:8888/?q=this%20search%20term&categories=social+media&language=en-US&format=json&time_range=day\nWhen you specify the category, it would be social+media (plus), not social%20media.. Thanks - I'll give it a try today.. That appears to have been it - thank you!. YaCy?. What does that do to those of us who use the search API extensively?  Getting a redirect to a site means that we don't get search results from that search engine, which pretty much hoses everything.. Wait a minute.  You said that it would be a good idea to send a redirect to a site when a !search is used.  That would, if I'm reading you correctly, the search API would return a redirect link instead of search results.  Is that what you meant?. I'm seeing something similar, only with PDFs.  I don't know if it's how I've got things set up (Searx as the front-end for a YaCy instance in intranet mode (which is working as expected when I interact with YaCy directly)), I've been trying to isolate the problem for the last couple of days.  Happens when I run the latest checkout (HEAD), tag 0.15.0, and tag 0.14.0.. ",
    "Glandos": "The current version show me no results in the category \"General\", since the last merge. Is it linked to that issue?\n. Nevermind. It's an issue with Debian Python version that merged some import SSL stuff in 2.7.8 (like SNI support), and GEvent doesn't implement the interface. Gevent thinks this is only for Python 3. So I am stucked, every HTTPS request fails.\n. On Debian/testing, whatever the virtualenv I have, I got the following error:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/gevent/greenlet.py\", line 327, in run\n    result = self._run(*self.args, **self.kwargs)\n  File \"/var/www/search.example.com/searx/venv/local/lib/python2.7/site-packages/grequests.py\", line 71, in send\n    self.url, **merged_kwargs)\n  File \"/usr/lib/python2.7/dist-packages/requests/sessions.py\", line 456, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/lib/python2.7/dist-packages/requests/sessions.py\", line 559, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/requests/adapters.py\", line 327, in send\n    timeout=timeout\n  File \"/usr/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 516, in urlopen\n    body=body, headers=headers)\n  File \"/usr/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 304, in _make_request\n    self._validate_conn(conn)\n  File \"/usr/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 724, in _validate_conn\n    conn.connect()\n  File \"/usr/lib/python2.7/dist-packages/urllib3/connection.py\", line 237, in connect\n    ssl_version=resolved_ssl_version)\n  File \"/usr/lib/python2.7/dist-packages/urllib3/util/ssl_.py\", line 123, in ssl_wrap_socket\n    return context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/usr/lib/python2.7/ssl.py\", line 350, in wrap_socket\n    _context=self)\nTypeError: __init__() got an unexpected keyword argument 'server_hostname'\n<Greenlet at 0xb65269dcL: <bound method AsyncRequest.send of <grequests.AsyncRequest object at 0xb64e508c>>(stream=False)> failed with TypeError\n. OK, that confirms my debugging session. But now, I have to sit and wait, cause I have no time to change something in a library I know nothing about\u2026\n. Woohooo! Partytime, I can use my own Searx again!\nThanks a lot for the (not so obvious) fix.\n. > But I like the idea anyway. Maybe we could do it the other way around : in the suggestion list, we could provide icons to indicate to which category this suggestion belong.\nI would love it. Really.\n. What about writing a small warning on this feature? I would see something like:\nWarning: Enable this feature will send every keystroke to the remote site. All requests are proxied (i.e. no cookie, no tracking) through the Searx instance, but the remote site could be able to identify you anyway.\nThis is a draft, and not perfect. It should be displayed next to the preference item.\n. ",
    "MarcAbonce": "Now it's really deprecated.\nI get \"This API is no longer available.\" from google news.\n. I'm currently working on this one here: https://github.com/a01200356/searx/commits/wikipedia_infobox\nIt cuts out the middleman by scraping wikipedia directly because I couldn't find in duckduckgo's api's documentation how to set the language (I don't even know if they support it at all).\nIt still needs testing though.\n. To test it you can use this key: '5952JX-X52L3VKWT8'\n. Right now it only works with the torrentz engine, though. Other files engines don't do this yet.. The latter (\"If few users are using an instance, then it is not hard for google etc to identify the users based off of search history\") could be solved by #519 (Proxying via Tor).\n. I just realised that the no API version doesn't work anymore.\nRecently, WolframAlpha's front-end was entirely rewritten in Angular. I don't know Angular, but it looks like now it gets all the data asynchronously instead of it being \"hardcoded\" in the page like before.\nSo, unless there is a way to get the results without simulating an entire browser, for now you'd have to get an API key, which for free is limited to 2000 queries per month.\nFor what it's worth, though, ddg_definitions does answer OP's question.\n. Wow. I hadn't thought of that.\nThis is great. Thanks.\n. The requests library doesn't handle SOCKS, only HTTP(S), I think.\nWhat worked to me was to combine Privoxy with Tor.\nSo you proxy searx (from the settings) to the port where privoxy is listening and you configure privoxy to forward everything to tor's port.\nAlso, don't forget to increase the timeouts in searx to account for the slower responses.\nI'm no security expert, so I don't know how secure is this configuration but it seems to work.\n. Also frinkiac, 1x and torrentz request captcha. The only thing that comes in my mind would be to relay the captcha to the user, so they can solve it themselves (#520). However, this would become annoying very quickly and users would blame us, rather than google et al. for requesting captchas.\n. related to #447 and maybe #446\n. Actually, it looks like Duckduckgo Definitions takes the language from the Accept-Language http header. The kl argument did nothing to me.\nBut it still doesn't work for all languages that Wikipedia supports. Chinese, Arabic and Hebrew don't return anything.\nTo merge the infoboxes, Wikidata now returns the id in the user's language.\n. Ok, now I also modified duckduckgo_definitions to accept more languages.\nThe infoboxes merge correctly as far as I've seen.\n. When I run it, it automatically enables all the engines. I tried resetting my cookies to see if it was an error on my settings, but the preferences changed again.\n. ok, i realised where the bug comes from.\napparently, blocked_engines was changed to disabled_engines in the cookies (8c4db08443c7a55344fa0011b89303467fa62efe).\nin your preferences template, the engines' checkbox_toggles check for the old variable.\n. currently working on this here: https://github.com/a01200356/searx/commits/wikidata\n. I added many, but not all, of the properties in the list. I tried to give as much relevant information without overwhelming the user with too much data (for example printing the full cast of a film would be too much, imo).\nNevertheless, it should be easy to add, remove or change the order of properties in the engine, so this feature can be tweaked as it gets used.\nThe extra requests are still synchronous though, so I just kept result_count to 1 to avoid problems.\n. By the way, ~~htk2rvtgj6vv3ppj.onion~~ is currently running this branch.\nEdit: ~~mze6ter6fchwojcr.onion~~ http://jiwfbtg2kfkadjku.onion. Sorry for the delay.\nI just updated the PR. But instead of having two settings files, I added the using_tor_proxy setting that should automatically set up all the other Tor specific configurations.\nWhen set to False or commented out, as default, everything should behave exactly the same as current version on upstream.. I just fixed the issue I had with not evil's encoding.\nAbout the last commit, I just assume that using Tor Browser's user agent would make our traffic seem more \"normal\" when using Tor than using a random user agent. However, this is merely an assumption on my part. I can't really tell if it actually helps or not.. With the fix in #1121, Google doesn't block Tor exit nodes any more.\nI also reverted the Tor Browser user-agent in Tor requests, I don't think it makes any difference anyway.\nAlso, DuckDuckGo currently uses the normal website because the hidden service is extremely slow, which makes it reach timeout most of the time.\nAn updated instance running this branch is in http://jiwfbtg2kfkadjku.onion. It's worth noting that autistici requires projects to share their policy and manifesto.\nHaving the documentation under a .io domain (https://asciimoo.github.io/searx/) is pretty much the opposite of anti-militarism, for example.\n. I just mean that if we're going to claim an ideology, we should at least be consistent with it.\n. Fixed!\nAbout the social media links, I hadn't really thought about it, but I added them anyway. The user can choose whether or not to click those links, just like with normal search results here.\n. The infoboxes should now merge correctly.\nMerging rows with ddg definitions works, but usually leads to some redundancy, such as \"Birth\" and \"Date of birth\" or \"Creator\" and \"Created by\". So I still left ddg disabled by default.\n. This PR should be ready to merge, btw.\nThere are probably still some things that could be tweaked, but these will only be found as more people use this feature.\n. Fixed in #609.. Yes. I did the no api version, so now it looks the same with both versions.\n. Lol, it looks like we both commited the same thing at the same time (#608).\n. Ok. I merged your commit and now it works.\nIt failed because of the engine's unit tests (in tests/unit/engines/test_wolframalpha_api.py), which expected the result title to be only \"Wolfram|Alpha\". I changed that.\n. Nice. So this PR should be the one being merged, since it's the most complete now.\n. You're right. I just changed it to try to guess the country if none is given.\nSo language support should work now, even if no country is specified by the user.\n.  #284 This has been an issue for almost a year now. :P\nI think it's as simple as changing the text to On/Off (as proposed in this duplicate: #579).\n. As far as I've seen only google and duckduckgo_definitions send the Accept-Language header. And it's never from the user's HTTP header directly, but from the preference cookies or from the query itself (:lang query), which need to be explicitly created by the user.\nSo, unless I'm mistaken, it already works pretty much as you say, @logouthere.\n. But it doesn't.\nI think there was some misinterpretation or something in #641. Nowhere in the code is the user's browser's HTTP headers actually consumed.\nWhen I print the outgoing search requests generated here for Google and Bing, I get:\npython\n[\n    (\n        <function get at 0x7f008dee00c8>,\n        'https://www.google.com/search?q=this+is+my+query&start=0&gws_rd=cr&gbv=1&lr=&ei=x',\n        {\n            'headers': {\n                'Accept-Language': 'en',\n                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n                'User-Agent': 'Mozilla/5.0 (X11; Linux x86; rv:44.0) Gecko/20100101 Firefox/44.0'\n            },\n            'cookies': {},\n            'hooks': {\n                'response': <function process_callback at 0x7f008d369140>\n            },\n            'timeout': 2.0,\n            'verify': True\n        },\n        u'google'\n    ),\n    (\n        <function get at 0x7f008dee00c8>,\n        'https://www.bing.com/search?q=this+is+my+query&setmkt=en-US&first=1',\n        {\n            'headers': {\n                'User-Agent': 'Mozilla/5.0 (X11; Linux x86; rv:44.0) Gecko/20100101 Firefox/44.0'\n            },\n            'cookies': {'SRCHHPGUSR': 'NEWWND=0&NRSLT=-1&SRCHLANG=en'},\n            'hooks': {\n                'response': <function process_callback at 0x7f008d3692a8>\n            },\n            'timeout': 2.0,\n            'verify': True}\n        ,\n        u'bing'\n    )\n]\nThe only outgoing request with an Accept-Language header here is for Google, which is set here (my actual browser's Accept-Language header is not en) based on the language parameter set here (self.lang is the language set on the cookie or in the query).\nOther engines might set this header, but never based on the user's Accept-Language header.\nIf you find an engine request that actually leaks user's data, though, please point it out so it can be fixed.\n. Supported languages are now read directly from their websites (or apis). I edited the PR description with more information to reflect this changes.. Yes, Qwant requires a country code to work. I will send a fix when I have time.\nMeanwhile, the workaround is to specify a country code (fr-FR instead of fr) when using Qwant.. Some times they return some slightly different results, but overall it seems like they indeed return the same.\nFor example:\n!bing !yahoo are yahoo and bing the same now?\n!bing !yahoo The Beatles Abbey Road\n!bing !yahoo how many roads must a man walk down\nOnly when the query is short there is variation:\n!bing !yahoo python\n!bing !yahoo Bee Gees. In the proxies section of your settings.yml file, you can use a socks5 proxy pointing to the port where you are running Tor. For example:\nproxies :\n    http : socks5://localhost:9050\n    https: socks5://localhost:9050\nThen you will probably also want to adjust your settings to disable engines that block Tor exit nodes and increase the timeout to account for Tor's slowness.\nOr, if you want, you can pull my branch in #565 which tries to do all this in an easier way and supports hidden engines as well. And if anything goes wrong or you find a bug, you can obviously report it there.. No uso Windows ni Docker, as\u00ed que no se c\u00f3mo instalarlo all\u00e1.\nPero cd searx es solo el comando en la l\u00ednea de comandos (cmd o PowerShell) para cambiarte al directorio searx donde est\u00e1 el c\u00f3digo.\nSi el comando no funciona, probablemente es porque no est\u00e1s en el mismo directorio en el que se descarg\u00f3 searx. Entonces tienes que ir al directorio desde la l\u00ednea de comandos con cd C:\\lugar-donde-se-descargo-searx/searx.. I think they only include artists that release their music under a free license, so Pink Floyd is not included.. It's already there https://github.com/asciimoo/searx/blob/master/searx/engines/yacy.py but it's currently commented out by default.  \nYou can enable it when deploying an instance, but then you're meant to deploy a local YaCy peer as well (because there's no single central YaCy peer to query, by design).. I ran it locally and I just have two minor points to nitpick:\nThe script fails with python 3. First, the versions list can't be sorted because some version numbers mix numbers and strings. Since you are already filtering out the ESR versions later on in the code, you could just exclude them from NORMAL_REGEX and the result should look the same. There's also an encoding error when writing the json file with python 3.\nAlso, when opening useragents.json, the relative path assumes the script is being run from searx/utils, so it fails if you run the script from any other directory. Importing searx_dir from searx and using that instead of the relative path solves this.. If you turn debug to False in searx's settings.yml they will appear.\nI don't know why, but it seems like it's the intended behaviour.. I just updated this and added a couple more unit tests.  \nAt this point, after #1085, none of the default engines in the general category even support a separate behaviour for the 'all' option.. I'm trying to patch this error upstream in Flask-Babel.\nFor me, it makes sense that Babel (the i18n library) would throw an exception if a locale is unknown. But I don't think Flask-Babel (the web framework component) should completely break, along with the application itself, just because someone added a .mo file in an unsupported locale.\nDevelopment over there seems to be quite slow, though, so it will probably take a while for a response.\nMore specifically for Interlingua, you could also contribute to the Unicode CLDR to update or fix whatever data is missing for Interlingua to be featured in the main repository. That's the dataset that Babel is based on. It seems like a really long and bureaucratic process though.. It seems like you need to install go.. Persian is now available in the search languages drop-down as \u0641\u0627\u0631\u0633\u06cc - fa-IR.. It seems like Google is trying to guess the user's language based on their location (or, in this case, the server's location).\nFor now, the only workaround I've found is to always specify a country code different from the US in the language parameter. So, for English results, you can set the language parameter to en-GB, like https://searx.me/?language=en-GB&query=%s\nEdit: Another workaround is to try different searx instances and pick one that returns Google results in English (or your preferred language) even with no language setting.. I've been working on this for a while in #565.\nYou can currently try out that branch in http://jiwfbtg2kfkadjku.onion\nSometimes Google will block with Captcha and sometimes if the relays are too slow the engines may reach timeout. But it's otherwise usable.. If you are using my branch, you have to uncomment the proxy lines in the settings.yml file. This assumes Tor is currently running in your server and listening to port 9050.. Right now, search languages are not manually added. Instead, we generate the language list by taking whatever languages are supported by most engines. Currently the threshold is set at 70% of the engines that accept a language parameter. For example, Slovak is currently supported by 12 engines out of 19 engines with language parameter. That's around 63% of the engines, which falls slightly short of the threshold.\nHowever, I've also noticed that this approach is not the best since adding a single engine can skew the list quite a bit. For example simply adding language support to Bing Images and Videos was enough to make Slovak fall below the threshold. There's also the fact that country codes do not require this threshold, so we get extremely uncommon options like Andorran Portuguese.\nI can think of two alternate approaches:\n * Set the threshold as a fixed number (I'm thinking 10 engines) instead. This would eventually increase the selectable languages as more engines get added and also ensures that a language that has been added will most likely not go away.\n * Only include languages which are supported by all general engines that are enabled by default (currently Wikipedia, Google and Bing). This would ensure that all selectable languages will always return results in the languages given the default values, regardless of what the more \"obscure\" engines choose to support.\nEither approach would get Slovak up there again.\nI could do either alternative, but I'd like to see other opinions.. I also want to point out that, even if a language doesn't appear in the list, you can still get results in that language using the : prefix. For example, searching :sk Jupiter should still return results in Slovak. Of course, this isn't really nice, but it's a useful trick for less common languages.. I'm currently working on this.. Lithuanian is also included in the new drop-down list since it actually passes the threshold.. As you already mentioned, all your queries work if you actually specify the correct language in searx. That is, if you explicitly request all English sites with \"Forum cliccando sul tasto rosso\", then it will return the English sites with that string. If you request Italian content, the original source in Italian will be shown.\nThe problem with letting Google \"magically\" guess the language on our behalf is that it leads to weird behaviour, as you can read on #1085 . For example, search.ch usually returns a mix of German and English results if you let it guess on your behalf, because of the server's location.\nSo, if we search in google.com, it will ignore the user's chosen language and still return results based either on the server's location or the query. On the other hand, if we enforce a domain, results will always be in the correct language but they will be filtered and may therefore miss more relevant results. Fixing this breaks #1085 and vice versa.\nThe other alternative would be to add the \"Automatic/All\" option again, although that has its own set of problems.. I hadn't noticed that pull request. It would probably be better to merge that one instead.. I think it's related to the HTTP headers being sent. If so, then #1330 or #933 should fix this.\nAt least that made it work in a local test.. > you may send about 20 requests only for one search engine.  \nMy only concern with this is that maybe some engines could consider 20 identical simultaneous requests as spam and then block us because of that.\nI think we should limit the number of simultaneous languages to avoid this risk.. Fixed. It should be working now.. I think that, when using Tor, Tor Browser's user agent would make our requests blend in better with \"normal\" Tor users, while using unique agents would only make our requests stand out compared to the other (non Searx) requests coming out of the same exit node.. ",
    "GreenLunar": "For now, I think searx should use favicon.ico until it will be deprecated. When favicon.ico be deprecated, we shall begin a new discussion in this concern.\n. I understand. Please provide me with example searx pages (not necessarily working searx node) and I will confirm to you as to whether they look proper or not. Contents of page About must not be affected, unless these contents would be translatable.\n. You can do it, using CSS.\nhttp://www.qupzilla.com/?lang=fa\nhttps://github.com/QupZilla/qupzilla.com/blob/master/css/rtl-helper.css\n. Looks good.\nTop bar 'searx home about preferences' and Filters (general, file etc.) should be reversed.\n\nyou can ask and provide traduction on transifex.org, if you want ;) \n\nHebrew translation is pending.\n. Alternatively, you can ask if RTL by reading an entry in xx.po files (GTK+ and Qt do so).  I do not know what would be the lesser expensive way.  See  _direction https://github.com/Tox/Tox-Website#language-file-metadata\n. Be RTL\n- While /about need not be RTL yet, its top bar should be RTL. EDIT: oscar theme\n- In /preferences, only Default categories row is placed properly. The rest of the rows, such as Search language is still left-to-right.\nBe LTR (must not be affected by RTL)\n- Table for maps that appear when clicking on Show Details (key value). See /stats for a good example, where RTL is respected, but contents  (e.g. vimeo 0.90) are always LTR.\n- I think the same too for /preferences#tab_engine, but I think we better not touch /preferences#tab_engine until we get to have more RTL users voicing their thoughts.\n. > I thought about that too, but here, it's an UX question. Does an RTL user has an habit to have the buttons to the left ?\nAbout what? (thought about that)\n\nFor the suggestions, I don't know how to do it exactly. If @pointhi know how to, he is welcome to fix that, but if not, it will take a bit of time, while I try to find the solution in bootstrap.\n\nYou can use an </html> tag, or &rlm: or &rle: to change direction. I would prefer an </html> tag over a Unicode Control Character.\n\nDon't forget to test the others themes too ;)\n\nJust did.  /preferences#tab_engine of  courgette theme can be set to be aligned from right-to-left (columns are properly placed, but everything is aligned from left-to-right).\n\n\nI think the same too for /preferences#tab_engine, but I think we better not touch /preferences#tab_engine until we get to have more RTL users voicing their thoughts.\n\nNo. Instead, place keywords in an own column. Please ignore this little thing, for the time being. EDIT: See #230.\n. > I took everything you said as being in Oscar. If otherwise, try to specify it.\nYou are correct.\n. > I thought about that too, but here, it's an UX question. Does an RTL user has an habit to have the buttons to the left ?\nRTL speaker: Yes.\nLTR speaker: Yes :P\nRun in terminal on Linux: LANG=ar_SY.utf8 command (GTK+ or Qt graphical application) and see for yourself.\n. > > Just did. /preferences#tab_engine of courgette theme can be set to be aligned from right-to-left (columns are properly placed, but everything is aligned from left-to-right).  \n\nAgain, it's not really clear. It's aligned to the left. Would you prefer the same columns order, but everything aligned to the right ?\n\nYes.\nPlease edit your comment and close <> with ``.\n. > See /stats for a good example, where RTL is respected, but contents (e.g. vimeo 0.90) are always LTR.\nBe LTR (must not be affected by RTL)\n- Statistics tables of courgette and default themes should be LTR because content is entirely in English. Maybe some RTL users would disagree.\n. > In /preferences, only Default categories row is placed properly. The rest of the rows, such as Search language is still left-to-right.\nNow, it looks fine, on wider screens, but there is a problem when view is smaller (see screenshots).\n \nPlease revert (not in repository)  last RTL-related commit in searx.me so I would be able to take a screenshot of preferences using oscar theme.\nEDIT: If there would not be smartphones, I would not bother on reporting this smaller-display issue.\n. Default Categories is not placed the same as the rest.\nI will test again with a WebKit web browser and report.\nP.S. there was a nicer new arrangement for this page a week ago or so.\n. RTL UI is fine.  When I would find problems, in mobile layouts, I guess, I would post about them here.\n. > Be LTR (must not be affected by RTL).\n- Table of Cookies (tab_cookies).\nTheme: Oscar (no other theme has access to cookie table)\n. Start with little code. I will report on problems over time.\n\nAlignement of suggestions\n\nMaybe.\n\nAlignement of cvs, json, xml\n\nNo.\n\nEngines in prefs ( google (go) appears as ( google (go. I think it will be solved by using a real rtl script but still)\n\nUse &lrm; -> (go)&lrm;\nhttp://www.fileformat.info/info/unicode/char/200e/index.htm\n\nAlignement of images results\n\nMaybe.\n\nSearch field in index not the right size\n\nWhere is that?\n\n@GreenLunar if you know how to test it, please do. It's hard to tell if RTL is correct using latin script, and my hebrew is a bit rusty...\n\nI need changes to be applied in a public searx node in order for me to test this.\n. > For the alignements, it's mainly to align the boxes to the right, not the text itself. The same for the images.\nPrecisely.\n\nThanks for the &lrm;, it was that :)\n\nYou are welcome.\n\nI'm sorry, but I can't provide a public searx node. So we'll have to wait until the code is merged, and we will see from there.\n\nNo problems.\n. This lesser than two days period, in which I am waiting to test searx in RTL, feels like a week, to me.\n. > But it still weird to have to delete this branch without merges ;)\nEven weirder, implementing RTL support before having RTL locale added :P\n. > Alignement of suggestions\nYes, set it to RTL. EDIT: oscar theme\n. I forgot to mention: Great job!\n. I have found this nylira/prism-break/pull/1123 but searx is not yet in https://prism-break.org/\n. > Probably, searx is simply unknown for most people, so they think it is irrelevant.\nNo, most of them are, just, dicks who are paid by governmental or private organizations.\n\nNEWS: Is SquirrelMail \"Notable?\"\nJul 23, 2010 by Paul Lesniewski\n  It seems that one Wikipedia user thinks that SquirrelMail is not considered \"notable\" enough to retain its own Wikipedia page. Do you agree? Disagree? Voice your opinion.\nUPDATE: As of November 2010, SquirrelMail's notability seems to have been commonly agreed upon and the notability warning has been removed. Thanks to everyone who contributed to the discussion and especially those who took it upon themselves to improve the article itself!\n\nhttps://en.wikipedia.org/wiki/Talk:SquirrelMail#notability\nhttp://squirrelmail.org/\nMy aunt got payed for several years by a Hasbara (our Ministry of Propaganda) division to write on Wikipedia articles and contents in favor of State of Israel.\nEDIT: Reading articles about or related to State of Israel in Hebrew Wikipedia is like reading stories of science fiction, and I am sad to admit it.\n. Potential places to spread searx at:\nhttps://wiki.archlinux.org/index.php/Squirrelmail\nhttp://www.yacy-websearch.net/wiki/index.php/En:Seeks\n~~http://directory.fsf.org/wiki/Searx~~ (Seeks is missing too)\nhttp://directory.fsf.org/wiki/Searx.me_search\nhttp://directory.fsf.org/wiki/Collection:PRISM\n. I suggest to point out the set of engines that searx provides.\nPlease help me spread searx at https://github.com/libreprojects/libreprojects/issues/115\n. PUUUU! HA! HA! HA! Geographically limited scope.\nAsk some of the administrators of the various searx nodes to limit the scope of their results to get in to the biased encyclopedia!\nMoney makes Wikipedia go ROUND AND ROUND AND ROUND...\nIt is not about free flow of information, it is to canal information for the masses, the way they want to!\nTry to insert [[searx]] to Metasearch engines section.\n. I have found a good resource that may help us at https://wiki.gnome.org/Engagement\n. I have removed http://metasearchenginewatch.com/ from my bookmarks; there is no mention of Seeks nor searx.\nThere is an old article (December 1st, 2011) about YaCy by Rob D. Young.\nMarketing searx is a difficult mission!\n. > Marketing searx is a difficult mission!\nIt appears that some even consider it as spam.\nhttps://github.com/The-Compiler/configs/commit/431e7a6d5edd45214046599fa5413efe30995833#commitcomment-13987605\n. Looks good!\nWhere do you want me to report on RTL bugs?\n. A good practice for theme makers is to add a switch to enable cache or any other feature that is disabled by default in a theme.\n. > Anyway, I strongly disagree ;)\nDo you disagree with adding switches in preferences of a theme?\n. I agree, but I did not propose to enforce, I have just stated that it would be a good practice.\n. Localized interface issue would occur only with Mozilla Gecko-based web browsers.\nDue to potential privacy issues, I am closing this report.\n. Would it be possible to pass &uilang=fr and redirect to the same URL without &uilang=fr?\nEDIT: Never mind. UI alone is too trivial to bother for.\n\n\nMaybe searx RSS URLs should have query string such as &uilang=fr so prefered language be kept in feed when using RSS readers.\n\nIf one would want to follow an RSS feed specific for a certain language, they can add language-specific terms or characters that would make results more specific to that language.\n. > You can add :hebrew or :he in the request string to choose the language of the results.\nI failed to do it. Please post a complete example string.\n. I was confused.  I am reopening this issue.  If header sends language preferences, then there should not be a severe issue, since user may change it at anytime\nP.S. User Agent, in and of itself, is the bigger problem.\nFeel free to close this issue.\n. I like it.  This theme lack a preferences link, it seems.\n. There is already https://github.com/asciimoo/searx/wiki/Searx-instances but it may be good to have it on website too, similarly to https://gnu.io/social/try/\n. Sorry, I did not set a preferred language.\n. Ever since they became proprietary. I have seen this argument rising up in several projects already.\nCopyQ, Exaile, Money Manager EX and even Gammu are currently being translated at https://hosted.weblate.org/ using Weblate which provides integration with Git.\nPlease DO move away from the translation platform in question.\n. BUMP!\n. I refuse to keep the translation platform in question.\nWhen more and more projects would show contempt for Proprietary Services that used to be also available as Free Software, they might reverse back.  GNU MediaGoblin has moved away from the translation platform in question to Pootle.  Also, we can gradually migrate translators and translators can even login to Weblate with GitHub and other social network accounts.\nP.S. I have agreed to register to the translation platform in question because it WAS available as Free Software.\nVOTE WITH YOUR WALLETS! VOTE FOR FREEDOM!\n. > Oh, and \"the translation platform in question\" isn't a bad word, you can use it.\nOnly if they pay me, every time I do ;-)\n\nFor migrating translators, they choose what they do. And some of them may not be on github, and some of them are not devs. And so, some of them will be lost when changing platform.\n\nTranslators can even login to Weblate with GitHub and other social network accounts, such as F and G.\n\nSearx's code and translations are still open source, no matter what. Even if transifex is closed source, and even if GitHub is closed source, and even if the routers at your FAI use closed source firmware.\n\nI use DD-WRT.  The translation platform in question was available as Free Software.\n. > Oh, and BTW, you prefer using an open source platform but recommand using closed source credentials ? That's weird ;)\nDid I recommend?\nI am not willing to  continue this thread with you.\nI think that migrating to Pootle or Weblate (even better) would be good.\n. Comment by nijel\n\nCurrently this feature is available only for commercial (non public) translations. This might change once nijel/weblate/issues/516 is implemented.\n\nDo not use Weblate, yet.\nWhen restricting access would be possible, I will ask, again, to migrate to Weblate.\n. Any news on this?\n. @ArtisTechMedia is it possible to crawl ccMixter.org?\nEDIT: It appears so, http://ccmixter.org/view/media/home\n. Should we close this issue and list the above websites at https://github.com/asciimoo/searx/wiki/possible-search-engines instead? (already listed)\n. Oh, it is not publicly visible\n\nOne of our mostly harmless robots seems to think you are not a human.\nBecause of that, it's hidden your profile from the public. If you really are human, please contact support to have your profile reinstated.\nWe promise we won't require DNA proof of your humanity.\n. Please fix this soon, it is a small fix.\n. On the days Identi.ca used to run a StatusNet (gnusocial) instance, there were many users marketing free software, especially decentralized ones, though not any decentralized software.\n\nStarting a (free) social profile on one of the above medium means, might significantly boost up the awareness to searx and Seeks by free software enthusiasts.\n. Useful resource http://whird.jpope.org/crossposting-to-all-the-places/\n@jpope777, please participate in this discussion.\n. @jpope777, I suggest you to start up a profile and searx staff might like to take over it :-)\nAlso you might be interested in reading these threads #222 #243\n. @jpope777, did you make any progress in this regard?\n. #466\n. You can start by user typing ip and the IP address of a user would be displayed in search suggestions.\n. I think there are enough, there are, at least, five public active Seeks nodes available.\n. https://seeks.mxchange.org/ should be https://seeks.mxchange.org/search.php\nSee https://github.com/beniz/seeks/issues/16#issuecomment-121771057\n. > For now searx is stateless, with the seeks protocol support this won't be the case any more.\nI disagree, I think you can have it both ways.  We can, as you pointed out, have it implemented as a plugin or as a switch (true/false configuration option).\n. Foren-\u00dcbersicht \u2039 YaCy Development \u2039 Jobs\nget YaCy to connect to the outside world\nBeitragvon Orbiter \u00bb Mi Jul 10, 2013 9:09 am\nhttp://forum.yacy-websuche.de/viewtopic.php?f=21&t=4702\nhttp://www.freelancer.com/projects/System-Admin-Troubleshooting/Help-get-YaCy-connect-the.html\nHelp me get YaCy to connect to the outside world not reaching any peers\nThis project received 7 bids from talented freelancers with an average bid price of $22 USD / hour.\n@yacy, are you interested in YaCy to be connected with searx and Seeks nodes?\n. Good idea.\nRelevant link https://codingteam.net/project/mysearch/bugs/show/3053\n. Great!\n. I did not know of the ability to incorporate GitHub comments into DIASPORA* https://pod.jpope.org/posts/669100\n. I think you should set the logo width to 100% on smaller screens, such as cellular phones, and add the transparent top bar, similarly to Sandy Seventies Speedboat theme of Mediagoblin, and no top of it add RSS link and other links of About, Home (may be in a form of the searx logo), and Preferences.\n. This ticket is proper for searx theme repository, when would be.\nClosing.\n@gnome-integration-team, feel free to contribute a GNOME theme ;-)\n. Prefer to display entries from OpenWrt Wiki and DD-WRT Wiki over Wikipedia.\nhttp://wiki.openwrt.org/start\nhttp://www.dd-wrt.com/wiki/index.php/Main_Page\n. I have added the above resources to https://github.com/asciimoo/searx/wiki/Infoboxes-brainstorming\n. Agreed.\n. Similarly to Ahmia, I suggest to make a list of check-sums, so that blacklisted websites would not be promoted by searx.\n. Thanks.\n@Cqoicebordel, I have updated the first post.  I am not sure if I should include Files from forum boards since it is too much work to keep up with changes, I think.\n. Do you mean that searx utilizes such techniques automatically (when File category is selected) or users can use the above techniques directly from searx or did you mean to both?\nI was thinking of the automated manner, the first one.\n. I have added two search engines to https://github.com/asciimoo/searx/wiki/possible-search-engines\nAdd search engines for Files, Audio and Video based upon advanced search operators\n. I figured to compose a file with common Video formats from which Searx would draw information, yet it would not be practical with Files search due to large range of file types.\n. @ukwt and @pointhi, may you suggest a better way to handle this?\n. Yes, client-side list is what I had in mind, but how can client-side list be implemented securely?\n. By no means I asked to find out where users are.\nMaybe I should have asked for a feature to use several searx nodes of different ip addresses/locations and sort them out in the node which operates as client.\nThis is what I ask.\n+-- searx node in never-land\n                              | \nUser < searx UI < filter node +-- searx node in wonder-land\n                              | \n                              +-- searx node in no-land\nEach node, if a searx server is connected to more than a node, searches for the same thing, according to user preference (e.g. by language) in different territories.  This is a way to circumvent censorship, maybe.\nIf you think the issue I have referred to has no new idea to implement on searx or is too complicated or expensive to implement, then please close this issue.\n. It appears to be working.  Shall we close this issue?\n. I don't see any difference.\n. It appears not to work when !wp :hu is not used https://searx.me/?q=k%C3%A1poszta&categories=none\n. > but you still have the \"powered by searx\" link at the end\nThis is also subjected to removal, as it is in many websites, mostly commercial websites, that are powered by Joomla or WordPress; nevertheless, I tend to support your request.\nAnother reason to implement this is to see how themes appear against different graphics, such scenario might lead to rethink the Searx logo and make Searx more attractive to new users.\n. This will probably be in next release.  See #526 and http://i.imgur.com/jHgJg2X.png\nThanks to @ukwt.\n. This has been accomplished in #526\n. Do you know how to do it using GitHub WebUI, without repeating the above edits, all over, again?\n. Feel free to reject this PR and make those changes yourself.  I am using pseudo names, anyway.\n. I just read the note says \"Add more commits by pushing to the master branch on GreenLunar/searx.\"\nI think it worked for me.\n. I guess, @jibe-b has referred to the button in Searx, not of a web browser.\n. I have the same problem.\nWeb browser RSS button refers to https://searx.me/?q=python&format=rss&category_general=1&pageno=1\nSearx button refers to https://searx.me/ which would not fit in a feed reader.\n. Big web companies have no interest in doing so, they like to foist their tracking services on us and they do not like to provide us with easy, documented, and simpler means to custom user preferences.\n. This Pull Request is here for reference and as a reminder.\nMaybe we should enhance the entire line.\n. I have changed the title and added to it \"worthy commercial interest groups\", which I doubt exist.\n. Certainly, but not publicly.  I did not open this thread to promote commercial anti-virus companies.  Anyhow, please focus on the subject thread.\n. @d9h02f\n\nYou don't want infoboxes written by antivirus companies, but you don't have a problem with them as long as said companies pay for them and are 'worthy', but then just after you write that you doubt those even exist.\n\nI think you have missed the point.  The \"anti-virus\" mentions are for realization only!\nI also see the problem with my wish, as you can tell.\n\nRight now, there are infoboxes with information from DuckDuckGo, and what do you know, searching for 'Cryptolocker' gets you a nice infobox with a short description of Cryptolocker. Searching 'GnuPG' gets you an infobox with a short description of GnuPG. So, what's the problem?\n\nYour comment is meaningless.  I will not respond to your next comments.\n@pointhi\n\nI also don't see what you want to achieve with this proposal.\n\nRevenue.\nThis is an idea.\nThink about it.\n. I think this issue has been resolved in #526 by @ukwt, even though not using an API.  See https://github.com/asciimoo/searx/pull/526/files#diff-30ca69a7974663a3b84d9047bd867a91\n. Duplicate of #447\n. Ping to @huzheng001 and @tuxor1337, in case you would find this interesting.\n. Related issue #60 (adding Widget support)\n. Also consider to forge/switch timezone once IP changes or every X minutes.\nBeware of changing time itself, because Tor would not let you in to the Tor network once your time has an unreasonable difference (e.g. a difference of an hour).\nNotice that only after I setup system clock to a reasonable time, Tor would connect.\nApr 13 16:55:29.000 [notice] Application request when we haven't used client functionality lately. Optimistically trying directory fetches again.\nApr 13 17:56:09.000 [notice] Your system clock just jumped 3595 seconds forward; assuming established circuits no longer work.\n$ tailf /var/log/tor/tor.log\nApr 13 16:54:48.000 [notice] Bootstrapped 5%: Connecting to directory server\nApr 13 16:54:48.000 [notice] Bootstrapped 10%: Finishing handshake with directory server\nApr 13 16:54:49.000 [notice] Bootstrapped 15%: Establishing an encrypted directory connection\nApr 13 16:54:49.000 [notice] Bootstrapped 20%: Asking for networkstatus consensus\nApr 13 16:54:50.000 [notice] I learned some more directory information, but not enough to build a circuit: We have no recent usable consensus.\nApr 13 16:54:50.000 [notice] Bootstrapped 25%: Loading networkstatus consensus\nApr 13 16:54:51.000 [notice] Application request when we haven't used client functionality lately. Optimistically trying directory fetches again.\nApr 13 16:54:52.000 [notice] I learned some more directory information, but not enough to build a circuit: We have no recent usable consensus.\nApr 13 16:55:05.000 [notice] Application request when we haven't used client functionality lately. Optimistically trying directory fetches again.\nApr 13 16:55:29.000 [notice] Application request when we haven't used client functionality lately. Optimistically trying directory fetches again.\nApr 13 17:56:09.000 [notice] Your system clock just jumped 3595 seconds forward; assuming established circuits no longer work.\nApr 13 17:56:09.000 [notice] Tried for 3680 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\nApr 13 17:56:09.000 [notice] Tried for 3681 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\nApr 13 17:56:09.000 [notice] Tried for 3681 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\nApr 13 17:56:09.000 [notice] Tried for 3681 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\nApr 13 17:56:09.000 [notice] Tried for 3681 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\nApr 13 17:56:09.000 [notice] Tried for 3678 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\nApr 13 17:56:09.000 [notice] Tried for 3664 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\nApr 13 17:56:09.000 [notice] Tried for 3664 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\nApr 13 17:56:09.000 [notice] Tried for 3664 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\nApr 13 17:56:09.000 [notice] Tried for 3640 seconds to get a connection to [scrubbed]:443. Giving up. (waiting for circuit)\nApr 13 17:56:11.000 [notice] Bootstrapped 80%: Connecting to the Tor network\nApr 13 17:56:11.000 [notice] Bootstrapped 90%: Establishing a Tor circuit\nApr 13 17:56:12.000 [notice] Tor has successfully opened a circuit. Looks like client functionality is working.\nApr 13 17:56:12.000 [notice] Bootstrapped 100%: Done\n. Perhaps there should be added a note for these search engines at bottom of page, or in configuration files.\n. Which is even better, if users would be satisfied with results that are not provided by it.\n. I think Searx should also offer a 16x16 icon.  Qupzilla displays a default search icon while Otter Browser displays Searx logo.\n. This solves #459 (F-Droid)\n. Should issues #543 (anti abuse systems) and #537 (download video plugin) be taken into consideration, too?\n. Do not forget to refresh and update translations.\n. > There is one serious problem with this approach though: it works very slowly, extracting links for each result takes a few seconds\nI agree with @pointhi. In no way, this process should occur automatically to several results at once.\nSuggestion: Once clicking on download video button, a popup with text \"fetching list of video formats\" with three squares (loading... indicator), and then the list of available video formats.\nthree squares - similarly to those displayed upon attempting to open map details.\n. I guess, better than a popup.  Open a new element, similarly to the table opened upon opening map details.\n. Well done!\n\nI think that table contains too much information, but condensing it into a small informative string could be problematic because field formats vary greatly across all supported engines.\n\nI suggest to add an option to filter unwanted file formats (may be called preferred media formats), when there are several file formats to select from.\nI am only downloading OGG (.oga or .ogv) or WebM when available.\n. A suggestion for an improved UI.\nSuppose I marked OGG as preferred format; results would be displayed as follows:\n```\n          OGG video 1\n          OGG video 2\n\nShow all available videos\n```\nBy clicking on Show all available videos, user un-hides all the rest of the available formats to download.\n. > Thanks @GreenLunar for the suggestion\nYou are welcome.\n\nthat's how I should have done it in the first place.\n\nI, too, did not think of it in the first place.  I guess you would have thought about this design by yourself, without me, sooner or later :)\n\nhttp://i.imgur.com/8kpmyPf.png\n\n~~I prefer No videos available to download over Nothing found.~~\nI prefer No available media to download over Nothing found.\n\nhttp://i.imgur.com/ED2BJJ9.png\n\nClick here to display all results does not sound good to me.\nSuggested text: display all available results.\n\nhttp://i.imgur.com/0U13LfR.png\n\nclick here to display other NUMBER results is better.\nPerhaps display other NUMBER results (without click here) would be best.\n. I forgot to point out that, it seems you have almost finished to work on this plugin :)\n. > Any other ideas or suggestions? Table sorting wouldn't hurt, I guess.\nI guess so.\n. > Any other ideas or suggestions?\nFiltering, like in spreadsheet editors; but this is for another time, I think.\n. @This is good.\nI meant to a fixed filter; that is, if available extensions are flv, ogg, webm, then user would have three menu-item entries to filter upon clicking.\nIs this plugin ready to be released?\n. If you need someone to perform QA experiments, I am willing to assist.\n\nI've spent a few hours experimenting and haven't found a solution that would both keep icons on one line with text and keep them to the right of the cell, at least without major layout changes and changing the code.\n\nI suggest not to include filter and sort buttons, for now.\n\nYeah, I think. There is one problem with some results though:\n\nI think we can waive some suggestions I have made over time, such as filtering and sorting; yet trimming results and a preference to set preferred formats resolutions etc. are more than enough; other enhancements may be implemented after the next release.\n\nDoes this look good enough?\n\nYes.\n. > Nah, it was almost ready, with the exception of those stubborn icons. But that seems to be fixed now.\nThis plugin has more than I have expected; I think, even less is enough for initial release.\nYet, thank you for your efforts.  You have done a great work!\n\nDownloading the code is easy\n\nI don't have the needed dependencies.\n. Searx would offer a torrent link or magnet link (similarly to magnet link of supported torrent websites).  To download directly, user would have to enter the webpage; perhaps a download link would fit, for this particular case.\nFor direct downloads, please read #426.\n. > https://dl.dropboxusercontent.com/u/18321926/cast.mkv\nWhat is the screen recording software you used to compose this screencast with indication of given pressed keys?\nBy the way, for being so productive, I think @ukwt should be joined as a member of searx organization.\n. Thank you for the information.\n@danielgtaylor, why did not you upload key-mon, key-train and the rest of your projects there to your domain here? << I will delete this line later because it is absolutely off topic.\n. Beautiful theme.\nThe dropdown menu appears to be buggy on small screens.\n. I think that every chance for an activity with Mozilla is an opportunity, because of its poor state.\nIf Mozilla would attempt to force or foist on Searx a different agenda, just break up with it.\n. There is also not Evil at http://hss3uro2hsxfogfq.onion/ (was TorSearch at http://kbhpodhnfxl3clb4.onion/)\n. Try searching for Doggy or Hamster.\nJavaScript script, even though can be disabled, can run on client-side, which consequently saves load on server-side.\n. Even though users can disable JavaScript, JavaScript script can run on client-side; Python, however, can only run on server-side.\nIf you want to do it in Python, I have no problem with that.\n. So be it in Python on server-side.  @kvch, why do you think this issue is invalid?\n\nhow would you even detect suspect results using JS? Downloading a blocklist every time?\n\nYes, a blocklist in a form of a keywords dictionary, which may be stored in offline cache.\n. The default theme is fine.\nThe bootstrap CSS files of the default theme are the files that I want to replace.. ",
    "soredake": "ping  @asciimoo . ",
    "henbus": "there is no folder oder link https_rules in /usr/local/searx/searx-ve/local/lib/python2.7/site-packages/searx-0.3.1-py2.7.egg/searx/\ni fidex it with:\nln -s /usr/local/searx/searx/https_rules/ /usr/local/searx/searx-ve/local/lib/python2.7/site-packages/searx-0.3.1-py2.7.egg/searx/https_rules\nthe install-script has little mistake.\n. ok, thanks.\nby-the-way a small mistake in the install-documentation under apache: mod-uwsgi is false, the right name is only uwsgi (in my debian version 7.7 mit apache 2.2.22), also\nsudo a2enmod uwsgi\n. ",
    "shea256": "I had this same problem.\nI was using gunicorn and moving away from gevent workers was the fix I went with.\n. ",
    "amitt001": "I think this problem still persists at least with python 2.7.9(atleast, on heroku).\nA workaround is to degrade to python version 2.7.8 or lower.\n. ",
    "avelino": "Is expected when resolves in Python version 2.7.9?\n. ",
    "FuqiangZhu": "I'm sorry.. ",
    "Timidger": "I'm using Firefox, but I use a plethora of different plugins so I'll disable them and check the results out then when I get back to my computer\n. It appears to be working, not sure where I had gone wrong. It might have been a misconfiguration on my broadband USB modem that I have just recently begun using. Thanks for helping me blunder through my mistakes.\n. ",
    "cy8aer": "Yeah thanks for the plugin: Open result links on new browser tabs\nClosing.\n. wadoku seems to have an api too (the whole server is oss): https://github.com/Wadoku - but it is rails. And yes: multi-engine would be nice if there is the possibility to have a module system (maybe?)\n. Works. Thank you.\n(language: de_DE)\n. hm, reading helps. I junked up my settings.yml...\n. I did not change my settings.yml the right way, sorry\n. Ahem thanks for implementing it. But two things: shortcut yandex is ya, shortcut for yacy is ya, prefer yn for yandex.\nSecond thing (yes this is an early implementation): yandex has international services so a localisation would be nice.\n. I am not sure but they seem to select it by domain: yandex.ru russian, yandex.com english, yandex.com.tr for turkey. Just look at http://www.yandex.com for other languages.\n. hm, now it is not working for me anymore (debian jessie):\nTraceback (most recent call last):\n  File \"/usr/share/searx/searx/__init__.py\", line 55, in <module>\n    environ['REQUESTS_CA_BUNDLE'] = certifi.old_where()\nAttributeError: 'module' object has no attribute 'old_where'\n. python 2.7.9, certifi 2015.04.28. Something too old (so need pip reinstall)? But 2015.04.28 looks to be used in comments above.\n. ```\nPython 2.7.9 (default, Mar  1 2015, 12:57:24) \n[GCC 4.9.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport certifi\ncertifi.old_where()\nTraceback (most recent call last):\n  File \"\", line 1, in \nAttributeError: 'module' object has no attribute 'old_where'\n.\ndir(certifi)\n['builtins', 'doc', 'file', 'name', 'package', 'path', 'core', 'where']\n```\n. hm, after realising that I am using a ve the system runs after upgrading to 2015.11.20.1. Thank you. That may not effect new installed environments but old installations (never touch a running system). Maybe a comment to upgrade to 2015.11.20.1 may be useful.\n. for the requirements name: I am using docker 1.8.3~ds1-2 (debian testing). But I have problems with simply copying files into a directory instead of naming it for a long time. Looks like this for me:\n\n\n\nStep 5 : RUN adduser -D -h /usr/local/searx -s /bin/sh searx searx  && echo '#!/bin/sh' >> run.sh  &&  echo 'sed -i \"s|base_url : False|base_url : $BASE_URL|g\" searx/settings.yml' >> run.sh  && echo 'sed -i \"s/image_proxy : False/image_proxy : $IMAGE_PROXY/g\" searx/settings.yml' >> run.sh  && echo 'sed -i \"s/ultrasecretkey/`openssl rand -hex 16`/g\" searx/settings.yml' >> run.sh  && echo 'python searx/webapp.py' >> run.sh  && chmod +x run.sh\n ---> Running in 37501430e394\n ---> 4a53733158d2\nRemoving intermediate container 37501430e394 \nStep 6 : COPY requirements.txt .\nstat /var/lib/docker/btrfs/subvolumes/f1325da84c75d4e52b00ab9c263718a6a456f51587769096f498b72b11b3e280/usr/local/searx/requirements.txt: not a directory\nWhen I for myself build containers I create separate files every time. This is better to maintain.\n. requirements.txt: I am using a btrfs based docker system (running with -s btrfs). And this may be the point why I need to explicitely need to name the target file in copy sections. This may be interpreted in another way like, say, aufs.\n. I'd like to know what happens there: https://forums.docker.com/t/copy-different-with-btrfs/5659\n. I tried it with uwsgi http port and apache reverse proxying via filtron and get:\nResponse error: the server closed connection before returning the first response byte. Make sure the server returns 'Connection: close' response header before closing the connection HTTP/1.1 200 OK\noften. Any hints to solve this? Reverse proxying apache -> uwsgi http port works smooth.\n. Standard Jessie 2.0.7.  I guess it is a frontend problem and the apache closes the connection to the uswgi.\nIt is a comprehension problem for me. We need a cascading of reverse proxies (so no mod_uwsgi for apache), right? Then I configure the apache proxy connecting to the filtron and then filtron to the locally running uwsgi of searx with an open http port.\nFiltron makes no keep-alive to searx but what makes apache by default (or ist this some misunderstanding by me that apache has some influence?)\nMaybe we need an reference documentation for the whole proxy chain in the classical installation document?\n. Yeah, that is the document I try it with. For me it fails with both http and http-socket. The first connection to the server works and with starting up the first search it creates the error. I tried http-keepalive on the uwsgi side but this makes no effect (just for playing around).\n. next test scenario: Going directly to the filtron port (no apache in front). Still the same problem. So it is definitely a problem filtron -> uwsgi for me. Sniffing the connection shows a direct tcp-rst from the uwsgi server with the second request and filtron puts HTTP 429 Too many requests to the front.\n. Just a question for the engines handling: Is it possible to rewrite a query? I am thinking about this:\nquery: bla.blabber.blab <--- This is a w3w string\nw3w engine gets (via api) coordinates. These coordinates could be used for quering via openstreetmap and photon. It is a search in two steps: you do not search the address but the w3w string and with the result you search in the other engines.\nSo the query string must be rewritten for the following engines?. Another thing: You have a map result (by quering an address or so). With photon and openstreetmap you have the links: show map and show details. As I understand this is done by the engines themselves. For w3w it would be nice to have an additional what3words link for quering the w3w string if clicked. But there is no mechanism to add this link without changing all other engines, isn't it?. Some further enhancement: why not make @ and # clickable too (to search in twitter)\n. hm, it did not work with searx so I tried the url via curl. It works on my lap, but not on the server. They seem to kick out my IP. When opening the site with a browser they do not show me a \"I am a human\" captcha. Boom\n. same problem with smaller desktop pages. Then the info box moves to the left (??) - oscar pointhi\n. Pull request is running... https://github.com/asciimoo/searx/pull/666 but this is simly xpath...\n. API exists: https://deusu.org/api/api_v1.html\n. Let's wait for MichaelSchoebel's API... As I wrote: my patch is quick and dirty.\n. @DanielGilbert sic! You are right.\n. IMHO it would be better to have the web servers create the headers if you want to have additional headers (like key pinning stuff).  But because Apache/mod_uwsgi does not work with header add I hard code my headers in webapp.py by now which is not very easy because I need to edit webapp.py with every git pull. Is there an mod_proxy_uwsgi description for searx? mod_proxy_uwsgi makes web server side headers possible.\n. after reverse proxying my uwsgi <-> apache connection by http all my headers are in apache now. So updating is easier now because there is no webapp.py patching anymore. Headers in apache is also good if you are intercepting by filtron (if it works) because then you do not have problems with loosing headers.\n. Google is dead for me (as startpage).\n. Seems that google sets GOOGLE_ABUSE_EXEMPTION cookie after captcha. And this cookie has a timeout date (sometimes). If there would be some proxying mechanism for the captcha for re-setting the cookie would be nice\n. what about forwarding the captcha to the frontend and save the cookie in the backend then?\n. Yeah but I did not know the name of the cookie 'GOOGLE_ABUSE_EXEMPTION'. Thanks to gszathmari. yes, this patch works for me.. That is an argument. Just wait, till I get transifex under control...\n. ... done (hopefully...)\n. hm, <b> is shown and rendered </b>. ... in addition: Ponthi is not rendered well (Line breaks in captions). I am sorry, reopening. I am not able to upload a screenshot.... \nJust search for \"searx\" and look at the entries... Looks for me like in this attachment done with newest version.\n. this looks fine with version 7e1f27e45924147cc2219ddb9299460f202b206b version (screenshot), d80fb2c8e8995facb3a25c152c47a93eecf1fee4 fails. \n\n. yes, works for me.. \n. created a patch by myself (pull request #782). Quite simple if you know what to do.... solves issue #781 . I am sorry: reverse proxy Header set Content-Security-Policy problem. Works again. This is more for the header settings discussion.... I am running a classical no-docker setup on Debian Jessie, uwsgi, apache2 according to this installation documentation https://asciimoo.github.io/searx/dev/install/installation.html.. ... python 2.7.9, uwsgi 2.0.7, apache 2.4.10. There is also no connection entry in my uwsgi log either.. After a connection try to the web page I cannot stop the uwsgi via systemd, I really need to kill -9 the uwsgi processes.. No, the log is empty - it begins to hang right before the connection itself is mentioned in the log.. It looks like a \"wait for thread\" race condition (just a comment of a moron). Then everything hangs and waits for a thread which does not come back.. does anyone has a clue how to set --lazy-apps in a debian jessie uwsgi environment? For me uwsgi does not start when start it by hand and the uwsgi call itself is deeply hidden in the init scripting of the package. Does uwsgi 2.0.7 have this option --lazy-apps?. Ok, got it inserted into /usr/share/uwsgi/init/specific_daemon but this does not look not wery legal. Anyways: now uwsgi is called with --lazy-apps and it works.. @gled-rs thank you for your legal configuration description. This should be added to the documentation https://asciimoo.github.io/searx/dev/install/installation.html. \n. +1 - just saw hooktube on boingboing. What about  a patch?. Just morty, uwsgi/debian-stretch is 2.0.14 should work now, but I need to check it again...\n[Update] I just have Problems with http error 429. Cannot find the right timings/limits so empty pages every second or third click...\nMorty\n[Unit]\nDescription=Morty Reverse Proxy\nAfter=network.target\n[Service]\nExecStart=/usr/local/bin/morty --key yourkeyyourkeyyourkey\n[Install]\nWantedBy=multi-user.target\nsearx settings.yml\nresult_proxy:\n    url : https://yoururl/morty/\n    key : yourkeyyourkeyyourkey\n... and some proxying for morty like apache:\nProxyPass /morty/ http://127.0.0.1:3000/\nProxyPassReverse /morty/ http://127.0.0.1:3000/\n. ... and they need an API Key not freely available. Not good.... (just a question, probably it is just crazy - I also do not want to have burocratic stuff in this project...) So I only need to declare something if I do something with personal data, I must not say: Hey, no data processed? Confused...\nBased on ct.de/dsgvo18 you can find a generator for privacy statements (german sorry - https://datenschutz-generator.de/). If you fill the form there are items about embeded services, like youtube and vimeo. Does this eventually have an effect to the Music and/or Video tabs? How would you handle it (declare, switch it off by default, does not have an effect)?. Ok, I built one for myself and prepared searx/templates/__common__/about.html. But now there is another (technical) issue: When updating searx from git this file is overwritten. It needs to be reinstalled after git reset. It would be nice to have a path where you can put your about.html and it is included from the theme's about.html. Is this possible with jinja2 (if file exist: include else include __common_/about.html)?. I fetch and reset. No commit. So the whole thing is original (except settings.yml). The benefit is that when reporting an issue there is no change which may disturb the debugging. Con: You need to have wishes (like an overwrite about.html) in the original code. But if every installation in the EU needs a modified about.html (or not).... What about an {% include \"__common__/aboutextend.html\" ignore missing %} at the end of searx/templates/__common__/about.html? This would extend the original file with your special extension if exists. aboutextend.html is not in the git repository and everything will be fine... Comments?. Stand by - let's see who is faster.... Hm, typo in the log description: aboutext.html -> aboutextend.html.. I create a new one.... Still depends on #1292. ",
    "Trim": "Hello,\nAbout tests: for me \"node\" means \"nodejs\", do you have it on your computer ?\nReading the Makefile, you need nodejs and grunt  installed on your computer to run tests.\n. Hello,\nAbout these buttons, I'm always sticked with them because I never remind if they are switch to new state or if they display the actual state. So each time I want to modify these settings I have to do a search to confirm that was blocked/allowed.\nIndeed, the red color means \"not permitted\" or \"locked off\" for me (as an European user) and the green the opposite. So here I have a meaning of \"Allow\" with the text and \"not permitted\" with the button color.\nI think the use of a switch will be more clear, because we don't use colors with different meaning depending on the user context (and it will be better for red-green color blind) and it can show the current state and the fact we can change it in the same widget.\nThanks for the good work !\n. Ok, I understand the idea to not have javascript. In that case, I'll prefer to have a table like this:\n| Engine | Enabled |\n| --- | --- |\n| Google | [x] |\n| Bing | [ ] |\n(with [ ] and [x] replaced respectively by an inactive and an active checkbox)\nThe big issue for me, is that, actually, we don't know if the color represents the same thing than the text. Indeed, it's very uncommon to see one widget giving us 2 completely different pieces of information.\n. @privacytoolsIO sorry, for the confusion, I've never developed any thing inside this project, I'm just using searx on my server and providing ways to improve the settings page ;)\n. Hello,\nI had same issue with Debian Jessie (with backports) and searx 0.8.0 (proxied by Apache 2.4.10).\nI have to disable Google engine (either from settings.yml or by cookie) to be able to have no internal server error (see https://s.adorsaz.ch ).\nHere is my last make production results:\n```\nmake production\nbin/buildout -c production.cfg \nDevelop: '/srv/opt/searx/searx/.'\n/srv/opt/searx/searx/eggs/setuptools-18.0.1-py2.7.egg/pkg_resources/init.py:203: RuntimeWarning: You have iterated over the result of pkg_resources.parse_version. This is a legacy behavior which is inconsistent with the new version class introduced in setuptools 8.0. In most cases, conversion to a tuple is unnecessary. For comparison of versions, sort the Version instances directly. If you have another use case requiring the tuple, please file a bug with the setuptools project describing that need.\n  stacklevel=1,\nUpdating omelette.\nomelette: Warning: (While processing egg cryptography) Package '_Cryptography_cffi_26cb75b8x62b488b1' not found.  Skipping.\nomelette: Warning: (While processing egg cryptography) Package '_Cryptography_cffi_590da19fxffc7b1ce' not found.  Skipping.\nomelette: Warning: (While processing egg cryptography) Package '_Cryptography_cffi_a269d620xd5c405b7' not found.  Skipping.\nUpdating pyscripts.\nUpdating supervisor.\nUpdating crontab_reboot.\nVersions had to be automatically picked.\nThe following part definition lists the versions picked:\n[versions]\ncollective.recipe.supervisor = 0.19\nmeld3 = 1.0.0\nsupervisor = 3.0\nz3c.recipe.usercrontab = 1.1\n Please modify /srv/opt/searx/searx/searx/settings.py\n Hint 1: on production, disable debug mode and change secret_key\n Hint 2: searx will be executed at server startup by crontab\n Hint 3: to run immediatley, execute 'bin/supervisord'\n```\n. ",
    "dimqua": "It may be better to improve the two others instead. For instance, courgette theme does not support DDG definitions.\n. Fixed.\n. BTDigg engine isn't working as expected. Please try it.\n. My two cents:\n- Duplicate results should be combined and placed on top. (like Ixquick do)\n- Categories like \"Images\" enable too many (all) engines by default. As result, searx isn't meeting expectations of new users by providing result from uncommon/unusual sources.\n. What about Zanata? And, btw, GitHub is also a proprietary service.\n. I believe this issue is not relevant anymore.. Please close the issue.. Any chance it will be merged any time soon?. It seems that Yandex doesn't work time to time only if you use one of the most popular instances like searx.me.. The engine has been updated a year ago, that's why it doesn't work anymore, I believe.\n\nI suspect Yandex have changed their API.\n\nSearx does not use their API, it parses HTML instead.\n. Yes, but it's not useful if you need to search on dozen websites, since you need to type something like site:example.com OR site:site:example2.com manually every time.. Yes, if it's possible to implement.. findx is already available: https://searx.me/?q=!fx%20findx. Gigablast.. Do you mean system requirements?. Duplicate of #1344. > if a remote engine like google supports advanced syntax, then searx supports it\n@asciimoo then it doesn't work for me as excepted:\nhttps://searx.me/?q=!sp+!go+tennis+AND+(football+OR+baseball)\nIn this example, the words OR and AND treated by searx as part of search query. Even if results are correct, searx should not highlight advanced syntax words in them.. Am I correct that archive.is and etymonline engines are already removed from master tree? If so, please close this issue.. There is subtitleseeker.gq, at least. But it seems like a clone of more popular subscene.com.. Why not use Bing engine instead?. Adding more collaborators to the project would be nice too.. > searx can also talk to yandex directly.\nThe yandex engine do not work: #938.. I've edited the wiki accordingly, thanks for raising this issue.. @micah why not use startpage engine instead?. > Is the project still alive?\nGood question, because I'm not aware about any alternatives.... I doesn't work for me with your instance (timeout), but it works with other gitea and gogs instances.. Don't mind if I push this fix for you? Then we can close the issue.. @Pofilo No. :-(. @Pofilo The changelog shouldn't be updated probably.. Thanks!. ",
    "julpec": "No, I don't need to change the default results language (location). I just want to change the default interface language (theme's language).\n. Yeah\u00a0!!! Excellent !\nThank you very much asciimoo !\n@Cqoicebordel : no, for me it didn't detect the browser default locale (iceweasel 31.3). I just tried witch chromium and you're right, the french local is detected\u2026\nWhatever, for interoperability, it's good to have a configurable option on the server\n. Hello,\nI had the same problem yesterday but not only with oscar and courgette but also with the default theme. So I just modified the proposition for the /etc/uwsgi/apps-available/searx.ini I found on https://github.com/asciimoo/searx/wiki/Installation.\nI don't undersand why it's not the same proposition than in https://about.okhin.fr/posts/Searx/\nSo, I add the last two lines and it was working well. Here's my searx.ini\n```\n[uwsgi]\nWho will run the code\nuid = searx\ngid = searx\nNumber of workers (usually CPU count)\nworkers = 4\nThe right granted on the created socket\nchmod-socket = 666\nPlugin to use and interpretor config\nsingle-interpreter = true\nmaster = true\nplugin = python\nModule to import\nmodule = searx.webapp\nVirtualenv and python path\nvirtualenv = /usr/local/searx/searx-ve/\npythonpath = /usr/local/searx/\nchdir = /usr/local/searx/searx/\n```\n. Still the same problem !\nBut I have test another param : yesterday, I have installed searx on another device and I didn't had the idea to test on it\u2026 it's working !\nSo, the problem come from the other one which is a ARM-base device (cubox-i4pro).\nI don't have the knowledge to test from which package the problem is coming.\n. Ok, nevermind\u2026 hopefully, I have to change this device so I think I will not have this problem in the next install.\nBy the way, I was a Jessie version of Debian\u2026\n. ",
    "linuxundich": "@Cqoicebordel \nYep, it answers queries and here's the  /etc/uwsgi/apps-available/searx.ini\n```\ncat /etc/uwsgi/apps-available/searx.ini\n[uwsgi]\nWho will run the code\nuid = searx\ngid = searx\nNumber of workers (usually CPU count)\nworkers = 2\nThe right granted on the created socket\nchmod-socket = 666\nPlugin to use and interpretor config\nsingle-interpreter = true\nmaster = true\nplugin = python\nModule to import\nmodule = searx.webapp\nVirtualenv and python path\nvirtualenv = /usr/local/searx/searx-ve/\n```\n@pointhi \nHere's a full log of a page load with the oscar theme. The webserver can load the files in /static/themes/oscar/ but not in /static/js/.\nxxx.xxx.xxx.xxx - - [17/Jan/2015:18:46:20 +0000] \"GET / HTTP/1.1\" 200 10862 \"http://searx.linuxandi.net/preferences\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\nxxx.xxx.xxx.xxx - - [17/Jan/2015:18:46:20 +0000] \"GET /static/css/bootstrap.min.css HTTP/1.1\" 404 305 \"http://searx.linuxandi.net/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\nxxx.xxx.xxx.xxx - - [17/Jan/2015:18:46:20 +0000] \"GET /static/themes/oscar/css/leaflet.min.css HTTP/1.1\" 304 188 \"http://searx.linuxandi.net/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\nxxx.xxx.xxx.xxx - - [17/Jan/2015:18:46:20 +0000] \"GET /static/themes/oscar/css/oscar.min.css HTTP/1.1\" 304 187 \"http://searx.linuxandi.net/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\nxxx.xxx.xxx.xxx - - [17/Jan/2015:18:46:20 +0000] \"GET /static/themes/oscar/img/searx_logo.png HTTP/1.1\" 304 189 \"http://searx.linuxandi.net/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\nxxx.xxx.xxx.xxx - - [17/Jan/2015:18:46:20 +0000] \"GET /static/js/jquery-1.11.1.min.js HTTP/1.1\" 404 305 \"http://searx.linuxandi.net/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\nxxx.xxx.xxx.xxx - - [17/Jan/2015:18:46:20 +0000] \"GET /static/js/bootstrap.min.js HTTP/1.1\" 404 305 \"http://searx.linuxandi.net/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\nxxx.xxx.xxx.xxx - - [17/Jan/2015:18:46:20 +0000] \"GET /static/js/require-2.1.15.min.js HTTP/1.1\" 404 305 \"http://searx.linuxandi.net/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\nxxx.xxx.xxx.xxx - - [17/Jan/2015:18:46:20 +0000] \"GET /static/js/typeahead.bundle.min.js HTTP/1.1\" 404 305 \"http://searx.linuxandi.net/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\nxxx.xxx.xxx.xxx - - [17/Jan/2015:18:46:20 +0000] \"GET /static/themes/oscar/js/searx.min.js HTTP/1.1\" 304 188 \"http://searx.linuxandi.net/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\n. @Cqoicebordel \nThanks, that worked. It think, you don't need that output of tree anymore. If not, here it is... http://pastebin.com/QcEi9jqQ\n. ",
    "brihx": "I have the same problem with my server : \nhttps://searx.brihx.fr/opensearch.xml\nIt's an Ubuntu 14.04.3 LTS\n. Thanks dalf your workaround solve my issue !\n. ",
    "bitwave": "+1 for this :D\n. +1 cool thx\n. ",
    "dannykorpan": "Hi,\nthank you for your quick answer!\nDisabling deflate module worked, but adding your code to my searx-apache.conf didnt worked.\n. I reenabled mod_deflate and added following to my apache.conf-file:\n<FilesMatch \\.xml$>\n  SetEnv no-gzip 1\n  </FilesMatch>\n. ",
    "aleksejrs": "\n\nMarketing searx is a difficult mission!\n\nIt appears that some even consider it as spam.\nThe-Compiler/configs@431e7a6#commitcomment-13987605\n\nThose seem to be somebody's personal config files that they shared for anyone interested in them, not a place to suggest your links for.. ",
    "JocelynDelalande": "\nselecting categories is different as in all the other search engines\n\nYes, to expand more on that,  (@pointhi, stop me if wrong) the fact you can select several categories at once by default is confusing, it's very-power-user to search images and files and web at once, so it's a lot of clicks to change which category you are searching in.\nThis one is annoying with all casual users I had to test my searx instance. As a matter of fact, the non-profit framasoft is hosting a big instance with customized-ui version of searx, they changed this behaviour: https://framabee.org/\n. ",
    "nRaecheR": "I've tried this \n```\n$ export HTTP_PROXY=\"http://myproxy:Port\"\n$ export HTTPS_PROXY=\"http://myproxy:Port\"\n$ python\n\n\n\nimport requests\nrequests.get(\"http://example.org\")\n```\n\n\n\nand it works but Searx don't use the proxy setting from global environment. Does it use python::requests at all?\n. Yes, it's with uswsgi and Apache as web server. \n. Is there anything new on this one? I had to shutdown my searx instance because it won't work anymore. . I had it running with 0.9.0 on a native installation, now I've migrated the whole server to docker usage and searx with 0.10.0 with the searx docker file. Unfortunately there is no way to customize the settings.yml in this scenario. . ",
    "hovancik": "Hi, so it this actively used?  https://www.transifex.com/asciimoo/searx/\nI have made Slovak translation but don't know how to proceed... \na bit of out topic: \nhow can I try it locally before contributing Slovak translation? I am on mac. \n. hi, I am trying to upload to tx and I am getting: \ntx set --auto-local -r searx.messagespo 'searx/translations/sk/LC_MESSAGES/messages.po' --source-lang en --type PO --source-file messages.pot --execute\n```\nUsage: tx [tx_options] set [options] [args]\ntx: error: The expression you have provided is not valid.\n``. Totally not sure whether this is related, but when I try:docker run -d --name searx -p 8888:8888 -e IMAGE_PROXY=True searxI am gettingdocker: Error response from daemon: oci runtime error: container_linux.go:247: starting container process caused \"exec: \\\"/usr/bin/tini\\\": stat /usr/bin/tini: no such file or directory\".`\nI am following https://github.com/asciimoo/searx/wiki/Installation on my macOS\n. ",
    "xavierle": "thank you for fast awser\nactually the group of users testing my install are focus on privacy but not that strong\nand they would expect answers to go quite fast like for example with duckduckgo\ni am checking if i find something with uWSGI / nginx\napache is usually not the way to make thing faster\nmy idea actually would be to cache any served pages for some hours/days : \nhttp://searx.instance/?q=keyword&pageno=1&category_general\nand to find a way that the cached page is served preferably\n. i made some test based on uWSGI cache but actually the normal key is by URI.\neach search creates a corresponding search URI in the links part :+1: \nhttp://searx.me/?q=cool&pageno=1&category_general\nbut then the searx appliance main URI remains the same :  \nhttp://searx.me/\nactually this makes a twist for me in the cache management\nIs it possible to that searx actualises the main URI corresponding to the actual search URI it writes in the link ?\nto show complete adress :+1:  \nhttp://searx.me/?q=cool&pageno=1&category_general\nso that we can cache this page more easily\n(i know this is not completely good as search engine preferences will be wrong, but maybe we can improve this later)\nmy long term idea is that common searched terms and corresponding search URI are prefeteched randomly by server (with no usage of specific cookie or user tracking)\n. ok this is working fine with 'get' for me \nand I prefer much better the behavior of searx that way\nis there a way to force the 'get' method ?\nis this something to be written in 'settings.yml' ?\nmerci !\n. actually i like this way for my own instance\ni do not intent to go that way for every one\nit seems  GET can be forced quite easily in the theme files\nand for my use searx is pretty better that way\nif i improve the caching i will post news there\n. i am reviewing a bit more about the code\nin  searx/searx/webapp.py\ndoes we see here the default values for search ?\nif request.method == 'GET':\nblocked_engines = get_blocked_engines(engines, request.cookies)\nelse: # on save\nselected_categories = []\nlocale = None\nautocomplete = ''\nmethod = 'POST'\nsafesearch = '1'\n. @dalf  yes it seems a kind of duplicate \nand yes\nI was looking for this part of the webapp.py file\nthank you for confirmation\ni will do the hack for now\nbonne journee\n. ok i close it\n. ",
    "andras-tim": "For note:\nI'm find another web based translator tool, the poeditor.com.\nThis is free for open source projects (https://poeditor.com/help/faq#What-are-Open-Source-projects)\nIn other hand, I'm full agree with @Cqoicebordel \n. ",
    "ldidry": "I don't think so (or at least, not now):\n- as I can see on your issue, plugins are not ready\n- right now, searx returns a false answer, which is really bad for the user and searx reputation\n. Don't you think it would be a good thing to merge this commit and revert it when the plugin system will be ready?\n. There is now a plugin for that, introduced in commit 00cc4dcbf44d9ecea89befb08cae4ee5561c4247\nI think this PR can be closed, no?\n. Yes, I know, but I like to get an history of my searches since I often re-search the same thing.\nBesides, it will stay on POST by default so it's up to the user to choose an non-private method.\nWhat do you prefer?\n- [ ] use the existing setting for method\n- [ ] add a new setting just for the browser's integration\n. Ok, I will propose a pull-request soon.\n. Yes, some resources could be used directly from oscar theme but for now, I prefer to manually make the diff to prevent incompatibilities. Once I'll have all the differences between oscar and frama in mind, I'll try to reduce the number of files of frama theme to use oscar resources.\n. Hello, I just reworked on that theme in order to use as much as oscar resources as possible. Could you reconsider merging it ?\nThank you.\n. I forgot: you can test this theme on https://framabee.org. The default theme is another one (which is really close), so you'll need to change your preferences to use the frama theme.\n. Thx!\n. I wait for the decision about merging the two plugins in one and will do it as soon as I have a \"go for it\"\n. Merged :smiley: \n. Changed.\n. ",
    "Lamessen": "plugin now exists for that\n. ",
    "davidhedlund": "Thank you.\nThis problem do not arise with Searx instances that are hosted as Tor Hidden Services.\n. I'm glad you guys took your time to explain this briefly.\n. ",
    "ilf": "Not every single one, but most. \"test\", for example.\nHow about just using GET instead of POST?\n. I would also find this incredibly useful. And I can report that friends that tried searx stopped using it, because results would only be in one language (mostly English).\nWhile on this, I would love for this to be passed via URL parameter like https://searx.me/?language=en,fr&q=test, so I don't have to set a cookie via preferences.. I second the request for an option to specify multiple languages. Something like https://searx.xyz/?q=test&language=de,en would be ideal.. @miicha Thanks, that's a great start!\nBut that one always searches in en_US plus the defined language in language=? That's okay for a first proof-of-concept, but really an ideal solution would allow specifying any two languages - and specify them in language= for links and browser-plugins etc.. ",
    "fr330n1y": "@pointhi\n\"image_proxy=1\" is not working. Searx isn't proxying images.\nIs this a bug?\n<a href=\"//c3.staticflickr.com/7/6142/5969514803_be9a3b42da_o.jpg\" data-toggle=\"modal\" data-target=\"#modal-4\">\n    <img src=\"https://c3.staticflickr.com/7/6142/5969514803_24976cc0e9_n.jpg\" alt=\"aaaaaaaaaaa 013\" title=\"aaaaaaaaaaa 013\" class=\"img-thumbnail\">\n</a>\nExpected:\n<img src=\"searx_image?x=afeiarfgb7er8ygfet7rgya3sgnte6g4e5tbg65egat5gh76tbf43gf6b\">\n. and in JSON: https://www.searx.me/?q=AAAAAAAAAAA!&locale=en&image_proxy=1&pageno=1&category_images=1&format=json\n\"img_src\": \"https://drscdn.500px.org/photo/68763637/w%3D280_h%3D280/bad92b5e878b1b8638073bc10b7804ad?v=0\",\n\"thumbnail_src\": \"https://drscdn.500px.org/photo/68763637/w%3D280_h%3D280/bad92b5e878b1b8638073bc10b7804ad?v=0\"\nExpected:\n\"img_src\": \"http://searx.test.server/searx_image?img=jgfhrega7yl37gbn37abgnaw7rfnawrfgab36fg3ag65\",\n\"thumbnail_src\": \"http://searx.test.server/searx_image?thumb=afhaw7y75ngy537fgyer7yfawlf74byt7fyr7afb5gal\"\n. > searx reads directly the value of the cookie\nHello @asciimoo, would you consider reading \"preferences\" from the URL?\n1. Searx read session cookie\n2. If not exist, Searx read URI (?language=it_IT&safesearch=0&image_proxy=1&q=AAAAAAA!)\nI'm happy with Searx's JSON API.\nSearx would become best & free & OPEN SOURCE alternative for \"Bing API(*1)\", \"Google Search API\", etc..\n*1 | http://searchengineland.com/bing-search-api-no-longer-free-118100\n. For autocomplete, how about adding SP?\nhttps://startpage.com/do/suggest?query=aaaa\n. ## Related: http://stats.searx.oe5tpo.com/\n...or, set up \"redirection server\" to rediret request to random-picked server.\nUser: https://random.searx.me/?format=rss&locale=it_IT&q=AAAAAAAAAA\nrandom.searx.me: Pick one from stats.searx.oe5tpo.com...\nrandom.searx.me: I pick https://searx.volcanis.me\nrandom.searx.me: Hey user, go there!\nrandom.searx.me: \"Location: https://searx.volcanis.me/?format=rss&locale=it_IT&q=AAAAAAAAAA\"\nUser: HTTP 302 Redirect https://searx.volcanis.me/?format=rss&locale=it_IT&q=AAAAAAAAAA\nsearx.volcanis.me: Welcome! Here's your result!\n. > search engines use the ip (of the searx instance) to \"adjust\" the result\nYeah, that's one of the reason why I hate google...\nhttp://dontbubble.us/\nhttps://searx.me/?q=guns (<--- where is NRA?)\n\nnot all searx instances are updated\n\nThen these old sites must be removed from redirection target.\nMaybe adding \"searx-auto-update\" and run it using cron-daily is a good start.\nsearx-auto-update\n1. Download raw ZIP(or released ZIP) from https://github.com/asciimoo/searx/\n2. If size(downloaded) != size(currently_installed) && unzip(downloaded)===true, unpack it and overwrite current folder.\n\nsome times, searx are customized\n\nI just want to receive JSON result from random servers.\n?format=json&engines=common&...&q=AAAAAA\n^ \"common\" engine which volunteer searx owner can't|souldn't edit.\n. ",
    "gggirlgeek": "So is there some documentation or can you tell us here what the search parameters are please? Are they basically the same as Google's?\nFrom what I've tested \"site:\" works lite Google. But \"time_range=\" does not work with, for example, \"year\" or \"10years\". The \"-\" exclude parameter doesn't seem to work but, strangely, it changes the results to contain less of the word specified.\nQuotes around a phrase does not work as expected either. I do not get results for the exact phrase.. +1. Yes please!\nI am surprised that there is not \"past year\". It is my default search parameter because older information is just too out of date. 24 months would be nice too. Most tech info is still relevant within 2 years.\nMeanwhile:  is there a workaround? I tried adding \"time_range=year\" but the page just reverted back to \"Anytime.\"  Also is there a way to specify a number of months, or even a specific date range?. ",
    "twelph": "That's a really nice switch! My vote is for that.\n. ",
    "BurungHantu1605": "You have my vote, too. Btw, iOS / Yosemite is also using a switch and they spend a lot of money and thoughts in usability.\n. @potato \"besides I think the current button behaviour is logical too\"\nYes, nobody said it's not logical. But the main rule of usability is \"don't make me think\" everything should be intuitive. The button represents the current state, but the label says the exact opposit.\n. @Trim: Are you planning to still use the switch and the checkboxes for noscript users? That would be a good compromise if you ask me. Btw, I just saw that Google is also using radio buttons / checkboxes and it works fine.\n. It took some time before it disappeared. Caching?\n. Is this how it works?\nsettings.yml\n    name : soundcloud\n    engine : soundcloud\n    shortcut : sc\n    private : True\n-> reload server\n. It works again, maybe it was just a temp. problem. Thanks\n. I can help with that. I'm a big affiliate marketer. A website of mine is currently participating in over 3000 affiliate programs.\n. @Cqoicebordel Sorry, I'm late. Just found this:\nhttps://s3.amazonaws.com/lists.disconnect.me/simple_ad.txt\nThis is used by the plugin \"Disconnect\" and should cover most of ads / affiliate links.\n. Check out the source code on www.privacytools.io - I've included it there. Maybe that helps?\n. @HLFH: Thanks for your great feedback and sorry that I got back to you so late. I'll discuss all your ideas with my team now. Please message me on reddit if you want to help setting up a github repo: https://www.reddit.com/message/compose/?to=BurungHantu thanks!\n. Hi guys, sorry for the late reply. I'm no server admin, I can't do all the things you recommended.\nAbout the traffic: around 1,500 unique users daily. some blocks are still enabled, for example youtube:\nhttps://www.privatesearch.io/?q=youtube&pageno=1&category_videos\nIt's just showing dailymotion results. Also DuckDuckGo results disappeared again.\n\"EDIT : Oh... Maybe it's your host that blocked your requests, because it thought that you were sending spams/DoS...\"\nThat was a good theory, but since some search engines are working again it's not valid anymore.\n\"Try to start searx with debug: True in settings.yml and see the logs.\"\nI managed to enable it, but I didn't find the log file. Can you tell me where it is? Thanks.\nHow are searches performed in general for Google for example? Are we all using an API, or are limited in any way by the amount of searches?\nThank you all for your help.\n. ",
    "potato": "+1 to 'main functionality of searx should always work without javascript'\nbesides I think the current button behaviour is logical too: the color of the button represents the current state of the given setting, the text on the button represents the action that will happen when the user clicks it.\nthe color is just a visual aid, for the red-green color blind the message is still clear imho\n\"when you click me, I will allow you anything. or will I?\" (Engine Button The First, 2013)\n. the default_theme in the ui section does exactly this afaik\n. ",
    "anton-dutov": "You're right it's way is more clean.\n. ",
    "pschwede": "I just was looking for something similar. So now I know it exists, thank you.\n. ",
    "g4jc": "Just tried per instructions, including make clean and make... still getting this. :(\n\nException happened during processing of request from ('127.0.0.1', 57830)\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/SocketServer.py\", line 295, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 321, in process_request\n    self.finish_request(request, client_address)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 334, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 651, in init\n    self.finish()\n  File \"/usr/lib/python2.7/SocketServer.py\", line 710, in finish\n    self.wfile.close()\n  File \"/usr/lib/python2.7/socket.py\", line 279, in close\n    self.flush()\n  File \"/usr/lib/python2.7/socket.py\", line 303, in flush\n    self._sock.sendall(view[write_offset:write_offset+buffer_size])\nerror: [Errno 32] Broken pipe\n\nException happened during processing of request from ('127.0.0.1', 58543)\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/SocketServer.py\", line 295, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 321, in process_request\n    self.finish_request(request, client_address)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 334, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 651, in init\n    self.finish()\n  File \"/usr/lib/python2.7/SocketServer.py\", line 710, in finish\n    self.wfile.close()\n  File \"/usr/lib/python2.7/socket.py\", line 279, in close\n    self.flush()\n  File \"/usr/lib/python2.7/socket.py\", line 303, in flush\n    self._sock.sendall(view[write_offset:write_offset+buffer_size])\nerror: [Errno 32] Broken pipe\n. I have been unable to find a solution to this so far. I have no IPTable rules setup. \nHere is my IPTables:\nroot@host:~# iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination         \nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination \nAnd here is the bug from latest git (compiled using 'make minimal' less than an hour ago):\nException happened during processing of request from ('127.0.0.1', 60115)\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/SocketServer.py\", line 295, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 321, in process_request\n    self.finish_request(request, client_address)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 334, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 651, in init\n    self.finish()\n  File \"/usr/lib/python2.7/SocketServer.py\", line 710, in finish\n    self.wfile.close()\n  File \"/usr/lib/python2.7/socket.py\", line 279, in close\n    self.flush()\n  File \"/usr/lib/python2.7/socket.py\", line 303, in flush\n    self._sock.sendall(view[write_offset:write_offset+buffer_size])\nerror: [Errno 32] Broken pipe\n. @dalf, I will try your suggestion as soon as I get a chance and follow with an update once I disable autocomplete.\nThe original problem I am experiencing however, is that any search query I make results in \"Connection timeout\" in Firefox. I have also tried other browsers and had someone else attempt to search from my node which also failed.\nDuring each failed attempt to search - the broken pipe error appears in logs, hence I think it is related.\n. I finally was able to determine this problem. I was actually running the service directly, and I realized that it worked on localhost, but not remotely after your curl suggestion. I seem to have discovered the reason. \nWhen your internet connection is very slow, unreliable, or proxied (e.g. any clients using privoxy + TOR), it results in a broken pipe.\nRunning nginx as reverse proxy and allowing a longer timeout actually seems to have partially-resolved this issue for me and normal use.\nFurther testing revealed that I can reproduce this bug on your main (searx.me) TOR instance nearly everytime however, (due to the added latency):\n1) Install Privoxy --> uncomment \" forward-socks5   /               127.0.0.1:9050 .\" in /etc/privoxy/config --> Start service\n2) Set your browser to use HTTP proxy 127.0.0.1:8118\n3) Make sure TOR is running\n4) Navigate to any \"Hidden Service\" Searx instance. (https://github.com/asciimoo/searx/wiki/Searx-instances)\n5) Try running a search\n6) Client will show timeout error\n7) Server will get bad pipe error.\nI am unsure if this is a bug in privoxy, or searx, or both. Please feel free to see if you can also reproduce with this setup. I will reach out to privoxy as well.\nThanks for your trouble shooting assistance!\n. I can confirm removing privoxy was the issue, and after a lot of debugging upstream (privoxy bug #1686) this has fixed it. :+1: \nApologies for confusing you all with this bug report, and thanks for pointing me in the right direction. :)\n. ",
    "amirouche": "The search form with JavaScript disabled use the html checkbox element to make a search within multiple category. A feature that is not available with Javascript enabled. \nThis \"multiple category search\" can not be ported in my plan using <input type=\"submit\" name=\"search_{{ category_name }}\"> (mind the name attribute) with javascript disabled. The name attribute is later used to dispatch the form action. This used in django admin to have several action in \u00abadd a {{ model }}\u00bb form.\nI think that keeping tab/window navigation within searx search working is a tremendous asset for searx. I use heavly tree style tab firefox extension. It makes my life much easier, keeping a visual aid of navigation history. (It's like before and after tabs open relative ff extension, which is now included in ff).\n. ",
    "Alventoor": "Ok and thank you for the fix on Transifex.\n. ",
    "nytche": "tree: command not found\npwd: /usr/local/searx\n. ",
    "NIXOYE": "@privacytoolsIO can you explain or share the code you used to embed the search their? I would like to make on with my searx instance. Also it seems https://www.privatesearch.io/ is down.\n. there should be a button to do easily change default search engine to searx both on desktop and mobile\n.  \"If you want to do so, you have to clone this repository, modify your files, and then build yourself from the embedded Dockerfile.\"\ncan you explain how to do this in laymen terms?\nI installed searx with docker with the instructions on https://github.com/asciimoo/searx/wiki/Installation \n\ndocker pull wonderfall/searx\ndocker run -d --name searx -p $PORT:8888 wonderfall/searx\n. does it matter if ive already done \ndocker pull wonderfall/searx\ndocker run -d --name searx -p $PORT:8888 wonderfall/searx\n\nalso, after i edit the html\\css and run my image im probably going to find more things i want to edit\nhow can i make updates to it with starting all over every time? \n. ok so i \n\ngit clone https://github.com/asciimoo/searx.git && cd searx\nThen, edit html\\css files in /searx/... and once you're done :\ndocker build -t whatever/searx .\nVoil\u00e0, you'll get your own Docker image of searx. Run a new container with :\ndocker run -d --name searx -p $PORT:8888 whatever/searx\n\nto update i have to\n\ndocker build -t whatever/searx .\nHere we build a new image, the old one is not referenced as whatever/searx anymore.\n\n(does ^ keep my edited html\\css?)\n\ndocker stop searx && docker rm searx\nWe remove the running container.\n\ndont i have to edit the html\\css again here and then run\n\ndocker run -d --name searx -p $PORT:8888 wonderfall/searx\nWe run a new container from the new builded image.\n. If anyone needs help with this here's what you do.\n\nfind html/css files which I believe is in searx/searx/static/ and edit the html/css.\nbuild the container with \ndocker build -t nix/searx .\nthen stop the container and remove it with \ndocker stop searx && docker rm searx\nand then loaded it back up with \ndocker run -d --name searx -p 80:8888 nix/searx\nThat's it!\n(Replace \"nix\" in the first and last command with whatever you want to name your container and the 80 in the last command if you want to use a different port.)\nThis issue can be closed.\n. @Wonderfall any guides on setting up searx as a hidden service?\n. @PwnArt1st its just being ignored. I also want to remove deviantart as a default engine as its results are pretty awful. I added the disabled : True to it also but still no luck. Am I supposed to do something else?\n. @Wonderfall your right, testing again.\n I edited my first post so i could copy and paste it to try it\nnow when i do docker start 633f70901ae25309628ca451b85e530ffe7ec8ae21c430429e2fdc234f8f3e20\ni get 633f70901ae25309628ca451b85e530ffe7ec8ae21c430429e2fdc234f8f3e20in the next line but it doesnt start\nif i do docker run whatever/searx I get\n```\nsed: searx.setting.yml: No such file or directory\nTraceback (most recent call last):\n  File \"searx/webapp.py\", line 826, in \n    run()\n  File \"searx/webapp.py\", line 781, in run\n    host=settings['server']['bind_address']\n  File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 772, in run\n    run_simple(host, port, self, *options)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 694, in run_simple\n    inner()\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 656, in inner\n    fd=fd)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 550, in make_server\n    passthrough_errors, ssl_context, fd=fd)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 464, in init\n    HTTPServer.init(self, (host, int(port)), handler)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 420, in init\n    self.server_bind()\n  File \"/usr/lib/python2.7/BaseHTTPServer.py\", line 108, in server_bind\n    SocketServer.TCPServer.server_bind(self)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 434, in server_bind\n    self.socket.bind(self.server_address)\n  File \"/usr/lib/python2.7/socket.py\", line 228, in meth\n    return getattr(self._sock,name)(args)\nsocket.error: [Errno 13] Permission denied\n```\n. @dalf hmm seems like changing the name from whatever/searx to tstnix/searx works. or anything other than whatever/searx works. Thanks for the help.\nthis can be marked as closed @Wonderfall \n. any idea what it is @asciimoo?\n. @asciimoo it was fixed after updating to 0.9 thanks a ton for the update!\non another note, how can i change the default search engines used for the music category or the videos category? I want to remove btdigg from being used in the videos and music section but not for files\n. @PwnArt1st ive never used it but it also looks nice.\n. @Wonderfall i ran \ndocker build -t nix/searx .\nthen did \ndocker stop searx && docker rm searx\nand then try \ndocker run -d --name searx -p 80:8888 nix/searx\nbut it doesnt run. If i check docker ps -a it says CREATED 14 seconds ago Exited 11 seconds ago\n. > sed: searx.setting.yml: No such file or directory\n\nTraceback (most recent call last):\n  File \"searx/webapp.py\", line 826, in \n    run()\n  File \"searx/webapp.py\", line 781, in run\n    host=settings['server']['bind_address']\n  File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 772, in run\n    run_simple(host, port, self, _options)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 694, in run_simple\n    inner()\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 656, in inner\n    fd=fd)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 550, in make_server\n    passthrough_errors, ssl_context, fd=fd)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 464, in init\n    HTTPServer.init(self, (host, int(port)), handler)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 417, in init\n    self.server_bind()\n  File \"/usr/lib/python2.7/BaseHTTPServer.py\", line 108, in server_bind\n    SocketServer.TCPServer.server_bind(self)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 431, in server_bind\n    self.socket.bind(self.server_address)\n  File \"/usr/lib/python2.7/socket.py\", line 228, in meth\n    return getattr(self._sock,name)(_args)\nsocket.error: [Errno 13] Permission denied\nsed: searx.setting.yml: No such file or directory\nTraceback (most recent call last):\n  File \"searx/webapp.py\", line 826, in \n    run()\n  File \"searx/webapp.py\", line 781, in run\n    host=settings['server']['bind_address']\n  File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 772, in run\n    run_simple(host, port, self, _options)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 694, in run_simple\n    inner()\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 656, in inner\n    fd=fd)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 550, in make_server\n    passthrough_errors, ssl_context, fd=fd)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 464, in init\n    HTTPServer.init(self, (host, int(port)), handler)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 417, in init\n    self.server_bind()\n  File \"/usr/lib/python2.7/BaseHTTPServer.py\", line 108, in server_bind\n    SocketServer.TCPServer.server_bind(self)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 431, in server_bind\n    self.socket.bind(self.server_address)\n  File \"/usr/lib/python2.7/socket.py\", line 228, in meth\n    return getattr(self._sock,name)(_args)\nsocket.error: [Errno 13] Permission denied\nsed: searx.setting.yml: No such file or directory\nTraceback (most recent call last):\n  File \"searx/webapp.py\", line 826, in \n    run()\n  File \"searx/webapp.py\", line 781, in run\n    host=settings['server']['bind_address']\n  File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 772, in run\n    run_simple(host, port, self, *_options)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 694, in run_simple\n    inner()\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 656, in inner\n    fd=fd)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 550, in make_server\n    passthrough_errors, ssl_context, fd=fd)\n  File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 464, in init\n    HTTPServer.init(self, (host, int(port)), handler)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 417, in init\n    self.server_bind()\n  File \"/usr/lib/python2.7/BaseHTTPServer.py\", line 108, in server_bind\n    SocketServer.TCPServer.server_bind(self)\n  File \"/usr/lib/python2.7/SocketServer.py\", line 431, in server_bind\n    self.socket.bind(self.server_address)\n  File \"/usr/lib/python2.7/socket.py\", line 228, in meth\n    return getattr(self._sock,name)(_args)\nsocket.error: [Errno 13] Permission denied\n. so apparently my problem was that i was trying to run it while root user. I got my 0.9 version going again but i still cant update to 0.10\n\n@Wonderfall could you ELI5 how to rebuild the image and the container?\n. @fmbento I tried this and got to get rid of a bunch of docker images that were taking up space (thanks) but it still didn't work. I ended up deleting everything in the searx folder with rm -f searx and then re installed searx with the install instructions at https://github.com/asciimoo/searx/wiki/Installation\n. @risoul i agree\nthe image search part should also allow for a next and previous button so you dont have to close the image and then click the next one each time. Allowing keyboard arrows to work like this would be nice as well\n. @Wonderfall any update on this? . @asciimoo how do we make it by default use GET so users coming to our site on chrome can browse without a problem?. any update on the dockerfile?. thanks @kvch trying to figure out how to have the advanced settings menus appear by default so its obvious you can also search for files\\images on my instance. . thanks so much @kvch its so much smoother now :). @kvch is it possible to edit this so it happens on the homepage as well? 0.12.0 seems to have this by default after searching but not on the homepage. I tried changing this but it wouldn't update the website\n\nOn Aug 26, 2017, at 7:43 AM, No\u00e9mi V\u00e1nyi notifications@github.com wrote:\nLinks of the navbar are in searx/templates/oscar/navbar.html.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "leepfrog-ger": "This is currently happening for me too.\nEngines that are disabled in settings.yml are showing up as enabled (and giving results) even when using an incognito session and/or clearing all cookies.\n. Thanks, will test this later!\n. ",
    "vindarel": "Hi devs,\nI tried a bit here: https://github.com/vindarel/searx/commit/0bcecd72a6181d72e1afd2c4cb87ea6e96945f91\nHowever it's not working yet and you may help me with a quick review ! (you can try on my server).\nThe thing is, I don't quite understand the type and use of self.query_parts (looks like we don't put all keywords in it, in the following class methods, why is it always accessed with self.query_parts[-1], why the -1 ?), and maybe I broke self.engines ? Since I don't see much docstring or unit test on parse_query, I thought I'd rather ask for a quick review.\nAny thoughts then ? Thanks !\n. Hi,\nyou can also link to the #searx tag:\n- https://pod.geraspora.de/tags/searx\n- https://framasphere.org/tags/searx\n. Looks like a duplicate of https://github.com/asciimoo/searx/issues/363 (+1!)\n. ",
    "qazip": "Is there any news regarding this? I also agree with the OP that one should be able to use \"!engine\" anywhere in search string... Ah, indeed. I had tried that before, but since I was expecting to be re-directed to youtube itself, I didn't notice all the results were from youtube.\nIt would be cool if there was an option to be redirected to the site itself, perhaps \"!!yt\".. \n. ",
    "autrui": "Unfortunately, I can't. My web hosting company told me that I couldn't make searx work in a subfolder because some of the config changes required clash with the way their servers are set up. Probably because there are multiple users on one server. \nSorry. Hope someone else can verify this. Thank you for your help!!!\n. ",
    "kallaballa": "nevermind. my mistake. the guide works for me\n. ",
    "Santobert": "Is there a way to store this setting in the settings.yml file?\nThank you\n. ",
    "gsantner": "@Eldouf \nsearx/settings.yml\nsearch:\n    safe_search : 0 # Filter results. 0: None, 1: Moderate, 2: Strict\n    autocomplete : \"\" # Existing autocomplete backends: \"dbpedia\", \"duckduckgo\", \"google\", \"startpage\", \"wikipedia\" - leave blank to turn it off by default\n    language : \"fr\"\nor by using the GET variant:\nhttps://DOMAIN/searx/?q=QUERY&category_general=on&time_range=&language=LANG. ",
    "upskaling": "if you put the french language research, the bug occurs.\nwhereas if you leave it in automatic its work, anyway.\n. it misses custom-select. ",
    "rrooij": "@asciimoo Cool!\nI'm also not sure about the second one. It'd be of help if someone from GNU could help. I interpreted this way because of section 6, however, I'm no expert on this matter.\n. ",
    "vyp": "IANAL so I could very well be wrong, but it's my understanding that free software in general has to distribute the source code in form the author originally writes it in. Meaning minified code does not fulfill that (even if it can be unminified).\nBut looking at the JS scripts, all the minified ones seem to be third party libraries (and all the ones that are searx specific seem to be unminified). If this is correct and the minified third party scripts are unmodified from the original ones, then I think it should be okay if there are just links to the original source code libraries themselves. But I'm not even sure if that is needed by the license, just it would probably clarify things/solve this issue.\nThat can still be a pita though, so maybe it's worth looking into something like bower if that really is an issue for the license.\n. ",
    "schwukas": "I'll give this one a bump. I think this would be a nice extra especially for users switching from popular search engines to searx. Is it only the review that is missing or does it actually need extra code? I would certainly try to help, but I'm not very proficient when it comes to threads in python,. ",
    "jpope777": "I'd be happy to provide a diaspora* account on my pod for an official Searx account. I do have twitter enabled on it so, you can easily cross post over. Post once and hit both places.\n\"The Federation\" (which includes D*, Friendica and Redmatrix) is full of privacy minded people and projects like Searx are right up our alley. :)\n. As a Seeks->Searx convert, are we sure there are enough Seeks nodes still running these days for this to be applicable? \nEven before my node finally died, I noticed that 99% of the nodes that I was peered with were no longer in existence...\n. +1\n. I can totally submit a PR for the theme. :)\nShould note, that theme is also included in Mediagoblin as well (see https://media.jpope.org/ for example).\n. @GreenLunar The comments are embedded into D* via the opengraph metadata.\n. ",
    "rascul": "I've been seeing this also, a page reload seems to \"fix\" it.\n. ",
    "gugod": "@dalf Thanks for the pointer.\nI modified the tests and made them pass. I only ran make tests.\n. ",
    "magaretha42": "That might be a workable idea, but I would rather it be on a per user basis.\nI was planning on coding to interface with other servers that have internal search with API access so that I can use the Access Control functionality in those applications to manage user data access (all handled on their end, I just need to send them the user/password when I run a request, .\nCan plugins modify the HTTP Request (specifically to send data)?  Perhaps when run on pre_search?\n. ",
    "1xPdd": "+1\nI've always wanted to blacklist sites from search results.\n. +1\nWould also be cool if one could set date ranges, e.g., 1998-2000, rather than simply newness, e.g., 2015-present.\n. The UI looks great! But, if I could, I'd really like to suggest (again) that dates should not be only measured as relative distance from the present as in last day/week/month. Library search engines and the likes allow dates to be limited to an arbitrary date range, as in from year to year. No general search engine that I know of does this. The advantage of a more flexible date adjustment is that it enables Searx to function as a tool for historical research. As to UI, it could be done via a slider or using a form.\n. Excellent. That's just what I was looking for. I'll give it a read and see if I come up with anything. Thanks.\n. ",
    "beniz": "oops, wrong rebase, will PR it again.\n. Oops, forgot to mention one of the issues: the results are stored into results.db which is written from where the webapp.py is started. Obviously this cannot left as is in case of a merge. I've notice that the setup.py arranges for a .searx file or repository but I haven't tested this type of installation yet.\nGiven proper instructions on a good location for results.db, I am willing to make the required changes.\n. Weird, make robot passes just fine for me with the fix, difficult to tell what's up with Travis right here...\nEDIT: started from scratch with same cloning as Travis, and make robot, so... .\n. open it up,  and I ll contribute happily! \nOn October 17, 2015 3:38:01 PM GMT+02:00, GreenLunar notifications@github.com wrote:\n\nRelated to #243\nI suggest to open a forum with help of Seeks and YaCy fans!\nThere will be a sub-forum for searx, Seeks, YaCy and more.  Such forum\nwould also be a central to propose new ideas and even promote new\nmeta-search projects!\nPing @beniz @yacy\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/asciimoo/searx/issues/448\n. \n",
    "icaroperseo": "Thanks for responding!\nOf the aforementioned list, in order of appearance, the following links I produce the same error: 1, 3, 6, 8, 9. (I tried only numbered from 1 to 10)\nThe following works:\nhttps://seeks.okhin.fr/\nhttps://searx.laquadrature.net/\nIt seems that the problem is related to the version of the search engine or so it seemed to me.\n. @asciimoo Spanish\n. It's works!!!\nYou rox guys!\n. ",
    "austin1029": "Fri Nov  4 19:38:28 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Fri Nov  4 19:38:28 2016] ***\nFri Nov  4 19:38:28 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nFri Nov  4 19:38:28 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nFri Nov  4 19:38:28 2016 - nodename: qwhyitserve\nFri Nov  4 19:38:28 2016 - machine: x86_64\nFri Nov  4 19:38:28 2016 - clock source: unix\nFri Nov  4 19:38:28 2016 - pcre jit disabled\nFri Nov  4 19:38:28 2016 - detected number of CPU cores: 8\nFri Nov  4 19:38:28 2016 - current working directory: /\nFri Nov  4 19:38:28 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nFri Nov  4 19:38:28 2016 - detected binary path: /usr/bin/uwsgi-core\nFri Nov  4 19:38:28 2016 - setgid() to 1008\nFri Nov  4 19:38:28 2016 - setuid() to 1008\nFri Nov  4 19:38:28 2016 - your processes number limit is 31688\nFri Nov  4 19:38:28 2016 - your memory page size is 4096 bytes\nFri Nov  4 19:38:28 2016 - detected max file descriptor number: 1024\nFri Nov  4 19:38:28 2016 - lock engine: pthread robust mutexes\nFri Nov  4 19:38:28 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nFri Nov  4 19:38:28 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nFri Nov  4 19:38:28 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nFri Nov  4 19:38:28 2016 - Set PythonHome to /opt/yunohost/searx/\nFri Nov  4 19:38:28 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nFri Nov  4 19:38:28 2016 - Python main interpreter initialized at 0x25488b0\nFri Nov  4 19:38:28 2016 - your server socket listen backlog is limited to 100 connections\nFri Nov  4 19:38:28 2016 - your mercy for graceful operations on workers is 60 seconds\nFri Nov  4 19:38:28 2016 - mapped 363840 bytes (355 KB) for 4 cores\nFri Nov  4 19:38:28 2016 - *** Operational MODE: preforking ***\nFri Nov  4 19:38:28 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nFri Nov  4 19:38:30 2016 - WSGI app 0 (mountpoint='') ready in 2 seconds on interpreter 0x25488b0 pid: 7860 (default app)\nFri Nov  4 19:38:30 2016 - spawned uWSGI master process (pid: 7860)\nFri Nov  4 19:38:30 2016 - spawned uWSGI worker 1 (pid: 7896, cores: 1)\nFri Nov  4 19:38:30 2016 - spawned uWSGI worker 2 (pid: 7897, cores: 1)\nFri Nov  4 19:38:30 2016 - spawned uWSGI worker 3 (pid: 7898, cores: 1)\nFri Nov  4 19:38:30 2016 - spawned uWSGI worker 4 (pid: 7899, cores: 1)\n[pid: 7897|app: 0|req: 1/1] 127.0.0.1 () {44 vars in 735 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/ => generated 10965 bytes in 126 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 7897|app: 0|req: 2/2] 127.0.0.1 () {42 vars in 711 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 5 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 7897|app: 0|req: 3/3] 127.0.0.1 () {42 vars in 678 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 95786 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 7898|app: 0|req: 1/4] 127.0.0.1 () {42 vars in 689 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 9 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 7896|app: 0|req: 1/5] 127.0.0.1 () {42 vars in 715 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 9 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 7897|app: 0|req: 4/6] 127.0.0.1 () {42 vars in 688 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 5193 bytes in 1 msecs via sendfile() (HTTP/1.1 200) 7 headers in 285 bytes (0 switches on core 0)\n[pid: 7898|app: 0|req: 2/7] 127.0.0.1 () {42 vars in 670 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/js/bootstrap.min.js => generated 31819 bytes in 1 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 7897|app: 0|req: 5/8] 127.0.0.1 () {42 vars in 710 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 659 bytes in 1 msecs via sendfile() (HTTP/1.1 200) 7 headers in 281 bytes (0 switches on core 0)\n[pid: 7898|app: 0|req: 3/9] 127.0.0.1 () {42 vars in 728 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 1 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 7899|app: 0|req: 1/10] 127.0.0.1 () {42 vars in 680 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/js/require-2.1.15.min.js => generated 15219 bytes in 10 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 7899|app: 0|req: 2/11] 127.0.0.1 () {42 vars in 748 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/themes/oscar/img/favicon.png => generated 2060 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 272 bytes (0 switches on core 0)\n[pid: 7899|app: 0|req: 3/12] 127.0.0.1 () {42 vars in 768 bytes} [Fri Nov  4 19:40:53 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 7899|app: 0|req: 4/13] 127.0.0.1 () {46 vars in 793 bytes} [Fri Nov  4 19:40:56 2016] POST /searx/ => generated 78565 bytes in 1254 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 7899|app: 0|req: 5/14] 127.0.0.1 () {42 vars in 734 bytes} [Fri Nov  4 19:40:57 2016] GET /searx/static/themes/oscar/img/icons/wikipedia.png => generated 3960 bytes in 1 msecs via sendfile() (HTTP/1.1 200) 7 headers in 272 bytes (0 switches on core 0)\n[pid: 7897|app: 0|req: 6/15] 127.0.0.1 () {52 vars in 1057 bytes} [Fri Nov  4 19:53:24 2016] GET /searx/ => generated 10965 bytes in 9 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 7898|app: 0|req: 4/16] 127.0.0.1 () {52 vars in 1057 bytes} [Fri Nov  4 20:03:23 2016] GET /searx/ => generated 10965 bytes in 122 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 7899|app: 0|req: 6/17] 127.0.0.1 () {52 vars in 1057 bytes} [Fri Nov  4 20:52:36 2016] GET /searx/ => generated 10965 bytes in 18 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 7897|app: 0|req: 7/18] 127.0.0.1 () {56 vars in 1172 bytes} [Fri Nov  4 20:52:38 2016] POST /searx/ => generated 65744 bytes in 1264 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 7897|app: 0|req: 8/19] 127.0.0.1 () {56 vars in 1172 bytes} [Fri Nov  4 20:52:49 2016] POST /searx/ => generated 52010 bytes in 976 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 7897|app: 0|req: 9/20] 127.0.0.1 () {52 vars in 1057 bytes} [Fri Nov  4 20:53:09 2016] GET /searx/ => generated 10965 bytes in 10 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nERROR:searx.search:engine crash: bing\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 582, in send\n    r = dispatch_hook('response', hooks, r, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/hooks.py\", line 31, in dispatch_hook\n    _hook_data = hook(hook_data, **kwargs)\n  File \"/opt/yunohost/searx/searx/search.py\", line 117, in process_callback\n    search_results = callback(response)\n  File \"/opt/yunohost/searx/searx/engines/bing.py\", line 80, in response\n    content = escape(extract_text(result.xpath('.//p')))\n  File \"/opt/yunohost/searx/searx/engines/xpath.py\", line 35, in extract_text\n    raise Exception('Empty url resultset')\nException: Empty url resultset\n[pid: 7897|app: 0|req: 10/21] 127.0.0.1 () {56 vars in 1172 bytes} [Fri Nov  4 20:53:33 2016] POST /searx/ => generated 59130 bytes in 971 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 7898|app: 0|req: 5/22] 127.0.0.1 () {52 vars in 1094 bytes} [Fri Nov  4 21:03:59 2016] GET /searx/ => generated 10965 bytes in 7 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 7898|app: 0|req: 6/23] 127.0.0.1 () {52 vars in 1094 bytes} [Fri Nov  4 21:05:22 2016] GET /searx/ => generated 10965 bytes in 8 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nFri Nov  4 21:07:59 2016 - SIGINT/SIGQUIT received...killing workers...\nFri Nov  4 21:08:00 2016 - worker 1 buried after 1 seconds\nFri Nov  4 21:08:00 2016 - worker 2 buried after 1 seconds\nFri Nov  4 21:08:00 2016 - worker 3 buried after 1 seconds\nFri Nov  4 21:08:00 2016 - worker 4 buried after 1 seconds\nFri Nov  4 21:08:00 2016 - goodbye to uWSGI.\nFri Nov  4 21:08:24 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Fri Nov  4 21:08:24 2016] ***\nFri Nov  4 21:08:24 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nFri Nov  4 21:08:24 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nFri Nov  4 21:08:24 2016 - nodename: qwhyitserve\nFri Nov  4 21:08:24 2016 - machine: x86_64\nFri Nov  4 21:08:24 2016 - clock source: unix\nFri Nov  4 21:08:24 2016 - pcre jit disabled\nFri Nov  4 21:08:24 2016 - detected number of CPU cores: 8\nFri Nov  4 21:08:24 2016 - current working directory: /\nFri Nov  4 21:08:24 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nFri Nov  4 21:08:24 2016 - detected binary path: /usr/bin/uwsgi-core\nFri Nov  4 21:08:24 2016 - setgid() to 1008\nFri Nov  4 21:08:24 2016 - setuid() to 1008\nFri Nov  4 21:08:24 2016 - your processes number limit is 31688\nFri Nov  4 21:08:24 2016 - your memory page size is 4096 bytes\nFri Nov  4 21:08:24 2016 - detected max file descriptor number: 1024\nFri Nov  4 21:08:24 2016 - lock engine: pthread robust mutexes\nFri Nov  4 21:08:24 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nFri Nov  4 21:08:24 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nFri Nov  4 21:08:24 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nFri Nov  4 21:08:24 2016 - Set PythonHome to /opt/yunohost/searx/\nFri Nov  4 21:08:24 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nFri Nov  4 21:08:24 2016 - Python main interpreter initialized at 0x26478b0\nFri Nov  4 21:08:24 2016 - your server socket listen backlog is limited to 100 connections\nFri Nov  4 21:08:24 2016 - your mercy for graceful operations on workers is 60 seconds\nFri Nov  4 21:08:24 2016 - mapped 363840 bytes (355 KB) for 4 cores\nFri Nov  4 21:08:24 2016 - *** Operational MODE: preforking ***\nFri Nov  4 21:08:24 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/webapp.py\", line 54, in <module>\n    from searx.engines import (\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 201, in <module>\n    engine = load_engine(engine_data)\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 60, in load_engine\n    engine = load_module(engine_name + '.py')\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 53, in load_module\n    module = load_source(modname, filepath)\n  File \"/opt/yunohost/searx/searx/engines/soundcloud.py\", line 61, in <module>\n    guest_client_id = get_client_id()\n  File \"/opt/yunohost/searx/searx/engines/soundcloud.py\", line 41, in get_client_id\n    response = http_get(\"https://soundcloud.com\")\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='soundcloud.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f61e9b21a10>: Failed to establish a new connection: [Errno -2] Name or service not known',))\nFri Nov  4 21:08:25 2016 - unable to load app 0 (mountpoint='') (callable not found or import error)\nFri Nov  4 21:08:25 2016 - *** no app loaded. going in full dynamic mode ***\nFri Nov  4 21:08:25 2016 - spawned uWSGI master process (pid: 1109)\nFri Nov  4 21:08:25 2016 - spawned uWSGI worker 1 (pid: 1461, cores: 1)\nFri Nov  4 21:08:25 2016 - spawned uWSGI worker 2 (pid: 1462, cores: 1)\nFri Nov  4 21:08:25 2016 - spawned uWSGI worker 3 (pid: 1463, cores: 1)\nFri Nov  4 21:08:25 2016 - spawned uWSGI worker 4 (pid: 1464, cores: 1)\nFri Nov  4 21:08:54 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1463|app: -1|req: -1/1] 127.0.0.1 () {42 vars in 793 bytes} [Fri Nov  4 21:08:54 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:09 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1462|app: -1|req: -1/2] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:09 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:09 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1462|app: -1|req: -1/3] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:09 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:10 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1462|app: -1|req: -1/4] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:10 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:10 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1463|app: -1|req: -1/5] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:10 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:10 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1463|app: -1|req: -1/6] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:10 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:10 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1463|app: -1|req: -1/7] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:10 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:11 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1462|app: -1|req: -1/8] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:11 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:11 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1464|app: -1|req: -1/9] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:11 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:11 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1462|app: -1|req: -1/10] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:11 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:11 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1463|app: -1|req: -1/11] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:11 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:11 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1462|app: -1|req: -1/12] 127.0.0.1 () {44 vars in 824 bytes} [Fri Nov  4 21:09:11 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:36 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1464|app: -1|req: -1/13] 127.0.0.1 () {42 vars in 793 bytes} [Fri Nov  4 21:09:36 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:38 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1463|app: -1|req: -1/14] 127.0.0.1 () {42 vars in 793 bytes} [Fri Nov  4 21:09:38 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:40 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1462|app: -1|req: -1/15] 127.0.0.1 () {42 vars in 793 bytes} [Fri Nov  4 21:09:40 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:09:52 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1462|app: -1|req: -1/16] 127.0.0.1 () {42 vars in 793 bytes} [Fri Nov  4 21:09:52 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:10:07 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1462|app: -1|req: -1/17] 127.0.0.1 () {42 vars in 793 bytes} [Fri Nov  4 21:10:07 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:10:14 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1463|app: -1|req: -1/18] 127.0.0.1 () {54 vars in 1009 bytes} [Fri Nov  4 21:10:14 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nFri Nov  4 21:10:21 2016 - ...brutally killing workers...\nFri Nov  4 21:11:29 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Fri Nov  4 21:11:29 2016] ***\nFri Nov  4 21:11:29 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nFri Nov  4 21:11:29 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nFri Nov  4 21:11:29 2016 - nodename: qwhyitserve\nFri Nov  4 21:11:29 2016 - machine: x86_64\nFri Nov  4 21:11:29 2016 - clock source: unix\nFri Nov  4 21:11:29 2016 - pcre jit disabled\nFri Nov  4 21:11:29 2016 - detected number of CPU cores: 8\nFri Nov  4 21:11:29 2016 - current working directory: /\nFri Nov  4 21:11:29 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nFri Nov  4 21:11:29 2016 - detected binary path: /usr/bin/uwsgi-core\nFri Nov  4 21:11:29 2016 - setgid() to 1008\nFri Nov  4 21:11:29 2016 - setuid() to 1008\nFri Nov  4 21:11:29 2016 - your processes number limit is 31688\nFri Nov  4 21:11:29 2016 - your memory page size is 4096 bytes\nFri Nov  4 21:11:29 2016 - detected max file descriptor number: 1024\nFri Nov  4 21:11:29 2016 - lock engine: pthread robust mutexes\nFri Nov  4 21:11:29 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nFri Nov  4 21:11:29 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nFri Nov  4 21:11:29 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nFri Nov  4 21:11:29 2016 - Set PythonHome to /opt/yunohost/searx/\nFri Nov  4 21:11:29 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nFri Nov  4 21:11:29 2016 - Python main interpreter initialized at 0x18ed8b0\nFri Nov  4 21:11:29 2016 - your server socket listen backlog is limited to 100 connections\nFri Nov  4 21:11:29 2016 - your mercy for graceful operations on workers is 60 seconds\nFri Nov  4 21:11:29 2016 - mapped 363840 bytes (355 KB) for 4 cores\nFri Nov  4 21:11:29 2016 - *** Operational MODE: preforking ***\nFri Nov  4 21:11:29 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nFri Nov  4 21:11:30 2016 - WSGI app 0 (mountpoint='') ready in 1 seconds on interpreter 0x18ed8b0 pid: 5192 (default app)\nFri Nov  4 21:11:30 2016 - spawned uWSGI master process (pid: 5192)\nFri Nov  4 21:11:30 2016 - spawned uWSGI worker 1 (pid: 5220, cores: 1)\nFri Nov  4 21:11:30 2016 - spawned uWSGI worker 2 (pid: 5221, cores: 1)\nFri Nov  4 21:11:30 2016 - spawned uWSGI worker 3 (pid: 5222, cores: 1)\nFri Nov  4 21:11:30 2016 - spawned uWSGI worker 4 (pid: 5223, cores: 1)\n[pid: 5222|app: 0|req: 1/1] 127.0.0.1 () {52 vars in 954 bytes} [Fri Nov  4 21:11:42 2016] GET /searx/ => generated 10965 bytes in 119 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nFri Nov  4 21:24:41 2016 - SIGINT/SIGQUIT received...killing workers...\nFri Nov  4 21:24:42 2016 - worker 1 buried after 1 seconds\nFri Nov  4 21:24:42 2016 - worker 2 buried after 1 seconds\nFri Nov  4 21:24:42 2016 - worker 3 buried after 1 seconds\nFri Nov  4 21:24:42 2016 - worker 4 buried after 1 seconds\nFri Nov  4 21:24:42 2016 - goodbye to uWSGI.\nSat Nov  5 06:07:49 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 06:07:49 2016] ***\nSat Nov  5 06:07:49 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 06:07:49 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 06:07:49 2016 - nodename: qwhyitserve\nSat Nov  5 06:07:49 2016 - machine: x86_64\nSat Nov  5 06:07:49 2016 - clock source: unix\nSat Nov  5 06:07:49 2016 - pcre jit disabled\nSat Nov  5 06:07:49 2016 - detected number of CPU cores: 8\nSat Nov  5 06:07:49 2016 - current working directory: /\nSat Nov  5 06:07:49 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 06:07:49 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 06:07:49 2016 - setgid() to 1008\nSat Nov  5 06:07:49 2016 - setuid() to 1008\nSat Nov  5 06:07:49 2016 - your processes number limit is 31688\nSat Nov  5 06:07:49 2016 - your memory page size is 4096 bytes\nSat Nov  5 06:07:49 2016 - detected max file descriptor number: 1024\nSat Nov  5 06:07:49 2016 - lock engine: pthread robust mutexes\nSat Nov  5 06:07:49 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 06:07:49 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 06:07:49 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 06:07:49 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 06:07:49 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 06:07:49 2016 - Python main interpreter initialized at 0x19fb8b0\nSat Nov  5 06:07:49 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 06:07:49 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 06:07:49 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 06:07:49 2016 - *** Operational MODE: preforking ***\nSat Nov  5 06:07:49 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/webapp.py\", line 54, in <module>\n    from searx.engines import (\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 201, in <module>\n    engine = load_engine(engine_data)\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 60, in load_engine\n    engine = load_module(engine_name + '.py')\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 53, in load_module\n    module = load_source(modname, filepath)\n  File \"/opt/yunohost/searx/searx/engines/soundcloud.py\", line 61, in <module>\n    guest_client_id = get_client_id()\n  File \"/opt/yunohost/searx/searx/engines/soundcloud.py\", line 41, in get_client_id\n    response = http_get(\"https://soundcloud.com\")\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='soundcloud.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fed45b0aa10>: Failed to establish a new connection: [Errno 101] Network is unreachable',))\nSat Nov  5 06:07:50 2016 - unable to load app 0 (mountpoint='') (callable not found or import error)\nSat Nov  5 06:07:50 2016 - *** no app loaded. going in full dynamic mode ***\nSat Nov  5 06:07:50 2016 - spawned uWSGI master process (pid: 1126)\nSat Nov  5 06:07:50 2016 - spawned uWSGI worker 1 (pid: 1445, cores: 1)\nSat Nov  5 06:07:50 2016 - spawned uWSGI worker 2 (pid: 1446, cores: 1)\nSat Nov  5 06:07:50 2016 - spawned uWSGI worker 3 (pid: 1448, cores: 1)\nSat Nov  5 06:07:50 2016 - spawned uWSGI worker 4 (pid: 1449, cores: 1)\nSat Nov  5 06:17:44 2016 - SIGINT/SIGQUIT received...killing workers...\nSat Nov  5 06:17:45 2016 - worker 1 buried after 1 seconds\nSat Nov  5 06:17:45 2016 - worker 2 buried after 1 seconds\nSat Nov  5 06:17:45 2016 - worker 3 buried after 1 seconds\nSat Nov  5 06:17:45 2016 - worker 4 buried after 1 seconds\nSat Nov  5 06:17:45 2016 - goodbye to uWSGI.\nSat Nov  5 06:18:07 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 06:18:07 2016] ***\nSat Nov  5 06:18:07 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 06:18:07 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 06:18:07 2016 - nodename: qwhyitserve\nSat Nov  5 06:18:07 2016 - machine: x86_64\nSat Nov  5 06:18:07 2016 - clock source: unix\nSat Nov  5 06:18:07 2016 - pcre jit disabled\nSat Nov  5 06:18:07 2016 - detected number of CPU cores: 8\nSat Nov  5 06:18:07 2016 - current working directory: /\nSat Nov  5 06:18:07 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 06:18:07 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 06:18:07 2016 - setgid() to 1008\nSat Nov  5 06:18:07 2016 - setuid() to 1008\nSat Nov  5 06:18:07 2016 - your processes number limit is 31688\nSat Nov  5 06:18:07 2016 - your memory page size is 4096 bytes\nSat Nov  5 06:18:07 2016 - detected max file descriptor number: 1024\nSat Nov  5 06:18:07 2016 - lock engine: pthread robust mutexes\nSat Nov  5 06:18:07 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 06:18:07 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 06:18:07 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 06:18:07 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 06:18:08 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 06:18:08 2016 - Python main interpreter initialized at 0x14098b0\nSat Nov  5 06:18:08 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 06:18:08 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 06:18:08 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 06:18:08 2016 - *** Operational MODE: preforking ***\nSat Nov  5 06:18:08 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/webapp.py\", line 54, in <module>\n    from searx.engines import (\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 201, in <module>\n    engine = load_engine(engine_data)\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 60, in load_engine\n    engine = load_module(engine_name + '.py')\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 53, in load_module\n    module = load_source(modname, filepath)\n  File \"/opt/yunohost/searx/searx/engines/soundcloud.py\", line 61, in <module>\n    guest_client_id = get_client_id()\n  File \"/opt/yunohost/searx/searx/engines/soundcloud.py\", line 41, in get_client_id\n    response = http_get(\"https://soundcloud.com\")\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='soundcloud.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fbd87a48a10>: Failed to establish a new connection: [Errno -2] Name or service not known',))\nSat Nov  5 06:18:09 2016 - unable to load app 0 (mountpoint='') (callable not found or import error)\nSat Nov  5 06:18:09 2016 - *** no app loaded. going in full dynamic mode ***\nSat Nov  5 06:18:09 2016 - spawned uWSGI master process (pid: 1121)\nSat Nov  5 06:18:09 2016 - spawned uWSGI worker 1 (pid: 1576, cores: 1)\nSat Nov  5 06:18:09 2016 - spawned uWSGI worker 2 (pid: 1578, cores: 1)\nSat Nov  5 06:18:09 2016 - spawned uWSGI worker 3 (pid: 1579, cores: 1)\nSat Nov  5 06:18:09 2016 - spawned uWSGI worker 4 (pid: 1580, cores: 1)\nSat Nov  5 06:23:50 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1578|app: -1|req: -1/1] 127.0.0.1 () {42 vars in 832 bytes} [Sat Nov  5 06:23:50 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 06:43:46 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1579|app: -1|req: -1/2] 127.0.0.1 () {52 vars in 993 bytes} [Sat Nov  5 06:43:46 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 06:44:01 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1580|app: -1|req: -1/3] 127.0.0.1 () {54 vars in 1042 bytes} [Sat Nov  5 06:44:01 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 06:57:47 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1580|app: -1|req: -1/4] 127.0.0.1 () {52 vars in 993 bytes} [Sat Nov  5 06:57:47 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 06:58:08 2016 - ...brutally killing workers...\nSat Nov  5 06:59:00 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 06:59:00 2016] ***\nSat Nov  5 06:59:00 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 06:59:00 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 06:59:00 2016 - nodename: qwhyitserve\nSat Nov  5 06:59:00 2016 - machine: x86_64\nSat Nov  5 06:59:00 2016 - clock source: unix\nSat Nov  5 06:59:00 2016 - pcre jit disabled\nSat Nov  5 06:59:00 2016 - detected number of CPU cores: 8\nSat Nov  5 06:59:00 2016 - current working directory: /\nSat Nov  5 06:59:00 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 06:59:00 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 06:59:00 2016 - setgid() to 12670\nSat Nov  5 06:59:00 2016 - setuid() to 12670\nSat Nov  5 06:59:00 2016 - your processes number limit is 31688\nSat Nov  5 06:59:00 2016 - your memory page size is 4096 bytes\nSat Nov  5 06:59:00 2016 - detected max file descriptor number: 1024\nSat Nov  5 06:59:00 2016 - lock engine: pthread robust mutexes\nSat Nov  5 06:59:00 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 06:59:00 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 06:59:00 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 06:59:00 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 06:59:00 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 06:59:00 2016 - Python main interpreter initialized at 0xf418b0\nSat Nov  5 06:59:00 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 06:59:00 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 06:59:00 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 06:59:00 2016 - *** Operational MODE: preforking ***\nSat Nov  5 06:59:00 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nSat Nov  5 06:59:02 2016 - WSGI app 0 (mountpoint='') ready in 2 seconds on interpreter 0xf418b0 pid: 5989 (default app)\nSat Nov  5 06:59:02 2016 - spawned uWSGI master process (pid: 5989)\nSat Nov  5 06:59:02 2016 - spawned uWSGI worker 1 (pid: 6017, cores: 1)\nSat Nov  5 06:59:02 2016 - spawned uWSGI worker 2 (pid: 6018, cores: 1)\nSat Nov  5 06:59:02 2016 - spawned uWSGI worker 3 (pid: 6019, cores: 1)\nSat Nov  5 06:59:02 2016 - spawned uWSGI worker 4 (pid: 6020, cores: 1)\n[pid: 6020|app: 0|req: 1/1] 127.0.0.1 () {52 vars in 993 bytes} [Sat Nov  5 06:59:08 2016] GET /searx/ => generated 10965 bytes in 115 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6017|app: 0|req: 1/2] 127.0.0.1 () {56 vars in 1108 bytes} [Sat Nov  5 06:59:10 2016] POST /searx/ => generated 52804 bytes in 1214 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6017|app: 0|req: 2/3] 127.0.0.1 () {52 vars in 1015 bytes} [Sat Nov  5 06:59:15 2016] GET /searx/preferences => generated 78620 bytes in 109 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6020|app: 0|req: 2/4] 127.0.0.1 () {52 vars in 993 bytes} [Sat Nov  5 07:00:08 2016] GET /searx/ => generated 10965 bytes in 12 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nERROR:searx.search:engine crash: kickass\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\n    raise ConnectionError(e, request=request)\nConnectionError: HTTPSConnectionPool(host='kickass.to', port=443): Max retries exceeded with url: /search/files/1/ (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fee66cdb990>: Failed to establish a new connection: [Errno 111] Connection refused',))\nWARNING:searx.search:engine timeout: btdigg\nERROR:searx.search:engine crash: btdigg\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 449, in send\n    raise ReadTimeout(e, request=request)\nReadTimeout: HTTPSConnectionPool(host='btdigg.org', port=443): Read timed out. (read timeout=2.0)\n[pid: 6018|app: 0|req: 1/5] 127.0.0.1 () {56 vars in 1108 bytes} [Sat Nov  5 07:00:20 2016] POST /searx/ => generated 13170 bytes in 2263 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nERROR:searx.search:engine crash: kickass\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\n    raise ConnectionError(e, request=request)\nConnectionError: HTTPSConnectionPool(host='kickass.to', port=443): Max retries exceeded with url: /search/test/1/ (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fee66cdba10>: Failed to establish a new connection: [Errno 111] Connection refused',))\nWARNING:searx.search:engine timeout: btdigg\nERROR:searx.search:engine crash: btdigg\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 449, in send\n    raise ReadTimeout(e, request=request)\nReadTimeout: HTTPSConnectionPool(host='btdigg.org', port=443): Read timed out. (read timeout=2.0)\n[pid: 6019|app: 0|req: 1/6] 127.0.0.1 () {56 vars in 1108 bytes} [Sat Nov  5 07:00:27 2016] POST /searx/ => generated 13162 bytes in 2210 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nERROR:searx.search:engine crash: kickass\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\n    raise ConnectionError(e, request=request)\nConnectionError: HTTPSConnectionPool(host='kickass.to', port=443): Max retries exceeded with url: /search/movie/1/ (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fee66c110d0>: Failed to establish a new connection: [Errno 111] Connection refused',))\nWARNING:searx.search:engine timeout: btdigg\n[pid: 6019|app: 0|req: 2/7] 127.0.0.1 () {56 vars in 1108 bytes} [Sat Nov  5 07:00:33 2016] POST /searx/ => generated 13170 bytes in 2023 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nERROR:searx.search:engine crash: btdigg\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 449, in send\n    raise ReadTimeout(e, request=request)\nReadTimeout: HTTPSConnectionPool(host='btdigg.org', port=443): Read timed out. (read timeout=2.0)\nERROR:searx.search:engine crash: kickass\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\n    raise ConnectionError(e, request=request)\nConnectionError: HTTPSConnectionPool(host='kickass.to', port=443): Max retries exceeded with url: /search/movie/1/ (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fee66c111d0>: Failed to establish a new connection: [Errno 111] Connection refused',))\nWARNING:searx.search:engine timeout: btdigg\n[pid: 6019|app: 0|req: 3/8] 127.0.0.1 () {56 vars in 1108 bytes} [Sat Nov  5 07:00:37 2016] POST /searx/ => generated 13170 bytes in 2025 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6020|app: 0|req: 3/9] 127.0.0.1 () {52 vars in 1057 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/ => generated 10965 bytes in 8 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6017|app: 0|req: 3/10] 127.0.0.1 () {56 vars in 1204 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 5 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 6018|app: 0|req: 2/11] 127.0.0.1 () {56 vars in 1208 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 7 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\nERROR:searx.search:engine crash: btdigg\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 449, in send\n    raise ReadTimeout(e, request=request)\nReadTimeout: HTTPSConnectionPool(host='btdigg.org', port=443): Read timed out. (read timeout=2.0)\n[pid: 6017|app: 0|req: 4/12] 127.0.0.1 () {56 vars in 1172 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 95786 bytes in 1 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 6020|app: 0|req: 4/13] 127.0.0.1 () {56 vars in 1184 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 9 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 6017|app: 0|req: 5/14] 127.0.0.1 () {56 vars in 1200 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 659 bytes in 1 msecs via sendfile() (HTTP/1.1 200) 7 headers in 281 bytes (0 switches on core 0)\n[pid: 6018|app: 0|req: 3/15] 127.0.0.1 () {56 vars in 1163 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/static/js/bootstrap.min.js => generated 31819 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 6019|app: 0|req: 4/16] 127.0.0.1 () {56 vars in 1174 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/static/js/require-2.1.15.min.js => generated 15219 bytes in 8 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 6020|app: 0|req: 5/17] 127.0.0.1 () {56 vars in 1181 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 5193 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 285 bytes (0 switches on core 0)\n[pid: 6019|app: 0|req: 5/18] 127.0.0.1 () {56 vars in 1222 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 1 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 6018|app: 0|req: 4/19] 127.0.0.1 () {56 vars in 1260 bytes} [Sat Nov  5 07:52:39 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 1 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 6018|app: 0|req: 5/20] 127.0.0.1 () {52 vars in 954 bytes} [Sat Nov  5 08:45:31 2016] GET /searx/ => generated 10965 bytes in 36 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6019|app: 0|req: 6/21] 107.179.235.68 () {34 vars in 461 bytes} [Sat Nov  5 08:59:04 2016] GET /searx/ => generated 10965 bytes in 33 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6017|app: 0|req: 6/22] 107.179.235.68 () {34 vars in 461 bytes} [Sat Nov  5 08:59:05 2016] GET /searx/ => generated 10965 bytes in 47 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6019|app: 0|req: 7/23] 107.179.235.68 () {44 vars in 559 bytes} [Sat Nov  5 08:59:05 2016] HEAD /searx/ => generated 0 bytes in 16 msecs (HTTP/1.1 200) 2 headers in 82 bytes (0 switches on core 0)\n[pid: 6019|app: 0|req: 8/24] 107.179.235.68 () {46 vars in 588 bytes} [Sat Nov  5 08:59:06 2016] POST /searx/ => generated 10965 bytes in 16 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6020|app: 0|req: 6/25] 127.0.0.1 () {54 vars in 1130 bytes} [Sat Nov  5 09:09:35 2016] GET /searx/ => generated 10965 bytes in 11 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6020|app: 0|req: 7/26] 127.0.0.1 () {52 vars in 1057 bytes} [Sat Nov  5 09:11:51 2016] GET /searx/ => generated 10965 bytes in 8 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6020|app: 0|req: 8/27] 173.199.65.30 () {42 vars in 646 bytes} [Sat Nov  5 09:59:11 2016] GET /searx/ => generated 10965 bytes in 21 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6017|app: 0|req: 7/28] 173.199.65.30 () {40 vars in 622 bytes} [Sat Nov  5 09:59:12 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (1 switches on core 0)\n[pid: 6018|app: 0|req: 6/29] 173.199.65.30 () {40 vars in 648 bytes} [Sat Nov  5 09:59:12 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 6020|app: 0|req: 9/30] 173.199.65.30 () {40 vars in 603 bytes} [Sat Nov  5 09:59:12 2016] GET /searx/static/js/bootstrap.min.js => generated 31819 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 6018|app: 0|req: 7/31] 173.199.65.30 () {40 vars in 611 bytes} [Sat Nov  5 09:59:12 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 95786 bytes in 5 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 6017|app: 0|req: 8/32] 173.199.65.30 () {40 vars in 613 bytes} [Sat Nov  5 09:59:12 2016] GET /searx/static/js/require-2.1.15.min.js => generated 15219 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 6020|app: 0|req: 10/33] 173.199.65.30 () {40 vars in 644 bytes} [Sat Nov  5 09:59:12 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 6018|app: 0|req: 8/34] 173.199.65.30 () {40 vars in 643 bytes} [Sat Nov  5 09:59:12 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 659 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 281 bytes (0 switches on core 0)\n[pid: 6017|app: 0|req: 9/35] 173.199.65.30 () {40 vars in 621 bytes} [Sat Nov  5 09:59:12 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 5193 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 285 bytes (0 switches on core 0)\n[pid: 6018|app: 0|req: 9/36] 173.199.65.30 () {40 vars in 631 bytes} [Sat Nov  5 09:59:12 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 6018|app: 0|req: 10/37] 173.199.65.30 () {40 vars in 697 bytes} [Sat Nov  5 09:59:13 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 6017|app: 0|req: 10/38] 173.199.65.30 () {46 vars in 761 bytes} [Sat Nov  5 09:59:15 2016] POST /searx/ => generated 56484 bytes in 1059 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 6019|app: 0|req: 9/39] 173.199.65.30 () {40 vars in 637 bytes} [Sat Nov  5 09:59:16 2016] GET /searx/static/themes/oscar/img/icons/wikipedia.png => generated 3960 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 272 bytes (0 switches on core 0)\nSat Nov  5 10:03:42 2016 - SIGINT/SIGQUIT received...killing workers...\nSat Nov  5 10:03:43 2016 - worker 1 buried after 1 seconds\nSat Nov  5 10:03:43 2016 - worker 2 buried after 1 seconds\nSat Nov  5 10:03:43 2016 - worker 3 buried after 1 seconds\nSat Nov  5 10:03:43 2016 - worker 4 buried after 1 seconds\nSat Nov  5 10:03:43 2016 - goodbye to uWSGI.\nSat Nov  5 10:04:10 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 10:04:10 2016] ***\nSat Nov  5 10:04:10 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 10:04:10 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 10:04:10 2016 - nodename: qwhyitserve\nSat Nov  5 10:04:10 2016 - machine: x86_64\nSat Nov  5 10:04:10 2016 - clock source: unix\nSat Nov  5 10:04:10 2016 - pcre jit disabled\nSat Nov  5 10:04:10 2016 - detected number of CPU cores: 8\nSat Nov  5 10:04:10 2016 - current working directory: /\nSat Nov  5 10:04:10 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 10:04:10 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 10:04:10 2016 - setgid() to 12670\nSat Nov  5 10:04:10 2016 - setuid() to 12670\nSat Nov  5 10:04:10 2016 - your processes number limit is 31688\nSat Nov  5 10:04:10 2016 - your memory page size is 4096 bytes\nSat Nov  5 10:04:10 2016 - detected max file descriptor number: 1024\nSat Nov  5 10:04:10 2016 - lock engine: pthread robust mutexes\nSat Nov  5 10:04:10 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 10:04:10 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 10:04:10 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 10:04:10 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 10:04:10 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 10:04:10 2016 - Python main interpreter initialized at 0xeac8b0\nSat Nov  5 10:04:10 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 10:04:10 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 10:04:10 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 10:04:10 2016 - *** Operational MODE: preforking ***\nSat Nov  5 10:04:10 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/webapp.py\", line 54, in <module>\n    from searx.engines import (\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 201, in <module>\n    engine = load_engine(engine_data)\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 60, in load_engine\n    engine = load_module(engine_name + '.py')\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 53, in load_module\n    module = load_source(modname, filepath)\n  File \"/opt/yunohost/searx/searx/engines/soundcloud.py\", line 61, in <module>\n    guest_client_id = get_client_id()\n  File \"/opt/yunohost/searx/searx/engines/soundcloud.py\", line 41, in get_client_id\n    response = http_get(\"https://soundcloud.com\")\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='soundcloud.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fdd3c5dfa10>: Failed to establish a new connection: [Errno -2] Name or service not known',))\nSat Nov  5 10:04:11 2016 - unable to load app 0 (mountpoint='') (callable not found or import error)\nSat Nov  5 10:04:11 2016 - *** no app loaded. going in full dynamic mode ***\nSat Nov  5 10:04:11 2016 - spawned uWSGI master process (pid: 1116)\nSat Nov  5 10:04:11 2016 - spawned uWSGI worker 1 (pid: 1439, cores: 1)\nSat Nov  5 10:04:11 2016 - spawned uWSGI worker 2 (pid: 1440, cores: 1)\nSat Nov  5 10:04:11 2016 - spawned uWSGI worker 3 (pid: 1441, cores: 1)\nSat Nov  5 10:04:11 2016 - spawned uWSGI worker 4 (pid: 1442, cores: 1)\nSat Nov  5 10:07:03 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1442|app: -1|req: -1/1] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 10:07:03 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:08:22 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1442|app: -1|req: -1/2] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 10:08:22 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:16:33 2016 - SIGINT/SIGQUIT received...killing workers...\nSat Nov  5 10:16:34 2016 - worker 1 buried after 1 seconds\nSat Nov  5 10:16:34 2016 - worker 2 buried after 1 seconds\nSat Nov  5 10:16:34 2016 - worker 3 buried after 1 seconds\nSat Nov  5 10:16:34 2016 - worker 4 buried after 1 seconds\nSat Nov  5 10:16:34 2016 - goodbye to uWSGI.\nSat Nov  5 10:16:34 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 10:16:34 2016] ***\nSat Nov  5 10:16:34 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 10:16:34 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 10:16:34 2016 - nodename: qwhyitserve\nSat Nov  5 10:16:34 2016 - machine: x86_64\nSat Nov  5 10:16:34 2016 - clock source: unix\nSat Nov  5 10:16:34 2016 - pcre jit disabled\nSat Nov  5 10:16:34 2016 - detected number of CPU cores: 8\nSat Nov  5 10:16:34 2016 - current working directory: /\nSat Nov  5 10:16:34 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 10:16:34 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 10:16:34 2016 - setgid() to 12670\nSat Nov  5 10:16:34 2016 - setuid() to 12670\nSat Nov  5 10:16:34 2016 - your processes number limit is 31688\nSat Nov  5 10:16:34 2016 - your memory page size is 4096 bytes\nSat Nov  5 10:16:34 2016 - detected max file descriptor number: 1024\nSat Nov  5 10:16:34 2016 - lock engine: pthread robust mutexes\nSat Nov  5 10:16:34 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 10:16:34 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 10:16:34 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 10:16:34 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 10:16:34 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 10:16:34 2016 - Python main interpreter initialized at 0x223e8b0\nSat Nov  5 10:16:34 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 10:16:34 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 10:16:34 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 10:16:34 2016 - *** Operational MODE: preforking ***\nSat Nov  5 10:16:34 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nSat Nov  5 10:16:36 2016 - WSGI app 0 (mountpoint='') ready in 2 seconds on interpreter 0x223e8b0 pid: 5733 (default app)\nSat Nov  5 10:16:36 2016 - spawned uWSGI master process (pid: 5733)\nSat Nov  5 10:16:36 2016 - spawned uWSGI worker 1 (pid: 5742, cores: 1)\nSat Nov  5 10:16:36 2016 - spawned uWSGI worker 2 (pid: 5743, cores: 1)\nSat Nov  5 10:16:36 2016 - spawned uWSGI worker 3 (pid: 5744, cores: 1)\nSat Nov  5 10:16:36 2016 - spawned uWSGI worker 4 (pid: 5745, cores: 1)\n[pid: 5744|app: 0|req: 1/1] 127.0.0.1 () {54 vars in 1100 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/ => generated 10965 bytes in 113 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 5745|app: 0|req: 1/2] 127.0.0.1 () {58 vars in 1227 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/static/css/bootstrap.min.css => generated 0 bytes in 11 msecs (HTTP/1.1 304) 4 headers in 191 bytes (0 switches on core 0)\n[pid: 5743|app: 0|req: 1/3] 127.0.0.1 () {58 vars in 1251 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 0 bytes in 6 msecs (HTTP/1.1 304) 4 headers in 189 bytes (0 switches on core 0)\n[pid: 5743|app: 0|req: 2/4] 127.0.0.1 () {58 vars in 1217 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/static/js/require-2.1.15.min.js => generated 0 bytes in 1 msecs (HTTP/1.1 304) 4 headers in 190 bytes (0 switches on core 0)\n[pid: 5745|app: 0|req: 2/5] 127.0.0.1 () {58 vars in 1224 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 0 bytes in 1 msecs (HTTP/1.1 304) 4 headers in 189 bytes (0 switches on core 0)\n[pid: 5745|app: 0|req: 3/6] 127.0.0.1 () {58 vars in 1215 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 0 bytes in 1 msecs (HTTP/1.1 304) 4 headers in 190 bytes (0 switches on core 0)\n[pid: 5743|app: 0|req: 3/7] 127.0.0.1 () {58 vars in 1206 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/static/js/bootstrap.min.js => generated 0 bytes in 1 msecs (HTTP/1.1 304) 4 headers in 189 bytes (0 switches on core 0)\n[pid: 5745|app: 0|req: 4/8] 127.0.0.1 () {58 vars in 1243 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 0 bytes in 1 msecs (HTTP/1.1 304) 4 headers in 186 bytes (0 switches on core 0)\n[pid: 5742|app: 0|req: 1/9] 127.0.0.1 () {58 vars in 1247 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 0 bytes in 7 msecs (HTTP/1.1 304) 4 headers in 189 bytes (0 switches on core 0)\n[pid: 5742|app: 0|req: 2/10] 127.0.0.1 () {58 vars in 1265 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 0 bytes in 1 msecs (HTTP/1.1 304) 4 headers in 190 bytes (0 switches on core 0)\n[pid: 5745|app: 0|req: 5/11] 127.0.0.1 () {58 vars in 1303 bytes} [Sat Nov  5 10:16:38 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 0 bytes in 1 msecs (HTTP/1.1 304) 4 headers in 188 bytes (0 switches on core 0)\n[pid: 5745|app: 0|req: 6/12] 127.0.0.1 () {56 vars in 1184 bytes} [Sat Nov  5 10:16:49 2016] POST /searx/ => generated 52707 bytes in 1389 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 5744|app: 0|req: 2/13] 107.179.235.68 () {44 vars in 665 bytes} [Sat Nov  5 10:28:00 2016] GET /searx/ => generated 10965 bytes in 21 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 5744|app: 0|req: 3/14] 107.179.235.68 () {42 vars in 700 bytes} [Sat Nov  5 10:28:01 2016] GET /searx/static/themes/oscar/img/favicon.png => generated 2060 bytes in 14 msecs via sendfile() (HTTP/1.1 200) 7 headers in 272 bytes (0 switches on core 0)\n[pid: 5744|app: 0|req: 4/15] 107.179.235.68 () {42 vars in 641 bytes} [Sat Nov  5 10:28:01 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 5 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 5744|app: 0|req: 5/16] 107.179.235.68 () {42 vars in 667 bytes} [Sat Nov  5 10:28:02 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 5742|app: 0|req: 3/17] 107.179.235.68 () {42 vars in 663 bytes} [Sat Nov  5 10:28:02 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 5743|app: 0|req: 4/18] 107.179.235.68 () {42 vars in 650 bytes} [Sat Nov  5 10:28:02 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 5742|app: 0|req: 4/19] 107.179.235.68 () {46 vars in 767 bytes} [Sat Nov  5 10:28:21 2016] POST /searx/ => generated 55400 bytes in 1270 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 5744|app: 0|req: 6/20] 107.179.235.68 () {42 vars in 656 bytes} [Sat Nov  5 10:28:23 2016] GET /searx/static/themes/oscar/img/icons/wikipedia.png => generated 3960 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 272 bytes (0 switches on core 0)\n[pid: 5742|app: 0|req: 5/21] 127.0.0.1 () {52 vars in 1068 bytes} [Sat Nov  5 10:40:17 2016] GET /searx/ => generated 10965 bytes in 21 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 5744|app: 0|req: 7/22] 127.0.0.1 () {56 vars in 1183 bytes} [Sat Nov  5 10:40:21 2016] POST /searx/ => generated 48357 bytes in 1281 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 5745|app: 0|req: 7/23] 127.0.0.1 () {52 vars in 1069 bytes} [Sat Nov  5 10:45:53 2016] GET /searx/ => generated 10965 bytes in 36 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nSat Nov  5 10:47:14 2016 - SIGINT/SIGQUIT received...killing workers...\nSat Nov  5 10:47:15 2016 - worker 1 buried after 1 seconds\nSat Nov  5 10:47:15 2016 - worker 2 buried after 1 seconds\nSat Nov  5 10:47:15 2016 - worker 3 buried after 1 seconds\nSat Nov  5 10:47:15 2016 - worker 4 buried after 1 seconds\nSat Nov  5 10:47:15 2016 - goodbye to uWSGI.\nSat Nov  5 10:47:15 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 10:47:15 2016] ***\nSat Nov  5 10:47:15 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 10:47:15 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 10:47:15 2016 - nodename: qwhyitserve\nSat Nov  5 10:47:15 2016 - machine: x86_64\nSat Nov  5 10:47:15 2016 - clock source: unix\nSat Nov  5 10:47:15 2016 - pcre jit disabled\nSat Nov  5 10:47:15 2016 - detected number of CPU cores: 8\nSat Nov  5 10:47:15 2016 - current working directory: /\nSat Nov  5 10:47:15 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 10:47:15 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 10:47:15 2016 - setgid() to 12670\nSat Nov  5 10:47:15 2016 - setuid() to 12670\nSat Nov  5 10:47:15 2016 - your processes number limit is 31688\nSat Nov  5 10:47:15 2016 - your memory page size is 4096 bytes\nSat Nov  5 10:47:15 2016 - detected max file descriptor number: 1024\nSat Nov  5 10:47:15 2016 - lock engine: pthread robust mutexes\nSat Nov  5 10:47:15 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 10:47:15 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 10:47:15 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 10:47:15 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 10:47:15 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 10:47:15 2016 - Python main interpreter initialized at 0x12eb8b0\nSat Nov  5 10:47:15 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 10:47:15 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 10:47:15 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 10:47:15 2016 - *** Operational MODE: preforking ***\nSat Nov  5 10:47:15 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nSat Nov  5 10:47:17 2016 - WSGI app 0 (mountpoint='') ready in 2 seconds on interpreter 0x12eb8b0 pid: 9341 (default app)\nSat Nov  5 10:47:17 2016 - spawned uWSGI master process (pid: 9341)\nSat Nov  5 10:47:17 2016 - spawned uWSGI worker 1 (pid: 9350, cores: 1)\nSat Nov  5 10:47:17 2016 - spawned uWSGI worker 2 (pid: 9351, cores: 1)\nSat Nov  5 10:47:17 2016 - spawned uWSGI worker 3 (pid: 9352, cores: 1)\nSat Nov  5 10:47:17 2016 - spawned uWSGI worker 4 (pid: 9353, cores: 1)\nSat Nov  5 10:53:10 2016 - SIGINT/SIGQUIT received...killing workers...\nSat Nov  5 10:53:11 2016 - worker 1 buried after 1 seconds\nSat Nov  5 10:53:11 2016 - worker 2 buried after 1 seconds\nSat Nov  5 10:53:11 2016 - worker 3 buried after 1 seconds\nSat Nov  5 10:53:11 2016 - worker 4 buried after 1 seconds\nSat Nov  5 10:53:11 2016 - goodbye to uWSGI.\nSat Nov  5 10:53:37 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 10:53:37 2016] ***\nSat Nov  5 10:53:37 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 10:53:37 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 10:53:37 2016 - nodename: qwhyitserve\nSat Nov  5 10:53:37 2016 - machine: x86_64\nSat Nov  5 10:53:37 2016 - clock source: unix\nSat Nov  5 10:53:37 2016 - pcre jit disabled\nSat Nov  5 10:53:37 2016 - detected number of CPU cores: 8\nSat Nov  5 10:53:37 2016 - current working directory: /\nSat Nov  5 10:53:37 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 10:53:37 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 10:53:37 2016 - setgid() to 12670\nSat Nov  5 10:53:37 2016 - setuid() to 12670\nSat Nov  5 10:53:37 2016 - your processes number limit is 31688\nSat Nov  5 10:53:37 2016 - your memory page size is 4096 bytes\nSat Nov  5 10:53:37 2016 - detected max file descriptor number: 1024\nSat Nov  5 10:53:37 2016 - lock engine: pthread robust mutexes\nSat Nov  5 10:53:37 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 10:53:37 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 10:53:37 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 10:53:37 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 10:53:37 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 10:53:37 2016 - Python main interpreter initialized at 0x1f848b0\nSat Nov  5 10:53:37 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 10:53:37 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 10:53:37 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 10:53:37 2016 - *** Operational MODE: preforking ***\nSat Nov  5 10:53:37 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/webapp.py\", line 54, in <module>\n    from searx.engines import (\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 201, in <module>\n    engine = load_engine(engine_data)\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 60, in load_engine\n    engine = load_module(engine_name + '.py')\n  File \"/opt/yunohost/searx/searx/engines/__init__.py\", line 53, in load_module\n    module = load_source(modname, filepath)\n  File \"/opt/yunohost/searx/searx/engines/soundcloud.py\", line 61, in <module>\n    guest_client_id = get_client_id()\n  File \"/opt/yunohost/searx/searx/engines/soundcloud.py\", line 41, in get_client_id\n    response = http_get(\"https://soundcloud.com\")\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='soundcloud.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7f594c16aa10>: Failed to establish a new connection: [Errno 101] Network is unreachable',))\nSat Nov  5 10:53:38 2016 - unable to load app 0 (mountpoint='') (callable not found or import error)\nSat Nov  5 10:53:38 2016 - *** no app loaded. going in full dynamic mode ***\nSat Nov  5 10:53:38 2016 - spawned uWSGI master process (pid: 1111)\nSat Nov  5 10:53:38 2016 - spawned uWSGI worker 1 (pid: 1454, cores: 1)\nSat Nov  5 10:53:38 2016 - spawned uWSGI worker 2 (pid: 1455, cores: 1)\nSat Nov  5 10:53:38 2016 - spawned uWSGI worker 3 (pid: 1456, cores: 1)\nSat Nov  5 10:53:38 2016 - spawned uWSGI worker 4 (pid: 1457, cores: 1)\nSat Nov  5 10:54:33 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1456|app: -1|req: -1/1] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 10:54:33 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:42 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1455|app: -1|req: -1/2] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:42 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:43 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1455|app: -1|req: -1/3] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:43 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:43 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1455|app: -1|req: -1/4] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:43 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:43 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/5] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:43 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:43 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/6] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:43 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:43 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/7] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:43 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:44 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/8] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:44 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:44 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/9] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:44 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:44 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/10] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:44 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:44 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/11] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:44 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:44 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/12] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:44 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:45 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/13] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:45 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:45 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/14] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:45 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:45 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/15] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:45 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:54:46 2016 - --- no python application found, check your startup logs for errors ---\n[pid: 1454|app: -1|req: -1/16] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:54:46 2016] GET /searx/ => generated 21 bytes in 0 msecs (HTTP/1.1 500) 2 headers in 83 bytes (0 switches on core 0)\nSat Nov  5 10:55:07 2016 - SIGINT/SIGQUIT received...killing workers...\nSat Nov  5 10:55:08 2016 - worker 1 buried after 1 seconds\nSat Nov  5 10:55:08 2016 - worker 2 buried after 1 seconds\nSat Nov  5 10:55:08 2016 - worker 3 buried after 1 seconds\nSat Nov  5 10:55:08 2016 - worker 4 buried after 1 seconds\nSat Nov  5 10:55:08 2016 - goodbye to uWSGI.\nSat Nov  5 10:55:09 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 10:55:09 2016] ***\nSat Nov  5 10:55:09 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 10:55:09 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 10:55:09 2016 - nodename: qwhyitserve\nSat Nov  5 10:55:09 2016 - machine: x86_64\nSat Nov  5 10:55:09 2016 - clock source: unix\nSat Nov  5 10:55:09 2016 - pcre jit disabled\nSat Nov  5 10:55:09 2016 - detected number of CPU cores: 8\nSat Nov  5 10:55:09 2016 - current working directory: /\nSat Nov  5 10:55:09 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 10:55:09 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 10:55:09 2016 - setgid() to 12670\nSat Nov  5 10:55:09 2016 - setuid() to 12670\nSat Nov  5 10:55:09 2016 - your processes number limit is 31688\nSat Nov  5 10:55:09 2016 - your memory page size is 4096 bytes\nSat Nov  5 10:55:09 2016 - detected max file descriptor number: 1024\nSat Nov  5 10:55:09 2016 - lock engine: pthread robust mutexes\nSat Nov  5 10:55:09 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 10:55:09 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 10:55:09 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 10:55:09 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 10:55:09 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 10:55:09 2016 - Python main interpreter initialized at 0xbac8b0\nSat Nov  5 10:55:09 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 10:55:09 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 10:55:09 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 10:55:09 2016 - *** Operational MODE: preforking ***\nSat Nov  5 10:55:09 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nSat Nov  5 10:55:10 2016 - WSGI app 0 (mountpoint='') ready in 1 seconds on interpreter 0xbac8b0 pid: 3983 (default app)\nSat Nov  5 10:55:10 2016 - spawned uWSGI master process (pid: 3983)\nSat Nov  5 10:55:10 2016 - spawned uWSGI worker 1 (pid: 4020, cores: 1)\nSat Nov  5 10:55:10 2016 - spawned uWSGI worker 2 (pid: 4021, cores: 1)\nSat Nov  5 10:55:10 2016 - spawned uWSGI worker 3 (pid: 4022, cores: 1)\nSat Nov  5 10:55:10 2016 - spawned uWSGI worker 4 (pid: 4023, cores: 1)\n[pid: 4022|app: 0|req: 1/1] 127.0.0.1 () {44 vars in 824 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/ => generated 10965 bytes in 179 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 4022|app: 0|req: 2/2] 127.0.0.1 () {48 vars in 975 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 0 bytes in 7 msecs (HTTP/1.1 304) 4 headers in 189 bytes (0 switches on core 0)\n[pid: 4021|app: 0|req: 1/3] 127.0.0.1 () {48 vars in 971 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 0 bytes in 8 msecs (HTTP/1.1 304) 4 headers in 189 bytes (0 switches on core 0)\n[pid: 4023|app: 0|req: 1/4] 127.0.0.1 () {48 vars in 951 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/static/css/bootstrap.min.css => generated 0 bytes in 10 msecs (HTTP/1.1 304) 4 headers in 191 bytes (0 switches on core 0)\n[pid: 4022|app: 0|req: 3/5] 127.0.0.1 () {48 vars in 941 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/static/js/require-2.1.15.min.js => generated 0 bytes in 2 msecs (HTTP/1.1 304) 4 headers in 190 bytes (0 switches on core 0)\n[pid: 4023|app: 0|req: 2/6] 127.0.0.1 () {48 vars in 967 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 0 bytes in 1 msecs (HTTP/1.1 304) 4 headers in 186 bytes (0 switches on core 0)\n[pid: 4021|app: 0|req: 2/7] 127.0.0.1 () {48 vars in 948 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 0 bytes in 2 msecs (HTTP/1.1 304) 4 headers in 189 bytes (0 switches on core 0)\n[pid: 4022|app: 0|req: 4/8] 127.0.0.1 () {48 vars in 930 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/static/js/bootstrap.min.js => generated 0 bytes in 1 msecs (HTTP/1.1 304) 4 headers in 189 bytes (0 switches on core 0)\n[pid: 4023|app: 0|req: 3/9] 127.0.0.1 () {48 vars in 989 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 0 bytes in 1 msecs (HTTP/1.1 304) 4 headers in 190 bytes (0 switches on core 0)\n[pid: 4020|app: 0|req: 1/10] 127.0.0.1 () {48 vars in 939 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 0 bytes in 8 msecs (HTTP/1.1 304) 4 headers in 190 bytes (0 switches on core 0)\n[pid: 4022|app: 0|req: 5/11] 127.0.0.1 () {48 vars in 1027 bytes} [Sat Nov  5 10:55:11 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 0 bytes in 3 msecs (HTTP/1.1 304) 4 headers in 188 bytes (0 switches on core 0)\nSat Nov  5 10:55:37 2016 - SIGINT/SIGQUIT received...killing workers...\nSat Nov  5 10:55:38 2016 - worker 1 buried after 1 seconds\nSat Nov  5 10:55:38 2016 - worker 2 buried after 1 seconds\nSat Nov  5 10:55:38 2016 - worker 3 buried after 1 seconds\nSat Nov  5 10:55:38 2016 - worker 4 buried after 1 seconds\nSat Nov  5 10:55:38 2016 - goodbye to uWSGI.\nSat Nov  5 10:55:39 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 10:55:39 2016] ***\nSat Nov  5 10:55:39 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 10:55:39 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 10:55:39 2016 - nodename: qwhyitserve\nSat Nov  5 10:55:39 2016 - machine: x86_64\nSat Nov  5 10:55:39 2016 - clock source: unix\nSat Nov  5 10:55:39 2016 - pcre jit disabled\nSat Nov  5 10:55:39 2016 - detected number of CPU cores: 8\nSat Nov  5 10:55:39 2016 - current working directory: /\nSat Nov  5 10:55:39 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 10:55:39 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 10:55:39 2016 - setgid() to 12670\nSat Nov  5 10:55:39 2016 - setuid() to 12670\nSat Nov  5 10:55:39 2016 - your processes number limit is 31688\nSat Nov  5 10:55:39 2016 - your memory page size is 4096 bytes\nSat Nov  5 10:55:39 2016 - detected max file descriptor number: 1024\nSat Nov  5 10:55:39 2016 - lock engine: pthread robust mutexes\nSat Nov  5 10:55:39 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 10:55:39 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 10:55:39 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 10:55:39 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 10:55:39 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 10:55:39 2016 - Python main interpreter initialized at 0x1f6f8b0\nSat Nov  5 10:55:39 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 10:55:39 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 10:55:39 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 10:55:39 2016 - *** Operational MODE: preforking ***\nSat Nov  5 10:55:39 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nSat Nov  5 10:55:40 2016 - WSGI app 0 (mountpoint='') ready in 1 seconds on interpreter 0x1f6f8b0 pid: 4304 (default app)\nSat Nov  5 10:55:40 2016 - spawned uWSGI master process (pid: 4304)\nSat Nov  5 10:55:40 2016 - spawned uWSGI worker 1 (pid: 4314, cores: 1)\nSat Nov  5 10:55:40 2016 - spawned uWSGI worker 2 (pid: 4315, cores: 1)\nSat Nov  5 10:55:40 2016 - spawned uWSGI worker 3 (pid: 4316, cores: 1)\nSat Nov  5 10:55:40 2016 - spawned uWSGI worker 4 (pid: 4317, cores: 1)\n[pid: 4315|app: 0|req: 1/1] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 11:21:17 2016] GET /searx/ => generated 10965 bytes in 131 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 4315|app: 0|req: 2/2] 127.0.0.1 () {46 vars in 908 bytes} [Sat Nov  5 11:21:21 2016] POST /searx/ => generated 58658 bytes in 1203 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 4315|app: 0|req: 3/3] 127.0.0.1 () {46 vars in 963 bytes} [Sat Nov  5 11:21:22 2016] GET /searx/static/themes/oscar/img/icons/wikipedia.png => generated 3960 bytes in 5 msecs via sendfile() (HTTP/1.1 200) 7 headers in 272 bytes (0 switches on core 0)\nERROR:searx.search:engine crash: kickass\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 437, in send\n    raise ConnectionError(e, request=request)\nConnectionError: HTTPSConnectionPool(host='kickass.to', port=443): Max retries exceeded with url: /search/torrent/1/ (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x7fbc0a601050>: Failed to establish a new connection: [Errno 111] Connection refused',))\nWARNING:searx.search:engine timeout: btdigg\n[pid: 4314|app: 0|req: 1/4] 127.0.0.1 () {46 vars in 908 bytes} [Sat Nov  5 11:21:40 2016] POST /searx/ => generated 13186 bytes in 2223 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nERROR:searx.search:engine crash: btdigg\nTraceback (most recent call last):\n  File \"/opt/yunohost/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 86, in get\n    return request('get', url, **kwargs)\n  File \"/opt/yunohost/searx/searx/poolrequests.py\", line 79, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/yunohost/searx/local/lib/python2.7/site-packages/requests/adapters.py\", line 449, in send\n    raise ReadTimeout(e, request=request)\nReadTimeout: HTTPSConnectionPool(host='btdigg.org', port=443): Read timed out. (read timeout=2.0)\n[pid: 4314|app: 0|req: 2/5] 127.0.0.1 () {46 vars in 908 bytes} [Sat Nov  5 11:21:43 2016] POST /searx/ => generated 61029 bytes in 1024 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 4316|app: 0|req: 1/6] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 11:30:55 2016] GET /searx/ => generated 10965 bytes in 115 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\nSat Nov  5 11:36:26 2016 - SIGINT/SIGQUIT received...killing workers...\nSat Nov  5 11:36:27 2016 - worker 1 buried after 1 seconds\nSat Nov  5 11:36:27 2016 - worker 2 buried after 1 seconds\nSat Nov  5 11:36:27 2016 - worker 3 buried after 1 seconds\nSat Nov  5 11:36:27 2016 - worker 4 buried after 1 seconds\nSat Nov  5 11:36:27 2016 - goodbye to uWSGI.\nSat Nov  5 11:36:27 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 11:36:27 2016] ***\nSat Nov  5 11:36:27 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 11:36:27 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 11:36:27 2016 - nodename: qwhyitserve\nSat Nov  5 11:36:27 2016 - machine: x86_64\nSat Nov  5 11:36:27 2016 - clock source: unix\nSat Nov  5 11:36:27 2016 - pcre jit disabled\nSat Nov  5 11:36:27 2016 - detected number of CPU cores: 8\nSat Nov  5 11:36:27 2016 - current working directory: /\nSat Nov  5 11:36:27 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 11:36:27 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 11:36:27 2016 - setgid() to 12670\nSat Nov  5 11:36:27 2016 - setuid() to 12670\nSat Nov  5 11:36:27 2016 - your processes number limit is 31688\nSat Nov  5 11:36:27 2016 - your memory page size is 4096 bytes\nSat Nov  5 11:36:27 2016 - detected max file descriptor number: 1024\nSat Nov  5 11:36:27 2016 - lock engine: pthread robust mutexes\nSat Nov  5 11:36:27 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 11:36:27 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 11:36:27 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 11:36:27 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 11:36:27 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 11:36:27 2016 - Python main interpreter initialized at 0x11a58b0\nSat Nov  5 11:36:27 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 11:36:27 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 11:36:27 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 11:36:27 2016 - *** Operational MODE: preforking ***\nSat Nov  5 11:36:27 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nSat Nov  5 11:36:29 2016 - WSGI app 0 (mountpoint='') ready in 2 seconds on interpreter 0x11a58b0 pid: 7680 (default app)\nSat Nov  5 11:36:29 2016 - spawned uWSGI master process (pid: 7680)\nSat Nov  5 11:36:29 2016 - spawned uWSGI worker 1 (pid: 7689, cores: 1)\nSat Nov  5 11:36:29 2016 - spawned uWSGI worker 2 (pid: 7690, cores: 1)\nSat Nov  5 11:36:29 2016 - spawned uWSGI worker 3 (pid: 7691, cores: 1)\nSat Nov  5 11:36:29 2016 - spawned uWSGI worker 4 (pid: 7692, cores: 1)\nSat Nov  5 11:37:22 2016 - SIGINT/SIGQUIT received...killing workers...\nSat Nov  5 11:37:23 2016 - worker 1 buried after 1 seconds\nSat Nov  5 11:37:23 2016 - worker 2 buried after 1 seconds\nSat Nov  5 11:37:23 2016 - worker 3 buried after 1 seconds\nSat Nov  5 11:37:23 2016 - worker 4 buried after 1 seconds\nSat Nov  5 11:37:23 2016 - goodbye to uWSGI.\nSat Nov  5 11:37:23 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Sat Nov  5 11:37:23 2016] ***\nSat Nov  5 11:37:23 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nSat Nov  5 11:37:23 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.36-1+deb8u2 (2016-10-19)\nSat Nov  5 11:37:23 2016 - nodename: qwhyitserve\nSat Nov  5 11:37:23 2016 - machine: x86_64\nSat Nov  5 11:37:23 2016 - clock source: unix\nSat Nov  5 11:37:23 2016 - pcre jit disabled\nSat Nov  5 11:37:23 2016 - detected number of CPU cores: 8\nSat Nov  5 11:37:23 2016 - current working directory: /\nSat Nov  5 11:37:23 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nSat Nov  5 11:37:23 2016 - detected binary path: /usr/bin/uwsgi-core\nSat Nov  5 11:37:23 2016 - setgid() to 12670\nSat Nov  5 11:37:23 2016 - setuid() to 12670\nSat Nov  5 11:37:23 2016 - your processes number limit is 31688\nSat Nov  5 11:37:23 2016 - your memory page size is 4096 bytes\nSat Nov  5 11:37:23 2016 - detected max file descriptor number: 1024\nSat Nov  5 11:37:23 2016 - lock engine: pthread robust mutexes\nSat Nov  5 11:37:23 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nSat Nov  5 11:37:23 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nSat Nov  5 11:37:23 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nSat Nov  5 11:37:23 2016 - Set PythonHome to /opt/yunohost/searx/\nSat Nov  5 11:37:23 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nSat Nov  5 11:37:23 2016 - Python main interpreter initialized at 0xea98b0\nSat Nov  5 11:37:23 2016 - your server socket listen backlog is limited to 100 connections\nSat Nov  5 11:37:23 2016 - your mercy for graceful operations on workers is 60 seconds\nSat Nov  5 11:37:23 2016 - mapped 363840 bytes (355 KB) for 4 cores\nSat Nov  5 11:37:23 2016 - *** Operational MODE: preforking ***\nSat Nov  5 11:37:23 2016 - added /opt/yunohost/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nSat Nov  5 11:37:25 2016 - WSGI app 0 (mountpoint='') ready in 2 seconds on interpreter 0xea98b0 pid: 8517 (default app)\nSat Nov  5 11:37:25 2016 - spawned uWSGI master process (pid: 8517)\nSat Nov  5 11:37:25 2016 - spawned uWSGI worker 1 (pid: 8527, cores: 1)\nSat Nov  5 11:37:25 2016 - spawned uWSGI worker 2 (pid: 8528, cores: 1)\nSat Nov  5 11:37:25 2016 - spawned uWSGI worker 3 (pid: 8529, cores: 1)\nSat Nov  5 11:37:25 2016 - spawned uWSGI worker 4 (pid: 8530, cores: 1)\n[pid: 8527|app: 0|req: 1/1] 127.0.0.1 () {54 vars in 1124 bytes} [Sat Nov  5 11:49:01 2016] GET /searx/ => generated 10965 bytes in 109 msecs (HTTP/1.1 200) 2 headers in 82 bytes (2 switches on core 0)\n[pid: 8528|app: 0|req: 1/2] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 12:06:25 2016] GET /searx/ => generated 10965 bytes in 116 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 1/3] 107.179.235.68 () {42 vars in 719 bytes} [Sat Nov  5 12:14:05 2016] GET /searx/ => generated 10965 bytes in 200 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8527|app: 0|req: 2/4] 107.179.235.68 () {44 vars in 767 bytes} [Sat Nov  5 12:14:05 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 14 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 2/5] 107.179.235.68 () {44 vars in 789 bytes} [Sat Nov  5 12:14:05 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 9 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 2/6] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 12:14:05 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 9 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 3/7] 107.179.235.68 () {42 vars in 718 bytes} [Sat Nov  5 12:14:05 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 5193 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 285 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 1/8] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 12:14:05 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 659 bytes in 18 msecs via sendfile() (HTTP/1.1 200) 7 headers in 281 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 3/9] 107.179.235.68 () {42 vars in 708 bytes} [Sat Nov  5 12:14:05 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 95786 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 4/10] 107.179.235.68 () {42 vars in 700 bytes} [Sat Nov  5 12:14:05 2016] GET /searx/static/js/bootstrap.min.js => generated 31819 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 2/11] 107.179.235.68 () {42 vars in 745 bytes} [Sat Nov  5 12:14:05 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 5/12] 107.179.235.68 () {42 vars in 710 bytes} [Sat Nov  5 12:14:05 2016] GET /searx/static/js/require-2.1.15.min.js => generated 15219 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 4/13] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 12:14:06 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 6/14] 107.179.235.68 () {42 vars in 718 bytes} [Sat Nov  5 12:14:06 2016] GET /searx/static/themes/oscar/img/favicon.png => generated 2060 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 272 bytes (0 switches on core 0)\n[pid: 8527|app: 0|req: 3/15] 107.179.235.68 () {42 vars in 719 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/ => generated 10965 bytes in 21 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 5/16] 107.179.235.68 () {44 vars in 767 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 5 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 8527|app: 0|req: 4/17] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 3/18] 107.179.235.68 () {44 vars in 789 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 7/19] 107.179.235.68 () {42 vars in 719 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/ => generated 10965 bytes in 21 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 8/20] 107.179.235.68 () {42 vars in 745 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 4/21] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 9/22] 107.179.235.68 () {44 vars in 767 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 10/23] 107.179.235.68 () {44 vars in 789 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 5/24] 107.179.235.68 () {42 vars in 708 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 95786 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 11/25] 107.179.235.68 () {42 vars in 718 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 5193 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 285 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 6/26] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 659 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 281 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 7/27] 107.179.235.68 () {42 vars in 700 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/js/bootstrap.min.js => generated 31819 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 8/28] 107.179.235.68 () {42 vars in 710 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/js/require-2.1.15.min.js => generated 15219 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 12/29] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 9/30] 107.179.235.68 () {42 vars in 745 bytes} [Sat Nov  5 12:14:30 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 13/31] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 12:14:31 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 10/32] 107.179.235.68 () {42 vars in 719 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/ => generated 10965 bytes in 188 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 14/33] 107.179.235.68 () {42 vars in 719 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/ => generated 10965 bytes in 17 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 11/34] 107.179.235.68 () {44 vars in 767 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 15/35] 107.179.235.68 () {44 vars in 789 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 6/36] 107.179.235.68 () {42 vars in 745 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 8527|app: 0|req: 5/37] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 16/38] 107.179.235.68 () {44 vars in 789 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 17/39] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 12/40] 107.179.235.68 () {44 vars in 767 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 13/41] 107.179.235.68 () {44 vars in 756 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 95786 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 18/42] 107.179.235.68 () {42 vars in 700 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/static/js/bootstrap.min.js => generated 31819 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 19/43] 107.179.235.68 () {42 vars in 710 bytes} [Sat Nov  5 12:15:29 2016] GET /searx/static/js/require-2.1.15.min.js => generated 15219 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 14/44] 107.179.235.68 () {42 vars in 718 bytes} [Sat Nov  5 12:15:30 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 5193 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 285 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 20/45] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 12:15:30 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 659 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 281 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 21/46] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 12:15:30 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 15/47] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 12:15:30 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 22/48] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 12:15:30 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 4 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 7/49] 107.179.235.68 () {42 vars in 719 bytes} [Sat Nov  5 12:19:48 2016] GET /searx/ => generated 10965 bytes in 21 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 16/50] 107.179.235.68 () {44 vars in 767 bytes} [Sat Nov  5 12:19:48 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 17/51] 107.179.235.68 () {44 vars in 789 bytes} [Sat Nov  5 12:19:49 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 8/52] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 12:19:49 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 9/53] 107.179.235.68 () {42 vars in 708 bytes} [Sat Nov  5 12:19:49 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 95786 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 18/54] 107.179.235.68 () {42 vars in 700 bytes} [Sat Nov  5 12:19:49 2016] GET /searx/static/js/bootstrap.min.js => generated 31819 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 19/55] 107.179.235.68 () {42 vars in 718 bytes} [Sat Nov  5 12:19:49 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 5193 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 285 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 10/56] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 12:19:49 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 659 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 281 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 11/57] 107.179.235.68 () {42 vars in 710 bytes} [Sat Nov  5 12:19:49 2016] GET /searx/static/js/require-2.1.15.min.js => generated 15219 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 20/58] 107.179.235.68 () {42 vars in 745 bytes} [Sat Nov  5 12:19:49 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 21/59] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 12:19:49 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 12/60] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 12:37:09 2016] GET /searx/ => generated 10965 bytes in 7 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8527|app: 0|req: 6/61] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 12:38:49 2016] GET /searx/ => generated 10965 bytes in 7 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 22/62] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 12:38:59 2016] GET /searx/ => generated 10965 bytes in 9 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 23/63] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 12:39:05 2016] GET /searx/ => generated 10965 bytes in 11 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 13/64] 127.0.0.1 () {54 vars in 1124 bytes} [Sat Nov  5 12:39:39 2016] GET /searx/ => generated 10965 bytes in 6 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 24/65] 127.0.0.1 () {54 vars in 1124 bytes} [Sat Nov  5 12:39:53 2016] GET /searx/ => generated 10965 bytes in 14 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8527|app: 0|req: 7/66] 127.0.0.1 () {52 vars in 1069 bytes} [Sat Nov  5 13:11:52 2016] GET /searx/ => generated 10965 bytes in 17 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 14/67] 127.0.0.1 () {52 vars in 966 bytes} [Sat Nov  5 13:12:17 2016] GET /searx/ => generated 10965 bytes in 8 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 15/68] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 14:00:54 2016] GET /searx/ => generated 10965 bytes in 8 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 16/69] 127.0.0.1 () {46 vars in 908 bytes} [Sat Nov  5 14:00:55 2016] POST /searx/ => generated 52808 bytes in 1142 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 17/70] 127.0.0.1 () {46 vars in 908 bytes} [Sat Nov  5 14:01:00 2016] POST /searx/ => generated 59506 bytes in 1143 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 23/71] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 14:34:44 2016] GET /searx/ => generated 10965 bytes in 7 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 25/72] 127.0.0.1 () {46 vars in 908 bytes} [Sat Nov  5 14:34:45 2016] POST /searx/ => generated 52804 bytes in 1103 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 24/73] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 14:38:16 2016] GET /searx/ => generated 10965 bytes in 8 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8527|app: 0|req: 8/74] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 14:46:19 2016] GET /searx/ => generated 10965 bytes in 8 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 18/75] 127.0.0.1 () {46 vars in 908 bytes} [Sat Nov  5 14:46:20 2016] POST /searx/ => generated 52804 bytes in 1047 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 26/76] 107.179.235.68 () {34 vars in 461 bytes} [Sat Nov  5 14:53:53 2016] GET /searx/ => generated 10965 bytes in 9 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 19/77] 107.179.235.68 () {34 vars in 461 bytes} [Sat Nov  5 14:53:54 2016] GET /searx/ => generated 10965 bytes in 10 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 27/78] 107.179.235.68 () {44 vars in 559 bytes} [Sat Nov  5 14:53:55 2016] HEAD /searx/ => generated 0 bytes in 12 msecs (HTTP/1.1 200) 2 headers in 82 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 20/79] 107.179.235.68 () {46 vars in 588 bytes} [Sat Nov  5 14:53:55 2016] POST /searx/ => generated 10965 bytes in 19 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 28/80] 107.179.235.68 () {36 vars in 532 bytes} [Sat Nov  5 15:06:06 2016] GET /searx/ => generated 10965 bytes in 7 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 25/81] 107.179.235.68 () {34 vars in 461 bytes} [Sat Nov  5 15:06:07 2016] GET /searx/ => generated 10965 bytes in 6 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 29/82] 107.179.235.68 () {44 vars in 559 bytes} [Sat Nov  5 15:06:08 2016] HEAD /searx/ => generated 0 bytes in 6 msecs (HTTP/1.1 200) 2 headers in 82 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 21/83] 107.179.235.68 () {46 vars in 588 bytes} [Sat Nov  5 15:06:08 2016] POST /searx/ => generated 10965 bytes in 20 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 26/84] 107.179.235.68 () {42 vars in 719 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/ => generated 10965 bytes in 19 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 27/85] 107.179.235.68 () {44 vars in 767 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 28/86] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8527|app: 0|req: 9/87] 107.179.235.68 () {44 vars in 789 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8527|app: 0|req: 10/88] 107.179.235.68 () {42 vars in 710 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/static/js/require-2.1.15.min.js => generated 15219 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 30/89] 107.179.235.68 () {42 vars in 718 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 5193 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 285 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 29/90] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 659 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 281 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 30/91] 107.179.235.68 () {42 vars in 708 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 95786 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 31/92] 107.179.235.68 () {42 vars in 700 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/static/js/bootstrap.min.js => generated 31819 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 31/93] 107.179.235.68 () {42 vars in 745 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 8527|app: 0|req: 11/94] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 15:07:22 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 1 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 32/95] 127.0.0.1 () {42 vars in 838 bytes} [Sat Nov  5 15:12:44 2016] GET /searx/ => generated 10965 bytes in 10 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 22/96] 127.0.0.1 () {46 vars in 953 bytes} [Sat Nov  5 15:12:46 2016] POST /searx/ => generated 52902 bytes in 1055 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 32/97] 127.0.0.1 () {42 vars in 838 bytes} [Sat Nov  5 15:13:12 2016] GET /searx/ => generated 10965 bytes in 9 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 33/98] 107.179.235.68 () {44 vars in 665 bytes} [Sat Nov  5 15:17:06 2016] GET /searx/ => generated 10965 bytes in 20 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 23/99] 107.179.235.68 () {44 vars in 750 bytes} [Sat Nov  5 15:18:14 2016] GET /searx/ => generated 10965 bytes in 21 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 34/100] 107.179.235.68 () {44 vars in 750 bytes} [Sat Nov  5 15:20:26 2016] GET /searx/ => generated 10965 bytes in 20 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 35/101] 107.179.235.68 () {42 vars in 719 bytes} [Sat Nov  5 15:20:26 2016] GET /searx/ => generated 10965 bytes in 21 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 24/102] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 15:20:26 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8527|app: 0|req: 12/103] 107.179.235.68 () {42 vars in 745 bytes} [Sat Nov  5 15:20:26 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 36/104] 107.179.235.68 () {44 vars in 789 bytes} [Sat Nov  5 15:20:26 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 25/105] 107.179.235.68 () {44 vars in 767 bytes} [Sat Nov  5 15:20:26 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 2 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 26/106] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 15:20:27 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 33/107] 107.179.235.68 () {42 vars in 719 bytes} [Sat Nov  5 15:25:08 2016] GET /searx/ => generated 10965 bytes in 21 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 27/108] 107.179.235.68 () {44 vars in 767 bytes} [Sat Nov  5 15:25:08 2016] GET /searx/static/css/bootstrap.min.css => generated 114586 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 290 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 34/109] 107.179.235.68 () {44 vars in 789 bytes} [Sat Nov  5 15:25:08 2016] GET /searx/static/themes/oscar/css/leaflet.min.css => generated 8027 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 35/110] 107.179.235.68 () {44 vars in 756 bytes} [Sat Nov  5 15:25:08 2016] GET /searx/static/js/jquery-1.11.1.min.js => generated 95786 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 36/111] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 15:25:08 2016] GET /searx/static/themes/oscar/css/logicodev.min.css => generated 6984 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8527|app: 0|req: 13/112] 107.179.235.68 () {44 vars in 758 bytes} [Sat Nov  5 15:25:08 2016] GET /searx/static/js/require-2.1.15.min.js => generated 15219 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 287 bytes (0 switches on core 0)\n[pid: 8529|app: 0|req: 37/113] 107.179.235.68 () {44 vars in 766 bytes} [Sat Nov  5 15:25:08 2016] GET /searx/static/themes/oscar/js/searx.min.js => generated 5193 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 285 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 37/114] 107.179.235.68 () {44 vars in 788 bytes} [Sat Nov  5 15:25:08 2016] GET /searx/static/plugins/js/search_on_category_select.js => generated 659 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 281 bytes (0 switches on core 0)\n[pid: 8528|app: 0|req: 28/115] 107.179.235.68 () {44 vars in 748 bytes} [Sat Nov  5 15:25:09 2016] GET /searx/static/js/bootstrap.min.js => generated 31819 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 286 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 38/116] 107.179.235.68 () {44 vars in 793 bytes} [Sat Nov  5 15:25:09 2016] GET /searx/static/themes/oscar/img/logo_searx_a.png => generated 33423 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 274 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 39/117] 107.179.235.68 () {42 vars in 740 bytes} [Sat Nov  5 15:25:09 2016] GET /searx/static/fonts/glyphicons-halflings-regular.woff => generated 23320 bytes in 3 msecs via sendfile() (HTTP/1.1 200) 7 headers in 284 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 40/118] 107.179.235.68 () {34 vars in 461 bytes} [Sat Nov  5 15:27:17 2016] GET /searx/ => generated 10965 bytes in 14 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8530|app: 0|req: 41/119] 107.179.235.68 () {34 vars in 461 bytes} [Sat Nov  5 15:27:17 2016] GET /searx/ => generated 10965 bytes in 9 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8528|app: 0|req: 29/120] 107.179.235.68 () {44 vars in 559 bytes} [Sat Nov  5 15:27:18 2016] HEAD /searx/ => generated 0 bytes in 20 msecs (HTTP/1.1 200) 2 headers in 82 bytes (0 switches on core 0)\n[pid: 8530|app: 0|req: 42/121] 107.179.235.68 () {46 vars in 588 bytes} [Sat Nov  5 15:27:18 2016] POST /searx/ => generated 10965 bytes in 11 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 8529|app: 0|req: 38/122] 127.0.0.1 () {42 vars in 793 bytes} [Sat Nov  5 15:27:24 2016] GET /searx/ => generated 10965 bytes in 7 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n. ",
    "koehn": "I migrated to gunicorn instead. \n. ",
    "kantord": "@pointhi thanks for the review. I'll get to it soon. About comments: what parts of the code, especially, which methods are most obscure or hard to understand? Beyond adding docstrings, I'd consider renaming them, too. \n. @pointhi I concur with your stand on hasattr() and friends. Documenting this with docstrings, I find a bit challenging, though. (I mean, is it something that should be in a docstring?) I added comments at least, and also made verify and validate protected, since they aren't and should not be ever called from outside settings management classes. I'd also like to add that, though it currently seems to be an overkill, it'd be better to use objects for each setting field so that the class could call their methods instead of putting all these methods into a single messy class. This could also remove a lot of code duplication, but I'm not sure that it'd worth implementing it currently. \n. @Cqoicebordel good point. I plan to rearrange this into a single commit, with a more helpful commit message.\n. This was originally designed like this so that the class would not remove cookies not handled by it, which was handy when doing TDD. (This way I could chunk up the refactoring process into tiny bits, so only one test failed at a time). However, now that no cookies unknown to the class are allowed, this change can indeed be made.\n. as_cookie_dict now basically handles the serialization process. I don't think it would be a good idea to store values serialized. So I'm afraid we do need at least some algorithm here :( Kinda just a map, though, but it may be too ugly to make it a one-liner. What do you guys think?\n. ",
    "thfree": "It would be very good if it is implemented.\n. ",
    "cranes-bill": "I'm also thank you!\nAnd just to say: when i'm using Yandex Images, i always have very good results, so perhaps needs to be included as separate engine!\nThanks again!\n. Yandex !bang does not give me any result on https://searx.me/ and locally it search yn word.\n. ",
    "moritan": "The size for an url is near 2000 chars so if you are in http GETmethod you may have reach the limit \n. First, it's return more info than \"format=json\" option ( format=json return just search result, not answers ,suggestions...)\nMy hack don't use cookies, but a json structure for settings, i think it's a more clean way to manage settings.\nI thinks it's more easy to create an site base on searx this way, because you could update the engine without to have to merge each time your ihm change.\nIf you use javascript framework like Angular, ember.js, or even just javascript, your client side could be serve by nginx/apache/etc, . \nYou could even do the client side with PHP , but it's not so hype ;)\n. Oh yes, i should have precise something,  I have duplicate 2 files webapp.py and search.py.\nOf course, this should be re-merge in one file.\nTo be clear, it's the first time i'm coding in python ( i'm a Java dev), so i create new file to avoid break existing code. \nThis PR main goal is to exchange with you ;)\nI read somewhere, there is a refactor of settings in progress, so i propose a posibility with searchAPI.py, but i'm agree that \"The can be Only one\". (Duncan McLeod get out of this code !)\ni don't have clear idea for your point 1. My first version of code update the /search entrypoint to add missing element, but it was not pretty.\nError return HTML page, by example and the resust was a big method not readable with \"if format == 'json' \" everywhere.\nSo i decide to split entrypoint in 2, one (the current) for the Human interaction with html templating  and the \" /api/* \" entypoint for machine interaction.\nI'm still not sur of something about the starting class. \nIf i want to start in rest-only mode ( to serve an android app for example), the actual api only is good because, it won't expose entrypoints that you won't use. The same way if you don't want to expose api.\nBut if you want both ?\nIt's really depend  on how you want to use searx engine and i don't know if flask allow to enabled/disabled entrypoint by settings.\nI will try to modify webapp.py to be able to use my version of search.py and merge rest-server.py in it.\n. ",
    "roscopecoltran": "Is the JSON API a feature officially added to searx ? it would be awesome and flexible. Awesome, my bad, I missed this part.\nThanks a lot !. Hi guys,\nHope u are all well !\n1. Pre-cache results (JSON format) for a large list of queries/keywords\nIs there any way to queue some queries and to refine the engines types based on the topology of the input. \nSome questions to u guys ^^:\n- Should I use celery, async.io, airflow ?\n- How to limit the number of parallel jobs ?\n- Would it be worth to add also a graph for engines in order to refine results ?\nI will use some AI/Machine Learning in order to dispatch the serialized input to all relevant endpoints/micro-services.\nExpected flow\n./Search \"tcp://:88888\"\n./VideoSearch \"tcp://localhost:8891\"\n./RecursiveSearch \"tcp://:8890\" \"tcp://localhost:9995\"\n./WebSearch \"tcp://localhost:8889\" (SEARX)\n./ImageSearch \"tcp://localhost:8892\"\nResulting in the following service tree:\nSearch\n          /      \\\n  VideoSearch   RecursiveSearch\n                  /      \\\n         WebSearch   ImageSearch\nDaily/Weekly digest / Chatbot\nUltimately, I would like to pre-cache some results and send to myself some alerts or to integrate it to chat bot.\nHave a good one !\nCheers,\nRichard\n. The closer architecture I found from my ideas is this project elasticfeed but it is not maintained and not as flexible as searx.\nrefs:\n  - https://github.com/feedlabs/feedify\n  - https://github.com/feedlabs/elasticfeed\n  - https://github.com/feedlabs/elasticfeed-plugins\n# server-engine:\n\n# Internal workflow:\n\n# structure:\n\n# Plugins management:\n \n. ",
    "GamingBacon": "Hmm, no that didn't fix it.\n. That did it! Woo!\n. ",
    "nettlebay": "Hello,\nsame problem with Ubuntu and Slimjet Browser (Chromium based). It can be fixed by changing Method: \"POST\" by \"GET\" in \"preferences\" \n. Thanks\nBut I guess this is possible only with my own Searx? \nBut I use only the standard Searx: searx.me\n. OK, thanks!\n. OK! Thanks\n. ",
    "vn971": "@pointhi here is the link I used:\nhttps://posativ.org/search/?q=http%20redirect&pageno=1&category_general\nThe behavior is the same in a clean firefox profile, so I guess the URL is enough. If there are any further questions, please write.\n. Glad that I accidentally found the bug!:)\n. Thanks!.\n. Closing the ticket: it seems like I use an old searx instance, and the bug was already fixed in the code and newer instances of searx.\nSorry for the confusion..(\n. ",
    "tristan-k": "```\n$ ls -la /home/tristank/venv/searx-ve/lib/python2.7/site-packages\ntotal 2032\ndrwxr-xr-x. 53 tristank tristank   4096  2. Okt 20:17 .\ndrwxr-xr-x.  4 tristank tristank   4096  2. Okt 19:24 ..\ndrwxr-xr-x.  5 tristank tristank   4096  2. Okt 19:25 babel\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 Babel-2.1.1.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 certifi\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 certifi-2015.9.6.2.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 cffi\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 cffi-1.2.1.dist-info\n-rwxr-xr-x.  1 tristank tristank 612543  2. Okt 19:25 _cffi_backend.so\ndrwxr-xr-x.  4 tristank tristank   4096  2. Okt 19:25 cryptography\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 cryptography-1.0.2.dist-info\ndrwxr-xr-x.  3 tristank tristank   4096  2. Okt 19:25 dateutil\n-rw-r--r--.  1 tristank tristank    126  2. Okt 19:24 easy_install.py\n-rw-r--r--.  1 tristank tristank    315  2. Okt 19:24 easy_install.pyc\ndrwxr-xr-x.  3 tristank tristank   4096  2. Okt 19:25 enum\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 enum34-1.0.4.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 idna\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 idna-2.0.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 ipaddress-1.0.14.dist-info\n-rw-r--r--.  1 tristank tristank  79659  2. Okt 19:25 ipaddress.py\n-rw-r--r--.  1 tristank tristank  75183  2. Okt 19:25 ipaddress.pyc\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 itsdangerous-0.24.dist-info\n-rw-r--r--.  1 tristank tristank  31840  2. Okt 19:25 itsdangerous.py\n-rw-r--r--.  1 tristank tristank  34323  2. Okt 19:25 itsdangerous.pyc\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 jinja2\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 Jinja2-2.8.dist-info\ndrwxr-xr-x.  5 tristank tristank   4096  2. Okt 19:25 lxml\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 lxml-3.4.4.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:24 _markerlib\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 markupsafe\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 MarkupSafe-0.23.dist-info\ndrwxr-xr-x.  3 tristank tristank   4096  2. Okt 19:25 ndg\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 ndg_httpsclient-0.4.0.dist-info\n-rw-r--r--.  1 tristank tristank    296  2. Okt 19:25 ndg_httpsclient-0.4.0-py2.7-nspkg.pth\ndrwxr-xr-x.  3 tristank tristank   4096  2. Okt 19:25 OpenSSL\ndrwxr-xr-x. 10 tristank tristank   4096  2. Okt 19:24 pip\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:24 pip-7.1.2.dist-info\ndrwxr-xr-x.  3 tristank tristank   4096  2. Okt 19:24 pkg_resources\ndrwxr-xr-x.  5 tristank tristank   4096  2. Okt 19:25 pyasn1\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 pyasn1-0.1.9.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 pyasn1_modules\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 pyasn1_modules-0.0.8.dist-info\ndrwxr-xr-x.  3 tristank tristank   4096  2. Okt 19:25 pycparser\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 pycparser-2.14.dist-info\ndrwxr-xr-x.  6 tristank tristank   4096  2. Okt 19:25 pygments\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 Pygments-2.0.2.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 pyOpenSSL-0.15.1.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 python_dateutil-2.4.2.dist-info\ndrwxr-xr-x.  3 tristank tristank   4096  2. Okt 19:25 pytz\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 pytz-2015.6.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 PyYAML-3.11.dist-info\ndrwxr-xr-x.  3 tristank tristank   4096  2. Okt 19:25 requests\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 requests-2.7.0.dist-info\ndrwxr-xr-x.  6 tristank tristank   4096  2. Okt 18:21 searx\ndrwxr-xr-x.  3 tristank tristank   4096  2. Okt 19:24 setuptools\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:24 setuptools-18.2.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 six-1.9.0.dist-info\n-rw-r--r--.  1 tristank tristank  29664  2. Okt 19:25 six.py\n-rw-r--r--.  1 tristank tristank  29162  2. Okt 19:25 six.pyc\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 speaklater-1.3.dist-info\n-rw-r--r--.  1 tristank tristank   5216  2. Okt 19:25 speaklater.py\n-rw-r--r--.  1 tristank tristank   8816  2. Okt 19:25 speaklater.pyc\ndrwxr-xr-x.  5 tristank tristank   4096  2. Okt 19:24 wheel\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:24 wheel-0.24.0.dist-info\ndrwxr-xr-x.  2 tristank tristank   4096  2. Okt 19:25 yaml\n-rwxr-xr-x.  1 tristank tristank 920947  2. Okt 19:25 _yaml.so\n$ pip install pyasn1-modules certifi\nRequirement already satisfied (use --upgrade to upgrade): pyasn1-modules in /home/tristank/venv/searx-ve/lib/python2.7/site-packages\nRequirement already satisfied (use --upgrade to upgrade): certifi in /home/tristank/venv/searx-ve/lib/python2.7/site-packages\nRequirement already satisfied (use --upgrade to upgrade): pyasn1>=0.1.8 in /home/tristank/venv/searx-ve/lib/python2.7/site-packages (from pyasn1-modules)\n$ python /home/tristank/venv/searx-ve/lib/python2.7/site-packages/searx/searx/webapp.py \nINFO:searx:Initialisation done\nINFO:searx.plugins.https_rewrite:29 rules loaded\nCRITICAL:searx.webapp:The pyopenssl, ndg-httpsclient, pyasn1 packages have to be installed.\nSome HTTPS connections will failed\nINFO:werkzeug: * Running on http://0.0.0.0:61110/\nINFO:werkzeug: * Restarting with reloader\nINFO:searx:Initialisation done\nINFO:searx.plugins.https_rewrite:29 rules loaded\nCRITICAL:searx.webapp:The pyopenssl, ndg-httpsclient, pyasn1 packages have to be installed.\nSome HTTPS connections will failed\n```\n. ",
    "EpisteX": "Thank you for your answer !\nBest regards.\n. ",
    "luc--": "You just need to do a little modification in the html code of the template you're using. By adding a target attribute to a tag with the value \"_blank\", it makes the click open a link.\nAssuming you're using the default template, you just have to replace : \nhtml\n<a href=\"{{ result.url }}\" rel=\"noreferrer\">{{ result.title|safe }}</a>\nby : \nhtml\n<a href=\"{{ result.url }}\" rel=\"noreferrer\" target=\"_blank\">{{ result.title|safe }}</a>\nin templates/default/result_templates/default.html\nPS : You can also add this attribute to every link of your template that you want to be opened in a new tab.\n. It seems you're right :)\nIt's however (perhaps) not hard to imagine to let the user choose what he wants in his preferences. I'll see that when I find some time for it... but as I've never work on searx it will certainly take me some time !\n. ",
    "BARETC": "I like to use a direct click with an open of a new tab and to keep the request page with its result in the original tab (for example like https://www.ixquick.com/)\nIt is possible by add inside the \nIs it posible to add an option to choose if you want add this option?\nSo the people like me that prefer to keep the original request tab and to open a new tab can choose this option\nAnd the people which prefer to not open a new Tab can not use this option\n. ",
    "erdmark": "The user can choose to open links in new tab by going to\npreferences -> General -> Results on new tabs\nBut it would be useful to be able to set the default value in settings.yml. I translated the about page yesterday so it is no final version, you can see it here: https://suche.ftp.sh/about. Ip addresses are considered to be personal data so except those engines that are run as a hidden service we have to process ip addresses in order to respond to search questions.\nAdditionally many hosters store logs with at least shortened Ip-addresses and urls with timestamps in it.\nBased on datenschutz-generator.de I created a privacy statement and adapted it for the use with my hoster Uberspace.\nabout20180518.txt\nWhen copying something into about.html be sure to use \"normal\" ASCII Characters, things like \"\u00f6\" or \"\u00df\" are not supported by Searx. After that restart Searx.\n. ",
    "songproducer": "I opened Yacy and searx chat rooms on Matrix:\nsearx:monero.fun\nyacy:monero.fun\nYou'll need a client like Riot.im. Now after configuring the document root in /etc/apache2/sites-enabled/000-default-le-ssl.conf\nI get the searx directory files showing up. Solved by replacing the default code in file /etc/apache2/sites-enabled/000-default-le-ssl.conf with  the suggested Apache Reverse Proxy with SSL code.. @asciimoo Sorry, how can I check my proxy config?. ",
    "jpcaruana": "Hi, \nthanks for sharing your opinion.\nIt is very easy to change an image: just replace it once and you're done. I think it would be great to be able to change the name on every pages by changing just once. A lot of open source sofware allow personalisation and they are perfectly fine with it (see wordpress for one example). I also think this is a better design to template (almost) everythink than to have hard coded values.\nIf you look at https://framabee.org/, there is no mention to searx everywhere like there is on the default theme (but you still have the \"powered by searx\" link at the end) - they removed the name everywhere and this instance is popular, and everybody knows it is searx powered. (I think your car analogy is unrelated to my suggestion.)\nps: marrant ton pseudo\n. ",
    "minami-o": "In the meantime, I realized that you already created a function to recover the base_url, get_base_url.\nIt solves the issue in the two mentioned cases (at least) in a more elegant way:\n\n\u00a0\u00a0\u00a0\u00a0resp = make_response(redirect(get_base_url()))\n. \n",
    "eht16": "That would be nice.\nI often use the primary clipboard (select some text and paste it with middle click) but if you have already text in the search box you cannot clear it by selecting it as it would override the previous clipboard content.\nI guess such a button should be easy to implement, probably in JS only?. ",
    "Gravefield-l": "Thx for your help. I still have a problem but it's a little different :\nhttp://sebsauvage.net/paste/?f7c48165329ffa9f#kf/nm0q4UjtLv0CytNJwl40UbQK2cjoUy+5QPbRdgrk=\nWhat can I do?\n. ",
    "teresaejunior": "Hi, @xinomilo! The Chrome profile was old, but the Chromium one is totally new!\n. I don't know if this is related, but the only thing I could find about the ERR_CONNECTION_RESET error for specific sites on Chrome in Debian, is this page talking about mod_spdy: http://blog.php-function.de/err_connection_reset-mediawiki-im-chrome-nach-debian-update/\n. On Chromium 46 it works OK!\n. OK, thank you for the fix!\n. ",
    "jason-speck": "I had the same issue, but only for google -- other engines worked fine.  The reporters workaround also \"fixed\" it for me.\n. ",
    "virse": "I tried it with make clean but the error is the same.\nOr have I misinterpreted something?\n. Hi asciimoo,\npython2 -c 'import requests; print requests.version' shows me\n2.2.1\n. Hi Asciimoo,\nsorry for pushing but should another version of the request package be installed?\nAnd if so how can I fix it?\nThx a lot!\n. Hi pointhi,\nthx for your help.\nI've installed requests version 2.8.1. \npython2 -c 'import requests; print requests.__version__'\n2.8.1\nI could now access Searx on https://searx.abc.de but when I start a search query then a 500 error in the browser appears.\nI'm using an Apache 2 as webserver and use the config as mentioned in the installation guide\n```\nServerName searx.abc.de\nSSLEngine on\nSSLCertificateFile /etc/ssl/certs/abc.de.crt\nSSLCertificateKeyFile /etc/ssl/private/abc.de.key\nSSLCertificateChainFile /etc/ssl/certs/ca-bundle.crt\n\n    Options FollowSymLinks Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n\n```\nDo I need to change anything in the /usr/local/searx/searx/settings.yml (Server part)\nserver:\n    port : 8888\n    bind_address : \"127.0.0.1\" # address to listen on\n    secret_key : \"1212\" # change this!\n    base_url : False # Set custom base_url. Possible values: False or \"https://your.custom.host/location/\"\n    image_proxy : False # Proxying image results through searx\nI alreayd tried port 443, bind-address localhost and abc.de.\nAlways the same error.\nThx again for helping me out!\n. Hi,\nuwsgi brings me in the log this one (apache logs are all \"clean\"):\nSSLError: bad handshake: Error([('SSL routines', 'SSL3_GET_SERVER_CERTIFICATE', 'certificate verify failed')],)\nIt's a self signed certificate. Could this be the cause of the error. and when so what can I do to fix it? Is there any config-file in searx where I can tell searx (or maybe uwsgi) to skip checking the self signed certificate?\nThx again!\n. Hi pointhi,\nI figured out that my installation is working when i disable Google as Search engine that caused this error:\nSSLError: bad handshake: Error([('SSL routines', 'SSL3_GET_SERVER_CERTIFICATE', 'certificate verify failed')],)\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /do/suggest?query=dgfgfg HTTP/1.1\" 200 0\nINFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): www.google.de\nWhen I googled ;o) I found this:\nSecurity: Verified HTTPS with SSL/TLS\nVery important fact: By default, urllib3 does not verify HTTPS requests.\nSo is there any possibility to make google https-search accessable via searx? Other than using\npip install certifi\nThx a lot!\n. For those who have the same problem just do \npip install certifi==2015.04.28\nand activate Google afterwards in your (personal) settings.\n. I'm not sure why, but pip was missing. After i did a reinstall of pip, everythin is up agaiin.\ngreets. I did a fresh installation of Searx and everything works now again.\nBG. ",
    "jibe-b": "Right, in the 'Links' pannel\n\nDownload results\n[csv][json][rss]\n\nclicking on the rss 'button' returns a RSS/XML file but does not open the same page as the one obtained with https://searx.me/?q=python&format=rss\nIn case the 'rss' button is intended to return this RSS/XML page, I'd suggest to add a 'Subscribe' button that sends to the https://searx.me/?q=python&format=rss page. One reason is that the 'Subscribe now' button is not by default on the UI of browsers.\nThanks anyway for this way to subscribe to an rss page for the moment.\n. In order for a browser to identify there is an RSS feed on the page, there is the need to have a link in the head. source: https://www.xul.fr/en-xml-rss.html#Browser\nIt might also be needed that this link and the one you click on (rss button in the right pannel) be exactly matching. source: just a guess, one should try.. @asciimoo, I try to aggregate as much information as possible to make it \"let's read the spec, let's implement it\" in case there is conscensus in adding this engine (and it could be me who implements it, indeed). What information lack?\nBy the way, I  sent a message to the BASE team.\n. Unfortunately, BASE requires authentication. I will discuss with them this point, even though I already guess possible reasons for locking. But I think there should not be neasons for keeping it locked.\nAnyhow, there are other search engines that index Open Archives:\n- OpenAire.eu , a European Union funded harvester and working group\n  limited API doc\n- OAISTER by worldcat.org \nAPI doc\nbut BASE seems the most mature.\n. Here is a first try: https://github.com/jibe-b/searx/blob/master/searx/engines/base.py\nI have not yet tried it.\n1. parsing could be done better. But it seems that results tree has a few flavors. Is there a document describing the result tree? @pietsch \n2. I have not yet found where in searx's code engines are listed to show up in the Preferences tab @asciimoo.\n. thanks @asciimoo. \n\n\"Internal Server Error\nThe server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.\"\n\nas I expected it.\n. good idea.\nI am looking at what the engine must return to satisfy the expectations from the main engine.\n. nice debugging interface :) @asciimoo \n. Good news, it basically works!\nAt least it does on my computer, but I added searx.useragent() in the parameters list to match the exception on BASE server.\nTo be fixed:\n- dates are not returned in a standard format (would you consider improve this server-side @pietsch?)\n- only the 10 first results are returned (the answer is probably in the API doc)\nYou were right @asciimoo, it wasn't really hard. What take some time is to dive into searx's code, but now I have an idea of the structure of the code.\n. @pietsch For the moment, I have seen  either in a %Y-%m-%d %H:%M:S format or %Y and searx takes only the former (I still need to check the exact date format expected by searx).\nI guess they are stored this way after harvesting, as replacing 2014 by 2014-01-01 00+00 does not have the same meaning. Depending on your answer, I may write a function that does the job of adding this pseudo-information.\n. @pietsch, how should I write the http request in order to obtain more than 10 results?\n. Dear @pietsch and @asciimoo, the base engine works on my machine :)\nhttps://github.com/jibe-b/searx/blob/master/searx/engines/base.py\nBASE returns published dates as %Y or %Y-%m or %Y-%m-%d so I simply added exception handling of datetime.strptime().\nDo I make a pull request?\n. @pietsch in its current state, the engine enables to perform basics search.\nNext, I will try to add regex search on the query in order to enable faceted search like \"author:Einstein -> in URL dccreator=Einstein\"\n. @pietsch though I am an open access advocate, I chose not to add OA boost in the engine (I consider adding a facet-like oa:true or maybe as a searx plugin). Is it wise or would you advise OA boost by default?\n. Ok @pietsch, I agree that the BASE default should be kept.\nThe API specification indicates \"Default: without boosting\" (p4). I will add oa boosting to the engine.\n. It is @pietsch! I was mistaken. The API has OA boost deactivated by default, and the web interface activates OA boost.\n. Thanks @pietsch! I'll try this evening on another machine/IP.\n. @asciimoo I cleaned my code.\nLast build failed with\n\nThe command \"./manage.sh pep8_check\" exited with 1.\n1.90s$ ./manage.sh styles\n[!] Building styles\nThe compress option has been deprecated. We recommend you use a dedicated css minifier, for instance see less-plugin-clean-css.\n\nwhich is related to older code.\n. Thanks for explaining in detail, you're right I didn't read the build log carefully enough. I am learning using travis while doing.\nI fixed this line.\n. It seems to be an old bug in the datetime module that can be solved either in altering datetime () \nhttp://stackoverflow.com/questions/10263956/use-datetime-strftime-on-years-before-1900-require-year-1900/32206673#32206673\nstrftime() is used in searx/webapp.py as well so I agree the exception handling. When webapp.py will support dates earlier than 1900, be it by using another module or by patching datetime.strftime, I will do the same in the base.py engine.\n. @kvch is it better to make the base.py return publisedDate only when greater than 1900 or to add the exception handling in webapp.py?\nThe first option limits changes but the second prevents this from happening with other engines.\n. ok, I'll do it. Just let me time to understand how webapp.py works.\n. @kvch what should publishedDate take as a value in case the exception is raised?\n. @kvch I have been using the engine during the last week for basic search, but I know the queries I made were in the bubble of my current work. Are there manual tests I should consider doing?\n. @kvch that's right, this time it was 42 steps, probably less next time ;)\nI guess I make a new pull request?\n. sorry @asciimoo, which note?\n. Dear @asciimoo and @kvch it's ready!\nit's rebased, commits squashed into two commits (though there is too much in the commit secondary text as I don't master the rebase command yet).\n. Thanks @asciimoo! I learned while doing this :)\n. @asciimoo How do you proceed in the diffusion of information about Searx?\nThe BASE engine may interest people in academia and I want to spread the news in the circles I know (young researchers in France).\nMaybe is it better to wait until a new release so that existing instances integrate BASE.\n@pietsch, would you participate in advertising to academics \"a (meta)-search engine with both normal search tab and scholar search tab\"?\n. @pietsch, I consider communication within a month. And I will discuss with others about it on the HackYourPhD fb group:\u00a0https://www.facebook.com/groups/499463776745202/ (and this PR can be closed)\n. Hi @asciimoo and all!\nThe next release could be released soon.\nI would like to let you know that I will be happy when it's released in order to advertise Searx to Scientists in order to increase the adoption of Searx in this community.\nThe communication plan is under construction in this pad, feel free to have a look and eventualy to add yor thoughts!\nBest regards,\n. Time goes by and this idea does not bring much attention, so let's say it's not for this time.\n. @GreenLunar, I have to admit I\u00a0don't understand your message. I\u00a0was thinking about the Science Sprint and the SOHA project mostly (and the use of Searx for Open Science).\n. ping myself to do it.. Now that I wrote the idea I was having, I see clues of this being at least partly achieved: https://github.com/asciimoo/searx/blob/4cffd78650c3f1dfce413ae0a1cd0453ebe6f277/searx/plugins/doai_rewrite.py\n. Scholarpedia doesn't provide an API, I started writting the engine and parse the html.. Modifying the mediawiki engine I don't get timeouts (maybe because I haven't finished to modify it ;) )\nbut I see that I do get results using the search interface but none using the API.\nFor example:\nhttp://www.scholarpedia.org/w/index.php?search=synteny -> one result\nhttp://www.scholarpedia.org/w/api.php?action=query&list=search&srsearch=synteny&format=json&sroffset=0&srlimit=10&srwhat=nearmatch -> no result\n. As the API seems not to be activated, I'll mimic what is done in this engine https://github.com/asciimoo/searx/blob/master/searx/engines/google.py. Error still appears. Failing to fix error.. Thanks for the pointer to the guidelines, I had forgotten since last year ;)\nRobot tests fail with:\n\nWebDriverException: Message: Reached error page: about:neterror?e=connectionFailure&u=http%3A//localhost%3A11111/missing_link&c=UTF-8&f=regular&d=Firefox%20can%E2%80%99t%20establish%20a%20connection%20to%20the%20server%20at%20localhost%3A11111.\n\nI may need help to understand why. I tried downgrading to Firefox 45, with no result.\nI tried changing all http to https, not better.. Cleaned up code in my repo and better open a new PR:\nhttps://github.com/asciimoo/searx/pull/1047. achieving #1036. I was lost with mess in my repo so I open a new PR.\nhttps://github.com/asciimoo/searx/pull/1045. New PR at https://github.com/asciimoo/searx/pull/1046. The point made by @asciimoo solves the question for CORE (I'll make it a non authenticated engine).\n@kvch \n\nDoes CORE worth it to leak that you are using a searx instance for searching?\n\nI consider that CORE would have been worth the compromise, as they are non-profit and working to increase availability to full-text Open Access scientific publications. This should indeed be debated.\nYet, whenever there is a leak of privacy (activating in preferences an engine with an API key for example), there should probably be a banner on the Searx instance, so that users know such engines are activated.\nHowever, I will raise the question again for other possible engines returning scientific publications. The biggest and most used indexes require authentication to access to their results. Although I don't agree with their policies, having these engines available as Searx engines may increase the usage of Searx in the scientific community.\n\nHow would the API keys be distributed? Would we have one for all of the instances or a key for every instance?\n\nThis is a question I asked to the CORE team. The engine won't be authenticated in the end, so we don't need to answer it. I had ideas but I should test them before presenting them.\n\nIf one is available for all of the instances, the key would be leaked, so their authentication would be useless.\n\nI don't realize at which point the key would be visible to users. Can you help me?\n\nHowever, if every instance has one key, the instance is leaked to CORE. Also it raises more questions on distributing the keys.\n\nSomehow, the API key would duplicate the already available information carried by the IP of the Searx instance.. The unauthenticated access to the data might be a bug.. It can surely be done. I hadn't checked json_engine so from now I'll use json_engine whenever possible.. Thanks for opening the discussion on query interpretation @siennk!\nI would add the author,  journal, publisher keywords supported by specialised (here academic publications) search engines. It is already done in BASE, even though way too basic https://github.com/asciimoo/searx/blob/master/searx/engines/base.py\nMy opinion would be to implement the first case first (I can do it for engines in category science), but a real query interpretation wouldqbe of a high value for Searx.\nIn particular if queries are interpreted by Searx, then would be faisible to check wether a query makes a good SPARQL query and pass it to wikidata. This is also a huge task and should not be considered before the query interpretation is implemented but I'd be glad to help implement it!. It would be about showing annotations that have been made on existing content.\nThe content of these annotations is valuable in itself (hence the engines should be used in at least one other category, such as science for publons)\nand my suggestion is about adding an entry to the user to search in comments that were made on other content. This is inspired by the wish of people at hypothes.is, in scientific peer review groups and W3C, to make annotation of content more adopted on the web.. ok @dalf!\nIt will make sense in category science too, as it is also intended to highlight the work done by developers for science.\nIt's a bit more work but I like the idea of showing infoboxes for package.\n. @Pofilo well I didn't do anything, hope to have time before September\u2026. Thank you @kvch!. I'll \"review\" the changes you made and learn from them :). Well, openairepublications and openairedatasets fail\n\nFile \"/searx/searx/engines/json_engine.py\", line 102, in response\n    url = query(result, url_query)[0]\nIndexError: list index out of range\nERROR:searx.search:engine openairepublications : exception : list index out of range\n\n@asciimoo Any hint on where the configuration of the json_engine would fail?. Thanks for doing this quickly but it still fails (indeed after pulling to last commit 181f1c6)\n\nERROR:searx.search:engine openairepublications : exception : list index out of range\nTraceback (most recent call last):\n  File \"/searx/searx/search.py\", line 118, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params, start_time, timeout_limit)\n  File \"/searx/searx/search.py\", line 110, in search_one_request\n    return engine.response(response)\n  File \"/searx/searx/engines/json_engine.py\", line 105, in response\n    url = query(result, url_query)[0]\nIndexError: list index out of range\n. @asciimoo it's working now\n\n\nBut quite slow\u2026\n\n(5sec timeout was not enough). I really like this idea!\nPlease ping me if nobody advances on this within three weeks please.. This issue seems to be solved:\n!wp :en searx\n\n:en !wp searx\n\n. ok, I read too fast.\nI'll let @asciimoo or others solve this ;). done in next commit\n. You're right, as Searx does not need a date.\nstill work to be done.\n. done in next commit\n. copy-pasting side effect ;)\ndeleted in next commit\n. ok.\npublishedDate is optional in next commit. How to declare in parse?\n. done in next commit.\n. Ok. done in next commit.\nThanks for explaining in detail.\n. elif done in next commit.\n. done in next commit.\n. @kvch This goes further than what I propose to add to Searx, so I will continue in a new branch base.py in this issue.\n. @kvch Just didn't want to be messy doing several things at a time.\nSo when we agree on base.py, I will merge it into this pull request\n. done in next commit.\n. It is the case. It is quite usual to deal with urllib2 and urllib.request but as engines are not supposed to make requests by their own, it has not been used until now.. I will make it configurable.\nAre the parameters in the object\n\n\nname : pubmed\n    parameter : True\n\n\naccessible from the engine, i.e. callable without having to parse settings.yml and match the item with the good name.\nOtherwise, I'll simply make it a parameter outside of the engine, but it will be less maintainable.. I finally don't add a parameter in settings. Only display DOI when available. And if a plugin handles DOIs, then it can be made clickable.. OA stands for Open Access, i.e. scientific publications that are at least available on the web without a paywall (webpage handling payment of a fee to access another webpage).\ndoia.net is one DOI resolver service, oadoi.org, doi.org are others. The plugin should have a name that is generic enough.\nThe plugin could be renamed to doi_resolver_rewrite too.. Sure!. ",
    "loganmarchione": "Are those accurate? I didn't see the searx-ve folder anywhere....\nOn December 19, 2015 4:22:59 AM EST, Thomas Pointhuber notifications@github.com wrote:\n\nHy, try to use this instructions for installation:\nhttp://asciimoo.github.io/searx/dev/install/installation.html\nYou have to note, we updated the dependencies, and also changed the\nlogical structure of settings.yml. Probably this is causing the\nproblems.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/asciimoo/searx/issues/471#issuecomment-165963378\n. No, I thought those instructions were no good because the searx-ve folder wasn't there in the first place for me to run these commands.  \n\nsudo -u searx -i\ncd /usr/local/searx\nvirtualenv searx-ve\n. ./searx-ve/bin/activate\npip install -r requirements.txt\npython setup.py install\nOn December 19, 2015 9:06:28 AM EST, Thomas Pointhuber notifications@github.com wrote:\n\nhave you created a virtualenv when installing searx?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/asciimoo/searx/issues/471#issuecomment-165987182\n. I tried to blow it away and start fresh using the recommended way, but I'll try it fresh with the virtual environment.\n\nOn December 19, 2015 10:04:07 AM EST, Thomas Pointhuber notifications@github.com wrote:\n\nwhen you look on the install instruction, there is the command\nvirtualenv searx-ve. When you installed searx in another way, the\nupdate instructions won't work\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/asciimoo/searx/issues/471#issuecomment-165994661\n. I tried again with the instructions you linked from a fresh install and it worked. I didn't realize I needed to use special instructions to update.\n\ncd /usr/local/searx\nsudo -u searx -i\n. ./searx-ve/bin/activate\ngit stash\ngit pull origin master\ngit stash apply\npip install --upgrade -r requirements.txt\nsudo service uwsgi restart\nHowever, when I search any pages, I'm now getting this message.\n```\nInternal Server Error\nThe server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.\n```\nBelow is the log from /var/log/uwsgi/app/searx.log. My site is here. I don't think the SSL error is on my end, is it?\nWARNING:searx.search.google engine:cannot fetch PREF cookie\nERROR:searx.webapp:Exception on /autocompleter [GET]\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/usr/local/searx/searx/webapp.py\", line 501, in autocompleter\n    raw_results.extend(completer(query.getSearchQuery()))\n  File \"/usr/local/searx/searx/autocomplete.py\", line 145, in google\n    + urlencode(dict(q=query)))\n  File \"/usr/local/searx/searx/autocomplete.py\", line 34, in get\n    return http_get(*args, **kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 82, in get\n    return request('get', url, **kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 75, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/adapters.py\", line 447, in send\n    raise SSLError(e, request=request)\nSSLError: bad handshake: Error([('SSL routines', 'SSL3_GET_SERVER_CERTIFICATE', 'certificate verify failed')],)\nERROR:searx.webapp:Exception on / [POST]\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/usr/local/searx/searx/webapp.py\", line 382, in index\n    search.search(request)\n  File \"/usr/local/searx/searx/search.py\", line 294, in search\n    engine.request(self.query.encode('utf-8'), request_params)\n  File \"/usr/local/searx/searx/engines/google.py\", line 209, in request\n    params['cookies']['NID'] = get_google_nid_cookie(google_hostname)\n  File \"/usr/local/searx/searx/engines/google.py\", line 149, in get_google_nid_cookie\n    resp = get('https://' + google_hostname)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 82, in get\n    return request('get', url, **kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 75, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/adapters.py\", line 447, in send\n    raise SSLError(e, request=request)\nSSLError: bad handshake: Error([('SSL routines', 'SSL3_GET_SERVER_CERTIFICATE', 'certificate verify failed')],)\n. @THS-on, same thing here. Just tried turning off Google search (left Google autocomplete on) and it worked fine. I'll use DDG for now.\n. @THS-on and @asciimoo, same results here. Just updated via these instructions. The 500 error is gone, but even with Google enabled, I get no Google results and the Google bang (!) does not work.\n\nLatest version fixed it for me. Close?\nEdit: This just fixed the 500 Error not the search itself. The search returns nothing when I use !go\n\n\n. @asciimoo and @misnyo, it works, thanks!\n@asciimoo, the reason I opened this issue initially was because there are three sets of installation instructions (below), and only the third worked for me. Which set is the right set? Are 1 and 2 supported anymore? Should the README be updated?\n- #1\n- #2\n- #3\n. I like it, thanks! It was just a little misleading that there were two installations on one page.\n. ",
    "THS-on": "I can confirm this issue. I'm running searx on Ubuntu 14.04 Nginx and have the same error message. \nIt is a problem with the google search because when I disable google search all is working fine.\n. Latest version fixed it for me. Close?\nEdit: This just fixed the 500 Error not the search itself. The search returns nothing when I use !go \n. @asciimoo on which system you are running searx and what python version?  I just did a fresh install on my server and have the same error as before. This is a log from my local install on Linux Mint 17.3:\nERROR:searx.search:engine crash: google\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/search.py\", line 38, in search_request_wrapper\n    return fn(url, **kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 82, in get\n    return request('get', url, **kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 75, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/adapters.py\", line 447, in send\n    raise SSLError(e, request=request)\nSSLError: bad handshake: Error([('SSL routines', 'SSL3_GET_SERVER_CERTIFICATE', 'certificate verify failed')],)\nEdit: Location of the server is France maybe that helps. \n. I just started a docker container from https://hub.docker.com/r/wonderfall/searx/ and this container is working fine. Maybe the issue is related to the Ubuntu installation because the container uses Alpine Linux.\n. ",
    "Liandriz": "Same here, before 0.8.1, got 500 errors on Google searches, now, blank result page.\n. ",
    "kujiu": "It's working fine with 0.8.1. Thanks for all.\n. ",
    "phaleon2": "I decided to to set a higher timeout  ( 8 seconds ) and I disabled duckduckgo in setting.yml in my own instance for now. \n. ",
    "hg": "@asciimoo this one can be closed I think, since searx now sorts files by the number of seeds.\nSorry for necroposting, but it does feel nice to see that issue count drop.. ",
    "glogiotatidis": "Good job @Wonderfall. I like alpine and more flexibility with env variables. Also I prefer more docker cmds for readability and cachability. \nThanks for pinging me @asciimoo \n. Nice! Thanks for updating :)\n. Since we're going for optimized dockerfile lines 41, 43 and 44 can be moved at the top, e.g. after line 4, to take full advantage of docker caching even when packages change.\n. :+1:  for extra settings\n. I cannot really see why having less layers is \"always preferred\". IMO this is less readable and it loses when it comes to image rebuilding. \nadduser, pip, sed and chown commands have nothing to do with the  apk command, are small and fast commands and can be separated into multiple layers.\nAlso if you break them into multiple cmds you can move COPY after the package install and therefore reuse the package install layers on new searx versions\n. Since BASE_URl and IMAGE_PROXY are env variables, these sed commands should be moved in a start up script executed on CMD so users can override them with docker run -e on run time.\n. :+1: nice! I generally go with debian:stable these days instead of python b/c I usually need packages not available in alpine. But in the case it's a perfect match.\n. ",
    "Wonderfall": "Thanks for your clever comments, @glogiotatidis, I'll take a look.\n. I made some changes, thanks to @glogiotatidis's suggestions.\nNow it should take more advantage from Docker caching, resulting in faster builds, but I wanted to keep the same size (I mean it could be even more optimised for caching, I know). \nWhat do you think? I'm still open to suggestions!\nEDIT : Sorry for multiplying commits, I'm not confortable with Git yet! Hope it's fine now.\nBy saying \"it could be even more optimised\", let me explain myself : pip and apk could be separated (may be better for caching), but if I do so, I can't delete unnecessary packages (~100MB wasted).\nhttps://pix.schrodinger.io/#nVrA0l9CILRmqGqt-m5o8w\n. Thanks for correcting. :+1: \nBut COPY requirements.txt . works for me (I just builded). I see no reason why it shouldn't.\nWhere did you get this? What's your docker version?\nOtherwise, I want to suggest something. What do you think about making a separate run.sh file ?\nDockerfile : https://pix.schrodinger.io/#V1LdhPcpQS8V7Zetieuhwg\nrun.sh : https://pix.schrodinger.io/#yPBW4Wr8PwDOpkYg3HJkJw\n(Just a suggestion.)\n. Okay, this is interesting to know. I'm using Docker 1.10-dev and overlay as the filesystem used by docker (Debian Jessie + Linux 4.4). I'll try to reproduce your issue.\n. A friend of mine tried with Docker 1.10.0 + btrfs on Jessie, it worked.\nI still don't know why he gets this message... but if it solves his problem, we can consider this.\n. How did you install searx with Docker?  If you want to do so, you have to clone this repository, modify your files, and then build yourself from the embedded Dockerfile.\nAnother, not recommended option : you can use docker exec -ti searx_container_name sh to modify your files directly within the container.\n. Pulling from Docker Hub is OK, but you shoudn't customise searx files with this method.\nEverything is already builded and all of your changes will be lost after recreating the container.\nThis is what you should do instead : \ngit clone https://github.com/asciimoo/searx.git && cd searx\nThen, you do your things... and once you're done : \ndocker build -t whatever/searx .\nVoil\u00e0, you'll get your own Docker image of searx. Run a new container with : \ndocker run -d --name searx -p $PORT:8888 whatever/searx\n. No it doesn't matter, you can even remove this image.\ndocker rm wonderfall/searx\n(you can list downloaded images with docker images and running containers with docker ps)\nWhenever you want to update, you have to build a new image and recreate the container.\ndocker build -t whatever/searx .\nHere we build a new image, the old one is not referenced as whatever/searx anymore.\ndocker stop searx && docker rm searx\nWe remove the running container.\ndocker run -d --name searx -p $PORT:8888 wonderfall/searx\nWe run a new container from the new builded image.\nAs far as I know there's no other way! docker-compose might help you too. Also don't forget to remove your old images, list them with docker images and remove them with docker rmi.\n. Your local changes are unchanged, you're just building a new image in order to include them.\nI advice you to read this : https://docs.docker.com/engine/userguide/dockerimages/\n. I'm convinced that a hidden service can be part of a solution. \nBoth server's identity and client's identity are masked.\nI've done that, it's very easy to setup. And as I can see, there's already some \"public\" hidden services of searx available. \n. You're saying you built an image named whatever/searx. But you ran a container with the image wonderfall/searx, which exists on Docker Hub, so Docker will download and then use this image instead of the one you've just built.\nAm I right or did you make a mistake when describing your problem?\n. Rebuild the image and recreate the container, it's as simple as that.\n. What is the output of docker logs searx ?\n. The Dockerfile needs an update. Perhaps I'll do it this weekend.\nIn the meantime you can use or build my image wonderfall/searx.. This fix should be preferred. Alpine version can also be updated to 3.5.. The fix should be merged, this message appeared a while ago. I still can't see why the removal is required but this is it... Searx still works but some search engines such as Google are unavailable due to HTTPS errors. Now it's resolved with this fix.. https://github.com/asciimoo/searx/pull/865 (you can close this)\nThe credit goes to him too. :). I don't think HPKP should be there, it's a real headache while using LE certs, and very risky to use.\nDNSSEC + DANE should be preferred to HPKP.\nAt least I agree HSTS is a must.. Just so you know, I recently updated my Docker image (wonderfall/searx).\nSearx 0.15 + Python 3. I'm planning on making a PR for the official Dockerfile.. This is totally right, this is what I do for my automated build : the key is generated the first time the container is started. It's not a problem here since the Dockerfile should be manually built : a unique secret key is generated during the build time. But I think like you, that moving this generation to run.sh brings more flexibility.\n. ",
    "GRMrGecko": "Interesting, it is working now. Thank you.\n. ",
    "misnyo": "Hello,\ncan you give me the version of openssl and certifi you are using?\npip freeze|grep certifi\necho -e \"import ssl\\nprint ssl.OPENSSL_VERSION_INFO\\n\"|python\nThe temporary workaround should be to upgrade the certifi package, since the requirements.txt doesn't contain versions, and I guess you have an obsolete version of certifi, which is not upgraded for some reason(which it should).\n. Either try to run it manually, (eg python searx/webapp.py) and you can see it in standard output, or you should see in the uwsgi logs or similar depending on your setup.\nMy guess would be that the requests are timing out for the first few times, so another way to debug it is to increase the timeout in settings.yml and see if it works.. You should find something like this in the debug log:\nDEBUG:requests.packages.urllib3.connectionpool:https://www.google.com:443 \"GET /search?q=searx&start=0&gws_rd=cr&gbv=1&lr=&ei=x HTTP/1.1\" 200 None\nTry to reproduce it with curl and a proper user agent, and find out how long the request takes and if there is an error in the result.\nIt is hard to find the problem at this point, it could be a networking error, ISP or QoS. Best practice is to run searx in a server environment, not on a desktop for reliable networking.. Does only google search take that long?(you might try it with !go [searchterm] to only use that one)\nDo you have enough memory in the server?(if not, it might happen that the application goes into swap, but I don't think it is the issue here)\nIf the http(s) query timeouts, it most likely have nothing to do with the code itself imo, I have no better idea than checking if everything is ok with the environment.. Also, you might try an alternate dns server, but that's just a hard guess.. Sorry for the accidental close of the PR, will update with the int_or_zero issue soon.. I guess this should be good for now;). @siddarth9 it depends on your webhosting, but generally it is advised to install searx on a vps with shell access. Does your webhosting provide that?. I'm not sure about if searx can be installed on cpanel, but I would go for the vps root access, and follow the instructions on the link provided by @dalf ;). I'll take a look on this soon.. @hulekgre what installation method have you followed?\nhttps://asciimoo.github.io/searx/dev/install/installation.html this one?\nWhat OS do you use?. Honestly I've never tried it on windows, and am not aware of anybody else, but I might give it a shot during the weekend. What you can also try in the meantime is to install it on a virtual machines with some linux, that's more preferred afaik.. Hi, you can find more info about how the engines work at https://asciimoo.github.io/searx/dev/engine_overview.html\nBasically the first you could try is copying the file, and changing this line: https://github.com/asciimoo/searx/blob/master/searx/engines/youtube_noapi.py#L23. In my tests, it uses the same ip for one request \"chain\".. ",
    "rototom": "No succes here, after changing those files.\nStill no results from google. (https://search.klein-hitpass.net)\n. `DEBUG:requests.packages.urllib3.connectionpool:\"GET /search?q=test&start=0&gws_rd=cr&gbv=1&lr=lang_de&ei=x HTTP/1.1\" 302 411\nDEBUG:searx.search:google redirect on: \nINFO:requests.packages.urllib3.connectionpool:Starting new HTTPS connection (31): ipv4.google.com\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /sorry/IndexRedirect?continue=https://www.google.de/search%3Fq%3Dtest%26start%3D0%26gws_rd%3Dcr%26gbv%3D1%26lr%3Dlang_de%26ei%3Dx&q=CGMSBAXmk9gYxIb9uQUiGQDxp4NL7jwCFAxC-NqkYPQbyffrQiXKuxA HTTP/1.1\" 503 3267\nERROR:searx.search:engine crash: google\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/search.py\", line 40, in search_request_wrapper\n    ret = fn(url, _kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 82, in get\n    return request('get', url, _kwargs)\n  File \"/usr/local/searx/searx/poolrequests.py\", line 75, in request\n    response = session.request(method=method, url=url, _kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n    resp = self.send(prep, _send_kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 597, in send\n    history = [resp for resp in gen] if allow_redirects else []\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 195, in resolve_redirects\n    _adapter_kwargs\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 582, in send\n    r = dispatch_hook('response', hooks, r, _kwargs)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/hooks.py\", line 31, in dispatch_hook\n    _hook_data = hook(hook_data, **kwargs)\n  File \"/usr/local/searx/searx/search.py\", line 117, in process_callback\n    search_results = callback(response)\n  File \"/usr/local/searx/searx/engines/google.py\", line 198, in response\n    raise RuntimeWarning('sorry.google.com')\nRuntimeWarning: sorry.google.com\n[pid: 976|app: 0|req: 336/336] 62.143.100.198 () {44 vars in 956 bytes} [Fri May 20 19:03:00 2016] POST / => generated 13664 bytes in 541 msecs (HTTP/1.1 200) 2 headers in 82 bytes (1 switches on core 0)\n[pid: 976|app: 0|req: 337/337] 62.143.100.198 () {44 vars in 956 bytes} [Fri May 20 19:03:24 2016] POST / => generated 13664 bytes in 16 msecs (HTTP/1.1 200) 2 headers in 82 bytes (2 switches on core 0)\n`\n. Ok, but why, and how can I deal with it?\n. Ok, when I try to search using google directly on my server (using lynx) I get a message from google, that my ip address is beeing blocked because of a high amount of requests.\nSo google suspects me to be a robot (what is not exactly untrue, is it?)\n. Thank you very much!. ",
    "pietsch": "BASE member of staff here. I like your idea, @jibe-b! We will discuss it in our team meeting tomorrow. There might be a way to authenticate searx instances automagically.\n. Good news: On behalf of the BASE team, I can confirm that BASE Search is interested in becoming integrated with Searx. We will implement an authentication mechanism that is compatible with Searx's distributed, easy-to-install approach, e.g. by matching the User-Agent HTTP header. Please state your preferences and the next steps to take here.\n. @asciimoo Oops, I wrote authentication when I meant authorization. BASE (Bielefeld Academic Search Engine) does not track anyone. Using the Web interface would cause a much higher load on our servers, and there would be no benefits for your users. On the contrary: The search API is faster.\nTo protect our search API against abuse, we have so far used an IP address whitelist. What we propose is to implement another authorization mechanism for you \u2013 perhaps based on the User-Agent HTTP header that Searx probably uses.\n. I just noticed that @jibe-b's original post contains a link to the wrong API (the OAI API which is for bulk data transfer which does not apply here). This is the search API we offer you: https://api.base-search.net/\n. Just leave it as it is (\"searx/$version\", if I read the code correctly).\n. Hi @jibe-b, you seem to have found the documentation \u2013 everything is in the PDF. I would have thought all dates are in standard format. Please send me specimens by e-mail. We could make changes if they don't break existing API clients.\n. @jibe-b Please look for the parameters hits and perhaps offset on page 4 of the documentation.\n. @jibe-b OA boost is the default in the BASE web interface. As a boost, it only affects ranking, so non-OA results are still included. If you want Searx to behave similarly, you should keep the OA boost on. If you think Searx users will prefer results with full text available, then you should keep the OA boost on, too. I think only a small minority will be interested in citations without full text.\n. The API specification should still be correct.\n. @asciimoo @jibe-b Good news: As discussed before, I have added a special authorization for Searx to the BASE HTTP API so that users do not have to register their IP address with BASE.\n. I am traveling this week. After that, I would love to chime in. I'll also do my best to persuade BASE, but they usually rely on word of mouth instead of PR.\n. @asciimoo, please reconsider this one. Yahoo gets all its results from Bing these days. Time and again, I get poor results listed first because they appear in both Bing and Yahoo. This flaw diminishes the usefulness of Searx.. I do not currently self-host a Searx instance. And I do not wish to keep cookies around. Why should Searx not work well out of the box?. Other things being equal, I would disable Yahoo by default as it is just a reseller these days.. ",
    "huzheng001": "OK! I get it!\n2016-02-18 23:58 GMT+08:00 GreenLunar notifications@github.com:\n\nLinks\nhttps://github.com/huzheng001/stardictd (StarDict server and on-line\ndictionary website)\nhttps://github.com/tuxor1337/stardict.js (JavaScript library for handling\ndictionaries in StarDict format)\nhttp://www.huzheng.org/stardict/\nDictionaries\nhttp://www.dictinfo.com/\nhttp://dicts.info/dictionaries.php\nhttp://wiktionary-export.nataraj.su/en/\nhttps://sourceforge.net/projects/glossary-fa/\nhttps://sourceforge.net/projects/xfardic/files/XDB%20Word%20Databases/\nPing to @huzheng001 https://github.com/huzheng001 and @tuxor1337\nhttps://github.com/tuxor1337, in case you would find this interesting.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/asciimoo/searx/issues/511#issuecomment-185789998.\n\n\n-- \nhttp://www.huzheng.org\n. ",
    "prolibre": "Surprise...\nBug with Firefox 45.0.1 (standard version)\nbut no bug in Firefox Dev edition (47.0a2)\nNo bug in chrome.\nI can reproduce the bug in instance : https://framabee.org/  but not on all engines\n. Yes thank you @ukwt. I did not say (sorry).\nI am French and the default encoding is fr_FR\n. good ! it works. Thank you very much and good day.\n. The Wikipedia tab (Infobox) in the language selected in the preferences ? :-)\nsorry for the translation...\n. Engines cannot retrieve results:\ngoogle (unexpected crash: CAPTCHA required) \nevery day or almost google blocks me... it's getting unmanageable for me.\nAnthony. Yes. In my instance ( https://www.heraut.eu/search/ ) I can change autocomplete ( ex. wikipedia or statpage) but can't desactivate it.\n. +1 I can confirm in debian with Apache/2.4.10 , uWSGI 2.0.7-debian and Python 2.7.9 : no response from server\n. @asciimoo yes ! :-) \u00e7a marche / it's OK !. the situation seems to be back to normal ... . hello @kvch : Do you know how the instances are removed ? I don't know how to delete an old instance ( https://www.heraut.eu/search/ ) ?\n. For more than a week... impossible for me to get out of it.\nDepuis plus d'une semaine... impossible pour moi d'en sortir.. Google certainly made some changes. For me google answers only one hour a day about... for the best days. I have three ip's and now they're all blocked. With my vpn I can still use google with CAPTCHA but it doesn't unlock searx (on the same ip).\n\nGoogle a certainement fait des modifications. Pour moi google ne r\u00e9pond qu'une heure par jour environ... pour les meilleurs jours. J'ai trois ip et maintenant elles sont toutes bloqu\u00e9es. Avec mon vpn je peux encore utiliser google avec le CAPTCHA mais cela ne d\u00e9bloque pas searx (sur la m\u00eame ip).. Very good fix for me. Have a nice day.. @kvch merci beaucoup !!! thanks very much !!!\n. +1\na dynamic blacklist (for a certain number of hours) of engines in error (for example google !)\n\n+1\nune blacklist dynamique (pour un certain nombre d'heures) des moteurs en erreur (pour exemple google !). merci / thanks @pointhi . https://github.com/asciimoo/searx/issues/1409 :-). ",
    "sachaz": "Thanks dalf's apache conf fixes this issue\npointhi's sould work but I need deflat...\n<3 <3 <3\n. Hi,\nI just updated https://searx.aquilenet.fr to 13.1 and I still got the same issue as Dominion0815\n. issue corrected, my instance was bugged by bots. Corrected with filtron:\nhttps://asciimoo.github.io/searx/admin/filtron.html\nhttps://github.com/asciimoo/filtron. here is a link on what I made to make it work (for me):\nhttps://atelier.aquilenet.fr/projects/services/wiki/Searx. google (unexpected crash: CAPTCHA required) \nis back again on our instance after some 4 months of happiness with Filtron. :(. Hoo thanks ! Sorry for the noise.\nIs it possible possible to set this value in the settings.yml ?. Ho nice thanks !\n@mangeurdenuage if I'm requesting the URL in SSL the text is encrypted ?. For me there is no issue on using the post method: all the traffic goes through the encrypted SSL  channel between the client and the server (https://en.wikipedia.org/wiki/Transport_Layer_Security).\nYou can test the SSL/TLS capabilities of Your Browser: https://www.ssllabs.com/ssltest/viewMyClient.html\nand the servers part: https://www.ssllabs.com/ssltest/index.html\nAs dalf mentioned in the browser history, the risk is on the user side in his browser history, the user he have to be aware on his system privacy (using an open source system) and by disabling these logs if needed.\nDNS is not related because the post request goes to the searx server through the SSL channel.\nOpen DNS server can be used like https://diyisp.org/dokuwiki/doku.php?id=technical:dnsresolver. sorry for already reported issue. ",
    "sut12": "I had the same Problem. I think dalfs solution should be mentoined in the installation wiki.\n. Hello,\ni used elinks. It is possible to save images. So i saved the captcha and downloaded it afterwards. For now it is working but i had to enter the captcha already more than once.\nI am not listed on any blacklist except on spamcannibal:\ngeneric/anonymous/un-named IP\nI need to check how i can get rid of this message. But im not sure if it has something to do with the google problem.\n. It is so wired. Yesterday evening it worked after i enterd the captchas. This morning it didnt work. \nNow it works but i didnt enter any captchas since yesterday...\n. I need google. But its working sometimes and sometimes not. I even tried to analyze with google webmaster tools...\nRight now its working - 2morrow maybe not.\n. I think that would be the best solution. \n. Thats exactly the point. cy8aer also pointed in the right direction: \n\nwhat about forwarding the captcha to the frontend and save the cookie in the backend then?\n\nI also tried to admin my domain with google webmaster tools. So that goole doenst ask for captcha anymore. It didnt help.. hmmm. I am on 0.12.0 and dont get google results. Maybe i should try to update xD.\nOh wait - rightn ow im getting results. But i am sure tomorrow it wont work again.. I was on 12.0 and it just worked for 3 days now. Today in the morning it didnt work again. No results from googel. So i did an update to 13.1. Now it works again. Changed nothing else. Lets see how long it works.. ",
    "wjh": "I believe this has been resolved, perhaps it should be closed?\n. ",
    "metal3d": "Just a precision, that's a docker image I built myself from you Dockerfile :)\n. PS: I just pull master, seeing that a new commit should change the problem 9331fc28a8ac2f898a437d126ee59353f7f1bfde => but it doesn't fix my problem\n. Yes sorry, I did a new build with no cache. The error is now from another provider.\nExample: https://search.metal3d.org/?q=gopher+golang&category_images=on then click the image:\n\nI've got:\n\n. Great ! it' working like a charm now. Thanks a lot !\nPS: if you want to add my server in the https://github.com/asciimoo/searx/wiki/Searx-instances page, no problem\n. I'll do it, no problem. And a big thanks for that project :)\n. ",
    "IguanaMeow": "I have solved it with the help of asciimoo on IRC using this method:\nhttps://github.com/asciimoo/searx/blob/master/searx/webapp.py#L697\n. location /searx {\n        proxy_pass             http://127.0.0.1:9999;\n        proxy_set_header       Host $host;\n        proxy_set_header       X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header       X-Scheme $scheme;\n        proxy_set_header       X-Script-Name /searx;\n        proxy_buffering        off;\n}\n. ",
    "Pofilo": "RSS are now implemented. You can see https://github.com/asciimoo/searx/issues/468.. Problem seems to be solved. Feel free to open again if it's not !. Duplicate https://github.com/asciimoo/searx/issues/468.. If this happens on your own instance of searx, you need to change the parameter base_url in the file searx/settings.yml (specifying that you use HTTPS).. Good idea, feel free to open a new WIKI page as a POC before spending too much time if some improvements are possible !. @ThePreviousOne an issue has been opened, thanks for the report (https://github.com/asciimoo/searx/issues/1350) !. @Vertux what procedure did you use to install Searx ?\nJust installed again according to the documentation and it works fine (v0.14.0).\nLet's try to understand and fix the problem !\nTo figure out if it is an apache misconfiguration, can you try to access http://localhost:8888 (or what you configured in setting.yml) directly on your server ?\nIf you only have access to the console (no desktop interface), you can install elinks (apt-get install elinks) and type elinks in your terminal and then the url.\nWikipedia page about elinks.\nYou should get something like that:\n\nIf it doesn't work, you'll get something like that:\n\nIf it works, can you give us your apache configuration ? (be careful to not reveal something private in it). No news of the problem. Feel free to open again if it's still relevant !. Problem seems to be solved. Feel free to open again if it's not !. This is not the first person asking for the problem with google.\nAs it is solved with a commit since the last release, maybe we can do a new release ?. Interesting, I will install filtron and give a feedback !. @miicha thanks for the feedback !\nHow did you add the cookies into google.py ? Can you give us a copy of the portion of the file ? (hidding all sensible values of course !!)\nIt seems we have a solution (I have some doubts about how long it will works, maybe the duration of the cookie), but it is not a solution we can fix with a PR. If we all have the same cookies, I think google will block them.. Simply because the functionality is not developed.\nFeel free to open a PR if you take the time to develop it.. The URL seems to have changed: https://btdb.to/.. Problem seems to be solved. Feel free to open again if it's not !. There is some documentation here: https://asciimoo.github.io/searx/admin/morty.html.\nFeel free to reopen if you still need some assistance !. No news of the problem. Feel free to open again if it's still relevant !. Can you reopen providing more information if you think the problem is still relevant ?. Thanks for the reminder :). @jibe-b you self assigned yourself, do you have some news ? :). No problem, just to know :)\nIt was to update the project https://github.com/asciimoo/searx/projects/3 to let people know someone was working on it !. Duplicate: https://github.com/asciimoo/searx/issues/910. I have the same issue on my own instance since yesterday (13.1 installed).\nIt used to work fine before and I don't have anything in the logs.\nIs there a version before 0.13 where it used to work ?. It is due to the CAPTCHAs, was it supposed to be solved ?. @asciimoo I move to v.0.13.1 the hour it was out !\nIn 21 hours without using Google, I still have the issue. \nI was also using autocomplete with Google (not since Yesterday tho).\nOther thing, my researches are in french (fr-FR so google.fr), maybe it can be related.. I'm the only one using my instance (logs are confirming that).\nI will check more precisely if something on my server is talking to google (or if something talks to google through my server).. Even in all languages I have the problem.\nThis is what I get from the logs:\nDEBUG:urllib3.connectionpool:https://www.google.com:443 \"GET /search?q=test&start=0&gws_rd=cr&gbv=1&lr=lang_en&ei=x HTTP/1.1\" 302 405\nDEBUG:urllib3.connectionpool:https://ipv4.google.com:443 \"GET /sorry/index?continue=https://www.google.com/search%3Fq%3Dtest%26start%3D0%26gws_rd%3Dcr%26gbv%3D1%26lr%3Dlang_en%26ei%3Dx&q=EgSVyjQ0GJmpmdEFIhkA8aeDS3PI0bodzHZIBxboq0U--AsOajJWMgFy HTTP/1.1\" 503 2702\nWe can see the request is correct according to https://github.com/asciimoo/searx/commit/6fdb6640d9e644860f60bf9cd59847c2d359fc8a.\nMy instance was shut down all the week-end + monday and nothing went to google from the IP of my server, I don't really understand what's happening ..\nEDIT: should I open a new ticket about that ?. @himbeere @asciimoo okay, thanks for the information.\nI'll switch back to 12.0 and give you a feedback too.. Okay, after the switch back, it was not working either (I mean google results).\nAfter 1 day off (instance of searx down), I still have the problem too (in 12.0).\nHowever in 13.1, I don't understand why https://github.com/asciimoo/searx/commit/6fdb6640d9e644860f60bf9cd59847c2d359fc8a is not working for me.\n. Okay, I also have (only used by myself with no request on google) an instance of openvpn on my server and somehow, google is able to detect it and block it.\nI'll try to make some researches about that to solve this problem.. Me too, the problem seems not to be related to searx directly.\nWhen searx is down for a week, I still have the issue (openvpn and unbound down).\nI don't know if my server has a service sending requests to google (I really don't think so) or if my IP has somehow been blacklisted at a moment or maybe this reason.. The result of a request asking for a captcha looks like that:\nhttps://ipv4.google.com/sorry/index?continue=https://www.google.com/search%3Fq%3Dtest...\nSo as we have a response, I guess this is why there is no logs about that.. You can simply add this button to your instance on the main page.\nAnd if you want (you should) to upgrade your instance, you just have to make a merge (you should never see any conflicts).\nEven if it is an option, it will quite be never used, so it is more complicated to add it (like which donation system should we use (paypal, patreon ...)).\nBut you can still add it and try to make a pull request, just my opinion :). Try it on searx.me or other instances (with a more recent version than framabee). . In the file searx/searx/templates/simple/preferences.html why don't we have\n<p style =\" margin:20px 0;\">{{ _('Search URL of the currently saved preferences') }} <small class=\"text-muted\">({{ _('Note: specifying custom settings in the search URL can reduce privacy by leaking data to the clicked result sites.') }})</small>:<br/>\n          <input readonly=\"\" class=\"form-control select-all-on-click cursor-text\" type=\"url\" value=\"{{ url_for('index', _external=True) }}?preferences={{ preferences_url_params|e }}{% raw %}&amp;q=%s{% endraw %}\">\n</p>\nas in the oscar template ?\nOtherwise, there is no way to have the url to save the settings using the simple theme.. You can find lists of instances here or here (it uses this tool but this one is more recent).\nSearx is not used to be slow, you should try an instance closer to where you live :). https://github.com/asciimoo/searx/issues/1089\nEven with the VPN down for a long time, my IP was still black listed.\nWhen you put the captcha on your browser, you are safe for some times but because they put a cookie ! That's why google is not answering to searx, they don't find a cookie so they ask for the captcha again.\nAnd as the captcha given to searx is never filled, I supposed our IP will remain in the black list longer :/. The problem is coming back for me too (I'm on last master commit).. With my IP address and a real browser, Google asks me for a captcha (once because they put a cookie after that).. Still happens sometimes on 0.14.0.. searx/settings.yml could be replaced by searx/settings.example.yml and then we can add searx/settings.yml to the .gitignore.\nThe downside is for updating searx according to the documentation.\nThis means it will not be possible to add engines with the current git stash apply which merge both files.. We may need to see the configuration of the vhost to help. Can you provide it ?. If you remove the API-Key and restart the service, the server is running well again ?\n(to be sure this is the cause of the bug). In the /about page, in the FAQ, there is a link to install in Firefox.\nFor example here: https://searx.me/about\nWe can maybe add a link for \"most used\" browsers.. When you mean solved, you mean how ?\nDid you find a solution to the captcha problem which leads in the google engine working well ? Did you disable the google engine ?. yourinstanceofsearx.com/preferences\nThen go to the engines and configure them.. Bad apache conf ..\nMy bad ..\nFor those interested:\n```\nProxyPass /m/ http://localhost:7776/\nProxyPassReverse /m/ http://localhost:7776/\nProxyPass / http://localhost:7777/\nProxyPassReverse / http://localhost:7777/`\n```. Well, result is not so good as well ..\n\nShould it be larger (or adding a padding to the container) ?. I think it's too late, too many forks and stars ..\nAs @asciimoo is active, there is no pb (for merging PR ..). I don't know if @dalf or @kvch can merge PR too.. Thanks for the link, it's up to @asciimoo to choose this kind of stuff I think.\nThe day @asciimoo will choose to stop the development of Searx (or if he don't have time anymore), it will be necessary to do that (otherwise, someone will have to make \"an official fork\"). However, I don't think it's really important to do it now (even if I understand your concern).\nTLDR: Up to @asciimoo and thanks for the link :). @Neustradamus what are the real benefits of doing that ?\nSearx is not a company but a free and open source software mostly developed by @asciimoo, it's normal that the repository is on his account.. Actually, you don't even need to switch several times, going directly on Files shows the error (on searx.me.\nRequest done:\nhttps://searx.me/?q=test&categories=files&language=en-US\nAnd I get the error.\nIf you take a look at piratebay.py, the request is done on https://thepiratebay.se/ and this site is actually not available.\nThere is a https://piratebay.to/, I don't know if it is a scam, the new domain name or a backup one.\nAnyway, researching on this url doesn't work so replacing the url on searx neither.\nSo the title of the issue is not good, this is a piratebay (engine) issue.. Take a look at:\nhttps://github.com/asciimoo/searx/blob/master/searx/engines/google.py\nAs CN is not used, I understand it takes the default one, but the result is not coherent.. Are you even with the 0.14 tag ?\nCan you give us a screenshot ?. Okay I get it !\nIf you take a look at: https://github.com/asciimoo/searx/search?utf8=%E2%9C%93&q=Suggestions&type=, you can see that the yahoo engine returns suggestions !\nSo if you have yahoo activated, you'll get suggestions ! (just an example)\nGoing back to the problem of the issue, does the json research use at least one engine returning suggestions ?. What do you mean by \"official searx instance\" ?\nYou can use every instances here https://github.com/asciimoo/searx/wiki/Searx-instances if you trust the owner.\nSearx is not a corporation, so there is not really \"official searx instance\" but I know https://searx.me/ is owned by @asciimoo and https://searx.pofilo.fr/ by me and it is the same code.\nSo correct me if I'm wrong, but use whatever instance you want if you trust the owner, otherwise, use your own instance. And @aurora-potter, if you use your own instance, you can use master branch to have the translations you did without waiting a new version of searx.\nBut you should understand that searx is not owned by a corporation but only during the free-time of some devoted developers !. > for some reason just have to switch on the search bar (instead of using unified search/address bar) and then you can add any custom search engine.\nYes this solution works but it may change in the future so adding this to docs seems a bit overrated. It's a Firefox UI problem !. > That's not good. I hope that it gets fixed soon.\nYou can see that nothing has be done regarding this bug, it's \"normal\" that it's not fixed yet.\nFeel free to open a PR :). ip is already working for ipv4.\nThere is also user agent and translation with this PR https://github.com/asciimoo/searx/pull/678\nFor the cache, you already have it on the right with the cached button.\ndl seems a bit more complicated, but others are possible as plugins like self_info.py (I may be wrong ..)\n\nPrivacy-respecting metasearch engine\n\ndefines searx, so dl can be part of it however :). @aurora-potter I also have the problem 4-5 times a week. My firefox is up-to-date and searx was added as a search engine (and I don't use Procon Latte).\nThis is not the source of the problem (even if I don't know where it comes from).. After a few days, it doesn't change anything. It still happens once a day at maximum (on 2 PCs - Windows 10 - Firefox 59.0.2).. I don't know who is the owner of http://stats.searx.oe5tpo.com/ nor how/how often the list is updated.\nI think the list came from https://github.com/asciimoo/searx/wiki/Searx-instances but has never been updated since (or long time ago).\nIt looks like stats come from https://github.com/dalf/searx-stats2 and your instance appears in https://github.com/dalf/searx-stats2/blob/master/instances.json.\nMaybe the owner of  http://stats.searx.oe5tpo.com/ is @dalf, maybe not ! We should wait for his answer and see if I'm totally wrong or partially right ^^. Well I have it on Jessie. But according to the official debian website it is also available on Stretch as python3-virtualenv.\nIs this package available to you ?\nI just found that so I don't really understand ... Well, I have:\ncat /etc/os-release\nPRETTY_NAME=\"Raspbian GNU/Linux 8 (jessie)\"\nNAME=\"Raspbian GNU/Linux\"\nVERSION_ID=\"8\"\nVERSION=\"8 (jessie)\"\nID=raspbian\nID_LIKE=debian\nHOME_URL=\"http://www.raspbian.org/\"\nSUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\"\nBUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\"\nDid you make a sudo apt-get update before trying to install the packages ?\nIf the apt-get update doesn't make it work, what are the content of /etc/apt/sources.list and /etc/apt/sources.list.d/raspi.list.\nI have:\n```\ndeb http://mirrordirector.raspbian.org/raspbian/ jessie main contrib non-free rpi\nUncomment line below then 'apt-get update' to enable 'apt-get source'\ndeb-src http://archive.raspbian.org/raspbian/ jessie main contrib non-free rpi\nand\ndeb http://archive.raspberrypi.org/debian/ jessie main ui\nUncomment line below then 'apt-get update' to enable 'apt-get source'\ndeb-src http://archive.raspberrypi.org/debian/ jessie main ui\n```\nAnd does apt-cache search python-virtualenv return python-virtualenv - Python virtual environment creator ?. Duplicate, see:\nhttps://github.com/asciimoo/searx/issues/1118\nhttps://github.com/asciimoo/searx/issues/729\nhttps://github.com/asciimoo/searx/issues/1089\nYou can close this one to avoid too much issues of the same issue.\nAnd you should try the search function of Github to avoid duplicates !\nNevertheless, if reading one of the above issues give you an idea on how to fix the problem, you are welcome :)\nI hope we'll find a solution ! I will make some researches this week-end (long one in France !). . Nah, you can install Searx wherever you want :)\nYou can find a list of instances here: https://github.com/asciimoo/searx/wiki/Searx-instances.\nLong because tuesday is a public day, so a lot of people are taking their monday too ^^\n@cosimone64 \n\nThis seems therefore to be an issue specific to searx.me.\n\nThis is not true, some instances are working well, some others are not. You can read the issues I was talking before.. @popeyesfx have you try @gr01d troubleshot ?\nYou can find the same troubleshot with screenshots here: https://github.com/asciimoo/searx/issues/686#issuecomment-396155336\nPlease, close the issue if the problem is solved.. Can you explain how to reproduce the bug ? In a specific configuration ?\nFor example if I want to reproduce the bug on https://searx.me/, how do I proceed ?. @stef made a perfect response according to me.\nThe code itself does not log IP or anything else. It depends of the configuration of the web server which is not the responsabiliy of Searx so the GDPR is not a problem for Searx :). >  When updating searx from git this file is overwritten. \nHow do you update from git ? A git pull will not erase your additional files (and merge the modfied ones).. It depends on what you modify, but a reset may not be necessary if you \"just\" add a new file.\nUp to you :). Sounds good to me :)\nCan you send a PR ?\nIf you can't, I can do it this evening (CEST time in 9 hours from this post) so just tell me.. Nice suggestion, feel free to open a PR :). Well, it is already implemented !\nFor example in the oscar theme:\n\nOr maybe you mean the green color ?. Duplicate https://github.com/asciimoo/searx/issues/686.. Did you try with last commit ?\n@asciimoo made https://github.com/asciimoo/searx/commit/f82ead3e303d75ba63a370dc038311e172e1330d while closing this issue.. Duplicate https://github.com/asciimoo/searx/issues/686.. This is due to the last line:\n\nSupport the development and maintenance of searx\n\nYou're right reporting this, @asciimoo should fix this easily ! (and the hidden line is quit important for him I suppose !). Shouldn't we merge https://github.com/asciimoo/searx/pull/933 instead of this one ?. Well, the URL is formatted after the function you see in duckduckgo.py.\nYou can verify it by yourself adding some logs.\nThe request done for test is in my case like this:\nhttps://duckduckgo.com/html?q=test&kl=us-en&s=0&dc=0\nIf I copy it in my browser, it does work (the result are given).\nIf you try this request with a wget or curl, you will never get any answer.\nThe same happens with https://duckduckgo.com/html?q=test.\nDo you have some info about a change of DuckDuckGo ?. Is your searx/settings.yml configured like that ?\n```\n    oadoi.org : 'https://oadoi.org/'\n    doi.org : 'https://doi.org/'\n    doai.io  : 'http://doai.io/'\ndefault_doi_resolver : 'oadoi.org'\n```. If you wanna use the Morty proxy, the doc is here:\nhttps://asciimoo.github.io/searx/admin/morty.html\nIf you wanna use a \"classic\" proxy, as shown in the comments of searx/settings.yml, the doc is here:\nhttp://docs.python-requests.org/en/latest/user/advanced/#proxies\nThe only things you may configure should be in searx/settings.yml.. What @GambaJo means is that if he makes a research in a language (let's say french), he will only get results from this language.\nAs a developer, a lot of results came in english on google/ddg .. So he doesn't get them with searx and is forced to make a new research specifying english.. Duplicate #1338.. @memorable111 caps lock is not necessary. Especially when you use a free and open-source software. Thanks !\nAs long as the certs are no longer valid, you can still use one of the multiple instances available here:\nhttps://github.com/asciimoo/searx/wiki/Searx-instances. @dimqua is right, this is a duplicate of #1344.\nBoth issues are suggesting to make researches in different languages (native for example Dutch for you + english). It makes a lot of sense but needs to be implemented.. For dictzone and mymemory translated, try en-fr search.\nI don't know for the others.. Other than if any site or search engine says they don't log IP addresses, search terms, but actually do, is there a particular problem?\nYeah it's a problem. That's why Searx exists and is open source, this way, you can audit the code and if you don't trust any instance, install it on your own server.. Great, thanks.. Yeah, duplicate of #1344.. Great thanks.. The documentation is correct (and is similar for apache), if I search enable vhost nginx, the first link I find is this one, it is well explained !\nhttps://www.digitalocean.com/community/tutorials/how-to-set-up-nginx-server-blocks-virtual-hosts-on-ubuntu-16-04. I agree the file needs to be in enabled at the end (directly or with a link), but it is a good practice to put it in available and make a link into enabled.\nI suggest you to read the PS of this post: https://serverfault.com/a/870618 :). It depends of your configuration. If you go into the Preferences in the plugin section, you have one name Infinite scroll. Is it activated ?. It should let the request, at least I have it. Which version of searx and which browser are you using ?. From what I can see a lot of people fixed majority of bugs in v0.14, community is around project.\n@ZEROF Can you create a new issue listing all those important PR ? I can have some time in the next weeks to test a bit and make some integration.. Can you provide a screenshot of the new look ? :). You also need to modify this file right ?. It is build from grunt but we used to commit it.\nLook into https://github.com/asciimoo/searx/tree/master/searx/static/themes/oscar/css.\nIt has a drawback to have the \"same thing\" twice on git, but the advantage that after a pull, we don't need to use make or anything else.\n. Great, thanks.. Is it happening with google as engine ?\nIf yes, it is a duplicate of a lot of other issues ;). theme_args :\n        oscar_style : logicodev-dark\nIt works fine on my instance. Did you have the correct indentation in the file ? Did you restart searx ?. And if you try to change another parameter (like default_theme), does it changes the theme ?\nJust to see if a change in configuration is taken in consideration.\nAlso, what is the version of your searx instance ? Are you on a tag, on master (if yes which commit) ?. There is a search bar in the issues section, you should search first before creating a 20th duplicate ;). Should we open a merge request to add this quick fix ? Or maybe adding it to the doc as it is an upgrade of pip.. Great, thanks.. Thanks @dimqua.. > \n\nI'm currently there. At this point, searx is no longer usable at all, and I'm going to have to shut it down if there is no way to work around this.\n\nThere is a lot of other engines. If you think Google will give you better results than others, it's because they have you profile and they can adapt theirs responses. So using Google through Searx is similar than using others engines.\nTo be sure to avoid those CAPTCHA errors, you can disable all google engines in the config file.. You are right, duplicate.. There is a PR for that:\nhttps://github.com/asciimoo/searx/pull/1420\n@asddsaz Is that what you had in mind ?. Merged with https://github.com/asciimoo/searx/pull/1450\nSorry, didn't see you did it first ... Great, thanks!. Thanks :). The fix does'nt change anything for me. I still don't have any results from qwant. Is it working on your side ?. The changelog and units tests also need to be updated.\n```\n$ grep -rni \"Ixquick\" .\n./searx/settings.yml:639:  - name : ixquick\n./searx/settings.yml:641:    base_url : 'https://www.ixquick.eu/'\n./searx/settings.yml:642:    search_url : 'https://www.ixquick.eu/do/search'\nBinary file ./searx/engines/startpage.pyc matches\n./searx/engines/startpage.py:76:        # block ixquick search url's\n./searx/engines/startpage.py:77:        if re.match(r\"^http(s|)://(www.)?ixquick.com/do/search\\?.*$\", url):\n./CHANGELOG.rst:87:- Multiple engine fixes (digbt, 500px, google news, ixquick, bing, kickass, google play movies, habrahabr, yandex)\n./tests/unit/engines/test_startpage.py:48:                <A class=\"proxy\" id=\"proxy_link\" HREF=\"https://ixquick-proxy.com/do/spg/proxy?ep=&edata=&ek=&ekdata=\"\n./tests/unit/engines/test_startpage.py:50:                    Navigation avec Ixquick Proxy\n./tests/unit/engines/test_startpage.py:53:                <A HREF=\"https://ixquick-proxy.com/do/spg/highlight.pl?l=francais&c=hf&cat=web&q=test&rl=NONE&rid=\n./tests/unit/engines/test_startpage.py:84:                <A class=\"proxy\" id=\"proxy_link\" HREF=\"https://ixquick-proxy.com/do/spg/proxy?ep=&edata=&ek=&ekdata=\"\n./tests/unit/engines/test_startpage.py:86:                    Navigation avec Ixquick Proxy\n./tests/unit/engines/test_startpage.py:89:                <A HREF=\"https://ixquick-proxy.com/do/spg/highlight.pl?l=francais&c=hf&cat=web&q=test&rl=NONE&rid=\n./tests/unit/engines/test_startpage.py:119:                <A class=\"proxy\" id=\"proxy_link\" HREF=\"https://ixquick-proxy.com/do/spg/proxy?ep=&edata=&ek=&ekdata=\"\n./tests/unit/engines/test_startpage.py:121:                    Navigation avec Ixquick Proxy\n./tests/unit/engines/test_startpage.py:124:                <A HREF=\"https://ixquick-proxy.com/do/spg/highlight.pl?l=francais&c=hf&cat=web&q=test&rl=NONE&rid=\n``\n. I confused changelog and readme, I'm a bit tired ^^. Did you follow the instructions on [https://asciimoo.github.io/searx/dev/install/installation.html](https://asciimoo.github.io/searx/dev/install/installation.html) ?\nYou should have done asudo useradd searx -d /usr/local/searxwhich creates the searx user.. This is the second time you open an issue because you didn't follow the installation guide. Please read it more carefully. \nOf course, we can improve it !. Hey, you can simply edit the page ;). Is this the classical URL given by an installation of Open Semantic Search ?\nYou should maybe put it insettings.yml`. This way, the configuration is only done in the configuration file.\n(So you can remove the comment maybe)\nWhat do you think ?. That's a small thing, but can you change 2547bc by 2547BC ?\nIt's just to have all colors written in the same way, thanks !. ",
    "fredhampton": "I have almost figured it out\u2026..\nBelow is what I have in the engine file\u2026\u2026.. can you take a look\u2026..I think that the # do search-request section is not correct\u00a0\nfrom urllib import urlencode\nfrom lxml.html import fromstring\nfrom searx.engines.xpath import extract_text\nengine dependent config\ncategories = ['news',]\npaging = True\nsearch-url\nbase_url = 'http://sparks.leola.me/'\nsearch_url = base_url + '?action=search&query={query}&search_more=feeds'\nspecific xpath variables\nresult_xpath = '//div[@class=\"result results_links results_links_deep web-result \"]' \u00a0# noqa\nurl_xpath = './/a[@class=\"result__a\"]/@href'\ntitle_xpath = './/a[@class=\"result__a\"]'\ncontent_xpath = './/a[@class=\"result__snippet\"]'\ndo search-request\ndef request(query, params):\n\u00a0 \u00a0 offset = (params['pageno'] - 1) * 30\nif params['language'] == 'all':\n\u00a0 \u00a0 \u00a0 \u00a0 locale = 'en-us'\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 locale = params['language'].replace('_', '-').lower()\nparams['url'] = url.format(\n\u00a0 \u00a0 \u00a0 \u00a0 query=urlencode({'q': query, 'kl': locale}),\n\u00a0 \u00a0 \u00a0 \u00a0 offset=offset)\nreturn params\nget response from search-request\ndef response(resp):\n\u00a0 \u00a0 results = []\ndoc = fromstring(resp.text)\n# parse results\n\u00a0 \u00a0 for r in doc.xpath(result_xpath):\n\u00a0 \u00a0 \u00a0 \u00a0 try:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 res_url = r.xpath(url_xpath)[-1]\n\u00a0 \u00a0 \u00a0 \u00a0 except:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\nif not res_url:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\ntitle = extract_text(r.xpath(title_xpath))\n\u00a0 \u00a0 \u00a0 \u00a0 content = extract_text(r.xpath(content_xpath))\n# append result\n\u00a0 \u00a0 \u00a0 \u00a0 results.append({'title': title,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'content': content,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'url': res_url})\n# return results\n\u00a0 \u00a0 return results\nOn April 27, 2016 at 11:51:55 PM, No\u00e9mi V\u00e1nyi (notifications@github.com) wrote:\nHey! Turn on debug logging as described in the documentation.\nhttps://asciimoo.github.io/searx/dev/quickstart.html#tips-for-debugging-development\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. I m receiving this message in the log file\u00a0\nFile \"/usr/local/searx/searx/engines/sparksnews.py\", line 40, in request\n\u00a0 \u00a0 params['url'] = search_url.format(query=urlencode({'query': query}))\nNameError: global name 'urlencode' is not defined\nThis is the content of sparksnews.py\nfrom urllib import quote_plus\nfrom json import loads\nfrom lxml import html\nfrom cgi import escape\nfrom dateutil import parser\nengine dependent config\ncategories = ['news',]\npaging = True\nsearch-url\nbase_url = 'http://sparks.leola.me/'\nsearch_url = base_url + '?action=search&{query}&search_more=feeds'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nspecific xpath variables\nresults_xpath = '//article'\nlink_xpath = './/small[@class=\"time\"]//a'\ntitle_xpath = './/h2//a//text()'\ncontent_xpath = './/p//text()'\npubdate_xpath = './/time'\ndo search-request\ndef request(query, params):\n\u00a0 \u00a0 params['url'] = search_url.format(query=urlencode({'query': query}))\n\u00a0 \u00a0 return params\nget response from search-request\ndef response(resp):\n\u00a0 \u00a0 results = []\nsearch_res = loads(resp.text)\n# return empty array if there are no results\n\u00a0 \u00a0 if not search_res.get('responseData', {}).get('results'):\n\u00a0 \u00a0 \u00a0 \u00a0 return []\n# parse results\n\u00a0 \u00a0 for result in search_res['responseData']['results']:\n\u00a0 \u00a0 \u00a0 \u00a0 # parse publishedDate\n\u00a0 \u00a0 \u00a0 \u00a0 publishedDate = parser.parse(result['publishedDate'])\n\u00a0 \u00a0 \u00a0 \u00a0 if 'url' not in result:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n# append result\n\u00a0 \u00a0 \u00a0 \u00a0 results.append({'url': result['unescapedUrl'],\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'title': result['titleNoFormatting'],\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'publishedDate': publishedDate,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'content': result['content']})\n# return results\n\u00a0 \u00a0 return results\nThanks so much for your help\nOn April 29, 2016 at 12:35:00 AM, Adam Tauber (notifications@github.com) wrote:\ntry this:\nsearch_url = base_url + '?action=search&{query}&search_more=feeds'\ndo search-request\ndef request(query, params):\n    params['url'] = search_url.format(query=urlencode({'query': query}))\n    return params\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. I m receiving this message in the log file\u00a0\nFile \"/usr/local/searx/searx/engines/sparksnews.py\", line 40, in request\n\u00a0 \u00a0 params['url'] = search_url.format(query=urlencode({'query': query}))\nNameError: global name 'urlencode' is not defined\nThis is the content of sparksnews.py\nfrom urllib import quote_plus\nfrom json import loads\nfrom lxml import html\nfrom cgi import escape\nfrom dateutil import parser\nengine dependent config\ncategories = ['news',]\npaging = True\nsearch-url\nbase_url = 'http://sparks.leola.me/'\nsearch_url = base_url + '?action=search&{query}&search_more=feeds'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nspecific xpath variables\nresults_xpath = '//article'\nlink_xpath = './/small[@class=\"time\"]//a'\ntitle_xpath = './/h2//a//text()'\ncontent_xpath = './/p//text()'\npubdate_xpath = './/time'\ndo search-request\ndef request(query, params):\n\u00a0 \u00a0 params['url'] = search_url.format(query=urlencode({'query': query}))\n\u00a0 \u00a0 return params\nget response from search-request\ndef response(resp):\n\u00a0 \u00a0 results = []\nsearch_res = loads(resp.text)\n# return empty array if there are no results\n\u00a0 \u00a0 if not search_res.get('responseData', {}).get('results'):\n\u00a0 \u00a0 \u00a0 \u00a0 return []\n# parse results\n\u00a0 \u00a0 for result in search_res['responseData']['results']:\n\u00a0 \u00a0 \u00a0 \u00a0 # parse publishedDate\n\u00a0 \u00a0 \u00a0 \u00a0 publishedDate = parser.parse(result['publishedDate'])\n\u00a0 \u00a0 \u00a0 \u00a0 if 'url' not in result:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n# append result\n\u00a0 \u00a0 \u00a0 \u00a0 results.append({'url': result['unescapedUrl'],\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'title': result['titleNoFormatting'],\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'publishedDate': publishedDate,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'content': result['content']})\n# return results\n\u00a0 \u00a0 return results\nThanks so much for your help\nOn April 29, 2016 at 12:35:00 AM, Adam Tauber (notifications@github.com) wrote:\ntry this:\nsearch_url = base_url + '?action=search&{query}&search_more=feeds'\ndo search-request\ndef request(query, params):\n    params['url'] = search_url.format(query=urlencode({'query': query}))\n    return params\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. I imported urlencode\u2026\u2026I think its close to working\u2026..\nI received the following errors in the log\u00a0\nINFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): sparks.leola.me\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /?action=search&query=Clinton&search_more=feeds HTTP/1.1\" 200 5724\nERROR:searx.search:engine crash: sparks news feeds\nand..\nFile \"/usr/local/searx/searx/engines/sparksnews.py\", line 50, in response\n\u00a0 \u00a0 search_res = loads(resp.text)\nThe sparksnews.py file contains\nfrom urllib import quote_plus\nfrom urllib import urlencode\nfrom json import loads\nfrom lxml import html\nfrom cgi import escape\nfrom dateutil import parser\nengine dependent config\ncategories = ['news',]\npaging = True\nsearch-url\nbase_url = 'http://sparks.leola.me/'\nsearch_url = base_url + '?action=search&{query}&search_more=feeds'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nspecific xpath variables\nresults_xpath = '//article'\nlink_xpath = './/small[@class=\"time\"]//a'\ntitle_xpath = './/h2//a//text()'\ncontent_xpath = './/p//text()'\npubdate_xpath = './/time'\ndo search-request\ndef request(query, params):\n\u00a0 \u00a0 params['url'] = search_url.format(query=urlencode({'query': query}))\n\u00a0 \u00a0 return params\nget response from search-request\ndef response(resp):\n\u00a0 \u00a0 results = []\nsearch_res = loads(resp.text)\n# return empty array if there are no results\n\u00a0 \u00a0 if not search_res.get('responseData', {}).get('results'):\n\u00a0 \u00a0 \u00a0 \u00a0 return []\n# parse results\n\u00a0 \u00a0 for result in search_res['responseData']['results']:\n\u00a0 \u00a0 \u00a0 \u00a0 # parse publishedDate\n\u00a0 \u00a0 \u00a0 \u00a0 publishedDate = parser.parse(result['publishedDate'])\n\u00a0 \u00a0 \u00a0 \u00a0 if 'url' not in result:\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\n# append result\n\u00a0 \u00a0 \u00a0 \u00a0 results.append({'url': result['unescapedUrl'],\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'title': result['titleNoFormatting'],\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'publishedDate': publishedDate,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'content': result['content']})\n# return results\n\u00a0 \u00a0 return results\nThanks for your help !!!!\nOn May 2, 2016 at 9:46:52 AM, No\u00e9mi V\u00e1nyi (notifications@github.com) wrote:\nYou have to import urlencode in order to use it.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. This is the stack trace\nINFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): sparks.leola.me\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /?action=search&query=Clinton&search_more=feeds HTTP/1.1\" 200 5726\nERROR:searx.search:engine crash: sparks news feeds\nTraceback (most recent call last):\n\u00a0 File \"/usr/local/searx/searx/search.py\", line 40, in search_request_wrapper\n\u00a0 \u00a0 ret = fn(url, _kwargs)\n\u00a0 File \"/usr/local/searx/searx/poolrequests.py\", line 82, in get\n\u00a0 \u00a0 return request('get', url, _kwargs)\n\u00a0 File \"/usr/local/searx/searx/poolrequests.py\", line 75, in request\n\u00a0 \u00a0 response = session.request(method=method, url=url, _kwargs)\n\u00a0 File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 468, in request\n\u00a0 \u00a0 resp = self.send(prep, _send_kwargs)\n\u00a0 File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 582, in send\n\u00a0 \u00a0 r = dispatch_hook('response', hooks, r, _kwargs)\n\u00a0 File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/requests/hooks.py\", line 31, in dispatch_hook\n\u00a0 \u00a0 _hook_data = hook(hook_data, _kwargs)\n\u00a0 File \"/usr/local/searx/searx/search.py\", line 117, in process_callback\n\u00a0 \u00a0 search_results = callback(response)\n\u00a0 File \"/usr/local/searx/searx/engines/sparksnews.py\", line 50, in response\n\u00a0 \u00a0 search_res = loads(resp.text)\n\u00a0 File \"/usr/lib/python2.7/json/init.py\", line 338, in loads\n\u00a0 \u00a0 return _default_decoder.decode(s)\n\u00a0 File \"/usr/lib/python2.7/json/decoder.py\", line 366, in decode\n\u00a0 \u00a0 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n\u00a0 File \"/usr/lib/python2.7/json/decoder.py\", line 384, in raw_decode\n\u00a0 \u00a0 raise ValueError(\"No JSON object could be decoded\")\nValueError: No JSON object could be decoded\nOn May 2, 2016 at 1:18:00 PM, No\u00e9mi V\u00e1nyi (notifications@github.com) wrote:\nIs it the whole stack trace?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. HTML\nOn May 2, 2016 at 1:26:10 PM, No\u00e9mi V\u00e1nyi (notifications@github.com) wrote:\nWhat is the content of resp.text? Is it JSON? Is it HTML?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. I m using dom = html.fromstring(resp.text) \u2026..It seems like there are results but they are not outputting to the page ?\nThe stack trace is\u00a0\nINFO:requests.packages.urllib3.connectionpool:Resetting dropped connection: sparks.leola.me\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /?action=search&query=Prince&search_more=feeds HTTP/1.1\" 200 6148\nINFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): sparks.leola.me\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /?action=search&query=Prince&search_more=feeds HTTP/1.1\" 200 6148\nINFO:requests.packages.urllib3.connectionpool:Resetting dropped connection: suggestqueries.google.com\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /complete/search?client=toolbar&q=P&hl=en HTTP/1.1\" 200 None\nINFO:requests.packages.urllib3.connectionpool:Resetting dropped connection: suggestqueries.google.com\nINFO:requests.packages.urllib3.connectionpool:Resetting dropped connection: sparks.leola.me\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /?action=search&query=Prince&search_more=feeds HTTP/1.1\" 200 6148\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /complete/search?client=toolbar&q=Prince&hl=en HTTP/1.1\" 200 None\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nthe content of the engine is below\u00a0\nfrom urllib import quote_plus\nfrom urllib import urlencode\nfrom json import loads\nfrom lxml import html\nfrom cgi import escape\nfrom dateutil import parser\nengine dependent config\ncategories = ['news',]\npaging = True\nsearch-url\nbase_url = 'http://sparks.leola.me/'\nsearch_url = base_url + '?action=search&{query}&search_more=feeds'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nspecific xpath variables\nresults_xpath = '//article'\nlink_xpath = './/small[@class=\"time\"]//a'\ntitle_xpath = './/h2//a//text()'\ncontent_xpath = './/p//text()'\npubdate_xpath = './/time'\ndo search-request\ndef request(query, params):\n\u00a0 \u00a0 params['url'] = search_url.format(query=urlencode({'query': query}))\n\u00a0 \u00a0 return params\nget response from search-request\ndef response(resp):\n\u00a0 \u00a0 results = []\ndom = html.fromstring(resp.text)\n# parse results\n\u00a0 \u00a0 for result in dom.xpath(results_xpath):\n\u00a0 \u00a0 \u00a0 \u00a0 link = result.xpath(link_xpath)[0]\n\u00a0 \u00a0 \u00a0 \u00a0 href = urljoin(url, link.attrib.get('href'))\n\u00a0 \u00a0 \u00a0 \u00a0 title = escape(extract_text(link))\n\u00a0 \u00a0 \u00a0 \u00a0 content = escape(extract_text(result.xpath(content_xpath)))\n# append result\n\u00a0 \u00a0 \u00a0 \u00a0 results.append({'url': href,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'title': title,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'content': content})\n# return results\n\u00a0 \u00a0 return results\nOn May 2, 2016 at 1:34:01 PM, No\u00e9mi V\u00e1nyi (notifications@github.com) wrote:\nThen you should use dom = html.fromstring(resp.text). The function loads is used in case of JSON response.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. I m using dom = html.fromstring(resp.text) \u2026..It seems like there are results but they are not outputting to the page ?\nThe stack trace is\u00a0\nINFO:requests.packages.urllib3.connectionpool:Resetting dropped connection:\u00a0sparks.leola.me\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /?action=search&query=Prince&search_more=feeds HTTP/1.1\"\u00a0200 6148\nINFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1):\u00a0sparks.leola.me\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /?action=search&query=Prince&search_more=feeds HTTP/1.1\"\u00a0200 6148\nINFO:requests.packages.urllib3.connectionpool:Resetting dropped connection:\u00a0suggestqueries.google.com\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /complete/search?client=toolbar&q=P&hl=en HTTP/1.1\" 200 None\nINFO:requests.packages.urllib3.connectionpool:Resetting dropped connection:\u00a0suggestqueries.google.com\nINFO:requests.packages.urllib3.connectionpool:Resetting dropped connection:\u00a0sparks.leola.me\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /?action=search&query=Prince&search_more=feeds HTTP/1.1\"\u00a0200 6148\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /complete/search?client=toolbar&q=Prince&hl=en HTTP/1.1\" 200 None\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nthe content of the engine is below\u00a0\nfrom urllib import quote_plus\nfrom urllib import urlencode\nfrom json import loads\nfrom lxml import html\nfrom cgi import escape\nfrom dateutil import parser\nengine dependent config\ncategories = ['news',]\npaging = True\nsearch-url\nbase_url = 'http://sparks.leola.me/'\nsearch_url = base_url + '?action=search&{query}&search_more=feeds'\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\nspecific xpath variables\nresults_xpath = '//article'\nlink_xpath = './/small[@class=\"time\"]//a'\ntitle_xpath = './/h2//a//text()'\ncontent_xpath = './/p//text()'\npubdate_xpath = './/time'\ndo search-request\ndef request(query, params):\n\u00a0 \u00a0 params['url'] = search_url.format(query=urlencode({'query': query}))\n\u00a0 \u00a0 return params\nget response from search-request\ndef response(resp):\n\u00a0 \u00a0 results = []\ndom = html.fromstring(resp.text)\n# parse results\n\u00a0 \u00a0 for result in dom.xpath(results_xpath):\n\u00a0 \u00a0 \u00a0 \u00a0 link = result.xpath(link_xpath)[0]\n\u00a0 \u00a0 \u00a0 \u00a0 href = urljoin(url, link.attrib.get('href'))\n\u00a0 \u00a0 \u00a0 \u00a0 title = escape(extract_text(link))\n\u00a0 \u00a0 \u00a0 \u00a0 content = escape(extract_text(result.xpath(content_xpath)))\n# append result\n\u00a0 \u00a0 \u00a0 \u00a0 results.append({'url': href,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'title': title,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'content': content})\n# return results\n\u00a0 \u00a0 return results\nOn May 2, 2016 at 1:34:01 PM, No\u00e9mi V\u00e1nyi (notifications@github.com) wrote:\nThen you should use dom = html.fromstring(resp.text). The function loads is used in case of JSON response.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. I m receiving results but they are not showing up in the search results \nDo I need to have any additional info for the engine in settings ?\nDEBUG:requests.packages.urllib3.connectionpool:\"GET /?action=search&query=Apple&search_more=feeds HTTP/1.1\" 200 5509\n. Thanks for your help !!!\nThis is what i have in settings file and it is not showing results on search page\u00a0\n- name : sparks social\n\u00a0 \u00a0 engine : xpath\n\u00a0 \u00a0 paging : True\n\u00a0 \u00a0 search_url : http://sparks.leola.me/?action=search&query={query}&search_more=all \u00a0\n\u00a0 \u00a0 results_xpath : //div[@class=\"description\"]\u00a0\n\u00a0 \u00a0 url_xpath: .//h3[@class=\"title\"]/a/@href \u00a0 \u00a0\n\u00a0 \u00a0 title_xpath: .//h3[@class=\"title\"]/a/text()\n\u00a0 \u00a0 content_xpath: .//div[@class=\"block_content\"]/p\n\u00a0 \u00a0 pubdate_xpath: .//span[@class=\"fu_date\"]/text()\n\u00a0 \u00a0 categories : general\n\u00a0 \u00a0 shortcut : so\n\u00a0 \u00a0 disabled : False\nThanks !!!!\nOn May 8, 2016 at 1:22:29 PM, No\u00e9mi V\u00e1nyi (notifications@github.com) wrote:\nI tested your XPATH expressions using Firebug. None of those worked. However, I have found a few examples which works for me. You could try these. :)\nTitle: //h3[@class=\"title\"]/a/text()\nURL: //h3[@class=\"title\"]/a/@href\nPubdate: //span[@class=\"feed_item_date\"]/text()\nContent: //div[@class=\"block_content\"]/p\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. Thanks !!!!\nThis works for the query = feeds\u00a0\nI tried the paths with this query\u00a0http://sparks.leola.me/?action=search&query={query}&search_more=all\u00a0\u00a0\nAnd it seems to not work\nOn May 11, 2016 at 1:51:53 PM, Adam Tauber (notifications@github.com) wrote:\ntry with these xpaths:\nname : sparks news feeds engine : xpath paging : True search_url : http://sparks.leola.me/?action=search&{query}&search_more=feeds results_xpath : //div[@class=\"feed_item_inner\"] url_xpath : .//h3[@class=\"title\"]//a/@href title_xpath : .//h3//a//text() content_xpath : .//p//text() categories : news shortcut : sn\nit works for me\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly or view it on GitHub\n. Thanks !!!\nOn June 10, 2016 at 2:30:03 AM, Adam Tauber (notifications@github.com) wrote:\n@fredhampton engines have weight 1.0 by default which affects result scoring. You can change it in settings.yml if you add weight: n to an engine.\nCurrency convert engine is a good example which has custom weight.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Thanks !!!\nOn June 9, 2016 at 8:26:02 PM, Alexandre Flament (notifications@github.com) wrote:\nThe results are merged using this implementation :\nhttps://github.com/asciimoo/searx/blob/master/searx/results.py#L145\nThen the results are scored using this implementation :\nhttps://github.com/asciimoo/searx/blob/master/searx/results.py#L196\nI don't think there is documentation for now.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @dalf \nI followed the steps but seem to be missing something concerning apache\nWhere is the place in apache2.conf to add \n\n    Options FollowSymLinks Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n\nDo I need to create a searx.conf file ?\nThanks for your help. I followed the steps but seem to be missing something concerning apache\nWhere is the place in apache2.conf to add\n\nOptions FollowSymLinks Indexes\nSetHandler uwsgi-handler\nuWSGISocket /run/uwsgi/app/searx/socket\nDo I need to create a searx.conf file ?\nThanks for your help. I added below to apache2.conf and could not reach searx on my severs localhost:8888\nThe uwsgi searx.ini is working.... I can see the entires in the log file....\nDoes it make any difference that I m using plesk on debian 8 ?\n\n    Options FollowSymLinks Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n\n. My searx files are here /use/local/searx \nIf this is the case should location be this   or  or . also receive this error message when running python webapp.py\nTraceback (most recent call last):\n  File \"webapp.py\", line 48, in \n    from flask import (\n  File \"/usr/local/lib/python2.7/dist-packages/flask/init.py\", line 19, in \n    from jinja2 import Markup, escape\n  File \"/usr/local/lib/python2.7/dist-packages/jinja2/init.py\", line 33, in \n    from jinja2.environment import Environment, Template\n  File \"/usr/local/lib/python2.7/dist-packages/jinja2/environment.py\", line 15, in \n    from jinja2 import nodes\n  File \"/usr/local/lib/python2.7/dist-packages/jinja2/nodes.py\", line 19, in \n    from jinja2.utils import Markup\n  File \"/usr/local/lib/python2.7/dist-packages/jinja2/utils.py\", line 624, in \n    from markupsafe import Markup, escape, soft_unicode\nImportError: No module named markupsafe\n. ",
    "mikhirev": "I also found that Plural-Forms header in Russian .po file was wrong.\n. Hi! Are there any problems with this PR? The fix is trivial, but it requires changes in translations. Sooner it will be merged and TX project synced, more time translators will have to fix translations.\n. Oops, I'm sorry. There are two possible solutions: use contexts or simply split the line in another place, e.g. before the comma. The first is better IMHO.\n. Please re-apply commit 242c9ba, it fixes another issue and does not break anything else. I'll fix ngettext/pngettext related stuff later.\nBTW Russian translation looks like a crap now. An I guess that not only Russian, but all languages that have more than 2 plural forms.\n. ",
    "LuccoJ": "Seconded. At least some search engines being used as backends (e.g. Google) support this, so it should be possible to provide it to some extent at least.\n. Bear with me and even though this is done and closed, consider adding to this interface a field to tell me what the shortcut for each engine is, not just its name. I need this to programmatically find a Searx instance that can/will provide a specific engine when a user asks for it using a !shortcut.\n. > I agree that it can be disturbing for some, but I don't see what we gain by displaying the exact number. But I am open to reasonable counter-arguments.\nI'll present my use case: fefore the Google search API became unavailable, I was using their hit count to assess how common a given word, phrase or expression was on the web compared to another. This was useful for my bot running on the ##English freenode channel.\nGoogle's hit count might not have been exceptionally accurate, either, but it was always the same for a given search query (at least within the same user's search bubble) and it was pretty plausible; it wouldn't range from a total of 24 to several hundred million for the quoted query \"I don't know\" like searx.me was doing for me at some point today.\n\nThe number changing behaviour however is a problem. Unfortunately, I was not able to reproduce it.\n\nI am thinking that maybe it was due to one of the engines responding in some of my queries and failing to respond in others, so its (presumably much bigger) number wasn't always taken into account.\nWhat if, for starters, instead of using only the biggest number, you used an average of all the numbers received? Or even better, if, say, two out of three numbers coming from the engines are in rough agreement, and only the third is completely different, ignore the third and average the other two.\nThis could take care of rogue engines that somehow decide one day to output \"24\" for a common phrase like \"I don't know\".\n\nAlso implementing it would be a tedious task. In order to accumulate all of the number of results ~70 pages has to be scraped or at least check if number of results is displayed.\n\nI agree and I don't think it is reasonable to implement this.\n. Cool, thanks. So now you merged your patch and mine into one? Since I see aspects of mine, but also applied to the noapi version of the engine.\n. Yes, it does. I'm not sure whether my patch is more comprehensive or yours is... with mine, you get a title like \"Wolfram|Alpha (2+2)\" and a content like \"Result: 4\" (I thought including \"Result:\" was important because it doesn't always say \"Result:\" and sometimes the output is quite obscure without that header to explain what it means), and it skips fields that just contain images or the like.\nMy patch appears to fail to meet some checks, though. I'm not sure I understand them.\n. We changed it a bit further to make sure a result is output even when the Wolfram API does not indicate any field as \"primary\"; in such cases, it will pick the first field after the \"Input\" one that contains any text at all. This is useful in a number of practical circumstances where, for whatever reasons, WA doesn't give any field the badge of \"primary\", even in cases where there's one field in total.\n. I hadn't really realized it was already there commented out. The commented out version says it worked erratically. This one has seemed to work okay for me so far, but at this point I suggest you reject or hold this merge request, as I'd like instead to add a few xpath configs for various other dictionaries, and I think we can review the matter again when I have them ready.\n. It has that, you can do exactly what you described... Try !yt for YouTube or !wp for Wikipedia or !was for Wolfram Alpha. You can also use !categories, like !video or !files.\n. I definitely vote \"keep\" as it's not at all uncommon for me to try to read pages that have ceased existing, but perhaps it could be hidden inside a less conspicuous dropdown arrow like Google have moved to doing.\n. Err, for the record, I thumbed up even though I meant \"keep\". It might be unintuitive that the thumbs-up means \"remove this\"... or I might simply be stupid.\nBut, a heads-up in case others did this.\n. I would make this a separate project. Searx would always have the ability to tightly integrate with it, but right now, Searx is good because what it does is relatively simply, yet stays up to date with what's required to keep working with its backends, as it's a type of project where bitrot runs fast and the main thing is to keep up... while making an \"actual\" search engine (a backend) is the opposite type of project, the one with slow planning and designing.. That should be fixed now, I reverted settings.yml to your version in master.\n. ",
    "ammarnajjar": "+1\n. An observation: I don't have that issue on my own hosted searx, also searx.me works fine, meanwhile https://search.klein-hitpass.net/ currently has that issue only when you choose to search in \"General\" (Allgemin) search.\n. Looks like the docs use https already:\n``` bash\n$ git clone https://github.com/asciimoo/searx.git -b gh-pages\n$ cd searx\n$ grep -nr 'git clone'\nstdout\ndocs/dev/quickstart.rst:19:    git clone https://github.com/asciimoo/searx.git\ndocs/dev/install/installation.rst:30:    sudo git clone https://github.com/asciimoo/searx.git\ndocs/dev/install/installation.rst:316:    git clone https://github.com/asciimoo/searx.git\n_sources/dev/install/installation.txt:30:    sudo git clone https://github.com/asciimoo/searx.git\n_sources/dev/install/installation.txt:316:    git clone https://github.com/asciimoo/searx.git\n_sources/dev/quickstart.txt:19:    git clone https://github.com/asciimoo/searx.git\n```\n. ",
    "Yodan": "+1\n. ",
    "logico-dev": "@PwnArt1st Can't replicate the bug, version of safari?\n\n. ",
    "jmaris": "This is really REALLY nice\n. Can be closed, sorry , I saw that thread but i didn't read down\n. ",
    "hdkw": "That'd be great! Searx would be the first engine to propose that!\nHere are the lists for filtering ads:\nhttps://easylist.github.io/\n. Hi! Any news for the possibility to search on adfree only websites?. ",
    "gregoirefavre": "No, I didn't do the dev's one which I just tried :\n`Collecting robotframework-selenium2library==1.7.4 (from -r /usr/local/searx/requirements-dev.txt (line 6))\n  Downloading robotframework-selenium2library-1.7.4.tar.gz (234kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 235kB 5.8MB/s\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n      File \"\", line 1, in \n      File \"/tmp/pip-build-afyawjqt/robotframework-selenium2library/setup.py\", line 7, in \n        from ez_setup import use_setuptools\n      File \"/tmp/pip-build-afyawjqt/robotframework-selenium2library/src/ez_setup.py\", line 106\n        except pkg_resources.VersionConflict, e:\n                                            ^\n    SyntaxError: invalid syntax\n```\n```\nCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-afyawjqt/robotframework-selenium2library/\n`\nThank for your answer. The problem now might (?) comes from python being python3 on my system and searx needs python2 ?\n. That's not so easy as lots of other packages requires python2 or python3.\nThanks for your help.\n. I learn quiete a few thanks to searx (and answers here) : it know works tremendously, all I needed was to install python2-virtualenv as my virtualenv was with python3 !!!\nThank you very much !!!\n. ",
    "opi": "@xinomilo 0.8.1 also have a bug with the parsing of google results, fixed in 0.9 ; I know several searx hosters which manually patch their 0.8.1 to keep google results.\n. ",
    "Erfahrungen": "\n@asciimoo I don't have any special setup (except some extra source IPs)\n\nWhere are you hosting those instances? Maybe you can give some advice on how to save money and create the ideal setup. Thanks in advance.\n. > by adding some unique features\nWhat do you have in mind?\n. ",
    "dadosch": "The API works like this:\nhttps://archive.org/help/aboutsearch.htm\n\nThe traditional method for API access for search is the Advanced Search API. The advanced search page describes the formats provided, and the query language for searching.\nWe limit the number of sorted paged results returnable to 10,000. Paged sorted results are supported only until the 10,000th result. For example, the search:\nhttps://archive.org/advancedsearch.php?q=subject:palm+pilot+software&output=json&rows=100&page=5\nshould be fine, but requesting page=10000 is rejected.\n\nIf you mean the Wayback Search, it is described here:\nhttps://archive.org/help/wayback_api.php\nMemento Protocol would maybe be more easier to implement. [http://timetravel.mementoweb.org/]\n. what about using temporary ipv6 addresses for google? Would that help, so that google can't flag us because of many requests per IP (is it??). It works! For nginx you need to add\nrewrite ^/searx(/.*)$ $1 break; to the location /searx/ block. The static files I am serving with nginx for now.\n```\n    location = /searx {\n        rewrite ^ /searx/;\n    }\nlocation /searx/static/ {\n        rewrite ^/searx/static(/.)$ $1 break;\n        root /srv/searx/searx/static;\n}\n    location /searx/ {\n        rewrite ^/searx(/.)$ $1 break;\n        try_files $uri @searx;\n    }\n    location @searx {\n        uwsgi_param SCRIPT_NAME /searx;\n        include uwsgi_params;\n        uwsgi_modifier1 30;\n        uwsgi_pass unix:/run/uwsgi/app/searx/socket;\n    }\n```\n. https://unsplash.com/search/photos\nParameters\nquery     Search terms.\npage  Page number to retrieve. (Optional; default: 1)\nper_page  Number of items per page. (Optional; default: 10)\ncollections   Collection ID(\u2018s) to narrow search. If multiple, comma-separated.\norientation   Filter search results by photo orientation. Valid values are landscape, portrait, and squarish.\nFrom: https://unsplash.com/documentation#search-photos. I get ./manage.sh: 8: ./manage.sh: npm: not found when I run manage.sh. (But I'm unsure wether this is because of a broken installation or not). Tests needed. ~~aarg i messed up the squash~~ This is better now. Are tests ok like this?. For me, it works fine with my own instance. I use https://myinstance.tld/searx?q=\\{@} (It's in a subfolder). Ok, I can change that.. ",
    "jan-kleks": "@Erfahrungen I mean stuff like this:\nUse adblocking list to limit search results to adfree websites #568 \ndirect torrent download via webtorrent #123\n [plugin] download video #537 \nSomething that would make searx stand out from the crowd. \n. @kvch \n\nJust to clear things up, is your problem is that a few search engines (Google, DDG) block your IP and does not return any results?\n\nNope, I have not experienced this issue, but I know it exists.\n\nIf so, I don't think that making searx.me the central instance for others is the way. [...]\nAnother type of search could be implemented where the request of the user would go through more instances without centralization, but still I don't think that is the way either.\n\nI don't want searx.me to act as the central instance, but as the central interface for other instances. Maybe the example with Tor nodes wasn't the best... I would like this central interface to automatically (but it also should be made configurable) switch instances under-the-hood. The switching could be based on time (e.g. once an hour), location (an instance closer to the user), amount of traffic (to avoid situations like the one with missing results, described by you), etc. I don't think it is necessary for the search to go through multiple instances at once either. As you can see, the system described here is still decentralized, but only the interface is central. And the user don't have to go through long lists of instances. Anyway, don't most people use searx.me as their main instance already?   \nMaybe an alternative would be to teach instances to talk to each other, so that any searx instance would be able to do this instance switching, and to share traffic. Hmm...\n. ",
    "davidar": "Ok\n. @asciimoo using the xpath engine instead now :)\n. @asciimoo Possibly?\nThe plugin isn't specific to this engine, as any engine could potentially return DOI links. I probably should have made these two separate PRs.\n. @asciimoo Done.\nI also updated the plugin so that it's able to catch more links now (not limited to just the doi.org domain)\n. Here's a quick snippet that does this for the oscar theme:\njs\n$(document).ready(function() {\n    var win = $(window);\n    win.scroll(function() {\n        if ($(document).height() - win.height() == win.scrollTop()) {\n            var formData = $('#pagination form:last').serialize();\n            $('#pagination').html('Loading next page...');\n            $.post('/', formData, function (data) {\n                var body = $(data);\n                $('#pagination').remove();\n                $('#main_results').append(body.find('.result'));\n                $('#main_results').append(body.find('#pagination'));\n            });\n        }\n    });\n});\nHowever, I'm not sure what the best way to generalise it for all the themes would be.\n. @asciimoo ah, that makes it easier :)\n. Another possibility could be to push the footer down slightly, to unconditionally force the existence of a scrollbar.\n. @asciimoo done. oops, fixed. Yeah, I wasn't sure about that. Fixed.\n. ",
    "firebovine": "I merged a01200356 's changes, and made two changes so in the event there is no primary text pod returned, it'll choose the first non-Input text pod (ie: test with a search !wa 127.0.0.1).\nSorry if I violated any git workflow! I think I did it correctly!\n. ",
    "mmuman": "Sh*, unit tests :D\n. Should be working now.\n. Oh I didn't notice searx did its own highlighting of the search term in a different way.\n. ",
    "Pulsera": "Sorry to be late to the conversation.\nBut how about when I go to searx.me I'm connecting to a random user? or its hosted by the creater for searx\n. ",
    "will76": "What about 'last year'? It's the best option to search for updated articles on a subject.. ",
    "ThePreviousOne": "@asciimoo searx.me certificate expired, thought I'd let you know\n. ",
    "mcl1234": "thanks a lot ! :)\n. ",
    "johnjago": "Closing because the issue was fixed (and the design changed anyway in #837, so this is no longer relevant).. ",
    "torchhound": "Sorry about that, I must have missed that. I'll fix it.\n. ",
    "nerzhul": "i should test the port revision on FreeBSD to ensure it's compatible with ports tree\n. I have upgraded my deps freebsd & searx are synced on same version but i have the following error:\n/usr/local/lib/python2.7/site-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.babel is deprecated, use flask_babel instead.\n  .format(x=modname), ExtDeprecationWarning\nCRITICAL:searx.plugins:missing attribute \"description\", cannot load plugin: <module 'searx.plugins.https_rewrite' from '/usr/local/lib/python2.7/site-packages/searx/plugins/https_rewrite.pyc'>\n. sorry this is due to local installation + package installation on my server, ignore the previous comment. The current version works correctly\n. @asciimoo i prepared the port diff to send to FreeBSD, when you tag, tell me, i will test the updated packaged version\n. just install python-certifi\n. if you move a project, github will redirect old project to new project (302) and forks are kept.\nLook at this project for example: https://github.com/nerzhul/ocsms. yeah i have a dockerhub image on ARM64 too, i hope we can have a new tag asap with new features and/or bugfixes.\nHe should find a good co maintainer, it's not easy. thanks @kvch . ",
    "MichaelSchoebel": "I'm the person running DeuSu. There is a free API in testing now. Will probably be ready for production use this month.\nThere is also https://deusu.org which has slightly different ranking. deusu.de ranks for a German speaking audience while deusu.org ranks for English speaking audience. There will be APIs for both.\n. ",
    "pydo": "Why can't nixos use the more recent version of the dependencies? Are they not using pip as the package manager? \n. @asciimoo seedpeer.eu has been disabled by default. Also sorry about the commit history, I'm not completely conformable with rebasing.\n. @asciimoo default categories have been set. Hopefully I haven't forgotten anything else.\n. All you did was change the TLD in the unittests\n. Where did you test this? I tested on https://searx.me/ and can confirm the behavior. But when testing locally and on other searx instances it worked fine. For example this instance https://searx.ch/ can use startpage just fine. I'm going to assume that startpage simple banned/block the ip of the main searx instance since it gets the most traffic.\nIf the issue is because startpage blocked searx ip then maybe this is just a UX issue. Maybe a message telling the user to add more search engine providers could be helpful.\n. @asciimoo this issue can be closed.\n. Did you run pip install -r requirements.txt from within a virtual environment?\nThe certifi package is in the requirements.txt\n. Can you provide more information? Like what search engines you were using. Whether you were testing locally or using a searx instance. Also if this is random it could just be requests timing out.\n. Just tested locally can confirm. The regex parsing string is what seems to be broken. I'll see about making a PR to fix it later.\n. So I found the swisscows endpoint that returns the search results.\nhttps://swisscows.ch/?query={query}+hundefutter&region=de-CH&uiLanguage=browser&_={Date}&apiGuard={token}'\nThe Date is a derived from javascript new Date().valueOf().\nThe api token comes from making a GET request to https://swisscows.ch/generateApiGuardToken,\nHere's some example code to retrieve the search results:\n```\nimport requests\nimport time\nimport datetime\nresp = requests.get('https://swisscows.ch/generateApiGuardToken')\napi_token = resp.json()['token']\ntime_stamp = time.mktime(datetime.datetime.now().timetuple()) * 1000\nheaders = {'X-Requested-With': 'XMLHttpRequest', 'Cookie': 'hash={}'.format(resp.cookies['hash'])}\nresp = requests.get('https://swisscows.ch/?query=dogdog+hundefutter&region=de-CH&uiLanguage=browser&_={}&apiGuard={}'.format(time_stamp, api_token), headers=headers)\nprint(resp.status_code)\nprint(resp.json())\n```\nThis problem is the request only works about 1 out of 10 times. Most of the time the 2nd request just returns a 403. \nIf anyone has any ideas on how to consistently call this endpoint so it returns 200 reponses please let me know. If not I suggest we drop support for swisscows.\n. So it looks like the path to your static files isn't setup right (css, js, img). If you open the network tab in your browser you can confirm this if you see a bunch of 404s.\nAlso have you tried deploying with docker? it might be easier\n. This is the regex that's the problem re.compile(r'(wkey|wemail)[^&]+&?'). It matches the wkey in viewkey.\nIt's in the tracker_url_remover.py\n. There is a workaround, you can use a searx instance hosted in a different country. \nhttp://stats.searx.oe5tpo.com/\nIf the country in question is not available you can always take the initiative to host an instance yourself.\n. > Thanks but I'm aware of this like you can see in my example case\nMy bad I only skimmed over your post.\nDo you know of any way of detecting when a search engine is omitting results due to take-down requests?\nI don't have any good ideas on how to tackle this problem.\n.  The pass is redundant\n. Also redundant\n. Some of the content on the site is not transmitted over https so you might have to disable it by default. One of the project's devs could probably chime in.\n. @Athemis Ah my mistake then. If searx is requesting everything over https, I guess there's no problem.\n. ",
    "Profpatsch": "It\u2019s not a problem with nix per-se, but rather every package manager. If the versions are fixed, the packager has to jump through hoops to get the versions right (in a consistent fashion with other packages). Nix makes this easier than most, since you could have more than one version of a package (which you also have to support afterwards).\n. > I think a lot people installs searx dependencies in a virtualenv. This is the documented way to install it.\nWhatever does that have to do with packaging for distros?\n. According to my searches in the source it\u2019s only used in one place, the image proxy:\nsearx/webapp.py\n106:app.secret_key = settings['server']['secret_key']\n283:    h = hmac.new(settings['server']['secret_key'], url.encode('utf-8'), hashlib.sha256).hexdigest()\n687:    h = hmac.new(settings['server']['secret_key'], url, hashlib.sha256).hexdigest()\nInstead of having a secret like this in the config file (which means it can\u2019t be checked in anymore), why not generate it and put it in the cache dir?. > which overwrites the value defined in settings.yml\nThe whole point is to remove the  secret_key, since it\u2019s sensitive data in the config which is completely unnecessary.\nWe could of course skip the dependency and implement this part of xdg-directory manually, but I don\u2019t see what harm a pure python implementation of xdg does.. > 0. Travis fails.\nThere were minor pep warnings, which I fixed.\n\nYou submitted multiple things in a single PR\n\nI can create a new PR for the dependencies if it is wanted.\n\nsetting it as an environment variable. This solution requires less change in the current behaviour then yours. The result would be the same.\n\nThat is not a correct.\nThere is no change in the practical behaviour, as the config variable is superfluous in any case, since the user cannot influence much by setting it manually. It could even be changed to not persist the key and just create one on program start, which would simplify the patch considerably actually. The new dependency could also be removed again.. The load-balancing aspect of a bigger setup is very valid, yes. The example of sharing links with friends reminds me of Windows working groups.\nWhat do you think of making local keys (different one for each run, so it doesn\u2019t have to touch the file system) the default case, optionally grabbing a provided key from the environment?\nI still think it should be removed from the config if possible.. > It looks as if this would make life much more difficult for those of us who use Searx for its API functionality only.\nCare to elaborate?. Right, that slipped in.. I rebased the line, it should be correct now.. ",
    "thiswillbeyourgithub": "@qazip I would prefer instead of \"!!yt\" to just make it so that when using a bang without keywords it goes straight to the website page by default. For example : !yt keyword search for keyword on youtube but !yt goes straight to youtube's startup page. It works this way on duckduckgo and qwant and I think that's the best way.. Well, you can apparently put language or bang first in any order but :en searx !go doesn't work as expected.\nI'd like to be able to type my keywords and then append a seach bang at the end if I want to.. @asciimoo just for the record, you mean that !! should send to the website of the bang?\nfor example : !!imdb test should lend you straight to the results of imdb on their website?\nTo put it in another way : !!imdb test on searx would be the same as !imdb test on ddg/qwant ?\n. @shumvgolove as a way to differentiate ! from !!. It was a mere proposal but it could work. The idea is that sometimes you want to go straight to a website (like the bangs of qwant or duckduckgo) and sometimes you just want the results returned from a website. I explained this more precisely above :).. I'm not a contributor but I don't think your question is clear. If I were you I'd rephrase this to reduce their workload :). @asciimoo as I said, the syntax is detailed in github (the site you linked is on github), but IMO few people who use searx will go there. At the minimum there needs to be a reference to the page you linked in the preferences page like Click here to see how to use the shortcuts.\nAnd as you said the shortcuts like \"wp\" are indeed in the preference page but it's not obvious how to use them. Someone who's never used ddg or qw will never think of adding a \"!\" before the shortcut and before the keywords.. ",
    "fmbento": "Consider removing the image before its rebuild (\"docker rmi\" of it -- in\ncase there's some entry that dockers considers to be already covered by an\nexisting layer of that image).\n2016-09-07 0:15 GMT+01:00 NIXOYE notifications@github.com:\n\nsed: searx.setting.yml: No such file or directory\nTraceback (most recent call last):\nFile \"searx/webapp.py\", line 826, in\nrun()\nFile \"searx/webapp.py\", line 781, in run\nhost=settings['server']['bind_address']\nFile \"/usr/lib/python2.7/site-packages/flask/app.py\", line 772, in run\nrun_simple(host, port, self, *\n_options) File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\",\nline 694, in run_simple inner() File\n\"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 656, in inner\nfd=fd) File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line\n550, in make_server passthrough_errors, ssl_context, fd=fd) File\n\"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 464, in init\nHTTPServer.init(self, (host, int(port)), handler) File\n\"/usr/lib/python2.7/SocketServer.py\", line 417, in init self.server_bind()\nFile \"/usr/lib/python2.7/BaseHTTPServer.py\", line 108, in server_bind\nSocketServer.TCPServer.server_bind(self) File\n\"/usr/lib/python2.7/SocketServer.py\", line 431, in server_bind\nself.socket.bind(self.server_address) File \"/usr/lib/python2.7/socket.py\",\nline 228, in meth return getattr(self._sock,name)(_args)\nsocket.error: [Errno 13] Permission denied\nsed: searx.setting.yml: No such file or directory\nTraceback (most recent call last):\nFile \"searx/webapp.py\", line 826, in\nrun()\nFile \"searx/webapp.py\", line 781, in run\nhost=settings['server']['bind_address']\nFile \"/usr/lib/python2.7/site-packages/flask/app.py\", line 772, in run\nrun_simple(host, port, self, *\n_options) File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\",\nline 694, in run_simple inner() File\n\"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 656, in inner\nfd=fd) File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line\n550, in make_server passthrough_errors, ssl_context, fd=fd) File\n\"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 464, in init\nHTTPServer.init(self, (host, int(port)), handler) File\n\"/usr/lib/python2.7/SocketServer.py\", line 417, in init self.server_bind()\nFile \"/usr/lib/python2.7/BaseHTTPServer.py\", line 108, in server_bind\nSocketServer.TCPServer.server_bind(self) File\n\"/usr/lib/python2.7/SocketServer.py\", line 431, in server_bind\nself.socket.bind(self.server_address) File \"/usr/lib/python2.7/socket.py\",\nline 228, in meth return getattr(self._sock,name)(_args)\nsocket.error: [Errno 13] Permission denied\nsed: searx.setting.yml: No such file or directory\nTraceback (most recent call last):\nFile \"searx/webapp.py\", line 826, in\nrun()\nFile \"searx/webapp.py\", line 781, in run\nhost=settings['server']['bind_address']\nFile \"/usr/lib/python2.7/site-packages/flask/app.py\", line 772, in run\nrun_simple(host, port, self, *\n_options) File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\",\nline 694, in run_simple inner() File\n\"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 656, in inner\nfd=fd) File \"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line\n550, in make_server passthrough_errors, ssl_context, fd=fd) File\n\"/usr/lib/python2.7/site-packages/werkzeug/serving.py\", line 464, in init\nHTTPServer.init(self, (host, int(port)), handler) File\n\"/usr/lib/python2.7/SocketServer.py\", line 417, in init self.server_bind()\nFile \"/usr/lib/python2.7/BaseHTTPServer.py\", line 108, in server_bind\nSocketServer.TCPServer.server_bind(self) File\n\"/usr/lib/python2.7/SocketServer.py\", line 431, in server_bind\nself.socket.bind(self.server_address) File \"/usr/lib/python2.7/socket.py\",\nline 228, in meth return getattr(self._sock,name)(_args)\nsocket.error: [Errno 13] Permission denied\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/680#issuecomment-245125453, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AHN-PLAUlJws7VM0nTFSJ67WVdMCUo3mks5qnfQWgaJpZM4J2U8R\n.\n. Sorry, missed this reply of yours.\n\ndocker rmi  or \nGet the image id via\ndocker images\nImage name would be, in your case, nix/searx\nClean up all the :\ndocker rmi $(docker images -q -f dangling=true)\nHope this helps, at least to free some space. Please do let us know how it\ngoes.\nThanks,\nFilipe\n2016-09-07 20:24 GMT+01:00 NIXOYE notifications@github.com:\n\n@fmbento https://github.com/fmbento so i do 'docker rmi (docker\nnumber)' ?\n@Wonderfall https://github.com/Wonderfall any thoughts on the logs?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/680#issuecomment-245389076, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AHN-PGLD3i_psVySeLIVkpz9mHOBHD1Aks5qnw97gaJpZM4J2U8R\n.\n. \n",
    "Eig8phei": "Thanks.\n. @potato This is for the style of the theme not the theme itself. For example the Pointhi style for the Oscar theme.\n. ",
    "TheChiefMeat": "I'd also like to see this so I can set the default style to Logicodev dark.. Nevermind, I used another version of Linux and it worked itself out. ",
    "silvernode": "I would like this as well. This would allow us to have a dark theme for all users by default. Old thread but I just named my image the same as the one in /usr/local/searx/searx/static/theme/oscar/searx_logo_a.png and overwrote it. For each style there is a different image to replace so keep that in mind.. Well it's happening again all of a sudden\nI get the red error on the side bar in my self hosted searx v0.14.0 saying Google wants a captcha :(. ",
    "nevtag": "```\n\nServerName searx.xxxx.de\n\n    Options FollowSymLinks Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n    Order deny,allow\n    Deny from all\n    Allow from ipservice-xxx.pools.vodafone-ip.de \n\nErrorLog /var/log/apache2/searx_de_error.log\nLogLevel emerg\nTransferLog /var/log/apache2/searx_de_access.log\n\n```\nWell, I have tested it, with proxypass to localhost:8888 and start searx from hand, and that works.\nSo I think it must be a configuration problem with apache or uwsgi\n. My workaround is using the apache proxypass, so I can use it on my virtual root server.\n```\n\nServerName searx.xxx.de\nProxyPreserveHost On\n  ProxyRequests Off\n  ProxyPass / http://localhost:8888/\n  ProxyPassReverse / http://localhost:8888/\n\n    Options FollowSymLinks Indexes\n    #SetHandler uwsgi-handler\n    #uWSGISocket /run/uwsgi/app/searx/socket\n    Order deny,allow\n    Deny from all\n    Allow from ipservice-xxxx.pools.vodafone-ip.de \n\nErrorLog /var/log/apache2/searx_de_error.log\nLogLevel emerg\nTransferLog /var/log/apache2/searx_de_access.log\n\n```\n. If I call searx.domain.com/search it is working.\n. The problem is solved with apache mod_uwsgi_proxy.\nsearx.ini\n```\n[uwsgi]\nsocket = 127.0.0.1:8080\nWho will run the code\nuid = searx\ngid = searx\ndisable logging for privacy\ndisable-logging = true \nNumber of workers (usually CPU count)\nworkers = 1\nThe right granted on the created socket\nchmod-socket = 666\nPlugin to use and interpretor config\nsingle-interpreter = true\nmaster = true\nplugin = python\nModule to import\nmodule = searx.webapp\nVirtualenv and python path\nvirtualenv = /usr/local/searx/searx-ve/\npythonpath = /usr/local/searx/\nchdir = /usr/local/searx/searx/\n```\napache searx.conf\n```\n    ProxyPreserveHost On\n    ProxyRequests Off\n    ProxyPass / uwsgi://localhost:8080/\n    ProxyPassReverse /  uwsgi://localhost:8080/\n```\n. First off all, this is not an error, it's a warning.\nAnd what to do is still standing in the warning: upgrade openssl above 1.0.2.\nBut searx will run with your old openssl version.\n. Sorry, was not yet fully awake when I replied.\nDid you install all the needed packages?\n. Ok, I test the installation on my jessie virtual machine clone(basic installation only web server, ssh deamon), and I must say it is running with no errors.\nWell the openssl warning was shown.\nI think delete the searx dir and try the installation again.\n. I have made some tests, on my gateway at home, ubuntu 16.04.1 LTS, the error did not occur.\nOn my virtual root server  the error occur. Same ubuntu version after upgrade, heavy work with change from php5 to php7.\nI will setup a proxy on my root server and will search about it over google to see if I got a captcha.\n. That's right, google throws a captcha.\nBut! If you have the chance that you can use a browser on that machine and google has asked you 5 or 6 times with the captcha, than it worked with searx.\nSeams so, that the ip removed from the list.\n. ",
    "Grotax": "I have the same problem followed this tutorial https://asciimoo.github.io/searx/dev/install/installation.html \nFound nothing in the apache2 log or the uwsgi/searx log also did enable debug mode in searx to get more infos but no errors. \nAlso note that \"preferences\" and \"about\" work.\n```\n\n\n                ServerAdmin ...\n                ServerName ...\n            ErrorLog ${APACHE_LOG_DIR}/error.log\n            CustomLog ${APACHE_LOG_DIR}/access.log combined\n\n            SSLEngine on\n\n            SSLCertificateFile ...     \n            SSLCertificateKeyFile ...\n            SSLCertificateChainFile ...\n\n            <Location />\n                Options FollowSymLinks Indexes\n                SetHandler uwsgi-handler\n                uWSGISocket /run/uwsgi/app/searx/socket\n            </Location>\n    </VirtualHost>\n\n\n```\n```\n[uwsgi]\nWho will run the code\nuid = searx\ngid = searx\ndisable logging for privacy\ndisable-logging = false\nNumber of workers (usually CPU count)\nworkers = 2\nThe right granted on the created socket\nchmod-socket = 666\nPlugin to use and interpretor config\nsingle-interpreter = true\nmaster = true\nplugin = python\nModule to import\nmodule = searx.webapp\nVirtualenv and python path\nvirtualenv = /usr/local/searx/searx-ve/\npythonpath = /usr/local/searx/\nchdir = /usr/local/searx/searx/\n```\nRunning it via python searx/webapp.py on my private ip did work for me.\n. Changed my config. Now using mod_uwsgi_proxy, works.\nHint package name in Ubuntu: libapache2-mod-proxy-uwsgi\nThanks :)\n. ",
    "MrPetovan": "@asciimoo I still have this issue. I don't want to open any more port than I need, and the socket /run/uwsgi/app/searx/socket is correctly created by uwsgi. I followed the Installation page to the letter, the check passed and I didn't run python searx/webapp.py again after exiting the virtualenv bash.\nSpecs:\n- Debian Jessie\n- OpenSSL 1.0.1t, getting the Warning shown in #688 \n- Apache 2.4.10\nConfig uwsgi:\n```\n[uwsgi]\nWho will run the code\nuid = searx\ngid = searx\ndisable logging for privacy\ndisable-logging = true\nNumber of workers (usually CPU count)\nworkers = 4\nThe right granted on the created socket\nchmod-socket = 666\nPlugin to use and interpretor config\nsingle-interpreter = true\nmaster = true\nplugin = python\nModule to import\nmodule = searx.webapp\nVirtualenv and python path\nvirtualenv = /usr/local/searx/searx-ve/\npythonpath = /usr/local/searx/\nchdir = /usr/local/searx/searx/\n```\nConfig Apache:\n```\n\n\n  ServerName searx.example.com\n\n    Options FollowSymLinks Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n  \n#CustomLog /dev/null combined\nSSLCertificateFile /etc/letsencrypt/live/searx.example.com/fullchain.pem\n  SSLCertificateKeyFile /etc/letsencrypt/live/searx.example.com/privkey.pem\nInclude /etc/letsencrypt/options-ssl-apache.conf\n\n\n```\nLog uswgi:\nFri Nov 11 06:06:09 2016 - *** Starting uWSGI 2.0.7-debian (64bit) on [Fri Nov 11 06:06:09 2016] ***\nFri Nov 11 06:06:09 2016 - compiled with version: 4.9.1 on 25 October 2014 19:17:54\nFri Nov 11 06:06:09 2016 - os: Linux-3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1+deb8u6 (2015-11-09)\nFri Nov 11 06:06:09 2016 - nodename: example.com\nFri Nov 11 06:06:09 2016 - machine: x86_64\nFri Nov 11 06:06:09 2016 - clock source: unix\nFri Nov 11 06:06:09 2016 - pcre jit disabled\nFri Nov 11 06:06:09 2016 - detected number of CPU cores: 8\nFri Nov 11 06:06:09 2016 - current working directory: /\nFri Nov 11 06:06:09 2016 - writing pidfile to /run/uwsgi/app/searx/pid\nFri Nov 11 06:06:09 2016 - detected binary path: /usr/bin/uwsgi-core\nFri Nov 11 06:06:09 2016 - setgid() to 1020\nFri Nov 11 06:06:09 2016 - setuid() to 1020\nFri Nov 11 06:06:09 2016 - your processes number limit is 31903\nFri Nov 11 06:06:09 2016 - your memory page size is 4096 bytes\nFri Nov 11 06:06:09 2016 - detected max file descriptor number: 1024\nFri Nov 11 06:06:09 2016 - lock engine: pthread robust mutexes\nFri Nov 11 06:06:09 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\nFri Nov 11 06:06:09 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/searx/socket fd 3\nFri Nov 11 06:06:09 2016 - Python version: 2.7.9 (default, Jun 29 2016, 13:11:10)  [GCC 4.9.2]\nFri Nov 11 06:06:09 2016 - Set PythonHome to /usr/local/searx/searx-ve/\nFri Nov 11 06:06:09 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\nFri Nov 11 06:06:09 2016 - Python main interpreter initialized at 0x24009f0\nFri Nov 11 06:06:09 2016 - your server socket listen backlog is limited to 100 connections\nFri Nov 11 06:06:09 2016 - your mercy for graceful operations on workers is 60 seconds\nFri Nov 11 06:06:09 2016 - mapped 363840 bytes (355 KB) for 4 cores\nFri Nov 11 06:06:09 2016 - *** Operational MODE: preforking ***\nFri Nov 11 06:06:09 2016 - added /usr/local/searx/ to pythonpath.\nWARNING:searx:You are using an old openssl version(OpenSSL 1.0.1t  3 May 2016), please upgrade above 1.0.2!\nFri Nov 11 06:06:12 2016 - WSGI app 0 (mountpoint='') ready in 3 seconds on interpreter 0x24009f0 pid: 30844 (default app)\nFri Nov 11 06:06:12 2016 - spawned uWSGI master process (pid: 30844)\nFri Nov 11 06:06:12 2016 - spawned uWSGI worker 1 (pid: 30877, cores: 1)\nFri Nov 11 06:06:12 2016 - spawned uWSGI worker 2 (pid: 30878, cores: 1)\nFri Nov 11 06:06:12 2016 - spawned uWSGI worker 3 (pid: 30879, cores: 1)\nFri Nov 11 06:06:12 2016 - spawned uWSGI worker 4 (pid: 30880, cores: 1)\nThanks in advance of any help.\nEdit: More details: the search/settings.yml references a localhost:8888 server but nothing is listening on it.\nsettings.yml \nserver:\n    port : 8888\n    bind_address : \"127.0.0.1\" # address to listen on\n    secret_key : \"[actual secret]\" # change this!\n    base_url : False # Set custom base_url. Possible values: False or \"https://your.custom.host/location/\"\n    image_proxy : False # Proxying image results through searx\nI thought uwsgi was taking care of that?\n. @asciimoo Thanks for the prompt response, others have circumvented the issue by manually running the Python app listening to the provided port, but I'd rather not do that. Do you have any clue/debug hints to fix this / page issue? I'm sorry I don't have any Python background to debug it myself.\nI'd love to have a functional searx node without having to resort to tricks.\n. For anyone else wondering, the package libapache2-mod-proxy-uwsgi doesn't work with Unix sockets until version 2.0.11. However, the Debian Jessie package still is in version 2.0.7 despite the fact that the Apache2 package is more recent and allows Unix sockets in ProxyPass directives.\n. I just tried to reinstall searx 0.12.0 from the installation docs and everything went well. Not sure what I did wrong the first time.. @Vertux I\u2019m sorry, I kept getting timeouts while retrieving results, so I removed searx entirely from my system.. ",
    "vedranmiletic": "I can confirm same issue on Apache 2.4.25 on Debian 9. Doesn't happen on Nginx with Debian 9. I'm using the same configuration as the installation docs.. I got it working with mod_proxy_uwsgi as suggested above, I'll provide a patch for the docs.. ",
    "valodiadeseynes": "I'm technically still having this issue with version 0.12.0 on Debian 9 (with apache2). However, I've found a solution by changing searx's location with apache from / to /searx and it works.\nHere is an excerpt from my apache config:\n            RewriteEngine on\n            RewriteRule   \"^/$\"  \"/searx/\"  [R]\n            <Location /searx>\n                Options FollowSymLinks Indexes\n                SetHandler uwsgi-handler\n                uWSGISocket /run/uwsgi/app/searx/socket\n            </Location>\n\nThe other solutions in this thread didn't work for me but this one does. Just throwing it out there in case somebody else is in my situation.. ",
    "Vertux": "This issue still persists with v0.14.0 (I just did a fresh install on a Ubuntu 16.04 server), the libapache2-mod-proxy-uwsgi solution, does not solve the problem for me. It would be nice if you could take a closer look into this issue @asciimoo \n@valodiadeseynes modification solves the issue but leads into another problem, which I have mentioned here, at that time I did nit know that the rewrite rule triggers the issue.\n@MrPetovan I really would like to know why the issue disappeared suddenly - I did the installation twice and followed exactly the installation docs like you, but it still does not work. I have no idea what to do.. @Pofilo thank you for the response.\nI did the \"classic\" installation with Git with default values. If I start searx on the console (without uwsgi), the access with links/curl works fine, no problem. But when Apache and uwsgi comes into play and I access the subdomain I get the \"Page not found\" @nevtag already mentioned. If I add \"/search/\" to the subdomain name I get the right search page, which works fine.\nAt the moment I have shut down my public searx instance. Just for information it is a V-Root-Server with Plesk running Ubuntu 16.04. It is really a weird behavior - although it is a pretty \"simple\" installation. \nI just got the idea, that ModSecurity could be the reason for these issues - I have to check this, if I have a little more time.\nAppendix: I did another test installation in a VM (Ubuntu 16.04) on a local server with Nignx, which runs perfectly fine. \n. I still got this problem with v0.14.0 no matter if I choose \"de_DE\" or \"en\" : \nhttps://searx.togart.de/searx/?q=f%25C3%25BCr\nMost of the time I use MacOS 10.13.3 + Safari 11.0.3 with the Extension AnySearch (Custom search URL: https://searx.togart.de/?q=@@@)\nI already have asked the developer of Anysearch for help but I did not get an answer yet.\nI am not sure if the issue is related to AnySearch or searx.\n. I can confirm this issue.. ",
    "matzZz": "Put \u201aDirectoryIndex disabled\u2018 in .htaccess.\nWorks for me.. ",
    "joshp23": "Running searx 14 on ubuntu 16.04 with apache2, same errors and behavior: \n-  when attempting to use wsgi according to the docs here I get a \"file not found\" error. \n-  when running manually with python searx/webapp.py and using mod_proxy in apache, it works.\n-  using the libapache2-mod-proxy-uwsgi solution provided by @nevtag works.\nIt would be great if this were in the docs ;). +1 for this. Being able to have DOI resolver on by default would be fantastic.. sorry, yes. ",
    "cubiq": "in case anyone is still having troubles I had to use the proxy approach but change socket = 127.0.0.1:8080 to http = 127.0.0.1:8080. Also I had to add buffer-size = 32768 to the searx.ini. has this been acknowledged?\nI'm having the same issues since 0.15.0\nI get various error messages like:\nEngines cannot retrieve results:\ngoogle images (unexpected crash: line 51: Tag wbr invalid (line 51))\nEngines cannot retrieve results:\ngoogle images (unexpected crash: line 39: htmlParseEntityRef: expecting ';' (line 39))\nEngines cannot retrieve results:\ngoogle images (unexpected crash: line 32: Tag footer invalid (line 32)). ",
    "juppin": "For me it works if i add \"DirectoryIndex disabled\" to the location block as @matzZz  mentioned for his htaccess file.\n<Location />\n          DirectoryIndex disabled\n          Options FollowSymLinks Indexes\n          SetHandler uwsgi-handler\n          uWSGISocket /run/uwsgi/app/searx/socket\n        </Location>\nPlease test this fix and update doc if it works for everyone.\nI think this will be the cleanest solution!. ",
    "steeff": "confirmed that DirectoryIndex disabled solved the issue for me. thanks. ",
    "zafai": "Thank you for you reply please read the traceback Part of the warning that is the reason the server is not starting. Thank you\n. No problem yes all packages that are in the wiki install manual but I still get this error \n. Thank you for your fast help i just try is again and with the fix from another thread it worked \nhttps://github.com/asciimoo/searx/issues/686\n. ",
    "MichaelZebra": "Hi mindstormer12\nThank you very much for your input / thoughts and interaction with the Project. I just want to state my thoughts, interpreting and analyzing your post. Startpage is creating - every time i save my settings - a new, but unique URL (not sure if UserAgent/IP or UserAgent/OS/IP/etc. are creating that url: further research is needed). \nHowever, this is a real privacy-, and also a security-issue for anyone associated with that URL, but specially for lawyers, journalists, medical personnel, opposition groups, whistle-blowers and many others. Basically it is an issue for the constitutionally guaranteed right for privacy of every citizen in the western hemisphere.\nWhy? => Because anyone could possibly observe that URL (now, or in future attacks against ssl/tls/dnssec/ecdh/rsa,sha256/sha512/shaN,cbc,whirlpool,AES,etc. using historically recorded network data like all the 'Services' do).\nTherefore it would be better to group preferences into vectors (or possibly technically into same xml or txt files) and deliver preferences for settings uniquely to all, who decided/took same preferences. This means basically n:1 correlations, many different people [n] chose the same profile of settings [1] => they receive the same URL. This makes it harder to identify unique individuals by GET or POST statements transferred over the network, which may be attacked by a foreign secret-, military-, governmental- or commercial-, - service.  \nTo ensure the protection of children and leaking the URL over time, it could be rolled over on a weekly basis. A cookie (cleanable by user = user control), but also a n:1 , unique URL for many users (users n : 1 url) would be therefore a better solution.\nTechnically it shouldn't be such a issue to pass [$searchterm( (if cookie || url )expired) ; to the $preferences; save(prefrences,$searchterm); saveNewPreferencesInCookie($UserCookie || $URL) ; pass($prefrencesCookie || $newURL){search($preferencesCookie||$newUrl){ do search} ]\n;) \n. @asciimoo :+1: True that.  But most of the visitors of searx won't set up a own instance and would probably use publicly available instances. This leads to another question: how to deliver click'n'play instances to everyone? And how to hinder data-hungry, evil companies of adopting and setting up own powerful instances and gathering data? Therefore the question minstormer12 raised, is quite important.\nThese 'private' instances should be easy to deploy, hardened and so on. Besides clouds (which i don't like very much), in a home or privately administered setting, it would need really good protection. Something like pfsense, ipfire, some vpn, enforced TLS or SSH, hardened entrypoints and so on. I don't want to be the negative, but own instances are raising some questions. My mom isn't able to set up her own instance, neither is my girlfriend. Though, both would be glad to have their own, private search engine. \nJust imagine: cheap hardware, like a RaspberryPi, an image to expand (containing pfsense or ipfire or any safe firewall before searx)... That would be a real change in control of data!  Hmmm i like! I know, this is completely out of focus of the project, but maybe someone reads this and gets an idea! :)\nBesides the project, which is very, very cool, we need really to think how to pass this beyond people who are not just aware, but also technically not fit enough. \n. Hi dalf\nSorry for the delay in responding. And also please excuse me on pushing security that hard. But privacy and security goes hand in hand. Especially since every single government of the free world decided to implement mass surveillance on it's own citizens. \nI do agree  on the TLS / Cookie point. URL's are way worse. But cookies have also some issues  though.  I'm thinking about access to local cache (from a server point of view: never trust the client),  intercepting them, using OAuth2, sharing cookies, etc.\nWhich leads me to another issue with TLS or SSL: RC4 and 3DES are still enabled in recent major browsers due to backwards compatibility. Just try in the url bar 'about:config' in Firefox and search for \"rc4\"... And therefore TLS is a weak point for an attacker to go for. Breaking weak algorithms in the network isn't that hard for an state level attacker or another well funded criminal organization. Cookies are not safe in transit or on non-trusted platforms. We should keep in mind that potentially journalists or opposition members in totalitarian systems will use these (some of them, publicly available) services. These people rely for their life and well being on a secure service. I know, I pushed it to far, right now. But that's something we should definitely keep in mind for further development. \n-> I agree therefore on a network level safety in cookies as better privacy than URL requests.\nFor the log thing. I am not so deep in apache, nor have I experience with nginx . But isn't it possible to disable logging? If so, the approach on easing the installation with scripts is a really good way to go!\nIf it should be python or rather GO, honestly I really can't tell. Since I am more in requirements engineering and not so much in programming (honestly no experience with GO) I cannot express a qualified opinion. But I think that python will just be better understood, since it's just way longer available and it is also already integrated in most 'nix platforms. No judging on that, just saying for the ease of spread... \nThis leads us right to the next very, very good idea you brought up: packaging. In my opinion this is crucial for success.  At least a .deb package should be available, given the fact that Debian based distributions are the most used and most wide spread nowadays. Indeed I am a Fedora (.rpm) child nowadays, but given the fact that Ubuntu and moreover the RaspberryPi and its deb-based OS are the most common ly widespread operating systems, it would be that: A .deb package. Hoping for .rpm as well, but that's just me, selfish and lazy...\nDOCKER on the other hand would be the best choice. It is for sure safer than any 'just' .deb or .rpm package. But docker is not that much known already. I know it could be packaged with docker - but this would require some very extensive documentation. But, using docker, we could attach and attract way more pro's in IT and it would make the hole thing more secure than just running it directly on a web server. \nAnother final thought for .rpm is handling / validating SELinux-rules. This is not easy. Just saying...\nThanks for reading it all way down here, here's a potato: \n(srxPt) \n:)\n. P.S.\nForgot about that: Not sure if it is possible to enforce it on a application level, but diffie-hellman-epheramal for negotiation and encapsulated blowfish in aes would it be. ;) \n. ",
    "eemantsal": "So, what's happened with this feature at the end? The technical debate doesn't clarify if the developers think the feature will be implemented some day.\nI, besides, would like to add that Startpage generates an obfuscated URL for the settings, which I suppose is what @MichaelZebra says that could be dangerous, but are you sure its an unique URL or every person with the same configuration would have the same URL? For example, if I configure SP and get this URL: https://www.startpage.com/do/mypage.pl?prf=0e\u00f1fb\u20ac90j\u011163\u00df5pc90a5qx32c\u0127\u00b5602rp507fd6 (a fake one I've just made), other user that may configure the options exactly like me would get a different URL? I have no idea about it, but seems a bit silly to me create different URLs for the same configurations.\nTo be honest I don't know exactly what that \"obfuscation\" process consist in; it sounds to me like a way to shorten the URL like bitly.com and many other sites do. You think it goes further than just shortening?\nBut anyway, even if SP creates an unique URL for each user when chosing the obfuscated URL option, it also gives the option of not obfuscating it and create a simple text URL with this form:\nhttps://www.startpage.com/do/mypage.pl?prfh=design_typeEEE1N1Nlang_homepageEEEs/air/tur/N1Nconnect_to_serverEEEeuN1Nresults_countEEE1N1Nlanguage_uiEEEturkceN1Ndisable_open_in_new_windowEEE1N1NlanguageEEEnederlandsN1NsslEEE1N1Ndisable_family_filterEEE1N1Nnum_of_resultsEEE20N1Ndisable_video_family_filterEEE1N1NsuggestionsEEE1N1Ngeo_mapEEE1N1N\nI have configured SP to show results in dutch and interface in turkish (weird, sure, but just for this example), disabled filters, force connection to european servers, and some couple more of options; and, if one pays attention to the URL, all the info in it seems to be relative to those settings and nothing else, I don't think there's any unique ID anywhere there, ergo I suppose everybody who sets the same configuration will get the same simple text URL and there's no big risk to be tracked.\nBut SP also has the option to save the settings just in a cookie, just like Searx does, or not saving at all and just using it for that very time, so the next time you visit the site you will visit the \"vanilla\" version. And of course it is not obligatory to use any customization, so prosecuted people living in dictatorships or pseudodemocracies, like whistleblowers, freedom fighters, etc, can perfectly keep using Startpage with its default values and URLs.\nWell, whta's wrong in making Searx behaving similarly? As long as it is always optional I don't see how can it be a threat for those people who are afraid for his liberty or even his life.\nPlease, consider implementing it, is a very useful and time saving feature, and, fortunately, most of us don't live in too authoritarian countries.. Yes, definitely. Sorry, I hadn't seen it.. Perhaps I haven't used the correct terminology. Pardon. :-s I'm not an informatics/web stuff professional and maybe I have some misunderstood concepts in my head. I thought the user agent in a web browser identifies the browser, its version, the operating system, screen resolution and several other things like the language settings. Excuse if my commentary was confusing.\nWhat I meant was that web browsers have a method to communicate the language setting to websites, be it their user agent string or other thing, and most search engines seem to identify it correctly and load their websites in the corresponding localized versions, and filter the search results to priorize the ones in the browser's language. Couldn't Searx do the same? As I said, it loads the interface in english and returns results in english.\nYou are right about the IP thing when behind a proxy, VPN or such, but do you really think that the majority of computer and mobiles users in all the world (who I suppose are your target; if you are just centering in the US and its satellites then I should probably stop talking), are using a proxy?\nThe IP thing in fact was suggested thinking on improving precision of certain search results: Using the users' IP to priorize results if search \"buy leather boots\" the search engine could priorize results for websites and shops, phisical or virtual, in the user's country, even in his city; if looking for some regional info, like national news, the search engine should priorize news sites from the user's country, which have probably better info and understanding than foreign media, even if written in the same language. AFAIK that's what Google/Startpage and others do (I suspect Duckduckgo doesn't and that's why the results when searching info outside the english speaker world are so mediocre).\nIf you are hungarian or chinese you are getting results from your own country almost always since those languages, even if so big as mandarin is, are very geographically localized, but think of hispanics or english speakers, who are spread across the world. The results may be way unprecise: I don't think that an equatorial guinean person is very interested in look for something common he wants to buy in stores located in Spain or Argentina nor I think a new yorker prefers to read the national news from jamaican or irish media.\nIf I'm confusing concepts here too, and search engines use other \"thing\" different to the users' IP to localize the search results, it doesnt really matter, I suppose you get the idea and can \"translate\" to the right concept.. > This is the Accept-Language HTTP header: https://www.w3.org/International/questions/qa-lang-priorities.en#background and searx uses it for the locale settings.\nOh, ok. Sorry for my ignorance xD\nI don't understand why my Firefox loads the wrong language. I'm on a fresh clean profile. Qupzilla shows Searx interface in my language and also does Firefox on mobile. I suppose this issue is just a problem of my Ffox installation on the PC and other users aren't affected. :-|\n\nAllowing this behavior in search language selection could be a configurable option for an instance in the future.\n\nGreat to read! I suppose I'm not the unique that most of the time wants information in their language.\n\nThis software is built and defaulted to support privacy minded users, even if some of the solutions are not the most comfortable for regular users with fancy smart phones and tablets.\nProxy/VPN/Tor usage is very popular in this field, I think.\n\nAh, well, I guess I misunderstood the target users for Searx; I thought you were aiming it for general use, like Duckduckgo, Startpage, Disconnect, etc, even if on PC, not only fancy portable devices.\nIt's a pity, but, well, it's your project and you are free to develop it as you want, of course.\n\n\ndo you really think that the majority of computer and mobiles users [...] are using a proxy?\n\nOf  course\n\nDon't cheat, little trickster. xP. You do know perfectly that the huge majority of the Internet users don't even know what is a proxy. (\u00ac.\u00ac)\n\nand I recommend it to you too. =]\n\nHehe, thanks, I already do, ;); although most of the time I set my VPN client to use proxies in my country, precisely because I'm a heavy search engines user, and, if I'm searching for something, I want the search engine to find what I want, not results that may interest to people who lives in other continent but probably not to me. Life is too short to waste it in wrong search results. :P. As long as a search engine doesn't log my IP, access times, user agent (a recommendation in return, in case you aren't aware of it already: Random Agent Spoofer, a nice extension for Firefox that will fake your browser's user agent periodically), etc, I am reasonably safe, after all I dont live in Afkhanistan or such; and nothing best to trust a search engine that having its code visible to any expert that wants to verify it, like Searx's code.\nAnyway, thanks for your explanations, I understand that your project is more focused on people from the english-\u00fcber-alles speaking \u201cgeeky\u201d world, very concerned that value privacy over everything even if that means lose basic efficacy and time. I don't share that vision, but it's a perfectly valid one and I wish you a big success with the project; it is still a very interesting search engine.\nCheers.\n. ",
    "ShalokShalom": "I am totally for this method, which gets offered by both; DuckDuckGo and Startpage/ixquick\nSave the settings to a specific URL, in order to share them and make it super secure, without any cookies.\nHere are some use cases:\n1) I think Pointhi is a bit more clear as the default theme and the first impression counts.\n2) The local settings do not apply with secure browsers like TOR. (Plus they even do not apply in my case at all without manual invention, which is, of course, an own issue that can get resolved.)\n3) Then, there is POST preset, which makes it impossible to share links.\nSo all of this is currently a little bit a blocker for me, in order to recommend Searx to my crowd.\nThanks a lot in advance; genius project, seriously.. https://github.com/asciimoo/searx/issues/958. Got this removed?. The very same happens here. German install, German browser (Firefox) and English/American UI and results, which means basically that the whole experience is confusing. . These are simply two different words, everybody understands both. . I agree. Its also for the users confusing. Looks like one language is left from a past clean up.\n+1. And i think Portugese_Brazil should get renamed into Brazil. . You can remove it from the page and if anything happens any time, can you restore it. \nWhere can I see both, to compare? I mean, as a list.. +1. Nice to know. This is something different, as i mean.. Still the case.. That its applies every time, when i click on save. . ?\nYou seriously dont get, what i mean. There is a save settings button on the bottom of the page.\nIt works sometimes, and sometimes not. . Ah yeah, and the function so create URLs, who save the settings, is gone.. > It is on the bottom of this page: https://searx.me/preferences\nYeah, now its back again. \nSeems both, the save function and the URL creation happen to dropout sometimes.. This is still broken. \nSometimes do I get the result of %s, when I use a saved URL:\n\nSometimes a white screen: \n\nAnd still, sometimes does a saved change in the settings not apply at all, which might be an issue on its own. It includes anyhow that the search URL does not get refreshed as well..\nAll reproducible on Opera, Chrome, Firefox, Qupzilla and Liri-Browser. . ",
    "risoul": "I second that, really annoying to multiply clic.\nOpen/type request with suggestion/show websites/clic image link to show images/view image or source\nGoogle studied the UI for years with million dollars, why no just copy what they do? it is clearly the best way to do.\n. ",
    "atomGit": "+1\n. > Does it happen with shorter searching phrases ?\nyes, but i can't reproduce it reliably \n\nSearx sends the user request to different search engine, and wait a global timeout : if the search engine didn't response before the timeout, the response is ignored.\n\ni suspect it isn't a timeout issue since the return of '0 results' can happen very quickly (a second or two)\n\nSearx keeps some HTTP connection to different search engine after a user request. - the HTTP connections expire after a time.\n\nsounds logical, but i don't recall this ever happening with other 3rd party services, such as DDG, Startpage, etc.\n\nAre you using another instance ?\n\nas in more than one browser tab with Searx loaded? i can't say for certain whether the '0 results' problem occurred only when i had multiple instances of Searx loaded, but i doubt it - i just repeated the search in the OP, once per tab in two tabs, and in both cases results were returned. can confirm this on searx.me. on second thought, i would vote to remove gigablast - it's old, unmaintained and i suspect it doesn't offer anything other indexes don't ??? just a thought. you should provide links and descriptions for those services - makes it easier on the devs - IVRE for instance - what/where is it?\nbitchute - youtube alternative and a great platform! should certainly be considered for inclusion in searx\nPunkSPIDER/Shodan/ZoomEye/Censys - vulnerability/security scanners/engines - that's a totally different audience then what searx intensions are i believe???\nAhmia - tor hidden services search engine\ndoes minds.com have a search appliance?\nyippy - general search, don't know if it's a meta, hybrid, index? - doesn't look like a very ethical company. https://invidious.snopyta.org/ is another instance of Invidious\nsnopyta.org offers a bunch of cool and privacy-enhanced services for those interested. you can just change \"method\": \"POST\", to \"method\": \"GET\", in your browser search config as well - if you're using Firefox, check out mozlz4-edit\nedit: note however that using GET raises a potential privacy issue. i kind of agree - this is one reason why i never enable Bing because that engine ignores various operators, including \"phrase searches\" and -negative keywords. Bigfinder - no SSL\nExalead - been around for a while, but i know nothing about them\nFastbot - requires JS\nExactseek - no SSL\nWitch - no SSL\nother than Exalead, these appear to be very poor meta engines, not indexes. ",
    "der-domi": "Great, Thanks! :-)\n. Thanks for your help! This seems to be the correct setting. For example is in https_rewrite.py default_on \"True\".\nCould it be that the infinite scroll currently (latest git master) not work? The content is not loaded when I reach the last search entry.\n. Yes, I restarted uwsgi and apache. But it still not work.\nI wondering. I updated to the latest git master version today and it works fine with scrolling. Then I made a fresh installation to a new directory and it seems that only the scrolling does not work.\nDoes make this sense?\n. Hello, I want to ask carefully what's the state with this issue?\nYou can reproduce the wrong behaviour on the public instance: https://www.heraut.eu/search/. Great! Thanks for the fix! :-)\nNo problem. I have time. :-). Did you set the search language to \"German (Germany) - de_DE\"? Otherwise it also works for me.\nIt doesn't work with several Firefox instances and an old Internet Explorer 11.\nAlso it doesn't work on searx.me and on my own instance (Debian8, Apache). So, it seems to be a source issue.\n. Great, thanks!\n. ",
    "rEnr3n": "It's not working for me on v0.15.0. Setting this on settings.yml.\nsearch:\n    ...\n    method : \"GET\"\nThe preferences in the UI say it's using POST. I don't see the search parameter in the URL when doing a search.. ",
    "ergo70": "Hm, it looks like if JSON arrays are parsed into lists of lists of UTF8 strings and that causes the error.\nSo, shouldn't \nfor suggestion in query(json, suggestion_query):\nactually be\nfor suggestion in query(json, suggestion_query)[0]:\nin the JSON engine code?\n. ",
    "ajvsol": "Awesome! Where is the preference to enable it on searx.me?. Oh cool I hadn't tried a search on this instance, thanks it works!. Ah just tested it, yes it looks like only searx.me was blocked from StartPage.\n. Legally merged but they still offer the separate searches (SP for Google proxy and IX for metasearch proxy).\n\nStartPage results in a classic interface are available at classic.startpage.com.\nCurrent Ixquick search results are still available at www.ixquick.eu in a modern interface.\n\nLink.\n. @dalf ah I see, thanks for your very comprehensive explanation! My takeaway is that SearchEncrypt is just using marketing jargon when in reality not all of their ciphers do have FS whereas Searx in contrast has FS for all of theirs.  \nSE is a metasearch engine so it uses Google etc like Searx. \nThe threat model I have is to limit the amount of information both the metasearch engine and the search engines themselves have about me. By using a metasearch engine anonymity can be gained compared to using the search engine directly, but then the metasearch engine has full info about the query.\nI was mainly hoping for a some sort of E2E encryption for queries, so that the metasearch engine relays the queries and gets results but is at no point aware of what the query was. . ",
    "jeroen7s": "when will the docker-container be updated to add this feature? :D. ",
    "Tobus": "Ok thanks, may I forgot this ;)\n. ",
    "ayaanalamansari": "hi, I am also facing the same problem, but certifi package is already installed.\nwhen running webapp.py, then it is running fine,\nBut after configuring uwsgi when testing uwsgi then it says the same error, \nUnable rectify what's the issue. Can anybody help?. ",
    "GinoHereIam": "Hi,\nI use my own instance: GinoHereIam's Searx instance\nhm might be timeout, how to figure out? Is there an option to increase the timeout?\nI use following search engines: Wikipedia, Bing, Currency, Wikidata, Google, Yahoo, dictzone\n. @asciimoo Is there a global timeout, which I can finetune afterwards?\n. Hi Alexandre,\nI will tell you in evening, right now I don't remember which engine it was. \nBest regards\nGino. ",
    "micressor": "I would welcome additional support for ATOM.. I confirm that docker build -t searx . is running (in master and v0.15.0 tag) on an error:\n[...]\nCollecting pycparser (from cffi!=1.11.3,>=1.8->cryptography>=2.2.1->pyopenssl==18.0.0->-r requirements.txt (line 7))\n  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\nException:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/pip/_internal/cli/base_command.py\", line 176, in main\n    status = self.run(options, args)\n  File \"/usr/lib/python2.7/site-packages/pip/_internal/commands/install.py\", line 346, in run\n    session=session, autobuilding=True\n  File \"/usr/lib/python2.7/site-packages/pip/_internal/wheel.py\", line 886, in build\n    assert have_directory_for_build\nAssertionError\n[...]. ",
    "Athemis": "Thanks for the comments! Pull request has been updated accordingly.\n. @kvch Your comments are addressed in the last commits. Thanks!\n. @kvch Done! Thanks for merging and the helpful advices :)\n. Which content exactly? I know, if you access the site directly, and are doing a search there, the images are loaded unencrypted, but the plugin does not crawl the site. Instead it queries the pdbe api using https. The preview image is loaded over https as well.\n. Ok, thanks again :)\n. Yep, that's a leftover because the original query was more complex ;)\n. Ok, then we can leave it out. I'm a bit worried though that a random user agent might lock searx out in the future. The pdbe api usage policy requires users to send a meaningful user agent. We can try though and send something more specific in case searx gets ever blocked.\n. ",
    "nerotic": "Great Adam,\nThanks for the update!\nMark Oren                         http://nerotic.net/\n magento specialist\nmark@nerotic.net [image: callto:neromadrid] callto:neromadrid [image:\ntel:+1-347-875-0046] <+1-347-875-0046> [image:\nhttp://www.linkedin.com/in/markoren] http://www.linkedin.com/in/markoren\nOn Sat, Oct 8, 2016 at 7:39 PM, Adam Tauber notifications@github.com\nwrote:\n\nThere are service independent networking issues. Working on the solution\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/725#issuecomment-252438023, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAdxgQz-u4xNfJ_H6PVLeUMddGj3FtUsks5qx9VhgaJpZM4KRxqx\n.\n. \n",
    "gszathmari": "I am having the same issue. It doesn't help if I solve the CAPTCHAs from a browser as the GOOGLE_ABUSE_EXEMPTION should be sent with my searx queries.. This is the cookie what Google sets if you solve the CAPTCHA. \nSo even though I solve the CAPTCHA from the same IP address as my searx instance, searx still fails to retrieve the results from Google. This is because searx is not sending in the GOOGLE_ABUSE_EXEMPTION cookie with the queries, so it is still presented by the CATPCHA. However, my browser is not asked to solve the CAPTCHA because it is sending in the GOOGLE_ABUSE_EXEMPTION cookies with the subsequent queries.\nHope this clarified up a bit\n. ",
    "zwnk": "i recently updated to 0.13.1 and get the same error. went back to 0.12 and get google results.. i pulled today and had the google captcha error.. ",
    "Dominion0815": "same problem here with 0.13.1:\n```\nERROR:searx.search:engine google : exception : CAPTCHA required\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/usr/local/searx/searx/search.py\", line 87, in search_one_request\n    return engine.response(response)\n  File \"/usr/local/searx/searx/engines/google.py\", line 217, in response\n    raise RuntimeWarning(gettext('CAPTCHA required'))\nRuntimeWarning: CAPTCHA required\n```. same problem here with 0.13.1:\n```\nERROR:searx.search:engine google : exception : CAPTCHA required\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/usr/local/searx/searx/search.py\", line 87, in search_one_request\n    return engine.response(response)\n  File \"/usr/local/searx/searx/engines/google.py\", line 217, in response\n    raise RuntimeWarning(gettext('CAPTCHA required'))\nRuntimeWarning: CAPTCHA required\n```. I'm running a Proxmox VM on my own dedicated server. And yes it is a public instance without other services.. Everything is fine, I'm not listed:\n\"POSSIBLY SAFE 0/96\" and \"IP Not Listed (Good!)\"\n. at this moment I have 9 connections and google problems.\nhere is my lynx output:\n```\n\nSubmit\n     ____________________\nAbout this page\n   Our systems have detected unusual traffic from your computer network. This page checks to see if it's really you sending\n   the requests, and not a robot. Why did this happen?\n   This page appears when Google automatically detects requests coming from your computer network which appear to be in\n   violation of the Terms of Service. The block will expire shortly after those requests stop. In the meantime, solving the\n   above CAPTCHA will let you continue to use our services.\n   This traffic may have been sent by malicious software, a browser plug-in, or a script that sends automated requests. If\n   you share your network connection, ask your administrator for help \u2014 a different computer using the same IP address may\n   be responsible. Learn more\n   Sometimes you may be asked to solve the CAPTCHA if you are using advanced terms that robots are known to use, or sending\n   requests very quickly.\n   IP address: 62.xx.xx.xx\n   Time: 2018-02-15T15:03:41Z\n   URL: https://www.google.com/search?q=mettwurst\n```\n. my workaround is to activate more default search engines like \"duckduckgo\" and \"startpage\". with 0.14.0 and filtron the google captcha problem still exists.... in your settings.yml set \"disabled\" to \"false\" or delete the line.\nexample:\n- name : startpage\n     engine : startpage\n     shortcut : sp\n     timeout : 6.0\n     disabled : False. ",
    "steckerhalter": "I'm using filtron but still get the captcha. Any tips maybe on configuring filtron or something?. @dalf ah maybe not, I'm using Apache. So let me check that, thanks.. @sachaz thanks I'll try that. for the record, I have configured filtron but google would still demand the captcha sometimes. the search is used by quite a few people. in the end we decided to disable google altogether.. I'd say the solution is simple: just don't use google. alternatively you can enable startpage which gives you the same results. it's just not worth it trying to satisfy the big g algos.. ",
    "miicha": "I have the same problems with google: now I'm using filtron and a new IP for the searx instance, but after one or two weeks I get the same problem again.\nNow I tried to copy all the google.com cookies (_ga, _gid, SNID, NID, DV, 1P_JAR, PAIDCONTENT,...) from firefox into my google.py (like in https://github.com/asciimoo/searx/pull/1121/) and google instantly works again (at least for a moment).\nTherefore my question would it be an idea (privacy?) to set these cookies?. It seems to be enough to add these 3 lines (for example before: params['url'] = search_url.format(offset=offset,):\nparams['cookies']['_ga'] = 'xxxxxxxxxxxx' # 2 years | Used to distinguish users.\nparams['cookies']['_gid'] = 'xxxxxxxxxxxxxx' # 24 hours | Used to distinguish users.\nparams['cookies']['1P_JAR'] = '2018-6-13-17' # changes hourly\ninfo about lifetime according to https://developers.google.com/analytics/devguides/collection/analyticsjs/cookie-usage and my own observation\nmaybe even 1 or 2 of these lines are sufficient.... As I wrote earlier, I usee filtron, but still got the google captcha crash.\nI configured filtron according to some tutorial I found online (most probably https://asciimoo.github.io/searx/admin/filtron.html and other) but did not check if it was working. Now I found time to check it, and guess what, it wasn't working. Additionally, I turned on logging to investigate what was going on. \nI found massive bot traffic on my domain, searching mostly for download stuff, requesting json output.\nAfter removing \"filters\": [\"Param:q\", \"Path=^(/|/search)$\"], from my filtron config file, it was finally working and I could turn of the logging again. \nUnfortunately I could not figure out, why the abovementioned filter was not working as intended, but I don't care to much.\nIf somebody is interested in the rules.json file I could post it somewhere (maybe in the filtron repo?). For me, it would be sufficient, if I can search in english and one other selected language. Therefore I implemented kind of a quick and dirty solution: https://github.com/asciimoo/searx/pull/1419.\nIt can be tested at: https://finden.tk/. You can find one, admittedly not perfect, solution in this PR: https://github.com/asciimoo/searx/pull/1419\nMaybe it can help you.. You can also view it live (together with my other PR (https://github.com/asciimoo/searx/pull/1419) at https://finden.tk\n\n. No it should be build from the .less files when running grunt in oscar folder.. Sorry, I didn't know...\nNow I built with grunt and commited the two minified css files.. I guess, it's about searx.me?\nBlocking with filtron seems to be enabled. My guess would be, that \"downloader\" via GET requests is blockt much stricter than via POST requests. Since the default values on the website are POST (for the search formular) you can search from the search formular, but possibly your browser added searx.me with GET requests. Therefore it might help to remove the search engine and add it again.\n. duplicate of https://github.com/asciimoo/searx/issues/729\nAlso see my comment there https://github.com/asciimoo/searx/issues/729#issuecomment-433948084\nYou can find a nice overview with load times and SSL ratings here: https://stats.searx.xyz/. @micah - I set very strict rules with different error messages for blocked reqests - something like:\n\"name\": \"IP limit\",\n        \"interval\": 60,\n        \"limit\": 2,\n        \"stop\": true,\n        \"aggregations\": [\"Header:X-Forwarded-For\"],\n        \"actions\": [\n            {\"name\": \"log\",\n             \"params\": {\"destination\": \"stderr\"}},\n             {\"name\": \"block\",\n             \"params\": {\"message\": \"IP-Blocked\"}}\n        ]\nThen I tried the site and did not get the \"IP-Blocked\" message after the second attempt. After some changes it was finally working...\n. you can test the fix at https://finden.tk. It can be tested together with my other PRs on https://finden.tk\nThis PR also includes the bugfix for startpage (#1464 ). Yes sure, no problem. The Problem with startpage pagination is the following:\n- to get the first result page a normal query without any state parameters is sufficient\n- to protect following result pages, startpate implemented some checks:\n  - the first result page contains two additional parameters (important) parameters\n  - these parameters have to be passed when trying to access following result pages\n- therefore my implementation uses does the following:\n  - extract these parameters from the first result page\n  - insert the parameters into the renderd html page as hidden form fields\n - reads the parameters from the submitted form for subsequent page requests\n- basically I was searching for a possibility to temporarly save startpage specific data on a per user and request basis\nMost probably there is a better solution or implementation to that problem. I only had a quick look to the soundcloud engine, but it seems that the guest_client_id is some \"semi constant\" value and not changing for every next page call?. That sounds nice.\nOnce I find some time, I will update the PR. I introduced engine_attributes in results.py. Is it what you imagined?\nI'm thinking about another nesting level like: engine_attributes[enginge_name]['key1'].\nFurthermore, I have to think about query.py and the html part.... ",
    "GitHubGeek": "Could it be Google tweaking their abuse algorithms to be super stringent? My instance is 100% private (login required) and I'm the only user. Still, the same error comes up after a handful of search.. ",
    "rtizzy": "I have confirmed I experience the same issue in Vivaldi (A chromium derivative) using either POST or GET methods. . ",
    "Zero3K": "Why is it not possible?. Any news regarding this issue?. That's not good. I hope that it gets fixed soon.. I agree with this suggestion.. I agree with this suggestion.. It looks like it was implemented recently or already implemented and that I just didn't see it.. ",
    "schwarzwhite": "Oh, hang on, I've just noticed this page is having the exact same problem:\nhttps://searx.me/\nIs this a bug in the latest code...?\n. Thanks for your update.\nDoes this require me to get your latest code?\nThe reason I as is because it seemed to magically start working again, but I can see it is broken now.\n. ",
    "kkaiser": "I can see this bug is marked as closed but in fact it still exists in 0.11.0 for example when bing returns only one result. You can try \"2016-03-26\" \"TKP New York Conference Center\" as a query to reproduce the bug.. Thanks for the quick answer. I think the IP has the most impact and was actually aware of that but it still surprises me how \"gmail\" can be a match for the entered query, even though it is a bit more complex. But thanks for the clarification. I just wanted to make sure this wasn't a mistake of any sort.. ",
    "Forbo": "Also getting a similar problem with !dc\n. ",
    "realitygaps": "I'd vote for keep, i've used it a few times just in the last week (where the original page was taken down by court order for example). \n. ",
    "negroblack": "You can just edit the template to remove it if you don't like it.\n. ",
    "thearkadia": "@kvch @asciimoo POST isn't working on chromium and it doesnt change to Get by default. @kvch any update on this?. thanks @kvch that worked. shouldn't the \"proxied\" icon now appear under the results where the search engine used is displayed though?. ",
    "nixoeen": "True, and this is how we are using it in http://forum.ubuntu.ir right now, but I had to make another server-side script to create the URL. It is not possible to directly do it with HTML without extra client-side or server-side script.\nAdding this feature makes it easier to integrate the search into websites, specially in template codes.\n. ",
    "NuLL3rr0r": "I believe @nixoeen has a valid point. It would be a hassle to implement it without this feature.. ",
    "dagaz": "Thaks for the clarification and the dependencylist.\nUnfortunally I could not downgrade the package directly. And it would be great if 1.5.7 would be supportet. (Therefore i let this issue open. If there is already an issue f\u00fcr 1.5.7 close this, please.)\nBut here is the solution for other Arch users:\nGet PySoks 1.5.6 from here: https://pypi.python.org/pypi/PySocks/1.5.6\nExtrackt the package, cd into the directory with the setup.py file and run:\nsudo python2 setup.py install\nThen start the service with \"systemctl start searx.service\" and check the status with \"systemctl status searx.service\". It shoud be running now.\nDo not forget to blacklist python2-socks in /etc/pacman.conf and run \"yaourt -Syyua\" to rebuild the database.. Works fine. Thanks for the note.. Also same here.\n(Searx running as a docker container build directly from sources.)\n. ",
    "sukramblak": "it works with any pysocks version above 1.5.7. The community version was updated to  1.6.4-1 on 01.12.2016. These should install automaticly now. I would suggest closing this bug report. ",
    "VoatSearch": "Hey guys, can I suggest/request you change the Voat search engine to https://searchvoat.co? It runs much faster and produces much better results than voat.co.  Duckduckgo uses it for their !voat bang.  There's a subverse for it on Voat at https://voat.co/v/VoatSearch.  The template is https://searchvoat.co/?all={query}.  What do you think?. The source isn't on github, does it need to be?. Not currently. ",
    "palexande": "In addition to Bitcoin, if searx is considering adding cryptocurrencies, please add Monero as well.  Monero is secure, private, and untraceable.\nThank you.. ",
    "suchkultur": "I opened #788 with a single commit history and close this.. I though about it and i guess you're right.. As suggested: disabled : True is added to the configuration.. ",
    "libBletchley": "Indeed we need a way to get CloudFlare pollution at least off the first two pages of results.  A crawler should be looking for CF-RAY in the HTTP headers, and the index should store that.  If a searx instance will have a pro-privacy target audience, then CloudFlare results have low relevancy.\nAnyone looking for rationale to suppress CloudFlare links - I'm collecting the CloudFlare issues here:\nhttps://github.com/privacytoolsIO/privacytools.io/issues/711#issuecomment-455895667. Ok, thanks.  I wondered about that.  I think I saw that among the sites with partial matches, the full string was present, but perhaps the full string just didn't get indexed for some reason.. anyonething.de is no longer a searx instance.  It is now a pastebin, apparently, and it still proxies through CloudFlare.  I have moved it to the bottom of the Offline section.  Since people find it useful to list offline instances, I think it's also useful to list the date things are noticed as being offline, so I added that for anyonething.de.\nThe original record simply wrote \"(Cloudflare)\".  I find this insufficient, as searx becomes more and more popular, lots of low-tech users will not know what that means.  So I changed it to: \"(Warning: uses CloudFlare)\" so that it's at least clear that it's an anti-feature.  It should probably go further and link over to an article elaborating on the problem with CF - but I didn't bother since it's offline.. It's certainly a good idea to tag the CF sites.  I'm not sure why you opened a ticket when it's a wiki that you can easily edit yourself.  If the idea is to discuss and reach agreement, this is my take on it--\nSimply writing \"(Cloudflare)\" is insufficient because searx is becoming more popular among people who have no clue what CloudFlare is.  So I have two suggestions:\n\n\nRename \"Running with an incorrect SSL certificate\" to \"Running with anti-features\", and then mark each record with \"(bad SSL cert)\" or \"(uses CloudFlare)\"\n\n\nReformat the whole list as a table.  The table can have columns for uptime, cloudflare, etc.\n\n\nIf some time passes and there is no objection to these ideas, I may act on them.. ",
    "ky0nch3ng": "error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n\nCleaning up...\nCommand /usr/bin/python -c \"import setuptools, tokenize;file='/tmp/pip_build_root/lxml/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /tmp/pip-3HpMms-record/install-record.txt --single-version-externally-managed --compile failed with error code 1 in /tmp/pip_build_root/lxml\nStoring debug log for failure in /root/.pip/pip.log\n\npip.log\n\nCleaning up...\n  Removing temporary dir /tmp/pip_build_root...\nCommand /usr/bin/python -c \"import setuptools, tokenize;file='/tmp/pip_build_root/lxml/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /tmp/pip-3HpMms-record/install-record.txt --single-version-externally-managed --compile failed with error code 1 in /tmp/pip_build_root/lxml\nException information:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 283, in run\n    requirement_set.install(install_options, global_options, root=options.root_path)\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 1436, in install\n    requirement.install(install_options, global_options, args, *kwargs)\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 707, in install\n    cwd=self.source_dir, filter_stdout=self._filter_install, show_stdout=False)\n  File \"/usr/lib/python2.7/dist-packages/pip/util.py\", line 715, in call_subprocess\n    % (command_desc, proc.returncode, cwd))\nInstallationError: Command /usr/bin/python -c \"import setuptools, tokenize;file='/tmp/pip_build_root/lxml/setup.py';exec(compile(getattr(tokenize, 'open', open)(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" install --record /tmp/pip-3HpMms-record/install-record.txt --single-version-externally-managed --compile failed with error code 1 in /tmp/pip_build_root/lxml\n\n. \u279c  searx git:(master) uname -a\nLinux ky0nubuntulax 4.9.0-040900-generic #201612111631 SMP Sun Dec 11 21:33:00 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\u279c  searx git:(master) cat /etc/lsb-release\nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=14.04\nDISTRIB_CODENAME=trusty\nDISTRIB_DESCRIPTION=\"Ubuntu 14.04.5 LTS\". ",
    "dakotaclemenceplaza": "This is really annoying thing as it differs from most search engines behaviour (or at least from google). I constantly press enter on an autosuggestion and become irritated for a second because nothing happens. . ",
    "vigneshchalapathy": "you can see the deployed image  here  http://192.168.99.100:32786/. yeah ok,I will correct that.But please suggest me a way to customize that\nto the core for our college project.I have installed that by docker and\nfind no way to customize that.\nI am attempting to make changes to the core logic.\nOn Sun, Jan 8, 2017 at 6:10 PM, Alexandre Flament notifications@github.com\nwrote:\n\nNote : 192.168.99.100 is a private IP : https://en.wikipedia.org/wiki/\nPrivate_network\nSo this IP can't be access from public Internet.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/818#issuecomment-271148571, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AOIT9stoe3aB4iVwDS8jHltH2ZqOnlCvks5rQNlAgaJpZM4Lc6X2\n.\n. Hello,\n\nI have  made changes to\nhttpd.conf file\nServerRoot \"C:\\xampp\\htdocs\\virtual\"\nand added this snippet\n\n    Options FollowSymLinks Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n\nI recive his error\nStatus change detected: stopped\nError: Apache shutdown unexpectedly.\nThis may be due to a blocked port, missing dependencies,\nimproper privileges, a crash, or a shutdown by another method.\nPress the Logs button to view error logs and check\n the Windows Event Viewer for more clues\nIf you need more help, copy and post this\n  entire log window on the forums\nOn Mon, Jan 9, 2017 at 7:16 AM, vignesh vicky <vigneshchalapathy11@gmail.com\n\nwrote:\nyeah ok,I will correct that.But please suggest me a way to customize that\nto the core for our college project.I have installed that by docker and\nfind no way to customize that.\nI am attempting to make changes to the core logic.\nOn Sun, Jan 8, 2017 at 6:10 PM, Alexandre Flament \nnotifications@github.com wrote:\n\nNote : 192.168.99.100 is a private IP : https://en.wikipedia.org/wiki/\nPrivate_network\nSo this IP can't be access from public Internet.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/818#issuecomment-271148571,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AOIT9stoe3aB4iVwDS8jHltH2ZqOnlCvks5rQNlAgaJpZM4Lc6X2\n.\n\n\n. Thano you,I will check. Can  searx  be installed with Django or http://www.ampps.com/downloads server. \n",
    "jfowl": "Can be closed because of 4a127b19de860a1f1e75f597dc0d53bfdd06e52e. according to https://privacore.github.io/:\n\nGoodbye \u2013 Findx is shutting down\nIt is over \u2013 before it really started. Findx is shutting down.\n\nCard https://github.com/asciimoo/searx/projects/3#card-10987305 could be removed. I do support !!bang for redirections and !bang for direct results in searx. It offers the best backwards compatibility.\nIn order to not get this too mixed up, I would propose to integrate this feature into the settings.yml (or if it ever get's changed into engines.yml):\n1) Making the shortcut setting an array.\n2) Adding a seperate bang_search_url setting.\nWe would need to hook in here: https://github.com/asciimoo/searx/blob/master/searx/webapp.py#L490-L507\n. What do the core contributers think about this? I think this is a very smart idea all in all.. I would like to question the usefulness of this, as metager is also a meta-search engine. It also combines different searches into one result list.\nPlus side: Servers in Germany (GDPR land).. @sorenstoutner\nI see two rather easy, but whacky solutions:\n\n\nSaving the safe-search preference into the search-url\n\n\n\nOverriding cookies for searx.me by you:\n\nThat way, your users must not do that on their own.\n\n\nBoth of them can be found here: https://searx.me/preferences.\nWhat are your requirements for a privacybrowser searx-instance?\nWould you be willing to add a DNS CNAME/A entry of search.stoutner.com for that?. Just as a note for future me: nginx-Cookie-Injection.\n- Run a single searx instance\n- nginx injects cookies based on subdomain (e.g. safe.searx.tld, dark.searx.tld, ...). JSON API:\nsearch_url :`https://jivesearch.com/?q={query}&o=json&p={pageno}`\n    paging: True\n    results_query : documents\n    url_query : id\n    title_query : title\n    content_query : description\n@brentadamson Is this also a meta-search engine? I saw that you have yandex listed as a search provider.. I don't know how useful it would be to use jivesearch as a source then, because searx can also talk to yandex directly.. @mahui625 I assume this solved your problems, could you close the issue then?\n. > I updated all user agnet and added other headers param.\nAccording to my experience, this does not help very much. After a certain amount of illegitimate searches (e.g. the ones via searx), Google puts your whole IP behind a captcha lock. I would recommend to use some proxies to avoid these issues in the future.. Documentation: https://hn.algolia.com/api\nIt states that it offers two API endpoints:\n- Sorted by relevance, then points, then number of comments: GET http://hn.algolia.com/api/v1/search?query=...\n- Sorted by date, more recent first GET http://hn.algolia.com/api/v1/search_by_date?query=...\nWhich endpoint should be used?. Here is the settings.yml snipet for the first api.\nyaml\n  - name: HackerNews\n    engine: json_engine\n    paging: True\n    search_url: https://hn.algolia.com/api/v1/search?query={query}&page={pageno}\n    results_query: hits\n    url_query: url\n    title_query: title\n    content_query: ''\n    categories: it\n    timeout: 2.0\n    first_page_num: 0\n    disabled: True\n    shortcut: hn\nTo use different api endpoints would require a custom engine instead of the json_engine, does @kvch want this? We could also modify json_engine to support dates, as this API provides them.. Working here too, @Pofilo were you able to fix it on your side?. Which pages are offered for download? The results? or even the searx interface itself?\nCan you upload the resulting file here?. No problem, you can do so now ;). Nginx does create some  logs on its own with the default settings. If you search for nginx log analizers you might find one.\n. No, sorry. Maybe google for \"nginx log extract number of unique ips\" or something along that line.. Just as a note for me, to implement it later:\n- name : npm\n    engine : json_engine\n    paging : True\n    search_url : https://api.npms.io/v2/search?q={query}&size=25&from={pageno}\n    results_query : results\n    url_query : package/links/npm\n    title_query : package/name\n    content_query : package/description\n    page_size : 25\n    categories : it\n    shortcut : npm. @kvch Could you review & merge this soonish, I kinda messed up my git history (still getting used to git collaboration) and would like to have a blank new plate for integrating some other fixes :). APKMirror does not have a search API, so this is a bit of work.\n- Search: https://www.apkmirror.com/?post_type=app_release&searchtype=apk&page={pageno}&s={query}\n- Results: actually not that unintuitive: in div#content are .approws with the actual results:\nhtml\n<a class=\"fontBlack\" href=\"{relative result link}\">{result title}</a>\nA thumbnail is also available.. @CaptainFrosty This has now been implemented, you may update your instance and close this issue ;). Hi!\nWhat action did you perform to produce this error?\nWhat version of searx were you running at that time?. On which instance did you try this?\nCould you send us a screenshot of that error page?. I had a look at the Code and it says:\n```python\n    # get safesearch\n    if 'safesearch' in form:\n        query_safesearch = form.get('safesearch')\n        # first check safesearch\n        if not query_safesearch.isdigit():\n            raise SearxParameterException('safesearch', query_safesearch)\n        query_safesearch = int(query_safesearch)\n    else:\n        query_safesearch = preferences.get_value('safesearch')\n# safesearch : second check\nif query_safesearch < 0 or query_safesearch > 2:\n    raise SearxParameterException('safesearch', query_safesearch)\n\n```\nSo maybe go with 0 (none), 1 (moderate) and 2 (strict).\nXML would be https://searx.de/?q=test&format=rss.. - They do seme to have some API here: https://www.peteyvid.com/api.php, did not contact them about it yet.\n- Theiy appear to be nice:\n\nPetey Vid respects your privacy, we dont save your IP address / search history.\nLet's explore!. Looks good, I like it. What about attribution though. It says:\n1,000 requests per day\nNo extra requests available\nNo uptime SLA\nBasic support\nNewsAPI.org attribution required\non https://newsapi.org/pricing.\n\n. ",
    "craftyguy": "So I'm using my distribution's (Arch Linux) package management system to install Searx and its dependencies, and non pip. This pulls the latest version of Searx from git, and installs all of the dependencies listed in the requirements.txt. I have confirmed that the versions of the dependencies installed on my system reflect those listed in requirements.txt (see pip freeze output attached)\npip_freeze.txt\n. ",
    "ston3o": "@asciimoo Thanks for your job, searx is really awesome ! I have added seed, leech, filesize and icon.. Thanks @Pireo ;) I didn't know this program !. ",
    "Pireo": "Guys, do you know this program? Sorry for the OT, but I thought you can help here too!\nhttps://github.com/qbittorrent/qBittorrent/wiki/Unofficial-search-plugins. ",
    "throgh": "Nothing possible: It is always failing when trying to setup the corresponding module:\nCommand /usr/local/searx/searx-ve/bin/python -c \"import setuptools, tokenize;__file__='/usr/local/searx/searx-ve/build/cffi/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-LYxLUf-record/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/searx/searx-ve/include/site/python2.7 failed with error code 1 in /usr/local/searx/searx-ve/build/cffi \n. Found out the problem: The version of \"virtualenv\" is way too old on Ubuntu 14.04.\nYay, now Searx v.0.11.0 is running!\nMy solution was:\npip install virtualenv (running therefore the newest version available)\nPerhaps this helps people with similar problems?. ",
    "r14c": "i would never want the application to generate its own secret key, that's definitely something that should be handled explicitly by the operator or some automation system. secret management is not the responsibility of the application. using environment variables to define secrets is a much cleaner solution imo. \nmy searx cluster uses a config file that is generated with confd, but i'd be happy if i could set the secrets directly with environment variables.. ",
    "conmarap": "You are actually right about that; It contains the instance IP. I was mostly thinking in terms of when you run it on your own or through a Tor network, so that you can have visibility over what identity you are given. Perhaps the plugin should be renamed to instanceip? Unless there is an elegant way of passing the request.remote_addr from Flask into the plugin, so that it could be passed to the ip-api request.. Well, the Tor exit nodes are known and are public. The thing is that no one - that is on the other side - knows who's connected to them and this would allow one to know their current (disposable) identity, since they switch every so often. It may be useless, but I found it useful while using it on a Raspberry Pi at home. However, this could still be used if request.remote_addr could be passed as a parameter in the GET request, so that the user's IP info would be found.. Ok, I'll create a separate pull request for that. Feel free to close this pull request if you don't see a use case for this plugin (I had seen that line, but was unsure what that was used for).\nEdit\nI'll keep this PR and reiterate.. Absolutely @kvch !. I tested with ./manage.sh tests locally. Not sure why it's failing, but I'll figure it out. In the meantime, @kvch I made the change that you requested and it looks something like this:\n\n. ping. No problem, @dalf! \ud83d\ude04 I actually changed it to https in cec07cf80ac3325b98f40d8fe7eb8e6d9a7f9161 (edit: I got what you mean after I read the #903), but I can turn it into an engine if that's what you'd like. What do you mean by merging the commits though?. I'll resubmit.. Any suggestions?. Maxmind's datasets are freely available, but they need an attribution link. So this could be handled internally, maybe? http://dev.maxmind.com/geoip/legacy/geolite/, otherwise I've set up a service on Heroku with HTTPS support https://geoipnode.herokuapp.com/json/, but doesn't provide any ISP information.. Ok, I can change it to ipinfo.io for now, and look into implementing it internally. The Maxmind datafiles have plenty of information, the same that ipinfo.io provides, so it should be fine.. Haha, yes it is. I'll take care of it. I was mostly testing and wanted to see why the assertion test is failing on travis ci.. ",
    "soykje": "Quick answer: I just saw in the instance preferences, that the prefixes were documented there... Fill a bit dumb, but if I may, is there a reason why there no redirection, as DuckDuckGo does?. ",
    "kskarthik": "You can use searx instance at search.disroot.org . https://search.disroot.org. ",
    "neggs": "Thanks dudes. I'll take it for a spin :). @virtadpt Which instructions did you follow? Docker? nginx? virtualenv ?. Thanks. @virtadpt What your DO spec and what port number do you use? Also, do you access it over the internet or internally within the DO network?\nCheers dude!. SOLVED\nFor anyone else having issues, here is my set-up:\nInstructions: https://asciimoo.github.io/searx/dev/install/installation.html, virtualenv & uwsgi  method\nPort 8080 used\nDigital Ocean - 2 Cores, 2GB mem, ubuntu 1604\nUFW is disabled\nIPTABLES, ports 8080 and 80 opened\nsudo iptables -A INPUT -p tcp --dport 80 -j ACCEPT\nIPTABLES redirected; port 8080 to 80\niptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080\nTested on ports 8080 and port 80.\n@virtadpt thanks for your help!\n. @asciimoo Yeah, I've tried that and I'm getting file not found (404). I've using searx with nginx with the following config:\n```\nserver {\n        listen   443;\n        ssl on;\n        ssl_certificate /etc/ssl/search404.io.pem;\n        ssl_certificate_key  /etc/ssl/search404.io.key;\nserver_name search404.io;\nroot /usr/local/searx;\n\nlocation / {\n        include uwsgi_params;\n        uwsgi_pass unix:/run/uwsgi/app/searx/socket;\n}\n\n```\nany ideas?\n. @asciimoo Many thanks, that worked a treat!!. @dalf  Thanks for the heads-up. I guess its low risk as search404.io uses POST requests but to be on the super safe side I'll remove it completely!\nThanks again!. Change has now been made. @dalf thanks again for pointing it out.. @dalf I've removed the additional requests so its now vanilla searx unless you spot anything else?\nThanks again.. @dalf I'll sort out the Roboto fonts locally - thanks for the link.\nIs it not recommended to use Cloudflare for certs/SSL in terms of privacy? If so I can use something else.. Thanks. I'll move to a direct model rather than a MITM, as security is king with searx. It's not much effort and as you point out, caching performance wont be a problem :)\nCheers.. Done. COMODO cert installed and getting an A+ rating on www.ssllabs.com.\nThis blog post helped getting the A+ rating; Optimizing HTTPS on Nginx\n. Hi Mate,\nThis is what I did for search404.io, although its a lazy static method. There's probably a better way, so please consider this a cheap and dirty way of doing things :)\n\nCopy searx\\searx\\static\\themes\\oscar to new theme name (in same folder)\nCopy searx\\searx\\templates\\oscar to new theme name (in same folder)\nPerform a global 'find and replace' within the searx\\searx\\templates[new name] replacing 'oscar' with new theme name. This is to fix hard coded folder paths with the template files.\nRestart searx\n\nLooking at what you have done above, it would seem that you haven't copied the static theme?. ",
    "mangeurdenuage": "There's a new vps provider with only libre hardware (deblobed AMD/PSP bios) if anyone interested.\nhttps://store.vikings.net/. @Popolon @dalf \nEven if it was possible why support a project when the developer of the project is datamining and selling the data of it's users ?\nhttps://mastodon.social/terms\nAnyway it would be possible with the fedivers tho. @dalf \n--I've made a quick look around of the UI : I can't find a search in all the post ?--\nIt's been a while since I have used mastodon but I don't recall a search function in it to directly search.\nI remember that the fediverse uses bots on mastodon to retrieve posts since the most active instances of the fediverse are blocked on the mastodon network.\n--searx will give more credit to this tool.--\nDo has you wish :)\nBut for myself I just don't support mastodon for various reasons, mostly it's because they are breaking the federate nature of the initial gnu social project and because of the possible embrace extend and extinguish that they may bring with server wide ridiculous blocking list and the on purpose protocol breaking. \nOf course if mastodon would re-federate and stop all unethical actions it would be beneficial instead of generating piles of complaints that waste time.\n--should a warning be displayed as in F-Droid--\nThe search via the searx would be more private than having to log in the mastodon instances of course but when they will be redirected to the mastodon network the TOS automatically applies (even if not read since the server is on American territory) thus meaning that the data and metadata will be exploited for diverse tracking and correlation.\nIn the TOS we can conclude that they use fingerprinting techniques.\nSee this part\n\" These cookies enable the site to recognize your browser and, if you have a registered account, associate it with your registered account.\"\nThey actually sell/share data with third parties\n\" This does not include trusted third parties who assist us in operating our site, conducting our business, or servicing you, so long as those parties agree to keep this information confidential. \"\nNote that even if those third parties agree to keep data confidential they contradict themselves with the line\n\" These third party sites have separate and independent privacy policies. We therefore have no responsibility or liability for the content and activities of these linked sites. \"\nData and even metadata shouldn't not be shared or sold (legally speaking metadata is still recognized has data that isn't personal identifiable information which is of course not true)\nfor those curious see:\nhttps://media.ccc.de/v/33c3-8414-corporate_surveillance_digital_tracking_big_data_privacy\nand\nhttps://media.libreplanet.org/mgoblin_media/media_entries/1529/144_7_gerwith.webm\nA warning would help for awareness and also stand to the neutral state of the searx engine.\nBut it would demand either for human participation or if it is possible to use some lists that is already existing, for example in the \u00b5block origin or umatrix lists.\nFor example in the \"hpHosts\u2019 Ad and tracking\" servers list there's a lot of trackers that are already listed such has google-analytics.com. @dalf \nSeems promising.. @dalf \nI just noticed something on loadaverage.\nWhen your research on it doesn't work at all LA proposes to use multiple engines and searx is part of them.\nSee for example. @dalf \nlike @a01200356 and I have mention it in the first post.\nLibre.fm only includes creations that are under a copyleft license.\nSee the Creative commons for more info about it.\nIt's basically the four freedoms but for content (art,music, pictures,3d model, game assets etc....).\nThey have a few terabytes of music on libre.fm.\nI don't especially know the artists in the database, that's why there's tags in it\nhttps://libre.fm/listen.php\nOf course the tags listed in the link aren't the only ones.\nIt would be interesting to make that if someone made a search for a particular music/artist then the search engine would also get what genre of music it is and use the tags on libre.fm to propose music that are under similar tags.\nOut of topic:\nThe GPL license was the first license considered has copyleft but due to the broken nature of copyright and misinformation or simply bad interpretations from developers there's now more software under permissive licenses like the MIT or BSD (which aren't copyleft).. @virtadpt \nThank for looking into it.\nI hope you'll have fun doing so.\n@dalf \nInteresting thanks for sharing and caring.\nMeanwhile I have found this for fireforks users\nhttps://github.com/traktofon/cf-detect\nIt's a cloudflare detection addon, under the mit license unfortunately, but does it's job.\nI wonder it a similar system could be done for searx to categorize websites and add a warning to people before the go on websites who uses cloudflare.\nThe addon doesn't detect a 100% of all cloudflare infrastructures for example if the server uses cloudflares DNS services it doesn't detect it or even cloudflare hardware.. Thanks @asciimoo \nSorry for the duplicata :/. @virtadpt\nThank you :)\nI have found the .onion for sci-hub\nFortunately if the request are done with the Tor link the captcha and cloudflare doesn't disturb the request.\nhttp://scihub22266oqcxt.onion/. @sachaz \nRemember that URLs aren't encrypted so the words that you search are in plain text in the URL. (If I don't mistake, if I am, please correct me).\nHave a good day ;). @sachaz \nWhen you say \" requesting the URL in SSL\" you mean for example a URL like this\nHTTPS://searx.gnu/search=pin+trees\nIf yes then no the URL itslef isn't necessarily encrypted.\nI recommend you to read this post #695 \nTo extend your knowledge about internet privacy please watch this conference (only 30 min)\nhttps://www.youtube.com/watch?v=eM4J7ljCExM. @dalf \nI thought the rest of the link got revealed too.\nMy mistake then.\nIsn't their a problem with DNS tho ? or it is just the beginning of the URL that gets revealed ?. @a01200356\nOh sorry for the waist of time.\nThank you for noticing this to me.\nIssue closed  then.. ",
    "remy18": "On the qwant website I can select france,gb or us.. ",
    "login2github": "@searxes\nI was waiting for a simple UI, and this will reduce my smartphone's bandwidth. GJ.\n@ghost\nI removed \"searx.xyz\" from 'SES_SERVERS'. You should do that too.\n@kvch\n\nIsn't this what searx engines would do in searx?\n\nYou're missing the point.\nSearx don't make a connection to other searx servers.\nI'm using searx.at, and if the website goes down/my network admin blocked it, I can no longer search anything.\nRelated to https://github.com/asciimoo/searx/issues/860 if they are down, the user will scratch their heads.\nIf the user install Searxes on their side, it will make a connection to all possible searx servers,\nreturning results when searx.at/searx.me is down.. ",
    "Zebrazilla": "I used to use bangs for instant redirect with wikipedia and youtube on ddg quite a bit and it's super useful in my book. It'd be great if redirection like this could be explicitly defined in settings for certain engines.. ",
    "juanitobananas": "\n865 (you can close this)\nThe credit goes to him too. :)\n\nOh, definitely! I completely missed that! :sweat_smile: . ",
    "bingodona": "\nabout HSTS there is a preload list : https://hstspreload.org/\n\nExcept that there's no HSTS header to begin with: https://hstspreload.org/?domain=searx.me. ",
    "cswag": "Works perfect, thanks :) I forgot to find and replace. ",
    "w4tdot": "https://searx.laquadrature.net/ - Apache/2.4.10 (Debian) Server at searx.laquadrature.net Port 443\nhttps://searx.me/ - nginx/1.10.1 (+ keep requesting there you go DoS)\nhttps://searx.prvcy.eu/ - nginx/1.10.3 (Ubuntu)\nhttps://searx.info/ - nginx\nhttps://searx.de/ - nginx\nhttps://searx.pw/ - Request URI is too long, but in POST other errors.\nhttps://searx.me/ - \"Too big request header\"\nhttps://search.disroot.org/ - \"Too big request header\"\nNo limits on the app behind the webservers?\n. ",
    "hylinux1024": "It seems to cannot reach soumdcloud.com,and i remove the code in settings.yml \n```\n- name : soundcloud\nengine : soundcloud\nshortcut : sc\n```\nit works!. ",
    "levinus": "Thank you :). Thank you! Runs now fine.\nActually I thought of this because the installation tutorial metions to turn debug off (in a step before installing uwsgi). I thought I tested it with debug off. My fault.\nuwsgi version is 2.0.12-debian from official ubuntu 16.04 package repo. See uwsgi log for further details.. ",
    "micah": "Adam Tauber notifications@github.com writes:\n\nI think this is not a searx specific bug. Pyparsing is not a direct dependency of searx. The root cause is that pip and setuptools are outdated.\npip install --upgrade pip && pip install --upgrade setuptools solves the problem.\n\nprobably, but searx could make this smoother for people who are in this\nsituation, no?\n. Hi @misnyo -- i'm wondering if you ever got a chance to look at this? I just tried again with ipv6 addresses set in source_ips and I'm seeing these engines fail:\nEngines cannot retrieve results:\nwikidata (request exception), startpage (request exception), bing (request exception), reddit (request exception). xinomilo notifications@github.com writes:\n\nstartpage, bing, reddit and others, aren't accessible to ipv6. no AAAA\nrecords.  in ipv6 vm, most engines produce same error.\n\nSame with duckduckgo, no AAAA records.\n\nwikidata is working here in a test ipv6 instance, so don't know why the exception there. what does uwsgi say? \n\nERROR:searx.search:engine wikidata : requests exception(search duration : 0.77158498764 s, timeout: 3.0 s) : HTTPSConnectionPool(host='www.wikidata.org', port=443): Max retries exceeded with url: /w/api.php?action=parse&format=json&page=Q10443&uselang=en&redirects=1&prop=text%7Cdisplaytitle%7Clanglinks%7Crevid&disableeditsection=1&disabletidy=1&preview=1&sectionpreview=1&disabletoc=1&utf8=1&formatversion=2 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f0b9dfe6ad0>: Failed to establish a new connection: [Errno -9] Address family for hostname not supported',))\nTraceback (most recent call last):\n  File \"/srv/searx/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/srv/searx/searx/search.py\", line 87, in search_one_request\n    return engine.response(response)\n  File \"/srv/searx/searx/engines/wikidata.py\", line 80, in response\n    htmlresponse = get(url)\n  File \"/srv/searx/searx/poolrequests.py\", line 133, in get\n    return request('get', url, **kwargs)\n  File \"/srv/searx/searx/poolrequests.py\", line 109, in request\n    response = session.request(method=method, url=url, **kwargs)\n  File \"/srv/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 512, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/srv/searx/searx-ve/local/lib/python2.7/site-packages/requests/sessions.py\", line 622, in send\n    r = adapter.send(request, **kwargs)\n  File \"/srv/searx/searx-ve/local/lib/python2.7/site-packages/requests/adapters.py\", line 513, in send\n    raise ConnectionError(e, request=request)\nConnectionError: HTTPSConnectionPool(host='www.wikidata.org', port=443): Max retries exceeded with url: /w/api.php?action=parse&format=json&page=Q10443&uselang=en&redirects=1&prop=text%7Cdisplaytitle%7Clanglinks%7Crevid&disableeditsection=1&disabletidy=1&preview=1&sectionpreview=1&disabletoc=1&utf8=1&formatversion=2 (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f0b9dfe6ad0>: Failed to establish a new connection: [Errno -9] Address family for hostname not supported',))\nException in thread 8075e2f3-96dc-4eac-afd8-06a2038f1345:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/usr/lib/python2.7/threading.py\", line 754, in run\n    self.__target(*self.__args, **self.__kwargs)\n  File \"/srv/searx/searx/search.py\", line 150, in search_one_request_safe\n    engine.suspend_end_time = time() + min(settings['search']['max_ban_time_on_fail'],\nNameError: global name 'settings' is not defined\nthat looks like there is no AAAA record for www.wikidata.org either, but\nit does have one:\n$host www.wikidata.org\nwww.wikidata.org has address 208.80.154.224\nwww.wikidata.org has IPv6 address 2620:0:861:ed1a::1\n. docker isn't closed source.... Hi, I'm just wondering what the status is of this pull request. Its been open for almost a year now and there has been no comment on it. This could help with the google captcha issue!. Hi @misnyo - any chance you can rebase this change off of master to resolve these conflicts? If you can do that, then I can try and apply it to my local searx and see if it works!. I've got this problem. I'm running filtron and morty and the latest searx code. I've got ddg, startpage and bing enabled and I've tried to ssh tunnel with the IP and browse with it for a while. I've checked my IPs and they do not appear on RBLs or block lists. I've also tried to use elinks locally on the machine to use google... neither the local or the ssh tunnel are asked for a captcha, and the problem still persists. \nIs there a way we could setup multiple IPv6 addresses on the machine and have searx rotate through them for the outgoing search results?. #1034 might fix this, but it has not received any response since it was submitted.. @miicha - I'm wondering how you were able to determine that filtron was not working? Is there a log somewhere that was showing you that this particular filter was bugged?. \nI'm currently there. At this point, searx is no longer usable at all,\nand I'm going to have to shut it down if there is no way to work around this.\n. ",
    "07416": "This error message appears to have been added in version 0.12.0:\n\nI can now say that also the other search engines are affected.. @kompowiec Can you elaborate? How you fixed this issue? Adding \"searx\" to the string doesn't solve the problem.. With my custom URL Searx performs a search instead.. @mrtargaryen I get a blank page with your URL in the second message.. @asciimoo Reason?. @asciimoo Will you disable Bing or Yahoo by default?. @kvch Yes. \nShould the saved preferences be restored when pasting the URL to the browsers address bar? Why &q=%s is added to the end of the string?. I started receiving German results approximately two months ago. Both Searx and Firefox language is set to en-us. I don't live in Germany and don't have a German IP.\nlanguage    en-US\nlocale  en. @kvch Enhancement request. This is especially an issue with the main Searx instance as it's unclear if the search fails or Google finds no results.\nAnd of course Google can give the \"no results\" error when searching for any uncommon word or string, sorry for being unclear.. ",
    "Rinma": "Yes this helped. Now it works. Thanks. ",
    "anarcat": "On 2017-04-08 09:28:29, Adam Tauber wrote:\n\n@anarcat searx uses the instance_name string from settings.yml for this purpose. It is \"searx\" by default, but you can modify it. E.g. firefox displays Add \"searx.me\" on searx.me.\n\nYeah, that's what we found in our deployment as well. I'm arguing the\ndefault should be changed.\n\nI don't think that $(hostname -f) would be a better default. It can be confusing on systems with no proper domain configuration (my hostname is often just a single character).\n\nWell, if not \"hostname -f\", something unique should be chosen, because\nthere are lots of conflicting instances out there.\nI would argue that having \"a single character\" is still better than\n\"searx\" as a default, because then it's unlikely to conflict with others\n(unless the same letter is chosen of course). Besides, as you said it's\nonly the default...\n(That's a rather strange convention, actually - i haven't seen many\nhosts with single-character hostnames...)\nFor the record, the proper way to get the FQDN in Python is\nsocket.getfqdn(). See this for a more in-depth discussion:\nhttp://stackoverflow.com/questions/11580023/how-do-i-get-my-computers-fully-qualified-domain-name-in-python\n. On 2017-04-08 11:02:00, Adam Tauber wrote:\n\nIt was just an example. What if the hostname isn't related to searx\n\nthen the admin needs to change the default. otherwise the search tag\nwill be a little weird, but this will still work.\n\nor there are multiple services under the same hostname?\n\nthen they would collide, by default, which is already the case.\n\nFQDN is not a unique identifier for services, so i still don't agree that it would be a better default.\n\nwell, it would more unique than a static default, so i think it would be\nbetter...\n. ",
    "Elianmauri123": "yes, well, i would like personalize it to make a little more personal, can i do?, have downloaded the git clone but i don't know to do after :/ what is the cd searx? where do i find it? i don't speak very well english but hope to understand me :), thanks. . ",
    "JamesColeman-SH": "Oh... It is a branch..... https://github.com/asciimoo/searx/pull/902. ",
    "Nikola-K": "I'm not too familiar with this project but based on the linked test which is most realistic for this type of project https://klen.github.io/py-frameworks-bench/#remote there doesn't seem to be much difference, in fact flask is better than some other frameworks. Is flask really the bottleneck for this project?\nIn my personal experience, unless you're going with an async type of framework the performance gains from switching from flask to falcon are negligible at best.. ",
    "Openmedianetwork": "The new version of mastodon has search built in https://github.com/tootsuite/mastodon/releases/tag/v2.3.0 ElasticSearch. ",
    "d0t1q": "The SSL encryption is done at the tcp layer between the server and client, everything sent via ssl is then encrypted (post/get). This means the URI is encrypted, DNS is not encrypted though so the domain itself is visible. . ",
    "drinckes": "Pull request is merged - it does need some cleaning up but should be functional.. ",
    "utiltiy": "Hi,\nif got the same problem. The workaround solved my problem and uwsgi starts now with my additional *.ini file.\nThanks!. ",
    "F3nd0": "I seem to be experiencing this issue as well.. ",
    "bamstam": "Any progress on this issue?\nFrom a translator's perspective it is highly inefficient to maintain two translations of the same language, if there is no significant difference (e. g. the formal \"Sie\" vs. the informal \"du\" in German).\nI would suggest deleting one of the two translations to avoid double work for translators in the future.. I paged the translators who contributed to both translations in the past and referred to this issue to get more input. I would recommend a coin flip :-). Both versions are fully translated and of comparable quality as far as I can see, so I guess it is safe to randomly pick one translation for deletion. y/n?. We still have two flavors of German :-) How to proceed? @Siggi0904 @kvch @ShalokShalom . ",
    "Siggi0904": "Hi, I didn't use these sotware but I'd like to help with translations.\nI copyed the non formal version to the fermal version and updated some strings.\nBut I can not change all strings because I'm not a reviewer.\nPlease update myself to a reviewer, then I will optimize many strings.\nThanks.. @kvch No, please read the transifex group guidelines. Only reviewer can change reviewed strings.\nAnd if the community will find a solution please delete the whole language in transfex.\n@ShalokShalom Download the files from transifex and use a tool to compare.\nTransifex itself can't do this.. Please delete all translators from DE_de in Transifex and remove this ressource.\nThanks.. ",
    "leedoyle": "@asciimoo updated the issue.. Steps to reproduce: try to search !yn test. ",
    "Youaresmarter": "It seems not working for now. I've checked 5 public searx instances and self-hosted docker instance. No luck with Yandex. Any ideas how to fix it? . ",
    "gled-rs": "just adding in uwsgi/app-enabled/searx.ini:\nlazy-apps = true\nenable-threads = true\nsolved the issue for me ( the enable-threads is the one that definitely helped ). I tend to recall that threads are disabled by default on uwsgi.. ",
    "lcpz": "@dalf Nice, just center again the results list and it'll be perfect, imho.. I suggest to use the searx word in blue from main page, in a fashion similar to SP, Google and others.. But it's not centered?. I did it. I don't understand how it works, though. Does it get a review there?. Requested .mo file added.\n\nYour translation page is wrong:\n\nthe Transifex instance to specify in tx init is just the default one (otherwise client won't login)\nthe tx set command currently does not work as is\n\nFor the point 2, I used -l it instead of --auto-local but it gave me chinese; I tried again, and it gave me hebrew. I ended up downloading the .po file manually.\n\nThe Transifex client stores password and token in plain (but the site keeps it secure by showing it to you only once!). Plus, it's really frustrating to use (see above).\nOne could just:\n\ngit clone searx\nmodify/create searx/translations/<lang>/LC_MESSAGES/messages.po\npybabel compile -d searx/searx/translations\n\n. @kvch Since it's intended for devs, I suggest to remove the use of Transifex client from your translation page and just add my previous commands, so that you won't loose other people like me either. \ud83d\ude03 . Maybe helpful: https://github.com/ipfs/archives/issues/32. ",
    "ng-0": "Doesn't \"minify\" add a dependency on NodeJS? I would rather avoid this due to the impossible to build from source state of nodejs packages and their cyclic dependencies.\nAs long as searx can be build without nodejs as a dependency, it can be integrated into systems such as Guix (working on it). Systemintegration of Nodejs is generally difficult if you care about accountability.. Okay. then let me rephrase my last comment, written at a time of the night when you shouldn't be writting comments:\nIf you are just talking about modifications to the files which do not require this in the setup process\n(so long as the build process does not demand, require npm packages) it's all good for me.. I wished I had a better explanation, but it fixed itself.. ",
    "jodo13": "I see, that is why I said too that by the results (at least I have).\nAnyway, I use laquadrature.net instance (0.11) and searx.me (0.12) and along the last week and these last couple of days, both couldn't retrieve from Google and Bing. Now Searx.me can't still retrieve from Google and intermittent with Bing.\nSo, let me tell it here, just in case something is broken too ;-)\nThanks anyway for all your work :). @kvch as the subject of the issue says, I'm talking about images engines, or images results, so, Bing Images :)\nAs told in the previous message to yours, it is intermittent with Searx.me instance. Sometimes display Bing images results, and some doesn't. Might be high load on Bing Images or whatever.\nThanks for the google fix. I hope to see it soon on instances :). ",
    "aurora-anon": "But why it says it ran out of limit?\nI see lotta searx clones that provide csv,json and xml output without any problems.\nWhat do they do?. If the user selects french as the search language in the preferences, use google.fr.\nyou get the idea?. There are also some other privacy-respecting search engines that actually use Google.\nIf you get in trouble with Google CAPTCHAs, use those services.. https://www.millionshort.com/. https://isp.netscape.com/. take a look at https://en.wikipedia.org/wiki/List_of_search_engines. I mean defining a custom site by the user, not the server.. Adding sites to the search engines by the user.. @asciimoo\nBut it is a server-to-server communication.\nThe other instances don't know who a search query belongs to.. @kvch Searx is now fully translated to Persian.\nPlease add fa and fa-IR as search languages in searx.. @ahangarha So why am I here for?. @ahangarha I know Persian.\nBut I don't know how to become a reviewer on searx's transifex project. Any ideas?. @ahangarha I don't think so. It works fine for me. Can you explain the problem in more detail?. @ahangarha That's not possible.\nGive me a screenshot.\nCheck the searx's settings page and enable all major search engines like bing, google, yahoo, yandex, duckduckgo & ixquick and disable the rest in general engines list.. @kvch nasty786. @ahangarha You're right. I've never noticed that.\nI'm a native Persian speaker, but I seldom write in Persian.\nThe problem is only from the search language.\nLook at this: https://searx.me/?q=%D9%86%D8%B1%D9%85%20%D8%A7%D9%81%D8%B2%D8%A7%D8%B1%20%D8%A2%D8%B2%D8%A7%D8%AF&categories=general&language=fa\nI've manually set the search language. It works fine.\nPersian has to be added to the searx's search languages.. @kvch Please add fa and fa-IR to searx's search languages.. @MarcAbonce @searx @kvch Please add 'nasty786' as a reviewer on searx's Transifex project.. @kvch I'm still unable to review strings.\nSee https://www.transifex.com/asciimoo/teams/6602/fa_IR/.\nIt says 'Only reviewers can perform mass actions'.\nI've received a notification from you, but I'm still not listed as a reviewer.\nI feel the problem is coming from Transifex itself.. @kvch I'm added as a Persian reviewer, not as Persian(Iran).\nThe language code I mean is fa_IR.\nPlease add me to the 'Persian' tranlators list and also to the 'Persian(Iran)' reviewers list.. @ahangarha Sorry, but please don't ruin my translation on searx's Transifex project.\nYou're degrading the translation quality.. @ahangarha I'm a Linguistics expert, I've translated texts written back in 1600.\nYou don't translate with much accuracy.. @kvch Searx is now 100% translated and reviewed in Persian(Iran).\nPlease update the source code snd also add fa-IR to searx's interface languages.\nAlso, please update the searx's version number, so other instances get the language update.. No, the results are very inappropriate.\nWhatever you search, you just see some results that aren't what you wanted.\ntry it out.. Compare search results from Google Play with searx.\nWhen it doesn't find anything near what you've searched, it will show you results too far from what you meant.\nJust like the Google itself.. +1. similar to issue 1256. @kvch Is searx.i2p an official searx instance?. +1. @letarg0 As I see Polish is available for both interface and search language, and searx's interface is 100% translated to Polish.\nIf you mean non of the searx instances mentioned above support Polish as an interface language could be due to a few possibilities:\n- They consciously removed Polish from the interface languages.\n- They haven't updated to the latest version of searx, so they would receive language updates for Polish.\n- It's caused by a misconfiguration; that's very unlikely.\nAnyway, the only official domain for searx is searx.me and it fully supports Polish language for both search and as an interface language.\nIf you don't wanna use the official searx website, you can find a searx instance that supports Polish from here.. @kvch Please check searx's Polish language support to see if the problem is coming from the source code.\nMany sites have partial or no support for Polish language, despite having the same version number as for this and this for more information.\n\nIf you're using Procon Latte, please fully whitelist the searx's domain name.\n\nPlease mention if the problem is solved.. @holymio I should also mention that the Searx's Firefox extension requires Javascript to be enabled. Check if it's coming from Javascript.\nAnyway, I recommend you to  manually add searx as a search engine, as it doesn't require any extension.. @asciimoo\n- You have to create a list for security recommendations or advices and add these and many other considerations to that list, so anyone creating an instance of searx would bring the highest level of protection.\n\nsearx.me is vulnerable to CVE-2016-1247 according to pentest-tools.com.. Also #1200. \n",
    "Quix0r": "Not helping here:\nhttps://searx.mxchange.org/\nIt says \"Not found\" and log shows:\n[pid: xxxxx|app: 0|req: 1/1] x.x.x.x () {48 vars in 699 bytes} [Sun Sep  3 00:29:09 2017] GET /errors/403.html => generat\ned 1272 bytes in 36 msecs (HTTP/1.1 404) 2 headers in 88 bytes (2 switches on core 0). ",
    "Eliesemoule": "Issue solved and install procedure updated to work on CentOS 7. ",
    "mrtargaryen": "This is the URL with the preferences set and it returns a blank page on https://searx.me on multiple browsers: https://searx.me/?results_on_new_tab=False&language=en-US&locale=en&disabled_plugins=Vim-like_hotkeys&image_proxy=True&autocomplete=startpage&safesearch=0&theme=oscar&enabled_plugins=Open_result_links_on_new_browser_tabs%2CInfinite_scroll%2CDOAI_rewrite%2CHTTPS_rewrite%2CTracker_URL_remover%2CSelf_Informations%2CSearch_on_category_select&disabled_engines=duckduckgo+images__images%2Cccc-tv__videos%2Cfree+software+directory__it%2Cvoat__social+media%2Cbase__science%2Cina__videos%2Cbing+images__images%2Cgoogle+play+apps__files%2Csubtitleseeker__videos%2Cpdbe__science%2Cnyaa__videos%2Cbing+news__news%2Ccrossref__science%2Cfdroid__files%2Csemantic+scholar__science%2Cseedpeer__files%2Ckickass__videos%2Cgoogle+play+music__music%2Cyandex__general%2Cdeezer__music%2Cgeektimes__it%2Cmymemory+translated__general%2C500px__images%2Cdigbt__videos%2Carchive+is__general%2Cgoogle+play+movies__videos%2Cdictzone__general%2Cseedpeer__music%2Ccurrency__general%2Cgitlab__it%2Cmixcloud__music%2Cqwant+social__social+media%2Csoundcloud__music%2Cdigbt__files%2Cerowid__general%2Cseedpeer__videos%2Cnyaa__images%2Cdigg__news%2Clibrary+genesis__general%2Cgoogle+scholar__science%2Creddit__general%2Cdailymotion__videos%2Cbitbucket__it%2Cfrinkiac__images%2Chabrahabr__it%2Cetymonline__general%2Ctokyotoshokan__music%2Cflickr__images%2Cframalibre__it%2Cmicrosoft+academic__science%2Cdigbt__music%2Cvimeo__videos%2Creddit__images%2Copenrepos__files%2Cqwant+news__news%2Carch+linux+wiki__it%2Clobste.rs__it%2Choogle__it%2Cswisscows__general%2Cqwant+images__images%2Ctokyotoshokan__videos%2Cqwant__general%2Cdeviantart__images%2Cnyaa__files%2Cswisscows__images%2Cgigablast__general%2Cscanr+structures__science%2C1x__images%2Cdigg__social+media%2Csearchcode+code__it%2Cspotify__music%2C1337x__videos%2Csearchcode+doc__it%2Cnyaa__music%2Ctokyotoshokan__files&enabled_engines=kickass__files%2Cduckduckgo__general%2Creddit__social+media%2Creddit__news%2Cixquick__general%2Cstartpage__general%2Cddg+definitions__general%2Ckickass__music&method=POST&categories=general&q=%s. Thank you for your reply,\nWhen I use https://searx.me and search for something I am given the option to view the results via a proxy. How do I set up a proxy so I can view sites through a proxy in my instance of searx on my imac?. Thanks,\nI tried to run $ go get github.com/asciimoo/morty in terminal on my imac but it returns a message -bash: $: command not found Terminal also doesn't recognise \"get\"\nLike I said I'm a total noob, I'll just keep reading what ever I can to get this working. \nThanks for your help :-). Hi,\nDoes anybody know how to install Morty on macOS 10.12.6\nWhen I follow the instructions here: https://github.com/asciimoo/morty   This is the result in Terminal:\niMac$ go get github.com/asciimoo/morty\n-bash: go: command not found\nI have tried asking elsewhere but have had no replies.\nI am only very new to doing things like this but I am very keen to learn but do not know where to learn this stuff. I don't want to waste anyones time but if someone could point me in the right direction I would be very grateful.\nThank you very much.\n. > It seems like you need to install go.\nThanks for the reply.\nI installed Go for macOS and restarted my Mac, then opened Terminal and typed in:\ngo get github.com/asciimoo/morty\nTerminal gave me this result:\nMy-iMac:~ iMac$ go get github.com/asciimoo/morty\npackage github.com/asciimoo/morty: mkdir /Users/imac/go: not a directory\nIn /Users/imac/ there is a empty text file named go but no directory.\nAny idea what I can do?\nSorry about this, I'm in uncharted territory for myself. . > Remove that file and create a folder with the same name (in console: rm /Users/imac/go && mkdir /Users/imac/go)\nThank you so much! \nI ran the tests and it appears to be working.\nCan I ask one last question if its not too much (if you're too busy I understand).\nIn order for the proxy to work in the search results how do I configure Searx/Morty to do that? On Searx.me (which I have been using for some time) I see the option to view the page via a proxy version of the page or the cached version. I'd like to be able to have that available to me on mine.\nAgain thank you very much for taking the time to help, its a bit of a learning curve for me. I've purchased a book on docker & command line which I'm intending to read..  searx supports result proxification using an external service: https://github.com/asciimoo/morty\n uncomment below section if you have running morty proxy\nresult_proxy:\n    url : http://127.0.0.1:3000/\n    key : your_morty_proxy_key\nSo the URL is  http://127.0.0.1:3000/ but the morty proxy key is that a specific number I need to input in that line? If so where do I find that? . > To sum up :\ndefine a key (whatever you want) on settings.yml\nwhen you start morty add the -key parameter and the shared key.\n\nSo lets say I make my key \"key123\", in the settings.yml I place http://127.0.0.1:3000/ (or do I place my searx url:3000?) and add my new key \"key123\"\nAnd what do you mean by when I start morty add the -key parameter and shared key, where in the settings of morty do I do that?\nSorry, I understand this might be frustrating for you but I'm struggling to understand.. > and use the following command to start morty:\n\"$GOPATH/bin/morty\" -key \"key123\" -listen \"127.0.0.1:3000\"\nAhhh thank you so much, now I understand. . I edited the settings.yml \n\nThen I activated morty as per your instructions \n\nI restarted Searx\nThe I tested Searx to see if the proxy was working\n\nI'm not sure what I am doing wrong, I followed what you wrote.\n. I uninstalled docker, and morty, and searx, and everything else. Then I reinstalled everything and followed your instructions. I did not edit any file other than the settings.yml, \nresult_proxy:\n    url : http://127.0.0.1:3000/\n    key : 7720517\nThen I ran this in Terminal\n/Users/imac/go/bin/morty -key 7720517 -listen 127.0.0.1:3000\nI then stopped Searx\ndocker stop $(docker ps -q)\nThen I restarted Searx\ndocker run -d -p 8888:8888 searx\n... and the search proxy is still not working. I really don't know what I am doing wrong. But I'll just have to use Searx locally without the result proxy.. \nWhen I typed in http://127.0.0.1:3000/ in to the url bar in firefox I got this message.\n\n. > It seems you built your own docker image. Have you built it again after you modified the settings.yml ?\nI uninstalled Docker and morty and then reinstalled everything following this guide:\nhttps://github.com/asciimoo/searx/wiki/Installation\nI then modified the settings.yml, but I did not then build it again.. Ok,\nSo, I followed this installation page https://github.com/asciimoo/searx/wiki/Installation\nFirstly I installed Docker: https://store.docker.com/editions/community/docker-ce-desktop-mac\nThen I installed GO: https://golang.org/doc/install\nThen in Terminal I ran this command: \ngit clone https://github.com/asciimoo/searx.git\nAfter that I ran this command:\ncd searx\nThen I ran this command:\ndocker build -t searx \nAfter this I ran this command to start SearX:\ndocker run -d -p 8888:8888 searx\nThen I installed Morty: https://github.com/asciimoo/morty\nMorty Installation and setup:\n$ go get github.com/asciimoo/morty\n$ \"$GOPATH/bin/morty\" --help\nTest:\n$ cd \"$GOPATH/src/github.com/asciimoo/morty\"\n$ go test\nBenchmark:\n$ cd \"$GOPATH/src/github.com/asciimoo/morty\"\n$ go test -benchmem -bench .\nThen I edited the settings.yml file in Searx (https://asciimoo.github.io/searx/admin/morty.html#how-to-setup-result-proxy)\nresult_proxy:\n    url : http://127.0.0.1:3000/\n    key : 7720517\nThen I executed this command in Terminal:\n\"$GOPATH/bin/morty\" -key \"7720517\" -listen \"127.0.0.1:3000\"\nI installed Searx on my iMac 27\" late 2015 macOS 10.12.6. and I also tested this on my Ubuntu 16.04 PC and in both cases the results search proxy does not work.\nThese are exactly the stepss I took to get the Result Search Proxy to work in Searx. It does not work.\n. After running /Users/Rick/go/bin/morty -key 7720517 -listen 127.0.0.1:3000 in Terminal (and then restarting Searx ($ docker stop searx and $ docker start searx) I accessed /Users/Rick/go/bin/morty and this is what I found;\n```\nLast login: Fri Aug 18 03:22:54 on ttys001\nRick-iMac:~ imac$ /Users/rick/go/bin/morty ; exit;\n2017/08/18 05:16:20 listening on 127.0.0.1:3000\n2017/08/18 05:16:20 Error in ListenAndServe:listen tcp4 127.0.0.1:3000: bind: address already in use\nlogout\nSaving session...\n...copying shared history...\n...saving history...truncating history files...\n...completed.\nDeleting expired sessions...none found.\n[Process completed]\n```\nI'm not sure if its relevant but there appears to be an error.\nEdit:\nMorty is using 127.0.0.1:3000 \nI ran a Port Scan:\nOpen TCP Port:     3000   hbci\nI have set the Morty Key in settings.yml: \n\nand ran this in Terminal:\n/Users/Rick/go/bin/morty -key 7720517 -listen 127.0.0.1:3000\n(And I have stopped and started Searx).\nSo why isn't Morty receiving a mortyhash parameter from Searx? @dalf @asciimoo \nCan anyone replicate this issue? I have on a PC running Ubuntu.. @adeweever91 Thank you so much, thats awesome. I set it up easily on local host and its working fine with HTTPS Everywhere blocking all unencrypted requests. Awesome!!. @adeweever91 I'm trying to edit the Searx settings.yml for Morty serch proxy but I'm unable to find it with this installation on Ubuntu. Where are your searx_ss files stored? Cheers.. ",
    "kompowiec": "I checked, if afther write simply searx it settings are saved in cookie. \nSent from my HUAWEI NEM-L21 using FastHub. hard to say, because for some reason the error no longer exists. I used this link now: http://tinyurl.com/yb9c88q5 and redirect normally on search page with string %s and work. Interestingly, this string is absent in shortly, with fewer settings URLs.\nedit: just like you.. ",
    "inattendu": "Nope, sorry i've missed that the apache part of the doc I've linked use UWSGI, i prefer using apache as a HTTP reverse-proxy with directives like  : \nProxyRequests Off\n        <Proxy *>\n                AllowOverride None\n                Require all granted\n        </Proxy>\n        ProxyPass / http://localhost:8888/\n        ProxyPassReverse / http://localhost:8888/. ",
    "angristan": "Hey, exact same issue and exact same config, any solution to this?\nhttps://searx.angristan.xyz/opensearch.xml. Doesn't seem so:\nroot@lyra ~# curl -I https://searx.angristan.xyz/opensearch.xml\nHTTP/1.1 404 Not Found\nServer: nginx\nDate: Mon, 05 Feb 2018 22:22:26 GMT\nContent-Type: text/html\nContent-Length: 162\nConnection: keep-alive\nVary: Accept-Encoding\nroot@lyra ~# curl -I https://searx.angristan.xyz/opensearch.xml\nHTTP/2 404 \nserver: nginx\ndate: Mon, 05 Feb 2018 22:22:43 GMT\ncontent-type: text/html\ncontent-length: 162\nvary: Accept-Encoding\nAlso here is where the file is located for me:\nroot@lyra ~# find /srv/searx/ -name opensearch.xml\n/srv/searx/searx/templates/__common__/opensearch.xml. @asciimoo I'm using uwsgi, so I had to start searx manually with python searx/webapp.py.\nIt works:\nroot@lyra ~# curl localhost:8888/opensearch.xml -I\nHTTP/1.0 200 OK\nContent-Type: text/xml; charset=utf-8\nContent-Length: 797\nServer: Werkzeug/0.14.1 Python/2.7.13\nDate: Mon, 05 Feb 2018 22:43:58 GMT\nShould we use Nginx as a reverse proxy?. (searx-ve) $ uwsgi --plugin python -w searx.webapp --http-socket 127.0.0.1:4000 --master --processes 2 -H env --enable-threads --lazy-apps\n*** Starting uWSGI 2.0.14-debian (64bit) on [Mon Feb  5 23:52:58 2018] ***\ncompiled with version: 6.2.1 20161124 on 07 December 2016 16:14:59\nos: Linux-4.9.0-5-amd64 #1 SMP Debian 4.9.65-3+deb9u2 (2018-01-04)\nnodename: lyra\nmachine: x86_64\nclock source: unix\npcre jit disabled\ndetected number of CPU cores: 2\ncurrent working directory: /srv/searx\ndetected binary path: /usr/bin/uwsgi-core\nyour processes number limit is 15732\nyour memory page size is 4096 bytes\ndetected max file descriptor number: 1024\nlock engine: pthread robust mutexes\nthunder lock: disabled (you can enable it with --thunder-lock)\nuwsgi socket 0 bound to TCP address 127.0.0.1:4000 fd 3\nPython version: 2.7.13 (default, Nov 24 2017, 17:33:09)  [GCC 6.3.0 20170516]\nSet PythonHome to env\nImportError: No module named site\nIt seems to be an env/path issue but I can't figure it out \ud83d\ude14 . hum thanks :D \nSo it's working:\nroot@lyra ~# curl localhost:4000/opensearch.xml -I\nHTTP/1.1 200 OK\nContent-Type: text/xml; charset=utf-8\nContent-Length: 797\nSo the issue is the Nginx configuration.. Here is my conf:\n```\nserver {\n        listen 80;\n        listen [::]:80;\n        server_name searx.angristan.xyz;\n        return 301 https://searx.angristan.xyz$request_uri;\n    access_log  /dev/null;\n    error_log /dev/null;\n\n}\nserver {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        server_name searx.angristan.xyz;\n    root /srv/searx/searx;\n\n    access_log /var/log/nginx/searx-access.log;\n    error_log  /var/log/nginx/searx-error.log;\n\n    include global/https.conf;\n    include global/static.conf;\n\n    location / {\n            include uwsgi_params;\n            uwsgi_pass unix:/run/uwsgi/app/searx/socket;\n    }\n\n}\n```\nAnd static.conf contains:\n```\nlocation ~* .(ico|pdf|flv|jpg|jpeg|png|gif|js|css|swf|x-html|html|css|xml|js|woff|woff2|ttf|svg|eot)$ {\n        expires 30d;\n        access_log off;\n        log_not_found off;\n}\n```\nSo it overrides the first location {}, and thus any .xml file isn't served by uwsgi. Just remove it and it's fine:\nroot@lyra ~# curl -I https://searx.angristan.xyz/opensearch.xml\nHTTP/2 200 \nserver: nginx\ndate: Mon, 05 Feb 2018 23:17:21 GMT\ncontent-type: text/xml; charset=utf-8\ncontent-length: 815\nstrict-transport-security: max-age=31536000; includeSubDomains; preload\nI encountered this issue with other services. Sorry to bother you.... For the record, fixed configuration:\n```\nserver {\n        listen 80;\n        listen [::]:80;\n        server_name searx.angristan.xyz;\n        return 301 https://searx.angristan.xyz$request_uri;\n    access_log  /dev/null;\n    error_log /dev/null;\n\n}\nserver {\n        listen 443 ssl;\n        listen [::]:443 ssl;\n        server_name searx.angristan.xyz;\n    root /srv/searx/searx;\n\n    access_log /var/log/nginx/searx-access.log;\n    error_log  /var/log/nginx/searx-error.log;\n\n    include global/headers.conf;\n    include global/https.conf;\n\n    location / {\n            include uwsgi_params;\n            uwsgi_pass unix:/run/uwsgi/app/searx/socket;\n            expires 30d;\n    }\n\n}\n```. Alpine 3.8 is out since! I will do some testing and make a PR if it works :). ",
    "adeweever91": "I created this image, it will run a searx instance using nginx/uwsgi and it will generate a self signed cert. Should work for HTTPS everywhere but youll have to accept the cert. \nhttps://hub.docker.com/r/adeweever91/searx_ss/. Sorry for the slow reply, if you run docker exec -it CONTAINER_NAME sh youll get a shell inside the container, then vi /usr/local/searx/searx/settings.yml youll be able to edit the settings file. save the changed then exit the shell inside the container and restart the container. Do you know how to use volumes in docker? if you do just mount your settings file with -v /path/to/your/settings.yml:/usr/local/searx/searx/settings.yml when you run the container and then you can just make changes to the file on your system and restart the container after you have made those changes. ",
    "Carl-Robert": "Regarding this issue;\nWhen I select a preferred language (Finland fi_FI), google shows only results in Finnish language, not the default results of google.fi. So it isn't just simply changing the domain, and hence not using the option myself (can't search big international sites like ebay, instead get random Finnish websites discussing these sites).\nSo having a toggle for just domain-changes would be appreciated, as in not also checking google's \"search using this language only\" option.. The default time limit (2 seconds or so) is way too short usually for my private instance. Try to increase that (put something like 20 seconds to be on the safe side) in the settings.yml under google or globally.\nShould stop the refreshing, though can sometimes be a bit slow (less than ten seconds though).. Thank you! Actually had to change the line number 35 to make it work (queries Youtube for results, but replaces the link with Hooktube, not perfect, but at least works).\nAdding for those looking to achieve this:\nChange the URL on line https://github.com/asciimoo/searx/blob/master/searx/engines/youtube_noapi.py#L32 to //hooktube.com/embed/{videoid}\nAlso change https://github.com/asciimoo/searx/blob/master/searx/engines/youtube_noapi.py#L35 to https://www.hooktube/watch?v=for the full site, or https://www.hooktube.com/embed/for just the video.\nNow it queries youtube.com for the results, yet presents the embed link on the searx page with hooktube.com version, and clicks go to either hooktube.com full version (needs javascript) or just the video.. DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.googleapis.com\nDEBUG:urllib3.connectionpool:https://www.googleapis.com:443 \"GET /youtube/v3/search?part=snippet&q=test&maxResults=20&key=myapikeyhere&relevanceLanguage=en HTTP/1.1\" 200 None\nERROR:searx.search:engine youtube : exception : 'videoid'\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/usr/local/searx/searx/search.py\", line 87, in search_one_request\n    return engine.response(response)\n  File \"/usr/local/searx/searx/engines/youtube_api.py\", line 54, in response\n    videoid = result['id']['videoid']\nKeyError: 'videoid'\nI'm sorry, but this is all the debug-log shows me. I don't see any further information on it.\nOr is there some way to get it to log more?. Alright, this is what it says now:\nDEBUG:urllib3.connectionpool:https://www.googleapis.com:443 \"GET /youtube/v3/search?part=snippet&q=test&maxResults=20&key=myapikeyhere&relevanceLanguage=en HTTP/1.1\" 200 None\n{u'snippet': {u'thumbnails': {u'default': {u'url': u'https://i.ytimg.com/vi/_nfREkI7Nsk/default.jpg', u'width': 120, u'height': 90}, u'high': {u'url': u'https://i.ytimg.com/vi/_nfREkI7Nsk/hqdefault.jpg', u'width': 480, u'height': 360}, u'medium': {u'url': u'https://i.ytimg.com/vi/_nfREkI7Nsk/mqdefault.jpg', u'width': 320, u'height': 180}}, u'title': u'MAKE THE HARDEST CHOICE EVER. PERSONALITY VIDEO TEST', u'channelId': u'UCYenDLnIHsoqQ6smwKXQ7Hg', u'publishedAt': u'2017-10-19T18:08:47.000Z', u'liveBroadcastContent': u'none', u'channelTitle': u'#Mind Warehouse', u'description': u\"BRAIN TIME\\u25bb https://goo.gl/tTWgH2 We're sure you've always wanted to know your true personality, the real one. Not the one that smiles to its colleagues and neighbors. And neither the one...\"}, u'kind': u'youtube#searchResult', u'etag': u'\"g7k5f8kvn67Bsl8L-Bum53neIr4/_80iqfXYbkSSuBdA1u6m600pjpI\"', u'id': {u'kind': u'youtube#video', u'videoId': u'_nfREkI7Nsk'}}\nERROR:searx.search:engine youtube : exception : 'videoid'\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/usr/local/searx/searx/search.py\", line 87, in search_one_request\n    return engine.response(response)\n  File \"/usr/local/searx/searx/engines/youtube_api.py\", line 55, in response\n    videoid = result['id']['videoid']\nKeyError: 'videoid'\n. Oh, that's certainly peculiar, since I don't remember modifying it. Anyways, glad it got solved and apologies for wasting time for some typing error.. ",
    "Quent-in": "Regarding the Occitan language, it is yet included in the Unicode CLDR Project.\nHow comes the list of language is not that complete?. ",
    "ahangarha": "I can do translation. Just let me know how to do. @aurora-potter Translation is done. it is ready to review and use.. Do you have any person to review Persian translations?. @aurora-potter I meant a person who can review Persian translation on the transifex, to review translations.. I didn't know you know Persian. So maybe I should share another problem on searx: it works very badly for searching based on Persian text. Should I open a new issue for this?. I can send you screenshot of search result. Unlike English serach, search\nin Persian language is almost unusable.\nOn Fri, Apr 6, 2018, 00:43 Aurora notifications@github.com wrote:\n\n@ahangarha https://github.com/ahangarha I don't think so. It works fine\nfor me. Can you explain the problem in more detail?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1002#issuecomment-379062236,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKuHYwnWupPXWjQ65YnA3YLRfYntmUNBks5tlnr7gaJpZM4O8Ofj\n.\n. @aurora-potter Please take a look on these two screenshots:\n\n\n. @kvch my username is ahangarha. Thanks to all\nOn Sun, Apr 8, 2018, 08:19 Marc Abonce Seguin notifications@github.com\nwrote:\n\nPersian is now available in the search languages drop-down as \u0641\u0627\u0631\u0633\u06cc -\nfa-IR.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1002#issuecomment-379517860,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKuHY9j5F3i4_Dv_KFBoeFYvVG6_aH6Qks5tmYjIgaJpZM4O8Ofj\n.\n. \n",
    "webworker01": "Thank you, I did that and nothing appeared in the browser. Where would the log be generated?. Thank you again, I found the logs in /var/log/uwsgi/app/searx.log\nWARNING:searx.search:engine timeout: google\nERROR:searx.search:engine google : HTTP requests timeout(search duration : 30.0335729122 s, timeout: 30.0 s) : ReadTimeout\nSeems that this still happens after setting timeout to 30 seconds.   Hangs for one request, then seems to work fine for others.  Any way to get more verbose logging? I just set in settings.yml > general : debug : true. I do have a server instance running at a hosting provider. \nA simple curl returns results fine and consistently\ncurl -A \"Mozilla/5.0\" \"https://www.google.com/search?q=test+1234\"\nI modified gen_useragent() in utils.py to always return \"Mozilla/5.0\" and seems better so far. Not sure if it might be network reliability like you said though.. Nevermind, that didn't fix anything :). Is there possibly a way to pass the captcha through to the searx interface? If so would this be a problem with privacy? \nPS Where can I find the log files?  . Thanks.. looks like it's captcha for me too.. I have a public searx instance that I'd like to keep public, but if the captcha thing keeps happening, I can't even use it myself!\nI'm putting up cloudflare in front of my instance now.. What are the pros and cons of doing this in relation to privacy?. @Dominion0815 yes this will be the next step if I can't make google reliable for everyone.. \nEdit: I took the step of changing default search engines to duckduckgo, startpage and bing. I want to keep Google on by default but as I get more traffic to my instance, I still kinda want google working for myself ;). Would it be possible to display the captcha somewhere on the web interface?. ",
    "siddarth9": "What was the secret key where do I can get that secret key to add in setting.yml. I'm interested in using on my own domain so I want it to use on live how can I install this on cpanel of my web host\nPlease help me find solution . Is there anyone who could help with this. My webhost is cpanel shared  hosting On vps have root access my webhost proveiders ehost.com \n.please help me find one solution for this . ",
    "KwadroNaut": "I was looking on to do this in Searx as well. Ideally there should be the 'all' or 'any' option, and the option to set your preferences to several search languages, 2 is too limited.. ",
    "alexis-": "This would be a tremendously useful feature !\nAs far as I'm concerned, it is the only thing missing in Searx as it is, and I would then recommend it to my friends (who all use this feature) without any hesitation as the best alternative to Google for privacy concerns.\nThanks again for all the effort going into this project.. ",
    "pedrom34": "+1 to that.. ",
    "hulekgre": "I installed it with python, but 'images' doesn't work. And it constant 505 error.\nWhat dependencies are needed, or I can't use it with just python?. @misnyo After downloading the .zip I installed 'setup.py' with python, and called dependencies through pip. To execute it: c:/python webapp.py. Your link's instructions are gnu-linux only.\nI'm on Windows 7, and it does work. But it's slow, and I can't unselect engines otherwise it gives me 505 until the default preferences is set. Search on images, videos, news etc. is not working either with the default or not.. With python3 I have this problem now:\nhttps://pastebin.com/JHLuyzdG\n\"Traceback (most recent call last):\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packa\nges\\yaml\\reader.py\", line 89, in peek\n    return self.buffer[self.pointer+index]\nIndexError: string index out of range\"\n\"    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\nUnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 1235: cha\nracter maps to <undefined>\". ",
    "theel0ja": "Replace it with invidio.us, hooktube just shows iframes.. https://github.com/asciimoo/searx/issues/1372. subtitleseeker.gq is down. subscene.com still works.. ",
    "codecov-io": "Codecov Report\n\n:exclamation: No coverage uploaded for pull request base (master@6ebfdf0). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1037   +/-\n=========================================\n  Coverage          ?   73.98%         \n=========================================\n  Files             ?      101         \n  Lines             ?     5520         \n  Branches          ?      965         \n=========================================\n  Hits              ?     4084         \n  Misses            ?     1206         \n  Partials          ?      230\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6ebfdf0...c12e0c3. Read the comment docs.\n. \n",
    "knyfe": "I also would appreciate more detailed instruction about setting up filtron with searx. I've referred to How to protect an instance, the filtron project page, and the threads @RaspyVotan mentions. Best I've managed is HTTP 429 errors in the browser and Response error: dial tcp4 127.0.0.1:8888: getsockopt: connection refused HTTP/1.1 200 OK Server: fasthttp from filtron, no matter what rate limits I set. My main objective here is to prevent google captcha blocking in searx.. Same problem here. Fresh install of Ubuntu 16.04, nginx, uwsgi and searx. nginx/error.log shows \"upstream sent too big header while reading response header from upstream\" even with proxy buffer settings in nginx.conf and a system restart.. Yep, that is where I put it. Still no luck. Same \"too big header\" error in log, 502 Bad Gateway on the site, and no cookies. Perhaps I am missing other needed nginx.conf settings or require different proxy buffer settings than yours? At this point I just have the default nginx.conf file with the added proxy buffer settings. My previous setup was working beautifully prior to my attempt to update yesterday. I've since installed on a completely new server and getting the same errors.. ",
    "scroom": "@RaspyVotan @knyfe I've published an article on my blog in which I describe how to setup Searx with Filtron. It's written in german. If you need help don't hesitate to ask:\nhttps://scroom.de/eine-eigene-searx-instanz-unter-ubuntu-16-04-aufsetzen/\n@asciimoo If you want to I could translate it and you could publish it in your wiki or somewhere else.. @asciimoo How can I do so? Is it some part of github?. Same problem here.. I'm running a VPS provided by Contabo.de. On the same server I'm running a mastodon instance. The searx instance is public.. @dalf Same here, everthing is fine:\nPOSSIBLY SAFE 0/96\nIP Not Listed (Good!) -> on all listed blacklists. Yes, if I switch back to noapi, everything is running well again.. Maybe I've added the wrong API-Key? I've added an Youtube APIv3 Key. Is it the right one?. This link isn't working for me on my instance. I will file a new bug report for this issue.. @dalf Haven't worked with Transiflex before but will give it a try.. @dalf Do I really have to install the client as described here: https://www.transifex.com/asciimoo/searx/\nOr is it possible to translate in the browser?. @kvch OK, I've already entered the german translation team. If the about page will become translatable I would like to help.. @erdmark That's great!\n@dalf Will the about page as a whole become translatable so that it can be translated into many other languages and displayed in the respective user language?. The provided opensearch.xml of searx.me is different to my instance. Here comes the opensearch.xml of searx.me:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<OpenSearchDescription xmlns=\"http://a9.com/-/spec/opensearch/1.1/\">\n  <ShortName>searx.me</ShortName>\n  <Description>a privacy-respecting, hackable metasearch engine</Description>\n  <InputEncoding>UTF-8</InputEncoding>\n  <Image>https://searx.me/static/themes/oscar/img/favicon.png</Image>\n  <LongName>searx metasearch</LongName>\n    <Url type=\"text/html\" method=\"post\" template=\"https://searx.me/\">\n        <Param name=\"q\" value=\"{searchTerms}\" />\n    </Url>\n</OpenSearchDescription>\nAnd on my instance the opensearch.xml is longer:\n```\nxml version=\"1.0\" encoding=\"utf-8\"?\n\nsearx.site\na privacy-respecting, hackable metasearch engine\nUTF-8\nhttps://searx.site/static/themes/oscar/img/favicon.png\n  searx metasearch\n\n\n\n TODO, POST REQUEST doesn't work \n\n\n\n\n\n```. @vinzruzell If I understand correctly, the translation of searx is done on Transiflex:\n\nhttps://www.transifex.com/asciimoo/searx/. ",
    "bendover22": "It's a long time since this \"bug\" (or ? known issue, maybe?) was filed.\nIf I go to https://searx.me (typed in Firefox address bar), go to Searx preferences page.\nMake a couple changes - mainly, \"results on new tab\" \nthen click SAVE at bottom of new page.\nI copy the URL & parameters just created, at bottom of prefs page.\nPaste the copied URL in Firefox's bookmark toolbar (to use repeatedly).  \nThe Searx URLs created this way always have \"%s\" at end of URL.\nWhen I R-click the new customized searx bookmark in Firefox bookmark toolbar & select \"open in new tab\" (I don't think that's where the issue is), it doesn't just open a tab with the custom URL search parameters, it does a search w/ RESULTS from searching for: \"%s\".  \nThat's not so bad, but wasted time, bandwidth.\nIf I leave \"%s\" in Searx's search box (on the page, not in browser toolbar), it actually searches for \"%sfootball\".\nIf I remove \"%s\" from Searx's search box & a search, any results clicked DON'T open in new tabs.\nBut the same \"open in new tab\" preference saved for say, Duck, Duck, Go's saved bookmark, DO open clicked search results links in a new tab.  Obviously, this can work - it does w/ Ixquick, DDG, etc.\nIf there's already a way around these couple of problems , please put something on the Searx help page & a link on the preferences page to those instructions:  how to save a custom bookmark (w/ pref= \"open in new tab\"), so it opens them in new tabs.  \nBecause going back to previous page, hundreds of times, instead of just clicking closed the most recently visited result page, if it's in a new tab - is quite annoying.\nSeveral other search engines also allow generating & installing  plugins to the browser's Search Engine Manager, using customized search parameters.\nThat is far superior to saving a bookmark, that doesn't seem to work properly.\nside Q: could you add, \"open\" in front of \"results 'IN' new tabs\"?  (shouldn't it be \"in\" a new tab)?. So what's the upshot of that?\nOther than if any site or search engine says they don't log IP addresses, search terms, but actually do, is there a particular problem?\nIf post method is used when opening links from the search page, are any of the customized search preferences forwarded, even though no search terms are forwarded (when post method is used)?\nHere are parts of customized open search plugins for Searx.me and Duck, Duck, Go.\nClearly, the parameters are stored in the plugin.  Are they also stored on Startpage's server?  For what purpose?  \nI didn't create either, so I'm not an expert on exactly what is or isn't forwarded from the search page to the sites, in either case.  Startpage says the preferences can be obfuscated, if desired.  \nI'm not sure of the value of that, unless even Startpage then has no way of understanding them.  But they must be able to.  They are obfuscated, not encrypted.  Maybe obfuscating them is for privacy on the user's own computer, if sharing with others? \n<SearchForm>https://www.startpage.com</SearchForm>\n<os:Url type=\"text/html\" method=\"POST\" template=\"https://startpage.com/do/search\" resultDomain=\"startpage.com\">\n  <os:Param name=\"query\" value=\"{searchTerms}\"/>\n  <os:Param name=\"language\" value=\"english\"/>\n  <os:Param name=\"prfh\" value=\"design_typeEEE0N1Nlang_homepageEEEs/air/eng/N1Nfont_sizeEEEmediumN1Nrecent_results_filterEEE1N1Nlanguage_uiEEEenglishN1Ndisable_open_in_new_windowEEE0N1NsslEEE1N1Ndisable_family_filterEEE1N1Nnum_of_resultsEEE20N1Ngeo_mapEEE1N1Npicture_privacyEEEonN1Ndisable_video_family_filterEEE1N1N\"/>\n</os:Url>\n</SearchPlugin>\n<SearchForm>https://duckduckgo.com/?kax=-1&amp;kp=-2&amp;kn=1&amp;k1=-1&amp;kam=osm&amp;kak=-1&amp;kaq=-1&amp;kap=-1&amp;kae=d&amp;k7=292929</SearchForm>\n<os:Url type=\"text/html\" method=\"GET\" template=\"https://duckduckgo.com/?kax=-1&amp;kp=-2&amp;kn=1&amp;k1=-1&amp;kam=osm&amp;kak=-1&amp;kaq=-1&amp;kap=-1&amp;kae=d&amp;k7=292929&amp;q={searchTerms}\" resultDomain=\"duckduckgo.com\">\n</os:Url>\n<os:Url type=\"application/opensearchdescription+xml\" method=\"GET\" template=\"http://mycroftproject.com/updateos.php/id0/ddg_dark_osm.xml\" rel=\"self\" resultDomain=\"mycroftproject.com\">\n</os:Url>\n</SearchPlugin>\n. @ Pofilo, I appreciate the comment, but I'm not sure you were answering the question I asked - or we're not on the same page?\nYou missed the important phrase, \"Other than...\", meaning, UNLESS a search engine violates their privacy policy (which they probably do NOT)...\nI meant, assuming a search engine isn't lying, and  DO NOT log or \"use\" any personal / pseudo personal data, search terms, etc., (as stated in their Privacy policy, etc.), then when any search engine uses the POST method - where no search data is sent to sites that you click in the search results,... \nwhy does it matter if a search engine's preference settings are stored \"on their server\" (if they are), or are stored in the search engine's (customized) URL?\n@asciimoo Something I don't understand:\n\nStartpage stores your personal settings on their servers. Searx doesn't persist any data, that's why saving user preferences is a bit more problematic.\n\nThe Startpage  prefs are clearly stored in the custom search plugin:\n<os:Url type=\"text/html\" method=\"POST\" template=\"https://startpage.com/do/search\" resultDomain=\"startpage.com\">\n  <os:Param name=\"query\" value=\"{searchTerms}\"/>\n  <os:Param name=\"language\" value=\"english\"/>\n  <os:Param name=\"prfh\" value=\"design_typeEEE0N1Nlang_homepageEEEs/air/eng/N1Nfont_sizeEEEmediumN1Nrecent_results_filterEEE1N1Nlanguage_uiEEEenglishN1Ndisable_open_in_new_windowEEE0N1NsslEEE1N1Ndisable_family_filterEEE1N1Nnum_of_resultsEEE20N1Ngeo_mapEEE1N1Npicture_privacyEEEonN1Ndisable_video_family_filterEEE1N1N\"/>\n</os:Url>\nCould you please explain why, if settings are stored in the plugin, why would Startpage need to store a user's settings on their server?   \nBack to general questions / comments:  Startpage doesn't send any of the Startpage  preferences the user selected to websites that are clicked, do they?\nSame for Searx (I think) - they don't send any user selected preferences for Searx.\nOnce a search is done in Searx using the preferences that are visible in the URL, when you click a search result URL, the Searx \"preference settings\" aren't sent to sites you click - are they?  (My understanding is, \"No.\").\nSo what does it matter whether the Searx prefs are shown in the URL; or with Startpage - (as you said) - the custom preferences are \"stored on their server?\"\nAgain, if the prefs are stored in a user's plugin (not a bookmark or cookie), why would Startpage or any engine also need to store the prefs on their server?  \nEither way, AFAIK, the sites visited don't see those customized settings when you click a search result -if you're using a custom SEARCH PLUGIN such as in Firefox (not a bookmark or cookie) .\nI was talking about two search engines, that BOTH care about users' privacy.  I was only asking why Searx chooses to store custom prefs in their URL, while Startpage doesn't  AND are there any real advantages or disadvantages to either method?\nSorry for the long post, but it seemed my 2nd comment / question was misunderstood.. ",
    "michaelschefczyk": "I have the same issue for more than a month now. I also use a local YaCy instance. You can reach it from the outside at https://yacy.schefczyk.net/. Dear No\u00e9mi V\u00e1nyi,\nyou are most welcome. I would only wish that yacy was able to deliver a higher ratio of meaningful results. The idea here is to pass searx searches to the instance to trigger more targeted searches. Despite all the resources consumed by running yacy, that does not really seem to improve quality all too much.\nRegards,\nMichael Schefczyk. ",
    "animeai": "I would very much like to see this. . ",
    "jugi1": "Thank you!\nPS\nPlease also add SERBIAN language sr-SR.. Where is it ?\nI can't find it.. Done with translation!\nhttps://www.transifex.com/asciimoo/searx/language/sr/\nIt should be:   \u0421\u0440\u043f\u0441\u043a\u0438  (\u0421\u0420\u041f\u0421\u041a\u0418)\nIt would be good if someone else check, correct and maybe propose better translation.. ",
    "aesthicc": "I second this. White burns my retinas at night. DuckDuckGo's appearance customization is wonderful.. Thanks!. ",
    "janit": "There was some traffic that looked malicious so I put it behind a password. In general are there incidents of robots crawling searx instances with terms such as:\n\"Alawar Crack 2017\"\n\"Sayonara Umihara Kawase game\"\n\"Bursa Otelleri 178 Otelde En Uygun Fiyat Neredekalco\"\n\"richi youtube\"\n\"Link Central SandWEB\"\n\"contactscontract.commondatakinds.phone.contact_id\". ",
    "fabionar": "@kvch I'm sorry but that isn't very helpful, what file are you talking about? There's no oscar/result_templates/default.html.. ",
    "pandage-search": "Could this be caused by certain characters in the results? Try \"harry potter\", for example, it fails every single time, but \"harry potter now\" does not.\nOther queries that fail include \"joan rivers\" and \"rome\". Perhaps they have something in common.. ",
    "hook321": "@jibe-b . ",
    "cclauss": "Sure.  manage.sh flake8 would make a lot of sense as long as it comes after flake8 is pip installed.\nWe need to fix the issues raised at https://travis-ci.org/asciimoo/searx/builds/301878446 before this can go live anyway.. ",
    "irule2day": "Google uses IP address to identify your country location and since searx is just a proxy search engine, it will return results based on the location of the searx instance server. In this case searx.me is located in germany, so you will get german related results.\nOne workaround for US based results is to obviously find an instance with US operated server and last time I checked, for this exact purpose, there were 2.\nhttps://search.jpope.org/ (which I use) and https://pigtown.news/searx\nSearx.me is overloaded anyway and it's always best to use an instance which is geographically closes to you for fastest search return. Cheers.. Ah yes I've also noticed they started blocking vpns for searching too with CAPTCHAs. Not a coder or anything, but it doesn't look like there is an easy solution for this.. ",
    "Metaception": "I would like to point out this is not the exact same result as before.\nA example would be searching dm5 on searx.me or one of the other 0.13 sites, only English results.\nHowever, if you search dm5 on search.kujiu.org (hosted in UK) or google.com, you get the expect Chinese results first.\nNot a big deal since most users use English, but I thought I would let you know. Mostly Google's fault to mess with this.. ",
    "MilkManzJourDaddy": "Obviously one would need to hit \u00absave\u00bb to save the cookie.  But preselecting variables would be a huge time saver.  :). Preselect the variables on the ../preferences page, server-side, if possible.  And give the user that pre-filled/selected page.  Obviously the user would still need to hit \u00absave\u00bb to save the cookie. But preselecting variables would be a huge time saver. :). Assuming you have saved the Preferences and currently have a cookie for such, splicing the Search URL with v0.13.x works. You should then have a workable link to pre-select Preferences if the cookie is lost, et cetera, ala: ../preferences?preferences=*.  But, of course, you will need to remove the search query substitution: &q=%s.. To clarify, when you save your preferences, i.e. on searx.me, the resultant Search URL of the currently saved preferences will resemble https://searx.me/?preferences=*&q=%s whereas the Asterisk * wildcard subtitutes for the Preference variable string.\u2008 But what you want to do is have it point to the preferences page with your preferences preselected, and strip off the query search string substitution i.e. ../preferences?preferences=*.  So e.g. for searx.me that would be https://searx.me/preferences?preferences=*.. @devwww , have a look at #1087 as it seems to fit your use-case.. \"@jtflynnz It's simple, really as I highlight in the issue for which this is a duplicate, #1087 [See https://github.com/asciimoo/searx/issues/1087#issuecomment-351580996\n.\u2008 \u21b5\u21b5\u00b6. @jtflynnz It's simple, really as I highlight in the issue for which this is a duplicate, #1087 See this comment.\u2008 . ",
    "Zeph33": "on my instance i fix this bug in file\n/searx/query.py line 55\npython\nraw_query_parts = re.split(r'(\\s+)' if isinstance(self.query,str) else b'(\\s+)', self.query)\nno test on pyhton 2.6  but I think it's more scalable ... PR created  https://github.com/asciimoo/searx/pull/1093. ",
    "muppeth": "Is there a way to solve the captcha issue once it occurs?\nI updated to latest searx (git pull of master) but google still timesout with 503 error. Checking google search with links from the searx host brings me to the warning page with captcha.. @asciimoo thanks. I've disabled google search engine until cooldown.. Hmmm... Got Captcha'ed last evening already but I decided to update to 13.1 and swich off google search for few hours. Once enabled again it worked fine. This mornining noticed we are blocked again :(. @asciimoo yes it is.. @Pofilo You should definately enable access logs and try to follow for a while whats going on. In my case it was a bot sending enormous amount of same type queries for a day or two ( Download mp3 or  Download etc). Then at some point it just stopped (I guess its the same bot jumping form one searx instance to another).\nSetting up filtron (something I havent done yet) will probably save you from this happening in the future.. Disroot admin here. Just did a quick check and seems like searx.me is not affected by it and on disroot it breaks when searching images. I will have a look into it, but if someone knows  right away what the issue may be, feel free to enlighten me :P\nUpdate: \nQuickly checked few other instances and some of them have the same issue. Seems like the ones runniing version 0.15 are free of the issue.. ",
    "devwww": "Hi kvch, thanks for your answer.\nI check it, after saving the preferences, I have not url which could save it.\nI use Framabee instance, maybe it's a different way than Searx.\nThe preferences are at https://framabee.org/preferences\nWhen I save, I return to https://framabee.org, nothing else is mentioned.. @MilkManzJourDaddy \nOk thanks. It seems it not yet supported by Framabee. I'll wait for that :). ",
    "jtflynnz": "Can someone clarify for me the way this works currently? I would be happy to help add a wiki/documentation entry for this process, however I currently seem to be missing something (ie maybe this is still WIP?):\n\n\nI configured one browser with my desired preferences, and pressed save.\n\n\nI re-navigated to Preferences, then I highlighted and copied the URL listed at the bottom of the page\n\n\nI opened an un-configured browser, pasted in this URL a couple different ways to attempt to fetch the new cookies and overwrite the existing.\n\nFirst I pasted raw, unchanged URL from the generated field (complete with &q=%s)\nNext I tried replacing the %s portion with something generic (ie TEST) .\nFinally I removed &q=%s completely, which broght up the main search page.\n\n\n\nIn each case, the default cookies were used, ignoring the preferences called out by the url. \nIs there a step that I am missing? I tried to see if there was anywhere I could paste the URL in settings and press save, but there did not appear to be something here (the URL \"field\" there is for the existing preferences only, right? I cannot simply paste over it with a new set of settings?).\nAny clarifications would be much appreciated.  . ",
    "waldenn": "Ah.. I did not realize it doesn't do its own indexing, so I guess instant-search is not an option.. Yes, for me too. However, I really want to do a fetch() in JS (from a CORS proxy) and am seeing the same issue there as with Wget. Which is why I referenced it (I had not tested with Curl though).\nI would like to understand why Wget is failing and Curl succeeding. Something seems weird when the exact  same GET returns different results. What is the difference here?. Oh it seems to be a character escaping issue. Wget works when you escape the characters. Thanks! Closing this :). ",
    "svofusihan": "Okay, right. But on Firefox, or even Edge, there is no need to click on the search button, the page is automatically reloaded and the results updated.. Default settings. So Oscar theme.\nInstance searx.me. But also tried others, same behaviour.\nDon't you reproduce it?. ",
    "TT-SWP13": "Thank you very much! I will try it on an several system.. ",
    "joshu9h": "Thanks asciimoo with quotes it's working\nnow i also see the morty hash link\nI see how it is working\nThis issue can be closed. Uwsgi log gives a HTTP/1.1 302\nuwsgi log\nhttps://pastebin.com/0AF5xGCK. If i only do a git pull it's working\nwhen i do after  a git stash apply i get the error 502 bad gateway\nhow does this work (dig)\ngit status showed me my problem\ni. I thought i found it but still the same problem bad gateway 502\nasciimoo where do i put\nhttp {\n  proxy_buffer_size   128k;\n  proxy_buffers   4 256k;\n  proxy_busy_buffers_size   256k;\n}\nI tried nginx.conf no result\n upstream sent too big header while reading response header from upstream,. Same here no luck also updated nginx from1.10 to 1.12\nthe extra nginx error debug \ngives\n2017/12/09 23:03:38 [debug] 1078#1078: 1 SSL_read: -1\n2017/12/09 23:03:38 [debug] 1078#1078: 1 SSL_get_error: 2\nsi i update to version 1.12 but the same\n. [enh] add dark version of oscar/logicodev \ngit checkout 46fb0d860e35a45658969c4e2ac306a1072bc331\nWorks fine and\n[enh] make custom oscar option configurable from url \ngit checkout e060aedc16bb2b9e5c1ee3fc69a0e07a3576a80c\ngives bad gateway 502\n. Did some testing updated to the latest master \nif i hash the following in preferences.py\nline 308 #else:\nline 309 #self.unknown_params[user_setting_name] = user_setting\nIt works no errors \nThe only thing the oscar Logicodev dark doesn't work \nHope this will make some understanding about the 502 bad gateway problem. for startpage i see it send the request and receive the response with the results but it doesn't show the \nfinal results it looks like the xpath variable is not correct but i don't know the correct one . It''s working fine here!. When you update searx to version 0,15 startpage is working again!. ",
    "HuKaers": "Since 6dec i do have the same problem here (uwsgi+nginx). Fresh install doesnt help.\nHow did you resolve this?. ",
    "mirabellette": "Hello kvch,\nThank you for your answer, I understand it.\nI check the website https://searxes.danwin1210.me/, it is very interesting but it not what I was thinking about.\nI will parse the wiki page and develop the tool. In fact, I think my little tool could work with a lot of sources. I just have to create an adapter to searx instance. Do you know where can I find the code source behind http://stats.searx.oe5tpo.com/ because I would like to reused some of there features.\nI will published a reply here when it will be available.. ",
    "fitojb": "Can you send your changes through https://www.transifex.com/asciimoo/searx/ instead?. ",
    "Jul10l1r4": "Yes. ",
    "scarejar": "My searx instances still have this problem. Is this something they'll have to manually do? Or is it an upcoming update that will fix it automatically ? . ",
    "genericsauce": "Seems like googles switched something around again, updated to latest and still complaining about captcha. @dalf I just tried this, a google search doesn't prompt a captcha but still my searx instance does. . ",
    "qutetemp": "There are a lot of instances which don't have google working. Might I suggest releasing a new version after something like this is fixed in order to ease up searx.me ?. ",
    "kettbi": "Hi,\nThe default log files directory is /var/log/uwsgi/app/\n. + I'm running a dedicated server from OVH\n+ It's a public instance (about 20 users realy use it)\n+ some other services are running on this server (apache with about 10-15 webservices : nextcloud, kanboard, tt-rss, humhub, ...), gitlab, openfire, ... \n. ",
    "BorealCoder": "@kvch Very nice fix - thank you!. ",
    "JosephKiranBabu": "@kvch Sorry, I should have posted the error log.\nI've installed Searx 0.13.1+dfsg1 from Debian (unstable). The Python version is 3.6.4~rc1-2 which is also a Debian package.\nAfter installation, I copied the settings.yml from examples into /etc/searx/ and then ran SearX using /usr/bin/searx-run.\nroot@freedombox:~# /usr/bin/searx-run \nTraceback (most recent call last):\n  File \"/usr/bin/searx-run\", line 11, in <module>\n    load_entry_point('searx==0.13.1', 'console_scripts', 'searx-run')()\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 572, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2752, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2405, in load\n    return self.resolve()\n  File \"/usr/lib/python3/dist-packages/pkg_resources/__init__.py\", line 2411, in resolve\n    module = __import__(self.module_name, fromlist=['__name__'], level=0)\n  File \"/usr/lib/python3/dist-packages/searx/__init__.py\", line 54, in <module>\n    settings = load(settings_yaml)\n  File \"/usr/lib/python3/dist-packages/yaml/__init__.py\", line 72, in load\n    return loader.get_single_data()\n  File \"/usr/lib/python3/dist-packages/yaml/constructor.py\", line 35, in get_single_data\n    node = self.get_single_node()\n  File \"/usr/lib/python3/dist-packages/yaml/composer.py\", line 36, in get_single_node\n    document = self.compose_document()\n  File \"/usr/lib/python3/dist-packages/yaml/composer.py\", line 55, in compose_document\n    node = self.compose_node(None, None)\n  File \"/usr/lib/python3/dist-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/usr/lib/python3/dist-packages/yaml/composer.py\", line 133, in compose_mapping_node\n    item_value = self.compose_node(node, item_key)\n  File \"/usr/lib/python3/dist-packages/yaml/composer.py\", line 82, in compose_node\n    node = self.compose_sequence_node(anchor)\n  File \"/usr/lib/python3/dist-packages/yaml/composer.py\", line 111, in compose_sequence_node\n    node.value.append(self.compose_node(node, index))\n  File \"/usr/lib/python3/dist-packages/yaml/composer.py\", line 84, in compose_node\n    node = self.compose_mapping_node(anchor)\n  File \"/usr/lib/python3/dist-packages/yaml/composer.py\", line 127, in compose_mapping_node\n    while not self.check_event(MappingEndEvent):\n  File \"/usr/lib/python3/dist-packages/yaml/parser.py\", line 98, in check_event\n    self.current_event = self.state()\n  File \"/usr/lib/python3/dist-packages/yaml/parser.py\", line 428, in parse_block_mapping_key\n    if self.check_token(KeyToken):\n  File \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 116, in check_token\n    self.fetch_more_tokens()\n  File \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 156, in fetch_more_tokens\n    self.scan_to_next_token()\n  File \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 777, in scan_to_next_token\n    self.forward()\n  File \"/usr/lib/python3/dist-packages/yaml/reader.py\", line 101, in forward\n    self.update(length+1)\n  File \"/usr/lib/python3/dist-packages/yaml/reader.py\", line 153, in update\n    self.update_raw()\n  File \"/usr/lib/python3/dist-packages/yaml/reader.py\", line 178, in update_raw\n    data = self.stream.read(size)\n  File \"/usr/lib/python3.6/encodings/ascii.py\", line 26, in decode\n    return codecs.ascii_decode(input, self.errors)[0]\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xd8 in position 2584: ordinal not in range(128). If the Unicode characters are removed from the settings.yml file then startup fails again - with the same error - as data/engines_languages.json has Unicode characters too.\nIf my fix is not comprehensive, kindly consider making changes in any other files where this might be a problem.. @kvch This is the specific section in settings.yml that has Unicode characters.\nyml\nlocales:\n    en : English\n    ar : \u0627\u0644\u0639\u064e\u0631\u064e\u0628\u0650\u064a\u064e\u0651\u0629 (Arabic)\n    bg : \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 (Bulgarian)\n    cs : \u010ce\u0161tina (Czech)\n    da : Dansk (Danish)\n    de : Deutsch (German)\n    el_GR : \u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac (Greek_Greece)\n    eo : Esperanto (Esperanto)\n    es : Espa\u00f1ol (Spanish)\n    fi : Suomi (Finnish)\n    fr : Fran\u00e7ais (French)\n    he : \u05e2\u05d1\u05e8\u05d9\u05ea (Hebrew)\n    hr : Hrvatski (Croatian)\n    hu : Magyar (Hungarian)\n    it : Italiano (Italian)\n    ja : \u65e5\u672c\u8a9e (Japanese)\n    nl : Nederlands (Dutch)\n    pt : Portugu\u00eas (Portuguese)\n    pt_BR : Portugu\u00eas (Portuguese_Brazil)\n    ro : Rom\u00e2n\u0103 (Romanian)\n    ru : \u0420\u0443\u0441\u0441\u043a\u0438\u0439 (Russian)\n    sk : Sloven\u010dina (Slovak)\n    sl : Slovenski (Slovene)\n    sr : \u0441\u0440\u043f\u0441\u043a\u0438 (Serbian)\n    sv : Svenska (Swedish)\n    tr : T\u00fcrk\u00e7e (Turkish)\n    uk : \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430 (Ukrainian)\n    zh : \u4e2d\u6587 (Chinese)\nRemoving everything except en : English fixes the problem.\nThese language strings are also present in data/engines_languages.json. I have handled similar issues in two more files - engines_languages.json and currencies.json\nThis fixes the Unicode character problems in all the files now.. @josch \nI am trying to run searx on this box.\nhttps://app.vagrantup.com/freedombox/boxes/plinth-dev\nI noticed that searx failed to run when I was logged in as root user, but works with vagrant user. It was because the locale settings were different. The vagrant user has utf8 in their locale but not the root user.\nThis issue can be reproduced in the following cases:\n$ LC_ALL=C python3 -c 'import searx'\n$ LC_ALL=POSIX python3 -c 'import searx'\nMaybe the CI server supports utf8 in its locale. \nIn short, searx doesn't work when the default locale on the server doesn't support Unicode.\nThis pull request makes the application independent of the locale settings of the system on which it is run. . Closed the bug on Debian BTS.\nhttps://bugs.debian.org/886093\n. @asciimoo Can we have a patch release with this fix included? Searx is still unusable for my use case.\nIf a release is difficult, can @josch  please make a new patch release for the Debian package?. Thanks for making the release as promised. Eagerly waiting for the release of the Debian package.. ",
    "SunilMohanAdapa": "I believe this patch is correct. My reasoning follows:\nLocales:\n\n\nLocale is a way to tell a program how to interact with the user, that is,\n  which currency to use, how to format a date and which language and character\n  encoding to use. This applies when showing messages, dates, currencies etc.\n  and when accepting input from the user. It does not dictate how a program\n  should do internal processing of strings and configuration parsing. These\n  files being read here are internal workings of the program, especially in case\n  of currencies and engine configuration. They are not user interaction.\n\n\nSearx has chosen to store the files in question as UTF-8 not as Ascii,\n  UTF-16 or UTF-32. Both JSON and YAML allow for UTF-8, UTF-16 and UTF-32[1][2].\n  As a consequence, Searx will have to read them as UTF-8 irrespective of how\n  it is interacting with the user (or which locale is set). Setting a locale\n  does not mean that Searx should read its own internal files in that locale.\n  Most programs operate internally in UTF-8 and interact with users in various\n  locales.\n\n\nFor a web program like Searx, the locale set in the environment is of much\n  less use. It has to interact with users of multiple locales at once. Locale is\n  specified on a per-request basis via HTTP header or via preference set in the\n  session. This is unlike the command line tools and GUI programs that interact\n  with a single user at a time and rely on locale set in the environment to read\n  user preference. Further, choosing to read an internal file as UTF-8 does not\n  mean Searx will not be serving users of different locales.\n\n\nI don't believe setting a locale to something other than UTF-8 means that\n  system has no Unicode support. What does having 'system having unicode\n  support' mean anyway in this context? We have programming language, its\n  libraries, the HTTP protocol and i18n/l10n library all supporting Unicode\n  unconditionally.\n\n\nChanging the system default locale:\n\n\nI just installed Debian from a latest testing ISO and during installation, I\n  selected 'No localization' selected instead of the default 'English'. I\n  installed 'searx' package and python3 -c \"import searx\" failed. The locale\n  in this case is 'C' and not 'C.utf8'. We will have go and update Debian's (and\n  all other such distributions') default locale to fix this problem.\n\n\nSay there is another daemon in the system that has its configuration files\n  encoded in UTF-16 (for some reason). It would not be possible to set the\n  default locale of the system to UTF-8 (for the sake of Searx) and UTF-16 (for\n  the sake of the other program) at the same time. This means that Searx will\n  have to be passed its locale separately during start. In this case, it is best\n  done in startup files (.service file or /etc/init.d scripts). Should there be\n  a provision such that administrators can set Searx's locale separately by\n  editing startup configuration?\n\n\nDifferent users on the system have different locales set in their environment.\n  System's default locale is simply a way to set, by default, what users are\n  most like to choose. On Debian, of the 492 locales, 189 were not using UTF-8\n  as character encoding. People use different default locales for great many\n  reasons. I don't think it is intended that Searx should catastrophically fail\n  in all of those cases, especially when it need not.\n\n\nPython's behavior:\n\nPlease note the following change in Python's behavior from Python 2 to\n  Python 3.\n\n$ LC_ALL=en_US.UTF-8 python2 -c \"open('hello.txt', 'r').read()\"\n$ LC_ALL=C python2 -c \"open('hello.txt', 'r').read())\"\n$ LC_ALL=en_US.UTF-8 python3 -c \"open('hello.txt', 'r').read()\"\n$ LC_ALL=C python3 -c \"open('hello.txt', 'r').read()\"\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/usr/lib/python3.5/encodings/ascii.py\", line 26, in decode\n    return codecs.ascii_decode(input, self.errors)[0]\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe0 in position 0: ordinal not in range(128)\n\nWhen opening a file in Python 3, the file encoding is assumed from the current\n  locale. So, the only correct way to read a known utf-8 file in all locales is\n  to explicitly specify its encoding as utf-8.\n\nAlternatives:\n\n\nThe suggestion to use codecs is also correct. Python seems to use this library\n  internally doing various kinds of string decoding.\n\n\nYet another way of doing this is open(file_name, encoding='utf-8').\n\n\nAll three solutions are roughly equivalent. In case of opening in binary,\n  UTF-16 and UTF-32 encoded files are also accepted as supported by JSON and\n  YAML specifications and their corresponding libraries.\n\n\nLinks:\n1) https://tools.ietf.org/html/rfc7159#page-9\n2) http://www.yaml.org/spec/1.2/spec.html#id2771184\n. @JosephKiranBabu @josch @asciimoo Thank you all very much.\nLooking forward to having Searx in FreedomBox real soon :). @josch, If we open as binary, we will be supporting all codecs valid under the YAML/JSON specifications (UTF-8, UTF-16, UTF-32). If an administrator creates a /etc/searx/settings.yml with UTF-16 encoding searx should not fail to load it as it is a valid YAML file.\nWhen given binary data, JSON and YAML parsers interpret them with the correct encoding automatically.\nConsider the following where binary works better than 'utf-8':\n```python\n\n\n\nimport yaml\nopen('/tmp/test.yml', 'w', encoding='utf-16').write('a: b')\n4\ncontent = open('/tmp/test.yml', encoding='utf-8').read()\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python3.5/codecs.py\", line 321, in decode\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\ncontent = open('/tmp/test.yml', 'rb').read()\ncontent\nb'\\xff\\xfea\\x00:\\x00 \\x00b\\x00'\nyaml.load(content)\n{'a': 'b'}\n```. \n\n\n",
    "funkyfuture": "maybe i'm missing some point, but i suggest to consider to use make.. Duplicate  of #1026.. ",
    "coaxial": "FWIW, I cleared my cache in Firefox and it went away. I couldn't reproduce using private mode so I figured it could be cached CSS maybe.. Clearing my cache fixed it. Thanks!. ",
    "yassineim": "Well no I don't get the website with other queries (unless it's just the problematic query with changed orders of words).\nI even tried with searx.me on Tor Browser and I still get that.\n-------- Original Message --------\nOn 27 Dec 2017, 21:12, No\u00e9mi V\u00e1nyi wrote:\n\nDoes it show up for other search querys, too? I haven't been able to reproduce the issue with several instances and searching for a few times.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.. \n",
    "hiragashi": "I get it here too - scared the bejeezus out of me. it's an old site, a bit like a rick roll. There are no site meta tags or anything. It's gotta be someone at Bing frigging around. ",
    "Silent-Hunter": "Appears to be fixed, I tried the same search string as yassinei and that site wasn't in the results.. ",
    "gr01d": "I tried it with my own instance and I got it but not on the first result.. I've removed this\n```\nsearx start\n\n    Options FollowSymLinks Indexes\n    SetHandler uwsgi-handler\n    uWSGISocket /run/uwsgi/app/searx/socket\n\n``\nand only addedProxyPass \"/searx\" \"http://127.0.0.1:4004/\"`\nbut the User interface of searx now has missing assets it is like a html only no images and  whenever I input at the search box it just redirect to my root url: http://mysite:port/ not going through http://mysite:port/srx\nI can access from the localhost that searx is running the curl 127.0.0.1:4004 I will try to change my searx port to somewhere other than the subdirectory install later.\nUPDATE: So I changed/added another port for searx in another apache2 sites file and it worked (I can access the searx filtron protected using another port in my server https://mysite:anotherport). Is there any configuration that will allow to run fitron on subdirectory install of searx (eg. https://mysite:port/searx/ and not on https://mysite:port). Filipino has 8 major dialects:  Bikol, Cebuano, Hiligaynon (Ilonggo), Ilocano, Kapampangan, Pangasinan, Tagalog, and Waray \nplease specify which dialect in the transifex. Tagalog is widely being taught and english. . @kvch please add cebuano to the translation (transifex), and change filipino (transifex) to tagalog. @vinzruzell  Tagalog is available here: https://www.transifex.com/asciimoo/searx/language/tl/\nFilipino is also available: https://www.transifex.com/asciimoo/searx/language/fil/\nYou can login to transifex using your github account. have you tried visiting the searx instance in your own server, (eg. using links/lynx/elinks visit the  127.0.0.1 port 8888 if you can see the searx interface then uwsgi searx is running if not then it is not).\ndouble check your searx configurations. uwsgi should be in  active (running) . 0.14.0 I confirm that: Engines cannot retrieve results:\ngoogle images (unexpected crash: No JSON object could be decoded) . You can run in on a Raspberry Pi. https://github.com/asciimoo/searx/issues/1338. ",
    "s8321414": "No Chinese(Taiwan) locales on Transifex, and I can't even add it on Transifex \ud83d\ude2e . Done \ud83d\ude03 . ",
    "apropostech": "Works fine for me in Firefox 58 in both URL and separate search bar.\nWhich search plugin do you use? Here's a source for some plugins, one of which I use (believe it's the one \"verified\" to work since the others are all new): http://mycroftproject.com/search-engines.html?name=searx.me\nIf I remember correctly, it's also necessary to set method to POST in the searx general settings.. ",
    "capiscuas": "We installed a recent instance of searx and one of the first petitions from users was that they couldn't copy paste the search request from the url, it is true that shows up in a right box like here:\n\nbut would be great if it can change the url everytime performing a search. Is there any admin preferences to make this happen or it hasn't been implemented yet?. ",
    "Quenty31": "I was talking about the CLDR sorry, it's in there.. I hope we find a solution :) . Now the PR is merged, when does searx will speak Occitan? :D . @asciimoo & @akx Have you got any news about this?. ",
    "akx": "Heya, I was led here by @Quenty31 's comment on Babel.\nI take it this is an i18n thing, and Babel is grumpy when it can't find a CLDR locale for the language when trying to manipulate a message catalog?. ",
    "lord-alfred": "https://www.themoviedb.org/ is better (API support). ",
    "RedSoxFan04": "asciimoo, I don't know why, but that doensn't work when I try it.. I don't know what the OP is doing differently, but it works fine for me in Firefox 58 on Mac OS Sierra.. ",
    "shumvgolove": "Yes, I'm sorry. Originally did not know about ! Only after reading the documentation I understood what was the point.. ",
    "dessalines": "I closed the previous issue and am posting my comment here since its the same idea as @thiswillbeyourgithub \nInstead of bangs searching the metasearch engines, it would be wonderful if bangs were redirects to use that sites own search engine. Duckduckgo has ~12k bangs... this is something that wouldn't be feasible or necessary for a metasearchengine.\nYou could simply added a json file that looks something like:\nbangs: [\n{\n  name: \"amazon\",\n  shortcuts : [\"a\", \"amazon\"],\n  url: \"https://www.amazon.com/s/?url=search-alias%3Daps&field-keywords=%s\"\n  ...\n},\nI'm switching from duckduckgo, and there are many bangs that I can no longer use now: !idope or !tpb for torrents, !w for wikipedia, !yt for youtube, !a for amazon, !gh for github...\nThe nice thing is that users could just add new urls to this json file in a clean way. \nedit: Also the list of bangs should all be in one place, right now its spread across many, I had no idea !stackoverflow worked until I randomly tried it. . This was really hard to find. . There are also 42 open pull requests.. #1146 . I'm going to close this and post my comments there. . > Getting a redirect to a site means that we don't get search results from that search engine\nWhat? Completely wrong. Try this search, and you'll find that yes, it does give you search results from that engine: https://duckduckgo.com/?q=!so+best+sorting+algorithm\n. ",
    "aleferov": "English is not my native language(((\nI installed in a subfolder Searx (domain.com/searx). And now I want to Searx worked at domain.com .\n. Thank very much!. I do not understand what needs to be done. Help me please. In which file do I need to change the code?. ",
    "lutzhorn": "This depends on your web server. Take a look at https://asciimoo.github.io/searx/dev/install/installation.html#web-server. > Is it possible via commande API to do a search and send all search in a\n\njson files.\n\nYes, use -F to specify the query and the format:\n$ curl -F q=Searx -F format=json https://www.lhorn.de/searx/\n. Am 20.01.18 um 23:05 schrieb astrowalker2013:\n\nbut the query display are 18 url find\nIs it possible to catch all result in one json.\n\nI don't know how to do this but\ncurl -F q=Searx -F pageno=2 -F format=json https://www.lhorn.de/searx/\n\nwill give you the second page.\n. Firefox recognizes searx as a possible search engine. The looking glass icon in the Firefox search box has a small plus icon which allows you to add searx as a search engine. Then navigate to about:preferences#search to make searx your default search engine.. More general: Goto https://the-searx-instance-you-use/about, scroll down, follow the directions in the \"How to add to firefox?\" section.. Do you meen 'steem' or 'steam'? Could you provide an URL to this search engine?. Do you mean this search? https://steemit.com/static/search.html?q=Foo. ",
    "IzzySoft": "@asciimo could that page linked as e.g. \"help\" from the search page itself? Like, next to \" about | privacy | preferences | help \"? That way it would be easy to find. Took me some hours to dig this up here, as even \"searxing\" for it gave no results.\nWhat I also miss is what other \"keywords\" can be used. Some can be guessed and tried when known from other search engines (like site:example.com), but for the \"average user\" it's hard. And I have the feeling something special might be lurking \"under the hood\" which I will forever miss unless being told about :wink:. ",
    "HongliYu": "Check the branch named fix-manage-sh @songproducer . ",
    "astrowalker2013": "Thanks \nit's OK however the json doesn't contain all result in  my serverer \nthe returne json give \nnumber_of_results | 19309\nbut the query display are 18 url find\nIs it possible to catch all result in one json.\nI see that it possible to retrive page by page with\n'-F pageno=\nfor the moment my only solution is to do a boocle with the 10 first page and concatenate the json.\nis it only solution ?\n. Thanks a lot \nnow I have other question to add in my searx instance , tor search ?\ni see that in engine /repetory all search instance, but I don't know how to create like the instance http://jiwfbtg2kfkadjku.onion an onion category.\nBest . ",
    "ShellCode33": "up ? :disappointed: This is a 10 seconds fix, I use your server everyday, love your work, this small improvement would be very appreciated.... All you have to do is to use GET method instead of POST (you can change that in the preferences of searx). I personally had to remove searx from Firefox's search engines and add it again, but it worked like a charm !. ",
    "usernameisntallowed": "I made an attempt on this if anybody want's to check it over https://github.com/asciimoo/searx/pull/1309 , thank you.. ",
    "DeadNumbers": "Go https://searx.me/about and scroll page down. ",
    "zer09": "Ohh thanks.. ",
    "Nickfost": "Steem Blockchain \nsteemit.com\nsteemd.com. https://www.asksteem.com/. \n. ",
    "IceGiant": "Will do.  I made the fix on my Windows box, so because I haven't had any luck getting the shell script to run in that environment, I'll see if I can get it working on my Mac.. ",
    "lonnieOST": "Thanks and sorry for the non-issue post.\nAlso, please close my follow-on post about the cluster DB question as well and I will send it over to the mailing list.\nI will see about joining the mailing list.\nThanks again,\nLonnie. I see.\nSo then who might I be able to ask about the DB usage within SearX?\nI am trying to find out if it clears its internal DB before each query or does it cache the query results from the external search engines and try to use them again if a query calls for some of these results?\nI'm not trying to change SearX at the moment, but am just trying to find out how the database is used and if queries are cached for later re-use. The documentation is not clear on this question.\nThanks again,\nLonnie. Will send to mailing list for further inquiry.\nThanks,\nLonnie. I see.   Now I seem to be getting results coming in.  I never was able to answer the CAPTCHA as it was the message from SearX (master latest build, v0.13.1 is what I think is the latest) when it could not get results from Google recently.\nThanks for the information.. solved.. Hello,\nNot sure completely on the solution, but I upgraded to the latest SearX 0.14 and it stopped showing.\nIt may reappear as I think that it is on the Google side and if so then I will look for a better solution for which I will post.\nCheers and have a good day.. Thanks for responding to my inquiry.\nCurrently, I am looking to install the first engine and if all goes well, I want to try and add more. I am coming more from a C/C++ background than Python which I do not know too well, but am going to try to add this one. \nOn, a second note, I am also needing to learn more about the SearX plugin system as I seem to understand that it has hooks for before and after the call with the results list. I want to be able to do an action after the query with the results collected and send them to another program as well as back to the user.\nI might like to ask for you help if I get stuck on some of this.\nThanks again.. Yea, I was looking at that documentation already as I also need to make a\nplugin that will use the Post_Search or On_Result to send all of the URL\nresults to another URL via an API call.\nI have not started on this yet, but I will effectively be collecting all of\nthe results each time and basically filling in a type of database.\nthanks\nOn Fri, Feb 16, 2018 at 2:17 PM, No\u00e9mi V\u00e1nyi notifications@github.com\nwrote:\n\nThere is laconic documentation on plugins: https://asciimoo.github.io/\nsearx/dev/plugins.html\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1196#issuecomment-366332422,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHutwgfEcTDbSAejxyv63Bnn-mUULks5tVdRlgaJpZM4SHrcA\n.\n. I have partially figured this out and am working on adding more engines.. Will look that up, thanks\n\nOn Fri, Feb 16, 2018 at 2:13 PM, No\u00e9mi V\u00e1nyi notifications@github.com\nwrote:\n\nThe templating language used by searx is Jinja2. It dumping its variables\ndoesn't seem straightforward.\nYou could try adding print(kwargs) to this line:\nhttps://github.com/asciimoo/searx/blob/master/searx/webapp.py#L390\nIt would print all variables passed to render a page.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1197#issuecomment-366331214,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHn1C4jQlChWQW3FNUQHv4GPy6Y-3ks5tVdNMgaJpZM4SIIRV\n.\n. Writing the plugin myself and its coming along great.. I have found a solution to this. Thanks.. So, I've been hacking a bit into this but still have no answer.\n\nI can put a \"print(search)\" inside my plugin function:\n\ndef on_result(request, search, result)\n      print(search)\n      return True\n\nbut it just returns an object address:\n\nBut I cannot find a way to have Python print out the fields within the dictionary as I am sure that perhaps of the \"request\", \"search\", or \"result\" objects, that one of them have the active category that SearX is using.\nCan you please tell me how to list the fields or some other way to get the current category when the URL arrives into the on_result() function?\nThis is my one hold up on the plugin to see if it works.\nAny ideas?\nCheer and Thanks in advance.\n. Morning All,\nI have had a small bit of success but am not completely there yet.\nWhat I found was that if, in my \"on_result\" function, if I do \"print(dict(result))\" then I get an example array returned for searching for \"cars\" in the images catagory.\n{'engine': 'google images', 'category': 'images', 'engines': set(['google images']), 'title': u'The Coolest, Most Expensive, or Rare Cars Photos - ABC News', 'url': u'http://abcnews.go.com/Business/photos/photos-coolest-expensive-rare-cars-17868294', 'positions': [100], 'parsed_url': ParseResult(scheme=u'http', netloc=u'abcnews.go.com', path=u'/Business/photos/photos-coolest-expensive-rare-cars-17868294', params='', query='', fragment=''), 'content': u'', 'score': 0.01, 'img_src': u'http://a.abcnews.com/images/Business/ht_batmobile_ll_131114_ssh.jpg', 'template': 'images.html', 'thumbnail_src': u'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS_Y5HysmBnH2keyPETLOHTRAgiX0au5Uis9MB0mEOT2nEKUB9w'}\nI can clearly see that the 2nd element is what I am looking for 'category' : 'images'\nNow I just need to learn the proper Python way to access that element as I am coming from a C/C++ background.\nAny ideas?\nCheers and have a great day. Ok All,\nJust found the answer:\nI use the \"get()\" and it works!!!!!\nprint(dict(result))\nprint(result.get('category'))    # returns the query category\nThe second one gives me what I want which is one of \"general, files, images, it, map, music, news, science, social media, and videos\"\nsuccess!!!\nI can close this ticket now. Maybe this will help someone else as well.\nCheer and have a GREAT day!!!. I forgot to mention that the rationale for this wanting to rename an existing category is because I would like to work on a template now and do not want to use the category name \"IT\". \nAlso, I was thinking to rename some of the other categories as well.\nOne other thing that I noticed was that, in the searx/templates/oscar/categories.html (for example)\n\"\n{% if rtl %}\n    {% for category in categories | reverse %}\n        \n{{ (category) }}\n    {% endfor %}\n{% else %}\n    {% for category in categories %}\n        \n{{ (category) }}\n    {% endfor %}\n{% endif %}\n\"\nIt \"seems\" that the categories are not explicitly laid out, but are generated from the script language variable:\n{% for category in categories %}\nI'm still looking into how the templates work and trying to  get a feel for the flow, but this is what I have found out so far.\nCheer and have a great day\n. Also, I do not know why when I post to the SearX issues that it bolds and makes larger my information as this is not meant to yell or anything. I just do not know why it does that as I do not put in any special characters.\nSorry if it offends and I'll try to find a better way to post the information.\n. Hi,\nI may have narrowed down the problem a bit.  Actually the outstep_sender.py is a small plugin that I have been trying to develop for SearX that would use the \"on_results\" function.\nWhat I wanted to do was to send each external URL that was collected out to another program via a small API call, but I think that the error is showing up because the the outstep_sender.ph is slowing down the system. I had noticed significant timeouts when trying to run it with the plugin. Hi Alexandre,\nLet me first say that I really like what I have seen with SearX and am\ntrying to make it a part of my project which is effectively a Hybrid search\nengine that has features of a Metasearch Engine (SearX frontend) and a\ncentral search engine (backend).\nWith that in mind, I want to collect all of the returned results that are\naggregated and displayed for the user from the query, and in a separate\nthread queue them (URL) up asynchronously to send to the backend server via\nan API call. based the SearX category that the URL is coming from within\nSearX, and hopefully not slow down the process.\nI implemented a small plugin \"outstep_sender.py\" as a test, but fear that\nthe on_result was slowing things down as I had mentioned and will be happy\nto see in the possible solutions that you gave may be helpful as a\ndedicated thread is probably what I need and will just have to figure out\nhow to do it as I am coming from a C/C++ background and am still trying to\nget a handle on Python so any help that you could offer would be greatly\nappreciated.\nThanks again and have a great weekend :)\nOn Sat, Mar 3, 2018 at 8:15 AM, Alexandre Flament notifications@github.com\nwrote:\n\nYes, there is a global timeout on each request: if a plugin is slow, there\nwill be more timeout or no result at all.\nThe solution is my opinion is to process the results asynchronously :\n\neither with a dedicated thread, and Queue\n   https://docs.python.org/2/library/queue.html.\neither using Celery http://www.celeryproject.org/\n\nBe careful, in production there will be more than one thread processing\nthe searx requests (see uwsgi configuration).\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1224#issuecomment-370146747,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHpaAFpXnhmXNkTqU_Uy7Ql1ExaUOks5tapdbgaJpZM4SaMzW\n.\n. You know, I was just thinking about something.\n\nMaybe the \"on_result\" function is not the proper place to be doing this\nfunction since I would guess that once the results have been aggregated\nthen there is a \"list\", which is normally send back to the user and\ndisplayed. I am wondering if perhaps that list could be sent to a thread in\nthe \"post_search\" hook. Then the thread could send out the URL's via the\nAPI call as it process the inbound queue of URL's.\nThis might be more skill with SearX and Python than I can do an may have to\ntry and find someone that might be able to help or get a developer for it.\nStill trying to figure out the best and quickest way to move forward but I\nthank you again.\nOn Sat, Mar 3, 2018 at 9:21 AM, Lonnie Cumberland lonnie@outstep.com\nwrote:\n\nHi Alexandre,\nLet me first say that I really like what I have seen with SearX and am\ntrying to make it a part of my project which is effectively a Hybrid search\nengine that has features of a Metasearch Engine (SearX frontend) and a\ncentral search engine (backend).\nWith that in mind, I want to collect all of the returned results that are\naggregated and displayed for the user from the query, and in a separate\nthread queue them (URL) up asynchronously to send to the backend server via\nan API call. based the SearX category that the URL is coming from within\nSearX, and hopefully not slow down the process.\nI implemented a small plugin \"outstep_sender.py\" as a test, but fear that\nthe on_result was slowing things down as I had mentioned and will be happy\nto see in the possible solutions that you gave may be helpful as a\ndedicated thread is probably what I need and will just have to figure out\nhow to do it as I am coming from a C/C++ background and am still trying to\nget a handle on Python so any help that you could offer would be greatly\nappreciated.\nThanks again and have a great weekend :)\nOn Sat, Mar 3, 2018 at 8:15 AM, Alexandre Flament \nnotifications@github.com wrote:\n\nYes, there is a global timeout on each request: if a plugin is slow,\nthere will be more timeout or no result at all.\nThe solution is my opinion is to process the results asynchronously :\n\neither with a dedicated thread, and Queue\n   https://docs.python.org/2/library/queue.html.\neither using Celery http://www.celeryproject.org/\n\nBe careful, in production there will be more than one thread processing\nthe searx requests (see uwsgi configuration).\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1224#issuecomment-370146747,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHpaAFpXnhmXNkTqU_Uy7Ql1ExaUOks5tapdbgaJpZM4SaMzW\n.\n\n\n. Thanks again for all of the help and I will try to use the above code to\nre-write my plugin to see if it will perform better.\n\nGreat project!!!\nOn Sat, Mar 3, 2018 at 9:54 AM, Adam Tauber notifications@github.com\nwrote:\n\nClosed #1224 https://github.com/asciimoo/searx/issues/1224.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1224#event-1502544877, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHryV2sdTPzDBeB0LzGZkO-Ob-IJOks5taq6rgaJpZM4SaMzW\n.\n. Thanks for the information and this is a real help for my target goals.\n\nIn looking things over it seems that I would still need to implement something like Python Flask to answer the API calls and add a bit for a JSON return.\nI would run this as a completely independent server and just send a query along with a category and return format (of course there might be some pagination stuff to be added)\nWould this be about right?\nThanks again :). Hi Again,\nI actually just tried to test it from the searx directory:\n~/searx/builds/searx$ python utils/standalone_searx.py --category general spacex\nand got this error:\nTraceback (most recent call last):\n  File \"utils/standalone_searx.py\", line 103, in <module>\n    sys.stdout.write(dumps(result_container_json, sort_keys=True, indent=4, ensure_ascii=False, encoding=\"utf-8\", default=json_serial))\n  File \"/usr/lib/python2.7/json/__init__.py\", line 251, in dumps\n    sort_keys=sort_keys, **kw).encode(obj)\n  File \"/usr/lib/python2.7/json/encoder.py\", line 209, in encode\n    chunks = list(chunks)\n  File \"/usr/lib/python2.7/json/encoder.py\", line 434, in _iterencode\n    for chunk in _iterencode_dict(o, _current_indent_level):\n  File \"/usr/lib/python2.7/json/encoder.py\", line 408, in _iterencode_dict\n    for chunk in chunks:\n  File \"/usr/lib/python2.7/json/encoder.py\", line 332, in _iterencode_list\n    for chunk in chunks:\n  File \"/usr/lib/python2.7/json/encoder.py\", line 408, in _iterencode_dict\n    for chunk in chunks:\n  File \"/usr/lib/python2.7/json/encoder.py\", line 442, in _iterencode\n    o = _default(o)\n  File \"utils/standalone_searx.py\", line 84, in json_serial\n    raise TypeError (\"Type not serializable\")\nTypeError: Type not serializable. Hello,\nI was just trying to run the standalone_searx.py with Python3 and am now getting:\n~/searx/builds/searx$ python3 utils/standalone_searx.py spacex\nTraceback (most recent call last):\n  File \"utils/standalone_searx.py\", line 59, in \n    \"categories\":args.category.decode('utf-8'),\nAttributeError: 'str' object has no attribute 'decode'\nAny ideas on how to get this program to run as I am not having much luck so far?\nThanks in advance.\n. @virtadpt Thanks You so very much.  This is AWESOME and exactly what I needed since I will be calling Searx from another server!!!!\nIf I would have known this then I would not have commissioned a freelance developer to try and work on a server daemon using Sanic (https://github.com/channelcat/sanic) which is much faster than Flask and the Searx (standalone_searx.py) CLI version. As my Python skills are not that strong yet and I needed a working daemon as part of a larger project that this will feed data into.\nI also just tried adding \"&pageno=1\" to a local Searx server that I have running and it also works great.\nDo you by chance, know where I can get a list of all the available URL parameters that I can pass in this call?\nThanks again. Hi Again,\nJust one more question about the SearX API and JSON return.\nI did an test with:\ncurl \"http://localhost:8888/?q=solar%20panels&categories=general&format=json&lang=en&pageno=1\"| python -m json.tool\npast of the JSON output shows:\n        \"positions\": [\n            10\n        ],\n        \"pretty_url\": \"http://www.freesunpower.com/solarpanels.php\",\n        \"score\": 0.1,\n        \"title\": \"Basic Tutorials: Solar Panels for Solar \\u2026\",\n        \"url\": \"http://www.freesunpower.com/solarpanels.php\"\n    }\n],\n**\"suggestions\": [],**\n\"unresponsive_engines\": [\n    [\n        \"yacy\",\n        \"request exception\"\n    ],\n    [\n        \"gigablast\",\n        \"unexpected crash: No JSON object could be decoded\"\n    ]\n]\n\nand the web interface has:\n\nActually, I do not know if image will come through, but on my we interface it does come through with some \"SUGGESTIONS\" in that section.\nMy question is as to why does the json \"suggesions\" not have the text for the suggestions that we web interface has since I would like to get that data as well, if possible.\nThanks again and it seems that I am almost there as far as having a metasearch engine that works really good.\nCheers,\nLonnie\nThe JSON return has most of the information, but I noticed that the \"suggestions : []\" was blank\n. @virtadpt I have a small API question.\nIn the past, you told me about how to access SearX via curl:\n\nYou can make Searx return search results as JSON documents by specifying the format. I do it like this: http://localhost:8888/?q=this%20search%20term&categories=general&format=json&lang=en&time_range=day\n\nand all works great, I must say first of all.\nThe only question is in changing the categories. I basically know how to use images, vodeos, etc.... but the \"social media\" category is 2 words and I did not know how to include this in the curl statement.\nI tried\nhttp://localhost:8888/?q=this%20search%20term&categories=social%20media&format=json&lang=en&time_range=day\nbut that does not seem to work. \nThe documentation is also not very clear on this:\nhttps://asciimoo.github.io/searx/dev/search_api.html\nAm, I missing something here?\nThanks\n. Hello,\nThat's the point. I would like to get back the suggestions in the JSON that\nare coming up on the web display.\nFor me when I run the query with curl, I do not get any suggestions[]\n\n        \"category\": \"general\",\n        \"content\": \"Manufacturer of solar panels, cells, kits, and\n\ntravel solar electric products. We provide commercial solar sign & flood\nlighting systems along with a large ...\",\n            \"engine\": \"bing\",\n            \"engines\": [\n                \"bing\"\n            ],\n            \"parsed_url\": [\n                \"http\",\n                \"www.siliconsolar.com\",\n                \"/\",\n                \"\",\n                \"\",\n                \"\"\n            ],\n            \"positions\": [\n                10\n            ],\n            \"pretty_url\": \"http://www.siliconsolar.com/\",\n            \"score\": 0.1,\n            \"title\": \"Solar Panels & Cells - Silicon Solar Store\",\n            \"url\": \"http://www.siliconsolar.com/\"\n        }\n    ],\n   * \"suggestions\": [],*\n    \"unresponsive_engines\": [\n        [\n            \"yacy\",\n            \"request exception\"\n        ],\n        [\n            \"gigablast\",\n            \"unexpected crash: No JSON object could be decoded\"\n        ]\n    ]\n}\n\nbut when I do the query through the web interface then I do get the\nsuggestions.\nHow do I get those suggestions in my JSON query as well?\nThanks,\nLonnie\nOn Thu, Mar 15, 2018 at 6:51 PM, Adam Tauber notifications@github.com\nwrote:\n\n@lonnieOST https://github.com/lonnieost it doesn't return suggestions\nfor me. The screenshot shows an infobox which is presented in the json too:\ncurl -q \"http://localhost:8888/?q=solar%20panels&categories=general&format=json&lang=en&pageno=1\" | python -m json.tool\n[...]\n    \"infoboxes\": [\n        {\n            \"content\": \"Solar panels absorb sunlight as a source of energy to generate electricity or heat.\",\n            \"engine\": \"wikipedia\",\n            \"id\": \"https://en.wikipedia.org/wiki/Solar_panel\",\n            \"img_src\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b8/Photovoltaik_Dachanlage_Hannover_-Schwarze_Heide-1_MW.jpg/300px-Photovoltaik_Dachanlage_Hannover-Schwarze_Heide-_1_MW.jpg\",\n            \"infobox\": \"Solar panel\",\n            \"urls\": [\n                {\n                    \"title\": \"Wikipedia\",\n                    \"url\": \"https://en.wikipedia.org/wiki/Solar_panel\"\n                }\n            ]\n        }\n    ]\n[...]\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1242#issuecomment-373548062,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHvOpO8pL2wcGf6RVO3X4raIogXncks5tevCLgaJpZM4Ss9Sl\n.\n. If you go to the web interface in a browser at http://localhost:8888 and\nenter a query \"solar panels\" for example then with the Oscar theme you will\nget a \"suggestions\" box on the right-hand side. That is what I mean as to\nthe web interface has suggestions and the JSON does not.\n\nOn Thu, Mar 15, 2018 at 7:11 PM, Adam Tauber notifications@github.com\nwrote:\n\n@lonnieOST https://github.com/lonnieost I don't get any suggestions\nfrom the web ui (http://localhost:8888/?q=solar%20panels&categories=\ngeneral) so the JSON response is correct.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1242#issuecomment-373551755,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHtfKeUdgtN9UAZl_1d7dFk-DQJEdks5tevUhgaJpZM4Ss9Sl\n.\n. I see a suggestions box:\n\nLower right-hand side.\nCheers,\nLonnie\nOn Thu, Mar 15, 2018 at 7:19 PM, Adam Tauber notifications@github.com\nwrote:\n\n@lonnieOST https://github.com/lonnieost there is no \"suggestions\" box\nfor me:\n[image: Uploading searsolarpanel.png\u2026]\nhttps://camo.githubusercontent.com/fe90ea2a6310e3564dd0665bad466cee40939da8/68747470733a2f2f692e696d6775722e636f6d2f4a70676661334e2e706e67\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1242#issuecomment-373553189,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHl20BziH0qzRV8SgbwOb00F-3cBZks5tevbwgaJpZM4Ss9Sl\n.\n. This is also running on the latest SearX 0.14 , I think that it is.\n\nOn Thu, Mar 15, 2018 at 7:22 PM, Lonnie Cumberland lonnie@outstep.com\nwrote:\n\nI see a suggestions box:\nLower right-hand side.\nCheers,\nLonnie\nOn Thu, Mar 15, 2018 at 7:19 PM, Adam Tauber notifications@github.com\nwrote:\n\n@lonnieOST https://github.com/lonnieost there is no \"suggestions\" box\nfor me:\n[image: Uploading searsolarpanel.png\u2026]\nhttps://camo.githubusercontent.com/fe90ea2a6310e3564dd0665bad466cee40939da8/68747470733a2f2f692e696d6775722e636f6d2f4a70676661334e2e706e67\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1242#issuecomment-373553189,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHl20BziH0qzRV8SgbwOb00F-3cBZks5tevbwgaJpZM4Ss9Sl\n.\n\n\n. Hello,\n\nthe change log that I have for this version shows:\n0.14.0 2018.02.19\n\nNew theme: oscar-logicodev dark\n\nNew engines\n\n\nAskSteem (general)\n\nAutocompleter fix for py3\nEngine fixes (pdbe, pubmed, gigablast, google, yacy, bing videos,\nmicrosoft academic)\n\"All\" option is removed from languages\nMinor UI changes\nTranslation updates\n\nOn Fri, Mar 16, 2018 at 4:43 AM, Pofilo notifications@github.com wrote:\n\nAre you even with the 0.14 tag ?\nCan you give us a screenshot ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1242#issuecomment-373642790,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHoGvzCI3VZUvRbYJkMrid5mtV-IXks5te3sqgaJpZM4Ss9Sl\n.\n. Hello All,\n\nNot sure why my images are not showing up but the screenshot that I have does show a \"Suggestions\" section \nLet me try again to send over the screen shot. 0.14.0 2018.02.19\n\n. Why is the issue closed?\nWe seem to have one version showing suggestions and one version not showing suggestions for the same web search and in both cases the JSON call does not return any suggestions.\n. Hello,\nI see what you mean, had them off  as \"default\".  What I was doing was to\nstart up the default engines and then while the server was running and then\nI would  enable \"Yahoo\" engine, for example and did not see JSON results\nafter that enabling in the preferences post-startup.\nThanks so very much as this really was making me pull my hair out trying to\ndetermine what was happening.\nYes, the issue is closed and I sincerely appreciate all of the help.\nHave a great weekend.\nOn Fri, Mar 16, 2018 at 9:52 AM, Adam Tauber notifications@github.com\nwrote:\n\n@lonnieOST https://github.com/lonnieost you have non-default engines\nenabled on your screenshot. Enable these engines when you create JSON\nrequest and it will return suggestions. The issue is closed, because searx\nworks as expected. If you specify yahoo engine in the json request too, it\nreturns suggestions: http://localhost:8888/?q=!yh%\n20solar%20panels&categories=general&format=json .\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1242#issuecomment-373719415,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHnby2Rs7MOTwQbHZDP1axN2-8sJWks5te8OsgaJpZM4Ss9Sl\n.\n. A follow on is that I have been looking at the SearX API information at:\n\nhttps://asciimoo.github.io/searx/dev/search_api.html\n. Thanks, but that did not really answer the question.\nI know that it can be adjusted in the configs, but was mainly wondering if\nit can be also controlled within the API call, for example with a\n\"&rows=10\" parameter addition to go with the \"&pageno=1\"\nOr, perhaps something like \"\"&start=0&rows=10\" for example in which you\ntell it a starting number and then how many rows to take from that point.\nThanks\nOn Sat, Apr 21, 2018 at 1:49 PM, Alexandre Flament <notifications@github.com\n\nwrote:\nLook at fltron https://github.com/asciimoo/filtron :\n543 (comment)\nhttps://github.com/asciimoo/searx/issues/543#issuecomment-225305326\nStep by step installation process written in German (not tested, maybe\noutdated) :\nhttps://scroom.de/eine-eigene-searx-instanz-unter-ubuntu-16-04-aufsetzen/\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1276#issuecomment-383316136,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHvdfg62i1B-noh0t6Zdoau3ZdJgwks5tq3EogaJpZM4TefDi\n.\n. Hello,\n\nCan you please clarify. Was this fixed?\nThanks,\n. Yes,\nI still get the same message:\nError! Engines cannot retrieve results.\ngigablast (unexpected crash: No JSON object could be decoded)\nThanks,\nOn Fri, May 18, 2018 at 12:40 PM, No\u00e9mi V\u00e1nyi notifications@github.com\nwrote:\n\nYes. Does it crash again?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1293#issuecomment-390264348,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHkyICIE6R19qUfyIBwmVtiTQvlGdks5tzvmZgaJpZM4T8I8P\n.\n. Hi,\n\nI just now did:\ngit clone https://github.com/asciimoo/searx.git\nstarted it up and went to localhost:8888\nand then\n/searx$ python searx/webapp.py\nERROR:searx.search:engine gigablast : exception : No JSON object could be\ndecoded\nTraceback (most recent call last):\n  File \"/home/lonnie/yacy-susper-susi-qs/test/searx/searx/search.py\", line\n104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/home/lonnie/yacy-susper-susi-qs/test/searx/searx/search.py\", line\n87, in search_one_request\n    return engine.response(response)\n  File\n\"/home/lonnie/yacy-susper-susi-qs/test/searx/searx/engines/gigablast.py\",\nline 82, in response\n    response_json = loads(resp.text)\n  File \"/usr/lib/python2.7/json/init.py\", line 339, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python2.7/json/decoder.py\", line 364, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python2.7/json/decoder.py\", line 382, in raw_decode\n    raise ValueError(\"No JSON object could be decoded\")\nValueError: No JSON object could be decoded\nOn Fri, May 18, 2018 at 12:58 PM, No\u00e9mi V\u00e1nyi notifications@github.com\nwrote:\n\nHave you pulled the latest changes?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1293#issuecomment-390269133,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHodEGv1gcmFn9bEYXwyAf_FwC_ziks5tzv21gaJpZM4T8I8P\n.\n. Hello,\n\nI actually just tried the latest SearX master version and am still getting the same Gigablast error:\nError! Engines cannot retrieve results.\ngigablast (unexpected crash: No JSON object could be decoded)\n. So then, from the API page would the time range parameter for the api curl\ncall look like:\ntime_range=day\nor\ntime_range=month\nor\ntime_range=year\nthus something like:\ncurl \"\nhttp://www.example.com/?q=solar%20panels&categories=general&format=json&lang=en&pageno=1&time_range=day\n\"\nCheers,\nOn Thu, Jun 21, 2018 at 5:44 AM, Adam Tauber notifications@github.com\nwrote:\n\nClosed #1328 https://github.com/asciimoo/searx/issues/1328.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1328#event-1693201651, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AXkxHlH1ZETinzvN1_5jVqv8nyecclifks5t-2r2gaJpZM4Uxt_e\n.\n\n\n-- \nThanks and have a great day,\nLonnie T. Cumberland, PhD.\nDIRECT: 313-333-2935\nEmail: Lonnie.Cumberland@Outstep.com\n. Thanks for the information on this. Very helpful.. ",
    "rKsanu2MMYvypWePtQWM": "Hello, what's the status of this issue?\nI'd love to see more languages in the future!. @cloo labas!\nI was thinking about what you wrote, and while your solution would kinda work, there should be a \"reversion\" to a user-choosable language or a language priority list if, for example, Lithuanian is not available in some search engines.. I just use :lt before every Lithuanian search. Works fine as a workaround.. ",
    "cloo": "I don't see the sense of having the threshold at all. If a search engine is incapable to deliver in that language, the search results from it are nil. If user tries using only engines without particular language support, he will naturally get no results and will have to refine the search. \nRefining search terms is a very natural and harmless everyday process :) Differently from the language code syntax, which is simple, but still, I found out about it only now, reading about this issue. Came because of Lithuanian here.. Labas :)\nAgree, automatic reversion, or even suggestion to do it manually would be \"nice to have\". \nBut for now, using searx, I'm simply unable to search for content in my language at all, even many search engines provide such results. So to get those I just have to go back to google'n'ducks. . I have deleted my comment before saw yours, because I actually found a way to do it nicely. \n(For the record - I complained that there is no way to add searx as search engine in firefox)\nSolution is simpler - for some reason just have to switch on the search bar (instead of using unified search/address bar) and then you can add any custom search engine.\nMaybe this instruction could be added to docs?. True, but as (hopefully) most users, at first I want to know about UI, and if possible, avoid understanding internals.\nDon't know your docs policy, so can't argue what fits in and what doesnt. \nIf it does, one could also suggest to describe procedures in other popular browsers as well. So just my two cents at this point.. ",
    "paul1149": "Beautiful. Thanks!. ",
    "yodahome": "Of course! Sorry, I forgot and updated the original post with the vHost.. ",
    "Venca24": "It is because of diacritics in the name of one folder in the absolute path in the computer.. There were no thumbnails yet before. I tried do my best.\n\nThe thumbnails are base64-encoded and given to the results by a JavaScript function.. Is it inconvenient for possible future debugging of the functionality? Do I need to remove them completely or is it enough just to comment them out?. ",
    "zuglufttier": "I have the same problem, this is from the logs on my own server:\nERROR:searx.search:engine google images : exception : No JSON object could be decoded\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/usr/local/searx/searx/search.py\", line 87, in search_one_request\n    return engine.response(response)\n  File \"/usr/local/searx/searx/engines/google_images.py\", line 69, in response\n    g_result = loads(resp.text)\n  File \"/usr/local/lib/python2.7/json/__init__.py\", line 339, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/local/lib/python2.7/json/decoder.py\", line 364, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/local/lib/python2.7/json/decoder.py\", line 382, in raw_decode\n    raise ValueError(\"No JSON object could be decoded\")\nValueError: No JSON object could be decoded\nERROR:searx.search:engine 500px : exception : 'photos'\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx/search.py\", line 104, in search_one_request_safe\n    search_results = search_one_request(engine, query, request_params)\n  File \"/usr/local/searx/searx/search.py\", line 87, in search_one_request\n    return engine.response(response)\n  File \"/usr/local/searx/searx/engines/www500px.py\", line 56, in response\n    for result in response_json['photos']:\nKeyError: 'photos'. ",
    "lantrix": "This seems to be mentioned as fixed in #1338 but it's awaiting the next release for inclusion.. ",
    "vinzruzell": "Hi. I want the Tagalog Translation ot cebuano.. ",
    "gruessung": "+1. ",
    "NorcuxOS": "+1. ",
    "yurivict": "No, I am creating the FreeBSD port, and it can't be installing tests.. I patched the upcoming FreeBSD package too. This bug is meant to \"upstream\" this, so that distros don't all need to patch.\n\nBTW, settings.yml, templates are required files. You shouldn't have patched them away.\n. > settings.yml is a configuration file and thus belongs into /etc/searx\nExactly: https://github.com/asciimoo/searx/issues/1211\n\nBut those are static files which are not Python scripts and thus they belong to /usr/share/searx instead.\n\nhttps://github.com/asciimoo/searx/issues/1212. > This is related to searx.org configuration. ... Most probably related to settings.yml :\nHere is a suggested solution: allow 3 possible values: Invalid, None, URL. Ship with Invalid, but don't allow to start searx with base_url: Invalid. searx should require the user to either set it to None or to URL.\nThis would prevent the user from leaving base_url: None accidentally. He can only set None consciously.\n. The port was labeled as py27-only due to some dependency, not due to errors in searx.\nClosing.. ",
    "Popolon": "You cab add it easily by adding in searx/settings.yml, in doi_resolvers section\u00a0: \nsci-hub.tw : 'http://sci-hub.tw/'\nThen you can choose it in preferences => General => Open Access DOI resolver (the name of this field is a mess as the plugin currently).\nThere is currently a bug with doi URL (see #1335). It use the name of the entry instead of the value for concatenation with doi.. This can be closed now.. I have the same issue. This part of the settings.yml is the default one, and I have\u00a0:\n```\ndoi_resolvers :\n  oadoi.org : 'https://oadoi.org/'\n  doi.org : 'https://doi.org/'\n  doai.io  : 'http://doai.io/'\ndefault_doi_resolver : 'oadoi.org'\n```\n. By disabling \"Avoid paywalls by redirecting to open-access versions of publications when available\" in Preferences => plugins => Open Access DOI rewrite, the url work correctly on http://dx.doi.org/xx.xxx/... So the problem seem to be only present in the case of default oadoi.. I'm don't know well python, but error seems to be in get_doi_resolver function (searx/plugins/oa_doi_rewrite.py)\nIf i replace return in this function like this, it works fine:\n```\nreturn doi_resolver\nreturn 'https://aodaoi.org/'\n\n```\n. I was wrong in the previous comment (and edited it) fir the name of the plugin, that's oa_doi_rewrite.py, not ao_doi_rewrite.py. I tried your trick it doesn't work, same result. Need to understand more deeply the structure and functions used in this one.. check passed after removing  spaces in middle of the line (didn't have problem with compilation, only with travis).. ",
    "Neustradamus": "Nothing is lost!. \"Fork\" has based on this: 394\n\"Watch\" and \"Star\" are separated but you can look the difference, it is better here: 180 / 2725 vs a very very little\n-> It is the best solution to move :). Hi guys,\nAny news about this request?\n@asciimoo, @dalf, @kvch, @Pofilo. @asciimoo, @dalf, @kvch, @Pofilo https://help.github.com/articles/about-repository-transfers/. @asciimoo, @dalf, @kvch, @Pofilo Why when the development will be stopped?\nThe main repository is here.\nIt is easy to do it right now without lost...\n. Any news?\n@asciimoo, @dalf, @kvch, @Pofilo\n. Any news?\n@asciimoo, @dalf, @kvch, @Pofilo. @asciimoo, @dalf, @kvch, @Pofilo Better with the main project in an organization and it is not needed to be a company.\nAnd there is already an organization and searx/searx (which mix people), easy to rename searx-archives and move it to searx/searx to be perfect.\nNo lost.. Yes all asciimoo/searx links will be redirected to searx/searx links.\nOf course in first, you must to move searx/searx to searx/searx-archives\nNo lost.. @asciimoo, @dalf, @kvch, @Pofilo: Happy new year guys!\nAny news about this ticket?. @asciimoo, @dalf, @kvch, @Pofilo: Any news?. ",
    "Anaud": "in searx/settings.yml, piratebay entry, delete \"url: https://pirateproxy.red/\"\nin searx/engines/piratebay.py replace 'https://thepiratebay.se/' by 'https://thepiratebay.org/'\nAnd it works\n . ",
    "WAZAAAAA0": "also, the Number of results: section starts working normally in Chinese. Alright I should've used search links in my report, here they are:\nen-US search: only finds unrelated trash, most probably coming from results without quotes\nzh search: found that 1 unique result word by word! No garbage results. \"Number of results\" counter also working correctly\nHave a side-by-side comparison too:\n\nSo which Google does SearX use when set to Chinese then? I assumed it falls back to google.com from that file. If this is true, then google.com works so much better than any other country-specific google.xx domain with SearX! Maybe SearX is using country domains erroneously by enabling Google's Pages from: X-COUNTRY setting? That's my best bet.. Today the Russian query temporarily stopped finding the correct result for some reason, so I made more example links with the Italian one. \"Chinese\" being more comprehensive is still real:\nQUERY: \"Forum cliccando sul tasto rosso\"\nSOURCE: http://orologi.forumfree.it/?t=72189596\n\u4e2d\u6587 - zh\nEnglish (United States) - en-US\nEDIT: it seems like en-US now finds it for some reason, but the problem still exists:\nEspa\u00f1ol - es\nFran\u00e7ais - fr\nI really believe this is a major flaw in SearX that should be taken care of. I've been using SearX in Chinese since I found about this problem. I've seen this problem occur to other metasearch engines too like StartPage and Ixquick, they say they're powered by Google but give less results than the real one, so it's hard to move away from it.. I did some further testing and the Chinese language apparently lost its \"superpowers\" recently on searx.me.\nsearx.ch which is stuck on 0.11.0 still retains them.\nhttps://searx.me/?q=%21google%20%22Forum%20cliccando%20sul%20tasto%20rosso%22&categories=none&language=zh\nhttps://searx.ch/?q=%21google%20%22Forum%20cliccando%20sul%20tasto%20rosso%22&categories=none&language=zh\n\nI'm gonna be an asshole and tag a bunch of recent contributors to notice this issue because I think it's so important @asciimoo @kvch @MarcAbonce @dalf @Pofilo\nI really want to replace Google lol. Thanks @MarcAbonce now that makes a bit more sense.\nBut I'd still take the magic IP guess over having less results any day personally. I didn't even notice the German IP \"bias\" while I was using searx in Chinese until you told me... what I did notice was a lot of \"annoying\" Chinese results because some search engines do listen to the setting, and even Google does to an extent.\nSo if the \"Automatic/All\" setting still existed I would use that in a heartbeat. What are these other problems related to re-adding this? Just don't make it the default option, maybe it would help?. I mean look at these results, Google DOES listen to my Chinese preference (unfortunately lol) even though the server IP location is Switzerland\n. ",
    "Yetangitu": "I guess what he means is whether searx gives consent for all those things which sites like Google ask you to give consent to (use of cookies etc.). Given the way searx works this is more or less impossible as the whole point of the exercise is to avoid the things which you're supposed to give consent to.\nAnother possibility is whether searx violates the terms of service of search engines which it uses to gather results. The answer to that is 'probably', see e.g. Google's terms of service:\nhttp://www.google.com/intl/en/mobile/xhtml/terms_of_service.html\n```\nNo Automated Querying\nYou may not send automated queries of any sort to Google's system...\n...\n- \"meta-searching\" Google;\n...\n``\nAs always it is up to the user to decide what to do but keep in mind that many of the countermeasures used by 'savvy' users of sites like Google (dropping or scrambling cookies, blocking access to resources, removing redirects, etc.) can also be construed as being against the terms of service. Pick your poison.. FYI, theFilesizereported by Recoll does not always correspond to the actual file size, this problem lies two steps upstream (engine -> _recoll-webui_ -> _Recoll_) so it is not something I can easily fix here. The discrepancy seems to arise from the way Recoll handles compound file types (i.e. compressed container files containing multiple individual files representing pages or chapters) like _epub_, it reports the size of the contained file instead of the containing file. For these files theTypewill also be reported incorrectly, for _epub_ containers it generally will reportType text/html.\n. URL handling sanitised and engine disabled by default. BTW, if this PR is to be merged I - or someone else - should add template/style support for the 'files' template to the other styles as it currently only works as intended in oscar/logicdev. I'll only spend time doing so if it will be merged as I use searx with a custom theme and style.. The commits above addsnumber_of_results` to the engine output. This only works in combination with a patched version of recoll-webui as the standard version doesn't produce result numbers.\nIt looks like recoll-webui is unmaintained (no commits since Sept. 2016) so I won't hold my breath for the PR to be merged.. PS hold a bit with merging, there are some features I'm adding at the moment (embedded preview) plus one part of the code which turns out to be specific to my network which needs to be generalised (the download logic).. The commit above adds preview support for audio, video and image types. It also adds a mandatory mount_prefix parameter to settings.yml with explanation on how to use it.. Try to run the webui by itself first using webui-standalone.py, this will produce somewhat more informative error messages. To run it in any sensible way you'll want to have a working Recoll installation (with the python-recoll module included) for it to dig through, apart from that it just depends on Python (2.x) plus some common modules, if so desired completed with ujson for somewhat faster JSON handling. \nI.e. install Recoll, have it index something, point the webui at this config and it should work. You don't need to start the Recoll GUI (I never touch it) to get it going. Here's a quick example Recoll config to get started:\nhttps://gist.github.com/Yetangitu/1bb4c5cd4b35e2911123d71b6ca3cc1c\nDump it in a directory (preferably hosted on SSD) which has enough capacity to hold the expected index file size, these can become quite big when indexing a large set.\nTo actually have Recoll index the  contents of the files you'll want to run the recollindex tool which is part of the Recoll package. I find it easiest to run it from a script:\nhttps://gist.github.com/Yetangitu/dbb624db032fc217cf97898008a27a71\n. Done. ",
    "Drose221193": "@asciimoo Anyone that uses Brave's BAT system Puts money in an account and every month part of their funds go to the sites they visit the most. So anyone that uses the browser and visits searx will have part of their monthly funds donated directly to searx. Its generally just another way to generate revenue for searx. It might not be a lot, but I assume that every little bit helps. As far as becoming verified. I am pretty sure you just need to prove that you are a legitimate site at this link here.\nhttps://brave.com/publishers/\n. ",
    "TriMoon": "PS: when you add the onion search plugin, it does not open a results page with the query you made.\nIt will open a blank search page of the searxes engine, maybe forgot to add a parameter?. ",
    "timscha": "I have the same problem.. Yes, you're right @asciimoo! I added \"lazy-apps = true\" to /etc/uwsgi/apps-available/searx.ini and searx is working again. Thank you!. ",
    "Gering-ding-ding-ding-din": "Same problem here. I'm using a browser which deletes all cookies by default when closed.\nThe settings URL is not working as described above.\nEvery time I close the browser all settings are gone.\nWould be great if it was possible to store all the settings in a config file.. Being able to store all settings in a config file would enhance privacy as well as usability.\nBecause websites would not be able to read those cookies if there weren't any.\nFurthermore users running there own instance could store different profile files with different settings.. ",
    "dixonge": "Just tested in MS Edge - works fine with searx.me, not with searx.site\nWhen I get no results, I am getting this message from searx:\n\"Sorry! we didn't find any results. Please use another query or search in more categories. \". ",
    "Hronom": "Same here https://searx.nulltime.net/ also not work, how to solve this problem? I'm using search through rest api.\nHere is my REST api call:\nhttps://searx.nulltime.net/search?q=Word of Warcraft&categories=news&time_range=year&language=en&format=json\nReturned result is next:\n{\n    \"number_of_results\": 0,\n    \"corrections\": [],\n    \"query\": \"Word of Warcraft\",\n    \"infoboxes\": [],\n    \"suggestions\": [],\n    \"results\": [],\n    \"answers\": [],\n    \"unresponsive_engines\": []\n}. ",
    "thagoat": "Sirks =). ",
    "holymio": "@aurora-potter can confirm what @Pofilo said. Freshly installed Firefox, went to searx.me. Clicked on the about page and clicked on install in \"Install searx as a search engine on any version of Firefox! (javascript required)\". Installation went fine. Made searx.me default in Firefox settings. Most of the times it works, but now and then you are just forwared to the searx.me page, where your entered search text is lost.. ",
    "debasebit": "I've been having this issue as well, but it looks to be a regression in Firefox (https://bugzilla.mozilla.org/show_bug.cgi?id=1435615). The workaround of setting browser.newtab.preload to true works for me.. ",
    "Noah-Huppert": "This extension breaks entirely when using Firefox containers.. ",
    "n8v8R": "FF 64.0a1 (2018-09-22) (64-bit) on W10 with searx.me assigned to open in a designated container\neach and every search from the browser's url/search bar presents the searx.me website with the search term/string from the url/seach bar gone/vanished/stripped.\nTried the various suggestions from this thread but to no avail. Such  behaviour is no observed with other search engines however, e.g. duckduckgo works as expected (as do others).\nThe only viable workaround thus far is to keep a tab with searx.me permanently open and revert to it when necessary, which is bit cumbersome in terms of workflow. What fixed it for me was to delete \"method\": \"POST\", from search.json.mozlz4 (located in the user profile folder).\nSince then it is working as expected in containers :sweat_smile: . ",
    "bjesus": "Any solution to this?\nI'm seeing the same behivor with Firefox Container Tabs. I've set searx to automatically open in a specific container, but every single search I type to the address bar just brings me to a fresh searx homepage and I need to re-search it.. ",
    "ewdissi": "\nAny solution to this?\nI'm seeing the same behivor with Firefox Container Tabs. I've set searx to automatically open in a specific container, but every single search I type to the address bar just brings me to a fresh searx homepage and I need to re-search it.\n\nI've the same problem :( tryed everything in this discussion without a solution. ",
    "maverick74": "Yes, but that requires you install it. In a public service, for example, you won't be able to access it unless it was already installed.\nSo, while i like the fact that it is an open distributed search engine, that leaves it out of the deck for now.. @asciimoo \n(first, happy new year :+1:)\nAre you aware, beside yacy, of any opensource search engine / crawler that can be used with searx?. Thank you!. i think #1426 is related to this. simpleshould be the default theme ;). ",
    "sven222": "I still have the captcha issue. Updated with git to the last master branch and I do not receive any search results. Just the Captcha messsage in red. Can I provide you some log files?. Same here, running on Debian 9 with nginx, latest master branch. ",
    "popeyesfx": "Well I have installed jessie and still get the same issue\ncat /etc/os-release\nPRETTY_NAME=\"Raspbian GNU/Linux 8 (jessie)\"\nNAME=\"Raspbian GNU/Linux\"\nVERSION_ID=\"8\"\nVERSION=\"8 (jessie)\"\nID=raspbian\nID_LIKE=debian\nHOME_URL=\"http://www.raspbian.org/\"\nSUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\"\nBUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\"\n`sudo apt-get install git build-essential libxslt-dev python-dev python-virtualenv python-babel zlib1g-dev libffi-dev libssl-dev\nReading package lists... Done\nBuilding dependency tree     \nReading state information... Done\nPackage python-virtualenv is not available, but is referred to by another package.\nThis may mean that the package is missing, has been obsoleted, or\nis only available from another source\nPackage python-dev is not available, but is referred to by another package.\nThis may mean that the package is missing, has been obsoleted, or\nis only available from another source\nHowever the following packages replace it:\n  python\nE: Unable to locate package libxslt-dev\nE: Package 'python-dev' has no installation candidate\nE: Package 'python-virtualenv' has no installation candidate\nE: Unable to locate package python-babel\nE: Unable to locate package libffi-dev\nE: Unable to locate package libssl-dev`\nso not sure what is going on here. Unless the default install line is now incorrect.\nI have python3.4.2 and 2.7.9 installed on a pi 3 so as far api can see it should work.\nwhat version of jessie are you using, should i run something before i start or have i just missed something\n. OK after some more research, i fixed the  install by using ibxslt1-dev, everything then installed and worked ok.\n. The problem wasn't solved as such, i just used a service and build a work around. ",
    "ahkuma": "Thanks guys. I didnt know how to switch instance I thought it was all in one domain. I will close it. @Pofilo why did you say its long?. Damn thats nice.. ",
    "rinpatch": "Sorry, forgot to run tests. Should be fixed now. ",
    "zero77": "https://searx.me/?q=1&categories=general\ni am getting the same error hear.. ",
    "Perflyst": "Updated, now correct?. my fault, just post requests. I think I have the correct indentation. In front of theme_args are 4 spaces. In front of oscar_style are 8 spaces.\nI also restarted nginx and uwsgi. Browser cache was cleared too.\nOther settings, like default engines, are working.\n/edit: I also tried this on another server with apache2. Not working.... default_theme : legacy is working without any issues. I am on v0.14.0 with commit 3ac578c0a8ea7d32c5f6302760c86d232ff29fa8.. This solution is not working here. For others on the issue, we diff'd settings.yml and preferences.py\nMy debug log:\n[2018-12-31 15:31:46,455] ERROR in app: Exception on / [GET]\nTraceback (most recent call last):\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1982, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1614, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1517, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1610, in full_dispatch_request\n    rv = self.preprocess_request()\n  File \"/usr/local/searx/searx-ve/local/lib/python2.7/site-packages/flask/app.py\", line 1831, in preprocess_request\n    rv = func()\n  File \"/usr/local/searx/searx/webapp.py\", line 404, in pre_request\n    preferences = Preferences(themes, list(categories.keys()), engines, plugins)\n  File \"/usr/local/searx/searx/preferences.py\", line 261, in __init__\n    'theme': EnumStringSetting(settings['ui']['default_theme'], choices=themes),\n  File \"/usr/local/searx/searx/preferences.py\", line 37, in __init__\n    self._post_init()\n  File \"/usr/local/searx/searx/preferences.py\", line 67, in _post_init\n    self._validate_selection(self.value)\n  File \"/usr/local/searx/searx/preferences.py\", line 62, in _validate_selection\n    raise ValidationException('Invalid value: \"{0}\"'.format(selection))\nValidationException: Invalid value: \"logicodev-dark\". Seems like this works now with v0.15.0, I changed nothing in the config from theme settings.\n@SitiSchu can you try again, please?. ",
    "GGBoCa": "Hey, thanks for reading and take a minute to help. To tell it simply Qwant doesn't work (qw normal, actuality, image or whatever). It's weird because if I try a new instance it works well, once, twice, then... it doesn't anymore. And sometimes (something like once a week) it works for one search. And I can't explain this myself. Unfortunately, I don't have too much informations to help. I use Firefox with many privacy addons, a VPN but qwant is the only instance (I tried) which doesn't work without telling there is a problem (like Gigablast and json stuff...).\nSorry to not be very very understandable.... I checked today. It qwant works between \"around\" 11am and 2pm (french time).. It use to work for this morning. Yesterday it doesn't. \nThere is not error message when it doesn't work. There is simply no result from qwant. As it doesn't exist.\nBut as I said it's OK today. I'll tell you if the problem is magically fixed on the next days.\nWait and see.\nTsch\u00fcss. For the first time ever (in my case), it works for more than one week now.\nSo the only solution is the problem was solved by writing on this forum. I'll logically tell you if my fridge or my car is out of order.\nThanks\nTsch\u00fcss\nGG. Salut,\nAlors je peux parler fran\u00e7ais ! Merci pour l\u2019info, cela ne m\u2019\u00e9tonne pas trop que Qwant ne r\u00e9ponde pas. La collaboration avec le monde du libre n\u2019a pas trop l\u2019air d\u2019\u00eatre dans le style de la maison.\nDans tous les cas, cela fait plusieurs semaines que cela fonctionne correctement, sans avoir \u00e0 faire de bidouille donc pour l\u2019instant plus de probl\u00e8me. J\u2019essaierai la technique du VPN si jamais \u00e7a ne remarche plus.\nMerci encore pour cette r\u00e9ponse personnalis\u00e9e et pour l\u2019ensemble de votre travail.\nSo see soon\nChristophe\n\\CH_ocolat_{0041.78.763.28.24}\n \\FR_omage_{0033.686.75.80.10}\n\nLe 29 juin 2018 \u00e0 17:35, elukerio-dev notifications@github.com a \u00e9crit :\nHey @GGBoCa\nwe are a french association named elukerio.org and we maintain searx.elukerio.org so we have already seen this problem many times with qwant (qw), it comes from the fact that the ip of our server is regularly banned by qwant due to the large number of requests that our instance. Indeed our instance is one of the most used (more than 2 million requests per month) and especially by French community which likes to use qwant. So the only solution we found is to connect with the VPN of our association to have the same public IP as our instance, then we go to qwant.com, and there the site thinks we are unwanted traffic and shows us a captcha. Then just fill it up, and the qwant engine starts working as before. It's really do-it-yourself but it has the merit to work during 2 weeks, then it's necessary to start again the manipulation. Otherwise we contacted qwant but no answer...\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.. \n",
    "elukerio-dev": "Hey @GGBoCa \nwe are a french association named elukerio.org and we maintain searx.elukerio.org so we have already seen this problem many times with qwant (qw), it comes from the fact that the ip of our server is regularly banned by qwant due to the large number of requests that our instance. Indeed our instance is one of the most used (more than 2 million requests per month) and especially by French community which likes to use qwant. So the only solution we found is to connect with the VPN of our association to have the same public IP as our instance, then we go to qwant.com, and there the site thinks we are unwanted traffic and shows us a captcha. Then just fill it up, and the qwant engine starts working as before. It's really do-it-yourself but it has the merit to work during 2 weeks, then it's necessary to start again the manipulation. Otherwise we contacted qwant but no answer.... ",
    "PiPc2": ".. .. ",
    "GambaJo": "Ah, ok. I used oscar theme too, but logicodev dark.\nOk, then I have to use only logicodev without dark.. @asciimoo You did't read my text. Thx for nothing.. @Pofilo This is exactly, what I ment. Thx!. @miicha Looks good to me (compared to duckduckgo)\n\n\n. ",
    "mrintegrity": "Would also love this feature! There is also other stuff like searching for \"22 USD in SEK\" which does the currency conversion for you, similar features exist for other search types too. Perhaps it is out of scope for this project however?. ",
    "lewistapetest": "openairepublications (unexpected crash: name 'basestring' is not defined), crossref (unexpected crash: name 'basestring' is not defined),openairedatasets (unexpected crash: name 'basestring' is not defined) is not fixe. Yeah it solve base engine but i still get error on other engine. ",
    "qu3stbaby": "I am using Apache2 on Ubuntu 16.04. uh,thank you @Pofilo . ",
    "infosisio": "@asciimoo It is still visible to me :/. @asciimoo I can put it on my to-do list.. @asciimoo This is very useful !. Same here. I find it quite annoying :/. ",
    "arthurmougin": "its not a searx issue, but the choise of hosters to allow or not the use of their hosted version by others...\nMaybe your request should be to add a list of Cross origin ressources sharing public instances :) that's something that could be done there.\nedit : you can use curl php to overpass the cors issue,\nin your situation, you call the php file on your server, the file do the curl request and return you the search result. ",
    "bourrel": "Unfortunately 500px shut down its API. IMO the best solution for the moment is to deactivate 500px (maybe set it deactivated by default ?) from the engines and wait till a solution is found.\nI can't reproduce the google images crash neither on searx.me nor in my personnal instance. Are you sure the instances you use are up-to-date ?. Indeed I have the same problem on a v14 instance but laquadrature doesn't have this problem and is currently running searx v13.1.\nAfter a quick look it might come from this line. I think the filename variable is incorrect, it should stop after the type (.jpgin my test) but seems to be a bit longer and contains some metadata about the image.\nI'll try to take a deeper look later.. This should fix the formatting.\nI also fixed the wikidata tests to work with the updated image path.. Could you be more explicit about these continual \"service errors\" ? Dit it happens on a public instance or in your private instance ? If it's a private, do you have some logs or something to show us ?\nThe searx.me certificate problem has already been reported : #1350.. This is not a problem, searx is just in development mode.\nTo change for production just replace line 2 in settings.yml with : debug: False.. Maybe you should create your own instance of searx and set the default level of SafeSearch to strict in the preferences. Unfortunately I don't know any public instance that has safe image search by default but you could check this list of instance : https://github.com/asciimoo/searx/wiki/Searx-instances. Can you try with this branch #1481 ? It updates jinja2 which seems to be part of the problem here.. Duplicate of #1502. ",
    "Failure404": "Same here:\ngoogle images (unexpected crash: No JSON object could be decoded). ",
    "Ryonez": "This is defiantly still an issue:\n\nFrom the logs:\n```\nTraceback (most recent call last):\nFile \"/usr/local/searx/searx/search.py\", line 104, in search_one_request_safe\nsearch_results = search_one_request(engine, query, request_params)\nFile \"/usr/local/searx/searx/search.py\", line 87, in search_one_request\nreturn engine.response(response)\nFile \"/usr/local/searx/searx/engines/www500px.py\", line 56, in response\nfor result in response_json['photos']:\nKeyError: 'photos'\nERROR:searx.search:engine google images : exception : No JSON object could be decoded\nTraceback (most recent call last):\nFile \"/usr/local/searx/searx/search.py\", line 104, in search_one_request_safe\nsearch_results = search_one_request(engine, query, request_params)\nFile \"/usr/local/searx/searx/search.py\", line 87, in search_one_request\nreturn engine.response(response)\nFile \"/usr/local/searx/searx/engines/google_images.py\", line 69, in response\ng_result = loads(resp.text)\nFile \"/usr/lib/python2.7/json/init.py\", line 339, in loads\nreturn _default_decoder.decode(s)\nFile \"/usr/lib/python2.7/json/decoder.py\", line 364, in decode\nobj, end = self.raw_decode(s, idx=_w(s, 0).end())\nFile \"/usr/lib/python2.7/json/decoder.py\", line 382, in raw_decode\nraise ValueError(\"No JSON object could be decoded\")\nValueError: No JSON object could be decoded\nERROR:searx.search:engine 500px : exception : 'photos'\nTraceback (most recent call last):\nFile \"/usr/local/searx/searx/search.py\", line 104, in search_one_request_safe\nsearch_results = search_one_request(engine, query, request_params)\nFile \"/usr/local/searx/searx/search.py\", line 87, in search_one_request\nreturn engine.response(response)\nFile \"/usr/local/searx/searx/engines/www500px.py\", line 56, in response\nfor result in response_json['photos']:\nKeyError: 'photos'\nERROR:searx.search:engine bing images : exception : expected string or buffer\nTraceback (most recent call last):\nFile \"/usr/local/searx/searx/search.py\", line 104, in search_one_request_safe\nsearch_results = search_one_request(engine, query, request_params)\nFile \"/usr/local/searx/searx/search.py\", line 87, in search_one_request\nreturn engine.response(response)\nFile \"/usr/local/searx/searx/engines/bing_images.py\", line 111, in response\nthumb_json_data = loads(_quote_keys_regex.sub(r'\\1\"\\2\": \\3', link.attrib.get('mad')))\nTypeError: expected string or buffer\nWARNING:searx.search:engine timeout: flickr\nERROR:searx.search:engine flickr : HTTP requests timeout(search duration : 3.29820203781 s, timeout: 3.0 s) : Timeout\n```\nRunning the latest docker image from wonderfall/searx. It does appear the image is out of date by 6 months.. I'd just like to point out what I'm after is a docker image. The server I use(unRaid) makes building an image locally an unideal job.\nThis build error probably explains why the image that was meant to be on version 15 isn't though.\nOn 25 January 2019 8:43:16 PM NZDT, Marco Balmer notifications@github.com wrote:\n\nI confirm that docker build -t searx . is running (in master and\nv0.15.0 tag) on an error:\n[...]\nCollecting pycparser (from\ncffi!=1.11.3,>=1.8->cryptography>=2.2.1->pyopenssl==18.0.0->-r\nrequirements.txt (line 7))\nDownloading\nhttps://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz\n(158kB)\nException:\nTraceback (most recent call last):\nFile\n\"/usr/lib/python2.7/site-packages/pip/_internal/cli/base_command.py\",\nline 176, in main\n   status = self.run(options, args)\nFile\n\"/usr/lib/python2.7/site-packages/pip/_internal/commands/install.py\",\nline 346, in run\n   session=session, autobuilding=True\nFile \"/usr/lib/python2.7/site-packages/pip/_internal/wheel.py\", line\n886, in build\n   assert have_directory_for_build\nAssertionError\n[...]\n-- \nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/asciimoo/searx/issues/1495#issuecomment-457484148\n\n-- \nSent from my Android device with K-9 Mail. Please excuse my brevity.. ",
    "KeKpetrLAWL": "same issue for me. would be cool to have saved fast switching . ",
    "oyren": "This prevents me from using searx. . ",
    "stresscult": "Same thing, I want but can't use searx because of it.. ",
    "avoidr": "I also frequently just use another search engine, because searx started to filter search results to show only english results (talk about filter bubbles\u2026). This is fine for people who speak only English, but I would like to see German and Polish, too, for instance. However, the user should not have to \"unfilter\" English; the default case should be that there is no language filter at all (with the option to exclude or prioritise languages).\nIn short: don't filter prematurely; let the user filter, not the server.. Here is something related: #1344. ",
    "girlwithnoname": "Thanks, after indenting the text it finally works.. ",
    "drannex42": "The https certs on the hosted instance of searx on serx.me have expired. If the owner of the website needs to update check out LetsEncrypt, just in case they haven't stayed in the loop as well as others. Open sourced by the EFF if I remember correctly. \ntl;dr https certificates on the hosted instance of searx.me have expired, nothing is wrong with the code itself. ",
    "dosch": "@dimqua how is that a duplicate? The referenced issue is a bug. This is a feature request.. Ah sorry. Before that # was linking to another issue, something with proxies. \nThis one is right indeed.. ",
    "AdamDenoon": "I'm using nginx (docker companion) as a reverse proxy. I will use its VIRTUAL_PORT environment variable to get it to listen on 8888. Thanks, and I'll let you know :). That was the issue \ud83d\udc4d I changed 80 back to 8888 in both places and added 8888 as the virtual port in my reverse proxy, and it came up perfectly. You rock!. ",
    "beatgammit": "Lol, nice. Does it sound like \"sucks\" in British English too? Because searx definitely doesn't suck. :smile: . ",
    "Eddi2015": "Yeah I also noticed they changed the lite page. Most likely they did some major changes.. ",
    "stef204": "@dalf \nThe new url fixes Qwant, thanks.\nDDG still broken here.. ",
    "masterzebra": "Hi, thank u soo much for ur help\nIm going to install with docker (i did with classic) and take the correct configuration on settings.yml\nAny suggestion to run a public instance it will be welcome ;)\nThanks again. Well\nI have done with dockers and run perfect, i have used hoellen/docker.\nNow i continue traying to run a public instance\nI suppose that i must install nginx, isnt it?\nAnd other question, i have read some about morty and filtron... i should install one of them too? With nginx or with out nginx?\nThanks for help, im not a expert. Hi @jfowl, any suggestion? I have never done before, and i just want to know how many people use my instance? Thx :). Hi @xinomilo, thank u soo much for ur help\nIm going to try it\nIm thinking that i use Filtron and maybe there are any command to do this inside Filtron. Hi @marathone, maybe i explain me wrong, im not native english speaker\nI dont want track users or any info, i just want know how many people use my instance, without any user information. ",
    "aliceinwire": "@kvch @asciimoo fixed pep8 issues. I think dictionaries need to have their own category. That's true using search special command is really useful.\nThe only problem that I can think of,\nis that having dictionary result coming up in a general search doesn't look right. \nWould be nice to have a dictionary category where I can activate the dictionary that I use, and I can look up on more than a dictionary that I choose.\n. sure, I know about changing it manually. Unfortunately, that means that everyone using my website have to speak my same languages.. you can still use a VPN, tor or sock proxy for keeping anonymity.. what is wrong with the default searx UI?\nI think is pretty similar. https://git.osuv.de/ is the official gitea website? or just an unofficial instance of gitea?. would be nice if the hostname is not hardcoded.\np.s. your https://git.osuv.de/m/dotfiles/src/branch/master/docker/searchx/gitea.py is giving me 500 internal server error. fixed :)\n. ",
    "SilverBut": "@MarcAbonce \n\nsome engines could consider 20 identical simultaneous requests as spam and then block us because of that.\n\nThat is why I did not implement it in the settings menu, neither in the homepage. For now, the only way to send a search with multiple languages is using syntax :en :ja somewords. \n\nI think we should limit the number of simultaneous languages to avoid this risk.\n\nFor self-hosted instances, it may not be necessary, since if you are hosting your own searx instance, you should know exactly what you are doing right now. However, for public instances, this limitation is pretty necessary, but should not be implemented in a settings menu. Instead, this should be a config item in settings.yaml.\n@dalf \n\nthe wikidata engine sends two requests for each language\n\nCould you give me a test case? It would be better if you can also provide what the request IS and what it SHOULD be. I have never used wikidata before, and I just search with some random text like test and english but neither of them returned any result. It just shows Sorry! we didn't find any results. Please use another query or search in more categories.\n\nabout infoboxes : count(infoboxes) = count(languages)*2\n\nBy mentioning infobox do you mean the box in the right column of result page? I will check it.\n\nmatch_language (in utils.py) modification can be skipped\n\nActually I didn't really understand why use match_language here. I had thought it was used to choose a good language code, for example, choose zh-CN for zh. I will modify it as your suggestions, thanks.. agree\u2026\nSent from my iPhone\n\nOn Sep 25, 2018, at 21:07, Ivan Skytte J\u00f8rgensen notifications@github.com wrote:\nIt sounds inefficient.to submit one query per language per search engine. The yahoo search engine supports multiple languages in a single query. The findx search engine will support multiple languages in a single query soon.\nHow about letting the engines/*.py have an optional variable 'multi_language_support=True'. If it exists and true then let the engine deal with the multiple languages, otherwise submit multiple queries.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "isj-privacore": "It sounds inefficient.to submit one query per language per search engine. The yahoo search engine  supports multiple languages in a single query. The findx search engine will support multiple languages in a single query soon.\nHow about letting the engines/*.py have an optional variable 'multi_language_support=True'. If it exists and true then let the engine deal with the multiple languages, otherwise submit multiple queries.. ",
    "beetleb": "Serious question: If I self host on my online web server and only I use that hosted version, what really is the benefit vs installing on my PC (other than I can access it outside of home)? In both cases, there is only one user - how is running it locally on my PC any worse privacy-wise?. @asciimoo \nThanks for the link. Reading it, it seems that from a privacy stance, running locally vs having my own private instance is pretty much the same.\n. ",
    "doronbehar": "Great, that worked. Just for the sake of clearness, I'd like to point out that at first that worked only when I performed searches from the home webpage but not from the browser's address bar. In order for that to work as well I had to remove and add again the search engine from my browser from the about page of my searx host.. ",
    "boapps": "libgen.io was able to find the MD5 hash for the \"12 rules for life\" book. I suggest you switch from bookfi.net to libgen.io for handling md5 search and use this format: \"http://libgen.io/book/index.php?md5=\".. ",
    "beatstick": "If you run your own instance you can replace the default skin by hand and show the dark skin everywhere:\nEnable the dark skin without going to preferences on the browser every time:\n/searx/searx/static/themes/oscar/css/logicodev.min.css << replace with logicodev-dark.min.css (rename it and remove old file)\n. I tried activating infinite scroll as well. I still only get one page of results. It makes no difference.. ",
    "WisdomCode": "I am no developer of this app, however, I meddled with this stuff and I've written a search engine into my config, so here comes an attempt to explain:\nAll the modification is done in the settings.yml, maybe known from the setup. Further down in this file, all the search engines are listed. You may extrapolate any information I may miss taking these as example values. Also, all of this takes image searches out of account, they are all made with custom engines, briefly mentioned below.\nTo add a new search engine, Start a new line with a - . \nEvery further value is added in this syntax: value type : value\nNow a list of value types:\nname: Some name for your search engine, like reddit, google, wikipedia\nengine: this is normally xpath, however, there are further options, I assume you choose xpath here\npaging: In doubt, leave false, allows to search also page 2 etc. For this to work, the value {pageno} has to be given for the search_url (see below)\nsearch_url: The url of the search, you have to firgure out how the url looks like if you search something. The part of the url that contains the search query has to be substituted with {query}, if you can search over multiple pages, the pagenumber has to be replaced with {pageno}\nurl_xpath: This is necessary if the engine is xpath. You leave here the xpath that will return all search result links (urls). To see what an xpath is/how it works, see https://www.w3schools.com/xml/xpath_intro.asp.\ntitle_xpath: This is necessary if the engine is xpath. The same as url_xpath, however, instead of the result links, this returns the titles of the results.\ncontent_xpath: The same as the above two, however, it returns the content (the short preview text most search engines provide).\ncategories: This presents in what categories you search. This may be general, files, music, news, videos etc.; you may add multiple, comma seperated ones to add to multiple searches. \ntimeout: The maximum amount of time the search may take, otherwise this engine is ignored. You have to time how long the site takes to load and take this length here. Giving too much time may slow down the entire search. \nshortcut: A shortcut for your search engine, wikipedia may be wiki etc.\nThis is not entirely exhausted, there is also a kind of javascript engine that is used in some occasions, and you may even write a completely own python script for extracting the information instead of the xpath.\nIf you write such a script, just call your engine like your script (without the py). Such engines are found in the engines folder, you may work out how exactly they work there (and add it here if you want). However, most of the time, xpath will deliver everything of use from the html file. \n. ",
    "tmikaeld": "The problem is that the sites are not enabled when they are in /sites-available/ and you can see in the tutorial that you linked to, that he symlinks from the sites-available towards sites-enabled to enable the sites.\n\nSo if that's what you want for searx, then:\nsudo ln -s /etc/nginx/sites-available/searx /etc/nginx/sites-enabled/searx\nShould be added to the instructions.. There is also different install instructions on github readme than on the github pages.\nI'm specifically talking about this one:\nhttps://asciimoo.github.io/searx/dev/install/installation.html#installation\nThat is linked from here: https://asciimoo.github.io/searx/index.html. @Pofilo Interesting discussion. So we can conclude that:\nsudo ln -s /etc/nginx/sites-available/searx /etc/nginx/sites-enabled/searx\nShould be added to the docs.. @kvch Done.. ",
    "henriquecapeleiromaia": "I have noticed the same behavior recently. Also running a self-hosted 0.14.0. Infinite scroll is on. . ",
    "jordemort": "Seeing the same problem on my instance (0.14, just refreshed my container from master today). ",
    "aCLr": "I've got the same error; 500 on http request.\nHere is form data:\ncategory_general=on&language=en-US&q=inoreader&time_range=\n. ",
    "slackyman76": "Hi No\u00e9mi,\nthank you for answering.\nI don't remember my queries... I tried several keywords for several\nsearches, but for about one or two minute(s) searx.me http://searx.me\nrefused to give me results, displaying that error message. Then it started\nto work again but sometimes it happens that I have to switch to a different\ninstance. Not a big issue, but I would like to know if that issue can be\nrelated to a searx.me http://searx.me overload.\nCheers,\nBert\nIl giorno mar 9 ott 2018 alle ore 22:58 No\u00e9mi V\u00e1nyi \nnotifications@github.com ha scritto:\n\n@slacykman76 what was your query? have you experienced this problem with\nthe same query on a different instance?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1405#issuecomment-428349389,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ApsjB_uCHzcgjtGlWKtNl48skeQTEajbks5ujQ3fgaJpZM4XA7Ra\n.\n. \n",
    "mrozo": "can you describe the problem? how to reproduce it?. ",
    "fauno": "i wouldn't know how to reproduce it because it's an internal error, but\nthese are the steps:\n\ngo to searx.me\nenter a search query\nsearx.me fails with \"error\"\nsearch box is empty\n\nit's a POST request so the query isn't part of the URL\n. ",
    "ZEROF": "Hi @asciimoo, can you check 50% of pull requests for start :) ? From what I can see a lot of people fixed majority of bugs in v0.14, community is around project. Hope you can find time for it.. Same error here, once when clicked on \"File\" search. . ",
    "veloute": "in the top right corner click preferences, then turn on safe search to either moderate or strict and see if that fixes your problem.. no problem, glad to help. :+1: . ",
    "WiseWomanVP": "Engine: https://searx.me/\nGood, moderate is good enough to reduce the porn hits to just one (at the top), strict makes them all go away. Now I get lots of dictionary suggestions at the top :) Thanks!. ",
    "wonnetz": "In Preferences/Engines/files, the default file search engines are PirateBay and Torrentz. These websites frequently go down and thus the file search option will fail. \nYou can change the Search Engine Preferences for Files to a couple different other things. I added the Google Play Search engine and tested to see if it would search for Clash Royale and it worked. . @kvch \nI switched to a different instance which is on version 0.15.0 and Bing Images is working, but Google Images is not working.\nThis is the new error I am getting: \n\n. ",
    "stemy2": "Yes.. ",
    "drakex90": "Would definitely be a good addition.. bumping this as it has been quite sometime these two haven't been working.. ",
    "massimilianoLe": "Hi there,\nI have the same exact issue as @Perflyst \n. After i added the lines 267-269 of this file https://github.com/asciimoo/searx/blob/master/searx/preferences.py\nit worked :-). ",
    "SitiSchu": "Same issue here tried with quotes as well to no avail. Got this in the logs though:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/weakref.py\", line 117, in remove\nTypeError: 'NoneType' object is not callable\nException ignored in: <function WeakValueDictionary.__init__.<locals>.remove at 0x7f2413b96950>\n(Its not really helpful but its all there is)\nIf I set the debug key to True it suddenly works so I can't provide the debug output.. Ok this can be fixed by installing the npm dependencies and building the styles. Then it works with logicodev-dark and debug mode turned off.. ",
    "joseph-cheung": "@dalf  It is said only to make a GET request to the URL will you get the decrypted one which is redirected to. But I'm too naive and inexperienced to make it. So It would be considerate of you to give us a hand. And thank you for your attention.. ",
    "wizzwizz4": "Can searX proxy the link? https://searx.com/blahblahblah/http://baidu.com/blah redirects to https://searx.com/blahblahblah/http://searchresult.org/result until a GET request stops bouncing around the web, then it redirects to the actual URL.. ",
    "Kris-ping": "marked because I need baidu avaiable.. ",
    "sorenstoutner": "The 0.15.0 release seems to have improved the state a bit, with terms like \"test\" no longer pulling up hard core pornography on the first page.  But there is still room for improvements in the default settings.. I have though a lot about that and will likely do so at some point in the future, but it isn't an option for me in the short term.  Are you aware of any public instance that has safe image search by default?. > I see two rather easy, but whacky solutions:\n\n1. Saving the safe-search preference into the search-url\n   ![image](https://user-images.githubusercontent.com/15788906/52870231-41e8fe80-313f-11e9-8a5d-4f6bd0eb1e7e.png)\n\n\nI have played around with saving preferences quite a bit.  But it can't accomplish what I need for two reasons.\n\n\nIt doesn't work correctly (perhaps I ought to file a separate bug about this).  If I use the custom URL to load a page or perform a search, it does so correctly.  But the first time I click on any link that leads to another part of the searx.me website, the preference string is stripped from the URL and the default settings are loaded.  So, for example, if I use the custom URL without the ?q= at the end as a homepage, as soon as I do a search using the search box on the screen the custom preferences are stripped out and I get a search with the default settings.  If I use the URL with the ?q= as a search query, the results under General use the saved preferences, but as soon as I switch it to Images the saved preferences are stripped out and a search is performed using the default settings.\n\n\nEven if the problems described in number 1 were resolved, using custom preferences doesn't work well for my purposes.  If I set a custom URL as the default in Privacy Browser, than any user can edit those settings and they would apply to everyone else who uses the browser.  If I include instructions for users to create their own custom URL then whoever is hosting the Searx instance can track individual users over time.  That is exactly the sort of tracking I am trying to avoid.\n\n\n\n2. Overriding cookies for searx.me by _you_:\n   ![image](https://user-images.githubusercontent.com/15788906/52870294-6e047f80-313f-11e9-9f87-0754da34762b.png)\n   That way, your users must not do that on their own.\n\n\nI do not currently have the ability to allow some cookies and disallow others for a particular domain, nor do I have the infrastructure in place to inject a specific cookie for a particular website.  Although that will be possible in a future release with the 4.x series of Privacy Browser, that will involve forking Android's WebView to create Privacy WebView, and isn't a good solution for the short term.\n\nBoth of them can be found here: https://searx.me/preferences.\nWhat are your requirements for a privacybrowser searx-instance?\nWould you be willing to add a DNS CNAME/A entry of search.stoutner.com for that?\n\nI can easily add a CNAME/A entry for search.stoutner.com that points to a Searx instance that has these as default settings.  I am just not in a position to host one myself right now.  Going forward, I intend to host my own instance and likely contribute to the project.  But even when I do, I assume that there are a number of people, besides just those who use Privacy Browser, who would like an easy URL to safely search images.  Something like safesearch.searx.me or safe.searx.me or clean.searx.me would be a nice addition to the standard build.\n. ",
    "n-o-o-b": "SafeSearch should be set to Moderate by default\nwould be nice to not have to see heads with holes blown through them when searching for something. ",
    "brentadamson": "It's undocumented but our API will be faster than scraping: https://jivesearch.com/?q=bob+marley&o=json. The API will always have a generous free tier, btw.. For right now it is. Eventually we'll have our own index but for right now we use Yandex.. ",
    "mahui625": "How do I uninstall this software? I want to reinstall the software again.. Thanks!\nxinomilo notifications@github.com \u4e8e2018\u5e7411\u670822\u65e5\u5468\u56db \u4e0b\u534811:46\u5199\u9053\uff1a\n\nremove folder /usr/local/searx, and start a new installation\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1435#issuecomment-441067916,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADNS63O_NqyLr3anfHYqkiMplxRAbqhwks5uxsbagaJpZM4YXwr8\n.\n\n\n-- \noffice site:http://www.act-cn.com.cn/\nhttp://mahui625.taobao.com/\nMSN:mahui625@hotmail.com\nSkype:mahui625\nPaypal:mahui625primax@126.com\n. ",
    "pExeen": "It seems like the syntax cffi!=1.11.3,>=1.7 that cryptography uses is not supported by pip 10.0.1.\nIn the Dockerfile file you can replace the line && pip install --no-cache -r requirements.txt \\ (line 38) with && pip install --upgrade pip && pip install --no-cache -r requirements.txt \\. ",
    "ceptonit": "\nIt seems like the syntax cffi!=1.11.3,>=1.7 that cryptography uses is not supported by pip 10.0.1.\nIn the Dockerfile file you can replace the line && pip install --no-cache -r requirements.txt \\ (line 38) with && pip install --upgrade pip && pip install --no-cache -r requirements.txt \\\n\nThat did the trick, thanks man!. > Should we open a merge request to add this quick fix ? Or maybe adding it to the doc as it is an upgrade of pip.\nThe merge request would be the cleanest way.\nI'm not sure how to do that though.. ",
    "bmagic": "I make a pull request with your solution. \nhttps://github.com/asciimoo/searx/pull/1449. The important thing is that it is integrated! Thanks.. ",
    "oskapt": "Cloudflare is the largest MitM proxy on the planet, and despite them saying that they don't look in the stream while they re-encrypt it, there's no proof that this is true. If they have unencrypted access to massive amounts of data as it traverses the Internet, there is no chance that state actors aren't foaming at the mouth to get access to that data. \nAnything that uses Cloudflare should be banned, blacklisted, and boycotted.. ",
    "immanuelfodor": "Maybe he could appoint/ask for co-maintainers to the repo who can merge, test and fix the many pull requests. This project should be kept alive, many people uses it and its everyone's best interest to protect our privacy. . @asciimoo would an unofficial searx fork with the same name be okay for you, Adam? We/some of the guys here or at #1409 would test and merge many of the existing PRs, and then create one huge PR for you to merge as 15.0? How is that sound to ease up the maintaining process for you? I'm/we're asking it with all due respect for your work so far and in the name of keeping this project alive, useful and prosperous. . Works great in Docker, thanks for the weekend's efforts! :). ",
    "FrostKnight": "Yes, I agree with this, he needs co-maintainers if he busy.. I hate to say this, and I mean, I REALLY HATE to say this... but searx may need to be forked if he doesn't find a co-maintainer.  It would be desperate though... \nI really think most would prefer to have this not happen.  but sooner or later, someone has to do something. Of course, you could make an unofficial searx fork until he gets back on his feet so to speak. Ask him if that's okay, same name, but different.. ",
    "asddsaz": "@Pofilo I'm not a \"UI\" expert but look at Google:\n\n\nIt looks very welcoming\nNow look at DuckDuckGo:\n\n\nAgain, very welcoming (just me?)\nNow look at SearX:\n\n\nIt is very dark, and to most people unfamiliar.\nI would say this is why search engines like DuckDuckGo are so popular.\nI'm not saying to duplicate Google, I just think SearX is very unfamiliar to new users and may persuade them from not using it.. ",
    "gjhklfdsa": "@asddsaz @Pofilo Jive Search (a free software SearX competitor) has a similar UI to google.\nJive Search: https://github.com/jivesearch/jivesearch. ",
    "markuman": "@aliceinwire the official site of gitea is https://gitea.io\nbut there is no official public hosted gitea service like gitlab.com. everyone can host gitea itself. therefore, you must replace https://git.osuv.de with your gitea domain.. @aliceinwire \n\np.s. your https://git.osuv.de/m/dotfiles/src/branch/master/docker/searchx/gitea.py is giving me 500 internal server error\n\nMy mariadb was not available because I needed to fix some other errors on my host. :)\n\nwould be nice if the hostname is not hardcoded.\n\nI have no Idea if it's possible to pass a variable from settings.yml to the engine file gitea.py. Do you know if it is possible?. Indeed. My gitea instance in running in a docker container in docker swarm mode behind a caddy proxy. For some reason, this setup results into a timeout. Butinternally my searx instance (also running in a docker container in the same swarm) uses the internal docker network. That works.. #1447. ",
    "hobbestigrou": "Please do can you update your commit message and referrer the ticket.. ",
    "idnovic": "both. I think when the user just does a search the first should be used. But If a user specifies a timerange the 2nd.. ",
    "SirTerrific": "\nfile qwant.py\nline 13\nWhen adding q=\nurl = 'https://api.qwant.com/api/search/{keyword}?count=10&offset={offset}&f=&q={query}&t={keyword}&uiv=4'\nIt's working again\nSorry figuring out how to push this\nRegards Jos\n\nNice, thanks. It works.\nWe need to fix startpage too. ",
    "rla": "This is not a bug. The GitHub API responses are paginated and the current repository happens to be on non-first page. You can try: https://api.github.com/users/asciimoo/repos?per_page=50&page=2. ",
    "martinvahi": "My mistake. Sorry. I also received same kind of explanation from the GitHub support. I thought that it's like SQL, where by default everything is returned and if one wants to avoid crashing due to lack of RAM, one explicitly describes a limit, \"count\", as part of the query, but it seems that the GitHub API uses an opposite approach.\nWell, at least it's not explicit censorship and that's a really nice thing to know. As it turns out, the \nhttps://api.github.com/user/20240/repos?page=2\ncontains the string \"searx\".\nSorry. My mistake. Also I thank everybody, including the GitHub support, for pointing out my mistake.. ",
    "milouse": "rebased on master. ping @dalf for a quick review if you want to :). rebased on master. I think you can close this issue now. ",
    "rnhmjoj": "I have configured searx to use the wolframalpha_api engine with a valid api key (I tested it with curl). Enabling the science category any query results in this error. . Thank you!. ",
    "slakedavid2": "Okay thanks for the quick response. ",
    "DD8372812": "Hello @joshu9h @xinomilo ,\nthanks for your answers.\nI am running 0.14.0 in Docker container wonderfall/searx.\nNow I tried hoellen/searx, it's on 0.15.0. Startpage runs now although it's marked still as \"not supported\". But: I'm running searx through a TOR proxy, SOCKS5. In wonderfall/searx it worked but not in hoellen/searx :( it seems searx ignores the proxy settings (I see regular internet connections on my firewall on 443). Need to ask for that in hoellen/searx board.. ",
    "Freeland3r": "I have done it the classic way, ok will check out the uwsgi and webserver instructions\n. systemctl status uwsgi.service tells me searx is invalid user. yep i did the user has been created\n. btw what is the password for the user searx?\n. Ok found a solution. If i dont have anything running in the port 80 (which means no nginx or apache), then i can change the settings.yml file to port 80 and it will display the searx on the domain.. ok so found the solution, i have been following the installation guide thank you. If i put the searx/webapp.py to nohup it works in the background and never shuts down. ",
    "georg-mayer": "Hello,\nI'm extremely sorry that I did not close this issue properly. I solved it a few days after posting, but forgot how. \nThanks for your help!\nCheers,\nGeorg. ",
    "marathone": "Isn't the point of users wanting to use a searx instance, the fact that\nno tracking is done? In fact my instance of searx states explicit that\ntracking\u00a0 isn't done.\nOn 2019-02-03 1:55 p.m., masterzebra wrote:\n\nHi @xinomilo https://github.com/xinomilo, thank u soo much for ur help\nIm going to try it\nIm thinking that i use Filtron and maybe there are any command to do\nthis inside Filtron\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/asciimoo/searx/issues/1492#issuecomment-460077936,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AUF9bYuugLNb5R540ilw8lb8dCPeecUyks5vJzC9gaJpZM4aKatE.\n. \n",
    "follower46": "I'm assuming that you're running searx through flask with the python command.\nIf you're wanting to test it or try modifications to the code (not run it in production) that's fine. You could use screen to keep the command running after you close the terminal.\nIf you're trying to deploy this you'll want a webserver to actually deliver the UWSGI application (like nginx or gunicorn).\nYou can find an installation guide here: https://asciimoo.github.io/searx/dev/install/installation.html#id8. ",
    "Ardakilic": "I've the same issue, current git master:\n\ngoogle images (unexpected crash: line 1079: htmlParseEntityRef: expecting ';' (line 1079))\n\nSometimes I also get this:\n\ngoogle images (unexpected crash: line 30: Tag footer invalid (line 30))\n\nAnd after all, I can't see any results from Google images.. @kvch \nFor example. Search for tremolo , and click images. I see this issue, but this is in every search result for me. I'm using the latest git master as of now, on my self-hosted server.. ",
    "its3ch0": "I'm using the instance bin.disroot.org and I'm getting this notice:\nEngines cannot retrieve results:\ngoogle images (unexpected crash: No JSON object could be decoded), bing images (unexpected crash: expected string or buffer), 500px (unexpected crash: 'photos'). ",
    "MAL-1": "I'm getting the same messages on my instance using Qwant image and Google image search, works fine using Unsplash so it seems to be something specific with those engines.. Add it to the https://github.com/asciimoo/searx/wiki/Searx-instances using the same format as everyone and they'll update it from there.. ",
    "ofcourseican": "Can also confirm that with this change the latest master is building correctly. \ud83d\udc4d . ",
    "EdisonJwa": "I was got the same error on Ubuntu 18.04 and Debian 9\npython 3.6 and 3.7. ",
    "ppss1806": "Through many attempts, I found that this problem comes from the Chinese selected by \"Interface language\".\nMy temporary solution is to change the \"default_locale\" value of the \u201csettings.yml\u201d to be non-Chinese.. ",
    "1265775896": "\nThrough many attempts, I found that this problem comes from the Chinese selected by \"Interface language\".\nMy temporary solution is to change the \"default_locale\" value of the \u201csettings.yml\u201d to be non-Chinese.\n\nIt works! Thank you so much!. ",
    "iiivii": "yes,I also have the same problem.. ",
    "wenxuanjun": "The theme is oscar. ",
    "juanfra684": "If you're asking what the web owner see as your origin, search \"what is my referer\" in searx and click in \"www.whatismyreferer.com\". That web will show you what is your browser sending to the web owner when using searx.\nOtherwise, if you want to know if google knows that you're searching something or using results from google, then no, they don't know.. ",
    "CaptainFrosty": "Appreciated.. ",
    "volth": "In settings.yml I just undid https://github.com/asciimoo/searx/commit/b72aec0a9b2b548d7f6a8ddecedd58f5392b8372. ",
    "colddaytk": "Oh,I am sorry to have delayed your coffee time.It may be a firefox problem,when I use Chrome or EDGE,there is no problem.. my searx is 0.15.0,when I click preference in firefox this problem will appear,but there is no problem when I use chrome or EDGE.. ",
    "jsthon": "nice work!. ",
    "aleex5": "there are some search engines that no longer work correctly, just as there are some search engines that do not work with certain languages, they should improve that. ",
    "chinapao": "\nAdd it to the https://github.com/asciimoo/searx/wiki/Searx-instances using the same format as everyone and they'll update it from there.\n\nDone.  Thank you very much.. ",
    "yohanboniface": "You may want to request Photon, which adds search as you type and fuzzy matching on top of Nominatim: http://photon.komoot.de/\n. ",
    "nicholasks": "I think it's a good idea. I'll make those changes.. ",
    "mweinelt": "Arch Wiki?. ",
    "pelag0s": "As those variables should be empty by default, this is technically not needed. But it is probably cleaner to define them nonetheless. I adjusted it accordingly.\nThanks for the hint.. "
}
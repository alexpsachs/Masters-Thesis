{
    "bd808": "Awesome work by the way. Your UI is miles ahead of the default one.\n. You're welcome. I'll do more when I get a chance. I dumped quite a few hours into my own UI for logstash but yours is progressing faster and getting really close to what I was wanting so I think the universe will be better served if I can help you along.\n. @cybervedaa kibana 3 has this and a lot more\n. This diff is pretty hard to read. Viewing the end result file may be easier.\nI'm not entirely happy with some of the method names, but I do think this is a step in the direction of a more maintainable flow. Doing all the work in one big global namespace gave me the heebie jeebies.\n. ",
    "rashidkpc": "Wow, thanks! I kept meaning to go back and do that. There's still a lot of logical cleanup I should do to make it easier to expand on and get rid of some duplicate code.\n. [master fd520dc] issue #6 - forced query. Added filter_string to config\n 2 files changed, 11 insertions(+), 1 deletions(-)\n. Or pass timezone in the json response\n. If you set date.timezone = UTC in php.ini does it work right? There might be a flaw in the way I select which indices to query. \n. Added in kibana-ruby  59289da\nTimezone can be set to 'user' to allow the user's browser to decide, or it can be set to a timezone string. \n. [master 1308a42] added results_per_page to config.php to make results per page configurable\n 3 files changed, 12 insertions(+), 5 deletions(-)\n. The pull request would be much appreciated.\n. Added try/catch for delete to fix the 'object doesn't support this action' issue. There's still some code in the recently added dateFormat function that ie8 doesn't like. Will investigate more.\n. You can pull master, however IE8 is still broken due to the dateFormat function. \n. Dance party: IE8 tested and seems to work. Safari too. I'm going to close this for now. If anyone finds issues with other browsers, open a new issue. \nCheers\nRashid\n. [master b2a8149] removed console.log lines\n 2 files changed, 11 insertions(+), 11 deletions(-)\n. I like this idea, I'm just not totally sure the best way to implement it. Trying to detect the field type from the ES mapping seems overly complex, plus the same field could possibly be defined as two different types. I'm thinking the best way would be to offer the option regardless of field type and simply tell the user it failed if facet fails. \n. May have to back this out. The stastical facet is proving to have the same issues as the terms facet, it can run elasticsearch out of memory and crash the cluster. Darn.\n. Glad its working for you. Be careful attempting to run it against non-numeric fields until I get some kind of logic in there. \n. This is long closed, but there is some logic in the ruby version to determine the field type without having to run the facet first.\n. thanks for the pull, looks good!\n. Fixed @ 7bacbc266c. Thanks!\n. I like that the graph shows the entire period searched instead of only the results, though I prefer the larger search bar as it makes complex searches easier to read. Couple other things:\nThe 'All Time' selection displays a funky graph since All Time = 100 years. There would need to be some coordination with the backend to remove the time filter and search _all. \nIn mozilla the contents of the nav bar appears to be a static width. At large resolutions you miss out on a wider search bar, at small resolutions the contents wraps to a new line sooner.\n. Thanks! Someone was asking about this in #logstash\n. Comment above referenced the pre-existing hash commit. I like the idea of being able to adjust bar granularity, though I'd like to rework it a bit. Does the legend accurately reflect the span of the bars?\n. We've retired the PHP version, but I still like this idea. Might look into doing the same thing with a new graphing library.\n. Note to self: This will affect any field stored as an array. \n. Fixed in kibana-ruby  9fee2db\n. Fixed, this broke with segmented loading. Now fixed.\n. closing as not reproducible, reopen if this reoccurs\n. This is an elasticsearch issue. The tokenizer will ignore most/all non-alphanumeric characters. You'd need to either set that field as not_analyzed or create an Analyzer that uses a different Tokenizer. \nThis might help:\nhttp://www.elasticsearch.org/guide/reference/index-modules/analysis/\n. Had to remove the code for the pull as it broke the this/not-this button functionality. Consider adding into the kibana-ruby branch?\n. Implemented in kibana-ruby e95dbe3f47\nMaybe, can someone test this for me? It seems to work for me, but I'm testing with a string containing json, so I'm not totally sure. \n. Likely an out of memory condition, increase your php memory_limit\n. php5-curl installed? Maybe open up a javascript console and see if there's any error there?\n. I wonder if its having trouble contacting your elasticsearch server? You might try finding me (or someone) in #logstash on freenode for some realtime debugging.\n. Fixed in kibana-ruby 4433601\n. I like where this is going!\n. As the php version has been retired I'm closing this. If you're still interested there is currently a fork of kibana-ruby that is implementing a flexible auth system, thanks!\n. Added to kibana-ruby 7bc2b05\n. Hmm, not sure what might be going on, but this doesn't quite function for me. Initial request works, but I don't get an index selector and subsequent search/reset operations fail. This is on Firefox 14\nThat chosen plugin is pretty slick though. I might have to steal that for the field selection section.\n. Output of getindices was [\"logstash\"]\n. Request => {\"search\":\"\",\"fields\":[\"\"],\"offset\":0,\"timeframe\":\"15 minutes\",\"graphmode\":\"count\",\"index\":null,\"stamp\":1345043134193}\nResponse => []\nNo errors. Maybe try to catch me on IRC today and we'll work through it?\n. As we've retired the php version and the js has significantly diverged I'm going to close this, though do I like the idea of better type seperation in kibana\n. Added in kibana-ruby  99d1856\n. Added in kibana-ruby    0c99dec\n. In the current version it would likely be quite an endeavor. A newer ruby based version should be out within a week with better query building and request making classes.\n. Try this: git clone --branch=kibana-ruby git@github.com:rashidkpc/Kibana.git\nI'll add the above to the README.md\n. Added in kibana-ruby 9ff3fe0\n. I'd like to do this, but make it configurable and disabled by default in KibanaConfig.rb\nSome day I'd like a switchable backend to allow people to make multiple accounts and tie them to filters\n. Awesome! Any interest in posting a sanitized example of your passenger/ldap setup here?\n. Just fixed the export thing, wasn't sending the headers. Gonna go ahead and close this for now. \n. Oops, thanks!\n. It appears that it can not resolve the name of your elasticsearch server. Be sure to set it in KibanaConfig.rb. Find me on irc.freenode.net in #logstash if you need more help\n. Yeah, this started with PHP 5.4 which turned on E_STRICT by default. I merged a pull to master that perhaps fixed the object creation warnings. Not sure if you're running a tagged version or master. You'll need to set the timezone in php.ini or in config.php near the bottom to avoid the timezone warning. \nEarly next week I'll be replacing master, currently PHP, with the shiny new Ruby version from this branch: https://github.com/rashidkpc/Kibana/tree/kibana-ruby\n. I run it under passenger with Apache, but it uses the Sinatra framework and can run its own little standalone web server. \nI'll look into packing up the whole shebang with jRuby, much like logstash.\n. Gonna close this since the PHP version is deprecated. I do want to wrap up Kibana in jRuby though.\n. This might be fixed, there was a bug in the current_index function that was affecting the stream\n. Ah yeah, that seems likely, I'm guessing you have Smart_index = false set in KibanaConfig.rb? I missed adding the check for Smart_index to the current_index function. I just added it and committed it. \n. recent pull from r_duran made smart indexing configurable\n. Couple options. If you want to redirect to stdout:\nruby kibana.rb 2>&1\nOr to a file:\nruby kibana.rb 2> stderr.log\nAlternatively, you can run kibana with rackup and a config.ru file via 'rackup config.ru'\nExample config.ru w/ logging:\nrequire 'kibana'\nrequire 'rubygems'\nrequire 'sinatra'\nset :environment, ENV['RACK_ENV'].to_sym\nset :app_file,     'kibana.rb'\ndisable :run\nroot_dir = File.dirname(FILE)\nset :root,root_dir\nstdout = File.new('sinatra-stdout.log','a')\nstderr = File.new('sinatra-stderr.log','a')\n$stdout.reopen(stdout)\n$stderr.reopen(stderr)\nrun Sinatra::Application\n. Oh yeah, fair to say this is done. switched to dot notation \n. It appears that it can not contact your Elasticsearch server. Make sure Elasticsearch is set in KibanaConfig.rb. If you continue to experience issues come see us in #logstash on irc.freenode.net\n. Haven't been able to reproduce this, though I did fix a but that affected streaming in IE today that could have been the cause. Going to close for now, if this is still happening with the latest version please reopen with your browser/ruby version\nThanks\n. Current plan is to make the Time field sortable in the interface vs a config option. \n. An interesting point, that does sound like a case for being able to set the default order. \n. I've added a Time_formation option to KibanaConfig.rb with b613b12\nSyntax is here, though I also included an example for iso8601 in the config file.\nhttp://blog.stevenlevithan.com/archives/date-time-format\n. thanks for the pull!\n. Fixed in    2ce740b\n. I like this, haven't had time to test. One concern is that it seems like it would break on searches for events that contain a | character\n. Appreciate the feedback from both of your guys. I went ahead and merged awheeler's pull request and pushed it out to the demo site. Thanks again to both of you :-D\n. What version of Kibana and Safari? I just loaded up the latest commit to the kibana-ruby branch in Safari 6.0 (7536.25) and things seem to look ok\n. Was able to reproduce. Issue with date parsing, sort of surprised that didn't show itself in older versions of Safari, odd. Fixed.\n. Tested on 6.0.1 on Mountain Lion, not able to replicate.\n. Presumably @fubar would be a top level field that exists outside of logstash's @fields?\n. Added an events/sec rate graph to the stream page. Still want to make a live button though. Watch for the graph to fill in from the right in the bar at the top.\n. Thank you for grabbing one I missed!\n. Hmm, I havent experienced this. What version of ruby are you on? Do you see the same behavior when running with KibanaHost set to 0.0.0.0?\n. Some digging led me to webrick's :DoNotReverseLookup setting. By default it is set to nil, setting it to true should resolve the issue. I'll look into a way to add this to Kibana\n. Per jordansissel's recommendation I added 'thin' to the gemfile so that webrick wouldn't be used by default. Should fix this\n. You're right, it somehow made it out. I readded it to kibana.gemspec. Do a pull and a 'bundle install' and kibana should switch to using thin instead of webrick\n. Great improvements! Thanks for taking the time out to figure that out and make it feel seemless. \n. Also, the changes are up on http://demo.kibana.org:5601 \nThanks again\nRashid\n. What browser were you seeing buttons only appearing for numbers in? I have it blanking out the buttons for fields that contain JSON objects since the buttons don't work correctly for fields containing objects. Though I'd like to fix that.\n. Probably better to fix oddly stored arrays, I'm guessing you'd rather store x-forwarded-for as an array of IPs than an array of quoted strings. The better way to fix this would really be for me to come up with a way to properly handle nested objects\n. This is definitely a bug, but I had to pull the change back out since it causes all arrays, even ones with a single element, to be displayed as:\n[\n  'something'\n]\nWhich breaks clickable query building. \n. Thanks for this! I made a few visual tweaks and removed the requirement for user_interval to be in the hash to avoid breaking existing bookmarks. \nI'd still like to tweak this further by adding a calculation of which options should be made available in the dropdown based on the time span. As is, if a user selects a huge range and groups by 'second' it can bog down their browser something fierce.\n. I'm not able to replicate this, do you get this every time you use the rss feed or only in some situations? Are you seeing the same behavior on http://demo.kibana.org:5601 ?\n. Pretty sure I fixed this, not sure why I couldn't replicate it, but it was likely related to the export issue you filed and occured when no fields were selected. Thanks for the tip!\n. Good find, this happened  when no fields were selected. Fixed in 4876699\n. Killer, thanks!\n. I mulled this over for awhile, but I think I'm going to stick with the ruby for now. A lot of users implement logic in ruby (for better or worse) for config parameters and I don't want to rob anyone of that. I did grab the Gemfile/gemspec changes though, thanks for those!\n. I'll probably stick with Sinatra. The backend of Kibana is so simple I feel like rails is overkill. \nIf you're interested in auth, jimi1283 has written some auth functionality here: https://github.com/jimi1283/Kibana I haven't had time to investigate it, but he's written it to use flexible auth/storage modules so in theory is could connect to any backend. I'd like to write Elasticsearch and maybe a simple file based backend before merging it in.\n. Thanks for the pull moll, just merged\n. Solid change, the libcurl dependency sucked\n. I'm not sure Kibana is the layer to do this on. You might be better off using statsd/graphite and alerting on that. I'll leave this open because I feel its valuable, but if its something you need soonly Kibana isn't really architected for this sort of thing. Thats not to say it won't fit at some point though.\n. I'll start by saying: This feature is important and well, inevitable. \nAs untergeek pointed out, Elasticsearch provides no security at all. While I take serious issue with that, there exists little other option for a transparent implementation. Logstash and Elasticsearch are Kibana's only absolute infrastructure dependencies and I'd like to keep moving parts to a minimum. \nThat said, when we do implement this, I would like to base it on the work done by @jimi1283 and @awheeler who have designed the start of a fantastic, and flexible, auth subsystem. We could provide the same sort of modularity for stored queries, with a choice of storage backends and Elasticsearch as the default. If someone wants redis, or mongo, or sqlite or X, it should be fairly easy to make that happen. \nSide note: Firewalls aren't a perfect security paradigm, but they're the best we have for Elasticsearch right now. I suggest firewalling off your ES cluster at the host level to only be accessible from hosts that need it. Puppet + exported resources + firewall module make this pretty convenient \n. Duplicate of #89\n. This should be fixed with the switch to dot notated fields\n. Kibana listens only on localhost by default\nSee KibanaConfig.rb line 16. Change 127.0.0.1 to 0.0.0.0 to bind to all addresses\n# The adress ip Kibana should listen on\n  KibanaHost = '127.0.0.1'\n. Looks like this is caused by kibana being unable to determine the type of the field requested. I'll investigate adding more error correction tomorrow. Do you experience this on every field? Or only syslog_facility?\n. What version of ES is this happening on?\n. Just committed ahweeler's potential fix, however I haven't been able to replicate this issue personally. Can someone give it a go?\n. Rad, thanks awheeler Closing this\n. mmm yes, I also want this feature. Shouldn't be hard to implement, flot supports stacked charts.\n. Agree this is important. I'll be merging this in the next few days. Thanks!\n. Done and done. Thanks!\n. Possible and not very hard. The challenge would be deciding what to highlight. Can't highlight everywhere, Elasticsearch doesn't do highlighting on _all. Could we just highlight on the user's Default_fields ? Though that would create some amount of confusion when a search returns results where the matched value is not in the default_fields. \nThoughts?\n. I've tried to answer this like 5 times and keep getting pulled away. \nThe problem: A lot of folks log XML, so Kibana escapes XML entities, including html tags so that they display correctly (function xmlEnt()). It also inserts invisible word break characters in a, sometimes ineffective, attempt to prevent horizontal scrolling (function wbr()). An unfortunate side effect of dynamic table column widths.\nMaybe solution: A custom tag. Could do something like @KIBANA_HIGHLIGHT_START@ and @KIBANA_HIGHLIGHT_END@. This would solve the xmlEnt() issue, but wbr() will need to be updated to not break those tags\nOrder of operations:\n1) Check for highlight field in ES response, if found, use this for the fields that contain highlights\n2) Pass to xmlEnt/wbr\n3) Replace our custom @@ tags with html tags\n4) Use non-highlighted field for the raw span\n5) Display highlighted field\n. Very very awesome. I'm going to take a look at this later today, but I'm really excited about this. Thanks!\n. I pulled this into the nitpers-pie-chart branch. We're working on some bugs in the analysis screens right now, but I'll merge it into kibana-ruby just as soon as those are taken care of.\n. this shouldn't be hard:\nvar canvas = $('#graph canvas')\nvar img    = canvas.toDataURL(\"image/png\");\n. Ok, shouldn't have been hard, but flot doesn't render the legend as part of the canvas. Hmmm. \n. Progress on this can be tracked in the kibana-ruby-auth branch that @jimi1283 and @awheeler have been working on. It is designed to be modular. An http auth module would be pretty trivial once we're ready to merge it in\n. very nice, thanks\n. Should be fixed, seems to have been caused by an unused polyfill. Tested in IE8 and everything looks ok now.\n. Definitely a bug, introduced with the new timepicker. Should be fixed, thanks!\n. This can be done by setting the KIBANA_CONFIG environmental variable\n. What does your KibanaConfig.rb look like?\n. Grab me (rashidkpc) on #logstash on freenode and we can debug it.\n. Closing as can not replicate. Find me on irc we'rll debug\n. The double spaces at the end of lines is intentional, Markdown treats them as line breaks\n. Fixed, thanks\n. Hmm, I can't seem to replicate this. Can you grab me on IRC in #logstash tomorrow and we can debug?\n. This comes down to the default elasticsearch terms analyzer for that field. You have a couple options\n1) change that mapping of that field. Here's untergeek's blog post on his ES mapping. See the bits about 'not_analyzed' http://untergeek.com/2012/11/05/my-current-templatemapping/\n2) Alternatively, you can do missing:user which will return events in which the field is not present or contains no terms. A single dash would count as no terms.\n. This is an elasticsearch configuration issue. You may consider setting @source_host to not_analyzed. Here's a good sample mapping:\nhttp://untergeek.com/2012/11/05/my-current-templatemapping/\n. A solid start, would like to see this work with all fields in default_fields. Thanks!\n. The largest reason we encounter for this is incorrect timestamps leading to events going into the \"wrong\" index. Logstash creates indices on UTC time, and Kibana uses that standard to deduce which index to look in for events in a given timeframe. Are you using the date{} filter? Shipping everything with a timezone? Grab me on IRC tomorrow and we'll get it sorted out.\n. Absolutely a great idea. Definitely planning to implement this.\n. You're really better off using Kibana 3 if you want this functionality. Click the little (i) icon on any panel to see the query that was used to generate it.\n\n. Sure why not. Implemented in Kibana 3\n. Merged duerra's pull. I agree that either a link to an external shortening service, or a built in shortener would be good additions.\n. Ah, I need to update those screenshots. Click on any field on the left and click one of the buttons at the button of the popover panel.\n. Just merged that pull\n. In general Kibana needs better error handling and better exposure and management of the ES state. This is on my list of priority items.\n. There was a breaking pull to the config file merged on accident yesterday, pulling today should fix it. Thanks\n. Hah, I woke up and remembered I forgot to change this back. I just did it, thanks\n. This causes a data reload for every column change. To get around that the columns are not written to the hash in real time you can get the columns how you want them, then hit search again.\n. Very nice. I'll work on getting the JS implemented for this shortly.\n. Thanks dav3860!\n. You can use the Terms option and map the fields you want to use it with to be not_analyzed. This has roughly the same affect as the Score function, but moves processing to Elasticsearch instead of ruby\n. A definite bug, also affects the highlighted events message.\n. Should be fixed in latest commit. \n. The PHP version of Kibana is deprecated. Please upgrade to kibana-ruby. Thanks!\n. The removal of the tOffset math breaks user configurable timezones in this.\n. 107.21.226.196 is an EC2 IP. Is 107.21.226.196 actually assigned to an interface? I'm guessing its probably nat'd to a private address. Run ifconfig to find the actual address of your instance and use that.\n. As this is EC2, you can't connect directly to your private IP, you need to connect through you public elastic ip. I'm closing this as it is not a Kibana issue.\n. Gonna go ahead and close this, Kibana requires the logstash schema. \n. Thanks Chris, I should probably note that Passenger is the recommended method for deploying to production\n. you probably need to re-run bundle install\n. @apeiron I manually committed the content-type fix yesterday. However that line is only one of the 200 changed by this pull. This pull will require extensive review\n. You can set the fields to highlight on in KibanaConfig.rb.\n. I know we discussed this on IRC, but for posterity:\n\"@timestamp\": {\n      \"type\": \"string\",\n      \"index\": \"not_analyzed\"\n    },\nshould be\n\"@timestamp\": {\n      \"type\": \"date\",\n      \"index\": \"not_analyzed\"\n    },\n. Duplicate of #114 \n. The issue with doing this is that there's essentially a watch on the URL that reloads the page when it changes. The best option I think would be a 'share' link on the page that compiles the hash and displays it in a modal.\n. electrical is likely correct, it sounds like you've installed on another server? Connecting to localhost would mean you've installed it on your workstation, if its on another system you need to put the IP address of that system (or 0.0.0.0) in KibanaConfig, then connect to it from your browser.\n. This is doable with apache+passenger without a patch:\nhttps://gist.github.com/semiosis/4658921\n. This is the expected default behavior. If you need to search for a value containing a hash you can enclose it in quotes eg field:\"some-stuff\". Analysis of indexed fields is down to your elasticsearch mapping, you may want to specify a different analyzer in your template\n. I haven't tested this yet, can anyone confirm if everything continues to work as expected in both local time and specific timezone modes?\n. Kibana acts as a REST client. REST clients can not join the cluster. The best way to deal with this is to install elasticsearch on your kibana host, set node.data: false, and point kibana at localhost.\n. Kibana 2 requires the logstash schema and time format\n. This is not a Kibana issue. The standard ES mapping using - to seperate tokens. You can either change that field to not_analyzed, pick a different tokenizer, or search with missing:clientip\n. Markdown messed with the comment, should've read:  missing:clientip\n. There's no way for Kibana to know what analyzer you are using for your fields. You might want to set a different analyzer for that field: http://www.elasticsearch.org/guide/reference/mapping/analyzer-field/\nAlternatively, you might want logstash to just drop that field all together if it doesn't contain an IP. \n. Logstash indexes into the UTC date, this pull would break most installations\n. Not a Kibana issue. Rubygems.org might be having an problem\n. This is down to elasticsearch mapping. You map want to change the analysis or tokenizing of that field\n. I've not been able to replicate this in Chrome 27.0.1453.110, but I went ahead and made the change sugested by 8ffae54. \n. Fix again, doh. Let me know if that works\n. 1. I'm not sure what the issue is with your @tags, might want to ask in the logstash channel on freenode\n2. no\n3. Query syntax is Lucene query string syntax, there is a primer on it at http://three.kibana.org/about.html\n4. Find me in #logstash on freenode, I'm not completely sure what you're looking to do here.\nAlso, please submit Kibana 3 issue in the appropriate repo:\nhttps://github.com/elasticsearch/kibana\n. you should probably have a look at kibana3: three.kibana.org\n. This is down to your elasticsearch mapping. Fields you use with derive queries should probably be set to not_analyzed. Also, please submit kibana 3 bugs on this repo: https://github.com/elasticsearch/kibana\n. You really want to use mod_passenger if you need this functionality\n. wrong repo try this one: https://github.com/elasticsearch/kibana/issues\n. Kibana uses Lucene query syntax. @fields.duh:[17 TO *]\nhttps://lucene.apache.org/core/3_5_0/queryparsersyntax.html\n. Kibana uses the logstash event format, it is not compatible with graylog2 indices\n. It is rather likely that your numbers are being stored as strings in elasticsearch. if you are using logstash and grok you might want to change your pattern to something like %{PATTERN:dur:int} to make sure that it is stored as an integer.\nClosing as not a Kibana bug\n. #logstash on freenode is a great resource for logstash questions. You can find me as rashidkpc in the #logstash channel.\n. The correct syntax is field:[5 TO 10] or perhaps field:[5 TO *]\nThere are no sum/count functions\n. Agreed, although wrong repository: https://github.com/elasticsearch/kibana/issues?direction=desc&sort=updated&state=open\n. You'll need to update your default field to be \"message\" instead of @message\n. in KibanaConfig.rb\n. Wrong repo correct repo is here:\nhttps://github.com/elasticsearch/kibana\nAlso the text panel's html mode strips unsafe tags via http://docs.angularjs.org/api/ngSanitize.$sanitize\n. Here you go: https://github.com/rashidkpc/Kibana/blob/gh-pages/images/metrics.png\n. \nHere's an even more complex one\n. I've tested on Firefox 24 on Ubuntu and MacOS, both are working as expect. This sounds like a configuration issue, perhaps related to browser cache of config.js. Clear firefox's cache, restart the browser and test again.\n. I recommend normalizing your timestamps into UTC. Development on Kibana 2 has stopped except for major security issues.\n. Sorry, I don't maintain the chef cookbook and it isn't part of this repository.\n. Kibana only supports elasticsearch\n. No, Kibana only supports Elasticsearch. There is no abstraction layer to allow for a different backend.\n. Wrong repo\n. It appears you've filed this in the wrong repository? See here: https://github.com/elasticsearch/kibana\n. Kibana is unable to contact your elasticsearch server, ensure that the address you've entered in config.js is reachable from your webserver.\n. You probably want discuss.elastic.co, this repo is long deprecated.. ",
    "phobos182": "I am experiencing some issues with this. Basically all of our Servers are set for UTC. The logs show up correctly in Logstash with the correct time. The \"Last 15minute\" feature does not work. I'm thinking it's because my timezone is US/Pacific. And it's calculating UTC incorrectly?\n. ",
    "robinbowes": "I'm not sure if I'm also getting bitten by this.\nAll my servers are in UTC. I'm in the UK, so UTC+1.\nIt seems that if the search goes back before midnight, then it is truncated at midnight.\nSo, right now, it's 13:05. Last 5m, last 60m, last 4h, last 12h all work OK. But anything before that results in the search being truncated at midnight UTC, ie. 2012-04-17T01:00:00.\nSimilarly, if I try and search in a time period over the midnight boundary, then the search is truncated at midnight.\neg. try and search 2012-04-15T22:00:00 to 2012-04-16T02:00:00 and the time period gets changed to 2012-04-16T01:00:00 to 2012-04-16T02:00:00\nI'll try and ping you in #logstash later on today.\n. Sorry for the late reply (I just noticed the msg!).\nIt seems that this was a chrome bug, ie. it worked OK in Safari + Firefox.\nR.\n. ",
    "shaftoe": "Awesome!\n. ",
    "robert-martens": "\nChrome \"18.0.1025.39 beta-m\" will load the main page and then the tab will freeze up.  Chrome's CPU usage jumps significantly.  Not sure why this happens.\nMSIE 8 needed to have the console logging removed from js files (similar to issue #15) and flot's excanvas.min.js added to index.php:\n\n<!--[if IE]><script language=\"javascript\" type=\"text/javascript\" \n  src=\"js/excanvas.min.js\"></script><![endif]-->\n. - Chrome \"17.0.963.56 m\" seems to work fine.\n. A search bar that grows might be nice, there are a number of textarea re-size scripts that could be looked into.\nFlexible \"All Time\" probably would not be hard to work out.\nDefinitely could benefit from some work on making it look correct at different resolutions (Bootstrap should help with this). The side bar also does not look good at lower resolutions.\n. ",
    "azulcactus": "To help with research, after issue #15's fix Firefox 3.6 works fine for me.\nMSIE 8 doesn't work for me, I get:\nUser Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C; .NET4.0E; .NET CLR 1.1.4322; MS-RTC LM 8)\nMessage: Expected identifier, string or number\nLine: 705\nChar: 13\nCode: 0\nURI: http://10.38.189.56/kibana/js/ajax.js\nIt seems to not like the trailing commas without anything after them in the creation of the graph series JSON.  If I remove them (there are about 8 or so, note my line numbers in ajax.js include change I have pending for push 18) I then get:\nWebpage error details\nMessage: 'JSON' is undefined\nLine: 97\nChar: 9\nCode: 0\nURI: http://10.38.189.56/kibana/js/ajax.js\nSome good info on that error here: http://stackoverflow.com/questions/5339232/json-is-undefined-error-in-javascript-in-internet-explorer\nOne of the comments there also notes the trailing comma issue.\nI added json2.js (https://github.com/douglascrockford/JSON-js) to index.php and now I do get data back in Kibana in IE 8 and a new error when it attempts to draw the chart:\nMessage: 'window.G_vmlCanvasManager' is null or not an object\nLine: 6\nChar: 11617\nCode: 0\nURI: http://10.38.189.56/kibana/js/jquery.flot.min.js\nThis is solved by adding excanvas.min.js as robert mentions.  After that, I have a working Kibana in IE 8!  The style sheet renders a bit strange but it is very usable.  I'd be happy to issue a pull request for my above changes.\n. Sure, type detection would be optional.  If you do a statistical facet with a non-numeric field you simply get an exception from ES, no big deal.  The screen may look something like:\n\n\n\n\nraw data appears at the bottom in a table view (user selected a 10 minute interval):\nRange                     Min            Max            Average          Total Count\n08:00:00-08:09:59    0.002         1.235           0.641             13221\n08:10:00-08:19:59    0.054         2.721           0.357             34427\n. This is awesome!  I've got a 2 node cluster, 4 GB heap per JVM, and about 50 million documents over 50 GB of storage.  It works seamlessly, but will keep testing.\n. Fixing this now\n. Done, submitted a pull request.\n. Ok, I see now.  I had a merge issue on my side.  I'll cancel this pull request and resubmit a new one just with the other two commits.\n. Yup, was planning to resubmit the commits for the bar granularity.  You can feel free to tweak.  All I'm doing is changing the relative multiplier done when you generate the bars.  After a change for another recent commit, you switched from /100 to 10, so the drop down changes the 10 multiplier to 1 (resulting in more granular bars) or to 100 (less granular bars).  All the legends and hovers look fine to me.\n. I want this too.  I had actually coded something to do this in the old PHP version of Kibana.  I had a \"Saves\" button on the right side of the search bar (between \"Search\" and \"Reset\") and it triggered a popup allowing you to either save your current \"query\" and assign it a name, or select an existing save from the drop down.  It simply reads/writes the URL hash plus your defined name to a file on the file system.  No need to encode/decode anything.\nI had hacked up my PHP Kibana pretty well, but if I can find time I'll see if I can extract that code and port the changes to Ruby.\n. I wouldn't try to mix authentication/authorization into this.  There isn't any reason for the two to be clubbed together.  Saves/recent queries can be supported without that, they just wouldn't be able to be associated to any account.  Not a big deal.  When account management comes along, then this could be tweaked.\n. I agree, that would be very nice!\n. I believe the \"Filter\" value in KibanaConfig.rb will automatically add a specific filter to every query, although I don't think there is any way to override it like you are asking.  As a temporary work around you could have two \"installs\" of Kibana, one with the filter and one without.\nSeems like adding a way to override the Filter override itself vi the UI would be a nice and easy enhancement.\n. ",
    "petebowden": "MSIE 8 - Error when clicking on search: Object doesn't support this action  ajax.js, line 67 character 5\nLooks like it has to do with the \"delete\" function and IE. Delete does not work with explicit assignment (this.x or window.x) in IE.\nSee: http://perfectionkills.com/understanding-delete/\nI'm trying to test whether just changing the value to null will work.\n. ",
    "tpischke": "IE8 not working with latest Kibana, installed today.  Page renders somewhat but no graph or results shown:\nWebpage error details\nUser Agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; InfoPath.2; .NET4.0C; .NET4.0E)\nTimestamp: Mon, 24 Sep 2012 14:26:50 UTC\nMessage: Object doesn't support this property or method\nLine: 124\nChar: 13\nCode: 0\nURI: http://setzehorn.bedag.ch:5601/lib/js/ajax.js\nMessage: Object doesn't support this action\nLine: 39\nChar: 5\nCode: 0\nURI: http://setzehorn.bedag.ch:5601/lib/js/ajax.js\nPS:  Tried to attach a screenshot, but it seems github doesn't support atttachments?\n. Great!  I don't suppose there's a nightly build or something I can try out?\n. Grabbed the latest code just to see if it worked a little better, but behavior is the same.  Here's the latest error info:\nWebpage error details\nUser Agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; InfoPath.2; .NET4.0C; .NET4.0E)\nTimestamp: Wed, 26 Sep 2012 15:52:33 UTC\nMessage: Exception thrown and not caught\nLine: 81\nChar: 20\nCode: 0\nURI: http://setzehorn.bedag.ch:5601/lib/js/lib/date.js\nMessage: Exception thrown and not caught\nLine: 81\nChar: 20\nCode: 0\nURI: http://setzehorn.bedag.ch:5601/lib/js/lib/date.js\n. Awesome, IE8 working great for me, Thanks!\n. This is something my group is hoping for as well.  I assume that it will be implemented sooner or later, and for that reason we have chosen kibana over graylog2 for it's simplicity and obvious strengths.\n. ",
    "kwilczynski": "+1 to this, very useful :)\n. +1 to this!\nIt would be amazing to see an ability to make anything in @fields immediately available as table, and as active elements on which you would be able to perform some basic analysis.\n. ",
    "cdeck": "The \"Trim Factor\" option in Kibana 3 gives me exactly what I was looking for when I wrote this feature request.  Thanks!\n. ",
    "arimus": "Ah yes, forgot to mention that I also included an RPM spec file, which I had to roll for our environment.  Sorry, should have included that in a separate pull request.\n. ",
    "tdesaules": "I'm looking it but I don't know how can I do it.\nI create a field named Uri with logstash but not in elasticsearch -_-\n. ",
    "jkoppe": "pull req in https://github.com/rashidkpc/Kibana/pull/48\n. yep, will take a look!\n. i tested this and still see Object[].  will find you on irc\n. ",
    "msacks": "php built with --with-curl=shared\ncurl-7.15.5-15.el5\nHangs on:\nKibana/loader2.php?page=eyJzZWFyY2giOiIiLCJmaWVsZHMiOlsiIl0sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiYWxsIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJzdGFtcCI6MTM0Mzc5NTM1NzAyNn0=&_=1343851483568\nRequest Headers\n    13:04:44.384\nAccept:_/_Accept-Encoding:gzip, deflateAccept-Language:en-us,en;q=0.5Connection:keep-aliveHost:somebox.comReferer:http://somebox.com/Kibana/index.phpUser-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10.7; rv:12.0) Gecko/20100101 Firefox/12.0X-Requested-With:XMLHttpRequest\nSent Cookie\n__gads:ID\n. Hey, thanks, that sounds good. When are your office hours?\n. ",
    "MikeWorth": "Please also note that this work was completed while working for Oxford University IT services for the purpose of crediting\n. Unfortunately, today is the last day of my internship working on this, hopefully all the changes I've made to date are useful. As much as I won't be working full time on this, feel free to contact me if you need my help understanding what was going on in my mind at any point or you have any other questions.\nThere is a good chance that someone at Oxford University IT Services will take the preliminary work I've done and put a full centralised log system into production, this may well include Kibana. If this is the case, it is likely that you will see more code appear in the ox-it/kibana repository.\nThanks,\nMike\n. ",
    "alcy": "Thanks. \n. ",
    "jbarciauskas": "What is the name of your logs index or indices? What is the output of Kibana/loader2.php?getindices=1\n. Were there any JavaScript errors? What does the subsequent call to\nloader2.php look like and return? I created a test logstash index with one\nevent and it seemed to work but it was a limited test data set.\nOn Aug 14, 2012 9:45 PM, \"Rashid Khan\" notifications@github.com wrote:\n\nOutput of getindices was [\"logstash\"]\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rashidkpc/Kibana/pull/53#issuecomment-7745976.\n. \n",
    "ThisGuyCodes": "It'd be a lot more useful if this was dynamic. Like being able to set the fields shown per search. We like to have some of the useful streams up on monitors in the office and different types of logs use different field names.\n. ",
    "thoughtpunch": "Thanks!\n. ",
    "ShoeKSU": "I'd really like to see the backend setup as you describe.  I was looking for some sort of multi-user plugin for Sinatra, but noticed their basic-auth fit the minimum requirements I was looking for at the time.  \nI'm working to get this tied to our LDAP infrastructure currently, not having much luck but haven't had much time with it either.  With the PHP version, it was easily accomplished via mod_auth_ldap and ssl directly with Apache, however, with Ruby, there seems to be a lot of options, but none that I have successfully implemented yet.  If I can tie it to LDAP, then I can limit it to specific groups of users, which is what I'd like to do.  I've tried to use Apache, but most of the passenger documentation/examples I've found deal mainly with Rails, which I'd like to avoid if all possible, just personal preference.  My next investigation is using the net-ldap option and seeing if I can find the right syntax for it.  \nAny example documentation that you've seen doing any of this would be greatly appreciated.  \nThank you for your continued work on this project!\n. So not an hour after I replied, I have Kibana-Ruby working with Apache-Passenger with SSL and LDAP authentication.   \nI'm still looking forward to the day when all the authentication is build directly into Kibana :)  Thanks.\n. Well.. I can, but.  I ran into a couple issues with it.  I got it work on Ubuntu 12.04 with apache's built in libapache2-mod-passenger and Ruby 1.8.7, but I tried up upgrade to Ruby 1.9.2 and use the \"passenger-install-apache2-module\" installation instructions, and I'm getting errors.  Kibana Loads, but I'm getting some Internal 500 errors so i'm trying to tack down the issue.\nIf you want some Vanilla Instructions for Ruby1.87 and libapache2-mod-passenger, I can provide those if interested.\nAlso,  I seem to be having an issue with the Export with the Ruby Version.  Instead of actually prompting me to save a CSV, like the PHP version, its opening it directly in the browser without any delimiters, so just straight information.  I've confirmed it happens on both Ruby 1.8.7 and 1.9.3.  Any idea why this is happening?\n. Sorry for the sloppiness of this.  It was taken from my notes over the last couple months and I haven't had time to clean it up yet.  I did cut out a few things since rashidkpc has made some updates, and included a config.ru so my own iteration is no longer necessary.  Hopefully this will get you pointed in the right direction.\nHere is a pretty basic setup, that I use.  One thing to note is that I'm using Ubuntu 12.04 and the default Apache2 Package that comes with it.  With this, I am assuming you already have the OS and Basic Apache installed.  Also note, that I use the libapache2-mod-passenger package to handle the Passenger.  I do install the Gem but I could not make passenger work with Kibana using the provided instructions with the gem install.  if anyone can get it working, I'd love to see a good example.\n1.  Install the base Packages needed to install and run Kibana solo (This installs Ruby 1.8.7):\n   apt-get install git-core ruby rubygems libcurl4-gnutls-dev\n2. Install Kibana as directed on the main page:   https://github.com/rashidkpc/Kibana \nFor the Lazy: \nInstall: \ngit clone --branch=kibana-ruby https://github.com/rashidkpc/Kibana.git \ncd Kibana \ngem install bundler \nbundle install \nConfigure: \nSet your elasticsearch server in KibanaConfig.rb: \nElasticsearch = \"elasticsearch:9200\" \n1. install the Passenger Gem: \n   gem install passenger\n2.   install a few needed packages:\n   apt-get install apache2-prefork-dev libapr1-dev libaprutil1-dev libapache2-mod-passenger\nIt usually enables by default, however, in case it does not:\na2enmod passenger\nservice apache2 restart  (or reload whichever you prefer)\n1.  Passenger looks for a folder called \"Public\" in the program directory to serve the static images. Since Kibana uses the directory called \"Static\" to provide this, we need to make a symbolic link:\n   ln -s /your/directory/Kibana/static /your/directory/Kibana/public\n2.  Now just create a Virtual Host pointed at the public directory, example provided:\n   <VirtualHost *:80>\n   ServerName kibana.server.com\n   DocumentRoot /your/directory/Kibana/public\n   <Directory /your/directory/Kibana/public>\n   Allow from all\n   Options -MultiViews\n   </Directory>\n   </VirtualHost>\nFor me, this is all it took to get it up and running on Ubuntu 12.04.  I tested these instructions with a Kibana build pulled on Oct. 29, 2012.  I have yet to translate this to another Distribution.  I've been running in a test production environment with an Rsyslog/Logstash/Elasticsearch/Kibana setup for 3 months.   My supervisors have been very pleased with the results and I'm planning to move it into full production and replace our Splunk environment within the next Month.\n. To be honest, I didn't know there was a Pie Chart on that module, and I just updated my copy to make sure I hadn't missed something.  So, if there is one, it must not be working.   I'm really only using it for Search/Research and Historical purposes.  Are you getting the Pie Chart when you run Kibana straight as a Sinatra App, by running it as \"ruby kibana.rb\"?  If so, then I guess Passenger must be missing some functionality there.\n. ",
    "paradoxbound": "I would really interested in any sort of Kibana passenger guide as it is on the short list things my manager wants/needs for centralised logging.\n. This would be massively useful to us too we have 4 main \"data centres\"; two in AWS, one in Rackspace Cloud and a physical cluster with a traditional hosting provider. The application stack often talks back and forth between them.  A unified view of this would be very helpful. The cost of shipping logs to a central location however is prohibitive. \n. ",
    "cdenneen": "I too would be interested in Kibana passenger guide\n. ",
    "nicovpp": "When I use passenger with Kibana, I am not getting the pie chart on \"terms\" module by example. \nDid you manage to make all functionnalities working on your side ?\nStandard functionnalities (research, historical chart...) are working fine for me\n. Yes all functionnalities are working when I launch Kibana as a Sinatra app (and the pie chart).\nI was trying to use passenger in order to use ldap authentication but it seems that I will have tu use basic authentication with a single user/password.\nThanks for your answer (and if someone manages to make everything run with passenger I am still interested...)\n. I am using rashidkpc-Kibana-41a1298\nI have alreay try to change the ElasticSearch Timeout without success :\nSet the Net::HTTP read/open timeouts for the connection to the ES backend\nElasticsearchTimeout = 50000 \n. I have just try Kibana's last version : rashidkpc-Kibana-v0.2.0-17-gedb4553\nI see that score / trend analysis are by default limited to the last 2000 events. If I increase this limitation  (to 50'000) I am still getting the same error in aroud 30s, and the request seems to be stopped in the same time\n. @rashidkpc -> Ok I wiil try soon to use the Terms option on ES not_analyzed fields and keep you informed about the result\n@dav3860 -> So you resolve this issue by using Passenger/NGINX. I have tried to make a similar setup (PASSENGER/HTTPD) but I am experiencing another issue: some charts are not displayed, as the pie chart in Terms option (see: https://github.com/rashidkpc/Kibana/pull/62). Is it working for you ?\n. @rashidkpc -> I have just tried on a not analyzed field for 200'000 events.\nThe behaviour is still the same for me. First ES process is taking CPU and then decrease. After, only kibana process is taking CPU and then I received Kibana's error\nPID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND\n  4386 root        20   0 2174m 1.1g  528 S 99.7 56.6   0:37.02 ruby\nAlso when this request failed I then need to kill kibana process.\nTerms option is working fine (answer in 2s) on these 200'000 events. \n. @dav3860 : Symbolic link was ok. I have just made again same setup (or probably not) and all graphs are now working but only on old versions of Kibana (Oct / Nov 2012)\nOn the last version of Kibana (Dec 12 and current version), I get a 500 error when trying to access the defaut page\nI received a 500 Internal Server Error from: api/search/eyJzZWFyY2giOiIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6OTAwLCJncmFwaG1vZGUiOiJjb3VudCJ9?_=1357925382246\nI do confirm that on old versions, httpd/passenger shows data for 200'000 events under score option\n. Problem solved !  I thought that passenger would write log in my virtualhost's log files but it was in apache's global error log (I didn't check this file last time).\nI needed to add rake gem, and change settings for gemfile.lock file. Everything works fine now and I am able to request data on a large number of events\nThanks ! \n. Indeed I have just changed my mapping options and it works fine\nThanks for these informations ! I do close this issue\n. Ok just see, same as #231 . Problem still occurs (I just download current version). Don't know if I should close this issue...\n. Hi !\nI am using passenger for running Kibana and already doing ldaps authentication on apache side.\nI do have also tried kibana-ruby-auth and really like the user managment / tag restrictions / favorite search, but I think that there are some important missing function for people using ldap(s) : \n- ldaps support (certificate / ...)\n- bindDN (or I maybe missed it)\n- (I do agree that auth and user managment should be split)\nHere is by example my apache configuration : \n<location \"/\">\n            AuthType Basic\n            AuthBasicProvider ldap\n            AuthzLDAPAuthoritative on\n            AuthLDAPBindDN \"CN=readonlyuser,ou=People,DC=myexample,DC=com\"\n            AuthLDAPBindPassword MySecretPassword:)\n            !# --> need to deal with binddn / pw\n            AuthLDAPURL \"ldaps://W.X.Y.Z/ou=People,dc=myexample,dc=com?uid?sub?(objectClass=*)\"\n            !# --> need to deal with certificate and port 636\n            AuthName \"Kibana Restricted Access\"\n            Require valid-user\n    </Location>\nBy the way, great job and would be great to merge :)\n. @dav3860 : Indeed it is not painful to provision users in kibana and specify ldap authentication for each one... Only a few users should be able to read logs. \nI wanted to put this version in my production environement but ldaps / bindDN are required... would be great if you implement these functions in the future.\nI had a look but am not able to modify ldap_auth file (seems to deal with NET::LDAP for simple_tls encryption, and OpenSSL::SSL & OpenSSL::X509 for certificate & SSL connection)... \n. Hi !\nI have tested your changes in my environment. Everything is now good for the research, except for the stream part.\nIn the stream part, the time is wrong displayed (my timezone is CEST and time displayed is CEST + 1 hour). Do you have an idea of what goes wrong ?\nThanks !\n. ",
    "garlandkr": "Same thing needs to happen in the README.md for the ruby branch\n. ",
    "falkenbt": "Ok I will check that out, thanks. I am running master. \nRegarding Ruby, which server would you recommend? Or do you include one (it seems so from the installation instructions). I'm not sure yet how the installation would work in our case (no internet access, no ruby (no gem) installed...\n. Wrapping it with jRuby would be great, this would really help us and probably many other who don't have ruby installed. This way it would be easy to get started without root, just like with logstash. \n. :+1: \n. ",
    "timbunce": "That didn't help, sadly. I get the same strack trace.\nLooking at the diff makes me wonder if the problem is related to our index names. We're using \"logstash-%{@source}-%{+YYYY.MM}\" so the current_index function will be guessing wrongly.\n. That worked, thanks.\nThough wouldn't it be better to move the \"logstash-%Y.%m.%d\" string itself into the config?\nNow that elasticsearch 0.19.8+ supports wildcards in indices, if I could specify the strftime format string in the config I'd be able to use something like:\nSmart_index = \"logstash-*-%Y.%m\"\nSeems like that would make 'smart index' much smarter.\n. Thanks, but none of those address the underlying issue that the access log is being sent to stderr which is also used for errors. So the errors and the log are mixed together.\nA simple fix would be to change the code so the web server access log is sent to stdout. I'd offer a patch but I'm not familiar with ruby/sinatra. [...later...] So I dug around and turned up https://github.com/sinatra/sinatra/issues/484 It seems lots of people are struggling with this. It does seem very odd to send access logs to stderr. Oh well.\n. A few options spring to mind:\n- reduce the size of the data structure for the default case. Specifically, in setHash, delete offset if 0 and search if empty. But that won't save much.\n- compress the JSON.stringify output with something like LZMA-JS. Bigger savings but more complex and would still be too long for non-trivial urls.\n- add an interface to a link shortener service. Something like the chain-link icon on maps.google.com. This seems like the best option.\n. ",
    "rurounijones": "Gah, none of these commits triggered a notification so I wasn't aware that they were done. Thank you very much for working on this.\n. ",
    "chriscareycode": "The answer you need is to disable the avahi-daemon service and it will fix it.\n. ",
    "dkowis": "I can reproduce it. No errors though.\nRuby 1.9.3p286 Firefox browser on the linux.\nNothing is displayed, and I just see a request every 10 seconds in the sinatra logs. I do see log entries showing up when I search for it, so they're getting into elastic, just not onto the stream.\nPerhaps it's because my logstash index is \noutput {\n  elasticsearch {\n    cluster => 'logstash'\n    embedded => false\n    index => \"logstash-%{+YYYY.MM}\"\n  }\n}\nI have in Kibana/KibanaConfig.rb\nSmart_index = true\n  Smart_index_pattern = 'logstash-%Y.%m'\nThat should be correct as I understand it.\n. ",
    "emadum": "Made some progress since I initially posted this comment, and the information I included was irrelevant. Turned out it is an issue involving timezones. My data is getting indexed into ES with timestamps that are 3 hours behind the actual time. Still haven't figured out why; but modifying the code for the /api/stream request in kibana.rb to look 3 hours in the past causes the stream to work. I should add that changing the Timezone setting in KibanaConfig.rb had no effect on the times being used to query ES in /api/stream.\nIt is likely that others experiencing this issue have a similar time mismatch problem.\n. ",
    "jlivas": "I would also love a way to flip the steam the other way around!\n. ",
    "LeonidShamis": "Hi we also would like an option to display events in a chronological order and not in reverse chronological order, as it is done in Kibana by default. Could it be made a configuration option?\nThank you.\n. Hi, thank you for the prompt response. Sort by time is also good. I'm not aware of the details and therefore my concern at this stage would be that if the selection (change of Time sort) is not made persistent (which introduces requirement for \"state\" management of user selections), then it would potentially be reset with each retrieve/filter of different data.\nAppreciate your help.\n. ",
    "flyasfish": "I think people who used to with one type of sort will not likely to change it frequently, so make it a configurable item will be better than a change on the UI.\n. Thanks it worked fine now after apply the change.\n. We have exactly the same problem with IE8.\nIE version: 8.0.6001.18702CO\nkibana version: rashidkpc-Kibana-v0.2.0-0-g41a1298.tar.gz\nWebpage error details\nUser Agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727)\nTimestamp: Tue, 20 Nov 2012 05:09:11 UTC\nMessage: Expected identifier, string or number\nLine: 679\nChar: 3\nCode: 0\nURI: http://sulog01:5601/lib/js/ajax.js\n. +1\nvery good feature will be extremely useful for debugging.\n. ",
    "dav3860": "No, it's better to have a sortable column in the UI. Maybe the columns layout could be added to a future \"saved searches\" feature. To me it makes sense to save a particular search with its user-defined columns layout.\n. I added a first attempt of support for favorites searches to kibana-ruby-auth. You can have a look at it.\n. Yes that would be a great feature. Ideally using external scripts and/or a user-defined query in the search bar.\n. I've got it working. I added a \"| highlight\" keyword to display highlighted results.\nBut I need to clean up the code and to fix a couple of issues. I make a pull request when possible.\n. Still needs to add a working \" | highlight @field\" keyword. But work needs to be done on the \"pipe\" mechanism.\n. I'm sorry Rashid, the issue is still here is IE8 :\nD\u00e9tails de l\u2019erreur de la page Web\nAgent utilisateur : Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET4.0C; .NET4.0E)\nHorodateur : Mon, 19 Nov 2012 10:03:42 UTC\nMessage\u00a0: Identificateur, cha\u00eene ou nombre attendu\nLigne\u00a0: 679\nCaract\u00e8re\u00a0: 3\nCode\u00a0: 0\nURI\u00a0: http://logsvr1/lib/js/ajax.js\nThere seems to be a problem with the \"click\" event.\n. Hi Rashid, \nI found a few typos in the javascript ajax.js :\n\n96c97\n<   var sendhash = window.location.hash.replace(/^#/, '');\nvar sendhash = window.location.hash.replace(/^#/, '');;\n677c678\n<     }\n},\n1238c1238\n<     grid: {show:false}\ngrid: {show:false},\n1355c1355\n<         color: \"#000\"\ncolor: \"#000\",\n\nNow, the main page is showing up. But there is still a problem with the datepicker in IE8. No event is fired after I select a date. The picker just disappears when clicked.\n. I must add that the @timestamp field in the elasticsearch index includes the milliseconds.\n. Thx !\n. I see this issue too with the column layout defined in the config file, which is not replicated in the \"stream\" view. \nBut I must add that if you manually rearrange the columns in the standard view and clic on \"Search\" to refresh the screen, then when you go to the stream view, it works. You can also bookmark the page. The encoded URL contains the column layout.\n. Thank you Rashid. I modified the mapping in elasticsearch using a template. It solved the issue.\n. I added this and made a pull request.\n. Fixed.\n. 1. Kibana is designed to work with logstash, not splunk. The naming convention in elasticsearch data mapping must match Logstash's. \n2. It's not really able (yet?) to display dashboards, just real time streams. You will probably have to plug something like Graphite for this purpose. Logstash has a statsd output for use with Graphite. \n. I had this issue too. As I understand it, it's the Thin web server that closes the connection from the client to the api after 30s if elasticsearch takes too long to respond. I didn't find a way to set a configurable timeout. So I moved to NGINX + Passenger and it solved the problem.\n. @nicovpp : yes the pie chart is OK. Remember you must create a symbolic link \"public\" pointing to the \"static\" directory for Kibana to work with Passenger. And maybe clear your browser cache.\nFor your 200,000 events tests, \"score\" is implemented in ruby, not elasticsearch. This explains why the ruby process is taking CPU too. To me, it's just that if the computation takes too long (>30s), the included Thin web server closes your connection to the Kibana API. \n@untergeek : in my case, it worked.\n. Elasticsearch isn't the problem (for the 30s timeout). It's clearly an issue with the web server that closes the connection (in my setup).\n. Check your web server's error logs.\n. See issue #176. The @mylog field needs to be set to not_analyzed in your elasticsearch mapping. Or use a custom analyzer.\n. I don't have this issue. \n\n\n\nMaybe you should try with the latest version. Have you tried to compare with a curl query ?\n. The mapping and naming conventions of the indices need to match logstash's.\n. Have a look at #212. It may be a timeout issue. \n. Yes, for role based access control for example. you can define groups and limit views to specific tags for example, letting sysadmins see the servers logs and network admins see the network equipment logs. Or to delegate a limited access to a helpdesk... Authentication could be done in LDAP too.\nWhat in more, you have a nice welcome page (better than a pop-up) and you can use the backend to store favorites or search history.\n. To me, it's a must-have for enterprise software.\n. Most of the job is already done in kibana-ruby-auth. I think it lacks a few things but more people would help to fix them if there was only one branch.\n. @nicovpp : I have added a pull request for code doing an auth and user/group management separation. For example, I now use LDAP only for authentication against Active Directory, and all users and groups are provisioned directly on Kibana in elasticsearch. You have to provision your users in Kibana (shouldn't be too painful if you don't have too much Kibana users, and can be scripted). And groups are internal to Kibana. This works as expected and no specific binding is needed. It's OK for a basic but usable feature and it can be extended in the future.\nFor LDAPS and bindDN, NET::LDAP provides them, so it should be possible to implement.\n@prymitive : yes, it's a good idea.\n. Separating identification, authentication and authorization doesn\"t mean you can't use LDAP as a source for user authentication and identification/management. You would just have to specify LDAP as \"auth_module\" and \"users_module\". But the full user management in LDAP (with groups assignment, etc) isn't ready yet. For the moment, with kibana-ruby-auth, you can just authenticate against LDAP and manage your users internally.\n. You can already manage roles with kibana-ruby-auth. You can allow users or groups to only see messages with specific tags.\n. In Kibana3, maybe we could use aliases. For each index, it's possible to create aliases with filters. So, it could be possible to create one for each user and each index, with a filter on specific types for example. Typically : \"logstash-xx-xx-xx\" would be the index, and \"user1-xx-xx-xx\" would be an alias for user1 limiting the view to one event type or more. Then with apache acting as a reverse proxy for ES, you could authenticate the user with LDAP for instance and rewrite all accesses to \"/logstash-xx-xx-xx\" to \"/%{REMOTE_USER}-xx-xx-xx\", thus transparently limiting the view to what the user is allowed to see.\nA script using the ES API could be scheduled to create the aliases. Currently, there is no way to automatically create aliases with filters on index creation, even using index templates, but there is a pull request for this, I think. Or course, it would be easier if this was integrated to Kibana.\nWith this method, no \"proxy\" or authentication/authorization system would be needed except apache/nginx and ElasticSearch, I imagine.\n. Obviously, you are right. I just added a first attempt to sanitize the user and group names.\n. I had a look at this. Try the \"fix_password\" branch in my Kibana repository. I'll make a pull request if it is fully functional for you.\n. - for the save option, have a look at the kibana-ruby-auth branch (experimental) or Kibana 3 (preview) : http://three.kibana.org\n- the search request and columns layout are encoded in the URL in base64 (see http://tools.web-max.ca/encode_decode.php to decode/encode them). They can be saved as bookmarks.\n. ",
    "r-duran": "It is because \"Analyze_limit\" is set to 2000 by default in KibanaConfig.rb. So if there is no log for some source_hosts in latest 2000 logs you will not see it in score table. You can increase that limit. \n. Well in splunk if you want to search for pipe char, it needs to be in a quoted search string like 'hostname:\"foo|bar\" AND scrip:192.168.7.44'\nI will implement it when I have time. If nobody volunteer to do it.\n. @awheeler \nI didn't  try it yet but it seems good to me, nice work.\n@rashidkpc \nKibana has great potential as a splunk like interface but it lacks features that graylog2 has like user management, saved streams and alarms. But graylog2 lacks elegant and functional interface that kibana has. It is pain to search by fields in graylog2. I wish you could join your forces.\nIt seems I will start to implement kibana features in graylog2 because the opposite is too much work.\nThanks for your inspiring works.\n. @awheeler \nI didn't discussed this with them yet. But I'm sure they will also want such UI functionality.\nGraylog2 is more than a UI for elasticsearch. It is more close to splunk feature wise than kibana but it looses in UI side. The problem is graylog2 has two different mechanism for searching and analyzing. It has quickfilter search which you need to fill multiple textboxes for your search, more of them if you want to search on fields. And it has ajax based  cli-like analytics component that has very ugly syntax, and uglier results representation compared to kibana's score, trend, stats mechanism.\n. ",
    "kakbit": "Yes, that helped.\nThank you!\n. Perfect,\nThank you!\n/Patric\nRashid Khan wrote:\n\nI've added a Time_formation option to KibanaConfig.rb with b613b12 \nhttps://github.com/rashidkpc/Kibana/commit/b613b12\nSyntax is here, though I also included an example for iso8601 in the \nconfig file.\nhttp://blog.stevenlevithan.com/archives/date-time-format\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/rashidkpc/Kibana/issues/78#issuecomment-8795652.\n. \n",
    "drewp": "Which date presentations did you want to change? The result of /api/search seems to be fine. Then there's a chopped one on each log line (but the time components are still in a good order), and a similar thing on the graph.\n. I saw this but I don't have time at the moment to investigate more. \nIt looked like the server was doing a dns lookup on client addresses before answering them (!), and that dns timeouts for my internal addresses were making things slow. I stopped caring because my setup plan is to have a local nginx proxying all my /kibana/ requests over to http://127.0.0.1:1234/, so kibana won't see any requests except from localhost.\n. Where did this fix get in? I don't think I have it, and I'm on the current rev (https://github.com/rashidkpc/Kibana/commit/81382c65fb9e63407347a12117b39c301f657d7a).\nIn fact, if I make one request to non-localhost, all subsequent requests, even to localhost, are very slow. strace shows a lot of fast futex and select wait calls that I don't see on a kibana process that hasn't been touched as non-localhost.\n. ",
    "awheeler": "I'd very much like to see this incorporated, and extended to support multiple fields:\n@source_host:nginx AND response:404 | score clienthost request\nThis example would give us an analysis of page not found requests by host and the actual failed request.\nI'll take a stab at this myself, though I'm not very familiar with Ruby.\nThe events with a | char can be addressed by skipping | inside of quotes -- I'll work on that too.\n. I've implemented that by assuming that there will never be a ' or \" after the real pipe char:\nvar pattern=/^(.*)\\|([^\"']*)$/;\n  if (pattern.test(window.hashjson.search) == true) {\n    var results = window.hashjson.search.match(pattern);\n    var search = $.trim(results[1]);\n    var fields = results[2].split(' ');\n    var field = fields.slice(2).join(',,');\n    var mode = fields[1];\nAlso got multi-field working by joining the multiple fields with ',,'.  But still need to fix the search and analyze buttons on the results.\nI had to manually apply your changes r-duran, as the code that I checked out didn't match your diffs.\n. @r-duran\nAre the graylog2 maintainers receptive to Kibana/Splunk-like features?\nI found Kibana through Graylog2, but when I couldn't see how to graph the results of a search on a timeline, I stopped considering it and then bumped across Kibana, which seemed to be much closer to what I was looking for (a Splunk replacement) -- with just the changes I'm making, I can do 80% of what I was doing with Splunk.\nIt would be very cool to migrate the Kibana front-end/features into Graylog2.\n. Now supports both score and trend, and the sub-search buttons work.  Pull request sent.\n. You can work around this somewhat as the url can be bookmarked, and it contains the complete query.\nThere is thought on how to implement this in the GUI though, so it is coming.\nThe hashcodes in the url can be decoded here: http://tools.web-max.ca/encode_decode.php using base64 decode.\n. Take a look at @rashidkpc kibana-ruby-auth branch to see how this is developing.\n. I have Identified the cause: in the file lib/kelastic.rb, in the collect_item_attributes function, the r array must be returned explicitly.\nA fix should be forthcoming.\n. I'm using elasticsearch-0.19.10-1.\n. ",
    "macbar": "On 28 Sep 2012, at 15:11, Rashid Khan notifications@github.com wrote:\n\nWhat version of Kibana and Safari? I just loaded up the latest commit to the kibana-ruby branch in Safari 6.0 (7536.25) and things seem to look ok\nkibana-ruby branch at commit e54ae68bd29a1d02556eee0311925c2140f14869\nSafari Version 6.0 (8536.25) (mountain lion)\n\nScreenshot at http://cl.ly/image/0T3b1V303S36\nThanks,\nPieter.\n. Super, thanks!\nPieter.\nOp 30-sep.-2012 om 00:33 heeft Rashid Khan notifications@github.com het volgende geschreven:\n\nWas able to reproduce. Issue with date parsing, sort of surprised that didn't show itself in older versions of Safari, odd. Fixed.\n\u2014\nReply to this email directly or view it on GitHub.\n. Hmm, I still get the 'invalid date' error... \n. \n",
    "khobbits": "Thought you might want a few details.\nhttps://www.evernote.com/shard/s18/sh/be08a4b6-c225-47b0-bd1e-318bf53eb240/8c61b29b76a96339fdc551883f6e81b3\nEver since I added json Apache logging to Logstash, Safari is unable to open log entries on Kibana, it seems that Safari can't handle the Apache timestamp.\nWorks fine in other browsers, investigation shows that it is caused by the UTC offset (+0100).\nPosts on stack overflow suggested that Safari doesn't support '+0100' only '+01:00'.\n. I'm using this locally, using it to stream certain log searches to a dashboard, works great.\n. ",
    "smetj": "Yes that's correct, \n. +1\n. ",
    "kmullin": "Also I wanted to formally say Thank You for an amazing application... :smile:\n. ",
    "jbussdieker": "This also would help with bundle install --deployment for creating standalone builds.\n. +1\n. ",
    "tintii": "Ruby version 1.8.7\nstill have the issue when running on 0.0.0.0, is there any way to change the webrick setting from outside kibana?\nthanks\nedit: changed to ruby 1.9.1 - set :DoNotReverseLookup => true, working like a charm so far.\n. ",
    "pippoppo": "Latest firefox and chrome on OSX. The problem is, I have a lot of fields like this:\n\"ap_xforwardedfor\":[\"\\\"10.0.0.3\\\"\"]\nI have a lot of fields with the double quotation marks, and it seems that they are identified as NaN.\n. ",
    "duerra": "Oh heck yes.  This, please.\n. I've submitted pull request #205 to account for the recommendation from @timbunce for improved json compression to help support more complex views better.\n. +1 for this bug fix.\n. ",
    "bobtfish": "I've re-worked the commit to avoid this issue.\n. ",
    "tuannvm": "Yes, I get this everytime. I checked your demo site, and rss feature is good, but not my deployment.\nI attached the image with my reply, please check it out.\nI have tried ruby 1.8 also, but it didn't work either.\nhttp://i47.tinypic.com/1rel41.png\nThanks!\n. Update: Kibana and Elasticsearch are on the same server so this error happen. If they are separated, it's ok.\nThanks Rashikpc!\n. Now it happened again with your latest build.\n. Thanks you, I really want to help. But I'm just a ruby beginner, will try\nhard to catch up.\n. ",
    "fredleger": "still hapenning for me\n. ",
    "fabn": "Obviously you're free to choose, but keep in mind that settingslogic allows erb into yaml as in\n``` yml\nconfig/application.yml\ndefaults: &defaults\n  cool:\n    saweet: nested settings\n  neat_setting: 24\n  awesome_setting: <%= \"Did you know 5 + 5 = #{5 + 5}?\" %>\ndevelopment:\n  <<: *defaults\n  neat_setting: 800\ntest:\n  <<: *defaults\nproduction:\n  <<: *defaults\n```\nRegards.\nP.S. I might need to integrate your project into a rails app as mountable engine, I have two options here:\n- enclose the sinatra logic into a class and mount it\n- rewrite your application code to a rails mountable engine\nWhile the first option is pretty simple to achieve, the second one is more powerful since in that way you may benefit of some rails advanced features such as assets precompilation (and automatic minimization) and a lot of other interesting features (I think you know that but I'm telling you just in case). Moreover with the second option I can meet a requirement (in which I'm personally interested) that otherwise can't be achieved (I guess): sharing rails authentication (using before filters in ApplicationController) with the main app.\nAre you interested in something like that or do you plan to remain with sinatra? If you're interested I can share my attempt and make it public in GH otherwise I'll do some quick and dirty work to get things done.\n. It was when I wrote it, however it has a problem with assets serving when mounted in path differents than /. Since I got no feedback on this I haven't investigated on it.\nI think it needs a helper for assets inclusion to be used in html templates, resque server do that in this way:\n- template code\n- helper function\nOr one can use sprockets, but I don't know how to use it in a rack application.\n. @jamtur01 thanks for the hint, I'll look at that.\n. Awesome thanks for that.\n. ",
    "bfritz": "+1 from me if curl is not necessary. As of a few days ago, curb was a pain to build on Debian Wheezy.  My work around and other details at bfritz/Kibana@077fbb1c76.\n. ",
    "moll": "I'm already working on replacing Curb with Ruby's net/http, so I'll get back to you.\n. And done: https://github.com/rashidkpc/Kibana/pull/108\nYou may use my fork of Kibana (ZIP download) until this gets merged to master if you're in a hurry.\n. ",
    "ghost": "I second this request ... most desirable function currently missing.\n. I assume linking this in .. https://github.com/mikel/mail  .. is what is required .... My programming is still extremely BASIC .... so I do not think I will be able to figure out this myself for a LONG time. \n. Hi there\nI am facing the same problem. Any chance you could post your working configuration ?\nThanks in advance\n. ",
    "electrical": "I think this is kinda connected to Saved searches...\nBeing able to define a query and have a small daemon running executing that query and if it returns something, or the count is higher then expected send an email or something.\n. damn. messed up the PR in #257 \nI'll keep this one open in case you don't want to do the PR #257 one\n. messed up my commits :-( sorry.\n. Is there any reason to have authentication done in kibana?\nIf one runs kibana via passenger can't one use ldap in there then? ( just wondering )\n. okay makes sense now :-) :+1: \n. Where i will be using Kibana we are using Ldap for authentication and have allot of users.\nNot sure if manual creation of users in kibana would be handy then.\n. Managing roles wouldn't be to bad to do in Kibana indeed.\nIm looking forward to this functionality :-)\n. Cool :-) looking forward to have it all merged.\n@rashidkpc How will functionality impact your new 'all js' version?\n. Did you also try to connect to 127.0.0.1:5601 instead of localhost?\nOtherwise you can also change it in the KibanaConfig file.\n. i guess the server ip is different then 127.0.0.1 ?\nYou might wanna change the binding in the KibanaConfig.rb file to let it listen to 0.0.0.0  ( all ip's )\nif it still doesn;t work then, maybe a firewall is blocking it?\n. Hi,\nWhat is the 'type' you gave to the file entry at the shipper? ( agent )\n. The 'type' is set at agent level and is never changed afterwards.\nYou will need to set the 'type' of the grok filter to the same one as the file input. 'all' in this case.\nThen it will work.\n. Your welcome :-)\n. ",
    "agarci40": "I started working on something like electrical described, but I found that the amount of work involved would be too much for me. For now I am going to simply write a cron script that communicates directly with elasticsearch and does the necessary matching, formatting, e-mailing. If someone can think of a better way of doing this, I would appreciate it.\n. I agree. Thanks for all your hard work.\n. ",
    "taidan19": "In know that Kibana doesn't necessarily require the use of logstash, but if you do use logstash, it has the ability to send emails out based on variety of criteria. That seems like the best layer to perform this at.\n. Two things are going on here.\n1) When you run ruby kibana.rb -h, the options that show up are generated by Sinatra, not Kibana.\n2) Kibana determines which port to run on by looking at the \"KibanaPort\" property defined in KibanaConfig.rb (it was on line 16 in my version of the file). The way kibana.rb is written, it always sets the port to this value, and it will override any value you set on the command line.\nIn other words, the output of -h is misleading because it isn't generated by Kibana; for the same reason, it also can't be changed.\nSecond, if you specify your port in KibanaConfig.rb, it should work as you expect.\n. ",
    "olej-a": "Use Kibana as Monitoring UI\nKibana as UI:\n- define monitoring alerts - query (saved searches) + query/dashboard after alert raised (optional) + raise conditions + check periodicity + alert action\n- investigate incident after alert (query or query/dashboard after alert raised) - technically link to Kibana's query/dashboard with parameters (fe. time of alert)\nMonitoring daemon:\n- read and execute stored searches, check raise conditions and raise alert (email, other connected service)\n- information in raised alert include link to query or query after alert raised/dashboard\nMain idea is that monitoring checking query should be very simple to check raise conditions and easy to process in small time intervals.\nQuery after alert can be more complex and can include histogram or whole dashboard information.\nIt can be very effective to investigate problem in time of alert.\nhints: Loggly, AppDynamics\n. ",
    "anandaverma": "I also felt this need and wrote my monitoring and emailing app completely in java. It's pretty basic but I am trying to make it much more generic - \nhttps://github.com/anandaverma/esmon\n. ",
    "Sid-Trikha": "@taidan19: Logsatsh has sending email option. How to check on unique count of field?? After that sending email is easy. Any idea??\n. ",
    "analytically": "+1\n. ",
    "jordansissel": "The use case for this is that I have puppet currently managing kibana with 'git pull' - 'git pull' fails if I have a local moddication to KibanaConfig.rb\n. I also verified that kibana works well using the threaded webserver logstash ships with (ftw) \n. Don't merge this yet, I'm still testing to make sure it works in logstash\n. Don't merge this yet, I'm still testing to make sure it works in logstash\n. 'gem install kibana' requires a number of things to have been done correctly, none of which are (and should not necessarily be) documented on the kibana site - you need ruby, ruby development packages, a C compiler, a C++ compiler, and many many other things - all of which are different on different OS platforms and distributions.\nNew users often stumble while trying to setup Kibana because they use OS distributions that punish them (for example, by not making it obvious they need a compiler or the ruby development packages instealled). As a result, new users will confuse a poor kibana installation experience (due to OS badness) with Kibana being a poor project.\nHappier users == more users == stronger community :)\nLogstash went through the same problems years ago, which is why you don't see logstash available from rubygems anymore, for exmaple ;)\n. ",
    "jgoldschrafe": "Seconding this. I know it's a pain to turn what's currently a fairly stateless application into something that will need to do significant CRUD work to implement features people are asking for, but there's a lot of potential in the (really fast!) search interface that's getting passed over by my group because Graylog2 can just do a lot more right now.\n. ",
    "crashdump": "This is definitely the missing feature of this UI !\n(And also save the columns somewhere..)\n. ",
    "invadersmustdie": "+1 for this feature\nI think it wouldn't be hard to implement a simple user-management using redis or another \"light\" storage solution, once you got there adding and sharing \"saved queries\" is just a minor step.\nI'm currently in the situation of introducing logstash + kibana as a splunk alternative, but \"saved queries\" seems to be a must-have feature for the client. Thus I'm willing to spent some time on this if you're up to this ..I'm not interested in maintaining a fork.\n. I must agree that the idea of using the already present ElasticSearch as storage is tempting, but in my opinion it is not clever to use the logstash \"data sink\" as store for users and other items (eg saved searches) .. that clearly violates the boundary between those two systems. Kibana is a wonderful drop-in replacement for the existing logstash-web frontend  and so as an \"drop-in\" it shouldn't mess around with primary data.\nAnyways, when you're going to use ElasticSearch as storage I would recommend using a separate instance .. though I still prefer using a more lightweight store like redis.\n. Again, it's tempting but lets assume the typical setup. There are shipper nodes and the indexer which are assembled in the preferred way:\nnode1:[logstash-shipper] -> node2:[(redis|messagebroker) ~> logstash-indexer -> elasticsearch]\nUsing the existing ElasticSearch as storage would make sense if Kibana is running on node2, but in a larger setup you will extract Kibana on a separate node and treat the indexer (node2) as a blackbox which provides a read-only interface to any frontend you want to use.\nThe resulting setup will look like this:\nnode1:[logstash-shipper] -> node2:[(redis|messagebroker) ~> logstash-indexer -> elasticsearch] <--(read)-- node3:[(Kibana|*frontend)]\nJust imagine someone else is going to use another logstash frontend and wants to store user related data, following your proposal it would be totally fine to use the ElasticSearch instance on node2. Looking at the scenario the architecture will result in a shared storage for unrelated systems (unnecessary coupling).\nIn the end it's Rashids (@rashidkpc) decision (not offloading any responsibility here ;) ), we got a bunch of good proposals and I'm sure we can keep this discussion running for some more days but lets wait for his opinion.\n. Fair enough. I took a peek into @jimi1283 implementation. The abstraction of the \"StorageModule\" provides an exchangeable interface I can work with.\nSo what's the plan on this feature request and how can we help?\nPS: There are ways of securing ElasticSearch (https://github.com/Asquera/elasticsearch-http-basic, https://github.com/sonian/elasticsearch-jetty, ...) and just because it can't be ensured on a practical level doesn't mean you can't do it on a conceptual level.\n. ",
    "pkubat": "No really \"the correct\" solution, but Kibana is already connect to a \"storage solution\" ... ElasticSearch.  Could just store it there.  Something to be said for KISS and less \"parts\"\n. ",
    "untergeek": "It wouldn't interfere with logstash at all!  Make an index, call it KibanaConfig or something like that.  It might not be the lightest weight, but it certainly wouldn't interfere with logstash's indexes if named differently.\nNot everyone will have redis, but everyone will have elasticsearch.  As silly as it sounds, the idea is simpler to handle in Elasticsearch.  It also becomes trivial to add other saved searches, cache results from logstash searches, etc.  I think using Elasticsearch is a fine idea.\n. Elasticsearch has no security whatsoever. There is no concept of read-only access to the elasticsearch data.  If one can access the elasticsearch IP on port 9200 to read, one will have access to do writes as well.  Therefore it won't matter where Kibana is, whether in another node or not.  My own setup has apache fronting kibana by way of mod_proxy, and then using passwords, SSL, and other apache security measures.\n. I would agree.  Whatever fields you have set in KibanaConfig should be the ones to have highlighting.\n. Second the motion on this one!\n. I would love something like this: per user filtering, or per group filters.  I had to make an install per filter and this would be really helpful.\n. What happens to elasticsearch if you run the search directly in the REST API (make a curl call)?\n--Aaron\nOn Jan 11, 2013, at 5:01 AM, nicovpp notifications@github.com wrote:\n\n@rashidkpc -> I have just tried on a not analyzed field for 200'000 events.\nThe behaviour is still the same for me. First ES process is taking CPU and then decrease. After, only kibana process is taking CPU and the I received Kibana's error\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND\n4386 root 20 0 2174m 1.1g 528 S 99.7 56.6 0:37.02 ruby\nAlso when this request failed I then need to kill kibana process.\nTerms option is working fine (answer in 2s) on these 200'000 events.\n\u2014\nReply to this email directly or view it on GitHub.\n. Worked wasn't exactly the question. The question was, \"What happens to elasticsearch when you make the query?\"  Descriptions of top, vmstat, etc. would be the idea here.  \n\nOn Jan 11, 2013, at 7:56 AM, dav3860 notifications@github.com wrote:\n\n@nicovpp : yes the pie chart is OK. Remember you must create a symbolic link \"public\" pointing to the \"static\" directory for Kibana to work with Passenger. And maybe clear your browser cache.\nFor your 200,000 events tests, \"score\" is implemented in ruby, not elasticsearch. This explains why the ruby process is taking CPU too. To me, it's just that if the computation takes too long (>30s), the included Thin web server closes your connection to the Kibana API. \n@untergeek : in my case, it worked.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "fseifts": ":+1:   I think Kibana's \"Score\" and \"Trending\" ability is AMAZING.  Plus, its seem way faster than Graylog2 0.9.6 -- I wish the two projects would merge :)\nHowever, Its quickly becoming evident that nothing comes without a cost.  I am only using Kibana for http/nginx access logs. There are a lot of entries I need to do (AND NOT OR)'s to get a decent view of my accessed URLs.  Not having saved searches is making this a nightmare.\n. ",
    "thomasfr": ":+1: \n. Just drop support for IE < 10 or/and add Google Chrome Frame. It just slows development for a great web app!\n. :+1: \n. Maybe, it would be enough to provide a button in the ui to quickly hide some event types.\n. :+1: \n. ",
    "joemiller": "+1\n. ",
    "gza": "As @awheeler said, you could bookmark the url or write them in a wiki until it is implemented\n. Hi,\nThe Url horrible hash-like string is a base64 encoding of kibana view and query.\ntry to decode it (without the #) you'll see a json string like this :\n{\"search\":\"apache\",\"fields\":[\"@fields.status\",\"@fields.request\",\"@fields.referrer\",\"@fields.received_from\",\"@fields.received_at\",\"@fields.method\",\"@message\"],\"offset\":0,\"timeframe\":\"86400\",\"graphmode\":\"count\",\"time\":{\"user_interval\":0},\"mode\":\"\",\"analyze_field\":\"\",\"stamp\":1355435411667}\nI guess you could write a tool to compose it.\n. Hi, it is a base64 encoding of your current search, timeframe, columns, etc parameters.\nI don't think it could be much shorter\n. ",
    "kitchen": "+1\n. ",
    "edgeofnite": "+1\n. ",
    "nenadl": "+1\n. ",
    "cadalso": "+1\n. ",
    "mrjcleaver": "+1 ( and you can cancel https://github.com/rashidkpc/Kibana/issues/340 , duplicate, sorry.)\n. Thanks ruckc!\n. +1 for needing this.\n. I agree\n. ",
    "sts": ":+1: \n. ",
    "Grauwolf": ":+1: \n. ",
    "roock": ":+1: \n. ",
    "mente": ":+1: \n. ",
    "thedug": "+1\n. ",
    "cybervedaa": "+1\n. ",
    "rahulgagrani": "now there appears to be a way you can pin a query, click on the green dot and it allows to pin the query.\n. ",
    "normanhh3": "In my recent experience it appears that pinning the query and then saving the dashboard does NOT mean save my pinned queries the next time I load this dashboard!  Yikes!  That seems like a major UI hole.  Am I missing an important step or is that just not supported?\nUpdate: Apparently it is intermittent. :-(  Maybe this is a case of eventually consistent two node cluster semantics?\nIf you are curious the default index for Kibana dashboard storage is \"kibana-int\".\n. ",
    "mattcadorette": "The output from kibana.rb:\n[30/Oct/2012 13:27:57] \"GET /api/graph/count/10000/eyJzZWFyY2giOiIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6OTAwLCJncmFwaG1vZGUiOiJjb3VudCJ9/?_=1351618077949 HTTP/1.1\" 200 3449 0.0174\nX.X.X.X - - [30/Oct/2012 13:28:00] \"GET /images/ajax-loader.gif HTTP/1.1\" 304 - 0.0019\nNoMethodError - undefined method []' for nil:NilClass:\n        ./lib/kelastic.rb:111:infield_type'\n        kibana.rb:204:in GET /api/analyze/:field/mean/:hash'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1264:incall'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1264:in compile!'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:835:in[]'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:835:in route!'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:851:inroute_eval'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:835:in route!'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:872:inprocess_route'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:870:in catch'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:870:inprocess_route'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:834:in route!'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:833:ineach'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:833:in route!'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:936:indispatch!'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:769:in call!'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:921:ininvoke'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:921:in catch'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:921:ininvoke'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:769:in call!'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:755:incall'\n        /usr/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/xss_header.rb:22:in call'\n        /usr/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/base.rb:47:incall'\n        /usr/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/path_traversal.rb:16:in call'\n        /usr/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/json_csrf.rb:17:incall'\n        /usr/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/base.rb:47:in call'\n        /usr/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/xss_header.rb:22:incall'\n        /usr/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/session/abstract/id.rb:205:in context'\n        /usr/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/session/abstract/id.rb:200:incall'\n        /usr/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/logger.rb:15:in call'\n        /usr/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/commonlogger.rb:20:incall_without_check'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:136:in call'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:129:incall'\n        /usr/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/head.rb:9:in call'\n        /usr/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/methodoverride.rb:21:incall'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/showexceptions.rb:21:in call'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:99:incall'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1389:in call'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1471:insynchronize'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1389:in call'\n        /usr/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/connection.rb:81:inpre_process'\n        /usr/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/connection.rb:79:in catch'\n        /usr/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/connection.rb:79:inpre_process'\n        /usr/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1037:in call'\n        /usr/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1037:inspawn_threadpool'\n        /usr/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1033:in initialize'\n        /usr/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1033:innew'\n        /usr/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1033:in spawn_threadpool'\n        /usr/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1023:indefer'\n        /usr/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/connection.rb:51:in process'\n        /usr/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/connection.rb:39:inreceive_data'\n        /usr/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:187:in run_machine'\n        /usr/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:187:inrun'\n        /usr/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/backends/base.rb:63:in start'\n        /usr/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/server.rb:159:instart'\n        /usr/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/handler/thin.rb:13:in run'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1350:inrun!'\n        /usr/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/main.rb:25\n        kibana.rb:307\n. 0.19.10\n. That fixed it for me, I can now view stats on numeric values --- thanks!\n. ",
    "TheGU": "I found exactly the same problem as this one. \nFor configuration\non logstash I follow this example : http://logstash.net/docs/1.1.4/tutorials/getting-started-simple\nfor Kibana I just extract and run.\nEverything work fine except when you click on stats button on the left fields.\n. Confirm that this error happen on every fields. \n. ",
    "jmilburn": "I just installed/configured logstash/redis/elasticsearch/kibana yesterday and am also seeing this. Happens on every field for me also.\n. I have applied the fix and it appears to work for most of them, and we get \"Statistical analysis unavailable for @message back to logs - I'm not able to analyze @message. Statistical analysis is only available for fields that are stored a number (eg float, int) in ElasticSearch\", which is fine as we are still configuring this.\nHowever, it is still not working for \"@tags\" and we get the errors below. Not sure if everyone has checked this and it may not be something we use but getting a 500 error is never ideal.\nKibana error - If it helps, I received a 500 Internal Server Error from: api/analyze/@tags/mean/eyJzZWFyY2giOiJtb2JpbGUiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjE0NDAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTM1MzMxOTc3NzUwOSwibW9kZSI6Im1lYW4iLCJhbmFseXplX2ZpZWxkIjoiQHRhZ3MifQ==?_=1353320847690\nKibana log error -\nNoMethodError - undefined method []' for nil:NilClass:\n    ./lib/kelastic.rb:111:infield_type'\n    kibana.rb:204:in GET /api/analyze/:field/mean/:hash'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1264:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1264\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:835:in []'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:835:inroute!'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:851:in route_eval'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:835:inroute!'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:872:in process_route'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:870:incatch'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:870:in process_route'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:834:inroute!'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:833:in each'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:833:inroute!'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:936:in dispatch!'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:769:incall!'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:921:in invoke'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:921:incatch'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:921:in invoke'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:769:incall!'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:755:in call'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/xss_header.rb:22:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/base.rb:47:in call'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/path_traversal.rb:16:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/json_csrf.rb:17:in call'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/base.rb:47:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-protection-1.2.0/lib/rack/protection/xss_header.rb:22:in call'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/session/abstract/id.rb:205:incontext'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/session/abstract/id.rb:200:in call'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/logger.rb:15:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/commonlogger.rb:20:in call_without_check'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:136:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:129:in call'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/head.rb:9:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/methodoverride.rb:21:in call'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/showexceptions.rb:21:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:99:in call'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1389:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1471:in synchronize'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1389:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/connection.rb:81:in pre_process'\n    /usr/local/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/connection.rb:79:incatch'\n    /usr/local/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/connection.rb:79:in pre_process'\n    /usr/local/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1037:incall'\n    /usr/local/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1037:in spawn_threadpool'\n    /usr/local/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1033:ininitialize'\n    /usr/local/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1033:in new'\n    /usr/local/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1033:inspawn_threadpool'\n    /usr/local/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:1023:in defer'\n    /usr/local/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/connection.rb:51:inprocess'\n    /usr/local/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/connection.rb:39:in receive_data'\n    /usr/local/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:187:inrun_machine'\n    /usr/local/lib/ruby/gems/1.8/gems/eventmachine-1.0.0/lib/eventmachine.rb:187:in run'\n    /usr/local/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/backends/base.rb:63:instart'\n    /usr/local/lib/ruby/gems/1.8/gems/thin-1.5.0/lib/thin/server.rb:159:in start'\n    /usr/local/lib/ruby/gems/1.8/gems/rack-1.4.1/lib/rack/handler/thin.rb:13:inrun'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/base.rb:1350:in `run!'\n    /usr/local/lib/ruby/gems/1.8/gems/sinatra-1.3.3/lib/sinatra/main.rb:25\n    kibana.rb:307\n84.19.53.21 - - [19/Nov/2012 10:11:58]\n. ",
    "dwinter3": "Yeah this http://people.iola.dk/olau/flot/examples/stacking.html is what I was looking for.  \nDavid Winter | Director of Hosted Solutions\nZenoss, Inc. | Transforming IT Operations\ndwinter@zenoss.com | +1.804.496.1731 | Skype: davewinter3\nOn Wednesday, October 31, 2012 at 6:49 PM, Rashid Khan wrote:\n\nmmm yes, I also want this feature. Shouldn't be hard to implement, flot supports stacked charts.\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/rashidkpc/Kibana/issues/124#issuecomment-9965094).\n. \n",
    "zombiepig01": "Add my vote to this (of course, maybe I should learn ruby and help out =8-).   \nBeing able to do something like '@type:blar split by @fields.foo' and have different colored boxes for each value of @fields.foo would be extremely helpful.  Also, something like: \n- (@type:blar AND @fields.foo:123) && (@type:bing AND @fields.x:999)\nand have the events appear as different colors.\nLogstash rocks, kibana makes it rockier!\n. ",
    "Yggdrasil": "Thanks!\n. Hi rashidkpc, could you have a look at merging this before things grow too far apart? Would be much appreciated. Thanks!\n. ",
    "josegonzalez": "@rashidkpc <3\n. Is this mergeable?\n. Is this mergeable?\n. ",
    "jamtur01": "Just use sinatra-static-assets.\n. @fabn I rebased this and fixed some minor issues - https://github.com/rashidkpc/Kibana/pull/246\n. I'd actually recommend adding https://github.com/emk/sinatra-url-for/ to Kibana - it means this works out of the box instead of hacking around it in deployment.\n. This breaks Kibana for me as the LogStash default indexes doesn't have hourly in them. Might be worth leaving as the default minus hours and then add a doc update explaining how to index hourly?\n. Or make it an array with both options?\n. See https://github.com/rashidkpc/Kibana/pull/198.\n. Ditto +1.\n. ",
    "craigshannon": "I also agree that highlighting only the fields defined within KibanaConfig should be adequate. I had a look in the query.rb code and I think something like the following could help do the trick in the section where the query is put together ~line 53. I'm not sure though how Kibana interprets the replies as this made no difference to the display of the results within the GUI:\n\"highlight\" => {\n      \"tag_schema\" => \"styled\",\n      \"fields\" => {\n          KibanaConfig::Default_fields => {\"pre_tags\" => [\"\"], \"post_tags\" => [\"\"] }\n          }\n       }\n. Does anyone know what code would need changed for this to work?\n. Has anyone been able to look into a solution for this? Looking at the elasticsearch docs this should be simple enough, but I think that the Kibana CSS might be doing something on top of the results that are returned?\n. Thanks for the response, I thought that Kibana must have been manipulating the reply from Elasticsearch to strip out the HTML tags. I will try and investigate further now that I know what Kibana is doing in the background. \nThanks!\n. Thanks, this will be very useful!\n. Is there any reason why this also isn't merged into kibana-ruby-auth?\nThanks.\n. ",
    "johntdyer": "+1\n. ",
    "Gabe82": "I rashidkpc i have a question for you. Is there a method to export kibana3 in pdf? I tried with wkhtmltopdf but i obtained only blanks pages. \nThanks in advance.\n. ",
    "darkwarriors": "there is any plan or update for this option?\n. ",
    "jdjsolano": "Hi guys! \nAny updates on this request? I think it would be a really helpful feature!\nThanks!\n. ",
    "prune998": "HTTP auth module was a way to have auth for Kibana (php) the easy way... \nIf Kibana have an internal login/group/log filter feature, it is even better.\nI saw they made a auth_ldap module, which is what I need/want/use with mod_auth_ldap for now.\nWaiting for the merge !!\n. ",
    "pixelvitamina": "+1\n. ",
    "eranrund": "Thanks for the quick response!\n. ",
    "timukas": "Hello,\nOnly this lines were changed:\n...\n  Elasticsearch = \"172.28.28.27:19200\"\n  KibanaHost = '172.28.28.27'\n...\nThe rest of file uses default settings.\nHere's line from config.php (older php-Kibana which works)\n...\n  'elasticsearch_server' => \"172.28.28.27:19200\",\n...\nThe rest of settings are also default.\nHere's some info about my setup:\nOS is CentOS 6.3\nElasticsearch version: 0.19.8\nRuby version: ruby 1.9.3p194 (2012-04-20 revision 35410) [x86_64-linux]\nHere's elasticsearch.yml settings:\n...\ncluster.name: logz\nnode.master: true\nnode.data: true\nindex.number_of_replicas: 0\npath.data: /elastic/20121010/elasticsearch\npath.work: /elastic/20121010/elasticsearch\npath.logs: /elastic/20121010/elasticsearch\nnetwork.host: 172.28.28.27\ntransport.tcp.port: 19300\nhttp.port: 19200\n...\nHope this helps,\n. ",
    "VoodooZ": "+1 Still applies to absolute latest git which really messes up the proper ordering of events for our application logs.\n. +1. Now that I got multiline java exceptions handled properly I see the need for this..\nKeep up the excellent work!\n. ",
    "netantho": "Ok, my bad. If you want there's just the grammar error for \"integration\" line 40.\n. ",
    "tkuther": "+1. This currently keeps me off using Kibana for weblogic logs.\nSomething like a configuration setting for max. @message lenght in collapsed view, and displaying the full event only in detail view would be really handy.\n. maxlines is now configurable, minor var cleanup\n. The last commit does this by removing the if field == '@message' condition. The loop this runs in already works on default_fields, so this should do it.\n. I used this for the last 3 months on a larger deployment for weblogic logs without issues. I'd be glad if it gets merged.\n. ",
    "heinemannj": "I see this issue too - but only in Kibana2.\nIn Kibana3 it works as espected ;-)\n. I think there is no further support for Kibana2\nSo upgrade to kibana3 and secure it with your webserver authentication features as described in Kibana3 FAQs\n. I've made some investigations with omelasticsearch, the output module for rsyslog.\nThe integration of rsyslog and elasticsearch was very easy.\nThe implementation of kibana in this case was very tricky ...\n100% aggreement for this pull request!\n. Have a look at request #240\nFor security reasons all back- and frontends should have authentication and role based access controls. \nElasticsearch has no authentication and authorization by default - with the implementation of elasticsearch jetty plugin ( https://github.com/sonian/elasticsearch-jetty#readme ) elasticsearch can now handle SSL connections, support basic authentication and authorization, and log all or some incoming requests in plain text or json formats.\nSo we have to discuss two things:\na) Securing the elasticsearch frontends like kibana\n100% agreement to above discussion ;-)\nb) Securing the backendsystem elasticsearch with basic authentication over jetty for example\nTherefor a possibility to define an elasticsearch UID and password in Kibana config file and further kibana code is mandantory.\n. Any new status on merging kibana-ruby-auth and kibana-ruby?\nWhat's about authentication and user management in Kibana3?\n. I absolutly agree!\n1++\n. I have testet several things:\nHighlighted_field = \"@source_host\" - works\nHighlighted_field = \"@message\" - works\nHighlighted_field = \"@message, @source_host\" - NOT working\nHighlighted_field = \"_all\" - NOT working\n# A field needs to be specified for the highlight feature. By default, \n  # Elasticsearch doesn't allow highlighting on _all because the field has to \n  # be either stored or part of the _source field.\n  Highlighted_field = \"_all\"\nMyt ES mapping:\n{\n    \"template_elma\" : {\n        \"template\" : \"rsyslog-*\",\n            \"settings\" : {\n                \"index.number_of_shards\" : 1,\n                \"index.cache.field.type\" : \"soft\",\n                \"index.refresh_interval\" : \"30s\",\n                \"index.store.compress.stored\" : true,\n                \"index.query.default_field\" : \"@message\",\n                \"index.number_of_replicas\" : 0\n            },\n            mappings: {\n                \"events\" : {\n                    \"_all\": { \"enabled\": true },\n                    \"properties\" : {\n                        \"@customer\" : { \"type\" : \"string\", \"index\": \"not_analyzed\", \"include_in_all\" : true },\n                        \"@fields\" : { \"dynamic\" : \"true\",\n                            \"properties\" : {\n                                \"facility\" : { \"type\" : \"integer\", \"index\": \"not_analyzed\" },\n                                \"facility_label\" : { \"type\" : \"string\", \"include_in_all\" : true },\n                                \"priority\" : { \"type\" : \"integer\", \"index\": \"not_analyzed\" },\n                                \"processid\" : { \"type\" : \"string\", \"index\": \"not_analyzed\" },\n                                \"program\" : { \"type\" : \"string\", \"index\": \"not_analyzed\" },\n                                \"severity\" : { \"type\" : \"integer\", \"index\": \"not_analyzed\" },\n                                \"severity_label\" : { \"type\" : \"string\", \"index\": \"not_analyzed\" }\n                            }\n                        },\n                        \"@message\" : { \"type\" : \"string\", \"include_in_all\" : true },\n                        \"@source\" : { \"type\" : \"string\", \"index\": \"not_analyzed\" },\n                        \"@source_host\" : { \"type\" : \"string\", \"include_in_all\" : true },\n                        \"@timestamp\" : { \"type\" : \"date\", \"format\" : \"dateOptionalTime\", \"index\": \"not_analyzed\" },\n                        \"@type\" : { \"type\" : \"string\", \"index\": \"not_analyzed\" }\n                    }\n                }\n            }\n        }\n    }\n}\n. Seems to be the same as https://github.com/rashidkpc/Kibana/issues/277\n. You can test it on my demo system: https://elma-demo.dyndns.org/\nUsername: guest\nPassword: elma4ever!\n. ",
    "xavierbaude": "Hello,\nTo avoid searching 2 hours in google, try this  \nYou can custom it with your own field.\ncurl -XPUT localhost:9200/template/logstash_per_index -d '{\n  \"template\": \"logstash*\",\n  \"settings\": {\n    \"index.query.default_field\": \"@message\",\n    \"index.cache.field.type\": \"soft\",\n    \"index.store.compress.stored\": true\n  },\n  \"mappings\": {\n    \"_default\": {\n      \"_all\": {\n        \"enabled\": false\n      },\n      \"properties\": {\n        \"@message\": {\n          \"type\": \"string\",\n          \"index\": \"analyzed\"\n        },\n        \"@source\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@source_host\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@fields.syslog_hostname\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@fields.syslog_program\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@source_path\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@tags\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@timestamp\": {\n          \"type\": \"date\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@type\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        }\n      }\n    }\n  }\n}' \n. ",
    "s3u": "The offending line is\nindex = indices.first\nLooks like an array check is missing.\n. ",
    "sgzijl": "As we speak, the last merges work fine for me. Anything else that must be done for this to merge in upstream?\n. ",
    "organicveggie": "User error. Log entries did not have a timezone. As a result, they were interpreted as UTC and got put into the wrong index for the last 6 hours of every day. \n. ",
    "blysik": "I'm having this problem too, but a little different.  @type:syslog works fine.  @type:apache has the issue where 15m has no results, 4h has plenty.\nI have Apache logging in json, and logstash shipping to redis to ES without any filtering.  Here's what my timestamps look like:\n\"@timestamp\": \"2013-02-11T19:40:12-0800\"\nIs there anything wrong with this format?\n. I think this problem is getting clearer.  My apache logs aren't the problem.  They obviously have a timezone.  However, my @type:syslog messages don't.  They are in localtime, PST, and don't include an offset or anything.  So the date filter defaults to UTC.  I have to add an offset to the syslog messages.\n. ",
    "stonith": "I'm doing this via apache reverse proxy.\n. I'd suggest setting up Kibana with mod_passenger (Phusion Passenger) for Apache. It performs better and autostart apache is already included.\n. ",
    "lingej": "+1\nProxyPass         /kibana-ruby/ http://localhost:5601/\n        ProxyPassReverse  /kibana-ruby/ http://localhost:5601/\n. +1\n. ",
    "ruckc": "With passenger... and DocumentRoot /var/www/html\nln -sf /opt/kibana/static /var/www/html/kibana\nkibana.conf\nRackBaseURI /kibana\n<Location /kibana>\nAuthType CAS\nrequire valid-user\n</Location>\n<Directory \"/var/www/html/kibana\">\nOptions -MultiViews\n</Directory>\n. ",
    "Nagilum23": "+1\nand also very good for learning!\n. ",
    "alouche": "Why not simply use something like Firebug (https://getfirebug.com/) and save the hassle to implement it in the app? See screenshot\n\nMore than enough IMHO.\n. External URL shortener require payment to make the generated urls private... most will be using the free API service which means there is a legal and security concern which is introduced when generating these short urls - assume for an instance, someone is crafting a query to look for an API/token key? Base64 encoded hash is obviously reversible and having these hash exposed is a concern.\nFurthermore, indices are rolled and previous ones often deleted, queries are often within small time frames unless your time frame is always bigger than the time within each rolled indice took place, I doubt whether persistent short urls do truly make sense, since these URLs in most case will be quickly outdated and no longer match existing events - IMHO, I think the concept of temporary urls (aka transient as I submitted are more suited for a search application).\n. ",
    "lloydde": "@alouche my Firebug output doesn't look like that. Does recent Kibana still use the same mechanism?\n+1\n. @rashidkpc very nice, very subtle!\nShould this issue be closed then?\n. ",
    "mrichar1": "Resolved this issue - had placed kibana in a directory outside of /var/www from where cgi-esque actions weren't allowed.  Moving to /var/www/kibana and adjustign conf fixed this problem.\n. ",
    "truthtrap": "I found the commit with a fix for this!\nThanks,\nJurg.\n. ",
    "utsahdevops": "Thanks. Works very well now. \nAlso wanted to thank you for this project. \n. ",
    "nitper": "here's a couple screenshots:\nunfiltered\n\nfiltered\n\n. Ah - I only use one index pattern. However, even your way with multiple index patterns would never work for any range >2 indices. \n@ https://github.com/rashidkpc/Kibana/blob/82992a260badf922cd0177191a28d9d38f85a34c/lib/kelastic.rb#L70\nThe loop condition was:\nruby\nwhile (step_time += KibanaConfig::Smart_index_step) <= to\nAnd requested indices were added like this:\nruby\nrequested << from.getutc.strftime(index)\nbut from was never incremented, so it would keep adding the first index until the loop finished.\nIn fact, the only reason yours worked with a range of 2 indicies is because of this line AFTER the loop:\nruby\nrequested << to.getutc.strftime(index) unless requested.include? to.getutc.strftime(index)\nWithout that, you wouldn't have even gotten your 2013.01 indices in there.\nI think now you basically just need to move step_time = from AFTER for index in index_pattern do\n. ",
    "vgirnet": "Oh, formatting issue.\nLine should start with dash (#), and next word should be \"description:\"\n\"# description: kibana daemon\"\n. ",
    "xiaclo": "I am experiencing the same issue.  It doesn't appear to be using ES when it times out, but a processing timeout after the ES query is done.  I can successfully run this up to about 30k queries.\nOddly, the CPU usage does not go down after the query times out as noted above.\nIs there a way to optimize these queries to be heavier on ES and lighter in the ruby-side processing, or at the very least, allow them to be multithreaded in some way?\n. ",
    "lucabelluccini": "Is it not possible to make the timeout be taken into account by Thin?\nI am in a really confined linux setup (no way to ask for installing something on our machines), so no nginx + passenger...\n. ",
    "jyotiruhil": "hi everyone\nnicovpp\ni got the same error\ncould you please tell me in detail to resolve the error\n500 internal server error\n. ",
    "michaelgibson": "What bug was fixed?\nThis fix breaks the expected results for me when using multiple index patterns, monthly indexing and the new year rolling over.\nSmart_index_pattern = ['logstash-log1-%Y.%m',\n                         'logstash-log2-%Y.%m']\nkelastic.rb:70 requested << step_time.getutc.strftime(index)\nWhen searching the last 30 days, requested ends up equaling these values\nlogstash-log1-2012.12\nlogstash-log1-2013.01\nlogstash-log2-2013.01\nThere should be a logstash-log2-2012.12 index but it is skipped. The previous commit was intended to resolve this issue by not incrementing the \"from\" value since that should remain static? \nkelastic.rb:70 requested << from.getutc.strftime(index)\nI am sure that had some other undesired effect. could you please let me know what it broke?\n. Yup that works.\nThanks!\nhttps://github.com/rashidkpc/Kibana/pull/237\n. Pull 250 should address this.\nhttps://github.com/rashidkpc/Kibana/pull/250\n. ",
    "kiranos": "Here is my screenshots with the same issue:\n\n\n\nI also have errors with alltime, I get far less results. Using latest kibana from website at the time:\nrashidkpc-Kibana-v0.2.0-19-ge8fe7f2.zip\n. Installed latest git master today and seems to be fixed for me\n. +1  would be nice to have this fixed\n. ",
    "pardeepchahal": "Thanks - After I made these changes , my server is up and running. But when\nI try to open in browser its not giving anything\nsudo ruby kibana.rb\n== Sinatra/1.3.3 has taken the stage on 5601 for development with backup\nfrom Thin\n\n\nThin web server (v1.5.0 codename Knife)\nMaximum connections set to 1024\nListening on 10.212.65.213:5601, CTRL+C to stop\n\n\n107.21.226.196 is static IP assigned to above machine and we have opened\nport 5601 for that machine.\nServer seems up and running but nothing is displayed in browser.\nOops! Google Chrome could not connect to 10.212.65.213:5601 error is coming.\nPlease suggest.\nOn Thu, Jan 10, 2013 at 10:01 PM, Rashid Khan notifications@github.comwrote:\n\n107.21.226.196 is an EC2 IP. Is 107.21.226.196 actually assigned to an\ninterface? I'm guessing its probably nat'd to a private address. Run\nifconfig to find the actual address of your instance and use that.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rashidkpc/Kibana/issues/236#issuecomment-12105265.\n\n\nWith Best Regards,\nPardeep Chahal | Lead Consultant - Release and Configuration Management\nPhone:  +91-9711689137 | Email: pardeep.chahal@hcentive.com\nSkype : pardeepchahal4 | B-34/2, Sector-59, Noida- 201301\nhttp://www.hcentive.com\nTechnology Solutions to Simplify Healthcare\n. ",
    "koddsson": "I updated ES to 0.20.2, logstash to 1.1.9 and kibana to the git version and that solved my issue.\n. ",
    "sacampa": "Hello.... did you find a solution for this:\n\"Is there a possibility to define UID and password in Kibana config file?\nI have to secure elasticsearch with jetty basic auth\"\nI have the same problem.\nRegards...\n. ",
    "frankyaorenjie": "Thanks! I'm rookie on elasticsearch. I've heard of mapping and naming before. There will be a long time for learning. Thank you, dav!\n. I've solved this problem, Chinese people can go to http://baniu.me/2013/01/kibana-in-flume/. Not-Chinese can email me baniu.yao at gmail.com.\n. ",
    "rtoma": "I have merged this pull request in a private fork. It works great!\nIn my setup I have logstash process multiple types, all writing to their own elasticsearch index.\nBefore the apply my events from multiple types were grouped by type and sorted by timestamp. Very annoying if you want to correlate an issue using multiple (log)types.\nWith this PR I now see events sorted by timestamp, no matter what type they are.\nThanks!\np.s. why is this excellent PR waiting 8 months now for merge? Is Kibana2 end of life?\n. I have merged this excellent PR to a private fork. And it does fix the annoying 'timeto' value.\nThanks for your work!\n. ",
    "tlieblfs": ":+1: I recently started playing with Kibana and didn't like how it was tied so closely to logstash (though I like logstash too). I was about to submit this same pull request but you beat me to it. :)\n. ",
    "bninja": "307 has similar change.. :+1: to either\n. :+1: need this too\n. oops, looks like #307 has taken care of this already\n. ",
    "alexrbct": ":+1: This is exactly what I have been looking for. Thx alouche!\n. ",
    "chris-wood-mobcast": "I installed passenger, which has fixed it.\nCheers,\nChris.\n. It's certainly seems to perform a lot better now.\nThanks again,\nChris.\nOn 18 January 2013 14:39, Rashid Khan notifications@github.com wrote:\n\nThanks Chris, I should probably note that Passenger is the recommended\nmethod for deploying to production\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rashidkpc/Kibana/issues/253#issuecomment-12424320.\n\n\nChris Wood\nSystems Administrator\nMobcast Services Ltd - a TESCO Company\n07917 758 107\nSkype: chriswood12385\n64 Great Eastern Street, London EC2A 3QR\n. ",
    "daveallen": "That is so weird.  I wiped out the install dir 3 times and ran bundle install all 3 times.  I even installed it in a completely separate directory by hand and ran bundle install and it did the same thing.  Now it won't fail any more.  lol  Thanks!  You must have scared it.\n. ",
    "prymitive": "+1 for splitting it, and maybe add an option to auto create user accounts in Kibana if they login correctly into LDAP\n. wrong branch, sorry\n. ",
    "diranged": "Just have to say that authentication on the frontend is important ... but so is the backend. I would love to put up an Apache or Nginx SSL reverse proxy in front of my ElasticSearch daemons and use HTTP AUTH (or SSL Certificate Verification) to make sure that my Kibana nodes have proper access to ElasticSearch directly, while no other clients do.\n. ",
    "goncha": "OK, I'll check it in the following days.\n. ",
    "masochist": "Can we get this pulled in, please? Right now I'm having to locally work around the content-type bug, which is icky.\n. ",
    "alinstoian": ":+1: \n. ",
    "lftgl": "+1\n. ",
    "outrunthewolf": ":+1: \n. ",
    "obaqueiro": "+1\n. ",
    "dmcd": "+1\n. ",
    "kobazik": "+1\n. ",
    "rasputnik": "This is a good idea.\nI'm running Kibana as a JRuby fat jar, so need an easy way to push in config settings \nwithout having to re-spin a WAR file. \nI was thinking about a config file though, are there downsides to that?\n. ",
    "robinsmidsrod": "I'm getting this issue as well, because ElasticSearch had an issue with memory and generated empty index. It would be useful if the index was either ignored or more clearly mentioned in the error message so that you know what index to investigate. Deleting the index solved the error message (and it did show up in the log file).\n. ",
    "garthk": "Ok, I think I've found the crux of the problem: ES and Kibana aren't handling time zones in the same way. \nChoosing \u201c12h ago\u201d, Kibana puts up a date range picker interface showing 2013-02-19 08:49:57 to 2013-02-19 20:49:57, matching the Tue Feb 19 20:49:57 EST 2013 timestamp on the hosts. \nThe most recent item shown is from 02/19 15:49:54 (with Timezone set to EST to match the host). That's from five hours ago, but at least some of Kibana is in the right time zone. \nThe graph isn't showing anything. The progress bar keeps advancing and then beginning again. \nKibana's first XHR request contains this ES query:\ncurl -XGET http://localhost:9200/_all/_search?pretty -d '{\n  \"query\": {\n    \"filtered\": {\n      \"query\": {\n        \"match_all\": {\n        }\n      },\n      \"filter\": {\n        \"range\": {\n          \"@timestamp\": {\n            \"to\": \"2013-02-19T20:49:57-05:00\",\n            \"from\": \"2013-02-19T08:49:57-05:00\"\n          }\n        }\n      }\n    }\n  },\n  \"from\": 0,\n  \"sort\": {\n    \"@timestamp\": {\n      \"order\": \"desc\"\n    }\n  },\n  \"size\": 50,\n  \"highlight\": {\n    \"fields\": {\n      \"true\": {\n        \"fragment_size\": 9999\n      }\n    },\n    \"pre_tags\": [\n      \"@KIBANA_HIGHLIGHT_START@\"\n    ],\n    \"post_tags\": [\n      \"@KIBANA_HIGHLIGHT_END@\"\n    ]\n  }\n}'\nThe ES response begins:\n\"took\" : 24,\n\"timed_out\" : false,\n\"_shards\" : {\n  \"total\" : 65,\n  \"successful\" : 65,\n  \"failed\" : 0\n},\n\"hits\" : {\n  \"total\" : 161145,\n  \"max_score\" : null,\n  \"hits\" : [ {\n    \"_index\" : \"logstash-2013.02.19\",\n    \"_type\" : \"syslog\",\n    \"_id\" : \"E_P0bhkIQoSdMob7d1crAg\",\n    \"_score\" : null, \"_source\" : {\"@source\":\"file://HOSTNAME/var/log/auth.log\",\"@tags\":[\"sysloggy\",\"grokked\"],\"@fields\":{\"timestamp\":[\"Feb 19 15:49:54\"],\"logsource\":[\"HOSTNAME\"],\"program\":[\"sshd\"],\"pid\":[\"11623\"]},\"@timestamp\":\"2013-02-19T20:49:54.255Z\",\"@source_host\":\"HOSTNAME\",\"@source_path\":\"/var/log/auth.log\",\"@message\":\"Feb 19 15:49:54 HOSTNAME sshd[11623]: Connection closed by 10.23.23.23 [preauth]\",\"@type\":\"syslog\"},\n    \"sort\" : [ \"2013-02-19T20:49:54.255Z\" ]\n  }\nI've checked the source host: it's logging with the correct time in EST. Feb 19 15:49:54 gets parsed to a @timestamp field of 2013-02-19T20:49:54.255Z by grok with a pattern of %{SYSLOGBASE}. \nKibana's fiter is asking for records to 2013-02-19T20:49:57-05:00 (-05:00 denoting EST), but ES is responding with records ending 2013-02-19T20:49:54.255Z (UTC). \nAt which end is it easier to fix this? What should I check next? \n. curl http://localhost:9200/_template/logstash_per_index returns:\n{\n  \"template\": \"logstash*\",\n  \"settings\": {\n    \"index.query.default_field\": \"@message\",\n    \"index.cache.field.type\": \"soft\",\n    \"index.store.compress.stored\": true\n  },\n  \"mappings\": {\n    \"_default_\": {\n      \"_all\": {\n        \"enabled\": false\n      },\n      \"properties\": {\n        \"@message\": {\n          \"type\": \"string\",\n          \"index\": \"analyzed\"\n        },\n        \"@source\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@source_host\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@source_path\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@tags\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@timestamp\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        },\n        \"@type\": {\n          \"type\": \"string\",\n          \"index\": \"not_analyzed\"\n        }\n      }\n    }\n  }\n}\nI don't believe I have any other views defined. \n. ",
    "stmontgomery": "If the page reloads when the URL changes, could the '+' and '-' links simply update the URL and let the watcher take care of loading the page? To work around this we've been clicking the '+' button and then clicking the 'Search' button which seems to re-compute the URL in the process including updating the list of fields.\n. ",
    "pdiaz1420": "[root@logsrv132att Kibana-0.2.0]# ruby kibana.rb\n== Sinatra/1.3.3 has taken the stage on 5601 for development with backup from Thin\n\n\nThin web server (v1.5.0 codename Knife)\nMaximum connections set to 1024\nListening on 127.0.0.1:5601, CTRL+C to stop\n\n\nIn the browser I use the server ip\nFrom: Richard Pijnenburg [mailto:notifications@github.com]\nSent: Friday, March 01, 2013 12:20 PM\nTo: rashidkpc/Kibana\nCc: Diaz, Philip\nSubject: Re: [Kibana] Kibana not loading in browser (#305)\nDid you also try to connect to 127.0.0.1:5601 instead of localhost?\nOtherwise you can also change it in the KibanaConfig file.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rashidkpc/Kibana/issues/305#issuecomment-14300396.\n. I was just about to reply back.  I changed the kibana host to my server and everything is good.  Thanks for all the help.\nFrom: Rashid Khan [mailto:notifications@github.com]\nSent: Friday, March 01, 2013 1:22 PM\nTo: rashidkpc/Kibana\nCc: Diaz, Philip\nSubject: Re: [Kibana] Kibana not loading in browser (#305)\nelectrical is likely correct, it sounds like you've installed on another server? Connecting to localhost would mean you've installed it on your workstation, if its on another system you need to put the IP address of that system (or 0.0.0.0) in KibanaConfig, then connect to it from your browser.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rashidkpc/Kibana/issues/305#issuecomment-14303411.\n. ",
    "butangero": "+1\n. ",
    "duylong": "https://github.com/duylong/Kibana/commit/e0b042bb1bbd3d73cabbee2a86dc394f3f0ea888\n. Sorry, my solution works with apache but not with ruby.\n. ",
    "chorsley": "Just gave it a try, and it fixes the problem listed above. Thanks for the quick response!\n. This sounds similar to #317 too. Potentially merge the two?\n. ",
    "furlongm": "I have tried with the following elasticsearch mapping but in kibana there is no difference in behaviour. Should there be, or are there further steps to make kibana not treat \"-\" as a token delimeter?\nindex:\n  analysis:\n    tokenizer:\n      token_filter:\n        word_delimiter:\n          preserve_original: true\n. ",
    "JiePan": "This bug annoys me too...\n. ",
    "bob-rove": "+1\n. This helped for me:\n1. in public/lib/js/ajax.js find definition $('#timeto').datetimeEntry({\n2. value of maxDatetime replace with new Date()\nbefore change:\n    maxDatetime: utc_date_obj(new Date()),\nafter change:\n    maxDatetime: new Date(),\nNOTE\nIn tar archive on page http://kibana.org/intro.html kibana version - 0.2.0. So ajax.js can be found here: static/lib/js/ajax.js\n. ",
    "ac0ra": "+1\n. ",
    "mmaassen": "+1 thank you. Helped me to\n. ",
    "cosmok": "+1\n@baggerist suggestion worked for me.\n. ",
    "Telmo": "it is already a gem, you just need to build it.\n. @jordansissel you will have the same issue with many of the gems out there, as a matter of fact any gem that has a c extension or depends on external libraries will have this issue. But Kibana does not suffer from this.\nBased on the kibana.gemspec file this is the list of gems it requires and their external dependencies:\n- sinatra: doesn't need to be compiled\n- json: I am assuming this is for Ruby 1.8 compatibility json is standard with 1.9, doesn't need to be compiled\n- fastercsv: this is for Ruby 1.8 compatibility, doesn't need to be compiled\n- tzinfo: doesn't need to be compile \n- daemons: doesn't need to be compiled\nThe only required gem that needs to be compiled is \"thin\" and that's because think requires eventmachine which needs to be compiled (debian/ubuntu provides the ruby-eventmachine deb, RH/CentOS/etc provide rubygem-eventmachine, OSX ... well if you are installing this in OSX its probably development.. you should probably know what you are doing and already using brew)\nGems are a cross-platform packaging system, only 1% of the gems need to be compiled, on top of that most modern OS already ship with a version of ruby packaged, and they offer this gems that need to be compiled as a native package, the mysql and mysql2 gems are infamous for this.\nNew users should just be able to do gem install kibana edit a config file and go. I agree that the current state of packaging of the project is not finalized but 95% of the work is already there and the gem does build without issues. \nThe KibanaConfig.rb should be a json or yml file or the configuration should be done via a block passed to a config class within Kibana. I may work on that and create a pull request.\nI don't think logstash is an application to mirror after. I understand the reasoning behing everything together as a shippable package for agents. In a blackbox situation this may be a good thing to do, but then I am forced to install java to get it up and running. What if my blackbox doesn't have java, what if I want logstash to use my already installed and perfectly functioning ruby instance. What if every agent decides to follow this pattern and you end up with 5 ruby instances individually installed in your system and java to run them all. Taking it to the extreme, can you imagine if whenever you install chrome it wanted to install ChromeOS along with it, it you want to install IE it would install its own version of windows with it, etc, etc.\nJar files are bad, they force me to install java. Vagrant is brilliant, but I don't want to spin a VM to run Kibana, and Vagrant installs its components using chef or puppet... which will do a gem install.\nAlso, one of the strengths that I've found with Kibana and that should be really highlighted in the documentation is that you do not need logstash to use Kibana, kibana talks to elasticsearch and as long as the data is in the expected format in elasticsearch what put it there or how doesn't matter. I am currently running fluentd instad of logstash and Kibana is working perfectly, it may have started as a logstash frontend but it has grown beyond that\n. @ludwigb3 your issue is not kibana but eventmachine (which is required by thin which in turn is required by kibana) depending of your Linux flavor there are packages available with it already compile (see my previous post)\nThe only required gem that needs to be compiled is \"thin\" and that's because think requires eventmachine which needs to be compiled (debian/ubuntu provides the ruby-eventmachine deb, RH/CentOS/etc provide rubygem-eventmachine, OSX ... well if you are installing this in OSX its probably development.. you should probably know what you are doing and already using brew)\nanother solution (although I don't recommend it) would be to break the thin dependency and have it use a different webserver to serve the app.\n. ",
    "ludwigb3": "I do not have a C-compiler  available on the Linux box made available to me [non-root].  Do I need at least a pre-processor?  At the moment I do not see the ruby.h files, except in the logstash directory.   Can I use that one?\nappadm@fldevlxapplog001 ~/Kibana/\n$ bundle install --path  /opt/app/.gem/ruby/1.8/bin/\nFetching gem metadata from http://rubygems.org/.........\nFetching gem metadata from http://rubygems.org/..\nResolving dependencies...\nInstalling rake (10.0.4)\nInstalling daemons (1.1.9)\nInstalling diff-lcs (1.1.3)\nInstalling eventmachine (1.0.0)\nGem::Installer::ExtensionBuildError: ERROR: Failed to build gem native extension.\n/usr/bin/ruby extconf.rb\nmkmf.rb can't find header files for ruby at /usr/lib/ruby/ruby.h\nGem files will remain installed in /opt/app/.gem/ruby/1.8/bin/ruby/1.8/gems/eventmachine-1.0.0 for inspection.\nResults logged to /opt/app/.gem/ruby/1.8/bin/ruby/1.8/gems/eventmachine-1.0.0/ext/gem_make.out\nAn error occurred while installing eventmachine (1.0.0), and Bundler cannot continue.\nMake sure that gem install eventmachine -v '1.0.0' succeeds before bundling.\n. Got the sysadm to install gcc & g++, and a fuller ruby install [including ruby.h].  Compiled & running.  Thanks.\n. @camunsy.  We had selected Splunk, but then balked on the price.   I have Logstash/ElasticSearch/Kibana2 working well in a proof of concept VM environment, using rsyslog and Nxlog as logshippers on Linux/Windows boxes.  Looks good, though hard core evaluation of its scalability & wide-spread usability with big data, applications, etc. will part of a formal project that is still gearing up.  And I have not implemented authentication yet [will use a web proxy, which is also needed for Kibana 3 & to protect ElasticSearch].\nFor queueing, I was planning to use Redis, but do not see how that would work w/ Nxlog, which might be able to do the queuing itself. Actually what I really would like to know, is how I can set the Logstash 'type' in Nxlog agent rather than doing it on the Logstash server.\n. ",
    "grayaii": "dang. just got bit by this too. I'm trying to to install Kibana, but I got stuck on eventmachine, same as ludwigb3.  I kinda agree with Jordan to make it easier to install this tool.  Now I have to put my beer down, hunt down the sys admins to trouble shoot this, wait for the them to install everything that is needed, and by that time, my beer will be too warm.\n. OK. I got the sys admin to figure this out.  I think he ended up yum installing various ruby packages until it worked. @madAndroid I'm a complete noob at this. On the elasticsearch machine I only have redis/logstash/elasticsearch installed, and I have like 25 other machine running logstash, shipping logs to the elasticsearch machine.  It works.  I have no standalone web server on the elasticsearch machine, but elasticsearch and logstash has it's own webserver running, which is how I do searches, so I'm not sure how to install Kibana3. (I guess I can try creating a \"_site\" directory under elasticsearch/plugin and dump the files from Kibana 3 there to see if that works. In fact, I've never even heard of Kibana 3. I originally did a google search for \"kibana\" which took me to http://kibana.org/intro.html, and I followed the instructions there (ie... bundle install).\n. ",
    "madAndroid": "aren't you glad that you don't have to do any of this compilation bulls**t with Kibana3?\n. ",
    "cmera": "Are you guys able to use this effectively as a tool to displace Splunk?\nOn Wednesday, July 17, 2013, madAndroid wrote:\n\naren't you glad that you don't have to do any of this compilation bulls**t\nwith Kibana3?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rashidkpc/Kibana/issues/324#issuecomment-21105704\n.\n\n\nchristopher l mera\ncamusny@gmail.com\n917.731.2081\n. ",
    "Vikash082": "Check ur Elastic Search setting , if it is the backend which kibana is using or anyother DB u re using with Kibana.\n. ",
    "Eternity-RS": "Forgot to mention , even predefined filters are also not visible on left side of Kibana UI.\nBR // \n. ",
    "ghubnik": "I am experiencing the same issue. Confirmed that the built-in ES web UI is functioning, not too overloaded - i.e. it's reasonably responsive and new logs are showing up all the time.\nI'm running Kibana on the same host as the elasticsearch node, so am using Elasticsearch = \"localhost:9200\" in KibanaConfig.rb.\nI'm not 100% confident I've got all the ruby gems set up correctly because the system is behind a firewall and I had to install Kibana on a host that had access to rubygems repository and then copy the gems over to the actual system using the technique outlined here.\nWhen I fire up \"ruby --debug kibana.rb\" I see this:\n\n[04:25]:[root@myhost:Kibana-0.2.0]# ruby --debug kibana.rb \nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems.rb:1113 - no such file to load -- rubygems/defaults/operating_system\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/config_file.rb:50 - no such file to load -- Win32API\nException `NoMethodError' at /usr/lib/ruby/1.8/rational.rb:78 - undefined method `gcd' for Rational(1, 2):Rational\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31 - no such file to load -- sinatra\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31 - no such file to load -- Win32API\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:38 - no such file to load -- Win32API\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31 - no such file to load -- json\nUsing Ext extension for JSON.\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31 - no such file to load -- xml/parser\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:38 - no such file to load -- xml/parser\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31 - no such file to load -- xmlparser\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:38 - no such file to load -- xmlparser\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:38 - no such file to load -- xmlparser\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31 - no such file to load -- xmlscan/scanner\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:38 - no such file to load -- xmlscan/scanner\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:38 - no such file to load -- xmlscan/scanner\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31 - no such file to load -- tzinfo\n./lib/query.rb:23: warning: method redefined; discarding old initialize\n./lib/query.rb:66: warning: method redefined; discarding old to_s\n./lib/query.rb:81: warning: method redefined; discarding old initialize\n./lib/query.rb:102: warning: method redefined; discarding old initialize\n./lib/query.rb:126: warning: method redefined; discarding old initialize\n./lib/query.rb:150: warning: method redefined; discarding old initialize\n./lib/query.rb:191: warning: method redefined; discarding old initialize\n./lib/query.rb:214: warning: method redefined; discarding old initialize\n./lib/client_request.rb:20: warning: method redefined; discarding old initialize\n./lib/client_request.rb:54: warning: method redefined; discarding old to_s\n./lib/client_request.rb:62: warning: method redefined; discarding old hash\n./lib/compat.rb:3: warning: method redefined; discarding old ruby_18\n./lib/compat.rb:6: warning: method redefined; discarding old ruby_19\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31 - no such file to load -- fastercsv\nException `LoadError' at /usr/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:31 - no such file to load -- thin\nException `NameError' at /usr/lib/ruby/gems/1.8/gems/eventmachine-1.0.3/lib/eventmachine.rb:1 - uninitialized constant EventMachine\n/usr/lib/ruby/gems/1.8/gems/thin-1.5.1/lib/thin/logging.rb:14: warning: instance variable @silent not initialized\n/usr/lib/ruby/gems/1.8/gems/thin-1.5.1/lib/thin/logging.rb:14: warning: instance variable @debug not initialized\n== Sinatra/1.4.2 has taken the stage on 5601 for development with backup from Thin\n/usr/lib/ruby/gems/1.8/gems/thin-1.5.1/lib/thin/logging.rb:15: warning: instance variable @silent not initialized\n>> Thin web server (v1.5.1 codename Straight Razor)\n/usr/lib/ruby/gems/1.8/gems/thin-1.5.1/lib/thin/logging.rb:14: warning: instance variable @silent not initialized\n/usr/lib/ruby/gems/1.8/gems/thin-1.5.1/lib/thin/logging.rb:14: warning: instance variable @debug not initialized\n/usr/lib/ruby/gems/1.8/gems/thin-1.5.1/lib/thin/logging.rb:13: warning: instance variable @silent not initialized\n/usr/lib/ruby/gems/1.8/gems/thin-1.5.1/lib/thin/logging.rb:13: warning: instance variable @trace not initialized\n/usr/lib/ruby/gems/1.8/gems/thin-1.5.1/lib/thin/logging.rb:15: warning: instance variable @silent not initialized\n>> Maximum connections set to 1024\n/usr/lib/ruby/gems/1.8/gems/thin-1.5.1/lib/thin/logging.rb:15: warning: instance variable @silent not initialized\n>> Listening on 0.0.0.0:5601, CTRL+C to stop\n\nAs it loads up, while running top, I hardly see any load at all from the ruby process. All the while, redis is bumping along at one or 2% CPU usage and the logstash monolithic jar JVM is consuming around 10% to 20% CPU. It looks like Kibana is doing no work at all.\nThe weird thing is that it looks like Kibana is not making any connections to port 9200 on the localhost. I've confirmed this by running tcpdump. When I do \n\n[04:36]:[root@myhost:~]# telnet localhost 9200\nTrying 127.0.0.1...\nConnected to localhost.\nEscape character is '^]'.\nblah\nConnection closed by foreign host.\n\nI see this\n\n[04:36]:[root@myhost:~]# tcpdump -nn -i lo port 9200\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on lo, link-type EN10MB (Ethernet), capture size 65535 bytes\n04:36:30.196429 IP 127.0.0.1.45701 > 127.0.0.1.9200: Flags [S], seq 2012529634, win 32792, options [mss 16396,nop,wscale 8], length 0\n04:36:30.196448 IP 127.0.0.1.9200 > 127.0.0.1.45701: Flags [S.], seq 2011457799, ack 2012529635, win 32792, options [mss 16396,nop,wscale 8], length 0\n04:36:30.196460 IP 127.0.0.1.45701 > 127.0.0.1.9200: Flags [.], ack 1, win 129, length 0\n04:36:33.330710 IP 127.0.0.1.45701 > 127.0.0.1.9200: Flags [P.], seq 1:7, ack 1, win 129, length 6\n04:36:33.330731 IP 127.0.0.1.9200 > 127.0.0.1.45701: Flags [.], ack 7, win 129, length 0\n04:36:33.333667 IP 127.0.0.1.9200 > 127.0.0.1.45701: Flags [F.], seq 1, ack 7, win 129, length 0\n04:36:33.333790 IP 127.0.0.1.45701 > 127.0.0.1.9200: Flags [F.], seq 7, ack 2, win 129, length 0\n04:36:33.333813 IP 127.0.0.1.9200 > 127.0.0.1.45701: Flags [.], ack 8, win 129, length 0\n\nHowever, when I start up Kibana it never gets beyond the message that it's \"fetching some interesting data\", and entering a search string also causes no output in the tcpdump.\nAnd the strangest thing of all, is that when I originally set up Kibana on the host that had access to rubygems, just to get started, I used an ssh tunnel to connect the ES server (I left Elasticsearch = \"localhost:9200\" in KibanaConfig.rb). It was working fine after I first set it up. Now that I try the same system, using ssh tunnelling, it's no longer working. Just like the instance I'm colocating with ES, when I'm not seeing any TCP packets from Kibana, but when I telnet the tunnelled port, I do see the packets.\nDo the exceptions in the debug output look like they're implicated in the symptoms I'm seeing? If not, is there further debugging I can try?\n. Update - it looks like the technique \"installing gems with no network\" that I used to try to satisfy dependencies was a bit too crude.\nRunning \"bundle check\" shows the following problems\n\n[22:18]:[kibana@myhost]# bundle check\nResolving dependencies...\nThe following gems are missing\n * diff-lcs (1.1.3)\n * eventmachine (1.0.0)\n * json (1.7.5)\n * rack (1.4.1)\n * rack-protection (1.2.0)\n * tilt (1.3.3)\n * sinatra (1.3.3)\n * thin (1.5.0)\n * tzinfo (0.3.35)\n * rspec-core (2.11.1)\n * rspec-expectations (2.11.3)\n * rspec-mocks (2.11.3)\n * rspec (2.11.0)\nInstall missing gems with `bundle install`\n\nFor comparison, \"gem list\" shows me I have slightly different versions of these gems installed:\n\n[22:37]:[kibana@myhost]# gem list\n*** LOCAL GEMS ***\nbundler (1.3.5)\ndaemons (1.1.9)\ndiff-lcs (1.2.3)\neventmachine (1.0.3)\nfastercsv (1.5.5)\njson (1.7.7)\nrack (1.5.2)\nrack-protection (1.5.0)\nrake (10.0.4)\nrspec (2.13.0)\nrspec-core (2.13.1)\nrspec-expectations (2.13.0)\nrspec-mocks (2.13.1)\nsinatra (1.4.2)\nthin (1.5.1)\ntilt (1.3.7)\ntzinfo (0.3.37)\n\nThe versions of these missing dependencies are different to what was installed.\nAs an extra data point, here's what Jordan provided as the correct list of dependencies.\n. ",
    "dkincaid": "I'm having the same issue here with a new install. Anyone have any ideas? My \"bundle check\" says that all the dependencies are satisfied. When I point my browser to \"http://localhost:5601\" it redirects to \"http://localhost:5601/#eyJzZWFyY2giOiIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6OTAwLCJncmFwaG1vZGUiOiJjb3VudCJ9\" then says \nOops! Something went terribly wrong.\nI'm not totally sure what happened, but maybe refreshing, or hitting Reset will help. If that doesn't work, you can try restarting your browser. If all else fails, it is possible your configuation has something funky going on. \nIf it helps, I received a 0 error from: api/search/eyJzZWFyY2giOiIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6OTAwLCJncmFwaG1vZGUiOiJjb3VudCJ9?_=1377282138169\n. ",
    "david-caro": "I have the same problem using the non-service version, and I've found out that if I specify http://localhost:9200 as the ES url in the configuration it works... (I created a tunnel from my machine to the actual ES server).\nIt seems that when trying to connect to non-localhost servers it requests the available methods with HTTP OPTIONS request and ES answers with PUT and DELETE only, maybe that's what it's stopping it from doing the usual POST request:\nRequest:\nOPTIONS /logstash-2013.09.05/_search HTTP/1.1\nHost: log-server.local:9200\nUser-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:23.0) Gecko/20100101 Firefox/23.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nOrigin: http://localhost\nAccess-Control-Request-Method: POST\nAccess-Control-Request-Headers: content-type,x-requested-with\nConnection: keep-alive\nPragma: no-cache\nCache-Control: no-cache\nResponse:\nHTTP/1.1 200 OK\nAccess-Control-Allow-Origin: *\nAccess-Control-Max-Age: 1728000\nAccess-Control-Allow-Methods: PUT, DELETE\nAccess-Control-Allow-Headers: X-Requested-With\nContent-Type: text/plain; charset=UTF-8\nContent-Length: 0\n. I've just tested putting an apache server in the middle that overwrite the Access-Control-Allow-Methods header to appends the GET and POST methods, but no luck.\nIt's really strange that the OPTIONS request is only done when the server is not localhost... I'll continue investigating.\n. Finally I was right, I forgot to add the Access-Control-Allow-Headers header too, with both headers it works. Here's the http config I used to test adding those headers:\n```\nListen *:9201\n\n  RewriteEngine On\n  Header set Access-Control-Allow-Methods \"PUT, DELETE, POST, GET\"\n  Header set Access-Control-Allow-Headers \"Content-Type,X-Requested-With\"\n  RewriteRule ^(.*)$ http://localhost:9200$1 [P,L]\n\n```\nThen using http://server-name:9201 as ES server in the configuration of kibana, it works (opening the ports too, of course). I think that it's more a problem of ES than kibana itself, ES should say that it supports those methods and those headers imho.\n. ",
    "jerryz1982": "This exception only occur when graylog2 is also storing logs onto elasticsearch. If graylog is stopped and elasticsearch is cleared, making logstash as the only source, the stat for @message would display a warning that stat is for number only rather than 500 internal server error, which i think is reasonable. \n. ",
    "lupujo": "Please reopen ticket.\n1 - Your suggestion doesn't work.\nAttached screenshots when searching for missing referrer - zero results (incorrect).\n\nand:\n\n2 - In Kibana UI, when clicking the \"Magnifying glass\" icon, the generated search query is in contrast to what you suggested above (at least this should count as a bug?):\n\nClicking the icon produces the following search page (mixed results - incorrect):\n\n. It's working - thank you.\nSo my bug report is that clicking the \"Magnifying Glass\" icon per screenshot above generates:\n   @fields.referrer:\"-\"\nInstead of:\n   missing:referrer\n. ",
    "Finkregh": ":+1: \n. ",
    "patrickshan": "+1\n. ",
    "ejsarge-gr": "Should not your event be written into logstash-2013-04-29? That is the logstash behaviour. There is currently a defect in the Apache Flume ElasticSearchSink where it chooses the wrong index - that defect has a patch available.\n. ",
    "kayrus": "input {\n  file {\n    path => [ \"/var/log/nginx/access_log\" ]\n    type => \"all\"\n  }\n}\noutput {\n  rabbitmq {\n    host => \"elastic\"\n    exchange_type => \"fanout\"\n    exchange => \"rawlogs\"\n    user => \"logging\"\n    password => \"pass\"\n  }\n}\n. I set type on the client side, \"all\" type on the server side and it works! Thanks a lot.\n. Actually I guess that this is Kibana bug. When I tried to search with the following string (backslashed slash):\nrequest:\"\\/?q=%D0%BE%D1%82%D0%BA%D1%80%D1%8B%D1%82%D0%BA%D0%B8\"\nsearch works well.\nIs it possible to fix it?\n. I mean the magnifier icon next to the field value.\n. ",
    "hmalphettes": "OK, I rebased on the new specs code for kelastic and added some specs to test the suggested changes when handling KibanaConfig::Elasticsearch.\nThanks for your attention.\n. I figured out that most of the action is happening on kibana-3: face-palm!\nUsecase is: run kibana on cloudfoundry.com and access elasticsearch hosted on cloudfoundry.com and protected by basic-auth; also protect the access to kibana.\nHere is my current solution if it is of interest to someone else:\nhttps://github.com/hmalphettes/kibana-proxy\n. ",
    "Leryan": "Yes, I've just seen the problem 5 minutes ago. Searching everywhere but the browser, it works with Firefox but not with Chrome and IE.\n. ",
    "bschrock": "Looks like this might be the problem?\nCaught this in Chrome developer tools:\nRefused to execute script from 'http://kibana.aws.ecnext.net:5601/js/timezone.js' because its MIME type ('text/html') is not executable, and strict MIME type checking is enabled. kibana.aws.ecnext.net/:1\nUncaught ReferenceError: tOffset is not defined\nAlso when I use git to update to master it works.\n. ",
    "ajardan": "Same thing, cannot view any data in latest Chrome, the page just keeps \"loading\"\n. ",
    "mikedougherty": "@bschrock is correct - the tOffset variable is not defined at the point that CreateLogTable is called. \nA non-trivial workaround is to use Chrome developer tools to enable a breakpoint before the exception occurs (~ ajax.js:958) and when the breakpoint gets hit, run window.tOffset = 0; in the console. Then, unset the breakpoint and resume execution. Kibana should work until the next page load.\n. ",
    "lnxmachine": "Dirty workaround, copy the contents of the /js/timezone.js from inside the chrome console (click the link it shows the contents), create a timezone.js file inside public/lib/js with the contents of the generated timzone.js.  Edit the index.html in the root of public, change the path to timezone.js:\n\nI don't  know anything about ruby, so I'm not sure how to fix the issue, but this allowed it to work again in Chrome until a fix is found.  I assume this probably breaks things for folks in different timezones.\n. ",
    "cakirke": "confirming i've seen this in Chrome on Windows and Mac, unsuccessful with lnxmachine's workaround\n. confirmed working on OSX 10.8.4/Chrome 27.0.1453.110\n. ",
    "JamesDooley": "It looks like this is due to the timezone.erb file not setting the mime type (or more specifically the kibana.rb file).\nIt does look like there was a patch submitted about 5 months ago that has not been merged in yet:\nhttps://github.com/goncha/Kibana/commit/8ffae5423547709228d65ef6d3d5754cae40f3e6\nIf you edit that line in the kibana.rb file and restart kibana you should be able to access the site using Chrome (and IE?)\n. Also looking at commit: https://github.com/rashidkpc/Kibana/commit/edae3863571f84ef80416527c1966b666022d482\nIt looks like you are calling erb :timezone twice now instead of just once. \nThanks for getting the fix pushed out Rashid.\n. ",
    "tanertopal": "+1\n. ",
    "giga-mk": "The problem is that when you use a wild card the query term gets lowercased by ES by default.\n(see: http://www.elasticsearch.org/guide/reference/query-dsl/query-string-query/ , lowercase_expanded_terms)\nI patched /common/lib/elastic.js to have {\"lowercase_expanded_terms\":false} as the default query at beginning of function ejs.QueryStringQuery (of course you need to generate/modify elastic.min.js too).\nWhile this ugly, it worked for me. I would propose some way to have this as a config setting, either globally or for the stringquery component\n. See this patch in elastic.js, for the min file you have to either generate it if you have the tools set up or find the right position by searching for e.g. ejs.QueryStringQuery.\nBest regards\n```\nejs.QueryStringQuery = function (qstr) {\n/*\n       The internal Query object. Use get().\n       @member ejs.QueryStringQuery\n       @property {Object} query\n       /\n  var query = {\n-      query_string: {}\n+      query_string: {\"lowercase_expanded_terms\":false}\n  };\nquery.query_string.query = qstr;\n```\n. this what i did for kibana3 (I used the elasticSearch API)\n\n# diff -u querySrv.js.orig querySrv.js\n--- app/services/querySrv.js.orig    \n+++ app/services/querySrv.js \n@@ -101,7 +101,7 @@\n               .size(q.size)\n               .facetFilter(ejs.QueryFilter(\n                 ejs.FilteredQuery(\n-                  ejs.QueryStringQuery(q.query || '*'),\n+                  ejs.QueryStringQuery(q.query || '*').lowercaseExpandedTerms(false),\n                   filterSrv.getBoolFilter(filterSrv.ids)\n                   )))).size(0);\n@@ -191,7 +191,7 @@\n       switch(q.type)\n       {\n       case 'lucene':\n-        return ejs.QueryStringQuery(q.query || '*');\n+        return ejs.QueryStringQuery(q.query || '*').lowercaseExpandedTerms(false);\n       case 'regex':\n         return ejs.RegexpQuery('_all',q.query);\n       default:\n\n. ",
    "Betsworth22": "Giga- Can you supply me with your modified elastic.js and elastic.min.js file? I'm not sure where to add the suggested fix above. My email address is cody.betsworth@gmail.com. If you don't want to send them can you describe where in these files I need to change these settings and what the end result should look like? Thanks for your time. \n. ",
    "vmassol": "I also need this and was sad to see it didn't make it in Kibana 3.0.0GA (see the issue I discussed at https://groups.google.com/d/msg/elasticsearch/ra4HDsQTcY4/KtRW9HdMEpkJ ). On that thread there was a solution to modify querySrv.js at line 106 but this files doesn't exist anymore in Kibana 3.0.0GA so I'm at a loss on how to port this change to our new install... Any help would be most welcome. Thanks.\n. Actually patching app.js (minified!) worked.\n. ",
    "kcbaltz": "I've tried the above patch to app.js in the latest Kibana 3 and it's not fixing the problem.  Specifically, I\"m trying to search access logs for \"POST*.html\" and it's not finding anything whereas a search for \"POST\" does.  I'm new to Elasticsearch and Kibana, so perhaps it's a setup issue.  I saw references to \"not analyzed\" and I don't know which kind of fields we have, so it's possible that's related.   \nUpdate:  I tried a query like the following and it also didn't return any hits, so maybe Kibana isn't the issue: \nhttp://localhost:9200/logstash-2014.08.01/_search?q=POST*html&lowercase_expanded_terms=false&pretty\n. ",
    "aochsner": "So, if you get mapping for an alias (http://host:port//_mapping) it returns the mapping for each index\nhttps://gist.github.com/aochsner/5732315\nI'm really really new to ruby so I'm not 100% sure what the right fix is...\n. This appears to be due to this change in the tzinfo gem http://rubyforge.org/tracker/index.php?func=detail&aid=7655&group_id=894&atid=3525 which became part of the 1.0 release.  Think we need to change timezone.erb to use utc_total_offset() now...will get a pull request going\n. Although (and frankly I'm so new to Ruby) I don't understand why my system used tzinfo 1.0.1 instead of 0.3.35 specified by the Gemfile.lock file\n. ",
    "aalwash": "Hi nodesocket,\nI had the same issue, if you do \nruby kibana-daemon.rb start -t\nThen you will see the error that is making it crash.\nI had a lot of missing packages and tmp directory wasn't created\nHopefully this will help other ppl as well\nGood luck\n. ",
    "Muxagen": "+1\n. ",
    "rajeshtg78": "if I use this syntax for number field , it is doing lexograohy search and not returning the expected result @fields.duh:[17 TO *]\n. Hi Rashid,\nCould you pls explain how can I use %{PATTERN:dur:int} ?\n\nThanks and Regards,\nRajesh\nFrom: Rashid Khan [mailto:notifications@github.com]\nSent: Tuesday, July 16, 2013 1:23 PM\nTo: rashidkpc/Kibana\nCc: Rajesh Gangadharan\nSubject: Re: [Kibana] How to use greater than or less than query (#384)\nIt is rather likely that your numbers are being stored as strings in elasticsearch. if you are using logstash and grok you might want to change your pattern to something like %{PATTERN:dur:int} to make sure that it is stored as an integer.\nClosing as not a Kibana bug\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rashidkpc/Kibana/issues/384#issuecomment-21070723.\n. I am not able to follow you. How can I go to logstash ?\n\nThanks and Regards,\nRajesh\nFrom: Rashid Khan [mailto:notifications@github.com]\nSent: Tuesday, July 16, 2013 2:20 PM\nTo: rashidkpc/Kibana\nCc: Rajesh Gangadharan\nSubject: Re: [Kibana] How to use greater than or less than query (#384)\nlogstash on freenode is a great resource for logstash questions. You can find me as rashidkpc in the #logstash channel.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rashidkpc/Kibana/issues/384#issuecomment-21074874.\n. We are using logstash tool only for our prod box ..\nBut duration query is not returning correct results, but for strings it does..\n@fields.syslog_program:\"offer-service\" AND @fields.dur:[\"100\" TO \"10000\"]\n\nThanks and Regards,\nRajesh\nFrom: Rajesh Gangadharan\nSent: Tuesday, July 16, 2013 2:40 PM\nTo: 'rashidkpc/Kibana'\nSubject: RE: [Kibana] How to use greater than or less than query (#384)\nI am not able to follow you. How can I go to logstash ?\n\nThanks and Regards,\nRajesh\nFrom: Rashid Khan [mailto:notifications@github.com]\nSent: Tuesday, July 16, 2013 2:20 PM\nTo: rashidkpc/Kibana\nCc: Rajesh Gangadharan\nSubject: Re: [Kibana] How to use greater than or less than query (#384)\nlogstash on freenode is a great resource for logstash questions. You can find me as rashidkpc in the #logstash channel.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rashidkpc/Kibana/issues/384#issuecomment-21074874.\n. ",
    "Smana": "Sorry that was really a noob one. I found out the answer in the config file.\nAnalyze_limit = 2000\nSee ya\n. ",
    "umarengasamy": "Is this related to time zone setting .?\nCaused by: java.lang.ArithmeticException: / by zero\n        at org.elasticsearch.common.joda.TimeZoneRounding$UTCIntervalTimeZoneRounding.calc(TimeZoneRounding.java:195)\n        at org.elasticsearch.search.facet.datehistogram.CountDateHistogramFacetCollector$DateHistogramProc.onValue(CountDateHistogramFacetCollector.java:105)\n        at org.elasticsearch.index.field.data.longs.SingleValueLongFieldData.forEachValueInDoc(SingleValueLongFieldData.java:105)\n        at org.elasticsearch.search.facet.datehistogram.CountDateHistogramFacetCollector.doCollect(CountDateHistogramFacetCollector.java:80)\n        at org.elasticsearch.search.facet.AbstractFacetCollector.collect(AbstractFacetCollector.java:89)\n. ",
    "odoucet": "This is the same error as #336 but my bug report is about error verbosity.\n. ",
    "alienzrcoming": "this is an out of the box installation with some logstash inputs & outputs, no filters.  can you offer up a hint of where this is configured?\n. thanks, got it and verified that it woks properly.  much appreciated\n. ",
    "dylanzr": "Actually, now that I don't erroneously quote my search it will process. Looking at the output from the events table I have modified my query. It looks like:\ncurl -X GET \"http://localhost:9200/logstash-2013.09.12/_search?from=0&size=1&pretty\" -d '{\n \"query\": {\n    \"filtered\": {\n      \"query\": {\n        \"bool\": {\n          \"should\": [\n            {\n              \"query_string\": {\n                \"query\": \"message:5\\\\/47\"\n              }\n            }\n          ]\n        }\n      },\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\n              \"match_all\": {}\n            },\n            {\n              \"bool\": {\n                \"must\": [\n                  {\n                    \"match_all\": {}\n                  }\n                ]\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}'\nI will investigate why the results are different from this query compared to what I expect from the above.\n. ",
    "bernhardschmidt": "I can confirm this issue. It could be related to the setup here. Our Kibana is on globally reachable (with Auth) SSL site, so the URL in the browser is https://server/kibana . But elasticsearch is not globally reachable, instead we use \"http://localhost:9200\" in config.js and port-forward 9200 on the client through SSH.\nThe following error is logged in the Firefox error console\n[18:29:06.293] Blocked loading mixed active content \"http://localhost:9200/_aliases\" @ https://server/kibana/src/vendor/angular/angular.js:10370\nChrome works fine.\n. More details here: https://blog.mozilla.org/tanvi/2013/04/10/mixed-content-blocking-enabled-in-firefox-23/\nSo as far as I can tell, the problem appears when you use HTTPS in the Kibana URL, but only HTTP to connect to elasticsearch. After disabling the protection on my server Firefox 24 renders Kibana just fine.\n. ",
    "alpha01": "This is a proxy config issue and not Kibana. Updating config.js fixed this: \nelasticsearch: \"https://\"+window.location.hostname+\":443\"\n. ",
    "dur3x": "Thanks for you fast response ;-) \nI think we will see the possibility to switch to Kibana 3\n. ",
    "holms": "So remove it from http://community.opscode.com/cookbooks/kibana\nCause it doesn't work, and everybody use kibana cookbook from luis anyway.\nMy libriarian pulls official one.\n. ",
    "andreamaruccia": "Please remove I've had the same issue, cheers :)\n. ",
    "anuragrk": "what webserver are you using ? If you are using Apache HTTP,place your kibana folder in htdocs folder inside the webserver directory.Restart Apache Server and test.\nHope this helps.\n. ",
    "e100": "I see the same issue in Firefox on Ubuntu 13.04, running kibana master.\n. ",
    "harrytruman": "I'm also encountering the same issue with Kibana 3 milestone 4.  Time intervals of less than 5 seconds being plotted on the graph will result in no bars displaying, e.g. 21:53:34 to 21:54:30.  Lines and points get displayed when bars do not.  I've tested with the current versions of Chrome, Firefox, Safari, and IE11.\nIs this a bug or merely a limitation?  I'd love if Kibana could simply switch from bars to lines for small time intervals.\n. ",
    "vijay2579": "Hi Rashid,\nYou have implemented using angularjs know so is it possible to make kibana\nto get data from mono database instead of elastic search\nRegards,\nVijay\nOn Sat, Dec 28, 2013 at 3:54 AM, Rashid Khan notifications@github.comwrote:\n\nKibana only supports elasticsearch\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/rashidkpc/Kibana/issues/417#issuecomment-31282928\n.\n. \n",
    "plockaby": "Whoops. I should have put this into the kibana3 repo.\n. ",
    "luongvinhthao": "Did you try nginx to proxy for web interface \n. ",
    "richadlr": "Thanks, created https://github.com/elasticsearch/kibana/issues/1009 instead.\n. ",
    "errordeveloper": "Not sure if there is a way to detect this somehow ... however, I guess there exists a gem for it :)\n. "
}
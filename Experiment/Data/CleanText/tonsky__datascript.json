{
    "tonsky": "I\u2019m not saying no, but let\u2019s first see where this version will lead us to.\n. thx!\n. Yes, components are probably needed. For now, however, we can also utilise the fact that you can put complex objects as values, like, put hasmap as a value. This does not work for Datomic, because they have strict schema, but for in-memory database it works fine. It should cover most of cases where components are needed in big Datomic.\n. I always was curious\u2014what\u2019s the grand idea behind marker protocols? I understand ordinary protocols, e.g. when something is ISequable, you can call -seq on it. But when something is ISequential, or IEntityMap\u2014what\u2019s the actual meaning of this? I guess, it means somebody decided to mark something that way, for the reasons unknown. But what should I do with that information? Should I mark my data structure ISequential? What would break if I did not? How is map marked as IEntityMap different from ordinary map? Why should it be treated differently? If I build entity myself, without doing entity call, should I do satisfy IEntityMap? Maybe I don\u2019t see something, but for me it\u2019s just useless to provide protocol without methods. Can you elaborate on that?\n. Sorry, I don\u2019t get it. Ok I have IEntityMap. What if I assoc? What if I dissoc? Why I cannot transact with map that come not from DB? For me, entity is just a map. You can put any map to database. It doesn\u2019t matter where it comes from. We shouldn\u2019t treat some maps different from others and build any expectations on that. Especially when it\u2019s as invisible as marker protocols are.\n. @robknight Data queried from DB is just data. It may come from q call of from entity call or from transaction report. These are all valid examples of data, and these are all perfectly legal ways to get it from DB. I don\u2019t understand why data that comes from entity should be treated special.\nWhat you say about Om cursors can be very easily implemented without such a strange thing as \u201cmarker protocols\u201d, in a very straightforward way: by providing an implementation for DataScript entity cursor:\n(defrecord DataScriptEntityCursor [conn eid ...]\n  IOmCursor\n  (transact! [_ k v]\n    (d/transact! conn [(assoc (d/entity @conn eid) k v)]))\n  (deref [_]\n    (d/entity @conn eid))\n  ...)\nI know how Om implements cursors (I wrote official wiki doc about them), but for me it always seemed like a lot of unnecessary magic. It\u2019s too fragile, hard to understand and has questionable user experience (now it\u2019s cursor, now it\u2019s not). Reliance of Om for third-party libraries to implement marker protocols is just a sign that something is wrong about that design.\nIt\u2019s easy to fix them, though: just make cursors explicit, like atoms or refs are explicit. Atoms do not pretend to work like a map or a vector. They are not ILookup nor ISequential. They  are containers with two operations: get value from it and put value inside. If cursors worked that way, it\u2019d be much easier to understand and work with them. @swannodette what do you think?\n. I won\u2019t merge IEntityMap.\nFollow refs is a subject to recursive infinite loops because it\u2019s not lazy.\nThanks anyway!\n. DataScript 0.3.0 now has lazy entities type & supports :db.type/ref auto-wrapping into entities\n. DataScript 0.3.0 now has lazy entities type & supports :db.type/ref auto-wrapping into entities\n. Agree\u2014will put it on todo list\n. Looks like entity cache would not work: entity has a reference to the DB, so every transaction we would have to throw away all the entities :(\n. Nice patch, thanks!\n. I have not decided yet if having integers-only ids can give me some implementation simplicity or performance benefits. It that would be the case, I may add such restriction later. Wouldn\u2019t commit to it yet.\n. Turned out to be a bug in entity call, thx!\n. Yeah, I forgot about that case :( Maybe we should go with something like Datomic\u2019s tempids or deduce value type from schema. Will think of something for the next version\n. I\u2019ll probably go with the schema, and leave negative ids as they works right now, without tempid wrapper. Entity to follow references is a little bit harder because there may be cycles so we need lazy entities for this to work properly. It\u2019s on the roadmap, but not on the closest one.\n. Check out test.datascript/test-resolve-eid-refs for usage example\n. DataScript DB is a persistent data structure. Database mutation is build in terms of pure functions. You can get full DataScript experience without touching atom/conn/listeners part: check out empty-db and with. For example, with works just like transact! except that it takes db and returns db, being, in fact, a pure function. Queries run over plain DB values, not over connections.\nThere\u2019s then a thin layer (optional, in fact), wrapping this DB into an atom (literally) and providing transact and listener facilities. It\u2019s build on top of the same immutable, pure primitives.\nDatabase swapping is atomic, there\u2019s no \u201ctemporary db states\u201d. Atom is atomically swapped from db-before to db-after when using transact!.\nYou can build your own mutation layer quite simply, just by using (swap! db-atom with ...) instead of transact!. You\u2019ll lose txReports, but overall experience will be the same. \nSo the only question remains, why TxReports?\nAtom\u2019s watch fn gives you just value before and after, but it does not capture the change. TxReport solves exactly that\u00a0problem: when using transact or transact!, it captures normalized deltas from db-before to db-after. Why may we need that?\nFirst, to monitor DB: you can run Datalog queries over tx-data to know when the part you\u2019re interested in have changed. It\u2019s much faster than run the same query over full DB. (See here, \u201cTransaction format happens to match database format...\u201d).\nSecond, it makes a great server sync format. You may add generic listener to DB that will mirror all changes to server backend for durability.\nThere\u2019s no incidental complexity to it. It\u2019s actually just more detailed version of atom, where not only value before and value after, but change itself is also data. All the data-oriented benefits apply.\n. What\u2019s wrong with the way it is right now?\n. This is excerpt from email discussion with @allgress :\n\nI also think that part of proper query analysis (the one you\u2019re doing by extracting indexes) is quite complex and hard to get exactly correct (it\u2019ll always be more of an approximation). So may be there should be simpler API, like, subscribe for datoms of this entity (by eid) or this attribute (by attr).\n\nI\u2019m not sure that the whole idea of \u201creverse\u201d query can be solved, even in datalog. But much simpler approach, like (listen! conn '[_ :user/name _]) or (listen! conn [room-id :room/messages '_]) might be doable \u2014 and, not ideal, but a huge win with a lot of use-cases. I\u2019m going to look into it soon.\n. @EwenG sorry, haven\u2019t looked at your code before, without analyse-q your API looks like what I\u2019ve planned, because we don\u2019t promise too much, only subscription to index we can efficiently support. I\u2019ll look closer into your implementation and report back\n. @jeluard yes, but haven\u2019t got time to implement it or do a review on PR. At some point, something like this, yes\n. I\u2019ve added custom serialisation for datascript/DB and corresponding read-db function. It is much more compact than direct record serialisation:\nhttps://github.com/tonsky/datascript/commit/696f59f9d44438c97e5055af950578ea4f480f9b\nPlease confirm that works for you\n. Resolved by 718dbb1b69b7d204972ff28caff577d05b745cfb\n. Hi @pangloss! Thanks for taking the time, it looks like a lot of work you\u2019ve done!\nI think lazy entity interface is useful and I\u2019d like to have one in DataScript. But I think it shouldn\u2019t support assoc/dissoc or incorporate any change semantic at all. Changeable entities that track their own history look like an ORM layer. It\u2019s really hard (if possible at all, I haven\u2019t seen one yet) to get right. ORM are always a mess and bring more trouble that solve. So I prefer entities to stay as \u201cview\u201d interface, but not as \u201cedit\u201d interface. It should also track references to other entities and components, as Datomic\u2019s one does.\nHistory may be useful, but I thought that for short-living, in-memory browser-based applications it\u2019s more important to save memory by default. So, if I work with my web app, I won\u2019t run out of memory after 10 minutes. Implementing history is trivial on top of transaction listener, for the ones who need it.\nChanging schema, again, I don\u2019t see the use case for it. You won\u2019t probably evolve user data while his browser window is open, will you? Am I missing something?\nI\u2019m not sure I understand what you\u2019re proposing for transact!, about raw data and string keys. Can you elaborate, maybe give some usage examples, and use cases?\n. I understand that it\u2019s based on immutable values, but still the usage pattern is very similar to ORM: you edit object and then expect, somehow, that these changes should make it back to the DB.\nDatomic transaction format is very simple and specific on what should happen. You can always see which changes are applied because they are explicit, right here in the code.\nYour proposal has two potential problems: it\u2019s implicit (changes are accumulated somehow, you do not see list of them build explicitly), and it\u2019s DB-version-dependent. When you say \u201capply this entity to current DB\u201d, it\u2019s unclear what should happen if entity is stale (based on DB few versions back). Take this example:\n(def e (entity db0 1)) ==> {:db/id 1 :name \"X\"}\n(transact! db0 [[:db/add 1 :name \"Y\"]]) ==> db1\n(transact! db1 [e]) ==> should name change back to \"X\"?\nAnd it gets more complicated for collections. You can, by mistake, remove items added by someone else. \nWe can solve this ambiguity by inventing some rules, but it will remain to be a source of confusion nevertheless. That\u2019s why here I\u2019m with Rich and Datomic team, vote for transaction format to remain explicit, and entities remain to be view-only tool.\n. I still don\u2019t see benefits of this way to build transactions. I prefer them to stay explicit, and don\u2019t mix entity (view) and transaction (update) together. These are simple, independent pieces. Editable entity is kind of a mix Rich warned about in \u201cSimple made easy\u201d. It may be easy, but not simple.\nThat said, I would happily accept lazy entity implementation built for view only, and working exactly as Datomic one does. I would very appreciate your help if you\u2019ll be willing to do this.\n. You\u2019re right about history support, it has to be implemented inside DB. I\u2019m still concerned about memory consumption (you can have tab open for days, and if you have grow-only DB, you\u2019ll eventually run out of RAM. Datomic in similar situation can swap to disk, browser can\u2019t), so maybe it should be an option, switched off by default.\nAlso, if we\u2019re about to track history, we should understand what kind of use cases are we going to support. Simplest one is log \u2014 all datoms, asserted and retracted, in transaction order. More interesting cases include \u201cget previous version of db\u201d (as-of call in Datomic), \u201cget db with all assertions and retractions\u201d (history call in Datomic).\nLet me think this through.\n. Thanks for taking your time! I\u2019ve made some comments on your commit. Turned out making a proper entity call is a lot of work and nuances :)\n. @pangloss I took some time to roll out lazy entities implementation. Not much of your code made it to the final version, but your code & discussion we had were a great inspiration. Thanks!\nCheck out 0.3.0 version: DataScript now has lazy entities type & supports :db.type/ref auto-wrapping into entities, also accessible from JS.\n. Yes, sure. One addition: I guess it should test for (and (coll? vs) (not (associative? vs))) so we filter out maps\n. Thanks for pointing that out \u2014 there was an error in touch implementation!\nAs for using touch in protocol implementations \u2014\u00a0for -count you need to enumerate entity datoms anyway, so I decided to cache them on the way. Same for -seq, although there might be lazy implementation of it (walking :eavt index), but again, not sure if it worth the effort. I guess nobody expect super-speed from these methods anyway.\n. Yes, you can do that, but it won\u2019t work with advanced compilation.\n. Also, in node.js env there\u2019s no js/window, in node it will fail too. I\u2019m not sure what\u2019s the best way to take here. I think that resolving fqn is more pain (you have to remember to export functions (and you may not know there\u2019s a problem unless you turn on advanced compile some time later), we have to repeat CLJS names transformation (which may change in a future)) \u2014\u00a0this is a part of code that just cannot be written reliably (at least for now).\n. Wow, cool, thanks! That helps\n. Indexes are required for fast query lookups and range searches. Slow access (full-scan) is already implemented for collections. IndexedDB would be a good data storage for DataScript, if only it had synchronous api. With callbacks, it seems very hard to use DataScript the ways I imagined it would be used.\n. It will ruin the whole \u201cdb as a value\u201d idea. You cannot do async callbacks during React render for example. Core.async callbacks will not allow for nested functions. And so on. Barriers where there shouldn\u2019t be any. It will force you to split model and view model, extracting latter from the rendering code. This is a wrong kind of decoupling, because rendering code has high cohesion, it\u2019s a mistake to break these apart because limitations of technology say so.\nAlso, it\u2019s not clear what benefits of storing data in IndexedDB are?\n. I\u2019m currently writing a blog post about this: https://github.com/tonsky/datascript-chat\nIf interested, you\u2019re welcome to take a look (still work in progress though)=\n. Here\u2019s a detailed walkthrough into datascript-chat sample application http://tonsky.me/blog/datascript-chat/\n. thanks for pointing that out, compare-key was unused\n. Another ClojureScript reader example can be found here (note that register-tag-parser! is preferable, of course, I just cannot use it in tests because it has side effects)\nI also have an example of using transit-cljs for seializing DB here\nMaybe it\u2019s time to create a doc/snippets.md with examples how common tasks can be done with DataScript\n. Now there\u2019s Tips&tricks page which has EDN serialization example and link to transit serialization\n. No good way yet, sorry\nOn Mon, May 2, 2016 at 9:17 AM Ahmed Fasih notifications@github.com wrote:\n\nSorry to comment on this old issue. Is there a recommended way to persist\non JVM? The most straightforward way I can think of is \"periodically spit\ndb to /tmp/random-file, then move /tmp/random-file to mydb.edn, overwriting\nthe latter\". If there was a way to avoid spitting the entire db when only a\nsingle fact changed, that'd be a nice optimization\u2014i.e., overwrite only the\nportions of the file that changed.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/26#issuecomment-216098174\n. Yep. I\u2019ll probably add pretty print writer for Entity type, but there\u2019s a twist: entities can reference other entities, and these references might become recursive. For example, your snippet suffer from that issue.\n. Well, I don\u2019t remember. Maybe it was an attempt to optimize rule calculation. Your patch worked \u2014 I wrote slightly different tests, but essentially it fixed it. Thanks!\n. @robert-stuttaford , sorry, I was really busy and haven\u2019t looked at your issue.\n\nFirst, make sure your project.clj looks like this:\n{ :id \"prod\"\n    :source-paths [\"src\"]\n    :compiler {\n      :externs  [\"datascript/externs.js\"]\n      :output-to     \"....min.js\"\n      :optimizations :advanced\n    }}\nSecond, what exact version of DS you use?\nThird, if that\u2019s possible, maybe you can give me access to source code so I can take a look at it? Maybe partially (I\u2019m interested most about parts where you create data, where you select it and where you display it, and project.clj of course). prokopov@gmail.com\n. Oh, missed that when looked at your github. Actually I have a hypothesis, try (.log js/console (pr-str query)) right before you execute a query. Judging from symptoms, it looks like that symbols in :find and in :where are, for some reason, different.\n. You have pretty complex build schema out there :) So this is what I believe is going on: you build core modules and radiant modules separately and then include them both on a single page. Advanced compilation doesn\u2019t work like that. If you compile something separately you\u2019ll probably get same symbols assigned in first and in a second case to very different things. Google Closure compile does aggressive renaming in order to minimize code size, so it renames everything to names like a, ab, ga, shortest names possible. If you do two independent compilations you\u2019ll get name clashes all around the place. It\u2019s miracle anything works at all. Try build single JS file via one compilation step and put everything in it.\nAlso you put Google Analytics to the page. It binds to ga symbol (unfortunately, quite short). I had a case once where even that caused name clash with compiled cljs code. So I ended up adding ga to externs file for that project. Consider that too.\n. @robert-stuttaford Can you keep me in the loop if you can make it work with cljs-compile? Maybe there\u2019s some incompatibility between DS and shadow-build, if that\u2019s the case, I would look into that deeper\n. Here\u2019s what I found:\nIf you put externs file to src/ folder of your project (I used src/stuttaford/ext.js), everything compiles and works nicely. But if you reference them as \"datascript/externs.js\" they don\u2019t work. Looks like shadow cannot load them from classpath for some reason.\nProbably related comment here https://github.com/thheller/shadow-build/blob/master/src/clj/shadow/cljs/build.clj#L910\nI\u2019ll close the issue for now as problem seems to be not in DataScript\n. I managed to get it work:\n``` diff\ndiff --git a/project.clj b/project.clj\nindex e9a55bf..fdb6418 100644\n--- a/project.clj\n+++ b/project.clj\n@@ -25,13 +25,13 @@\n   :profiles {:dev {:source-paths [\"dev\"]\n                    :dependencies\n                    [[org.clojure/tools.namespace \"0.2.7\"]\n-                    [thheller/shadow-build \"0.9.3\" :exclusions [org.clojure/clojurescript]]\n-                    [org.clojure/clojurescript \"0.0-2342\"]\n+                    [thheller/shadow-build \"0.9.5\" :exclusions [org.clojure/clojurescript]]\n+                    [org.clojure/clojurescript \"0.0-2371\"]\n                     [om \"0.7.3\"]\n                     [prismatic/om-tools \"0.3.3\" :exclusions [org.clojure/clojure]]\n                     [racehub/om-bootstrap \"0.2.9\" :exclusions [org.clojure/clojure]]\n                     [sablono \"0.2.22\" :exclusions [com.facebook/react]]\n-                    [datascript \"0.4.0\"]\n+                    [datascript \"0.5.1\" :exclusions [org.clojure/clojurescript]]\n                     [com.cemerick/url \"0.1.1\"]\n                     [cljs-http \"0.1.16\"]\n                     [secretary \"1.2.1\"]]\n@@ -49,6 +49,5 @@\n                           sablono.core\n                           secretary.core\n                           datascript]\n-           :externs     [\"datascript/externs.js\"]\n            :modules     [{:id :codex   :main stuttaford.codex}\n                          {:id :radiant :main stuttaford.radiant}]})\ndiff --git a/dev/shadow.clj b/dev/shadow.clj\nindex 313a812..7938c9e 100644\n--- a/dev/shadow.clj\n+++ b/dev/shadow.clj\n@@ -46,7 +46,8 @@\n                            :pretty-print  true}\n               :production {:optimizations :advanced\n                            :pretty-print  false\n-                           :externs       (compose-externs externs)}))))\n+                           :externs       (compose-externs externs)\n+                           :ups-externs   [\"datascript/externs.js\"]}))))\n(defn configure-source-paths [state source-paths]\n   (reduce (fn [state source-path]\n```\nStill puzzled though what\u2019s the difference between :externs and :ups-externs and why react/externs/react.js works in the former but datascript/externs.js only works in the latter.\nHere\u2019s part I looked at: https://github.com/clojure/clojurescript/blob/master/src/clj/cljs/closure.clj#L188\n. @thheller totally agree! thanks for bringing up that issue. Posted on google group: https://groups.google.com/d/topic/clojurescript/LtFMDxc5D00/discussion\n. Converting data is a performance hit, of course. But DS does not require namespaced attributes, :name or :title would work fine. It\u2019s convenient to use namespaced attrs so you can differentiate entity types by attribute name (e.g. :person/name is certainly a person, but :name could be person, car, dog, anything). If you don\u2019t have this ambiguity, you\u2019ll do fine without namespace. Another option is to have special \u201c:type\u201d attribute.\nBiggest penalty comes not from attributes, but from managing cross-entities references. In DS, entities should reference each other by :db/id attribute, it should be a number and it should be unique among all entities. So you cannot directly assign entities ids = PK from SQL database, as in different tables there may be overlapping PKs.\nThere\u2019re two solution to this. First is to let DS assign ids and store PKs in external id attribute. This is how we convert to DS format in acha-acha: https://github.com/clojurecup2014/acha/blob/clojurecup/src-cljs/acha.cljs#L418\nSecond is to tune your DB so that different tables have non-overlapping id ranges. It allows for PK to be used directly as :db/id in DataScript. We moved to this solution in Acha-acha, as it eliminates lookup by external id at conversion step and makes server pushes much easier to handle. Here\u2019s an example for SQLite https://github.com/clojurecup2014/acha/blob/0.2.0/src-clj/acha/db.clj#L67\n. I\u2019m trying to understand what this feature request is about. Are we talking about something like this?\nDataScript stores transaction id in datoms already (tx), so only thing missing is ability to reference id of current transaction inside transaction data?\nI don\u2019t want to implement automatic :db/txInstant generation because I want to keep ability for DS database to work in a constant space. This can be added by user manually when needed.\n. Well, I\u2019m following Datomic model here, and it has the same flaws: entities are hard to compare due to their lazy nature. Entities are way to look and navigate around database, and it\u2019s hard to tell, for implementing equality, what parts of DB are you looking at. You can access other entities from the one you\u2019re looking at. You have User entity, but you can use it to render User, his company, friends list, etc.\nThat makes it hard to use entities as \u201cdata source\u201d for Om, Quiescent or any other render that relies on equality for shouldComponentUpdate optimizations. If immediate User attributes have changed or not tells you nothing about if you should re-render your component. If you want that optimization, you\u2019ll need to know what attributes you\u2019re going to use, then just convert entity to hashmap and compare hashmaps.\nIn my usage of DataScript, I was using entities, but I dropped shouldComponentUpdate checks and re-rendered everything on each frame. Not as perfect as Om or Quiescent, but works fine. This is the same approach bare React uses.\nOf course, if you have better ideas on the issue, I\u2019d love to hear them.\n. I don't want uncontrolled references expansion. Uncontrolled reference expansion may lead to very serious perf problem without programmer\u2019s knowledge (and no explicit way to mitigate that). Even if entity is small, it may have just one reference and now you\u2019re walking whole DB. Datomic has \u201ccomponent\u201d feature to support that, and eventually I want the same for DataScript. \nI also don't agree it\u2019s faster that re-rendering. It isn\u2019t. React does no real job unless output of your \u201crender\u201d function changes. When I say \u201cre-render everything\u201d, it means we generate all virtual DOM, but React knows exactly the part of real DOM that needs to be changed. And it works, and performs really well, and that\u2019s what they expect you to do on bare React anyway.\nNow, in case of re-rendering, we\u2019re rebuilding whole virtual DOM tree, but while doing so, we\u2019re accessing only properties we actually need. If we start to optimize entity equality, we\u2019ll need to fetch all properties and expand all references beforehand, then do the equality check, and only after we decide if we need to build virt dom branch. You can avoid virtual dom calculation, but add much more fuzz just in order to do so. I really doubt it will be an optimization at all. I mean, in most cases, it\u2019ll simply perform worse.\nIf you still think that\u2019s a good idea you can write your own entity function and experiment on your app with different equality strategies. There\u2019s no magic inside DataScript entity, it can be implemented in user\u2019s code as well.\n. \u0441\u043f\u0430\u0441\u0438\u0431\u043e!\n. Well, it\u2019s mostly \u201calpha quality\u201d for three reasons:\n- Few projects I\u2019m aware of use it, so I cannot reliably estimate amount of bugs unfound\n- I still want to be able to do breaking changes, as not everything I planned is yet implemented\n- Documentation and error reporting are not in place yet\nApart from that, it works pretty well (I was surprised myself when started to use DS that I don\u2019t fix bugs at all, it just works). Developer experience still sucks though, because if you made a mistake in a query, it doesn\u2019t tell you where, just produces strange results. If that happens, I\u2019ll be here to help.\nI cannot promise anything about timing of going to beta phase yet, as I don\u2019t have a stable working schedule on DS. All I can promise is that I can assist on DS adoption with latency small enough.\n. Oh boy, that\u2019s a lot of stuff\n. @dthume cool, will look into your patch soon. Looks like a lot of good stuff\n. @dthume wow, thank you for putting that much effort into DS! That\u2019s huge!\nI looked into the code, looks good. Test suite is amazing, parser is very good, pull loop is a little bit heavy-weight (still studying your code here).\nSo, my question is, do you have time and desire to keep working on this until it\u2019s merged into master? Right now there\u2019s couple of changes I want to make (I described them in comments, please take a look). If you want to put some more time and finish this, please create a PR and we can continue discussion/code refinement/reviews there. If you\u2019re busy right now, I can take your commit as-is and continue to work on it by myself.\nAnyway, thanks again, this is a really bold addition (biggest in DataScript so far).\n. Added in 0.9.0 by @dthume \n. Underscores sure are a problem. When you write [:db/add X :test/_bar Y] it reads backwards, meaning \u201cput a reference from Y to X via :test/bar property\u201d. So it expects both X and Y to be integers. In your case, you put hashmap at Y pos, and that wouldn\u2019t work as an entity id.\nSo two problems for me to fix here:\n1. Error message should be improved (sorry about that)\n2. Error should be thrown in second case (you don\u2019t get it here by pure luck):\n(d/transact! conn [{:db/id -1 :test/_bar {:baz \"qux\"}}])\n. datomic allows to specify maps when creating nested relation, but ds does not support this feature yet. what about backward relation, I'm no sure that even datomic supports that=\n. Please check this out: should work https://www.npmjs.org/package/datascript\n. Yep, I see a lot of value in server-side DataScript. My plan is to cover most of the basics of browser version and then build a Clojure port. It would take too much effort to develop two versions in parallel as I still change a lot of stuff and figure out some things/insights, and I don\u2019t want to do it twice.\nSo the answer is yes, there\u2019ll be a Clojure version.\n. This could be added, but would make sense only for apps that use multiple DataScript DBs.\nNote that Datomic\u2019s DB id does not change as you make transaction to the single database. It\u2019s a constant id assigned to DB at its creation (like \"datomic:free://localhost:4334/xxx\") to distinguish different connections to different transactors.\nE.g. this code will still return true in Datomic:\n(let db1 (d/db conn)\n      _   @(d/transact conn [[:db/add id :name \"abc\"]])\n      db2 (d/db conn))\nAnother strategy is to have DB change its id after each transaction, effectively make all entity equal checks to return false (pessimistic case: if DB changed in any way, we behave like all entities have changed). Not sure is this strategy is valuable or not, I mean, I don\u2019t see cases when this saves us anything, it\u2019s almost the same as not doing equality checks at all.\nI do not have strong opinion on this, but both options seems lousy to me.\nMaybe a pragmatic solution is to use pull API when you need optimization for compare and, if using entities, do not rely on entities equality checks at all.=\n. No, it\u2019s cool to have it all in one commit. See my comments though https://github.com/montyxcantsin/datascript/commit/3b35adc42a933aaa71cf7d62b786d361f8c4ef7a\n. Sorry, my bad \u2014 aggregation fns will never see empty coll anyway. Merged into dev, thx!\n. Merged into dev, thx\n. I can reproduce that, thx, looking into right now\n. Fixed in 0.7.2, thanks for the heads-up!\n. I\u2019m planning to add this functionality as I\u2019m often find myself in need for reset! to swap DB value without transaction, but with listeners. Best you can do right now is probably something like this:\n(defn db-reset! [conn db]\n  (let [report (datascript.core/map->TxReport { :db-before @conn, :db-after db})]\n    (reset! conn db)\n    (doseq [[_ callback] @(:listeners (meta conn))]\n      (callback report))))\nIt depends on some internals, but should fit for now until I add something like this to the API.\n. @cigitia yep, sounds reasonable\nOn Sat, Feb 28, 2015 at 10:03 PM cigitia notifications@github.com wrote:\n\nIt\u2019ll be wonderful when this gets added. When it does get implemented, it\nmight be best to implement it as a transaction function that can be used in\ntransact!, like :db/add and :db.fn/cas, rather than a function separate\nfrom transact! that acts directly on connections.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/45#issuecomment-76531892.\n. Just added reset-conn! function to reset database value inside a conn and trigger all listeners. Usage example: https://github.com/tonsky/datascript/blob/master/test/datascript/test/conn.cljc\n\n@cigitia I would pass on your idea for now, if you need that kind of function inside a transaction it\u2019s not that hard to build for youself. Transactions totally support user-provided functions\n. @nahuel,\n:db/id is not a real attribute. It is used to create \u201cmap\u201d forms of an entity at insert stage. When you write\n(d/transact! db [{:db/id -1 :name \"Mazinger\"}]\nit means, essentially, this: take all key-value pairs from this map, convert them to [:db/add eid attr value] form where attr=key from map, value=value from map, and eid (entity id) is a special key :db/id from map. It\u2019s just a convention. Results from entity calls support this convention too, as they have to represent datoms as maps.\nBut inside DataScript everything is stored as flat datoms of form [e a v], no maps. If your entity has 10 attributes, there\u2019ll be 10 distinct datoms. Maps are just a convenient view of these internals.\nQueries are written in pattern-match style over actual flat datoms, so you cannot use :db/id as it is not an actual datom attribute. When matching over entity id, just use first position, not last one:\n(d/q `[:find ?name\n       :where [~new-id :name ?name]] @db)\nBTW it\u2019s not recommended to use actual values inside query. For parametrized queries, use addtional :in agruments:\n(d/q '[:find ?name\n       :in $ ?e\n       :where [?e :name ?name]] @db new-id)\n. Sorry, I don\u2019t understand what the issue is. I get #{[5] [3] [4] [1] [6] [2]} as a result, which seems ok.\nNamespaces have no special meaning in DataScript. :name and :city/name are two completely different keywords.\n. The snippet you\u2019ve provided, if I execute it literally, as is, it returns #{[5] [3] [4] [1] [6] [2]}, at least for me. Please help me to reproduce what you\u2019ve experienced\n. Problem is with you using keyword for :db/id instead of number. This leads to these strange behaviours you observe. I don\u2019t have validation here (yet), but you can\u2019t do that. Entity ids are always integers. I will reopen this issue to add :db/id validation in the next version.\n. @jumblerg just pushed 0.8.0\n. Sure, eventually :)\n. That\u2019s right, it\u2019s a rewrite of a query engine, it\u2019s not used by default\nbecause it doesn\u2019t do rules, fn/pred calls and probably something else.\nOn Wed, Feb 24, 2016 at 2:44 PM Bahul Neel Upadhyaya \nnotifications@github.com wrote:\n\nThere's an experimental query namespace called v3 that negation and joins\nwork on, some other things don't though so your mileage may vary.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/50#issuecomment-188141118.\n. > As far as I can tell, query_v3 doesn't get exposed via the JS interop\nlayer. Just to be clear, negation and disjoins only work in it?\n\nYes. It\u2019s not considered part of DS API in Clojure either. It\u2019s just where\ndevelopment happens.\nOn Wed, Feb 24, 2016 at 11:26 PM Terin Stock notifications@github.com\nwrote:\n\n@bahulneel https://github.com/bahulneel You put it in quotes like that. [image:\n:smile:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/50#issuecomment-188362499.\n. I doubt I\u2019ll be able to revive query_v3 any time soon, so more realistic approach would be to add those to the current query impl. Phew. Issue from 2015. Finally implemented not/or/not-join/or-join in 0.17.1. Enjoy!. That\u2019s amazing! Thanks for such a big contribution!\n. Well, idents are trickier because they not only affect input (you specify keyword, it gets resolved to eid), but also output (e.g. in Datomic results you won\u2019t get entity id or attribute id if that eid has ident, you\u2019ll get keyword in instead).\n\nDatomic sure needs idents to save bandwidth/storage (so they can store integer attribute ids instead of strings/keywords which take a lot of space). They explicitly don\u2019t recommend using idents for anything else but attrs and enum values (see http://docs.datomic.com/identity.html#sec-3).\nBut for in-memory implementation like DataScript you don\u2019t need to save space (you operate with pointers anyway, so all keywords point to the same location in memory). You can also use keywords directly to name attributes and enum values. I find this model easier to understand and easier too implement too.\nIdents are also not free, they have to be resolved twice (on input and on output). That\u2019s extra hashmap or index lookup per, say, returned entity.\nSo I\u2019m still deciding if idents are needed. If you guys can name couple of use-cases, it would be helpful.\nMeanwhile, ident resolution can be emulated with [:db/ident :special-key] kind of lookup refs.\n. I understand that. My question is how are you using idents in Datomic then.\n. Ok, I\u2019ll look into it later\nOn Sat, Mar 7, 2015 at 11:01 AM cigitia notifications@github.com wrote:\n\nI'd like to affirm that I find idents useful in both use cases mentioned\nby others above:\n- I use idents to easily use singleton objects, such as an \u201capp\u201d\n  entity that contains things like whether a certain UI mode is on or off. If\n  one needed to store the text currently entered into a box an application\u2019s\n  UI, there is then no other place to store it other than some single\n  entity\u2014and that single entity will need to be modified in transactions in\n  the future.\n- I also use idents to refer easily to enumerated objects, such as\n  states or categories of other entities, in code that needs to refer to\n  specific categories. Those categories might also have attributes of their\n  own, such as end-user display names, but each one changes the application's\n  logic in a different way, so the code need to be able to directly refer to\n  them, especially in transactions.\nWithout idents, as far as I know, in order to modify any of those\nsingleton and singleton entities in transactions, their IDs must be stored\nbeforehand separately from the database, either manually hardcoded into\ncode using fragile magic numbers\nhttps://en.wikipedia.org/wiki/Magic_number_(programming)#Unnamed_numerical_constants\u2014or\nmanually retrieved sometime after the database is initialized with their\nthem. Those IDs must then be manually passed into every transaction that\ninvolves them. Storing all of these ID constants and using them in\ntransactions aren\u2019t a big deal when there is only one singleton object, but\nthey become an issue when there are many singleton or enumerable objects\nfor which to store IDs, and lookup keys don't really help with this.\nThe Entity and Pull APIs also would benefit from idents, giving the\nability to easily retrieve all attributes of singleton entities at once,\nwithout having to manually store their IDs elsewhere\u2014I use this while\nserializing a singleton object's state into a non-EDN text format.\nIdents would be very useful.\nThanks again for DataScript, though; I'm thankful for how amazing it\nalready is.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/52#issuecomment-77673902.\n. Yep, looks painful :)  I\u2019ll try to add idents to DataScript soon\n\n(extend-type UUID\n  IComparable\n  (-compare x y))\nthis should probably go as a patch to CLJS itself. Only compare UUIDs\nas strings, without datascript semantics\nOn Sun, Mar 8, 2015 at 12:17 PM Dave Dixon notifications@github.com wrote:\n\nI've been able to work around some of the Datomic/Datascript differences\nwith a few small utility functions. See gist:\nhttps://gist.github.com/sparkofreason/6b3ffd63d148cd7dc37a\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/52#issuecomment-77735347.\n. Yep. It came from limitation of Datomic, found here:\nLookup refs cannot be used in the body of a query though they can be used as inputs in a parameterized query.\n\nI believe it\u2019s an implementation detail of their query parser which cannot tell the difference between function call, predicate and lookup ref (they all use list/vector form as a distinguisher).\nI\u2019ll look into it, it seems like most of the time we can tell the difference though. So I don\u2019t see why DS shouldn\u2019t allow this kind of syntax. Thanks for pointing that out!\n. Eventually there will be a Clojure version. Unfortunately it\u2019s not a straightforward port, mostly because performance optimizations are platform-specfic. I\u2019m planning on doing this after all main features of Datomic API will be implemented in DataScript, so I\u2019ll avoid developing 2 implementations at the same time.\n. Well, test suite can almost transparently being copied to clj\u2014there\u2019s nothing cljs-specific in tests or datascript api at all. Other stuff will need more creative approach: e.g., btset needs a complete rewrite, query partially depends on #js stuff, debug uses a lot of cljs-dependent things. Plus there\u2019re protocol differences in base clojure/clojurescript classes. You can experiment with this to identify possible problems and solutions, but I\u2019m not sure I\u2019ll be able to merge your branch as-is, a lot is changing right now, and merge will probably be a disaster.\n. @benfleis \nWow! That\u2019s a lot of changes :)\nSome basic points:\nDefinetely cljc, not cljx. I admire cljx very much, but everything will only move toward cljc now, no need to do this work twice.\nAs of 1.6 vs 1.7, I would go with 1.7 because by the time we got this working it\u2019ll probably be widespread.\nDefinetely performance, not code beauty. E.g. I prefer (.-added datom) to (:added datom) because it translates to direct property access, not a virtual function call. On both CLJ and CLJS. It relates mostly to Datoms, and to datoms inside query.cljs as these are the most hot places. I was even thinking of using positive/negative ints as tx to represent added/retracted instead of separate field (same as Datomic does). So this place is pretty serious :)\nI also prefer not to add additional deps until absolutely necessary. I believe we can handle UUID and seqable? without introducing dependency.\nI also believe we should feel native for the platform. E.g. you cannot specify FQN functions in queries in CLJS due to lack of ns-resolve (cannot call a fn by name), but in Clojure it\u2019s possible \u2014 we should use it.\nThe idea behind deftype Datom was to make it as small as possible, because there\u2019ll be a lot of them. DB and other stuff can be done as defrecord, so we can avoid reimplementing existing defrecord conveniences.\nIs there a reason why you\u2019ve changed (FilteredDB. \u2026) to (->FilteredDB \u2026)? Just curious.\nThe general idea is not to implement APIs where they make no sense. E.g. assoc for entity, we don\u2019t need it. Etc.\n+;; XXX why wouldn\u2019t the RHS of an attr be :v instead of \u201cv\u201d? i\u2019m\n+;; assuming it \u201cjust works\u201d in CLJS, but in CLJ it fails to access\n+;; correctly, since the actual index is :v, not \u201cv\u201d\n+;; to deal with this, Datom\u2019s will handle keys as\n+;; symbol/keyword/string\nThis is an low-level cljs implementation detail I made use of. This was the fastest way to store \u201cthis is what you need to get out of datom\u201d kind of information. I use it later as (aget datom \u201ce\u201d) for example. Almost as fast as native property access. Have to think what Clojure equivalent will look like.\nAlso BTSet will probably need separate implementation (maybe even pure-java) as it\u2019s too low-level to be multilingua efficiently.\n```\n+cljs (def Exception js/Error)\n```\nClever! We could probably use ExceptionInfo here though. I believe it is cross-platform.\n(reduce #(conj %1 (entity db (.-e %2))) #{} datoms) #+cljs -) ;; XXX what is this trailing dash?\nLOL I have no idea :) Probably typo :)\nvalue vs. nuisance\nNot sure what do you mean, can you elaborate?\n\u2014\nAll that being said (and repeating again: it\u2019s a huge amount of work! Thanks for putting this together), I think we have to think of some strategy how we can get this merged. It cannot go as one patch: there\u2019s a lot of decisions in there, and there\u2019s no way I can assess them all at once. Also CLJS code is evolving, and I cannot keep track of all changes happening in both master and clj PR.\nAs I see it, it can be a series of small patches which will smoothly move us towards the end goal. We start introducing portability piece-by-piece, and these changes will co-exist in master. It won\u2019t compile to clj at first, but I want code parts we both agree on to be there anyway: so changes can be made to master and CLJ version at once.\nYour current branch can work as a source of ideas and a roadmap. It can be sliced and discussed piece-by-piece as a separate PRs.\nJust a heads-up: I don\u2019t recommend touching query right now: it\u2019ll be rewritten soon. Perf is a mess right now: it deserves proper cleaning before any port work can be put into it.\nFirst thing to do I guess is to set up cljc compilation to the project. What do you think?\nAnd we definetely should do it on top of latest master :)\n. @benfleis great! Let\u2019s start with updated deps & cljc renamings. Basically we can rename everything to cljc and it should still work. It won\u2019t compile under clojure, but should work as before under cljs.\n\n(FilteredDB. \u2026) to (->FilteredDB \u2026)\n\nIt would be interesting to dig this question. It probably won\u2019t affect anything so I agree on -> form, but we should probably do these renamings in a single separate commit as well.\n\nUUID\n\nI have squuid code working for Clojure, I\u2019ll add it as we have cljc in place.\nLet\u2019s start with this and see where to go after that.\nI also added you to contributors to tonsky/datascript, please create a branch here (e.g. not in your fork), it\u2019s much simpler to work this way. You can still create pull requests from tonsky/datascript to tonsky/datascript, but I\u2019ll see your branch in my locally checked out git repo.\nThanks! Looking forward to see what will come out of that :)\n. DataScript is CLJ/CLJS cross-platform for some time now. Closing this\n. Yeah, this one is subtle and not properly documented.\nSame as Datomic, DataScript has notion of partitions. There\u2019s no public interface to it, but internally there\u2019re 2 partitions: entity ids [0\u20130x20000000) and transaction ids [0x20000000\u2013\u221e). When you create new entity, its id gets allocated sequentially from first range, and each transaction gets id sequentially from second range (see datascript.core/tx0 as a basis for transact-id partition).\nNow, when you assign entity id manually, DataScript has to account for that. When you add new entity with manually assigned id X, database\u2019s value of max-eid gets advanced to max(old max-eid, X). But it\u2019s also perfectly correct to add facts about transactions. E.g. if you add [e a v] where e is from transact-id partition, it should refer to existing transaction id and should not advance max-eid of entity id partition. (I\u2019m not sure it works as I described at the moment, but it should, and eventually it will).\nAnswering your questions:\n1. Yes, when exceeding entity-ids exception should be thrown.\n2. It is safe, but notice that such ids will intersect with transaction ids. Usually there\u2019s nothing wrong with that.\nValue 0x2000 0000 was not chosen at random. JS engines like V8 use much more efficient unboxed representation for signed 31 bit integers. This leaves us interval of [0\u20130x3FFF FFFF] for positive ids. I just split it in half, allocated first half for entity ids and second half for transaction ids. Datomic uses 64bit signed longs for entity ids and much wider range per partition, unfortunately, we have no chance to reproduce that efficiently in JS.\n. Well, separate attribute for external ids are still a viable option. With lookup refs it\u2019s quite easy to use now. I prefer it that way: eids are fast, but when you need more rich referencing capabilities, you use external ids and pay some price for resolving them.\n. Well, I actually don\u2019t understand the problem of single-segment namespace. What breaks if I use one? Even in Clojure?\n. Yes, I\u2019ve seen it. Still don\u2019t understand what breaks if I use\nsingle-segment namespace. Unability to define  seems\nlike something I can live with\nOn Fri Feb 20 2015 at 01:39:09 GMT+6 Bruce Hauman notifications@github.com\nwrote:\n\nIt's a ClojureScript issue:\nhttp://dev.clojure.org/jira/browse/CLJS-1004\nclojure/clojurescript@dc47f1f\nhttps://github.com/clojure/clojurescript/commit/dc47f1fa1483871641c34f765034efedb7238f28\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/57#issuecomment-75120744.\n. @gtrak Eventually I\u2019ll fix this, for now use {:warnings {:single-segment-namespace false}} compiler option. Also you should require datascript, not datascript.query, and use datascript/q fn\n. Fixed in 0.13.0 (datascript \u2192 datascript.core)\n. Will take a look at this a little bit later. If I remember correctly, components fetch is unconditional, so if they are in a cycle, it\u2019s an infinite loop. Datomic hangs on this too. Idea is to not put your components in a cycle. Probably there\u2019s no reason to emulate that behaviour of Datomic here.\n. I think there\u2019s no reason for allowing infinite recursion for components. I guess Datomic does that is a bug. So we\u2019ll fix that eventually, @dthume maybe you can take a look?\n. Fixed in #61 and #62\n. Thanks \u2014 it was a bug indeed!\n. Well, I totally agree there\u2019re an implied constraints about components. Unfortunately, it\u2019s hard to actually enforce it: there\u2019re actually a lot of indirect ways to create loops, and whole DB should be checked on each transaction that no new violations have been introduced.\n\nProbably nobody will ever find it out, if components will be uses as we expect them to be used. But just in case, it\u2019s good that system does not fall apart if such problems will be introduced by someone.\nAgain, in normal usage these situations should not ever happen. So this is just some sort of defensive mechanism, just in case.\nThanks for the PR, I\u2019ll look into it shortly\n. Just to be more clear: we work in expectation that all these constraints holds true. E.g. reverse ref navigation from component returns single result. User can make two entities to reference single component, but then it\u2019s his problem to deal with it.\n. Merge this for now so it\u2019s in master, needs more work \u2014 see #62\n. @dthume I added you to collaborators, so you can create branches in this repo instead of in a fork if you wish. It\u2019s actually will be slightly easier to me to handle them this way\n. Also I think recursion in components and in refs should work more or less the same, ideally via the same algorithm. And they should share this \u201cseen refs\u201d stack for sure\n. Amazing! Thanks, merged to master\n. What is your source? Db? Datoms? Entities?\n``` clj\n(d/q '[:find [?e ...]\n       :in $ ?n\n       :where [?e :name n]]\n      db \"Ivan\")\n;; => [1, 2, 3]\n(filter (fn [datom]\n          (and (= (.-a datom) :name)\n               (= (.-v datom) \"Ivan\")))\n        datoms)\n;; => [#datascript.core/Datom{:e 1, :a :name, :v \"Ivan\"}, ...]\n(filter (fn [entity]\n          (= (:name entity) \"Ivan\"))\n        entities)\n;; => [{:db/id 1, :name \"Ivan\"}, {:db/id 2, :name \"Ivan\"}, ...]\n``\n. Oh, you\u2019re asking aboutdatascript/filter`. Try this:\nclj\n(d/filter db\n  (fn [db datom]\n    (and (= (.-a datom) :name)\n         (= (.-v datom) \"Ivan\"))))\nThis will only keep datoms which are about :name ([* :name *]), so essentially querying such db will get you just\n{:db/id 2, :name \"Ivan\"}\nwithout any other properties of entity.\nYou probably want to keep any attributes of entity if that entity has a :name attribute equal to \"Ivan\". Then\nclj\n(d/filter db\n  (fn [db datom]\n    (let [eid    (.-e datom)\n          entity (d/entity db eid)]\n      (= \"Ivan\" (:name entity)))))\n. Just run a periodic task to clean up the data. Simple :db/retract eid attr\nvalue will do.\nDataScript does not store history, so if you retract a datom, it\u2019ll be\nremoved from DB completely. This way DS can work in a fixed memory\nfootprint.\nOn Sun, Mar 8, 2015 at 11:38 PM uwo notifications@github.com wrote:\n\nCould anyone recommend how I might approach evicting data, perhaps in an\nLRU style? I display a lot of time series data that I assume will\neventually blow up if I store it all. Thanks for datascript, tonksy!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/64.\n. thanks for pointing that out!\n. Sorry it took so long :)\n. Yep, that should be fixed. I\u2019ll address that in the next version\n. It\u2019s because values should be comparable:\n\n```\n(compare [:value 2] [:value \"foo\"])\n\n```\nComparison between different keys is handled by DataScript. There\u2019s a guard that first compares types, then values. So it\u2019s ok to have :key 2 and :key \"foo\". But here you have two vectors and comparison between them is done using CLJS default compare.\n. It\u2019s useful to have total order of all values of all types (e.g.: Erlang). I don\u2019t know why it is in CLJS, unfortunately. It gets even funnier: you cannot compare array-map with hash-map for example :)\n. I\u2019m not sure what the right solution to this problem is. Clojure does work the same way:\n(compare [1 \"a\"] [1 :key])\n=> java.lang.ClassCastException: clojure.lang.Keyword cannot be cast to java.lang.String\nFix to this might be a language design-level decision.\nIn Datomic you can put only basic types to values. Maybe we can think of something for DataScript too. I don\u2019t want to give up ability to store any values. Maybe \u201cnot-comparable\u201d schema attribute. I\u2019ll think about it.\n. @sonwh98 nope, you have to define IComparable for the objects you\u2019re storing. Or put them in a wrapper that does define that.\n. Try (cond (identical? x y) 0 (< x y) -1 :else 1) as an implementation\n. Ok guys, I now has a real solution for this. Read the changelog for 0.12.0 version. Thanks for waiting! // cc @sonwh98 @myguidingstar \n. @carocad not currently. I\u2019ll probably do this https://github.com/tonsky/datascript/issues/274 at some point in the future, that should solve it for Clojure. Consider using something else for a key for now (e.g. string representation of what you currently have there?). Nice! Our first real step towards CLJ version :)\n- Can you make semantic-test-slice work with CLJS too? If we\u2019re aiming at impl transparency, tests should be the same for both imls.\n- I don\u2019t get why are you testing (sorted-set-by cmp-s), not btset. I understand that it\u2019s just a wrapped sorted-set, but still, we should test around public interface, not impl detail. Correct me if I miss something.\n- I think helpers at the beginning can be safely dropped (decode-path, dump)\n- These\n(defn btset-conj [set key] (conj set key))\n(defn btset-disj [set key] (disj set key))\nshould read\n(defn btset-conj [set key _] (conj set key))\n(defn btset-disj [set key _] (disj set key))\nright?\nThanks!\n. btset-conj with 3 args was just a way to provide faster comparator where it made sense. Since we\u2019re using stub impl for btset for now, it\u2019s ok to ignore that optimization as well. datascript.core will still pass custom comparator there, but we can safely ignore that, all we lose is some perf. When we had proper btset impl, it will make use of it.\n. $ is a default source name. If you omit :in, it assumes :in $. If you don\u2019t specify source in a pattern, it assumes $, e.g. [$ ?a :foo 1].\nIn your second case you\u2019re using non-default source name in :in, but default in pattern. This should work:\n(d/q '[:find ?a\n         :in $x\n         :where [$x ?a :foo 1]]\n       [[1 :foo 1]])\n. It\u2019s a bug in DS and also you shouldn\u2019t wrap schema in Array. Correct code should be:\nd.init_db([[1, \"name\", \"Ivan\"],\n            [1, \"age\", 17],\n            [2, \"name\", \"Igor\"],\n            [2, \"age\", 35],\n            [1, \"friend\", 2]],\n  {\"friend\": {\":db/valueType\": \":db.type/ref\"}});\n. @motosota just pushed new release for you https://github.com/tonsky/datascript/releases/tag/0.11.0\n. So I read all the diff, here are my notes, in read order:\n- Point of deftype on Datom was to avoid unnecessary fields (meta, extended attrs map) so it\u2019s a lightweight as possible. For all other cases defrecord + extend-type works just fine and is cross-platrofm, so let\u2019s stick to that. The parts where you\u2019re trying to implement what defrecord natively implements are most fragile ones, so let\u2019s try keep them to minimum. val-at-db, assoc-db and the likes should go\n- entryAt and equiv are swapped in Datom/clj version\n- Let\u2019s move macros from macros.clj to core.cljc and deftrecord to parser.cljc, and remove macros.clj\n- For partially implemented methods, we should throw UnsupportedOperationException in clj, this is idiomatic in Java. Also please remove assoc-datom/empty-datom and the likes, they clutter the namespace. It\u2019s ok to just throw in-place, no need to generalize such task\n- entryAt should return clojure.lang.MapEntry, not vector\n- About cmp-val/cmp-val-quick formatting. I prefer to have one function header, and then conditional reader in the fn body. This way it reads: there\u2019s one fn, but impl may differ. It\u2019s easier to understand: ok, I know fn exist, always. On the contrary, the way you have it now reads: there\u2019s some fns that work only for CLJS, and here are some fns that work only for CLJ. It\u2019s a subtle difference, I know, but I prefer it my way.\n- cmp-val-quick: note that compare-keywords is already gone, we resort to default compare now in CLJS too\n- In cmp-datoms-*-quick indentation is broken (should be 4 spaces instead of 3)\n- get-datoms-as-tuples will materialize vector of all datoms in memory, might be big. We can probably use lazy seqs here somehow\n- #?(:clj (do \u2026)) \u2192 #?@(:clj [\u2026])\n- What\u2019s the point of db*? Also, I prefer map->DB for db creation as there\u2019s a lot of keys in DB\n- Why empty-db is overloaded? I think arg should always mean the same thing (schema in our case)\n- Why removing ^boolean annotation for is-attr/multival/\u2026 methods?\n-    (not (or (array? vs)\n+    (not (or (#?(:cljs array? :clj vector?) vs)\nwe have array? defined in datascript.core earlier, no need for conditional read here\n- (assert db) \u2014 probably forgot to remove\n- retract-components: you replaced transducer form with lazy one, why? It\u2019s less performant that way\nPARSER\n-    (when-not (empty? undeclared-vars)\n+    (when (seq undeclared-vars)\nempty? is allocation-free and in most cases just and integer check. seq creates a new object if sequence is not empty (ChunkedSeq for vector, KeySeq for set, etc). Also there\u2019re optimizations (in CLJS at least) for conditions which can be proved to have boolean type. In case of seq it has to call additional truth_ function to coerce to boolean. So please don\u2019t do this\nQUERY\n-  ([] (Relation. {} [#js[]]))\n+  ([] (Relation. {} [#?(:cljs #js [])]))\nThis is not correct for CLJ version. There must be a single 0-tuple element in this rel.\n-  (or (seqable? x) (array? x)))\n+  (or (dc/seqable? x) #?(:cljs (array? x))))\nagain, dc/array? to the resque\n- Is it possible to keep types from parser namespace-qualified? I prefer not to use use/refer until absolutely necessary.\nQUERY_V3\n-  (or (seqable? x) (array? x)))\n+  (or (dc/seqable? x) (array? x)))\nneeds dc/array? too\nENTITY\n\nHow come clojure.walk needs assoc?\nWhat\u2019s TouchableEntity? Why do we need this?\nWe used set! for Entity to keep mutable cache. It can be changed to volatile! ref, so impl can be shared between clj and cljs. I very much prefer it is shared\n\nTESTS\n\nDoes clojure.test fail to print ExceptionInfo data too? If it does print it, we probably doesn\u2019t have to patch test failure printer.\n\n-      (is (= (-> (e 10) :children first :father) (e 10)))\n+      (is (some #(= (e 10) (:father %)) (-> (e 10) :children)))\nIt\u2019s strange you had to change that test. Can you explain why?\nEverything else\nFormatting: looking at conditional reader forms, it\u2019s hard to read now. My proposal: let\u2019s treat them as any other form. So top-level forms included into #?() will be indented, but I think it\u2019s ok. Like this:\n```\n?(:clj\n(defn array? [o]\n    (-> o type .isArray)))\n```\n\nclojurescript.test suggests that :none optimization doesn\u2019t work with tests \u2014 so i switched it to :whitespace (and renamed for disambiguation); thoughts?\n\nHow do you run tests? I open dev.html in Chrome and connect with REPL, works perfectly with any optimization level.\n\nunclear how much to name w/ \u2018-\u2018 prefix\n\nI\u2019m ok to drop it for fns, but keep for protocols. Please don\u2019t do this over the whole codebase, only the fns you touch.\n\nsome functionality moved from datascript -> datascript.core\n\nSure, it\u2019s absolutely logical.\n\ntouch API \u2014 if datoms search comes back empty, shouldn\u2019t it still be marked as touched?\n\nDoesn\u2019t really matter, touch is for interactive exploration anyway. There\u2019s no place for it in a working program.\n\nI wonder what you\u2019d think of a different form \u2014 adding another clause (DS specific, ignored by Datomic, at least in {} form), like :binding or :function, which could itself be a map or sequence of symbol, and resolved function. This would give a language that works identically in CLJS, and could be ignored/dropped in CLJ, since resolve can actually be called there.\n\nGood idea. Will check out how to make it so Datomic ignores these.\n\nMy intention is to fix these things up, and merge all into a single clean diff in the move-to-cljc branch.\n\nThis is kinda hard to read. It took me an hour to speed-read, and I\u2019m sure I miss a lot of important nuances. Diff is very big and it mixes a lot of things. As a result, while I\u2019m not 100% sure of everything is this diff, I cannot merge anything from it. Another downsides:\n- I should re-read everything on every iteration, even stuff I already reviewed and we both agree on.\n- There\u2019s no feeling of moving forward\n- Patch rots. I commit to master constantly, and you\u2019ll spend a lot of time rebasing. It\u2019s tedious, and you can easily miss something (e.g. compare-keywords).\nThat\u2019s why I prefer small steps and clean small separate diffs. Not put everything in one \u201cclean\u201d patch (it\u2019s a long road until it\u2019s clean), but separate patches by logically independent pieces. E.g. (Record.) \u2192 (->Record) migration, it\u2019s easy to review and merge if diff contains only this.\nI understand that you\u2019ve tried to get to compiled code and working tests first and these are the minimal changes needed to do this. But it doesn\u2019t mean patch has to be one giant diff between master and this.\nAlso, it\u2019s important to me to have some confidence in the code I merge, so it\u2019s hard to accept patch that somehow works and is intended to be fixed later. I need it to be incomplete, but \u201cright\u201d from the beginning.\n. > The factor here is that equivalence and hash values cannot be overridden with a defrecord in CLJ\nAh, shit :( I\u2019ll look for a better solution for this\n\nThis is not allowed in top level forms. There's a bug filed in CLJ someplace about it. It took me a while to discover this though.\n\nThere was another bug about it, and it was fixed in beta2. Looks like they still have to work it out :(\n\nIt caused test failures in CLJ world; iirc, it's because the funcs don't actually return bools, but instead truthy things. Could also have wrapped return w/ (boolean).\n\nYep, it\u2019s better to make them actually return bools.\n\n(into (empty )) turns into a series of conjs. Since it's essentially a keyed structure, these conjs become assocs.\n\nDatomic does not implement assoc/empty on them. I think we shouldn\u2019t too. It just doesn\u2019t make sense. datascript.test.core could be rewritten without it (or maybe change postwalk to prewalk?)\n\n\nWe used set! for Entity to keep mutable cache. It can be changed to volatile! ref, so impl can be shared between clj and cljs. I very much prefer it is shared\n\nI spent a bit of time trying to get :volatile-mutable working with deftype. I failed. I asked about it on #clojure, and nobody seemed to even know of a real example, or how my example work. So I gave up. I'll give it another shot.\n\nNo, I mean, there\u2019s new ref type: volatile (e.g. (volatile! 7)), it\u2019s like atom (vswap!, vreset!), but faster. Used internally by reducers. It works the same in clj and cljs.\n\nThe original test was dependent upon ordering within the set, which didn't work in CLJ.\n\nOh, it\u2019s probably because entity id 101 does not declare father attribute. We should fix that instead: \n(d/db-with\n  [{:db/id 1, :children [10]}\n   {:db/id 10, :father 1, :children [100 101]}\n   {:db/id 100, :father 10}\n   {:db/id 100, :father 10}]))\n\nTrue. You still prefer to leave it as is?\n\nYes.\n. Hope you don\u2019t mind I didn\u2019t used your patch :) Check out 0.11.1 version which has this issue fixed\n. Wow that is tricky. So -1 gets resolved to 2 first, and then it conflicts with upsert in a second clause.\nThis won\u2019t work in current design, because in DS every transaction clause is executed one after another. Each intermediate DB state is materialized in between. So it\u2019s not possible to \u201cundo\u201d first clause or change tempid binding. It allows us to have intermediate DBs passed into db.fn/call. If you add some data and then has db.fn/call in the same transaction, fn\u2019ll see that data. Also, if you insert new record with new unique attribute, you can use lookup refs to it in the same transaction.\nThis is different from Datomic. Datomic first resolves all tempids and calls all transaction functions, passing the same db-before value to all of them. So later transaction clauses cannot see results of earlier transaction clauses, and sometimes you have to split your transaction to several clauses to get around that. But as a benefit you can re-decide tempid bindings.\nThis is more like a design issue here. I\u2019m currently happy with DS design, as it has its benefits and more intuitive in my opinion. If this is a serious issue for you, please elaborate more. Right now it looks like it can be easily avoided.\n. Well, output is order-dependent anyway (you can\u2019t swap additions and retractions for example). In case of tempids, we can sort it out even in the current model, actually. Just need to add some backtracking. Don\u2019t expect it soon though :)\n. @levand it does not fails, there\u2019s plenty of :db/add -1 in the test base: https://github.com/tonsky/datascript/search?utf8=\u2713&q=%3Adb%2Fadd+-1\nIt only causes problems when -1 is used several times during same tx, and it is forced to be resolved to different values (e.g. first it allocates new id, then it is resolved via upsert attribute, etc).\nYes, I\u2019m going to fix this, though cannot promise you exact timeline\n. Oh. Can you check with Datomic? It might be intentional.\nOn Thu, Aug 27, 2015 at 6:51 PM Luke VanderHart notifications@github.com\nwrote:\n\nIt does fail, if :id is :db.unique/identity and there already exists a\ndatom with that ID.\nThe map form does an upsert, the vector form throws a uniquness exception.\nI'll see if I can get you a PR with a test case later today.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/76#issuecomment-135412111.\n. Thanks! You\u2019re right, upserts are not resolved in vector form at all :( Will fix\n. Listeners are all called inside d.transact fn. First it applies a transaction, then swaps conn value, then call all listeners. By the end of d.transact all listeners will already be called. Maybe that\u2019s your issue? How is render fn gets value of actual DB? What\u2019s db1 = d.db(conn); there for?\n. Yep, you don\u2019t need db1. Just use d.db(conn) where you need latest DB.\n. - I\u2019m thinking now: entities can be accessed from multiple threads. We probably should use atoms for cache instead of volatiles (volatile\u2019s vswap! is safe in single thread only). Sorry I mislead you with my initial suggestion.\n- Please set (empty [e]         (entity (.-db e) (.-eid e))) to throw\n- We probably need IFn implemented too?\n- What about removing clojure.walk/postwalk from test? Can you do it in this patch too?\n. Nice!\n\nSome comments :)\nUnsupportedClassVersionError \u2192 UnsupportedOperationException\n(entryAt [d k] (some->> (val-at-datom d k) (clojure.lang.APersistentMap k)))\nthere should be MapEntry, not APersistentMap\nassoc is actually needed (we need to be able to (assoc datom :e 18) for example). It should throw on unexpected keys though (IllegalArgumentException)\n(defn datom [e a v & [tx added]]\n  (->Datom e a v (or tx tx0) (if (nil? added) true added)))\ndoes deftype also declares ->Datom fn?\n(defn- equiv-datom [^Datom d o] \u2192 (defn- equiv-datom [^Datom d ^Datom o]\n(apply pr (map (partial get d) [:e :a :v :tx :added]))) this probably better be expanded, so there\u2019s no allocations and no lazy sequences on printing. It\u2019s fine for interactive println/debug stuff, but same method will be used for serializing datoms to EDN, and for logs probably.\n#?(:cljs\n         (if (identical? t1 t2)\n           (compare o1 o2)\n           (garray/defaultCompare t1 t2))\n         :clj\n         (combine-cmp\n          (compare (str t1) (str t2))\n          (compare o1 o2))))\nHere this dance around identical? is actually needed. For same-type values we can do fast (identical? t1 t2) check and move straight to comparing values. In your clj case you always convert class to string and then compare these strings. Also getName is ~40% faster on a class than toString\n(if (identical? t1 t2)\n  (compare o1 o2)\n  (compare (.getName ^Class t1) (.getName ^Class t2))\n. Another thing: I managed to measure (Datom.) vs (->Datom) perf (second one is a fn call, first one is a direct new Datom JVM instruction. Difference is not that big (~5%), but still, as we already got (Datom.) everywhere, can you not change it to (->Datom), just leave it as-is? It will also make diffs cleaner. Sorry that I haven\u2019t decided that earlier\n. Ah, decisions. There\u2019s no strong reasons neither for Datom. nor ->Datom. In cases like this I prefer just not to change things, leave them as-is. I\u2019m not ready to do a proper research on how bad that will affect perf. Maybe a lot. Maybe slightly. Maybe not even slightly. Why do this while we can just use best possible alternative out there without much hassle? Import is not that hard, and we have pretty small codebase. Let\u2019s just keep Datom. as it was before.\n. @dthume fancy take a look?\n. Awesome! Thanks a lot!\n. We do not support Clojure at the moment. What you see is a slow iterative process that should eventually get us there. But not yet\n. Great!\n- val-at-db, assoc-db: these should be provided by defrecord now\n- I now see that you were right about moving empty-db to core. I\u2019m sorry, can you do that and drop empty-db-from-db?\n- We should probably use hash-ordered-coll for hash-db cljs version (was a bug before). Also I think we can change to (.-eavt db) there instead of (-datoms db :eavt [])\n- Would (defn ^Boolean work for cljs? Boolean is java class.\n- In clojure version of defrecord-ext it\u2019s safe to use field names (e.g. just eavt instead of (.-eavt this)). I think it\u2019s much cleaner, can you change to that? I\u2019ll see what can be done for CLJS version about that.\nThanks!\n. About DB/seq/set/protocols stuff. Let\u2019s leave implementations exactly as they are, with all current flaws. Let\u2019s revisit that decision in a separate ticket, later. It has to move one side or another, but let\u2019s focus on getting this work first\n. Just realized hasheq and hashCode should be cached on JVM. You\u2019re right about DB/FilteredDB portability. OTOH the whole impl is so trivial we could inline it, without externalizing to hash-db at all.\nThis is what I meant by reusing empty-db: (-empty [db] (empty-db (.-schema db))\nAbout moving fns to the core. Let\u2019s postpone that decision. I\u2019ll see what is the best way to organize things is.\n. You\u2019ve moved max-eid (if (pos? len) (.-e (aget datoms (dec len))) 0) in let in init-db, it was important where it was because I\u2019ve used mutable .sort on datoms. Can you return it back and add some comment about that?\n. I\u2019ve fixed max-eid myself, see last push. Otherwise, looks good.\n. I want to check that ^:Boolean thing, and also new defrecord that lets use local field names in cljs\n. ^Boolean thing is definitely not working:\n```\n(defn ^boolean test-aaa [x]\n  (== x 1111))\n(defn ^Boolean test-bbb [x]\n  (== x 2222))\n(if (test-aaa 0)\n    (println \"aaa\"))\n  (if (test-bbb 0)\n    (println \"bbb\"))\ndatascript.core.test_aaa = (function datascript$core$test_aaa(x){\nreturn (x === (1111));\n});\ndatascript.core.test_bbb = (function datascript$core$test_bbb(x){\nreturn (x === (2222));\n});\nif(datascript.core.test_aaa.call(null,(0))){\ncljs.core.println.call(null,\"aaa\");\n}\nif(cljs.core.truth_(datascript.core.test_bbb.call(null,(0)))){\ncljs.core.println.call(null,\"bbb\");\n}\n```\nCan you revert it to something like this (I can confirm it does work):\n(defn #?@(:clj  [^Boolean test-ddd] \n          :cljs [^boolean test-ddd]) [x]\n  \"abc\")\n. - I this is-filtered is sufficient (name comes from Datomic), let\u2019s remove filtered-db?\n- I think db? should return true for both DB and FilteredDB\n. (set! *data-readers* (merge *data-readers* '{datascript.core/Datom datom-from-reader\n                                                datascript.core/DB    db-from-reader})))\nset! seems to work only inside (binding) form. Let\u2019s just provide data_readers.clj for Clojure case. CLJS branch of this is fine.\n. (defn- resolve-datom [db e a v t]\n-  (when a (validate-attr a))\n+  (when a (validate-attr a [e a v t]))\nThis and similar places. Second argument is for error reporting, so it should help find what\u2019s wrong. I suggest something like:\n(defn- resolve-datom [db e a v t]\n  (when a (validate-attr a (list 'resolve-datom 'db e a v t)))\n...\nThis way in error report you\u2019ll see something like Bad entity attribute 177 at (resolve-datom db 1 177 nil nil), expected keyword or string\n. Decided to use (and (instance? ISearch o) (instance? IIndexAccess o) (instance? IDB o)) for db? check\n. Can you move make-obj-array to datascript.core and rename to something shorter like arr? We\u2019ll be using this a lot, probably. Also it\u2019ll look nice to have arr and array? co-located.\n. ICache could be dropped. I already forgot what was my intention, it\u2019s clearly not working in the current form\n. Please keep CLJS version of mapa in query_v3. It was made this way during optimization works. Clojure version could be slow for now. Also, in query_v3, make-array should be changed to arr of ours.\n. Can your serialization test be moved to datascript.test.serialization?\n. - :datoms in DB are now double-wrapped in [[]], they shouldn\u2019t be:\n(is (= (pr-str db) \"#datascript/DB {:schema nil, :datoms [[1 :foo \\\"bar\\\" 25]]}\"))\n- For clojure, we should check both clojure.core/read-string with default params and clojure.edn/read-string with d/data-readers\n- in -index-range, please fix validate-attr the same way you did for resolve-datom\n- you haven\u2019t removed filtered-db? yet\nOtherwise looks great. I\u2019ll look into serialization/test issue\n. of course, terribly sorry, you\u2019re right\n. Oh, porting all these protocols is going to be very tough :(\nquery_v3:\n(#?(:cljs array-reduce :clj reduce) ;; iterate over corresponding rel1 tuples\nreduce and array-reduce have different argument order. Maybe create dc/array-reduce and use it here?\n\nlookup_refs, parser, parser_query, parser_rules, and all the other places:\n```\n?(:clj\n(import '[clojure.lang ExceptionInfo]))\n```\nCan you move it into ns declaration? I want to be as declarative as possible\n\n(defn- retract-components [db datoms]\n-  (into #{} (comp\n-              (filter #(component? db (.-a %)))\n-              (map #(vector :db.fn/retractEntity (.-v %)))) datoms))\n+  (->> datoms\n+       (filter #(component? db (.-a %)))\n+       (map #(vector :db.fn/retractEntity (.-v %)))\n+       (into #{})))\nWhy changing transducers form to lazy seq one? We\u2019re on 1.7 anyway\n\n(defn entity-map [db e]\n   (when-let [entity (d/entity db e)]\n     (->> (assoc (into {} entity) :db/id (:db/id entity))\n-         (clojure.walk/postwalk #(if (de/entity? %)\n-                                     {:db/id (:db/id %)}\n-                                     %)))))\n+         (clojure.walk/prewalk #(if (de/entity? %)\n+                                  {:db/id (:db/id %)}\n+                                  %)))))\nWhy change here?\n\n;; use aget in cljs, get in clj\n(def get-compat #?(:cljs aget :clj get))\nThis is still going through additional fn call. This place is very hot. Can you write it that way so CLJS version stays exactly what it was (putting conditionals directly in every call place, not abstracting to fn). \n\nI\u2019m also not happy with this:\n(defn tuple-key-fn [getters]\n   (if (== (count getters) 1)\n     (first getters)\n     (let [getters (to-array getters)]\n       (fn [tuple]\n-        (list* (.map getters #(% tuple)))))))\n+        (list* (map #(% tuple) getters))))))\nMaybe it needs a conditional too.\n. Which version of Clojure are you at? Must be 1.7.0-beta2 or later (latest\nATM is 1.7.0-RC1)\nOn Sun, May 24, 2015 at 11:50 PM Thomas Deutsch notifications@github.com\nwrote:\n\nI get this error when i compile my project with datascript 0.11.2:\nclojure.lang.ExceptionInfo: Could not locate datascript/core__init.class or data\nscript/core.clj on classpath:  at line 1 file:/C:/Users/deutsch/.m2/repository/d\natascript/datascript/0.11.2/datascript-0.11.2.jar!/datascript/core.cljc {:tag :c\nljs/analysis-error, :file \"file:/C:/Users/deutsch/.m2/repository/datascript/data\nscript/0.11.2/datascript-0.11.2.jar!/datascript/core.cljc\", :line 1, :column 1}\ndatascript-version: 0.11.1 works fine for me.\ni use [org.clojure/clojurescript \"0.0-3211\"].\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/84.\n. Are you sure it\u2019s related to DataScript? I do not modify strings in any way. Maybe it\u2019s CLJS issue?\n. > datascript can not (= (fncall ...) (fncall ...)) and needs two pattern variables instead\n\nYou\u2019re right, that\u2019s the limitation of Datalog. You can introduce new variables in rule as well:\n(def filter-rule\n  '[[(fits ?t1 ?t2 ?first ?last)\n     [(?last ?t1) ?x]\n     [(?first ?t2) ?y]\n     [(= ?x ?y)]]])\n. It should work. Another way to get entity data is (d/pull db eid '[*]). Also you\u2019ll probably need special handling for references.\n. Very strange \u2014 usually DS throws on identity violations. I\u2019ll check into it\nOn Mon, Jun 15, 2015 at 4:18 PM Thomas Deutsch notifications@github.com\nwrote:\n\nYou are right, it should have worked, but in my case - one of the\nattributes was a\n{:db/unique :db.unique/identity}\nso i needed to delete the form entity first, before i could make the\ntransaction.\nNice, that datascript prevented this mistake, but i got no error.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/88#issuecomment-112006831.\n. Don\u2019t use syntax quote, use regular quote (')\nThe way you write it now count gets replaced with cljs.core/count or hello-world.core/count, depending on what was found in your current ns. DS expects count without a namespace\n\n``` clj\n(def conn (create-conn))\n(transact conn\n  [[:db/add 1 :user/age 22]\n   [:db/add 2 :user/age 45]\n   [:db/add 3 :user/age 48]])\n(q '[:find ?age\n     :where [?e :user/age ?age]] @conn) ;; => #{[22] [45] [48]}\n(q '[:find (count ?age)\n     :where [?e :user/age ?age]] @conn) ;; => ([3])\n```\n. Yep, I\u2019ll close this as a dup for #50. I\u2019m working on it\n. Great fix, thanks! Merged here https://github.com/tonsky/datascript/commit/a7eb43edf5f265a65d49a901f988fe56b8d3a7ab\n. So I went with my own implementation b/c I needed to cover more cases (browser repl, test compilation). Check it out here: https://github.com/tonsky/datascript/commit/4b63a1a48366f702bc541eb07b86b4c2ae3054f4\n. Thanks! Merged https://github.com/tonsky/datascript/commit/18ab268d4682f2ef0c75ce42548494726009f82f\n. Care to provide a PR? It should be trivial\nOn Sat, Jun 27, 2015 at 3:34 PM Thomas Deutsch notifications@github.com\nwrote:\n\nI would love to have the option to pass a optional timestamp to the squuid\nconstructor.\none example scenario:\nI store all entities in firebase. For every entity, the key is the squuid\nfrom datascript. The value is a transit-string.\nIf every squuid will contain the due-date of that item ( not the\ncreation-time ) - the backend server will be able to detect all the items\nwith that due-date without the need to transit every value. This would be\nperfect for many cron-jobs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/95.\n. Decided to go with my own patch \u2014\u00a0yours has a bug, it was faster to fix it by myself: https://github.com/tonsky/datascript/commit/f2f05fa7ef956358ef5680e861b11a6e84087be3\n\nNote that only seconds, not milliseconds, go to squuid (so they fit into 32 bit). Also note that the purpose of squuid was to provide monotonic unique decentralized identities. The fact that they\u2019re using time is just a coincidence. It would be reasonable not to rely on that, and store timestamp explicitly. I\u2019m adding sqquid/1 mostly for the symmetry of the API.\n. Decided to go with my own patch \u2014 yours has a bug, it was faster to fix it by myself: f2f05fa\n. They are out of scope for DS, but I believe it\u2019s a good opportunity for a\nlibrary.\nAbout good practices, consider this\nhttp://tonsky.me/blog/the-web-after-tomorrow/\nOn Sun, Jul 12, 2015 at 11:15 PM Alexey notifications@github.com wrote:\n\nFirst of all thanks for a great tool!\nIs there any plans to support replication/sync DS over wire? It's hard to\noverestimate the usefulness of such feature for collaborative apps. And\nalso I didn't found a way/practice how to sync DS to the server operational\ndatabase. Or such topics is out of scope of DS?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/100.\n. Sounds like a good idea. Please use datascript.core/db? for the check\n. does lein clean help?\n\nOn Tue, Jul 21, 2015 at 5:44 PM mprokopov notifications@github.com wrote:\n\nI've got this issue upon compile since v0.11.5. Any previous version does\nnot throw this exception.\njava.io.FileNotFoundException: The file resources/public/js/compiled/out/datascript/btset.cljc does not exist.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/103.\n. Thanks, a great catch!\n. Are you talking about clojurescript keywords? They are just wrappers around strings, so don\u2019t expect perf boost there\n. Right. CLJS identical check for keywords is reference comparison, whether for JS strings it is not. On the other hand, DS relies on compare, not on equality/identity check. Compare for keywords is slightly more complicated than compare on strings. Overall, you should not lose anything using strings\n. This is now obsolete due to https://github.com/tonsky/datascript/commit/a69960f1b23d22ecfddc7685b494ff80c3734d60\n. DS doesn\u2019t know what to do with types anyways. Only use-case I can think of is to use schema just to preserve Datomic schema. In that case, it would be valuable to have all the types Datomic support. Can you add the rest? http://docs.datomic.com/schema.html#sec-1-1\n. How do you see it? DS understands any type that is IComparable. I don\u2019t want to limit that in any way.\n. I spent some time thinking about it. I want this validation to be meaningful, do not lie, and be helpful. Round-tripping is just one, very specific use-case, I don\u2019t want to design API around that single issue.\n\nE.g. you can put vector as a value. DataScript allows that. It\u2019s not a hack, it\u2019s part of the contract. Why can\u2019t you specify vector as a type in the schema?\nOn the other hand, there\u2019s no long/int/floats in JS, just number which is 64 bits floating point. So it doesn\u2019t actually makes sense to let you use them in the schema.\nThere will also be no validation once you\u2019ve specified data type. This is counter-intuitive (why else would I specify data type if it isn\u2019t validated later?)\nGiven all that, I think I\u2019ll postpone this decision until real reason for types in the schema rises. Once we have a use-case for that, it\u2019ll be clear how types in the schema should work, should be validation, should we allow for arbitrary types etc. Sorry it took so long.\n. @milt You were right about commit, that helped a lot. Thanks! DataScript had a bad luck using both '$ and '. symbols in the API extensively, exactly the two who was conflicting.\nWaiting for this to be applied: http://dev.clojure.org/jira/browse/CLJS-1432\n. Issue has been fixed in 1.7.122. Check it out\n. @ThomasDeutsch that\u2019s another bug. Values should be comparable. Maps are not comparable. Either wrap them or extend to implement IComparable. I\u2019m going to add \u201cdon\u2019t index\u201d attribute, but only in a next version\n. I fixed that, but wrote my own test, so closing this. Thanks for bringing my attention to the issue!\n. That\u2019s interesting! Is it ClojureScript? I once got very annoying bug with transit/uuids where transit introduced their own UUID class and regular UUIDs were not comparable with UUIDs read from transit.\nCan you run (compare (:item/id *1) #uuid \"27c1f4e4-ce58-4a46-9c36-d9bd1f573309\"), also (= .. ..) and probably (type ..) for both?\n(d/entity @conn [:item/id nil]) is hilarious too. Do you, by any change, store any nulls in your DB?\nCan you reliably reproduce it? Can I have a test case? Also please report versions of everything (clojure, DS, cljs, compilation mode)\nThanks!\n. Here\u2019s where it all started\nhttps://github.com/cognitect/transit-cljs/pull/10\nYou\u2019re happy we\u2019ve found it that quick :) I was pulling out my hair when I\nfirst encountered it. Since then, I always do\nclojure\n(transit/read\n  (transit/reader :json { :handlers { \"u\" uuid } })\n  s)\n(make transit read cljs.core/uuid from #uuid tag)\n. @MattParker89 do I understand correctly that this issue is resolved when you add { :handlers { \"u\" uuid } } to the reader?\n. Thanks @jaen for the help! This turns into a nice forum :)\n(btw some fns work without :in, here is the list https://github.com/tonsky/datascript/blob/0.11.6/src/datascript/query.cljc#L140-L145)\nOk, let\u2019s let this open, as there\u2019s two things that should be changed in DS:\n1. subs should be included to built-ins\n2. exception should be thrown on unknown function\n. Added subs to built-ins in bcdb222f7b8883aec6e3a5889bf880f31a14ba6e\nException on unknown built-in will be thrown in ff887c8f69aa3da466f5e25679daf0865c64e4d0\n. No. transact! uses db-with internally.\nOn Fri, Sep 4, 2015 at 1:10 AM Valentin Waeselynck notifications@github.com\nwrote:\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/112.\n. Sure, go ahead: https://github.com/tonsky/datascript/wiki/FAQ\n\nOn Fri, Sep 4, 2015 at 3:30 PM Valentin Waeselynck notifications@github.com\nwrote:\n\nThanks! Maybe I could start a FAQ page in the Wiki for this sort of stuff?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/112#issuecomment-137688736.\n. Thanks! Great fix\n. Released as 0.12.2\n. thx\n. I think check should be IDeref for both Clojure and ClojureScript. We need conn to deref to DB, so we should check exactly for that.\n. Merged in 0.13.1\n. It\u2019s a mistake on your side somewhere. Please recheck\n\nclj\n(-> (d/empty-db { :contacts/tag { :db/cardinality :db.cardinality/many }})\n    (d/db-with  [{:db/id 123 :contacts/tag #{:a :b}}])\n    (d/db-with  [{:db/id 123 :contacts/tag #{:c}}])\n    (d/datoms :eavt))\n\u2193\n(#datascript/Datom [123 :contacts/tag :a 536870913 true]\n #datascript/Datom [123 :contacts/tag :b 536870913 true]\n #datascript/Datom [123 :contacts/tag :c 536870914 true])\nclj\n(def conn (d/create-conn { :contacts/tag { :db/cardinality :db.cardinality/many }}))\n(d/transact! conn [{:db/id 123 :contacts/tag #{:a :b}}])\n(d/transact! conn [{:db/id 123 :contacts/tag #{:c}}])\n(d/datoms @conn :eavt)\n\u2193\n(#datascript/Datom [123 :contacts/tag :a 536870913 true]\n #datascript/Datom [123 :contacts/tag :b 536870913 true]\n #datascript/Datom [123 :contacts/tag :c 536870914 true])\n. I would make a great, small, reusable library. Want to write one?\n. Once we have NOT, it should be pretty straightforward:\nclj\n(d/q '[:find ?b\n       :in $ [?ref ...]\n       :where [_ ?ref ?b]\n              (not [?b _ _])]\n     db\n     (for [[attr props] (:schema db)\n           :when (= :db.type/ref (:db/valueType props))]\n       attr))\nIt\u2019s not very performant, but should work.\nWithout it, simples way to do this is just to walk over the database and build such a list on your own:\nclj\n(let [eids (for [datom (d/datoms db :eavt)]\n             (:e datom))\n      refs (for [[attr props] (:schema db)\n                 :when (= :db.type/ref (:db/valueType props))\n                 datom (d/datoms db :avet attr)]\n             (:v datom))]\n  (clojure.set/difference (set refs) (set eids)))\n. I\u2019m not sure I understand you right. To get DB from Conn just do deref or @. It\u2019s the recommended way.\n. Great! Feel free to ask more :)\n. There was a problem with how I handled binding. Thanks for reporting!\n. Released 0.13.2\n. I\u2019m thinking about allowing raw datoms participate in transactions. I do not promise you it\u2019ll be with-datom, but it will be something, I think.\n\ndatomic sometimes sends the retraction datom after the addition datom\n\nCan you give more specific example? When Datomic does that, and what exact sequence of datoms causes NPE in DataScript? If you add and then delete the same datom it should work ok.\n. Ok here\u2019s what I did:\n- you can pass datoms to transact!, it will keep transaction number and respect :added attribute (addition/retraction)\n- you can pass transaction number to :db/add as well: [:db/add 1 :name \"Ivan\" 100]. Yes it means you can mess up transaction numbers, so be prepared to face the consequences\n- I changed let to if-let in with-datom so your use-case should work now\n- If you still prefer with-datom feel free to continue to using it via var dereferencing, I kept it private for now\n. Entity can\u2019t have only :db/id attribute. It can\u2019t even have :db/id attribute at all. It\u2019s a special, reserved name to represent entity id for cases when you look at entity in map form. If you remember, all data is stored as tuples <e, a, v> where e is that :db/id. If you have no attributes, there\u2019ll just be no records about that entity.\nDoes it help? If you\u2019ve managed to create entity with :db/id as an actual attribute, it is a bug: validation should\u2019ve catched that. Please share details.\n. Sorry, but (d/transact! conn [{:db/id -1}]) doesn\u2019t creates a thing. Try printing (d/datoms @conn), or (:tx-data (d/transact! conn [{:db/id -1}])).\nWhat you see from d/datoms is how data is actually stored. Think about it in terms of how things are implemented: if you\u2019re not storing an attribute for the entity, there\u2019s not place for entity id as well. Entity only exists if you\u2019re saying something about it.\n. Thanks!\n. Released as 0.13.3\n. Can I see a schema?\nOn Sun, Nov 8, 2015 at 6:48 PM lorddoig notifications@github.com wrote:\n\n(d/transact! conn [{:db/id -1\n                    :name name\n                    :id }])\nThis throws with the string representation of the UUID \"is not seqable\".\nCasting to string first works fine.\nAm I misusing/abusing datascript? Have I missed something? Or is this a\nbug? Other issues suggest UUIDs are supported.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/128.\n. Cannot do anything unless I have more details from you\n. It is modelled after Datomic, all the APIs, in some sense, parts of the\narchitecture. They work together rather well. But it\u2019s completely\nindependent project, Cognitect wanted this to be stated clear, so no\nconfusion.\n\nOn Tue, Nov 17, 2015 at 4:34 AM Chet Corcos notifications@github.com\nwrote:\n\nDataScript is built totally from scratch and is not related by any means\nto the popular Clojure database Datomic\nThey seem peculiarly similar -- are they really not meant to work together?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/129.\n. No. There\u2019re some project aiming to seamless sync between Datomic and\nDataScript. E.g. http://github.com/sgrove/dato\n\nOn Thu, Nov 19, 2015 at 11:56 AM Chet Corcos notifications@github.com\nwrote:\n\nClosed #129 https://github.com/tonsky/datascript/issues/129.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/129#event-468808124.\n. Frequent in-place updates, I think.\n\nOn Fri, Nov 20, 2015 at 3:31 AM Chet Corcos notifications@github.com\nwrote:\n\nawesome! Just curious, what are types of queries or systems that datomic\nis necessarily not so good at?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/129#issuecomment-158203897.\n. This is q4:\n\n[\"q4\" '[:find ?e ?l ?a :where [?e :name \"Ivan\"]\n                              [?e :last-name ?l]\n                              [?e :age ?a]\n                              [?e :sex :male]]]\nWhat this query does is quite different from map lookup:\nFor 20000-person database, there\u2019s not a one, but ~2500 Ivans. Half of them have :sex :male, so the size of the result set is ~1250 tuples. Allocating result set, filling it with tuples and creating tuples themselves is all part of the query, and it\u2019s not free. DataScript also does deduplication of result (you wont\u2019s see same tuple twice in the results), which is not free as well.\nAlso note that this query has 3 joins (2500 \u00d7 20000, 2500 \u00d7 20000, 2500 \u00d7 10000) which are also not free. Because of the way data is stored (flat sorted sets, not hierarchical nested maps), you can\u2019t \u201cjust lookup\u201d rest of the structure given its id.\nBottom line is, yes, if you can do with hierarchical nested maps, and don\u2019t need to query them other than lookup-by-id, you better off with them.\nDataScript is in different category, so expect different tradeoffs: query speed depends on the size of result set, you need to sort clasuses to have smaller joins, accessing entity properties is not free given its id, etc. As a benefit, you gain ability to query dataset for different projections, forward and reverse reference lookups, joins between different sets, etc. And direct index lookup (datascript.core/datoms) is still fast and comparable to lookup in a map (at least comparable, think binary lookup vs hashtable lookup, logarithm vs constant). Queries do much more than that.\n. I\u2019m optimizing certain sorts of queries in https://github.com/tonsky/datascript/blob/master/src/datascript/query_v3.cljc. But it won\u2019t be orders of magnitude probably. I don\u2019t think it\u2019s possible to have complex queries and high performance at the same time. When you need speed, use direct index lookup.\n. You can materialized queries right now, utilizing DS transaction queue notification. Every time there\u2019s a transaction, update your view.\nImagine you have a query like this:\n[:find ?e ?l ?a\n :where [?e :name \"Ivan\"]\n        [?e :last-name ?l]\n        [?e :age ?a]\n        [?e :sex :male]]\nAnd got a transaction like this:\n[[:db/add 788 :age 15]]\nWhat do you suggest your algorithm should do? Given that query was run before, and you have cached result of it and everything else you need. Just want to understand what are you proposing.\n. What's pulled entity. I thought you were talking about caching query results. Query results consist of a lot of entities \n-----Original Message-----\nFrom: \"vibl\" notifications@github.com\nSent: \u200e09.\u200e12.\u200e2015 18:35\nTo: \"tonsky/datascript\" datascript@noreply.github.com\nCc: \"Nikita Prokopov\" prokopov@gmail.com\nSubject: Re: [datascript] Idea for a reactive and faster alternative toDatascript (#132)\nThe entity involved in the transaction could be pulled from the Entities index (one lookup) after the index has been updated with this transaction (so that the pulled entity contains the new data).\nAll the \"delta queries\" (i.e., queries without entity joins) could then be run against this one entity, which would be very cheap.\nThen, the query results (if any) could be merged into the corresponding views.\nIn your example, let's say the pulled entity is :\n{:db/id      788\n :name       \"Ivan\"\n :last-name  \"Petrov\"\n :sex        :male\n :age        15\n :salary     50000 }\nThe query result would be:\n{:db/id      788\n :last-name  \"Petrov\"\n :age        15 }\nWhich, merged into the view corresponding to this query, would update it pretty efficiently.\nWhat do you think? \nVianney\n\u2014\nReply to this email directly or view it on GitHub.\n. Ok, yes, you can pull up other attributes of an entity from a transaction.\nWhat do you suggest we should do next? How to insert it into the query?\n(btw pulling entity attributes is a very fast operation in current impl \u2014\nbasically two binary searches, so I doubt it\u2019ll change anything in time\ncomplexity of query algorithm. We should probably move on to discuss\nmodifications that you suggest for the query)\nOn Wed, Dec 9, 2015 at 7:45 PM vibl notifications@github.com wrote:\n\nWhen I write \"pulling an entity\", I mean \"querying the database to extract\nthe data of an entity\".\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/132#issuecomment-163240089.\n. What do you call a \u201cdelta query\u201d? A query without join is just a single index lookup. They\u2019re already pretty fast, why optimise them further? And, if they are so fast, why run all of them on transaction time instead of running them on demand?\n. @metasoarous well, I\u2019d like to hear you thoughts, of course. I agree that recomputing queries would be barely useful from perf standpoint. Feel free to open up new issue to discuss this\n. The :where clauses are analyzed in order they\u2019re specified, there\u2019s no\nanalysis or reordering in DS. Just move not= clause later when both ?word-1\nand ?word-2 are already defined, and it should work. Also you can get rid\nof (= 3 ?length):\n\n(d/q '[:find ?word-1 ?word-1.word ?word-2 ?word-2.word\n       :where\n         [?word-1 :length 3]\n         [?word-2 :length 3]\n         [(not= ?word-1 ?word-2)]\n         [?word-1 :annagram ?ann]\n         [?word-2 :annagram ?ann]\n         [?word-1 :word ?word-1.word]\n         [?word-2 :word ?word-2.word]]\n       @conn)\nOn Fri, Dec 4, 2015 at 9:50 AM Howard M. Lewis Ship \nnotifications@github.com wrote:\n\nDatascript 0.1.13\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/134#issuecomment-161866582.\n. Sorry @zilti, docs are not there yet.\n\nDatomic team has excellent intro videos explaining basic concepts:\n- http://www.youtube.com/watch?v=RKcqYZZ9RDY\n- http://www.youtube.com/watch?v=bAilFQdaiHk\nThis is one of the best ways to learn about and master DataLog:\n- http://www.learndatalogtoday.org\nMost of the API is similar to Datomic which is documented here:\n- http://docs.datomic.com/query.html\n- http://docs.datomic.com/transactions.html\n- http://docs.datomic.com/schema.html\n- http://docs.datomic.com/identity.html\n- http://docs.datomic.com/indexes.html\n- http://docs.datomic.com/pull.html\nAfter that, there\u2019s a short section explaining how DataScript is different:\n- https://github.com/tonsky/datascript#differences-from-datomic\nA good idea might be to listen to a webinar where I write an app using most of DataScript features:\n- https://vimeo.com/114688970\nHope that helps!\n. I\u2019ll maintain an actual list of resources here: https://github.com/tonsky/datascript/wiki/Getting-started\n. @kristianmandrup this is a bad idea, you\u2019ll have to do a lot of repetitive manual conversions to bridge with cljs api from js.\nDataScript has a dedicated JS api, it\u2019s better to use it. Here\u2019s a lot of examples: https://github.com/tonsky/datascript/blob/master/test/js/tests.js#L101\n. nice!\nOn Mon, Feb 29, 2016 at 4:06 PM Kristian Mandrup notifications@github.com\nwrote:\n\nI started this api overview\nhttps://github.com/tonsky/datascript/wiki/API-overview wiki page.\nPlease help out ;)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/135#issuecomment-190136972.\n. Thanks Thomas! So tired here fixing all the Datomic inconsistencies :)\n\nOn Sun, Dec 6, 2015 at 10:06 AM Thomas Spellman notifications@github.com\nwrote:\n\nDatomic supports binding a subquery pattern in a query's pull function:\n(d/q '[:find [(pull ?e subquery) ...]\n       :in $ ?a subquery\n       :where [?e ?a]] @conn join-key [(hash-map join-key query)])\nThis syntax fails in Datascript with:\nerror {:message \"Cannot parse pull expression, expect ['pull' src-var?\nvariable (constant | variable)]\", :data {:error :parser/find, :fragment\n(pull ?e subquery)}}\nThis does however work using regular variable binding syntax ?subquery\neven though this fails on Datomic:\n(d/q '[:find [(pull ?e ?subquery) ...]\n       :in $ ?a ?subquery\n       :where [?e ?a]] @conn join-key [(hash-map join-key query)])\nThat said, I think Datascript's syntax is more intuitive but is just\ndifferent than the Datomic docs http://docs.datomic.com/query.html\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/136.\n. Well, it is in a sense that I haven\u2019t thought about someone would use it this way. I\u2019ll think what can be done about that. Meanwhile, you can rewrite your query like this:\n\n(d/q '[:find ?e\n       :in $ ?client-squuid\n       :where [?t :thing/squuid ?client-squuid]\n              [?e :appointment/client ?t]]\n     (d/db conn)\n     #uuid \"5664cb92-8256-49f9-a7ff-30d3d00939c4\")\n. Sorry, I\u2019ll look into it soon.\nOn Wed, Jan 6, 2016 at 3:47 AM Aaron Brooks notifications@github.com\nwrote:\n\nI've created issue #141 https://github.com/tonsky/datascript/issues/141\nto track this as an issue. I probably should have filed an issue first. Let\nme know what the preferred procedure is for future issues/pull-requests.\nThanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/pull/138#issuecomment-169143532.\n. Thanks, this is really a proper fix in just the right place. And test coverage \u2014 thanks for great PR!\n. Interesting idea! I can totally see the benefits of using mori together with datascript. You\u2019re right about building it together though \u2014 google closure compiler needs to share information about which minifications it does. I can probably get a DS built as a module compatible with specific mori build, but with every change in mori we\u2019ll have to re-build DS to match new minifications.\n\nI think more productive way to solve this is to use DS and mori in their source code form, compile them together, then use.\nI think I can add another namespace that will expose API to JS without intermediate conversions. That shouldn\u2019t be a problem\n. > Although that would require using Closure for my own code as well, right?\nNot necessarily, no. DS and mori can still mark API fns with ^:export so their names won\u2019t be minified with Google Closure compiler.\n. Here\u2019s what I did: I marked all fns in datascript.core with ^:export, meaning their names would not be touched by google closure minifier. What you need now is just to run a compiler and point it to both datascript and mori sources at the same time, and use datascript.core functions instead of datascript.js. Let me know if you have any issues with that\n. As of DataScript 0.16.0 I\u2019ve removed externs from datascript.core as they were disabling dead code elimination (see #191). @typeetfunc if you want to continue use datascript.core directly from JS, just add externs fine when building datascript-mori. Merged\n. Added in bcdb222f7b8883aec6e3a5889bf880f31a14ba6e\n. A good idea indeed! Thank you\n. Problem here is that DataScript does not understand ref when specified in a\nform of a map.\nDatomic seems to allow using such maps in certain cases, so I\u2019ll have to\ninvestigate and let DataScript do the same.\nUntil then, just use {:foo 1} instead.\nOn Sun, Mar 20, 2016 at 7:45 PM Petter Eriksson notifications@github.com\nwrote:\n\nUse case:\n- I want to be able to describe 1-to-1 relationships in my schema.\n- My solution is to create unique refs in my schema.\nDid:\n- Added an attribute :foo to my schema with both :db/valueType\n  :db.type/ref and :db/unique db.unique/identity.\n- Transacted an entity associating :foo to a map (ref)\nExpected:\n- :foo to associated with the reference, which is unique in the db.\n- Refs can be unique and used in this way in Datomic.\nHappened:\n- Error: #error {:message \"Expected number or lookup ref for entity\n  id, got {:db/id 1}\", :data {:error :entity-id/syntax, :entity-id {:db/id\n  1}}}\nCode to repro:\n(require '[datascript.core :as d])\n(def conn (d/create-conn {:foo {:db/valueType :db.type/ref\n                                :db/unique    :db.unique/identity}}))\n(d/transact conn [{:db/id 1}{:foo {:db/id 1}}]) ;; throws\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/147\n. @kristianmandrup wow! that\u2019s a lot of resources! Added a link here: https://github.com/tonsky/datascript/wiki/Getting-started\n. You can just extend IDeref (cljs) or implement IDeref (clj) and make sure it has DB inside:\n\nclj\n(defn ^:export conn? [conn]\n  (and #?(:clj  (instance? clojure.lang.IDeref conn)\n          :cljs (satisfies? cljs.core/IDeref conn))\n    (db/db? @conn)))\nWould that work?\n. You get me confused. You want to keep listeners functionality but re-implement Conn as a protocol? What's the idea behind this? How will it be different from DataScript's Conn?\n. Yeah, but protocol cannot carry implementation anyways. So its either protocol or implementation, not both\n. > Wait, is this because, like Datomic, variables need binding at invocation time through the use of [ ] in the rule definition?\nNo, DataScript doesn\u2019t support that. Also I believe it\u2019s just validation check in Datomic, not an actual perf optimization. The only value type supported ATM is :db.type/ref. For anything else, just\ndon\u2019t specify :db/valueType\nOn Wed, Mar 23, 2016 at 4:19 PM David Meister notifications@github.com\nwrote:\n\nI have an attribute that looks like this in my schema:\n:foo/name {:db/unique :db.unique/identity\n                  :db/valueType   :db.type/keyword}\nI'm adding to it from quite a few different places in my code, but it's\nalways a keyword. I was hoping to not have to write :pre conditions to\nenforce the keyword-ness, and let the enforcing happen at the db level, but\nthe above schema does not work.\nI get this error:\ndb.cljc:536Uncaught #error {:message \"Bad attribute specification for\n{:foo/name {:db/valueType :db.type/keyword}}, expected one of\n{:db.type/ref}\", :data {:error :schema/validation, :attribute :foo/name,\n:key :db/valueType, :value :db.type/keyword}}\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/152\n. I mean, values of all types are supported, just don\u2019t specify type of them\nin a schema\n\nOn Wed, Mar 23, 2016 at 4:32 PM Nikita Prokopov prokopov@gmail.com wrote:\n\nThe only value type supported ATM is :db.type/ref. For anything else, just\ndon\u2019t specify :db/valueType\nOn Wed, Mar 23, 2016 at 4:19 PM David Meister notifications@github.com\nwrote:\n\nI have an attribute that looks like this in my schema:\n:foo/name {:db/unique :db.unique/identity\n                  :db/valueType   :db.type/keyword}\nI'm adding to it from quite a few different places in my code, but it's\nalways a keyword. I was hoping to not have to write :pre conditions to\nenforce the keyword-ness, and let the enforcing happen at the db level, but\nthe above schema does not work.\nI get this error:\ndb.cljc:536Uncaught #error {:message \"Bad attribute specification for\n{:foo/name {:db/valueType :db.type/keyword}}, expected one of\n{:db.type/ref}\", :data {:error :schema/validation, :attribute :foo/name,\n:key :db/valueType, :value :db.type/keyword}}\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/152\n. There\u2019re few complications: see my comments https://github.com/tonsky/datascript/pull/107\n. Just pushed 0.15.1\n. Database is nothing more but a pointer. Just forget about old one and\ncreate a new one.\n\n\nIf you want to keep a connection, but replace a DB it references, do\n(d/reset-conn! conn (d/empty-db schema))\nOn Thu, Mar 24, 2016 at 7:07 PM sandric notifications@github.com wrote:\n\nSorry for maybe a dumb question - I'm new to datalog. My particular\ninterest is in complete and fast destroy of whole database - for now I\niteratively delete all keys but thats too long. I need that because of lein\nfigwheel not reloading somehow datascript database - after each change to\nproject its repopulates with fixtures, and the same is with compiled js\nimported into rails app - doe to turbolinks I guess. thx.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/154\n. Great, thanks! Nice contribution.\n. Just pushed 0.15.1\n. Added it to demo section\n. Do you have any use-case for that? If you use db-with with tx-meta, tx-meta will be ignored and thrown away.\n. I think signature is enough, no? In what scenario would you see a comment\nbut wouldn\u2019t see a signature?\n\nOn Mon, Apr 25, 2016 at 8:55 PM lvh notifications@github.com wrote:\n\nSort-of. I have some code that wishes it could call both db-with and with\nwith the same signature. I could see why you wouldn't want to take an\nunused argument though; but then perhaps you'd prefer a comment?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/pull/158#issuecomment-214374619\n. Can you give an example what kind of comment would it be? And where should\nit be located?\n\nOn Tue, Apr 26, 2016 at 2:05 AM lvh notifications@github.com wrote:\n\nIt's not so much that you would see one and not the other; just that if\nyou don't know/forgot tx-meta is a transaction-level only thing (as opposed\nto tx-data, which affects the DB), that might be confusing since the two\nare obviously related fns; hence why I went for the comp first.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/pull/158#issuecomment-214466722\n. Ok :)\n. I don\u2019t want stuff in schema not meaning anything. And I can\u2019t figure out how to add types and enforce their meaning, at least not now. See prior history in #107 and #152 \n. Hi,\n\nmetadata would mean an additional field to store, additional copy operation to do and additional checks when working with Datoms. Because datoms are so low-level and on a hot path all the time, it would be too expensive to have \u201cjust in case\u201d.\nTry using attributes instead\n. Nice!\n. Not exactly. Last time I looked at them, they were source of a great\nconfusion (e.g. entity api would return idents but pull api will return\nids, can\u2019t use them in queries, etc). Much easier to use keywords directly.\nAlso there\u2019s a certain performance hit associated with idents.\nBut you can always create :db/ident attribute in schema, mark it as\n:db.unique/identity and use lookup refs to access entities you need\nreference via keywords\nOn Tue, Jun 21, 2016 at 6:59 PM Isaac Zeng notifications@github.com wrote:\n\nDoes DataScript think support :db/ident?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/issues/162, or mute the thread\nhttps://github.com/notifications/unsubscribe/AARabDYCNAZcHKve_3hqfCNs1zeFawLxks5qN-AjgaJpZM4I6tHO\n.\n. Awesome! Thanks\n. Just pushed 0.15.1\n. No access\n\nOn Wed, Jul 6, 2016 at 9:47 PM Nick Alexander notifications@github.com\nwrote:\n\n@tonsky https://github.com/tonsky you're welcome! We love DataScript\nand have learned a lot from it. We (a team at Mozilla) are starting a small\nproject to build a similar Datomic-API-alike over at\nhttps://github.com/mozilla/datomish. Early days yet but we'd love to have\nyour thoughts.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/pull/164#issuecomment-230814201,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AARabJF0vveCnxycunGY1uq4OZMDyaZeks5qS84ggaJpZM4JAjFf\n.\n. Nice! I already smell CREATE TABLE EAVT; :))\n\nDo you plan to reuse any parts of DataScript code? I think query engine\nwould be nice to have, but I\u2019m not sure how easy it\u2019d be to reuse it. Do\nyou plan queries at all?\nOn Wed, Jul 6, 2016 at 9:52 PM Nick Alexander notifications@github.com\nwrote:\n\n@tonsky https://github.com/tonsky sorry about that -- didn't realize we\nhadn't opened it up yet. I've added you as a collaborator.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/pull/164#issuecomment-230815784,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AARabLtTwXHbjTlJkY8c2vJ_C5c3jGVhks5qS888gaJpZM4JAjFf\n.\n. Are you using JVM version, right? It\u2019s very interesting to get to the bottom of the issue. \u201cComparison method violates its general contract!\u201d sounds very suspitious\n. Funniest thing: java.util.Comparator returns int, so (- long1 long2) was coerced to int, possibly with the loss of sign.\n\nFixed by replacing it with Long/compare call. Also found a nullpointer in init-db when all datom eids were bigger than tx0. Fixed that too.\nTry 0.15.2\n. Yay! Happy I was able to help.\nOn Thu, Jul 14, 2016 at 7:06 PM Wes Brown notifications@github.com wrote:\n\n@tonsky https://github.com/tonsky The test case that I have above works.\nI also tested it by creating large DataScript DBs, dumping it out to\nDatoms, and then initializng the database two different ways. So my test\ncovered three ways to create a database:\n- Vector of maps with large long for :db/id, which worked before this\n  fix. (d/db-with (d/init-db [] schema) fs)\n- Vector of dumped Datoms, using (d/db-with (d/init-db [] schema) ds)\n  which worked and was faster than working with maps.\n- Vector of dumped Datoms, using (d/init-db ds schema), which the\n  failing case fell within.\nBelow shows the logs that indicate the performance of each type.\n16-07-14 12:53:20 nephelai.local DEBUG [lg-databroker.util:?] - [11128ms] Transformed 149994  facts into DS DB with 645518 datoms\n16-07-14 12:53:20 nephelai.local DEBUG [lg-databroker.util:?] - [119ms] Transformed DB into datoms\n16-07-14 12:53:26 nephelai.local DEBUG [lg-databroker.util:?] - [5994ms] Inited DB by transacting datoms onto empty db\n16-07-14 12:53:27 nephelai.local DEBUG [lg-databroker.util:?] - [1034ms] Inited DB with datoms\n16-07-14 12:53:28 nephelai.local DEBUG [lg-databroker.util:?] - [330ms] DB comparisons came out true\n16-07-14 12:53:49 nephelai.local DEBUG [lg-databroker.util:?] - [11016ms] Transformed 150000  facts into DS DB with 647398 datoms\n16-07-14 12:53:49 nephelai.local DEBUG [lg-databroker.util:?] - [94ms] Transformed DB into datoms\n16-07-14 12:53:55 nephelai.local DEBUG [lg-databroker.util:?] - [6117ms] Inited DB by transacting datoms onto empty db\n16-07-14 12:53:56 nephelai.local DEBUG [lg-databroker.util:?] - [840ms] Inited DB with datoms\n16-07-14 12:53:56 nephelai.local DEBUG [lg-databroker.util:?] - [236ms] DB comparisons came out true\n16-07-14 12:54:14 nephelai.local DEBUG [lg-databroker.util:?] - [10968ms] Transformed 150000  facts into DS DB with 645122 datoms\n16-07-14 12:54:14 nephelai.local DEBUG [lg-databroker.util:?] - [96ms] Transformed DB into datoms\n16-07-14 12:54:20 nephelai.local DEBUG [lg-databroker.util:?] - [6113ms] Inited DB by transacting datoms onto empty db\n16-07-14 12:54:21 nephelai.local DEBUG [lg-databroker.util:?] - [954ms] Inited DB with datoms\n16-07-14 12:54:21 nephelai.local DEBUG [lg-databroker.util:?] - [302ms] DB comparisons came out true\n16-07-14 12:54:38 nephelai.local DEBUG [lg-databroker.util:?] - [10862ms] Transformed 150000  facts into DS DB with 646740 datoms\n16-07-14 12:54:39 nephelai.local DEBUG [lg-databroker.util:?] - [89ms] Transformed DB into datoms\n16-07-14 12:54:45 nephelai.local DEBUG [lg-databroker.util:?] - [6098ms] Inited DB by transacting datoms onto empty db\n16-07-14 12:54:45 nephelai.local DEBUG [lg-databroker.util:?] - [801ms] Inited DB with datoms\n16-07-14 12:54:46 nephelai.local DEBUG [lg-databroker.util:?] - [295ms] DB comparisons came out true\n- Initializing 150K maps into an empty DataScript database takes on\n  average 1100ms\n- Dumping a DataScript database containing ~650K Datoms takes on\n  average 95ms\n- Using db-with with ~650K Datoms on an empty DataScript database\n  takes on average 6100ms\n- Using init-db with ~650K Datoms takes about ~850ms.\nSo this fix gave me an order of magnitude increase in performance in\ncreating a database relative to the original maps, and about 8 times an\nincrease relative to db-with which was my workaround before this fix.\n\ud83d\udc4d Thank you very much.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/issues/165#issuecomment-232659799,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AARabB2zjRt2SRTXWrPnfK5JGIgXUDKIks5qVjRRgaJpZM4JDxFc\n.\n. Practically, I\u2019d probably suggest you to go with third option. Yes, max-eid is not advanced, but you can see it as a plus: you\u2019ve taken id management into your own hands, so DataScript algorithm steps away. You can actually mix both, continue using DS-generated ids at the same time, because they won\u2019t overlap in any foreseeable future.\n\nThere\u2019s a fourth option, actually, which we kind of use right now in Cognician when syncing between Datomic and DataScript. You can\u2019t tell Datomic which exact entity id you want (unlike DS, which is unfortunate). So we\u2019re not relying on entity ids at all: each entity also has an :.../uuid attribute (:user/uuid, :resource/uuid, :session/uuid etc). When syncing, we calculate dependencies based on uuids, not entity ids. In fact, we don\u2019t care about entity ids and if they match between Datomic and DataScript. We go as far as don\u2019t send entity ids over the wire at all. We rely on default allocator in both. But we track uuids and make sure dependencies are tracked right in each case. It\u2019s a little more work, and I\u2019d certainly prefer not doing it if that was possible (e.g. if Datomic\u2019d allowed for manually assigning entity ids), but it works ok in practice.\n. > Do you know of any other issues that going past the tx0 limit might cause, other than the auto increment?\nNothing else\n\nI saw that some performance issues might arise (#56), any idea how much of a concern this is in practise?\n\nI guessed that, no idea how serious it is.\n\nalso, is there any way to hook into the way that DS chooses new entity IDs, either through next-eid or messing with the temp IDs through listen! or similar?\n\nNo, but you can always put a specific id you want into transaction\n. Modern Datomic lets you use any unique value in lookup refs. Maybe it wasn\u2019t the case when they were first introduced, or I haven\u2019t checked that properly. Anyways, fixed now\n. Yes, downside of DataScript approach is that you have to think about order of clauses in a transaction. That probably hurts most when doing bunch of retractions.\nUpside of DataScript approach, on the other hand, is that you can do all insertions you need in a single transaction and use lookup refs to entities you\u2019ve inserting in the same tx, whether in Datomic neither lookup refs nor idents won\u2019t work until you commit a transaction, so to use them you have to split your tx in multiple ones, which I think is much worse. It means: thinking about order (anyways!), having to attach tx-meta to multiple transactions, no guarantee stuff won\u2019t happen in between when you split your transaction into several ones.\nAs you can see, if you\u2019re planning to rely on lookup refs, you have to think about the order you insert your stuff in anyways, but in DataScript you can put them in a single transaction, which I think worth minor inconveniences.\n. Closing since I don\u2019t plan to fix #172. Replacing schema is trivial to do in user code. What I don't want to do is to provide schema altering fn that will do that and ignore all the issues that arise from the fact that data stored in DS might not suit new schema (unique values, ref type, arities). If I ever provide such fn, it should deal with all the issues (~the same way Datomic deals with them, checking constraints, raising errors, not allowing certain migrations, etc). I'm not against changing schema, but for now only user of DS library is responsible for all the inconsistencies. That's why I think such a fn should not be included in DS yet (but all the tools to build it yourself are there already)\n. I think schema migration should work one of three possible ways:\n1. Always accept changes if they are safe (one to many, unique to non-unique)\n   - adjust internals if needed (index to no index and vice-versa)\n2. Validate changes if they're allowed but can be unsafe (many to one, non-unique to unique)\n   - throw if constraint is not satisfied by new schema\n3. Deny impossible changes (value type)\nImportant here is that DataScript will not change any user data as a result of schema alteration. That way user will have a chance to clean up data in the way that suits them, and DataScript will keep them safe by providing guarantees.\nThis would be an important piece of DataScript and I'll be happy to include it.\n. datascript.perf isn't included in JAR file, it's dev-time-only namesapce. query-v3 is not ready for use, and it depends on it, that's why you get an error\n. Wait, but will that work?\nclj\n  (ds/q '[:find ?e\n          :where\n          [?a :db/unqiue :db.unique/identity]\n          [?a :db/ident ?attr]\n          [?e ?attr _]]\n    db)\n. Yes, there are differences, all documented and discussed:\n- I have nothing against reified schema & schema migrations, it just seemed we can get away without them so far. Here are my thoughts on schema migrations https://github.com/tonsky/datascript/issues/174\n- Integer ids for attributes, a billion of special cases for idents, and that abstraction being leaky and leading to lots of mistakes all lead to a very conscious decision not to repeat that mistake in DS  (my thoughts https://github.com/tonsky/datascript/issues/162)\n- There\u2019re also transaction differences you probably should be aware of https://github.com/tonsky/datascript/issues/172\nWhether or not coping with all this worth it is your call. But I\u2019m curious, why did you wanted to use both Datomic and DataScript in the first place? And why interoperability? What code did you planned to share between the two?\n. Just released 0.15.3 with your PR, I assume I can close this\n. Thanks, good stuff!\n. I think you\u2019re using JS API wrong.\ndatoms/seek-datoms accept variable arguments, not an array. Change d.datoms(db, \":eavt\", [\"name\", \"Ivan\"]) to d.datoms(db, \":eavt\", \"name\", \"Ivan\"). There\u2019re two tests showing it https://github.com/tonsky/datascript/blob/ba1246704c1dfe48a5f0abc330a5098b5f79edce/test/js/tests.js#L429-L434\nQueries over native JS arrays work well too. Check these tests out https://github.com/tonsky/datascript/blob/ba1246704c1dfe48a5f0abc330a5098b5f79edce/test/js/tests.js#L347-L379\nI\u2019m not sure what was your intention doing this:\nassert_eq(\"Ivan\", d.q('[:find ?c . :in $ ?e :where [?e \"name\" ?c]]', db, [\"name\", \"Ivan\"]));\nbut if you can describe to me what were you trying to do I might be able to help you write the query for that.\n. You\u2019re right, lookup refs were broken in JS API for index APIs and queries. I didn\u2019t realise you were trying to use them because for them to work you need to mark your attributes as :db/unique in schema.\nI fixed that & added correct test cases. See https://github.com/tonsky/datascript/commit/4b7e2c447da7253be5eb846b231ab831542aba7e\n. Thanks for raising the issue.\nYes, that code could be called in some circumstances (see https://github.com/tonsky/datascript/blob/7dada20fa98df268fdb05083638781ae7d63b3b3/test/datascript/test/query_fns.cljc#L262-L268). It\u2019s kind of a hack to call predicates that way, and will be removed at some point in the future, but so far it\u2019s possible and better to have it fixed, so thank you!\nAs of speed improvement, your changes didn\u2019t do much of a difference, but I came up with my own version that gives 25-30% boost for predicate/fn calls in queries. See https://github.com/tonsky/datascript/blob/7dada20fa98df268fdb05083638781ae7d63b3b3/src/datascript/query.cljc#L422-L441\n. I believe it does, yes. Queries and recursive rules especially are some of the least optimized parts of DataScript, plenty of other opportunities there to speed that stuff up\n. thx!\n. Sorry \u2014 I\u2019m on vacation currently. If you\u2019re eager to fix, you\u2019ll probably\nhave to look somewhere around\nhttps://github.com/tonsky/datascript/blob/d45ddf6e73a542e3bcf6e8845c0c21d2eea20f21/src/datascript/db.cljc#L934-L949\nAlso, I\u2019m sure there\u2019re plenty of ways to work arounds this. Just don\u2019t use\nthat particular combination, for example.\nOn Mon, Oct 24, 2016 at 12:42 AM Conor White-Sullivan \nnotifications@github.com wrote:\n\nAny word on this? Pretty critical for something I'm building -- happy to\nhelp try to fix it if you've got any guidance.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/issues/182#issuecomment-255619948,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AARabJWEAsuWUmhPpQlq7Ov-b3X_oocxks5q2-LLgaJpZM4KLveC\n.\n. Sure. Please provide a PR\n. Oh, great! Seems like I accidentally fixed your issue :) Pretty bad one, took me couple of hours figuring of debugging before I understood what\u2019s going on. Interesting idea! I think, this should be done a little more in line with how Datomic works. E.g. using :db/ident for function name (keyword), :db/fn for actual function value (IFn) and [<ident> & <args>] syntax for calling it in a transaction. Do you mind making these changes?\n. Thanks! I\u2019m adding array-map, list and set for completeness too https://github.com/tonsky/datascript/commit/b864f53644d0e77ec3134fba24afb6ca1c17103e\n. I\u2019ve merged it anyways. Seems impactful. thanks!. Looks like there\u2019s no performance gain here. I\u2019d prefer to keep transient code then, as it could be shared between two platforms.\n\nTransients are not as bad as they seems. Inserting long sequences into transients is faster than into ArrayList, because Arrays list needs to grow and copy memory because of that. Transients just allocate more chunks. Converting transient \u2192 persistent is noop as far as I remember. Yeah but change was for Clojure part only too. There\u2019s no ArrayList in CLJS. Right, I missed the js/Array change for some reason...\nI have JS benchmarks, just run them, no perceived difference still. Maybe because those arrays are usually not that big at all, and transients are good enough for them.\nWithout patch:\n[~/Dropbox/ws/datascript/bench] ./bench -c jvm btset\nRunning btset within {BENCH_PROJECT datascript-jvm, BENCH_BUILD 362/6a25c6c/0.15.5-12} ...\nReflection warning, /private/var/folders/hc/c2bz_59n0_94ryjcs5s2tpg00000gn/T/form-init592992162979874177.clj:1:960 - call to static method invokeStaticMethod on clojure.lang.Reflector can't be resolved (argument types: unknown, java.lang.String, unknown).\n     btset   from-seq        100             0.017273245941274936\n     btset   from-seq        500             0.11483531044764031\n     btset   from-seq       5000             1.656449809817017\n     btset   from-seq      10000             3.5051755344045574\n     btset   from-seq      20000             7.443530250021389\n     btset   from-seq      50000             20.508167860507964\n[~/Dropbox/ws/datascript/bench] ./bench -c js btset\nRunning btset within {BENCH_PROJECT datascript-v8, BENCH_BUILD 362/6a25c6c/0.15.5-12} ...\nCompiling ClojureScript...\nCompiling \"target/datascript.js\" from [\"src\" \"bench/src\" \"test\"]...\nSuccessfully compiled \"target/datascript.js\" in 52.43 seconds.\n     btset   from-seq        100             0.04097039041363866\n     btset   from-seq        500             0.23085446866563938\n     btset   from-seq       5000             2.9754969970268363\n     btset   from-seq      10000             6.266580268740654\n     btset   from-seq      20000             13.151868975162506\n     btset   from-seq      50000             35.272413699825606\nWith patch appied:\n[~/Dropbox/ws/datascript/bench] ./bench -c jvm btset\nRunning btset within {BENCH_PROJECT datascript-jvm, BENCH_BUILD 362/6a25c6c/0.15.5-12} ...\nReflection warning, /private/var/folders/hc/c2bz_59n0_94ryjcs5s2tpg00000gn/T/form-init5875200550994434144.clj:1:961 - call to static method invokeStaticMethod on clojure.lang.Reflector can't be resolved (argument types: unknown, java.lang.String, unknown).\n     btset   from-seq        100             0.0182669416981745\n     btset   from-seq        500             0.1221851848602586\n     btset   from-seq       5000             1.6847006749610107\n     btset   from-seq      10000             3.5501816173051965\n     btset   from-seq      20000             7.60013460708516\n     btset   from-seq      50000             20.94306889951229\n[~/Dropbox/ws/datascript/bench] ./bench -c js btset\nRunning btset within {BENCH_PROJECT datascript-v8, BENCH_BUILD 362/6a25c6c/0.15.5-12} ...\nCompiling ClojureScript...\nCompiling \"target/datascript.js\" from [\"src\" \"bench/src\" \"test\"]...\nSuccessfully compiled \"target/datascript.js\" in 52.441 seconds.\n     btset   from-seq        100             0.03907611320377327\n     btset   from-seq        500             0.2311327584345952\n     btset   from-seq       5000             3.0398544333197854\n     btset   from-seq      10000             6.281387156248092\n     btset   from-seq      20000             13.191514800116419\n     btset   from-seq      50000             35.486351232727365. Yes. Sorry I forgot about that issue. Yes it\u2019s totally possible. Consider\nclj\n(js/JSON.stringify\n  (into-array \n    (for [d (d/datoms db :eavt)]\n      #js [(:e d) (name (:a d)) (:v d)]))). CLJS issue http://dev.clojure.org/jira/browse/CLJS-1871. Thanks :). Integer ids were needed because they\u2019re much more performant than strings. In fact, it\u2019s not that important that they are integers, rather, that they every id is comparable to any other id in a database.\nFor non-datascript datasources, there should be no limitation to what id should look like. E.g. in queries, if you add something different from DataScript DB, there should be no entity lookups, asserts for integer etc. If you\u2019re seeing this, please report, and I\u2019ll probably fix it.. So pull isn\u2019t working? Heh, I\u2019m not sure whether it should or not... Entity, pull and similar APIs were not meant for pluggable storages. The query engine was, it (should) be able to filter arbitrary collections. Try (ds/q '[:find (pull $x ?e [*]) :in $x :where [$x ?e ?a ?v]] @ds-db)\n. You\u2019re relying on a fact that if length of a tuple is equal to the length of the index then we\u2019re copying full tuple, keeping its layout. It is accidentally true for small tuples (those who use <8 attributes, thus array-map for :attrs, it automatically keeps order). If we hit 9+ attributes, (keys (:attrs rel1)) would return arbitrary ordering and layout of :attrs and actual layout of resulting tuples would be different.\nIn theory, it\u2019d be preferable to keep layout, and if the code would work that way your assumption would be true. If you figure out a way to get there, it\u2019ll be great. If you can work around it, not relying on that fact, it might be great too.. > When joining two tuples, the output tuple ordering must match the given\n:attrs\nThis is correct to some extent: we don\u2019t need that, but it would be nice\nand sane to have that. Prob. more efficient too (my guess)\nOn Mon, Mar 6, 2017 at 5:01 PM Wes Brown notifications@github.com wrote:\n@tonsky https://github.com/tonsky Interesting point, I hadn't realized\nthat. What is the impact of layout on a tuple join? I hadn't realized that\nthere was an implicit requirement, which I will try to state clearly:\n\nWhen joining two tuples, the output tuple ordering must match the\n   given :attrs.\n\nDo we have a test case for this scenario?\nLet me think on how to solve this. I've actually never run into this\nscenario on my particularly heavy use case. :)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/pull/203#issuecomment-284420165, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AARabLgiWWdFgANcSrakKq0kEQHwNLmeks5rjB_IgaJpZM4LVdXe\n.\n. > We could put a conditional that measures the number of attributes given. If l1 or l2 exceeds 8\nNo, this is too unreliable. CLJ/CLJS versions might change that parameter without a warning. It\u2019s also wouldn\u2019t be obvious from the code what are we relying on and why. We should use correct data structures. @wbrown-lg seems like an overkill to pull in a whole lib of fully implemented persistent data structure for such a small task. Can\u2019t we build it using vectors for example? (if I remember what this PR was about correctly). Right. I mean, it will have the same efficiency even, since array map lookups are same linear scans. Maybe we should rather use native arrays insted, might be even faster. How exactly did you trigger those warnings? I tried to reproduce myself but didn\u2019t see any. I also didn\u2019t see any noticeable perf increase. What version of cljs are you on?. So there was some compilation/name munglig issue on some versions of the compiler (can\u2019t say I remember what they were, maybe something different.) They do disappeared when I upgraded to 1.9.494. Maybe be give it a go?. Seems like 494 fixed it. I can reliable reproduce it on 1.9.473 and 1.9.293 on both DataScript repo and @AdamFrey repo. In both cases 494 works flawlessly. @seantempesta just released 0.16.0. Just tried, and it works fine for me:\n(d/q '[:find ?val \n       :in $ ?contains-fn\n       :where [_ :attr ?val]\n              [(?contains-fn #{\"s1\" \"s2\" \"s3\"} ?val)]]\n     (-> (d/empty-db)\n         (d/db-with [[:db/add 1 :attr \"x\"]\n                      [:db/add 2 :attr \"s1\"]\n                      [:db/add 3 :attr \"y\"]\n                      [:db/add 4 :attr \"s2\"]]))\n      contains?)\nBut I\u2019ll accept a PR that adds contains? if you fire one. Should\u2019ve been\nclj\n(let [db (d/empty-db {:k {:db/unique :db.unique/identity}})]\n  (d/q '[:find ?v\n         :in $ ?k\n         :where [?e :k ?k] \n                [?e :v ?v]]\n    (d/db-with db \n      [[:db/add -1 :k \"key\"]\n       [:db/add -1 :v \"value\"]]) \"key\")). Sure, but DataScript implementation depends on datom fields not being renamed. I have to include those. As for entity, what I was doing is aimed for JS only, that\u2019s right.. @zarkone are you sure it\u2019s datascript problem? maybe your build just doesn\u2019t know how to lookup externs served with libraries?. DataScript is only faster if it queries its indexes. If you query a collection it doesn\u2019t really matter how many positions are in your patterns. So if you put it all into DataScript DB you could probably win some speed on queries (by the expense of creating such indexes, of course, and you\u2019ll have to change your queries). > Gotcha. But I did understand correctly that datascript can only use those indexes if the data is really in EAVT form, right?\nNot only in EAVT form, but also stored in native DataScript indexes.\n\ndatascript also can't accidentally do anything with a sorted set that's smarter than just having a vec\n\nNo, not really.. (datascript.core/db-with (datascript.core/empty-db) [<tx-data>]). No, that\u2019s totally correct. It does work that way. Ok thanks will take a look. Should be fixed in 0.16.1. repo https://github.com/shparun/datascript_0.16.1_declared_bug\npossible cause https://github.com/tonsky/datascript/commit/19bdbd1beb5d027f5cc99e1c006002f2ac6bbd3a\nreported by: @shparun. I don\u2019t remember at this point. What does Datomic do?. best way to fix :) Thanks!. thanks!. Wait, but transacting { \":db/id\": 1, \"aka\": {\":db/id\": 2 }} shouldn\u2019t work in CLJ/CLJS either. You need to use entity id, not map: { \":db/id\": 1, \"aka\": 2}. Well I need some time to load up all the details on how this is supposed to work :) I\u2019ll check with Datomic and if it\u2019s a legitimate behaviour I\u2019ll see that it\u2019s supported in all platforms including JS. Oops. I didn\u2019t notice you were proposing a PR. I thought it\u2019s an issue and fixed it myself :) Gave you credit in CHANGELOG though. thx!. Sure, easy to add. As for errors, it might not throw if at that point there\u2019re no matching tuples so there\u2019s nothing to apply it to. Well, transact-async was only added for Datomic API compatibility. I don\u2019t think there\u2019s any value for it besides that. If you feel that you need to squeeze more perf out of it, just wrap normal transact into nextTick. Unless you have a strong case for nextTick to being there, of course. Thanks, I\u2019ve seen it too but haven\u2019t looked into it yet. I\u2019ll take a look. It\u2019s caused by defrecord in parser https://dev.clojure.org/jira/browse/CLJS-2237. or isn\u2019t supported in DataScript (yet). Neither is not. You can use rules to emulate or semantics though. Here https://github.com/tonsky/datascript/blob/master/test/datascript/test/query_rules.cljc#L42-L53\nDifferent branches of a rules are interpreted with implicit or http://docs.datomic.com/query.html#rules. README already says that not, not-join, or and or-join are not implemented. Changed wording a little https://github.com/tonsky/datascript/commit/eaa83844efda2e8d83e80521dd06a15a154c710b. I fixed that in https://github.com/tonsky/datascript/commit/898938250fd1bce7757c1a265e6633dea7e7d3fa. Just released 0.16.2 with that fix. I\u2019m not aware of any. Maybe you can use custom-built predicates for that\nthough\nOn Thu, Oct 5, 2017 at 3:06 AM John Conti notifications@github.com wrote:\n\nTo read the Datomic description of negation\nhttp://docs.datomic.com/query.html#not-caluses leads me to suspect that\nthere is a way to write (a more complicated) query without not that would\nhave the same behavior as not by using sub-queries. Am I mistaken?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/issues/238, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AARabCV9foG2EHhqYx5cdEm-qexwENXaks5so-VZgaJpZM4PuKc5\n.\n. Implemented not/or/not-join/or-join in 0.17.1. Enjoy!. @Harleqin it\u2019s actually fine. Predicate would work if both i1 and i2 are bound, what @Tavistock has is assignment, given the value of i1, calculate and assign value of i2. Hm. I remember hitting it but can\u2019t find any code or even an issue. Let\u2019s consider this a bug. It still needs Java wrappers, right? That means you have to use some intermediate library. Can\u2019t you AOT in there?\n\nLong-term plans are to add Java API to DataScript directly, but I doubt it\u2019ll happen any time soon. Meanwhile feel free to do what works best for you. I was thinking of copying Datomic API actually. But haven\u2019t look into it at all, so it might be not that good of an idea. @andrerichards if you still need AOT-compiled code it\u2019s deployed under classifier now:\n[datascript \"0.16.9-aot1.9\"]. thx!. Try clojure.core/re-pattern. Are you running this from Clojure or ClojureScript? If the latter, you\u2019ll need to pass fn as another input :in $ ?re-pattern and then call it as [(?re-pattern ?matcher) ?regex]. Yeah, you can\u2019t use it because it was not bundled with the JS file unfortunately. You can use JS to construct the pattern you need though, ClojureScript regexes are just JS regexes. maybe because it doesn\u2018t know what ?match is. You can\u2019t mix string and keyword attributes. I\u2019d love to read brief description of what such process (converting to self-hosted CLJS) looks like. What does it involve? Is it possible to have single codebase that works both ways (normal CLJS and self-hosted)?\nRealistically, though, I doubt I\u2019ll have any time in the near future to review patches and merge into master. It\u2019s been hard lately to find time to support any DS activity at all :(. yes, I prefer it dependency-free. tempids are there for you to being able to track back what ids were assigned to entities you care about. If you didn\u2019t include :db/id in transaction, how do you expect to find it in tempids? Under which key?. Ok, for parity with Datomic we can implement the same behaviour. This is new behaviour\u2014when DS was written, Datomic didn\u2019t allow skipping db/ids at all. are you up for building it?. Yeah, thanks!\nOn Tue, Jan 23, 2018 at 10:57 PM Richard Newman notifications@github.com\nwrote:\n\nThere's a chance, but no commitment from me! Figured it was worth filing\nan issue anyway.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/issues/247#issuecomment-359911018,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AARabI_2eNbJRnprlCZZFqWikujBO3Nbks5tNjmZgaJpZM4RqLxw\n.\n. Objects are not serialized, but they are compared using CLJS compare https://github.com/tonsky/datascript/blob/eaa83844efda2e8d83e80521dd06a15a154c710b/src/datascript/db.cljc#L290. That means if you ever look up by value, you can only look up by primitives: strings, numbers, cljs keywords. If it works for some objects it\u2019s most probably just by accident. Here\u2019s the code:\n\nhttps://github.com/clojure/clojurescript/blob/9ddd356d344aa1ebf9bd9443dd36a1911c92d32f/src/main/cljs/cljs/core.cljs#L2345-L2369\nI guess maybe first case falls under (identical? (type x) (type y))? Not sure. > That must mean datascript must be doing a shallow or deep copy of the normal object that is being inserted.\nDataScript certainly does not do that. Check your tests. I\u2019m sorry, you\u2019re right. DS does tries to convert entities to CLJS values and back\nhttps://github.com/tonsky/datascript/blob/eaa83844efda2e8d83e80521dd06a15a154c710b/src/datascript/js.cljs#L36. usually it can store incomparable values. You can\u2019t store them cardinality-many attributes, you can\u2019t make them indexed or unique. Otherwise it should be fine.. cool, thanks. thanks for reporting!. great! I\u2019ll look into it. this happens because ?x is a free variable at this point. It has no values. Change the order of statements, it\u2019ll bind ?x to entity ids and after that you\u2019ll be able to call your fn, because DataScript would know where to take ?x values from.\n'[:find ?y\n     :where\n     [?x _ _]\n     [(clojure.core/identity ?x) ?y]]. Yeah, would make a great addition!\nOn Fri, Feb 16, 2018 at 4:17 AM Kenny Williams notifications@github.com\nwrote:\n\nDatomic now supports strings as tempids in transactions (see here\nhttps://docs.datomic.com/on-prem/transactions.html#creating-temp-id).\nWould you accept a PR adding string tempid support to DataScript?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/issues/251, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AARabGKmAQ0QI1IFNmBPM8iGjr4M7SCYks5tVNcegaJpZM4SHwcn\n.\n. Thanks! Check out v 0.16.4. I honestly never understood value of seek-datoms. It\u2019s basically \u201citerate from X till the very end of index\u201d, which will include all sort of garbage at some point. Maybe you can give me examples?\n\nThe value of datoms, on the other hand, I see very clearly and use it very often. The normal Clojure (reverse (d/datoms ...)) will return reverse iterator that would lazily walk your sequence, so I don\u2019t think there need to be standalone rdatoms call (this in unlike in Datomic, which will actually first realize return result of datoms and then reverse it in memory, so they have stuff to work on :). (just to clarify: datoms is better than seek-datoms because it ends iteration where you expect it to end, e.g. (datamos db :aevt :attr) will only iterate over datoms with :attr attribute, wheter seek-datoms will finish with :attr and continue to different attributes). I see now, rseek-datoms is not the same as (reverse (seek-datoms)). Ok I see no harm it being added to the API then. Thanks! Check out v 0.16.4. Thanks! Should be fine, check out v 0.16.4. DataScript relies on sorted sets (B+ trees in current impl) with range queries. Do they have anything like that?. Sorted set is not the same as just Set. Set is just yes/no, DataScript needs efficient ranges (all values from X to Y), and it needs custom comparison logic too. Sorry, I don\u2019t understand how could one implement sorted set over a set or a map?. What does child.get('collection/_children') returns? I suspect that if relationship is marked as :db/isComponent then walking it backwards returns single entity, not a collection (for normal references it would be a collection). Yes I have\nhttps://github.com/tonsky/datascript/blob/4df745223ad6c799f6a5887b598a72e4e5c60181/test/js/tests.js#L278-L283. That would be great! Not sure how much could be told through docstrings (esp. query part, takes whole article to explai), but some docstrings are certainly better than nothing!. Added in https://github.com/tonsky/datascript/commit/5cf55d6fb40d8578728229675e566c8e5255d0b2. JFYI I\u2019m working on a solution to this, slightly different from what you\u2019ve proposed. Take a look at 0.16.5. thanks! Good catch. Replace (atom with (datascript.core/conn-from-db. Thanks! I\u2019ll take a look\nOn Wed, May 16, 2018 at 2:04 PM, Bahul Neel Upadhyaya \nnotifications@github.com wrote:\n\nHi,\nI get some odd behaviour when using the vector function within a query.\nTake the following query:\n(datascript.core/q\n  '{:find [?product-type ?v]\n    :where\n          ([?id-10 :product-type ?product-type]\n           [(vector ?product-type) ?v])}\n(-> (datascript.core/empty-db)\n      (datascript.core/with [{:product-type \"A\"}\n                             {:product-type \"B\"}])\n      :db-after))\n=> #{[\"A\" [\"B\"]] [\"B\" [\"B\"]]}\nI initially thought this was due to function calls, however, when I\nprovide a different implementation of vector then the problem goes away:\n(datascript.core/q\n  '{:find [?product-type ?v]\n    :in [$ ?vector]\n    :where\n          ([?id-10 :product-type ?product-type]\n           [(?vector ?product-type) ?v])}\n(-> (datascript.core/empty-db)\n      (datascript.core/with [{:product-type \"A\"}\n                             {:product-type \"B\"}])\n      :db-after)\n  (fn [& args] (vec args)))\n=> #{[\"A\" [\"A\"]] [\"B\" [\"B\"]]}\nNOTE: This does not happen with the clj version, only with cljs (on node).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/issues/262, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AARabCfHNFqXK6QV_FhLNSBWCAQrDwPFks5tzAetgaJpZM4UBGVV\n.\n. Wow, lots of effort here! I don\u2019t think I see how something like this could be merged into DataScript database. My concerns are existing clients, breaking changes, compatibility and JS portability. But I\u2019m happy to see it exists as a separate project on its own.\n\nCouple of questions\u00a0though, purely out of curiosity:\n\nWhy double? Wouldn\u2019t simply using long give you more bits to work with?\n\n\nThis patch should reduce memory usage by at least 50%.\n\n\nHave you actually measured it delivers on that goal?\n\n\nI could not figure out how to benchmark Datascript.\n\nAdd to profiles.clj:\n:bench  { :source-paths [\"bench/src\"] }\nThen run (in datascript dir):\nlein with-profiles +bench trampoline run -m datascript.bench/bench-all. Functions can\u2019t take tempids. I\u2019ll make sure cas throws in that case. Yes. :db/cas as well. thx! Pushed 0.16.6. Hm, so you\u2019re trying storing JS object as a value. Not gonna work at the moment \u2014 DS does total JS to  CLJ conversion under the hood of JS API. It wouldn\u2019t be that way if it was JS-first but unfortunately it\u2019s CLJS-oriented. Don\u2019t have any ideas how to work around that too. If you store plain values (not objects) all should be fine though. I\u2019m sorry, I\u2019m in no state to do any work right now, but I\u2019ll take a look eventually. Thanks!. Thanks! Merged. Hi, I\u2019ll take a look eventually, maybe not soon. Sorry, and thanks!. Thanks! I\u2019ll take a look later. Why are you depending on q3? OR/NOT?. Ok cool. I\u2019ll try to deliver those either in the main query branch or in q3. Ok I\u2019ll merge it but without bench, simply removing its use from query_v3. try 0.16.7. Thanks! I\u2019ll take a look later. Will do shortly!. Thx! Merged in 0.16.7. This is intentional. init-db is fast but it\u2019s only fast because it has certain limitations. It was not supposed to be part of public api though, it\u2019s used internally for fast serialization/deserialization. I\u2019ll mention that in the docs when I get to write some. Yeah, it definitely should complain if input is not what\u2019s expected. Well this should work. Thanks for reporting, I\u2019ll take a look later. You can always pass your own fn to the query:\n```\n(def db (-> (d/empty-db {:friend {:db/type :db.type/ref\n                                  :db/cardinality :db.cardinality/many}})\n            (d/db-with [{:db/id 1, :name \"1\", :friend [2 3]}\n                        {:db/id 2, :name \"2\", :friend [3]}\n                        {:db/id 3, :name \"3\"}])))\n(d/q '[:find ?e ?f\n       :in $ ?fn\n       :where [?e :name _]\n              [(?fn $ ?e :friend) ?f]]\n      db\n      (fn [db e a]\n        (->> (d/datoms db :eavt e a)\n             (mapv :v)))) ; => #{[2 [3]] [1 [2 3]] [3 []]}\n```\nThere are also missing? and get-else supported in both Datomic and DataScript: https://docs.datomic.com/on-prem/query.html#get-else\nI guess it really depends on your task, but what exactly are you trying to accomplish?. This worked for me\n```\n(require '[datascript.core :as d])\n(def db (-> (d/empty-db)\n          (d/db-with [{:db/id 1 :widget/name \"a\"}\n                      {:db/id 2 :widget/name \"b\"}\n                      {:db/id 3 :widget/name \"c\"}\n                      {:db/id 1 :gadget/name \"x\"}\n                      {:db/id 3 :gadget/name \"y\"}\n                      {:db/id 4 :gadget/name \"z\"}])))\n(d/q '[:find ?e ?w ?g\n       :where [?e _ _]\n              [(get-else $ ?e :widget/name ::no-value) ?w]\n              [(get-else $ ?e :gadget/name ::no-value) ?g]]\n      db)\n```\nUnfortunately get-else doesn\u2019t accept nil as default value, but using any other value instead works.. Ok, thanks for letting me know!. Yeah you can\u2019t do that. You can\u2019t dynamically choose functions/sources. Not difficult but might be pretty costly for situations where there\u2019s only one functions, which is a majority of cases. You usually apply one fn per all datoms, not a fn per datom. In the latter case, just select what you need and apply fn yourself. Or call apply with fn as an argument. I prefer entity to stay read-only interface.. It\u2019s strange that second way works. Are you sure it does what you want? Tempid resolution in DS is top-to-bottom, with no backtracing, so changing this behaviour (adding unification) would need a huge refactoring.. Right! I forgot about this. I\u2019ll take a closer look into the issue soon. thank you!. looking.... I see what you mean. You are correct, of course. But in this particular case that cycle is used (as you can see from a comment) to test chunked iterator performance. So it is already deeply relying on implementation details.. That\u2019s the expected behaviour. If you want those to be automatically removed, you can mark a relation in schema as :db/component true. Thank you!\n. how did you measure it? jvm or js?. I can\u2019t check if you are using integers that don\u2019t exist in JavaScript because you can\u2019t represent them in JavaScript :). Yeah, sorry, I confused 2^53 (upper limit for JS integers) and 2^32 (fast JS integers). I\u2019m not sure where bit operations come from, I\u2019ll have to check.. Thanks, I\u2019ll take a look shortly. Hey @refset, what those two lines are about? Why are they necessary?. ",
    "dangoor": "OK, cool.\n. ",
    "boxed": "woops. Fixed :P\n. ",
    "swannodette": "@tonsky Marker protocols are extremely useful - we now know that the map came from a DataScript database. For example you could imagine in Om being able to call transact! on an IEntityMap instance and because we know it's an IEntityMap and not just a plain map we can trigger a DataScript DB transaction.\n. @tonsky entities from the database simply need to be marked (think about defrecord and the record? predicate), I don't think my idea needs any more justification.\nIf you don't want to add things like this I will go ahead with starting a separate client side Datomic-like effort with the properties that I believe are needed.\n. @tonsky marker protocols are not strange ISequential has no methods and is incredibly useful. Being able to detect entity maps is going to be useful in many cases particularly for library designers. Like I said I don't think this needs any justification beyond what I've already stated.\nCursors are a useful underlying concrete implementation of an abstraction, but they were designed such that if some better underlying concrete implementation came along users could migrate to that implementation with zero headache. Exposed cursors prevents flexibility to move to some other more appropriate underlying implementation - whether that's entity maps, bound queries, etc. etc. So while it may seem like \"magic\" this is actually an important tradeoff I'm not going to give up on yet. It's not by coincidence that internally Om relies very little on the implementation details of cursors.\nYou've done a great job blazing a trail with DataScript, however I have some ideas of my own how the code should actually be implemented and I've ended up starting on my own effort. Thanks much!\n. This can be resolved by https://github.com/tonsky/datascript/pull/4\n. ",
    "robknight": "@tonsky I think the idea here isn't about changing how DataScript treats records that are received in transactions, but about enabling third-party libraries to change their behaviour when interacting with data queried from the DB.\nOm manages application state by wrapping values in 'cursors'.  Right now there are cursor implementations for standard maps and vectors, and calling om/transact! on these just updates the values stored in an atom.  I'm guessing that what @swannodette wants to do here is create an Om cursor implementation for DataScript entities.\nWithout the marker protocol, Om will just treat entities from DataScript as plain maps.  They'll get wrapped in a standard map cursor, and any time om/transact! is called on them, a new value will be stored in the state atom.  However, if Om can detect (via the marker protocol) that it's dealing with a DataScript entity, it can wrap the entity in a different cursor implementation.  This cursor implementation would trigger a DB transaction whenever om/transact! is called on it, enabling Om to use DataScript as a back-end for application state instead of plain ClojureScript maps and atoms.\n. ",
    "montyxcantsin": "Do you have a particular strategy in mind for this?\n. That is a good point. I suppose it would work if the fn had :export meta but I agree that feels awkward. \n. good point about js/window as well. It was a fun hack but I concur this should be closed for now (maybe when we have cljs-in-cljs such things will be more viable). Also datascript already supports passing in fns via :in which is \"good enough\" for most use cases anyway. \n. Ok changed the error message and added the handling for multival attributes. \n. ",
    "yusefnapora": "Have you given this any more thought?  It seems like specifying the value type in the schema has the advantage that you could return referenced entities in the d/entity function, similar to Datomic.  So, in frankiesardo's example, you could do\nClojure\n(def e (d/entity db 1))\n(get-in e [:components 0 :name]) ; => \"foo\"\nOf course, d/entity would have to be rewritten to support that.  But it seems like for that to be possible, you would want the type to be in the schema.\n. ",
    "ul": "+1 to proposal weight ;-)\n. m.b. not key inside data structure, but in metadata?\n. ",
    "frankiesardo": "Thank you for taking the time to state your design ideas so clearly, I think it will make a great start for a datascript wiki page :+1: \nI generally agree with everything you said and probably I should have read your code more carefully and expressed myself better. As you say, transact!, create-conn and the likes are a thin layer on top of a persistent data structure: maybe the usage examples could help a future reader first introducing the purely functional operations on datascript and leaving the atom manipulation at the end of the README as an optional nice-to-have.\nI especially liked your explanation for TxReports. But what if the latest tx-data is included as yet another key inside datascript data structure? That way every new database version would be self-explicative.\n. Well yeah, alongside av, max-eid and the other keys\n. ",
    "narma": "Small tip about whitespace changes, to view diff via github without whitespace changes just add w param with true value to URL, for example: https://github.com/tonsky/datascript/pull/12/files?w=true\n. I think it related to this issue https://github.com/tonsky/datascript/issues/50\n. ",
    "EwenG": "Thanks for your work on this allgress. I made a few updates there https://github.com/EwenG/datascript/tree/analyze-q. Updates include:\n- Merge with the 0.2 version of datascript\n- listen! takes the index keys as argument instead of a query. I think this make the listen! function more flexible.\n- The callbacks are stored in a map from indexkeys to callbacks. This allows the transact! logic to notify the listeners using a few map lookups instead of iterating all listeners.\n- I added a few other things like the defquery macro (syntactic sugar around analyze-q).\nI don't know if this should be merged into datascript. I still think there is something wrong with modifying the listen! API. But I am also concerned with how datascript would scale when a lot of callbacks are registered, especially in the context of a GUI where a lot of events can be generated (think mouse dragging events). I would love to know tonsky opinion on this.\n. Isn't it what allgress \"index keys\" already do? analyze-q is just syntactic sugar to extract those index keys ('[ :user/name ] or '[room-id :room/messages '_]) from a query. \nThis is indeed an approximation since it is \"only\" a static analysis.\n. Yes datascript already generates transaction ids. But we can't use them to annotate transactions since we want the DB to work in constant space as you pointed out.\nWhat I would like is something like this:\n```\n(transact! conn [{:db/id            -1\n                    :attr   \"attr\"}]\n              \"my-tx-metadata\" //Note the additional parameter.\n)\n(listen! conn (fn [{:keys [tx-metadata]} //Note the new key in the transaction report.\n                   //You can use the tx-metadata here even though\n                   //it was not added to the database (since we want \n                   //to be able to work in constant space.)\n]))\n```\n. ",
    "kahunamoore": "This sounds a lot like what the RETE algorithm does - it effectively builds a custom network/indexes that exactly fit the set of queries/rules known to the system and shares common sub expressions for efficiency. Trades memory for runtime.\n. No, we are using the Clara rules engine which gives you these features (and more) by design.\n. ",
    "jeluard": "Are you still considering this feature? Looks extremely useful.\n. ",
    "atroche": "@allgress are you guys still using this for your own apps? how is it working for you?\n. Hi guys,\nI just posted a $USD300 bounty for this issue. Feel free to pitch in with cash or, y'know, claim the bounty =)\n. Thanks @tonsky. It's definitely not the end of the world =)\nOn 11 December 2015 at 21:18, Nikita Prokopov notifications@github.com\nwrote:\n\nWell, it is in a sense that I haven\u2019t thought about someone would use it\nthis way. I\u2019ll think what can be done about that. Meanwhile, you can\nrewrite your query like this:\n(d/q ':find ?e\n       :in $ ?client-squuid\n       :where [?t :thing/squuid ?client-squuid]\n              [?e :appointment/client ?t]\n     #uuid \"5664cb92-8256-49f9-a7ff-30d3d00939c4\")\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/137#issuecomment-163901698.\n\n\n-- Alistair\n. ",
    "sparkofreason": "@atroche We're using Datascript, but not the allgress branch with query-based notifications. We've moved away from the single state atom approach, and have really embraced the immutable aspect of Datascript. Our web components might receive an instance of a Datascript DB via an attribute from a parent, or create it themselves through a service call. Child components often publish relevant changes through events, from which listeners can update their own instance as appropriate. Since we use freactive, sometimes we'll make a lens which represents a specific query or \"view\" on the DB.\nWe've found that this approach allows us to keep the Datascript instances fairly small (for our apps, at least), and haven't had any perf issues which would require query-specific notifications.\n. +1\n. +1. Lookup refs definitely a big improvement when working with data from Datomic, but :db/ident support for ref attributes would be extra tasty.\n. Our main use-case is the ability to share query definitions between Datomic and Datascript. We're pushing the \"Datomic-as-protocol\" idea as far as we can, and it's working out very well, but API differences like this add friction. For instance, we're able to develop disconnected from Datomic using Datascript along with some CLJX. This makes for rapid client development, but then we want to hook up to a server and use the same queries built in the disconnected case.\n. We have various type \"enums\" which we mix and match, mainly as filters for list views. The types are also used as metadata to specify which attributes go with which entity types, for the purpose of auto-building editor forms, etc.\n. For my case, I think it would suffice to handle only the input side of things. That avoids the necessity to pull from Datomic all of the enum entities in addition to the data which references them.\n. I've found away to work around this and keep things consistent on the Datomic/Datascript sides, so I'll retract my request for this feature.\n. I've been able to work around some of the Datomic/Datascript differences with a few small utility functions. See gist: https://gist.github.com/sparkofreason/6b3ffd63d148cd7dc37a\n. Not that painful, really. If it were a choice between using the utility functions or having idents but degraded Datascript perf, I'd take the first option. Personally I think the problem is really with Datomic. The enum abstraction is leaky when used through the pull API, would rather see it fixed there.\n. Thanks for the link. Seems like they should be able to support that, at least in value position. Two main use-cases were to bridge the lack of :db/ident support, and allow sharing of the same queries between Datomic and Datascript. The latter is moot, and the former is really it's own issue, so closing this.\n. Not sure how to avoid the massive whitespace changes caused when wrapping an s-expression. The relevant changes are at lines 998 and 1105.\n. ",
    "metasoarous": "Hadn't seen this thread before... I'll just chime in to mention that this sounds a lot like parts of https://github.com/mpdairy/posh. In fact, there's an analyze-q there as well with output results that look a lot like these, so I'm wondering if @mpdairy actually looked at that in his implementation. To my knowledge he hasn't hooked any of that up for generalized listeners, but there's a single listener that goes through the active subscriptions, checks datoms against the generated patterns, and decides whether or not to update the queries based on this. So, if anyone reading this thread is interested in this line of thinking, you may want to check out Posh.\nThat having been said, I'd like to point out that there's some discussion on #132 in the direction of incremental materialized view maintenance which would go one step further in actually preventing re-computation of queries, instead using previous results and tx-diffs to compute the new query results far more efficiently. \n. Any updates on this? I've been doing something similar to @sparkofreason with entity type enums with references to associated attributes for semi-automated form and view rendering. I'm actually going to be talking about this at Clojure/West, FWIW.\nThe biggest pain in this work (for me personally) has been navigating these differences with respect to idents in Datomic vs DataScript in queries and pulls. I could probably live without modifying query output, for the most part, if that made the problem easier.\n. I've been working on the basis of a solution for this: https://github.com/metasoarous/datsync. I noted Dato before starting this, and was excited. But 1) I wanted to use Reagent (Dato seems bound to Om), and 2) I saw there hadn't been much work on Dato for some months. So I built a very simple basis on which I hope we can build. Right now, out of the box (explored?) functionality is limited to entire-db replication, with Datomic serving the role of \"central/canonical\" source of truth (all transactions go through it). Soon, we'll start working on problems like windowing/scoping (al a @tonsky's Web After Tomorrow) and optimistic updates as well. I've got some ideas and sketches along these lines, but haven't needed it in my work-work yet. I'll be talking about all this at Clojure/West if anyone interested is attending.\n. @j-pb Hear, hear! I concur; Incremental view maintenance is the right direction. At least, that's the opinion I've formed after researching this question these last few months.\nI'd argue Posh does about as good a job as can be expected using DataScript as-is to build materialize views that don't update any more frequently than necessary. At the time of your writing, I think you had to specify the patterns manually, but now they're computed directly from the queries. However, they don't always do the right thing in various edge cases. And at the end of the day, having to recompute the queries remains a huge waste.\nIt's not perfectly clear to me from the Shallow dive into DataScript internals post whether DataScript does fully-naive or semi-naive query evaluation (my understanding is that most of the difference is in how rules are processed, and there isn't a lot of detail about that in the post). In short though, the semi-naive approach ends up being more efficient than fully-naive by reducing log scans, and uses the ideas behind incremental view maintenance to do this. So as hinted by @j-pb, if DataScript is already using this methodology, we may already be part of the way there, and if not, going in this direction may speed up one off query execution while also paving the way for reactive, incrementally maintained, materialized query views.\n@tonsky What do you think about this idea? If you're generally amenable to considering it, I could open up a new issue to discuss particulars of this approach and make a rough proposal along the lines of what it might look like, at least from the API side.\n. @carocad This sounds basically like posh, without the machinery for deciding whether to rerun queries.. > I've been starting to wonder if something like this would be good enough.\nI'll beg to differ. For small apps, this may be fine. But I don't know why you'd want to hem yourself in this way. Too many queries or too much data and you're going to wish that you had Posh's tx-pattern filters.\n\nit just so happen that the machinery is backed inside reagent.\n\nI think you're missing something; Yes Reagent has functionality to track which queries have updated. However, what it doesn't have that Posh has (and how could it without assuming you would be working with DataScript), is the ability to look at the datoms created in a transaction and compare with patterns in your queries to figure out which queries have a snowball's chance in hell of needing to be updated, and only update those queries. It's not perfect, and misses some edge cases like recursive rules, but it's better than nothing in my opinion.\nThe real question is why you'd want to implement you're own over use Posh; If for fun, then great! But if you're hoping folks are actually going to use it, it's going to have to have a feature that makes it worth choosing over Posh.. Hmm... ok. Sorry for the noise then. It's possible this was actually a posh bug (I'll chat with @mpdairy about this). I've already rewritten the code, but if I revisit and reproduce the error, I'll try to narrow things down and reopen if I find an edge case in DataScript.\n. Yeah; that's exactly the kind of thing I think this would make simpler to implement. @mpdairy has already worked up quite a bit of really great functionality in https://github.com/mpdairy/posh for connecting DataScript to Reagent. So that's not the problem really... It's just that in thinking about additional features for posh (and other libs we're working on, like https://github.com/metasoarous/datsync), we've realized having a more extensible notion of what it means to be a connection would be helpful for implementing certain features.\n. Just realized I never responded back on this thread...\nThe one thing that's not dealt with seamlessly by simply extending/implementing IDeref is the transaction listener functionaliy. Since libraries like posh depend on listeners for doing their thing, it would be nice if a Conn (or whatever) protocol reflected some abstraction around this listener functionality. I'm not sure what that would be yet, and this also isn't super high priority for us right now, so no pressure on our end. For now just wanting to push the conversation forward in case others decide to take up the charge.\n. > How will it be different from DataScript's Conn?\nThat's what setting things up as a protocol leaves up for grabs. It could be different in whatever ways someone might want it to be different. Someone could certainly do that right now (and I did tinker with it at one point), but it requires \"copying over\" the transaction listener implementation. Which is maybe fine, and there might not be a good/clear way around it (that's worth the trouble anyway).\n. Umm... yes, agreed. I wasn't thinking clearly...\nThe benefit would be that datascript.core/transact could be based on a protocol method, making it possible to swap different implementations of the Conn without having to resort to using different transact API/function. My ill thought through point about listeners was supposed to be: Just implimenting IDeref isn't enough, because the handling of the listeners isn't orthogonal to swap! in the current implementation, and this forces API consumers to use a separate transact function (not just implementation) if they want to use a different conn implementation. This can potentially complicate things for libraries that would which to abstract over such differences.\nYou are (of course) correct that the implementations don't carry through if you're using protocols. However, it's also possible that by pulling apart DataScript's implementation properly, you could have some reusable implementations of the transaction report handling side of things. That's roughly the direction I was thinking in, but not thinking through (or at least explaining) clearly.\nAgain, this issue isn't pressing at the moment. So I'd say until there's the need for this indirection around transactions and conn implementation, we table discussion, since the particulars of what might be needed in this indirection are really dependent on what those needs are as they arise.\n. A bit of context: In working with posh, lookup refs can be rather problematic, erroring out and not recovering when a posh query uses a lookup ref for data that isn't loaded yet. This can be worked around in posh, to some extent, by watching for this sort of situation and trying to handle gracefully, but it would be a lot easier if DS copied Datomic's behavior here, since client code can usually deal with {:db/id nil} gracefully a lot more easily. And also, I'd rather posh and DS be on the same page in how they handle edge cases like this (where possible).\nIn any case, I'm happy to try and help with this if it's something you're keen on. I would just need a little guidance about where it would be best to introduce a fix.\nI don't recall at the moment whether this is a problem with q as well, but I can chime back here as I work around this.\n. Realized the Datomic behavior is slightly more nuanced than I realized.\nWhen a lookup ref (or eid) can't be resolved, it return nil, unless the pull expression contains :db/id, in which case it returns {:db/id nil}.\n. I believe ... works. Supporting recursion depth would be nice though.\n. (side note: * can be a symbol; e.g.: '[* {:person/friend 2}])\n. I think this is in line with Datomic's behavior (http://blog.datomic.com/2014/02/datomic-lookup-refs.html).\n. I stand corrected; Good to know.\n. Fair enough. In my case (datsync), the only schema changes that come through are ones that had already passed through Datomic, so they'd more or less been \"vetted\". But you're right that there shouldn't be surprises in something that's a core part of datascript, and it would be easy for someone not thinking about the consequences to goof. Here are my thoughts on these issues:\n- ref vs other types: Datomic actually doesn't let you change :db/valueType. So we could scan the datoms and make sure no attributes are being set as reference attributes when they've already been used for other data (and vice versa).\n- uniqueness: Changing from unique to non-unique is pretty straight forward with Datomic, but the other direction is only allowed in certain cases (values are already unique and have an index, or there are no values). We could mimick that or simply disallow depending on how much work we're willing to put into it.\n- cardinality: cardinality one to many is straight forward, but many to one requires deciding which value to keep. I think this may actually be straightforward; Under the new schema I think passing in the old datoms would deduplicate all but one of the values. If I'm wrong about that we could either disallow once there are multiple values, or filter all but the most recently asserted value.\nHere are the relevant Datomic docs for reference: http://docs.datomic.com/schema.html#Schema-Alteration.\nI get that since it's possible for users to deal with these issues themselves when they need it, this isn't a high priority in your book. However, I think for new users especially, it may not be obvious how might use init-db and swap! to do this themselves, let alone all the issues mentioned above. And I think this puts a damper on iterative/interactive development. With that in mind, if someone came up with an implementation that dealt with all these issues sufficiently, would you be open to including it in DataScript?\n. Agreed; Sounds good.\n. FWIW @tonsky, this was the original inspiration behind #150. At the time, Datsync and Posh were both needing to override various behavior of vanilla DataScript, and thought protocols might simplify things. Upon further reflection though, much of the behavior we wanted to override could be expressed via transaction middleware. I think if we get the shape of the tx-middleware idea right, it will be a much more composable way of getting much of the desired flexibility.\n@bamarco Would you please describe the signature of the tx-middleware function to clarify how it handles database, tx-data and tx-report?\n@tonsky Please let us know if you have any questions or concerns.\n. ",
    "whilo": "Yes, it works for us. I would phrase the routine load-db and not read-db, as I associate string reading with read, otherwise it looks like the proper serialisation, better than my quick hack. Thank you!\nNext we were interested in accessing datascript on the JVM, too. We would like to port it with the help of cljx, what is your take on that?\n. Just to note, I am interested in this as well and have had a look into it several times in the last years. While I use DataScript and would love to see a durable index, my primary focus is on building a distributed data management system with replikativ which can also be used for Dat* replication. As I have focused on similar problems to have cljs compatible code, I have decided for core.async and defined IO protocols, e.g. for storage with konserve on top of supervised async. \nThere was quite a bit of boring yak-shaving involved, but I would suggest to break the problems down and build a set of robust cross-platform abstractions to build things like a durable DataScript. In general cross-platform cljs does not have a lot of composable building blocks yet and the asynchronous nature of JavaScript sadly requires non-blocking interfaces on the JVM as well, which barely any Clojure library considers. While promises and other async solutions have in part the benefit of not transforming all your code with core.async, the facilities provided by having very concise async code should be seriously considered. core.async is also fairly solid now. For concrete implementations of limited asynchronous functionality, callbacks are generally prefered for libraries, because they do not push core.async on the user. But once we talk about exposed protocols and interfaces, I would argue that concise blocking semantics allow better composition with less glue code, this is why core.async is considered to be the async library for the Clojure ecosystem. For example error-handling is not easily handled well in an async setting in cljs; or to model rendevouz points (where the sender only is unblocked once the reader consumes the value) between different async libraries is also non-trivial.\nIn the direction of building blocks I am at the moment primarily interested in having a persistent durable index data-structure (only a first experiment) which is implemented against such portable protocols (e.g. konserve). This would help me to have nearly optimal delta compression for CRDT metadata in replikativ and in general would allow to build fairly sophisticated snapshottable IO infrastructure including an async query engine on Datom indices for DataScript. In particular I am having a look at the hitchiker tree atm. to port it on konserve (instead of redis). Do you think some common infrastructure undertakings like this are reasonable or do you see obstacles?\n. @theronic how do you envision write coordination? like in datomic with a single transactor?. ",
    "pangloss": "Hi @tonsky, thanks for having a look at this! I wanted to mention that I'm very impressed by the overall quality of your work on this library!\nI probably should not have bundled all of these separate changes together as one pull, but I just extracted this from my own project and didn't want to spend a lot of time on it. If you are not interested in the history and schema stuff, I can easily pull that out. That said, on to the meat of the thing.\nIf you look closer at how I've implemented Entity, I think your valid concerns will be satisfied. I absolutely agree that an ORM is not a good idea. Fortunately that is not at all what I've done. I've created immutable Entity objects that do not change or execute transactions against their underlying database. They have the same behaviour as PersistentHashMap with only 2 exceptions: \n- If the db schema has an attribute marked as a rel in the schema, you can only assoc an entity to that attribute (or a seq of entities for multiref res)\n- After you have used assoc/dissoc to create an 'altered' Entity (the original unaltered one still exists because each entity is immutable), you can optionally pass it to the transact fn, which will extract a minimal list of changes that have been applied to the entity and then execute those changes against the database. Before you pass the entity to transact, the db is unaffected.\nThe result is that Entity can act as a useful transaction builder, saving you from having to manually create the command vectors to apply to the database, while also saving you from having to separately keep track of multiple changes or falling back to applying work in progress changes to the database before you are really ready.\nHere's a couple of simple examples of using Entity:\n``` clojure\n(let [user (new-entity @conn)\n      user (assoc user :name \"Eugene\" :password \"password\")\n      group-id (ffirst (d/q '[:find ?e\n                              :where [?e :group-name \"Power Users\"]]\n                            @conn))\n      group (entity @conn group-id)\n      user (assoc user :groups [group])\n      user (update-in [user :groups 0 :user-count] inc)\n  ; At this point, the database is unmodified (entity does not even hold a\n  ; reference to the conn atom)\n  report (transact! conn [user])\n\n  ; Now, we can resolve user entity's tempid against the report.\n  user (entity report user)]\n\n; the original group entity was unmodified. This is the same behavior you would get\n  ; with a structure of hash-maps:\n  (assert (not= (:user-count group) (get-in user [:groups 0 :user-count])))\n; As expected, the modified group is not equal to the unmodified one. This\n  ; and the previous example are the same before or after the call to transact.\n  (assert (not= group (get-in user [:groups 0])))\n; But if we get a new version of the group from the db, it will have the update.\n  (assert (= (:user-count (entity @conn group)) (get-in user [:groups 0 :user-count]))))\n```\nLet's discuss this and then I'll go into detail about the other stuff, just to simplify the conversation.\nRegards,\nDarrick\n. I agree that the transaction format is simple in Datomic. In practice you still to have to generate those transactions somehow in your app and from my experience that generation code can become complex. All these entities do is help you to generate them. No magic is involved. If you want to see what commands will be produced you can get those directly (that's all I have the transact function do internally).\nIn fact the list of changes are built explicitly at the time that -assoc is called (see -with-command). The design eliminates the potential problem that you mention of ambiguity. If you make a change to an attribute or relationship, then when that changed entity is applied to any version of the db, the result will be that for each attribute that you changed, the current value in the db will be updated. The same thing goes for multirels. If a multirel is changed, it prepends :db.fn/removeAttribute before adding each element in the multirel to the entity. That has the benefit of pushing resolution down to the moment the transaction is executed and eliminates any possible ambiguity. (This isn't too efficient right now in some cases, but is easy to optimize based on the work I've already done elsewhere.)\nI think this design fits with the \"Simple Made Easy\" ideal as well as with Clojure's data structures and enables you to work with your data using all of Clojure's powerful data manipulation tools.  It also does not preclude using datascript's current toolset as demonstrated by the fact that this pull request makes no changes to the core at all.\n. Ok, I'll make a new pull with just Entity sans a couple protocols.\nThe only other thing from this pull that I think is still worth discussing is the idea of keeping DB history in the DB object. You said that you don't want to waste memory, and I agree. However, I don't think there is any significant overhead in keeping history. The only cost is the vector that contains the history datoms, because the datoms themselves are already being stored in the BTSets and are merely shared with the history vector. Another consideration may be if there are many versions of the DB hanging around, but even then the cost should not be onerous as you should get good structural sharing out of the PersistentVector that is storing the history. If saving memory is the only consideration, I don't believe the memory savings are worth the loss of the very useful capabilities enabled by preserving a DB's history. Thoughts?\n. I forgot to add that while it is relatively easy to keep track of the history yourself as you suggest, that has disadvantages. For instance it's not really possible when using the with feature. Also, if you do add history in the simple way I did here, by assoc'ing it to the db after the fact, you lose the ability to compare the :tx_after db with the resulting db from the connection atom because the tx_after version does not have the updated history attached to it.\nThe only clean way to do it \u2013 and enable people to create libraries that make use of history, for instance \u2013 is to build the history during the transaction.\n. Thanks very much for reviewing the code. I've responded to your comments.\nI unfortunately have to switch to a more pressing project starting today so may not be able to do anything further here for a couple of weeks or more, I'll re-engage afterwards if you like, or at least be interested in what you make of this in the mean time.\n. Hi David, what you are describing can easily be done without any change to\ndatascript using just a simple macro. Here is a quick example I threw\ntogether just to get you started.\n(def global-conn (d/create-conn {}))\n(defmacro property-setter [validate-fn k]\n  `(let [k# ~k]\n(defn ~(symbol (str \"set-\" (name k))) [entity# new-value#]\n   (when-not (~validate-fn new-value#)\n     (throw (ex-info \"invalid\" {:property k# :value new-value#})))\n   (d/transact global-conn [[(:db/id entity#) k# new-value#]]))))\n(property-setter keyword? :my-keyword)\nOn Mon, Apr 4, 2016 at 7:44 AM, David Meister notifications@github.com\nwrote:\n\n@tonsky https://github.com/tonsky well no, not complications, you just\nsaid in #107 https://github.com/tonsky/datascript/pull/107 that you're\nlooking for a use case.\nIf not schemas directly ported from datomic, some kind of validation at\nthe db level would be great!\nMy use case is that:\n- I want to write some nicely named utility functions set-foo!,\n  set-bar!, etc...\n- Those functions do some transactions under the hood\n- The functions are all working with the same type of data and the\n  same bits of datoms in the db\n- I don't really want to have to sprinkle the same validation calls\n  through all my utility functions, I'd rather just have the db tell me when\n  I screw up\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/152#issuecomment-205260589\n. \n",
    "lynaghk": "I dug into this a bit more, and the problem is actually just that I used a set instead of a vector.\nMore specifically, the datascript.core/explode function checks with sequential? (https://github.com/tonsky/datascript/blob/5ad4116fc5bd8254ed2598c0711ff90b745fac82/src/datascript/core.cljs#L154).\nI'll look into fixing this so that cardinality many attributes can accept sets, vectors, and lists.\n. Converted this issue to a pull request.\nThis commit fixes the issue and updates the existing test to check for proper behavior with vectors, sets, and lists.\nPlease let me know if you'd like to see any changes in this commit.\n. Actually, there is an issue with the map version as well: It seems that if the tempid is resolved via the db.unique attribute first, then additional fields can be asserted:\nclojure\n(let [tid (ds/tempid :db.part/user)]\n    (ds/transact! conn [{:db/id tid :person/name \"foo\"}\n                        {:db/id tid :extra \"value\"}]))\nworks fine, but if you try to assert other fields before the db.unique attribute, the transaction will fail:\n``` clojure\n(let [tid (ds/tempid :db.part/user)]\n    (ds/transact! conn [{:db/id tid :extra \"value\"}\n                        {:db/id tid :person/name \"foo\"}]))\n;; 1. Unhandled clojure.lang.ExceptionInfo\n  ;;  Cannot resolve upsert for {:db/id 2, :person/name \"foo\"}: {:db/id\n  ;;  2, :person/name \"foo\"} conflicts with existing #datascript/Datom [1\n  ;;  :person/name \"foo\" 536870913 true]\n```\n. @dwwoelfel Ah, thanks for pointing out the related issue!\nI'm not trying to sync datomic and datascript --- just persist a subset of datascript datoms to disk and then load them back up again.\nBased on @tonsky's comments in #76 it seems like there's a deliberate difference between datascript and datomic in this case, so I'll just need to add a special case to save/load entities with db.unique attributes.\nClosing this issue as a duplicate of #76.\n. ",
    "boxxxie": "Use core.async? \nHow hard would it be to add an async API?\nMaybe a use-channels binding?\n. this helped me a lot.\nmake a PR for the readme.\n. ",
    "dahjelle": "I'd agree with @tonsky that the \"db as a value\" idea would be sad to lose\u2014I've spent quite a bit of time connecting an async data source like IndexedDB to work with React, and it basically involves duplicating all of the data so that React sees it. \nThat said, it would be really nice to have a Datalog implementation with the option to hook in external, async indexes (like from IndexedDB or, in my case, PouchDB). I'm giving that a shot\u2014though, since I'm quite the n00b with ClojureScript and Datalog versus JS, I'm just throwing in callbacks. I'll report back with a link to the fork if I get anywhere useful. ;-)\n(P.S. If anyone has good core.async examples, I'd love to see them\u2026especially on how to, say, implement in core.async a reduce function whose reducer function is async. As I said, I'm a n00b. ;-) )\n. Well, as I said, I'm quite the n00b with ClojureScript, but here's my fork of Datascript that uses external indexes. Feel free to critique or contribute!\n. Rebased on top of latest changes and fixed the whitespace to clean up the diff a bit.\n. Yeah, I'm quite new to ClojureScript, so wide open to suggestions of better ways of doing things. :-D I'm happy to update this pull request (if others would find it useful) to whatever the best way of doing it is.\nLet me see if I understand your comment correctly:\n1. In the :dev profile in project.clj, change the source paths to\n:source-paths [\"bench/src\" \"test\" \"dev\"]\n1. In the dev directory, put the repl.clj.\nWould the :aliases section stay the same, except for the path? Or were you saying that is the bit that could be moved into user.clj? Is user.clj a file leiningen treats specially?\nSorry for all the questions\u2014thanks again for any suggestions!\n. Awesome! Thanks!\n. ",
    "AdrianoFerrari": "+1 to that. It would go a long way to helping us newcomers.\n. ",
    "piranha": "You might be interested at looking what I'm doing here: https://github.com/piranha/showkr/tree/ds\nI think it's not exactly idiomatic or the best code, but I really hope it's not very bad. Plus I plan to clean everything up. :) I need to figure out the best way to convert data from flickr's api to my datascript though, I'm not entirely happy with what I've got now.\n. I couldn't find a protocol for Clojure's side, so I left instance? in place.\n. Ok, also I'm not sure why tests failed - looking at that.\n. this? :)\n. ",
    "mike-thompson-day8": "Here's a Gist which shows how to combine Reagent with datascript. \nhttps://gist.github.com/allgress/11348685\nReagent is a ClojureScript library that wraps React. \nhttp://holmsand.github.io/reagent/\nBTW, you'll find it hard to use OM (an alternative to reagent which is popular) with datascript because OM is prescriptive about the structure of the data it works with ... it has to be hierarchical (think nested maps) which isn't how datascript is organised.\n. ",
    "fasiha": "Sorry to comment on this old issue. Is there a recommended way to persist on JVM? The most straightforward way I can think of is \"periodically spit db to /tmp/random-file, then move /tmp/random-file to mydb.edn, overwriting the latter\". If there was a way to avoid spitting the entire db when only a single fact changed, that'd be a nice optimization\u2014i.e., overwrite only the portions of the file that changed.\n. Aha! I was wondering why this didn't work:\nclj\n(d/q '[:find ?film-name (distinct ?actor)\n       :in $ ?a1 ?a2\n       :where\n       [?film :film/name ?film-name]\n       [?film :film/cast [:actor/name ?a1]]\n       [?film :film/cast [:actor/name ?a2]]\n       [?film :film/cast ?actor]\n       ]\n     @conn\n     \"Nancy Allen\"\n     \"Peter Weller\")\n; => ClassCastException clojure.lang.Symbol cannot be cast to java.lang.String  java.lang.String.compareTo (String.java:111)\nwhile this did:\nclj\n(d/q '[:find ?film-name (distinct ?actor)\n       :where\n       [?film :film/name ?film-name]\n       [?film :film/cast [:actor/name \"Nancy Allen\"]]\n       [?film :film/cast [:actor/name \"Peter Weller\"]]\n       [?film :film/cast ?actor]]\n     @conn)\n; => ([\"RoboCop\" [46 47 45]])\nand this is why :D luckily, I'm making my query map programmatically and can work around this that way. Thought I was going crazy because the query map worked, strings as literal arguments worked, just strings as input arguments didn't.\n. ",
    "theronic": "Assuming sparse per-user data, I think a consistent (even if non-performant) DataScript persistence story for user-data + small global facts stored in H2, Firebase or Postgres (on Heroku) would go a long way to encouraging DataScript adoption for its intended use. I've considered making a Firebase adapter keyed on user id, e.g. /users/tonsky/eavt, /users/tonsky/avet etc. where each client maintains their own indexes, just so I don't have to do the whole Datomic thing. Sure, if you're doing event streams/CQRS, could just store and re-run these, but making DataScript seamless would be super tasty :)\nThat said, I may not understand the underlying storage principles well.. @aaronc I have a similar use-case, primarily wanting to use DataScript as a two-way cache with less impedance mismatch to Datomic so I can query a set of relatively small user-specific data in the browser. Queries spanning many users' data can happen on the server.\nCould you show an example of how you are bringing together all these elements, or point me in the right direction? Any pitfalls?. @whilo just shouting some encouragement - would love to see a strong story for distributed state handling based in DataScript :). Would it be possible to get a spec warning when passing in compile-time known {...}? (I wasn't using instrumentation at the time). ",
    "ghost": "It looks like the following will do it:\n(w/prewalk (fn [f] (if (instance? datascript.impl.entity.Entity (into {} f)) (d/entity db id))\n. > your snippet suffer from that issue\n\nUnderstood.  In my case, I know I'm pulling a tree of data so I don't have to worry about cycles.\n. Thank you!. \n",
    "robert-stuttaford": "Thanks @tonsky! \nI'm using version 0.4.0. The externs are definitely in place.\nAll the source code is here:\nhttps://github.com/robert-stuttaford/stuttaford.me/blob/master/src/stuttaford/radiant.cljs\nhttps://github.com/robert-stuttaford/stuttaford.me/blob/master/src/stuttaford/radiant/\nThe datalog stuff is here:\nhttps://github.com/robert-stuttaford/stuttaford.me/blob/master/src/stuttaford/radiant/datalog.cljs\nAnd the database is created here:\nhttps://github.com/robert-stuttaford/stuttaford.me/blob/master/src/stuttaford/radiant/model.cljs#L20-L25\nThe source data is in the source of the page at:\nhttp://www.stuttaford.me/radiant-advanced/\nIt's the really big EDN blob.\nThanks for taking the time to look into it!\n. Anticipating that, I had already printed the output of several things directly on the page at:\nhttp://www.stuttaford.me/radiant-advanced/\nIn order, it's the query as successfully parsed by Datascript, the entire database, and finally the result of that query on that database.\nHere's the source:\nhttps://github.com/robert-stuttaford/stuttaford.me/blob/master/src/stuttaford/radiant/datalog.cljs#L57-L59\nYou can see that the symbols are identical; for example, here's the parsed query:\nclojure\n{:find [?e ?a ?v], :in [$], :where [[?e ?a ?v]]}\n. I'm using shadow (https://github.com/thheller/shadow-build) instead of cljsbuild which supports the underlying Google Closure module capability - that is, multiple interoperating js files. core.js and radiant.js are compiled at the same time with the same :advanced name alterations. \nIf this were not so, a lot more than just the Datalog query would be broken, as all the 3rd-party libraries are in core.js. Om, Om-tools, Om-bootstrap, Datascript etc. And aside from the Datalog thing, they all work fine.\nEven so, I'll build it as a single JS and see if that helps.\nThanks for the tip about Google Analytics, I'll include externs for that as well.\n. Wow @tonsky, great detective work! Thanks for digging into this so deeply, and for fixing it. It is much appreciated. I never knew about :ups-externs before; you're right, it is very odd. @thheller, any thoughts?\n. ",
    "thheller": "Hey, never heard of :ups-externs either. Putting \"datascript/externs.js\" into the normal :externs option doesn't work?\nSeems like a 3rd party library is supposed to ship a deps.cljs to inform the cljs.compiler about its externs, but looking at the warning it either never reached a stable point or was abandoned because nobody used it.\nhttps://github.com/clojure/clojurescript/blob/9fd6bf5bd55421c3d5becacc5230ed661d6fb3c3/src/clj/cljs/closure.clj#L838\n. It seems like a useful option to have though. Otherwise everyone that uses a library like datascript must remember to also provide the \"datascript/externs.js\" in the build.\nMaybe it would be prudent to discuss this issue in the CLJS Jira or Google Group, seems like a quality of life change that should have a default solution not one that every build tool/library handles differently.\n. ",
    "afhammad": "Thanks for the informative answer @tonsky.\n. ",
    "mdhaney": "Seconded!  I was going to suggest this myself.  \nEven though the history isn't stored, it would be very useful to annotate transactions so that listeners can monitor the transaction stream for certain transactions and respond accordingly.\n. Yes, being able to reference the transaction id inside a transaction is all that's needed.  Something simple like {:db/id :tx} would work, and would be similar to how Datomic does it.\n. My use case is that I create an \"app\" entity for various housekeeping stuff, and it's inconvenient to have to store the entity id to refer to after creating this entity.  I've been cheating by using 0 for the eid, but that felt kludgy to me.\nIdents would work here, but lookup refs should work just as well, I.e. I can add something like :entity.type :app and use that as a lookup ref to easily refer to it.\nSo yeah, if there are performance implications, I would rather not have you pursue it.\n. ",
    "krisajenkins": "Thanks for the reply! It's a shame, but I if that's what Datomic does, it makes sense to follow suit.\nSo how about something like d/touch that expands recursively?\n\"Circular references,\" you say. \"BOOM!\" you say.\nHmm...okay then. How about something like d/touch that expands recursively, but leaves circular references unexpanded?\n1. You'd avoid the BOOM!\n2. Such an expanded entity would play nicely with Om's idea of change. If any attribute changes, the expanded version changes.\n3. It should be easy to code. Just walk the tree, keeping track of which references you've touched. Don't touch any reference you've already seen.\n4. Would it perform? Well, to steal a quote, if you've got a small entity it's fast. If you've got a large entity, well, at least it's faster re-rendering every time. ;-)\nI'm happy to try a PR if you think this is a good approach.\n. ",
    "pasviegas": "This would be pretty nice :D\n. ",
    "asmala": "@tonsky, that's what I thought when I saw DataScript for the first time... :smile: It's an awesome piece of work! Am definitely keen to help out when my day job quiets down a bit.\n. ",
    "dthume": "FWIW, I took a stab at implementing pull and pull-many here. I haven't raised a pull request (although I can if you're interested), because there are still a few bits that need resolved:\n- ~~Although the JS API \"works\", it doesn't correctly handle :db/id yet (see the test).~~\n- It isn't yet integrated to the find specifications recently added to query.\n- (Probably most importantly) I haven't validated that it completely matches the behaviour of datomic.\nThe patch also feels a tad large, although a loop/recur based approach was the only way I could see to avoid blowing the stack on large pulls.\n. I'm more than happy to make the changes you've mentioned, although I'll be travelling quite a bit for the next week so they may be slow in coming. I've done most of the small ones, and I'll raise a PR containing everything so far as you suggested, or in case you want to go on ahead with it before I get to it.\n. I've added the conversion from raw parsed records into a canonical representation as you suggested - good call! the resulting code is smaller now. I've also shaved down the pull loop as much as I was able to.\n. Have you tried cleaning out any build artifacts with, e.g, lein clean? I've run a clean build here and d/pull is definitely found.\n. No problem; I'd actually noticed the difference from datomic when working through #58, and was planning to raise a separate issue about it.\n. I've pushed to the issue-62 branch, if you'd like to review the changes. It seems to bring datascript in line with datomic, and fix the infinite loop.\n. Sure!\n. Fix for the given test case pushed to: issue-80; I can't see any outstanding problems with it so, assuming you agree, I think it's safe to merge.\n. Gah apologies, didn't mean to jump the gun and close it :-)\n. ",
    "scttnlsn": "@tonsky Okay, thanks.  I'll have to read the Datomic docs about this.  I don't get any error when there's only a single attribute (like no. 2 above).\n. ",
    "potetm": "Regarding the datomic behavior, that's not how I read those docs at all, but I've verified that what you say is true.\nI agree with your assessment.\nI do, however, think it would be valuable to go ahead and follow their API and run the check on the db id, even though it doesn't fix my use case.\n. ",
    "jdkealy": "AMAZING! \n. ",
    "cigitia": "It\u2019ll be wonderful when this gets added. When it does get implemented, it might be best to implement it as a transaction function that can be used in transact!, like :db/add and :db.fn/cas, instead of (or in addition to) a function like the above code, which acts directly on connections.\n. I'd like to affirm that I find idents useful in both use cases mentioned by others above:\n- I use idents to easily use singleton objects, such as an \u201capp\u201d entity that contains things like whether a certain UI mode is on or off. If one needed to store the text currently entered into a box an application\u2019s UI, there is then no other place to store it other than some single entity\u2014and that single entity will need to be modified in transactions in the future.\n- I also use idents to refer easily to enumerated objects, such as states or categories of other entities, in code that needs to refer to specific categories. Those categories might also have attributes of their own, such as end-user display names, but each one changes the application's logic in a different way, so the code need to be able to directly refer to them, especially in transactions.\nWithout idents, as far as I know, in order to modify any of those singleton and singleton entities in transactions, their IDs must be stored beforehand separately from the database, either manually hardcoded into code using fragile magic numbers\u2014or manually retrieved sometime after the database is initialized with their them. Those IDs must then be manually passed into every transaction that involves them. Storing all of these ID constants and using them in transactions aren\u2019t a big deal when there is only one singleton object, but they become an issue when there are many singleton or enumerable objects for which to store IDs, and lookup keys don't really help with this.\nThe Entity and Pull APIs also would benefit from idents, giving the ability to easily retrieve all attributes of singleton entities at once, without having to manually store their IDs elsewhere\u2014I use this while serializing a singleton object's state into a non-EDN text format.\nIdents would be very useful.\nThanks again for DataScript, though; I'm thankful for how amazing it already is.\n. Idents are still important for managing many singleton and enumerated entities, particularly in transactions and query/entity/pull inputs. That Gist above would help out with resolving idents in pull-API entities, but it does seem painful, and it only works for the pull API anyway, not transactions, where I find the lack of idents the most painful.\nIf output performance still is that large of a concern, it might be mitigated in a couple of ways:\n- By supporting idents in transactions and query/pull inputs but not resolving them in query/entity/pull outputs. Support for idents in output matters less, at least for me, since I would be able to predict wherever an ident would be expected to appear in an output's datoms and do a simple check myself later.\n- Or by adding ident resolution as a conditional option in the API's functions, which would enable only people who are willing to pay for ident resolution to get it.\n- Or by providing utility functions similar to that of the Gist above that would transform query inputs, pull inputs, query outputs, entities, etc., replacing idents within them with their IDs, or vice versa.\n- Or something else I haven't thought up of.\nI think I prefer the first choice the most. But in the end, as always, it's difficult to predict just how much ident resolution would affect performance in general anyway without actually trying it. It might even end up not being a significant problem at all.\nBut either way, idents remain important in general, though, and it would be really nice to give at least the option to use them. \n. > What breaks if I use one? Even in Clojure?\nFor Clojure, the main reason appears to be the that in the Stack Overflow page above, which says, \u201cIf you pre-compile a one-segment Clojure namespace, you'll get a Java class in the default package. If anyone at any time wants to use your library from Java, he will be stopped dead by this triviality,\u201d though I haven\u2019t yet tested this myself.\nThe main thing is that changing the namespace doesn't seem very costly and brings benefits. The only anti-change arguments that I can think of include:\n1. Changing a library's namespace breaks backwards compatibility and this may be too costly for its current users, even for pre-1.0 libraries such as DataScript.\n2. Single-segment namespace names may be considered by some to be aesthetically prettier than multi-segment names.\n3. Refactoring the library's source code for the namespace changes may be difficult.\nNone of these three reasons to not change it seem very compelling to me by themselves, though I may be mistaken. The arguments for changing it seem more compelling to me: single-segment names interact with state in users' JavaScript programs in hidden and unexpected ways, creating a leaky abstraction; they may interfere with Java interoperability; and they are inconsistent with other Clojure/ClojureScript libraries as well as the core Clojure API itself\u2014even if these might not be show-stoppers, they do seem more serious than any of the three anti-change reasons above.\nNevertheless, I suspect that the library creator finds one of those anti-change reasons (or another reason) compelling enough to keep single-segment namespaces\u2014though I would be interested in knowing which ones they happen to be: backwards compatibility, aesthetics, refactoring difficulty, or something else. (DataScript is still pre-1.0, after all.)\n. Thanks for considering it; if Datomic also freezes on this, it's understandable to think more about whether to check for this and what to do about it. There are three options that I can currently think of:\n- Leave the infinite-loop behavior completely in; no checks at all.\n- Use assert to check for infinite loops to throw errors before they occur; can be enabled or disabled for different compiler builds using the :elide-asserts option; leaves in infinite loops only on optimized builds for which :elide-asserts is true.\n- Check for infinite loops and terminate them, causing circular references to be resolved without throwing errors or infinitely realizing more objects.\nIn addition, the touch API probably also suffers from this same problem since, when used on an entity, it recursively realizes its components too; I haven't yet tested this, though.\n. Thanks!\n. Just tested again in DataScript\u00a00.11.1; the circular references still occur.\n. Thank you super much! DataScript is amazing!\n. ",
    "nahuel": "Now it's crystal clear, many thanks for the detailed response!\n. Thanks for the clear and detailed explanation. Just for the record, it will be very nice to have also a string-based eids partition. I think string eids can be very helpful in the mapping scenario as described in #31, for example, to use external database UUID's as entity-ids (maybe sacrificing a little performance to make queries more elegant). \n. ",
    "jumblerg": "excellent - thanks!\nSent from my phone, please excuse the brevity and typos,\n\nOn Jan 18, 2015, at 6:14 AM, Nikita Prokopov notifications@github.com wrote:\nClosed #47 via 1f1ed58.\n\u2014\nReply to this email directly or view it on GitHub.\n. this query works, however, when i drop the city namespace from the name attribute.  i've also noticed that namespaces don't seem to be used with any of the attributes in the unit tests (beyond those specific to the database).\n. when i use :name it returns as you described, but when i qualify an attribute with a namespace (e.g. :city/name), the same query only returns a single result.\n\ni'm happy to test this further to narrow down the problem if you're unable to reproduce the case i provided\nSent from my phone, please excuse the brevity and typos,\n\nOn Jan 18, 2015, at 5:21 AM, Nikita Prokopov notifications@github.com wrote:\nSorry, I don\u2019t understand what the issue is. I get #{[5] [3] [4] [1] [6] [2]} as a result, which seems ok.\nNamespaces have no special meaning in DataScript. :name and :city/name are two completely different keywords.\n\u2014\nReply to this email directly or view it on GitHub.\n. apologies, i left something out of the case i provided you earlier\n\n``` clojure\n(def db (atom (d/empty-db)))\n(swap! db d/db-with\n  [\n   {:db/id               :some/ident\n    :entity/label        \"Some Thing\"\n    :entity/children     [1 2 3] }\n{:city/name           \"New York City\"\n    :city/location       \"New York\"\n    :city/image          \"/img/home/NewYorkCity.jpg\" }\n   {:city/name           \"Rio de Janerio\"\n    :city/location       \"Brazil\"\n    :city/image          \"/img/home/RioDeJaneiro.jpg\" }\n   {:city/name           \"Auckland\"\n    :city/location       \"New Zealand\"\n    :city/image          \"/img/home/Auckland.jpg\" }\n   {:city/name           \"Berlin\"\n    :city/location       \"Germany\"\n    :city/image          \"/img/home/Berlin.jpg\" }\n   {:city/name           \"Amsterdam\"\n    :city/location       \"Netherlands\"\n    :city/image          \"/img/home/Amsterdam.jpg\" }\n   {:city/name           \"Paris\"\n    :city/location       \"France\"\n    :city/image          \"/img/home/Paris.jpg\" } \n   {:hostel/name         \"My Hostel\"} ])\n(print :city-ids (d/q '[:find ?e :where [?e :city/name]] @db))\n;; :city-ids #{[5] [4] [2]}\n```\n- eliminating the namespaces from the city entities produces the expected result\n- removing :entity/label on the first entity produces the expected result\n- substituting the vector on entity children with a string produces the expected result\n- removing :entity/children on the first entity produces the expected result\n- removing the first entity entirely produces the expected result\n- adding another entity like the first one causes only one result to be returned by the query as i submitted earlier\n. my datalog queries continue to fall on entities of all varieties, but whenever i drop the namespace qualifier from the attribute keyword, they succeed afterwards.\n. ah - many thanks! i was confused by your discussion of idents in the documentation.  i understand datomic wouldn't permit a non-interger in the :db/id position, but got the impression that was somehow supported here.  since the ids deviate from the datomic specification,  it would be helpful to have a sample transaction in the map/entity format like https://github.com/Datomic/day-of-datomic/blob/master/resources/day-of-datomic/social-news.edn.\nin the absence of :db/ident, i presume there's no way to maintain a reference from one entity to another using a keyword? or have enums?\n. just wanted to let you know that everything works great now; the terrifying non-deterministic queries have gone away entirely. could you push a 0.7.3 jar when it's convenient with the corrected (d/entity db nil) behavior i submitted on the the previous ticket?\nthanks again!\n. caused by the same problem as #48.\n. ",
    "bahulneel": "Is there any progress on this as it would be extremely useful for me?\n. There's an experimental query namespace called v3 that negation and joins work on, some other things don't though so your mileage may vary.\n. If you just need a disjoint you can can the same effect with two rules with the same head.\n. Based on your example you could create a rule called (name-or-age ?name ?age) with two bodies e.g.:\nclojure\n[[(name-or-age ?e ?name ?age)\n  [?e :name ?name]]\n [(name-or-age ?e ?name ?age)\n  [?e :age ?age]]]\nThen in the query:\njs\nds.q('[:find ?e :in $ % :where (name-or-age ?e \"Oleg\" 10)]', db, rules);\n(not sure what the js native way of making the rules data structure is)\n. I've shown a method of achieving both using some custom predicates in this ticket: https://github.com/tonsky/datascript/issues/238. Hi,\nHere's a way to implement not-join that should work for most simple cases: https://gist.github.com/bahulneel/c44869fd0078135918b693b145da722f\nIt works by modifying the query to select the positive side of the join, then uses the results as the argument for the negative side. If we take the set difference, we have the values of the join vars that don't match the negative part of the query.\nIt is not really efficient as we need to run the positive terms twice, however, if the join includes entities (or other indexed values), then this should be minimal.. This also works:\n```clojure\n(defn not-join-pred\n  [db vars where & join]\n  (let [vals+ (d/q {:find vars\n                    :in ['$ [vars '...]]\n                    :where where}\n                   db\n                   [join])]\n    (nil? (seq vals+))))\n(println\n  (d/q '{:find [?a ?b]\n         :in [$ ?not-join]\n         :where\n         [[?e :a ?a]\n          [?e :b ?b]\n          [(?not-join $\n                      [?e]\n                      [[?e :c true]]\n                      ?e)]]}\n       (d/db conn)\n       not-join-pred))\n```. Here's or-join for completeness:\n```clojure\n(defn or-join-pred\n  [db vars wheres & join]\n  (let [vals (map #(d/q {:find vars\n                         :in ['$ [vars '...]]\n                         :where [%]}\n                        db\n                        [join])\n                  wheres)]\n    (first (remove nil? (map seq vals)))))\n(println\n  (d/q '{:find [?a ?b]\n         :in [$ ?or-join]\n         :where\n               [[?e :a ?a]\n                [?e :b ?b]\n                [(?or-join $\n                           [?e ?b]\n                           [[(even? ?b)]\n                            [?e :c true]]\n                           ?e ?b)]]}\n       (d/db conn)\n       or-join-pred))\n```. @jconti thanks for your comments, I was beginning to think that no one had noticed this? I'd be interested to see how you've used rules to wrap some of the complexity.\nP.S. I figured out the implementation by reading the Datomic docs on the subject.. Excellent work, thanks @tonsky!. ",
    "terinjokes": "Disjoins seem to be implemented, at least as part of 0.15, but I can't get them to work. When I try the following code, adapted from test/query_or.cljc, I get the error \"Query for unknown vars: [?e]\".\njavascript\nvar db = ds.db_with(ds.empty_db(), [\n  {\":db/id\": 1, \":name\": \"Ivan\", \":age\": 10},\n  {\":db/id\": 2, \":name\": \"Ivan\", \":age\": 20},\n  {\":db/id\": 3, \":name\": \"Oleg\", \":age\": 10},\n  {\":db/id\": 4, \":name\": \"Oleg\", \":age\": 20},\n  {\":db/id\": 5, \":name\": \"Ivan\", \":age\": 10},\n  {\":db/id\": 6, \":name\": \"Ivan\", \":age\": 20}\n]);\nds.q('[:find ?e :where [(or [?e \":name\" \"Oleg\"] [?e \":age\" 10])]]', db);\nAm I doing something wrong, or are \"or\"s still not ready?\n. As far as I can tell, query_v3 doesn't get exposed via the JS interop layer. Just to be clear, negation and disjoins only work in it?\n@bahulneel Thanks, I saw disjoins were possible beforehand in the blog post linked above. I'm new to datalog (just started 12 hours ago or so), so haven't quite figured how to do write a or-equivalent rule just yet. I'll look into it today.\n. @bahulneel You put it in quotes like that. :smile:\n. @typeetfunc thanks, I'll avoid using keywords there.\n. ",
    "comnik": "Sorry for reviving this, but I'd be up for taking a stab at this. @tonsky does it make more sense to wait for / contribute to query_v3 or add negation and disjunction to the current version of the query engine?. Update: Disabling the aforementioned function calls in js.cljs seems to cause no troubles. Something else does: Minification is causing the JS objects representing cljs values in mori to be different from those in datascript, even though they contain the exact same information.\n. For completenesses sake:\nI did a quick experiment, exposing all of mori directly through datascript (to have google closure munge them consistently). As mentioned above, I disabled the clj->js conversions, but kept the keyword -> string mapping. This produced the results I hoped for, allowing me to implement the shouldComponentUpdate optimization as usual.\nSo it works, but I can't think of a way to make this a nice, optional extension on top of datascript.\n. Using mori and DS from source seems like the approach with the least friction.\nAlthough that would require using Closure for my own code as well, right?\nA direct-access namespace would be helpful in any case, because exposing mori through DS  is basically just copying files, changing namespaces and changing the mori-export macro to create Closure symbols like datascript.js.name instead of mori.name.\n. Thank you. It might take some time until I get to it, but I'll try it out and let you know.\n. @typeetfunc Thank you, I will check that out as well, unfortunately occupied with other projects atm.\n@kristianmandrup:\nMori, in contrast to other libraries for JS, is a wrapper around ClojureScript's (CLJS) immutable datastructures. Likewise, datascript is built in CLJS, with the API additionally being exposed to JS. In the process, datascript has to convert it's internal, CLJS results into JS objects. In my use-case, I'm tying together datascript and mori in a JS application. That means a datascript result will first be converted to a JS object and then back to an immutable structure (via mori). This is of-course wasteful.\nAdditionally, React in combination with immutable data, allows for a simple but highly effective optimization, because it allows to compare two state-trees by reference, not by walking through both trees. Unfortunately, I can't make use of this optimization for data returned from datascript (which is all the data in my case), since I'm always creating a new mori object during the conversion, even though the underlying datascript structure might not have changed.\nBy using in both in combination, as discussed in this thread, I can just cut out the cljs->js->cljs conversion entirely.\n. http://www.learndatalogtoday.org/ is helpful, although it's aimed at Datomic and only covers the query language.. ",
    "FrigoEU": "I'm eager to try this out, but I can't seem to get this to work. I'm using datascript 0.9.0, but keep getting errors that pull is not available, neither as pull-api or in queries. Do I need to pull in a special package or something? Probably doing something very stupid...\nProject.clj: \n:dependencies [[org.clojure/clojure \"1.7.0-alpha4\"]\n                 [org.clojure/clojurescript \"0.0-2727\" :scope \"provided\"]\n                 [com.cognitect/transit-clj \"0.8.259\" :exclusions [org.msgpack/msgpack org.clojure/test.check]]\n                 [com.cognitect/transit-cljs \"0.8.192\"]\n                 [datascript \"0.9.0\"]\n                 [leiningen \"2.5.0\"]]\n(ns myproject.core\n  (:require [datascript :as d]\n            [datascript.core :as dc]\n            [cognitect.transit :as transit]\n            [clojure.string :as str])\n  (:require-macros [myproject.macros :as m]))\n(d/pull @conn '[:student/firstname :student/lastname] 1)\n. I could have sworn I did that before... Anyway it's working now and looking good. Great PR!\n. ",
    "wilkerlucio": "I agree with @cigitia, I often find myself wanting these singletons in my app, I would really like to see those in Datascript.\n. ",
    "kristianmandrup": "I'm only just getting started with Datascript, trying to select all attributes of an entity using this example http://www.learndatalogtoday.org/chapter/4, which relies on :db/ident. How else would I achieve that without keep record of said metadata elsewhere?\nTo get the actual keywords we need to look them up using the :db/ident attribute:\njs\n[:find ?attr\n :where\n [?p :person/name]\n [?p ?a]\n [?a :db/ident ?attr]]\nBut I guess you could use Lookup refs to achieve the same effect? If I understand correctly from entity-identifiers lookup refs are like custom primary keys, whereas ident is generated?\n. How about some notes on using Datascript from a Javascript project.\nI see the core API is defined with export annotations in core.cljs \nWould be nice with a quick guide and bullet list of main API.\n. Install datascript via npm or directly use the min.js release\nRead up on js interop\nlook at core and other main namespaces.\nUse datascript core API like this ;)\njs\nvar d$ = datascript.core;\nd$.q(...);\nmunging function names\nI started this wiki page on the topic\n. Thanks @tonsky. I found the js api and listed it here... (WIP)\nhttps://github.com/tonsky/datascript/wiki/Javascript-API\nToo bad the functions don't yet have doc strings to describe their basic functionality. Then you could also generate docs for each release. Which cljs doc tool is most used/mature?\n. I started this api overview wiki page. Please help out ;)\n. @tonsky Could you please help document how to upsert data ;) Thanks!\nI don't quite understand the test example.\n. Another option would be to wrap the query result on the JS side with an immutable data structure such as ancient-oak\n. Pardon me for asking, but what is the use case for transforming a Datalog query into a Mori structure? I thought it was mainly useful for wrapping a query result?\n. \"That means a datascript result will first be converted to a JS object and then back to an immutable structure (via mori). This is of-course wasteful.\"\nThis was exactly what I was after/questioning as well. So datascript-mori is actually an efficient \"bridge\" which directly converts to mori without the intermediate to Javascript conversion?\nYes, I'm aware of the benefits of immutable datastructures for application state comparisons for efficient diffs and re-renders. Why I'm interested in this datascript/mori approach! :)\n. Thanks @typeetfunc for describing the motivation behind the babel plugin. Could you also add similar motivation, examples etc. for datascript-mori :)\n. Thank you so much. I had not found Datomic api and only just found the gitter channel a minute ago ;) I will ask such questions there in the future.\nI'm trying to collect all of the various Datomic/Datascript material out there into a more concise form:\nhttps://github.com/kristianmandrup/datascript-tutorial\nYou are welcome to chip in if you like.\n. ",
    "augustl": "I ended up storing my singleton \"bookkeeping entity\" outside of datascript, in a plain old atom. Works well enough!. ",
    "jeroenvandijk": "Thanks for the quick response. I asked on the Datomic mailinglist [1] and I'm more convinced now that a Clojure DataScript provides new use cases. I'll try with Datomic first, but I'm might go the DataScript route if it turns out to difficult or to hacky. \n[1] https://groups.google.com/forum/#!topic/datomic/Cq7ULo0sSBc\n. FYI, I made a start https://github.com/tonsky/datascript/compare/master...jeroenvandijk:feature/clojure-port. I try to get the test suite over to cljx, step by step, until I have both cljsbuild test and test passing via:\nlein do clean, cljx once, cljsbuild test, test\nNot sure how much time it will take me yet. I've copied the cljx approach from Prismatic Schema. Does this sound ok to you?\n. That makes sense. I'll regard the first as an experiment. Thanks\nOp 28 feb. 2015 09:32 schreef \"Nikita Prokopov\" notifications@github.com:\n\nWell, test suite can almost transparently being copied to clj\u2014there\u2019s\nnothing cljs-specific in tests or datascript api at all. Other stuff will\nneed more creative approach: e.g., btset needs a complete rewrite, query\npartially depends on #js stuff, debug uses a lot of cljs-dependent things.\nPlus there\u2019re protocol differences in base clojure/clojurescript classes.\nYou can experiment with this to identify possible problems and solutions,\nbut I\u2019m not sure I\u2019ll be able to merge your branch as-is, a lot is changing\nright now, and merge will probably be a disaster.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/54#issuecomment-76517139.\n. Just a FYI, I had the following issue due this single namespace (I feel kind of stupid about it):\n\n``` clojure\n(ns my-namespace\n (:require  [adgoji.datascript :as datascript]\n            [datascript :as d])\n(defonce conn (datascript/setup-conn))\n```\nThis resulted in\nUncaught TypeError: Cannot read property 'call' of undefined\nsetup-conn was really defined in adgoji.datascript though. It took me a while before I figured datascript was not only available via alias d but also as datascript itself\n. Thanks!\n. Nice!\n. ",
    "hansgru": "+1 for a Clojure version\n. ",
    "benfleis": "As github has already added, there is a beginning of a clojure port now: https://github.com/tonsky/datascript/pull/68\nIt doesn't merge, and needs more work, but currently seeks feedback.\n. Sorry for late reply. Had a nasty flu since Friday of last week.\n\nWow! That\u2019s a lot of changes :)\n\nIndeed! More than I hoped for. I tried to minimize superfluous changes (and removed many superfluous to create this version of the diff). My mileage varied...\nOkay, there's tons of carrying on, below, so I'll go with the \"big steps\" thoughts first. Then you can read below so long as your interest allows :)\nMy idea for discrete steps (with the provision that I haven't yet had a real chance to play with cljc -- that will be tomorrow):\n1a. Move all the files that need moving. I hate meaningless diffs. Do you?\n1b. Update all existing build stuff to work just like it does now.+\n2. Start slicing the \"inspiration\" branch into low hanging fruit which can be more-or-less directly applied, and work them in, on the basis of functional tests.\u2021 Low hanging fruit should constitute \"minor\" changes since fork, with no real impact on performance for the CLJS side.\n3. Once the low hanging fruit is done, start playing catch-up with the changes that have come in the last 2 months, and continue to work toward feature parity. Still, all CLJS side tests must run, maintaining good performance characteristics.\n4. Once feature parity is achieved, start profiling CLJ side to see what needs tweaking. Tweak.\n5. Profit. Or at least smile.\n- AFAICT, git stays most sane w/ a commit of just file moves, and a follow up commit with changes. Thus 1a + 1b would be done together, as 2 commits, 1 push. And with some timing awareness and communication to other people working against the tree.\n  \u2021 (This was my own approach, although the current tests tend to be end-to-end, requiring many other functional things to perform simple tests. Perhaps this could be finagled along the way.)\nIt feels like these agree with you basic ideas below. If it sounds good, then I'll have a more thorough look @ CLJC, propose a set of file renames / build mods (from head, of course) to make it all work the same, and go from there. I think your biggest first task will be just reviewing that, and understanding whether it has a big impact on other devs -- maybe it's a small enough group that not tooooo many people will be throwing things at my head.\nNow, to the carrying on.\n\nDefinitely cljc, not cljx.\n\nEasy to agree on, given the 1.7 status. I doubt this will be particularly difficult, given that it's mostly just read restructuring and file renames.\n\nDefinitely performance, not code beauty. E.g. ...\n\nYeah, when I first started, I wondered how much of it was necessary, but as I got deeper, it appeared that you've got some code to support it.\n\nI also prefer not to add additional deps until absolutely necessary. I believe we can handle UUID and seqable? without introducing dependency.\n\nseqable? is trivial to copy; UUID - could use your implementation, I think. I know nothing about that one, but had no such aversion at that moment :)\n\nI also believe we should feel native for the platform.\n\nAgreed. I suspect that these things will also come out quickly as Issues/PRs if people get their hands on a working version.\n\nIs there a reason why you\u2019ve changed (FilteredDB. \u2026) to (->FilteredDB \u2026)? Just curious.\n\nWarranted curiosity -- I had problems with the repl holding a ref to the previous class object, whereas it seems to always get the \"right one\" when using the -> version. I've experienced this in other projects with heavy use of protocols and the repl, so I did it without looking back. I would love for somebody to tell me \"I'm doing it wrong\" and that something else was causing my problems, but I've quit and restarted too many repls to fight it.\n\nAlso BTSet will probably need separate implementation (maybe even pure-java) as it\u2019s too low-level to be multilingua efficiently.\n\nYeah, I thought best to start simple, measure performance, then see where we go. This works, in ~20 LOC, so I was pleased afterward :)\n\n+cljs (def Exception js/Error)\nClever! We could probably use ExceptionInfo here though. I believe it is cross-platform.\n\nCredit due: stolen from prismatic. Probably ExceptionInfo is enough, although I applied Throwable for maximum paranoia in some cases.\n\nvalue vs. nuisance\nNot sure what do you mean, can you elaborate?\n\nWho knows... that was so long ago. I think I was abstractly referring to performance versus code sharing.\n\nAll that being said (and repeating again: it\u2019s a huge amount of work! Thanks for putting this together), I think we have to think of some strategy how we can get this merged. It cannot go as one patch: there\u2019s a lot of decisions in there, and there\u2019s no way I can assess them all at once. Also CLJS code is evolving, and I cannot keep track of all changes happening in both master and clj PR.\nAs I see it, it can be a series of small patches which will smoothly move us towards the end goal. We start introducing portability piece-by-piece, and these changes will co-exist in master. It won\u2019t compile to clj at first, but I want code parts we both agree on to be there anyway: so changes can be made to master and CLJ version at once.\nYour current branch can work as a source of ideas and a roadmap. It can be sliced and discussed piece-by-piece as a separate PRs.\n\nThis is exactly what I expected. Piece by piece, until a certain unity is found.\n. @seantalts It's in progress; I have a very large diff that is being starting to be incorporated one piece at a time. Check out https://github.com/tonsky/datascript/pull/78 for the latest bit (from earlier today).\n. All of the tests now pass in Clojure, as well as ClojureScript. There are most certainly still bugs, rough edges, changes still to come, etc., but as of https://github.com/tonsky/datascript/commit/6d30234532c63b92e469dba9c0f0d06afd1f9d3c , it's in there. I doubt that it's officially supported yet, but it can be played with. To run the CLJ tests, use lein test-clj; haven't yet tracked down why plain ol' lein test doesn't work, but it doesn't (for me, at least).\n. All of this reworked and ultimately incorporated in, via a series of pulls:\nhttps://github.com/tonsky/datascript/pull/70\n  https://github.com/tonsky/datascript/pull/71\n  https://github.com/tonsky/datascript/pull/78\n  https://github.com/tonsky/datascript/pull/79\n  https://github.com/tonsky/datascript/pull/82\n. @tonsky damnit. will withdraw while I figure out what i've done wrong here.\n. Everything sounds good. 2 things worth telling:\n\nI don\u2019t get why are you testing (sorted-set-by cmp-s), not btset. I understand that it\u2019s just a wrapped sorted-set, but still, we should test around public interface, not impl detail. Correct me if I miss something.\n\nYou are correct - I originally wrote it since the first btset version I wrote w/ sorted-set-by passed the existing tests, but was actually broken, so I went back a step and wrote the sorted-by-set tests to make certain it was correct. I will keep the tests, but w/ the btset API. Should have done that in the first place, but didn't think much about it from that POV.\n```\n(defn btset-conj [set key] (conj set key))\n(defn btset-disj [set key] (disj set key))\n->\n(defn btset-conj [set key ] (conj set key))\n(defn btset-disj [set key ] (disj set key))\n```\nI debated this, and in the end thought it better to have it fail when compiling in CLJ, since it feels misleading to accept and ignore that argument, since it would have a clear semantic expectation attached to it. I thought the alternative would be to leave the argument, with an assert failing any time it's not nil. Whatcha think?\n. ## Big Picture\n\nThat's why I prefer small steps and clean small separate diffs. Not put everything in one \u201cclean\u201d patch (it's a long road until it's clean), but separate patches by logically independent pieces. E.g. (Record.) \u2192 (->Record) migration, it's easy to review and merge if diff contains only this.\n\nI'm on board. Will split out into much smaller chunks.\nSpecific responses\n\nSo I read all the diff, here are my notes, in read order:\n- Point of deftype on Datom was to avoid unnecessary fields (meta, extended attrs map) so its a lightweight as possible. For all other cases defrecord + extend-type works just fine and is cross-platrofm, so let's stick to that. The parts where you're trying to implement what defrecord natively implements are most fragile ones, so let's try keep them to minimum. val-at-db, assoc-db and the likes should go\n\nCompletely agreed. The factor here is that equivalence and hash values cannot be overridden with a defrecord in CLJ. Initially, I only changed it when necessary for this reason. Afterward I believe I switched at least one CLJS defrecord -> deftype for the sake of behavioral consistency between the versions. \nIf extend-type allows it in CLJ, then I will gladly revert -- I prefer taking what the language(s) give us, as much as possible. (I don't recall any longer whether I tried redefining equiv/hash in extend-type in the first place.) \n\n\nentryAt and equiv are swapped in Datom/clj version\nLet's move macros from macros.clj to core.cljc and deftrecord to parser.cljc, and remove macros.clj\nFor partially implemented methods, we should throw UnsupportedOperationException in clj...\nentryAt should return clojure.lang.MapEntry, not vector\n\n\nCheck.\n\n\nAbout cmp-val/cmp-val-quick formatting. I prefer to have one function header, and then conditional reader in the fn body...\n\n\nCool -- I generally prefer it that way too, but gave both perspectives so you can see and compare.\nRe cmp funcs, easy fix.\n\n\nget-datoms-as-tuples will materialize vector of all datoms in memory, might be big. We can probably use lazy seqs here somehow\n\n\nI will have a look -- I remember considering this when I originally wrote it, but I am no expert on lazy, or perhaps lost my intention.\n\n\n#?(:clj (do \u2026)) \u2192 #?@(:clj [\u2026])\n\n\nThis is not allowed in top level forms. There's a bug filed in CLJ someplace about it. It took me a while to discover this though.\nhttp://dev.clojure.org/jira/browse/CLJ-1706\n\n\nWhat's the point of db*? Also, I prefer map->DB for db creation as there's a lot of keys in DB\n\n\nOriginally to declare. Will clean this up.  map->DB doesn't exist if it's deftype.\n\n\nWhy empty-db is overloaded? I think arg should always mean the same thing (schema in our case)\n\n\nNot smart -- original thinking was (empty foo) -> (empty-db db) consistency, but that could be privatized into a * variant.\n\n\nWhy removing ^boolean annotation for is-attr/multival/\u2026 methods?\n\n\nIt caused test failures in CLJ world; iirc, it's because the funcs don't actually return bools, but instead truthy things. Could also have wrapped return w/ (boolean). \n\nwe have array? defined in datascript.core earlier, no need for conditional read here\n\nYep, that was written before I wrapped it, needs fix.\n\n\n(assert db) \u2014 probably forgot to remove\nretract-components: you replaced transducer form with lazy one, why? It's less performant that way\n\n\nAlso originally written versus 1.6, all of those should be reverted (there are several).\nOther than Entity, I'll skip the rest, since we'll move to a more per-file thing. I will come back to refer to these when it's time.\n\nPARSER\nempty? is allocation-free and in most cases just and integer check. ... So please don't do this\n\nCheck.\n\nQUERY\n-  ([] (Relation. {} [#js[]]))\n+  ([] (Relation. {} [#?(:cljs #js [])]))\nThis is not correct for CLJ version. There must be a single 0-tuple element in this rel.\n\nThis should have had a ; XXX incorrect for CLJ after it, as a marker. not fully imported.\n\nENTITY\n\nHow come clojure.walk needs assoc?\n\n\nhttps://github.com/clojure/clojure/blob/2224dbad5534ff23d3653b07d9dc0a60ba076dd7/src/clj/clojure/walk.clj#L49\n(into (empty <defrecord>)) turns into a series of conjs. Since it's essentially a keyed structure, these conjs become assocs.\n\n\nWhat's TouchableEntity? Why do we need this?\n\n\nI no longer recall why I made it a separate protocol. Probably no good reason :)\n\n\nWe used set! for Entity to keep mutable cache. It can be changed to volatile! ref, so impl can be shared between clj and cljs. I very much prefer it is shared\n\n\nI spent a bit of time trying to get :volatile-mutable working with deftype. I failed. I asked about it on #clojure, and nobody seemed to even know of a real example, or how my example work. So I gave up. I'll give it another shot. \n\nTESTS\n\nDoes clojure.test fail to print ExceptionInfo data too? If it does print it, we probably doesn't have to patch test failure printer.\n\n\nAhh, interesting. I literally just ported it blindly, without testing it directly. Probably not needed.\n\n-      (is (= (-> (e 10) :children first :father) (e 10)))\n+      (is (some #(= (e 10) (:father %)) (-> (e 10) :children)))\nIt's strange you had to change that test. Can you explain why?\n\nThe original test was dependent upon ordering within the set, which didn't work in CLJ.\n\nEverything else\nFormatting: looking at conditional reader forms, it's hard to read now. My proposal: let's treat them as any other form. So top-level forms included into #?() will be indented, but I think it's ok. Like this:\n\nEasy peasy.\n\n\nclojurescript.test suggests that :none optimization doesn't work with tests \u2014 so i switched it to :whitespace (and renamed for disambiguation); thoughts?\n\nHow do you run tests? I open dev.html in Chrome and connect with REPL, works perfectly with any optimization level.\n\nI'd been running with :none as well, but switched to :whitespace when I saw that. I figured if the author says it doesn't work, then it's unwise to ignore. \n\n\ntouch API \u2014 if datoms search comes back empty, shouldn't it still be marked as touched?\n\nDoesn't really matter, touch is for interactive exploration anyway. There's no place for it in a working program.\n\nTrue. You still prefer to leave it as is?\n. Everything in here got merged in, one way or another. Closing the pull, and removing the branch.\n. I only used vreset! to avoid the race condition; since it's a cache, the worst case of a race is that the same cache element(s) get loaded twice. But your call -- it's not too hard to switch to atom.\nI missed empty, will fix that one. IFn, will add. I was originally driven by making unit tests succeed, so some of the API elements are probably missing, when never used in the tests. (Also {key,entry,value_set} are missing, for example.) I think I was more thorough with later structures, but this first one left me wondering how far to go, and I never followed up by asking.\nAs to walk, I was separating out the test stuff and trying to keep the blocks much smaller; I am happy to dive into the rabbit hole, but the first version of this patch is what happens when following dependencies :) My branch will certainly reflect the walk change as we go, so I'm not worried about missing it.\n. @tonsky ok, need to port the macros too - i had the old file still in my working tree and didn't realize it. will have fix shortly.\n. @tonsky FYI, I looked at doing the proper macro migration, but that won't work until core.cljc is fully ready to compile in CLJ, meaning the pr-str* form must be wrapper, and perhaps others. Soon...\n. @tonsky\nThe misnamed classes are easy to fix -- one was a completion error, and the other an obvious mental miss.\nI debated whether to keep assoc w/ Datom -- I had it in my original patch; I will re-add it.\nThe map thing w/ string output -- I had fixed it in one place, and will do the same there.\nI think I know why the identical? check got changed - I had initially tried to reduce it to a compare, which doesn't work w/ all of the types, and went straight to strings. I am clearly not keeping optimization as high in my head as you are used to, and will push further in that direction.\nAs to Datom., within core.clj, it's trivial. It's in the other files that it makes a difference -- then we have to add an import for every place that it's wanted. Intuitively I would argue for cleanliness and better dev-env over 5% gains, but I lack context to judge how important this particular 5% is -- I assume that query speed is the place where performance is most critical, and wonder how this affects queries. What are your thoughts here? I would for now propose using Datom. in core.cljc, and ->Datom outside. Whatcha think?\n. These are easy -- funny that I forgot to drop the assoc/etc. operators.  The Boolean thing worked, because it got defd above, just like with the exception types.\nThe field name thing can work in the (:clj ,,,) wrapping, so we can make that switch there.\nI also thought, after writing the description that 'extendable' isn't right, but 'replaceable' is, since that what it actually allows.\nAs to empty-db, there is still a need for 2 variants -- in an old patch, i combined them into a single func w/ an overloaded arg. I either need to do that, OR have 2 related variants, perhaps (empty-db ...) which is public and conforms to the current public API, and (empty-db* ...), which would fulfill the normal  (-empty ...) need.\nShould/may I also move the other related -db funcs into core? (I think empty, init, and the validation funcs.) I feel like they belong there next to the rest of the DB code, and they're all single use.\nAnd as to protocols, I agree -- it was more to point it out as something to think about.\n. Also realize later: hash-db needs to work s/ FilteredDB too, so the type hint should go away, and then -datom is the right thing. Or am I missing sth? (This was my original intention, just the type hint is wrong.)\nAnd the empty variant thing -- that's actually only the case if we worry about the other members being initialized differently, so that's actually not quite true. Can be used from both by passing the schema in from the defrecord calls. I'll try to get a patch up later today.\n. doh, I see your last comment just now after pushing. /me -> chat.\n. re: hash caching -- it would either need to acquire an atom or other referential thing, or become a deftype. thoughts?\n. Random thought -- should the hash include the schema? Since equiv covers it, not necessary, but still feels weird.\n. @tonsky btw, I think the last diff contains everything we've talked about, here & in chat, so it should be ready for review.\n. Boolean update is in place. As to the defrecord mods -- I assume you mean to do it by only emitting 'dupcliate' code in extend-type? Or just to make a similar macro for cljs side, so that the output is the result of defrecord, across the board?\n. All looks good; serialization already moved in my branch, and I wondered about and debated putting a comment about mapa, but knew you'd see it and let me know if it was intentionally structured that way.\n. @tonsky\nWas looking for an example of data_readers.clj, and came immediately upon this thread, saying that libraries shouldn't be defining it.\nhttps://groups.google.com/d/msg/clojure/B4_uGy1VhnA/pZdF3Eq3j5oJ\nAnd yet, some do. Trivial for it to be there, or not, but this pushes me to think better of defining ds/data-readers as the map, and letting users do as they please. (CLJS has a register mechanism, and is fundamentally ... more compatible.)\n. re :datoms: shouldn't it be exactly that, a vector of vectors? (or more generically, a seq of datoms, which are themselves seqs in this context?)\n. EDIT: @tonsky Ignore this filthy lie. Cleaning the repl made these forms work.\nusing clojure.core read-string w/ data_readers.clj fails similarly to the commented out test below. i haven't yet solved this; if you've got an idea, let me know, otherwise this simply doesn't work atm :(\n. > (#?(:cljs array-reduce :clj reduce) ;; iterate over corresponding rel1 tuples\n\nreduce and array-reduce have different argument order. Maybe create dc/array-reduce and use it here?\n\nI will try areduce instead.\n\n#?(:clj (import '[clojure.lang ExceptionInfo]))\nCan you move it into ns declaration? I want to be as declarative as possible\n\nEasy peasy. I debated which was clearer in this context, since it was intended to echo the (def Exception js/Error) type things.\n\n(defn- retract-components [db datoms] ...)\nWhy changing transducers form to lazy seq one? We\u2019re on 1.7 anyway\n\nTransducer part was forgotten, and will be fixed. The order swapping that came with it was not accidental. Speaking to you offline about that. (After: reverting entirely. My misunderstanding.)\n\n-         (clojure.walk/postwalk #(if (de/entity? %)\n+         (clojure.walk/prewalk #(if (de/entity? %)\nWhy change here?\n\nIf we use postwalk, then Entity needs to support assoc, which we don't want to do. See https://github.com/tonsky/datascript/pull/74#issuecomment-98659268\n\n(def get-compat #?(:cljs aget :clj get))\nThis is still going through additional fn call. This place is very hot. Can you write it that way so CLJS version stays exactly what it was (putting conditionals directly in every call place, not abstracting to fn).\n...\n-        (list* (.map getters #(% tuple)))))))\n+        (list* (map #(% tuple) getters))))))\n\nWill do.\n. @dahjelle Hey, I don't speak for @tonsky obviously, but I might suggest making a 'dev' dir, and putting repl.clj in there, and updating the project.clj to include that as a src dir in the dev profile. (Locally I do this with a dev/user.clj, which contained my own hooks for node repl.) In fact, if you put that into user.clj, it will automatically be available from your repl, if that's the desired effect.\n$0.02 -> done.\n. source-paths update is correct.\nif user.clj exists in a source path, it is already loaded when you open a repl, since the repl starts in user namespace. thus, it's handy to have a user.clj contain things that you want to \"just be available\", without having to explicitly eval that file.\nthus, if you want that code to be automatically available upon starting a normal repl, put it in user.clj instead of repl.clj.\nmake sense?\n. ",
    "seantalts": "Any updates on this? I'm personally hoping to use the querying capacity (datalog implementation, basically) against my own implementation of the IDB / ISearch / IIndexAccess protocols under clojure if possible. I'd be willing to do work in that direction if it needs doing? \n. Sweet, thanks.\n. ",
    "igorhub": "\n(FilteredDB. \u2026) to (->FilteredDB \u2026)\n\nI remember having the same problem. I asked the question on\nstackoverflow and was told that (FilterDB. \u2026) form is low-level and not\nintended for creating records. (Sorry can't find the link to discussion)\n. ",
    "bhauman": "+1  I'm going to plus one this.  I designed figwheel to not hot load code with warnings, I have to turn off this very helpful feature when I use datascript.\nRevision:\nActually thats not true it only warns on initial analysis.\n. It's a ClojureScript issue:\nhttp://dev.clojure.org/jira/browse/CLJS-1004\nhttps://github.com/clojure/clojurescript/commit/dc47f1fa1483871641c34f765034efedb7238f28\nAnd all the resources linked by the OP.  Not enough?\n. Just to be clear. As I mentioned above you don't have to turn off the Figwheel feature the prevents the loading of code that is generating warnings.  The ClojureScript single segment namespace warning only occurs once on initial analysis.\nSo there is really no show stopper here.  This is just a nice to have.\nI would add that the possibility of someone having a DOM Id of \"datascript\" is much higher now that datascript exists. And it's a really tough one to debug, as code is getting loaded a DOM element gets returned instead???\nBut I don't see this as something that requires urgent attention at all.\n. ",
    "sveri": "+1 for the reasons namespaces exist in general. I find them very useful and it's a pity so many great libraries in clj / cljs land don't make proper use of them.\n. Basically you are saying: I am the only one to release a library with the namespace datascript. It's just a way to avoid ambiguity in naming.\nUsually you would choose a ns like: com.yourdomain.datascript or if you don't own a domain com.github.tonsky.datascript.\nI guess this just became a common practice because it has some small advantages compared to the slightly longer import name.\n. I know that these reasons are not the same, I just wanted to add some more reasons to switch to a different namespace.\n. ",
    "bendisposto": "sveri: No, that is not the reason. The problem is indeed that the dependency resolution mechanism returns a DOM node with the same name if present (see http://dev.clojure.org/jira/browse/CLJS-1004)\nHowever, I would also like to see that changed in datascript because I don't want to turn off the \"do not load if there are warnings\" feature in figwheel. \n. ",
    "gtrak": "I get these errors in the node repl and can't seem to use the datascript.query namespace because of it:\n```\nWARNING: datascript is a single segment namespace at line 1 .repl/datascript.cljc\nWARNING: datascript is a single segment namespace at line 1 file:/home/gary/.m2/repository/datascript/datascript/0.11.1/datascript-0.11.1.jar!/datascript.cljc\nquery.core> (require '[datascript.query :as q])\nnil\nquery.core> (q/q\n             '[:find ?e\n               :where [?e :age 42]])\nExceptionInfo ReferenceError: datascript is not defined\n    at repl:1:69\n    at repl:9:3\n    at [stdin]:41:34\n    at b (domain.js:183:18)\n    at Domain.run (domain.js:123:23)\n    at Socket. ([stdin]:40:25)\n    at Socket.EventEmitter.emit (events.js:95:17)\n    at Socket. (stream_readable.js:746:14)\n    at Socket.EventEmitter.emit (events.js:92:17)\n    at emitReadable (_stream_readable.js:408:10)  clojure.core/ex-info (core.clj:4591)\n```\n. ",
    "lbradstreet": "That was quick. Thanks @tonsky!\n. ",
    "ThomasDeutsch": "I am rewriting the datascript todo app.\nFor the filtering, i would like to pass a filtered db to my todo-list ( like you described in the video ).\nThe last example is just what i needed. Thanks!\n. Thank you for your quick reply. \nI was on clojure 1.6.0  -  i changed to 1.7.0-beta2  and it is working now. \n. For one single file in my project - the editor choosed to set the encoding to ANSI - instead of UTF-8. :(  Thank you again for your quick response.\n. Is it a good idea to use a rule for this scenario?\n``` cljs\n(def contains-rule\n  '[[(cont ?s1 ?s2 ?s)\n     [(= ?s1 ?s)]]\n    [(cont ?s1 ?s2 ?s)\n     [(= ?s2 ?s)]]])\n;; get all combinations of ?s1 and ?s2 where selected-eid is\n;; equal to ?s1 or ?s2\n(defn find-combinations [db selected-eid]\n  (d/q '[:find ?s1 ?s2\n         :in $ % ?s\n         :where\n         [?s1 :service/uuid ?s1uuid]\n         [?s2 :service/uuid ?s2uuid]\n         [(not= ?s1uuid ?s2uuid)]\n         (cont ?s1 ?s2 ?s)]\n            db contains-rule selected-eid)\n       )\n```\n. Thank you!\n. You are right, it should have worked, but in my case - one of the attributes was a \n{:db/unique :db.unique/identity}\nso i needed to delete the form entity first, before i could make the transaction.\nNice, that datascript prevented this mistake, but i got no error.\n. when you look at the save-customer example -\ni get an error when i use a :db/id that is > 0 in that transaction - not so if i use a negative :db/id\n. pull request\nI think this is not correct - damn, sorry. I am working on a new pull request.\n. https://github.com/tonsky/datascript/pull/97\n. Thank you very mutch. Sorry about not getting it right.\n2015-07-19 15:03 GMT+02:00 Nikita Prokopov notifications@github.com:\n\nDecided to go with my own patch \u2014 yours has a bug, it was faster to fix it\nby myself: f2f05fa\nhttps://github.com/tonsky/datascript/commit/f2f05fa7ef956358ef5680e861b11a6e84087be3\nNote that only seconds, not milliseconds, go to squuid (so they fit into\n32 bit). Also note that the purpose of squuid was to provide monotonic\nunique decentralized identities. The fact that they\u2019re using time is just a\ncoincidence. It would be reasonable not to rely on that, and store\ntimestamp explicitly. I\u2019m adding sqquid/1 mostly for the symmetry of the\nAPI.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/tonsky/datascript/issues/95#issuecomment-122660061.\n. i still get an error, using: \n\n[org.clojure/clojure \"1.7.0\"]\n[org.clojure/clojurescript \"1.7.122\"]\n[datascript \"0.11.6\"]\nError-message:\n```\nCannot compare {0 [540 960]} to {0 [540 1140]}\ncljs$core$compare @ core.cljs:1668datascript$core$cmp_val_quick @ core.cljc?rel=1441350387628:293\n```\n. Thanks for the help.\nIt is easy to create such an entity: \nclj\n (d/transact! conn [{:db/id -1}])   ;; simple as that.\nWhy would i do that?\nIn my app, a placement can contain multiple bookings.\nWhen i create a placement, a new booking will be added to this placement, too.\nclj\n(defn new-placement []\n    (let [new-booking {:db/id -2}\n          new-placement {:db/id -1\n                 :placement/show.booking-form? true\n                 :placement/bookings [-2]\n                 :placement/selected-booking -2}]\n      (d/transact! conn [new-booking\n                         new-placement\n                         [:db/add 0 :scheduler/selected-placement -1]])))\nIt will be easy for me to find an attribute i can fill with some default data - but using only {:db/id ..} was convenient.\n. ",
    "uwo": "Ah! I didn't read the retract source closely enough. Next time I\"ll try to put questions like this in Gitter.\nThanks!\n. ",
    "sherbondy": "Hey, you know what, my reset_conn suggestion is essentially a repeat of this issue:\nhttps://github.com/tonsky/datascript/issues/45\nSo I'll close this issue, and will open a new one if #45 gets resolved without being exported to JS-land :)\n. ",
    "myguidingstar-zz": "What's the point of that guard? To prevent duplicated k/v pairs? Then values of different types should be valid, too.\nActually I used similar code to store some rum/sablono templates (as clojurescript vectors) to datascript. Now I have to wrap each template in a list as a workaround.\nThe problem is values of different entities are checked with compare, too?\nclj\n(let [conn (d/create-conn {})]\n  (d/transact! conn\n               [{:db/id -1 :key [:value 2]}\n                {:db/id -2 :key [:value \"foo\"]}]))\n;; => exception again\n. haha, thanks. Turns out that wrapping things in lists is the way to compare them all\n(compare (list (array-map :a 1 :b 2))\n         (list (hash-map :a 1 :b 2)))\n;; => -1\n. Reported as Clojurescript bug instead\nhttp://dev.clojure.org/jira/browse/CLJS-1200\n. ",
    "sonwh98": "I am running into this problem trying to store javascript objects. anyway around this?\n. I am trying to store functions. When I tried to extend IFn to impelment IComparable, it doesn't seem to work \n(extend-protocol IComparable\n                 FamousBox (^number -compare [x y]\n                                             (famous-compare x y))\n                 Vec3 (^number -compare [x y]\n                                        (famous-compare x y))\n                 Quaternion (^number -compare [x y]\n                                              (famous-compare x y))\n                 Spring (^number -compare [x y]\n                                          (famous-compare x y))\n                 RotationalSpring (^number -compare [x y]\n                                                    (famous-compare x y))\n                 Node (^number -compare [x y]\n                                        (famous-compare x y))\n                 IFn (^number -compare [x y]\n                                       (println \"compare functions x=\" x \" y=\" y)\n                                       (= x y))\n                 )\nThe other types work but not IFn. The println statement is never reached. Any idea what I am doing wrong?  The type of javascript functions is cljs.core.IFn right?\n. Thanks @tonsky . For the benefit of others who might be running into the same problem. IFn  can't be extended because its a protocol.\nThe solution is to extend function. \n(extend-protocol IComparable\n    function (^number -compare [x y]\n                                       (println \"compare functions x=\" x \" y=\" y)\n                                       (your-compare-function x y))\n. thank you! it needs an :in \n(d/q '[:find ?r :in ?subs :where [(?subs \"abc123\" 0 3) ?r]] subs)\n. thanks! you're right. The problem was how I converted datomic schema into datascript. it was done improperly and :contacts/tags was not included. It would have been nice if datomic schema can be exported to datascript without any conversion\n. ",
    "carocad": "I just stumbled into this .... is there a way to solve this problem in Clojure (not clojurescript) since there no IComparable protocol there :/\nIn my case the value that is causing the problem is precisely the one used for uniqueness (:db/unique :db.unique/identity) so I cannot \"just ignore it\".. @tonsky fair enough. Thanks for the support. For the time being I am just \"hoping\" that the users won't do that since it is quite weird to use numbers in some cases but strings on others. \nSo if it happens then I just show that error. Btw, I saw that the Clojure comparators page now has an entry on cross type comparators, maybe you could use some of that code. \nHope it helps\nhttps://clojure.org/guides/comparators. I recently launched rata a very thin layer on top of Datascript for reactive queries with reagent atoms. Views are re-rendered automatically based on the latest results of the queries.\nI tried to keep it as lean as possible (just around 20 loc) so I depend a lot on both Datascript query caching and reagent result caching.\nIt is not based on any fancy papers but it sure works \ud83d\ude05. Your feedback is most welcome :) . > @carocad This sounds basically like posh, without the machinery for deciding whether to rerun queries.\nit is :)\n... \nit just so happen that the machinery is backed inside reagent. You can check reagent's track function. It automatically decides which queries to re-run based on the components lifecycle and reuses the result of the functions to avoid multiple executions. > Have you been using this approach on any large projects?\n@Jumblemuddle no, so far my frontend apps are not that big. I am looking forward to some feedback on that to be honest :)\n\nThe real question is why you'd want to implement you're own over use Posh; If for fun, then great! But if you're hoping folks are actually going to use it, it's going to have to have a feature that makes it worth choosing over Posh.\n\n@metasoarous there is no grand vision on this. I am not trying to make everybody switch, this is not a competition. I saw an opportunity to make it simpler and I took it. Wether people use it in production, play around with it or take it as inspiration is up for everyone to decide.\n\nIt's not perfect, and misses some edge cases like recursive rules, but it's better than nothing in my opinion.\n\nThat is a feature of rata.  Every query and pull pattern that works in Datascript will automatically work in rata. Every performance improvement in Datascript and reagent will benefit rata automatically. Simplicity is a feature.. Thanks for the fast reply, I was not aware of this :). ",
    "dwwoelfel": "I can't say for sure yet whether it's a serious issue, still working through some edge cases. Making the outcome order-dependent is going to cause other problems, though. Here's an example resolving tempids with refs where changing the order will result in an exception:\n``` clojure\n(deftest test-upsert\n  (let [db (d/db-with (d/empty-db {:name  { :db/unique :db.unique/identity }\n                                   :friend { :db/type :db.type/ref }})\n                      [{:db/id 1 :name \"Ivan\"}\n                       {:db/id 2 :name \"Petr\"}])\n        touched (fn [tx e] (into {} (d/touch (d/entity (:db-after tx) e))))]\n;; This test passes\n(testing \"upsert with references\"\n  (let [tx (d/with db [{:db/id -2 :name \"Petr\"}\n                       {:db/id -1 :name \"Ivan\" :friend -2}])]\n    (is (= {:name \"Ivan\" :friend {:db/id 2}} (touched tx 1)))\n    (is (= {:name \"Petr\"} (touched tx 2)))))\n\n;; Changing the order causes it to fail with\n;;  {:error :transact/upsert, :attribute :name, :entity {:db/id 3, :name Petr}, :datom #datascript/Datom [2 :name Petr 536870913 true]}\n(testing \"upsert with references, reversed order of txes, fails\"\n  (let [tx (d/with db [{:db/id -1 :name \"Ivan\" :friend -2}\n                       {:db/id -2 :name \"Petr\"}])]\n    (is (= {:name \"Ivan\" :friend {:db/id 2}} (touched tx 1)))\n    (is (= {:name \"Petr\"} (touched tx 2)))))))\n\n```\n. It looks like you're going down the same path I did in this issue: https://github.com/tonsky/datascript/issues/76\nAre you also trying to sync datoms from a datomic db to a datascript db? \nThe workaround I'm using is to separate the datoms with unique attributes and transact those (in map form, of course) first. Then take the tempids from the tx-report and reduce through the rest of the datoms, replacing tempids with the ids they resolved to and building up maps from the datoms. It looks something like (I haven't actually run this):\n``` clojure\n(def datoms [#datom[tid :extra \"value\" tx0 true]\n             #datom[tid :person/name \"foo\" tx0 true]])\n(let [{adds true retracts false} (group-by :added datoms)\n      {unique true rest-datoms false} (group-by unique-attr? adds)\n      ;; first transact unique ids resolve tempids,\n      ;; for our example, will look like [{:db/id tid :person/name \"foo\"}]\n      {:keys [tempids]} (d/transact! db-conn (map (fn [d]\n                                                    (assoc {:db/id (:e d)}\n                                                           (:a d) (:v d)))\n                                                  unique))]\n  ;; Replace resolved tempids in the 2nd transaction\n  ;; For our example, It will look something like [{:db/id 1 :extra \"value\"}]\n  (d/transact! db-conn (concat retracts\n                               (vals (reduce (fn [acc d]\n                                               (update acc (:e d) (fn [m]\n                                                                    (assoc m\n                                                                           :db/id (get tempids (:e d) (:e d))\n                                                                           (:a d) (:v d)))))\n                                             {} rest-datoms)))))\n```\nI think that will probably fail if there are more than 1 unique attrs for a single entity. \n. ",
    "levand": "@tonsky Your explanation of clause ordering and tempid resolution makes sense.  But do you plan to fix the issue that upserts always fail when in vector form, regardless of clause ordering?\nCurrently:\n(d/with db [{:db/id -1, :id \"foo\"}]) ;; succeeds\n(d/with db [[:db/add -1 :id \"foo\"]] ;; fails\nIs that the intended behavior?\n. It does fail, if :id is :db.unique/identity and there already exists a datom with the same attribute (in other words, the 1-clause transaction is just re-asserting something already in the DB). \nThe map form does an upsert, the vector form throws a uniquness exception.\nI'll see if I can get you a PR with a test case later today.\n. Test case: https://github.com/tonsky/datascript/pull/109\nThis works fine in Datomic.\n. This issue is a blocker for my work on Arachne; I am using spec heavily and will have to temporarily disable DataScript support until it works with Clojure 1.9.\n. Badass, thanks!\n. That should work... unfortunately not for me though, since I'd like the same query to work with both Datomic and DataScript.\nSo I probably overstated the problem. The primary issue isn't that it can't be done... there are definitely ways to code around it it. The actual problem is the fundamental incompatibility with Datomic caused by the lack of idents and reified schema.\nAnd maybe that's on me, for trying to treat them as more similar then they are. Perhaps sticking with one and dropping support for the other is the best. But I suspect lots of people are in more or less the same boat, too, since the API similarity is one of DataScript's attractive features to start with.\n. Well it's a long story, but to put it briefly: \nI'm working on Arachne, a modular application development framework for Clojure (https://www.kickstarter.com/projects/1346708779/arachne-rapid-web-development-for-clojure). The premise of Arachne's architecture is to build a rich model of your entire application, as much as possible, in a \"config database.\" I want to use Datomic (and/or DataScript) for many reasons, but mostly: (1) I want a database-as-a-value, (2) it's easy to work with from Clojure (3) it has a powerful and extensible schema, so each module can \nThe current implementation is actually an abstraction layer that works with both DataScript and Datomic, using the \"greatest common denominator\" of functionality and papering over some simple differences (like tempids.) For testing and validation, I actually have a multiplexing implementation that runs every operation against both Datomic and DataScript and asserts that all results are equivalent before returning.\nIt's essential that the code for all modules work with both Datomic and DataScript, because you want modules to be usable no matter what implementation the user chooses; I don't want to split into two separate sets of \"Datomic modules\" and \"DataScript modules.\"\nSo far, this has worked fine; I am restricted to the common denominator of functionality between the two, but that is sufficiently large that I haven't had many serious problems until now. But as I'm building a type/ontology system on top of the config model, to allow modules to create richer schemas, I find myself really needing to incorporate attributes into queries.\nSo I'm at a point where I could support either Datomic or DataScript pretty easily, but both only with some effort (at this point, it looks like that would involve walking query data to \"fix it up\" before passing to the actual implementation (gross)). \nThe problem is that if I pin on just one implementation, I lose one of two groups of people:\n- Those who will not write software that has a closed-source dependency.\n- Those who have an existing investment in Datomic and want Cognitect support and backing \nOn a technical level, too, I want to preserve Datomic compatibility since it's only using Datomic that we can actually persist configurations, and track them over time in a durable database. Not everyone needs that, but those who do really want it.\nSo I'm not quite sure what to do. I realize this probably doesn't help, but that's the outline of my current situation. (and why I'm making all these weird requests ;)\n. Actually, I just thought of one small-ish change that would also allow me to paper over this difference relatively easily.\nIf DataScript added support for symbol-based query functions (even only when running in the JVM), I could write an (attr-eid) function that did the right thing depending on the platform, and use that pervasively.\n. PR #178 enables a workaround, if it looks ok to you.\n. Yep, workaround is in place. Thanks!\n. ",
    "brandonbloom": "I know this issue has been closed, but I wanted to throw in my two cents regarding the design question here: Datomic's behavior makes a lot more sense to me: 1) order-independent id unification and 2) transaction before/after dbs rather than database function before/after dbs. As it is now, hashing order can cause random uniqueness failures if you try to batch add a few maps, where datomic will happily accept the transaction with last-written wins. If your data is internally consistent, such as the same entity twice in the result of a d/pull, then all the writes are the same.. This test fails, but passes if you swap the order of keys in the map. I'll try to see if I can fix it.\nclojure\n(deftest test-ref-upsert\n  (let [db (d/empty-db {:name {:db/unique :db.unique/identity}\n                        :ref {:db/valueType :db.type/ref}})]\n    (are [tx res] (= res (tdc/all-datoms (d/db-with db tx)))\n      [(array-map :ref {:name \"Ivan\"} :name \"Ivan\")]\n      #{[1 :name \"Ivan\"]\n        [1 :ref 1]}))). ",
    "motosota": "So I'm probably doing this wrong..\nconn and db1 are both defined as variables at the start of the application page.\nvar d = datascript;\nvar db1;\nvar conn;\n...\nI'm just using db1 as the single database used by the application\nconn is set after retrieving schema and initialising:\nconn = d.create_conn(schema);\ndb1 is set after each transaction:\ndb1 = d.db(conn);\nRender gets value of actual db (db1) directly:\nvar data = d.q('query our data here', db1);\n  React.render(\n    <MyApp data={data} />,\n    document.getElementById('content')\n  );\n}, false);\nSo in summary I'm viewing conn as something that doesn't change after creating the db, db1 is forced to change after each transaction - and db1 is then used directly for React queries\n. Thanks for that, makes a lot of sense.\nI've come up with a slightly different solution to the problem.\nThe background is that I'm listening to transaction changes in datomic, and pushing allowed datoms from that transaction to subscribed datascript clients.\nThis can sometimes result in new entities being created (which the client receives) referencing existing entities that the client doen't know about (because at the time it collected it's initial allowed datoms, that entity wasn't part of the allowed list).\nAn example would be a (new) service at an (existing) buliding - the client needs to know that it's missing the building information and then fetch it (since the creation of the service with a ref to the building now gives the client the right to know about the building).\nBased Rich Hickey's answer here: https://groups.google.com/forum/#!searchin/datomic/existence/datomic/jZYXqtB4ycY/sbfHvVm6P5oJ and based on the datomic documention which says \"Entities are not suitable for existence tests, which should typically be performed via lookup of a unique identity.\", I have decided to avoid existential angst..\nSo I am creating a uuid for all entities. My business rules will mean that a uuid can never be retracted.\nSo then in the client I will lookup all entities which have  a :service/entity reference to them, but do not have a :uuid attribute (using the \"not\" workaround in your tips and tricks wiki). That will identify my \"missing\" entities - the client can then request the datoms for those entities to get itself in-sync.\n. ",
    "pithyless": "or is just sugar syntax for rules, which are supported.\n(defn find-combinations [db selected-eid]\n ;; get all combinations of ?s1 and ?s2 where ?s is\n ;; equal to ?s1 or ?s2\n  (d/q '[:find ?s1 ?s2\n         :in $ % ?s\n         :where\n         [?s1 :service/uuid ?s1uuid]\n         [?s2 :service/uuid ?s2uuid]\n         [(not= ?s1uuid ?s2uuid)]\n         (matches ?s ?s1 ?s2)]\n       db\n       [[(matches ?s ?s1 ?s2)\n         [(= ?s ?s1)]]\n        [(matches ?s ?s1 ?s2)\n         [(= ?s ?s2)]]]\n       selected-eid))\nEDIT: looks like I posted too late :)\n. DataScript follows the Datomic api very closely, so the Datomic documentation is a good resource: http://docs.datomic.com/clojure/#datomic.api/touch\nThere is also a lot of good getting started material here: https://github.com/tonsky/datascript/wiki/Getting-started\nI'm sure @tonsky would appreciate if you ask API questions on gitter instead of opening up GitHub issues: https://gitter.im/tonsky/datascript\nEdit: I hope my original comment wasn't off-putting; I didn't mean to sound critical. To answer your question more directly, usually DataScript will just return a \"reference\" to an entity (e.g. #{1}). In order to get any attribute-values out, you will have to dereference the entity (by asking specifically for some attribute, e.g. (:name eid)). But sometimes you're just interested in all the values, so touch just recursively dereferences all attributes for you (e.g. (touch eid) => {:name \"...\", :age \"...\", ...})\n. ",
    "lvh": "I'm very interested in this too. I also agree it's out of scope, though :) I wonder if anyone else is working on something like this already? It seems like ferrying txns to the client shouldn't be terribly difficult with tools like sente...\n. Could you post the expansions as well? (Some sample data would also be helpful, basically something to easily reproduce the problem)\n. Sort-of. I have some code that wishes it could call both db-with and with with the same signature. I could see why you wouldn't want to take an unused argument though; but then perhaps you'd prefer a comment?\n. It's not so much that you would see one and not the other; just that if you don't know/forgot tx-meta is a transaction-level only thing (as opposed to tx-data, which affects the DB), that might be confusing since the two are obviously related fns; hence why I went for the comp first.\n. Having tried the comment, if this PR isn't something you think is reasonable then personally I think the comment is probably worse anyway, so I guess I'm closing this PR instead :)\n. There is also a deftype that does this (bitset), but that shouldn't matter for now; @puredanger has suggested on Clojure Slack shouldn't do that either, though.\nI have filed an issue with Clojure as well, since I'm not entirely sure who eventually resolves this and how: http://dev.clojure.org/jira/browse/CLJ-2020\n. Removing it entirely would be a performance regression on <1.9.0, right? What would be the impact of just renaming it?\n. Whoa, awesome! thanks @tonsky!\n. Gotcha. But I did understand correctly that datascript can only use those indexes if the data is really in EAVT form, right?\n(Unfortunately, I don't think I can actually get it in that form, because it's a pretty gnarly-shaped tree. Maybe one day.)\nThanks!. (Oh, and also: if I understood your comment correctly, datascript also can't accidentally do anything with a sorted set that's smarter than just having a vec, right?). Ah, I assumed something called (d/datoms db :eavt) or equivalent, hence needing EAVT-ish structure. How do I index a set of facts? (I am volunteering to write prose docs.). Cool, thanks. I tried that {:db.fn/add *facts*}, but that pegged my CPU at 100% for a while and eventually threw an exception so big Emacs crawled to a halt and I had to kill -9 it. What does that look like when you have a seq of facts, and not tx-data?\nDoes the pull API similarly only work with something in a db? Does it work even if it's not in EAVT form?. Cool. In that case, I think I've found a bug. Here's a sample repro:\n```clojure\n(def prefix [\"us\" \"il\" \"chicago\" \"get-gadget-attribute\"])\n(def db (->> (for [i (range 10)\n                   attr-name [:has-gizmo :color]\n                   :let [attr-val (case attr-name\n                                    :has-gizmo (even? i)\n                                    :color :red)\n                         ;; the argument passed to the REST service in my db\n                         args {:gadget-id i :attr-name attr-name}]]\n               [(conj prefix args attr-name attr-val)\n                (conj prefix args :gadget-id i)])\n             (apply concat)))\n(d/q '[:find ?id\n       :where\n       [?country ?state ?city \"get-gadget-attribute\" ?args :gadget-id ?id]\n       [?country ?state ?city \"get-gadget-attribute\" ?args :has-gizmo true]]\n     db)\n;; => #{[4] [6] [8] [0] [2]} as expected\n(d/q '[:find ?id\n       :where\n       [?country ?state ?city \"get-gadget-attribute\" ?args :gadget-id ?id]\n       [?country ?state ?city \"get-gadget-attribute\" ?args :has-gizmo false]]\n     db)\n;; => #{[7] [9] [3] [5] [1]} as expected\n(d/q '[:find ?id\n       :in $ %\n       :where\n       (gadget-attr ?id :has-gizmo false)]\n     db\n     '[[(gadget-attr ?id ?attr ?val)\n        [?country ?state ?city \"get-gadget-attribute\" ?args :gadget-id ?id]\n        [?country ?state ?city \"get-gadget-attribute\" ?args ?attr ?val]]])\n;; #{[4] [7] [6] [9] [3] [8] [0] [5] [2] [1]} ???\n(d/q '[:find ?id\n       :in $ %\n       :where\n       (gadget-attr ?id :has-gizmo true)]\n     db\n     '[[(gadget-attr ?id ?attr ?val)\n        [?country ?state ?city \"get-gadget-attribute\" ?args :gadget-id ?id]\n        [?country ?state ?city \"get-gadget-attribute\" ?args ?attr ?val]]])\n;; #{[4] [6] [8] [0] [2]} as expected\n```\nWould you like me to file a separate issue? I think the issue is with falsy values; something is checking for boolean truth and it really wants to check withnil?/some?.. There's some extra noise in there that isn't necessary, so I narrowed it down to the minimal repro:\n```clojure\n(def db [[1 :x true] [2 :x false]])\n(d/q '[:find ?id :where [?id :x true]] db)\n(d/q '[:find ?id :where [?id :x false]] db)\n(def rules '[[(prop ?id ?attr ?val) [?id ?attr ?val]]])\n(d/q '[:find ?id :in $ % :where (prop ?id :x true)] db rules)\n(d/q '[:find ?id :in $ % :where (prop ?id :x false)] db rules)\n```. Whoa, awesome. Thanks!. ",
    "sgrove": "@dwwoelfel and I have been working on this pretty hard recently, and hopefully should have something to show soon, perhaps an OSS demo today or tomorrow + blog post shortly thereafter.\nHere's a video of what it's like https://www.dropbox.com/s/20fms9czpoacg14/dato_todomvc.mp4?dl=0 and the code that's powering it (missing ~10 lines for the datomic schema though) https://gist.github.com/sgrove/8af86d66075639c431be\n. @saulshanabrook No, work is happening on Dato, with a big Datomic/DS \"webpeer\" change PR opened opened by @dwwoelfel. I need to update our internal app to use it, and DatodoMVC (I think Daniel's done this already though), and then merge it into master.\n. Should there be any kind of dev-time warning about value types that DS doesn't understand? It might be nice to highlight that these are \"advanced features\" and can be dangerous if the dev doesn't understand the difference between Datomic/DS.\n. This PR is probably good to go as is, warnings can go in a separate PR.\nThe warning would happen in dev at db instantiation time (or migration time, if DS ever supports that, as in #14 -  which would help a bit for Dato), and would just have a few cases where:\n1. ClojureScript/JavaScript doesn't have the type\n2. Round-tripping is impossible, even with e.g. transit-cljs  (e.g. doubles won't be serialized as such client-side)\nThis would just be to help people diagnose why the data they're getting from DS/transit isn't going into Datomic properly.\n. ",
    "fckt": "@sgrove can't w8 )\n. ",
    "saulshanabrook": "@sgrove Have you stopped work on that for the moment?\n. ",
    "thedavidmeister": "@sgrove is there any progress on this, or way to get at this outside of Dato?\n. @tonsky oh, is there a particular reason for that? it would be really helpful to support this.\n. @tonsky what would actually be involved in adding a few extra types? is it more than just doing a type check and maybe error on an attempted transact!?\n. @tonsky well no, not complications, you just said in #107 that you're looking for a use case.\nIf not schemas directly ported from datomic, some kind of validation at the db level would be great!\nMy use case is that:\n- I want to write some nicely named utility functions set-foo!, set-bar!, etc...\n- Those functions do some transactions under the hood\n- The functions are all working with the same type of data and the same bits of datoms in the db\n- I don't really want to have to sprinkle the same validation calls through all my utility functions, I'd rather just have the db tell me when I screw up\n. @tonsky thanks for the help! i'm not 100% sure how your option 4 is different pragmatically to my option 2? (which is what I'm trying at the moment, the default allocator works the way that I described my counter works, i think).\nThe problem I'm facing (that I think you would face too) is that I'm trying to do simple chronological sorts of elements in the UI by calling datoms and using the :avet index. This totally breaks down pretty quickly when dealing with removing the original eids in the backend storage, and as soon as you're expecting some other client to send you new datoms/entities, they may need to insert an eid halfway through your eids after latency, etc...\nThe chronological sort seems like a pretty common use case to me because that basically means that as users create things in the UI from top to bottom, things will continue to be laid out in the same order on the next page refresh. However, this might well end up being too naive and I'll need to add timestamps or weights as attributes to entities later.\nAnyway, I have a few questions to help me get my head around the problem:\n- Do you know of any other issues that going past the tx0 limit might cause, other than the auto increment?\n- I saw that some performance issues might arise (https://github.com/tonsky/datascript/issues/56), any idea how much of a concern this is in practise?\n. also, is there any way to hook into the way that DS chooses new entity IDs, either through next-eid or messing with the temp IDs through listen! or similar?\n. @wbrown-lg yeah i ended up doing a bit of \"translation\" of eids on datoms between client and backend db, just using a millisecond timestamp atm as for my use-case it currently only needs to be unique per-user.. I'm seeing this issue sporadically. Sometimes (on the same codebase) pseduo names will break and sometimes they work. No idea what triggers the breakage.. 1.9.229. i upgraded to 1.9.473 and it got worse.\nStarted seeing \"Lookup ref attribute should be marked as :db/unique: [:project/id nil]\" with pseudo names turned on.\nI'll try a higher version of cljs.. i can't upgrade any further until https://github.com/google/closure-compiler/issues/2336 lands. kk, i just successfully moved to 1.9.521 for cljs, which means google must have pushed a new release for the compiler.\ni'll let you know if i ever see this error again, thanks!. \ud83d\udc4d . the generated code it is complaining about looks like this:\ndatascript.parser.Query.prototype.cljs$core$IEquiv$_equiv$arity$2 = (function (this39772,other39773){\nvar self__ = this;\nvar this39772__$1 = this;\nreturn (!((other39773 == null))) && ((this39772__$1.constructor === other39773.constructor)) && (cljs.core._EQ_.cljs$core$IFn$_invoke$arity$2(this39772__$1.find,other39773.find)) && (cljs.core._EQ_.cljs$core$IFn$_invoke$arity$2(this39772__$1.with,other39773.with)) && (cljs.core._EQ_.cljs$core$IFn$_invoke$arity$2(this39772__$1.in,other39773.in)) && (cljs.core._EQ_.cljs$core$IFn$_invoke$arity$2(this39772__$1.where,other39773.where)) && (cljs.core._EQ_.cljs$core$IFn$_invoke$arity$2(this39772__$1.__extmap,other39773.__extmap));\n});. great, thanks!. ",
    "mprokopov": "Great! \nlein clean\ndid solve the problem.\n. ",
    "vvvvalvalval": "Yes I am. \nThis thread led me to think keywords would be faster to check for equality - but I believe you, you've probably been further down this road than me.\n. Thanks! Maybe I could start a FAQ page in the Wiki for this sort of stuff?\n. ",
    "milt": "1.7.48 also bumps the closure deps, so that might also be something to look at.\n. Awesome, thanks!\n. ",
    "CodingAgainstChaos": "The transit issue was it! When I did (d/entity @conn [:item/id (transit/uuid \"27c1f4e4-ce58-4a46-9c36-d9bd1f573309\")]) it worked.\nI'll try to recreate the nil issue in a project and share it with you. I don't have nulls in my DB. \nDeps\n[org.clojure/clojure \"1.7.0\"]\n[org.clojure/clojurescript \"0.0-3308\"]\n[datascript \"0.11.6\"]\nThis is in dev. No optimizations. I'm also using fig wheel.\nThanks for the help and this project!\n. @tonsky Took me a little while to get around to it but here's a project with an example of the nil problem\nhttps://github.com/MattParker89/ds-nil\n. ",
    "jaen": "Tak a look at https://github.com/tonsky/datascript#project-status and this part in particular:\n\nInterface differences:\n- Custom query functions and aggregates should be passed as source instead of being referenced by symbol (due to lack of resolve in CLJS)\n\nso I just think that's a limitation of how datascript & Clojurescript are written. Not sure if now that cljs is self-hosted this can be worked around or not.\nYou'd have to do something like\n(d/q '[:find ?r ?subs :where [(?subs \"abc123\" 0 3) ?r]] subs)\nif I recall correctly.\n. ",
    "taylorSando": "This seems to be related to #111 \nI had worked around it like this:\nclojure\n(clojure.walk/prewalk (fn [form]\n  (if (instance? com.cognitect.transit.types.UUID form)\n     (cljs.core/uuid (str form))                                        \n    form)) value-returned-from-reader)\nBut the handler solution is easier.\n. ",
    "rhacker": "@tonsky That is exactly what I'm talking about. I'm very new to datomic and datascript.\nThanks \n. ",
    "DomKM": ":string/locale/cn is an invalid keyword literal. (keyword \"foo/bar/baz\") creates a keyword but :foo/bar/baz is not a valid keyword literal. Stick with valid keywords since behavior that has to do with invalid keywords is not guaranteed. I would change your locale keyword to :string.locale/cn.\n. ",
    "bnomis": "Thanks\n. ",
    "petterik": "Sweet! Thanks!\n. @shparun showed that if a type hint is added to the ^:declared defn, then it won't throw NoSuchMethodException.\nI added a case showing that the ^:declared defn only needs to match primitive type hints:\nhttps://github.com/petterik/datascript_0.16.1_declared_bug/commit/8143a6248f8a8159c680ea21a853ae3a551aa423\nI went through all the ^:declared defns and type hinted the ones that had primitive \"real\" defns to create #230. Getting entities by lookup ref also fails when compiling with direct-linking true.\nTest throws assertion error:\nclj\n(defn get-unique-by-value-test-case []\n  (let [conn (d/create-conn {:uni {:db/unique :db.unique/identity}})]\n    (d/transact conn [{:uni :foo}])\n    (assert (some? (:db/id (d/entity (d/db conn) [:uni :foo]))))))\nDoes not fail with direct-linking and datascript 0.15.5.\nDoes not fail with datascript 0.16.1 and direct-linking false.\nFails with datascript 0.16.1 and direct-linking true.\nVerified that my pull-request #230 does not fix this issue.\nCommit for the new test case:\nhttps://github.com/petterik/datascript_0.16.1_declared_bug/commit/6467514dd5ada8a33993bf453bea5c7afe073197. Does not entirely fix #219. Closing.. ",
    "ccorcos": "Interesting. Well Datomic is closed source I'm just realizing. It would be sweet if this were just a seamless client side cache / pub/sub for datomic... This isn't meant to be a fully fledged database is it?\n. awesome! Just curious, what are types of queries or systems that datomic is necessarily not so good at?\n. well the whole point is that its not in-place... lol\n. ",
    "jbsar": "Thanks for your eye-opening answer!\nI guess I will have to find another solution to hold the state of my application. Do you think there's a chance of drastic performance improvements happening (especially in CLJS)?\n. ",
    "vibl": "I presented an idea that might speed up queries: https://github.com/tonsky/datascript/issues/132\nIt'd be great if you could give some feedback on it. It looks too easy to be right...\nVianney\n. All this performance data begs a question: could there be better performing algorithms for Datascript? Some that perform in O(log n) rather than O(n) or worse?\n. @lsdafjklsd Which part of this idea sounds like which part of Om Next?\n. Ok, I guess this idea is so stupid nobody even bothers saying it is. :-)\nFair enough!\nVianney\n. Thanks @laforge49!\nYes, it's understandable that people don't have the time to test every idea out there, and don't want to express an opinion until they have done so.\nI was so sure that my idea had a big flaw I hadn't seen that I thought it would be pointed out by an experienced developer.\n. The entity involved in the transaction could be pulled from the Entities index (one lookup) after the index has been updated with this transaction (so that the pulled entity contains the new data).\nAll the \"delta queries\" (i.e., queries without entity joins) could then be run against this one entity, which would be very cheap.\nThen, the query results (if any) could be merged into the corresponding views.\nIn your example, let's say the pulled entity is :\n{:db/id      788\n :name       \"Ivan\"\n :last-name  \"Petrov\"\n :sex        :male\n :age        15\n :salary     50000 }\nThe query result would be:\n{:db/id      788\n :last-name  \"Petrov\"\n :age        15 }\nWhich, merged into the view corresponding to this query, would update it pretty efficiently.\nIf the pulled entity's :name is not \"Ivan\" or its :sex is not :male, then there would be nothing to update.\nIt's true that this kind of \"delta queries\" would also work with Datascript as is. On the other hand, the live queries that are not \"delta\" (the one with entity joins) might not be possible to implement in an efficient way with Datascript according to this thread: #12.\nWhat do you think? \nVianney\n. When I write \"pulling an entity\", I mean \"querying the database to extract the data of an entity\". Indeed, this has nothing to do with caching query results.\n. > What do you suggest we should do next? How to insert it into the query?\nI don't understand your question. Can't \"delta queries\" be executed against a single entity like I described? And if so, don't you agree that the result of this can be merged into a view to update it efficiently?\nAs I wrote earlier, the \"delta queries\" you asked about are not the difficult part here (they can already be implemented in Datascript as is) so we should indeed probably move on to discuss the map-based indexes I suggested. If they can serve queries in a matter of microseconds (as opposed to milliseconds), then they would make live queries possible: any query could be executed again each time the index is updated, because it would be so cheap to do so. Do you have questions about this part of my proposal?\nVianney\n. You're right, I shouldn't have mentioned \"delta queries\" at all in my proposal (there are only two sentences about them btw) or in my messages. They are not important if we can implement a much faster version of Datascript (they would be if we can't) . Can we forget about them and move on to discuss the nested map indexes I suggested?\nHere I rewrote my idea focusing on just nested map indexes to simplify the reading:\nNested map indexes\nIndexes would be nested Clojure maps with three levels. For example, the AVE index: {:attributes {values {eids entities}}}.\nThey would be highly denormalized in order to minimize lookups: entities would be replicated as maps of {:attributes values} everywhere in the index, apart in type/ref which would only reference eids (to avoid infinite loops in joins). Eids would be referenced as an attribute in the AVE index.\nSnapshots of indexes could be stored locally and on the server to speed-up application start-up, i.e. to avoid recomputing indexes from the datom store each time the application is started. They could also be used for an undo-redo feature.\nHow much faster?\nThe Datascript benchmark1 includes a query (q4) that runs in 40 milliseconds for 20000 entities (see #130):\n[:find ?e ?l ?a :where [?e :name \"Ivan\"]\n                                     [?e :last-name ?l]\n                                     [?e :age ?a]\n                                     [?e :sex :male]]\nLet's imagine how this query would run on map-based indexes:\n1. In the AVE index, look for the :name attribute, then look for the \"Ivan\" value and return the map of entities.\n2. In the AVE index, look for the :sex attribute, then look for the :male value and return the map of entities.\n3. Intersect these two maps.\n4. Keep only the selected attributes (eid, :last-name and :age) in the result.\nThis query would probably take no more than 4-40 microseconds, even on a large dataset (cf. benchmarks of CLJ and CLJS maps lookups).\nComments are welcome!\nI'm not experienced enough in Clojure to hack together a proof of concept (with Datalog and Pull queries) yet. I should be in a few weeks/months though. Please keep me in the loop if you're considering working on it.\nI am quite new to this community and I don't even know if it is useful to present an idea like this as a suggestion without any code. I probably have overlooked things.\nWhat do you think?\nVianney\n. Glad to see that I added useful \"noise\".\n. ",
    "mhuebert": "I've been gradually replacing my queries with index lookups and seeing orders-of-magnitude speedups.\nEg. this recursive pull query was taking ~150 milliseconds with my dataset:\nclj\n(d/q '[:find (pull ?e [:uuid :node/label :node/ui-selected :node/order :node/cell :ui-collapsed {:node/_up ...}]) .\n                       :in $ ?uuid :where [?e :uuid ?uuid]] @db uuid)\nI replaced it with this function and the time dropped to ~0.1 milliseconds:\nclj\n(defn get-node-tree [eid]\n  (-> (select-keys (d/entity @db eid) [:uuid :node/label :node/ui-selected :node/order :node/cell :ui-collapsed])\n      (assoc :node/_up (map (comp get-node-tree first) (d/datoms @db :avet :node/up eid)))))\n. ",
    "EugenDueck": "@mhuebert That is a huge performance gain! It seems this would be one of the probably many cases where a query optimizer (if something like that already exists in datascript - I'm just starting to look into the code) would shine. Once optimized, repeated invocations of the same query would exhibit hand-optimized performance.. Here's one such case, using version 0.15.5.\nFirst a query that works:\n```\ndatascript-test.core=> (ds/q '[:find ?e ?a ?v :where [?e ?a ?v]] ds-source)\n{[\"a\" \"x\" 1234]}\nAnd now a similar query that does not work, involving a `pull` - where the problem seems to arise:\ndatascript-test.core=> (ds/q '[:find (pull ?e [*]) :where [?e]] ds-source)\nExceptionInfo Expected number or lookup ref for entity id, got \"a\"  clojure.core/ex-info (core.clj:4617)\ndatascript-test.core=> (pprint *e)\nerror {\n:cause \"Expected number or lookup ref for entity id, got \\\"a\\\"\"\n :data {:error :entity-id/syntax, :entity-id \"a\"}\n :via\n [{:type clojure.lang.ExceptionInfo\n   :message \"Expected number or lookup ref for entity id, got \\\"a\\\"\"\n   :data {:error :entity-id/syntax, :entity-id \"a\"}\n   :at [clojure.core$ex_info invokeStatic \"core.clj\" 4617]}]\n :trace\n [[clojure.core$ex_info invokeStatic \"core.clj\" 4617]\n  [clojure.core$ex_info invoke \"core.clj\" 4617]\n  [datascript.db$entid invokeStatic \"db.cljc\" 748]\n  [datascript.db$entid invoke \"db.cljc\" 727]\n  [datascript.db$entid_strict invokeStatic \"db.cljc\" 754]\n  [datascript.db$entid_strict invoke \"db.cljc\" 753]\n  [datascript.pull_api$pull_spec$fn__1307 invoke \"pull_api.cljc\" 268]\n  [clojure.core$map$fn__4781$fn__4782 invoke \"core.clj\" 2633]\n  [clojure.lang.PersistentVector reduce \"PersistentVector.java\" 341]\n  [clojure.core$transduce invokeStatic \"core.clj\" 6600]\n  [clojure.core$into invokeStatic \"core.clj\" 6614]\n  [clojure.core$into invoke \"core.clj\" 6604]\n  [datascript.pull_api$pull_spec invokeStatic \"pull_api.cljc\" 268]\n  [datascript.pull_api$pull_spec invoke \"pull_api.cljc\" 266]\n  [datascript.query$pull$iter__3075__3079$fn__3080$fn__3086 invoke \"query.cljc\" 740]\n  [clojure.core$map$fn__4789 invoke \"core.clj\" 2651]\n  [clojure.lang.LazySeq sval \"LazySeq.java\" 40]\n  [clojure.lang.LazySeq seq \"LazySeq.java\" 49]\n  [clojure.lang.RT seq \"RT.java\" 521]\n  [clojure.core$seq__4357 invokeStatic \"core.clj\" 137]\n  [clojure.core.protocols$seq_reduce invokeStatic \"protocols.clj\" 24]\n  [clojure.core.protocols$fn__6738 invokeStatic \"protocols.clj\" 75]\n  [clojure.core.protocols$fn__6738 invoke \"protocols.clj\" 75]\n  [clojure.core.protocols$fn__6684$G__6679__6697 invoke \"protocols.clj\" 13]\n  [clojure.core$reduce invokeStatic \"core.clj\" 6545]\n  [clojure.core$into invokeStatic \"core.clj\" 6610]\n  [clojure.core$mapv invokeStatic \"core.clj\" 6618]\n  [clojure.core$mapv invoke \"core.clj\" 6618]\n  [datascript.query$pull$iter__3075__3079$fn__3080 invoke \"query.cljc\" 737]\n  [clojure.lang.LazySeq sval \"LazySeq.java\" 40]\n  [clojure.lang.LazySeq seq \"LazySeq.java\" 49]\n  [clojure.lang.RT seq \"RT.java\" 521]\n  ...\nHere's the code preceding above queries. I removed everything not involved in this particular case, including stuff actually necessary for most other queries, like indexed access etc.\n(require [datascript.core :as ds])\n(require [datascript.db :as dsdb])\n(deftype AnyOldMapToDatascriptSource [data]\n        dsdb/IDB\n    dsdb/ISearch\n    (-search [db [p-e p-a p-v :as pattern]]\n      (->> data\n           ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n           ;; filter entities\n           ((fn e-filter [data]\n              (if (nil? p-e)\n                data\n                (list [p-e (data p-e)]))))\n           ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n           ;; filter attributes\n           ((fn [data] (if (nil? p-a)\n                         data\n                         (remove nil?\n                                 (map (fn attr-filter-mapper [[k a-v-map]]\n                                        (if-let [v (a-v-map p-a)]\n                                          (if (or (nil? p-v) (= p-v v))\n                                            [k {p-a v}])))\n                                      data)))))\n           ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n           ;; format for datascript\n           (mapcat (fn mapcatter [[e m]] (map (fn datomizer [[a v]] (dsdb/datom e a v 1 true)) m)))))\n\n    dsdb/IIndexAccess\n  )\n\n(def data { \"a\" { \"x\" 1234 }})\n(def ds-source (AnyOldMapToDatascriptSource. data))\n``. I haven't looked at all that's involved here yet, but from an outsider's perspective I'd say (repeating myself): Why arbitrarily restrict the type? I haven't checked if it breaks something, but not raising an exception in theentidfunction fixes this case for me. If my change would make pull and entity slower, I'd of course understand thatentidwants to enforcenumber?.. I see. I guess that makes sense.pullhas to pull from some db, whereas the find clause can just return the bound variables.. Otoh,pull` could internally be provided with the input that the entity id in question comes from. That's how datomic seems to do it, where the grammar doesn't even allow specifying the input in a pull expression.\nThe following works in datomic:\n(dt/q '[\n  :find (pull ?e [*]) (pull ?f [*])\n  :in $a $b\n  :where [$a ?e :order/id] [$b ?f :account/user-name]]\n  (db dt-conn) (db dt-conn))\nNot sure though if datascript strives to be as close to datomic's specs as possible, so I'll just leave this issue closed.. ",
    "haywoood": "Wouldn't this be re-inventing the wheel a bit? Sounds like om.next? \n. ",
    "laforge49": "I think it likely that at some point alternatives to the current\nimplementations of datascript/om.next will be developed. Things tend to\nevolve rapidly. I've contributed to open source for years and not getting a\nresponse to an idea is more common than not.\nDon't despair. Fork & play. Your depth of understanding will increase\nmanifold.\n. ",
    "j-pb": "@vibl\nNothing of this is new,it's called incremental view maintenance in database speak.\nIf you're interested I could share my folder with papers and books on this. If you look at the most basic way to evaluate datalog programs (semi-naive bottom up), you'll see that it in fact already implements such an algorithm as it iterates the query over old and newly generated facts (which are often denoted as the delta) until a fixpoint is reached.\nIt becomes somewhat tricky however as facts are removed.\nI think Datascript/Datomic already implements a system very similar to what you describe.\nSo in order to not reinvent the wheel I'd highly recommend a quick google for scientific papers\nand books as you come up with ideas :), then you can jumpstart off the stuff people tried already and don't have to repeat their failures.\nIf you want to follow the stream approach for example you will find that there has been some work done already..\nAlso I'd like to know where you get your timing estimates from (no more than 4-40 microseconds) ;P.\n@tonsky \nEven though this proposal looks somewhat naive, with the proposed implementation just adding noise so far, the general gist of it is correct I think.\nDatascript could profit immensely from materialised views and their incremental maintenance.\nQueries that contain quite a few joins and recursion can have a somewhat gnarly runtime, however when evaluated incrementally their cost can be amortised quite nicely, because the fact-deltas (the sets of added and removed facts within one transaction) are typically small and a join of a X b can be written as (\u2206 aX b) U (a X \u2206b).\nAdditionally systems like reagent would profit immensely from the ability to specify reactive queries, that get reevaluated as new facts come in. Note that this doesn't have to be eager as you mentioned earlier, it is possible to cash the old query result, while collecting the delta sets, until the materialised query is actually requested (on request-animation-frame or similar), at which point the incremental evaluation algorithm will be run. And best of all we get this basically for free with a datoms tx (even though it's somewhat unfortunate that datascript currently doesn't keep retractions).\nPosh tries to solve this problem by providing a query (pattern) to query whether a query needs updating (yo dawg!), but imho this makes matters worse, because now one has to worry about two parts that might go out of sync. A datascript query is already a perfectly fine source of information on when to update itself.\nOn a additional note, I think this thread shows, that it would be immensely useful to place datascript on a rock-solid theoretical foundation. Not only would it allow us to harvest the years of research done in this field more easily, but it would also give us some common terminology when talking about concepts.\nCheers, JP\n. ",
    "Conaws": "@j-pb I'd love your folder of papers @vibl What's your email?  A friend and I are looking into a way to sync multiple datascript dbs together with Horizon or some other kv store, some stuff here I think might apply. \n. Any word on this?  Pretty critical for something I'm building -- happy to help try to fix it if you've got any guidance.\n. Another strange thing I'm finding\nWhen I say something like \n{:db/id -4\n                   :user/name \"Stu\"\n                   :user/can.teach [{:interest/title \"Datomic\"}\n                                    {:interest/title \"Homeschooling\"}\n                                    {:interest/title \"Clojure\"}\n                                    {:interest/title \"C.S. Lewis\"}\n                                    ]}\nthen when I query for user/interests all of those items also show up, but when I query for user/wants.to.learn they don't\nSo it seems like user/interests has become a superset of all the references to things with a :interest/title \nquite confused as to how this is happening. Well, nevermind all this -- restarting figwheel cleaned it all up. ",
    "Jumblemuddle": "Has there been any more thought put into this recently?. @carocad, I've been starting to wonder if something like this would be good enough. It's way simpler, and I don't think the performance overhead of re-running all active queries upon a transaction will be too impactful with the amount of data in an average application.\nHave you been using this approach on any large projects?. > I'll beg to differ. For small apps, this may be fine. But I don't know why you'd want to hem yourself in this way. Too many queries or too much data and you're going to wish that you had Posh's tx-pattern\nI think the question is how small does the app have to be for it to not matter. My guess would be that quite a few apps simply wouldn't notice the performance impact. Often times, re-running all Datascript queries is 'fast enough'\u2122.\nAs @carocad stated, simplicity is a feature. I've messed with Posh in the past, but I ended up not liking it due to some of the edge cases I ran into. I don't want to add another layer of complexity to my app.. Oh, apparently :default can be used in a different way than in Datomic.\n(d/pull @conn '[:alpha [:default :bravo 0]] 1)\nIs that documented somewhere? I just got it from the source.. Thanks, @tonsky!. ",
    "bhurlow": "Hi @j-pb, could I also request your email to ask for these papers? would love to read  . I'd also love to use datascript with lumo. @djwhitt's first pass branch worked for me without a hitch  . ",
    "hlship": "Datascript 0.1.13\n. That worked, thanks. Still pinning down when a symbol is bound or unbound, I guess.\n. ",
    "levitanong": "@zilti In the readme, in the Project Status section, it says:\n\"No docs at the moment, use examples & Datomic documentation.\"\nSo you can use the datomic documentation API. There is a separate section that details how different the datascript api is from datomic's.\n. ",
    "aadrian": "+1 for JS docs.\n. ",
    "typeetfunc": "\nthis is a bad idea, you\u2019ll have to do a lot of repetitive manual conversions to bridge with cljs api from js.\n\n@tonsky \nhttps://github.com/typeetfunc/datascript-mori/blob/master/release-js/test/onlyCljsApiUsage.spec.js\nExample of usage only CLJS API from JS without any conversions(except mori.parse for parsing EDN string). \nIn real-world JS app (in particular React App) you forced to use immutable data structures(otherwise your application will very slow). When using DataScript JS API, you need to convert result of query(q, pull etc.) from JS data structures to immutable data structures(Immutable.js, Mori, etc.) - is not good for perfomance and code cleanliness.\nIt is also possible to combine CLJS API(for quering) and JS API(for other) - see example\n. @tonsky Are there any reasons to use conversion clj->js? Major part of javascript community uses immutable data-structure(for speeding up the React rendering). \n. @comnik https://github.com/typeetfunc/datascript-mori. See examples - usage of only CLJS API and usage of combination JS API and CLJS API\nAlso, I have wrote babel-plugin for precompiling EDN string to mori structures(for syntax check and minimize runtime overhead)\n. > what is the use case for transforming a Datalog query into a Mori structure?\n@kristianmandrup I add some motivation in Readme\n@comnik Great explanation! If you have questions about datascript-mori - let me know.\n. @metasoarous What do you think about stream(or property) interface(RxJS) for connection? See rxjs datascript integration example (WIP version)\nThere are many good reactive library(Rx, Most, Kefir etc). Integration Datascript with any of these very simple.\n. @terinjokes \nhttps://tonicdev.com/parabellum06-94/56f396a75d25f21100563b7d\nIn schema all string transformed to keyword.\nHowever, string in entities are not transformed.\n\":my/tid\" !== :my/tid\nJust do not use keyword-like string as attributes.\n. ",
    "abrooks": "I've created issue #141 to track this as an issue. I probably should have filed an issue first. Let me know what the preferred procedure is for future issues/pull-requests.\nThanks!\n. Thanks for a great project \u2014 I hope to contribute more in the future!\n. ",
    "visibletrap": ":+1: \n. ",
    "brunobraga95": "have you found a solution for this?. ",
    "hden": "Works pretty well for me, see this gist. Do you have a minimum sample for your code?\n. ",
    "venturecommunism": "I think what you want to do is currying of react containers (higher order components) and passing down the connection as a prop with a subscribe method (rx observable) (for reads) along with a map of actions / event handlers (for writes).\nIf it can work in this architecture, it advances it significantly down the path to standalone clients with all the functionality of meteor.\nhttps://kadirahq.github.io/mantra/#sec-State-Management\nhttps://kadirahq.github.io/mantra/#sec-Containers\nhttps://github.com/kadirahq/react-komposer#using-with-redux\nhttps://github.com/kadirahq/react-komposer#using-with-rxjs-observables\nRight now the rx-datascript example gives me a datascript parse error on datascript-mori's vector.\nI think if I could get a query as an rx observable I could emulate the Om.Next architecture in pure javascript with Mantra which is essentially similar.\n. This is basically what I was talking about:\nhttps://github.com/venturecommunism/stack/blob/6ab35c5f8d9bd16032dd81b7ababa96df73b1fe9/client/src/configs/context.js\nand then we can transact inside commands like:\nhttps://github.com/venturecommunism/stack/blob/6ab35c5f8d9bd16032dd81b7ababa96df73b1fe9/client/src/modules/cartscape/commands/togglefullscreen.js\npure javascript / react\n. ",
    "neverfox": "I'm having a similar problem with a recursive ancestor rule. The following query never returns, but if I leave off the last anc clause it returns almost instantly (though that does me no good because it cannot be guaranteed to be right). Also , if I'm dealing <= 4 tokens, the query is fast even if complete (i.e. all necesary anc clauses). I don't know why it would suddenly get hosed at 5 tokens since by the time it gets to that last  clause, shouldn't it have narrowed down the range of possible bindings for ?p?\n```\n(def schema\n  {:child      {:db/valueType   :db.type/ref\n                :db/cardinality :db.cardinality/many\n                :db/isComponent true\n                :db/index       true}\n   :token/text {:db/index true}})\n(def rules\n  '[[(anc ?par ?child)\n     (?par :child ?child)]\n    [(anc ?anc ?child)\n     (?par :child ?child)\n     (anc ?anc ?par)]])\n(d/q '[:find ?p\n            :in $ %\n            :where\n            [?t0 :token/text \"central\"]\n            [?t1 :token/text \"square\"]\n            [?t2 :token/text \"of\"]\n            [?t3 :token/text \"Djemma\"]\n            [?t4 :token/text \"El-Fna\"]\n            [?t0 :token/index ?i0]\n            [?t1 :token/index ?i1]\n            [?t2 :token/index ?i2]\n            [?t3 :token/index ?i3]\n            [?t4 :token/index ?i4]\n            [(- ?i1 ?i0) ?diff0]\n            [(= ?diff0 1)]\n            [(- ?i2 ?i1) ?diff1]\n            [(= ?diff1 1)]\n            [(- ?i3 ?i2) ?diff2]\n            [(= ?diff2 1)]\n            [(- ?i4 ?i3) ?diff3]\n            [(= ?diff3 1)]\n            [?p :sentence/index _]\n            (anc ?p ?t0)\n            (anc ?p ?t1)\n            (anc ?p ?t2)\n            (anc ?p ?t3)\n            (anc ?p ?t4)]\n          @conn rules)\n```\nConsider the following data:\n[{:sentence/index 0\n                      :child [{:parse/tag \"ROOT\"\n                               :child [{:parse/tag \"S\"\n                                        :child [{:parse/tag \"NP\"\n                                                 :child [{:token/index 0\n                                                          :token/text \"central\"}\n                                                         {:token/index 1\n                                                          :token/text \"square\"}\n                                                         {:token/index 2\n                                                          :token/text \"of\"}\n                                                         {:token/index 3\n                                                          :token/text \"Djemma\"}\n                                                         {:token/index 4\n                                                          :token/text \"El-Fna\"}]}]}]}]}\n {:sentence/index 1\n                      :child [{:parse/tag \"ROOT\"\n                               :child [{:parse/tag \"S\"\n                                        :child [{:parse/tag \"NP\"\n                                                 :child [{:token/index 0\n                                                          :token/text \"central\"}\n                                                         {:token/index 1\n                                                          :token/text \"square\"}\n                                                         {:token/index 2\n                                                          :token/text \"of\"}\n                                                         {:token/index 3\n                                                          :token/text \"Foo\"}\n                                                         {:token/index 4\n                                                          :token/text \"El-Fna\"}]}]}]}]}]\nIf you try the query above, it will run with anc clauses through ?t1 but hangs if you add ?t2 or beyond.\nThis hangs as you might expect:\n(d/q '[:find ?t0 ?t1 ?t2\n                                  :in $ %\n                                  :where\n                                  [?p :sentence/index _]\n                                  (anc ?p ?t0)\n                                  (anc ?p ?t1)\n                                  (anc ?p ?t2)]\n                                @conn rules)\nSo that suggests to me that the bindings are not narrowing from the top down and the hang is because those anc clauses are running independently (to be bound after it completes).. Wait, is this because, like Datomic, variables need binding at invocation time through the use of [ ] in the rule definition?. I see. Thanks. If you do get a chance, I would appreciate your insight on my example. To get around it, I basically had to run the query without\n[?p :sentence/index _]\n(anc ?p ?t0)\n(anc ?p ?t1)\n(anc ?p ?t2)\n(anc ?p ?t3)\n(anc ?p ?t4)\nreturning tuples of ?t bindings and then loop through the set of tuples using the above as the where clause. It just doesn't like doing it all at once.. ",
    "zilti": "Well, this is embarassing. I forgot setting the type to rel. That's what I get for late-night coding...\n. ",
    "gfZeng": "I think the enumerators use db/ident better than keyword.\nwell, the db/ident support additional attribute,\nand also can look up entity via keyword\n. How about this way\nclojure\n(let [conn (d/create-conn {})]\n    (d/transact! conn [{:db/id -1\n                        :name  \"Maksim\"\n                        :age   45\n                        :sym   'foo}])\n    (prn\n     (d/q '[:find  ?n ?a\n            :in $ ?sym\n            :where\n            [?e :sym ?sym]\n            [?e :name ?n]\n            [?e :age  ?a]]\n          @conn 'foo)))\n. ",
    "ncalexan": "@tonsky you're welcome!  We love DataScript and have learned a lot from it.  We (a team at Mozilla) are starting a small project to build a similar Datomic-API-alike over at https://github.com/mozilla/datomish.  Early days yet but we'd love to have your thoughts.\n. @tonsky sorry about that -- didn't realize we hadn't opened it up yet.  I've added you as a collaborator.\n. ",
    "wbrown-lg": "Monkey-patching cmp-num with:\nclojure\n(defn cmp-num [n1 n2]\n  (if (and n1 n2)\n    (if (= n1 n2)\n      0\n      (if (< n1 n2)\n        -1\n        1))\n    0))\n... appears to solve this issue, and with negligible performance impact.  Am I missing something here? \n. A faster version here:\nclojure\n(defn cmp-num [n1 n2]\n  (if (and n1 n2)\n    (let [n1 (long n1)\n          n2 (long n2)]\n      (if (= n1 n2)\n        0\n        (if (< n1 n2)\n          -1\n          1)))\n    0))\nReduced ingest time from about 25000ms to 22000ms on 1.2mm Datoms.\n. @tonsky Yeah, this is the JVM version.  It's the interaction between cmp-num using subtraction, TilSort and Long.\nSome interesting discussion about TimSort and contracts -- http://stackoverflow.com/questions/19325256/java-lang-illegalargumentexception-comparison-method-violates-its-general-contr\nI've been using :db/id numbers derived from the cryptographic identity of the facts that I am putting in the Datascript database, and allows me to use raw Datoms between Datascript instances.  This gives me about 63-bits of identity.\nClojure\n(defn insert-identity\n  [m]\n  (let [signed-identity (hash->long (hash-object (murmur3-128 1234) nippy-funnel\n                                                 (dissoc m :db/id)))\n        unsigned-identity (if (bit-test signed-identity 63)\n                            (bit-not signed-identity)\n                            signed-identity)]\n    (assoc m :db/id unsigned-identity)))\n. @tonsky That's funny and explains a lot!  Thanks for fixing this, as this means that I can remove my monkey-patched function.  Trying it.\n. @tonsky The test case that I have above works.\nI also tested it by creating large DataScript DBs, dumping it out to Datoms, and then initializng the database two different ways.  So my test covered three ways to create a database:\n- Vector of maps with large long for :db/id, which worked before this fix.  (d/db-with (d/init-db [] schema) fs)\n- Vector of dumped Datoms, using (d/db-with (d/init-db [] schema) ds) which worked and was faster than working with maps.\n- Vector of dumped Datoms, using (d/init-db ds schema), which the failing case fell within.\nBelow shows the logs that indicate the performance of each type.\n16-07-14 12:53:20 nephelai.local DEBUG [lg-databroker.util:?] - [11128ms] Transformed 149994  facts into DS DB with 645518 datoms\n16-07-14 12:53:20 nephelai.local DEBUG [lg-databroker.util:?] - [119ms] Transformed DB into datoms\n16-07-14 12:53:26 nephelai.local DEBUG [lg-databroker.util:?] - [5994ms] Inited DB by transacting datoms onto empty db\n16-07-14 12:53:27 nephelai.local DEBUG [lg-databroker.util:?] - [1034ms] Inited DB with datoms\n16-07-14 12:53:28 nephelai.local DEBUG [lg-databroker.util:?] - [330ms] DB comparisons came out true\n16-07-14 12:53:49 nephelai.local DEBUG [lg-databroker.util:?] - [11016ms] Transformed 150000  facts into DS DB with 647398 datoms\n16-07-14 12:53:49 nephelai.local DEBUG [lg-databroker.util:?] - [94ms] Transformed DB into datoms\n16-07-14 12:53:55 nephelai.local DEBUG [lg-databroker.util:?] - [6117ms] Inited DB by transacting datoms onto empty db\n16-07-14 12:53:56 nephelai.local DEBUG [lg-databroker.util:?] - [840ms] Inited DB with datoms\n16-07-14 12:53:56 nephelai.local DEBUG [lg-databroker.util:?] - [236ms] DB comparisons came out true\n16-07-14 12:54:14 nephelai.local DEBUG [lg-databroker.util:?] - [10968ms] Transformed 150000  facts into DS DB with 645122 datoms\n16-07-14 12:54:14 nephelai.local DEBUG [lg-databroker.util:?] - [96ms] Transformed DB into datoms\n16-07-14 12:54:20 nephelai.local DEBUG [lg-databroker.util:?] - [6113ms] Inited DB by transacting datoms onto empty db\n16-07-14 12:54:21 nephelai.local DEBUG [lg-databroker.util:?] - [954ms] Inited DB with datoms\n16-07-14 12:54:21 nephelai.local DEBUG [lg-databroker.util:?] - [302ms] DB comparisons came out true\n16-07-14 12:54:38 nephelai.local DEBUG [lg-databroker.util:?] - [10862ms] Transformed 150000  facts into DS DB with 646740 datoms\n16-07-14 12:54:39 nephelai.local DEBUG [lg-databroker.util:?] - [89ms] Transformed DB into datoms\n16-07-14 12:54:45 nephelai.local DEBUG [lg-databroker.util:?] - [6098ms] Inited DB by transacting datoms onto empty db\n16-07-14 12:54:45 nephelai.local DEBUG [lg-databroker.util:?] - [801ms] Inited DB with datoms\n16-07-14 12:54:46 nephelai.local DEBUG [lg-databroker.util:?] - [295ms] DB comparisons came out true\n- Initializing 150K maps into an empty DataScript database takes on average 11000ms\n- Dumping a DataScript database containing ~650K Datoms takes on average 95ms\n- Using db-with with ~650K Datoms on an empty DataScript database takes on average 6100ms\n- Using init-db with ~650K Datoms takes about ~850ms.\nSo this fix gave me an order of magnitude increase in performance in creating a database relative to the original maps, and about 8 times an increase relative to db-with which was my workaround before this fix.\n:+1:  Thank you very much.\n. @thedavidmeister An alternative is to hash the squuid into a 2^53 bit number, but depending on how many entities you need to be tracking, you risk collision with a 53 bit resolution of the hash.  The nice thing about this is that the datoms are universally consistent, and can be cached and transmitted over the wire.  You can literally concatenate arrays of Datoms that meet your criteria and build a Datascript database out of them.. @rauhs With your three patches as is, I've measured the improvement on Clojure.\nAlready Sorted\nThis is running (btset/-btset-from-sorted-arr eavt datascript.db/cmp-datoms-eavt)) where eavt is an already sorted Array of 612,278 Datoms.\nWe're roughly 30% faster here.\nBefore\n```\nEvaluation count : 28680 in 60 samples of 478 calls.\n             Execution time mean : 2.138965 ms\n    Execution time std-deviation : 91.406289 \u00b5s\n   Execution time lower quantile : 2.009637 ms ( 2.5%)\n   Execution time upper quantile : 2.294474 ms (97.5%)\n                   Overhead used : 1.733760 ns\nEvaluation count : 28680 in 60 samples of 478 calls.\n             Execution time mean : 2.122734 ms\n    Execution time std-deviation : 80.877181 \u00b5s\n   Execution time lower quantile : 2.004751 ms ( 2.5%)\n   Execution time upper quantile : 2.265796 ms (97.5%)\n                   Overhead used : 1.733760 ns\nEvaluation count : 28800 in 60 samples of 480 calls.\n             Execution time mean : 2.254085 ms\n    Execution time std-deviation : 355.100629 \u00b5s\n   Execution time lower quantile : 2.012477 ms ( 2.5%)\n   Execution time upper quantile : 3.215074 ms (97.5%)\n                   Overhead used : 1.733760 ns\n```\nAfter\n```\nEvaluation count : 37140 in 60 samples of 619 calls.\n             Execution time mean : 1.736268 ms\n    Execution time std-deviation : 513.272135 \u00b5s\n   Execution time lower quantile : 1.529144 ms ( 2.5%)\n   Execution time upper quantile : 2.699856 ms (97.5%)\n                   Overhead used : 1.853299 ns\nEvaluation count : 35160 in 60 samples of 586 calls.\n             Execution time mean : 1.665738 ms\n    Execution time std-deviation : 164.643719 \u00b5s\n   Execution time lower quantile : 1.550275 ms ( 2.5%)\n   Execution time upper quantile : 1.841930 ms (97.5%)\n                   Overhead used : 1.853299 ns\nEvaluation count : 38460 in 60 samples of 641 calls.\n             Execution time mean : 1.633537 ms\n    Execution time std-deviation : 76.458608 \u00b5s\n   Execution time lower quantile : 1.544358 ms ( 2.5%)\n   Execution time upper quantile : 1.813583 ms (97.5%)\n                   Overhead used : 1.853299 ns\n```\nSequence of Datoms\nWith the same 612,278 Datoms sent through shuffle into a sequence, and calling (btset/-btset-from-seq shuffled datascript.db/cmp-datoms-eavt)), performance is measurably worse in Clojure, by about 15%.\nBefore\n```\nEvaluation count : 180 in 60 samples of 3 calls.\n             Execution time mean : 503.835419 ms\n    Execution time std-deviation : 28.740828 ms\n   Execution time lower quantile : 476.988330 ms ( 2.5%)\n   Execution time upper quantile : 546.276716 ms (97.5%)\n                   Overhead used : 1.733760 ns\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 506.634884 ms\n    Execution time std-deviation : 17.976805 ms\n   Execution time lower quantile : 475.626436 ms ( 2.5%)\n   Execution time upper quantile : 529.728618 ms (97.5%)\n                   Overhead used : 1.733760 ns\nEvaluation count : 180 in 60 samples of 3 calls.\n             Execution time mean : 486.397585 ms\n    Execution time std-deviation : 12.555172 ms\n   Execution time lower quantile : 466.890178 ms ( 2.5%)\n   Execution time upper quantile : 508.851682 ms (97.5%)\n                   Overhead used : 1.733760 ns\n```\nAfter\n```\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 575.776478 ms\n    Execution time std-deviation : 14.773797 ms\n   Execution time lower quantile : 556.128659 ms ( 2.5%)\n   Execution time upper quantile : 602.985644 ms (97.5%)\n                   Overhead used : 1.789224 ns\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 581.704069 ms\n    Execution time std-deviation : 21.792113 ms\n   Execution time lower quantile : 555.450997 ms ( 2.5%)\n   Execution time upper quantile : 616.506345 ms (97.5%)\n                   Overhead used : 1.789224 ns\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 595.049536 ms\n    Execution time std-deviation : 16.353099 ms\n   Execution time lower quantile : 560.935834 ms ( 2.5%)\n   Execution time upper quantile : 622.134988 ms (97.5%)\n                   Overhead used : 1.789224 ns\n```\nBased on this, I'll sniff around and see where the issue is.\n. So, it was an error in my testing methodology.  In a moment of inattention, I hadn't realized that the contents of shuffled was not the same between the two comparison runs.  The performance of sorting is highly dependent on the ordering of elements already sorted.\nShuffled\n(bench (btset/-btset-from-seq shuffled datascript.db/cmp-datoms-eavt)))\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 575.373735 ms\n    Execution time std-deviation : 24.730330 ms\n   Execution time lower quantile : 545.909796 ms ( 2.5%)\n   Execution time upper quantile : 619.733675 ms (97.5%)\n                   Overhead used : 1.781517 ns\nSorted\n(bench (btset/-btset-from-seq eavt datascript.db/cmp-datoms-eavt))\nEvaluation count : 840 in 60 samples of 14 calls.\n             Execution time mean : 78.704518 ms\n    Execution time std-deviation : 3.499717 ms\n   Execution time lower quantile : 74.594049 ms ( 2.5%)\n   Execution time upper quantile : 83.802979 ms (97.5%)\n                   Overhead used : 1.781517 ns\nSo, nearly an order of magnitude difference between the shuffled and eavt states.  So I stored my shuffled Datoms to disk so that future tests have valid comparisons.\nBefore\n```\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 542.094752 ms\n    Execution time std-deviation : 19.344584 ms\n   Execution time lower quantile : 513.211344 ms ( 2.5%)\n   Execution time upper quantile : 575.168861 ms (97.5%)\n                   Overhead used : 1.760894 ns\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 551.542106 ms\n    Execution time std-deviation : 22.488158 ms\n   Execution time lower quantile : 516.241581 ms ( 2.5%)\n   Execution time upper quantile : 586.084196 ms (97.5%)\n                   Overhead used : 1.760894 ns\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 554.059653 ms\n    Execution time std-deviation : 19.143848 ms\n   Execution time lower quantile : 524.977362 ms ( 2.5%)\n   Execution time upper quantile : 587.817335 ms (97.5%)\n                   Overhead used : 1.760894 ns\n```\nAfter\n```\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 538.346048 ms\n    Execution time std-deviation : 17.467862 ms\n   Execution time lower quantile : 511.220657 ms ( 2.5%)\n   Execution time upper quantile : 574.468539 ms (97.5%)\n                   Overhead used : 1.750156 ns\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 534.930876 ms\n    Execution time std-deviation : 15.523200 ms\n   Execution time lower quantile : 511.575519 ms ( 2.5%)\n   Execution time upper quantile : 564.947339 ms (97.5%)\n                   Overhead used : 1.750156 ns\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 539.962625 ms\n    Execution time std-deviation : 17.743601 ms\n   Execution time lower quantile : 513.518721 ms ( 2.5%)\n   Execution time upper quantile : 568.271199 ms (97.5%)\n                   Overhead used : 1.750156 ns\n```\nStack\nI also tested using a java.utils.Stack rather than an ArrayList.  You can actually preallocate a stack by using the ensureCapacity method.\nThe diff is below:\ndiff\n@@ -188,21 +190,22 @@\n   [min-len max-len arr]\n   (let [chunk-len avg-len\n         len (da/alength arr)\n-        acc #?(:clj (java.util.ArrayList. ^int max-len) ;; will be ok if too large\n+        acc #?(:clj  (doto (java.util.Stack.)\n+                       (.ensureCapacity len)) ;; will be ok if too large\n                :cljs (js/Array.))]\n     (when (pos? len)\n       (loop [pos 0]\n         (let [rest (- len pos)]\n           (cond\n             (<= rest max-len)\n-            (#?(:clj .add :cljs .push) acc (cut arr pos))\n+            (.push acc (cut arr pos))\n             (>= rest (+ chunk-len min-len))\n             (do\n-              (#?(:clj .add :cljs .push) acc (cut arr pos (+ pos chunk-len)))\n+              (.push acc (cut arr pos (+ pos chunk-len)))\n               (recur (+ pos chunk-len)))\n             :else\n             (let [piece-len (half rest)]\n-              (#?(:clj .add :cljs .push) acc (cut arr pos (+ pos piece-len)))\n+              (.push acc (cut arr pos (+ pos piece-len)))\n               (recur (+ pos piece-len)))))))\n     #?(:clj (.toArray acc)\n        :cljs acc)))\n@@ -227,8 +230,9 @@\n   (if (sorted-arr-distinct? arr cmp)\n     arr\n     (let [al (da/alength arr)\n-          acc #?(:clj  (doto (java.util.ArrayList. (count arr)) ;; will be ok if too large\n-                         (.add (da/aget arr 0)))\n+          acc #?(:clj (doto (java.util.Stack.)\n+                        (.ensureCapacity al)\n+                        (.push (da/aget arr 0)))\n                  :cljs #js [(da/aget arr 0)]) ]\n       (loop [i 1\n              p (da/aget arr 0)]\n@@ -237,7 +241,7 @@\n             (if (== 0 (cmp e p))\n               (recur (inc i) e)\n               (do\n-                (#?(:clj .add :cljs .push) acc e)\n+                (.push acc e)\n                 (recur (inc i) e))))))\n       #?(:clj  (.toArray acc)\n          :cljs acc))))\nIt's much cleaner code-wise than using an ArrayList and we don't need platform-specific code to operate on the stack.\nUnfortunately, it's slightly slower than the original code.\n```\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 565.754245 ms\n    Execution time std-deviation : 19.197602 ms\n   Execution time lower quantile : 531.111827 ms ( 2.5%)\n   Execution time upper quantile : 598.117426 ms (97.5%)\n                   Overhead used : 1.759473 ns\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 567.580395 ms\n    Execution time std-deviation : 18.988969 ms\n   Execution time lower quantile : 536.946825 ms ( 2.5%)\n   Execution time upper quantile : 597.693054 ms (97.5%)\n                   Overhead used : 1.759473 ns\nEvaluation count : 120 in 60 samples of 2 calls.\n             Execution time mean : 576.187052 ms\n    Execution time std-deviation : 26.000749 ms\n   Execution time lower quantile : 540.841802 ms ( 2.5%)\n   Execution time upper quantile : 642.019980 ms (97.5%)\n                   Overhead used : 1.759473 ns\n```\nDo you have any profiling or benchmarks for Clojurescript?. @tonsky Keep in mind that my measurements were based on Clojure.  We don't seem to have a comparable tool to criterium for ClojureScript.  @rauhs might indeed get a performance gain on ClojureScript, but I don't have the tools he used to measure and reproduce it.. @tonsky Right, no difference for Clojure.  The question is, does avoiding transient help in CLJS?. @tonsky It doesn't look like that there's any difference for the benchmarks we've run.  I share your sentiment, that unless @rauhs can produce benchmarks or show his use case, that I'd rather keep the code as platform-independent as possible by using transient.. I can attest to the query engine working well when the IDB and ISearch protocols are implemented.  However, in that case, it does one pattern at a time, which can be extremely slow and inefficient.  I ended up writing my own query-engine that used some of the datascript.query and datascript.parser functions.  But maybe the right way would be to have another layer in between.  Hmm.\nentid being allowed to be other than a number would make my use-case much easier, as I wrote a binary identity-to-long translator.\nclj\n(defn insert-identity\n  [m]\n  (let [signed-identity (hash->long (hash-object (murmur3-128 1234) nippy-funnel\n                                                 (dissoc m :db/id)))\n        unsigned-identity (if (bit-test signed-identity 63)\n                            (bit-not signed-identity)\n                            signed-identity)]\n    (assoc m :db/id unsigned-identity)))\nThis works, but I have nightmares about collisions with my 63-bit unsigned long EIDs, and I haven't worked out the CLJ-to-CLJS story yet.  When I transit-encode Longs, I end up getting goog.math.Longs.  Any possibility that eids can be allowed to be goog.math.Longs?  That may be a worthy project.\n. @tonsky Interesting point, I hadn't realized that.  What is the impact of layout on a tuple join?  I hadn't realized that there was an implicit requirement, which I will try to state clearly:\n\nWhen joining two tuples, the output tuple ordering must match the given :attrs.\n\nDo we have a test case for this scenario?\nLet me think on how to solve this.  I've actually never run into this scenario on my particularly heavy use case. :). @tonsky So, I was thinking about it.  We could put a conditional that measures the number of attributes given.  If l1 or l2 exceeds 8, then we set acopy? to false.\nThis gives us:\n The performance advantage of an acopy for a lot of scenarios.\n Predictable attribute ordering.\nBut this relies on a certain guarantee:\n* That the tuples in question are always array-maps produced by Clojure when <8 attributes.\nAre we comfortable with that?. @tonsky Fair points about not relying on implementation details of the platform.\nI've been examining and thinking about the logic here:\n\nWe do an acopy if and only if both following conditions match:\nTuple t is an array.\nThe length of the array of indexes, idx matches the length of tuple t.\n\nSo, we only do an acopy if and only if the tuple is an array, so at this point, the ordering of a hash-map isn't a concern.\nIn what situations would join-tuples receive an array tuple t where the array of indexes, idx, would differ in ordering from tuple t when the array of indexes, idx matches the length of the array tuple?\nI think that if we clarified that use case and functional description, we can probably solve this.\nAlternatively, I can remove the acopy entirely, but it would reduce the performance gains tremendously.. @tonsky I expanded the scope of my thinking and examined the Relation records.  Your comment about using 'proper data structures' nudged me along that path.\nCouple suggestions:\n array-map ordering is guaranteed at creation time, regardless of number of key-values.  It is when you assoc new key-values onto the array-map that the ordering is not guaranteed.\n   * Could we alter the Relation record to require an array-map?\n Use a different data-structure to represent the attribute to index associations, perhaps a deftype?. @tonsky Would you be OK with introducing frankiesardo/linked?  It's a fairly nice ordered map implementation for both CLJ and CLJS.\nhttps://github.com/frankiesardo/linked. @tonsky We could.  Rather than use a hash map to contain the relation to key index, we could use a vector instead in :attrs.  Unless we're joining very large numbers of attributes in tuples, it should still perform pretty well.  If we need efficiency, we could build a lookup hash map inside the function itself.. This would be particularly interesting for my use case.\nAlthough I've taken the functional approach of just using Datascript as a source of useful functions for my own Datalog engine, such as the query parser, the hash-join functions.  Datascript fundamentally is a memory-based database, and I've shifted away from holding arrays in memory to applying matching and join functions against a custom-written lazy TupleStream interface.  This allowed me to deal with and return result sets measured in the gigabytes.. Maybe a java.util.Stack instead?  You're effectively using this as a stack.  This would allow you to get rid of the Clojure vs Clojurescript specific operations for .push, and .add unifying them all into .push.. Do we know that what's provided is always going to be a sequence of Datoms?  It may be helpful to provide into-array a type.\nAlthough, the da namespace functions already type-hints for [[Ljava.lang.Object;. If the code is performance critical, I've seen a boost from a pattern of such:\nClojure\n    (case (cmp e p)\n      0 form\n      default)\nIt's a micro-optimization that avoids the evaluation of if.. Do we know what the overhead of into-array is, if the seq being provided is already an java.lang.Array?. Replace (count arr) with al, as we already have that bound.. ",
    "mpdairy": "Yeah, this would be nice!\n. ",
    "urzds": "Is this being worked on?\n. ",
    "alexandergunnarson": "@tonsky I have an implementation of what you and @metasoarous describe in posh.sync.schema (mainly in ensure-schema-changes-valid of a Posh PR I'm working on). I'll PR it to DataScript once it's stable. Just thought I'd give you an opportunity to take a look.. ",
    "puredanger": "meta and __extmap have always been off limits and marked in the docstring - I would take the  prefix there to imply that this is a naming convention for internal stuff and suggest that datascript's use of __hash and __hasheq was treading on the obvious future field names for a record caching its hash values. I would certainly prefer datascript to switch to a different convention (or maybe remove it entirely if the new behavior satisfies the same need).\n. I have declined the Clojure ticket - I do not believe it should have been considered valid to modify a record's hash semantics - those are controlled by the language (and should match those of a map). \nIf you want control over hashing, you should use a deftype, where it's perfectly ok to extend IHashEq. I would say even there that using a __-prefixed field name is inadvisable, but is ok.\nI cannot answer the question of performance if this code is modified. It may be necessary to use a deftype for those records prior to 1.9.\n. ",
    "serebrianyi": "Sorry, I might really misunderstand something, but it doesn't seem like d.datoms/d.seek-datoms/d.q accept lookup refs right now, but d.entity does: assert_eq(\"Ivan\", d.entity(db, [\"name\", \"Ivan\"]).get(\"name\"));\nd.datoms(db, \":eavt\", \"name\", \"Ivan\") actually fails with \"Expected number or lookup ref for entity id, got \\\"name\\\"\"\nThe query I used is just a random one to test lookup refs inside of queries.\nAnd thanks a lot for datascript, it's a really wonderful thing!\n. Perfect, thank you!\n. Are you sure that it wasn't supposed to work? \nBecause if I run the CLJ/CLJS version, it works just fine I think\n(let [db  (-> (d/empty-db {:aka { :db/cardinality :db.cardinality/one \n                                  :db/isComponent true\n                                  :db/valueType :db.type/ref}})\n                (d/db-with [{:db/id 1 :name \"Ivan\"}])\n                (d/db-with [{:db/id 2 :name \"Petr\"}])\n                (d/db-with [{:db/id 1 :aka {:db/id 2}}]))]\n   (d/pull db '[*] 1))\nresults in  {:db/id 1, :aka {:db/id 2, :name \"Petr\"}, :name \"Ivan\"}, which is also consistent with Datomic as far I understand.\nWhat I'm actually trying to do is to transact to Datascript the result of Datomic pull - it works in Datomic, but was not working in the JS version of Datascript.\nOr am I missing something completely here?. Thank you!. ",
    "alexei-matveev": "Hi! Does the public API rely on tuple-idx being strings (\"e\", \"a\", ..)?\nBecause when with \"nth\" instead of \"get\" here: (#?(:cljs aget :clj nth) tuple tuple-idx) I get another 10 % speed up in the code below. (At the expense of breaking tests, of course).\n```\n;; This is significantly slower than SQLite code below:\n(defn- bench-O2 [n]\n  (let ;; rules:\n      [r (quote [[(follows ?a ?b)\n                  [?a ?b]]\n                 [(follows ?a ?b)\n                  [?a ?x]\n                  (follows ?x ?b)]])\n       ;; query:\n       q (quote [:find ?a ?b\n                 :in $ %\n                 :where (follows ?a ?b)])\n       ;; data:\n       $ (mapv vector (range n) (map inc (range n)))]\n    (d/q q $ r)))\n;; (time (count (bench-O2 200)))\n;; \"Elapsed time: 7549.735629 msecs\"\n;; \"Elapsed time: 6100.434851 msecs\" (after addressing PR-180)\n;; \"Elapsed time: 5434.237636 msecs\" (after get -> nth)\n;; 20100\n;; drop table if exists db;\n;; create table db (x integer, y integer);\n;; insert into db\n;; with recursive\n;;      cnt (x, y) as\n;;      (values(0, 1)\n;;         union all\n;;       select\n;;         x + 1,\n;;         x + 2\n;;       from cnt\n;;       where x + 1 < 200)\n;; select x, y from cnt;\n;; select count(*) from (\n;; with recursive\n;;      follows (a, b) as\n;;      (select x as a, y as b from db\n;;      union all\n;;      select f.a as a, d.y as b\n;;      from follows as f\n;;      join db as d\n;;      on f.b = d.x)\n;; select a, b from follows\n;; )\n;; ;\n;; sqlite> .read recursive.sql\n;; Run Time: real 0.000 user 0.000000 sys 0.000000\n;; Run Time: real 0.000 user 0.000000 sys 0.000000\n;; Run Time: real 0.001 user 0.004000 sys 0.000000\n;; count (*)\n;; 20100\n;; Run Time: real 0.047 user 0.044000 sys 0.00000\n```\n. ",
    "marcolisi": "Thanks!\nDone in release 0.16.3. Thank you! \nAnd thank you for the explanation, makes sense to me now.  :). ",
    "refset": "Okay I think I've just about managed it! How does that look?\nNote that a key benefit of this functionality is the ability to trivially serialise transaction maps which contain transaction functions (a challenging constraint of the existing option).\n. Sorry, I inadvertently (automatically!) closed this when trying to sort out the branches in my fork. I'd be happy to try to rebase these changes sometime.. For everyone's benefit, the previous issue on this topic is here: https://github.com/tonsky/datascript/issues/22. Consider the following :eavt index:\ne1 :a1 v1\ne2 :a1 v2\ne2 :a1 v3\ne2 :a2 v4\ne2 :a2 v5\ne2 :a2 v6\ne2 :a3 v7\ne3 :a4 v8\n...my use-case is to visually navigate up and down the :eavt index using a cursor which is able to address the individual datoms (by tracking [:loc/e :loc/a :loc/v]). seek-datoms enables me to begin iterating from the exact cursor location in the downward direction, but without rseek-datoms (or my equivalent workaround) I can't see a way to iterate upwards that doesn't involve inefficient scanning. For instance, given my cursor is currently addressing [e2 :a2 v5], I want the user to be able to \"scroll up\" (visually) and see that the datom immediately above is [e2 :a2 v4].\n@rauhs Simply doing (reverse (seek-datoms ...)) starts the iteration upwards from bottom of the index up to the exact cursor location, but I actually want to iterate upwards from the exact cursor location to the top of the index. This is what my workaround achieves.. Hey! Yeah it's a pretty unfortunate solution but I haven't been able to think of a clearer way to solve it. Essentially the problem is that with this :db/ident semantic the let binding (let [[op e a v] entity] means op is the :db/ident but e could be anything (including a string), because it is simply the first argument for the :db/fn function. If the first argument happens to be a string or a negative number then (tempid? e) returns true, when it needs to be false if op is a :db/ident. These two lines essentially mean (and (tempid? e) (not (custom-op? op)). Note that :db/add doesn't appear in the vec on the second line. Perhaps this is worth pulling into a new function, somehow.... ",
    "rauhs": "I have a schema with about 100 attributes (and more to come) with a largish SPA. On my fast desktop the db init can take around 100ms but I also target mobile and then the init becomes 2-3x slower. One slow component was rschema during profiling, hence the PR.\nThe code is a little less readable due to not using for.\nThe result is about a 5x speedup when I measure it with simple-benchmark. . I've since switched to precomputing the rschema (in a macro) to further speedup startup. So this is now obsolete.. Yeah I don't need that change in. Keeping it like this is fine! Cheers. Datomic doesn't make them equal unless the T is also equal:\nclojure\n(:import (datomic.db Datum))\n(= \n  (Datum. 0 5 \"f\" 1)\n  (Datum. 0 5 \"f\" 3))\n;; => false\nNotable: It uses the :db/id of the attribute in the Datom. And they use the lowest bit to encode assertion vs retraction.\n. Obsolete. I fixed that in the CLJS compiler.. This seems to work:\nclojure\n(defn slice-one\n  \"Like slice but returns a single Datom.\"\n  ([btset key] (slice-one btset key (.-comparator btset)))\n  ([btset key cmp]\n   (loop [node (.-root btset)\n          path empty-path\n          level (.-shift btset)]\n     (let [keys-l (node-len node)]\n       (if (== 0 level)\n         (let [keys (.-keys ^Leaf node)\n               idx (binary-search-l cmp keys (dec keys-l) key)]\n           (when (and (not (== keys-l idx))\n                      ;; Check if we actually found it:\n                      (zero? (cmp key (aget keys idx))))\n             (aget keys idx)))\n         (let [keys (.-keys ^Node node)\n               idx (binary-search-l cmp keys (- keys-l 2) key)]\n           (recur (aget (.-pointers node) idx)\n                  (path-set path level idx)\n                  (- level level-shift))))))))\nAvoid the rseek and any more traversal. This can now be used in entity and in the db transacting code. It'd probably also be good to add -datom to IIndexAccess. Also: Implemented bounded-count for an Iter.. Just want to continue dumping my thoughts: I ended up implementing this for my datascript fork (which adds a few other features) in a slightly different way:\nInstead of bounded counting an Iter (which still walks over 1-2 btset leaves) and then again iterating over it when the bounded count was <=20, I now just iterate over the Iter right away but stop after 20 attributes (so as to avoid realizing an entity which has many attributes or --more commonly-- an entity that has a ref-many with many references).\nSo entity/lookup-entity is now roughly:\n\nIf in cache -> return\nIf not. (cond touched ...)\na) if touched is nil: This is the very first attribute access, do the (db/-search ...) and start reducing over the Iter. Add each attribute to the cache but STOP once we reach 20 attributes or the Iter is reduced over. If we hit the limit of 20: We now have to see if we were in the middle of adding a mutli-ref attribute, since we have to remove these again from the cache if we were. Set the touched to either true (didnt hit the limit) or false (we did hit the limit and there was more in the btset Iter).\n  b) If touched is true => not found\n  c) If touched is false => get the attribute with an Iter and add to cache.\n\nFor the majority of Entities, this means we only once search the btset and get all the attributes in the cache quickly. For entities with many attributes this is still fast since we're still at near-constant time here.. Extending the type to be IComparable is probably tough if you use a mangled JS build. Though it'd be easy if you built datascript yourself (you can see shadow-cljs to get you a webpack compatible build of datascript). Right now the simplest (hacky) workaround would be to always add an array and give the first element in the array an \"ID\" that's unique and primitive (string,number,bool) and store the actual payload (your object) as the second value of the array:\njavascript\n[12 {x: 12, other: \"bar\"}]\n[15 {x: 15, other: \"foo\"}]\nThose value will be comparable to CLJS and will also properly be sorted (important for initializing the DB which uses arr.sort()).\nFWIW, I think CLJS could be more lax about this and allow all objects to cljs.core/compare as long as both have a valueOf function, which is required to return a primitive value by the JS standard. Though I'm not sure such a change would be accepted (feel free to open a ticket about it).. If you store values in Datascript and query by them (as above) you absolutely need to make your values comparable. If you really just care for identity and don't have a natural ordering for your values then I'd do the following:\n\nGenerate a unique ID for each object, (1, 2, 3....), add this ID to your JS object which also has pointers.\nAttach this unique ID to an indexed datascript attribute. Something like object/pointers-id\nAttach the actual payload (your JS object with pointers) to some other datascript attribute. Something like object/pointers.\n\nThen only ever query by object/pointers-id and get the value from the entity on the pull. That'd be less hacky and scale well.. I don't know the JS side API of datascript so I can't help you there. Forget about the difference between Obj vs Class instance. Both won't work, you just can't query by values which are not comparable. Try implementing my idea above. Pseudo code:\n```\ndb = d.empty_db({\"obj-id\", {\":db/index\" true}});\ndb1 = d.db_with(db, [[':db/add', 1, 'obj', obj1], [\":db/add\", 1 \"obj-id\" obj1.id]\n                     [':db/add', 2, 'obj', obj2], [\":db/add\", 2, \"obj-id\", obj2.id]\n;; Now query by obj id:\nd.q('[:find (pull ?e [*]) :in $ ?obj :where [?e \"obj-id\" ?obj]]', db1, obj1.id);\n```\nUse a factory method to get you a new object with a newly generated id.. FYI This is super easy with just adding a: (es6-iterable TheType) under the type you want to make iterable.. Doesn't this already exist? The btset slices implement -rseq which gives you an efficient reverse view on the iter.. ",
    "whitecoop": "Is this possible through the JS API? I see the datoms() function is exposed, but I'm still getting a function returned from that rather than data... it'd be great if I could just get an array of arrays ([[e, a, v, t, a-r], ...]).. ",
    "mikeivanov": "Gerne!. ",
    "wbrown": "I was thinking about this some more, while figuring out how to write a general-purpose layer where we provide a schema of attributes to e-a-v-tx mappings.  eids being integer only is a specific optimization for BTSets.  Once a pattern returns tuples, anything above this shouldn't care about what type the eids are.  I know that the hash-join and the collect code doesn't care.\nSo, if we are creating Datoms, or persisting to BTSets, then certainly, we should enforce the numeric type restriction.  But for other data sources, I don't think we should.\nIf we permit relax the eid constraint this way, it makes it much easier for us to write IDB and ISearch adapters against various backends.  I would really like the ability to use pull syntax against these adapters.. ",
    "wambat": "Same issue, looks like it cycles in recursion here \nhttps://github.com/tonsky/datascript/blob/master/src/datascript/parser.cljc#L55. ",
    "anmonteiro": "FWIW, cljs.core/seqable? changed in the latest release of ClojureScript. See https://github.com/clojure/clojurescript/commit/101d7d9e03e90518e6769781dd33fbe6387d2d44. ",
    "seantempesta": "Are you planning to release this?  I just started using 15.5 with Clojurescript 1.9.494 and ran into this issue.  . Thank you!. ",
    "claj": "Thanks a ton for fixing this!. The thing about datomic and datascript is that you often can create two queries on the exact same dereffed db, then merge the two result sets, either by a third query or some other means.\nIn this particular example, I would create two result-sets for id:s, make a union of them and then do pull in a query to get the outer join.\nLike: \n```\n(let\n[db @conn \n res-1 (q '[:find [?e ...] :where ...] db)\n res-2 (q '[:find [?e ...] :where ...] db)\n joined (clojure.set/union res-1 res-2)\n results (pull-many db [:gadget/name :widget/name] joined)]\nresults)\n```. I didn't know but bitwise operations in javascript limits the result to 32 bits:\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Bitwise_Operators\nYour eid 285873023227265 has the following bit representation (without any padding)\n(Long/toBinaryString 285873023227265)\n\"1000001000000000000000000000000000001010110000001\"\nuser> (count (Long/toBinaryString 285873023227265))\n49\nI suspect your eids get cut at 32 bits in the bit operations which sort of works as long as it is made consistent.\nI suspect you are using Datomic eids in your datascript db. It's usually better to introduce some external and more controllable index when syncing between these to different databases.. I would suspect the bit shifts in the bt-set, where they are picked out to be used as indexes in the nested bransch-arrays:\nhttps://github.com/tonsky/persistent-sorted-set/blob/0683019c890de91b375baf4c7c8523d16d586e2a/src-clojure/me/tonsky/persistent_sorted_set.cljs#L46\n. ",
    "zarkone": "Not sure if that related but I also had problems with datascript, compiling with shadow-cljs.  \nFor example, this code works without optimizations, but doesn't work with :advanced enabled, i.e. in :realease mode:\n(d/q '[:find [?atom ...] :in $ ?url\n                         :where [?e :url ?url]\n                                [?e :atom ?atom]]\n                   @db url)\nI've solved it without patching anything by adding contents of  datascript/externs.js to my manual-externs.js file I already had for externs: \nRelated part of shadow-cljs.edn:\n:release {:compiler-options {:externs [\"manual-externs.js\"]\n                             :optimizations :advanced}}. ",
    "DalekBaldwin": "Actually, it looks like this can be demonstrated with just one entity:\n```clojure\n(let [conn\n      (ds/create-conn\n        {:a {:db/valueType :db.type/ref}\n         :b {:db/valueType :db.type/ref}})\n  _\n  (ds/transact! conn [{:db/id 0\n                       :a     0\n                       :b     0}])]\n\n(ds/q '{:find  [?p ?q]\n          :in    [$ %]\n          :where [(bar ?p ?q)]}\n    @conn\n    '[[(foo ?p ?q)\n       [?p :b ?q]]\n  [(bar ?p ?q)\n   (baz ?p ?r)\n   (baz ?q ?r)]\n\n  [(baz ?p ?q)\n   [?p :a ?r]\n   (foo ?r ?q)]]))\n\n;; => #{}\n```\nWith the simplifications from above yielding the correct result #{[0 0]}.. Can now confirm: I see the same behavior in CLJS.. ",
    "colindresj": "The scenario I'm thinking of is syncing with a datomic db. Or, really any large batch of transactions that need to be processed. Saving those few ms on every loop could help if you're doing other UI rendering not based on the data in datascript.\nWrapping transact would definitely work, but just updating transact-async felt more natural to me, and it's relatively low-cost code-wise.. ",
    "Odinodin": "As a workaround I've added :language_in ecmascript5 as a cljs compiler option to my project.. ",
    "bamarco": "As currently written, tx-middleware is a function which takes a transaction-fn and returns a transaction-fn. The transaction-fn has the signature of datascript.db/transact-tx-data which takes in an initial-tx-report and some txs. \nclj\n(defn schema-middleware [transact]\n  (fn [report txs]\n    (let [{:as report :keys [db-after tx-data]} (transact report txs)\n          db-after' (transduce\n                    (filter schema-datom?)\n                    conj-schema-datom\n                    db-after\n                    tx-data)]\n      (if (= (:schema db-after) (:schema db-after'))\n        report\n        (assoc report\n          :db-after (replace-schema db-after (:schema db-after')))))))\n  . ",
    "skbach": "Thanks Nikita\nDo you have an example of that in the tests or a doc somewhere?\nI saw a lot of \"or\" testing in the src so I guess you attempted to\nimplement it and hit a snag?\nOn Thursday, August 17, 2017, Nikita Prokopov notifications@github.com\nwrote:\n\nClosed #235 https://github.com/tonsky/datascript/issues/235.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/tonsky/datascript/issues/235#event-1209642929, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AARhWrbplnG-oVdH72Vba6Qm85Je0Ndbks5sY_JigaJpZM4O5oBo\n.\n\n\n-- \nRegards,\nScott Klarenbach\nInvisible Robot Technologies\ninvisiblerobot.io\n604-537-1856\nscott@i scott@pointyhat.canvisiblerobot.io\nSuite R - 2404 Guelph Street\nVancouver, BC, V5T 3P3\n\nTo iterate is human; to recur, divine\nThe information contained in this message is intended only for the use of\nthe individual or entity to which it is addressed. It may contain\ninformation that is privileged, confidential and protected from\ndisclosure. If you have received this communication in error, please notify\nus immediately by replying to this message and deleting it from your\ncomputer.  \n. ",
    "pharcosyle": "I spent a bunch of time trying to get or to work as well before I saw this. Perhaps it's worth adding to the readme which currently says only that not, not-join, and or-join are unimplemented.. You're right. I read that half a dozen times and didn't see the or. I think my brain just mushed it and \"and\" together on account of how they're both conjunctions.. ",
    "souenzzo": "This works\n(let [c (d/create-conn {:user/username {:db/unique :db.unique/identity}})\n      id (d/tempid :db.part/user)\n      username \"foo\"]\n  (d/transact c [{:user/username username}\n                 [:db.fn/cas id :user/username nil username]]))\nThis don't:\n(let [c (d/create-conn {:user/username {:db/unique :db.unique/identity}})\n      id (d/tempid :db.part/user)\n      username \"foo\"]\n  (d/transact c [[:db.fn/cas id :user/username nil username]\n                 {:user/username username}]))\nI'm on datascript 0.16.2 + clj 1.9.0-beta1. Tested on 0.18.2 now both examples thows\nExecution error (ExceptionInfo) at datascript.db/transact-tx-data (db.cljc:1209).\nCan't use tempid in '[:db.fn/cas -1000002 :user/username nil \"foo\"]'. Tempids are allowed in :db/add only\nSo I will close this issue.. In datomic, I can call :db.fn/cas. ",
    "jconti": "This is clever.  I like that I can take ad-hoc solutions out of my code +1, but a little disappointed that a fair amount of the declarative quality of the query is lost with what ends up being a hack on the where syntax.\n@bahulneel Thank you!. @bahulneel That is really brilliant work.  I did not realize how close your final result was to the Datomic implementation.  I used rules to wrap complex queries requiring not and or and the results are very satisfying.. Wow super big thanks to @tonsky and @bahulneel .  I have received so much utility from this project and the contributions you made to this case.  Hopefully I will be able to get some support for the continued health of the project around here.  Really amazing work!. ",
    "Harleqin": "I'm not sure, but shouldn't the near definition use a predicate?\n[(near ?i1 ?i2)\n [(= (inc ?i1) ?i2)]]\n[(near ?i1 ?i2)\n [(= (dec ?i1) ?i2)]]. Need the type hints be so ugly?  Couldn't `^Object` work?. `maybe` for the Datomic example is defined here: https://github.com/Datomic/day-of-datomic/blob/bdb8b4c40c0d8d18b1465b6a7f2f2ce4da226991/src/datomic/samples/query.clj\n\nDatascript has a different model of where the schema is kept, but this should be translatable.. ",
    "andrerichards": "\nIt still needs Java wrappers, right?\n\nYes\n\nCan\u2019t you AOT in there?\n\nHave tried to specify AOT in intermediate library (for datascipt.db namespace) but Leiningen does not compile those classes (in the intermediate library's build). Will investigate this option further.\n\nLong-term plans are to add Java API to DataScript directly, but I doubt it\u2019ll happen any time soon. \n\nNice. We are obviously happy to contribute whatever we do. We are following the built-in JavaScript wrapper fairly closely, including porting the JS tests to Java. We are changing method names to be more idiomatic Java - dbWith instead of db_with, etc. Parameter and return types are set to the types declared in Clojure (DB, TxReport) - hence the need for AOT. \nIf you have any specific ideas or preferences, we are happy to follow those conventions, if it might save you work later. If you have not thought about it, or is not ready to comment, we'll continue on our merry way :)\n. I started off with Datomic API, but it is largely based around Java Interfaces (Connection, Database, Datom, Entity are all interfaces). Currently none of the DataScript records implement those interfaces and I did not want to get into the heart of DataScript too much. \nIf you decide to go that route and define + implement the interfaces inside DataScript, it could work well.\nIt would mean the difference between:\npull(db, ...)\nand\ndb.pull(...) \nThe last one is certainly more 'object-orienty', but I decided that for our use static methods are more straightforward to implement right now, and are good enough. \n. I guess the biggest advantage of copying Datomic API is that users can copy Datomic examples.\nI could give the Datomic API a go, and provide you with feedback on any issues I run into, if that would help - even if it is throwaway work.\n. I did a very, very minimal Datomic-like API implementation to test the possibility and the impact on DataScript code.\nSee the comments below these commits:\n\n53dd897\ne51e856\n\nIn short:\n\nneed to declare some Java interfaces, e.g. datascript.java.Database\nimplement them, e.g. implement Database in datascript.db.DB\nin many cases implementation simply requires a call to the corresponding function in datascript.db, e.g.\n(defrecord-updatable DB\n   ...\n   Database (entid [db entityId] (entid db entityId))\n   ...)\nin some cases it requires moving the function from datascript.core to datascript.db, as we cannot call datascript.core from datascript.db, e.g. db-with:\n(defrecord-updatable DB\n   ...\n   Database (with  [db tx-data]  (db-with db tx-data))\n   ...)\nI think Database.pull might require biggest such change, as there are a bunch of functions declared in a completely different namespace, and many/most of them might have to be moved to datascript.db\n\nWould you be OK with doing it like this, or does it interfere with some other things you have in mind (or maybe you have other suggestion)?  \nI am quite happy to proceed further along this path, and see if I run into any other issues - but it would be useful to know that you are OK with this, at least in principle.\nIf you do not have time to look into this and comment now, it's fine. I understand you might be busy  :)\n  . ",
    "jenyckee": "Still Unknown function. I am running this from the Javascript API\n```\n  const reMatch = (e) => {return true}\nreturn ds.q([:find ?e \n    :in $ ?re-pattern :where \n    [?e \":person/Voornaam\" \"${name}\"]\n    [(str \"${name}\" ?match) ?matcher]\n    [(?re-pattern ?matcher) ?regex]\n    [(re-find ?regex ?aname)]\n    [?e \":person/Voornaam\" ?aname]]\n  , state.db, reMatch)\n. So why does this not give me the list of id's that match `name` please?\n  const reMatch = (e) => new RegExp(e)\nreturn ds.q([:find ?e \n    :in $ ?re-pattern :where \n    [(str \"${name}\" ?match) ?matcher]\n    [(?re-pattern ?matcher) ?regex]\n    [(re-find ?regex ?aname)]\n    [?e \":person/Voornaam\" ?aname]]\n  , state.db, reMatch)\n```\nInstead it returns the whole db .... Why do I get \nError: Cannot compare fetching to :person/Voornaam\nin\n`[:find ?e :in $ [?name] :where\n    [?e :person/Voornaam ?name]]`\nand not in\n`[:find ?e :in $ [?name] :where\n    [?e \":person/Voornaam\" ?name]]`. ",
    "arichiardi": "@djwhitt Not surprising, I am interested ;) Feel free to hit me with questions about macrovich as well (I ported honeysql with it)\n. So cljs-js support is supposedly very easy if you don't have many macros in the project. The problem with macros is that in cljs-js while they still need to be in a separate compilation stage (the JVM one - Clojure), they cannot make use of any Clojure.\nAlso, sometimes rules in cljs-js around definition time are weird. For more info, Macrovich has a good intro readme:  https://www.npmjs.com/package/macrovich. ",
    "djwhitt": "@arichiardi is right. Macros are the big issue. In self-hosted CLJS they're evaluated by the CLJS compiler rather than the Clojure compiler. That means regular ':clj' reader conditionals around macros need to be replaced by something that evaluates them in CLJS, but only if self-hosted CLJS is being used for compilation. macrovich provides macros to do this. They're not much code though, so they could be easily incorporated into the DataScript code base if adding a dependency isn't desirable. \nAnother related issue is that namespaces containing macros in CLJS are evaluated twice. Macrovich provides a 'usetime' macro to prevent non-macro code from getting evaluated when macro namespaces are evaluated the first time. \nOh, and of course, any JVM specific code in macro bodies themselves needs to be placed behind conditionals and a JS version added.\nI think the double evaluation is the most annoying part. If we don't want to wrap 'usetime' calls around tons of code, some macros should probably be moved into separate namespaces.\n@tonsky given your time constraints I'll try to come up with the most reasonable patch I can manage and make a PR that includes explanation for my changes. Then you can review it as you find time. I'd like to see this happen, but I'm not on a deadline and I totally understand not having time. One question though, do you want to keep DataScript dependency free or is a macrovich dep ok?. ",
    "fdserr": "I don't know about \"syncing schemas\", but that'd greatly improve porting queries and transactions to/from Datomic. Any new on this?. ",
    "kennyjwilli": "I care about all the entities I am transacting. In my case, I don't have a need to explicitly assign a tempid before transacting. Really all I am after is getting a list of :db/id's for entities that were created (not updated or retracted) from a transaction report. \nIt looks like Datomic automatically assigns a tempid to all entities in the transaction that do not have a :db/id. Here is an example of transacting the map form without including a :db/id.\nclojure\n(let [conn (d/connect db-uri)]\n  @(d/transact conn [{:foo \"a\"}]))\n=>\n{:db-before datomic.db.Db,\n @af119e05 :db-after,\n datomic.db.Db @9bc720dd,\n :tx-data [#datom[13194139534327 50 #inst\"2018-01-09T07:28:11.646-00:00\" 13194139534327 true]\n           #datom[17592186045432 69 \"a\" 13194139534327 true]],\n :tempids {-9223301668109598114 17592186045432}}\n  . ",
    "rnewman": "There's a chance, but no commitment from me! Figured it was worth filing an issue anyway.. ",
    "CMCDragonkai": "What does CLJS compare do with objects that are instantiated from classes? The above code shows that it works for new DummyObj.\nThis would be a very useful function for me, because I need a table that I can query that stores pointers. And some rows may store the same pointer. And then I would update all rows with the same pointer to point to something new. It would support an immutable keyless B+tree.. I really need it to compare based on pointer equality of the object itself. Are you saying to tag each object created with a special unique id before inserting into datascript?\nAlso I'm sure there are certain value types that cannot be ordered, I wouldn't think of pointers to objects as being ordered. So I'm not sure what kind of benefit sorting is for this situation.. I found that identical? in CLJS maps directly to ===: https://stackoverflow.com/a/13005218/582917. Thanks for the advice, however I'm not familiar with clojure. What would those 3 steps look like in JS?. Still I'm confused why would there be a different behaviour from using just {} vs new DummyObj. The code samples pointed out by @tonsky doesn't appear to deal with the difference. In JS, both are typeof Object, and both are instanceof Object. The only difference is that obj1.constructor === DummyObj and ({}).constructor === Object.. Another test:\n```js\nconst d = require('datascript');\nconst obj1 = { x: 1 };\nconst obj2 = { x: 1};\nconst db = d.empty_db();\nconst db1 = d.db_with(db, [[':db/add', 1, 'obj', obj1], [':db/add', 2, 'obj', obj2]]);\nd.pull(db1, '[*]', 1).obj === obj1; // false (there was a parentheses typo here)\n```\nIt shows that these are no longer the same object. That must mean datascript must be doing a shallow or deep copy of the normal object that is being inserted. (Later I found out that it was in fact a deep copy.)\nI think the docs should make clear that when inserting JS objects, if they are literal objects, they get copied, while if they are class instantiated objects, they are inserted by reference. This occurs even when the class instantiated objects are deeply nested.. @tonsky Have you tried running this?\n```\nconst d = require('datascript');\nconst obj1 = { x: 1 };\nconst obj2 = { x: 1};\nconst db = d.empty_db();\nconst db1 = d.db_with(db, [[':db/add', 1, 'obj', obj1], [':db/add', 2, 'obj', obj2]]);\nd.pull(db1, '[*]', 1).obj === obj1; // false\n```\nIt shows that with obj1 which is just plainly {x: 1}, which is added into the DB. Then when I pull it out, I compare it with obj1 using ===. It returns false. I'm running on Node v8.7.0. I copy it verbatim and run it. That's what happens. If it's not copying it, then what is d.pull(db1, '[*]', 1).obj?\nI've tested again with new Object({a:1}), it is the same result as a normal literal object. But as soon as it is a class instantiation, then it does return true when doing ===. It even happens for deep objects.. @rauhs Just a clarification, does this mean datascript cannot index things that are not ordered (like using hash indexing)? I just tried it:\nError: Cannot compare [object Object] to [object Object]. I'm making an adapter to make sure all my object keys are given unique numbers so they can be indexed by datascript.\nBut I had a thought experiment as to whether datascript in the future could index JS objects. Well I found that other than ES6 Map and WeakMap, there's no other easy way to index object keys in JS. But I looked at Facebook's immutable.js codebase, and here's their implementation for \"hashing\" JS objects that can be used as keys in their Immutable Map and Ordered Map. https://github.com/facebook/immutable-js/blob/7f4e61601d92fc874c99ccf7734d6f33239cec8c/src/Hash.js#L85-L153\nMaybe a feature request for the future?\nThere's also a discussion about this feature: https://github.com/facebook/immutable-js/issues/84 Previously immutable.js also couldn't store objects as keys, but after that commit, objects could be stored as keys for immutable sets, maps and orderedmap.. Here we go: https://github.com/MatrixAI/js-object-tagger. BTW @rauhs even if I use object tagging to allow object keys to be indexed by proxy of the numeric tag. I still need to make sure my objects are class instantiated (not new Object() as it doesn't work), because as demonstrated before, datascript copies literal objects on insertion. I just tried with the pull API, and it did this again. However the entity API is strange as instead of giving back my object, it gives back some different kind of object (seems like another entity itself).\n\nI think the docs should make clear that when inserting JS objects, if they are literal objects, they get copied, while if they are class instantiated objects, they are inserted by reference. This occurs even when the class instantiated objects are deeply nested.\nhttps://github.com/tonsky/datascript/issues/248#issuecomment-360121042\n\nI hope one day this feature will be made explicit, the ability to make sure even literal objects are stored by reference and not copied.\n\nFound another hack to get referenced objects: Object.create(null) creates an object with undefined constructor.. ",
    "corporatepiyush": "Hazelcast supports java.util.Set \nhttp://docs.hazelcast.org/docs/latest-development/manual/html/Distributed_Data_Structures/Set.html\nso as long as any implementation conforms to java.util.Set interface that should work.\n. or else it can be done in reverse way. \nWe can target the Protocol/Interface which datascript uses for Database operations and we can hook Hazelcast's distributed data structures into it and we will be able to create Datomic like distributed DB'.  java.util.SortedSet can be implemented easily over Hazlecast's Set or Map (ignoring values)\nhttps://docs.oracle.com/javase/8/docs/api/java/util/SortedSet.html\nBut the way I see it BTSet is not compatible to that interface either\nhttps://github.com/tonsky/datascript/blob/master/src/datascript/btset.cljc . By implementing the abstract methods in SortedSet interface\nFor example. java.util.TreeMap can accept anything which implements java.util.Map interface\nhttps://docs.oracle.com/javase/8/docs/api/java/util/TreeMap.html\nPerhaps you are concerned about to carry over the same structure of BTSet to Hazelcast. That may not be possible and that's fine. \nIf Hazelcast and its Data Structures sounds more confusing, lets focus on my basic requirement \nWhat I am interested in implementing\n1.  Adaptor Pattern  - Adapt java.util.SortedSet methods into BTSet and utilize its methods internally \nor\n2.  datascript accepts (different protocol/interface than java.util.SortedSet) which takes care of data retrieval and persistence through any other custom solution.. @wbrown-lg That's precisely what I am after, decouple query engine from storage layer.\nCould you please share GIST if possible ? or perhaps library if you made it open source.. ",
    "pedroteixeira": "child.get('collection/_children') is returning a native js array with no eids and no .get :/. @tonsky btw, do you already have unit test for something like this, i.e. nav back then forward?\nBecause I'm still not sure if just a \"local issue\" of mine. I'll try to use to use the datoms index directly for now.. Ok, I remebered that I'm using the datascript module exported from https://github.com/typeetfunc/datascript-mori \nSince you have the unit tests, I will close this issue, probably something else interfering.\nThanks for replying.. Ok, I thought it could be a regression.  In a JS-only environment, I managed to use datascript-mori to wrap the entity API, to try workaround the js API returning different objects than inputted. Perhaps something more efficient could be done internally? Perhaps just letting values as they were inputted (without conversion)?\nNot ideal, but the following is a temptative solution using a wrapper class (typescript) -- if it helps anyone in the future:\n```\nimport { mori } from 'datascript-mori';\nclass DBEntity implements IDatabaseEntity {\n  entity: any;\nconstructor(dsentity) {\n    this.entity = dsentity;\n  }\nget(key) {\n    const value = this.entity.get(key);\nif (value && value.eid) {\n  return new DBEntity(value);\n\n} else if (value != null && mori.isSeqable(value)) {\n  return mori.toJs(value);\n}\n\nreturn value;\n\n}\n}\n```. ",
    "shparun": "works now, thank you. reproduced on jvm with [org.clojure/clojure \"1.10.0\"], [datascript \"0.18.1\"]. ",
    "jeaye": "That did the trick, thank you! If you don't mind, I updated the wiki to contain this info as well (at the bottom of the section): https://github.com/tonsky/datascript/wiki/Tips-&-tricks#edn-serialization. ",
    "ronbu": "Thanks for the feedback. I will close this PR.\nIf anybody is interested, my fork can be found at https://github.com/ronbu/datascript\n\nWhy double? Wouldn\u2019t simply using long give you more bits to work with?\n\nIt's the only number type available on JavaScript Engines.\nUsing two 32bit number values is another option i need to investigate. \n\nHave you actually measured it delivers on that goal?\n\nI measured memory usage in Chrome with the following code:\nRegular Datascript (167 MB)\nclj\n  (def db\n    (d/init-db (for [i (range 1e6)]\n                (d/datom i :name i))\n              {:name {:db/order 0}}))\nWith optimized Datom type (76 MB)\nclj\n(def db\n    (d/init-db (for [i (range 1e6)]\n                (d/datom i 0 i))\n              {:name {:db/order 0}}))\n. ",
    "fversnel": "I see there are merge conflicts I'll resolve them first and then re-submit this pull request. ",
    "luchiniatwork": "Hey @tonsky ... I would love to see this in. Anything I can do to help?\nI'm currently publishing a library that depends on DataScript's datascript.query-v3 and this is a blocker (or I would have to fork on clojars myself - which I would rather not do).. Yes! Both actually.. Hey @tonsky ... sorry to be a pest... I'll be demoing the stuff I'm working on at the next Clojujre/NYC Meetup.\nI can demo out of my fork but it would be lovely if I could demo out of maven directly and this PR enables me to do that. Thanks!. Thank you buddy! This is great!. It worked perfectly. Thanks @tonsky !. ",
    "Mechrophile": "yeah, for what it's worth, I also find this prevents me from importing query-v3 in cljs.   It's actually been an immense problem, and I'd also really like to help.. Hurray!!!. ",
    "magnars": "This issue is stopping us from upgrading to JDK11. Please consider merging this PR. \ud83d\udc4d . They probably do not need to be so ugly. Clojure core uses ^objects.. ",
    "alzadude": "I'm basically looking at ways to do a full outer join in the traditional sense..\nFor example given \"widgets\":\n[{:db/id 1 :widget/name \"a\"}\n {:db/id 2 :widget/name \"b\"}\n {:db/id 3 :widget/name \"c\"}]\nand \"gadgets\":\n[{:db/id 1 :gadget/name \"x\"}\n {:db/id 3 :gadget/name \"y\"}\n {:db/id 4 :gadget/name \"z\"}]\nI'm trying to investigate if it would be possible to produce a full outer join (say on id for example, but could be a different attribute):\n```\n{[1 \"a\" \"x\"]\n[2 \"b\" nil]\n  [3 \"c\" \"y\"]\n  [4 nil \"z\"]}\n```\nIf that makes sense?. ",
    "ghadishayban": "thanks @tonsky . ",
    "darkleaf": "If I understand correctly datomic doesn't allow to pass functions as inputs.\nSo this is datascript specific feature.\nIs it posible to implement dynamic function choose? \nHow difficult is it to do this?. Which main pattern? Is single function specified as input or it's just a rule?  \n```clojure\n(ns data ...)\n(defn foo [x] false)\n(let [db (-> (d/empty-db)\n             (d/db-with\n              [{:foo :bar}]))]\n  (d/q '{:find [?e]\n         :where [[?e ?a ?v]\n                 [(data/foo ?v)]]}          \n       db))\n```\nI want to validate my datoms with map like this:\nclojure\n{:ns/key string?\n :ns/key1 (fn [x] true)\n :ns2/key int?\n;;...\n}. Yep, I can use index scan for this kind of validation.. Tricky workaround:\nclojure\n(let [db (-> (d/empty-db)\n             (d/db-with [{:a 0\n                          :b 0}]))]\n  (d/q '{:find  [?a ?fn ?res]\n         :in    [$ [[?a ?fn]]]\n         :where [[_ ?a ?v]\n                 [(clojure.core/apply ?fn ?v []) ?res]]}\n       db [[:a inc]\n           [:b dec]]))\n. @tonsky Do you have an idea how to fix this? I may help.. Thanks!. ",
    "Cortys": "Yes, it seems to work as expected irrespective of the number of unified tempids:\nclojure\n(deftest unification\n  (let [n 100\n        db (d/db-with (d/empty-db {:x {:db/cardinality :db.cardinality/many}\n                                   :key {:db/unique :db.unique/identity}})\n                      (concat ; Allocate n tempids:\n                              (map #(vector :db/add (- %) :x %) (range n))\n                              ; Unify them:\n                              (map #(vector :db/add (- %) :key \"foo\") (range n))))]\n    ; All n tempids are resolved to the same entity:\n    (is (= (d/pull db '[*] [:key \"foo\"])\n           {:db/id 0 \n            :key \"foo\"\n            :x (vec (range n))}))))\nAs far as I understand this works because retry-with-tempid is called after each addition of :id.\nThis effectively unifies one tempid -t with [:id \"foo\"] at a time. The old entity id allocated-id that -t was assigned via [:db/add -t :x t] is replaced with upserted-id, i.e. the id of [:id \"foo\"].\nIn total this performs O(n^2) operations but at least it works reliably. I'm however not quite sure why the unification order matters.. ",
    "mpenet": "Criterium quickbench, jvm. ",
    "tomconnors": "Yes, I am using datomic eids on the frontend, and I agree I should stop doing that.\nIMO datascript should either explicitly not support longs as :db/id values by documenting the accepted values and throwing if values outside that range are used or the current behavior should be considered a bug.. Not sure I understand. 285873023227265 is a valid number in js, it just happens to be larger than a 32 bit int. So couldn't datascript just complain if given an entity id above 2,147,483,647? Or, ideally, accept any valid js number as an entity id.\nIf this is a wontfix type of thing, maybe a note about it in the docs would help someone in my situation in the future.. "
}
{
    "tonyg": "I've added the channel as the first argument rather than the last.\n. Yep, you have to get rabbitmq-codegen from hg.rabbitmq.com. I've just added a stanza to the Makefile that will do this, I'll commit it in a second.\n. Fixed with 19fd60e33f41d339b9eb9a56fc2a613207e2e20c.\n. Python 2.4 support. closed by 2d2550558d7c66d18dc9a519e72a93c196a170a1\n. Thanks for the patches! Gratefully applied.\n. Does this still happen in current pika? I can't see how the dispatcher instvar could be anything but a RabbitDispatcher instance.\n. Argh! I missed that entirely :-) I'll take another look.\n. Only close the dispatcher if it hasn't already been closed. closed by 20d33074b2405b44d7873e0457dbbd3c9b0cce8e\n. Thank you for that. Embarrassingly, it turns out that I had been using asyncore.loop() directly instead of pika.asyncore_loop(), which means that pika's asyncore timeouts weren't firing. It should be fixed now: please reopen or refile if that's not the case.\n. Sorry about the bug in basic_get! And sorry it's taken so long to address it. I've just committed a few changes which make basic_get do the right thing. Regarding basic_ack, there's no response expected from the server, so None is about all it can usefully say. The credentials object at login is a stab at breaking away from the endemic hardcoding of the PLAIN SASL mechanism that most other Rabbit libraries have.\nSo I hope that clears things up a bit :-) I'm about to add a bit more documentation, too; please do get in touch or file another issue if you would like more clarification, or of course if basic_get is still broken in some way.\n. Hmm. This is interesting. There's a dependency on rabbitmq-codegen which would need to be automagically pulled in somehow. Any suggestions? I have a few vague ideas, but nothing specific enough yet.\n. That is an excellent point, and all by itself will make working with pika more approachable for people. Thank you for reminding me of that idea. I think that's what I'll do.\n. Check in generated spec.py so pika can be used OOTB. closed by 863a019d1e7d993573088e3acab00dd72840f7b6\n. Closed by 2e30b67570ef62e31c684fed1d85ccc6bcdcd9e6 and a15276058750b3683e9787ea79d629aa7528fc49.\n. May not be worth it - 0-8 is dead, and 0-9-1 is stable...\n. Flush outbound_buffer when performing async operations. Closed by fa15758.\n. Yep ;-)\n. I think you want \"tablesize += 9\"!\n(See also http://github.com/tonyg/pika/issues#issue/3 for the 0-9-1 table field types.)\n. ... and struct.pack('>cQ'), as well.\n. The corresponding clause in the server is\nparse_field_value(<<\"l\", Value:64/signed, Rest/binary>>) ->\n    {long, Value, Rest};\nso maybe\nelif isinstance(value, long):\n    pieces.append(struct.pack('>cq', 'l', value))\n    tablesize += 9\nwould work?\n. Actually I'd really appreciate your thoughts on issue 3 as well. My main worry is how to support byte, short, and the other stupid integer-widths* in as pythonic a way as possible. Should there be a pika.ByteHolder class, for instance? Or a pika.Table with a setByte method etc? How best to mark a quantity for encoding in a certain way?\n- In my (very much minority, it seems) opinion all these fixed-width encodings should go, replaced by a single variable-width integer encoding. Languages which restrict integer widths are then free to choose how they map to an API, and sane languages can just treat ints as ints as ints...\n. Hmm, yes, Marek reported this long ago and I never got around to doing anything about it. I'll see what can be done. Any pointers?\n. Actually it's odd that published messages aren't sent before the channel close! Both asyncore and blocking adapters should drain the outbound_buffer, which sends the publishes and the Channel.Close method, before waiting for the reply.\nCan you attach a small program I can run to reproduce the problem?\n. Flush outbound_buffer when performing async operations. Closed by fa157582e7545a139688c7e006902cd82486fa49 and #10.\n. Correct struct.pack - closed by 6a2c0c82b99f0666665dd253cfb7f4b8900b192c\n. Yep, sounds like the heartbeat parameter should be an int with 0 meaning disabled. Presumably code supplying bool values should have them cast to int to provide some kind of backwards-compatibility. It looks like a small code change, but since it has consequences for the API, Gavin, what do you think? Change ConnectionParameters's __init__ method's heartbeat argument to be int instead of bool?\n. Can you give us a little test-case that we can use to find the problem, please? I would guess that a program demonstrating the problem could be written in fewer than 10 lines.\n. @nanonyme: Yes. The coercion was happening implicitly (and, I must say, unexpectedly) as part of \"''.join(pieces)\" in the frame marshalling code.\nThe underlying problem is that supplying a unicode string as a message property or method parameter is an error; I've got a tentative fix I'll apply to my own pika fork. @gmr, when I've committed it to my fork, would you mind reviewing it and pulling it if you think it's acceptable?\n. ",
    "domas": "Yea, it works fine now tnx.\n. ",
    "majek": "Here's how I updated the table.py code to do that:\nhttps://github.com/majek/puka/commit/2e18ff22cd9959288a12a8fdd7c92c5019c13605#L0R149\n. I'm not really sure if the code you suggested does the right thing. Check out tests on issue #27.\n. fixed on issue #27.\n. Ask: I can confirm that it works. Here's a simple test:\nhttps://github.com/majek/dump/blob/master/pika/test_table_encoding.py\nDoes it convince you?\n. gmr: yes, it should cast the value to long on response, my fault.\nask: I'll try that.\n. ask: x-expires seems to be working fine with long integer:\nhttps://github.com/majek/dump/blob/master/pika/test_table_encoding.py#L17\n. The spec is contradictory. It says about decimals that: \"They are encoded as an octet representing the number of places followed by a long signed integer\", but the grammar contradicts that and says: \"decimal-value = scale long-uint\". \nWe should treat the decimal value as\u00a0signed\u00a0integer. So instead of\n    struct.pack(\">cBI\")\nThere should be\n    struct.pack(\">cBi\")\n. You're right. It should be:\ndiff --git a/pika/table.py b/pika/table.py\nindex 49fcfa3..bef77f3 100644\n--- a/pika/table.py\n+++ b/pika/table.py\n@@ -79,7 +79,7 @@ def encode_table(pieces, table):\n             else:\n                 # per spec, the \"decimals\" octet is unsigned (!)\n                 pieces.append(struct.pack('>cBI', 'D', 0, int(value)))\n-            tablesize = tablesize + 5\n+            tablesize = tablesize + 6\n         elif isinstance(value, datetime.datetime):\n             pieces.append(struct.pack('>cQ', 'T', calendar.timegm(value.utctimetuple())))\n             tablesize = tablesize + 9\n. I added some tests for that on issue #27.\n. Master.\n. In this code I tried to fix what I knew was broken in pika: decimals, long integers. For the array type the code in async-refactor branch uses \"encode_table/decode_table\" which encodes stuff differently than other clients. You don't have to use my code, but at least please take a look at the doctests! (to make sure this codec encodes stuff the same way as other clients)\n. AMQP 0-9-1 says:\n\n4.2.5.4 Timestamps\nTime stamps are held in the 64-bit POSIX time_t format with an accuracy of one second. By using 64 bits\nwe avoid future wraparound issues associated with 31-bit and 32-bit time_t values.\n\nPika code treats the value as seconds:\n\nelif kind == 'T':\n       value = datetime.utcfromtimestamp(struct.unpack_from('>Q', encoded, offset)[0])\n\nWhy do you say that timestamp type is microseconds?\n. ",
    "gmr": "Added in commit 8392551 to async-refactor branch.  Thanks majek.\n. Will take a stab at this.\n. Then I'll close this, if you change your mind, reopen?\n. Investigated now?\n. Applied majek's changes.\n. It does implement the \"l\" long-long-uint data type but unless I am just missing it, it appears the doctest tests do not test for this specifically. I will look to add a test to the doctest that does this.\n. On closer inspection of the doctest there is a long reencode test. I added one to encode only test. Both pass, however, I noticed that table.py does not cast the value to a long so it comes back as an int of the same value. Any opinion on if it should cast it back to a long type?\n. Change applied for next version per pull request.\n. This has been fixed in the next version currently in the async-refactor branch.\n. All of this code has been refactored quite a bit. If you're interested, check out the async-refactor branch.\n. Can you provide distilled sample code that'll demonstrate the behavior you don't like? Pika has no knowledge of threads, so as long as your thread code isn't trying to access your pika connection in a thread where the thread doesn't have access to it, there shouldn't be a problem.\n. Re-open if it's still an issue.\n. Hi Sigurd,\nThanks for the patch. I've applied the concept to the async-refactor branch which is slated to be the next version of Pika.\n. Still tweaking this a bit, will create a single commit for a pull in a branch when I'm satisfied it's working as I expect.\n. This has been changed for the upcoming release. New tests are both functional and unit and include documentation. I'm going to close this out.\n. Tested this against the asyncore-refactor branch and fixed a problem there. Is now working as expected in that branch which will be the next major release of Pika.\n. Could you check out the async-core branch and run examples/demo-receive.py and see if this behavior exists in the new branch?\n. I do not see this behavior anymore in the new branch and the asyncore class has been rewritten. In addition, it will be the recommendation to use the SelectConnection class instead of the AsyncoreConnection class in the future.\n. The API has changed with regard to any of the async drivers, and now uses callback passing style development. Please check out the docs at http://tonyg.github.com/pika\n. Fixed in async-refactor commit 75f80e1\n. Applied to async-refactor branch in commit 809e87b\n. Are these against master or async-refactor?\n. I've done some updates to table.py taken from the puka fork in the async-refactor branch. Can check that out and apply changes to that for a pull?\n. Applied to master and will port over to async-refactor.\n. In the next version, which I will be releasing an alpha of this week, you can register for callbacks on Channel.Close an Connection.Close. If there is a adapter level (asyncore) timeout, it will invoke the connection close event passing the socket error that occurred, which clients which added callbacks to the on close event can receive.\n. Applied to both master and async-refactor.\n. Actually impacted all methods expecting synchronous replies in BlockingAdapter. Fixed in commit 5e519f0 and will be included in 0.9.3 which I'll push out by the end of the week. In the interim you can install HEAD by doing:\n    pip install git+git://github.com/tonyg/pika.git#egg=pika \ntest.py:\n    import pika\n    from pika.adapters import BlockingConnection\nparameters = pika.ConnectionParameters(host='localhost')\nconnection = BlockingConnection(parameters)\nchannel = connection.channel()\nreply = channel.queue_declare(durable=False,\n                              exclusive=False,\n                              auto_delete=True)\nprint 'My new queue is : %s' % reply.method.queue\nconnection.close()\ngmr-0x04:pika gmr$ python test.py \n    My new queue is : amq.gen-D8637ho0Box93xDTMRjVDA==\nSorry, should have read the ticket more carefully. Thanks for the bug report.\n. I'll see if I can throw something together in the next day or so.\n. There is a new required parameter, on_open_callback. The channel_number is optional.\nhttps://github.com/tonyg/pika/blob/master/pika/connection.py#L493\nSee the first example:\nhttp://tonyg.github.com/pika/communicating.html\n. @dgorissen: if you don't mind please try and grab the master branch and see if it addresses your issue.\n. The first UserWarning is a duplicate call to Channel.Close inside Pika, I need to track that back.\nThe 2nd one is Pika trying to let you know that it thinks that RabbitMQ is applying TCP back-pressure. RabbtiMQ applies tcp back-pressure when you're publishing too fast for the available memory. Pika's just looking at a rolling window of average frame size and lets you know when you have passed a threshold of too much data in the buffer.\nOdd on the key error, I'll try and reproduce prior to 0.9.5's release, though I really want to get it out the door.\n. Hmm you shouldn't have to do that, it tells me you're receiving frames on a different channel than you're sending. The code is a bit hard to follow, how is channel getting in tmp[0]?\nYou might benefit from using the Blocking Adapter.\n. I've applied what you've provided in spirit, albiet in a bit of a different way. Thanks for the patch and let me know if you have any issues with the change in 0.9.3.\n. Thanks and applied!\n. In my preliminary testing, I am not finding any problems with using unicode anywhere. I'm writing functional tests for this now and will close the issue. If you see any issues with 0.9.6 please update this ticket.\n. I have confirmed with your gist, will see what the difference is between your gist and the unittests, integrate that difference and go from there.  It is worth noting that https://github.com/pika/pika/blob/master/tests/functional/unicode_tests.py runs using entirely unicode values.\nPerhaps it's something in Blocking Connection.  I'll update this ticket when I find it.\n. Hmm in looking at the test code, the difference is the test is assigning it specifically as unicode vs not:\n\n\n\nexchange = \"\u0623\u0631\u0646\u0628\"\nrepr(exchange)\n\"'\\xd8\\xa3\\xd8\\xb1\\xd9\\x86\\xd8\\xa8'\"\nexchange = u\"\u0623\u0631\u0646\u0628\"\nrepr(exchange)\n\"u'\\u0623\\u0631\\u0646\\u0628'\"\n\n\n\nI don't believe an explicit cast is the correct behavior in every case, it's probably better to catch the exception on the unicode error and then do the casting, from a \"pythonic\" behavior sense.\nIn Python 3 we could use a byte data type and it wouldn't matter, however since we are constrained in this version to support Python 2.4, I'll try and come up with something that supports those in the most idiomatic way possible.\n. The issue comes down to the use of str vs unicode in the frame encoding and there are not many good ways to fix this in the core 0.9 tree. I have this fixed in the \"2\" tree which i'm figuring out next steps for. I might be able to back port the changes, but it's a fundamental change in everything from the codgen to the encode/decode parsers.\n. Sorry, this is still an issue, working on an appropriate fix.\n. This is now correctly addressed in master, sorry for the delay.\n. Fix made, needs unit tests\n. I need to think on this change, off hand it seems that it could cause cross thread issues where your messages could be delivered to the wrong channel. I am working on fixing the thread safety issues in 0.9.x pika.\n. inferno-: you can't have a connection in one process and try and consume from another.  Use one connection per process.\n. Pika is now thread safe again as of fc6eb5134349fffdaf4d09f7f12b0a85da7bc3ee. I have tested with multiple threads in multiple processes at the same time.  Also nosetests now runs successfully in one python interpreter for all tests, indicating that the thread-safety issue is resolved.  These updates will be in 0.9.5 to be released soon.\n. Removing all of the pika.log functionality from the demos and cleaning them up for 0.9.5, Sorry for the confusion.\n. Applied in 2834ef9a9e11bb580e8d\n. This is fixed in master and will be in 0.9.5 to be released soon.\n. I'll go hunt back and see what I can find, but the only things keeping 0.9.5 from being released is my updating the documentation, doing a full release test against every RabbitMQ version from 2.0+, which I intend to do on Monday, and GitHUb moving it to its new location at http://github.com/pika/pika.\nIt should work, if you just want to try using master.\n. There is already a fix for this in the git repo and will be in the next fix. Thanks for the report.\n. Timeout support has been added to BlockingConnection in commit d695185aab90a625ce13 and will be in 0.9.5.\n. Thanks, applied.\n. I'm still working on trunk, this is intentional.\n. Thank you, this was applied.\n. Yes this is fixed in the soon to be released 0.9.5. If you're brave feel free to try and run this from master.\n. This is a known bug in 0.9.4 and is fixed in 0.9.5 which will be released soon. If you're brave, feel free to test the master branch.\n. This is a bug in the current version. If you'd like to get the new version with this fixed in your hands, checkout the master branch. I should be releasing 0.9.5 this week.\n. Yes, there was a bug in previous versions that did this, are you using 0.9.5?\n. The 0.9.5 version in pypi is broken? I've installed it without issue. Can you provide a backtrace?\n. Thanks for checking :)\n. 2.4->2.6 . Indeed your method is preferred. I had not intended to release a new version of the 0.9 series, but I will change this and commit to master. I'll sit on 0.9.5 for a bit and see if there are any other issues before releasing 0.9.6.\n. isCallable is deprecated as of Python 2.0.\nI've updated this to test a couple of ways, first using collections.Callable if it is available (2.6+) and hasattr call for 2.4 and 2.5. This tests clean for me and is in commit c71c56b0cc48f2c9c612.\n. From the traceback it looks like you're in an active basic consume and issue close?\nIf that is the case, can try sending a basic.cancel and waiting for that to finish before issuing the close? Close should shut everything down cleanly, trying to figure out an edge case where the channel can go away before the cancelok.\n. In essence, the behavior change has to do with the new stop_consuming method changing the blocking behavior of start_consuming. The bug was that the path of being able to close while consuming never had a path to run before. This is fixed in  f412014. Feel free to compile from source if you need it sooner than 0.9.6, which should drop in the next week or two.\n. You can get around this by specifying the connection poller as select.select for now. I'll look to get a fix in for 0.9.6.\n. Applied for 0.9.6, thanks for the patch.\n. Err wrong button, sorry.\n. There is a set flow to disconnecting, what are you trying to do specifically? Is this related to SimpleReconnection not working?\n. Yeah, there's a bug in SRS handling in 0.9.5. I'll be fixed for 0.9.6.  Will note this ticket in the commit when I fix it (hopefully in the next day or two).\n. Reconnection strategies are being removed in favor of examples of how to handle yourself.\n. Removing reconnection strategies in favor of a demo for how to implement at the client level.\n. I'll take a look at this for 0.9.6\n. This only fixes the async stack, working on BlockingConnection.\n. Thanks for the update!\n. Interesting, care to provide a patch? If you fork and commit, I'll review and apply if I don't see any issues. Sounds like you went about how I envisioned adding stuff to credentials.py.  Thanks!\n. I actually wrote the credentials module with external authentication support built in. \nYou could use your own class extending PlainCredentials and add it to the VALID_TYPES list without ever touching Pika's code.\nThis does give me an idea of what you're doing, I don't think it's going to add to the module.  I'm thinking documentation on how to write your own credentials object will bridge the gap.\n. If you want very verbose logs you can import pika.log and call pika.log.setup which is a passthrough (with some extra options) for logging.basicConfig http://docs.python.org/library/logging.html#logging.basicConfig. If you set it up with logging.DEBUG you will find it to be very verbose, but it should give me a clue as to what's ups. If you capture that file and give me a place to download it, I can see what's up.  Note that it will include all your data pushed over the queues.\n. I did, I didn't see anything off hand that explained the behavior. I'm working on having something that might help in 0.9.6 if you want to test with that. It might be worth just seeing if the same behavior happens in 0.9.6p0 (Git master) right know.\n. Can you provide a traceback of the error or do the messages just not come through?\n. Happy to look into it further. 0.9.6 should be out soon, if you're able to test with it.\n. This also may be inline with a bug I discovered today where BlockingChannel may send messages continuously without regard to inbound frames from RabbitMQ. I have fixed that in commit fc6bc1b.\n. Hmm I think I know what this is, has to do with changes to the TornadoConnection object and the recursion bug I found.  Will figure out a better way to handle it.\n. This has been fixed in master which I've not pushed yet... have a few things to do before I'm comfortable with people trying it out\n. The error is because you're trying to write data to the socket after it has gone away. Most likely if this is not intentional, your broker is disconnecting you. Have you looked in your rabbit logs?\nRegarding the proper format for the logging call, the preferred method of passing arguments to debug, error, info, warn, in the  logging module, using a comma and passing the arguments as individual arguments is the canonical way to pass the data, not constructing the string prior to passing it.\n. Right, what is happening is you are disconnected from RabbitMQ but still trying to use Pika on the tornado IO loop. You need to either 1) reconnect the pika connection if it's not intentional or 2) delete the pika connection when you tell it to disconnect.\nWhat's the use case for the disconnection while your main app is still running?\n. This is fixed I think.\n. You indeed found an issue and I've fixed it. Sorry for the very long delay. \n. Cool thanks, will look at adding.\n. Couple of things to be aware of:\n1) self._connection.channel() creates a new channel each time it's called\n2) Even though blocking connection is blocking the protocol is asynchronous. You've told the server to send you another message by acking the existing message and then shutdown the consumer. If you change the order and do stop_consuming, then ack, you might get better results.\nThat being said, I'll make the library more gracefully handle messages received after a consumer is removed.\n. This is fixed in 0.9.6\n. As I'm diving deeper in Pika 2, what should I be considering in making something that will plug in with twisted as easily as possible?\nAlso take a look at the Asyncore, Tornado and Select connection adapters and let me know if you don't think it'd be as easy as an assignment of the Twisted reactor as the ioloop.\n. This is fixed in master, sorry for the delay.\n. I have a few more fixes to commit. I sent an email regarding removing ReconnectionStrategies to the RabbitMQ list, havent' seen a response. Should I not have any objections or new bugs, I'm planning on releasing 0.9.6 next week.\n. Sure, makes sense to me.\n. This is fixed.\n. lxml is an XML parser used to parse the AMQP specification XML document to generate the docstrings in the codegen process.\nI'll document use and installation, however for now you need to have libxml2 installed and then a pip or easy_install lxml should get you going.\n. This should be fixed in pika master.\n. Unfortunately this does not pull cleanly. I am looking to pull as much in as makes sense and will credit you for the changes made.  Thanks for the contribution, the site of the pull request made this a bit daunting ;-)\n. This has been addressed\n. Because pika is asynchronous at its core, exceptions will be raised when the events that cause them occur, which may happen out of what would seem like the order you expect them in.\n. Please run the following code:\n```\n!/usr/bin/python\nimport pika\nprint pika.version\n```\n. Thank you for verifying, there have been numerous issues like this where users report they're using the pypi installed 0.9.5 and in fact, are not. I'll try and reproduce your problem.\n. Thanks for the test case, I've been able to reproduced and it is fixed. Sorry for the long delay.\n. I am working on Pika 2 to support python 2.6->3.x. Does this break any 2.4+ compatibility? We need to leave that in the 0.9 branch.  Thanks for the contribution!\n. Sorry, I can't support Python 3 and address the other issues at this time. I will make sure Pika 2 supports 2.6 -> 3.x.\n. Thanks\n. If you are working in an asynchronous fashion, you can not suspend the python interpreter in the fashion you are, otherwise no events will get processed.\nIf you have real work that blocks execution in the python interpreter, you might want to consider using threads and keeping all pika use in one thread and pushing your blocking work to the other using queues http://docs.python.org/library/queue.html\n. Same here.\n. This issue should be fixed at this point, sorry for the delay in response.\n. Thanks!\n. Sorry for the delay, I've added a fix.\n. Interesting, more details would be good... which connection adapter were you using?\n. No, I've not spent any time with gevent yet and want to get the growing buglist tackled before doing so.\n. Closing due to lack of time.\n. Hmm, so the reason I went with warning.warn is it can be caught and dealt with from a code perspective.\nImagine the case where you want to, in code, do something with regard to back-pressure or the tornado io loop issue.\nMoving to logging.warning would remove the ability to do that, no?\n. Will close this issue and include information on how to turn off the warning in the documentation. If you want to discuss further, please re-open.\n. After some investigation, it looks like your main issue is your time.sleep.  In master (0.9.6pre1) with a slight change:\n```\nstdlib\nimport time\nimport logging\nPika\nfrom pika.adapters import SelectConnection\nfrom pika.connection import ConnectionParameters\nclass ReconnectionRunner(object):\n    def init(self):\n        self.host = '127.0.0.1'\n        self.virtual_host = '/'\n        self.exchange = 'amq.direct'\n        self.routing_key = b''\n        self.counter = 0\n        self.conn_params = ConnectionParameters(self.host, virtual_host=self.virtual_host, connection_attempts=None)\ndef run(self):\n    self.conn = SelectConnection(self.conn_params, self.on_connected)\n\n    try:\n        self.conn.ioloop.start()\n    except KeyboardInterrupt:\n        self.conn.close()\n\ndef on_connected(self, conn):\n    self.conn.channel(self.on_channel_open)\n\ndef send_message(self):\n    if self.conn.is_open:\n        self.counter += 1\n        message = \"Hello World #%i: %.8f\" % (self.counter, time.time())\n        self.conn.ioloop.add_timeout(1, self.send_message)\n        logging.info(message)\n    else:\n        logging.info('Connection is closed')\n\ndef on_channel_open(self, channel):\n    self.send_message()\n\nif name == 'main':\n    logging.basicConfig(level=logging.INFO)\n    ReconnectionRunner().run()\n```\nOutput:\ngmr-0x02:pika gmr$ python test.py \nINFO:pika.adapters.base_connection:Connecting fd 3 to 127.0.0.1:5672\nINFO:root:Hello World #1: 1348803426.84262705\nINFO:root:Hello World #2: 1348803426.84271693\nINFO:root:Hello World #3: 1348803427.84378910\nINFO:root:Hello World #4: 1348803428.84498692\nINFO:root:Hello World #5: 1348803429.84631896\nINFO:root:Hello World #6: 1348803430.84748793\nINFO:root:Hello World #7: 1348803431.84881592\nINFO:root:Hello World #8: 1348803432.84926796\nINFO:root:Hello World #9: 1348803433.84974909\nWARNING:pika.connection:Disconnected from RabbitMQ at 127.0.0.1:5672 (320): CONNECTION_FORCED - Closed via management plugin\nINFO:root:Connection is closed\n. Code would be helpful. This is not a normal behavior and I wonder if there is something in your code that is more specific to what the problem could be.\n. Thanks, I will apply a fix for this in 0.9.6, working on that now.\n. I believe this is fixed, but need to verify.\n. I've added one more layer of validation, it will indeed be in the forthcoming 0.9.6 release. If you'd like to test your use case, I'd be most appreciative.\n. Thank you for the patch\n. This was addressed in 41b32922 and b0ec60a7\n. Sorry for the delay in reply but:\n[DEBUG    2011-12-10 23:43:05,096  #46394] CallbackManager: Removed {'one_shot': True, 'handle': <bound method SelectConnection._on_remote_close of <pika.adapters.select_connection.SelectConnection object at 0x100ad1c90>>} for \"0:Connection.Close\"\n[DEBUG    2011-12-10 23:43:05,096  #46394] CallbackManager: Removed empty key \"0:Connection.Close\"\"\n[DEBUG    2011-12-10 23:43:05,097  #46394] CallbackManager: Calling <bound method SelectConnection._on_remote_close of <pika.adapters.select_connection.SelectConnection object at 0x100ad1c90>> for \"0:Connection.Close\"\n[DEBUG    2011-12-10 23:43:05,097  #46394] CallbackManager: Removed {'one_shot': True, 'handle': <bound method Channel.__on_close of <__main__.Channel object at 0x100ad1fd0>>} for \"1:_on_channel_close\"\nIt looks like RabbitMQ is closing your connection on you, perhaps because you're redeclaring an exchange that is already declared with different parameters?\nThe other errors are most likely just due to not having a graceful way to handle remote closes without your own callback for that.\nIf this is still an issue for you, can you please confirm?\n. Thanks for the clarification and fix.\n. 0.9.6 is coming soon.\n. IIRC we're matching the protocol in definitions, but I'll double check and make sure it's not lower in error.\n. AMQP 0.9.1 has prefetch count as a short:\nhttp://www.rabbitmq.com/amqp-0-9-1-reference.html#basic.qos.prefetch-count\n. Most likely it was a more core bug in Pika 0.9.6pre0. I just tested without an issue.\ndemo_send: Sending \"Hello World #1721\"\ndemo_send: Received delivery confirmation for message_id 1721, success: True\ndemo_send: Sending \"Hello World #1722\"\ndemo_send: Received delivery confirmation for message_id 1722, success: True\ndemo_send: Sending \"Hello World #1723\"\ndemo_send: Received delivery confirmation for message_id 1723, success: True\ndemo_send: Sending \"Hello World #1724\"\ndemo_send: Received delivery confirmation for message_id 1724, success: True\n. It appears, and I'm going back a bit, that the flush_outbound method is no longer used in the original intent so the removal was to surface issues like this. If you have example code that demonstrates your use case and problem, please send it over and I'll take a look.\n. You can not use sleep(3) in an async adapter, it will muck things up.  To sleep 3 seconds, you need to do self.conn.add_timeout(3, self.other_method)\n. So originally Pika did not stop the event loop, then it was requested that it did. I think the right solution is a flag in the Tornado adapter to tell it to stop the ioloop on disconnect.\nAny disagreement with that?\n. I added a stop_ioloop_on_disconnect flag to all async adapters. Sorry for the delay.\n. Didn't you comment on the message list that you had a simple patch that fixed the behavior you were seeing?\n. Thanks for the update.\n. Yes, if you have a patch for the TornadoConnection adding StackContext, please submit. I've been wanting to add the StackContext to the Torando adapter but have been more focused on overal cleanup and testing.\n. Going to keep this open for the StackContext addition\n. Closing, I don't think I'll get StackContext in before 0.10\n. This has been fixed somewhere along the line, indirectly. Sorry for the delay in reply.\n. Please paste the traceback.\n. I have verified this exists when no parameters are passed in and will address it.\n. This is a bit involved, I will circle back around to it, but it looks good!\n. I need to add the stack_context support, but I reduced the Tornado connection support. I will keep the StackContext ticket open. Let me know if this addresses your issues.  Thanks for your patience.\n. Thanks for the fix\n. Thanks for the patch\n. Thanks for the patch\n. BlockingConnection will now raise an exception when it is disconnected, this should be addressed.  Please reopen if not.\n. If you are using an async connection adapter, you add a callback with connection.add_on_close_callback to be notified when your connection closes. If using BlockingConnection you should catch pika.exceptions.AMQPConnectionException\n. Yes, it was removed from master. Thanks for the heads up on BlockingConnection not being up to date.\n. This is no longer broken in blocking connection, thanks.\n. Thanks for the patch\n. Thanks for the bug report, I've fixed this in 86a860abf5\n. Fixed in commit b66cfae7f4\n. Can you retest with 0.9.6p1 from master and see if this still happens?\n. Please reopen if so.\n. It's not dead, I've just been busy trying to add test coverage while keeping Pika 2 moving along.  We have so many patches in 0.9.6, I've not been comfortable releasing it.\n. This has been fixed somewhere along the line with the changes to BlockingConnection. Sorry for the late reply.\n. This was added in c6181fbfb4\n. I think that the new TornadoConnection refactor should help, let me know if you still have issues. Thanks for your patience.\n. This is fixed/addressed in 64d7ddc5f261a2f9aa\n. This should be fixed in a864fb5726\n. Some of these patches made it in, some did't. Thanks for your contributions!\n. I need to add the stack_context support, but I reduced the Tornado connection support. I will keep the StackContext ticket open. Let me know if this addresses your issues.  Thanks for your patience.\n. Changed the way to handle this by having a stop_ioloop_on_close flag, so everyone that has a different opinion on how this can be handled, can handle it their own way.  Thanks for the patch and your patience.\n. You could do a passive exchange declare to see if the exchange exists. If your channel and connection are not closed, than RabbitMQ isn't flagging a problem.\n. basic_get does take a callback in everything but the BlockingConnection. Also, you shouldn't need to call start_consuming() with basic_get().  start_consuming is for using Basic.Consume.\n. 0.9.5 does support sending Confirm.Select with Channel.confirm_select()\n. Which version of python should I test this against?\n. Fixed in a864fb5726\n. It seems the spirit of this pull request is addressed in others.  Thanks's for the contribution.\n. Thank you for the fixes, due to the clean up the logic with regard to socket_timeout, the fixes are deprecated. I appreciate the contribution!\n. I could use some help with tests to validate the changes. That's the biggest holdup on 0.9.6 right now.\n. The particular issue outlined should be fixed, please test and reopen if not.\n. I think it's more appropriate to create a URIConnectionParameters object.\nparameters = URIParameters('amqp://user:pass@somehost/vhost')\nSSL on %2F host:\namqps://user:pass@localhost:5672/%2F/\nBut what about:\n- channel_max\n- frame_max\n- ssl_options\n- connection_attempts\n- retry_delay\n- socket_timeout\n- locale\n- backpressure_detection\n. Perhaps as query parameters? Will do a rough implementation as that.\namqps://user:pass@localhost:5672/%2F?channel_max=500\n. Can you please break these into a pull request per changeset? I'd like to apply some and not others. For example, mfkenny's has already been applied.\nThe _sanitize was in process of being removed.\n. Tests are broken, I have been rewriting in a new branch. I've fixed this error in master however.\n. Thanks, I meant to ticket this. After removal of Reconnection Services (see RabbitMQ list), I plan on fixing this and releasing 0.9.6, baring any issues that come out of the latest cleanup, bugfix commits.\n. This should be addressed.\n. Thanks for the bug report, fix coming in shortly.\n. Thanks, it's similar to what I did.\n. That's unrelated, is a logging module error, since there's a warning being logged before logging is setup. I'll remove the warnings.\n. re tests, I've been rewriting them from the ground up with a better methodology, not everything is covered. The master branch is significantly better off than where it was when that ticket was created and all knowing issues have been addressed. I think the asyncore and twisted connection adapters still need a bit of cleanup, testing and love and I want to clean up channel a bit more, then 0.9.6 will be released.\n. Thanks for the fix\n. Thanks for the change (and the test!)\n. It was, the IOLoop is supposed to be a singleton in Tornado:\nhttps://github.com/facebook/tornado/blob/master/tornado/ioloop.py#L19\nYou're running more than one ioloop in a single process?\n. It's also worth noting it's extremely trivial for you to create your own custom Tornado handler based on the existing one:\n```\nclass MyTornadoConnection(tornado_connection.TornadoConnection):\n    def init(self, parameters=None,\n                 on_open_callback=None,\n                 stop_ioloop_on_close=False,\n                 ioloop_instance=None):\n        super(MyTornadoConnection, self).init(parameters, on_open_callback,\n                                                stop_ioloop_on_close)\n        self.ioloop = ioloop_instance\ndef _adapter_connect(self):\n        \"\"\"Connect to the RabbitMQ broker\"\"\"\n        super(TornadoConnection, self)._adapter_connect()\n        self.ioloop.add_handler(self.socket.fileno(),\n                                self._handle_events,\n                                self.event_state)\n        self._on_connected()\n\n```\n. Thanks for the fix\n. Thanks, need to verify, Channel is the class I've not had a chance to clean up as much as I've had liked.\n. Are you aware of the Metronome plugin for RabbitMQ? It's an example plugin for RabbitMQ developers:\nhttps://github.com/rabbitmq/rabbitmq-metronome\nhttp://www.rabbitmq.com/plugin-development.html#plugin-hello-world\nIf that doesn't do what you like, I know of a few other projects that do the same type of thing, or am I missing the point?\n. Which version of RabbitMQ?  I had this implemented and just accepted a patch to remove it because the author claimed that RabbitMQ was sending an error on the CloseOk.\nhttps://github.com/pika/pika/pull/183\n. Added back in b63b4c8073da\n. This commit does not fix the behavior pointed out in the test. Trying to track back why it's happening.\n. The blocking code has been cleaned up significantly for 0.9.6, which is about to be released. You can test with your code by installing via pip:\npip install -e git+git://github.com/pika/pika.git@0.9.6p3#egg=pika-0.9.6p3 \n. The blocking code has been cleaned up significantly for 0.9.6, which is\nabout to be released. You can test with your code by installing via pip:\npip install -e git+git://github.com/pika/pika.git@0.9.6p3#egg=pika-0.9.6p3\nOn Mon, Oct 15, 2012 at 10:44 AM, Rikard Hult\u00e9n notifications@github.comwrote:\n\nWe ran a RabbitMQ cluster behind a load balancer and tried different\nconfigurations in the lb when suddenly the our web servers ran 100% cpu and\neach request were producing as much of the following line as it had time to:\nERROR:pika:BlockingConnection: Socket Error on 688: 10054\nfollowed by endless amounts of\nERROR:pika:BlockingConnection: Socket is closed\nWe are not able to reproduce it, but obviously there is an infinite loop\nsomewhere (we think in pika) which is not desirable?\npika 0.9.5\npython 2.6.6\nrabbitmq 2.8.7\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/186.\n\n\nGavin M. Roy\nChief Technology Officer\nhttp://www.meetme.com/\n100 Union Square Drive\nNew Hope, PA 18938\np. +1.215.862.1162 x263\nf. +1.215.862.0465\nhttps://www.facebook.com/pages/MeetMe/21931227129\nhttps://twitter.com/meetme\n    http://www.youtube.com/user/MeetMeVideos\nThe public market leader in social discovery. (NYSE MKT: MEET)\n. I am working on the documentation at the moment. Once I have that done, I am uploading to pypi.\nOn Thursday, October 18, 2012 at 5:53 PM, shreddd wrote:\n\nLooks good. Thanks! Any sense for when 0.9.6 will make it into pypi?\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/pika/pika/issues/187#issuecomment-9582759).\n. Will push out a fix shortly.\n\nOn Thursday, October 18, 2012 at 6:29 AM, senseysensor wrote:\n\nIn the latest git commit (e4e54af (https://github.com/pika/pika/commit/e4e54aff13)) I have the following exception when try start ioloop on SelectConnection (connection.ioloop.start()). Any advice?\nTraceback (most recent call last):\nFile \"/home/2gk/spider/www/AlarmSpider_p1/src/core/rabbit_mq.py\", line 608, in start_loop\nself._conn.ioloop.start()\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre4-py2.7.egg/pika/adapters/select_connection.py\", line 102, in start\nself.poller.start()\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre4-py2.7.egg/pika/adapters/select_connection.py\", line 385, in start\nself.poll()\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre4-py2.7.egg/pika/adapters/select_connection.py\", line 440, in poll\nself._handler(fileno, event, write_only=write_only)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre4-py2.7.egg/pika/adapters/base_connection.py\", line 280, in _handle_events\nself._handle_read()\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre4-py2.7.egg/pika/adapters/base_connection.py\", line 308, in _handle_read\nself._on_data_available(data)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre4-py2.7.egg/pika/connection.py\", line 794, in _on_data_available\nself._process_frame(frame_value)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre4-py2.7.egg/pika/connection.py\", line 848, in _process_frame\nself._deliver_frame_to_channel(frame_value)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre4-py2.7.egg/pika/connection.py\", line 508, in _deliver_frame_to_channel\nreturn self._channels[value.channel_number].transport.deliver(value)\nAttributeError: 'Channel' object has no attribute 'transport'  \n\u2014\nReply to this email directly or view it on GitHub (https://github.com/pika/pika/issues/188).\n. Fixed\n. Thanks, I'll fix the exception to read channel_max must be <= 65535 and > 0.\n\nWill check hearbeat interval.\n. This issue is duplicated by issue #198\n. Added repr to 8894961126\n. Which version?\nimport pika\nprint pika.__version__\nThis should not be in 0.9.6p5\n. No, I know what it is... I had seen it myself, but thought I had fixed it.  It's just a warning that the code issues when it notices a duplicate callback and bails on trying to add a new one.\n. This is nothing wrong with your code and it's not a serious problem with Pika. I do wish to correct the behavior and it is on my list of things to address prior to releasing 0.9.6. Basically there is a callback being added in two different locations when you are opening a channel.\nIt's nothing to worry about, the fact that you are receiving the message indicates that the callback manager is functioning properly and preventing the situation from creating a more serious bug.\nThis is not closed because I have not had time to trace down where the 2nd callback is being created from and intend to do so once I have the channel unittests fully committed.\n. Can you send the full traceback?\n. This may be expected behavior or not. This exception is raised when you try and do operations that require the connection to be open.\nWhat I'm trying to figure out from the traceback is if RabbitMQ is closing your connection on publish for some reason, in which case you should get a different exception.\n. So it sounds like you're being disconnected, perhaps due to timeout? Might want to tune up the timeout to .5 or 1 and see what behavior you find. You should probably handle the exception and reconnect however, as it looks like you're using it in a Django application, and so you'd want to automatically handle disconnects.\n. Which version?\nimport pika\nprint pika.__version__\n. Also are you letting the connecter \"breathe\" before doing anything or are you just opening and closing? I can lower the threshold of forced writes/reads, but performance will suffer.\nPerhaps the right thing to do is to force this after each non-publish call.\n. I've created a patch that forces data events on rpc commands. This should address what you were seeing, but it may slow performance.\n. Using the following code, I do not see a channel leak:\nimport pika\nconnection = pika.BlockingConnection()  \nwhile True:\n  channel = connection.channel()\n  channel.close()\n. So you received a socket timeout and didn't register a \"on channel close\" or \"on connection close\" event. What's your expectation of behavior if you're disconnected and not listening for asynchronous events?\n. What happens with this? https://gist.github.com/3919853 (Is rough and untested).\nBTW the specific error is a socket timeout, changing line 66 from using logging.INFO to logging.DEBUG would tell what specifically is happening callback wise.\n. Yeah but the issue is a socket.timeout -- do you mind trying with the asyncore adapter and the tornado adapter and see if you see the same behavior?\n. You might want to try increasing the socket timeout in the connection parameters then, say from .25 to .5:\nparameters = pika.ConnectionParameters(socket_timeout=0.5)\nMeanwhile I'll look to see if I can spot any reason the SelectConnection object would be more susceptible to timeouts.\nYou'e on OSX 10.6? Python version?\n. Thanks for getting back to me\n. Correct, you use the logging module to adjust the logger's settings. If you don't want to do it in code (I avoid that), check out the logger configuration mechanisms: http://docs.python.org/library/logging.config.html\n. You should inject connection.process_data_events() into the flow. Issuing a sleep does not allow the frames to be processed, it just causes the python process to sleep. You should avoid using time.sleep() in your code under most circumstances.\n. I added a new method to handle this. When you set a heartbeat, you're defining a contract with the broker. When you actually sleep for 9 seconds, you're violating the contract and RabbitMQ is disconnecting you. By using connection.sleep() you'll allow pika to continue to process events from RabbitMQ such as heartbeat frames violating the Heartbeat contract.\nimport pika\nconnection = pika.BlockingConnection(pika.ConnectionParameters(heartbeat_interval=3))\nchannel = connection.channel()\nchannel.queue_declare(queue='hello')\nconnection.sleep(9)\nchannel.basic_publish(exchange='',\n                                 routing_key='hello', \n                                 body='Hello world')\nprint \" [x] Sent 'Hellow world'\"\nconnection.close()\n. Thanks, I was actually afraid this was an issue based upon something I saw last night. I'll look to address for pre6 tomorrow.\n. This is a newly introduced bug in channel.Channel that I created when trying to force the correct behavior for the noreply flag when issuing RPC commands.\nYou see when you issue a RPC command like Queue.Declare, you can tell RabbitMQ that you do not want to receive a Queue.DeclareOk frame in response, that it should just do the work and be quite about it. This is not the default behavior however, and there is a bug where the channel.Channel class is behaving like you are passing in noreply but the RPC request that is being sent to the server is not telling RabbitMQ not to send a reply. Thus the Channel.DeclareOk reply is unexpected and the exception is being thrown. I will be fixing this, most likely at some point today.\n. This is a duplicate of issue #190\n. I'm trying to get it to pypi ASAP, this week ideally. 0.9.6 is on p6 at this point and can be installed via pip:\npip -e git+git://github.com/pika/pika.git#egg=pika-0.9.6p6\n. Thanks for the heads up\n. What connection adapter are you using? If you're using the blocking adapter, you're likely seeing it check (which it previously did not do) to see if there are any pending async frames from RabbitMQ to it. (Connection.Close, Channel.Close Heartbeat, etc).\n. If not, please let me know what adapter, platform (linux, osx, etc) and python version.\n. I've just added a new method to BlockingChannel: force_data_events(bool).  If you set this to False, it will bypass the check.  Be sure to read the docs:\nhttp://pika.readthedocs.org/en/latest/communicating.html#pika.adapters.blocking_connection.BlockingChannel.force_data_events\nPlease reopen the ticket if this does not address your issue.\n. Thank you for the detailed bug report. The issue was passing in non-unicode binary data caused the unicode decode error. The frame dispatcher should have caught the exception but did not.\n. It needed significant cleanup for 0.9.6 so I have pushed it to 0.9.7. I'd say it'll resurface some time in November.\n. This is tentatively going to make it back in with 0.9.10.\n. I've identified the problem and there are two ways to look at it.\nThe main issue is that you're issuing synchronous commands and not waiting\nfor the callback before issuing the next.\nHere's an example of how you could do it:\nimport logging\nimport pika\nclass Test(object):\n```\nEXCHANGE = 'message'\ndef init(self):\n    self._queues = ['this', 'test', 'other']\n    self._connection = None\n    self._channel = None\ndef connect(self):\n    return pika.SelectConnection(on_open_callback=self.on_open)\ndef on_open(self, unused_connection):\n    print 'Connection opened'\n    self._connection.channel(on_open_callback=self.on_channel_open)\ndef on_channel_open(self, channel):\n    print 'Channel opened'\n    self._channel = channel\n    self.setup_queue(self._queues.pop(0))\ndef setup_queue(self, queue_name):\n    print 'Declaring %s' % queue_name\n    self._channel.queue_declare(self.on_declareok, queue_name)\ndef on_declareok(self, frame):\n    print 'Binding %s' % frame.method.queue\n    self._channel.queue_bind(self.on_bindok, frame.method.queue,\n                             self.EXCHANGE, frame.method.queue)\ndef on_bindok(self, unused_frame):\n    if self._queues:\n        self.setup_queue(self._queues.pop(0))\n    else:\n        print 'Queues declared and bound, exiting'\n        self._connection.close()\ndef run(self):\n    self._connection = self.connect()\n    self._connection.ioloop.start()\n```\nlogging.basicConfig(level=logging.INFO)\ntest = Test()\ntest.run()\nThat being said, pika could handle this situation better. If you're not\npassing callbacks, it should create a stack of expected replies and dequeue\nfrom those replies when they've been received. I'm going to ticket this for\n0.9.7.\nGavin\n. Note for ticket: Action item is for synchronous RPC methods in pika.channel.Channel, to append a stack of expected replies and remove them upon receipt.\n. This duplicates issue #211 and #192\n. It should support them and might not be in the documentation as BlockingChannel extends Channel.\nsyntax is blocking_channel.basic_ack(delivery_tag=foo), blocking_channel.basic_nack(delivery_tag=foo)\nLet me know if it's just an oversight in documentation or if you're really having an issue.\n. I've seen a weird behavior, basic_nack is there, but is misbhaving. Will issue a fix for the issue related to what I am seeing.\n. Thanks for raising the issue, is fixed for 0.9.7 which should be released shortly.\n. Please provide the traceback.\n. You didn't get any exception raised about the Channel being closed before\nthat?\n. Ok, so this is expected. You need to setup a on_channel_closed callback.\nYou're getting the exception because you're not responding to RPC calls from RabbitMQ to you.  I'm almost done with a \"full stack\" publisher and consumer example that I will publish on my blog and put into the docs that should help.\n. Here's an example of how to handle connection and channel closures:\nhttps://gist.github.com/3988513\nIt looks like what you're trying to achieve is batched acknowledgment? \n. That's pretty weird. I ran your code without a problem:\nhttps://gist.github.com/3988967\nWhat version of RabbitMQ, python and pika are you using?\nOn Wed, Oct 31, 2012 at 2:27 PM, flopez notifications@github.com wrote:\n\nI've tried to do it with your example, but the result was the same. It\nseems that is not calling to on_connection_closed. Here is the code\nhttps://gist.github.com/3988798\nAnd the traceback:\nINFO 2012-10-31 19:26:11,462 main connect 51 : Connecting to\namqp://guest:guest@localhost:5672/%2F\nINFO 2012-10-31 19:26:11,462 pika.adapters.base_connection create_and_connect_to_socket\n164 : Connecting fd 3 to localhost:5672\nINFO 2012-10-31 19:26:11,466 main on_connection_open 91 : Connection\nopened\nINFO 2012-10-31 19:26:11,466 _main add_on_connection_close_callback 65 :\nAdding connection close callback\nINFO 2012-10-31 19:26:11,466 main open_channel 306 : Creating a new\nchannel\nINFO 2012-10-31 19:26:11,467 main on_channel_open 127 : Channel opened\nINFO 2012-10-31 19:26:11,467 main add_on_channel_close_callback 100 :\nAdding channel close callback\nINFO 2012-10-31 19:26:11,467 main setup_exchange 140 : Declaring\nexchange proban\nINFO 2012-10-31 19:26:11,468 main on_exchange_declareok 152 : Exchange\ndeclared\nINFO 2012-10-31 19:26:11,468 main setup_queue 163 : Declaring queue\nproban\nINFO 2012-10-31 19:26:11,468 main on_queue_declareok 177 : Binding\nproban to proban with proba\nINFO 2012-10-31 19:26:11,469 main on_bindok 289 : Queue bound\nINFO 2012-10-31 19:26:11,469 main start_consuming 276 : Issuing\nconsumer related RPC commands\nINFO 2012-10-31 19:26:11,469 main add_on_cancel_callback 187 : Adding\nconsumer cancellation callback\nINFO 2012-10-31 19:26:11,470 main on_message 226 : Received message # 1\nfrom None: Message\nINFO 2012-10-31 19:26:11,471 main on_message 226 : Received message # 2\nfrom None: Message\nINFO 2012-10-31 19:26:11,471 main on_message 226 : Received message # 3\nfrom None: Message\nINFO 2012-10-31 19:26:11,472 main on_message 226 : Received message # 4\nfrom None: Message\nINFO 2012-10-31 19:26:21,473 main acknowledge_message 208 :\nAcknowledging message 4\nINFO 2012-10-31 19:26:21,474 main acknowledge_message 208 :\nAcknowledging message 3\nINFO 2012-10-31 19:26:21,474 main acknowledge_message 208 :\nAcknowledging message 2\nWARNING 2012-10-31 19:26:21,475 pika.channel _on_close 779 : Received\nremote Channel.Close (406): PRECONDITION_FAILED - unknown delivery tag 4\nINFO 2012-10-31 19:26:21,475 main acknowledge_message 208 :\nAcknowledging message 1\nWARNING 2012-10-31 19:26:21,476 main on_channel_closed 115 : Channel\nwas closed: (406) PRECONDITION_FAILED - unknown delivery tag 4\nINFO 2012-10-31 19:26:21,476 pika.connection close 585 : Closing\nconnection (200): Normal shutdown\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/206#issuecomment-9956303.\n\n\nGavin M. Roy\nChief Technology Officer\nhttp://www.meetme.com/\n100 Union Square Drive\nNew Hope, PA 18938\np. +1.215.862.1162 x263\nf. +1.215.862.0465\nhttps://www.facebook.com/pages/MeetMe/21931227129\nhttps://twitter.com/meetme\n    http://www.youtube.com/user/MeetMeVideos\nThe public market leader in social discovery. (NYSE MKT: MEET)\n. Actually in looking at your logs, pika is doing the right thing:\n```\nINFO 2012-10-31 19:26:21,473 main acknowledge_message 208 : Acknowledging message 4\nINFO 2012-10-31 19:26:21,474 main acknowledge_message 208 : Acknowledging message 3\nINFO 2012-10-31 19:26:21,474 main acknowledge_message 208 : Acknowledging message 2\nWARNING 2012-10-31 19:26:21,475 pika.channel on_close 779 : Received remote Channel.Close (406): PRECONDITION_FAILED - unknown delivery tag 4\nINFO 2012-10-31 19:26:21,475 __main_ acknowledge_message 208 : Acknowledging message 1\nWARNING 2012-10-31 19:26:21,476 main on_channel_closed 115 : Channel was closed: (406) PRECONDITION_FAILED - unknown delivery tag 4\nINFO 2012-10-31 19:26:21,476 pika.connection close 585 : Closing connection (200): Normal shutdown\n```\nIt's not raising an exception and it's calling the callbacks to tell you that the channel was closed. Why RabbitMQ is saying unknown delivery tag is pretty odd. I'll send an email to the rabbitmq list.\n. Can you change the logging level to DEBUG for the pika logger?\npika_logger = logging.getLogger('pika')\npika_logger.level = logging.DEBUG \nAnd retest? I'd like to see the actual frames going back and forth.\n. I was able to recreate it, but not with the level of logging needed, and only after a few thousand messages. I'm leaning towards an issue in RabbitMQ, but will know more in a bit.\n. I was able to recreate with frame debugging on, it doesn't look like a logic error in pika, will see what the RabbitMQ folks come back with.\nhttps://gist.github.com/3989351\n. So this is pretty elusive. I'm working on trying to recreate on my home machine with the tracing tool and am about 325,000 messages in without doing so.\nOne thing you could be doing is acking the highest # you have and setting multiple = True\nso if you receive delivery_tag 1 through 10, doing a basic_ack(10, True) will ack 1 through 10.\nThis would speed up your consumer too.  I'll let you know when I recreate it and where it progresses from there.\n. Oh and  Matthias brought up \"acknowledging messages in reverse order of delivery is bad for performance\"\n. BTW if you want to join in on the fun of trying to find it you can download the Java client from:\nhttp://www.rabbitmq.com/download.html\nAnd then run the tracer:\nhttp://www.rabbitmq.com/api-guide.html#tracer\nIt acts as a proxy recording the protocol back and forth. If it happens for you, send in the output from the tracer.\n. I've been unable to find a cause or replicate consistently. Closing for now.\n. Thanks, late night editing I guess.\n. If you set the timeout to 10000 seconds, how long do you expect for it to take to timeout?\n. Sure, in pika 0.9.5, it would call socket.setblocking(1) which would override whatever was passed into settimeout. In 0.9.6 there is a connection timeout value and a socket timeout value.\nIn your example, you're bypassing pika's control of the socket and implementing it yourself, and Pika should have already connected to RabbitMQ by the time you're setting the timeout.\nThe main behavioral change you're running into, however, is the BlockingConnection trying to enforce protocol correctness, fixing a whole slew of bugs in 0.9.5. You've changed the socket timeout, outside of pika's ability to control it, and it's blocking on a read for 10,000 seconds checking for a RPC reply.\n. What are you trying to achieve with setting the timeout so high? You can probably achieve the same thing using Pika's API instead of bypassing it.\n. No worries. I'm going to mark this is as closed. If you have problems using Pika's API for changing timeouts, let me know.\n. The problem is type is a reserved word. All versions prior to 0.9.6 had a broken behavior of overriding the type method with the value of the type string. I will add it back with a deprecation warning, but will remove it prior to 1.0.\n. Dupe of #102. It is in the list of things to look at.\n. It does, it expects you to wait for a synchronous call to finish before calling another. I'm thinking about ways to make this easier to deal with.\n. The example at https://github.com/pika/pika/issues/204#issuecomment-9924606 shows one way, the other is to pass in \"nowait=True\" as one of the arguments.\n. Err, I missed that you were calling basic_consume. I'll try and get a patch out today.\n. The core issue is the same as #192. Working on a way to handle this better. Closing this ticket and tracking on #192.\n. I believe this was fixed in pull request #215.\n. You're being disconnected from RabbitMQ. The exception should have the reply_code and reply_text explaining why.\n. Thanks for the fix\n. Thanks for the fix\n. Confirmed, fix committed, is a bug in blocking_connection only.\n. Confirmed, this is due to the new callback behavior. Will get a fix out ASAP.\n. Edit: nevermind, I see the issue.\n. Those are debugging messages which can be useful when trying to figure out problems. You should get a logger and set the level for pika to a level you feel is appropriate.\npika_loger = logging.getLogger('pika')\npika_logger.setLevel(logging.WARNING)\nor try using the DictLogger config.\n. Sorry for the delay in fixing this.\n. Can you check your RabbitMQ logs and see if you can spot the source of why the Channel is being closed? It will usually indicate if it has closed the channel remotely and why.\n. You're getting a RPC command from the server that your consumer has been cancelled and you're not handling it. This is the correct behavior. You should be catching the ConsumerCancelled exception, as it's RabbitMQ telling you that it is no longer going to send you messages.\n. Can you elaborate on the use case of binding with an empty routing key? Like empty bodies, I don't know if this is valid and need to investigate.\n. Checked the spec and there are no rules around an empty binding key. I changed the logic to look for None instead of no value, which should address your issue.\n. Hmm I thought I had left it so it wouldn't do connection retries period (all of the async adapters). Will take a look.\n. Either RabbitMQ is not running or you're being disconnected on connect (due to a bad username/vhost/etc).\n. I can see that, I think it's one of those use cases that pika's never supported. I'll have to think about how best to handle this. I hadn't ever considered it valid, but will revisit the spec and dig a bit deeper on this.\n. Fair enough, I'll look to replicate and see why it's failing, I imagine it's because a body frame is not sent if body-size is 0, but we'll see..\n. The issue was RabbitMQ (at least as of 3.0) does not send empty body frames. I'll vet this against older versions but it should resolve the issue.\n. Hmm, I'm happy to move toward renaming callback arguments prior to 1.0 to make it consistent. \n. Wow that's an odd one, what is get_system_state() doing, passive queue declarations?\nIt looks like the connection to RabbitMQ is being remotely closed and the arguments that the exception should be getting with the close code and text are not passed in.\n. Thanks for the patch\n. Thanks for the clarification, the fix is to change the check from str to basestring.\n. @steveboyle I've changed this to be unicode, but what do you mean when host is valid ip4 address? What type of data are you passing in?\n. I'm going to assume it was a unicode string and that the patch fixes it. If it does not, please reopen.\n. Agreed, good find. Thanks for the fix\n. The main reason you see this is Python is not a real-time system and operates in a single thread. In async development, you do not have hard (or even) soft guarantees for timers firing when you want them to.  The timeouts are the first time pika can fire the event due to other processing in the Python interpreter.\n. I believe this is fixed in master at this point.\nPlease reopen with some test code if it's still a problem:\nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5671\nERROR:pika.adapters.base_connection:socket error: Connection refused\nWARNING:pika.adapters.base_connection:Could not connect due to \"Connection refused,\" retrying in 2 sec\nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5671\nERROR:pika.adapters.base_connection:socket error: Connection refused\nWARNING:pika.adapters.base_connection:Could not connect due to \"Connection refused,\" retrying in 2 sec\nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5671\nERROR:pika.adapters.base_connection:socket error: Connection refused\nWARNING:pika.adapters.base_connection:Could not connect due to \"Connection refused,\" retrying in 2 sec\nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5671\nERROR:pika.adapters.base_connection:socket error: Connection refused\nWARNING:pika.adapters.base_connection:Could not connect due to \"Connection refused,\" retrying in 2 sec\nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5671\nERROR:pika.adapters.base_connection:socket error: Connection refused\nWARNING:pika.adapters.base_connection:Could not connect due to \"Connection refused,\" retrying in 2 sec\nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5671\nERROR:pika.adapters.base_connection:socket error: Connection refused\nWARNING:pika.adapters.base_connection:Could not connect due to \"Connection refused,\" retrying in 2 sec\nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5671\nERROR:pika.adapters.base_connection:socket error: Connection refused\nWARNING:pika.adapters.base_connection:Could not connect due to \"Connection refused,\" retrying in 2 sec\nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5671\nERROR:pika.adapters.base_connection:socket error: Connection refused\nWARNING:pika.adapters.base_connection:Could not connect due to \"Connection refused,\" retrying in 2 sec\nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5671\nERROR:pika.adapters.base_connection:socket error: Connection refused\nWARNING:pika.adapters.base_connection:Could not connect due to \"Connection refused,\" retrying in 2 sec\nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5671\nERROR:pika.adapters.base_connection:socket error: Connection refused\nWARNING:pika.adapters.base_connection:Could not connect due to \"Connection refused,\" retrying in 2 sec\n. Thanks for the patch\n. Updated in master, which should show up on readthedocs soon.\n. If I'm reading the comments and code right, this should address the issue.\n. Thanks for the report\n. Really you should use what's best for your project. Async does have a few advantages, primarily that it will handle the bi-directional RPC better than the BlockingConnection does -- if RabbitMQ sends methods to your app such as Channel.Close, etc, Async will respond more quickly to it. The async code and adapters are also less complex, so there's less to do internally in pika.\nFrankly, because of the way Python works, you're going to block somewhere when you're running code. The main rule of thumb in async dev is don't do any long-running single methods like time.sleep() where you'll freeze up the python interpreter.\n. What adapter are you using?\n. Is the connection time elevated for both select and blocking or just blocking? I think I understand why blocking would take longer but not any of the async adapters.\n. BTW that's awesome that you're monitoring it at that level. What are you using to instrument the call times?\n. That's great, I use graphite as well. I meant more on the instrumenting pika perspective. Are you manually collecting timing data?\nI have a version of pika that instruments itself with newrelic and I was thinking about how to implement more generic instrumentation hooks.\n. I am investigating to see if there's a way to speed it up while still doing what it needs to do.\n. I have an experimental fix that's looking encouraging, I need to test it on linux, but basically the jist is I'm using poll/epoll/select to see if there is data to read, prior to doing the blocking read.\nPrior to the poller:\ngmr-mbp:pika gmr$ python send.py \nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5672\nINFO:pika.adapters.blocking_connection:Adapter connected\nPublished 10000 messages in 8.6953 seconds (1150.05 messages per second)\nWith the poller:\ngmr-mbp:pika gmr$ python send.py \nINFO:pika.adapters.base_connection:Connecting fd 3 to localhost:5672\nINFO:pika.adapters.blocking_connection:Adapter connected\nPublished 10000 messages in 1.4642 seconds (6829.44 messages per second)\n. I believe this changeset will address the issue for you. If you could, please test it and let me know. You can install via pip using the command:\npip install -e git://github.com/pika/pika.git#egg=pika-0.9.9p0\n. I'm going to close the ticket out. If you find this does not address the issue, please reopen.\n. Thanks for the fixes\n. This is most likely a bug that was recently fixed in master and will be fixed in 0.9.9 (coming soon).\nIf you want to test, you can install github master via pip.\n. I'm planning on addressing what tickets I can this weekend and releasing it.\n. Can you please provide test code to prove this out? I've set heartbeats to 1 second (which is a fairly low value, I'd expect 3 seconds to be the floor of what you'd find useful (TCP retry timeout)... and not seen any delay like your logs point out.\n. Closing, re-open if still an issue with 0.9.10\n. I believe the ConnectionClosed exception is only used in the BlockingConnection adapter because I was trying to make sure that people did not have to register callbacks.\nI think the exception would be raised prior to callbacks you register since they should be called in the order they were registered and the routine raising the exception is the first thing to register for a Channel.Close or Connection.Close.\nI could add a flag to the construtor that lets you turn off these exceptions?\n. @Isowen:\nThe channel would not send a Close.Ok because there was no Channel.Close issued. In your first flow:\n1. Server sends Connection.Close\n2. Client closes connection\n3. Client attempts to send Close.Ok\n3 should not be happening and I believe is fixed by a patch that's made it in to the soon to be released 0.9.9\n. This duplicates #242 \nThank you for the additional information.\n. When an auto-delete exchange is created, it will go away as soon as any bound queues are removed. As I am understanding what you are detailing, this is the expected behavior for an auto-delete exchange/queue etc. Am I misunderstanding what you are saying?\n. You probably want to use sender confirmations or the mandatory flag when publishing. RabbitMQ will not tell you that an message can not be published if you do not ask it to.\n. As I understand your problem, RabbitMQ and Pika are behaving as expected per the specification. I'm going to close this out unless I hear otherwise.\n. This is likely fixed in 0.9.10p0 which will be released shortly. I did update my code to bind multiple queues, etc and it worked fine for me:\nhttps://gist.github.com/gmr/5157001\n. Thanks for the patch\n. Do you still see this as an issue? I see you closed your pull request...\n. a uuid is quite a bit of computational overhead and is not compatible with all IOLoops supported. I'll figure something out and get it in with 0.9.10.\n. This should address the issue you are running into and benched out 30% to 40% faster than uuid generation.\n. Thank you for the patch\n. Thanks for the bug report and fix.\n. This examples shows how to use delivery confirmations with the BlockingConnection adapter:\n```\nimport pika\nimport time\nimport logging\nlogging.basicConfig(level=logging.INFO)\nITERATIONS = 100\nconnection = pika.BlockingConnection(pika.URLParameters('amqp://guest:guest@localhost:5672/%2F'))\nchannel = connection.channel()\nchannel.confirm_delivery()\nstart_time = time.time()\nfor x in range(0, ITERATIONS):\n    if not channel.basic_publish(exchange='test',\n                                 routing_key='',\n                                 body='Test 123',\n                                 properties=pika.BasicProperties(content_type='text/plain',\n                                                                 app_id='test',\n                                                                 delivery_mode=1)):\n        print 'Delivery not confirmed'\n    else:\n        print 'Confirmed delivery'\nchannel.close()\nconnection.close()\nduration = time.time() - start_time\nprint \"Published %i messages in %.4f seconds (%.2f messages per second)\" % (ITERATIONS, duration, (ITERATIONS/duration))\n```\nAnd here's an example with an asynchronous adapter:\n```\nimport pika\nfrom pika import spec\nimport logging\nITERATIONS = 100\nlogging.basicConfig(level=logging.INFO)\nconfirmed = 0\nerrors = 0\npublished = 0\ndef on_open(connection):\n    connection.channel(on_channel_open)\ndef on_channel_open(channel):\n    global published\n    channel.confirm_delivery(on_delivery_confirmation)\n    for iteration in xrange(0, ITERATIONS):\n        channel.basic_publish('test', 'test.confirm',\n                              'message body value',\n                               pika.BasicProperties(content_type='text/plain',\n                                                    delivery_mode=1))\n        published += 1\ndef on_delivery_confirmation(frame):\n    global confirmed, errors\n    if isinstance(frame.method, spec.Basic.Ack):\n        confirmed += 1\n        logging.info('Received confirmation: %r', frame.method)  \n    else:\n        logging.error('Received negative confirmation: %r', frame.method)\n        errors += 1\n    if (confirmed + errors) == ITERATIONS:\n        logging.info('All confirmations received, published %i, confirmed %i with %i errors', published, confirmed, errors)\n        connection.close()\nparameters = pika.URLParameters('amqp://guest:guest@localhost:5672/%2F')\nconnection = pika.SelectConnection(parameters=parameters,\n                                   on_open_callback=on_open)\ntry:\n    connection.ioloop.start()\nexcept KeyboardInterrupt:\n    connection.close()\n    connection.ioloop.start()\n```\nIt appears to be working for me. Do you have any details of the problem you are encountering?\n. I'll have to bang at this a bit before 0.9.10 to make sure we're good with it being added back in. Thanks for the contribution!\n. Without looking at your publishing code, it's hard to say, but your code should work fine with the blocking_connection adapter.\nPerhaps because auto_delete=True, your queue is being deleted before your publisher is publishing?\n. Change time.sleep(5) to connection.sleep(5) and I think you'll get your expected behavior. This was the case with my test of your code in pika 0.9.10p0 with RabbitMQ 3.0.2.\nYou can not use time.sleep with Pika and have it behave properly as that blocks all IO operations. Re-open if it doesn't fix your problem.\n. It appears the _cleanup() method in the channel was not being invoked when the channel was closed from either the client side or the server side. This is now automatically addressed if a channel is closed via your code or via the server and redundantly cleared when removing channels from a connection.\n. BTW, thanks for the very detailed error report, made this easy to fix.\n. Don't create your own consumer tag, let Pika manage that.\n. Thanks for helping with this @atatsu\n. Most likely it's due to heartbeat timeouts -- either disable heartbeats or set them to a very large #.  RabbitMQ 3.0+ turns them on by default.\n. Can you please paste the full traceback?  \nOn Tuesday, January 29, 2013 at 8:13 PM, Vishal Goklani wrote:\n\nYes, I am using the default channel for everything. (presumably this is bad)  \nMy base code is here: https://gist.github.com/4669710  \nI followed the RabbitMQ tutorial. Could you please point me to a better example.  \nMy callback function is passed in, and the messages are a list of JSON objects that are produced by the producer and consumed by the consumer.  \nThanks!  \nOn Jan 29, 2013, at 8:07 PM, Nathan Lundquist  wrote:  \n\nHow are you using your channels? Are you using the same channel to do everything? As in declaring queues/exchanges, setting up bindings, consuming queues?  \n\u2014\nReply to this email directly or view it on GitHub.  \n\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/pika/pika/issues/266#issuecomment-12868632).\n. No, I am referring to the python traceback when the exception is raised in your application. That error tells us the problem but not where it is occurring or why. The Python traceback when the app breaks is more useful.  \n\nOn Tuesday, January 29, 2013 at 8:22 PM, Vishal Goklani wrote:\n\nThere is no full traceback from pika, I only see these messages:  \nERROR:pika.adapters.base_connection:Socket Error on fd 4: 104  \nAre you referring to the RabbitMQ logs?  \nOn Jan 29, 2013, at 8:07 PM, Nathan Lundquist  wrote:  \n\nHow are you using your channels? Are you using the same channel to do everything? As in declaring queues/exchanges, setting up bindings, consuming queues?  \n\u2014\nReply to this email directly or view it on GitHub.  \n\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/pika/pika/issues/266#issuecomment-12868900).\n. If you're not getting the error message from:\n\ntry:\n    # foo do whatever here\ncatch ConnectionClosed as error:\n    print 'Connection was closed due to: %s' % error\nThen the RabbitMQ logs are the next place to look.\n. It appears that you are blocking in your consumer for longer than Pika has to respond to timeouts. The telling line is:\n=ERROR REPORT==== 30-Jan-2013::04:32:21 === \nclosing AMQP connection <0.515.0> (10.169.2.187:35624 -> 10.169.2.187:5672): \n{heartbeat_timeout,running} \nYou can either turn them off or make them much longer. In pika it's heartbeat_interval=0 to turn them off or heartbeat_interval={seconds} to set how many seconds you want them run. My guess is your consumer is blocking in Python processing for a fair amount of time if this is happening.\nAre you using time.sleep or any such thing?\n. Use BlockingConnection.sleep() instead of time.sleep():\nhttps://pika.readthedocs.org/en/0.9.9/connecting.html#pika.adapters.blocking_connection.BlockingConnection.sleep\n. Wow you just missed 0.9.9 :-(\nI'm working on the 0.9.10 fork at the moment but will take a look at your patches ASAP.\n. Trying to shape up 0.9.10 into being release worthy. A little more testing and some confirmations of bugs being fixed  and I should be able to push it out in the next week or two.\n. 0.9.10 will drop in the next few days.\n. Thanks for the detailed bug report. Fortunately, this is just a warning and can be ignored. I'll look at updating the channel shutdown flow a bit so that it doesn't happen in the first place.\n. Thanks for the patch\n. Can you provide in any other way than a rar file?\n. Yeah a zip is fine\n. Please try with 0.9.10p0 (github master) I have a feeling it is a resolved problem with read recursion.\n. Actually in running your tests, I see the issue is that pika is asynchronously dispatching bind and not waiting for bindok. I'll see if I can fix it pretty quick.\n\nOk scratch that, it gets even more complex -- Because you're using a mix of async and blocking, you're breaking out of the blocking state of the queue.bind. If you were consuming one queue, you could get by this with the generator based consumer (I think)...\nAny reason to not just go full async?\nMeanwhile I guess I need to work out something to keep callbacks from being invoked until after blocking processes have finished blocking. It doesn't look good without a major refactor of the BlockingConnection.\n. I don't see being able to do this in the immediate future. I am going to close this ticket, recommending you move from BlockingConnection to SelectConnection or TornadoConnection. Here's an example consumer, fully fleshed out that might help:\nhttps://pika.readthedocs.org/en/latest/examples/asynchronous_consumer_example.html\n. Thanks for the patch\n. Thanks for the patch\n. I believe the change just committed should address this.\n. Thanks for the patch\n. You're not including a routing key/binding?\n. Thanks for the detailed info. Fix committed.\n. I updated the example to show how to deal with this. Should be updated in the \"latest\" branch in pika.readthedocs.org\n. Closing due to lack of response, I can not replicate\n. The intent for 0.9.10 was to replace all of Pika's frame encoding and decoding with https://github.com/pika/pamqp which does a better job at handling all of this.\nDue to the CPU utilization bug in the BlockingConnection, this is likely to slip to 0.9.11 so there is a stable version prior to ripping out some of pika's internals and replacing it with pamqp.\nI thought that this issue was already addressed in the pika 0.9 tree but if not, it will be with the pamqp change (which incidentally is a step towards pika being Python 3 compatible).\n. Found the problem, was not the header values themselves, but the keys. Fixed for 0.9.10.\n. This is fixed in master/0.9.10p0 in my testing:\nINFO       2013-03-13 19:21:15,703 __main__                       connect                              49  : Connecting to amqp://guest:guest@localhost:5672/%2F\nINFO       2013-03-13 19:21:15,703 pika.adapters.base_connection  _create_and_connect_to_socket        165 : Connecting fd 3 to localhost:5672\nINFO       2013-03-13 19:21:15,706 __main__                       on_connection_open                   93  : Connection opened\nINFO       2013-03-13 19:21:15,706 __main__                       add_on_connection_close_callback     64  : Adding connection close callback\nINFO       2013-03-13 19:21:15,706 __main__                       open_channel                         310 : Creating a new channel\nINFO       2013-03-13 19:21:15,707 __main__                       on_channel_open                      143 : Channel opened\nINFO       2013-03-13 19:21:15,707 __main__                       add_on_channel_close_callback        116 : Adding channel close callback\nINFO       2013-03-13 19:21:15,707 __main__                       setup_exchange                       156 : Declaring exchange message\nINFO       2013-03-13 19:21:15,708 __main__                       on_exchange_declareok                168 : Exchange declared\nINFO       2013-03-13 19:21:15,708 __main__                       setup_queue                          180 : Declaring queue queue_one\nINFO       2013-03-13 19:21:15,708 __main__                       setup_queue                          180 : Declaring queue queue_one\nWARNING    2013-03-13 19:21:15,708 pika.channel                   _on_close                            822 : Received remote Channel.Close (406): PRECONDITION_FAILED - parameters for queue 'queue_one' in vhost            131 : Channel was closed: (406) PRECONDITION_FAILED - parameters for queue 'queue_one' in vhost '/' not equiva\nPlease let me know if you still see this issue.\n. That's pretty odd -- I send unicode all the time. Which version and connection adapter? Post 0.9.6(?) Unicode handling was cleaned up quite a bit.\n. I take that back, I am replicating, will see what I can find.\n. As i thought through it, all my UTF-8 content is consumed, not published. I've applied a fix that will get released with 0.9.10.\n. Unfortunately it bypasses the CallbackManager for keeping track of callbacks so I can't accept it. I am aware of the problem and will have a fix up shortly. Sorry!\n. This is interesting -- I've not looked at this in some time. I'll dig in and see if I can replicate.\n. Can you try and install github master and see if it fixes the problem for you? I could see a possibility in the base connection where if there is always data to read, how one could get into a situation where a write would not be preferred.\nI've switched the order of events to write before reading, which should defer any frame dispatching until outbound frames have been written.\n. I believe this is fixed, closing ticket due to lack of response.\n. Huh. So the main issue with the recursion is that if you're always receiving frames and not writing them, you'll always pick up new frames in the handle read loop. I had thought that was fixed by forcing writes. I may have to rethink the core blocking connection ioloop to truly address this.\n. Working on a prototype to fundamentally change the BlockingConnection and BlockingChannel objects which should address this issue while adding some thread safety to it as well.\n. I just pushed 0.9.14, though it did not address everything that was intended to be addressed.\n. This is not a bug. RabbitMQ is closing the channel because you're violating the protocol. You can not re-ack the same message.\n. Thanks for the fix\n. Thanks for the fix\n. Thanks, guess I shouldn't be committing at 1am without actually testing.\n. Correct, you catch a AMQPConnectionException when the BlockingConnection is closed. You do not use callbacks.\n. This has been resolved in 0.9.10p0 and will be released as 0.9.10 in the near future.\n. Huh probably more to do with refactors that happened post 0.9.10. Will look into it.\n. I hope to spend enough time with regression tests over the next week or two to push it out. I've been working on an automated build platform with Jenkins and Vagrant to make sure that there is enough coverage. Sorry for the wait.\n. The example is updated to show the proper API expectations, thanks for the report.\n. Yes, the callback should take the connection, reply code and reply text, not just the method frame or reply code and reply text. 0.9.10 will be dropping soon and the docs will be updated to reflect this.\n. What version of Python?\n. Thanks for the bug report, fixed.\n. So it's connected, polling for data and not finding anything.\nI suspect the issue is blocking heartbeats from processing. I'd turn off heartbeats by setting them to 0 in your connection parameters or setting them to something large like 600.\nLet me know if that helps.\n. By default, RabbitMQ 3.0 changed this behavior by sending a heartbeat interval in the negotiation. IIRC in pika, it's implicitly 0, not explicitly.  Try setting to 0 to force it off.\nOn Thursday, March 7, 2013 at 2:16 AM, Charles Moyes wrote:\n\nBlocking heartbeats from processing makes sense that it would cause this problem. What confuses me is that the default heart beat interval in the connection parameters section of the docs says it's zero (source: https://pika.readthedocs.org/en/latest/connection.html). Nonetheless, I'm going to try running it with it explicitly set to zero, and I will write back with whether this solves the issue. On the other hand, I'd like to avoid setting it to an arbitrarily large number. That feels like a hack to me. If I switched to SelectConnection, would this allow my worker client to respond to heartbeats?\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/pika/pika/issues/297#issuecomment-14546049).\n. I will close this for now, but am curious about how this turns out. Please let me know.\n. Thanks for the update! Sorry for the problems.\n. This is addressed in 0.9.10p0 which you can install via pip (from github master) or wait for 0.9.10 to be released in the next few days.\n. There would be a core performance penalty there too. I can't think of a way to introduce that behavior without adding an eval to every decode or refactoring frame decoding to be modular. How about just casting every message body as a str when it comes through? it times well over a few iterations in my shell:\n\n```\ngmr-mbp:pika gmr$ time python -c 'foo = \"bar\"; print str(foo)'\nbar\nreal    0m0.023s\nuser    0m0.016s\nsys 0m0.006s\ngmr-mbp:pika gmr$ time python -c 'foo = u\"bar\"; print str(foo)'\nbar\nreal    0m0.023s\nuser    0m0.016s\nsys 0m0.006s\n```\n. I could see always making it UTF-8 -- the spec does indicate it can be any type of content:\n\"AMQP strings are variable length and represented by an integer length followed by zero or more octets of\ndata. AMQP defines two native string types:\n- Short strings, stored as an 8-bit unsigned integer length followed by zero or more octets of data. Short\n  strings can carry up to 255 octets of UTF-8 data, but may not contain binary zero octets.\n- Long strings, stored as a 32-bit\"\nAnd the only mention of content within the body refers to it as a \"Opaque binary payload\" which to me describes more of a byte style agnostic payload. Unfortunately with Python 2, if that's only str, the people using unicode data will think it's broken and if it's unicode the people using str payloads will think it's broken. As such, going to UTF-8 breaks stuff a lot for other users.\nI am working on a branch of pika that abstracts all of the frame encoding and decoding to pamqp (https://github.com/pika/pamqp) where it's able to be more agnostic than pika is with everything tightly coupled.\n. Sorry I was not clear.\nPika tried to correctly determine if the body type is UTF-8 or str and returns either a unicode or str object accordingly.  Enforcing it to be one instead of both means that those expecting UTF-8 will not get it or those expecting str will not get it.\nThe AMQP spec is fairly agnostic on the problem with regard to body. Since there's no native byte stream in Python 2, it's difficult to enforce an agnostic behavior and those who are expecting UTF-8 payloads will consider pika broken if it returns a str. Is that more clear? If I change it to never consider the body unicode, I guarantee I will have numerous issues opened in response.\nOn your end, casting the response adds very little overhead and guarantees consistency.\n. I just accepted a patch that provides a \"force_binary\" when creating a channel that checks the flag in decoding and it will be released in 0.9.13\n. This is already resolved in 0.9.10p0 and will be released in the next few days to pypi as 0.9.10.\n. Good catch, was merely a state issue, the connection was/should have been closed by that point.\n. While the title of your issue is quite misleading, I have updated the BlockingChannel to make sure that after a channel is manually closed, the internal state flag is set to such.\n. this is a different issue, but thanks for noting it.\n. Eh?\n```\n\n\n\nimport pika\npika.version\n'0.9.12'\nconnection = pika.BlockingConnection()\nchannel = connection.channel()\nprint connection._has_open_channels\nTrue\nchannel.close()\nassert not connection._has_open_channels\nconnection._has_open_channels\nFalse\n```\n. This behavior in pika predates my involvement. The code path looks like it will always defer the minimum value above 0 provided by the server or client.\n\n\n\nIn this case RabbitMQ did not start sending a heartbeat interval value until 3.0, which in this case is 600 seconds. I will ask on the RabbitMQ list to see if there is any reason to keep this behavior and not instead always use what the client asks for, unless the value is never set by the client.\n. You could change the default value for RabbitMQ. I am about to send an email to the list. Based upon the response, we can have a fix as soon as tomorrow:\nhttp://lists.rabbitmq.com/pipermail/rabbitmq-discuss/2013-March/025936.html\n. Response on the list is this is the behavior of the official clients (.net, erlang, Java) -- I've asked if that behavior should be changed. I'm inclined to do so but want to see the conversation develop a little more.\n. I am sitting on the fence after the last reply on the email thread. Is there a reason to have it be beyond 10 minutes as opposed to just disabling it?\n. Are you on the RabbitMQ mailing list? I'd appreciate it if you could respond to Matthias:\nhttp://lists.rabbitmq.com/pipermail/rabbitmq-discuss/2013-March/025951.html\n. You can signup for the list here: https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss -- if you reply without being on the list, your message will be moderated which means it may or may not make it to the public list.\n. I pushed a change set up, fixing this about 20 minutes ago. Sorry for the bug.\n. This changeset was intentional and documented in the release notes. The docstring was omitted in the update and will be fixed.\n. Absolutely, let me look and see if I can spot it, I need to learn more about what is going on in that module, hang tight a minute or two.\n. BTW I will push 0.9.11 tonight due to this and one other regression issue introduced with 0.9.10\n. No problem. I just pushed and announced 0.9.11 with this fix.\n. Good catch, sorry for the trouble. Will push 0.9.11 out tonight.\n. Thanks for the report, should be fixed.\n. Thanks, will update to read close the channel, which will in turn close the connection.\n. Pika currently does not support IPv6. I'll look to add support for it in the next release. Unfortunately it's something that explicitly has to be checked for in the socket stack and properly handled.\n. Sure, sounds great, thanks.\nOn Mon, Mar 25, 2013 at 4:06 PM, Alessandro Tagliapietra \nnotifications@github.com wrote:\n\nSeems that just using\nif (socket.has_ipv6):\n    self.socket = socket.socket(socket.AF_INET6, socket.SOCK_STREAM, 0)\nelse:\n    self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)\nin base connection works, should I create a pull request?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/309#issuecomment-15421070\n.\n\n\nGavin M. Roy\nChief Technology Officer\nhttp://www.meetme.com/\n100 Union Square Drive\nNew Hope, PA 18938\np. +1.215.862.1162 x263\nf. +1.215.862.0465\nhttps://www.facebook.com/pages/MeetMe/21931227129\nhttps://twitter.com/meetme\n    http://www.youtube.com/user/MeetMeVideos\nThe public market leader in social discovery. (NYSE MKT: MEET)\n. Thanks for the issue, I've actually been working on cleaning up the entire opening and closing sequences for connections for 0.9.13 and have some unit tests that I want to write.\n. Is more of a debugging warning for me than anything else. I've removed it, thanks for the heads up.\n. I wish you would have synced up, I'll review these, but the goal is to yank out a bunch of stuff and use pamqp to add Python 3 compatibility.\n. To be clear, for a patch of this size, a discussion would be great prior to the investment of work.\nWe have to backwardly support 2.5 for the time being, though that will be deprecated.\nI really appreciate the contribution, but due to the size of the patches I can't accept it as is. If you want to help get 3 put into place, I would greatly appreciate it but we need to do it at a measured pace and in line with the roadmap.\n. I'm sorry but I can't accept this right now. It is on the road map, I will ping you once I rip out the amqp related methods and replace them with pamqp -- at which point it should be much easier and much smaller to get this done.\n. No problem, I had to update the code a bit, with your patch you would only try and connect on the first IP address resolved and if RabbitMQ was not listening on IPv6 but the hostname resolved to both an IPv4 and IPv6 address (like localhost on my machine) it would fail.\nI appreciate the patch, it sent me in the right direction!\n. You're using Github master and a pre-release of 0.9.13 version. This is a bug with a refactor of connection handling. If you do: pip uninstall pika;pip install pika the error should go away. I'll track this down ASAP, thanks for the detailed report\nGavin\nOn Sat, Apr 6, 2013 at 8:55 AM, Will Daly notifications@github.com\nwrote:\n\nHi,\nI'm trying to use rabbitmq and pika on localhost for testing purposes, and I am receiving an IncompatibleProtocolError.\nHere are the steps I'm following:\n1) I start up rabbitmq-server, and when I run rabbitmqctl status I get:\nStatus of node rabbit@localhost ...\n[{pid,390},\n {running_applications,\n     [{rabbitmq_management_visualiser,\"RabbitMQ Visualiser\",\"3.0.4\"},\n      {rabbitmq_management,\"RabbitMQ Management Console\",\"3.0.4\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.0.4\"},\n      {rabbit,\"RabbitMQ\",\"3.0.4\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.2.10\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.0.4\"},\n      {webmachine,\"webmachine\",\"1.9.1-rmq3.0.4-git52e62bc\"},\n      {mochiweb,\"MochiMedia Web Server\",\"2.3.1-rmq3.0.4-gitd541e9a\"},\n      {xmerl,\"XML parser\",\"1.3.2\"},\n      {inets,\"INETS  CXC 138 49\",\"5.9.2\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.7.1\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.0.4\"},\n      {sasl,\"SASL  CXC 138 11\",\"2.2.1\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"1.18.3\"},\n      {kernel,\"ERTS  CXC 138 10\",\"2.15.3\"}]},\n {os,{unix,darwin}},\n {erlang_version,\n     \"Erlang R15B03 (erts-5.9.3.1) [source] [64-bit] [smp:8:8] [async-threads:30] [hipe] [kernel-poll:true] [dtrace]\\n\"},\n {memory,\n     [{total,35653280},\n      {connection_procs,5696},\n      {queue_procs,27576},\n      {plugins,152984},\n      {other_proc,9868108},\n      {mnesia,62360},\n      {mgmt_db,47384},\n      {msg_index,22960},\n      {other_ets,1239568},\n      {binary,522528},\n      {code,18264216},\n      {atom,695185},\n      {other_system,4744715}]},\n {vm_memory_high_watermark,0.4},\n {vm_memory_limit,6523774566},\n {disk_free_limit,1000000000},\n {disk_free,199230787584},\n {file_descriptors,\n     [{total_limit,156},{total_used,4},{sockets_limit,138},{sockets_used,1}]},\n {processes,[{limit,1048576},{used,196}]},\n {run_queue,0},\n {uptime,2667}]\n...done.\n2) I run the following code (RABBITMQ_USER='guest', RABBITMQ_PASS='guest', RABBIT_HOST='localhost')\ncredentials = pika.PlainCredentials(settings.RABBITMQ_USER,\n                                            settings.RABBITMQ_PASS)             \nparameters = pika.ConnectionParameters(heartbeat_interval=5,\n                                               credentials=credentials,\n                                               host=settings.RABBIT_HOST)       \nself.channel = None\nself.connection = pika.SelectConnection(parameters, self.on_connected)\n....\nself.connection.ioloop.start()\n3) I get the following traceback:\nTraceback (most recent call last):\n  File \"/usr/local/Cellar/python/2.7.3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/Users/will/queue/consumer.py\", line 237, in run\n    self.connection.ioloop.start()\n  File \"/Users/will/python/lib/python2.7/site-packages/pika-0.9.13p0-py2.7.egg/pika/adapters/select_connection.py\", line 108, in start\n    self.poller.start()\n  File \"/Users/will/python/lib/python2.7/site-packages/pika-0.9.13p0-py2.7.egg/pika/adapters/select_connection.py\", line 336, in start\n    self.poll()\n  File \"/Users/will/python/lib/python2.7/site-packages/pika-0.9.13p0-py2.7.egg/pika/adapters/select_connection.py\", line 351, in poll\n    return self._handler(self.fileno, ERROR, error)\n  File \"/Users/will/python/lib/python2.7/site-packages/pika-0.9.13p0-py2.7.egg/pika/adapters/base_connection.py\", line 315, in _handle_events\n    self._handle_error(error)\n  File \"/Users/will/python/lib/python2.7/site-packages/pika-0.9.13p0-py2.7.egg/pika/adapters/base_connection.py\", line 291, in _handle_error\n    self._handle_disconnect()\n  File \"/Users/will/python/lib/python2.7/site-packages/pika-0.9.13p0-py2.7.egg/pika/adapters/base_connection.py\", line 240, in _handle_disconnect\n    self._adapter_disconnect()\n  File \"/Users/will/python/lib/python2.7/site-packages/pika-0.9.13p0-py2.7.egg/pika/adapters/base_connection.py\", line 131, in _adapter_disconnect\n    self._check_state_on_disconnect()\n  File \"/Users/will/python/lib/python2.7/site-packages/pika-0.9.13p0-py2.7.egg/pika/adapters/base_connection.py\", line 143, in _check_state_on_disconnect\n    raise exceptions.IncompatibleProtocolError\nIncompatibleProtocolError\nAny idea what could be causing this?  I'm running Mac OS X Mountain Lion.  Any help would be greatly appreciated.\nThanks!\n-- Will\nReply to this email directly or view it on GitHub:\nhttps://github.com/pika/pika/issues/317\n. Can you provide the first 5 commits from \"git log\" in the version you are running?\n\nI do not replicate the problem with master and know I ran into it with BlockingConnection and fixed it, a few days before you reported it.\nThanks!\n. I'm going to mark this as closed, I think you got a copy of master before the fix came in. If not, please provide the git log and re-open.\n. Sorry about that, I saw that today in another issue as well.\n. Which version? I believe this is a fixed bug in the pending 0.99.13 release.\n. Sorry for the reply, I think this issue is fixed at this point. I'll be releasing 0.9.13 shortly.\n. I do see a bug in that, will fix.\n. nod thanks for identifying was what I intended to fix as well. No harm if you beat me to fixing it with a pull request ;-)\n. Fixed in 0.9.13p2\n. Removed support for python <= 2.5\n. Seems a bit odd to mix connection adapters like this in the same process. I can't support the complexities with this at the moment and don't know if I can/would in the future.\n. Change the prefetch count with the channel.basic_qos() method to the number of messages you want RabbitMQ to send.\n. auto_delete=True removes the consumer when you close your channel that the queue was created on.\n. basic_nack is the negative version of basic_ack, both are considered a final action on the message in the queue. That aside, disconnecting from RabbitMQ removes the auto_delete objects that use it. Is part of the AMQP spec, no consideration of contents of said queues is made.\n. Thanks, will make sure a fix is put in for 0.9.13 release.\n. This should be resolved.\n. So what were you running into specifically with master? All of my tests with regard to reconnection were behaving as I would expect them to.\n. Nevermind, I see you detailed that in #322. I unfortunately can't use this because it's blocking in the reconnect code and we refactored the code to specifically not block. I'll look at getting a fix in today though.\n. I do appreciate the fix, it's only the blocking part that's an issue.  Nothing in the code should use time.sleep and nothing should block in a loop like that, otherwise you'll block async stacks from performing other tasks while waiting.\n. Thanks for the patch, looks good.\n. Sure, would be helpful.\n. Thanks for the bug report\n. I merged in a changeset that does this a week or two ago -- feel free to test GitHub master. I'm working through testing for release in the next week or two.\n. What's the use case for this? Stopping the IOloop in a message handler could cause time sensitive operations like heartbeat responses to be missed.\n. I'll look into the impact of this outside of the context you're submitting for it. Not opposed in principle, just need to look at the impact.\n. Cool, thanks.\n. Thanks!\n. Thanks for the correction\n. Thanks for the bug report. This will be fixed in 0.9.14\n. Thanks!\n. Do you mind testing against the current HEAD in master to see if the problem goes away? A problem was identified that a frame was not fully sending as expected.\n. I am aware of the AMQP spec. I'll look into why you're running into this issue shortly. There is a reason for the READ to WRITE ratio and the logic there trying to force reads for apps that write without reading from the stack.\n. Closing due to lack of updates. 0.9.14 may have addressed, may have not. BlockingConnection sucks and needs to be rewritten from scratch or removed.\n. Looks good, thanks for the patch. I did not run into the entire frame in testing, what OS are you running on?\n. Thanks for the detailed notes, I usually only test on localhost so it's probably related to non-localhost sending. I appreciate it!\n. Sorry, totally didn't think about this when trying to streamline the codebase. I'll have it fixed soon.\nOn Monday, May 27, 2013 at 7:07 PM, eandersson wrote:\n\nI think this goes beyond the missing size attribute, or the behavior is different in 0.9.13. Even after adding a new implantation for size, I was still unable to get back-pressure working.  \n\u2014\nReply to this email directly or view it on GitHub (https://github.com/pika/pika/issues/347#issuecomment-18517656).\n. Please give this a try if you can.\n. Great, this will be included with 0.9.14\n. Yeah if you can, please test with Github master -- I'll be releasing 0.9.14 soon which addresses this issue.\n. You don\u2019t by chance have the AMQP 1.0 plugin enabled and are connecting to that port? The error is that RabbitMQ is returning an unexpected version of AMQP (Not 0.8 or 0.9.1) when Pika tries to negotiate a connection.\n\nFrom:\u00a0Rohit Yadav Rohit Yadav\nReply:\u00a0pika/pika reply@reply.github.com\nDate:\u00a0December 16, 2013 at 8:04:21 AM\nTo:\u00a0pika/pika pika@noreply.github.com\nSubject:\u00a0 Re: [pika] frame_error on basic_publish (#349)\nI see this issue with RabbitMQ 3.2.2 and pika 0.9.13 but not with RabbitMQ 3.2.1\n\u2014\nReply to this email directly or view it on GitHub.\n. Looks like loader.io is injecting JSON into the AMQP protocol stream, perhaps thinking it\u2019s HTTP?  \nOn Tuesday, December 17, 2013 at 4:08 AM, Rohit Yadav wrote:\n\n{amqp_error,frame_error,\n\"type 3, first 16 octets = <<\\\"{\\\"ua\\\":\\\"loader.io (http://loader.io)\\\">>: {invalid_frame_end_marker,\\n 34}\",\nnone}\n. Perhaps you can provide test code showing the error. \u00a0The scenario you\u2019re presenting looks like the socket is being opened and JSON is being published as part of the connection negotiation phase, not as a stand-alone message.\n\nFrom:\u00a0Rohit Yadav Rohit Yadav\nReply:\u00a0pika/pika reply@reply.github.com\nDate:\u00a0December 17, 2013 at 2:11:58 PM\nTo:\u00a0pika/pika pika@noreply.github.com\nSubject:\u00a0 Re: [pika] frame_error on basic_publish (#349)\nHi again Gavin,\nI think you missed by point twice now: With pika 0.9.12 and 0.9.9 I'm able to publish message to the broker but not with 0.9.13;\nIn case that confused you I'm pushing newline separated json message to rabbitmq and pulling it out from a consumer written in Python/pika 0.9.13. The json message contains ua (user agent) etc. Please, don't get confused by it, what RabbitMQ is reporting is that the message published to the broker from the consumer has some protocol issues. Hope this clearly explains you the case, let me know if you have further question that may help you nail down this issue. Thanks and regards.\n\u2014\nReply to this email directly or view it on GitHub.\n. So the main issue here is message frames are not published to the stack atomically. So if you have a consumer that publishes on response, it's possible to intermix the response frames. I've recently fixed this issue in rabbitpy and have been contemplating the fix for pika. I have a few constraints around this issue, one being time, the other is work on the next major revision of pika. \nYou can try and avoid the issue by not acking until after you publish, I would expect that to address most issues here.\n. Why is this an issue?  Where would you have them installed on a users system?\n. Added\n. Anything custom in your CentOS 6.4 configuration with regard to network stack? Do you have an IPv6 addr enabled?\nCan I get a stacktrace with a little more context around the exception so i can catch it?\n. Hmm apparently this is the report port being parsed in as a string from googling a bit and reading blogposts on this exception. The only odd thing is Pika should be checking to make sure the port is an int. Are you changing this via attribute in your code by any chance?\n. This is pretty odd, few things:\n1. Is IPv6 Enabled?\n2. What are the entries from /etc/hosts for localhost for IPv4 and IPv6?\nIn theory it may be a problem with IPv6 being disabled but the first entry in hosts for localhost. I'll add an exception handler for this at the right spot, but it'd be good to know what the issue is that is causing it. \n. The fact that we're using this when socket.SOCK_STREAM and the documentation at http://docs.python.org/2/library/socket.html#socket.getprotobyname indicates it is probably not needed. I'll dig into some of my IP books to see if this is the case.\n. So in my testing it appears this happens when there is a DNS resolution failure, oddly enough. I've refactored the code a small bit and added logging when this happens and why.\nIf you can test with this latest patch, it should detail why the problem is happening in pika.adapters.base_connection at the CRITICAL logging level.\n. Thanks for the pull request!\n. The NoneType attribute error issue has been resolved post 0.9.9.\n. Thanks!\n. Thanks\n. Looks like this was fixed in 0.9.10\n. There was a reason to make it less than a second. If I recall, it will dramatically degrade write performance at a second.\n. Not directly, more due to my book. I've actually been using rabbitpy to prototype what I intend to do with BlockingConnection. I want to do a little more testing on the multi-threaded Connection class with Queues to communicate before I 100% commit to it in Pika.\nBecause it's a core change, I want to make sure it's solid before I introduce it into Pika.\nEdit: The core change if you're interested revolves around multiple IO threads: https://github.com/gmr/rabbitpy/blob/master/rabbitpy/io.py\n. @rafaelmagu the problem is 0.9.8 is flawed in that you have the ability to completely ignore anything RabbitMQ is requesting from you unless you force it to come up from air in your own code.\nI have a working POC of using Python threads for BlockingConnection and I'll be looking to release it \"soon.\"\nFrankly, I consider Pika's blocking implementation clumsy and poorly implemented (and yes I wrote most of it). Ideally more people could take a look at GitHub and see how it works for their blocking, non-async needs. I'd love to see Pika go async only since the two models are vastly different.\n. Yes, I did see it, since it's not fixing an actual problem it's a lower priority to get into the next version than bug and behavior fixes. It seems like it will be useful for making the code more easily support Python 3. Thanks for the patch, I do hope to get it merged in soon.\n. I'd prefer small changes one changeset at a time, it makes code review much easier.\n. Thanks!\n. Currently, Pika has no concept of threads. I am working on a branch where I am changing the core of how Connection (and all adapters) work in a background thread.\nThe key thing in the ticket is there is a message being raised:\nNo handlers could be found for logger \"pika.adapters.base_connection\"\nThis is likely saying what is going on, add this code:\nimport logging\nlogging.basicConfig(level=logging.INFO)\n. Thanks!\n. Thanks!\n. There is interest and a roadmap to get there. The pika-python3 port doesn't follow the same direction of pulling out the core AMQP marshaling and demarshaling and moving it to the pamqp library (https://github.com/gmr/pamqp).\nThe road to Python 3 is more around cleaning up a lot of things than brute force conversion.\n. @ztane The biggest issue has to deal with Unicode/str/bytes handling and it's not a trivial issue. AMQP specifically calls for UTF-8 in some areas and not others. Handling both properly in Python 2 and 3 while trying to keep backwards compatibility for the average user is not trivial.\nThis is a solved issue in https://github.com/gmr/pamqp and the plan is to migrate the underpinnings of pika to use it for frame marshaling and unmarshmaling.\nMeanwhile there are other core issues such as BlockingConnection that need to be addressed in unison.\nThe desire for Python 3 support is not lost on me, and is something that I too want to address, but won't likely happen for another few months given my current schedule and obligations.\n. Not quite, but close. There's an issue that is illustrated in the unit tests by a change I recently made where instead of logging an error condition that should never happen, it raises an exception.\nSaid error condition is happening in all adapters it appears, so we need to trace down how it's happening and get it fixed before I release 0.10.\n. I believe at this point it's resolved and we're just waiting for 0.10 to stabilize enough for a pre-release to be pushed to pypi.\n. What version of RabbitMQ are you running? That message means the very first connection frames are incompatible, which should not happen.\n. @6ruff actually it is the correct behavior, and when you use the logging module correctly, you shouldn't have an issue.\nYou should use propagate=True on the pika logger to get all of the child loggers.\nSee http://docs.python.org/2/howto/logging.html#logging-flow\nThe last paragraph of http://docs.python.org/2/howto/logging.html#loggers\nAnd finally: http://docs.python.org/2/library/logging.html#logging.Logger.propagate\n. I'm a bit concerned about registering for signals here and responding to them. What about people who use Pika in apps that do signal management?\n. Sorry I didn't catch this before... will review in depth to consider the implications.\n. I like what you're doing, but doesn't it change the import signature where someone has to run the method?\nLike:\nTornadoConnection = pika.TornadoConnection()?\n. While I appreciate the effort, I'm not going to be able to use this. It seems to do much work and adds too much code without providing much value. Thanks and sorry!\n. Commenting it out will break functionality, you should remove the .decode('utf-8') portion. I'm about to push a changeset that reverts all the utf-8 casting behaviors.\n. Good catch, thanks!\n. I'm sitting on the bugfix since anything with BlockingConnection is a hack and I need to figure out a better way to deal with it.\n. I am actively working on 0.10.0 and will be releasing a release candidate soon. It will contain a refactor of BlockingConnection that should help address all of the issues pending with it.\n. I'm working to release the 0.9.14 release with the pending fixes this week. 0.10 is turning into a more substantial refactor. I'm trying to find where the disconnect() method is being invoked from, as the tracebacks don't match what I'm seeing in the code.\n. @eandersson are you seeing this in master? It's this line in 0.9.13:\nhttps://github.com/pika/pika/blob/0.9.13/pika/adapters/blocking_connection.py#L318\nAnd subsequently removed, but I have reports of it happening in master.\n. Thanks for the confirmation!\n. @bjoernhaeuser this will continue to be an issue until pika 0.10 unless you enforce the BlockingConnection.process_data_events() method.\n. You would just inject this as part of your flow. Calling it on every publish may be overkill however. You may want to have a counter that calls it every n messages for better performance. 0.10 will introduce a IO thread to handle this in the background.\n. Your analysis is correct on this, what is your max reconnection threshold set to?\n. The behavior I would expect should the socket connection be disrupted is that the socket would time out on a read, returning an OS level error on the socket that pika would catch. I'll have to play around to figure out exactly what is happening.\n. That would be an issue, is that what you're setting it to?\n. Thanks!\n. There is no concept of threading in Pika. If you take actions on a connection outside of the thread it's running in, there is no guarantee the async internals will handle it correctly.\nIf you're using threading and the BlockingConnection, you might want to check out rabbitpy. Depending on feedback and direction, I am considering deprecating BlockingConnection for rabbitpy since it's mutli-threaded and solves many of the current internal issues in BlockingConnection. It should be fully thread safe.\n. In rabbitpy the communication is managed in specifically named and managed threads. Per our discussion on rabbitpy code, some concepts are likely to make it back into BlockingConnection.\n. Thanks, this class is likely to be fundamentally changed in the near future.\n. What is your workflow that you have > MAXINT channels in a single connection? Seems like an odd condition to run into.\n. My bad, yes not MAXINT but it seems like the easier fix is just to wrap and check if the value is used and open, no?\n. I appreciate the patch and will steal your tests, but will move to a list comprehension for performance reasons. Lines 1128 through 1148 can be reduced to:\n[x + 1 for x in sorted(self._channels.keys() or [0]) if x + 1 not in self._channels.keys()][0]\n. Tracking in issue #460\n. Thanks!\n. You're running a version of pika that is very old. Please update to the a newer version.\n. What version?\n. Thanks!\n. Thanks, I don't intend to address threads in Pika until 0.10, and even then it will be limited in where it is addressed.\n. Thanks!\n. Sorry for the delay in reviewing this. I'm not keen on adding locking to the core classes to support threading, where the use cases for threading are not totally clear in an async driver.\nAre you adding this for threading support in BlockingConnection?\n. Sorry, will close this for now, if you want to pick up the discussion, please re-open.\n. Thanks!\n. Thanks!\n. No, I'm not going to change the base class used by all the other adapters to change a Tornado behavior.\nWhat error are you trying to fix? I use pika extensively with Tornado and am not aware of any issues.\n. You've traced back close(self, reply_code=200, reply_text='Normal shutdown') back to pika.connection.Connection? You're aware about what super(BaseConnection, self).close(reply_code, reply_text) does right? The following method (pika.connection.Connection.close) is invoked by BaseConnection.close on the line invoking super:\n``` python\n    def close(self, reply_code=200, reply_text='Normal shutdown'):\n        \"\"\"Disconnect from RabbitMQ. If there are any open channels, it will\n        attempt to close them prior to fully disconnecting. Channels which\n        have active consumers will attempt to send a Basic.Cancel to RabbitMQ\n        to cleanly stop the delivery of messages prior to closing the channel.\n    :param int reply_code: The code number for the close\n    :param str reply_text: The text reason for the close\n\n    \"\"\"\n    if self.is_closing or self.is_closed:\n        return\n\n    if self._has_open_channels:\n        self._close_channels(reply_code, reply_text)\n\n    # Set our connection state\n    self._set_connection_state(self.CONNECTION_CLOSING)\n    LOGGER.info(\"Closing connection (%s): %s\", reply_code, reply_text)\n    self.closing = reply_code, reply_text\n\n    if not self._has_open_channels:\n        # if there are open channels then _on_close_ready will finally be\n        # called in _on_channel_closeok once all channels have been closed\n        self._on_close_ready()\n\n```\n. Closing due to what I believe is a misconception about how the close code path works.\n. Thanks!\n. Correct, this is fixed in master and will be released soon.\n. 0.10.0 will be this month. I'm working through the final bits on the refactor of BlockingConnection.\n. Here are my current plans for its release (and 0.10 to boot): http://gavinroy.com/posts/pika-0-9-14-and-0-10-plams.html\n. Thanks!\n. Force binary is no longer part of pika and it will no longer be opinionated as to the unicode/utf-8 nature of messages or attributes.\n. FRAME_MAX has also been revered for 0.10.0.  Thanks for the report\n. There is a travis-ci project setup at:\nhttps://travis-ci.org/pika/pika\nBut it won't be responsible for bundling and tagging. The impact of releases on devs is pretty big for tagged releases. If you want to run off of github master, pip allows you to do so. Thanks for the suggestion.\nGavin\n. This is no longer required, as auto-unicode casting has been removed.\n. Thanks for the contribution though!\n. Thanks!\n. I am actively working on this, a quick hack would look like:\ndef disconnect(self):\n\u00a0 \u00a0 \u00a0 pass\n\u00a0 \u00a0 BlockingConnection.disconnect = disconnect\nFrom:\u00a0Konstantin Melnikov Konstantin Melnikov\nReply:\u00a0pika/pika reply@reply.github.com\nDate:\u00a0February 3, 2014 at 11:57:35 AM\nTo:\u00a0pika/pika pika@noreply.github.com\nSubject:\u00a0 Re: [pika] BlockingConnection has no attribute disconnect (#435)\nExperience the similar problem with pika==0.9.13.\nRabbitMQ 3.2.3, Erlang R14B04\nUpdate to the latest master didn't help. Is there some work around?\nStacktrace:\nFile \"C:\\Users\\kmelnikov\\Projects\\ROCS2\\rocs2\\common\\broker.py\", line 154, in set_up_queue_for_response\nself.channel.queuedeclare(queue=back_q_name, exclusive=True, durable=False)).method.queue\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 867, in queue_declare\nNone, replies)\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 1104, in rpc\nself.wait_on_response(method_frame))\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 1121, in sendmethod\nself.connection.send_method(self.channel_number, method_frame, content)\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 249, in send_method\nself.sendmethod(channel_number, method_frame, content)\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\connection.py\", line 1489, in sendmethod\nself.sendframe(frame.Method(channel_number, method_frame))\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 388, in sendframe\nsuper(BlockingConnection, self).sendframe(frame_value)\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\connection.py\", line 1476, in sendframe\nself.flushoutbound()\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 348, in flushoutbound\nif self.handlewrite():\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\base_connection.py\", line 338, in handlewrite\nreturn self.handleerror(error)\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\base_connection.py\", line 282, in handleerror\nself.handledisconnect()\nFile \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 318, in handledisconnect\nself.disconnect()\nAttributeError: 'BlockingConnection' object has no attribute 'disconnect'\n\u2014\nReply to this email directly or view it on GitHub.\n. This is fixed in master and will be tagged with 0.9.14 release.\n. Interesting error, you had a channel close while iterating the dict creating the list... Fortunately an easy fix.\n. Thanks for the report\n. Are you by chance in a multi-threaded app? I put a wrapper around the call to prevent this happening.\n. The main point of this is not to ignore RPC requests from RabbitMQ. If you're always writing and not checking to see if RabbitMQ has sent a heartbeat or what not, you'll have problems. I've updated 0.9.14 with your pull request, however 0.10 will be a re-write of how all this works to match how I have it working in rabbitpy.\n. Since it's async, you would want to register for the on_close callback for the connection. I'm not a fan of throwing exceptions unless it's in BlockingConnection. It can seriously mess up asynchronous applications. The callback mechanisms are there to catch errors like this.  Thoughts?\n. I changed the logging to an exception. It shouldn't happen, but it's better to let you deal with the fallout instead of ignore it.\n. Thanks!\n. This should address that for you, LMK.\n. Could be how nose handles subprocess testing, not sure. The error is that the socket specified is closed and it could not write to the socket. Perhaps you should mock the pika.Connection.send_method method ot not actually use the socket.\n. Cool :)\n. Thanks for the PR. I've reviewed your code and have some comments. Please rebase (not merge) current master prior to issuing an update to this PR.\n. I'm going to close this for now, please re-open if you have the opportunity to address the changes that I pointed out.\n. Thanks!\n. Here are my current plans for its release (and 0.10 to boot): http://gavinroy.com/posts/pika-0-9-14-and-0-10-plams.html\n. 1. Durable just tells RabbitMQ that your queue should survive a reboot.\n2. HA queues do come with a performance penalty proportionate to the number of nodes in the ha-queue definition.\n3. BlockingConnection is known to be slower since we have to force interrupts to deal with AMQP's bi-directional RPC. If you want a blocking style AMQP adapter, I recommend looking at the rabbitpy AMQP adapter, where I worked through this issue with threads. I'll be taking a similar approach with pika 0.10\n4. You mentioned writing to disk and using delivery-mode 2 in your properties. Yes, under-provisioned IO on messages that must be persisted to disk is like having a database server with a write only workload. Provision your IO subsystem accordingly. This is not a RabbitMQ performance issue but rather an under-provisioned server issue.\n. \"setting delivery mode to 2 with or without HA also slows it down dramatically\"\ndelivery-mode 2 means every message is written to disk. If the slowdown is not acceptable, then you need faster IO channels for storage. HA slows things down because RabbitMQ has to coordinate message storage on two or more nodes instead of one.\npika has nothing to do with performance of delivery-mode 2 and HA vs delivery-mode 1 and single nodes.\nAnd yes, unfortunately it has taken me a month to reply to.\n. So the crux of your problem is the disproportionate performance of the BlockingConnection? The main issue there is the BlockingConnection does not behave appropriately with regard to RabbitMQ and how AMQP is a bi-directional RPC protocol. This is something that is slated to be fixed in a future version, but basically it will publish without constraint or regard to RabbitMQ, and can cause issues.\nThe async versions should behave the way the protocol is intended to behave. You might want to check out rabbitpy which should get you both good performance and non-async/more idiomatic python coding, if you're not looking for a full async stack.\n. This is already addressed in master, and will be part of 0.9.14. It does not require the addition of the connect method.\n. Looks good, thanks!\n. Thanks for this. It's worth noting the code will need to change in 0.10 since you're referencing basestring which is not available in Python 3.\n. It would be better to get a clean PR, I'm fine with a change to a list in theory. I'll be reviewing the code either tomorrow or Monday.\n. Thanks\n. Shouldn't it be:\nbytes_written += self.socket.send(frame)\nErr nevermind, re-reading\n. Thanks for the fix!\n. Since it's async, you need to listen for the Queue.DeclareOk by passing in a callback handler. From the docs\n```\nParameters:\ncallback (method) \u2013 The method to call on Queue.DeclareOk\nqueue (str or unicode) \u2013 The queue name\npassive (bool) \u2013 Only check to see if the queue exists\ndurable (bool) \u2013 Survive reboots of the broker\nexclusive (bool) \u2013 Only allow access by the current connection\nauto_delete (bool) \u2013 Delete after consumer cancels or disconnects\nnowait (bool) \u2013 Do not wait for a Queue.DeclareOk\narguments (dict) \u2013 Custom key/value arguments for the queue\n\n```\n. The point is you need to use a callback. The method will not return anything.\n. Doesn't matter who. Any method that wants to get data back from Queue.DeclareOk asynchronously needs to have a callback for when that frame is returned. If you dont want to consume until the Queue.DeclareOk is returned, then issue the Basic.Consume in the callback you passed into queue_declare().\n. I have verified this exists when no parameters are passed in and will address it.\n. Thanks, addressed and added more descriptive exception checking for the edge case caused in the example.\n. Reference implementation in #404 \n. Thanks Charles, I appreciate it and it was great meeting you at pycon!\n. Unfortunately a traceback doesn't do much to help us understand what's going on. Closing due to ticket age and lack of additional context.\n. #1 is something that's up to the user of the client library to do. The goal is to be non-opinionated in SSL use. You basically have the full set of SSL socket options available to you.\n2 would be too opinionated to put into the core at this time.\nPerhaps an example of pika with SSL using these options would be helpful in the docs?\n. Thanks\n. Cool, I am getting errors with libev in my dev environment, do you know if the homebrew libev+pyev should work?\n. Thanks, I'll dig in tonight and see if I can figure out what's going on.\n. This should be addressed, sorry for the very long time to update\n. Have you had a chance to test against GitHub master?\n. Cool, just making sure. Working to get 0.9.14 out the door -- is almost ready.\n. Thanks for this, I had another commit that did not get pushed as I had spotty internet access while on vacation. I just pushed it, which addresses this PR as well.\n. Can't delete, but looks like you were able to close it\n. Sounds like you don't have a callback handler registered for on_close of connections or channels. You might want to listen to those so your app knows when it's been disconnected.\n. Thanks!\n. Added methods that more explicitly allow the registration of callbacks for these frames. Sorry for the delay in response.\n. Looks good. Re coveralls, I'll be looking into that, it's annoying.\n. Thanks for the PR. Unfortunately this will impact more than Tornado, so I'll need to reflect on how to best handle the change in write behavior before accepting.\n. I will address this issue in a different way but I do appreciate the PR.\n. Agreed with @eandersson \n. We always need to be checking for frames sent from the server without regard to what your application is doing. The correct fix is to use signals to wake a blocking read on the socket for writing frames. Unfortunately, this really is a complete restructure/rewrite of BlockingConnection which is intended for 0.10 as I mentioned in your other ticket.\nYou may want to check out rabbitpy, as it's a well-behaved AMQP client that implements the proper bi-directional RPC nature of AMQP without a hack like BlockingConnection.\n. Without seeing your code, I can't address what the issue may be, but if you're using BlockingConnection, I could see that being problematic. It is a problematic adapter and requires a complete overhaul which is intended for 0.10.0. \nOf course part of the problem is that amqplib and others that are not async or that do not run a background IO thread are not well behaved AMQP clients. AMQP is a 2 way RPC protocol, meaning the server and the client must respond to RPC requests (protocol level RPC, not application level RPC). It's a hard thing to get correct in a single-threaded, non-asynchronous style adapter, if not impossible.\nI'd be curious how your test holds up against rabbitpy, which implements similar constructs to how I intend for BlockingConnection to work for 0.10.\n. Thanks, I addressed your comments on the rabbitpy project.\n. Closing since tests don't pass, will implement similar logic.\n. Hopefully df7d3b742bb addresses what you all are running into here.\n. I moved the reset to a more appropriate method, but appreciate the patch\n. Thanks for the issue.\n. Nothing should have changed that impacted that code, perhaps something on the RabbitMQ side? Flow is only partially supported. Closing due to age, no real idea what's going on here or the use case for Channel.Flow these days.\n. With the 0.10.0 change, I'm ok with the API change. Thanks for the patch!\n. Thank you!\n. I didn't see an explicit reason why the tests are failing, but I'm reluctant to accept the commit until the tests pass.\n. I've removed the None parameter and added type checking.  Thanks for the PR.\n. Correct, sorry for the delay in reply.\n. Thanks for the PR and the analysis.\n. Closing due to not using native adapters for behavior which seems wrong. Thanks for your patch. If you want to (and it's still needed), please resubmit without the select based implementation, based upon current master.\n. Thanks for the PR\n. I like this, but need to cherry pick stuff since not all of it applies anymore.\nI'll credit you but commit the changes directly in my own branch.\n. Thank you for your demo and PR. I'm going to close this out since #509 addressed it directly.\n. Thanks for this, I ended up using a different approach in de8b5450, but appreciate your take.\n. We'll add more documentation around this, but it's worth noting the examples on http://pika.readthedocs.org generally have docstrings for these methods like:\n``` python\n    def on_message(self, channel, basic_deliver, properties, body):\n        \"\"\"Invoked by pika when a message is delivered from RabbitMQ. The\n        channel is passed for your convenience. The basic_deliver object that\n        is passed in carries the exchange, routing key, delivery tag and\n        a redelivered flag for the message. The properties passed in is an\n        instance of BasicProperties with the message properties and the body\n        is the message that was sent.\n    :param pika.channel.Channel channel: The channel object\n    :param pika.Spec.Basic.Deliver: basic_deliver method\n    :param pika.Spec.BasicProperties: properties\n    :param str|unicode body: The message body\n\n    \"\"\"\n    LOGGER.info('Received message # %s from %s: %s',\n                basic_deliver.delivery_tag, properties.app_id, body)\n    self.acknowledge_message(basic_deliver.delivery_tag)\n\n```\n. Not your fault the documentation is not clear, just wanted to point out what was there.\n. Looks like you're doing cross-thread channel/connection use which is not supported.\n. Why do you want them to be named, the behavior of the example works as is. Closing, feel free to reopen if I'm missing something.\n. Due to the numerous other big changes (Python 3 support, etc) I can't merge this, can you please resubmit a new PR based upon master?  Thanks!\n. Seems reasonable\n. I'm not convinced this is the right way to handle this. IncompatibleProtocolError is raised when you open a connection and the remote side closes the connection before the AMQP connection can be negotiated. One could make a case that it should do an on_connection_error and that should contain the callback -- or that the TornadoConnection adapter some of Tornado's built in constructs to capture errors at this stage.\n. I think your problem may live with your ELB configuration. I use pika to talk to RabbitMQ through ELB and do not run into any issues. I am also not convinced the approach is correct.\nI appreciate the PR but will not be merging it at this time.  Thanks for your contribution and raising the issue.\n. Thanks :)\n. Thanks\n. Are you doing one connection per thread? And are you blocking in any way? (long running queries, time.sleep, etc)\n. Since you're using BlockingConnection and an old version, I'm not sure how well you will fare. You might want to try rabbitpy as it provides both thread-safety and a more pythonic way of interacting with RabbitMQ for non-asynchornous and threaded clients.\n. Thanks for the update\n. Thanks everyone for the info and attempts to clarify. The easy fix here is to not log the file descriptor number since it's not really required for the context of what the error is anyway. I've pushed a change that addresses that.\n. Hi there! Thanks for the patch. I'm curious why you think it needs changing. pika.PlainCredentials is made available as an alias for convenience, pointing to pika.credentials.PlainCredentials. Did the example not work for you? If so, what version of pika were you using?\n. Closing due to lack of response\n. Thanks\n. There will be a 0.9.15 on pypi once we work down the backlog of PRs and issues.\n. Have you examined the impact that your change to BaseConnection will have on all of the connection adapters?\n. Would be nice to support the address array support similar to how the Java client does this. Thanks for the suggestion.\n. Re \"If x-cancel-on-ha-failover is given to a consumer and add_on_cancel_callback is set. The callback is not called on failover\"\nWhich version of pika and RabbitMQ are you using? Which connection adapter?\n. That should be working. My understanding of the HA part of the functionality is:\n- A cluster exists with at least 2 nodes\n- A mirrored queue that is declared on node 1 and mirrored to node 2\n- A consumer exists on node 2 that passed {x-cancel-on-ha-failover: True} when issuing the Basic.Consume for said queue\n- node 1 is shutdown, causing the owner of the queue to migrate to node 2\n- Consumer on node 2 is sent a Basic.Cancel this happens, causing any of the cancellation callbacks to fire.\nIs that your understanding as well?\n. Seems reasonable, I'll take a look ASAP.\n. I couldn't raise a condition in testing where AttributeError was being caught and removed the except block for it in a previous commit. Thanks for the PR :)\n. This is fixed in master and will be in 0.9.15.\n. Thanks, removed the exception handling for AttributeError after testing that it did not cause an issue.\n. Thanks for the PR, I'll take a look ASAP.\n. @vitaly-krugl is this still the case with the updates to SelectConnection?\n. Thanks for the PR, I'll take a look ASAP.\n. Thanks for the PR, I'll take a look ASAP.\n. Will merge with the intention of doing a full regression test before releasing.\n. This is fixed, it should not be using socket.sendall which is the most likely cause of the issue.\n. Thanks for the PR, I'll take a look ASAP.\n. LGTM, thanks\n. Merged your PR.\n. I'll take a look at this ASAP.\n. There are quite a few changes in this PR. Please rebase down to a single commit, and reopen in a new PR referencing this one.\n. This was addressed by removing the except AttributeError block in e7b6d73aa2a38\n. Seems reasonable for 0.9, thanks.\n. I believe this was fixed in #555, please reopen if not.\n. Glad to see this is progressing, please rebase off of master and format with yapf.\n. Thanks\n. Could it be that something is listening on Port 21 and Pika's waiting for a response? Pika expects an AMQP server on the other side and doesn't try and detect improper protocols or put a timer on how long it should take to negotiate an AMQP connection.\n. I would seem that it's not a totally transparent proxy or something else is going on between squid and your AMQP connection. IncompatibleProtocolError is raised during the initial connection stage and the client is disconnected by the remote end.\n. You might want to try with haproxy. Last I checked, Squid was primarily a caching HTTP proxy and not protocol agnostic. This may have changed, of course.\n. Sounds like Squid is acting like an HTTP proxy and not a TCP/IP proxy. Closing this out since it's not a Pika issue.\n. Thanks for your work on this\n. There are a lot of commits in this and I'd rather not review the whole chain, any chance of a rebase flattening the PR?\n. This is quite a few change sets to sort through, any chance of a rebase squashing them down?\n. Looks good, though the initial look has me wondering if you're ever setting self.params.heartbeat to 0. Did I miss it? How are you letting the server know what you're selecting?\n. Your understanding is correct, I just didn't look at the patch in the full context. self.params.heartbeat is what is sent back and the tuning is replacing the value.  Thanks for the PR\n. Looks like there's some feedback that needs to be incorporated and then a rebase to flatten again?\n. @ztane if you can rebase off of master, I'll freeze any additional PRs until yours is merged in. Please let me know when/if you can do that so I can coordinate with other people fixing things. I misunderstood your comments in the PR as being issues, not reasoning as to why the changes are there.\n. I've not had a chance to review it but from your screenshot it looks like it nukes property that defines it as pika. I'm not a fan of that. I'm not opposed to allowing the addition of new properties, though I don't think it should be in the constructor. Most static analysis and lint tools complain about the argument counts already.\n. Will address this when https://github.com/vitaly-krugl/pika/pull/4 is merged into master here.\n. Are you doing any processing of data events? You have to let the poor adapter breath, it will not unless you tell it to or let it catch up with itself.\n. In principle I agree, but in a single thread with no background IO management, it can't with a big performance trade-off (which I was fine with, but users were not)\n. Thanks :)\n. Thanks\n. Confirmed, thanks\n. Thanks!\n. Thank you for the contribution though :)\n. :+1: \n. @vitaly-krugl where do you stand on this PR?\n. What should I make of the failed tests? Merge anyway?\n. Is this in master? A specific version? All of the async adapters should be using the same connection negotiation code, so that's a pretty odd one there.\n. Seams reasonable to me\n. @Dmitry-N-Medvedev your error is unrelated and is due to this:\npython\nrmq/infra/__init__.py\", line 96, in connect\n    pika.ConnectionParameters(self.connectionParameters)\nIt appears that self.connectionParameters is already an instance of pika.ConnectionParameters.\nPika 0.10.0 works fine with Python 3.5.1 (I use it heavily in production with this combination).\n. Pika is not thread safe. Your best bet is to run a IOLoop per thread and never let any context of pika cross threads.\n. What version are you running? You might want to try pika from master. We're about to tag 0.10.0dev0 as a pre-release for pypi.\n. K.. yeah I'd try pika master -- BlockingConnection has been rewritten to be less buggy than 0.9.14.\n. Also know that using time.sleep in your code with Pika will result in the python interpreter stopping and no actions taking place at all.\n. Well, Channel.Close is a synchronous RPC call to the RabbitMQ server while Basic.Publish is not. You'll not return from Channel.Close until RabbitMQ has received all of the publish frames. My point is time.sleep will prevent Pika's async underpinnings from doing anything to make sure your messages get anywhere.\n. You can make it sync for a cost (performance penalty) by enabling publisher confirmations or using transactions.\n. I meant sync with RabbitMQ via AMQP, not sync with the socket.\n. There's no guaranteed way to make class destructors (object.del) work properly. The invocation of them is up to the interpreter's gc and relying on them would make pika more opinionated than it should be (imo). FWIW gmr/rabbitpy is more opinionated on these things and when you use the context manager, will shutdown the channel and connection for you.\nhttp://stackoverflow.com/questions/6104535/i-dont-understand-this-python-del-behaviour has a good answer describing what I'm referring to re object.del\n. Yup, 0.10.0b2 is up on pypi with this.\n. 0.10.0b1 is on pypi as a pre-release and barring any major issues with this or subsequent pre-releases, 0.10.0 will be released on July 27th. \n. Closing for now due to lack of tests or agreed upon method of doing this. Thanks for the PR, please feel free to re-open if you add test coverage.\n. Thanks for the contribution!\n. No -- think we need another pre-release or should we just let 0.10.0 out?\n. At the very minimum, your PR needs to pass the unit tests for all supported versions of Python. At the moment it appears to fail for Python 3. You can check out the details in the link from Travis-CI below.\n. Thank you for the contribution!\n. Thanks for the detailed analysis. Have you verified this is an issue in 0.10.0/master?\n. Per the exception, you're having some sort of network related problem connecting to RabbitMQ:\npika.exceptions.AMQPConnectionError: Connection to 192.168.1.80:5672 failed: timeout\nThat means the socket timed out connecting to 192.168.1.80 on port 5672.\n. RabbitMQ does not listen on 5672/UDP, so it's not that. The Docker port mapping looks accurate for a single node. My guess is that you're not pointing to the right IP addr.\n. Ok, but the issue is clearly that your app can't connect to RabbitMQ. I'm not sure how to help you troubleshoot beyond that. Have you tried connecting to the RabbitMQ instance running in the container from anything/anywhere else?\n. No it's not -- the exception is pretty clear that your app can not connect to RabbitMQ -- that is the only issue that can be divined from the information provided.\n. @shinji-s do you have time to address these items? We'd like to see this included with 0.10.0 and is the last gating item for that release.\n. @vitaly-krugl waiting on your +1 for merge.\n. Looks like this is broken in Python 3.5 -- if you can look into why, that'd be ideal.\n. Thanks!\n. That'd indicate the socket was remotely closed. It'd be worth trying to reproduce with debug logging turned on. At the start of your app do:\npython\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nand attach the output of the pika debug logs. If there are large amounts of duplicated log entries because  it's not immediately reproducible, feel free to exclude. I'd be looking for connection level logging events that include any socket messages or errors.\n. For the next major version of pika (1.0.0), we'll be moving to pamqp which impacts much of this (#645).\nThat being said, this should be fixed for the 0.10 branch with as few changes as possible.\nIt'd also be code to get a unit test in that covers the failing case.\n. You'd use Channel.bash_publish and set the values in the headers property as they demonstrated in the readme\npython\nchannel.basic_publish(exchange='test',\n                      routing_key='test',\n                      body='Hello World!',\n                      properties=pika.BasicProperties(content_type='text/plain',\n                                                      delivery_mode=1,\n                                                      headers={'x-delay': 5000}))\n. I like the idea, but unfortunately this will be overwritten when the codegen is run. Please modify https://github.com/pika/pika/blob/master/utils/codegen.py#L283 to get your changes in.\n. You'll need to reimplement this on top of the changes in master, so I'm going to close this and look for a 2nd PR.  Thanks :)\n. I do try and keep it up to date, and the goal was to strip down the behaviors, but I'll close this since it was more for me to maintain spec stuff in one spot. If the project wants to do so going forward, that's cool, but I won't leave it as a to-do.. When you have a handler for on_close_callback, when it's closed closed due to a socket error, you will get 0 like you saw. If you're closed by RabbitMQ, you'll get the info you send. \nIt seems like it should be reasonable to add the socket exception message in the reply_text variable in the callback though.\n. That means that the connection to your RabbitMQ server was closed -- you should make sure RabbitMQ is running and accepting connections.\n. This seems like the correct behavior to me. Thanks!\n. Yes\n. FWIW I use pika's Tornado adapter very heavily with 0.10 (and all previous versions). \nOne thing that's a bit concerning is you point out that the app is blocking for 4+ seconds... which is generally not advisable for an async app. Usually for blocking operations, you'll want to spawn some sort of background processing thread. Check out this blog post:\nhttp://lbolla.info/blog/2013/01/22/blocking-tornado\nThat being said, without pika logs and/or RabbitMQ logs that demonstrate the issue, it's not something we can reproduce or troubleshoot. pika has very verbose debug logging which should help.\n. @dwt in a single threaded, async env, all blocking is blocking.\nre logging: sounds like celery or tornado-celery or something is playing with the logging config.\n. Are you talking about the BlockingConnection adapter or pika as a whole?\n. Correct, you might want to check out http://pyrabbit.readthedocs.org for admin related behaviors\n. I'm good to merge this if the test coverage is improved to cover the change.\n. Don't use time.sleep... that blocks Python from running. Use BlockingConnection.sleep:\nhttp://pika.readthedocs.org/en/0.10.0/modules/adapters/blocking.html#pika.adapters.blocking_connection.BlockingConnection.sleep\nIf you're emulating some long running processing per message, you're best to use a background thread for the actual processing and poll for completion before moving on to the next message.\n. Why are you creating a new channel every time? Why not pool the channels as well, or an object for Connection+Channel?\nChannels are really just an RPC feedback mechanism so you get exceptions and such for your commands when they fail. Should be no harm in holding them in your pool. They should be the primary thing APIs interact with instead of connections.\n. Correct, you can use transactions, mandatory publishing, or publisher confirmations to get that kind of information. RabbitMQ.com is a good resource for such info.\n. It looks like the except block is doing exactly what it should do.. if there's an underlying OS socket error that is not expected other than the OS wants to block the socket from being written to, it raises the exception.\nThis might be a better question for @vitaly-krugl since this is relatively new code. You might be able to bypass the error entirely, but I'd be hesitant to do Windows specific branching logic there. @vitaly-krugl any reason you wouldn't want to just fail silently out if the socket is gone when stopping? And why raise an exception when stopping if the IOLoop or socket go wonky?\nI'm open to patches to improve Windows support, but not at the cost of readability or functionality in the library.\n. No updates or replies in some time. Closing.\n. How are you reestablishing the connection?\n. Thanks :)\n. Thanks\n. I don't think we need both...\n. Reviewed the changes you made, they work for me.\n. I don't have any blocking concerns. Please do review the comments for future consideration.\n. Other than the things I pointed out that deserve conversation or consideration, this looks good. Do you mind rebasing once we've come to a decision about the few points of concern?\n. Looks good :+1: \n. Why would you start the connection before forking a child process? Just open a connection per child process.\n. I'm not keen on the amount of changes here that seem to be for a less than ideal use case.\nI see threading as a bad thing in general. It's rarely properly implemented, hard to do properly, and Python's support for threading is terrible.\nI could see putting some primitives in place to help with it, but I do not see changing the core asynchronous nature of Pika to support BlockingConnection, when I've wanted to remove it for some time.\n. Reading back to the original intent of the patch, I think it adds needless complexity. I could see adding a Connection level primitive for checking to see that the outbound write buffer is empty. I do appreciate the time and consideration you made in the patch, but don't feel it's right at this time.\n. Doh I thought this was off of 712.  Mind rebasing for me to merge?\n. The error is because your code on line 332 is trying to start an already running Tornado IOLoop. Try commenting out line #332.\n. I'll have to check out what you have going on. I run Tornado + pika in production with very heavy use without memory leaks, so it'll be interesting to troubleshoot.\n. First off, don't use time.sleep with Tornado (or any IOLoop based implementation) -- you'd be better off scheduling a callback on the IOLoop (See http://www.tornadoweb.org/en/stable/ioloop.html#tornado.ioloop.IOLoop.add_timeout)\nI'd say the main bug you're reporting here is that pika.exceptions.IncompatibleProtocolError is being raised because the state is never being cleared when disconnected.  I'll take a look and see what's up there. You should be getting AMQPConnectionError each time.\nWhat version of pika are you using?\n. | The pika.exceptions.IncompatibleProtocolError is being raised when rabbitmq is back up again.\nThat does help confirm the theory of the connection state not being reset on disconnect. I'll take a look and try and get a fix in soon.\n. This is trivial enough, I think it's fine. Thanks for the PR.\n. Are you sure this is the right fix? What about\nhttps://github.com/vitaly-krugl/pika/blob/issue728-backpressure-divide-by-zero/pika/connection.py#L2120\nWon't this PR count the # of bytes sent to be 2x what they are?\n. The reason is detailed in your logs:\n2016-10-09 20:43:49 [pika.adapters.base_connection] ERROR: Socket Error: 104\n2016-10-09 20:43:49 [pika.adapters.base_connection] WARNING: Socket closed when connection was open\nhttps://www.google.com/search?client=safari&rls=en&q=google+unix+socket+error+104&ie=UTF-8&oe=UTF-8 yields that 104 is \"Connection reset by peer\" which would indicate the socket was remotely closed. \nThere re too many possibilities to enumerate here, but one thing to take a look at is if you have heartbeats enabled and are not responding to them and RabbitMQ is disconnecting you.  You might want to take a look at your RabbitMQ server logs to see if it says anything about the connection your seeing reset.\n. I've got some time set aside this weekend to review things with a goal to get a pre-release of the next version out.. Without seeing your code, it's hard to tell what is wrong other than a lack of exception handling. When the connection is closed, the exception contains the reason. So will your RabbitMQ logs.. do not use time.sleep\nhttp://pika.readthedocs.io/en/0.10.0/modules/adapters/blocking.html#pika.adapters.blocking_connection.BlockingConnection.sleep. The class is fully documented @ https://pika.readthedocs.io/en/0.10.0/modules/parameters.html#connectionparameters\nI'm not sure what you mean by \"by in my application its uses as a endpoint mean port are not required.\"\nIf you're using the default port number, you do not need to specify them.. https://pika.readthedocs.io/en/0.10.0/modules/parameters.html#urlparameters. Seems like the only way to make this work implicitly (which goes against the zen of Python) would be to have if condition blocks in the marshaling code, which will require refactoring the codegen.py.\nhttps://github.com/pika/pika/blob/master/utils/codegen.py#L129\nI would probably refactor it to inspect the value instead of the type, when it's a numeric, and then marshal appropriately. The solution is not to just change all ints to longs. That'll eat up extra bytes and may end up breaking frames.\nI can't say I agree with the sentiment but understand where it's coming from.\n. Monkey-patching socket.socketpair should be avoided. I'd just keep all socketpair definition and use for pika within the pika package.. Looks like I created a conflict in cleaning up the unit tests. Mind rebasing the tests and resolving?. Since it's async, are you letting the IO Loop process events while you're doing work or are you blocking the whole time?\nYou might want to implement something like this in a tight inner loop when processing a big message on an async stack: \nhttp://rejected.readthedocs.io/en/3.19.5/api_consumer.html#rejected.consumer.Consumer.yield_to_ioloop. AMQP is a bi-directional RPC protocol. Blocking communication with the server in single threaded/processing apps is less than ideal and can create unpredictable results.\nSorry I see you're using BlockingConnection but linked to the async example.\nIf you have a for loop in your code when processing a message, I'd suggest you invoke this on each iteration:\nhttp://pika.readthedocs.io/en/0.11.2/modules/adapters/blocking.html#pika.adapters.blocking_connection.BlockingConnection.process_data_events\nBTW the error looks like your TCP connection is being severed either by the client OS, server OS, or a proxy in-between. You could look in RabbitMQ logs to see if it similarly complains about the socket going away.  Enabling heartbeats and letting the IO Loop \"breath\" by invoking process_data_events as mentioned above should keep you from being disconnected.\nFor long running single actions (ie converting media files, etc) where you can't do such a thing, I'd recommend processing the action in a thread and wait on it to complete -- IE a new thread that you join(), then ack the message -- per message.. Try something like this:\n```python\nimport re\nimport json\nimport threading\nfrom google.cloud import bigquery\nimport pandas as pd\nimport pika\nfrom unidecode import unidecode\ndef process_export(url, tablename):\n    df = pd.read_csv(csvURL, encoding=\"utf-8\")\n    print(\"read in the csv\")\n    columns = list(df)\n    ascii_only_name = [unidecode(name) for name in columns]\n    cleaned_column_names = [re.sub(\"[^a-zA-Z0-9_ ]\", \"\", name) for name in ascii_only_name]\n    underscored_names = [name.replace(\" \", \"_\") for name in cleaned_column_names]\n    valid_gbq_tablename = \"test.\" + tablename\n    df.columns = underscored_names\n# try:\ndf.to_gbq(valid_gbq_tablename, \"some_project\", if_exists=\"append\", verbose=True, chunksize=10000)\n# print(\"Finished Exporting\")\n# except Exception as error:\n#     print(\"unable to export due to: \")\n#     print(error)\n#     print()\n\ndef data_handler(channel, method, properties, body):\n    body = json.loads(body)\nthread = threading.Thread(target=process_export, args=(body[\"csvURL\"], body[\"tablename\"]))\nthread.start()\nwhile thread.is_alive():  # Loop while the thread is processing\n    channel._connection.sleep(1.0)\nprint('Back from thread')\nchannel.basic_ack(delivery_tag=method.delivery_tag)\n\ndef main():\n    params = pika.ConnectionParameters(host='localhost', heartbeat=60)\n    connection = pika.BlockingConnection(params)\n    channel = connection.channel()\n    channel.queue_declare(queue=\"some_queue\", durable=True)\n    channel.basic_qos(prefetch_count=1)\n    channel.basic_consume(data_handler, queue=\"some_queue\")\n    try:\n        channel.start_consuming()\n    except KeyboardInterrupt:\n        channel.stop_consuming()\n    channel.close()\nif name == 'main':\n    main()\n```. There's a lot of work in this PR that I'd say misattributes the changes to removal of 2.6 support. Code formatting and style changes aren't the same as removing 2.6.\nI don't disagree with the code formatting changes, but I'd rather separate concerns TBH.. That being said I'm a big +1 for dropping 2.6 support as long as we're very clear in versioning that it is a breaking change for anyone who's stuck in Python pre 2013. (2.6 EOL was 2013).. I'm fine with that. I've got a pattern of use with travis that works pretty good for collective coverage now too. I'll submit a PR that cleans up  travis and removes pyev.. Closing this one too until I confirm the weirdness is not being weird.. EDIT: So it looks like the PR job is run against your repo and it can't submit coverage. I'll add a condition to the job for now, but need to figure out a better way to handle it.. I'm slightly concerned that this is the first major deviation (that I can think of) from the AMQP spec in the API behavior for argument naming. Does renaming beat better educational materials here?. I'm ok with a rename if we respect the kwarg for no_ack and clearly document the new behavior including respecting the \"official\" no_ack kwarg. @vitaly-krugl?. Yeah, that no_ack continues to be supported via kwargs and is documented as such, but is not explicitly part of the function signature.. I don't think there are changes moving away from the AMQP protocol as specified elsewhere are there? I'd like to keep it for those of us who know what the API should be.\nThe RabbitMQ team wants to change it because supporting new users who don't read docs is hard (which is totally understandable and has been demotivating to me).\nI thought supporting the \"correct\" name and semantics via kwargs while changing the explicitly defined parameters was a good compromise.. Just dropping a note to let you know I've seen this and am digesting it.. Updated to redirect to \"Stable\" docs. You could have walked over to my desk :-p.. Updated. You could have walked over to my desk :-p.. I prefer pep8 style from yapf to black. I don't see how black's formatting preferences make substantive changes to the process from yapf's. If I were still the only maintainer, I would have closed this.. Yeah, I'd just include the first paragraph bits in the top comment of the file.\n. I'd prefer not to use six, but rather to start focusing on Python 3 first with backwards compatibility patched in for two.\nIs the desire to use six to just bypass having to deal with the differences?\n. This would probably be a pretty handy function to abstract out to the top level of a connection if it matches the format in the RabbitMQ management plugin.\n. Maybe deprecate this?\n. Why the removal of the params from the docstring? Is it a dupe for the init comment? It might be better to move the init up from a presentation perspective in sphinx. I guess let's see how that turns out.\n. At this point, why not from pika import compat? I don't consider this a blocker, just more of a thought about the coding style of importing a bunch of methods like that.\n. I wonder if we can/should remove this soon given connection.blocked has been around for some time at this point.\n. Yeah, that seems reasonable.\n. IIRC _rpc doesn't handle the full Basic.GetOk method, header, and body frames.\n. Why the tuple and trailing comma? Creating an extra data structure for the formatting seems unnecessary.\n. Shouldn't this come after  the check on 1031?\n. Style nit: redundant parentheses.\n. Hmm I seem to remember something about consuming with noack=True with messages in the socket recv buffer prior to the Channel.Close being issued. Even with a high QoS prefetch, could you not have frames that have yet to be decoded when you go to close a channel?\n. IIRC the original implementation was fairly agnostic to which side of the conversation it was on.\n. Just to reiterate, I think there is a case where you could have a pile of frames on the stack that are unprocessed when a Channel.Close is emitted. My thinking is that issuing the close should be similar to Basic.Cancel. I guess one could make the argument that both should not be respected until their respective Ok frames are received though.\n. IIRC xrange has some portability issues -- pypy? I don't recall exactly what the issue was that had me go back to it.\n. I prefer '{}'.format() as well. Perhaps post merge cleanup at some point.\n. It implemented the semantics where it could be used as either.\n. IIRC this approach caused problems in the past with interspersed frames due to IOLoop yielding or some such behavior? I don't remember the details, but the write buffer behavior was to make sure that multi-frame deliveries were atomic in being written.. I'd like to merge this as something atomic that works. I don't know what will or will not break in 3.7 and I'd rather get us to a baseline before tackling any 3.7 specific changes.\n934 should rebase based upon this add add a 3.7-dev version in IMO..",
    "itkach": "Hm... I spoke too soon - there's more to it than just simplebuffer.py. After applying the patch I'm getting another error:\nerror: uncaptured python exception, closing channel  (exceptions.AttributeError:'module' object has no attribute 'unpack_from' [/usr/lib/python2.4/asyncore.py|read|69] [/usr/lib/python2.4/asyncore.py|handle_read_event|391] [/usr/lib/python2.4/site-packages/pika/asyncore_adapter.py|handle_read|38] [/usr/lib/python2.4/site-packages/pika/connection.py|on_data_available|216] [/usr/lib/python2.4/site-packages/pika/codec.py|handle_input|112] [/usr/lib/python2.4/site-packages/pika/codec.py|_waiting_for_header|125])\nLooks struct.unpack_from() which was added in Python 2.5 is used in many places...\nIs there any chance the code can be changed to be Python 2.4 compatible? For those using CentOS there's no simple way to upgrade to a newer Python version.\n. It does. I think this is because RabbitDispatcher sets itself to None in handle_close(): \nhttp://github.com/tonyg/pika/blob/master/pika/asyncore_adapter.py#L68\n. ",
    "enlavin": "Based on your patch I did some more monkeypatching and I have been able to run under python 2.4 apparently without issues:\n```\ndiff --git a/pika/init.py b/pika/init.py\nindex 0a21b6f..fa8c502 100644\n--- a/pika/init.py\n+++ b/pika/init.py\n@@ -63,3 +63,13 @@ from pika.blocking_adapter import \\\ndef repl_channel(host = '127.0.0.1', args):\n     return BlockingConnection(ConnectionParameters(host, args)).channel()\n+\n+try:\n+    import struct\n+    getattr(struct, \"unpack_from\")\n+except AttributeError:\n+    def _unpack_from(fmt, buf, offset=0):\n+        slice = buffer(buf, offset, struct.calcsize(fmt))\n+        return struct.unpack(fmt, slice)\n+    struct.unpack_from = _unpack_from\n+\ndiff --git a/pika/simplebuffer.py b/pika/simplebuffer.py\nindex 2db1d0b..5a9dcae 100644\n--- a/pika/simplebuffer.py\n+++ b/pika/simplebuffer.py\n@@ -60,6 +60,11 @@ try:\n except ImportError:\n     import StringIO\n+try:\n+    getattr(os, \"SEEK_END\")\n+except AttributeError:\n+    os.SEEK_SET, os.SEEK_CUR, os.SEEK_END = range(3)\n+\nclass SimpleBuffer:\n     \"\"\"\n@@ -145,7 +150,7 @@ class SimpleBuffer:\n def __nonzero__(self):\n     \"\"\" Are we empty? \"\"\"\n\n\nreturn True if self.size else False\n\nreturn self.size > 0\ndef len(self):\n     return self.size\n@@ -155,4 +160,4 @@ class SimpleBuffer:\ndef repr(self):\n     return '' % \\\n-                    (self.size, self.size + self.offset, self.read(16), '...' if self.size > 16 else '')\n+                    (self.size, self.size + self.offset, self.read(16), (self.size > 16) and '...' or '')\n```\n. \n\n",
    "stevvooe": "How often does spec.py change? Does it need to be built on each client? Maybe distribute it already built with the source, much the same one might do with swig.\n. ",
    "ask": "I did try that as well, without success:\nelif isinstance(value, long):\n        pieces.append(struct.pack('>cQ', 'Q', value))\n        tablesize += 9\n. Did you confirm that this works?  AFAIR I tried this exact implementation and then it didn't work.  Just want to confirm so I can apply the same to amqplib.\n. majek: I never tried with basic_publish, the problem was with having it work with the x-expires argument to queue_declare.  I can swear I remember trying it with that patch, (that argument can be signed int now though, so not that important for me anymore).\n. Awesome! I'll test the fix to confirm\n. ",
    "shogsbro": "I made & tested the changes required, which can be viewed here:\nhttp://github.com/shogsbro/pika/commit/f8771938e74bed1601854b589cd3753d0a803589\n. After another review of the code I start to understand why it receives the ChannelHandler.  The Channel() object is returned to the client app on connection.channel(), but never stored or used within pika's connection object. \nI can see 2 solutions:\n1) Change the connection.channel() function to store a map of Channel() objects, and use that to pass the Channel object to the callback (relatively trivial, but feels a bit hacky).\n2) Merge the Channel() and ChannelHandler objects (potentially has other ramifications, but seems cleaner)\n. ",
    "annwitbrock": "Finally installed 0.9.3 on Windows.\nUninstalled pika 0.5.2 first then used:\npip install -e git+http://github.com/tonyg/pika.git@v0.9.3#egg=pika-v0.9.3\n(objected to easy_install, wanted -b option, and didn't like pip --upgrade option)\nTried it on Ubuntu too (pip --upgrade OK). Ran our example send.py and it suddenly needs 2 arguments to Connection.channel()\n. ie pika.AsyncoreConnection.channel()\n. ",
    "bkjones": "+1 \nI've been using gmr's code for two production services for some time. I haven't tested the code in this particular commit, but what's been done up to this point would appear to be solid work. \n. These changes look refreshingly boring - thanks @zlane \nI have obligations of my own, but I can try really hard to put this changeset through its paces over the next week or so. Of course, if someone else got to it sooner, all the better :) \n. yeah, @ztane - I ran tests straight out of your fork and all's well: \nshell\n__________________________________________________________ summary __________________________________________________________\n  py26: commands succeeded\n  py27: commands succeeded\n  py33: commands succeeded\n  py34: commands succeeded\n  congratulations :)\ns757-2:pika brianj$\nThis was against a locally running rabbitmq 3.2.4. \nA rebase would be great at this point. \n. ",
    "kisielk": "I understand how the new continuation passing model works. I'm just having trouble getting this seemingly simple use case implemented.\nBasically I should be able to:\n1. Declare a connection, channel, and callback queue\n2. Use basic_publish to send a message using a routing key\n3. Wait for the response to come back\n4. Return the response to the caller\nThen at some point be able to call the RPC function again and do steps 2-4 without reconnecting.\nIn the example with Pika 0.5 this was seemed possible because the connection, channel, and queue could all be declared before the asyncore_loop was entered.\nI've tried to emulate the same behavior by starting and stopping the ioloop of SelectConnection, but it seems that the second time ioloop.start() is called it doesn't enter the event loop again.\nI realize that github probably isn't the best place to ask for help on this, is there another way you'd prefer to be contacted?\n. Looks good to me :) Thanks for fixing it.\n. ",
    "dgorissen": "I have been trying to do exactly the same thing and came up with something that seems to work but is a massive hack (see: http://pastebin.com/kP6vWczt). \nHowever, sometimes it still crashes with:\n\nFile \"build\\bdist.win32\\egg\\pika\\channel.py\", line 487, in _on_basic_deliver\nKeyError: 'ctag0'\n\nAnd googling around on this error gives: http://cms02.phys.ntu.edu.tw/tracs/env/ticket/320\nHavent been able to resolve it yet.\nI tried creating a new Blocking connection instead, but for some reason that interferes with the main SelectConnection object (e.g., its on_connection callback gets triggered for some strange reason).\nLooking forward to the proposed changes in pika 2.0\n. @gmr: thanks for the response. Using master things seem a bit more stable, however all problems are not yet solved it seems :/\nMy test case involves 5 processes (single machine) who need to coordinate to do a task. This involves 1 process making 2 identical rpc calls at different times using the posted code.  Running my test case causes a repeatable crash the second time the rpc call is made:\nFile \"build\\bdist.win32\\egg\\pika\\channel.py\", line 439, in _on_basic_deliver\nKeyError: 'ctag0'  (full stacktrace: http://pastebin.com/LSU8a2yV)\nIn debugging mode I have also noticed two new warnings.  Not sure where they are coming from:\npika-0.9.5b-py2.6.egg\\pika\\callback.py:69: UserWarning: CallbackManager.add: Duplicate callback found for \"2:Channel.Close\"\npika-0.9.5b-py2.6.egg\\pika\\connection.py:642: UserWarning: Pika: Write buffer exceeded warning threshold at 131072 bytes and an estimated 16 frames behind\nMy environment: 2.6/pydev/winXP 64\n. Thanks for the reply.  Strange.  My test case sends about 10 messages over the space of 30 seconds so nothing dramatic.  However, 5 of these messages are about 680 kb so maybe that is causing problems.  Will compress them and see if that changes anything.  Will also try to create a minimalist test case that causes the crash.\n. Ok, the crash disappears if I add the following two lines before line 439 in channel.py:\nif consumer_tag not in self._pending:\n                self._pending[consumer_tag] = list()\nNot sure if this is correct, but it does remove the crash.  The two warnings remain though.\n. I kept on running into strange errors or behaviors, both with the current code and with the blocking adapter (Im sure many of which can attributed to my own code).  I eventually reverted back to my Kombu-based code as means of comparison.  With a little tweaking it now all works perfectly so I kept it that way.  Will test again with pika 2.0 once its out.\n. Just wasted a lot of time on this problem, is there an easy patch we can apply to 0.9.4? (browsing the commit log does not reveal a simple fix)\n. ",
    "vladev": "The same confusing behavior is observed when vhost=u''.\n. ",
    "hntrmrrs": "It appears this issue is not fixed, as demonstrated by the following gist:\nhttps://gist.github.com/959237\nI think that payload in the _marshal function should be encoded as a bytestring (possibly UTF-8?) before concatenation. You can see this demonstrated in the exception handler in my above gist.\n. Perhaps it would be useful to provide a configuration setting which allows the client to specify whether (and how) they want unicode instances encoded as bytestrings. The default could be 'utf-8' and rather than handling the exception just test if the pieces during marshalling are instances of unicode and encode them accordingly.\nI think it might be too \"late\" to catch the UnicodeDecodeError, potentially really hurting the marshalling performance. That's just a gut feeling though; I haven't tested it.\n. ",
    "hownowstephen": "I'm having an issue that I think is related to this\nFile \"/Users/stephen/devel/sweetiq/env/src/pika/pika/frame.py\", line 36, in _marshal\n    payload = ''.join(pieces)\nexceptions.UnicodeDecodeError: 'ascii' codec can't decode byte 0xe8 in position 4: ordinal not in range(128)\nUsing the 0.9.5 release - is this still an outstanding issue, and if so, what can I do to fix it?\n. ",
    "rickardb": "I have the same problem, using blocking connection and different processes with the multiprocessing module (creating new connection in each process).\nCode example: https://gist.github.com/854486\n. I am creating a new connection in the subprocess.\n. ",
    "MyGGaN": "I experience the same issue, but v.0.9.3 works just fine.\nI think the problem is in the ConnectionState class in pika/connection.py. And was introduced by the edb9c3704c87d0a3915679ec788ef1b244e29836 commit.\n. ",
    "williamsjj": "That would explain why copying in the spec file from v0.9.1 didn't help.\n. ",
    "reduxdj": "\nand also how to use routing keys, please.\n\nThanks\n. verified this is working .9.5b thanks\n. ",
    "vermoudakias": "Yep, master works smoothly.\nThanks for your effort on this nice project.\nMichael.\n. I had a similar case (recursion error when ack'ing messages with prefetch count) using the select_connection adapter and it seems that this patch \nhttps://github.com/vermoudakias/pika/commit/2e9c88380d52bfe2487b42fbef52f26371515a7b resolved the error.\n. It isn't an error, just a warning. You can make it disappear by replacing this line:\npython\nchannel.exchange_declare(exchange='direct_logs', type='direct')\nwith this one:\npython\nchannel.exchange_declare(exchange='direct_logs', exchange_type='direct')\nMike.\n. Can you post your code? Or just the relevant sections?\nThe message:\nNo handlers could be found for logger \"pika.connection\"\nis shown because pika needs a logger named \"pika\" to have been defined by the application.\nOther than that (the warning message), it does not cause a problem (that's why your message is sent).\n. The exception is raised because you hit CTRL-C (it's normal). If you want you can handle it:\npython\ntry:\n    channel.start_consuming()\nexcept KeyboardInterrupt:\n    # Do cleanup or something...\n    pass\n.  Can you send the full traceback?\n. Possibly a pika bug.\nTry to comment out the line 549 in \"pika\\spec.py\".\n. Great! Definitely not a complete fix, just a hint.\n. ",
    "arturz": "Forgot about the Markdown, sorry!\n. Thanks, just downloaded the 0.9.5 beta, works fine.\n. ",
    "slyngshede": "Aah, installing from Git fixed it. The Easy_install / pip version is broken. It might be an idea to either make that clear, or upgraded it. \nThanks.\n. Aaah, sorry, I installed via pypi a few weeks ago, before 0.9.5 was uploaded.\n. I started capturing the VERY verbose logs. I'll let it run for a while until I'm sure that it fail and ship the log to you.\n. Did you get the logging info I send you? I couldn't really make see anything weird in it. \n. I tried upgrading Pika, it might have worked, but there where other issues. My messages seems to be to large for the 0.9.6 branch. Most of the messages where sadly corrupted by Pika. I'll try again when the next version is ready. \n. We've tried doing some testing with py-amqplib, and we're seeing the same issue. I think it's the same problem as have been reported to the py-amqplib developers:\nhttp://code.google.com/p/py-amqplib/issues/detail?id=32\n. I'm starting to question if this is worth debugging on any further. It really looks like it's Windows being stupid. \nHeartbeats aren't really working either, I tried solving the problem by enabling heartbeats and simple reconnect when the connection dies, it works create on Linux, but on Window you get connected to the queue for a brief second and promptly disconnected, but the on_close callback is never called.  \nDepending on how you look at it, it seems that it's more a question of fixing things in Windows, given that py-amqplib also stalls, that fixing something in Pika. \n. ",
    "philwhln": "Has there been new release that supports this? This is a year old, but 0.9.5 seems to still be the current version. If I download from github directly what is the most stable branch / commit that supports this?\n. ",
    "kil": "Sorry for the late reply. Just tested with 2b6627f3 and it works fine. Thanks!\n. ",
    "ak1394": "Actually, it looks simply calling _on_connection_closed() from _adapter_disconnect() is not enough to fix reconnecting behavior, as it stops IOLoop() from acting on timeouts.\n. Yes, I'm using SimpleReconnection strategy, and expecting it to try to reconnect when RabbitMQ server is stopped and restarted.\n. That's perfect! Thanks a lot.\n. Don't know, I think we ended up replacing custom python code with celery\nand left it at that.\nAnton\nOn Dec 15, 2011 4:52 AM, \"Benno Rice\" \nreply@reply.github.com\nwrote:\n\nDid this ever get fixed? I've just spent several hours bashing my head\nagainst trying to make reconnection work and then found this. =/\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/pika/pika/issues/57#issuecomment-3156780\n. \n",
    "jeamland": "Did this ever get fixed? I've just spent several hours bashing my head against trying to make reconnection work and then found this. =/\n. ",
    "wwsmith": "I found a very similar problem in BaseConnection._handle_read. It is similar enough that I'm not sure a new issue is needed for it.\nWhat happens is that the socket.recv() raises an ssl.SSLError: [Errno 2] _ssl.c:1348: The operation did not complete (read). This looks to correspond to the OpenSSL error SSL_ERROR_WANT_READ. The proper way to handle it appears to be to try the read again until it succeeds.\nMy simple work around is to just loop in handle_read until data is received or an exception other than that SSLError occurs.\n. Is below what you're looking for?\nA question related to this patch: Are there reasons why a pika user might want to define their own credentials (without modifying the pika code)? I did it because of a case that pika didn't handle. I had to do something similar for a bit with the RabbitMQ Java client to work around a bug - I defined my own SaslConfig and then told the Java client to use it via ConnectionFactory.setSaslConfig(), but I could do this without modifying the RabbitMQ Java client. I'm not sure if there are situations where it would be useful to be able to register a Credentials class from user code into pika...\nThanks for the fixes!\n$ git diff\ndiff --git a/pika/credentials.py b/pika/credentials.py\nindex 24c99be..438e452 100644\n--- a/pika/credentials.py\n+++ b/pika/credentials.py\n@@ -55,6 +55,25 @@ class PlainCredentials(object):\n             self.username = None\n             self.password = None\n+class ExternalCredentials(object):\n+\n-    TYPE = 'EXTERNAL'\n  +\n-    def init(self):\n-        self.erase_on_connect = False\n  +\n-    def response_for(self, start):\n-        \"\"\"\n-        Validate that our type of authentication is supported\n-        \"\"\"\n-        if ExternalCredentials.TYPE not in start.mechanisms.split():\n-            return None, None\n  +\n-        return ExternalCredentials.TYPE, \"\"\n  +\n-    def erase_credentials(self):\n- pass\n+\n# Append custom credential types to this list for validation support\n-VALID_TYPES = [PlainCredentials]\n  +VALID_TYPES = [PlainCredentials,ExternalCredentials]\n-----Original Message-----\nFrom: gmr [mailto:reply@reply.github.com] \nSent: Tuesday, May 03, 2011 12:35 AM\nTo: Warren Smith\nSubject: Re: [GitHub] EXTERNAL credentials [pika/pika GH-60]\nInteresting, care to provide a patch? If you fork and commit, I'll review and apply if I don't see any issues. Sounds like you went about how I envisioned adding stuff to credentials.py.  Thanks!\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/pika/pika/issues/60#comment_1092174\n. ",
    "kherrala": "I spent some time toying with this issue and wrote a simple unit-test testing my patch. I can confirm it is related to packets over about 512 kB. I replaced the socket.send() and socket.recv() calls with loops as suggested earlier, but did not use exact same buffer every time. I also added code to ignore SSL_ERROR_WANT_READ and _WRITE exceptions in handle_exception().  I think looping needs to be done always anyway to ensure some data is sent?\n. I believe this might turn out quite hard to fix, because it is related to an issue with Python. That's why I never faced the WRITE_PENDING error by myself with 2.7.1. There seem to be workarounds which cause blocking as has been discussed earlier. Perhaps there could be a temporary fix to get this working? See these for reference:\nhttp://bugs.python.org/issue8240\nhttp://bugs.python.org/issue12343\nhttp://bugs.python.org/issue3890\nhttp://bugs.python.org/issue8222\nI'm also looking into another SSL issue which could be related to handling error conditions, when socket is disconnected because network interface is down. I will publish report when I have it confirmed.\n. I believe this issue has been fixed now. I have a quite large number of busy SSL clients using Tornado and none have failed so far.\n. I've had some problems with detecting disconnect myself, but I believe the problem here is different. I think the reason is that the send loop is blocking the ioloop thread in on_channel_open, so that it can never resume updating connection state etc. You might have to do the sending from another thread bound to the channel object.\n. I would argue that optional support for 2) is required in core, because it is too cumbersome to override this logic. Would it be to opinionated to have hostname check in place if \"cert_reqs\" is set as in part 1) ? Python 2.x can be supported using https://pypi.python.org/pypi/backports.ssl_match_hostname\nI can make a pull request for this change if needed.\n. @vitaly-krugl Any comment on this?\n. @vitaly-krugl I have released some of the SSL patches we are currently running in production on hundreds of clients with some times poor connectivity. They can be found from my fork in master branch. These are essentially the same changes as in my previous pull request #501. Unfortunately I don't have the time to verify them on current master but they are working fine on my original fork based on Pika 0.9.14. Maybe this can be useful for you if you are intending to fix this?\nFrom my second patch it should be obvious that MITM protection cannot be implemented using ssl_options or the current API. \nI haven't looked at SSL renegotiations, but they don't seem to be a problem for us. I believe they are handled using Python socket wrapper and standard event processing automatically?\n. I don't see why anyone would like to override certificate name validation in HTTP clients so why would it be required here? See for example sane default behaviour found here: https://github.com/shazow/urllib3/blob/master/urllib3/connection.py\nIn my opinion every TLS client library should do these check at the least. This kind of MITM vulnerability would be found in a normal security audit very easily.\nAnother rather more opinionated suggestion is to depend on certifi (http://certifiio.readthedocs.org/en/latest/) used by the requests library.\n. +1 for the attached pull request. Does it break BlockingConnection or why shouldn't we have this as temporary fix for such a major issue?\n. It seems there's another bug handling ssl.SSL_ERROR_WANT_READ in _handler_error(). Current behaviour is to disconnect immediately. I seem to be having all sorts of strange disconnect behaviour running on weak 3G connection.\n. Sure if you can make it work that way, but the current behaviour is to loop eternally doing the handshake and never return control back to running the IOLoop. The other problem was not catching socket exceptions during handshake, which is not handled well by the caller.\n. Any plans getting this to master? It has been working fine for me. Using standard select() during initial handshake doesn't seem like a big issue to me.\n. ",
    "agronholm": "So have you pushed it now? TornadoConnection seems to be the only viable connection to be used with threads. I hope 0.9.6 is released soon.\n. Hm, the example above does not seem to hog the CPU. This is with Tornado 2.0 and pika 0.9.5.\n. ",
    "rckclmbr": "I believe I resolved this in c0f92c0cba8878ba6e65ddbee651454c7cf38916.  The issue was that it was scheduling/running its state management every .25ms, when it should have been a lot less than that (I set it to 250ms, which was probably what was intended).\n. wulczer committed a twisted impl: https://github.com/pika/pika/pull/81\n. s/socket value/error value/.  :)\n. Thanks a lot, I'm going to need this very soon.\n. ",
    "SlNPacifist": "OK, but the main problem still happens: calling self.socket.fileno() causes error(EBADF, 'Bad file descriptor').\nI use Ubuntu 10.04 with python 2.6 and pycurl 7.19.0-3\n. ",
    "mianos": "You can make this low priority now. rabbitmq was overkill for my project and this problem stopped my progress so I have ported over to pyzmq under zeromq. Sorry if you wasted time looking at it for me, although I suspect it may be a usage  problem and not a bug.\n. ",
    "seletz": "Please, better use smth like::\nsocket.SOL_TCP = socket.IPPROTO_TCP\nas a fix for jython (see also https://github.com/andymccurdy/redis-py/pull/124)\n. ",
    "yihuang": "I've got this problem too, it there any way to work around this?\n. ",
    "oubiwann": "Hrm, after looking at this some more, it might just make more sense to implement a Twisted AMQP library that uses much of Pika as its basis... \n. ",
    "dbrobins": "I believe I'm seeing this issue too: similar situation, draining existing messages under begin_consume before start_consuming has been called. Thank you for the analysis; I hadn't got that far yet - just some basic debugging - but it all confirms that it's the same case. I'll try your workaround.\n. The workaround works - of course it's just the same as doing \"while not self._received_response:\" since wait can't change outside, so it just stops other functions from kicking send_method out of its loop early.\n. ",
    "leonmax": "I have the same problem, Thank Mikko. I guess I'll look at the original code to see what's happened to the 'self.wait' later, but your workaround works perfectly for now.\n. ",
    "seriyps": "+1, got the same error in the same situation\n. +1\nimport pika\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\nchannel = connection.channel()   # <- exception there\n. Following monkey-patch works for me:\n```\nfrom pika.callback import CallbackManager\ndef sanitize(self, key):\n    if hasattr(key, 'method') and hasattr(key.method, 'NAME'):\n        return key.method.NAME\nif hasattr(key, 'NAME'):\n    return key.NAME\n\nif hasattr(key, '__dict__') and 'NAME' in key.__dict__:\n    return key.__dict__['NAME']\n\nreturn str(key)\n\nCallbackManager.sanitize = sanitize  # monkey-patch\n```\n. ",
    "soichih": "I am seeing the same error.\n. Thanks. When will this be released to pip?\n. When will the fix to this issue released? pip's latest version available is 0.9.13, and we are hit by this problem. Is there a workaround until the fix is released?\n. ",
    "feihuroger": "hi,my native language is not English, I hope I can articulate my thinking.\nIn  AMQP 0-9-1 Complete Reference Guide (http://www.rabbitmq.com/amqp-0-9-1-reference.html)\ndefine  tune(short channel-max, long frame-max, short heartbeat)\nheartbeat is short int.\nI can't  find how to set frame heartbeat in rabbitMQ server , then  rabbitMQ server  always tune  frame heartbeat as 0.\nSo, when I use pika 0.9.5 connect rabbitMQ,  the heartbeat be set to True , check interval is 1 second. With my trail WAN, 1 second interval is too short,  always cause exception of  AMQPConnectionError .\nI have to change source connection.py ,comment code Line 106,107, bypass check type of heartbeat.\nAnd in my consume code, assignment heartbeat as int\nconnection = pika.BlockingConnection(    pika.ConnectionParameters( host='xxx.yyy.zzz.aaa', heartbeat=5 ))\nthen my app check interval is  5 second. Everything is OK.\nbtw:\nWith the connection.py line 87  and line 101 ,has the same code :\n\n# Validate the frame_max type coming in\n    if not isinstance(frame_max, int):\n        raise TypeError(\"frame_max must be an int\")\n\nIs somthing wrong ?\n. hi, I download the latest  develop source from git, the heartbeat type is be set int. It is fine.\nbut  the same code is still here ,Line 120.\n\n# Validate the frame_max type coming in\n    if not isinstance(frame_max, int):\n        raise TypeError(\"frame_max must be an int\")\n\n. ",
    "alertedsnake": "Aha!  I just noticed this one myself!\n. ",
    "mattbornski": "I ran into a similar-looking issue.  When I have heartbeats enabled using 0.9.5, a timeout will trigger, attempting to log a message, but I receive a NameError because the object \"log\" is not defined in the scope of BlockingConnection.\nTraceback (most recent call last):\n  File \"./examples/ping.py\", line 18, in \n    ping()\n  File \"./examples/ping.py\", line 15, in ping\n    print spamqp.receive('receive_pongs') + '!'\n  File \"/Users/mattborn/tools/spamqp/src/spamqp.py\", line 64, in receive\n    (method, header, body) = channel.basic_get(queue=queue_name)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 318, in basic_get\n    self.basic_get_(self, self._on_basic_get, ticket, queue, no_ack)\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pika/channel.py\", line 469, in basic_get\n    no_ack=no_ack))\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 244, in send_method\n    self.connection.process_data_events()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 103, in process_data_events\n    self.process_timeouts()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 157, in process_timeouts\n    log.debug('%s: Timeout calling %s',\nNameError: global name 'log' is not defined\nWhen heartbeats are disabled using 0.9.5, this error does not occur.\nEDITed to add version number, and note that in master with heartbeats enabled this error does not occur.  My issue is resolved by using the bleeding edge version.\n. ",
    "blue-libris": "wulczer: you are AWESOME. Thanks!\n. ",
    "wulczer": "Oops, will provide a fix.\n. Fixed in by branch fix-twisted-heartbeat and pull request sent. Thanks!\nPS: Yo were right about _adapter_disconnect having to call transport.loseConnection, otherwise heartbeat failures would not close the TCP connection. Although if the connection to Rabbit is completely severed (emulated by adding a DROP iptables rule), the connection never gets closed, because what Pika does is it sends a Close frame and waits for a Close-Ok before actually dropping the connection. That's a separate issue though, and possibly not a real problem.\n. I'll try to reproduce it and see if I can come up with a solution.\n. I investigated a little bit and I think I found the core reason for what you are seeing.\nThe exception refers to the previous exchange, but that's as good as it gets with AMQP. When you send a Basic.Publish, there is no confirmation of whether it succeeded or not. If there was an error, you get a Channel.Close message with the reason, but this can happen after you already sent another Basic.Publish message.\nThe way the Twisted wrapper works is that basic_publish always succeeds (because there is no way of knowing whether it succeeded or not!). So what happens is:\n- you call basic_publish\n- Basic.Publish is sent through channel 1\n- the wrapper assumes it's been successful, as the protocol does not mandate any confirmation to be received\n- the server detects a nonexistent exchange and sends a Channel.Close message saying '404 - NOT_FOUND'\n- the wrapper sees this message and marks the channel as closed, any method used on it will now fail\n- you try to call another basic_publish\n- since the channel is already closed, it fails with the original Channel.Close error\nUntil there, it's all good, or as good as it gets when using a protocol that doesn't tell you if what you sent got delivered or not.\nThen starts the trouble. Since you don't pass the channel number to connection.channel(), it gets autogenerated and Pika uses the internal channels list it keeps for that. But the previous channel has already been closed and removed from the list, so you get 1 again. But that channel has already been closed and the server sends back a 503 COMMAND_INVALID error.\nChanging your second example to use first connection.channel(1) and then connection.channel(2) prevents it from hanging.\nNow I think that's a bug in Pika, because it should not reuse the channel number. Note that even using a SelectConnection adapter, you'd get the same problem because of channel number reusing. I'll try to confirm that you indeed cannot reuse channel numbers within the same connection and if so, will propose a patch that will fix that in the base Connection class.\nMeanwhile, I believe that both this and #85 can be argued to be invalid and another ticket regarding channel number reusing should be opened.\nThank you for drawing attention to this issue and for the test cases, they helped a lot!\nPS: The program was hanging because due to channel number reusing the callbacks of different channels were being clobbered in the callback manager, as you could in the warning message that was produced.\n. After looking at this problem a bit more: there might be something wrong in how the Twisted adapter interacts with the channel management code, I'm digging into it.\nAbout different exceptions: once you send the publish with a wrong exchange name, then channel gets closed by the server and any operation you will try with it afterwards will fail with the same error - the one you get after the first erroneous publish.\nYou don't get the error immediately, because publishing always succeeds, unless the channel is closed. You could even manage to send several publish methods before getting the close message from the server, at which point you don't know which publish failed, but you do know that one of them, and all of the later ones, have been discarded because the channel is already closed.\nI know it's confusing :( So let me try to draw the message flow once again:\n- app -> server : publish to exchange \"foo\"\n- (app has no way of knowing if this succeeded or not, prepares to sending another message)\n- server -> app : your channel got closed with a 404 error\n- (app realises that the channel is now closed, so instead of sending another publish message, it raises a ChannelClosed error)\n- (from here on after, every operation the app will attempt using the closed channel will immediately raise an ChannelClosed error with a 404 code)\nHope that cleared things up a bit, I'm off to investigate the channel number reusing issue :)\n. The corresponding pull request has been merged, this ticket can probably be closed.\n. An attempt at explaining what's going on here (and why that's OK) is here: https://github.com/pika/pika/issues/84#issuecomment-2161890\n. Do you have a way of reproducing the write loop? I'm using TwistedConnection in production and never had problems with a busy loop. Admittedly I'm not using the latest git, but pretty close to it.\n. Gah, I'm stupid. I forgot that TwistedConnection and TwistedProtocolConnection are two different things... and I wrote that code! :)\nYeah, we're using TPC, not TC (and I would actually strongly recommend TPC if you have the choice). There's code in examples/demo_twisted.py that can run either TPC or TC. Do you see the issue when running that?\nI'll try it independently tomorrow and see if I can reproduce it. If there's a bug there, it's probably caused by the recent changes, so a little bisecting to be done there.\nSorry for the TC/TPC confusion and thanks for the report!\n. Hi, I wrote the Twisted connection code. Could you explain a little more why would this be necessary? Ideally with a short example of code that would benefit from having this feature.\n. I'm still not sure I understand why do you have to keep your custom function for handling data attached to the Pika Channel object.\nSince you control the implementation of the function processing the data (that's handle_payload in demo_twisted.py), you can call whatever code you want - and if you want it to behave differently depending on the channel, just make it dispatch on something like channel.channel_number.\nThis means you could for instance to something like that:\n``` python\ndef got_channel(self, channel):\n    self.channel = channel\n    HANDLERS[channel.channel_number] = my_channel_handler\ndef handle_payload(self, channel, method_frame, header_frame, body):\n    HANDLERSchannel.channel_number\n```\nI agree it's annoying that the object returned from connection.channel() is not the same as the object you get from the queue returned by basic_consume, but I don't see how this would be easily fixable...\n. We're using pika with the Twisted adapter in production (although it's a fairly old version with some patches backported manually).\nI'd be happy to help with porting it to the latest pika version. I'll try downloading master and running our internal test suite against it and will fix whatever bugs crop up.\n. I tested with git HEAD at that time and it blew up in several places, some on which were issues already reported and since fixed.\nI still intend to port the Twisted adapter to current Pika...\n. Turns out that the old adapter words with current pika after a few cosmetic changes.\nOur internal test suite passes and some smoke tests of the applications confirm that everything is in order.\nI'll be opening a pull request to bring the adapter back and hope to find time to write a few tests and documentation to avoid breakage in the future.\n. Great, thanks!\n. Hi!\nCould you attach simple test cases demonstrating the first two problems?\nAs for the signature of channel_closed, the docstring in channel.py says that callbacks added in add_on_close_callback should receive a frame parameter, so I believe the correct code in TwistedChannel would be:\npython\ndef channel_closed(self, method_frame):\n    # enter the closed state\n    self.__closed = exceptions.ChannelClosed(method_frame.method.reply_code, method_frame.method.reply_text)\n    # ... code continues\nI'll do some tests to see if that's the case and if so, submit a patch.\nThe problem of the reactor being stopped is probably linked to e03c7adbb609f91e7523057a33c05817f096acad. Seems that Twisted{Protocol,}Connection should set stop_ioloop_on_close to False.\nThanks for the report!\n. I've submitted a pull request fixing some of the issues you raised. I could still use a test case demonstrating the other problems.\n. There was an example in the repo once, but it didn't make it through the reshuffling of documentation started with 3fe67be0e9bf818a63d2473a05c686293c12ad21\nI was meaning to recover it and re-submit it for inclusion in the current docs...\n. Hi, could you post a self contained example that I could run? The one here is missing the definition of self._fail_push_event - are you sure you're not returning None from that method?\nThanks!\n. I just tried this with latest pika master: https://gist.github.com/wulczer/5347359\nIt prints publishing repeatedly (assuming you can connect to RabbitMQ with guest/guest and have an exchange called exchange created).\nAfter I kill -9 the server process, I'm seeing FAILURE [Failure instance: Traceback (failure with no frames): <class 'pika.exceptions.ChannelClosed'>: (0, 'Not specified')]\nThis suggests that pika is correctly detecting server death.\nThe example you provided is not really self-contained, there's code missing which prevents me from running it. Would it be possible for you to create a minimal example, similar to mine, that exhibits the bad behaviour?\n. Oh, to have that example work on current pika master (7c4cc75c6f57031507bf00cb3e18af41a7641931 as of this writing) you need to patch TwistedConnection.__init__ to read:\npython\n    def __init__(self, parameters=None,\n                 on_open_callback=None,\n                 on_open_error_callback=None,\n                 stop_ioloop_on_close=False):\n        super(TwistedConnection, self).__init__(parameters,\n                                                on_open_callback,\n                                                on_open_error_callback,\n                                                None,\n                                                IOLoopReactorAdapter(self,\n                                                                     reactor),\n                                                stop_ioloop_on_close)\n(note the extra None parameter)\n. TwistedConnection is also missing an update here:\nhttps://github.com/pika/pika/blob/7c4cc75c6f57031507bf00cb3e18af41a7641931/pika/adapters/twisted_connection.py#L260\nbase_connection.BaseConnection.__init__ accepts 6 parameters and only 5 are passed to the superclass constructor.\n. Coming right up :)\n. Hi!\nIt would be useful if you could produce a self-contained example that someone could run.\nThe example code worked fine when I tried with pika git HEAD, so it's going to be hard to see what your problem could be without more info.\nThanks!\n. Thanks for the full example! I just tried with pika HEAD and it looked like it was working fine:\n- it printed test and then hung\n- I published a message in the test_exchange exchange with test_key as the routing key\n- the program printed test1 and test\nSo as soon as a message is delivered, the Deferred returned from test() fires and test() is called again by the LoopingCall\n. Hey, as the original author of the adapter I'm OK with removing it if no one (including me) finds the time to add test coverage.\nI'll do my best to get something written, but I can't promise anything and if you decide it's time to cut a 1.0.0 and I haven't gotten my act together, go ahead and get rid of it.. Thank you indeed @abompard - and apologies for not keeping up with the maintenance of the adapter. Three cheers for the Twisted community!. ",
    "nkvoll": "There is also probably a race condition if the connection to the AMQP server fails for some reason (connection severed for example) before the heartbeating even has started, but this is rather easily solved at the application layer (which is what I ended up doing), by adding a heartbeat-checker that is able abort and retry the connection if nothing happens for X seconds during the time between when twisted says it has established the connection and the time when pika and the server has started the heartbeating :)\n. Just let me know if you need any more information or need me to test anything on my end :)\n. Here is another smallish example that reproduces the same error:\n```\nimport pika\nfrom pika import exceptions\nfrom pika.adapters import twisted_connection\nfrom twisted.internet import defer, reactor, protocol\ndef wait(n=0):\n    \"\"\" A simple asynchronous sleep. \"\"\"\n    d = defer.Deferred()\n    reactor.callLater(n, d.callback, None)\n    return d\n@defer.inlineCallbacks\ndef run(connection):\n    # open a channel and declare a queue:\n    channel = yield connection.channel()\nprint 'publishing to a nonexistant exchange'\nyield channel.basic_publish(exchange=\"nonexisting_exchange\", body=\"my message\", routing_key='')\n\n# wait a second. waiting for shorter amounts makes the program end before the error situation happens\n# on my laptop, values as low as 0.001 (waiting 1 ms) reproduces the error\nyield wait(1)\n\ntry:\n    yield channel.basic_publish(exchange=\"hello there\", body=\"my message\", routing_key='')\nexcept exceptions.ChannelClosed as e:\n    print 'Channel %i closed: %s' % (channel.channel_number, e)\n    print 'Note the exception message refers to the previous exchange, not the current one'\n\nprint 'getting a new channel, which hangs'\nchannel2 = yield connection.channel()\n\nprint 'all done'\nreactor.stop()\n\nparameters = pika.ConnectionParameters()  \ncc = protocol.ClientCreator(reactor, twisted_connection.TwistedProtocolConnection, parameters)\nd = cc.connectTCP('localhost', 5672)\nd.addCallback(lambda protocol: protocol.ready)\nd.addCallback(run)\nreactor.run()\n```\nNote that the message of the exception I'm catching refers to the previous basic_publish call, and not the current one.\n. I just created a new issue for the exception messages actually belonging to previous calls to pika: https://github.com/pika/pika/issues/85\nOne important difference between the exception raised by that usage of the blocking api and the ones raised in this ticket is that in this issue \"ChannelClosed\" is raised, while in the other one \"AMQPChannelException\" is raised, and the connection may still be used\n. Thanks for a great and thorough explanation!\nReusing channel numbers when there has been no exception seems to work fine:\n```\nimport time\nfrom pika.adapters import BlockingConnection\nfrom pika.connection import ConnectionParameters\nfrom pika import BasicProperties, exceptions\nconnection = BlockingConnection(ConnectionParameters('localhost'))\nchannel = connection.channel()\nprint 'sending on 1st channel', channel.channel_number\nchannel.basic_publish(exchange='', body='my message first', routing_key='nonexistent_does_not_matter')\nchannel.close()\nI dont think this sleep is necessary\ntime.sleep(1)\nchannel2 = connection.channel()\nprint 'sending on 2nd channel:', channel2.channel_number\nchannel2.basic_publish(exchange='', body='my message first', routing_key='nonexistent_does_not_matter')\nchannel2.close()\nprint 'done'\n```\nWhich makes sense, otherwise Im forced to close the connection and reconnect to the server if I run out of channel numbers (due to exceptions on the channels, for example) during the lifetime of my application.\nAnother thing I'm still confused about is why the 2nd sample in this ticket(https://github.com/pika/pika/issues/84#issuecomment-2150490) and the sample in #85 results in different exceptions being thrown.\n. I just looked at the tracebacks from the exceptions being raised, and it seems to me that https://github.com/pika/pika/blob/master/pika/adapters/blocking_connection.py#L282 should be raising a ChannelClosed, and not a AMQPChannelError?\nSo the only outstanding question now is what the semantics are around reusing channel numbers, whether it should be possible (according to the spec and/or rabbitmq).\n. +1 on this pull request.\nTake this example use case: publishing to a nonexistent exchange (or even declaring a nonexistent exchange passively) raises an error on the channel and closes it. Without this patch, that channel number can never be re-used, which is a huge issue for long-running services, or applications that get the name of the exchange from some kind of user input. \n. ",
    "AaronVoelker": "As stated at the top, I am using Pika 0.9.5 (verified again by pika.version). Thanks.\n. Wondering if this was able to be reproduced. I was thinking of trying an alternative method of opening two separate async connections under the same IOLoop. Would this be a viable workaround? Thanks.\n. Update: This problem doesn't seem to occur when using a TornadoConnection instead of a BlockingConnection. I have been using this as a workaround for the issue, and have yet to run into any problems.\nEdit: Sorry about the repeated close/reopens. I am too used to forums where the \"Post\" button is the left button, not the right.\n. ",
    "khattori": "I also faced a similar problem.\nI found that the problem was caused by BlockingChannelTransport.send_method.\nThe method should be reentrant.\nI propose the following patch:\n```\ndiff adapters/blocking_connection.py.bak adapters/blocking_connection.py\n239d238\n<         self.wait = wait\n242c241\n<         while self.wait and not self._received_response:\n\n\n    while wait and not self._received_response:\n\n```\n. \n",
    "rdunklau": "Yes, it is.\nThis is only a rough 2to3 conversion, with modifications to address the unicode/bytes differences.\n. ",
    "skrat": "Is there anything in particular that I could help with? To speed up the transition? I'd like to use it as celery AMQP backend with py3k.\n. ",
    "sweenzor": "I've had good luck using multiprocessing for this. Which includes a thread-safe clone of Queue.Queue:\nhttp://docs.python.org/library/multiprocessing.html#exchanging-objects-between-processes\n. ",
    "mmb": "Thanks for the explanation and info.\n. ",
    "daf": "Yeah, the docs are wrong here, I think.  I get two params: code and text, which describe the reason for closing the channel.  A \"good\" close code is either 0 or 200.\n. Thinking further about this, I believe I may not have uncovered a real issue in pika, but a flaw in using gevent with pika (discussion in #102).  I think the CallbackManager will eventually clean things up, but as it stands now, control is yielded back to my code while the CallbackManager is in the middle of calling a list of callbacks.\nThis manifests in all sorts of problems, but is indicative of the types of issues with gevent.  I'll close this bug for now and continue in the gevent related issue.\n. We've been running a project on it for nearly a year now, using gevent's normal monkey patching mechanisms.  It mostly works ok, but we've built a layer on top of pika that has to go to lengths to make sure you don't do something we've noticed as bad.\nWe manually patch in some fixes for #97 (my bug) that might be related to gevent.\nI've noticed sometimes we get a:\n[snip]/eggs/pika-0.9.5-py2.7.egg/pika/callback.py:70: UserWarning: CallbackManager.add: Duplicate callback found for \"0:Connection.CloseOk\"\nWhich definetely appears related to the monkey patching - there's a race between channel closing (as called by the connection.close method) and the call at the end of connection.close which checks if there are any channels and if it should call the same method.  I think it's mostly harmless but is one of the alarms when using this with gevent.\n@steeve Can you elaborate more on your concurrency issues?\n. Ah, that reminds me.  I had to plug a fix in for a similar sounding issue - pika/gevent would sometimes try to ack in the middle of a publish (which is a surprising three AMQP commands) and Rabbit would panic close everything.\nHad to use Wireshark to figure out what was happening.  Wish Rabbit would at least tell you which channel caused the panic, that would've helped narrow it down.\n. This looks like it might address #97?\n. Argh, nevermind.  We're modifying the callbacks directly in our own code base in order to skirt around some gevent interaction issues.  I had forgotten this interaction.\n. ",
    "mcfletch": "Hrm, no, don't bother to pull, there are more failures needing to be addressed to make the reconnection strategy work properly...\n. ",
    "ericbarch": "I'm also seeing this same exact issue with SelectConnection. \npython\npika/callback.py:69: UserWarning: CallbackManager.add: Duplicate callback found for \"1:Channel.Close\"\n  (self.__class__.__name__, prefix, key))\nI've reached the same conclusion as daf, the callbacks are not being removed upon channel close.\n. ",
    "RJ": "I am setting the header myself using microseconds, which looks like it's against the spec. I saw it was a 64bit field and assumed a high precision timestamp would be fine.\nSo pika isn't at fault; I will close the issue.\nPerhaps the erlang client I'm using should complain if I use a value > 2^32 as a timestamp.\n. I have this issue too, can confirm that patch fixes it\n. ",
    "ceymard": "Okay nevermind, I was trying to send while some calls to queue_bind were not finished (I was not checking their callbacks).\nBut that said, maybe a more explicit exception than just terminating the program could have helped me understand what was wrong :) (sheer luck helped here)\n. I was using the SelectConnection with all of its asynchronous goodies.\nTechnically this is what I do everytime the client is launched :\n- 1. Open the connection\n- 2. Create the channel\n- 3. (re)Create a topic Exchange\n- 4. (re)Create several Queues that I know I'll use (there are 7 of them)\n- 5. Bind these queues to the topic with some routing keys\n- 6. Launch the rest of the code via a callback that I supplied at the beginning of the whole process\nThe rest of the code sends 10 random messages to my exchange.\nWhat happened before is that I launched the callback at first right after the operation 5, without waiting for their callbacks to complete (ie, 7 queue_bind were called but not checked for completion).\nWhen doing that, all the messages didn't arrive consistently to the exchange to my great surprise.\nI then started fiddling around and instead of calling basic_publish directly, I created a function that did and I added a timeout (via connection.add_timeout(1, callback)) to call it.\nBy doing that, all the messages arrived consistently, everytime.\nSo, I thought that something was wrong with my code at some point, that I was not waiting correctly.\nI then removed the add_timeout to basic_publish directly 10 times, but I added the timeout on my main function (ie. after all the initialization, still not waiting for 5. to complete). This also led to all messages getting there.\nThen, I realized I was not waiting for my callbacks, waited for them, called the rest of the code and voila, it worked.\nThere was a bizarre behaviour from the program : every time that it failed to send the messages in a way or another, the process would exit with a return code of 0. Every time it did work, it stayed in the main loop (that I never shut down explicitely).\nSo in the end, I don't know if queue_bind is \"blocking\" something in some way, but it's very probable that sending messages while it's still in the process of binding does alter it negatively in some way and leads to the unwanted shutdown or the selectconnection loop - without raising an exception. \n. I have narrowed it down to the fact that apparently at some point the Header puts some unicode in its pieces, and this triggers the conversion of the payload string to unicode with the aforementioned exception.\n. sorry about the long delay in replying, as I ugly-hot-fixed the issue to advance and have been fairly busy on the side.\nI'd be happy to paste the incriminating code. It is however for a client on a project with a NDA.\nI'll try to give an example ASAP\n. Hello,\nSorry for the long delay, but we at last tracked down the issue with a colleague.\nThe faulty code was ours : we constructed a basic property that way :\npython\n        channel.basic_publish(exchange=exchange_name, routing_key=self.routing_key, body=body,\n            immediate=True,\n            properties=BasicProperties(delivery_mode=self.delivery_mode,\n                                       reply_to=self.callback_queue,\n                                       correlation_id=unicode(self.last_correlation_id)),\n            **kwargs\n            )\nWhere correlation_id should have been str(...)\nThe fact that it was unicode triggered a conversion in unicode of all the pieces in Frame._marshal resulting in the problem I had.\nSo in the end, correlation_id MUST be a str, I just missed that. Maybe it should be coerced to str automatically like the body ?\n. ",
    "dsuch": "I can't help you out with it directly but would like to let you know that there's a ZeroMQ/gevent glue layer at https://github.com/traviscline/gevent-zeromq - who knows it, maybe it will serve as a source of inspiration.\nCheers!\n. Hmm.. as a matter of fact, could it be done for all the UserWarnings like here?\nTornado IOLoop may be running but Pika has shutdown.\nImagine several instances of an application running on different systems, all using a centralized logging mechanism, like syslog and friends. All the messages go  to the central logger except for the warnings which stay behind on each of the boxes. \nWouldn't it be nicer to drop the UserWarnings? What do you think Gavin?\n. OK, thanks Gavin! (sorry to be late)\n. ",
    "constfun": "Any news on this? Since pika is a pure python implementation. Would gevent.monkey do the job?\n. ",
    "steeve": "I've tried it with Celery, and on a Gevent reactor, there indeed seem to be concurency issues!\n. I'd have to test it again, but I had some framing errors if I recall correctly\n. ",
    "sergedomk": "Another developer and I wrote a gevent adapter for my current employer a while back. For the most part, it has been working flawlessly with no monkey patching. I will be updating it to work with the latest pika version. We've only seen one issue so far and it looks like it might be issue #97.\nAfter I make the update, I'll submit a pull request for adding the adapter to pika.\n. I would have replied earlier but needed to confirm that we could work around the issue. We ended up just writing our own adapter for gevent, which appears so far to be working well. We will submit it back to the pika community when we're sure we've got all the kinks out.\nThat said and knowing that you've since closed this issue, I'd like to bring up that I didn't make my original post on a whim. I assumed that there really was a bug based on three things:\n1) It was apparent that the callback code had been removed in September, but the commit made no statement about it or to any effect that it was intended.\n2) The pika documentation clearly states that you can add an on_close callback for the blocking connection adapter.\n3) You actually can add the callback, it just doesn't do anything.\nGiven that the current behavior is expected, you might want to prevent others from reporting this is a bug by fixing the documentation and explicitly deprecating the add_on_close_callback() method on the blocking connection.\n. ",
    "nanonyme": "Coercing str to unicode or the other way around is EVIL. It leads hard to debug encoding/debugging errors. (it assumes ASCII is used no matter what your unicode objects or bytestrings contain) Imo better to just raise an exception immediately if wrong string type is given.\n. ",
    "diegorusso": "Hello, any update on this?\n. Thanks a lot for the update. Anyway this will be integrated in 0.9.6 release, won't it?\n. ",
    "charleslaw": "I ran into this issue as well.\nSorry if my terminology isn't right in advance.\nPika sends a frames in multiple chunks (looks like 3 sets of data).  When sending out a frame, the pika code calls the poll() function (I added this call back into my local repo) to push the data to the socket.  While running the poll() pika is reading the socket.\nSo lets say 1 of the 3 sets of the data for frame A have been sent, and pika is running the poll() code as a message comes in.  Pika receives this message and calls your on_message received call back.  My callback, which I would bet is very similar to your's, processes the message and sends a reply using basic_publish().  This basic_publish() calls the poll() code again, and starts sending frame B.  Meanwhile, frame A is only partially sent, and this error comes up.\nI added an optional parameter to the poll function which lets it run write_only.  This way as you are forcing a message out, another message cannot be received, and you would never call a callback which might include a basic_publish.\nSee https://github.com/openx/pika.  Also, if you see any issues with this fix, please let me know.\n. Additional clarification:  There was a call to flush() which was commented out in https://github.com/pika/pika/commit/50d842526d9f12d32ad9f3c4910ef60b8c301f59 .  I added this back into my local version as my producer was publishing multiple messages inside a callback.  Without the flush, the only place the poll() was called was in the ioloop.  So the poll() was not reached until all my messages had been published (so the callback was complete) and the ioloop was on its next iteration.  This was causing lots of warnings about falling behind on publishing when I was actually writing very slowly.  Adding the flush/poll back in made it possible for pika to actually send the messages to the queue before finishing the loop inside the callback.\n. This can be closed now, the PR from the openx/pika repo fixed this issue\n. I am not sure how threading affects this, but you might want to checkout #173 I never linked it to this issue.. Duplicate PR\n. Sure, I'll break it up.\n. Actually that does exactly what I'm looking for.  Thanks!\n. ",
    "galindro": "@charleslaw / @gmr  could you please reopen this issue? The problem is occurring with python 3.6.4 + pika 0.11.2 + RabbitMQ 3.7.2. \nThis error is occurring in my code when I try to publish a message with a thread that is started from within my callback function. I'm using two different channels: one for the callback and other for the publish.. @charleslaw After read the oficial pika FAQ, I've noticed that pika isn't thread safe. However, I could use add_callback_threadsafe to avoid thread problems with BlockingConnection object. \nThe code that implements the aforementioned method was merged into master branch by @vitaly-krugl at Feb 15, 2018. So it isn't released yet... This is why it isn't present on 0.11.2, which is the latest released version.\n@vitaly-krugl is there any ETA to release a new pika version with this change?\n. Done @vitaly-krugl: https://groups.google.com/forum/#!topic/pika-python/1HpYYA7pWxs. ",
    "vitaly-krugl": "The code has changed a lot since #173 :) with more changes on the way as we head towards v1.0.0.  There are also some API changes as it will be harder or impossible to change API for a while after the v1.0.0 release.\nThe rabbitmq-users google group would be the appropriate forum for these questions and tends to be a very responsive group.\ncc @charleslaw @galindro . If you need to reconnect, I believe that the most robust way is to create a new adapter instance (BlockingConnection, SelectConnection, etc.) versus calling the old adapter instance's connect() method. Using connect() for reconnecting relies on internal member variables being properly re-initialized, which relies on some duplicate initialization logic that can easily get out of sync with that in the constructor.\nMy preference would be to get rid of the public connect() method altogether from all of the adapters, which would eliminate extra code paths to test and maintain.\n. lambda is another way, in addition to functools.partial, to pass extra args to your own callback. Finally, in OOP, the callback will often be an instance method, with the instance already having the necessary context without additional args.. @jonathanhogg, thank you for your feedback. Some functions take multiple callback arguments, so they can't all be named \"callback\" (e.g., the connection's constructor). There hasn't been any activity on this issue for several years, so I am going to close it. If you have additional suggestions, please open a new issue. Thank you.\n. @base698: see pika.connection.ConnectionParameters. The adapters take an optional pika.connection.ConnectionParameters instance as a constructor argument.\n. The new BlockingConnection implementation in master is based on SelectConnection, and this recursion should no longer happen with the updated API/implementation.\n. @gmr: let's close this issue. See my comment above.\n. This was fixed in 0.10.0\n. UPDATE Setting a longer heartbeat should be possible now. I just ran @wedaly's multiprocessing code sample on Mac OS X El Capitan and Python 2.7.10 using pika from the current master branch without any exception. It ran until I closed the connection from the RabbitMQ Management GUI interface.\n@Ralithune, are you also using the multiprocessing pattern as in @wedaly's example? Are multiple threads involved? Which platform, python version, and pika version? \nTake a look at DEBUG-level logs from pika. Pika logs a lot of info at DEBUG level, and you can usually figure out what's going on. Also, Wireshark is a great (and free) protocol analyzer that has AMQP support. Also, take a look at Connection code beginning with Connection.connect() to get a better understanding of the connection flow.\nIn the present (and former) SelectConnection implementation, IncompatibleProtocolError  means that the socket connection was disconnected somewhere after opening of the socket and before Connection.Start method from the server. At the moment, socket connection is established in the scope of the SelectConnection's constructor.\n. @raissanucci, you're probably overwhelming something - either your system's capacity to deal with some many processes or file descriptors or RabbitMQ's file descriptor capacity. You should also take a look at RabbitMQ server logs. Default RabbitMQ is preconfigured for a very small number of file descriptors. Also, your system might be configured for a small number of total file descriptors. \nAlso, make sure that you're not reusing the same connection on multiple processes.\nThose are the first places I would look.\nSee also the last paragraph in https://github.com/pika/pika/issues/317#issuecomment-192587913.\nFinally, you probably don't want to be commenting in a closed issue. Your scenario with 900 processes is different from the issue reported here.  It's better to open a new one.\n. @eandersson, you asked the perfect question. This is user error: the code provided by @thomdixon at https://github.com/thomdixon/pika_issue_349 corrupts the data stream by attempting to use the same socket connection from two processes. When you create a multiprocessing.Process, you get a fork of the parent process that clones the parent's memory and file descriptors, including pika.Connection's socket file descriptor. As the result, the two processes (parent and forked) end up sharing the exact same socket connection. Subsequently, publishing from one process and creating a consumer from another process results in corruption of the data stream on the single shared TCP/IP connection.\nIn addition, pika.Connection and Channel are not thread-safe either, and using them with multiple threads will lead to race conditions, unless you very carefully guard every call that you make into pika with mutexes, being careful to avoid deadlocks. Not recommended.\nIf you have a threaded or multiprocessing environment, I recommend creating an individual pika Connection for each thread or process. Don't share.\nPika maintainer @gmr's https://github.com/gmr/rabbitpy package appears to allow a thread per channel in a connection shared between multiple channels. I haven't tried it myself yet, but that is one possibility for multi-threading (but sharing a connection among processes would still lead to data stream corruption)\n. @thomdixon, thanks for updating the test. I will try it a bit later on my system.\n. @thomdixon, I tested your updated single-process https://github.com/thomdixon/pika_issue_349 on my MacBookPro with pika 0.9.14 and was able to reproduce the failure. However, when I installed pika from master, I was no longer able to reproduce the issue.\nThis issue was most likely already addressed by PR #509 in response to issue #507. #507 appears to be a duplicate of this issue.\n. @thomdixon, I would like to see many of those PRs validated, merged, and released soon, as our code is impacted by a number of those stability issues in 0.9.14.\n. This should be addressed in the released Pika 0.10.0. Thank you.\n. Addressed in the new BlockingConnection implementation in master.\n. @julio-vaz, this should be addressed as of pika 0.10.0.\n@gmr: let's close this issue. Thx.\n. Presently passing tests on Python 3.3, 3.4, and 3.5.\n. Addressed in @gmr's recent commit to pika master. Ok to close this.\n. The tornado adapter now provides an additional parameter in the constructor called stop_ioloop_on_close that defaults to False. This should address the issue.\n. This is most likely fixed by PR #537 \n. Still a problem with the latest version and pika master?\n. Closing due to inactivity. Please open a new issue if this is still a problem the the latest release.\n. Many error-handling improvements were made in release 0.10.0 and additional ones in pika master. If this is still an issue with the latest pika master, please open a new issue with all the necessary info (how to reproduce, platform, python version, pika version/commit, traceback, DEBUG-level logs, etc.)\n. Thank for reporting. BlockingConnection was rewritten for the 0.10.0 release. I am not aware of this issue following the rewrite. Please open a new issue if this still occurs with Pika 0.10.0.\n. This is most likely related to the implementation of your AMQP broker. AMQP closes the channel when it needs to return an error from a channel operation. If you're using RabbitMQ, it's handling of queue.delete became idempotent sometime around version 3.3.x. So, if your RabbitMQ broker was older, it would close the channel if your tried to delete a non-existent queue. A newer RabbitMQ broker would treat this as success.\n. Closing due to inactivity. Please see my feedback in an earlier comment.\n. Thank you for reporting. It's no longer an issue with the latest pika 0.10.0.\nIn [6]: try:\n        raise ConnectionClosed()\nexcept:\n        print('%s' % (sys.exc_info(),))\n   ...:     \n(<class 'pika.exceptions.ConnectionClosed'>, The AMQP connection was closed: (), <traceback object at 0x10480b3b0>)\n. @unthingable: I have always thought of SelectConnection's poller as an internal component of SelectConnection, and not a general-purpose I/O loop for my program.\n. Many changes/fixes were made to SelectConnection's ioloop code, communications, and error-handling, and connectivity in general, in the 0.10.0 release and subsequently more in Pika master. Also, see the new SelectConnection example in this PR #710. Please post your results with pika 0.10.0 and latest pika master. Thank you.\n. I would have expected socket timeout to kick in. This is most likely now fixed in pika master via PR #533.\nThis is a feature of TCP/IP. The default TCP keepalive timeout is typically measured in hours. When the server shuts down gracefully, it makes an attempt to reset open TCP/IP streams, so the remote peers normally recover quickly. However, if a server goes down hard (e.g., kernel panic, hardware failure, pulling the network cable or the power cable), then no one resets the connections, leaving the peers stranded, especially if the peer happens to be reading from the socket at that time. The peer's TCP/IP stack won't discover the disconnect in this scenario until it eventually exercises its keepalive (if supported/enabled) and never hears back, at which point it would close the stream locally.\nI recently experienced this with the requests package (using default infinite timeout), where a bunch of processes became deadlocked while reading data from a server that malfunctioned. As an experiment, I left them alone and they remained deadlocked for several days until I went in with gdb and forced their socket FDs to close.\nSee https://delog.wordpress.com/2013/08/16/handling-tcp-keepalive/ \n. BlockingConnection was rewritten in the 0.10.0 release. In the new version, Heartbeat timeout should cause the call to unblock. Please try to reproduce with 0.10.0 and open a new issue if it exhibits a problem in this scenario.\n. @gmr, I didn't find multi-threaded usage examples on https://github.com/gmr/rabbitpy, so I can't properly asses its suitability.\nThank you,\nVitaly \n. @gmr, this is fixed in master via e7b6d73aa2a38b7e2d4ac57e111bec9f9d518e08\n. Thank you. This is fixed in pika 0.10.0:\n```\nIn [5]: %paste\nimport pika\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nparameters = pika.ConnectionParameters('localhost')\nconnection = pika.BlockingConnection(parameters)\nchannel = connection.channel()\nDeclare queues for the consumer\nchannel.queue_declare(queue='q1')\nchannel.queue_declare(queue='q2')\nPublish messages\nfor i in range(3):\n    message = 'message ' + str(i)\n    channel.basic_publish(exchange='',\n                          routing_key='q1',\n                          body='q1: ' + message)\n    channel.basic_publish(exchange='',\n                          routing_key='q2',\n                          body='q2: ' + message)\n    print 'Published', message, 'with routing keys q1 and q2'\nconnection.close()\n-- End pasted text --\nPublished message 0 with routing keys q1 and q2\nPublished message 1 with routing keys q1 and q2\nPublished message 2 with routing keys q1 and q2\nIn [6]: %paste\nimport pika\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\ndef print_data(channel, method, properties, body):\n    print 'body:', body\n    channel.basic_ack(delivery_tag = method.delivery_tag)\nparameters = pika.ConnectionParameters('localhost')\nconnection = pika.BlockingConnection(parameters)\nchannel = connection.channel()\nchannel.queue_declare(queue='q1')\nchannel.queue_declare(queue='q2')\nchannel.basic_consume(print_data, queue='q1')\nchannel.basic_consume(print_data, queue='q2')\nchannel.start_consuming()\nconnection.close()\n-- End pasted text --\nbody: q1: message 0\nbody: q1: message 1\nbody: q1: message 2\nbody: q2: message 0\nbody: q2: message 1\nbody: q2: message 2\n```\n. Fixed via PR #533\n. Ok, with PRs #533  and #542 merged, the situation is a lot better, but not perfect yet.\nUsing the simple steps outlines at the top of this issue's description, we now correctly get ConnectionClosed: (320, \"CONNECTION_FORCED - broker forced connection closure with reason 'shutdown'\") after calling channel.queue_declare(...). At this point connection.is_open now correctly evaluates to False; however, channel.is_open (arguably) incorrectly still evaluates to True.\n@gmr, referring to the above, should it be a goal that if the connection is forced closed (either gracefully or due to loss of connection), then its channels should also be forced into a channel.is_closed = True state. As I describe in the next two paragraphs, this may result in data/file corruption via an accidental write to a reallocated file descriptor.\nSo, after the initial ConnectionClosed exception, if one were to attempt another operation on that channel - e.g., channel.queue_declare(queue=\"abc\", passive=True, nowait=False) - then the original issue error: (9, 'Bad file descriptor') resurfaces as depicted by the traceback below. From the debugging point of view, I would have preferred to get either a ConnectionClosed or a `ChannelClosed`` exception instead of error: (9, 'Bad file descriptor').\nTo make matters much worse, the app might have opened a file for writing between the ConnectionClosed exception and the repeated attempt to operate on the channel, so the file would now have the same file descriptor as the just-closed socket. And the accidental attempt to operate on the channel following ConnectionClosed will result in data corruption of the file. E.g., this could be a multi-threaded app, where one thread deals with AMQP and another thread deals with writes to files.\n```\nchannel.queue_declare(queue=\"abc\", passive=True, nowait=False)\n\nerror                                     Traceback (most recent call last)\n in ()\n----> 1 channel.queue_declare(queue=\"abc\", passive=True, nowait=False)\n/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py in queue_declare(self, queue, passive, durable, exclusive, auto_delete, nowait, arguments)\n    932         return self._rpc(spec.Queue.Declare(0, queue, passive, durable,\n    933                                             exclusive, auto_delete, nowait,\n--> 934                                             arguments or dict()), None, replies)\n    935 \n    936     def queue_delete(self,\n/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py in _rpc(self, method_frame, callback, acceptable_replies, content, force_data_events)\n   1170             replies.append(key)\n   1171         self._send_method(method_frame, content,\n-> 1172                           self._wait_on_response(method_frame))\n   1173         if force_data_events and self._force_data_events_override is not False:\n   1174             self.connection.process_data_events()\n/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py in _send_method(self, method_frame, content, wait)\n   1191         while wait and not self._received_response:\n   1192             try:\n-> 1193                 self.connection.process_data_events()\n   1194             except exceptions.ConnectionClosed:\n   1195                 # No further I/O is possible, so propagate exception\n/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py in process_data_events(self)\n    240         \"\"\"\n    241         try:\n--> 242             if self._handle_read():\n    243                 self._socket_timeouts = 0\n    244         except socket.timeout:\n/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py in _handle_read(self)\n    355 \n    356         \"\"\"\n--> 357         if self._read_poller.ready():\n    358             super(BlockingConnection, self)._handle_read()\n    359             self._frames_written_without_read = 0\n/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py in inner(*args, kwargs)\n     42         while True:\n     43             try:\n---> 44                 return f(*args, kwargs)\n     45             except select.error as e:\n     46                 if e[0] != errno.EINTR:\n/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py in ready(self)\n     89         else:\n     90             ready, unused_wri, unused_err = select_function([self.fd], [], [],\n---> 91                                                             self.poll_timeout)\n     92             return bool(ready)\n     93 \nerror: (9, 'Bad file descriptor')\n``\n. Should be okay to close once PR #567 is merged. PR #567 resetsBlockingConnection._read_poller, making it impossible to access the closed file descriptor\n. Fixed via #533, #542, and #567 \n. @modeyang, a pika connection runs entirely in the user's thread. So, if you don't call it's API periodically, you're not letting it deal with events, including heartbeats. Blocking connection hassleep()andprocess_data_events()` methods that may be called periodically to keep the I/O going.\n. @modeyang, the trick here is acknowledging the message before the heartbeat expires, right? Have you tried configuring heartbeat for a much longer interval - long enough to process your longest task? \nAlso, a multi-threaded solution could work, as long as you're not accessing the connection/channel from multiple threads; here is an example of how it could work:\nConsumer thread:\n1. Configure everything as you do now, but instead of start_consuming(), have your own loop that periodically calls connection.process_data_events(), while between calls waiting on an python thread-safe Queue.Queue (with a small timeout) for delivery-tags of completed tasks from  your worker thread. When it gets  deliver-tag, it ACKs it and continues with the process_data_events() loop as before until the next delivery-tag. \n2. The consumer callback passes the task to a thread (either starts a new thread for each task, or uses a python thread-safe Queue.Queue to pass tasks to a long-running thread.\nWorker thread:\n1. gets the message from the consumer thread as described above\n2. processes the long-running task and place's the task message's delivery-tag (it's in the message's method frame) into the python thread-safe Queue.Queue that the consumer waits on, as described under \"Consumer thread\" above.\n. In addition to my earlier comment, it's now possible to set longer heartbeat timeouts via PR #666, which is now in Pika master (but not released yet). Please try out the version from Pika's master branch, including setting a longer heartbeat timeout, and open a new issue if you're still having trouble with it.\n. See comment https://github.com/pika/pika/issues/892#issuecomment-356783950. add_callback_threadsafe in pull request #956 might help with this. See this example. Closing: cannot reproduce with 24c3f1d6bdab3d609721dd721b9ac87771906789\n. Actually, it's still a problem in the latest pika master:\nin ConsumerCancelled. __repr__(), self.args[0] is a Basic.Cancel instance, which has no reply_code attribute, so  ConsumerCancelled. __repr__() blows up when it tries to access it: AttributeError: 'Cancel' object has no attribute 'reply_code'\nCANCEL EXC ARGS: (,) DIR args[0]: ['INDEX', 'NAME', 'class', 'delattr', 'dict', 'doc', 'format', 'getattribute', 'hash', 'init', 'module', 'new', 'reduce', 'reduce_ex', 'repr', 'setattr', 'sizeof', 'str', 'subclasshook', 'weakref', '_set_content', 'consumer_tag', 'decode', 'encode', 'get_body', 'get_properties', 'nowait', 'synchronous']\n. @purpleP, I see that there are threads involved in your example. Pika connections are not thread-safe, so you may not access a connection or any of its channels from multiple threads.\n. In released versions of pika, connections are not thread-safe. See https://github.com/pika/pika/blob/master/docs/faq.rst.\n. This specific portion of #578 had to be undone, because it broke other code paths. see #679.\n. A CPU-bound application in the Tornado framework could be a bit of an oxymoron :) If this is still an issue, would you be able to suggest a solution in the form of a pull request? Closing due to inactivity. Thank you.\n. @vak and @pfreixes: BlockingConnection has been completely re-implemented recently. Please re-test.\n. Closing due to inactivity. Please open a new issue if this is still a problem with the latest pika release (0.10.0 at the time of this writing)\n. This should be fixed in master branch now. Please re-test against master.\n. Closing due to inactivity. This should be fixed in the current release 0.10.0.\n. @hupantingxue, please provide enough information for someone to easily reproduce the issue. Also, please attach debug-level logs of the session. Finally, please try to reproduce it with the latest release pika version 0.10 and report your findings in a comment here.\n. @kherrala, I will need to wrap my brain around SSL once again, before I can answer your question. Another area that pika probably doesn't support is ssl renegotiation, as pointed out in #703.\nhttps://github.com/openwebos/libpalmsocket may be a good algorithmic reference for re-handshakes, etc. I rewrote libpalmsocket a while back, adding support for SSL renegotiation, certificates, and workarounds for a number of OpenSSL bugs.\n. @gmr, would enabling the user to pass a callback to perform whatever certificate validation the user code deems necessary eliminate the issue of introducing opinionated code in pika core? The callback would be called upon completion of ssl handshake, and would have access to SSLContext.\nFrom my past work on what is now https://github.com/openwebos/libpalmsocket, I recall apps needing a feature of this kind, as the type of validation sometimes differed from app to app. I also recall apps needing to save and reuse ssl contexts in order to speed-up SSL connection establishment a lot.\n. @gmr, I would like to reopen this issue. I think that pika needs to provide a mechanism to enable users to configure server_hostname + context.check_hostname (server_hostname could be a switch for context.check_hostname) as well as deal with additional critical security configuration as in the excerpt from ssl.create_default_context below. My rationale follows:\nI took a look at the builtin ssl.py, and see that it supplies a convenience public API function for creating contexts that represent recommended practices: \n- ssl.create_default_context(...): doc: Create a SSLContext object with default settings. NOTE: The protocol and settings may change anytime without prior deprecation. The values represent a fair balance between maximum compatibility and security.\nssl.create_default_context enables context.check_hostname = True, which will cause ssl.match_hostname to be called upon completion of the handshake in SSLSocket.do_handshake (on python 2.7, too). However, if context.check_hostname is set, then SSLSocket.do_handshake expects server_hostname to have been provided by the user, otherwise do_handshake raises ValueError. \nIf pika were to support check_hostname, then it would need to provide a mechanism for passing and applying server_hostname, since ssl.wrap_socket doesn't take it as an arg (at least not on python 2.7.10) \nThere are additional security measures that ssl.create_default_context enables that I recall from my prior experience were critical for avoiding known vulnerabilities:\n```\n    # SSLv2 considered harmful.\n    context.options |= OP_NO_SSLv2\n# SSLv3 has problematic security and is only required for really old\n# clients such as IE6 on Windows XP\ncontext.options |= OP_NO_SSLv3\n\n# disable compression to prevent CRIME attacks (OpenSSL 1.0+)\ncontext.options |= getattr(_ssl, \"OP_NO_COMPRESSION\", 0)\n\n```\n. Hi @gmr, I reopened this issue to keep it on our radar per rationale in https://github.com/pika/pika/issues/464#issuecomment-212016002.\nI think there are changes that we can make, which would not be opinionated.\n. Also, see https://www.rabbitmq.com/ssl.html\n\nAs of RabbitMQ 3.4.0, SSLv3 is disabled automatically to prevent the POODLE attack\n. Note that pika is now python 2.7 + and python 2.7.9 has ssl.match_hostname(), so this wouldn't need https://pypi.python.org/pypi/backports.ssl_match_hostname. cc @tiran \n\nI am closing this issue because the code in pika master now allows the application to control all the SSL features called out in the description of this issue.\nIn pika master targeting pika v1.0.0 release, the connection parameters support passing SSLContext and server_hostname via SSLOptions. This enables the app to control all the SSL features called out in the description of this issue. In particular:\n\nThe user can set context.verify_mode = ssl.CERT_REQUIRED; and\nThe user can set context.check_hostname = True\nThis causes SSLSocket.do_handshake() to call ssl.match_hostname(), which is supported since python 2.7.9.. I opened issue #1035 to make sure we have adequate test coverage for these scenarios.. @leth, that's the nature of BlockingConnection. If you're not executing code in BlockingConnection, then it's not processing input either. If you're using RabbitMQ that supports Publisher Acknowledgements, then you can enable it on a given channel via channel.confirm_delivery(), and you should then get the \"exchange not found\" error before basic_publish returns.\n. Works as intended. Let's close.\n. Please see my earlier comments. This feature works as intended in BlockingConnection. SelectConnection and other asynchronous adapters can be more responsive if used appropriately in an I/O-bound applications.\n. @joeclarkia, the issue that you were experiencing is typical of the single-threaded implementations. You might be better off with multiple channels over a single connection.\n\nrabbitpy may be another option as it uses an extra thread per connection to keep things alive.\n. Please see my earlier reply. Closing due to inactivity.\n. @niteeshm, please enable pika DEBUG-level logging, reproduce this failure, and attach the full pika log to this issue.\n. Pika 0.9.x is quite old. In particular, 0.10.0 had a major rewrite of the BlockingConnection.\n. @bra-fsn and @xintron, this issue should also be resolved natively in SelectConnection once PR #549 is resolved and merged.\n. @shiroyuki, regarding\n\nmulti-thread application tries to use one or many blocking channel in any threads,\n\nIs your application accessing a channel or channels belonging to the same connection from multiple threads? Can you add a simple code snippet to reproduce this failure?\n. The issue at hand, http://bugs.python.org/issue8865, occurs only during sharing of the \"same poll object across multiple threads\".\nExcerpt from http://bugs.python.org/issue8865\n\nit strikes me that this should not be a very common problem.  how many applications are going to share the same poll object across multiple threads?\n\nissue8865 also mentions memory corruption when same poll object is shared across threads.\nThis implies that this application is attempting to share the same connection across multiple threads, which is not supported in pika.\nThis PR needs to be rejected since pika doesn't support concurrent multi-threaded use of a single connection.\nCC @gmr \n. Using select() here explicitly will interfere with the native framework's (e.g., Tornado) concurrency model. ***But the major issue with forcing the use of select() here is that it has serious limitations with respect to max socket file descriptor number; so, this would cause failures (or require changes) in programs that create many thousands of sockets and other file descriptor-based objects. poll/epoll/etc. don't have that problem.\n. @kherrala, that's a good point about dependency on garbage-collection. I wasn't aware of this socket subtlety in python. Thanks!\n. @adityagupta104, blocking connection in master has been rewritten from scratch as a wrapper around SelectConnection. \n. Busy polling should be fixed in the current Pika release 0.10.0.\n. :thumbsup:, please\n. As of c7a72cdd4f5f7e1d692bf42943c9f3810f6bb1b7, it's partially-fixed: connection.is_open now correctly reports False, but channel.is_open still reports True\n. Possibly fixed by PR #685 or before it. Need to implement an acceptance test that reproduces the scenario by severing the connection and verifies that both connection.is_open and channel.is_open return False (and similarly foris_closed`).\n. @lukebakken ...\nThis was happening in proprietary code of my former employer. If I recall correctly, the code would perform some blocking operation that on occasion caused the RabbitMQ server's heartbeat timeout logic to drop the connection. Here are two test cases that I can think of:\n\n\nThe natural way (this would simulate what happened in client code when I reported this issue)\n1.1 Using BlockingConnection, create a connection with a non-zero heartbeat and channel\n1.2 time.sleep long enough to cause RabbitMQ server to drop the connection\n1.3 Attempt to publish a message on that channel created earlier and verify that pika ConnectionClosed exception was raised.\n1.4 Verify that connection.is_open and channel.is_open are both False and connection.is_closed and channel.is_closed are both True\n\n\nThe sneaky way (not 100% sure if it would have the same effect as the \"natural way\", but I think it should and should be much faster for test run-time if it works as expected):\n2.1 Using BlockingConnection, create a connection and channel\n2.2 Access the connection's private socket and use socket.shutdown method to shut it down in both directions (but don't close the socket).\n2.3  Attempt to publish a message on that channel created earlier and verify that pika ConnectionClosed exception was raised.\n2.4 Verify that connection.is_open and channel.is_open are both False and connection.is_closed and channel.is_closed are both True. Thanks @lukebakken. It would be awesome to have a test, so that if this ever breaks again we would catch it immediately. #2 could be handy for it :).. @wtarr, please describe how you cause the connection to terminate. This would be useful to help us understand whether the exception you're experiencing is a side-effect of another issue that precedes it. Also, please include the complete traceback. Thx.\n. @wtarr, thanks, that helps a lot. Ooops, sorry my reference to PR #542 was incorrect since your problem is with SelectConnection, not BlockingConnection.\n. @wtarr, could you please provide the entire pika logs from your failing scenario?\n. Could a dev with access to Windows try to debug this issue and report on the true root cause here? The complete script is below. According to the posts above, here are the steps to reproduce the AttributeError: 'NoneType' object has no attribute 'fileno' issue:\n\nUse pika sources from the official 0.9.14 release\nMake sure rabbitmq is running. \nThe script below assumes rabbitmq is running on the same localhost as the test script and that it has the default credentials. If this differs from your setup, then update the url that is passed to the ExampleConsumer constructor in main() as needed for your configuration.\nStart the script and give it a few seconds to initialize\nStop rabbitmq; here, it would be good to try two ways of stopping it, one way for each test cycle.\n   4.1. Gracefully, via rabbitmqctl stop (or as described by @wtarr). The graceful method will cause Channel-Close and Connection-Close requests to be sent by rabbitmq to the client\n   4.2. Abruptly, by killing the rabbitmq process. This will cause the socket to be closed without any AMQP protocol niceties.\nWait for the script to crash as described by @wtarr above.\n\nOnce you're able to reproduce this issue (AttributeError: 'NoneType' object has no attribute 'fileno'), please root-cause the problem and post your findings in a comment in this issue.\n```\nimport logging\nimport time\nimport pika\nLOG_FORMAT = ('%(levelname) -10s %(asctime)s %(name) -30s %(funcName) '\n              '-35s %(lineno) -5d: %(message)s')\nLOGGER = logging.getLogger(name)\nclass ExampleConsumer(object):\n    \"\"\"This is an example consumer that will handle unexpected interactions\n    with RabbitMQ such as channel and connection closures.\nIf RabbitMQ closes the connection, it will reopen it. You should\nlook at the output, as there are limited reasons why the connection may\nbe closed, which usually are tied to permission related issues or\nsocket timeouts.\n\nIf the channel is closed, it will indicate a problem with one of the\ncommands that were issued and that should surface in the output as well.\n\n\"\"\"\nEXCHANGE = 'message'\nEXCHANGE_TYPE = 'topic'\nQUEUE = 'text'\nROUTING_KEY = 'example.text'\n\ndef __init__(self, amqp_url):\n    \"\"\"Create a new instance of the consumer class, passing in the AMQP\n    URL used to connect to RabbitMQ.\n\n    :param str amqp_url: The AMQP url to connect with\n\n    \"\"\"\n    self._connection = None\n    self._channel = None\n    self._closing = False\n    self._consumer_tag = None\n    self._url = amqp_url\n\ndef connect(self):\n    \"\"\"This method connects to RabbitMQ, returning the connection handle.\n    When the connection is established, the on_connection_open method\n    will be invoked by pika.\n\n    :rtype: pika.SelectConnection\n\n    \"\"\"\n    LOGGER.info('Connecting to %s', self._url)\n    return pika.SelectConnection(pika.URLParameters(self._url),\n                                 self.on_connection_open,\n                                 stop_ioloop_on_close=False)\n\ndef close_connection(self):\n    \"\"\"This method closes the connection to RabbitMQ.\"\"\"\n    LOGGER.info('Closing connection')\n    self._connection.close()\n\ndef add_on_connection_close_callback(self):\n    \"\"\"This method adds an on close callback that will be invoked by pika\n    when RabbitMQ closes the connection to the publisher unexpectedly.\n\n    \"\"\"\n    LOGGER.info('Adding connection close callback')\n    self._connection.add_on_close_callback(self.on_connection_closed)\n\ndef on_connection_closed(self, connection, reply_code, reply_text):\n    \"\"\"This method is invoked by pika when the connection to RabbitMQ is\n    closed unexpectedly. Since it is unexpected, we will reconnect to\n    RabbitMQ if it disconnects.\n\n    :param pika.connection.Connection connection: The closed connection obj\n    :param int reply_code: The server provided reply_code if given\n    :param str reply_text: The server provided reply_text if given\n\n    \"\"\"\n    self._channel = None\n    if self._closing:\n        self._connection.ioloop.stop()\n    else:\n        LOGGER.warning('Connection closed, reopening in 5 seconds: (%s) %s',\n                       reply_code, reply_text)\n        self._connection.add_timeout(5, self.reconnect)\n\ndef on_connection_open(self, unused_connection):\n    \"\"\"This method is called by pika once the connection to RabbitMQ has\n    been established. It passes the handle to the connection object in\n    case we need it, but in this case, we'll just mark it unused.\n\n    :type unused_connection: pika.SelectConnection\n\n    \"\"\"\n    LOGGER.info('Connection opened')\n    self.add_on_connection_close_callback()\n    self.open_channel()\n\ndef reconnect(self):\n    \"\"\"Will be invoked by the IOLoop timer if the connection is\n    closed. See the on_connection_closed method.\n\n    \"\"\"\n    # This is the old connection IOLoop instance, stop its ioloop\n    self._connection.ioloop.stop()\n\n    if not self._closing:\n\n        # Create a new connection\n        self._connection = self.connect()\n\n        # There is now a new connection, needs a new ioloop to run\n        self._connection.ioloop.start()\n\ndef add_on_channel_close_callback(self):\n    \"\"\"This method tells pika to call the on_channel_closed method if\n    RabbitMQ unexpectedly closes the channel.\n\n    \"\"\"\n    LOGGER.info('Adding channel close callback')\n    self._channel.add_on_close_callback(self.on_channel_closed)\n\ndef on_channel_closed(self, channel, reply_code, reply_text):\n    \"\"\"Invoked by pika when RabbitMQ unexpectedly closes the channel.\n    Channels are usually closed if you attempt to do something that\n    violates the protocol, such as re-declare an exchange or queue with\n    different parameters. In this case, we'll close the connection\n    to shutdown the object.\n\n    :param pika.channel.Channel: The closed channel\n    :param int reply_code: The numeric reason the channel was closed\n    :param str reply_text: The text reason the channel was closed\n\n    \"\"\"\n    LOGGER.warning('Channel %i was closed: (%s) %s',\n                   channel, reply_code, reply_text)\n    self._connection.close()\n\ndef on_channel_open(self, channel):\n    \"\"\"This method is invoked by pika when the channel has been opened.\n    The channel object is passed in so we can make use of it.\n\n    Since the channel is now open, we'll declare the exchange to use.\n\n    :param pika.channel.Channel channel: The channel object\n\n    \"\"\"\n    LOGGER.info('Channel opened')\n    self._channel = channel\n    self.add_on_channel_close_callback()\n    self.setup_exchange(self.EXCHANGE)\n\ndef setup_exchange(self, exchange_name):\n    \"\"\"Setup the exchange on RabbitMQ by invoking the Exchange.Declare RPC\n    command. When it is complete, the on_exchange_declareok method will\n    be invoked by pika.\n\n    :param str|unicode exchange_name: The name of the exchange to declare\n\n    \"\"\"\n    LOGGER.info('Declaring exchange %s', exchange_name)\n    self._channel.exchange_declare(self.on_exchange_declareok,\n                                   exchange_name,\n                                   self.EXCHANGE_TYPE)\n\ndef on_exchange_declareok(self, unused_frame):\n    \"\"\"Invoked by pika when RabbitMQ has finished the Exchange.Declare RPC\n    command.\n\n    :param pika.Frame.Method unused_frame: Exchange.DeclareOk response frame\n\n    \"\"\"\n    LOGGER.info('Exchange declared')\n    self.setup_queue(self.QUEUE)\n\ndef setup_queue(self, queue_name):\n    \"\"\"Setup the queue on RabbitMQ by invoking the Queue.Declare RPC\n    command. When it is complete, the on_queue_declareok method will\n    be invoked by pika.\n\n    :param str|unicode queue_name: The name of the queue to declare.\n\n    \"\"\"\n    LOGGER.info('Declaring queue %s', queue_name)\n    self._channel.queue_declare(self.on_queue_declareok, queue_name)\n\ndef on_queue_declareok(self, method_frame):\n    \"\"\"Method invoked by pika when the Queue.Declare RPC call made in\n    setup_queue has completed. In this method we will bind the queue\n    and exchange together with the routing key by issuing the Queue.Bind\n    RPC command. When this command is complete, the on_bindok method will\n    be invoked by pika.\n\n    :param pika.frame.Method method_frame: The Queue.DeclareOk frame\n\n    \"\"\"\n    LOGGER.info('Binding %s to %s with %s',\n                self.EXCHANGE, self.QUEUE, self.ROUTING_KEY)\n    self._channel.queue_bind(self.on_bindok, self.QUEUE,\n                             self.EXCHANGE, self.ROUTING_KEY)\n\ndef add_on_cancel_callback(self):\n    \"\"\"Add a callback that will be invoked if RabbitMQ cancels the consumer\n    for some reason. If RabbitMQ does cancel the consumer,\n    on_consumer_cancelled will be invoked by pika.\n\n    \"\"\"\n    LOGGER.info('Adding consumer cancellation callback')\n    self._channel.add_on_cancel_callback(self.on_consumer_cancelled)\n\ndef on_consumer_cancelled(self, method_frame):\n    \"\"\"Invoked by pika when RabbitMQ sends a Basic.Cancel for a consumer\n    receiving messages.\n\n    :param pika.frame.Method method_frame: The Basic.Cancel frame\n\n    \"\"\"\n    LOGGER.info('Consumer was cancelled remotely, shutting down: %r',\n                method_frame)\n    if self._channel:\n        self._channel.close()\n\ndef acknowledge_message(self, delivery_tag):\n    \"\"\"Acknowledge the message delivery from RabbitMQ by sending a\n    Basic.Ack RPC method for the delivery tag.\n\n    :param int delivery_tag: The delivery tag from the Basic.Deliver frame\n\n    \"\"\"\n    LOGGER.info('Acknowledging message %s', delivery_tag)\n    self._channel.basic_ack(delivery_tag)\n\ndef on_message(self, unused_channel, basic_deliver, properties, body):\n    \"\"\"Invoked by pika when a message is delivered from RabbitMQ. The\n    channel is passed for your convenience. The basic_deliver object that\n    is passed in carries the exchange, routing key, delivery tag and\n    a redelivered flag for the message. The properties passed in is an\n    instance of BasicProperties with the message properties and the body\n    is the message that was sent.\n\n    :param pika.channel.Channel unused_channel: The channel object\n    :param pika.Spec.Basic.Deliver: basic_deliver method\n    :param pika.Spec.BasicProperties: properties\n    :param str|unicode body: The message body\n\n    \"\"\"\n    LOGGER.info('Received message # %s from %s: %s',\n                basic_deliver.delivery_tag, properties.app_id, body)\n    self.acknowledge_message(basic_deliver.delivery_tag)\n\ndef on_cancelok(self, unused_frame):\n    \"\"\"This method is invoked by pika when RabbitMQ acknowledges the\n    cancellation of a consumer. At this point we will close the channel.\n    This will invoke the on_channel_closed method once the channel has been\n    closed, which will in-turn close the connection.\n\n    :param pika.frame.Method unused_frame: The Basic.CancelOk frame\n\n    \"\"\"\n    LOGGER.info('RabbitMQ acknowledged the cancellation of the consumer')\n    self.close_channel()\n\ndef stop_consuming(self):\n    \"\"\"Tell RabbitMQ that you would like to stop consuming by sending the\n    Basic.Cancel RPC command.\n\n    \"\"\"\n    if self._channel:\n        LOGGER.info('Sending a Basic.Cancel RPC command to RabbitMQ')\n        self._channel.basic_cancel(self.on_cancelok, self._consumer_tag)\n\ndef start_consuming(self):\n    \"\"\"This method sets up the consumer by first calling\n    add_on_cancel_callback so that the object is notified if RabbitMQ\n    cancels the consumer. It then issues the Basic.Consume RPC command\n    which returns the consumer tag that is used to uniquely identify the\n    consumer with RabbitMQ. We keep the value to use it when we want to\n    cancel consuming. The on_message method is passed in as a callback pika\n    will invoke when a message is fully received.\n\n    \"\"\"\n    LOGGER.info('Issuing consumer related RPC commands')\n    self.add_on_cancel_callback()\n    self._consumer_tag = self._channel.basic_consume(self.on_message,\n                                                     self.QUEUE)\n\ndef on_bindok(self, unused_frame):\n    \"\"\"Invoked by pika when the Queue.Bind method has completed. At this\n    point we will start consuming messages by calling start_consuming\n    which will invoke the needed RPC commands to start the process.\n\n    :param pika.frame.Method unused_frame: The Queue.BindOk response frame\n\n    \"\"\"\n    LOGGER.info('Queue bound')\n    self.start_consuming()\n\ndef close_channel(self):\n    \"\"\"Call to close the channel with RabbitMQ cleanly by issuing the\n    Channel.Close RPC command.\n\n    \"\"\"\n    LOGGER.info('Closing the channel')\n    self._channel.close()\n\ndef open_channel(self):\n    \"\"\"Open a new channel with RabbitMQ by issuing the Channel.Open RPC\n    command. When RabbitMQ responds that the channel is open, the\n    on_channel_open callback will be invoked by pika.\n\n    \"\"\"\n    LOGGER.info('Creating a new channel')\n    self._connection.channel(on_open_callback=self.on_channel_open)\n\ndef run(self):\n    \"\"\"Run the example consumer by connecting to RabbitMQ and then\n    starting the IOLoop to block and allow the SelectConnection to operate.\n\n    \"\"\"\n    self._connection = self.connect()\n    self._connection.ioloop.start()\n\ndef stop(self):\n    \"\"\"Cleanly shutdown the connection to RabbitMQ by stopping the consumer\n    with RabbitMQ. When RabbitMQ confirms the cancellation, on_cancelok\n    will be invoked by pika, which will then closing the channel and\n    connection. The IOLoop is started again because this method is invoked\n    when CTRL-C is pressed raising a KeyboardInterrupt exception. This\n    exception stops the IOLoop which needs to be running for pika to\n    communicate with RabbitMQ. All of the commands issued prior to starting\n    the IOLoop will be buffered but not processed.\n\n    \"\"\"\n    LOGGER.info('Stopping')\n    self._closing = True\n    self.stop_consuming()\n    self._connection.ioloop.start()\n    LOGGER.info('Stopped')\n\ndef main():\n    logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)\n    example = ExampleConsumer('amqp://guest:guest@localhost:5672/%2F')\nwhile True:\n    try:\n        example.run()\n        LOGGER.info('We have exited the run() method, will retry in 10 seconds')\n    except Exception as error:\n        LOGGER.exception('Exception output %s an attempt to re-establish a connection will occur in 10 seconds', error)\n    time.sleep(10) # try try try again\n\nif name == 'main':\n    main()\n```\n. @awelzel, it looks like the issue has been addressed by the merged PR #672. Could you please close this PR? Thx!\nCC @gmr \n. Thank you.\n. @rmb938, does the broker node close the connection following the consumer cancellations?\nI don't think that it would be a good idea for pika to silently reconnect, because certain resources and operations on them are meaningful only on the channel/connection from which they were obtained. For example, it would not be meaningful to use a new channel to Ack a message that was received on a closed channel/connection.\n. Per https://www.rabbitmq.com/ha.html:\n\nthey can consume with the argument x-cancel-on-ha-failover set to true. Their consuming will then be cancelled on failover and a consumer cancellation notification sent. It is then the consumer's responsibility to reissue basic.consume to start consuming again.\n\nThis talks about simply reissuing basic.consume to get going again, not reconnecting.  If the connection isn't dropped in this case, when would pika make use of multiple hosts in connection parameters? I think it would make sense only when the connection is dropped?\n. @rmb938, makes sense. Thanks for the explanation!\n. @luiscoms: This feature is presently  only in pika's master branch for now and is\ntargeting pika v1.0.0 release (no ETA yet). For SelectConnection and other\nnon-blocking adapters, you would access it via the create_connection()\nclass method documented in\npika.adapters.base_connection.BaseConnection.create_connection() in pika's\nmaster branch.\nFor example, see https://github.com/pika/pika/blob/761ef5cf8e484fe8d52909e767ceef69afc82d3a/tests/acceptance/async_adapter_tests.py#L288 (you would use pika.SelectConnection.create_connection instead of connection_class.create_connection)\nNote that multiple hosts in this feature are used only during the initial connection establishment. When the connection fails after establishment, it's up to the user to re-connect (pika won't do it automagically). There are tradeoffs with the different approaches. However, the app will need to re-create channel(s) and consumer(s) anyway, and it already has the logic to create the connection, so it's no big deal to re-connect. One school of thought says that explicit is better than implicit.\ncc @rmb938 . @gmr, this fixes #532 and adds 4 unit test methods for BlockingConnection. Also, along with PR #542, this should help address #412\n. @gmr: Yes, it seems to be still the case; see https://github.com/pika/pika/blob/3125c79b1b71e4e605e7a129c4d309b3edbb796f/pika/adapters/base_connection.py#L270-L274: it calls self._on_connection_closed(None, True); and _on_connection_closed(...) calls self._on_disconnect(reply_code, reply_text), but nothing in this case set up self.closing, so reply_code and reply_text are zero and some default reply_text.\n. Fixed by PR #685\n. Will open another one, hopefully coveralls will be in a better mood\n. This fixes #535. Need to re-run the build, as it failed while github was under severe DDoS attack.\n. @gmr, I removed blocking_connection.py changes from this PR that overlapped/collided with changes in PR #542.\n. Thanks @robochat, I will see about cleaning up the warning.\n. @robochat, would you mind filing your feedback about the warning \"Unknown state on disconnect: 6\" as an Issue in pika? Thx!\n. PR #545 (at least at commit 635957f8a780814cb6eaa91c886a891b31518662) appears as if it would also fix this issue, because it catches the timeout and wouldblock errors from the send() and re-queues the unsent data for sending later when the socket becomes writable again.\n. @rmb938, there are a couple of reasons for this that I can think of:\n1. When the exchange is an empty string, the broker will attempt to deliver the message to a queue with the queue name specified by the routing_key parameter that you passed to basic_publish. If a queue with that name (and on the same vhost) doesn't exist at the time of basic_publish, then the message would not be delivered.\n2. In some testing that I was doing last night on using BlockingConnection to send a large number (e.g., 50000) of messages with very large message bodies (e.g., 100KB per message body), I discovered that messages were not being delivered upon return from basic_publish, and instead still remained in the BlockingConnection's outbound_buffer list. When you're experiencing this, try calling the close() method of the BlockingConnection instance that was used to send the missing messages and see if those messages show up.\nPlease reply here with your findings.\nBest,\nVitaly\n. Very strange situation with coverall failure here: my test only added about 5 lines of code, so I don't see why it's unhappy.\n. @weinv, this is to be expected. If delivery-confirmation is not enabled, the message return back to client will happen asynchronously at a slightly later time. Enable delivery confirmations via channel.confirm_delivery() once before you start publishing and you should get the desired outcome.\n. Hi @markcf, are you thinking that you would call something like getWaitingMessageCount() and if it returned non-zero N you would then start a consume() loop and let it run only for N iterations and then break out to avoid blocking in the generator while waiting for the next messages to arrive? If so, you might end up with a poorly behaved polling-style application in terms of CPU consumption. You might be better off with something like the Twisted Connection or see if pika BlockingConnection is compatible with Gevent.\n. If I am not mistaken, the proposed change in base_connection.py might fix #538 as well.\n. @markcf, I examined the pika API and found a comparable solution for your polling use case that should work well using the existing pika API:\n1. Register your consumer and provide a callback via basic_consume(). Your callback will be called whenever pika BlockingConnection receives a message destined for this consumer.\n2. If you prefer not to process incoming messages directly in the callback function, you can have the callback function simply append the message to your own queue (or collections.deque, etc.). Then, you can check the length of your own queue and process the messages as you had intended.\nI am guessing that your polling application makes periodic calls to BlockingConnection.process_data_events() to keep the I/O and timers going.\n. This is what I had in mind, which adds a very tiny amount of very simple user code:\n```\nmessages = []\nchannel.basic_consume((lambda *args: messages.append(args)), queue=\"my_input_q\")\nAnd when you want to process messages that arrived:\nwhile messages:\n  channel, method, properties, body = messages.pop(0)\n  \n```\nMy concern with the proposed API has to do with exposing internal implementation details of the generator and possibly making it more difficult to maintain/evolve in the future.\n. @rmb938, how are you causing ioloop.start() to terminate?\n. Hi @rmb938, I just ran the following simplistic program against pika 0.9.14 and didn't get a crash:\n```\nIn [34]: import pika; import threading\nIn [35]: def f():\n    c = pika.SelectConnection(on_open_callback=(lambda c: c.channel(lambda ch: None)))\n    t = threading.Timer(5, c.ioloop.stop)\n    t.start()\n    c.ioloop.start()\n    c.close()\n   ....:     \nIn [36]: f()\nIn [37]: pika.version\nOut[37]: '0.9.14'\n```\nNOTE: my successful run above was on Mac OS X Yosemite\nWould you mind putting together a very simple bare-bones script (akin to mine in simplicity) that reproduces the problem and post it as a comment in this issue?\nAlso, on which platform are you experiencing this issue?\n. Thanks. I am also able to reproduce it on Amazon's CentOS release 6.3 (Final) using pika 0.9.14\n. CC @wjps\nThe original problem has to do with with misplaced clean-up logic in select_connection.PollPoller.start(). In the existing implementation, there is no clean way to gracefully close the connection after PollPoller.start() terminates as the result of the call to ioloop.stop(), which sets the .open member to False. Referring to the code below, after the while self.open: loop terminates, the function unregisters self.fileno, which explains the subsequent IOError: [Errno 2] No such file or directory in the context of Connection.close()\n```\n    def start(self):\n        \"\"\"Start the main poller loop. It will loop here until self.closed\"\"\"\n        was_open = self.open\n        while self.open:\n            self.poll()\n            self.process_timeouts()\n            self._manage_event_state()\n        if not was_open:\n            return\n        try:\n            LOGGER.info(\"Unregistering poller on fd %d\" % self.fileno)\n            self.update_handler(self.fileno, 0)\n            self._poll.unregister(self.fileno)\n        except IOError as err:\n            LOGGER.debug(\"Got IOError while shutting down poller: %s\", err)\n``\n. @wjps, this seems like a good strategy. On a related note, have you had experience withselectors` in python 3.4 or its 2.x back-port https://github.com/berkerpeksag/selectors34? Could that be a good substitute/foundation for SelectConnection's own management of the ioloop pollers?\n. @wjps, I concur that additional thought needs to be given to the large number of adapters and their structure. Fortunately, some of them have a small code footprint and get most of their functionality from BaseConnection and Connection.\nI recently prototyped a new blocking-style adapter implemented solely on top of SelectConnection versus BlockingConnection, which has a fair amount of protocol logic and its own ioloop.\nMy prototype, called SynchronousConnection, is located in this branch https://github.com/vitaly-krugl/pika/tree/pika-synchronous. Since it's built on top of SelectConnection, it gets SelectConnection's performance benefits and a reduced maintenance/testing footprint. Mush of the API is the same as in BlockingConnection; however I took the liberty to experiment with the consumer interface there.\nA recent test that I ran on my Mac (https://github.com/vitaly-krugl/pika-perf) that simply published 10000 1K messages, showed roughly the following Real Time (per time utility) performance characteristics:\nSelectConnection: approx. 1 sec\nSynchronousConnection: approx. 2 sec\nBlockingConnection: approx. 4 min (most of it waiting, but kernel and user CPU time was still several times more than SelectConnection and SynchronousConnection)\n. @wjps, yes I prototyped it with the thought of replacing BlockingConnection. At the moment, it's a functional proof of concept. I had a couple of reasons for contemplating a different consumer API versus those in BlockingConnection:\n1. It seemed confusing to have multiple ways in BlockingConnection of accomplishing essentially the same thing\n2. In the generator-based consumer, if you cancel a consumer, there was not an apparent mechanism for extracting the the messages that arrived for this consumer between the time that you cancel the consumer and the arrival of Basic-CancelOk, particularly for messages received under no-ack.\nThat said, if we're to replace BlockingConnection, we would need to provide a backward-compatible API.\nBefore investing any more effort in it (tests and the remainder of the BlockingConnection APIs), I need to run it by @gmr to see if he would be open to this degree of change.\n. @wjps, I reviewed the PR and found a number of issues here. Please take a look at my feedback. Thx\n. @wjps, I modified one of my feedback comments and added some more. I think I am done with the review now.\n. @jan1206, thanks for the update!\n. Yes, that is the right thing to do. Thanks!\n. @gmr, I resubmitted this PR in hopes that coveralls would be better behaved this time. All the tests passed. This PR simply deletes two lines of code, so I don't understand why coveralls is complaining.\n. Sincere thanks @wjps for your work on this! I am done with my feedback.\n. @wjps, it does sound like a race condition. Can you try to reproduce it by setting up a long-running test loop and letting it run for a day in the failing environment?\nLooking at the failed test's log, something is clearly out of whack. Either something is impacting the processing/order of synchronous or other commands or something external deleted the queue. \nqueue q53882832 declaration appears to succeed at first:\npika.callback: DEBUG: Removing callback #0: {'callback': <bound method TestZ_PublishAndConsume.on_queue_declared of <select_adapter_tests.TestZ_PublishAndConsume testMethod=start_test>>, 'only': None, 'one_shot': True, 'arguments': {'queue': 'q53882832'}, 'calls': 0}\nBut then, basic_consume returns NOT_FOUND - no queue \\'q53882832\\':\npika.channel: INFO: <METHOD(['channel_number=1', 'frame_type=1', 'method=<Channel.Close([\\'class_id=60\\', \\'method_id=20\\', \\'reply_code=404\\', \"reply_text=NOT_FOUND - no queue \\'q53882832\\' in vhost \\'/\\'\"])>'])>\npika.channel: WARNING: Received remote Channel.Close (404): NOT_FOUND - no queue 'q53882832' in vhost '/'\n. Looks good to me\n. @gmr, this PR fixes a BlockingConnection regression introduced by df7d3b742bb8f9ef830454f82d4679bb0f219dca and also fixes a few other cases related to loss of connection; it also adds several acceptance tests for loss of connection scenarios.\n. Fixed some issues and squashed all changes\n. @wjps, thanks for the report. Is it reproducible on your travis setup? It seems to be passing on the official pika setup. I see that the tests were running at the latest cd8c9b08e8a121acfb8b53344292d7b2ff4eecc6. I will take a look after work (PDST).\n. I pulled up the log from my browser's history:\n```\nBlockingConnection TCP/IP connection loss in CONNECTION_PROTOCOL ... ok\nBlockingConnection TCP/IP connection loss in CONNECTION_START ... ERROR\nBlockingConnection TCP/IP connection loss in CONNECTION_TUNE ... ok\nBlockingConnection reconnect with downed broker ... ok\nBlockingConnection resets properly on TCP/IP drop during channel() ... ok\n======================================================================\nERROR: BlockingConnection TCP/IP connection loss in CONNECTION_START\n\nTraceback (most recent call last):\n  File \"/home/travis/build/wjps/pika/tests/acceptance/blocking_adapter_test.py\", line 155, in start_test\n    self.connection.connect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 234, in connect\n    self._adapter_connect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 310, in _adapter_connect\n    self.process_data_events()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 242, in process_data_events\n    if self._handle_read():\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 358, in _handle_read\n    super(BlockingConnection, self)._handle_read()\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 389, in _handle_read\n    self._on_data_available(data)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1334, in _on_data_available\n    self._process_frame(frame_value)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1401, in _process_frame\n    if self._process_callbacks(frame_value):\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1371, in _process_callbacks\n    frame_value)  # Args\n  File \"/home/travis/build/wjps/pika/pika/callback.py\", line 62, in wrapper\n    return function(tuple(args), kwargs)\n  File \"/home/travis/build/wjps/pika/pika/callback.py\", line 94, in wrapper\n    return function(*args, kwargs)\n  File \"/home/travis/build/wjps/pika/pika/callback.py\", line 238, in process\n    callback(args, keywords)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1319, in _on_connection_tune\n    self._send_connection_open()\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1504, in _send_connection_open\n    self._on_connection_open, [spec.Connection.OpenOk])\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1488, in _rpc\n    self._send_method(channel_number, method_frame)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1554, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 423, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1539, in _send_frame\n    self._flush_outbound()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 377, in _flush_outbound\n    if self._handle_write():\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 417, in _handle_write\n    return self._handle_error(error)\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 323, in _handle_error\n    self._handle_disconnect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 273, in _handle_disconnect\n    self._adapter_disconnect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 325, in _adapter_disconnect\n    self._check_state_on_disconnect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 160, in _check_state_on_disconnect\n    raise exceptions.ProbableAccessDeniedError\nProbableAccessDeniedError: \n-------------------- >> begin captured logging << --------------------\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.adapters.base_connection: INFO: Connecting to ::1:58952\npika.adapters.base_connection: WARNING: Connection to ::1:58952 failed: [Errno 111] Connection refused\npika.adapters.base_connection: INFO: Connecting to 127.0.0.1:58952\npika.callback: DEBUG: Processing 0:Connection.Start\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.Start\"\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Processing 0:Connection.Tune\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.Tune\"\npika.connection: DEBUG: Creating a HeartbeatChecker: 580\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Processing 0:Connection.OpenOk\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.OpenOk\"\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.adapters.blocking_connection: INFO: Closing connection (200): Normal shutdown\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Processing 0:Connection.CloseOk\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.CloseOk\"\npika.heartbeat: DEBUG: Removing timeout for next heartbeat interval\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Incremented callback reference counter: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\npika.callback: DEBUG: Added: {'callback':  at 0x7f13477da5f0>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.adapters.base_connection: INFO: Connecting to ::1:58952\npika.adapters.base_connection: WARNING: Connection to ::1:58952 failed: [Errno 111] Connection refused\npika.adapters.base_connection: INFO: Connecting to 127.0.0.1:58952\npika.callback: DEBUG: Processing 0:Connection.Start\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 1 registered uses left\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #1: {'callback':  at 0x7f13477da5f0>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.Start\"\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Calling  at 0x7f13477da5f0> for \"0:Connection.Start\"\npika.callback: DEBUG: Processing 0:Connection.Tune\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.Tune\"\npika.connection: DEBUG: Creating a HeartbeatChecker: 580\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.adapters.base_connection: ERROR: Fatal Socket Error: error(32, 'Broken pipe')\npika.heartbeat: DEBUG: Removing timeout for next heartbeat interval\npika.adapters.base_connection: ERROR: Socket closed while tuning the connection indicating a probable permission error when accessing a virtual host\npika.callback: DEBUG: Incremented callback reference counter: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\npika.callback: ERROR: Calling > for \"0:Connection.Tune\" failed\nTraceback (most recent call last):\n  File \"/home/travis/build/wjps/pika/pika/callback.py\", line 238, in process\n    callback(*args, keywords)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1319, in _on_connection_tune\n    self._send_connection_open()\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1504, in _send_connection_open\n    self._on_connection_open, [spec.Connection.OpenOk])\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1488, in _rpc\n    self._send_method(channel_number, method_frame)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1554, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 423, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1539, in _send_frame\n    self._flush_outbound()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 377, in _flush_outbound\n    if self._handle_write():\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 417, in _handle_write\n    return self._handle_error(error)\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 323, in _handle_error\n    self._handle_disconnect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 273, in _handle_disconnect\n    self._adapter_disconnect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 325, in _adapter_disconnect\n    self._check_state_on_disconnect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 160, in _check_state_on_disconnect\n    raise exceptions.ProbableAccessDeniedError\nProbableAccessDeniedError\n--------------------- >> end captured logging << ---------------------\nName                                Stmts   Miss Branch BrMiss  Cover   Missing\n-------------------------------------------------------------------------------\npika                                   20      4      0      0    80%   7-12\npika.adapters                          20      7      0      0    65%   32-33, 38-40, 44-45\npika.adapters.asyncore_connection      64     10      8      5    79%   51, 67-72, 98, 138-139\npika.adapters.base_connection         234     67     87     32    69%   16-17, 62, 117-120, 164, 191-192, 217-224, 232-245, 256, 259-261, 294, 297-298, 302-303, 309-320, 329, 341-342, 353-355, 358-359, 365, 369-381, 408-410, 414-415, 454\npika.adapters.blocking_connection     438    197    149     97    50%   31, 45-47, 76-77, 90-92, 144, 154, 167, 201, 217, 219, 243, 245, 253, 285-288, 337-338, 348, 392-394, 406, 409-411, 426-428, 490-495, 510-516, 539-582, 612, 626, 637-643, 655-667, 677-694, 720-728, 766, 786-787, 824-830, 845-846, 867-868, 894-897, 926-929, 947-948, 959-960, 978-980, 986-987, 994-999, 1003, 1007, 1015, 1020-1021, 1044-1045, 1054-1055, 1066-1067, 1070-1071, 1074-1084, 1092-1093, 1104-1105, 1108-1111, 1123-1130, 1133-1135, 1155, 1161, 1170-1172, 1194-1196, 1207, 1219\npika.adapters.libev_connection        122     28     38     15    73%   97, 128, 133, 145, 147, 153, 162, 164, 166, 176-177, 184-185, 211-214, 229-230, 234-245, 250-251\npika.adapters.select_connection       237     66     85     41    67%   74-76, 107-109, 158-163, 180-182, 215-216, 230, 309-316, 330-332, 360, 382-383, 393-416, 424-429, 438-451, 513-516\npika.adapters.tornado_connection       24      0      4      1    96% \npika.adapters.twisted_connection      193    132     44     43    26%   31-32, 35-37, 40-42, 45-48, 70-75, 79-89, 98-111, 118-122, 129-131, 140-170, 173-174, 177-178, 183-185, 211, 219, 223-226, 230-233, 239-240, 246-253, 295, 300-301, 308, 317-318, 325-327, 332, 335, 339-342, 345, 348-349, 370-371, 382, 388, 392, 402-407, 422-424, 430, 434-436, 439-440, 444, 449-451, 454-458\npika.amqp_object                       25      0      6      0   100% \npika.callback                         172      3     86      3    98%   224, 273-274\npika.channel                          342      9    124      9    96%   67, 300, 622, 716, 827-828, 908, 1097, 1136\npika.connection                       571     47    235     42    89%   13, 96, 135, 150, 180, 182, 197, 261, 504, 642, 653, 687, 764, 853, 862, 912, 973, 1013-1019, 1028-1033, 1037-1038, 1047, 1069, 1140, 1178, 1204-1205, 1281, 1351, 1395, 1409, 1432-1434, 1477, 1482, 1533-1534, 1541, 1588, 1608\npika.credentials                       28      0      6      0   100% \npika.data                             144     21     68     10    85%   25, 135-136, 140-141, 145-146, 150-151, 160-161, 165-166, 175-176, 180-181, 193-196\npika.exceptions                        83     24     12      7    67%   7, 15, 21, 29, 42, 49, 56, 62-66, 72, 78-82, 88, 95, 101, 107, 113, 119, 133, 139, 145, 159\npika.frame                             89      0     14      0   100% \npika.heartbeat                         65      0      8      1    99% \npika.utils                              3      0      0      0   100% \n-------------------------------------------------------------------------------\nTOTAL                                2874    615    974    306    76% \n----------------------------------------------------------------------\nRan 544 tests in 60.538s\nFAILED (SKIP=16, errors=1)\nThe command \"nosetests -c nose.cfg --with-coverage --cover-package=pika\" exited with 1.\nDone. Your build exited with 1.\n```\n. @wjps, strangely, when running your loop on Fedora Linux (via VirtualBox) as well as natively on Mac OS X, I can reproduce a different exception that makes absolutely no sense (since I don't configure for SSL). Investigating. . .\n```\nERROR: BlockingConnection reconnect with downed broker\nTraceback (most recent call last):\n  File \"/Users/vkruglikov/ossdev/pika/tests/acceptance/blocking_adapter_test.py\", line 109, in start_test\n    channel = self.connection.channel()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 200, in channel\n    self._channels[channel_number] = BlockingChannel(self, channel_number)\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 473, in init\n    self.open()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 876, in open\n    self._rpc(spec.Channel.Open(), self._on_openok, [spec.Channel.OpenOk])\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 1169, in _rpc\n    self._wait_on_response(method_frame))\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 1190, in _send_method\n    self.connection.process_data_events()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 242, in process_data_events\n    if self._handle_read():\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 358, in _handle_read\n    super(BlockingConnection, self)._handle_read()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py\", line 373, in _handle_read\n    except ssl.SSLWantReadError:\nAttributeError: 'module' object has no attribute 'SSLWantReadError'\n``\n. @wjps, it looks like you addedssl.SSLWantReadError` exception handler in the \"write starvation\" PR #578.  It turns out that this exception was added in python 3.3, so it's not compatible with 2.x and is causing failures on 2.x that looks like this:\n```\nERROR: BlockingConnection resets properly on TCP/IP drop during channel()\nTraceback (most recent call last):\n  File \"/Users/vkruglikov/ossdev/pika/tests/acceptance/blocking_adapter_test.py\", line 57, in start_test\n    channel = self.connection.channel()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 200, in channel\n    self._channels[channel_number] = BlockingChannel(self, channel_number)\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 473, in init\n    self.open()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 876, in open\n    self._rpc(spec.Channel.Open(), self._on_openok, [spec.Channel.OpenOk])\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 1169, in _rpc\n    self._wait_on_response(method_frame))\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 1190, in _send_method\n    self.connection.process_data_events()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 242, in process_data_events\n    if self._handle_read():\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 358, in _handle_read\n    super(BlockingConnection, self)._handle_read()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py\", line 373, in _handle_read\n    except ssl.SSLWantReadError:\nAttributeError: 'module' object has no attribute 'SSLWantReadError'\n```\nPer https://docs.python.org/3/library/ssl.html:\n```\nexception ssl.SSLWantReadError\nA subclass of SSLError raised by a non-blocking SSL socket when trying to read or write data, but more data needs to be received on the underlying TCP transport before the request can be fulfilled.\nNew in version 3.3.\n```\n. I have 2.7.6 on my Mac and recent installation of Fedora Linux came with 2.7.8.\n. Thanks for the followup. \n. @ztane, if #569 supersedes this PR, would you mind closing this PR?\n. @robochat, this recent change in Master should have addressed your issue: https://github.com/pika/pika/commit/294904ee13d34abcea23527e87dd48c991542760#diff-1a5308d8b684fdafac1f89e376e8933dR161\n. @robochat: yes, your understanding is correct. Please post the result of your testing. Thanks!\n. @robochat, would you mind re-testing against master and closing the issue?. This should have been addressed as noted above.\nThanks,\nVitaly\n. Fixed via https://github.com/pika/pika/commit/294904ee13d34abcea23527e87dd48c991542760#diff-1a5308d8b684fdafac1f89e376e8933dR161\n. Excellent! Some food for thought, though:\n1. all_adapter_tests.py might not be a good name, because it doesn't include all adapters; specifically, it doesn't test the BlockingConnection adapter. Perhaps async_adapter_tests.py?\n2. When making a change in a single module (e.g., select_connection.py), it's convenient to quickly run just the tests that are pertinent to SelectConnection. How can this be done using the new refactored test code here?\n. NoneType would be from data = self.socket.recv(self._buffer_size) in _handle_read. self.socket gets set to None when the connection is closed.\n@sulsj, the best way to get help is to write a small, simple program that reproduces your failure and post it here. Please wrap your code snippets and tracebacks in github markdown for code to preserve indentation: 3 back-ticks before and 3 back-ticks after.\ndef foo():\n   . . .\nAlso, does your code work okay against pika master? Please post the results of that, too.\nFinally, is your app sharing the same pika connection among multiple threads?\n. @sulsj, I ran this simple script successfully on both Mac OS X and Fedora Linux using the latest pika master and Rabbit 3.5.1. I am posting the script here in case it may be of any help:\n```\nimport sys\nimport pika\nMAX_MESSAGES = 10000\ndef main():\n    conn = pika.BlockingConnection()\n    channel = conn.channel()\nchannel.queue_declare(queue='hello', auto_delete=True)\n\ndef on_incoming(channel, method, in_props, body):\n    corr_id = int(in_props.correlation_id)\n    print >> sys.stderr, \".\",\n\n    channel.basic_ack(delivery_tag=method.delivery_tag)\n\n    if corr_id >= MAX_MESSAGES:\n        channel.stop_consuming()\n\n    props = pika.BasicProperties(correlation_id=str(corr_id + 1))\n    channel.basic_publish(exchange='',\n                          routing_key=method.routing_key,\n                          properties=props,\n                          body=body)\n\nconsumer_tag = channel.basic_consume(on_incoming,\n                                     queue='hello',\n                                     no_ack=False)\n\n# Kickstart by publishing a single message\nchannel.basic_publish(\n    exchange='',\n    routing_key='hello',\n    properties=pika.BasicProperties(correlation_id=str(1)),\n    body='Hello World!')\n\nprint >> sys.stderr, \"start_consuming()\"\nchannel.start_consuming()\n\nchannel.close()\n\nif name == \"main\":\n    main()\n``\n. @sulsj, BlockingConnection won't do any hearbeat processing on its own, since it doesn't have its own thread of execution. Its I/O loop runs only when you make a call that requires I/O. I believe that the recommended way to keep things alive is to make periodic calls to the connection'sprocess_data_events()` method (see http://pika.readthedocs.org/en/latest/modules/adapters/blocking.html).\nThat said, your app needs to be prepared to deal with connection loss. I am not aware of any 100% effective method to keep the connection alive with any client. And even if it's not the client, then it could be the server that gets restarted, causing connections to be dropped.\n. https://github.com/gmr/rabbitpy could be a better option for your use case, since it uses a background thread for I/O. I don't have firsthand experience with rabbitpy, though.\n. @sulsj, it sounds like your issue was related to RabbitMQ dropping connections because your workers were busy for a long time preventing the heartbeats from being serviced. If that's the case, would you mind closing this issue?\n. UPDATE: add_callback_threadsafe() in pull request #956 might help with this. See this example. With this feature, you can offload the lengthy processing of your message to another thread, which would then delegate channel.basic_ack() to your BlockingConnection thread via BlockingConnection.add_callback_threadsafe(), assuming your BlockingConnection is being serviced via BlockingConnection.sleep(), BlockingConnection.process_data_events, BlockingChannel.consume(), BlockingChannel.start_consuming(), or similar.. CC @wjps \n. Fixed, thanks\n. CC @wjps \n. @wjps, oops, sorry about that. I missed the def __getattr__ in IOLoop. Closing the issue.\n. @gmr, this PR should complete the fix for issue #412 \n. This fixes broken tests from PR #555 with the following failure signatures:\n```\nERROR: start_test (select_connection_ioloop_tests.IOLoopTimerTestKqueue)\nTraceback (most recent call last):\n  File \"/Users/vkruglikov/ossdev/pika/tests/unit/select_connection_ioloop_tests.py\", line 30, in setUp\n    self.ioloop = select_connection.IOLoop()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/select_connection.py\", line 101, in init\n    self._poller = self._get_poller()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/select_connection.py\", line 119, in _get_poller\n    poller = KQueuePoller()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/select_connection.py\", line 398, in init\n    super(KQueuePoller, self).init()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/select_connection.py\", line 157, in init\n    self.add_handler(self._r_interrupt.fileno(), self.read_interrupt, READ)\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/select_connection.py\", line 264, in add_handler\n    self.update_handler(fileno, events)\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/select_connection.py\", line 431, in update_handler\n    self._kqueue.control([event], 0)\nAttributeError: 'KQueuePoller' object has no attribute '_kqueue'\n```\n```\nERROR: start_test (select_connection_ioloop_tests.IOLoopThreadStopTestKqueue)\nTraceback (most recent call last):\n  File \"/Users/vkruglikov/ossdev/pika/tests/unit/select_connection_ioloop_tests.py\", line 52, in start_test\n    self.start()\n  File \"/Users/vkruglikov/ossdev/pika/tests/unit/select_connection_ioloop_tests.py\", line 38, in start\n    self.ioloop.start()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/select_connection.py\", line 302, in start\n    self.poll()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/select_connection.py\", line 465, in poll\n    fd_event_map[fileno] |= self._map_event(event.filter)\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/select_connection.py\", line 440, in _map_event\n    if kevent.filter == select.KQ_FILTER_READ:\nAttributeError: 'int' object has no attribute 'filter'\n```\nCC @wjps: I think you mentioned that you didn't have access to Mac OS X to test changes to KQueuePoller. I noticed the failures on my MacBookPro and fixed them.\n. I thought I disabled the trailing white space eliminator on my editor, but somehow they still got eliminated, making the diff a little more involved than I intended, but not too bad, so I'm keeping it.\n. @ztane, I am encountering some issues with SelectConnection after the latest merges: some messages don't finish sending before the ioloop terminates. Which issues are you seeing? Thx.\n. Ok, thanks @ztane. I was just wondering whether you were seeing the same problem that I was seeing after recent commits. Best.\n. I think that Gavin said that he would hold off on other PRs until yours is merged, so hopefully you're really close. Good luck.\n. Here is the log of a SUCCESSFUL run using pika at the known good commit 409670bdbea13d11a43d3d02162d49a200e450a5 that precedes the late April PR merges:\n2015-05-13 17:08:58,137 root(8366) - INFO - runSelectPublishTest: impl=SelectConnection; exchange=; numMessages=1; messageSize=1024; deliveryConfirmation=True\n2015-05-13 17:08:58,137 pika.callback(8366) - DEBUG - Added: {'callback': <bound method SelectConnection._on_connection_error of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-05-13 17:08:58,137 pika.callback(8366) - DEBUG - Added: {'callback': <function onConnectionOpen at 0x10b337a28>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-05-13 17:08:58,137 pika.callback(8366) - DEBUG - Added: {'callback': <function onConnectionClosed at 0x10b337aa0>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-05-13 17:08:58,137 pika.callback(8366) - DEBUG - Added: {'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,138 pika.adapters.base_connection(8366) - INFO - Connecting to 127.0.0.1:5672\n2015-05-13 17:08:58,138 pika.adapters.select_connection(8366) - DEBUG - Starting the Poller\n2015-05-13 17:08:58,138 pika.adapters.select_connection(8366) - DEBUG - Using KQueuePoller\n2015-05-13 17:08:58,138 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(4)\n2015-05-13 17:08:58,139 pika.adapters.select_connection(8366) - DEBUG - Starting IOLoop\n2015-05-13 17:08:58,139 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(1)\n2015-05-13 17:08:58,139 pika.callback(8366) - DEBUG - Processing 0:Connection.Start\n2015-05-13 17:08:58,139 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,140 pika.callback(8366) - DEBUG - 0 registered uses left\n2015-05-13 17:08:58,140 pika.callback(8366) - DEBUG - Removing callback #0: {'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-05-13 17:08:58,140 pika.callback(8366) - DEBUG - Calling <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>> for \"0:Connection.Start\"\n2015-05-13 17:08:58,140 pika.callback(8366) - DEBUG - Added: {'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,140 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(4)\n2015-05-13 17:08:58,140 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(1)\n2015-05-13 17:08:58,140 pika.callback(8366) - DEBUG - Processing 0:Connection.Tune\n2015-05-13 17:08:58,140 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,140 pika.callback(8366) - DEBUG - 0 registered uses left\n2015-05-13 17:08:58,140 pika.callback(8366) - DEBUG - Removing callback #0: {'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-05-13 17:08:58,140 pika.callback(8366) - DEBUG - Calling <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>> for \"0:Connection.Tune\"\n2015-05-13 17:08:58,141 pika.connection(8366) - DEBUG - Creating a HeartbeatChecker: 580\n2015-05-13 17:08:58,141 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(4)\n2015-05-13 17:08:58,141 pika.callback(8366) - DEBUG - Added: {'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,141 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(4)\n2015-05-13 17:08:58,141 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(1)\n2015-05-13 17:08:58,141 pika.callback(8366) - DEBUG - Processing 0:Connection.OpenOk\n2015-05-13 17:08:58,141 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,141 pika.callback(8366) - DEBUG - 0 registered uses left\n2015-05-13 17:08:58,141 pika.callback(8366) - DEBUG - Removing callback #0: {'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-05-13 17:08:58,141 pika.callback(8366) - DEBUG - Calling <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>> for \"0:Connection.OpenOk\"\n2015-05-13 17:08:58,141 pika.callback(8366) - DEBUG - Added: {'callback': <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,141 pika.callback(8366) - DEBUG - Processing 0:_on_connection_open\n2015-05-13 17:08:58,142 pika.callback(8366) - DEBUG - Calling <function onConnectionOpen at 0x10b337a28> for \"0:_on_connection_open\"\n2015-05-13 17:08:58,142 root(8366) - INFO - Select opening channel...\n2015-05-13 17:08:58,142 pika.callback(8366) - DEBUG - Added: {'callback': <bound method SelectConnection._on_channel_closeok of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,142 pika.callback(8366) - DEBUG - Added: {'callback': <bound method Channel._on_getempty of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-05-13 17:08:58,142 pika.callback(8366) - DEBUG - Added: {'callback': <bound method Channel._on_cancel of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-05-13 17:08:58,142 pika.callback(8366) - DEBUG - Added: {'callback': <bound method Channel._on_flow of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-05-13 17:08:58,142 pika.callback(8366) - DEBUG - Added: {'callback': <bound method Channel._on_close of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,142 pika.channel(8366) - DEBUG - Adding in on_synchronous_complete callback\n2015-05-13 17:08:58,142 pika.callback(8366) - DEBUG - Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,142 pika.channel(8366) - DEBUG - Adding passed in callback\n2015-05-13 17:08:58,142 pika.callback(8366) - DEBUG - Added: {'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,142 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(4)\n2015-05-13 17:08:58,143 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(1)\n2015-05-13 17:08:58,143 pika.callback(8366) - DEBUG - Processing 1:Channel.OpenOk\n2015-05-13 17:08:58,143 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,143 pika.callback(8366) - DEBUG - 0 registered uses left\n2015-05-13 17:08:58,143 pika.callback(8366) - DEBUG - Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-05-13 17:08:58,143 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,143 pika.callback(8366) - DEBUG - 0 registered uses left\n2015-05-13 17:08:58,143 pika.callback(8366) - DEBUG - Removing callback #0: {'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-05-13 17:08:58,143 pika.callback(8366) - DEBUG - Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10b02e210>> for \"1:Channel.OpenOk\"\n2015-05-13 17:08:58,143 pika.channel(8366) - DEBUG - 0 blocked frames\n2015-05-13 17:08:58,143 pika.callback(8366) - DEBUG - Calling <bound method Channel._on_openok of <pika.channel.Channel object at 0x10b02e210>> for \"1:Channel.OpenOk\"\n2015-05-13 17:08:58,143 pika.callback(8366) - DEBUG - Added: {'callback': <function onDeliveryConfirmation at 0x10b337938>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-05-13 17:08:58,144 pika.callback(8366) - DEBUG - Added: {'callback': <function onDeliveryConfirmation at 0x10b337938>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-05-13 17:08:58,144 pika.channel(8366) - DEBUG - Adding in on_synchronous_complete callback\n2015-05-13 17:08:58,144 pika.callback(8366) - DEBUG - Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,144 pika.channel(8366) - DEBUG - Adding passed in callback\n2015-05-13 17:08:58,144 pika.callback(8366) - DEBUG - Added: {'callback': <bound method Channel._on_selectok of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,144 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(4)\n2015-05-13 17:08:58,144 root(8366) - INFO - SelectConnection: enabled message delivery confirmation\n2015-05-13 17:08:58,144 root(8366) - INFO - Select publishing...\n2015-05-13 17:08:58,144 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(5)\n2015-05-13 17:08:58,144 root(8366) - INFO - Published 1 messages of size=1024 via=<class 'pika.adapters.select_connection.SelectConnection'>\n2015-05-13 17:08:58,144 pika.channel(8366) - INFO - Channel.close(0, Normal Shutdown)\n2015-05-13 17:08:58,144 pika.channel(8366) - DEBUG - Adding in on_synchronous_complete callback\n2015-05-13 17:08:58,144 pika.callback(8366) - DEBUG - Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,144 pika.channel(8366) - DEBUG - Adding passed in callback\n2015-05-13 17:08:58,144 pika.callback(8366) - DEBUG - Added: {'callback': <bound method Channel._on_closeok of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,145 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(5)\n2015-05-13 17:08:58,145 pika.connection(8366) - INFO - Closing connection (200): Normal shutdown\n2015-05-13 17:08:58,145 pika.callback(8366) - DEBUG - Added: {'callback': <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,145 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(5)\n2015-05-13 17:08:58,145 pika.adapters.select_connection(8366) - DEBUG - Stopping the poller event loop\n2015-05-13 17:08:58,145 pika.adapters.select_connection(8366) - DEBUG - Starting IOLoop\n2015-05-13 17:08:58,145 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(5)\n2015-05-13 17:08:58,145 pika.callback(8366) - DEBUG - Processing 1:Confirm.SelectOk\n2015-05-13 17:08:58,145 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,145 pika.callback(8366) - DEBUG - 0 registered uses left\n2015-05-13 17:08:58,145 pika.callback(8366) - DEBUG - Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-05-13 17:08:58,145 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,145 pika.callback(8366) - DEBUG - 0 registered uses left\n2015-05-13 17:08:58,145 pika.callback(8366) - DEBUG - Removing callback #0: {'callback': <bound method Channel._on_selectok of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-05-13 17:08:58,145 pika.callback(8366) - DEBUG - Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10b02e210>> for \"1:Confirm.SelectOk\"\n2015-05-13 17:08:58,145 pika.channel(8366) - DEBUG - 0 blocked frames\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Calling <bound method Channel._on_selectok of <pika.channel.Channel object at 0x10b02e210>> for \"1:Confirm.SelectOk\"\n2015-05-13 17:08:58,146 pika.channel(8366) - DEBUG - Confirm.SelectOk Received: <METHOD(['channel_number=1', 'frame_type=1', 'method=<Confirm.SelectOk>'])>\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Processing 1:Basic.Ack\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Calling <function onDeliveryConfirmation at 0x10b337938> for \"1:Basic.Ack\"\n2015-05-13 17:08:58,146 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(5)\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Processing 1:Channel.CloseOk\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - 0 registered uses left\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Removing callback #0: {'callback': <bound method SelectConnection._on_channel_closeok of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - 0 registered uses left\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - 0 registered uses left\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Removing callback #0: {'callback': <bound method Channel._on_closeok of <pika.channel.Channel object at 0x10b02e210>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-05-13 17:08:58,146 pika.callback(8366) - DEBUG - Calling <bound method SelectConnection._on_channel_closeok of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>> for \"1:Channel.CloseOk\"\n2015-05-13 17:08:58,147 pika.callback(8366) - DEBUG - Incremented callback reference counter: {'callback': <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\n2015-05-13 17:08:58,147 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(5)\n2015-05-13 17:08:58,147 pika.callback(8366) - DEBUG - Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10b02e210>> for \"1:Channel.CloseOk\"\n2015-05-13 17:08:58,147 pika.channel(8366) - DEBUG - 0 blocked frames\n2015-05-13 17:08:58,147 pika.callback(8366) - DEBUG - Calling <bound method Channel._on_closeok of <pika.channel.Channel object at 0x10b02e210>> for \"1:Channel.CloseOk\"\n2015-05-13 17:08:58,147 pika.callback(8366) - DEBUG - Clearing out '1' from the stack\n2015-05-13 17:08:58,147 pika.adapters.select_connection(8366) - DEBUG - Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>(1)\n2015-05-13 17:08:58,147 pika.callback(8366) - DEBUG - Processing 0:Connection.CloseOk\n2015-05-13 17:08:58,147 pika.callback(8366) - DEBUG - Processing use of oneshot callback\n2015-05-13 17:08:58,147 pika.callback(8366) - DEBUG - 1 registered uses left\n2015-05-13 17:08:58,147 pika.callback(8366) - DEBUG - Calling <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>> for \"0:Connection.CloseOk\"\n2015-05-13 17:08:58,147 pika.heartbeat(8366) - DEBUG - Removing timeout for next heartbeat interval\n2015-05-13 17:08:58,147 pika.adapters.base_connection(8366) - WARNING - Unknown state on disconnect: 6\n2015-05-13 17:08:58,147 pika.adapters.select_connection(8366) - DEBUG - Stopping the poller event loop\n2015-05-13 17:08:58,147 pika.callback(8366) - DEBUG - Added: {'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x10b014750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-05-13 17:08:58,147 pika.connection(8366) - WARNING - Disconnected from RabbitMQ at localhost:5672 (0): Not specified\n2015-05-13 17:08:58,148 pika.callback(8366) - DEBUG - Processing 0:_on_connection_closed\n2015-05-13 17:08:58,148 pika.callback(8366) - DEBUG - Calling <function onConnectionClosed at 0x10b337aa0> for \"0:_on_connection_closed\"\n2015-05-13 17:08:58,148 root(8366) - INFO - Select connection closed (0): Not specified\nThe above log is from a SUCCESSFUL run using pika at the known good commit 409670bdbea13d11a43d3d02162d49a200e450a5 \n. @wjps, the first bad commit for this is 7f91a684f4b8548ab18d9f2a3cd0d5f49fb60e3d \"Make SelectConnection behave like an ioloop\". Would you mind taking a stab at this? I am able to reproduce this easily on Fedora Linux running in VirtualBox as described at the top of this issue. My code snippet might also make for a good test to add to async tests along with the fix.\n. @wjps: any thoughts on this bug?\n. Thanks for tackling it @wjps!\n. Regarding:\n\nI'm also slightly confused by the second call to connection.ioloop.start() in onChannelOpen, was that intentional?\n\nI think that I misinterpreted the pika async example (http://pika.readthedocs.org/en/latest/examples/asynchronous_consumer_example.html) in thinking that the call to connection.ioloop.start() is needed after requesting a close. I am glad that it exposed a bug, though, so the time spent was not a waste. The example does this in def stop(); I revisited the example after your question, and discovered that stop() is called only after CTL-C causes example.run() to abort, so it has to call self._connection.ioloop.start() in order to get the I/O loop running again and complete the graceful shutdown sequence.\n. @wjps, I retested with your fix and the hang is gone on both Fedora and Mac OS X.\n. Fixed, thanks!\n. What about tests?\n. @mffrench - not so much a complaint, but rather peer code review feedback on areas impacting quality and stability of the code base.\nBest,\nVitaly\n. @gmr, I added the copyright header per https://github.com/vitaly-krugl/inetpy/blob/master/LICENSE.md, which is where the code for forward_server.py came from. Please let me know if this should be of a different form.\n. +1\n. This has been fixed in the new implementation of BlockingConnection.\n. No longer an issue, since BlockingConnection is now implemented as a client of SelectConnection.\n. @gmr, I was using BlockingConnection, which should process data events on its own.\n. process_data_events() gets called all over the place in BlockingConnection/BlockingChannel code and I leave _force_data_events_override at default (None), so it should not interfere with event processing in _rpc(), either:\nif force_data_events and self._force_data_events_override is not False:\n            self.connection.process_data_events()\n. I really don't see any code in the implementation that could possibly purge anything from the BlockingChannel._replies list.\n. No longer an issue in the new SelectConnection-based implementation of BlockingConnection.\n. @wjps, this is ironic: the code snippet to reproduce #570 came from my performance test pika_perf.py that worked \"properly\" because of this bug. Once you fixed this bug, my test stopped working \"correctly\" :-)\n. @wjps and @gmr: I thought you might be glad to know that SelectConnection picked up a lot of basic_publish performance after the latest changes. Both RabbitMQ 3.5.1 and tests were running on my dev MacBookPro with SSD.\nBefore io loop and starvation fixes:\n- Without pubacks\n$ time python pika_perf.py publish --impl=SelectConnection --exg=\"\" --msgs=100000\n  real    0m11.629s\n  user    0m7.640s\n  sys 0m2.741s\n- With pubacks\n$ time python pika_perf.py publish --impl=SelectConnection --exg=\"\" --msgs=100000 --pubacks\n  real    0m16.863s\n  user    0m12.521s\n  sys 0m3.133s\nAfter the fixes:\n- Without pubacks\n$ time python pika_perf.py publish --impl=SelectConnection --exg=\"\" --msgs=100000\n  real    0m6.392s\n  user    0m4.620s\n  sys 0m1.723s\n- With pubacks\n$ time python pika_perf.py publish --impl=SelectConnection --exg=\"\" --msgs=100000 --pubacks\n  real    0m8.519s\n  user    0m6.926s\n  sys 0m1.572s\n. @paulinus, I don't see this problem in master at c7a72cdd4f5f7e1d692bf42943c9f3810f6bb1b7.\nSee BaseConnection._handle_write change in PR #578\n. @tonyennis, AMQP signals errors (exchange not found in your case) by closing the channel and including the error code and text message in the Channel.Close method. So, it's normal for the channel to close under these circumstances, but not the connection.\nYou need to provide additional information to help debug this, including a small fully-functional script that reproduces the issue, pika logs at DEBUG level, and complete tracebacks for both exceptions. Also, describe your runtime environment, pika version, and anything else that might be helpful.\nTo switch pika logging to DEBUG level, you may try something like this in your script (in addition to configuring logging; refer to the python logging module for instructions):\nimport logging\nlogging.getLogger(\"pika\").setLevel(logging.DEBUG)\n. I just added more info in my comment. And yes, dump the relevant info into a comment or the body of the issue.\n. It's a bug in the recently-released Jython 2.7: see http://bugs.jython.org/issue2273\n. This was a problem in Jython. The Jython issue http://bugs.jython.org/issue2273 appears to have been fixed. There is also a new pika release 0.10.0. Please retest with the fixed Jython and open a new issue if this is still a problem.\n. @cgabard: blocking_connection has been completely re-implemented in master as a mostly thin wrapper around SelectConnection. This change is no longer necessary and is not compatible with the new implementation. \n. @gmr: this is the new BlockingConnection implemented as a client of SelectConnection that we discussed over email. I wanted to run it by you. It doesn't have the test work completed yet (coming soon), so it's not ready for merging. \nMy publisher performance test shows startling performance differences between the new implementation and legacy:\nThese performance tests were publishing 10,000 messages 1024 bytes in size with and without publisher acknowledgments. They were executed on my MacBookPro with SSD against RabbitMQ 3.5.1 running locally.\n- With pub-acks, it's 3 seconds (new) vs. 2 Minutes (legacy)\n- Without pub-acks, it's 1 second (new) vs. 37 seconds (legacy)\n```\nNew with publisher acknowledgments:\ntime python pika_perf.py publish --impl BlockingConnection --exg=\"\" --msgs 10000 --pubacks\nreal    0m2.981s\nuser    0m1.632s\nsys 0m0.330s\nNew without publisher acknowledgments:\ntime python pika_perf.py publish --impl BlockingConnection --exg=\"\" --msgs 10000\nreal    0m0.904s\nuser    0m0.629s\nsys 0m0.230s\nOld with publisher acknowledgments:\ntime python pika_perf.py publish --impl BlockingConnection --exg=\"\" --msgs 10000 --pubacks\nreal    2m11.383s\nuser    0m7.180s\nsys 0m1.683s\nOld without publisher acknowledgments:\ntime python pika_perf.py publish --impl BlockingConnection --exg=\"\" --msgs 10000\nreal    0m37.236s\nuser    0m2.177s\nsys 0m0.763s\n```\nCC @wjps \n. @gmr: with respect to API changes, you mentioned that you weren't happy with the legacy BlockingConnection API and implementation and that you were open to change. The new implementation preserves many of the legacy APIs with the following exceptions:\n1. Legacy BlockingConnection/BlockingChannel methods that take callbacks have been removed.\n   - Justification: this is a blocking/synchronous interface vs. other adapters that are continuation-callback-based interfaces.\n2. The nowait args have been removed (defaulted to False in legacy API) and are internally forced to False\n   - Justification: In the blocking interface, we want to correlate a failure with the API that caused it, not get that error when calling some other API later\n3. Replaced the consumer API mechanisms (there were 2 or 3 different ways to consume before) with a single, more generic event mechanism accessed via BasicChannel.get_event() method that also supports an optional inactivity timeout. This mechanism presently returns two types of events to the user: consumed messages and broker's consumer cancellation notices. And there is a generator iterator method that wraps around it with the usage semantic: for evt in channel.read_events():\n   - Justification: in the legacy implementation, there were multiple API mechanisms for accessing consumers, including one with callbacks, and there was no support for inactivity timeouts. The new event-based semantic replaces those with a single mechanism that can be easily and naturally extended to support additional event types, such as timers, AMQP flow-control, etc.\n4. While preserving the legacy basic_publish, also added a new publish method that communicates unroutable (Basic.Return) and Nack'ed messages via exceptions\n   - Justification: these exceptions contain the returned messages along with the Basic.Return method that contains broker's error code and error messages. This useful diagnostic information isn't exposed by the legacy BlockingChannel.basic_publish.\n. @gmr, did you intend to merge this PR in its current state?\n. @andreas-kopecky, fixes are coming rapidly, including the queue_declare: see PR #590. This PR got merged before I had the time to write all the tests, so expect some instability until the tests are all in.\nAn earlier comment in this PR summarizes the API changes, including get_event as the canonical mechanism for consuming (and read_evens as the generator iterator). This mechanism supports an optional inactivity timeout, making it easy to perform some periodic activity (or break out of the loop) in case no messages are arriving for a while.\nPreviously, recursion resulting from callbacks was a source of instability and robust support for it is difficult to maintain in a blocking-style interface. Eliminating recursion from the interface should improve reliability. And performance got a huge boost as well.\n. Thanks @andreas-kopecky for the help offer! I have some ideas about how to re-introduce some of the deprecated API's or something similar to them to ease the transition for apps migrating to 0.10.x version of pika. \nPresently, I could really use help with Acceptance tests in blocking_adapter_test.py. I fixed up the existing tests there in PR #590 (https://github.com/vitaly-krugl/pika/blob/new-blocking-connection/tests/acceptance/blocking_adapter_test.py). It would be great to add acceptance tests for public BlockingChannel methods (pick your favorite ones :) ). I am going to be off the grid for almost two weeks, so I am afraid I won't be able to help during that time.\n. @andreas-kopecky, please let me know which BlockingChannel methods you will be instrumenting in blocking_adapter_test.py, so that we don't step on each other. Thanks!\n. @andreas-kopecky: I went ahead and implemented many acceptance tests for BlockingConnection and BlockingChannel (and fixed some bugs) via PR #591. We still need acceptance tests for the following:\n1. ~~BlockingChannel.read_events generator iterator~~\n2. ~~BlockingChannel.basic_nack~~\n3. ~~BlockingChannel.cancel_consumer~~\n4. BlockingChannel.basic_qos (something more sophisticated than the one in TestPublishAndConsumeAndQos in the above-mentioned PR #591)\n5. ~~BlockingChannel.basic_recover~~\n6. ~~BlockingChannel.basic_reject~~\n7. BlockingChannel.exchange_bind/exchange_unbind\n8. ~~BlockingChannel.tx_select/tx_commit/tx_rollback~~\n9. ~~Publishing and consuming a large message: e.g., 0.5 MBytes~~\n10. ~~Publishing/consuming many messages: e.g., 1000~~\n11. Validation of BasicProperties contents integrity through publish/consume cycle\n12. Validation of BasicProperties contents integrity through publish/basic_get cycle\n. @andreas-kopecky, for acceptance tests, you just need to know the APIs at user level; implementation details are not important. Just need to call the API, and verify that the return value (or callback or other side-effect) matches the API's documentation. Here is an example of the one that I worked on over the weekend: https://github.com/pika/pika/blob/master/tests/acceptance/blocking_adapter_test.py\n. @ztane: would you mind taking a look at the test failure? They are passing for python 2.6 and 2.7, but failing for both 3.x. Many thanks!\n. @ztane @everilae @analytik @bkjones: Could you guys please help debug this Python 3.x test failure? Many thanks in advance. All the tests pass on Python 2.6 and Python 2.7, but several tests fail in Travis CI build on 3.3 and 3.4.\nI instrumented one of the failing tests, blocking_adapter_test.TestBasicGet, with some extra logging. An excerpt from the test output is below. The last successful operation in this test is \"ENABLED PUB-ACKS\". Then the operation times out 15 seconds later as indicated by the label \"TIMED OUT\". The remaining steps should have completed in a fraction of a second. There are a couple of suspicious log messages from the pika.callback module, such as \"Values in  do not match for queue\"\n```\nblocking_adapter_test: INFO: 2015-06-06 18:44:14.680085 ENABLED PUB-ACKS (start_test (blocking_adapter_test.TestBasicGet))\npika.channel: DEBUG: Adding in on_synchronous_complete callback\npika.callback: DEBUG: Added: {'only': None, 'arguments': {'queue': 'TestBasicGet_q078c11140c7c11e5b541c714b39b9a0d'}, 'one_shot': True, 'calls': 1, 'callback': >}\npika.channel: DEBUG: Adding passed in callback\npika.callback: DEBUG: Added: {'only': None, 'arguments': {'queue': 'TestBasicGet_q078c11140c7c11e5b541c714b39b9a0d'}, 'one_shot': True, 'calls': 1, 'callback': >}\npika.callback: DEBUG: Processing 1:Queue.DeclareOk\npika.callback: DEBUG: Values in  do not match for queue\npika.callback: DEBUG: Arguments do not match for {'only': None, 'arguments': {'queue': 'TestBasicGet_q078c11140c7c11e5b541c714b39b9a0d'}, 'one_shot': True, 'calls': 1, 'callback': >}, ['])>]\npika.callback: DEBUG: Values in  do not match for queue\npika.callback: DEBUG: Arguments do not match for {'only': None, 'arguments': {'queue': 'TestBasicGet_q078c11140c7c11e5b541c714b39b9a0d'}, 'one_shot': True, 'calls': 1, 'callback': >}, ['])>]\nblocking_adapter_test: INFO: 2015-06-06 18:44:29.677356 TIMED OUT (start_test (blocking_adapter_test.TestBasicGet))\n```\n. Thank you @everilae \n. @everilae, now we know that your change fixed Py3, but it broke Py2.x. I will take the next stab at it.\n. @everilae, it doesn't seem to make sense that user code (the acceptance test in this case) should be responsible for encoding\n. @everilae, both 2.x and 3.x pass now\n. @gmr: this is ready to go. All tests pass\n```\nReintroduced BlockingConnection.process_data_events.\nImplemented Acceptance tests for most BlockingConnection and BlockingChannel publish API methods.\nFixed get_vent and cancel_consumer bugs.\nAdd logging in TestBasicGet to help debug test failure under Python 3.x\nbytes fixes by @everilae\nRemoved invalid reference self.socket.socket.shutdown in base_connection.py. forward_server cleanup.\nFixed blocking_adapter_test.py to work on python 2.6/2.7 (after changes to make it work on 3.x broke 2.x)\nAdded a test for publishing/consume a huge message and another test for publishing/consuming many messages.\nTest consumer cancellation and no_ack true/false side-effects\nFixed blocking_adapter_test.py failure on python 3.x\n``\n. Thanks @everilae !\n. I agree that the library needs to do the right thing. The library needs to be careful, though, not to incur unnecessary overhead of re-encoding data that is already in a compatible format.\n. Perhaps these things should be symmetric and pika should just accept and return everything in bytes only. This would avoid unnecessary overhead of encoding in pika and do away with the current asymmetric situation. Since AMQP assumes the data is \"bytes\", only the application can possibly know what the target format should be. pika should just stick to moving bytes, and no one needs to get hurt :)\n. Gavin, as I thought more about it, I concluded that changing the entire message consumer API would be too much of a burden for the many pika users who use the BlockingConnection adapter. So, I am going to bring back the legacy basic_consume/basic_cancel/consume/cancel API (but do it in a way that checks recursion) and get rid of the newget_event` mechanism.\nCC @andreas-kopecky\n. @andreas-kopecky: I liked it, too, but I think it would be too disruptive for many other users doing the switch to pika 0.10.0. The thinking is along the lines of\n\nDon't let perfect be the enemy of good\n. Gavin, I think I have the legacy consumer mechanism under control now. Will work on updating tests next as I find bits of time to work on it (on vacation now)\n. @gmr, upon first look, my take was that there should be a shared solution that works for all adapters versus one-off variation for Twisted. I haven't had the time to figure it out, though. Perhaps someone can jump in and help. @wjps? @ztane?\n. @gmr, I don't get any error at all when running python twisted_service.py from the examples folder. I tried with both 0.9.14 as well as tip of master and had no luck reproducing this issue. I am running MacOS X Yosemite, python 2.7.6; twisted 13.2.0. Also tried with twisted 15.2.1 (no error). Are you able to reproduce it?\n\n@gaochunzy, please describe your environment (OS, python version, twisted version, etc.), pika version, as well as how you run the twisted_service.py example to reproduce the problem. Also, attach the full log from running the failing example and the complete traceback that you encounter. You can initialize pika logging by adding the following at the top of twisted_service.py:\n```\nimport logging\nlogging.basicConfig(format='%(asctime)-15s %(message)s')\nlogging.getLogger('pika').setLevel(logging.DEBUG)\n``\n. Hmmm...twisted_service.pydoesn't seem to do anything when you run it aspython twisted_service.py. How is one supposed to runpython twisted_service.py` to get it to do something?\nOh - got it! it's twistd -ny twisted_service.py\n. Ok, so I am now able to reproduce the error by publishing to foobar exchange and routing key request1 or request2:\nvkruglikov@vkruglikovs-MacBook-Pro:~/ossdev/pika/examples (twisted)$ twistd -ny twisted_service.py\n2015-06-26 00:57:59,699 __builtin__ INFO Running twisted_service\n2015-06-26 00:57:59,699 __builtin__ INFO created service.Application\n2015-06-26 00:57:59,699 __builtin__ INFO PikaService.__init__\n2015-06-26 00:57:59,699 __builtin__ INFO Created PikaService\n2015-06-26 00:57:59,699 __builtin__ INFO called ps.setServiceParent\n2015-06-26 00:57:59,699 __builtin__ INFO created TestService\n2015-06-26 00:57:59,699 __builtin__ INFO called ts.setServiceParent\n2015-06-26 00:57:59-0700 [-] Log opened.\n2015-06-26 00:57:59-0700 [-] twistd 15.2.1 (/System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python 2.7.6) starting up.\n2015-06-26 00:57:59-0700 [-] reactor class: twisted.internet.selectreactor.SelectReactor.\n2015-06-26 00:57:59,701 __builtin__ INFO PikaService.startService\n2015-06-26 00:57:59,701 __builtin__ INFO PikaService.connect\n2015-06-26 00:57:59,701 __builtin__ INFO PikaFactory.__init__\n2015-06-26 00:57:59-0700 [-] Starting factory <__builtin__.PikaFactory instance at 0x10c7d5d40>\n2015-06-26 00:57:59,701 __builtin__ INFO PikaFactory.startedConnecting\n2015-06-26 00:57:59-0700 [AMQP:Factory] Started to connect.\n2015-06-26 00:57:59,701 __builtin__ INFO TestService.startService\n2015-06-26 00:57:59,701 __builtin__ INFO PikaService.getFactory\n2015-06-26 00:57:59,701 __builtin__ INFO PikaFactory.read_messages\n2015-06-26 00:57:59,701 __builtin__ INFO PikaFactory.read_messages\n2015-06-26 00:57:59,701 __builtin__ INFO PikaFactory.read_messages\n2015-06-26 00:57:59,703 __builtin__ INFO PikaFactory.buildProtocol\n2015-06-26 00:57:59-0700 [AMQP:Factory] Connected\n2015-06-26 00:57:59,704 pika.callback DEBUG Added: {'callback': <bound method PikaProtocol.connectionFailed of <PikaProtocol object at 0x10c7d6b10>>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-06-26 00:57:59,704 pika.callback DEBUG Added: {'callback': <bound method PikaProtocol.connectionReady of <PikaProtocol object at 0x10c7d6b10>>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-06-26 00:57:59,704 pika.callback DEBUG Added: {'callback': <bound method PikaProtocol._on_connection_start of <PikaProtocol object at 0x10c7d6b10>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,723 pika.callback DEBUG Processing 0:Connection.Start\n2015-06-26 00:57:59,724 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,724 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,724 pika.callback DEBUG Removing callback #0: {'callback': <bound method PikaProtocol._on_connection_start of <PikaProtocol object at 0x10c7d6b10>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,724 pika.callback DEBUG Calling <bound method PikaProtocol._on_connection_start of <PikaProtocol object at 0x10c7d6b10>> for \"0:Connection.Start\"\n2015-06-26 00:57:59,724 pika.callback DEBUG Added: {'callback': <bound method PikaProtocol._on_connection_tune of <PikaProtocol object at 0x10c7d6b10>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,725 pika.callback DEBUG Processing 0:Connection.Tune\n2015-06-26 00:57:59,725 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,725 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,725 pika.callback DEBUG Removing callback #0: {'callback': <bound method PikaProtocol._on_connection_tune of <PikaProtocol object at 0x10c7d6b10>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,725 pika.callback DEBUG Calling <bound method PikaProtocol._on_connection_tune of <PikaProtocol object at 0x10c7d6b10>> for \"0:Connection.Tune\"\n2015-06-26 00:57:59,725 pika.connection DEBUG Creating a HeartbeatChecker: 580\n2015-06-26 00:57:59,725 pika.callback DEBUG Added: {'callback': <bound method PikaProtocol._on_connection_open of <PikaProtocol object at 0x10c7d6b10>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,726 pika.callback DEBUG Processing 0:Connection.OpenOk\n2015-06-26 00:57:59,726 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,726 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,726 pika.callback DEBUG Removing callback #0: {'callback': <bound method PikaProtocol._on_connection_open of <PikaProtocol object at 0x10c7d6b10>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,726 pika.callback DEBUG Calling <bound method PikaProtocol._on_connection_open of <PikaProtocol object at 0x10c7d6b10>> for \"0:Connection.OpenOk\"\n2015-06-26 00:57:59,727 pika.callback DEBUG Added: {'callback': <bound method PikaProtocol._on_connection_closed of <PikaProtocol object at 0x10c7d6b10>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,727 pika.callback DEBUG Processing 0:_on_connection_open\n2015-06-26 00:57:59,727 pika.callback DEBUG Calling <bound method PikaProtocol.connectionReady of <PikaProtocol object at 0x10c7d6b10>> for \"0:_on_connection_open\"\n2015-06-26 00:57:59,727 __builtin__ INFO PikaProtocol.connected\n2015-06-26 00:57:59,727 pika.connection DEBUG Creating channel 1\n2015-06-26 00:57:59,727 pika.callback DEBUG Added: {'callback': <bound method PikaProtocol._on_channel_cleanup of <PikaProtocol object at 0x10c7d6b10>>, 'only': <pika.channel.Channel object at 0x10c7e0490>, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,727 pika.callback DEBUG Added: {'callback': <bound method Channel._on_getempty of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-06-26 00:57:59,727 pika.callback DEBUG Added: {'callback': <bound method Channel._on_cancel of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-06-26 00:57:59,727 pika.callback DEBUG Added: {'callback': <bound method Channel._on_flow of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': False, 'arguments': None}\n2015-06-26 00:57:59,727 pika.callback DEBUG Added: {'callback': <bound method Channel._on_close of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,727 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,728 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,728 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,728 pika.callback DEBUG Added: {'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,729 pika.callback DEBUG Processing 1:Channel.OpenOk\n2015-06-26 00:57:59,729 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,729 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,729 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,729 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,729 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,729 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,729 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Channel.OpenOk\"\n2015-06-26 00:57:59,729 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,729 pika.callback DEBUG Calling <bound method Channel._on_openok of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Channel.OpenOk\"\n2015-06-26 00:57:59,729 pika.callback DEBUG Added: {'callback': <bound method TwistedChannel.channel_closed of <pika.adapters.twisted_connection.TwistedChannel object at 0x10c7e03d0>>, 'only': <pika.channel.Channel object at 0x10c7e0490>, 'one_shot': False, 'arguments': None}\n2015-06-26 00:57:59,729 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,730 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,730 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,730 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e11b8>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,730 pika.callback DEBUG Processing 1:Basic.QosOk\n2015-06-26 00:57:59,730 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,730 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,731 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,731 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,731 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,731 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e11b8>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,731 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Basic.QosOk\"\n2015-06-26 00:57:59,731 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,731 pika.callback DEBUG Calling <function single_argument at 0x10c7e11b8> for \"1:Basic.QosOk\"\n2015-06-26 00:57:59,731 __builtin__ INFO PikaProtocol.setup_read: exchange=foobar; routing_key=request1, callback=<bound method TestService.respond of <__builtin__.TestService instance at 0x10c7d5b48>>\n2015-06-26 00:57:59,731 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,731 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,731 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,731 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e1230>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,732 pika.callback DEBUG Processing 1:Exchange.DeclareOk\n2015-06-26 00:57:59,732 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,732 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,732 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,732 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,732 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,732 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e1230>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,732 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Exchange.DeclareOk\"\n2015-06-26 00:57:59,732 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,732 pika.callback DEBUG Calling <function single_argument at 0x10c7e1230> for \"1:Exchange.DeclareOk\"\n2015-06-26 00:57:59,733 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,733 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,733 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,733 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e1140>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,735 pika.callback DEBUG Processing 1:Queue.DeclareOk\n2015-06-26 00:57:59,735 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,735 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,735 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,735 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,735 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,735 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e1140>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,735 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Queue.DeclareOk\"\n2015-06-26 00:57:59,735 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,735 pika.callback DEBUG Calling <function single_argument at 0x10c7e1140> for \"1:Queue.DeclareOk\"\n2015-06-26 00:57:59,735 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,735 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,735 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,735 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e1230>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,736 pika.callback DEBUG Processing 1:Queue.BindOk\n2015-06-26 00:57:59,736 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,737 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,737 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,737 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,737 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,737 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e1230>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,737 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Queue.BindOk\"\n2015-06-26 00:57:59,737 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,737 pika.callback DEBUG Calling <function single_argument at 0x10c7e1230> for \"1:Queue.BindOk\"\n2015-06-26 00:57:59,737 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,737 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.9dc0c17f24c7467d9706055db8ea4964'}, 'calls': 1}\n2015-06-26 00:57:59,737 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,737 pika.callback DEBUG Added: {'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.9dc0c17f24c7467d9706055db8ea4964'}, 'calls': 1}\n2015-06-26 00:57:59,737 __builtin__ INFO PikaProtocol.setup_read: exchange=foobar; routing_key=request2, callback=<bound method TestService.respond of <__builtin__.TestService instance at 0x10c7d5b48>>\n2015-06-26 00:57:59,738 pika.callback DEBUG Processing 1:Basic.ConsumeOk\n2015-06-26 00:57:59,738 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,738 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,738 pika.callback DEBUG Comparing {'consumer_tag': 'ctag1.9dc0c17f24c7467d9706055db8ea4964'} to {'consumer_tag': 'ctag1.9dc0c17f24c7467d9706055db8ea4964'}\n2015-06-26 00:57:59,738 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.9dc0c17f24c7467d9706055db8ea4964'}, 'calls': 0}\n2015-06-26 00:57:59,738 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,738 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,738 pika.callback DEBUG Comparing {'consumer_tag': 'ctag1.9dc0c17f24c7467d9706055db8ea4964'} to {'consumer_tag': 'ctag1.9dc0c17f24c7467d9706055db8ea4964'}\n2015-06-26 00:57:59,738 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.9dc0c17f24c7467d9706055db8ea4964'}, 'calls': 0}\n2015-06-26 00:57:59,739 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Basic.ConsumeOk\"\n2015-06-26 00:57:59,739 pika.channel DEBUG 1 blocked frames\n2015-06-26 00:57:59,739 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,739 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,739 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,739 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e1398>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,739 pika.callback DEBUG Calling <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Basic.ConsumeOk\"\n2015-06-26 00:57:59,739 pika.channel DEBUG Discarding frame <METHOD(['channel_number=1', 'frame_type=1', \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.9dc0c17f24c7467d9706055db8ea4964'])>\"])>\n2015-06-26 00:57:59,739 pika.callback DEBUG Processing 1:Exchange.DeclareOk\n2015-06-26 00:57:59,739 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,739 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,739 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,740 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,740 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,740 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e1398>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,740 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Exchange.DeclareOk\"\n2015-06-26 00:57:59,740 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,740 pika.callback DEBUG Calling <function single_argument at 0x10c7e1398> for \"1:Exchange.DeclareOk\"\n2015-06-26 00:57:59,740 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,740 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,740 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,740 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e1230>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,741 pika.callback DEBUG Processing 1:Queue.DeclareOk\n2015-06-26 00:57:59,741 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,742 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,742 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,742 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,742 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,742 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e1230>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,742 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Queue.DeclareOk\"\n2015-06-26 00:57:59,742 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,742 pika.callback DEBUG Calling <function single_argument at 0x10c7e1230> for \"1:Queue.DeclareOk\"\n2015-06-26 00:57:59,742 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,742 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,742 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,742 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e1320>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,743 pika.callback DEBUG Processing 1:Queue.BindOk\n2015-06-26 00:57:59,743 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,743 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,743 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,743 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,743 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,743 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e1320>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,743 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Queue.BindOk\"\n2015-06-26 00:57:59,744 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,744 pika.callback DEBUG Calling <function single_argument at 0x10c7e1320> for \"1:Queue.BindOk\"\n2015-06-26 00:57:59,744 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,744 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.fdb3979eb009418e8e615242ba32bc34'}, 'calls': 1}\n2015-06-26 00:57:59,744 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,744 pika.callback DEBUG Added: {'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.fdb3979eb009418e8e615242ba32bc34'}, 'calls': 1}\n2015-06-26 00:57:59,744 __builtin__ INFO PikaProtocol.setup_read: exchange=foobar; routing_key=request3, callback=<bound method TestService.respond of <__builtin__.TestService instance at 0x10c7d5b48>>\n2015-06-26 00:57:59,745 pika.callback DEBUG Processing 1:Basic.ConsumeOk\n2015-06-26 00:57:59,745 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,745 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,745 pika.callback DEBUG Comparing {'consumer_tag': 'ctag1.fdb3979eb009418e8e615242ba32bc34'} to {'consumer_tag': 'ctag1.fdb3979eb009418e8e615242ba32bc34'}\n2015-06-26 00:57:59,745 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.fdb3979eb009418e8e615242ba32bc34'}, 'calls': 0}\n2015-06-26 00:57:59,745 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,745 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,745 pika.callback DEBUG Comparing {'consumer_tag': 'ctag1.fdb3979eb009418e8e615242ba32bc34'} to {'consumer_tag': 'ctag1.fdb3979eb009418e8e615242ba32bc34'}\n2015-06-26 00:57:59,745 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.fdb3979eb009418e8e615242ba32bc34'}, 'calls': 0}\n2015-06-26 00:57:59,745 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Basic.ConsumeOk\"\n2015-06-26 00:57:59,745 pika.channel DEBUG 1 blocked frames\n2015-06-26 00:57:59,745 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,745 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,745 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,745 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e12a8>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,745 pika.callback DEBUG Calling <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Basic.ConsumeOk\"\n2015-06-26 00:57:59,746 pika.channel DEBUG Discarding frame <METHOD(['channel_number=1', 'frame_type=1', \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.fdb3979eb009418e8e615242ba32bc34'])>\"])>\n2015-06-26 00:57:59,746 pika.callback DEBUG Processing 1:Exchange.DeclareOk\n2015-06-26 00:57:59,746 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,746 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,746 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,746 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,746 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,746 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e12a8>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,746 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Exchange.DeclareOk\"\n2015-06-26 00:57:59,746 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,746 pika.callback DEBUG Calling <function single_argument at 0x10c7e12a8> for \"1:Exchange.DeclareOk\"\n2015-06-26 00:57:59,746 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,747 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,747 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,747 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e1398>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,748 pika.callback DEBUG Processing 1:Queue.DeclareOk\n2015-06-26 00:57:59,748 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,748 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,748 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,748 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,748 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,748 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e1398>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,748 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Queue.DeclareOk\"\n2015-06-26 00:57:59,748 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,748 pika.callback DEBUG Calling <function single_argument at 0x10c7e1398> for \"1:Queue.DeclareOk\"\n2015-06-26 00:57:59,749 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,749 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,749 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,749 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e11b8>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:57:59,749 pika.callback DEBUG Processing 1:Queue.BindOk\n2015-06-26 00:57:59,750 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,750 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,750 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,750 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,750 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,750 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e11b8>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:57:59,750 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Queue.BindOk\"\n2015-06-26 00:57:59,750 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,750 pika.callback DEBUG Calling <function single_argument at 0x10c7e11b8> for \"1:Queue.BindOk\"\n2015-06-26 00:57:59,750 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:57:59,750 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.16c48c86ed984deebc335a3b18e31da4'}, 'calls': 1}\n2015-06-26 00:57:59,750 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:57:59,750 pika.callback DEBUG Added: {'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.16c48c86ed984deebc335a3b18e31da4'}, 'calls': 1}\n2015-06-26 00:57:59,750 __builtin__ INFO PikaProtocol.send\n2015-06-26 00:57:59,751 pika.callback DEBUG Processing 1:Basic.ConsumeOk\n2015-06-26 00:57:59,751 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,751 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,751 pika.callback DEBUG Comparing {'consumer_tag': 'ctag1.16c48c86ed984deebc335a3b18e31da4'} to {'consumer_tag': 'ctag1.16c48c86ed984deebc335a3b18e31da4'}\n2015-06-26 00:57:59,751 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.16c48c86ed984deebc335a3b18e31da4'}, 'calls': 0}\n2015-06-26 00:57:59,751 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:57:59,751 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:57:59,751 pika.callback DEBUG Comparing {'consumer_tag': 'ctag1.16c48c86ed984deebc335a3b18e31da4'} to {'consumer_tag': 'ctag1.16c48c86ed984deebc335a3b18e31da4'}\n2015-06-26 00:57:59,751 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.16c48c86ed984deebc335a3b18e31da4'}, 'calls': 0}\n2015-06-26 00:57:59,751 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Basic.ConsumeOk\"\n2015-06-26 00:57:59,752 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:57:59,752 pika.callback DEBUG Calling <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Basic.ConsumeOk\"\n2015-06-26 00:57:59,752 pika.channel DEBUG Discarding frame <METHOD(['channel_number=1', 'frame_type=1', \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.16c48c86ed984deebc335a3b18e31da4'])>\"])>\n2015-06-26 00:59:37,532 __builtin__ INFO PikaProtocol._read_item\n2015-06-26 00:59:37-0700 [Pika:<=] foobar (request2): 'request2-message'\n2015-06-26 00:59:37,533 __builtin__ INFO TestService.respond\n2015-06-26 00:59:37,533 __builtin__ INFO PikaFactory.send_message\n2015-06-26 00:59:37,533 __builtin__ INFO PikaProtocol.send\n2015-06-26 00:59:37,533 __builtin__ INFO PikaProtocol.send_message\n2015-06-26 00:59:37-0700 [Pika:=>] foobar (response): 'request2-message'\n2015-06-26 00:59:37,533 pika.channel DEBUG Adding in on_synchronous_complete callback\n2015-06-26 00:59:37,533 pika.callback DEBUG Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:59:37,533 pika.channel DEBUG Adding passed in callback\n2015-06-26 00:59:37,533 pika.callback DEBUG Added: {'callback': <function single_argument at 0x10c7e12a8>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2015-06-26 00:59:37,534 pika.callback DEBUG Processing 1:Exchange.DeclareOk\n2015-06-26 00:59:37,534 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:59:37,534 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:59:37,534 pika.callback DEBUG Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:59:37,534 pika.callback DEBUG Processing use of oneshot callback\n2015-06-26 00:59:37,534 pika.callback DEBUG 0 registered uses left\n2015-06-26 00:59:37,535 pika.callback DEBUG Removing callback #0: {'callback': <function single_argument at 0x10c7e12a8>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\n2015-06-26 00:59:37,535 pika.callback DEBUG Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10c7e0490>> for \"1:Exchange.DeclareOk\"\n2015-06-26 00:59:37,535 pika.channel DEBUG 0 blocked frames\n2015-06-26 00:59:37,535 pika.callback DEBUG Calling <function single_argument at 0x10c7e12a8> for \"1:Exchange.DeclareOk\"\n2015-06-26 00:59:37,535 __builtin__ ERROR basic_publish failed\nTraceback (most recent call last):\n  File \"twisted_service.py\", line 138, in send_message\n    yield self.channel.basic_publish(exchange=exchange, routing_key=routing_key, body=msg, properties=prop)\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/twisted_connection.py\", line 131, in basic_publish\n    return defer.succeed(self.__channel.basic_publish(*args, **kwargs))\n  File \"/Users/vkruglikov/ossdev/pika/pika/channel.py\", line 338, in basic_publish\n    (properties, body))\n  File \"/Users/vkruglikov/ossdev/pika/pika/channel.py\", line 1150, in _send_method\n    self.connection._send_method(self.channel_number, method_frame, content)\n  File \"/Users/vkruglikov/ossdev/pika/pika/connection.py\", line 1570, in _send_method\n    self._send_message(channel_number, method_frame, content)\n  File \"/Users/vkruglikov/ossdev/pika/pika/connection.py\", line 1600, in _send_message\n    self._flush_outbound()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py\", line 282, in _flush_outbound\n    self._handle_write()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py\", line 427, in _handle_write\n    bw = self.socket.send(frame)\nAttributeError: 'NoneType' object has no attribute 'send'\n2015-06-26 00:59:37-0700 [AMQP:Protocol] Error while sending message: 'NoneType' object has no attribute 'send'\n. @gmr: I am finding that the twisted_service.py example worked in 0.9.14, but started failing in master at or before 409670bdbea13d11a43d3d02162d49a200e450a5. If I run the example from 409670bdbea13d11a43d3d02162d49a200e450a5 against 0.9.14, all is good. However, if I run the same example against pika installed from master at 409670bdbea13d11a43d3d02162d49a200e450a5, then I get a failure that is similar (connection.socket=None) to the one that we get against tip of master;\nSo, the error occurred after f8c263f234cca6b0f573ac63268a4034e32bd3eb (0.9.14) and up to 409670bdbea13d11a43d3d02162d49a200e450a5 \n015-06-26 01:24:23,389 pika.callback DEBUG Calling <function single_argument at 0x108973230> for \"1:Exchange.DeclareOk\"\n2015-06-26 01:24:23,390 __builtin__ ERROR basic_publish failed\nTraceback (most recent call last):\n  File \"twisted_service.py\", line 125, in send_message\n    yield self.channel.basic_publish(exchange=exchange, routing_key=routing_key, body=msg, properties=prop)\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/twisted_connection.py\", line 131, in basic_publish\n    return defer.succeed(self.__channel.basic_publish(*args, **kwargs))\n  File \"/Users/vkruglikov/ossdev/pika/pika/channel.py\", line 300, in basic_publish\n    (properties, body))\n  File \"/Users/vkruglikov/ossdev/pika/pika/channel.py\", line 1057, in _send_method\n    self.connection._send_method(self.channel_number, method_frame, content)\n  File \"/Users/vkruglikov/ossdev/pika/pika/connection.py\", line 1510, in _send_method\n    self._send_message(channel_number, method_frame, content)\n  File \"/Users/vkruglikov/ossdev/pika/pika/connection.py\", line 1540, in _send_message\n    self._flush_outbound()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py\", line 243, in _flush_outbound\n    self._manage_event_state()\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py\", line 389, in _manage_event_state\n    self.ioloop.update_handler(self.socket.fileno(),\nAttributeError: 'NoneType' object has no attribute 'fileno'\n2015-06-26 01:24:23-0700 [AMQP:Protocol] Error while sending message: 'NoneType' object has no attribute 'fileno'\n. @gmr, the above and the fact that all tests are passing would suggest that there isn't an adequate test for the twisted adapter that would catch this error (or the test is skipped).\n. @gmr: Running git bisect from f8c263f (good) to 409670b (bad) finds the first bad commit:\n```\nvkruglikov@vkruglikovs-MacBook-Pro:~/ossdev/pika ((de8b545...)|BISECTING)$ git bisect bad\nde8b5450799c08e590b34a661ae424c4a623df23 is the first bad commit\ncommit de8b5450799c08e590b34a661ae424c4a623df23\nAuthor: Gavin M. Roy gavinr@aweber.com\nDate:   Mon Oct 13 15:28:01 2014 -0400\nEnsure frames can not be interspersed on send\n\nAddress #507 by adding a write lock and making message sending append to the output buffer directly instead of frame-by-frame.\n\n:040000 040000 d23ec256dd8f421685bfb4e83df5f576fa0184dd 6cca9a230ad13f8c1cfa7e528b3cf802f1f54a37 M  pika\n```\nAgain, any fix needs to be accompanied by a twisted adapter test that would catch this issue.\n. @gaochunzy (CC @gmr), I found a better existing integration point that eliminates the need for duplicating the \"business logic\". Please do the following (this will help a lot, and will be much appreciated):\n1. Reset your current changes (perhaps, just close this PR, sync with master, and start fresh)\n2. Fix the pika tests to catch this this bug; verify that the test fails\n3. Remove TwistedProtocolConnection._send_frame.\n4. Override BaseConnection._flush_outbound(self) in TwistedProtocolConnection and implement it as something like this:\ndef _flush_outbound(self):\n             while self.outbound_buffer:\n                 marshaled_frame = self.outbound_buffer.popleft()\n                 self.bytes_sent += len(marshaled_frame)\n                 self.frames_sent += 1\n                 self.transport.write(marshaled_frame)\n5. Debug; verify that the test succeeds.\n6. Squash all your changes into a single commit, rebased against upstream/master. It helps a lot to have fewer commits when doing git bisect, etc.\nThis should do it.\n. Thank you, closing on the account of user's confirmation of the fix.\n. +1\n. The project could really use more contributions from the community to improve the test coverage situation.\nI added a bunch of BlockingConnection acceptance tests over the weekend, but much more is needed in the various adapters and core code, including extensive unit tests.\n. @ztane, this wsa fixed by PR #672\n. fixed by PR #672\n. Good catch! This actually applies to the poll methods in all of the pollers. That section of code in all of the poll methods should either be written in a loop or it should allow all exceptions to be raised and then have a decorator to retry on EINTR. As a loop, it would look something like this \"traditional\" implementation:\nwhile True:\n        try:\n            read, write, error = select.select(self._fd_events[READ],\n                                               self._fd_events[WRITE],\n                                               self._fd_events[ERROR],\n                                               self.get_next_deadline())\n            break\n        except select.error as error:\n            if error.errno == errno.EINTR:\n                continue\n            else:\n                raise\nCC @wjps \n. And, come to think of it, BaseConnection._handle_read and BaseConnection._handle_write also need a loop like that around their socket.recv and socket.send calls.\nCC @ztane @wjps \n. PR #604 should fix this issue\n. Updated PR #604 to address @ztane's code review feedback.\n. Socket closed when connection was open should be gone with PR #685\n. @ThomasKliszowski, changes have been made in pika's connection-management and error-handling. Would you mind trying this out against the current pika master and posting your findings here?\n. +1\nVery clean! I love it how the usage is now very intuitive and the spec.py code is a lot more compact.\n. @everilae: thanks for all the hard work.\n. @everilae, after PR #602 got merged, there are now some merge conflicts in your PR that need to be resolved. Would you mind resolving those merge conflicts, rebasing against upstream/master and squashing your changes into a single commit? Also, the merged PR #602 includes a number of new blocking adapter acceptance tests that had to use the as_bytes and _p3_as_bytes that need to be removed when merging with your changes. I tried to preserve the existing blocking adapter test code as much as possible to make it easier to merge. I hope it's not too bad. Thanks!\n. Gavin, this PR gets pika back to being able to deal with exchange/queue/routing_key names intuitively as strings/unicode in both Python 2.x and 3.x. Please consider merging after the above-described merge conflict and test changes are resolved. Thx.\n. Thanks, @everilae! What would be the correct workflow to handle the merge/rebase in this case? I don't have a good handle on this workflow, yet.\n. +1 \n. thx for the merge/rebase info, @everilae \n. @everilae, what would be an example of float or double from the broker that pika would be decoding? Thx.\n. @everilae, I am not finding any references to actual usage of floats, doubles, and decimals in 0.9.1 AMQP specs. It sound like they could easily show up in something like the headers table of BasicProperties, though.\n. I am going to be mostly without access to Internet for the next week or so. I will check in when I can.\n. @everilae, where are we with this one? Thanks.\n. @everilae: At some point, @gmr suggested switching to https://github.com/gmr/pamqp/tree/master/pamqp, which apparently deals with PY3, floats/doubles, and the like.\n. Is pika limited to RabbitMQ? Is pika expected to work with other AMQP brokers besides RabbitMQ? If it is, then there might compatibility issues as well based on @everilae's comments above.\n. Thanks for looking after this.\n. Gavin, please don't merge until tests are passing.\n. I am going to rebase and squash shortly.\n. Hi Gavin, I think that this PR is ready to merge. It's rebased against master and squashed, and all tests pass now.\n. @fgrim, @gmr: I can confirm that AsyncoreConnection is failing to connect and is failing to notify user of the connection failure on python 2.7.6 running on Mac OS X Yosemite. My RabbitMQ is v3.5.1. I see this failure with both pika 0.9.14 as well as the master branch. \nHowever, I don't think that the failure is because pika is not respecting negotiated data sizes. The failure is happening during the Connection.Open/Connection.Open-ok cycle, and I doubt that a very large data frame would be sent at that time. It smells like some sort of data corruption.\nTo make matters worse, since AsyncoreConnection is losing connection with broker immediately after Connection.Start-ok, but is not informing the user of the failure (no exception or anything), the AsyncoreConnection tests are most likely failing silently, fooling the build system that they passed. I noticed a few seconds of delay when AsyncroreConnection acceptance tests are running. This must be the same delay of a few seconds that I see when the code snippet below fails. ProbableAuthenticationError is most likely getting suppressed by the asyncore.dispatcher!\nSo, it's very likely that AsyncoreConnection has been broken since at least 0.9.14, but the breakage has been missed by the tests and the build system.\n- Given that this is the first complaint about the breakage in AsyncoreConnection, it appears that this adapter is not used much any longer. Can it be removed from pika?\nI borrowed the following connection logic from the async acceptance tests and applied it to AsyncoreConnection. It fails every time.\n```\nimport logging\nimport pika\nLOG_FORMAT = ('%(levelname) -10s %(asctime)s %(name) -30s %(funcName) '\n              '-35s %(lineno) -5d: %(message)s')\nconnection = None\ndef run():\n  global connection\n  connection = pika.AsyncoreConnection(\n    pika.URLParameters('amqp://guest:guest@localhost:5672/%2f'),\n    on_connection_open,\n    on_connection_open_error,\n    on_connection_closed)\nconnection.add_timeout(15,\n                         on_timeout)\nconnection.ioloop.start()\ndef on_connection_open(conn):\n  print \"connected!\"\ndef on_connection_open_error(conn):\n  conn.ioloop.stop()\n  raise AssertionError('Error connecting to RabbitMQ')\ndef on_connection_closed(conn, reply_code, reply_text):\n  \"\"\"called when the connection has finished closing\"\"\"\n  print \"Connection Closed!\"\ndef on_timeout():\n  print \"Timer expired, closing!\"\n  connection.close()\ndef main():\n  logging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT)\nrun()\nif name == 'main':\n  main()\n```\nThe client log looks like this:\nDEBUG      2015-06-21 21:49:08,914 pika.callback                  add                                  164 : Added: {'callback': <function on_connection_open_error at 0x1042d5c80>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-06-21 21:49:08,914 pika.callback                  add                                  164 : Added: {'callback': <function on_connection_open at 0x1042d5c08>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-06-21 21:49:08,914 pika.callback                  add                                  164 : Added: {'callback': <function on_connection_closed at 0x1042d5cf8>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-06-21 21:49:08,914 pika.callback                  add                                  164 : Added: {'callback': <bound method AsyncoreConnection._on_connection_start of <pika.adapters.asyncore_connection.AsyncoreConnection object at 0x1042cf910>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nINFO       2015-06-21 21:49:08,916 pika.adapters.base_connection  _create_and_connect_to_socket        212 : Connecting to ::1:5672\nWARNING    2015-06-21 21:49:08,916 pika.adapters.base_connection  _create_and_connect_to_socket        227 : Connection to ::1:5672 failed: [Errno 61] Connection refused\nINFO       2015-06-21 21:49:08,916 pika.adapters.base_connection  _create_and_connect_to_socket        212 : Connecting to 127.0.0.1:5672\nDEBUG      2015-06-21 21:49:08,916 pika.adapters.asyncore_connection start                                85  : Starting IOLoop\nDEBUG      2015-06-21 21:49:08,935 pika.callback                  process                              220 : Processing 0:Connection.Start\nDEBUG      2015-06-21 21:49:08,936 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-06-21 21:49:08,936 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-06-21 21:49:08,936 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method AsyncoreConnection._on_connection_start of <pika.adapters.asyncore_connection.AsyncoreConnection object at 0x1042cf910>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2015-06-21 21:49:08,936 pika.callback                  process                              234 : Calling <bound method AsyncoreConnection._on_connection_start of <pika.adapters.asyncore_connection.AsyncoreConnection object at 0x1042cf910>> for \"0:Connection.Start\"\nDEBUG      2015-06-21 21:49:08,936 pika.callback                  add                                  164 : Added: {'callback': <bound method AsyncoreConnection._on_connection_tune of <pika.adapters.asyncore_connection.AsyncoreConnection object at 0x1042cf910>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n^[[A^[[AERROR      2015-06-21 21:49:11,940 pika.adapters.base_connection  _handle_read                         411 : Read empty data, calling disconnect\nERROR      2015-06-21 21:49:11,941 pika.adapters.base_connection  _check_state_on_disconnect           171 : Socket closed while authenticating indicating a probable authentication error\nDEBUG      2015-06-21 21:49:11,941 pika.adapters.asyncore_connection stop                                 89  : Stopping IOLoop\nDEBUG      2015-06-21 21:49:11,941 pika.callback                  add                                  164 : Added: {'callback': <bound method AsyncoreConnection._on_connection_start of <pika.adapters.asyncore_connection.AsyncoreConnection object at 0x1042cf910>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nerror: uncaptured python exception, closing channel <pika.adapters.asyncore_connection.PikaDispatcher 127.0.0.1:5672 at 0x1042d4a70> (<class 'pika.exceptions.ProbableAuthenticationError'>: [/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/asyncore.py|read|83] [/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/asyncore.py|handle_read_event|449] [/Users/vkruglikov/ossdev/pika/pika/adapters/asyncore_connection.py|handle_read|61] [/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py|_handle_events|364] [/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py|_handle_read|412] [/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py|_handle_disconnect|288] [/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py|_adapter_disconnect|154] [/Users/vkruglikov/ossdev/pika/pika/adapters/base_connection.py|_check_state_on_disconnect|173])\nvkruglikov@vkruglikovs-MacBook-Pro:~/TempOnDesktop$ echo $?\n0\n. Hi Gavin, if the AsyncoreConnection adapter is not widely used, as might be evidenced by the lack of earlier feedback about this critical failure, could this adapter be removed from the pika package altogether to reduce the maintenance burden?\n. PR #607 removes the AsyncoreConnection adapter from pika (and addresses this issue)\n. Hi Gavin, this issue can be closed since we retired AsyncoreConnection.\n. @ztane, this should address #596. Please review.\n. CC @wjps \n. Thank you for the feedback, @ztane. I fixed things up according to python documentation, whereby the socket and select base exception classes socket.error and select.error were replaced by OSError in python 3.3. select.error.args[0] is used to access errno in python 2.6/7.\n. @gmr, I think we're good to go with this PR. Thx.\n. @beruic, is this in theory or during actual use of connection.py from master? The reason that I ask is that connection.py imports basestring from pika.compat, which redefines basestring here https://github.com/pika/pika/blob/9f7f24331c15ad8775d97b750f05322129b5c632/pika/compat.py#L14 for python 3.x as basestring = (str,)\nCC @everilae\n. @beruic, would you mind helping debug why basestring from pika.compat is not having the desired effect?\n. @beruic, please add a comment with a complete traceback from this failure.\n. Do you have an import in your pika/pika/connection.py that is exactly like this the following?\nfrom pika.compat import basestring, url_unquote, dictkeys\n. @beruic, I suspect that the pika package that you're using is not pika from the tip of master that recently got python 3.x support. You're probably executing an older version of pika, perhaps inadvertently. When I need to uninstall an old package, I run the pip uninstall <package name> command multiple times until pip tells me that it can't find the package. This gets rid of it from multiple places, such as the user installation and system installation that I wasn't even aware existed, etc. Once I get rid of all of them, then I go ahead and install the new one. This avoids conflicts.\n. @beruic, please close this issue. Python 3.x support is a new feature for the next release.\n. Hi Gavin, given that there are so many changes slated for the 0.10.0 release, I thought I'd introduce a CHANGELOG into the package. It describes the changes in BlockingConnection adapter as well as several core changes that I was familiar with made by other contributors.\n. Thanks for the analysis @everilae - your conclusion is 100% spot on!\n. @everilae, this should be fixed by PR #610\n. @everilae, the fix PR #610 has been merged into master. Would you mind verifying the fix and closing this issue? Thx!\n. Thank you\n. @everilae, this should address the race condition that you discovered in blocking adapter acceptance tests per #609.  I was not able to reproduce the error on my MacBookPro, but was able to confirm presence of race condition upon examination of the test's code.\n. @gaochunzy, I reviewed your PR. It still had one conflicting method _send_frame that needed to be removed and it turns out that _flush_outbound can even be simpler than I thought initially. I submitted PR #614  as an example for you of what needs to be done to fix the bug.\nPlease start with my example PR #614 and also add a basic acceptance test in tests/acceptance/twisted_protocol_connection_test.py (new test script) for TwistedProtocolConnection that at least connects, creates a channel, creates a queue, publishes a message to that queue, consumes the message from the queue and validates its contents, deletes the queue. Thx!\nWe don't need _send_frame and _send_message in TwistedProtocolConnection . We only need _flush_outbound.\n. @gmr: This example PR is the correct fix for #613. #613 is incomplete. Feel free to merge this PR and close #613.\nNotable is that there aren't any tests for the Twisted adapter. I filed issues #615 and #616 for adding the tests, so hopefully someone proficient with Twisted can pick those up.\n. Is there a Twisted user in the community that could take on this task?\n. Is there a Twisted user in the community that could take on this task?\n. @gmr: API work in BlockingConnection and BlockingChannel should be complete as of this PR.\n. @mpolk, SelectConnection's ioloop.stop is thread-safe as of pika v0.10.0. Please verify.\n. Per my earlier comment, SelectConnection's ioloop.stop is thread-safe as of pika v0.10.0, so closing the issue. Thank you for reporting.\n. @skftn: thanks for your contribution. Would you mind contributing some basic acceptance tests for Twisted connection? See https://github.com/pika/pika/blob/master/tests/acceptance/blocking_adapter_test.py as an example. Even just a handful of tests (e.g., connect, create channel, publish, get, consume) would be great. There aren't any tests at all for Twisted right now. Many thanks in advance!\n. @skftn: I was thinking about acceptance tests for twisted adapters, rather than unit tests, just to get started. The project uses the python unittest framework for all its tests - both unit and acceptance.\nHere is an example of a basic test script from python unittest framework; the test method names need to begin with \"test_\" to be recognized by the framework; you can have helper/private methods with names that don't begin with \"test_\"; it's desirable to use test method names that are descriptive of what's being tested (long names are okay in these tests).\n```\nimport unittest\nclass TestStringMethods(unittest.TestCase):\ndef test_upper(self):\n      self.assertEqual('foo'.upper(), 'FOO')\ndef test_isupper(self):\n      self.assertTrue('FOO'.isupper())\n      self.assertFalse('Foo'.isupper())\ndef test_split(self):\n      s = 'hello world'\n      self.assertEqual(s.split(), ['hello', 'world'])\n      # check that s.split fails when the separator is not a string\n      with self.assertRaises(TypeError):\n          s.split(2)\nif name == 'main':\n    unittest.main()\n```\nDefinitely use unittest's specialized self.assert* methods rather than python's builtin assert. Unittest's specialized assertion methods output useful info when assertions fail.\nThe framework instantiates a fresh instance of the test class for each test method. Don't define a constructor in your test class; if you need to initialize something common for each test method, then override the def setUp(self) framework \"fixture\" method and do it there.\nThere is a lot of flexibility for structuring these tests: you can have a dedicated test class for each test method (as in https://github.com/pika/pika/blob/master/tests/acceptance/blocking_adapter_test.py) or multiple test methods per test class (as in the above example). It's up to you.\nThe Twisted adapter to be tested is in https://github.com/pika/pika/blob/master/pika/adapters/twisted_connection.py. There are actually two adapters there: TwistedConnection and TwistedProtocolConnection. Ideally, both need to be tested, but practically just start with the one that you're most comfortable with. You would create tests for TwistedConnection in pika/tests/acceptance/twisted_connection_test.py and similarly for TwistedProtocolConnection in  pika/tests/acceptance/twisted_protocol_connection_test.py.\nAt the beginning of each test method, register a test timeout timer of 10 seconds with twisted ioloop that invokes a method in your test class that in turn calls self.fail('test timed out'). In the unittest framework, the setUp \"fixture\" is a handy way to do it for each test method if you will have multiple test methods in a single test class. setUp instance method gets called by the framework just before each test method. For example:\n```\nclass _TwistedTestCaseBase(unittest.unittest.TestCase):\n    def setUp(self):\n        \"\"\"This method is called by unittest framework on a fresh test class instance just before calling\n        a test method; a fresh test class instance is constructed by the framework for each test method\n        \"\"\"\n        # 1. register a 10-second test timeout timer with twisted ioloop that will call _on_test_timed_out;\n        #     this should be the same ioloop that will be used by the test code.\n        # 2. call self.addCleanup to schedule cancellation of the test timeout timer; the cleanup\n        #     callback will be called by the unittest framework after the test method exits.\ndef _on_test_timed_out(self):\n    self.fail('test timed out: %r' % (self,))\n\nclass TestTwistedConnection(_TwistedTestCaseBase):\n    ... your test method(s) here\n```\nEach test is a granular basic usage workflow; e.g.,\n1. test method test_connect_and_close: initiate the connection sequence, wait for the expected callback (or equivalent) that indicates successful connection establishment; if you get the \"connection closed\" or \"connection error\" callback while waiting for connection to complete, you need to fail the test by calling self.fail('some useful message'); after connection establishment completes successfully, initiate connection close and wait for the corresponding connection-closed callback; once you get the \"connection closed\" callback, then do what's needed in twisted to allow the ioloop to unblock so that the test method may return.\n2. test method test_open_channel_and_close: similarly to test_connect_and_close, create connection and open channel; verify that the expected callbacks (or equivalent) were called and fail if an unexpected callback happens first; close the channel and verify the expected callback. Allow the test method to return.\n3. test method test_publish: open connection and channel; create a queue (use self.addCleanup to register a callback that deletes that queue); publish a message (exchante='', routing_key='queue name'); read the message back and verify the body and other attributes of the message; using basic_get is probably the simplest way to read the message in this test method.\n4. test method test_basic_consume: open connection and channel; create  queue;  publish a message; use basic_consume to consume the message; verify message body and other attributes.\nhttps://github.com/pika/pika/blob/master/tests/acceptance/blocking_adapter_test.py is an example of an acceptance test suite for the pika blocking adapter. You can find comparable test methods there and perhaps borrow the code for message validation, etc. Note the use of self.addCleanup immediately after creation of resources to schedule their destruction and the use of uuid to avoid collisions, especially when tests are executed in parallel. The self.addCleanup callbacks are called by the test framework after the test method exits. self.addCleanup is also very handy for scheduling things to be unregistered at the end of each test method's execution: for example, this would be handy for canceling the test timeout timer.\n. @skftn: issue #616 is tracking acceptance tests for the pika Twisted adapter\n. @pfreixes, you're probably using the official 0.9.14 release. You can work around it by forcing the flushing of output buffers via a synchronous command such as a passive queue declare. This problem should be fixed in pika master, which will be part of pika 0.10.0. Please try it from pika master.\n. @pfreixes: Thanks for the info; I will look into it later today.\n. @pfreixes, here is what must be happening, and it's not a bug in pika: When you publish a bunch of messages without publisher acknowledgements, they get queued up in the socket buffer; after the app exits, the operating system (kernel) continues to deliver the content of the socket buffer to the destination (RabbitMQ broker); it will take some time to flush the socket buffer.\nTry calling channel.close() instead of time.sleep(10); the channel.close() call will synchronously flush the socket buffer and complete when Channel.CloseOK is received from the broker.\n. Gavin, to be true to the blocking nomenclature, the new BlockingConnection/BlockingChannel implementation always flushes Connection.outbound_buffer to the socket before returning control from any of the sending methods. In @pfreixes's case, many of the messages were still in the kernel's socket buffer and should eventually reach RabbitMQ broker, even if the app exited immediately after stuffing them all into the socket.\n@pfreixes, if you block somewhere in your code outside of pika (or call time.sleep(x), which is equivalent), then things like heartbeat processing will not happen in BlockingConnection and RabbitMQ will eventually drop your connection (depending on heartbeat settings). If you need to call time.sleep(x), you may call BlockingConnection.sleep(x) instead to permit pika to process heartbeats, etc. And there is also BlockingConnection.process_data_events(...) that you may call periodically, if needed.\n. For example, if you execute the following script, all 100,000 messages will eventually show up in the queue \"test\" (assuming it exists). However, I personally prefer to gracefully close the channel/connection when I no longer need them.\n```\nfrom pika import BlockingConnection\ndef main():\n  ch = BlockingConnection().channel()\nfor i in xrange(100000):\n    ch.basic_publish(\"\", routing_key=\"test\", body=\"message-%s\" % (i,))\nif name == \"main\":\n  main()\n``\n. Got it, thanks for the clarification.\n. @pfreixes, the side-effects that you're seeing are normal for how the operating system supports TCP/IP sockets. Immediately afterpython publisher_2.pycompletes, some of the messages are still in the kernel's socket buffer, so RabbitMQ hasn't received them all yet, and it most likely explains why you see 221 instead of 0. When you call./purge.sh`, it purges only what's in the queues at this instant, but the remaining messages from kernel's socket buffer continue to be delivered to RabbitMQ, which most likely explains your 503 result above.\nTry waiting a few seconds after calling python publisher_2.py. Start calling ./rabbitmqadmin.py, etc. after several seconds elapsed. If you still don't get the expected results, please try to debug the issue. If you still need help with it, please create a complete simple-as-possible test that reproduces the problem that you're seeing and attach the test to this github issue.\n. @pfreixes, thanks for the info. This sounds like it could be an issue in RabbitMQ. Would you mind posting it on rabbitmq-users google group? \n. Thanks for your follow-up @pfreixes!\n. Hi Gavin: These changes need to be in the 0.10.0 release; please merge when you get the chance so they can get tested by the community. thx.\nAlso, please note that there is a change in CHANGELOG.rst regarding BlockingChannel.confirm_delivery.\n. Thx!\n. @happybob007: this is a know issue with the legacy blocking client. The blocking client has been completely rewritten, and this should no longer be an issue in version 0.10.0. The prerelease version of 0.10.0 is on PyPi already - please give it a try. You should see a noticeable performance boost there as well, particularly with basic_publish. I install the prerelease on my MacBook like this: pip install --pre --user pika>=0.10.0.\n. Fixed in pika 0.10.0. Please close this issue.\n. @reddec, please fix the issues noted in other comments and add tests for your change, before this PR may be merged. We need tests along with PR's to maintain quality of the adapter. The tests need to confirm that:\n1. the connection was indeed closed for non-system exception and that the exception is not suppressed\n2. the connection is not closed for system exception (one that is not derived from Exception; e.g., SystemExit) and  that the exception is not suppressed\n. Also, please add a test for this in tests/acceptance/blocking_adapter_test.py near the other connection tests at the top. This could be something as simple as:\n```\nclass TestConnectionContextManagerClosesConnection(BlockingTestCaseBase):\n    def test(self):\n        \"\"\"BlockingConnection: connection context manager closes connection\"\"\"\n        with self._connect() as connection:\n            self.assertIsInstance(connection, pika.BlockingConnection)\n            self.assertTrue(connection.is_open)\n    self.assertTrue(connection.is_closed)\n\nclass TestConnectionContextManagerClosesConnectionAndPassesOriginalException(BlockingTestCaseBase):\n    def test(self):\n        \"\"\"BlockingConnection: connection context manager closes connection and passes original exception\"\"\"\n        class MyException(Exception):\n            pass\n    with self.assertRaises(MyException):\n        with self._connect() as connection:\n            self.assertTrue(connection.is_open)\n\n            raise MyException()\n\n    self.assertTrue(connection.is_closed)\n\n```\n. Thanks @reddec, the code looks good. Please rebase against master and squash into a single commit to prepare for merging.\n. @markunsworth: what if the user needs to pass fqdn?\n. @markunsworth: please add tests to your PR.\n. Looks good to me, thanks!\n. @gmr: I am concerned about #637 - I think @shinji-s might have something there with the EINTR issue on Ubuntu. If it's true, it would be good to get #637 into the release. I gave him an idea on how to write the EINTR test, so I am hoping this will happen soon.\n. @shinji-s - thanks for the info about the timeout error.\nHowever, the IOError commit caf0e4e0148a029fb212264f11f718ecc9f206d6 is incorrect. Let's deal with select errors in a separate PR, where you will need to add tests for each of SelectConnection's poller types (select, kqueue, poll, epoll) to validate that your change does the correct thing. We can't go on belief alone. One reason why caf0e4e0148a029fb212264f11f718ecc9f206d6 is incorrect is that on Python 2.7, select.select raises select.error, not IOError:\n```\nPython 2.7.6 (default, Sep  9 2014, 15:04:36) \nType \"copyright\", \"credits\" or \"license\" for more information.\nIPython 1.1.0 -- An enhanced Interactive Python.\n?         -> Introduction and overview of IPython's features.\n%quickref -> Quick reference.\nhelp      -> Python's own help system.\nobject?   -> Details about 'object', use 'object??' for extra details.\nIn [1]: import select\nIn [2]: try:       \n    select.select([99], [], [])\nexcept Exception as e:\n    raise\n   ...: \n\nerror                                     Traceback (most recent call last)\n in ()\n      1 try:\n----> 2     select.select([99], [], [])\n      3 except Exception as e:\n      4     raise\n      5 \nerror: (9, 'Bad file descriptor')\nIn [3]: repr(e)\nOut[3]: \"error(9, 'Bad file descriptor')\"\nIn [4]: type(e)\nOut[4]: select.error\nIn [5]: e.class.bases\nOut[5]: (Exception,)\n```\nAs you can see, it's not IOError. However, things may be different with select.kqueue, select.poll, and select.epoll (maybe?). That's why you need to do some more work on it and submit it in a separate PR along with tests for each poller type that verify the fix.\n. @shinji-s: to summarize, please:\n1. Remove the select/IOError commit from this PR, since it's incorrect (as described in my earlier comment), and also so we can focus on fixing a single problem.\n2. Address feedback https://github.com/pika/pika/pull/630#discussion_r35355729 regarding checking if k in self._timeouts: before calling t['callback']().\n3. Add acceptance tests that verify your timer fix. These tests probably belong in the async adapter test suite under tests/acceptance/async_*. We need tests before a PR may be merged.\n. Thank you @shinji-s!\n. @billbr, this should be addressed by PR #685\n. @shinji-s: thanks for your latest round of changes and for implementing additional tests. Please see my new feedback.\n. @shinji-s: just a couple of nits, otherwise ready to go.\n. LGTM\nThanks @shinji-s !\n. @pfreixes: We took care of some of the connection cleanup issues in the 0.10.0 pre-release.\n@gmr: Looking at the implementation of HeartbeatChecker._close_connection below, it looks like there are a few potential problems there as well:\n1. It doesn't take continuation-style adapters into account: when it calls self._connection.close(...), that operation will not complete in continuation-style adapters before self._connection.close returns. This is because the graceful close operation needs to send message(s) to the broker and wait for response(s), which won't complete in the context of that function.\n2.  Since the heartbeat checker has determined that there is a networking issue, there is probably no value in attempting to perfrom the graceful connection close operation.\n3. The call to the private method self._connection._on_disconnect seems out of place here. There should probably be a reasonable code path in the Connection class that's responsible for this chunk of the adapter's business logic.\ndef _close_connection(self):\n        \"\"\"Close the connection with the AMQP Connection-Forced value.\"\"\"\n        LOGGER.info('Connection is idle, %i stale byte intervals',\n                    self._idle_byte_intervals)\n        duration = self._max_idle_count * self._interval\n        text = HeartbeatChecker._STALE_CONNECTION % duration\n        self._connection.close(HeartbeatChecker._CONNECTION_FORCED, text)\n        self._connection._adapter_disconnect()\n        self._connection._on_disconnect(HeartbeatChecker._CONNECTION_FORCED,\n                                        text)\n. @pfreixes and @gmr: The closing of the connection in this scenario should be addressed by PR #685\n. @bobcyw, the traceback that you provided suggests that you're using pika 0.9.14 or older with Python 3.x. Python 3.x compatibility was added in pika 0.10.0, which is presently available as a pre-release on PyPi. I install the pre-release on my system using pip install pika --pre --user.\nThat said, as @gmr suggested, in addition to using the wrong pika version, you might also have a networking issue in your Docker configuration.\n. I concur with @gmr \n. Appears to be user error in docker/network configuration. Closing.\n. I agree that it would be great if BlockingConnection could handle heartbeat timeouts in the background. This issue has become more noticeable since a recent version of RabbitMQ broker reduced its default heartbeat timeout to 1 minute from almost 10 minutes.\nShort of that, I think it would be good to have a mechanism in pika such that the user could ask for a longer heartbeat timeout and have it respected.\n. Michael Klishin (Pivotal/RabbitMQ) recommends that the clients should follow what the Java client does and accept the max of the two values, not the min. See https://groups.google.com/forum/#!topic/rabbitmq-users/FpUdTaIr8CI.\nI opened #665 for picking the max during heartbeat timeout negotiation.\n. Once PR #666 is merged, you should at least be able to configure a longer timeout.\nPlease note that prior to RabbitMQ v3.5.5, the default heartbeat timeout from RabbitMQ was 580 seconds (almost 10 minutes).  RabbitMQ v3.5.5 dropped the default to just 60 seconds. See https://www.rabbitmq.com/heartbeats.html\n. Regarding the workaround, it depends: if you have to publish many messages at once, then opening and closing an AMQP connection for every message may punish your performance (and broker's as well?). Keep in mind that setting up an AMQP connection involves a TCP/IP connection + AMQP handshake; and then opening a channel is another handshake; all these operations add latencies.\nThere are other scenarios, where opening a connection per messages would be entriely reasonable.\n. @nickcash's PR #666 \"Pick the max of the heartbeat timeouts as result of negotiation\" has just been merged into pika master. We're going to keep the current default behavior (accept server's value), but an app can now configure the heartbeat timeout for a given connection via ConnectionParameters or URLParameters as follows:\n- the tuning algorithm will pick the max(client_value, server_value); 0 is treated as the highest possible value  (zero=disable), and will disable heartbeats altogether for the given connection.\n. Closing issue, since using pika master, an application can now set a larger heartbeat timeout per PR #666. Thank you.\n. add_callback_threadsafe() in pull request #956 might help with this. See this example. With this feature, you can offload the lengthy processing of your message to another thread, which would then delegate channel.basic_ack() to your BlockingConnection thread via BlockingConnection.add_callback_threadsafe(), assuming your BlockingConnection is being serviced via BlockingConnection.sleep(), BlockingConnection.process_data_events, BlockingChannel.consume(), BlockingChannel.start_consuming(), or similar.. @shinji-s: Thanks for staying on top of this.\nRegarding\n\n... I can't write a test code that generates EINTR exception. Could you provide direction on how I proceed with the issue?\n\nYou would need to do something like this in the test methods:\n1. Create a connected socket pair. get_interrupt_pair in select_connection.py is an example of how to do this in a portable way (you cannot pass a pipe to select on Windows). NOTE: Be mindful that some developers will be running the tests on Windows, so your SelectPoller EINTR test will need to be compatible with Windows, whatever that entails. You will use the poller on one of the sockets and your signal handler function will send a character via the other socket.\n2. Register a handler for signal.SIGUSR1; your signal handler function should set a flag that it was called, then send a character on the \"sending\" socket of the above-created socket pair and return (see https://docs.python.org/2/library/signal.html). Sending the character will cause the poller's poll() method to call your socket event handler and return quickly. Wrap the rest of the test method's code in try/finally and unregister the signal handler in finally so that it won't interfere with other tests.\n3. Instantiate the poller class to be tested\n4. Call the poller's add_handler method to add a handler for the fileno() of one of the connected sockets from the above step (but don't send any data at this point, so that the poll method would block). Use select_connection.READ as the event mask. The handler function that you register via add_handler should assert to make sure that it was called with the expected select_connection.READ event and also set a flag to record that it was called.\n5. Call the poller's add_timeout method to register a timeout function; e.g., 1 second. Your timeout handler function should raise an exception that causes the test to fail; the exception should have a meaningful text message that helps to debug.\n6. Immediately before calling the poller's poll() method, start a thread that will use os.kill() to send the signal signal.SIGUSR1 to the test's process (see os.getpid()). The thread function should time.sleep() for a little bit (e.g., 0.1 sec) BEFORE calling os.kill() to give the main thread enough time to call the poller's poll() method. Wrap the poll() call in try/finally, and join the thread object in the finally to make sure that the thread will be stopped inside this test method and won't impact other tests.\n7. Call the poller's poll() method.\n8. Assert that your signal.SIGUSR1 handler and  select_connection.READ handler functions were called.\nYou could also use the mock package to patch your new select_connection. _is_resumable method to check that it was called and also see/log which exception was passed to it. Your patch function should call through to the original _is_resumable. Be sure to patch using a context manager so that the function will get un-patched before other tests will be executed.\n. @gmr, we're still waiting for @shinji-s to add the tests to this PR.\n. @shinji-s, let's get the tests into this PR before merging.\n. @shinji-s - thanks!\n. @shinji-s: thank you for adding the tests and additional exceptions. I reviewed the changes and left feedback for you here.\n. @shinji-s, thank you for following up. I noted a few more issues. We're almost there!\n. @shinji-s: awesome, thank you! Please rebase/squash against master, so that there will be a single commit with your changes instead of 15; and we should be good to go after that. Best.\n. @shinji-s: I usually run git rebase -i master if my branch's base was in local master or git rebase -i upstream/master if my branch's base was from upstream/master\n. @shinji-s: LGTM @ 59dfa06cb18ce53e12979547312941eba04b0a6d\nCC @gmr \n. @gmr, whenever you get a chance, I think we're good to merge this.\n. What is the version of pika and the operating system/version?\n. If there was a connection-related exception from the same connection earlier, and your code ignored it, then you'd get the exception AttributeError: 'NoneType' object has no attribute 'send' the next time you tried to use the same connection for sending.\n. @KenjiTakahashi, there are a bunch of exceptions that are defined in https://github.com/pika/pika/blob/master/pika/exceptions.py. Many reflect details of the AMQP protocol and RabbitMQ extensions and are not explicitly documented in pika's public methods. Become familiar with all of them.\nThe one related to a connection becoming closed is ConnectionClosed. Another one that may be of interest is ChannelClosed.\nAMQP communicates errors on connection-oriented methods by closing the connection, which should be translated to the ConnectionClosed exception in BlockingConnection, in which case an error code and error message are also provided by the broker. Pika also raises ConnectionClosed when a connection is lost with the broker.\nIn addition to ConnectionClosed, there are several exception classes derived from AMQPConnectionError for failures that may occur during connection establishment; e.g., IncompatibleProtocolError, AuthenticationError, etc.\nAMQP usually communicates errors on channel-oriented methods by closing the channel and providing error code and error message. In pika's BlockingConnection, this translates to the ChannelClosed exception. However, according to the interpretation of RabbitMQ developers, the AMQP protocol defines this somewhat loosely, so RabbitMQ may close the entire connection instead of just the channel to signal certain \"very serious\" errors on some channel-oriented methods. In that case, you would get a ConnectionClosed exception instead of ChannelClosed. For example, if you pass an invalid exchange type when declaring an exchange on RabbitMQ, RabbitMQ will signal that failure by closing the connection. So, you should be prepared to handle any of these exceptions.\n. I posted links to some examples on your stackoverflow question.\n. @rajesh-kanakabandi: BlockingConnection is implemented to top of SelectConnection. Here is an example of consuming from multiple queues on a shared connection/channel via BlockingConnection: https://github.com/vitaly-krugl/pika/blob/94f86187bf8cbd6cdc18a52f17c564fbb2c78169/tests/acceptance/blocking_adapter_test.py#L1550-L1649. Since BlockingConnection's consumer is implemented on top of SelectConnection's consumer, this example validates this functionality in SelectConnection as well.\nI think that you will get more timely help if you post a complete and simple example that reproduces your issue. The code that you posted on http://stackoverflow.com/questions/32104906/how-to-asynchronously-consume-from-2-rabbitmq-queues-from-one-consumer-using-pik is not enough.\n. Closing due to inactivity. Appears to be user error.\n. CC @ztane\n. This known issue should be addressed in the pre-release version of pika 0.10.0 (pip install pika --pre). Please post your results with it. Thanks!\n. One way that recursion could happen with pika 0.9.14 is if you were consuming and publishing on the same connection. Let's say every time your consumer callback was called, you published something in response from the consumer callback's scope. If you had more messages to consume from a queue than python's recursion limit, you could end up in a sequence like this:\n1. Consumer callback is called\n2. The app publishes a messages via basic_publish from the scope of the consumer callback\n3. The consumer callback is called again before basic_publish returns\n4. The app publish another message from the second consumer callback\n5. The consumer callback is called again before the second basic_publish returns; and so on.\n6. We're now a couple of levels deep, and the stacked calls will continue to increase until the app fails with recursion error.\nNOTE: Be sure to read the release notes for 0.10.0, as there are some API changes (they shouldn't affect most developers, but it's better to be safe than sorry).\nYou might also find that publishing with BlockingConnection in pika v0.10.0 is a lot faster in many cases than in the preceding versions. In some of my tests, I clocked a 40X - 60X performance improvement. The one case where I saw only a minor performance improvement (3X ?) is when publishing a persistent message to a durable queue using the mandatory flag with publisher acks turned on (confirm_delivery mode) - in this case, RabbitMQ itself becomes the bottleneck.\n. Thanks for the feedback. Let's go ahead and close this issue.\nBest,\nVitaly\n. Is this still desirable? pamqp hasn't been active since 2015 and using pamqp would introduce an external dependency into pika core.. @ulyktey: please describe how to reproduce this scenario in RabbitMQ (out of free file/socket descriptors).\n. On my MacBookPro, this file is located at /usr/local/etc/rabbitmq/rabbitmq-env.conf. I'll give it a try after work.\n. @ulyktey: please reproduce the issue with logging.basicConfig(level=logging.DEBUG) and attach pika log output as a comment here. Thank you.\nI tried your instructions on my MacBookPro, and got an exception instead of a hang:\n```\nIn [16]: par = pika.ConnectionParameters(host='localhost', port=5672, credentials=pika.PlainCredentials('guest', 'guest'))\nIn [17]: conn = pika.BlockingConnection(parameters=par)\nDEBUG:pika.adapters.select_connection:Using KQueuePoller\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nINFO:pika.adapters.base_connection:Connecting to 127.0.0.1:5672\nWARNING:pika.adapters.base_connection:Connection to 127.0.0.1:5672 failed: [Errno 61] Connection refused\nWARNING:pika.connection:Could not connect, 0 attempts left\nDEBUG:pika.callback:Processing 0:_on_connection_error\nDEBUG:pika.callback:Calling > for \"0:_on_connection_error\"\n\nConnectionClosed                          Traceback (most recent call last)\n in ()\n----> 1 conn = pika.BlockingConnection(parameters=par)\n/Users/vkruglikov/Library/Python/2.7/lib/python/site-packages/pika/adapters/blocking_connection.pyc in init(self, parameters, _impl_class)\n    337             stop_ioloop_on_close=False)\n    338 \n--> 339         self._process_io_for_connection_setup()\n    340 \n    341     def _cleanup(self):\n/Users/vkruglikov/Library/Python/2.7/lib/python/site-packages/pika/adapters/blocking_connection.pyc in _process_io_for_connection_setup(self)\n    372         \"\"\"\n    373         self._flush_output(self._opened_result.is_ready,\n--> 374                            self._open_error_result.is_ready)\n    375 \n    376         if self._open_error_result.ready:\n/Users/vkruglikov/Library/Python/2.7/lib/python/site-packages/pika/adapters/blocking_connection.pyc in _flush_output(self, *waiters)\n    393         \"\"\"\n    394         if self._impl.is_closed:\n--> 395             raise exceptions.ConnectionClosed()\n    396 \n    397         # Conditions for terminating the processing loop:\nConnectionClosed:\n``\n. @gmr: I am able to get pika.BlockingConnection() constructor to hang on Fedora running in VirtualBox with RabbitMQ 3.5.4 andulimit -n 100in/etc/rabbitmq/rabbitmq-env.confusing both pika 0.9.14 and 0.10.0. On 0.10.0, it blocks in the call toBlockingConnection._process_io_for_connection_setup()->_flush_output()->EPollPoller.poll()`, while waiting for the connection handshake to complete after the socket got connected.\nThis is a condition where RabbitMQ is intentionally under-resourced. Pika Connection class could potentially set a timeout at the beginning of the connection setup handshake and abort the connection setup if this timer fires before the connection setup completes. This intentional FD under-allocation seems somewhat of a malicious case. Thus, I am not sure whether this can happen with a normal configuration and whether pika needs to do anything about this case at all. \nGavin, what do you recommend?\nCC @ulyktey \n. @ulyktey: I also followed up in https://groups.google.com/forum/#!topic/rabbitmq-users/OioSFq1veFQ. When you set the ulimit to such an insanely low value, other rabbitmq issues arise: rabbitmqctl stop hangs, for example. Please see the reply from Michael Klishin (one of RabbitMQ maintainers) on https://groups.google.com/forum/#!topic/rabbitmq-users/OioSFq1veFQ.\n. @gmr: I am inclined to write this off as invalid RabbitMQ configuration, based on Michael Klishin's comments in https://groups.google.com/forum/#!topic/rabbitmq-users/OioSFq1veFQ\n. @gst, I've considered it further and agree with your assessment.\n. Also, please see this discussion: https://groups.google.com/d/msg/rabbitmq-users/OioSFq1veFQ/L1iHPX4JGQAJ \"rabbitmqctl stop hangs if RabbitMQ is out of FDs\"\n. @lukebakken, this is one of the issues that is being addressed by my refactoring PR #1002. Should I re-assign this issue to myself?. PR #685 should address this issue.\n. fixed in master, please close\n. Fixed in Pika master, closing\n. @jackeisenbach: see the official RabbitMQ documentation at https://www.rabbitmq.com/.  There is also a google group for RabbitMQ users that is very active: https://groups.google.com/forum/#!forum/rabbitmq-users\n. @antime, this issue is closed. If you are still experiencing an issue that you think is caused by pika, please open a new issue, and provide a full description of your issue, including python version, os and version, description of network topology, short and simple functional script that reproduces the issue on your system, DEBUG-level pika logs demonstrating the issue from beginning, and network AMQP logs (use WireShark to collect AMQP logs in AMQP mode)\n. @devpmn, this issue is closed. ConnectionClosed may occur for different reasons. The RabbitMQ Users goole group would be a more appropriate place to ask for help.. @gmr: while this fixes up the examples to work for python 3.x, it breaks the examples for users of python 2.7.x, since the print function is not available there by default.\n. Unable to reproduce with pika 0.10.0 with the following changes in example:\n```\n    def on_exchange_declareok(self, unused_frame):\n        \"\"\"Invoked by pika when RabbitMQ has finished the Exchange.Declare RPC\n        command.\n    :param pika.Frame.Method unused_frame: Exchange.DeclareOk response frame\n\n    \"\"\"\n    LOGGER.info('Exchange declared')\n    # self.setup_queue(self.QUEUE)\n\n    self._channel.queue_declare(self.on_queue_declareok)\n    self._channel.queue_declare(self.on_queue_declareok)\n    self._channel.queue_declare(self.on_queue_declareok)\n    self._channel.queue_declare(self.on_queue_declareok, 'test1')\n    self._channel.queue_declare(self.on_queue_declareok, 'test2')\n    self._channel.queue_declare(self.on_queue_declareok, 'test3')\n    self._channel.queue_declare(self.on_queue_declareok)\n    self._channel.queue_declare(self.on_queue_declareok)\n    self._channel.queue_declare(self.on_queue_declareok)\n    self._channel.queue_declare(self.on_queue_declareok, 'test4')\n    self._channel.queue_declare(self.on_queue_declareok, 'test5')\n    self._channel.queue_declare(self.on_queue_declareok, 'test6')\n\ndef setup_queue(self, queue_name):\n\"\"\"Setup the queue on RabbitMQ by invoking the Queue.Declare RPC\ncommand. When it is complete, the on_queue_declareok method will\nbe invoked by pika.\n\n:param str|unicode queue_name: The name of the queue to declare.\n\n\"\"\"\nLOGGER.info('Declaring queue %s', queue_name)\nself._channel.queue_declare(self.on_queue_declareok, queue_name)\ndef on_queue_declareok(self, method_frame):\n    \"\"\"Method invoked by pika when the Queue.Declare RPC call made in\n    setup_queue has completed. In this method we will bind the queue\n    and exchange together with the routing key by issuing the Queue.Bind\n    RPC command. When this command is complete, the on_bindok method will\n    be invoked by pika.\n\n    :param pika.frame.Method method_frame: The Queue.DeclareOk frame\n\n    \"\"\"\n\nLOGGER.info('Binding %s to %s with %s',\nself.EXCHANGE, self.QUEUE, self.ROUTING_KEY)\n    print 'queue: ' + method_frame.method.queue\n    pass\n    # self._channel.queue_bind(self.on_bindok, self.QUEUE,\n    #                          self.EXCHANGE, self.ROUTING_KEY)\n\n```\nGet this output:\n$ python pika652nodeclareok.py \nINFO       2016-02-27 15:59:08,359 __main__                       connect                              50  : Connecting to amqp://guest:guest@localhost:5672/%2F\nINFO       2016-02-27 15:59:08,361 pika.adapters.base_connection  _create_and_connect_to_socket        212 : Connecting to 127.0.0.1:5672\nINFO       2016-02-27 15:59:08,364 __main__                       on_connection_open                   63  : Connection opened\nINFO       2016-02-27 15:59:08,364 __main__                       add_on_connection_close_callback     72  : Adding connection close callback\nINFO       2016-02-27 15:59:08,364 __main__                       open_channel                         115 : Creating a new channel\nINFO       2016-02-27 15:59:08,365 __main__                       on_channel_open                      127 : Channel opened\nINFO       2016-02-27 15:59:08,365 __main__                       add_on_channel_close_callback        137 : Adding channel close callback\nINFO       2016-02-27 15:59:08,365 __main__                       setup_exchange                       164 : Declaring exchange message\nINFO       2016-02-27 15:59:08,366 __main__                       on_exchange_declareok                176 : Exchange declared\nqueue: amq.gen-JjeEbuyVnPwmvuzapRi4Vw\nqueue: amq.gen-tVv6HrC0PUptYGlioQqekw\nqueue: amq.gen-1QZm6eIitGT-vdtA5YqZaA\nqueue: test1\nqueue: test2\nqueue: test3\nqueue: amq.gen-TzN09hNaGweAM2WZNClmGQ\nqueue: amq.gen-1VS81hd8VBcf89qfS4tWzQ\nqueue: amq.gen-uV5PuRvBKvv-cNnvDmnlFA\nqueue: test4\nqueue: test5\nqueue: test6\n. Also, can't reproduce with this change:\nself._channel.queue_declare(self.on_queue_declareok)\nself._channel.queue_declare(self.on_queue_declareok, 'test1')\nself._channel.queue_declare(self.on_queue_declareok)\nself._channel.queue_declare(self.on_queue_declareok, 'test2')\nself._channel.queue_declare(self.on_queue_declareok)\nself._channel.queue_declare(self.on_queue_declareok, 'test3')\nself._channel.queue_declare(self.on_queue_declareok)\nself._channel.queue_declare(self.on_queue_declareok, 'test3')\nself._channel.queue_declare(self.on_queue_declareok)\nself._channel.queue_declare(self.on_queue_declareok, 'test4')\nself._channel.queue_declare(self.on_queue_declareok)\nself._channel.queue_declare(self.on_queue_declareok, 'test5')\nself._channel.queue_declare(self.on_queue_declareok)\nself._channel.queue_declare(self.on_queue_declareok, 'test6')\nThe output is \n$ python pika652nodeclareok.py \nINFO       2016-02-27 16:02:40,209 __main__                       connect                              50  : Connecting to amqp://guest:guest@localhost:5672/%2F\nINFO       2016-02-27 16:02:41,344 pika.adapters.base_connection  _create_and_connect_to_socket        212 : Connecting to 127.0.0.1:5672\nINFO       2016-02-27 16:02:42,470 __main__                       on_connection_open                   63  : Connection opened\nINFO       2016-02-27 16:02:42,471 __main__                       add_on_connection_close_callback     72  : Adding connection close callback\nINFO       2016-02-27 16:02:42,471 __main__                       open_channel                         115 : Creating a new channel\nINFO       2016-02-27 16:02:42,472 __main__                       on_channel_open                      127 : Channel opened\nINFO       2016-02-27 16:02:42,472 __main__                       add_on_channel_close_callback        137 : Adding channel close callback\nINFO       2016-02-27 16:02:42,472 __main__                       setup_exchange                       164 : Declaring exchange message\nINFO       2016-02-27 16:02:42,473 __main__                       on_exchange_declareok                176 : Exchange declared\nqueue: amq.gen-_p16zykolDtjz0tgHGKDbg\nqueue: test1\nqueue: amq.gen-8fkknVZw1Bzi-_dQmHZVzw\nqueue: test2\nqueue: amq.gen-pxtQnebsRRKcjMcxNgWAmg\nqueue: test3\nqueue: amq.gen-bZoQv7uFT6-Mi85fSeKhiA\nqueue: test3\nqueue: amq.gen-qOK-FmSkqKWTsqQTXtf0yA\nqueue: test4\nqueue: amq.gen-N1PQoV7h5dEOHBr3FHqDrA\nqueue: test5\nqueue: amq.gen-FSQfhxjLmawC4qDZQtTcPg\nqueue: test6\n. @morden2k, I was unable to reproduce this issue against pika 0.10.0 running on Mac OS X El Capitan and python 2.7.10. I am closing this issue. For your reference, I published a gist of my test script at https://gist.github.com/vitaly-krugl/2243c86c24709cebf500. Please try it against the pika 0.10.0 and against Pika master and open a new issue if you can still reproduce it.\n. @replay, thanks for the report.\nI'm thinking out-loud here, please read on and bear with me ... \nI am going to provide some background that will help me, you, and others think about possible solutions.\nOne of the pesky problems with the previous generations of BlockingConnection were issues related to reentrancy and unexpected callbacks; these have lead to instability in both BlockingConnection and user code. E.g., call publish; before the publish returns, you get a consumer callback; you call publish again (the first publish call hasn't even returned yet); you get another consume callback before the second publish returns (the first publish hasn't returned either), and so on. This would lead to bad things such as RuntimeError: maximum recursion depth exceeded or execution of user code in ways that the programmer didn't anticipate from a blocking type of interface (it's BlockingConnection, after all); when using a blocking interface, you expect to not have to deal with the \"callback hell\" that you would normally associate with asynchronous interfaces.\nThe new generation of BlockingConnection addresses the above-mentioned reentrancy-related issues among others (e.g., performance). However, it is now apparent that it didn't anticipate the type of issue that you are reporting. \nIt's a bit of a slippery slope when a synchronous (blocking) API has to confront asynchronous events via callbacks. If this may result in callbacks/reentrancy (possibly nested, too) in the midst of any blocking API call, then it defeats the simplicity and robustness of using a synchronous interface, and it becomes even more complex than the  environments of asynchronous interfaces, such as SelectConnection or TwistedConnection: at least in those environments your code never gets reentered while it's in the midst of an API call and is allowed to run to completion of the current event without any reentrancy whatsoever.\n@replay, what do your app(s) need to do upon on_blocked_connection_callback? Would they need to raise an exception to interrupt the current blocking call? Would they need to make pika API calls on the same connection or channel as the active blocking call, and which ones? Would they need to set a timer in the connection?\nFurthermore, I would expect this problem to not be limited to the consume() method. For example, what would happen when a connection is blocked and you placed the channel in delivery-confirmation mode (BlockingChannel.confirm_delivery()) and then tried to publish a message. Would the code also hang as it did with consume()? (or any other blocking API that waits for some form of confirmation (ok) from the broker, for that matter). @replay, would you mind trying it out with publish (and some others?) and report your findings here? \n. @replay, thank you for the test results and the clarification of your use-case.\nI was also thinking along the lines that exceptions for signaling such things as blocked/unblocked connections might be cleaner than callbacks in a synchronous interface. This might necessitate API changes. Here are some additional things to consider in the design:\n1. Existing code that uses pika expects the callback and doesn't expect the new exception(s), so it would break; this could be easily addressed by an opt-in via a new optional arg in the BlockingConnection constructor that defaults to the old behavior.\n2. If an API call gets interrupted by such an exception (e.g., ConnectionBlocked, ConnectionUnblocked, ConnectionTimerExpired, etc.), then how would the user continue after the interruption?\n   - Considering that the API call probably already sent some frame(s) to the broker and was waiting for the reply from broker, would the channel and connection be compromised (in a bad state) after this? Would the user have to retry the call? What if this was something \"destructive\", such as basic_get or basic_publish? \n   - To handle it gracefully, the API might need to change to return a \"promise\" from each API method that the user could wait on for completion and result. This way, if the call is interrupted by one of those transient conditions, the user could handle the exception and resume waiting on the \"promise\".\nI think that in the near term, it might be more practical to allow the connection-blocked/unblocked (and some others, such as timer?) callbacks to flow through at any time, but with severe (documented) restrictions on what the user code may do with the source BlockingConnection/Channel instance from such callbacks.\n. @replay: Sorry that I have not been able to give this any attention lately. I am quite a bit swamped for another few days. Thanks for your patience.\n. @replay, I've been swamped, but I have not forgotten. Please take a look at PR #701. It implements a feature that enables users to configure a Connection.Blocked timeout such that the connection gets torn down whenever connection remains blocked for the given timeout seconds, resulting in ConnectionClosed exception in BlockingConnection.\nI am choosing not to expose the blocked/unblocked callbacks in unsafe nested contexts for now, assuming that the new feature will address the real issue for most users.\nAlso, see https://github.com/vitaly-krugl/pika/blob/blocked-handling/pika/adapters/blocking_connection.py#L277-L309 for an explanation of the recursion/reentrancy protection in BlockingConnection implementation and additional insights into handling of Connection.Blocked and Connection.Unblocked.\n. Awesome, @appetito! If it matches the interface of SelectConnection/TornadoConnection/LibevConnection, then you can easily add it to the async adapter acceptance test suite via async_test_base.AsyncAdapters.\n. @base698, http://stackoverflow.com/questions/33739147/never-ending-message-loop-same-message-redelivered-in-python-rabbitmq-consumer appears to be a different issue, related to RabbitMQ's heartbeat timeouts.  I posted an answer to that question on Stackoverflow.\n. @base698: I added a couple of comments to clarify the heartbeat situation in the above-referenced Stackoverflow post as user1778420. The issue discussed in the above-referenced Stackoverflow post is related to heartbeats, which is different from the issue logged here by @gst.\n. @gst, please implement tests that validate this scenario and fix.\n. @gst: sorry, but I don't have admin privileges on this one, so can't restart the build. @gmr is the owner of this repo.\nI noticed that there was an additional failure the 3.4 and the 3.5 builds that was logged, but suppressed by the io-loop, so that test was a false negative. The test timeout timer fires from the context of the io-loop, and a lot of the io-loop implementations suppress exceptions in the handlers. This also needs to be fixed to avoid false negatives:\n1. in AsyncTestCase.setUp: add self._timed_out = False\n2. in AsyncTestCase. on_timeout: change self.logger.debug to self.logger.error and add self._timed_out = True\n3. in AsyncTestCase.start: after the line self.connection.ioloop.start(), add self.assertFalse(self._timed_out)\nVerify that access denied invokes on open error callback (TornadoConnection) ... ok\nPublish a message and consume it (LibevConnection) ... Exception ignored in: <bound method LibevConnection._timer_callback of <pika.adapters.libev_connection.LibevConnection object at 0x7f7994a21dd8>>\nTraceback (most recent call last):\n  File \"/home/travis/build/pika/pika/pika/adapters/libev_connection.py\", line 242, in _timer_callback\n    callback_method(**kwargs)\n  File \"/home/travis/build/pika/pika/tests/acceptance/async_test_base.py\", line 85, in on_timeout\n    raise AssertionError('Test timed out')\nAssertionError: Test timed out\nok\n. @gst, sorry about getting you sidetracked into AsyncTestCase changes. Perhaps the AsyncTestCase changes could be a separate PR, along with a fix for the Publish a message and consume it (LibevConnection) failure.\n. @gst, also: the BlockingChannel.basic_nack failure in https://travis-ci.org/pika/pika/jobs/98017989 appears to be a race condition in the test. I just got a similar failure in BlockingChannel.basic_reject in my build https://travis-ci.org/pika/pika/jobs/98058578. When I implemented those tests, I assumed that the passive queue_declare calls would flush the data path to the broker and guarantee that the nack or reject would be handled before the passive queue_declare response. However, it looks like we need a retry loop (with a timeout) around those checks.\n. @gst, I discovered something else: those failures that you're getting in the Travis builds in \"Publish a message and consume it (LibevConnection)\" from TestZ_PublishAndConsume test class appear to be triggered by another test that runs before it. If I @unittest.skip() TestZ_AccessDenied, then TestZ_PublishAndConsume doesn't fail. Perhaps TestZ_AccessDenied is messing something up, and is the place to debug.\nI am pretty certain about it now: I renamed the bad test to TestAZZZ_AccessDenied in order to get it to execute earlier, and that caused another earlier \"Connect, open channel and disconnect (LibevConnection)\" test to time out, instead of \"Publish a message and consume it (LibevConnection)\". See https://travis-ci.org/pika/pika/jobs/98061777\n. @gst, I finally cracked the LibevConnection test timeout problem: libev caches the current time at each event. LibevConnection uses a global default libev I/O loop by default. The cached time is frozen after the last event in the \"Verify that access denied invokes on open error callback (LibevConnection\" test. There are five more \"Verify that access denied invokes on open error callback\" tests after it, using other ioloops (TornadoConnection, SelectConnection, etc.) at about 3 seconds each (about 15 seconds total).\nThus, the next LibevConnection test \"Publish a message and consume it (LibevConnection)\" runs about 15 seconds later, and sets a test timeout of AsyncTestCase.TIMEOUT (15) seconds, but Libev's default ioloop's cached time is about 15 seconds in the past at that moment, so the test timeout timer fires almost immediately when the \"Publish a message and consume it (LibevConnection)\" test starts running.\nIn my experiments, I was able to fix this problem by adding a call to self.ioloop.update() in the default ioloop case within LibevConnection.__init__ as follows:\nif custom_ioloop:\n            self.ioloop = custom_ioloop\n        else:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", RuntimeWarning)\n                self.ioloop = pyev.default_loop()\n                self.ioloop.update() # ZZZ added this to fix the erroneous test timeout\n. Hi @gst, here is what I think is needed:\n1. Move the unrelated change bd8a6ee8612d661be17e595af382b1b7a2cefbcb \"Adapt AsyncTestCase for previous fix\" to a separate PR, combined with the self.ioloop.update() fix described in my earlier comment. Otherwise, the travis build will keep failing for this PR.\n2. See comment https://github.com/pika/pika/pull/656/files#r48185505 about the change in _process_io_for_connection_setup.\n3. Move the mock tests introduced by a1b17d36d83fa1e931b3459d65423edaee17ac20 \"add tests for connection_attempts scenarios\" to unit tests https://github.com/pika/pika/blob/master/tests/unit/blocking_connection_tests.py\n4. I think that it might be better to add the new acceptance tests from https://github.com/vitaly-krugl/pika/blob/81cbba61f4d8db1fd6c1db1650fbfe3acb0390df/tests/acceptance/blocking_adapter_test.py#L225-L259 as part of this PR, since it's tightly coupled to your change in this PR. Let me know if you prefer that I submit it via a separate PR.\n5. I filed the issue #677 \"Transient failures in acceptance test blocking_adapter_test.py\" that can be implemented by someone later. This concerns the failures discussed in this comment: https://github.com/pika/pika/pull/656#issuecomment-166210529 here.\n. @gst, regarding point 4: I think that we need both tests. The unit test cases that you implemented test that certain methods get called the correct number of times. The acceptance test cases help confirm that the BlockingConnection adapter is able to make a real live connection to the broker and does not get confused under the circumstances. \nIncidentally, I don't recall seeing a test like that in async_adapter_tests.py.\n. @gst, regarding\n\nI'm not sure what must/should be done finally here.. :? \n\nAre you referring just to point 4 or more broadly?\n. @gst, I described that in https://github.com/pika/pika/pull/656#issuecomment-166426610 and the clarified point 4.\n. @gst: LGTM after the unused method _check_open_error is removed.\n. LGTM\n. @sazary: the way that AMQP communicates failures is by closing the channel or connection on which the error occurred. RabbitMQ sometimes closes the connection instead of the channel for some errors on channel-issues commands that they deem very serious, such as trying to declare an exchange of unknown type. The \"close\" message provides a code and a message. With SelectConnection, you need to register for the connection's \"on-close\" callback as soon as you instantiate the connection object and register an \"on-close\" callback on the channel as soon as you instantiate the channel. If the connection gets closed, then you can no longer use any of the channels that you created from that connection. If only a channel gets closed, then you can no longer use that channel, but should be able to continue using other non-closed channels on the connection and create additional channels on that same connection.\n. This is user error per my earlier comment. Please close the issue. CC @gmr\n. Closing per earlier comment.\n. Could someone with Twisted expertise tackle this one?\n. pika.connection.Connection calls on_closed_callback that is passed to the constructor or registered via Connection.add_on_close_callback(). The raise exceptions.ConnectionClosed in self._send_frame indicates that the app didn't register or ignored on_closed_callback and continued to use the connection after that either directly or indirectly.. Issue has been stale for a while and based on the back-and-forth appears to be user error. Closing. Please reopen if you have new details and a simpler reproducible example to share.. @glyph, @jeremycline - if you're interested in preserving Twisted support in Pika, please see issue #1031.. @gst, I am unable to reproduce the KeyError exception against pika master using your instructions. The test TestNoAccessToFileDescriptorAfterConnectionClosed in https://github.com/pika/pika/blob/master/tests/acceptance/blocking_adapter_test.py verifies this exact scenario; it makes sure that the exception is ConnectionClosed in this case.\n. This bug affects all pika adapters, not just BlockingConnection. It's the result of https://github.com/pika/pika/pull/578/files#diff-1a5308d8b684fdafac1f89e376e8933dR267; prior to this change, the writing to the socket and the corresponding detection of \"errno 104 connection reset by peer\" would have occurred later on in the context of the io-loop (after Channel.open() returned).\nThis problem can occur in all pika adapters, since they are all derived from BaseConnection that contains the changed _flush_outbound method.\n. Hi @gst, I am unable to reproduce the KeyError exception against pika master using instructions from #659. The test TestNoAccessToFileDescriptorAfterConnectionClosed in https://github.com/pika/pika/blob/master/tests/acceptance/blocking_adapter_test.py verifies this exact scenario.\nHere is my session:\nIn [1]: import pika\nIn [2]: c = pika.BlockingConnection()\nNow, stop RabbitMQ via rabbitmqctl stop before executing ch = c.channel()\nIn [3]: ch = c.channel()\nConnectionClosed                          Traceback (most recent call last)\n in ()\n----> 1 ch = c.channel()\n/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.pyc in channel(self, channel_number)\n    705 \n    706             # Drive I/O until Channel.Open-ok\n--> 707             channel._flush_output(opened_args.is_ready)\n    708 \n    709 \n/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.pyc in _flush_output(self, waiters)\n   1181         self._connection._flush_output(\n   1182             self._channel_closed_by_broker_result.is_ready,\n-> 1183             waiters)\n   1184 \n   1185         if self._channel_closed_by_broker_result:\n/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.pyc in _flush_output(self, *waiters)\n    420                                     result)\n    421                     raise exceptions.ConnectionClosed(result.reason_code,\n--> 422                                                       result.reason_text)\n    423                 elif not self._user_initiated_close:\n    424                     # NOTE: unfortunately, upon socket error, on_close_callback\nConnectionClosed: (320, \"CONNECTION_FORCED - broker forced connection closure with reason 'shutdown'\")\n. @gst, I am beginning to think that there is a deeper problem here that is perhaps tied to pika on python 3.x or on the different platform than I am running. By the way, what is your environment? Are you running on Unix or Linux and on which version?\nWould you mind reproducing the failure with DEBUG logging turned on and posting pika's complete log here? Turn on DEBUG logging before creating the pika connection\nimport logging\nLOG_FORMAT = ('%(levelname) -10s %(asctime)s %(name) -30s %(funcName) '\n              '-35s %(lineno) -5d: %(message)s')\nlogging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT)\nI am running on Mac OS X Yosemite 10.10.5 with python 2.7.10, and here is what I get (note my modified command that stops rabbitmq inline):\n$ python -c 'import pika; print(pika); c = pika.BlockingConnection(); import subprocess; subprocess.check_call([\"rabbitmqctl\", \"stop\"]); c.channel()'\n<module 'pika' from '/Users/vkruglikov/ossdev/pika/pika/__init__.pyc'>\nStopping and halting node rabbit@localhost ...\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 707, in channel\n    channel._flush_output(opened_args.is_ready)\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 1183, in _flush_output\n    *waiters)\n  File \"/Users/vkruglikov/ossdev/pika/pika/adapters/blocking_connection.py\", line 422, in _flush_output\n    result.reason_text)\npika.exceptions.ConnectionClosed: (320, \"CONNECTION_FORCED - broker forced connection closure with reason 'shutdown'\")\n. @gst, I think it's related to timing and that there really is a race condition in pika's implementation. Perhaps that specific python version is exposing it better than the others. Posting the DEBUG-level log would help me get a handle on it. Many thanks.\n. @gst - thx!\n. @gst, I am able to reproduce this on Fedora with Linux 3.17.4 and python 3.4:\n\nThis was most likely introduced by this change: https://github.com/pika/pika/pull/578/files#diff-1a5308d8b684fdafac1f89e376e8933dR267; prior to this change, the writing to the socket and the corresponding detection of \"errno 104 connection reset by peer\" would have occurred later on in the context of the io-loop (after Channel.open() returned).\nThis problem can occur in the raw SelectConnection (on which BlockingConnection is now based) and probably in the other adapters, since they are all derived from BaseConnection that contains the changed _flush_outbound method.\nI am now wondering which other code paths might be broken similarly to Chanell.open.\nCC @wjps\n. @gst: I can also reproduce the KeyError exception with python 2.7.8 on my Fedora VirtualBox instance.\nI wish I understood why TestNoAccessToFileDescriptorAfterConnectionClosed ( https://github.com/pika/pika/blob/94f86187bf8cbd6cdc18a52f17c564fbb2c78169/tests/acceptance/blocking_adapter_test.py#L196-L218) doesn't catch this problem in pika's Travis builds.\n. @gst: besides the 80-char/line limit feedback, your fix in Connection.channel() looks reasonable. The only other thing that I recommend is to add a comment before channel = that explains why the code is being careful with the index-based access. It's not obvious why this is being done, and some other maintainer could easily \"optimize\" the access back to the way it was before your change. \n. I do wish that we had a test that could reproduce the issue in pika test suite. I am also concerned that there might be additional fallout from https://github.com/pika/pika/pull/578/files#diff-1a5308d8b684fdafac1f89e376e8933dR267. A code review may be necessary to identify additional flaws like the one that you fixed.\n. Regarding # Haven't run into this one yet, log it, if it needs to be tested, it should be tested at its own layer in base_connection_tests.py\n. @gst, I also discovered that it often takes more than one write to a remotely-closed socket to get the error. That's why the test https://github.com/pika/pika/blob/94f86187bf8cbd6cdc18a52f17c564fbb2c78169/tests/acceptance/blocking_adapter_test.py#L196 was not reproducing the problem.\n. @gst, as I suspected, https://github.com/pika/pika/pull/578/files#diff-1a5308d8b684fdafac1f89e376e8933dR267 broke more than just the Connection.channel() case that this PR is trying to address. For example, Connection._on_connection_tune gets into trouble if connection reset is discovered while executing in Connection._send_connection_tune_ok, when BaseConnection._flush_outbound is calling BaseConnection._handle_write. In that case, before Connection._on_connection_tune calls Connection._send_connection_open, the connection is already in the closed state and the ON_ERROR_CALLBACK has already been sent. However, _on_connection_tune still goes on to call _send_connection_open, which is not happy about being called in the closed state and trips up on a non-specific ConnectionClosed exception inside Connection._send_frame.\nAll this is happening because pika's code was not originally designed to be writing to (or reading from) the socket from any methods, except from ioloop callbacks. Unfortunately, the PR that contains the change https://github.com/pika/pika/pull/578/files#diff-1a5308d8b684fdafac1f89e376e8933dR267 didn't take that into account, causing all sorts of havoc in Connection.channel(), Connection._on_connection_tune, etc.\nThere are certainly likely to be other places in pika that won't behave properly under the circumstances as the result of that change. It's a slippery slope.\nTo make the long story short, I think that at this time, it's more practical to do the following:\n1. Undo all the changes made so far in this PR. It's a shame after all this effort, but there is no other practical choice.\n2. Remove the self._handle_write() call from BaseConnection._flush_outbound and restore the docstring to what it was before that change. Also, add a comment in BaseConnection._flush_outbound for future maintainers, describing why it's presently not possible to call self._handle_write() in that context - essentially, something like this \"We don't call _handle_write() from this context, because pika code was not designed to be writing to (or reading from) the socket from any methods, except from ioloop handler callbacks\" and cite an example or two.\n   - https://github.com/pika/pika/pull/578/files#diff-1a5308d8b684fdafac1f89e376e8933dR267\nRemoving that self._handle_write() call should fix the KeyError issue that you've been dealing with in this PR, the _on_connection_tune issue, and all others problems like them.\n. @gst, I will be offline for a few days. Please see my latest recommendation in my previous comment. \nIn the meantime, I figured out how to make the test https://github.com/pika/pika/blob/94f86187bf8cbd6cdc18a52f17c564fbb2c78169/tests/acceptance/blocking_adapter_test.py#L196 reproduce that KeyError in the current pika master branch. \nWhen I am back online in a few days, I will submit a PR with that test change. It would be better to have the self._handle_write() removal fix and the TestNoAccessToFileDescriptorAfterConnectionClosed test change in the same PR; otherwise, a PR with the test change alone will break the Travis test runs (Connection.channel() KeyError exception, etc.)\nIf you like, go ahead and close this PR, and I will submit a new PR that includes both self._handle_write() removal fix and an improved TestNoAccessToFileDescriptorAfterConnectionClosed that will demonstrate that the KeyError and _on_connection_tune issues are fixed. Please let me know.\nBest regards and many thanks for your effort.\n. @gst, per my prior comment, I submitted a new PR #679 with a more general fix that resolves #659 as well as other similar bugs and also updates the blocking_adapter_test.py acceptance test suite to verify the fix in the Connection.channel case. Your work here helped me figure out how to reproduce the issue with the help of SO_LINGER in the ForwardServer test utility.\nWould you mind reviewing the new PR #679? \n. thx\n. @hoverzheng, please provide additional information that could be helpful for understanding the problem, such as version of pika, operating system where the problem occurred, whether the problem is easily reproducible, a complete code sample to reproduce the problem, whether you made specific changes in RabbitMQ configuration, your (non-confidential) connection parameters, anything else that might be helpful.\nIf you can reproduce the hang by running your script from the terminal, then once it hangs you might be able to use CTL-C to interrupt the program and get a traceback of where it was blocked. That traceback would be handy to have here. \n. @vmarkovtsev, you just provided the key information for solving this issue. You said you use one thread to basic_consume and another to close the connection. pika is not thread-safe at all; accessing the same pika connection  from multiple threads will result in precisely the type of issue that you're having.\n. @vmarkovtsev: I can't think of any safe way that doesn't involve polling. In the past, I used to deal with this by setting up a timer via the connection, and every time that the timer fires, the code would check a flag and raise an exception if the flag was set. This exception would break out from start_consuming.\nIf your app only needs to consume from a single queue, then you can use the BlockingChannel.consume(...) method instead of basic_consume + start_consuming. Starting with pika v0.10.0, the consume method has an additional arg inactivity_timeout that you can make use of for polling. See https://github.com/pika/pika/blob/87d41009cd53a529939a508a41f5f7e148d0b729/pika/adapters/blocking_connection.py#L1698 and the docstring below.\n. @hoverzheng: are you also using multiple threads to interact with a pika connection?\n. That's a good point - if your code lends itself well to cooperative multitasking, then one of the other adapters could do the trick.\n. I would like to help resolve the hang issue, but the information provided is not sufficient for it. Many of the BlockingConnection tests create/close channels and close connections, and the tests don't hang; thus, I am at a loss.\nCC @gmr \n. Closing issue since sharing of a connection by multiple threads is officially not supported by the API.\n. Hi @dwt, it sounds like your app\n1. Sends a request via RabbitMQ to a background worker using TornadoConnection, and then waits for response; \n2. The request message has a TTL associated with it via x-message-ttl; \n   - What's the value of x-message-ttl?\n3. The request is handled by the background worker that takes >4-5 seconds to process the request, and then sends a response via RabbitMQ;\n   - How long does the background worker actually take to process the message? I am assuming it processed it and published the response successfully.\n4. Does the response message also have x-message-ttl and what is it's value?\n5. The response message times out after x-message-ttlassociated with the response.\nIs the above correct? If not, could you please describe the flow of things in a little more detail? Simple code or pseudocode might work. \nWhich version of RabbitMQ are you using?\nNote that in RabbitMQ 3.5.5, the default heartbeat timeout apparently changed from 580 seconds to 60 seconds - https://github.com/rabbitmq/rabbitmq-server/releases/tag/rabbitmq_v3_5_5\n. @dwt, it would be awesome if you could provide a simple standalone app (without celery) that reproduces this problem. Thx.\n. @dwt  and @rbu, thanks for your responses. A few more questions/requests for each of you:\n1. Which versions of RabbitMQ are you using in the failing scenarios?\n2. Is there any multithreading going on in your apps (or Celery, for that matter) that could result in more than one thread interacting with the same pika connection? We have to rule this out, since Pika is not thread-safe, and multiple thread accesses will result in bad things.\n3. Please run your app with pika DEBUG logging enabled, reproduce the failure, and attach the pika log output in this issue:\nLOG_FORMAT = ('%(asctime)s %(name)s - %(levelname)s - %(funcName)s '\n                 '%(lineno)s: %(message)s')\n   logging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT)\nThank you\n. @dwt, I am not familiar with Celery. When using pika from a standalone python script, I am able to switch all logging, including pika's, to DEBUG level using the logging.basicConfig code from my earlier comment. In python logging, log level determination logic parses the logging context name hierarchy, eventually reaching the root logger, if necessary. It's important to set the logging level before using pika. It's also important that some other code in Celery or your app doesn't override the logging configuration. I was expecting to see a lot of DEBUG-level logging info in your log output, but there wasn't any.\n. @Strawhatfy, thanks for writing the test. I am able to reproduce the issue that you're seeing with nowait_test.txt. If I comment out the basic_cancel call, then all expected queues get declared.\n. Here is the debug log for @Strawhatfy's nowait_test.txt where queues 4 and 5 don't get declared. Note the 3 blocked frames log entry near the end. Also note that Adding in on_synchronous_complete callback is missing right after that line. That looks suspicious -- are those frames not getting flushed?\n$ python nowait_test.py \nDEBUG      2015-12-18 09:48:01,800 pika.callback                  add                                  164 : Added: {'callback': <bound method TornadoConnection._on_connection_error of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-18 09:48:01,800 pika.callback                  add                                  164 : Added: {'callback': <bound method TestCase.on_connect of <__main__.TestCase object at 0x10ad60390>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-18 09:48:01,800 pika.callback                  add                                  164 : Added: {'callback': <bound method TornadoConnection._on_connection_start of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nINFO       2015-12-18 09:48:01,802 pika.adapters.base_connection  _create_and_connect_to_socket        212 : Connecting to 127.0.0.1:5672\nDEBUG      2015-12-18 09:48:01,803 pika.callback                  process                              220 : Processing 0:Connection.Start\nDEBUG      2015-12-18 09:48:01,804 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,804 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,804 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method TornadoConnection._on_connection_start of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,804 pika.callback                  process                              234 : Calling <bound method TornadoConnection._on_connection_start of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>> for \"0:Connection.Start\"\nDEBUG      2015-12-18 09:48:01,804 pika.callback                  add                                  164 : Added: {'callback': <bound method TornadoConnection._on_connection_tune of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,804 pika.callback                  process                              220 : Processing 0:Connection.Tune\nDEBUG      2015-12-18 09:48:01,804 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,805 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,805 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method TornadoConnection._on_connection_tune of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,805 pika.callback                  process                              234 : Calling <bound method TornadoConnection._on_connection_tune of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>> for \"0:Connection.Tune\"\nDEBUG      2015-12-18 09:48:01,805 pika.connection                _create_heartbeat_checker            1003: Creating a HeartbeatChecker: 60\nDEBUG      2015-12-18 09:48:01,805 pika.callback                  add                                  164 : Added: {'callback': <bound method TornadoConnection._on_connection_open of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,805 pika.callback                  process                              220 : Processing 0:Connection.OpenOk\nDEBUG      2015-12-18 09:48:01,805 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,805 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,805 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method TornadoConnection._on_connection_open of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,805 pika.callback                  process                              234 : Calling <bound method TornadoConnection._on_connection_open of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>> for \"0:Connection.OpenOk\"\nDEBUG      2015-12-18 09:48:01,805 pika.callback                  add                                  164 : Added: {'callback': <bound method TornadoConnection._on_connection_closed of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,806 pika.callback                  process                              220 : Processing 0:_on_connection_open\nDEBUG      2015-12-18 09:48:01,806 pika.callback                  process                              234 : Calling <bound method TestCase.on_connect of <__main__.TestCase object at 0x10ad60390>> for \"0:_on_connection_open\"\nDEBUG      2015-12-18 09:48:01,806 pika.connection                _create_channel                      991 : Creating channel 1\nDEBUG      2015-12-18 09:48:01,806 pika.callback                  add                                  164 : Added: {'callback': <bound method TornadoConnection._on_channel_cleanup of <pika.adapters.tornado_connection.TornadoConnection object at 0x10ad60510>>, 'only': <pika.channel.Channel object at 0x10ad60fd0>, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,806 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_getempty of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-18 09:48:01,806 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_cancel of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-18 09:48:01,806 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_flow of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-18 09:48:01,806 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_close of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,806 pika.channel                   _rpc                                 1130: Adding in on_synchronous_complete callback\nDEBUG      2015-12-18 09:48:01,806 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,806 pika.channel                   _rpc                                 1135: Adding passed in callback\nDEBUG      2015-12-18 09:48:01,806 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,807 pika.callback                  process                              220 : Processing 1:Channel.OpenOk\nDEBUG      2015-12-18 09:48:01,807 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,807 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,807 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,807 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,807 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,807 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,807 pika.callback                  process                              234 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>> for \"1:Channel.OpenOk\"\nDEBUG      2015-12-18 09:48:01,808 pika.channel                   _on_synchronous_complete             1088: 0 blocked frames\nDEBUG      2015-12-18 09:48:01,808 pika.callback                  process                              234 : Calling <bound method Channel._on_openok of <pika.channel.Channel object at 0x10ad60fd0>> for \"1:Channel.OpenOk\"\nDEBUG      2015-12-18 09:48:01,808 pika.channel                   _rpc                                 1130: Adding in on_synchronous_complete callback\nDEBUG      2015-12-18 09:48:01,808 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'queue': '1'}, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,808 pika.channel                   _rpc                                 1135: Adding passed in callback\nDEBUG      2015-12-18 09:48:01,808 pika.callback                  add                                  164 : Added: {'callback': <functools.partial object at 0x10ad5fba8>, 'only': None, 'one_shot': True, 'arguments': {'queue': '1'}, 'calls': 1}\nINFO       2015-12-18 09:48:01,808 root                           on_channel_open                      41  : q-9 declared\nINFO       2015-12-18 09:48:01,808 root                           on_channel_open                      43  : consumer started: ctag1.7bd386acfde64b20ae182d5e1de8be57\nINFO       2015-12-18 09:48:01,808 root                           on_channel_open                      45  : consumer_cancel called\nINFO       2015-12-18 09:48:01,808 root                           on_channel_open                      48  : calling queue_declare for q=4\nINFO       2015-12-18 09:48:01,808 root                           on_channel_open                      48  : calling queue_declare for q=5\nDEBUG      2015-12-18 09:48:01,808 pika.callback                  process                              220 : Processing 1:Queue.DeclareOk\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  _dict_arguments_match                350 : Comparing {'queue': '1'} to {'queue': '1'}\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'queue': '1'}, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  _dict_arguments_match                350 : Comparing {'queue': '1'} to {'queue': '1'}\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  remove                               269 : Removing callback #0: {'callback': <functools.partial object at 0x10ad5fba8>, 'only': None, 'one_shot': True, 'arguments': {'queue': '1'}, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  process                              234 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>> for \"1:Queue.DeclareOk\"\nDEBUG      2015-12-18 09:48:01,809 pika.channel                   _on_synchronous_complete             1088: 7 blocked frames\nDEBUG      2015-12-18 09:48:01,809 pika.channel                   _rpc                                 1130: Adding in on_synchronous_complete callback\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'queue': '2'}, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,809 pika.channel                   _rpc                                 1135: Adding passed in callback\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  add                                  164 : Added: {'callback': <functools.partial object at 0x10ad5fc58>, 'only': None, 'one_shot': True, 'arguments': {'queue': '2'}, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,809 pika.callback                  process                              234 : Calling <functools.partial object at 0x10ad5fba8> for \"1:Queue.DeclareOk\"\nINFO       2015-12-18 09:48:01,809 root                           on_queue_declared                    52  : queue_name=1\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  process                              220 : Processing 1:Queue.DeclareOk\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  _dict_arguments_match                350 : Comparing {'queue': '2'} to {'queue': '2'}\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'queue': '2'}, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  _dict_arguments_match                350 : Comparing {'queue': '2'} to {'queue': '2'}\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  remove                               269 : Removing callback #0: {'callback': <functools.partial object at 0x10ad5fc58>, 'only': None, 'one_shot': True, 'arguments': {'queue': '2'}, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  process                              234 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>> for \"1:Queue.DeclareOk\"\nDEBUG      2015-12-18 09:48:01,810 pika.channel                   _on_synchronous_complete             1088: 6 blocked frames\nDEBUG      2015-12-18 09:48:01,810 pika.channel                   _rpc                                 1130: Adding in on_synchronous_complete callback\nDEBUG      2015-12-18 09:48:01,810 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'queue': '3'}, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,810 pika.channel                   _rpc                                 1135: Adding passed in callback\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  add                                  164 : Added: {'callback': <functools.partial object at 0x10ad5fcb0>, 'only': None, 'one_shot': True, 'arguments': {'queue': '3'}, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  process                              234 : Calling <functools.partial object at 0x10ad5fc58> for \"1:Queue.DeclareOk\"\nINFO       2015-12-18 09:48:01,811 root                           on_queue_declared                    52  : queue_name=2\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  process                              220 : Processing 1:Queue.DeclareOk\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  _dict_arguments_match                350 : Comparing {'queue': '3'} to {'queue': '3'}\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'queue': '3'}, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  _dict_arguments_match                350 : Comparing {'queue': '3'} to {'queue': '3'}\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  remove                               269 : Removing callback #0: {'callback': <functools.partial object at 0x10ad5fcb0>, 'only': None, 'one_shot': True, 'arguments': {'queue': '3'}, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,811 pika.callback                  process                              234 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>> for \"1:Queue.DeclareOk\"\nDEBUG      2015-12-18 09:48:01,811 pika.channel                   _on_synchronous_complete             1088: 5 blocked frames\nDEBUG      2015-12-18 09:48:01,812 pika.channel                   _rpc                                 1130: Adding in on_synchronous_complete callback\nDEBUG      2015-12-18 09:48:01,812 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'queue': '9'}, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,812 pika.channel                   _rpc                                 1135: Adding passed in callback\nDEBUG      2015-12-18 09:48:01,812 pika.callback                  add                                  164 : Added: {'callback': <functools.partial object at 0x10ad5fd08>, 'only': None, 'one_shot': True, 'arguments': {'queue': '9'}, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,812 pika.callback                  process                              234 : Calling <functools.partial object at 0x10ad5fcb0> for \"1:Queue.DeclareOk\"\nINFO       2015-12-18 09:48:01,812 root                           on_queue_declared                    52  : queue_name=3\nDEBUG      2015-12-18 09:48:01,812 pika.callback                  process                              220 : Processing 1:Queue.DeclareOk\nDEBUG      2015-12-18 09:48:01,812 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,812 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,812 pika.callback                  _dict_arguments_match                350 : Comparing {'queue': '9'} to {'queue': '9'}\nDEBUG      2015-12-18 09:48:01,812 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'queue': '9'}, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,812 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,812 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,813 pika.callback                  _dict_arguments_match                350 : Comparing {'queue': '9'} to {'queue': '9'}\nDEBUG      2015-12-18 09:48:01,813 pika.callback                  remove                               269 : Removing callback #0: {'callback': <functools.partial object at 0x10ad5fd08>, 'only': None, 'one_shot': True, 'arguments': {'queue': '9'}, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,813 pika.callback                  process                              234 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>> for \"1:Queue.DeclareOk\"\nDEBUG      2015-12-18 09:48:01,813 pika.channel                   _on_synchronous_complete             1088: 4 blocked frames\nDEBUG      2015-12-18 09:48:01,813 pika.channel                   _rpc                                 1130: Adding in on_synchronous_complete callback\nDEBUG      2015-12-18 09:48:01,813 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.7bd386acfde64b20ae182d5e1de8be57'}, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,813 pika.channel                   _rpc                                 1135: Adding passed in callback\nDEBUG      2015-12-18 09:48:01,813 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.7bd386acfde64b20ae182d5e1de8be57'}, 'calls': 1}\nDEBUG      2015-12-18 09:48:01,813 pika.callback                  process                              234 : Calling <functools.partial object at 0x10ad5fd08> for \"1:Queue.DeclareOk\"\nINFO       2015-12-18 09:48:01,813 root                           on_queue_declared                    52  : queue_name=9\nDEBUG      2015-12-18 09:48:01,813 pika.callback                  process                              220 : Processing 1:Basic.ConsumeOk\nDEBUG      2015-12-18 09:48:01,813 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,813 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,814 pika.callback                  _dict_arguments_match                350 : Comparing {'consumer_tag': 'ctag1.7bd386acfde64b20ae182d5e1de8be57'} to {'consumer_tag': 'ctag1.7bd386acfde64b20ae182d5e1de8be57'}\nDEBUG      2015-12-18 09:48:01,814 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.7bd386acfde64b20ae182d5e1de8be57'}, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,814 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-18 09:48:01,814 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-18 09:48:01,814 pika.callback                  _dict_arguments_match                350 : Comparing {'consumer_tag': 'ctag1.7bd386acfde64b20ae182d5e1de8be57'} to {'consumer_tag': 'ctag1.7bd386acfde64b20ae182d5e1de8be57'}\nDEBUG      2015-12-18 09:48:01,814 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10ad60fd0>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.7bd386acfde64b20ae182d5e1de8be57'}, 'calls': 0}\nDEBUG      2015-12-18 09:48:01,814 pika.callback                  process                              234 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x10ad60fd0>> for \"1:Basic.ConsumeOk\"\nDEBUG      2015-12-18 09:48:01,814 pika.channel                   _on_synchronous_complete             1088: 3 blocked frames\nDEBUG      2015-12-18 09:48:01,814 pika.callback                  process                              234 : Calling <bound method Channel._on_eventok of <pika.channel.Channel object at 0x10ad60fd0>> for \"1:Basic.ConsumeOk\"\nDEBUG      2015-12-18 09:48:01,814 pika.channel                   _on_eventok                          993 : Discarding frame <METHOD(['channel_number=1', 'frame_type=1', \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.7bd386acfde64b20ae182d5e1de8be57'])>\"])>\n. Got it! It's a pika bug, indeed. Since there were several back-to-back nowait=False requests after declaration of the first queue, they are all initially queued up on Channel._blocked queue. One, by one, they get processed as the result of the Channel._on_synchronous_complete callback. However, in the case that basic_cancel() is called with nowait=True, it passes acceptable_replies=[] to Channel._rpc(). _rpc() checks if the method frame is synchronous, which is hardcoded to True for spec.Basic.Cancel, and so it places the channel in blocking mode, but since nowait=True caused acceptable_replies=[] to be passed, _rpc doesn't register Channel._on_synchronous_complete callback. \nSince there is no completion event for the Basic.Cancel and Channel._on_synchronous_complete doesn't get called, the processing of the remaining blocked frames never resumes, and so queue=4 and queue=5 declarations never get processed.\nCC @gmr, @Strawhatfy, @dwt\n. @gmr, @Strawhatfy, @dwt: PR #675 should resolve this issue\n. libev_connection appears to be the only adapter in pika that does anything with signals:\npika/adapters/libev_connection.py:import signal\npika/adapters/libev_connection.py:    If an on_signal_callback method is provided, the adapter creates signal\npika/adapters/libev_connection.py:    signal. See pyev/libev signal handling to understand why this is done.\npika/adapters/libev_connection.py:    :param on_signal_callback: Method to call if SIGINT or SIGTERM occur\npika/adapters/libev_connection.py:    :type on_signal_callback: method\npika/adapters/libev_connection.py:                 on_signal_callback=None):\npika/adapters/libev_connection.py:        :param on_signal_callback: Method to call if SIGINT or SIGTERM occur\npika/adapters/libev_connection.py:        :type on_signal_callback: method\npika/adapters/libev_connection.py:        self._on_signal_callback = on_signal_callback\npika/adapters/libev_connection.py:        LOGGER.debug('init io and signal watchers if any')\npika/adapters/libev_connection.py:        # reuse existing signal watchers, can only be declared for 1 ioloop\npika/adapters/libev_connection.py:            if self._on_signal_callback and not global_sigterm_watcher:\npika/adapters/libev_connection.py:                    self.ioloop.signal(signal.SIGTERM,\npika/adapters/libev_connection.py:            if self._on_signal_callback and not global_sigint_watcher:\npika/adapters/libev_connection.py:                global_sigint_watcher = self.ioloop.signal(signal.SIGINT,\npika/adapters/libev_connection.py:            if self._on_signal_callback:\npika/adapters/libev_connection.py:            if self._on_signal_callback:\npika/adapters/libev_connection.py:    def _handle_sigint(self, signal_watcher, libev_events):\npika/adapters/libev_connection.py:        \"\"\"If an on_signal_callback has been defined, call it returning the\npika/adapters/libev_connection.py:        self._on_signal_callback('SIGINT')\npika/adapters/libev_connection.py:    def _handle_sigterm(self, signal_watcher, libev_events):\npika/adapters/libev_connection.py:        \"\"\"If an on_signal_callback has been defined, call it returning the\npika/adapters/libev_connection.py:        self._on_signal_callback('SIGTERM')\n. @chengchengpei: which kernel are you using where this problem occurs? What's the uname -a ?\n. @chengchengpei, it's not in the release yet. 0.10.0 is at b907f91 (https://github.com/pika/pika/releases/tag/0.10.0), which predates @shinji-s's patch.\n@chengchengpei, would you mind installing pika from master branch and verifying the fix? It would be great to get this patch released.\n. @chengchengpei: thank you for testing it.\n@gmr: Gavin, would you mind cutting a new release?\n. @chengchengpei: This works for me (in ipython):\n```\nIn [1]: import pika\nIn [2]: pika.version\nOut[2]: '0.10.0'\nIn [3]: pika\nOut[3]: \n```\nAlso:\n```\nIn [15]: import pkg_resources\nIn [16]: pkg_resources.get_distribution(\"pika\")\nOut[16]: pika 0.10.0 (/Users/vkruglikov/Library/Python/2.7/lib/python/site-packages)\n``\n. @chengchengpei, I am not familiar with the.virtualenvs` directory hierarchy. I am guessing you or something on your system is using python virtualenv tools. \nIn python, it's possible for a module to be installed in multiple places at the same time, including as root, --user, etc. When I want to make sure that the pika package is completely uninstalled, I run pip uninstall pika multiple times, until it tells me that there aren't any more. If pip was installed at the system level, you might get a permissions error message when uninstalling, in which case you need to re-run the command with sudo pip uninstall pika. After that, when you install pika, it should be the only version on your system.\n. @chengchengpei, regarding your vhost checks and creation API questions, rabbitmq has a REST API that you can use, assuming you have the RabbitMQ Management Plugin installed (I don't remember if it gets installed by default, but if you can access your RabbitMQ broker's web GUI, then the management plugin should be in place). For example, if your rabbitmq broker is running on localhost, then you can get a summary of the API by browsing http://localhost:15672/api.\nHere is an example of python code that creates a vhost: https://github.com/numenta/numenta-apps/blob/1ff572a21a5c27fd290822e572ce33f42e1ee19e/nta.utils/nta/utils/test_utils/amqp_test_utils.py#L145-L160\nhttps://groups.google.com/forum/#!forum/rabbitmq-users is a good group to search and post for RabbitMQ usage.\n. > pika/pika can not check or create vhost?\n@chengchengpei: I don't think so. I guess that the rationale is that pika implements the AMQP API, including RabbitMQ extensions. Vhosts and REST API are not part of AMQP.\nLet's move this conversation out of this pika issue; the unrelated comments make it difficult for someone to make sense of the actual issue. Best places for questions like these are either https://groups.google.com/forum/#!forum/rabbitmq-users or https://groups.google.com/forum/#!forum/pika-python.\nBest,\nVitaly\n. Closing, fix is in master via PR #637, as noted and confirmed by @chengchengpei in earlier comments.\n. Fixed via #666 - in master\n. Thanks, @nickcash for the quick follow up. Would you mind adding a few simple unit tests in tests/unit/connection_tests.py for the change? But please hold off on that until @gmr agrees to this change in behavior (see my note below). If he does, then the following test cases would be good to cover:\n1. client passes 0, so _send_connection_tune_ok should send 0 heartbeat timeout to broker in spec.Connection.TuneOk\n2. Broker passes bigger value, so  _send_connection_tune_ok should send the server's heartbeat timeout to broker in spec.Connection.TuneOk\n3. User passes bigger value, so  _send_connection_tune_ok should send the user's heartbeat timeout to broker in spec.Connection.TuneOk\n4. Same values from both broker and user\nIs there a possibility that the server might send 0?\n@gmr, could you please take a look? This change was recommended by Michael Klishin in https://groups.google.com/forum/#!topic/rabbitmq-users/FpUdTaIr8CI.\n. @nickcash, I was wondering that myself. I would like to hear @gmr's opinion on it.\n. Gavin, what's your opinion regarding @nickcash comment concerning the channel max, etc.?\n. @nickcash, regarding\n\nSo maybe a better change would be to switch _combine() to use max instead? If matching the Java behavior is desired, that is.\n\nMax is not appropriate for resource limits, such as frame size, max channels, etc. See the following excerpt from AMQP 0-9-1 spec and the last line, in particular.\nPer https://www.rabbitmq.com/resources/specs/amqp0-9-1.pdf:\n\n2.3.3 Protocol Negotiation\nThe AMQP client and server negotiate the protocol. This means that when the client connects, the server\nproposes certain options that the client can accept, or modify. When both peers agree on the outcome, the\nconnection goes ahead. Negotiation is a useful technique because it lets us assert assumptions and\npreconditions.\nIn AMQP, we negotiate a number of specific aspects of the protocol:\n\ue00c The actual protocol and version. The server MAY host multiple protocols on the same port.\n\ue00c Encryption arguments and the authentication of both parties. This is part of the functional layer,\nexplained previously.\n\ue00c Maximum frame size, number of channels, and other operational limits.\nAgreed limits MAY enable both parties to pre-allocate key buffers, avoiding deadlocks. Every incoming\nframe either obeys the agreed limits, and so is \"safe\", or exceeds them, in which case the other party IS\nfaulty and MUST be disconnected. This is very much in keeping with the \u201cit either works properly or it\ndoesn't work at all\u201d philosophy of AMQP.\nBoth peers negotiate the limits to the lowest agreed value as follows:\n\ue00c The server MUST tell the client what limits it proposes.\n\ue00c The client responds and MAY reduce those limits for its connection.\n. @nickcash: +1 for this change; could you please add test coverage to facilitate merging? See https://github.com/pika/pika/pull/666#issuecomment-159404462. Thank you.\n. That's great, @nickcash, thank you.\n. ~~Hi @nickcash, I implemented the new heartbeat tuning test logic that corresponds to your changes in this PR. This is the commit that contains the test change: https://github.com/vitaly-krugl/pika/commit/6ef71f2d12d19128075624587d5fc21f6b4b8644. I wasn't able to submit a PR into your fork, but you can probably pull my commit into your branch.~~\n\n~~If you like my test logic commit, please pull it into your pull request so that we can merge your PR.~~\nIs there a github workflow that permits me to submit a PR into your fork?\nThanks,\nVitaly\n. @nickcash: UPDATE - I created a new commit https://github.com/vitaly-krugl/pika/commit/cb57ec406d0b9321cf4fd391ef890cc9e272c658 to replace the one from my earlier comment that treats 0 from client and server the same and documented the reasoning behind it and how it applies to heartbeat negotiation per AMQP 0.9.1 specification. The commit also has new test logic to cover all the cases. Please consider merging it into your PR. Thx.\n. @nickcash, please see my Pull Request https://github.com/nickcash/pika/pull/2 in your fork for tests and other changes related to this PR instead of the previously-mentioned commit.\n. Thanks, and I appreciate your initiative in laying the groundwork.\n. @nickcash, please take a look at the new PR https://github.com/nickcash/pika/pull/3. It should resolve the merge conflict against master.\n. Thanks @nickcash, merging ...\nCC @gmr \n. @avihoo - it's possible, if the PR was merged after release 0.10.0.. Hi @avihoo - I don't know when the next release will go out. People sometimes uses PIP to install directly from a SHA in master when they need a fix that hasn't been released yet. I am sorry I can't offer you a better answer at this time.. @linnik: PR #691 should fix this resource warnings.\n. @taoqf, for questions, a better option is to post on the rabbitmq-users google group.\n. Instead of posting questions in a closed issue, try here: https://groups.google.com/forum/#!forum/rabbitmq-users\n. @gmr and @JetDrag: @wjps added \"interrupt\" socket pair logic as part of PR #555. I recall that he intended for it to work on Windows, but I don't know whether he had access to a Windows system to test the logic. There are references to Win(dows) in the SelectPoller.get_interrupt_pair docstring.\nAs always, usage from multiple threads is something to be wary of.\nFor debugging, it would help if the user posted DEBUG-level pika logs of the entire session. Hopefully, the session that reproduces it would be a short and as simple as possible to make it easier to debug.\n@JetDrag,  which python version are you using to reproduce this exception on Windows? Are you using multiple threads in your application? Are you able to run all the pika unit and acceptance tests on your Windows machine? Can you post DEBUG-level logs from a simple pika session that reproduces the issue?\n. @JetDrag,  which python version are you using to reproduce this exception on Windows? Are you using multiple threads in your application? Are you able to run all the pika unit and acceptance tests on your Windows machine? Can you post DEBUG-level logs from a simple pika session that reproduces the issue? Also, please post a complete traceback. The one you provided originally is not sufficient.\n. The \"Bad file descriptor\" bug is now fixed in the pika master branch. Specifically: https://github.com/pika/pika/blob/c36b16e5aaaa82f938afa91a65432a4b82da2a56/pika/adapters/select_connection.py#L544. See https://github.com/pika/pika/pull/735.\nPlease try with pika installed from the current master branch.\n. When a connection is closed, all channels on that connection are also (implicitly) closed. You cannot continue to use channels from a previously-closed connection.\n. @atuljangra, please check against pika master branch. This issue should be resolved via PR #685, which addressed a lot of issues related to server-initiated closing of connections.\n``` ipython\nIn [1]: import pika\nIn [2]: c = pika.BlockingConnection()\nIn [3]: ch = c.channel()\nAfter creating the channel, I used the RabbitMQ Management Web UI to force the connection closed\nIn [4]: c.process_data_events()\nConnectionClosed: (320, 'CONNECTION_FORCED - Closed via management plugin')\nIn [5]: c.process_data_events()\nConnectionClosed\nIn [6]: ch.is_open\nOut[6]: False\nIn [7]: ch.queue_delete(\"abcdef\")\nChannelClosed\n```\n. @gmr: ~~could you please revert this PR; it breaks all the Python 3 builds~~\nOoops, I think I was wrong, strike that!\n. @adrianheron: In pika master, the socket is configured to be non-blocking after connection is established, so you should never see a socket timeout error after that. During connection establishment, a different error-handling strategy is used, so it would never end up in _handle_error during connection establishment.\nPlease provide the following additional information:\n1. What's the platform\n2. Which version of pika are you using and on which python version?\n3. Steps to reproduce (if known/reproducible)\n4. Full traceback of the exception\n5. pika log output  of the failing session\n. @adrianheron, would you mind attaching a complete traceback? Also, debug-level pika logs? thx\nAnd which version of pika are your reporting?\n. @adrianheron: Last night, I verified the issue as follows, using BlockingConnection (which is now implemented on top of the SelectConnection/BaseConnection/Connection stack):\n1. Connect to Rabbit MQ on remote machine\n2. Disconnect client from network (pull the Ethernet cable)\n3. Attempt to create a channel on that connection.\nThis triggered error(60, 'Operation timed out') (ETIMEDOUT) when the socket tried to send. On pika 0.10.0, this results in socket.timeout from _handle_error just as you indicated.\nThere is already a fix for this issue in master via PR #685 \nat line https://github.com/pika/pika/pull/685/files#diff-1a5308d8b684fdafac1f89e376e8933dL326\nWith PR #685, the connection will be dropped and the Connection's on_close_callback should be called with the error information instead of raising socket.timeout.\n. Fixed via PR #685 in pika master (not released yet)\n. CC @gmr: Hi Gavin, please review this fix.\n. @gmr, the failed test BlockingChannel.basic_nack single message is unrelated to the changes in this PR; it's a known issue described in #677. \nPlease re-start the build. Thanks!\n. @quantum5, the tests are failing for python 3.x, because it only defines a single integer type: int. You will need to solve this via the https://github.com/pika/pika/blob/master/pika/compat.py module, probably by defining is_interger(value): in the if not PY2: block, it should check only for int, and in the other block (python 2.x) it should check for (int, long).\n. @quantum5, that's a good thing to fix; https://www.rabbitmq.com/amqp-0-9-1-reference.html#domain.delivery-tag defines it as longlong, which is defined as 64-bit integer. The code in pika's spec.py uses >Q to pack and unpack the tag: pieces.append(struct.pack('>Q', self.delivery_tag)), with Q defined as \"unsigned long long\" in https://docs.python.org/2/library/struct.html. Hopefully, for this purpose, the Q is compatible with AMQP's 64-bit integer.\n. @gmr, the Travis status build of this PR appears to be missing.\n. Please add tests to the PR. Thx\n. @quantum5, I implemented tests for your PR; please consider merging the tests from https://github.com/vitaly-krugl/pika/commit/25d917b8a0dfa322aa26debb4dc72e226ef0f7b1 into your PR.\nI would like to get this merged into master soon.\nI was trying to create a pull request into your pika fork, but couldn't get DMOJ/pika for the base fork.\n. @quantum5, I saw the breakage on py3 from my tests. I will submit a PR into your fork when I have a confirmed fix. Thx.\n. PR https://github.com/DMOJ/pika/pull/1 should fix the build breakage\n. @quantum5 Something went wrong with the merge - the PR picked up a lot of changes: 23 files and 35 commits. I created a clean test PR that has both your and my commits combined in it and it passes all checks: see PR #705\n. @quantum5, I am closing this PR via PR #705 that I created from yours and which is rebased against current Pika master branch. I credited you in version history here: https://github.com/pika/pika/pull/705/files#diff-cfc895343b31cee85bb3efb82bc19fb2R20\n. @gst: LGTM! Thank you.\nThis bug in the test framework must have gone unnoticed for a while.\n. @gst, could you please merge/rebase against master to resolve the merge conflict? Thx\n. @gst, regarding \n\nthere, not 200% sure about how I fixed the merge conflict.. can you double-check it ? \n\nI do it like this: merge with the new master, then rebase against the new master.\n. LGTM - thanks\n. @gst, this is a more general solution that is intended to replace PR #660. Please review.\n. @wjps: Hi Will, it turns out that the \"tornado write starvation\" PR #578 broke a number of code paths in pika. This PR undoes the breaking change in BaseConnection._flush_outbound(). The comment https://github.com/pika/pika/pull/660#issuecomment-167213898 has a more detailed analysis of the problem.\n. It looks like Travis CI is still having issues: https://www.traviscistatus.com/. All of the failures are in the initialization: The command \"sudo apt-get update -qq\" failed and exited with 100 during .\n. Resolved merge conflict and rebased against new master\n. Resolved merge conflict\n. @JetDrag, the asynchronous publisher example had a number of other problems, besides the one that you were trying to address. I went ahead and cleaned it up in a new PR. Please take a look at PR #710. It should address your issue and several others. If you are able to reproduce your issue on PR #710 using the latest Pika from master, please document the steps to reproduce in a comment in PR #710.\nMany thanks!\n. @JetDrag, would you mind responding to my previous comment?\nThanks!\nBest,\nVitaly\n. @JetDrag, I haven't heard back from you. I believe that PR #710 addresses your issue and a number of others as well. I am going to close your PR in favor of #710. If #710 doesn't address your issue, please respond to all of my questions in https://github.com/pika/pika/pull/681#issuecomment-185562871. Than you.\n. Here is the wireshark capture of the doomed session:\npikavhost.txt\n. I am working on the fix\n. Can anyone help me figure out why assert_any_call is failing to match in the 3 unit test methods in the python 2.6 build https://travis-ci.org/pika/pika/jobs/100035773 as well as the python 2.7 build?  Those same unit tests are passing on all of the Python 3.x builds.  Many thanks.\nP.S. the other failing builds are due to segfault that will be addressed by PR #683\nCC @gst, @gmr \n. @gst, I was off the grid for a few days. Just got back today. Thanks for looking at this problem.\nI don't know which version of mock it uses, but https://github.com/pika/pika/blob/master/test-requirements.txt specifies mock without a version. This would imply that it would load the most recent version. \nI like your idea about adding pip freeze to travis test runs. Perhaps this is something that could be added via https://github.com/pika/pika/blob/master/.travis.yml?\n. @gst, I am able to reproduce these problems with mock 1.3.0 on python 2.7.10.\nI've had nothing but trouble with the latest mock.\nThe tests work fine with mock 1.0.1, but fail beginning with mock 1.1.0\n. @gst, getting rid of the spec arg in the mock allows the tests to pass now. I also switched to a context manager for patching, but I don't think that the context manager is important in this case. However, the spec/autospec/spec_set are really valuable, and it's a shame that I can't get the tests to pass using them.\nI filed an issue with the mock project: https://github.com/testing-cabal/mock/issues/338\n. @gst, would you like to submit a PR with the pip freeze change? thx\n. Resolved merge conflict and rebased\n. Hi @boyxuper, I don't have a good handle on signal handling in python. Good documentation on what is and what is not safe to do from a signal handler in Python doesn't seem to exist. For example, in the C language, there are very few things that are asynchronous-signal-safe. Adding a timeout would certainly not be signal safe - I would safe that the result of adding a timeout or interacting with a connection from a signal handler is undefined (in other words, bad things can happen).\nWhy not just raise an exception from your SIGTERM handler? Wouldn't that interrupt the consume loop?\nHere is another approach that would be safe, but involves polling: In Pika 0.10.x, the BlockingChannel.consume API has a new parameter inactivity_timeout (take a look at the docstring in the source code). You could have your signal handler set a flag somewhere and then poll periodically from your consume loop with the help of the inactivity_timeout parameter. This should be safe and supported. I personally don't like polling for obvious reasons, but sometimes that's all you can do.\nFinally, you should not be accessing connection._impl for anything. _impl is private and will change without notice. For supported public API, take a look at the latest BlockingConnection and BlockingChannel source code: https://github.com/pika/pika/blob/master/pika/adapters/blocking_connection.py.\n. Also, instead of filing issues in the repo, it's better to ask questions on the pika google group: https://groups.google.com/forum/#!forum/pika-python\n. @boyxuper:\nPika API is not thread-safe, and you have to assume that it's at least as sensitive to signal handlers.\n1. Unfortunately, I don't have time to look for references about handling signals, you will need to do the research yourself. If I recall correctly, python runtime invokes your signal callback between python opcodes. Imagine that the timer-management structure is already being modified at the very moment when the signal occurs and your signal callback is invoked before the previous timer modification completes. This can result in corruption of the timer management structure.\n2. Yes, raising an exception from your SIGTERM handler will randomly interrupt consumer execution. Are you concerned that you might lose a message or incompletely process a message that is being processed at the time? A robust production app would need to be able to cope with sudden failure, such as segmentation fault, out-of-memory (and out-of-memory killer), kernel-panic, etc. If your app can already deal with those, then interrupting a consumer is no big deal.\n3. Yes, you can use the inactivity_timeout feature to check a flag that would be set by your signal handler and simply break out of the loop and/or cancel the consumer.\n4. Why are you using connection._impl.add_timeout (unsupported) instead of connection.add_timeout (supported)?\n. @boyxuper, you're welcome.\n. Thanks, @xiaopeng163 \n. @awelzel, the use case in test_queue_declare_nowait_send_method above and in the attached example is invalid, because it's invalid to pass a completion callback with an asynchronous request: it's impossible for this callback to ever be called with nowait=True, so calling queue_declare this way is an application error.\n. @awelzel: Thanks for thinking about it and your feedback. Good point about the message in the exception. The parameter check in _rpc() also catches internal bugs. What would be a good place for a more user-friendly check?\n. @awelzel, I took another look. I think I can simply change the current cryptic message \"A populated callback must be accompanied ...\" to something more meaningful.\n. @awelzel - would this work https://github.com/pika/pika/pull/690/files#diff-831cec7ce02158498fb9bb8d1fd55451R1128 ?\n. @awelzel, can this issue be closed, since the source of the issue is application error?\n. Please close.\n. LGTM, @gst \nThis will be good for debugging issues, such as those that I encountered with mock while working on #685.\n. Resolved merge conflict and rebased\n. Resolved merge conflict and rebased against new master\n. @gmr, do you mean that you're okay if a PR contains multiple commits? Some projects ask for single commit per PR, and I wasn't sure about pika.\n. Please post the source code that you use for setting your project log level to DEBUG\n. Regarding\n\nwhen I set the level to DEBUG, i will get message like this\n\nHow do you set the level to DEBUG?\n. In my own debugging, if I want to see just my own app's DEBUG logs, and leave everyone else's unchanged, I simply set the log level of my own app's log context root to DEBUG. For example, if my app creates loggers like this: logging.getLogger(\"taurus.engine\"), logging.GetLogger(\"taurus.collector\"), etc., and I want to see just my app's own DEBUG-level logging output, I simply change the log level for my own app and leave everyone else's alone like this: logging.getLogger(\"taurus\").setLevel(logging.DEBUG)\n. @awelzel: I use logging.config.fileConfig with disable_existing_loggers=False, as in this code: https://github.com/numenta/numenta-apps/blob/83bddd39ad36d0b102ca8f68c087717d470b5cc0/nta.utils/nta/utils/logging_support_raw.py#L286, where the root defaults to INFO per https://github.com/numenta/numenta-apps/blob/master/nta.utils/conf/logging.conf.\nIn trying to come up with a simpler example, I wonder if the following would work for setting my own logger to DEBUG, while keeping everyone else's at INFO, for example:\npython\nlogging.basicConfig(level=logging.INFO)  # set everyone to INFO\nlogging.getLogger(\"taurus\").setLevel(logging.DEBUG)  # set just my own level to DEBUG\nOkay, here is an actual example via 2.7.10 python shell; observe that other_logger.debug(...) doesn't produce anything, as intended:\n``` python\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogging.getLogger(\"taurus\").setLevel(logging.DEBUG)\nother_logger = logging.getLogger(\"blah.something\")\nmy_logger = logging.getLogger(\"taurus.something.else\")\nother_logger.debug(\"other logger says hello at DEBUG level\")\nother_logger.info(\"other logger says hello at INFO level\")\nINFO:blah.something:other logger says hello at INFO level\nmy_logger.debug(\"my logger says hello at DEBUG level\")\nDEBUG:taurus.something.else:my logger says hello at DEBUG level\nmy_logger.info(\"my logger says hello at INFO level\")\nINFO:taurus.something.else:my logger says hello at INFO level\n```\n. @xiaopeng163, I think this resolves your question. If it does, please close the issue.\n. The rabbitmq-users group is a much better place to search and ask question like this one, since pika merely returns these values from the broker. See https://groups.google.com/forum/#!searchin/rabbitmq-users/message-count/rabbitmq-users/AM23w2CDxsw/y9I4LrM4U2MJ\n. @sashas-lb: it really cannot enter the master branch without a comprehensive test suite. It's impossible to maintain, otherwise. See discussion in previous comments. Testing is really the bulk of the work in these adapters.\n. I added a comprehensive test suite single-handedly when I rewrote the BlockingConnection adapter, so it can be done if someone is sufficiently motivated.\nThe twisted adapter and others without sufficient test coverage have a history of breaking and being unstable. It was a conscious decision taken by @gmr and myself to not allow it to happen again in this project.\nIt's a common policy among established, stable projects to not accept submissions without adequate test coverage.\n. @carlosefr, my recommendation is to start writing tests and see how far you you will get. I think you might be surprised by how much progress you will make, as was I when working on BlockingConnection rewrite.\nComprehensive tests have another benefit: they serve as code examples and lend stability to the API, helping catch accidental changes of the API. \n. The core if fairly well tested - either directly or indirectly (via existing BlockingConnection and  async adapter tests).\nThis leaves:\n1. Asyncio API review/acceptance by the project's owner (@gmr); I am not at all versed in AsyncIO yet.\n2. Unit tests of the Asyncio adapter code (which is a pretty small adapter as far as the amount of code in it is concerned, so should not be too much effort)\n3. Asyncio adapter integration tests (since these are user-level, there should be sufficient documentation and existing integration tests can serve as template).\n. P.S., I agree that the core could be better documented. While rewriting the BlockingConnection adapter, I made an effort to improve the quality of docstrings in the core and elsewhere where I had trouble understanding during my development effort.\n. A split like this will bring on the typical version compatibility management/coordination issues. As a contributor, I found the current structure to be more efficient for making changes that affect the core and one or more adapters. Although, I understand that the current situation may be overwhelming for a single maintainer.\n. The split might also cause loss of a common interface in adapters where this is possible and practical (e.g., SelectConnection, TornadoConnection, LibevConnection).\nAlso, we are presently able to have a common acceptance test suite for the above-mentioned async adapters, but might not be able to do that after the split.\n. The concept makes sense to me. Have not completed the review yet.\n. Hi @gmr, I completed the code review. I found a couple of issues in the tests and also noted a suggestion for simplifying the test logic in the adapters.\n. @gmr: it's been bugging me a bit whether this should be part of ConnectionParameters and URLParameters. It's beginning to feel like these new client settings belong in ConnectionParameters and URLParameters, along with the likes of locale and virtual_host, instead of additional constructor args in all of the adapters. The rationale is that all of them are used during the connection handshake, just like locale and virtual_host.\nIf you agree, then one challenge that might come up is how to encode the client overrides dict in the URL for URLParameters. There might be an easy solution for it. The user can serialize the overrides dict fragment via JSON and then combine it with the rest of their URL (using urllib.quote, if needed). URLParameters can then reverse the process for this new parameter.\n. @gmr: Hi Gavin, would you mind considering an alternative proposal for setting the client overrides via ConnectionParameters/URLParameters? Please read the rationale in my previous comment here. The alternative proposal is a functional implementation against my own fork https://github.com/vitaly-krugl/pika/pull/4 and is on top of PR #714. It includes support in all of the connection Parameters classes as well as tests. Please note that only three modules are affected by this proposal: connection.py and two test scripts.\n. Hi @Ignalion, could you please attach a DEBUG-level pika log under these conditions? Also, just to be sure, are you by chance accessing the same pika connection (or any of its objects) from multiple threads?\n. @Ignalion,  thanks for the logs. I confirm that this log doesn't show anything beyond initialization (setting up of the consumer). Clearly, processing of messages should not depend on the 5-second timeout.\nNext step is to instrument  with additional logging, reproduce the issue, and attach the new log (with timestamps, just as your previous log):\nIn Connection._on_data_available:\n- log \"_on_data_available\" and the length of data_in on entry\n- log consumed_count and frame_value immediately after self._read_frame() returns.\nIn BlockingChannel._on_consumer_message_delivery:\n- On entry, log \"_on_consumer_message_delivery\", method and properties args, and length of body arg\nIn BlockingChannel._dispatch_events:\n- On entry, log \"_dispatch_events\", channel number, and length of self._pending_events\n- Inside the if type(evt) is _ConsumerDeliveryEvt: block, log evt.method, evt.properties, and length of evt.body\nIn BlockingConnection._dispatch_channel_events:\n- Log \"_dispatch_channel_events\" and self._channels_pending_dispatch on entry\n- Log \"Nested dispatch or dispatch blocked higher in call stack\" just before the return statement inside if not dispatch_acquired:\nBy the way, it would really help if you could submit a small, simple, fully-functional code sample that reproduces the issue.\n. @Ignalion, did you instrument the log statement for \"Nested dispatch or dispatch blocked higher in call stack\"?\nIt would be best to attach a code sample for reproducing.\n. @Ignalion, ~~I looked at your code snippets; don't see anything wrong with it. The problem must be in BlockingConnection.~~\nWell, actually, I see this bit of suspicious code here: response_dfr.addCallback(process_response) and then process_response performs ch.basic_publish. This might explain things. From which thread/context will process_response be called? I suspect that it's going to be called from a different thread, in which case it would cause the race condition that's responsible for the problem you're describing. \n. @Ignalion, would you mind attaching more of that log? The 9 lines don't seem to be enough to figure out what state the connection and channel were in. The more, the better. Thx.\n. @Ignalion: I created the following program that tries to emulate what your code is doing, but without the deferred business. So far, I've been running it on fedora and mac os x, but haven't been able to reproduce the issue. Would you mind taking a look and giving this a try in your environment. My goal is to have a \"normal\" BlockingConnection application (without twisted) that reproduces this issue. I welcome any changes to this problem that would help reproduce the issue that you're experiencing.\n```\nimport logging\nimport sys\nimport time\nimport pika\nclass IssueReproducer(object):\ndef init(self):\n    self.lastRxTime = None\n    self.queue = None\n    self.rxCount = 0\ndef run(self):\n    conn = pika.BlockingConnection()\n    conn.add_on_connection_blocked_callback(self.onConnBlocked)\nch = conn.channel()\n\n#ch.confirm_delivery()\n\nself.queue = ch.queue_declare(exclusive=True).method.queue\n\nch.basic_publish(exchange=\"\", routing_key=self.queue, body=\"1\")\nch.basic_publish(exchange=\"\", routing_key=self.queue, body=\"2\")\n\nself.lastRxTime = time.time()\n\nch.basic_qos(prefetch_count=1)\nch.basic_consume(self.onRx, queue=self.queue)\n\nch.start_consuming()\n\ndef onConnBlocked(self):\ng_log.debug(\"ZZZ CONNECTION WAS BLOCKED\")\npass\n\ndef onRx(self, ch, method, props, body):\n    now = time.time()\n    elapsed = now - self.lastRxTime\n    self.lastRxTime = now\nself.rxCount += 1\n\ng_log.debug(\"Rx # %s body=%s at %s elapsed=%s\",\nself.rxCount, body, now, elapsed)\nassert elapsed < 3, \"rx message exceeded timeout; elapsed={}; body={}\".format(\n  elapsed, body)\n\nch.basic_ack(delivery_tag=method.delivery_tag)\n\ng_log.debug(\"Publishing body = %s\", body)\nch.basic_publish(exchange=\"\", routing_key=self.queue, body=body)\n\ng_log.debug(\"Done publishing body = %s\", body)\ntime.sleep(0.2)\ndef main():\n  global g_log\nLOG_FORMAT = ('%(levelname) -10s %(asctime)s %(name) -30s %(funcName) '\n                '-35s %(lineno) -5d: %(message)s')\n  logging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT)\ng_log = logging.getLogger(\"TESTAPP\")\nIssueReproducer().run()\nif name == \"main\":\n  main()\n``\n. @Ignalion, also: try adding%(thread)dto your app's logging format, then add a statement to log from yourprocess_responseclosure (inside _on_request). After reproducing the problem, check the logs to see if the thread id of theprocess_response` log ever differs from the thread id of the pika logs. \n. @Ignalion, regarding\n\nUPD: I managed to reproduce the issue on my env even with running your script in parallel mode. It's very weird actually.\n\nWhat do you mean by \"running your script in parallel mode\"? What's \"parallel mode\"? Ho do you do that?\n. @Ignalion, when you said that you were running it in \"parallel mode\", I thought you meant that you changed something in my script to make parts of it run in parallel. About how long did it take to reproduce the problem using my script?\nIn that case, you know the next thing I am going to ask you :) is the detailed DEBUG-level logs from reproducing the bug with my script with the following:\n1. Uncomment all of the g_log statements in my script.\n2. Instrumentation per https://github.com/pika/pika/issues/698#issuecomment-175149306\nAlso, along with the logs, please post the version of my script that you ran to reproduce the issue.\nMany thanks.\n. @Ignalion Got it! Since your app is in the Twisted environment, pika's Twisted adapters should be a better fit, and without any need for running a separate thread.\nI don't have experience with Twisted and Pika's Twisted adapters, but it looks like you could submit a simple PR for pika's twisted_connection.py to make it work with an application-provided reactor, plus tests.\nThe other asynchronous adapters in pika, such as SelectConnection and TornadoConnection, now accept an optional ioloop arg that overrides the default ioloop. So, you could add an additional optional reactor kwarg to the Twisted TwistedConnection and TwistedProrocolConnection to override the default one (similarly to how the other adapters process the optional ioloop arg). In that case, you don't need to call start/stop to get the adapter going if your app is already responsible for running that reactor. Together with stop_ioloop_on_close=False, this just might work!\nYou could probably easily try this out by patching twisted_connection.reactor with your app's custom reactor before instantiating TwistedConnection or TwistedProrocolConnection. \n. @Ignalion, the script below (a specialized adaptation of your script) demonstrates that the pika connection is being accessed from another thread.\nTo reproduce:\n1. Execute the script\n2. When the log output to the terminal stops, interrupt execution via CTL-C\nThe last line on the terminal will show that a different thread was used, for example: CRITICAL   2016-01-29 16:28:10,377 root                           process_response                     68  : process_response cmd_id=1 called from another thread=140735189172224; pika_thread=123145306509312\nI don't know Twisted enough (at all) to understand its interaction with Posix threads and why the deferred callback in my script is not getting executed automatically until I interrupt the program via CTL-C. It would still be cool to know why and how to get the callback to trigger automatically in my script, but not critical. The main point is that it shows that the mechanism can result in access to a pika connection from multiple threads.\nDEBUG      2016-01-29 16:28:06,909 pika.callback                  process                              234 : Calling <bound method Channel._on_eventok of <pika.channel.Channel object at 0x100af0650>> for \"1:Basic.ConsumeOk\"\nDEBUG      2016-01-29 16:28:06,909 pika.channel                   _on_eventok                          1000: Discarding frame <METHOD(['channel_number=1', 'frame_type=1', \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.2899148cf17649b9ab422f945e9c6fad'])>\"])>\nINFO       2016-01-29 16:28:06,909 root                           run                                  97  : RPC listener started\nDEBUG      2016-01-29 16:28:06,909 root                           _on_request                          78  : RPC: got command 1\nDEBUG      2016-01-29 16:28:06,909 root                           retry                                29  : Put response for command 1\nDEBUG      2016-01-29 16:28:06,910 root                           _on_request                          85  : Leaving _on_request\n^CDEBUG      2016-01-29 16:28:10,377 root                           retry                                26  : result is available now: 1\nCRITICAL   2016-01-29 16:28:10,377 root                           process_response                     68  : process_response cmd_id=1 called from another thread=140735189172224; pika_thread=123145306509312\n``` python\nimport logging\nfrom Queue import Queue, Empty\nimport os\nimport threading\nfrom time import time\nfrom twisted.internet import reactor\nfrom twisted.internet.defer import inlineCallbacks, Deferred, returnValue, TimeoutError\nimport pika\ndef sleep(t):\n  awaken_later = Deferred()\n  reactor.callLater(t, awaken_later.callback, None)\n  return awaken_later\n@inlineCallbacks\ndef retry(func, exc, results_q, cmd_id, interval=0.1, timeout=30, ):\n  max_time = time() + timeout\n  while time() <= max_time:\n    try:\n      result = func()\n      logging.debug(\"result is available now: %s\", result)\n      returnValue(result)\n    except exc:\n      logging.debug(\"Put response for command %s\", cmd_id)\n      results_q.put(cmd_id)\n      yield sleep(interval)\n  else:\n    logging.critical(\"Timed out!!!\")\n    raise TimeoutError\nclass RPCServer(threading.Thread):\ndef init(self, *args, kwargs):\n    super(RPCServer, self).init(*args, kwargs)\nself.pika_thread_id = None\n\nself.command_id = 0\n\nself.connection = pika.BlockingConnection()\nself.channel = self.connection.channel()\n\nself.queue = self.channel.queue_declare(exclusive=True).method.queue\n\nself.results = Queue()\nself.daemon = True\n\n@property\n  def _next_command_id(self):\n    self.command_id += 1\n    return str(self.command_id)\ndef _on_request(self, ch, method, props, body):\ndef process_response(response):\n  if threading.current_thread().ident != self.pika_thread_id:\n    logging.critical(\"process_response cmd_id=%s called from another thread=%s; pika_thread=%s\",\n                     response, threading.current_thread().ident, self.pika_thread_id)\n    os._exit(1)\n\n  next_id = self._next_command_id\n  ch.basic_publish(exchange='',\n                   routing_key=self.queue,\n                   body=next_id)\n  logging.debug(\"RPC: Got response=%s, published next command: %s\", response, next_id)\n\n\nlogging.debug('RPC: got command %s', body)\n\nch.basic_ack(delivery_tag=method.delivery_tag)\n\nresponse_dfr = retry(self.results.get_nowait, exc=Empty, timeout=3600, results_q=self.results, cmd_id=body)\nresponse_dfr.addCallback(process_response)\n\nlogging.debug(\"Leaving _on_request\")\n\ndef run(self):\n    self.pika_thread_id = threading.current_thread().ident\nself.channel.basic_publish(exchange=\"\", routing_key=self.queue, body=self._next_command_id)\n\n\nself.channel.basic_qos(prefetch_count=1)\nself.channel.basic_consume(self._on_request, queue=self.queue)\n\nlogging.info('RPC listener started')\nself.channel.start_consuming()\n\ndef main():\nLOG_FORMAT = ('%(levelname) -10s %(asctime)s %(name) -30s %(funcName) '\n                '-35s %(lineno) -5d: %(message)s')\n  logging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT)\nRPCServer().start()\nreactor.run()\nif name == \"main\":\n  main()\n```\n. A pika Connection is not thread-safe. You can have multiple pika Connections, each accessed from its own Posix thread, just not shared between multiple threads.\nIt sounds like the issue is resolved, so closing it.\n. @yangyuliang1981, as you discovered, a single connection in pika is not thread-safe. Depending on your use cases, you may add your own mutual exclusion around calls into BlockingConnection/BlockingChannel to make it thread-safe, but it might be inefficient.\nIf your code is I/O-bound, then one efficient way would be to switch to an asynchronous programming framework without threads, using one of the other adapters in pika, such as SelectConnection, TornadoConnection, TwistedConnection.\nOptions outside of pika that I have familiarity with:\n1. rabbitpy documentation describes thread-safe access with channel per thread: https://rabbitpy.readthedocs.org/en/latest/threads.html. I don't have experience with rabbitpy; it's authored by pika's maintainer @gmr.\n2. haigha (https://github.com/agoragames/haigha) used with gevent engine (\"green threads\"; cooperative multitasking). haigha with gevent engine worked well for me in a product that I worked on a few years ago. This model is well suited for I/O-bound apps.\n. @yangyuliang1981, please come back and share your response when you're ready. I will close the issue for now.\n. @shensiduanxing, please describe how to reproduce the problem against the latest pika master branch and provide a small script to reproduce it. Note that many connectivity problems have already been fixed in 0.10.0 and in the latest pika master. Also, we need tests along with the fix. Thx.\n. @shensiduanxing, could you please provide the requested info? See my previous comment.\n. @shensiduanxing, I haven't heard back from you in a while. As I explained in my comment https://github.com/pika/pika/pull/700#issuecomment-180067262, Pika 0.10.0 and the latest Pika in master address many connectivity issues as well as improve error-handling. I cannot accept your PR without knowing how to reproduce the issue and without tests. I am going to close this PR. If you're still able to reproduce the issue against the latest Pika master, then please answer all my questions from https://github.com/pika/pika/pull/700#issuecomment-180067262 and reopen your PR. Thank you.\n. @gmr, please take a look when you have a moment\n. @gmr, thank you for merging. I replied to all of your feedback in this PR, including several questions for you.\n. Ooops, went to wrong repo\n. The SSL functionality could probably use some attention and definitely needs tests. Care to contribute?\n. https://github.com/openwebos/libpalmsocket may be a good algorithmic reference for re-handshakes, etc. I rewrote libpalmsocket a while back, adding support for SSL renegotiation, certificates, and workarounds for a number of OpenSSL bugs.\n. @deadtrickster, I posted on the RabbitMQ users group about RabbitMQ-triggered renegotiations here https://groups.google.com/forum/#!topic/rabbitmq-users/g7LkNe9sKDw.\nAt this time, we don't have any reason to believe that RabbitMQ would automatically trigger SSL/TLS Renegotiation.\nI am going to close this issue for now. If you gain any further specific insights about how this might be triggered, please post them here, and we may reopen this issue. Thx.\n. Tests passed. Submitted changes as PR into DMOJ:patch-1, so closing\n. @gmr, this replaces PR #676 with proper merge against Pika master. FYI\n. BlockingConnection objects in pika are presently not thread-safe. As you've observed, you may create multiple connections and use each such connection (and its channels) from a dedicated thread, but never share a given connection and its channels across threads.\n. @irshad-qb: sorry, I am not an expert on pika-pool. I would think that as long as multiple threads are not concurrently using a connection or any channel derived from it, then you should be okay. You might want to log thread ids and channels/connections being used to see if there is something unexpected going on\n. Please provide additional information:\n1. Platform (windows/linux/etc. and version) where the pika code that produced the above log was executing\n2. Network topology; e.g., using ELB, firewall, proxy\n3. A short code sample that represents how your app is using pika. Are threads involved, by the way?\n4. More logs: it would help to see more of the DEBUG-level pika logs, starting from the moment that BlockingConnection was established. Can you create a gist and link to it from this issue?\n. From past experience with AWS ELBs, I recall some additional trouble that they might present: they used to have a fixed 60-second timeout on connections, such that if there was no traffic on a connection for that amount of time, the ELB would sever that TCP/IP socket connection.\n. Ok. By the way, recent versions of RabbitMQ (3.5.5+?) default to a much shorter heartbeat timeout of 60 seconds (instead of 580). So, if newer RabbitMQ with default config doesn't hear from your app on a given connection in 60 seconds, it drops the connection.\n. Could this be it - from http://man7.org/linux/man-pages/man7/tcp.7.html?\n\ntcp_retries2 (integer; default: 15; since Linux 2.2)\n              The maximum number of times a TCP packet is retransmitted in\n              established state before giving up.  The default value is 15,\n              which corresponds to a duration of approximately between 13 to\n              30 minutes, depending on the retransmission timeout.  The\n              RFC 1122 specified minimum limit of 100 seconds is typically\n              deemed too short.\n. No publish timeout in pika. A more practical feature might be to set up TCP keep-alive on the socket (not just turn it on, but also configure the number of retries and period between retries). I was thinking of adding such a feature. Would you mind trailblazing it?\n. @markunsworth, the AMQP heartbeat is intended to address this condition. If no traffic is received from RabbitMQ within the specified heartbeat timeout, then the socket connection needs to be torn down.\n\nI believe I have root-caused the problem. Upon detection of heartbeat timeout, instead of abruptly terminating the TCP/IP connection (i.e., closing the socket), HearbeatChecker initiates graceful AMQP connection-close, including a few pikaisms :) inside Connection.close (gracefully closing channels, consumers, etc.).\nPerforming graceful AMQP connection-close and included actions (graceful closing of channels, etc.) upon AMQP heartbeat timeout is counter to the purpose of the heartbeat timeout, which is to detect lost connections, and results in a lengthy hang until TCP packet retries are exhausted and the TCP/IP stack eventually concludes (in about 15 to 30 minutes) that the TCP stream is dead.\nThe note NOTE: this won't achieve the perceived effect of sending... doesn't apply in this case, because there is at least one channel open and Channel.close() will wait to get a channel.close-ok, which will never come in the case of a disrupted connection.\nI think that the following fix *in Pika* should resolove it:\n1. Remove the self._connection.close() call\n2. Leave self._connection._on_terminate() in place - it should be sufficient to tear things down.\ncc @lukebakken . @lukebakken, the changes I was referring to in https://github.com/pika/pika/issues/708#issuecomment-370105566 need to be made in Pika, not in user's code. I updated my comment to make this clear and am reopening this issue.. On second thought, I probably goofed with my assessment in https://github.com/pika/pika/issues/708#issuecomment-370105566. Since the heartbeat logic runs in asynchronous code, the self._connection.close() call shouldn't block, and the subsequent _on_terminate() call drop the connection. So, although self._connection.close() is pointless when connection is lost, it's also harmless, and should not be responsible for the hang reported here.. Closing since per @markunsworth's comment https://github.com/pika/pika/issues/708#issuecomment-187124672, he couldn't reproduce the issue once equipment was moved to a private subnet.. Thanks for reporting. It definitely doesn't sound right. So that I may understand this better, which usage scenario are you anticipating for the multiple outstanding Basic.Get requests versus using the typically more efficient Basic.Consume?\n. Thanks @dschep, I agree that either documentation or functionality needs to be addressed. Just wanted to get a better handle on the usage scenario.\n. @JetDrag, thanks for teaching me about the GFW - I didn't know this term before :)\nSorry, I am not sure what you mean by \"set the connection error event priority highest?\". How might we go about doing that?\nRegarding\n\nIf the server has the same existing exchange,queue,bind relationship Pika said connection is closed during those things declaring.\n\nThe message in the exception or in the pika DEBUG logs should give you more information about why it closed. Perhaps you were trying to declare something with exclusive, but another connection was already using that object. Perhaps your app used an invalid exchange type. It could be a number of things. As usual, a simple script that reproduces the problem would be a good way to proceed after looking at DEBUG logs.\n. @JetDrag, thanks for the update. \nRegarding timeout and event priority, it sounds like you're talking about  the SelectConnection.add_timeout API. It wold probably add some unnecessary complexity in the timer logic to accomplish this, and it should be easy to manage it in the app's own logic (just cancel the timer and start a new one when needed).\nWhen you say \"when the eventloop is blocked e.g blocked on comsume\", I think you mean the SelectConnection's consume callback is called and your own code is blocking it, right? The async connection types (SelectConnection, TornadoConnection, and TwistedConnection) are intended for I/O-bound applications and all execute entirely in the user thread. Therefore, if the user thread is blocked inside a callback, then the pika async adapter can't execute any other code until control returns from the callback to the async adapter's event loop.\n. @JetDrag, I recently prototyped \"ThreadedConnection\" in https://github.com/vitaly-krugl/pika/tree/threaded-connection, which is a superset of \"BlockingConnection\", and has a background thread that handles connection-level I/O with the AMQP broker. This prototype ThreadedConnection will send and receive heartbeats and other data while the user thread is blocked. It also lets you share a single physical connection from multiple threads via connection \"cloning\".\nThreadedConnection is just a prototype, so it's not good for production. In particular, if you're consuming, you want to set a reasonable Basic.QoS and only use noack=False consumers. Otherwise, it will end up loading the contents of the entire queue into the app's memory, if the app can't keep up with the incoming messages; this could be particularly bad for the noack=True consumers, since the broker will discard all sent messages, even if the app runs out of memory and crashes. I will submit a PR after resolving some of these issues.\n. Right, since everything executes in the same thread, when the thread is blocked in the callback, IO is impossible in pika.\nWhen you use multiple threads, be sure to avoid calling into the same connection (or any of its channels) from multiple threads. Pika connections are not thread-safe.\nFYI,\nVitaly\n. @JetDrag, there are a couple of good groups for questions like this:\n1. Concerning pika: https://groups.google.com/forum/#!forum/pika-python\n2. Concerning RabbitMQ and miscellaneous AMQP clients: https://groups.google.com/forum/#!forum/rabbitmq-users\nPlease re-post your question on one of those groups and I will reply there. Perhaps the question and answer will be useful for someone else, too. Those groups are more natural places for people to look for and get answers on this topic.\nBest,\nVitaly\n. @gmr, I believe that I addressed your core concerns with robust rationale for why the implemented behavior is correct in those cases. Let's discuss further, if needed. Either email or Google Hangouts would work, or we can continue the discussion in this PR.\nI am going to add a commit with corrections for the several style-related issues that you pointed out and rebase. I can add more changes after that if we conclude that they are needed.\n. @gmr, I rebased against master and made several small changes - see my previous comment about addressing your concerns, etc.\nI am afraid I also squashed my changes as part of the rebase. I do it automatically out of habit from other projects that prefer single-commit PRs. Unfortunately, I realized it too late. Sorry.\n. @gmr, please review and consider for merging\n. @Johnathon332, you will want to try a few things:\n1. Before you start running it in another process, are you able to get it to work in the main process?\n2. I shy away from instantiating communications objects in one process and using them in another. This is because sockets and other kernel objects that they create in the main process (anything that has file descriptors) tend to get duped into the forked process. This might create problems related to the lifetime of those duped file descriptors. For example, when the other process closes a duped socket, the connection might not get closed, because the main process might not have closed its instance of that same connection. It's better to pass the arguments to the other process and let the other process instantiate ExampleConsumer, etc.\n3. pika has pretty extensive debug-level logging for debugging connectivity and other problems. Turn on debug-level logging before you create ExampleConsumer. E.g., import logging; logging.basicConfig(level=logging.DEBUG); then, use the logging information to debug the problem.\nAlso, post information about your environment that would be helpful for debugging, such as versions of pika and python, the operating system, DEBUG-level logs, etc.\n. Is there any other log output from pika besides the one line that you posted?\n. Also, it might help to attach (as a file or link to a gist) a complete, simple, functional script that demonstrates the issue. In your attachment, rid everything from ExampleConsumer that isn't necessary to reproduce this issue.\n. Closing due to inactivity\n. Was meant for my own fork, first.\n. @GayatriNittala: It looks like you're running on Windows. Windows CI testing is lacking in the pika project. It would be nice to get something like AppVeyor integrated into the pika project.\nTo debug this, configure logging to output (e.g., import logging; logging.basicConfig(level=logging.DEBUG)) and modify the section of code that calls select.select in SelectConnection like the following. The log output should help you figure out which invalid argument is being passed to select. Please post here what you find out.\n```\n        while True:\n            try:\n                 read_fds = self._fd_events[READ]\n                 write_fds = self._fd_events[WRITE]\n                 error_fds = self._fd_events[ERROR]\n                 next_deadline = self.get_next_deadline()\n             LOGGER.info('Calling select.select(%r, %r, %r, %r)', \n                         read_fds,\n                         write_fds,\n                         error_fds,\n                         next_deadline)\n\n             read, write, error = select.select(read_fds,\n                                                write_fds,\n                                                error_fds,\n                                                next_deadline)\n\n```\n. Thanks for the info. It's a Windows-specific bug in pika. When there aren't any file descriptors, the code should be using time.sleep() (or something like that) instead of select.select(). It's a simple fix, but not a high-priority one. Your app can leave connection_attempts at 1, and have its own simple loop that performs the retries\nThanks for reporting.\n. @GayatriNittala, this should now be fixed in pika's master branch as part of PR #735. See https://github.com/vitaly-krugl/pika/blob/c36b16e5aaaa82f938afa91a65432a4b82da2a56/pika/adapters/select_connection.py#L708-L713\n. Thank you for your submission. Please accompany your submission with a test that verifies your change.\n. That's a good point. In master, this is now done in Connection._on_terminate. Please submit a pull request that changes that LOGGER.warning call to LOGGER.info. Thanks.\n. Closing via merged PR #720.\nAlso, thanks @tripleee for the workaround tip against the released 0.10.0.\n. @Pankrat: thanks, your assessment makes sense. That this doesn't cause trouble is covered by existing tests, so no new tests are needed with this PR. Merging...\n. @jbpineiro, there are no plans at this time to add support for other SSL modules. Please describe your environment, including why you're unable to make it work with file paths. \n. It looks like the python community is working to add support for in-memory certs and cert chains, but it likely won't be backported to 2.x; reference https://bugs.python.org/issue16487\n. @Giri-Chintala: so, your security policy for AWS Lambda won't even allow you to create temp files?\n. Hmmm..., I wonder if you can work around this limitation via Filesystem in Userspace (FUSE). E.g., https://pypi.python.org/pypi/fusepy/. This way, the certs could be stored entirely in memory, but accessed via the file API just like ordinary files.\n. @Giri-Chintala, please update us here on your experience with working around the issue via FUSE or anything else. If you can get it to work, then a recipe would be most appreciated for the benefit of the other community members. Thank you.\n. @Giri-Chintala:  regarding\n\ni tried using fuse in my local, but unfortunately i am unable to use in my AWS Lambda as i don't get root access\n\nDo you mean that you would need root permissions to set this up?\n. I am afraid that the current implementation of pika and built-in ssl module don't support string certs. Builtin SSL module appears as if it will support string certs on Python 3.6 or 3.7. And, as has been mentioned, there are other SSL options, such as pyopenssl.\nOne thing you may try is to fork pika and modify it to use pyopenssl or similar instead of or in addition to the builtin SSL library.\nBy the way, you never mentioned which version of python you're using.\n. @dukhlov, what's the use case for this pull request? Which problem(s) are you trying to solve?\n. Hi @dukhlov, thanks for the background concerning your PR:\n1. Regarding bug-fix releases, that's managed by @gmr. (+1)\n2. Regarding sharing a single physical AMQP connection in a multi-threaded environment, I recently created a fairly well-functioning prototype \"ThreadedConnection\" that shares almost all of the current BlockingConnection's implementation and its entire API (the current BlockingConnection is also based on SelectConnection, as you are most likely aware). You can \"clone\" a ThreadedConnection instance via the ThreadedConnection.clone_connection method, which creates a new instance of ThreadedConnection that shares the same physical AMQP connection with the broker. Each cloned instance of ThreadedConnection may be used in its own thread and it gets the complete BlockingConnection API, including creating many channels, consumers, etc. See https://github.com/vitaly-krugl/pika/blob/threaded-connection/pika/adapters/threaded_connection.py. It presently passes all of the generic BlockingConnection tests (fails a few tests that rely on implementation-dependent logic that differs between BlockingConnection and ThreadedConnection).\n   - If you would like to try it, set a reasonable QoS value for use with noack=False consumers to avoid overwhelming your app's RAM (in case your consumer logic can't keep up with incoming messages). noack=True consumers are a bit of a challenge for blocking-style interfaces in general for RabbitMQ, since RabbitMQ doesn't fully support Basic.Flow, so the blocking interface might end up consuming too much of the queue into RAM before the user of the API can process those messages (I have some ideas for mitigation, but haven't implemented them in the prototype yet)\n. @dukhlov, is your implementation of blocking connection open-source? If so, please point me to it - I would like to learn how it works. Thx.\n. @dukhlov, regarding your questions, I had several goals in my prototype, including:\n1. Each thread has access to the full pika BlockingConnection and BlockingChannel API\n2. Connection Heartbeats are sent/processed independently of what the client threads are doing to avoid lost connections due to heartbeat timeouts and enable more reasonable heartbeat timeout values (such as 60 seconds default heartbeat timeout in recent RabbitMQ releases)\n3. Generally-applicable, application-independent solution\nThe Gateway Connection represents the shared AMQP connection running in its own thread.\nI would like to explore simpler implementations, if practical.\n\nEverything should be made as simple as possible, but not simpler. (A. Einstein)\n. Hi @dukhlov, \n\nSorry, but pika's core adapters do not support threading: see \"Is Pika thread safe?\" in  http://pika.readthedocs.org/en/0.10.0/faq.html. There is some locking support in the Connection class (_write_lock) in master that is incomplete and is in the process of being removed. Threading support is a non-goal and a counter-pattern of pika core.\n. @dukhlov regarding\n\nYes I saw it but thought that threading support will be completed soon. It is interesting why do you want to remove it? Is overhead for locking in so significant?\n Show all checks\n\nIt's an additional burden on maintenance without a broadly-applicable benefit. Each of the async adapters is intended for single-threaded use in an I/O-bound environment. Hence, no need for locking.\n. @pgreaney, assuming your thorughput computation logic works properly, there are likely a few things that are specific to your implementation that are slowing things down. Here are some to consider\n1. The dreaded cpython global interpreter lock (A.K.A. GIL - https://wiki.python.org/moin/GlobalInterpreterLock)\n2. Context switches from having so many threads\n3. delivery_mode=2: this is persistent mode, which likely requires RabbitMQ to do more work\n4. immediate=True; not sure whether this slows things down, but it might.\n5. Network latency\nI have in the past performed performance benchmarking using a single publisher thread (no consumer, and I think with default delivery_mode), achieving much higher throughput (8,000 per second or so? running everything on a 3 year-old development MacBookPro). See https://github.com/vitaly-krugl/amqp-perf. NOTE I haven't used that tool in a while, so use at own risk \ud83d\ude03 .\nAnother aspect that's troublesome in your benchmark code (although no bearing on throughput) is that the publisher code runs an infinite loop that incessantly publishes messages. There are a couple of problems, especially if this type of logic were to be used in production code:\n1. It doesn't behave like an I/O bound application for which the async adapters were intended, and can overwhelm the program's memory if networking cannot keep up.\n2. No opportunity to handle incoming messages (e.g., channel errors, heartbeats and Connection.Blocked/Unblocked from broker)\n3. This won't work in the next release.The released 0.10.0 pika had a commit that allowed code like your publishing loop to make some progress, actually attempting to send the data to the non-blocking socket without relying on ioloop to write to the socket when it detects that the socket is writeable. Unfortunately, that PR introduced some bugs, and we had to revert the part of the PR that attempted to send data in the context of the publisher. Thus, this type of loop won't work with the next release of pika.\n. Just noticed that the benchmark publisher declares the queue as non-durable (_channel.queue_declare(self.on_queue_declareok, queue_name)), so not sure whether delivery_mode=2 will have any effect at all; the message certainly won't get stored if the queue is non-durable; I don't remember whether RabbitMQ would complain (i.e., raise error) if you tried to send persistent messages to a non-durable queue (certainly sounds inconsitent). FYI.\n. @pgreaney, it looks like the performance issue is somewhere on your side. I outlined some potential causes in https://github.com/pika/pika/issues/723#issuecomment-212298726. \nBelow is a chart of a pika.BlockingConnection-based consumer and producer running concurrently from two processes and achieving throughputs of about 8K messages per second.\npika.BlockingConnection in pika v 0.10.0 and current pika master branch is implemented as a layer on top of pika.SelectConnection.\nBoth clients (using pika from master branch) and RabbitMQ 3.6.0 were running on the same machine (MacBookPro) for minimal latency.\nThe channels were created as follows:\nimport pika\nconn = pika.BlockingConnection()\nch = conn.channel()\nHere is the chart with 8K messages/sec publisher and consumer throughputs: \n\n. And here is a chart with acks. The BlockingConnection-based consumer is ack'ing, which slows down the draining of the queue due to BlockingConnection's synchronous characteristics. In this case, I would expect a SelectConnection consumer to do better. But, the performance is still pretty decent for a pure python synchronous implementation. \n\n. Hi @dschep, it looks the Channel._on_getok_callback is not getting reset when Basic.GetEmpty is received (instead of Basic.GetOk). This is what's causing the failure of the new logic. This is easy to fix in the Channel's _on_getempty callback. Please include unit/acceptance tests in your PR that correlate to all your changes.\nThank you,\nVitaly\n. @gmr, I rebased and create a PR from my alternate proposal for Client Properties overrides.\n. @abakhru, I would like to welcome you to the community. Would you be able to investigate and contribute a fix? Thank you.\nVitaly\n. I am able to get it to work using pika from current master branch at commit 51fd186f1fabe0da8582609d266df1e28e57e3f4 and RabbitMQ 3.6.0. I suspect that the release pika 0.10.0 would work, too.\nNote the use of ssl_version=ssl.PROTOCOL_TLSv1.\nSee https://www.rabbitmq.com/ssl.html for certificate creation and rabbitmq SSL configuration instructions.\n```\nimport ssl\nimport pika\ncp = pika.ConnectionParameters(\n    ssl=True,\n    ssl_options=dict(\n        ssl_version=ssl.PROTOCOL_TLSv1,\n        ca_certs=\"/Users/me/rabbitmqcert/testca/cacert.pem\",\n        keyfile=\"/Users/me/rabbitmqcert/client/key.pem\",\n        certfile=\"/Users/me/rabbitmqcert/client/cert.pem\",\n        cert_reqs=ssl.CERT_REQUIRED))\nconn = pika.BlockingConnection(cp)\nch = conn.channel()\nch.queue_declare(\"sslq\")\nOut[10]: \"])>\nch.publish(\"\", \"sslq\", \"abc\")\nch.basic_get(\"sslq\")\nOut[17]: \n(,\n ,\n 'abc')\n```\nI followed the setup instructions in https://www.rabbitmq.com/ssl.html, including rabbitmq.config and generation of all the certificates, and was able to get it to work with pika.BlockingConnection.\nBoth the client and rabbitmq server were running on my MacBookPro. I created rabbitmq.config in its default location for OS X: /usr/local/etc/rabbitmq/rabbitmq.config\nIts contents (using certificate directory structure and contents created following instructions in https://www.rabbitmq.com/ssl.html):\nNote that in my test, I configured rabbit to accept SSL connection only on the local interface.\n```\n[\n  {rabbit,\n    [ \n      {ssl_listeners, [{\"127.0.0.1\", 5671}]},\n  %% Configuring SSL.\n  %% See http://www.rabbitmq.com/ssl.html for full documentation.\n  %%\n  {ssl_options, [{cacertfile,           \"/Users/me/rabbitmqcert/testca/cacert.pem\"},\n                 {certfile,             \"/Users/me/rabbitmqcert/server/cert.pem\"},\n                 {keyfile,              \"/Users/me/rabbitmqcert/server/key.pem\"},\n                 {verify,               verify_peer},\n                 {fail_if_no_peer_cert, true}]}\n]\n\n}\n].\n```\n. @gmr: Hi Gavin, in your comment, you are referring to line 332. I don't see the user's code in this issue that this line number references. Is there an attachment somewhere that I am failing to see? Thx\n. @apti-tids, indeed - there are several issues in https://github.com/pika/pika/blob/master/docs/examples/tornado_consumer.rst. I recently fixed and cleaned up the asynchronous publisher example, which had a similar structure and similar problems ( https://github.com/pika/pika/pull/710). This change also made that example consistent with the latest changes in pika's master branch.\nIn fact, there is an almost identical asynchronous consumer example that uses SelectConnection instead of TornadoConnection: https://github.com/pika/pika/blob/master/docs/examples/asynchronous_consumer_example.rst that likely suffers from the same issues.\nGiven that TornadoConnection and SelectConnection interfaces are virtually identical, the implementations of both consumer examples should be identical as well, differing only in the class of Connection (TornadoConnection vs. SelectConnection).\nIdeally, a single asynchronous consumer implementation can be used for both adapters, if the example script simply accepted the adapter's class name (TornadoConnection or SelectConnection) as a positional command-line arg.\n@apti-tids, would you be able to submit a PR that fixes both consumer examples (TornadoConnection and SelectConnection), using a single, shared implementation, referring to the changes in the asynchronous publisher example's framework (https://github.com/pika/pika/pull/710) as a model, where appropriate?\nAs part of this change, please follow this roadmap for simplicity and maintainability per DRY principle:\n1. As suggested above, create a single, functional script that accepts the asynchronous adapter name as the only positional argument (\"TornadoConnection\" or \"SelectConnection\"). Assuming the variable holding the class name in the new script is adapter_class_name, the script can load the appropriate adapter via:\n``` python\n   import pika\ndef main(adapter_class_name):\n       adapter_class = getattr(pika, adapter_class_name)\n       conn = connectionClass(pika.URLParameters(self._url),\n                              self.on_connection_open,\n                              stop_ioloop_on_close=False)\n   . . .\n   ``\n2. Name this scriptasync_consumer.py(or similar) and place it underpika/examples(instead ofpika/docs/examples).\n3. Indocs/examples/tornado_consumer.rst`:\n   - Replace all the code in this .rst file with something like\nasync_consumer.py \"pika.TornadoConnection\"\n   - Then, use Spinhx's literalinclude keyword to pull in the source code of your new async_consumer.py implementation into this rst file. See literalinclude in http://openalea.gforge.inria.fr/doc/openalea/doc/_build/html/source/sphinx/rest_syntax.html\n4. Do same with docs/examples/asynchronous_consumer_example.rst, except of course\n   async_consumer.py \"pika.SelectConnection\"\n. There are a couple of additional benefits from placing the example scripts as actual code (versus .rst file) under pika/examples/:\n1. As you can see here, the examples tend to get out of touch with reality, because they don't get tested. I am thinking that the example python scripts should be tested as part of the CI builds. This will require some additional work in the examles to make them suitable for testing, so placing the examples in pika/examples as actual python code is the first step in that direction.\n2. Developers can run them directly, without having to copy/paste them from .rst files or web pages.\n. @yangyuliang1981, backpressure will likely get deprecated. The blocked/unblocked notifications are the preferred mechanism. See https://github.com/pika/pika/blob/master/docs/version_history.rst\n. RabbitMQ's Connection.Blocked is different from TCP/IP stream back-pressure.\nRabbitMQ sends Connection.Blocked to publishers when it is running low on resources (e.g., low memory or disk space). If you registered via add_on_connection_blocked_callback before Connection.Blocked arrived, then your callback should be called upon arrival of this message. When a connection is blocked via Connection.Blocked, RabbitMQ won't process any incoming traffic from that connection (commands/messages/acks/nacks/etc., nothing) on that connection until the connection becomes unblocked. When the connection becomes unblocked, RabbitMQ will send Connection.Unblocked.\nRabbitMQ uses back-pressure when it can't keep up with incoming data on a connection (simply stops reading from connection until it catches up).\n. @gmr\nHi Gavin, we discussed recently the possibility of deprecating the backpressure detection mechanism in pika, and I added deprecation comments as the result of that discussion. However, some (new?) mechanism may still be needed to help users of async adapters avoid out-of-memory conditions resulting from buffering of outbound data in the connection.\nImagine an app that reads data from TCP/IP stream and publishes it to AMQP, and assume that the source data is coming in much faster than RabbitMQ is able to absorb. In this case, RabbitMQ will be applying backpressure, and the pika connection's outbound buffer will be growing continuously. This will lead to out-of-memory condition in the app eventually.\nPerhaps a simple, user-configurable high-water/low-water notification mechanism may be created for outbound buffer that replaces the current somewhat more complicated backpressure detection mechanism?\n. @yangyuliang1981, would you mind adding a unit test to tests/unit/connection_tests.py that reproduces the ZeroDivisionError that you reported?\n. There is clearly a BUG in Connection._send_message in pika maser that is likely responsible for this ZeroDivisionError exception.  Connection._send_message updates self.frames_sent, but not self.bytes_sent. Compare with Connection._send_frame.\n. @gmr, this bug (see my previous comment) needs to be fixed before next release.\n. @yangyuliang1981, the 5 seconds to send/receive a message sounds like something may be wrong with your RabbitMQ or network configuration. Pika runs many acceptance tests that send/receive many messages. If they took 5 seconds per message, then the acceptance test runtimes would be very long. Here is an example set of recent test runs: https://travis-ci.org/pika/pika/builds/115000880 for your reference.\nFor example, when I run RabbitMQ on my MacBookPro while connected to my company's VPN, sending and receiving a message takes a very long time (RabbitMQ ends up performing a ton of failing DNS resolutions that slow it down a lot)\nTo test pika's performance on Travis, implement a simple send/receive stress test with timing in pika/tests/acceptance/ directory and submit a dummy PR. Travis instances are pretty slow, but you shouldn't get anything close to 5 seconds per message.\n. @yangyuliang1981, the ZeroDivisionError fix should be very trivial\nFirst, implement a couple of simple unit tests in tests/unit/connection_tests.py (mocking out the actual I/O like the other tests in that file) :\n1. Test that Connection._send_message updates both self.frames_sent and self.bytes_sent correctly and run the test against the current code base to confirm that it detects the inconsistency in self.bytes_sent and fails.\n2. Test Connection._detect_backpressure after calling Connection._send_message. Run the test against current code base to confirm that it fails with ZeroDivisionError.\nThen, implement the simple fix in Connection._send_message.  Right after self.frames_sent += len(write_buffer), add:\nself._bytes_sent += sum(len(data) for data in write_buffer)\nThen run the tests to verify that they now all pass.\nFinally, add a short note at the bottom of the Next Release section in docs/version_history.rst about fixing the ZeroDivisionError backpressure detection bug.\nCould you do this and submit a PR?\n. @yangyuliang1981, mock is documented here http://www.voidspace.org.uk/python/mock/.\nRegarding\n\nThen I am confused about how to schedule the publishing process\n\nthe answer depends on where the data that you want to publish comes from and the programming model that your app is using. SelectConnection, TornadoConnection, and TwistedConnection adapters are built around the concept of \"ioloop\" event loop mechanism, where the application uses the primitives provided by the specific ioloop to schedule and perform all I/O and other operations in the same thread.\nFor example, if your data comes from a TCP/IP socket, you would create and associate a pika async adapter (one of Select/Tornado/TwistedConnection) with an ioloop, and then start the ioloop (e.g., ioloop.start()), which blocks inside the ioloop's code \"forever\" using select, epoll, kqueue, or similar system mechanism, waiting for registered events of interest to happen. When the AMQP connection is established, your app would get a callback from pika and create a pika channel, and when the channel creation completes (get callback), your app would create/connect a non-blocking input socket, then register the input socket with the ioloop to notify your app when the socket is readable. When your input socket becomes readable, the ioloop notifies your code (e.x., via callback function), your code reads data from the socket, parses it into messages, and publishes it via the async pika connection's channel, and returns control back to the ioloop.\nThe idea behind the event loop concept is that it reduces the amount of context switches and memory used by applications versus the multi-threaded model. Code that works best with this concept is I/O-bound code that uses the ioloop for all blocking operations, such as waiting for sockets to become readable, writeable, etc.\nAnother model is the blocking model, which might be more appropriate if your data comes from a source that doesn't work with the ioloop concept, or you don't need the performance benefit of the ioloop and wish to use simpler logic instead. For example, let's say your code reads data from MySQL and publishes to AMQP. Pika has an adapter for this called pika.BlockingConnection. See https://www.rabbitmq.com/tutorials/tutorial-three-python.html and http://pika.readthedocs.org/en/0.10.0/examples.html.\n. Hi @yangyuliang1981, in the interest of time, I fixed this via PR #738. Please review. Thank you.\n. @yangyuliang1981 Regarding\n\n@vitaly-krugl \nAs you said, \"your app would create/connect a non-blocking input socket, then register the input socket with the ioloop to notify your app when the socket is readable. When your input socket becomes readable, the ioloop notifies your code (e.x., via callback function), your code reads data from the socket, parses it into messages, and publishes it via the async pika connection's channel, and returns control back to the ioloop.\"\nI try to realize it ,but not works.\nThe code in selectconnection just connect \nThe error is \"OSError: [Errno 9] Bad file descriptor\"\n```\nin open channel callback\nread_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\nread_sock.setblocking(0)\nread_sock.connect(('127.0.0.1', 0))\nconnection.ioloop.add_handler(DeviceIO.read_sock.fileno(), DeviceIO.read_interrupt2, DeviceIO.connection.READ)\nin read_interrunpt2 function\nos.read(interrupt_sock, 512)\nin another thread, try to send data\nos.write(read_sock.fileno(), 'hello')\n```\n\n@yangyuliang1981, your datagram programming code isn't quite right. With datagram socket communication (unlike TCP streams), there is no real connection made. You can use connect on a datagram socket to set up the destination address, so that you can use send instead of sendto. The receiving datagram socket needs to be bound on that address and port number via bind. See https://wiki.python.org/moin/UdpCommunication#Sending and https://wiki.python.org/moin/UdpCommunication#Receiving\n. @yangyuliang1981, also note that datagram socket communication is not reliable; packets may get lost and there are no retries.\n. Thank you, @yangyuliang1981 \n. I create pull request #1042 to clarify the semantics of add_on_connection_blocked_callback and add_on_connection_unblocked_callback.. Backpressure detection has been fixed since the opening of this issue, so closing it. If the issue still exists in the latest release (0.12.0), please reopen.. Thanks for the update. I am going to close the issue in this case.\n. @jaygorrell: Calling the Tornado consumer example's run method as-is only makes sense if you haven't started the ioloop yet. In your issue description, you said that you start the ioloop yourself (\"which happens as soon as I start the ioloop immediately after\"), so you need to modify the example's run method.  \nThe run method in the Tornado Consumer example does these two things:\nself._connection = self.connect()\n            self._connection.ioloop.start()\nIf you already started the ioloop, and if it's the same loop that your TornadoConnection should be running on, then you probably don't want the additional self._connection.ioloop.start(). \nAlso, if your app uses a non-default ioloop (i.e., not ioloop.IOLoop.instance()), then you need to pass the ioloop that you're using to the TornadoConnection constructor, if you want TornadoConnection to run on the same ioloop. If you don't pass an ioloop to TornadoConnection, then it defaults to ioloop.IOLoop.instance().\nFinally, make sure you're not calling the example's run method from a repeating timer. Instrument the consumer methods with log or print statements and see what is actually happening there.\n. Enable DEBUG-level logging before creating TornadoConnection, and review the pika (as well as your own) log output to make sure that nothing strange is going on. Something like this should work: \n```\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n```\n. Thanks for the update. Closing as sue to libcurl, not caused by pika.\n. See http://pika.readthedocs.org/en/latest/faq.html regarding pika and thread-safety\n. @gmr, ~~this would likely be fixed when~~ consistent connection cleanup will require addressing this and similar TODOs in connection.py are addressed: https://github.com/pika/pika/blob/5dc90ad8bce20687009d9e9ed358c84c4666d3f1/pika/connection.py#L1186\nI added those TODOs when making my previous pass through connection.py. I am planning to work on this shortly.\n. @holljen, a number of connection error handling inconsistencies have already been addressed in pika master. Are you getting the same results with current pika from the master branch?\n. @holljen and @gmr: based on the logs, it looks like the application code is attempting to reconnect directly from the \"on connection error\" callback. If this is true (and it appears incredibly likely that it is), then it would explain why the state is not cleaned up yet at the time of the call;  see https://github.com/pika/pika/blob/5dc90ad8bce20687009d9e9ed358c84c4666d3f1/pika/connection.py#L1187-L1190.\nThe application code should instead register a reconnection timer, and perform reconnection after the \"on connection error\" callback returns to pika connection.\n. Also, my personal preference is not to re-connect an existing failed connection instance, but rather to instantiate a new connection instance after connection failure. Reconnecting an existing connection relies on a second code path for initializing connection state and connecting, which is likely not sufficiently tested, and on occasion gets out of sync with the main code path. Discarding the old connection instance and creating a new one should be more stable long-term.\n. @holljen: sorry, maybe my mistake in interpreting the logs and tracebacks. I see that pika.Connection._on_connection_error() gets called, which means that your code doesn't supply an \"on connection error\" callback to the connection constructor, so the default just raises exceptions.AMQPConnectionError. \nIf your code doesn't re-connect the existing connection object (but creates a new connection object instead), then @gmr's theory\n\nThat does help confirm the theory of the connection state not being reset on disconnect\n\nmight not apply here, since that would only come into play if you tried to re-connect an existing connection object (via its connect()) method.\nPika performs an opinionated analysis of the failure and translates it into exceptions that are guesses as to the cause of connection failure. For example, if the connection is lost while the connection is in the CONNECTION_PROTOCOL state, pika assumes that the broker dropped the connection because the protocol version supplied in the initial handshake message by the client is not acceptable by the broker, so pika maps the error to the IncompatibleProtocolError exception.\n. To summarize the potential differences in behavior: In your case, since you don't pass the \"on connection error\" callback to the tornado adapter's constructor, if socket.connect() fails, you will get the AMQPConnectionError exception from the default pika.Connection._on_connection_error handler. On the other hand, if socket.connect() succeeds, then the pika connection transitions to the CONNECTION_PROTOCOL state; in this state, pika.Connection sends frame.ProtocolHeader() to the broker, and expects to receive Connection.Start from broker. If connection is lost before Connection.Start is received, then the released pika would raise IncompatibleProtocolError.\nNote that with the latest changes in pika master branch, pika's async adapters communicate connection errors to the application via callbacks instead of exceptions in most cases.\n. If rabbitmq comes up in a bad state (low on resources, or something else wrong with networking), then you might experience a problem like this when trying to connection again even when using a new connection object. Did you check rabbitmq logs? Also, WireShark now has support for AMQP and I've used it in the past to debug connectivity problems quite effectively.\n. Thanks for the update. Regarding exchange type arg, this was a deprecated parameter (bad idea to name args and other variables using builtin names) that finally got removed; see https://github.com/pika/pika/blob/master/docs/version_history.rst\n. Because I/O operations performed by async adapters are typically performed in the context of the ioloop, the exceptions were problematic (the app was not in position to handle those exceptions), so we started favoring the callbacks over exceptions. \nThe second callback provides more detailed info.\n. Is it okay to close this issue?\n. @yangyuliang1981: pika is not thread-safe. These problems are caused by race conditions that your code creates. See http://pika.readthedocs.org/en/latest/faq.html. \n. Hi @bb-work, BlockingConnection implementation is synchronous and runs entirely only in your thread when you make a call to the connection or channel. It has no background thread. So, if the time between I/O calls into your pika connection/channel exceeds the connection heartbeat timeout, RabbitMQ broker will close your connection. Rabbit's default connection timeout used to be 580 seconds, but they dropped it down to 60 seconds in recent versions.\nIf this is the problem, then you can work around it by increasing the heartbeat timeout. You can change the default heartbeat timeout in your RabbitMQ configuration. You should also be able to increase the connection heartbeat timeout or completely disable it from the client side via pika.ConnectionParameters, but there is a caveat that you need to be aware of: The presently-released versions of pika (on PyPi) don't allow you to increase the heartbeat timeout (it picks the lowest of the two values: the one proposed by RabbitMQ during connection setup and what's provided by the user in pika ConnectionParameters). The version in pika master changed this to pick the max of the two.\nSo, from the client side, you should be able to disable the heartbeat timeout via ConnectionParameters by setting the heartbeat value to 0 (please post here if this doesn't work), or you can use the version of pika from Master to specify a higher timeout value (in seconds) that would meet your requirements.\n. @bb-work, besides the suggestions in my previous comment, you should also check the RabbitMQ logs to help diagnose the issue. I am going to close this issue for now. If you have further insights related to it, please post them here, and we may reopen it if needed.\n. Hi @gmr,\nAs discussed, I created an appveyor.yml configuration file for pika in this PR. I also had to fix a number of pika and test bugs that showed up on windows.\nThe full test suite ran successfully on appveyor  from my fork (https://ci.appveyor.com/project/vitaly-krugl/pika/build/1.0.25).\nAfter merging this change, you will need to go to appveyor.com and sign up for a free open-source account for pika and point it to the pika repo. It's really quick to do this: see https://www.appveyor.com/docs#step-2-add-your-project. The configuration should be very simple, since we're using appveyor.yml (in pika's root).\nSo far, I configured installation of pika and test requirements, download and starting of the necessary support tools (erlang, rabbitmq, etc.), and running of the test suite. Next steps (not included in this PR) might include generating and uploading Windows pika wheels and/or other artifacts.\n. Now generates and saves pika wheel as appveyor artifact.\n. Connection and Channel classes are at the asynchronous (non-blocking) core of pika. The context manager methods don't make sense in async API. They are more appropriate for the blocking API. See __enter__/__exit__ in the BlockingConnection class.\n. @pfhayes, PEP-343 on BlockingChannel sounds reasonable to me. @gmr?\nWe're hoping to cut a new release in the near future, so this change might not get merged until after the release.\nPR #629 added PEP-343 in BlockingConnection. See that PR for test examples. Pika's channels have an additional caveat: if attempting to close a channel that's already closed, the channel raises ChannelClosed exception. So, you will need to take this into account in both implementation and tests.\n. Please addressing test name feedback and squash into a single commit.\n. Thank you @pfhayes, LGTM\n. @gmr, you mentioned that you wanted to hold off on non-critical merges until after the upcoming release, so I am not merging this PR right now, unless you decide to.\n. CC @gmr: Ready for review\n. @grm: No, _send_frame and _send_message are two distinct, non-overlapping code paths presently in pika; reference https://github.com/vitaly-krugl/pika/blob/issue728-backpressure-divide-by-zero/pika/connection.py#L2136-L2139\nif content:\n            self._send_message(channel_number, method, content)\n        else:\n            self._send_frame(frame.Method(channel_number, method))\nTheir implementations are orthogonal.\n\nWon't this PR count the # of bytes sent to be 2x what they are? \n. Rebased against 51fd186f1fabe0da8582609d266df1e28e57e3f4\n. CC @gmr This is ready for review\n. Rebased against 51fd186f1fabe0da8582609d266df1e28e57e3f4\n. \ud83d\udc4d . According to @solarce's comment in https://github.com/travis-ci/travis-ci/issues/5941, RabbitMQ is not a supported service on OSX out-of-the-box in Travis-CI, yet. Also, see https://docs.travis-ci.com/user/osx-ci-environment/. \n\nAnd here is the reference to present lack of official support for Python Runtime in Travis-CI under OS X: https://github.com/travis-ci/travis-ci/issues/2312\n. @AdamMiltonBarker, which version of pika are you using?\nThe fact that you're getting IncompatibleProtocolError exception, suggests that socket connection actually succeeded, but then the connection got dropped for some reason. Have you checked RabbitMQ logs? Also, WireShark is an amazing tool for network analysis. The recent versions of WireShark have support for AMQP protocol (use amqp as Display Filter).\nTo get more logs out of pika, run something like the following before instantiating BlockingConnection:\n```\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n```\n. @AdamMiltonBarker, I suspect that there is a connectivity issue; for example, the incorrect configuration of the RabbitMQ SSL port 5671 without additional SSL settings in your example to reproduce the problem (instead of 5672).\nIf you really intended to use SSL to connect to RabbitMQ, then you would need to configure the additional SSL settings in pika.ConnectionParameters. ~~However, I suspect that SSL support might be broken in pika at the moment~~ Note that there are several open ssl-related issues posted about it in this repo.\n. I am running rabbitmq v3.6.0 in the following example.\nYou can verify TLS connectivity to rabbitmq without pika first: openssl s_client -connect 127.0.0.1:5671 -tls1, assuming that the broker is running on the same machine, or use different address if on a different machine. This should connect with a recent rabbitmq broker configured with default SSL version options. If this doesn't work, then I wouldn't expect it to work with pika either, and you need to solve your rabbitmq configuration and networking first.\nI followed the setup instructions in https://www.rabbitmq.com/ssl.html, including rabbitmq.config and generation of all the certificates, and was able to get it to work with pika.BlockingConnection, after all.\nBoth the client and rabbitmq server were running on my MacBookPro. I created rabbitmq.config in its default location for OS X: /usr/local/etc/rabbitmq/rabbitmq.config\nIts contents (using certificate directory structure and contents created following instructions in https://www.rabbitmq.com/ssl.html):\nNote that in my test, I configured rabbit to accept SSL connection only on the local interface.\n```\n[\n  {rabbit,\n    [ \n      {ssl_listeners, [{\"127.0.0.1\", 5671}]},\n  %% Configuring SSL.\n  %% See http://www.rabbitmq.com/ssl.html for full documentation.\n  %%\n  {ssl_options, [{cacertfile,           \"/Users/me/rabbitmqcert/testca/cacert.pem\"},\n                 {certfile,             \"/Users/me/rabbitmqcert/server/cert.pem\"},\n                 {keyfile,              \"/Users/me/rabbitmqcert/server/key.pem\"},\n                 {verify,               verify_peer},\n                 {fail_if_no_peer_cert, false}]}\n]\n\n}\n].\n```\nAfter creating the rabbitmq.config file, I restarted the rabbitmq broker. I used rabbitmq-server (without the -detached arg) to start it back up and verify that it started successfully.\nThe following pika commands worked for me:\n```\nimport ssl\nimport pika\ncp = pika.ConnectionParameters(ssl=True, ssl_options=dict(ssl_version=ssl.PROTOCOL_TLSv1))\nconn = pika.BlockingConnection(cp)\nch = conn.channel()\nch.queue_declare(\"sslq\")\nch.publish(\"\", \"sslq\", \"abc\")\n```\n. And here is a more sophisticated example that forces mutual cert verification: https://github.com/pika/pika/issues/726#issuecomment-213853787\n. @AdamMiltonBarker, PR #742 contains examples of TLS connectivity using mutual and server-only certification authentication. I verified those on OS X, in addition to all the info in my previous comments.\nAt this point, your post at https://groups.google.com/forum/#!topic/rabbitmq-users/u9hpB1ewDjQ should be more productive. I am going to close this issue for now.\n. Hi @hasB4K, sorry for not replying earlier. I am and will continue to be overwhelmed with other issues for the foreseeable future, so deferring to @gmr.\nBest regards,\nVitaly. Please post the following:\n1. Operating System where pika was used\n2. pika version (or whether you're using pika from pika master branch)\n3. rabbitmq version\n4. DEBUG-level log from pika for the affected period\n5. rabbitmq log for the affected period\n. Also, you say \"sometimes hangs for quiet a while\". Does it eventually complete or does it hang forever?\n. In your traceback, _flush_output (called from basic_cancel) is waiting for the connection's output buffer to be flushed and for Basic.CancelOk response from rabbitmq.\nSince you're consuming with no_ack=True, there might be a bunch of incoming messages in rabbitmq's output sockbufs and pika connection's input sockbufs already, so Basic.CancelOk won't be able to arrive until rabbitmq handles the Basic.Cancel request and all those messages in sockbufs are flushed through the end-to-end connection.\nAnd while one of the consumers is being cancelled, rabbitmq continues to stream messages for the other  no_ack=True consumers that you created (\"In our setup we have a few (blocking) channels on which we do .consume\").\nInstead of relying on BlockingConnection.close() to close the channels and cancel their consumers implicitly, try canceling all the consumers on all the channels explicitly first, before you call BlockingConnection.close(). Log the time that it takes to cancel each consumer. This might give you some additional insight.\n. @mishas, thanks for the update. Definitely, sharing a connection from multiple threads will lead to problems such as the one that you describe here. I am going to assume that the multi-threaded access is the cause and close this issue. If the problem still persists after fixing multithreading on your end, please open a new issue.\n. @hasB4K, it appears that #765 was due to openssl eliminating the weak crypto. See https://github.com/pika/pika/issues/765#issuecomment-247906951.\n\n\"Verification of signatures using the MD5 hash algorithm is disabled in Red Hat Enterprise Linux 7 due to insufficient strength of this algorithm. Always use strong algorithms such as SHA256.\"\n. UPDATE - I opened issue #981 to deal with the enablement of SSL connectivity to RabbitMQ in the build/test environments.\n\n@lukebakken, I would like to fix/clean up SSL support in pika. Could you please help by enabling RabbitMQ to accept SSL connections in addition to plaintext on all Travis-CI and AppVeyor builds? Also provide rudimentary documentation where the client test code can find the necessary artifacts (e.g., public cert).\nPlease make sure that the certs never expire or at least don't expire for many years.\nSee https://github.com/pika/pika/blob/master/docs/examples/tls_mutual_authentication.rst and https://github.com/pika/pika/blob/master/docs/examples/tls_server_uathentication.rst for examples.. @lukebakken  - but don't we want to have TLS/SSL test automation in place? I think we need it. In the past, such things got broken periodically because there were not tests. Having an open ticket for it is a good way to make sure it doesn't fall through the cracks.. @lukebakken , thanks for the clarification. I remember a while back you had to disable TLS tests (in one of the environments, perhaps?) due to an issue with getting RabbitMQ server to install properly for TLS there or something like that.. Apparently, that workaround from https://github.com/conda-forge/pika-feedstock/blob/28de089e73481cf3b4e6d2ee0b7b60bb55356788/appveyor.yml#L45-L53 is for a different problem that has already been fixed by AppVeyor according to reply from Feodor in http://help.appveyor.com/discussions/problems/4490-undesired-rebuilding-of-all-open-prs-after-a-merge-into-master. So, closing... \n. It would be interesting to know why RabbitMQ decided to close AMQP connection. It might be some sort of networking issue when Vagrant is in the picture\nSeveral things would help:\n1. Version of rabbitmq, pika, and relevant OS\n2. Simplified python script using BlockingConnection that reproduces the problem on your rabbit+docker+vagrand configuration\n3. Debug logs from pika: import logging; logging.basicConfig(level=logging.DEBUG) before creating BlockingConnection. \n. Please attach a simple script using BlockingConnection that reproduces the lost connection.\nFrom what the pika logs show, it appears that pika simply detected a lost socket connection. Pika in the master branch has had a number of improvements to connection error handling, so you might want to repeat your experiment.\nIt could be an error somewhere else in your stack that causes the connection to drop. Take a careful look through rabbitmq logs.\n. @Silver90 Is it okay to close this issue?\n. Thanks for the update. Closing...\n. @SamuelRamond: See chanel method confirm_delivery.and https://www.rabbitmq.com/confirms.html\n. @SamuelRamond: there are likely a number of possible reasons. Here is one that I am aware of:\n- Socket buffering in the kernels (on both sides) and the network (e.g., routers): whatever pika writes to a socket goes through buffering in the kernel and more of that in the network. So, the kernel can accept the published data before it receives the TCP/IP signaling that the connection is being shut down.\nDepending on implementation, there could also be buffering at the app level. BlockingConnection in 0.9.14 could leave data in the outbound buffer and return from the basic_publish call. In pika 0.10.0, BlockingConnection API methods block until all outbound data buffered by pika is written to the socket. These implementation details may change in the future, of course.\nIn my tests, where I needed to reproduce sudden loss of connection, I've also come across cases where one side would initiate closing of the TCP/IP stream, and the other side would still be able to write to the socket at least once without getting an error, unless the peer set SO_LINGER to 0 seconds before closing the socket (which is not the norm).\n. BlockingConnection in pika 0.10.0 is a complete re-write as a shim around SelectConnection, with significantly improved publish performance.\n. Which OS and version of pika are you using?\nIf you're wrapping your entire pika session in a Popen, and then reaping the subprocess (confirming that it completed), I don't see how pika could be contributing to memory leaks, since the operating system is responsible for cleaning up memory upon process termination. Are you sure that the subprocesses are terminating in a timely manner?\nIt would help to isolate the code into a simpler app; for example, eliminate Django, and just implement a simple python app that publishes messages using subprocess.Popen just like you were with Django, and checking to make sure that the process completes, etc., and tracking memory usage.\nPost the simple code example that reproduces the problem without Django.\n. Thanks for the update, closing...\n. @Zhomart, thank you for your submission, but it's not clear exactly which problem you're trying to solve. Please describe the specific scenario in detail. And how will this change help you solve it?\n. @Zhomart, thanks for following up. Your example doesn't show how TestRpcServer.tearDown gets called. If you're calling TestRpcServer.tearDown from another thread, then you're causing a race condition, because pika connections are not thread-safe.\nSo, your change might appear like it's helping, but the race condition can do much worse things, such as corrupting memory and the state of the connection.\n. @Zhomart, perhaps you can try the following:\n1. From the thread that runs your pika connection, use your connection's add_timeout to schedule a timer callback;\n2. In the callback, check a flag in a member variable or global variable that lets it know if it should stop; if the flag is set, then the timer callback would call stop_consuming; if the flag is not set, it re-schedules the timer for a new interval to check the flag.\n3. When the start_consuming call returns, RpcServer.run closes the channel.\n4. When your other thread wants the server to stop, it simply sets the above-mentioned flag and joins the connection thread.\n. @Zhomart - I'm glad the suggestion worked for you.\n. That's a correct observation about pika.BlockingConnection. It's completely synchronous. If you are not doing any I/O on the given pika connection, you need to periodically call the connection's BlockingConnection.process_data_events or BlockingConnection.sleep method in order to process heartbeats.\nI am working on a threaded prototype.\n. If you have a long computation, then you can set a large heartbeat timeout (longer than your anticipated compute time) or turn off heartbeat altogether. Note that the 0.10.0 version of pika doesn't facilitate setting a longer heartbeat timeout - pika in master branch has a fix for it.\nAnother option is to periodically call BlockingConnection.process_data_events while performing your long computation. Make sure that you call BlockingConnection.process_data_events frequently enough to avoid heartbeat timeout.\npika connection instances are not thread-safe, so calling BlockingConnection.process_data_events from another thread will cause bad things to happen.\nLibevConnection and SelectConnection also execute entirely in a single thread, so if you computation blocks for a long time (not using the adapter's ioloop), then it will not help with the timeout issue.\n. @tcwalther, I had the threaded connection prototype working in https://github.com/vitaly-krugl/pika/tree/threaded-connection, and passing most tests. However, things got busy everywhere else. One of the challenges with the threaded connection that I haven't solved yet is what happens in the noack case: since the connection \"gateway\" is running in its own thread, it would keep reading from the broker indefinitely, and would eventually siphon off all messages from the queue or run out of memory if the consumer doesn't keep up.\n. add_callback_threadsafe in pull request #956 might help with this.. @pat1, if the processing takes so long, then it's not a good match for the asynchronous programming model. Asynchronous programming model works well for I/O-bound processing that uses a common ioloop for all blocking operations within a thread.\nI am also surprised that with heartbeat_interval=0 you would get heartbeat timeouts. That should completely disable heartbeats. Please check the rabbitmq log to see if the connection is dropped due to heartbeat timeout or something else.\nYour app is not accessing the same pika connection (and/or its channels) from more than one thread, is it? pika connections are not thread-safe, so you would expect to have trouble in that case.\nThe following would be really helpful for debugging:\n1. your environment (os, os version, etc.)\n2. rabbitmq version\n3. A small, simple script that easily reproduces the lost connection. You may use time.sleep instead of your long-running processing operation.\n. What does the network look like? Is there a proxy or similar between the client and rabbitmq broker?\n. Also, what does the RabbitMQ log have to say about this? Please post the RabbitMQ log that shows the disconnect.\n. @pat1, the rabbitmq log doesn't look like the failure is related to heartbeat. Note that the error is reported as \"send failed ... timeout\" just 30 seconds after \"accepted AMQP connection\". A send timeout would usually mean that transmit retries were exhausted and the peer is unreachable.\nThis is what a heartbeat timeout log entry looks like:\n```\n=INFO REPORT==== 31-May-2016::13:30:38 ===\naccepting AMQP connection <0.1559.0> (127.0.0.1:63742 -> 127.0.0.1:5672)\n=ERROR REPORT==== 31-May-2016::13:33:38 ===\nclosing AMQP connection <0.1559.0> (127.0.0.1:63742 -> 127.0.0.1:5672):\nmissed heartbeats from client, timeout: 60s\n```\n. @pat1, thank you for following up. My expectation would be that RMQ should be doing non-blocking I/O, and should not fail this way. This sounds to me like a bug in RMQ.\nI would like to investigate and follow up on this with the RMQ team. Your code snippet https://gist.github.com/pat1/4017d6565501b657731560af3d2e0b9e contains only the consumer. Would you mind also adding a small script that populates the queue with the size and number of messages that will surely reproduce the failure? Using pika.BlockingConnection would make it super simple to do by avoiding all those callbacks. Thank you.\n. Thank you, @pat1.\n. add_callback_threadsafe in pull request #956 might help with this. See this example. @tkisme, thank you for the your question. Please use https://groups.google.com/forum/#!forum/pika-python to ask questions.\n. @Dmitry-N-Medvedev, please use https://groups.google.com/forum/#!forum/pika-python for pika questions.\n. The pull request #956 might be helpful with the new method add_callback_threadsafe.\nIn general, you might need to wrap the connection and use a decorator for reconnecting.\nFor concept, see _RETRY_ON_AMQP_ERROR decorator and _ChannelManager transport wrapper class. This specific implementation is now using the haigha client under the covers (for licensing reasons), but used to use pika BlockingConnection in the same way.. This should be addressed by pull request #1028. Closing.... @reallistic, could you please be more specific about \"The latest code still has some bugs\"? Are you referring to #759?\n. @lukebakken, there is no reference to the PR that fixes this issue. Did you close it by accident? Thx.. @Serafean, thanks for the feedback. I added the inactivity_timeout feature and, in retrospect, I think that I would now prefer yield (None, None, None) in this case. However, the current behavior is already in an official release and likely already in use, so I would hate to break those apps.\nAnother option would be to add a second example to the method's docstring:\n```\nfor msg in channel.consume('queue', inactivity_timeout=5):\n    if msg is None:\n        print \"got inactivity timeout\"\n        continue\nmethod, properties, body = msg\nprint body\nchannel.basic_ack(method.delivery_tag)\n\n```\n@gmr, could you please chime in with your decision on the above two options (or suggest something else)? Thx.\n. Yeah, I would like to clean this up; waiting for guidance from @gmr.\n. Anyone wish to submit a PR for yield (None, None, None), including updated tests and documentation?\n. Technically, since pika is < v1.0, it should be reasonable to make breaking improvements like this, and I think we did a little of that in the previous release cycle.\n. @hduzh, have you made any progress resolving this? Please update this issue. Thx.\n. @joshpurvis @xiaopeng163 @mlmarius: The async examples were not in a good shape. I refactored the async publisher example via this pull request: #710 a while ago. That addressed a number of problems like the one reported here. Perhaps someone can refactor the async consumer example along the same lines as #710?. When TCP/IP is trying to transmit on a failed network or to a downed peer, the local tcp/ip stack will eventually detect the problem and place the local socket in a failed state once all the retries are exhausted without an ACK from remote.\nHowever, when just waiting to receive something, such as a message from Rabbit MQ, no retries are involved on the receiving end, so the TCP/IP stack can't  tell on its own (by default) that the other peer is unreachable.\nIf you have AMQP heartbeats enabled, then heartbeat messages will be sent on the socket connection per negotiated AMQP heartbeat configuration. Again, when the remote peer is unreachable for any reason, the TCP/IP retries of the heartbeat message will eventually be exhausted, and the local socket will be placed in a failed state. Also, AMQP heartbeat logic should be able to detect dead connections.\nAt the TCP/IP level, there is a thing known as TCP Keepalive. This may be configured independently of AMQP heartbeats - directly on the connection's socket via socket options (Pika doesn't provide an API for this yet). When TCP Keepalive is enabled on the socket, the local TCP/IP stack will periodically send keepalive messages to the remote peer's TCP/IP stack on the same socket. When the configured TCP keepalive retries are exhausted, the local TCP/IP stack will place the local socket in the failed state, similarly to what happens when sending regular data and retransmissions are exhausted. See https://en.wikipedia.org/wiki/Keepalive and http://tldp.org/HOWTO/TCP-Keepalive-HOWTO/overview.html.. cc @gmr \n@mnoky, you haven't provided any debug-level logs of your session, so it's hard to surmise exactly what is happening. Most likely, your connection is subject to being blocked by RabbitMQ that is signaled to the client via connection.blocked, which is RabbitMQ's extension to AMQP protocol; see https://www.rabbitmq.com/connection-blocked.html.\nYou can often gain valuable insight by enabling DEBUG-level logging before starting the connection and observing the log. Personally, I really like the free WireShark protocol analyzer from www.wireshark.org, as it now supports AMQP.\nI added a feature to ConnectionParamters via the blocked_connection_timeout argument that might/should help if you're talking to a real RabbitMQ server (it won't help in the fake nc -k -l 5672 case); see  https://github.com/pika/pika/blob/5dc90ad8bce20687009d9e9ed358c84c4666d3f1/pika/connection.py#L596-L603\nNote that the blocked_connection_timeout arg is not in the released version of Pika, so you would need to get Pika from master to try it.\nPlease post how it went. If blocked_connection_timeout didn't work for you, please collect AMQP protocol information from your session via WireShark and post a gist link here. I am going to close the issue for now, but can re-open if needed.\n. I am glad that blocked_connection_timeout in master branch worked for you. Thanks for following up.\nRegarding:\n\nOne followup question: Is an official new release due anytime soon?\n\n@gmr has been pretty busy, but I hope some cycles will free up soon for an official release.\n. @mnoky, I can't speak for myself, as I am not a major user at this moment, although I continue to pitch in. However, here is a recent post on google groups that is quite positive: https://groups.google.com/forum/#!topic/pika-python/RRt3zBeDhQU. Perhaps you can follow up there?\n. @fengyiyingdong, does the information in pull request #742 help with your issue?\n. @fengyiyingdong, thank you for the update.\n. @JonathanBrannock, it would help if you provided a minimal code sample that reproduces the issue.\nIn the absence of such sample, one thing jumps out in particular:\n\nA child process started from handle_delivery raises exceptions.ConnectionClosed()\n\nThis sounds like your app forks a child process from the handle_delivery callback that continues to use the connection/channel that was created in the parent process. This would imply that both child and parent processes are reading/writing the same socket without any coordination.\nThis leads to corruption of the traffic on the shared TCP/IP stream, and is an unsupported configuration.\n. Also, depending on the forking parameters, the file descriptors (including sockets) that were created in the parent might be automatically closed in the fork, which would explain \"The AMQP connection was closed: ()\" in the child process.\nIn any case, it sounds like your app is using pika in an unsupported way.\n. An instance of a pika connection and anything derived from it (such as a channel) must all be used within the same thread. See \"Is Pika thread safe?\" in http://pika.readthedocs.io/en/latest/faq.html.\n@JonathanBrannock, since this is user error rather than a bug in pika, I am going to close this issue. A great forum to post your question and get help from experienced users is https://groups.google.com/forum/#!forum/rabbitmq-users. There is also https://groups.google.com/forum/#!forum/pika-python, but it gets far less traffic.\nOn occasion, you might get a bit of misleading advice from someone who claims that they are using the same pika connection from multiple threads, sent billions of messages, and never ever had any trouble. Take such advice with a grain of salt \ud83d\ude03 \n. @Michael754267513, I wrapped your sample and traceback in \"```\" to make the code easier to read (indentation and all)\n. Thanks for the update!\n. Hi Mosquito, thank you for submitting this PR. I took a brief look, and a few things jumped out right away that are lacking:\n1. Documentation (helpful docstrings)\n2. Usage examples\n3. Tests\n. I would recommend starting with helpful docstrings and usage examples that would help us evaluate the API. Once the API is reviewed/refined, then tests would be the next step before the PR may be considered for merging.\nBest regards,\nVitaly\n. @mosquito, it looks like the resulting adapter instance would be API-compatible with TornadoConnection and SelectConnection adapters.\nThis would make it super easy to plug it into the async adapter acceptance test framework simply by adding a tiny function similar to tornado_test in https://github.com/pika/pika/blob/7d0ef0bb9db4aa9f9e7020e9539c171c4ce5c45d/tests/acceptance/async_test_base.py#L191.\nYours will need to have a skip decorator so that it only runs on versions of python 3.x, where the compatible asyncio is available.\n. @gmr, I am not at all familiar with asyncio at this point. I am counting on you for evaluating the suitability of this adaptation to the asyncio model. Thx.\n. Thanks, @mosquito. Integrating into the acceptance test framework gives us a better idea where the breaking points might be. Presently, asyncio is failing some tests on python 3.4+.\nI also see a bunch of new ResourceWarning messages at the end of the logs. They indicate that there are new resource leaks. This is something to watch out for. Perhaps fixing the broken functionality will resolve the leaks as well. If not, then the leaks need to be fixed. You will want to compare to other recent PR builds.\n. @antoineleclair, I would never call something from a signal handler that's not explicitly documented as \"signal-safe\". That's asking for trouble. Also, using the private instance variable _consumer_infos is problematic, because it may change in the future.\nThe following may work for you, using documented public API:\n1. In sigterm_received, set a global variable to True (e.g., g_stop_requested = True) or something that's equivalent and signal-safe.\n2. Use BlockingConnection.add_timeout to request a 1-second timer callback\n3. In the callback, check if your g_stop_requested (or equivalent) is set, and call BlockingChannel.stop_consuming to cancel all consumers, if so. If not, request a new timer callback.\nI am generally not crazy about polling, but BlockingConnection implements the synchronous command metaphor, so there is not much else you can do, I think. Also, calling channel.connection.process_data_events(time_limit=1) is also polling and using a timer under the covers.\nHope this helps!\n. Please close the issue if my reply resolved the problem.\n. @antoineleclair - And another thing, if there is the possibility that your RabbitMQ server may run low on resources (low disk space, low RAM), then RabbitMQ will go into blocking mode (won't be reading from your connection), which may block your basic_ack or basic_publish or a synchronous request indefinitely until resources free up. In that case, the timer callbacks won't be processed until RabbitMQ becomes unblocked.\nFor that, there is a new feature in master branch (not released yet), that allows you to set a blocked connection timeout. Find out more here: https://github.com/pika/pika/blob/7922e076c2b5ded29156b9523dc5cf8178c3f1ce/pika/adapters/blocking_connection.py#L285-L307\n. @antoineleclair, thank you for your update and feedback.\nA pika connection is not thread-safe. Same concerning signal-safety. Supporting those paradigms typically introduces significant complexity and performance issues and is outside the scope of this library.\n. @hodgestar, a pika connection is not thread-safe, so nothing should be calling add_timeout when the select loop is sleeping. See\n- http://pika.readthedocs.io/en/latest/faq.html\n. In order to support making add_timeout thread-safe, simply writing to _w_interrupt would not suffice. The event loop (at least the timer management) would need to be made thread-safe, too.\n. @hodgestar: That said, I think that a thread-safe add_callback in Tornado is a good example of something that may be worthwhile to have in BlockingConnection: http://www.tornadoweb.org/en/stable/ioloop.html#callbacks-and-timeouts. The callback itself could then make other calls to the pika connection safely. \n. @hodgestar, please feel free to open a new issue regarding add_callback. And if you're inclined to submit a pull request, including tests, for review, that would be welcome, too.\n. @kirkdsayre, I would expect self._poll.close() to be called inside the finally block within _PollerBase.start, after the polling  loop (while not self._stopping:) exits and nesting level unwinds back down to 0. Referring to the code excerpts below, here is the call chain: _PollerBase.start/finally: -> self.deactivate_poller() -> self._uninit_poller() -> if hasattr(self._poll, \"close\"): self._poll.close()\nWould you mind posting a minimal code snippet that reproduces the leak?\n```\nclass PollPoller(_PollerBase):\ndef _uninit_poller(self):\n        \"\"\"Notify the implementation to release the poller resource\"\"\"\n        if hasattr(self._poll, \"close\"):\n            self._poll.close()\n```\n```\nclass _PollerBase(_AbstractBase): \n   def deactivate_poller(self):\n        \"\"\"Deactivate the poller\n        \"\"\"\n        self._uninit_poller()\ndef start(self):\n        \"\"\"Start the main poller loop. It will loop until requested to exit\n        \"\"\"\n        self._start_nesting_levels += 1\n    if self._start_nesting_levels == 1:\n        LOGGER.debug('Entering IOLoop')\n        self._stopping = False\n\n        # Activate the underlying poller and register current events\n        self.activate_poller()\n\n        # Create ioloop-interrupt socket pair and register read handler.\n        # NOTE: we defer their creation because some users (e.g.,\n        # BlockingConnection adapter) don't use the event loop and these\n        # sockets would get reported as leaks\n        with self._mutex:\n            assert self._r_interrupt is None\n            self._r_interrupt, self._w_interrupt = self._get_interrupt_pair()\n            self.add_handler(self._r_interrupt.fileno(),\n                             self._read_interrupt,\n                             READ)\n\n    else:\n        LOGGER.debug('Reentering IOLoop at nesting level=%s',\n                     self._start_nesting_levels)\n\n    try:\n        # Run event loop\n        while not self._stopping:\n            self.poll()\n            self.process_timeouts()\n\n    finally:\n        self._start_nesting_levels -= 1\n\n        if self._start_nesting_levels == 0:\n            LOGGER.debug('Cleaning up IOLoop')\n            # Unregister and close ioloop-interrupt socket pair\n            with self._mutex:\n                self.remove_handler(self._r_interrupt.fileno())\n                self._r_interrupt.close()\n                self._r_interrupt = None\n                self._w_interrupt.close()\n                self._w_interrupt = None\n\n            # Deactivate the underlying poller\n            self.deactivate_poller()\n        else:\n            LOGGER.debug('Leaving IOLoop with %s nesting levels remaining',\n                         self._start_nesting_levels)\n\n```\n```\nclass EPollPoller(PollPoller):\n    \"\"\"EPoll works on Linux and can have better performance than Poll in\n    certain scenarios. Both are faster than select.\n    \"\"\"\n    POLL_TIMEOUT_MULT = 1\n@staticmethod\ndef _create_poller():\n    \"\"\"\n    :rtype: `select.poll`\n    \"\"\"\n    return select.epoll()\n\n```\n. @kirkdsayre, thank you for following up.\n. Hi @senseysensor, please post on https://groups.google.com/forum/#!forum/rabbitmq-users or https://groups.google.com/forum/#!forum/pika-python. Thanks!\n. Pika has no auto-reconnect. You can't continue to use the connection after it disconnects. Also, in case there are threads involved, a pika connection is not thread-safe.\n. It may help to capture and analyze the AMQP protocol using the freely available WireShark protocol analyzer.\n. If this were a bug in pika, it would make all the pika tests that declare and bind a queue take a very long time to complete, but they don't take that long. So, I am going to guess that the issue is either in your app or with how you use pika.\nA good place to start is:\n1. Create a tiny, focused code sample that reproduces your issue\n2. Examine the server logs\n3. Examine AMQP traffic using the free WireShark protocol analyzer\n4. Examine pika's debug-level logs.\n. Your app is not using a connection (or its channels) from multiple threads, is it?\n. @lexdene, which specific problem is this addressing?\n. @lexdene - thank you!\n. Based on the Pika logs, it seems like the client thinks that the server closed the connection: \"Socket closed when connection was open\". Perhaps there is something wrong with your docker/network configuration?\nAlso, recent RabbitMQ servers default to 60 seconds heartbeat timeout (it used to be almost 10 minutes in older versions of RabbitMQ). The ability to increase this is broken in the released versions of pika, but is fixed in pika master. If you have control over the RabbitMQ server configuration, you should be able to increase the default heartbeat timeout that way, too.\n. @NicolasDuran, thank you for the update.\n. @nicgirault, it looks like epoll was being used and event 25 is the combination of EPOLLIN, EPOLLERR, and EPOLLHUP, meaning that the other end (presumably RabbitMQ server) closed the connection.. I recall from some time ago that RabbitMQ by default comes configured to support a very small number of file descriptors, and you need to change its configuration to support a much larger number of descriptors to use in a real environment. This comes up a number of times in the rabbitmq-users google group, for example here.. Do these examples help? https://github.com/pika/pika/pull/742/files. Closing as duplicate of #760. Thank you for the question. I am going to close this issue since the RabbitMQ Google Group is the more appropriate forum for asking questions. See https://groups.google.com/forum/#!forum/rabbitmq-users. @szepeviktor, please re-post your question on the rabbimtmq-users group https://groups.google.com/forum/#!forum/rabbitmq-users, which is a better forum for asking questions.. @szepeviktor, thank you for sharing the answer.. Let's see what rabbimtmq-users group  has to say about your questions.. Thank you for your question. Please repost on the rabbitmq-users google group.. Hi, have you checked server logs? Look for heartbeat timeout errors. What's the max time that your app takes to process a message? As in max time between calls into the channel to retrieve the next message?. If server logs don't help, Wireshark now includes support for the AMQP protocol and is great for tracking down comms issues.\nAnother possibility is threading: is your app multi-threaded? A given Connection instance in Pika is not thread-safe, and using it from more than one thread will corrupt the data path at some point, leading to closed connections.. Try the other things I suggested above\n\nHi, have you checked server logs? Look for heartbeat timeout errors. What's the max time that your app takes to process a message? As in max time between calls into the channel to retrieve the next message?\nIf server logs don't help, Wireshark now includes support for the AMQP protocol and is great for tracking down comms issues.. Those parameters are in master, but have not been officially released. This is what you have in the release 0.10.0: https://github.com/pika/pika/blob/0.10.0/pika/connection.py#L304-L339. @lukebakken, this should be fixed now because we removed the nowait parameter, right?. IIRC, Acn and Nack should be mutually-exclusive, so you may be violating AMQP when you do both. Take a look at server-side logs after this happens. Also, please repost your question on the RabbitMQ-Users google group.. @lukebakken, two additional items that would help greatly is if the user provided both of the following:\n1. Complete DEBUG-level pika logs leading up to and including the failure.\n2. WireShark AMQP log of the problem. Also, a very common problem is when users use the same pika connection from more than one thread. Sometimes they are not even aware that their app is accessing the same connection (or channels from the same connection instance) from more than one thread.. Thanks for the update @hannuvisti !. Good point @markunsworth. I am going to close this issue. @amulyas, please post your question on the rabbitmq-users google group.. @mounikakuchana, a single pika connection is not thread-safe, per documentation. As such, each thread must have its own pika connection. . Per pika faq:\nPika does not have any notion of threading in the code. If you want to use Pika with threading, make sure you have a Pika connection per thread, created in that thread. It is not safe to share one Pika connection across threads.\n\nConnections are cheap now-a-days. RabbitMQ, AFAIK, is quite capable of supporting a huge number of connections when configured to do so. Also, modern socket I/O waiting techniques, such as epoll on Linux and Kernel Queues, are very efficient at handling multiple sockets, and pika's Blocking Connection takes advantage of those mechanisms via the underlying SelectConnection adapter. So, what is the allure of sharing a single pika connection across multiple threads?\nThat said, I prototyped a ThreadedConnection derivative of BlockingConnection a couple of years ago that allows a single physical Pika connection to be shared across multiple threads using BlockingConnection API and keeps the AMQP heartbeat going (prevents connection timeout) even when individual threads take a very long time to process a message or block excessively for some other reason. Although the latter is convenient, the throughput in the multi-threaded implementation degraded a great deal to something like 70% below the single-threaded BlockingConnection implementation (I don't remember the exact details of RX vs TX degradation). Perhaps with more engineering time and resources performance could be improved. In my prototype, the socket connection to RabbitMQ is serviced by a separate thread, which is also responsible for servicing AMQP heartbeats. Besides performance, there was one other problem that needed to be solved in this prototype: throttling messages to QoS-less consumers when the consumer can't keep up with incoming messages; the prototype just ends up caching all such incoming messages in RAM, which may be an issue in production scenarios. If someone wishes to explore my ThreadedConnection prototype, you may find it in this branch: https://github.com/vitaly-krugl/pika/tree/threaded-connection.. @mlmarius, could you please post:\n1. description of what you mean by \"things break\"\n2. the failure traceback and \n3. link to debug-level log leading up to and including the failure\n4. What's the relationship between this issue and #762. When TCP/IP is trying to transmit on a failed network or to a downed peer, the local tcp/ip stack will eventually detect the problem and place the local socket in a failed state once all the retries are exhausted without an ACK from remote.\nHowever, when just waiting to receive something, such as a message from Rabbit MQ, no retries are involved on the receiving end, so the TCP/IP stack can't  tell on its own (by default) that the other peer is unreachable.\nIf you have AMQP heartbeats enabled, then heartbeat messages will be sent on the socket connection per negotiated AMQP heartbeat configuration. Again, when the remote peer is unreachable for any reason, the TCP/IP retries of the heartbeat message will eventually be exhausted, and the local socket will be placed in a failed state. Also, AMQP heartbeat logic should be able to detect dead connections.\nAt the TCP/IP level, there is a thing known as TCP Keepalive. This may be configured independently of AMQP heartbeats - directly on the connection's socket via socket options (Pika doesn't provide an API for this yet). When TCP Keepalive is enabled on the socket, the local TCP/IP stack will periodically send keepalive messages to the remote peer's TCP/IP stack on the same socket. When the configured TCP keepalive retries are exhausted, the local TCP/IP stack will place the local socket in the failed state, similarly to what happens when sending regular data and retransmissions are exhausted. See https://en.wikipedia.org/wiki/Keepalive and http://tldp.org/HOWTO/TCP-Keepalive-HOWTO/overview.html.. @mlmarius: FRAME_MAX_SIZE determines the max frame size. Each message that's larger than this is divided into multiple frames. The total message size is limited by available memory (client/server) and also possibly disk size (server).\nWhich pika version are you using and on which platform? Which version of RabbitMQ server?\nPlease post a minimal, self-contained snippet of code that reproduces your issue.\nI had no problem executing the following snippet of code using pika 0.10.0 on OS X. It's using BlockingConnection, which builds on top of SelectConnection:\nc = BlockingConnection()\nch = c.channel()\nch.publish(\"\", \"\", \"a\" * 100000000, properties=None, mandatory=False, immediate=False). Is your app using more than one thread, by chance?. Good job figuring this out @mlmarius ! The original example should have used different variables/values for the property headers versus the message data to make it less error-prone.\nAlso, pika could probably do a better job validating the size of the header before trying to send it out.\nI just opened issue #832 to make the async publisher example less error-prone. It's a simple change that would help the next developer avoid this mistake. Would you like to tackle it?\nAnd I am going to close this issue since you have it resolved.. @aswinQuiklo, rabbitmq-users google group is likely a better place to ask your question.. @mvallebr, is your app using threads?. I get a ConnectionClosed exception with Pika 0.10.0 with the following simple snippet of code executed in IPython:\n```\nIn [7]: from pika import BlockingConnection\nIn [8]: import time\nIn [9]: c = BlockingConnection()\nIn [10]: time.sleep(180) # default late-version rabbitmq heartbeat interval is 60 sec\nIn [11]: c.channel()\nConnectionClosed                          Traceback (most recent call last)\n in ()\n----> 1 c.channel()\n/Users/vkruglikov/Library/Python/2.7/lib/python/site-packages/pika/adapters/blocking_connection.pyc in channel(self, channel_number)\n    696 \n    697             # Drive I/O until Channel.Open-ok\n--> 698             channel._flush_output(opened_args.is_ready)\n    699 \n    700 \n/Users/vkruglikov/Library/Python/2.7/lib/python/site-packages/pika/adapters/blocking_connection.pyc in _flush_output(self, waiters)\n   1172         self._connection._flush_output(\n   1173             self._channel_closed_by_broker_result.is_ready,\n-> 1174             waiters)\n   1175 \n   1176         if self._channel_closed_by_broker_result:\n/Users/vkruglikov/Library/Python/2.7/lib/python/site-packages/pika/adapters/blocking_connection.pyc in _flush_output(self, *waiters)\n    424                     # an error\n    425                     LOGGER.critical('Connection close detected')\n--> 426                     raise exceptions.ConnectionClosed()\n    427                 else:\n    428                     LOGGER.info('Connection closed; result=%r', result)\nConnectionClosed: \n```. Please provide minimal, self-contained code that reproduces your issue reliably, as well as any additional information necessary for reproducing it.. Examining the client-side debug-level logs and also the RabbitMQ logs should give you a clue about the event that triggers the issue.. Thanks @lukebakken . I was able to reproduce the issue on OS X with stock pika 0.10.0 using @westphahl's code snippet from #803 to which I added some printfs to help grok the flow of events:\n```python\nimport logging\nimport pika\nclass Consumer:\ndef run(self, queue):\n    self.connection = connection = pika.BlockingConnection()\n    channel = connection.channel()\n\n    channel.basic_qos(prefetch_count=1, all_channels=True)\n    channel.queue_declare(queue=queue, durable=False,\n                          exclusive=False, auto_delete=False)\n    channel.basic_consume(self.on_message, queue)\n    connection.add_timeout(1, self.on_timeout)\n    try:\n        channel.start_consuming()\n    except KeyboardInterrupt:\n        channel.stop_consuming()\n\ndef on_timeout(self):\n    print \"ZZZ on_timeout called\"\n    self.connection.add_timeout(1, self.on_timeout)\n\ndef on_message(self, channel, method, properties, body):\n    print \"ZZZ on_message called, starting connection.sleep(200)\"\n    self.connection.sleep(200)\n    print \"ZZZ calling basic_ack\"\n    channel.basic_ack(method.delivery_tag)\n    print \"ZZZ returned from basic_ack\"\n\nif name == \"main\":\n    logging.basicConfig(level=logging.INFO)\n    consumer = Consumer()\n    consumer.run(\"foobar\")\n```\nIt fails about 200 seconds after publishing an event into the \"foobar\" queue:\nINFO:pika.adapters.base_connection:Connecting to 127.0.0.1:5672\nINFO:pika.adapters.blocking_connection:Created channel=1\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_timeout called\nZZZ on_message called, starting connection.sleep(200)\nZZZ calling basic_ack\nZZZ returned from basic_ack\nZZZ on_timeout called\nERROR:pika.adapters.base_connection:Fatal Socket Error: error(32, 'Broken pipe')\nWARNING:pika.adapters.base_connection:Socket closed when connection was open\nWARNING:pika.connection:Disconnected from RabbitMQ at localhost:5672 (0): Not specified\nCRITICAL:pika.adapters.blocking_connection:Connection close detected\nTraceback (most recent call last):\n  File \"pika_nonactionable_events.py\", line 36, in <module>\n    consumer.run(\"foobar\")\n  File \"pika_nonactionable_events.py\", line 17, in run\n    channel.start_consuming()\n  File \"/Users/vkruglikov/Library/Python/2.7/lib/python/site-packages/pika/adapters/blocking_connection.py\", line 1681, in start_consuming\n    self.connection.process_data_events(time_limit=None)\n  File \"/Users/vkruglikov/Library/Python/2.7/lib/python/site-packages/pika/adapters/blocking_connection.py\", line 647, in process_data_events\n    self._flush_output(common_terminator)\n  File \"/Users/vkruglikov/Library/Python/2.7/lib/python/site-packages/pika/adapters/blocking_connection.py\", line 426, in _flush_output\n    raise exceptions.ConnectionClosed()\npika.exceptions.ConnectionClosed\nRMQ log reports:\n```\n=INFO REPORT==== 25-Jul-2017::16:21:08 ===\nconnection <0.774.0> (127.0.0.1:54745 -> 127.0.0.1:5672): user 'guest' authenticated and granted access to vhost '/'\n=ERROR REPORT==== 25-Jul-2017::16:24:08 ===\nclosing AMQP connection <0.774.0> (127.0.0.1:54745 -> 127.0.0.1:5672):\nmissed heartbeats from client, timeout: 60s\n```. @oleynikandrey: what did you expect to happen and what happens on 0.10.0 in this scenario? Thank you.. There might be some outstanding requests that were interrupted as the result of the restart. Pika lets the app know via exception so that the app can take whatever app-specific measures it needs to recover.. I agree with @hairyhum's assessment that explicit reconnection by user promotes better app code quality. It helps ensure that apps don't miss these important transitions, just as exceptions in OO programming (versus return values in non-OO) help make sure that errors are not ignored.\nRelated to a specific connection instance, apps will have channels created from the original connection that should no longer work. If the channels are \"restored\" automatically, then it leads to terrible side-effects - for example, a message received on a specific channel number may only be ACK'ed on that channel (per AMQP). If you automatically \"restore\" a channel after automatically reopening a connection, then the app will get unexpected errors when ACKing messages that were received on the original channel before reconnection.. > How is this adapter cooperative? I don't see anywhere where you are releasing control to other greenlets in your code. You are still just using the normal stdlib socket after setting it to block. \nIt looks like the implementation is relying on the application to call gevent's monkey.patch_all(). However, this doesn't seem to be communicated via user documentation anywhere in this pull request.. > It seems like this could be more elegantly fixed by overriding _create_tcp_connection_socket to return a gevent socket. Then no patching is required\nYeah, it seems that creating the appropriate gevent socket would be preferable to monkey-patching, which could break some auxiliary things, such as the test framework and other adapters being tested in the same run. And it would remove the need by the application to guess what needs to be patched.. @cainbit, @13steinj, @lukebakken: please hold off on the gevent effort. I am refactoring core/adapter/transport interfaces in pika via pull request #1002. #1002 facilitates a cleaner integration and more generic testing of adapter integrations.. @lukebakken, given the following:\n\nIncompatibility of the callback-based API implementation in this PR with the spirit of the gevent programming model.\nLack of complete test suite\nLack of support from contributor\n\nI am inclined to close this pull request.. If you periodically call BlockingConnection.process_data_events or BlockingConnection.sleep() it will attempt to send a heartbeat when it's time, and should raise ConnectionClosed when it fails to send on a closed TCP/IP stream.\nIn practice, you have to be prepared any time for a dropped connection (RabbitMQ issue, network issue, etc.). Checking a connection just before sending something is not guarantee that connection won't be lost while your're sending, so you need to anticipate re-connection and retries. For example, see _RETRY_ON_AMQP_ERROR decorator and _ChannelManager wrapper of AMQP client. Pika's BlockingConnection is not at all thread-safe (there is no panacea either). Also, cPython threading and Global Interpreter Lock (GIL) are dubious for performance and concurrency.\nI am guessing that you're processing the messages in a separate thread because it takes too long to process each one and this is causing the default AMQP heartbeat to time out and the connection to be dropped.\nA common technique that I recall from some other event frameworks is to enable callbacks to be requested from the event loop in a thread-safe manner with the callback itself being called in the context of the event loop's thread. This would be a more generally useful and flexible mechanism to facilitate the sort of thing you're trying to do, I think.\nJust for reference, I created a functional prototype of thread-safe connection sharing via BlockingConnection-style API in https://github.com/vitaly-krugl/pika/tree/threaded-connection/pika. This prototype runs the client stack on a separate thread, such that application clients can take as long as they wish to process messages and not worry about missed heartbeats.. pull request #956 will enable you to accomplish this via new method add_callback_threadsafe.. You will be able to process your message on another thread, and then delegate the call to channel.basic_ack to your pika thread that is executing BlockingChannel.consume via BlockingConnection.add_callback_threadsafe. See this example in docstring and working code in acceptance test. Yes, should be interrupted!\nOn Feb 17, 2018 1:36 AM, \"Bogdan Popa\" notifications@github.com wrote:\n\n@vitaly-krugl https://github.com/vitaly-krugl great! I've scanned the\nPR, but I don't have the pika implementation in my head atm. I assume this\nline\nhttps://github.com/pika/pika/pull/956/files#diff-40dd0dca78d99b5a2a1315abbcd71b70R543\nensures the polling mechanism's loop is interrupted and the callback is run\nimmediately in the connection's thread, correct? If so, that looks great!\nHope it gets merged. :D\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/892#issuecomment-366429443, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9Kp9XbMncWok9PsafK1F78jyFfrArks5tVp2PgaJpZM4QamaV\n.\n. To clarify, BlockingConnection implements reentrancy control to avoid runaway recursion involving callbacks. In particular, it would not get interrupted in the middle of sending a command or message to the broker until that request completes and control goes back to one of the event-loop drivers. On the other hand, it should get interrupted very quickly if simply waiting inside one of the event loop drivers BlockingConnection ...sleep() ...process_data_events() ...start_consuming() ...consume().\n\nOn the other hand, asynchronous adapters in Pika (such as SelectConnection, essentially anything other than BlockingConnection) shouldn't suffer from reentrancy control and should be more responsive under most, if not all, conditions.. @michaelklishin  - there are far too many other code paths in pika blocking connection that are not thread safe, so a one-off solution is lackluster. \nThe inactivity_timeout feature in BlockingChannel.consume can be used to easily solve the problem in application code without adding a hack in pika.\nP.S., If someone is interested in pursuing threading support, I created a functional prototype of thread-safe connection sharing via BlockingConnection-style API in https://github.com/vitaly-krugl/pika/tree/threaded-connection/pika. @sbidin, @lukebakken: I think I may have found a possible bug in user code that can explain this issue:\npython\n    def _on_message_nack(self, tag_to: int, multiple: bool) -> None:\n        tag_from = self.last_nack + 1 if multiple else tag_to\n        ....\nI think that the line tag_from = self.last_nack + 1 if multiple else tag_to should instead be\npython\ntag_from = max(self.last_nack, self.last_ack) + 1 if multiple else tag_to\nbecause the multi-nack would presumably apply to all messages since the most recent ack or nack from the broker.\nAnd ditto regarding user's _on_message_ack() implementation.. @lukebakken, since the queue is configured to Max out at 10, I suspect this\nbecomes reproducible when the consumer process is occasionally not keeping\nup with the producer.\n\n. @galindro, please post the entire traceback of the exception that you would see without calling rmq_connection.close().\n\nOne of the things that surprises me is that I don't see a log entry resulting from the failure associated with the exception.. @lukebakken, wasn't this fixed not long ago. I am struggling to find the relevant pull request.. Thanks @lukebakken - I missed that this issue is also sending invalid arg(s).. Closing since it was fixed by PR #991.. Thank you.. @lukebakken - what's the story with codecov/patch failure, though?. @lukebakken - Your call. I am the wrong person to ask, as I am a bit pedantic about such things :).. @lukebakken, this should be fixed now because we removed the nowait parameter, right?. Current releases of pika are not thread-safe, so you can't access a connection from multiple threads. However, pull request #956 will enable you to offload the time-consuming step to a thread and then delegate the ACK request for the message via new method add_callback_threadsafe, while your pika connection is keeping heartbeats alive via connection.process_data_events or channel.start_consuming, etc., and without the need for complicated locking.. @michaelklishin, if I am not mistaken, the application in question here is using a single pika connection from multiple threads. This is unsupported per pika implementation and FAQ and has been - not surprisingly - shown time and again to cause hangs and other issues by those who tried.\nFurther, if you look at popular and efficient python ioloops such as Tornado, Twisted, and now Python3 asyncio, none of them are thread-safe, but they do offer a single method to enable threads to interact safely with the event loop. Pull request #956 adds this capability to pika via the add_callback_threadsafe method. @stuartspotlight should be able to delegate the lengthy processing of the message to another thread and then have that thread safely ACK the message through the instance of BlockingConnection via the new method add_callback_threadsafe without relying on the unsafe \"pacemaker\", assuming the ioloop of the BlockingConnection instance is kept running via the likes of BlockingConnection.process_data_events(),  BlockingChannel.start_consuming(), BlockingChannel.consume(), or BlockingConnection.sleep().. The code included by the author of this issue accesses and modifies the connection's and ioloop's data structures and other resources from two separate threads, which is a classic race condition recipe.. @lukebakken, I think this one is safe to close. The approved mechanism add_callback_threadsafe is now available on all supported I/O loops in master.. > Also doc says that both recv and send may cause both SSLWantReadError or SSLWantWriteError, but pika now handles only one.\nThat possibility of either SSLWantReadError or SSLWantWriteError in recv and send may be triggered by TLS renegotiation. Being addressed in the refactoring pull request #1002. I could definitely use some help writing test code to validate that it works in the renegotiation scenario.. See comment https://github.com/pika/pika/issues/892#issuecomment-356783950. And which version of Pika are you using?. add_callback_threadsafe in pull request #956 likely offers a more efficient solution without polling.. @lukebakken, before I get into the review, I just wanted to note that twisted and tornado both allow args and kwargs to be passed along with callback; e.g., call_later(self, delay, callback, *args, **kwargs. And python's new asyncio allows just args: call_later(delay, callback, *args) in Python 3.6.4.\nAlso, pika's use of the arg name deadline in timer APIs is confusing (at least to me). Something like time.time() + 10 (i.e., seconds from epoch) is what I think of as deadline. The values that we pass to add_timeout are better named as delay or seconds_from_now (IMHO :).. @lukebakken, regarding args/kwargs - I was just brainstorming. Since Python has functools.partial, the other school of thought might say that it's easy enough for clients to use it and there is no need for pika to deal with callback args/kwargs at all.\nI don't think I ever missed the ability to pass args/kwargs in pika callbacks, so I would just as soon go without them.. @lukebakken - please refrain from pushing commits to this PR until I am done reviewing. Otherwise it's a little like trying to hit a moving target :).. docs/examples/tornado_consumer.rst:            self._channel.queue_declare(self.on_queue_declareok, queue_name). @lukebakken, here is some food for thought: in methods where the value of the callback arg is correlated with the value of nowait, such as in Channel.basic_cancel, the nowait arg is redundant. If callback is None, then you can surmise that nowait should be True and if callback is not None, then it implies that nowait is False.\nSo, let's get rid of the redundant nowait arg in all those cases. This will also reduce confusion and simplify/eliminate validation logic. It seems like a kludge to have both args.. @lukebakken - regarding indentation and statically-detected errors: please run pylint on all of the modified code. It's really good at finding some bugs and reporting formatting convention violations. pika already provides a pylintrc at the root level that is configured per our conventions.\nOn my mac, I install pylint via pip install pylint --user. Then run it like this from within the project's directory (if you run it inside our tree, it automatically discovers our pylintrc): pylint pika/channel.py.\nThat said, in a functional PR like this one, please fix formatting errors that are introduced by your changes, not the legacy formatting.. @lukebakken: we should really stick to PEP-0008. I am surprised that yapf snuck into pika in a non-PEP8-compatible way. This also conflicts with the pylintrc settings that had been used to validate majority of the code, including for maintaining consistent formatting. The vast majority of the pika code was PEP8 compatible. \nRegarding indentation, see https://www.python.org/dev/peps/pep-0008/#indentation.. Other than fixing pylint violations found in the new changes within this PR, let's schedule a separate follow-on PR to clean up pylint findings in the body of code, as presence of existing pylint warnings/etc. makes it difficult to notice newly-introduced violations in pylint output (same idea as resolving compiler warnings in existing code to make it possible to spot new ones easily).\nIn some cases, pylint has false positives, so we will need to suppress those in the affected code sections with pylint pragmas.. I don't like the empty string default. I think it's error-prone. I would\nget rid of the default value on queue.\nOn Feb 7, 2018 7:29 AM, \"Luke Bakken\" notifications@github.com wrote:\n@lukebakken commented on this pull request.\nIn pika/channel.py\nhttps://github.com/pika/pika/pull/941#discussion_r166653242:\n\n\ndef queue_declare(self, callback,\ndef queue_declare(self,\n                       queue='',\n\n\nbunny does not use the empty string as the default, which I like better.\n@vitaly-krugl https://github.com/vitaly-krugl what do you think?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/941#discussion_r166653242, or mute the\nthread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KjTNjE4vK4WhKC7qelAIC_VhYAalks5tScFYgaJpZM4R05h3\n.\n. @lukebakken, I am planning to give it a final once-over on Thursday. Thanks!. cc @lukebakken \nReady for review. I tried to use weakref and it resulted in test failures. I will be working through it shortly. @lukebakken - this is ready for review. Thanks!. @lukebakken, do you anticipate an opportunity to review this in the near future? My next PR #956 that implements the thread-safe callback request feature in BlockingConnection and SelectConnection builds on this PR. Thx!. Thank you for reviewing, @lukebakken!. Probably want to wait for #1002 to be completed before adding support for something like this.. For someone familiar with AMQP, auto-ack would be an unfamiliar term. For the uninitiated, auto-ack  might even imply that pika would automatically ack on their behalf or something like that.. If they are passing it as a keyword arg, then they would need to rename the arg as well. If I recall correctly, the AMQP specification recommends preserving protocol-specific arg names in client APIs. And keeping them same makes it easier for users to correlate between the AMQP protocol specification and client API.. If the team is bent on renaming it, go ahead, but ate you suggesting that\nwe should support both arg names?\nOn Feb 12, 2018 4:22 PM, \"Gavin M. Roy\" notifications@github.com wrote:\n\nI'm ok with a rename if we respect the kwarg for no_ack and clearly\ndocument the new behavior including respecting the \"official\" no_ack\nkwarg. @vitaly-krugl https://github.com/vitaly-krugl?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/951#issuecomment-365108912, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KuWQuCR9R6YWJDD8O0Mn7rRPgNzMks5tUNWlgaJpZM4R-5d3\n.\n. If other incompatible changes were already made in the API towards 1.0, is\nthere any point making an exception and supporting no-ack?\n\nOn Feb 12, 2018 4:51 PM, \"Gavin M. Roy\" notifications@github.com wrote:\n\nYeah, that {{no_ack}} continues to be supported via kwargs and is\ndocumented as such, but is not explicitly part of the function signature.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/951#issuecomment-365114259, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KjJJ1GM1b0AXwPUzVGyNiypvsixAks5tUNybgaJpZM4R-5d3\n.\n. Correct, no changes moving away from AMQP\n\nOn Feb 12, 2018 5:16 PM, \"Gavin M. Roy\" notifications@github.com wrote:\n\nI don't think there are changes moving away from the AMQP protocol as\nspecified elsewhere are there? I'd like to keep it for those of us who know\nwhat the API should be.\nThe RabbitMQ team wants to change it because supporting new users who\ndon't read docs is hard (which is totally understandable and has been\ndemotivating to me).\nI thought supporting the \"correct\" name and semantics via kwargs while\nchanging the explicitly defined parameters was a good compromise.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/951#issuecomment-365118879, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KhX6k5PK-lM46XDSJDijNFciTyNrks5tUOJ5gaJpZM4R-5d3\n.\n. The fact that arg order got changed already makes the API backward\nincompatible, so there is really no point having the no-ack support via\nkwargs\n\nOn Feb 25, 2018 10:17 PM, \"Wu Yongwei\" notifications@github.com wrote:\nRenaming no_ack to auto_ack makes sense. However, should we, maybe in a\nlimited transitional period, use **kwargs to map no_ack to auto_ack, and\n(optionally) give a warning?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/951#issuecomment-368401497, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KugsVtJS1TMqXucaT6Ws-jOEKum5ks5tYkxagaJpZM4R-5d3\n.\n. cc @lukebakken It would be great to get this API change in before pika 1.0 as it has implications on the callback arg name changed in your recent PR #941. > I'm wondering about how I can replace the existing behavior with regard to consumer tag with a server-generated one ...\n@lukebakken, I supposed the easiest for now is to continue to generate the consumer tag in pika as we do presently.\nIn theory, you could cache the consumer callbacks in a list and then match them with tags returned in consumer-ok, which should return in the given order. Another option could be to have an API method that the app could call to register the consumer callback for a consumer tag once it is known. Still, continuing to generate the consumer tag in pika seems like the safest and simplest approach with no apparent downside. . method_completed, message_delivered, message_acked,  and message_nacked all sound like booleans. Truthfully, I am also not crazy about naming something generically just callback, as it's not sufficiently self-documenting. Using completion as an example, we could probably adopt an existing convention from elsewhere - e.g., on_completion, completion_callback, completion_cb.  And for others - e.g., on_message, on_message_delivery, delivery_cb, on_message_ack, message_ack_cb, etc. And, of course, naming consistency is critical among all of them.\nRegarding splitting ack_nack_callback into two callbacks, I am not averse to it.. @lukebakken, I wanted to push a small PR to your fork/branch, but couldn't find the source fork and branch of your PR, and couldn't find how to do it by googling. Do you know how to discover it in github?\nIn the interest of time, I added a few lines to tests/unit/channel_tests.py disabling several pylint warnings that were responsible for a bunch of pylint output that we don't care about in test modules, which made it difficult to see issues that we actually care about. Here is the change:\n```\n$ git diff @~..@\ndiff --git a/tests/unit/channel_tests.py b/tests/unit/channel_tests.py\nindex 2c6ff5c..85446a5 100644\n--- a/tests/unit/channel_tests.py\n+++ b/tests/unit/channel_tests.py\n@@ -12,6 +12,10 @@ import mock\nfrom pika import channel, connection, exceptions, frame, spec\n+# Disable protected-access, missing-docstring, and invalid-name,\n+# too-many-public-methods, too-many-lines\n+# pylint: disable=W0212,C0111,C0103,R0904,C0302\n+\n```. @lukebakken, you don't like Git Workflow?. I got into the git workflow habit after accidentally pushing something directly to master :). One of the handy things about forks is that other people can push PRs to your fork/branch, and you can accept/reject them, as I wanted to do above.. Okay, please ping me when you're ready for the final review.. @michaelklishin @lukebakken  Please don't merge - I am going to provide feedback shortly. Okay, I would still prefer not supporting the old arg since we're moving to 1.0 and introduced other incompatibilities that will break existing usage of those same API methods. But feel free to override my recommendation so that we can move forward. Let's at least:\n\nCheck to make sure that nothing unexpected is in kwargs -\n see my code example.\nUse a shared function to process the kwargs, if practical.\nEmit the deprecation and incompatibility warnings that @michaelklishin and I discussed.\nImplement missing tests\n. > Oh the thousands of code examples on the Web that will be copied only to immediately fail\u2026\n\nSchool of hard knocks of life :). Okay to merge after addressing feedback about redundant test and AMQP no-ack correlation in auto_ack arg documentation . Anyone have any idea why the Coverage travis-ci job is always failing in my PR?\ncc @michaelklishin, @lukebakken . @shinji-s, thank you for looking into this. Your assessment makes a ton of sense. If that's the case, then the selected mechanism is inappropriate for the project's needs, since most 3rd-party PRs would naturally come from forks and reviewers as well as contributors need practical means to assess impact of those PRs on code coverage.. @lukebakken - this pull request contains a large number of commits - 39 presently; would it be better if I squashed them into a single commit?. Thanks @lukebakken !. cc @lukebakken . @adah1972, please link to the pika documentation that permits ack'ing a message from another thread. It's not thread-safe according to pika FAQ.\nHowever, pull request #956 will enable you to accomplish this task via new method add_callback_threadsafe.. It's not thread-safe to ACK directly from another thread in pika.\nI believe @lukebakken is targeting #956 for the 1.0.0 milestone. I don't know if any stable releases are planned before then.. Beta will be announced on the rabbitmq-users google group and available from pypi. However, once desired changes - like #956 - are merged into master, you can always try pika from the master branch (at your own peril :).. As far as how to do this in the currently-released pika, people have resorted to polling via timers or inactivity timeouts in the pika connection. E.g., the other thread finishes processing the message and appends the channel/delivery tag (or callback encompassing whatever is needed) to a list. The app logic in the timer handler or inactivity timeout processes the queued up requests. It's not optimal, as there is a tradeoff between latency and CPU utilization with polling.. Yes, that's the concept prior to pull request #956. If #956 is accepted, you will be able to use add_callback_threadsafe. It will be more responsive than polling.. @adah1972, thank you for trying add_callback_threadsafe and your feedback concerning it and arg changes for the 1.0.0 release target.\nThe maintainers are making a revision sweep of the APIs that have accumulated many inconsistencies and redundancies over the pre-1.0.0 years. I'm cc'ing Luke in case he wishes to chime in on the subject.\ncc @lukebakken . @adah1972, add_callback_threadsafe is the only thread-safe method in pika, so delegating basic_cancel via add_callback_threadsafe from another thread should be correct, but calling basic_cancel or anything else directly from another thread is asking for trouble (i.e., race conditions)\n\nIs basic_cancel thread-safe or not?. Regarding pika versioning, locking down a package version is not unusual. Virtualenv, docket, etc. are commonly used to isolate one app and its dependencies from another.\n\nPika release notes should contain sufficient information to help developers with the upgrade. And test suites associated with the applications should be instrumental in helping developers fix remaining incompatibilities. \nGoing to 1.0.0 is the last opportunity in a very long time to clean up the API. . Okay, just keep in mind that basic_cancel() is not thread-safe and calling it from another thread may lead to unpleasant outcomes.. Hi, it's only in master. We're heading towards 1.0.0 with no announced date\nyet.\nOn Sat, Mar 24, 2018, 7:51 PM ehp006 notifications@github.com wrote:\n\n@vitaly-krugl https://github.com/vitaly-krugl Are the changes for #956\nhttps://github.com/pika/pika/pull/956 only on master or are they in a\npika release yet ? If not, is there a target date to have #956\nhttps://github.com/pika/pika/pull/956 in a release ?\n--thanks\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/963#issuecomment-375941306, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KkDPX-nFgykAbAxl4RK0wD_77Vrkks5thwYfgaJpZM4SJOOo\n.\n. Incidentally, if you're using one of the non-blocking adapters, such as\ntornodo or asyncio connection, the native event loops tornado.IOLoop etc.\nAlready support that and are the recommended way to add a callback to the\nloop. Only blocking connection users should be using\nblockingconnection.add-callback-threadsafe directly.\n\nOn Fri, Apr 20, 2018, 11:10 AM Luke Bakken notifications@github.com wrote:\n\n@stugots https://github.com/stugots - yes it applies to all\noperations.\nPlease give Pika 0.12.0b2 a try, which contains #956\nhttps://github.com/pika/pika/pull/956 and allows methods to be executed\non the correct thread via add_callback_threadsafe.\nPika 0.12.0b2 can be installed with this command:\npip install pika --pre\nThank you!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/963#issuecomment-383178340, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KpeSWS5aUhaxqju7SxLquvqWGmx-ks5tqiR9gaJpZM4SJOOo\n.\n. Closing because pika doesn't support acknowledging messages directly from another thread.. Per 0.9.1 AMQP under tune-ok: \nshort heartbeat\nThe delay, in seconds, of the connection heartbeat that the client wants. Zero means the client does not want a heartbeat.\n\nI shy away from having pika make an opinionated decision about this. AMQP spec facilitates this, and there is nothing wrong with pika facilitating it as well. It's easy and let the user decide. Pika defaults to accepting the server's proposal via DEFAULT_HEARTBEAT_TIMEOUT = None. But, if the user has a need to disable timeout, they should be able to do just that.\nAlso, see the pika API documentation for setting heartbeats in connection parameters: https://github.com/pika/pika/blob/f29d904b0030079488489af9b8c35401b24fc87c/pika/connection.py#L368-L372. https://www.rabbitmq.com/heartbeats.html: \"Heartbeats can be disabled by setting the timeout interval to 0. This is not a recommended practice.\".\nI think we should support disabling it with the warning above in pika's heartbeat arg documentation. It used to work like this until about 5 months ago.. The present default is to accept server setting. I meant a warning in the documentation - such as \"Heartbeats can be disabled by setting the timeout interval to 0. This is not a recommended practice.\", not a warning emitted by code. See my original comment:\n\nI think we should support disabling it with the warning above in pika's heartbeat arg *documentation*. It used to work like this until about 5 months ago.. This is how it used to work, which I argue is preferential from user perspective based on extensive usage of several years: 781c548b324eee7bbb50e53d3f86420ad6a3301a. \n\nAs a user, I needed to set a heartbeat value that made sense for my application, which was higher than the value proposed in server's tune method. But I still wanted a heartbeat to keep the connection alive and detect lost connection, just not as frequently as the server proposed. Since I don't always have control over the server's proposed value (not atypical), the client library needs to enable the app to get what it needs, especially when this is consistent with the AMQP 0.9.1 specification. This became especially critical after RabbitMQ reduced its default proposed heartbeat from 300 to 60 or so.\nAs it stands now, there is no way for an application to specify a heartbeat timeout greater than that proposed by the server.\nA more flexible approach would be to allow the app to pass a callable that accepts the heartbeat value proposed by server and returns the heartbeat value that the app wants to use. The result should be honored as long as it's non-negative, otherwise raising ValueError. This callable would be called during negotiation once the server's proposed value is known. One way to instrument this in python is to allow either an integer or a callable to be passed for the heartbeat arg to Parameters/ConnectionParameters. URLParameters is a different story, perhaps it's constructor could be extended to accept an additional callable arg (e.g., heartbeat_tune_callback) for negotiating heartbeat that would be mutually-exclusive with the heartbeat value provided in the URL.\nPer AMQP 0.9.1 tune documentation:\n\ntune(short channel-max, long frame-max, short heartbeat)\nPropose connection tuning parameters.\nThis method proposes a set of connection configuration values to the client. The client can accept and/or adjust these.\n\nThe application is the ultimate client, and the AMQP library should not interfere with the app's ability to use the AMQP protocol.. Also, this docstring needs to be updated: https://github.com/pika/pika/blob/3d3b95d31b67dfeaf5ef43650c162e25169336e6/pika/connection.py#L368-L374\nAnd this one: https://github.com/pika/pika/blob/3d3b95d31b67dfeaf5ef43650c162e25169336e6/pika/connection.py#L615-L617\nAlso this one: https://github.com/pika/pika/blob/3d3b95d31b67dfeaf5ef43650c162e25169336e6/pika/connection.py#L743-L745. The callback manager's callback return value is not saved. Don't use\ncallback mgr for this. Just save the callback in the connection parameter\ninstead of the heartbeat value. During negotiation, if the heartbeat value\nis callable, call it and save the return value in it's place, after proper\nvalidation and continue with negotiation using that value.\nOn Feb 22, 2018 1:29 AM, \"Darren Demicoli\" notifications@github.com wrote:\n\nHi @lukebakken https://github.com/lukebakken. Yes i am implementing the\nsuggested changes. Quick question, I'd like to reuse the CallbackManager\ninstance in Connection to implement the suggested callback for heartbeat\ntuning. However, I am missing if there is a way to get the return value of\nthe user's callback after invoking self.callbacks.process(...,\nON_CONNECTION_TUNE,...). In CallbackManager I do not see the return value\nof the callbacks being saved. Is this the correct way or should this be\ndone differently?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/966#issuecomment-367619668, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9Kgz3zPUML1ZaGaiXpvR7UdkdJmz_ks5tXTNugaJpZM4SMrWe\n.\n. What's the benefit of using call back manager for this versus saving the\nvalue in parameters and calling it yourself when you need it if the value\nis a callable?\n\nOn Feb 22, 2018 6:39 AM, \"Vitaly Krug\" vitaly.krug@gmail.com wrote:\n\nThe callback manager's callback return value is not saved. Don't use\ncallback mgr for this. Just save the callback in the connection parameter\ninstead of the heartbeat value. During negotiation, if the heartbeat value\nis callable, call it and save the return value in it's place, after proper\nvalidation and continue with negotiation using that value.\nOn Feb 22, 2018 1:29 AM, \"Darren Demicoli\" notifications@github.com\nwrote:\n\nHi @lukebakken https://github.com/lukebakken. Yes i am implementing\nthe suggested changes. Quick question, I'd like to reuse the\nCallbackManager instance in Connection to implement the suggested callback\nfor heartbeat tuning. However, I am missing if there is a way to get the\nreturn value of the user's callback after invoking self.callbacks.process(...,\nON_CONNECTION_TUNE,...). In CallbackManager I do not see the return\nvalue of the callbacks being saved. Is this the correct way or should this\nbe done differently?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/966#issuecomment-367619668, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9Kgz3zPUML1ZaGaiXpvR7UdkdJmz_ks5tXTNugaJpZM4SMrWe\n.\n\n\n. @darcoli: I agree with your philosophy regarding:\nYes not allowing the callback return None was actually on purpose. The rationale behind it is that if the client passes a callback heartbeat, then an intention to control the heartbeat is shown and the client then needs to specify it. Of course, if the client wants to accept the server's proposal, it should just return with the same value that was passed as an argument to the callback. But, of course, I can change it to allow None to be returned if you think it makes more sense to do so ;). cc @lukebakken . cc @lukebakken @gmr . It's worth looking into.\n\nOn Feb 21, 2018 10:49 AM, \"Luke Bakken\" notifications@github.com wrote:\n\nDo you think it would be possible to automate the selection of the correct\nconnection class based on what library is available and / or what IOLoop\nmay already be running? The Cassandra python driver does something like\nthat\nhttps://github.com/datastax/python-driver/blob/master/cassandra/cluster.py#L89-L100\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/969#issuecomment-367429892, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KqykSM7MxZiRRh1cUNvTgaoMUmAPks5tXGUTgaJpZM4SN9-g\n.\n. I am hoping to get the apps away from accessing the connection's internal ioloop, which is often an adapter on top of the native I/O loop. I would not want them to depend on any connection.ioloop API (that's should really be just for the benefit of adapter implementations), calling it's start/stop method, etc. (it shouldn't be even providing a start() and stop() methods). Instead, apps should be interacting directly with the native I/O loop - calling the native loop's start(), run_forever(), stop(), add_timeout(), etc.. I am closing this issue and will open a new one with another proposal.. @lukebakken, I am refactoring the transports in a newer pull request, making the changes in this PR obsolete, so closing... Sorry if you've already spent time reviewing it.. @lukebakken - this is ready for code review. > because distutils installs only packages listed under packages keyword\n\nSo, one option was to list compat/__init__.py in setup.py instead of turning it into compat module? Just curious.. Thanks @anton-ryzhov!\n@lukebakken - I like the find_packages() recommendation for auto-discovery. What do you think?. Ready for review. @lukebakken,  I am working on fixing pika's SSL code paths and am blocked by this issue.  I created this issue from my comment in #744.. @lukebakken, as a stepping stone, would it make sense to enable anonymous SSL/TLS connections to be made to RabbitMQ (assuming python's ssl supports it)? Such connections should not require certificates. I am hoping that this might be a lot easier/quicker to orchestrate in the build systems while still allowing a fair amount of testing of pika's SSL support to be implemented.. Yeah, that test \"Setup 5 timeout handlers and observe them get invoked one by one\" has been flaky. I am guessing that the test machines are very slow which sometimes causes a conflict with the selected granularity of timers. Since it's a unit test, it should be mocking time.time() for control and speed of execution instead of slowing down the testing by making the test wait for the actual timeouts in real time.. Hi @lukebakken, I noticed that AppVeyor build stopped running in my PR. Is it related to the work that you're doing on PR #984?. Replacing by pull request #1002. The test \"Setup 5 timeout handlers and observe them get invoked one by one\" has been flaky. I am guessing that the test machines are very slow which sometimes causes a conflict with the selected granularity of timers. Since it's a unit test, it should be mocking time.time() for control and speed of execution instead of slowing down the testing by making the test wait for the actual timeouts in real time.. @lukebakken, prior to this PR, the builds normally stopped as soon as the first test failure occurred. Now, if anything fails in the plaintext part of nosetests run, the PIKA_TEST_TLS=true will still run, so it takes much longer to discover that failure (i.e., to see red). See this PR build for example: https://travis-ci.org/pika/pika/jobs/349485923. > good find, this commit will address it: 3f0e739\nThank you. A debug pika log from beginning leading to the hang would be very nice.\nI am surprised that pika.BlockingConnection() constructor returns at all without encountering an error and raising an exception. How does the AMQP handshake even complete if the server is not AMQP?. @lukebakken - This is not terribly urgent, but I would like to get the requested info to understand it better.. 1. To reiterate, it's strange that AMQP handshake would succeed when connected to Redis. @lukebakken  - any idea? Are you sure that the BlockingConnection constructor ever returns?\n2. If it's actually stuck in the constructor (i.e., never gets to the connection.channel() call), then (like #647), the situation would be helped by having a built-in AMQP handshake timeout that then calls Connection._on_terminate().\n3. If it magically completes the AMQP handshake, then the situation would be helped by fixing the heartbeat timeout logic as I described in https://github.com/pika/pika/issues/708#issuecomment-370105566.. @lukebakken, thank you for confirming that it gets stuck in the handshake. So, the channel = connection.channel() call is unnecessary for reproducing this issue.\nSo, having a handshake timeout in Connection that calls Connection._on_terminate(...) should resolve both #647 as well as this issue.\n. @michaelklishin, do you recommend making the handshake timeout configurable by user? And what do your recommend as the (default) value?. Thank you!. I am refactoring the transport in PR #1002, which includes an overall TCP/[SSL]/AMQP connection timeout, so should also address this hang.. @lukebakken, this is one of the issues that is being addressed by my refactoring PR #1002. Should I re-assign this issue to myself?. cc @lukebakken . cc @lukebakken . Ready for review.\ncc @lukebakken . cc @lukebakken, @michaelklishin \nOn a related note, what's the typical use case for pika.URLParameters? Is the URL-based configuration style widely used? The reason I ask is if you need to throw in things like ssl_options, then the nesting makes the resulting query args pretty messy for someone to compose by hand, to read, and maintain. For example, this set or parameters represented as python dict:\n{'blocked_connection_timeout': 10.5,\n 'channel_max': 3,\n 'connection_attempts': 2,\n 'frame_max': 40000,\n 'heartbeat': 7,\n 'locale': 'en_UK',\n 'retry_delay': 3,\n 'socket_timeout': 100.5,\n 'ssl_options': {'ca_certs': None,\n  'cert_reqs': 0,\n  'certfile': None,\n  'ciphers': None,\n  'keyfile': None,\n  'npn_protocols': None,\n  'server_hostname': 'blah.blah.com',\n  'ssl_version': 2},\n 'tcp_options': {'TCP_KEEPIDLE': 60, 'TCP_USER_TIMEOUT': 1000}}\n... looks like this when represented as query args; note the ton of escaping that deals with the nested dicts towards the end:\nIn [50]: urlencode(qa)\nOut[50]: 'blocked_connection_timeout=10.5&channel_max=3&connection_attempts=2&frame_max=40000&heartbeat=7&locale=en_UK&retry_delay=3&socket_timeout=100.5&ssl_options=%7B%27keyfile%27%3A+None%2C+%27certfile%27%3A+None%2C+%27ssl_version%27%3A+2%2C+%27ca_certs%27%3A+None%2C+%27cert_reqs%27%3A+0%2C+%27npn_protocols%27%3A+None%2C+%27ciphers%27%3A+None%2C+%27server_hostname%27%3A+%27blah.blah.com%27%7D&tcp_options=%7B%27TCP_USER_TIMEOUT%27%3A+1000%2C+%27TCP_KEEPIDLE%27%3A+60%7D'\nIs this really convenient/readable for users? Wouldn't something like JSON or YAML be less error-prone, more readable, and easier for users to maintain?. > Shovel, Federation and some clients use URI parameters ...\nThank you for the link. The description in a subset of connection options uses a flat structure, which suits URI notation well. Pika uses nested legacy structures (e.g., dicts within dict), which looks messier in a URI.\n. > What do you think about removing SSLOptions entirely since it is being used only for the server_hostname parameter?\n@lukebakken, I prefer grouping information that belongs together in an object. This way\nwe have a natural place (i.e., a namespace) to add other bits in the\nfuture, it facilitates clean consistency checking, and it also eliminates\nsome confusing situations, such as the user specifying the server hostname\nwithout the SSL context, making it easier to understand and less\nerror-prone.\nAs for the URL case, flattening the query args might make it simpler for\nsomeone to use it. However, I haven't seen complaints yet, and this stuff\nhas been this way for a long time. So, either no one really cares about\nusing URL Parameters with pika or they are fine with the status quo. :)\nOn Mar 6, 2018 10:55 AM, \"Luke Bakken\" notifications@github.com wrote:\n@lukebakken commented on this pull request.\nIn pika/connection.py\nhttps://github.com/pika/pika/pull/987#discussion_r172621853:\n\n\nself.keyfile = keyfile\nself.key_password = key_password\nself.certfile = certfile\nself.server_side = server_side\nself.verify_mode = verify_mode\nself.ssl_version = ssl_version\nself.cafile = cafile\nself.capath = capath\nself.cadata = cadata\nself.do_handshake_on_connect = do_handshake_on_connect\nself.suppress_ragged_eofs = suppress_ragged_eofs\nself.ciphers = ciphers\n\nProtect against accidental assignment of an invalid attribute\n\nslots = ('context', 'server_hostname')\n+\ndef init(self, context, server_hostname=None):\n\n\nWe're keeping SSLOptions around just for server_hostname. We could get rid\nof this class and add ssl_server_hostname to ConnectionParameters. Just an\nidea.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/987#pullrequestreview-101670756, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KgD_sTjKxpP9bxyHI8bnt5FiHw8nks5tbtslgaJpZM4SbSkT\n.\n. Thank you @lukebakken!. @lukebakken, are you planning to make another sweep and rename all callbacks in pika, or is this something else?. Making the callback names more descriptive would help with making the API more self-documenting. But things are not bad now, so I'm not certain it's worth the extra effort. I'm okay to keep as they are.. @lukebakken, regarding documentation comment in https://github.com/pika/pika/issues/988#issuecomment-370978795, I was hoping to get the apps away from accessing the connection's internal ioloop, which is often an adapter on top of the native I/O loop. I would not want them to depend on any connection.ioloop API (that's should really be just for the benefit of adapter implementations), calling it's start method, etc. (it shouldn't be even providing a start() method). Instead, apps should be interacting with the native I/O loop directly - calling the native loop's start(), run_forever(), etc.\nI started going on this path in my WIP PR https://github.com/pika/pika/pull/982.. @lukebakken, would you mind adding a comment describing the root cause? This will help me with the code review.. What's the full traceback and exception from the following?\nc = pika.BlockingConnection()\nch = c.channel()\nch.queue_declare(queue=[1, 2, 3])\n. @lukebakken, regarding\n\n\"This applies to BlockingConnection\"\n\nDoes this issue not exist in an asynchronous connection?. @lukebakken: Okay, so this now makes sense - the finally: block in the original BlockingConnection example and pika.BlockingConnection() as context manager in this new example will deadlock (trying to auto-close the channel) on the erroneously blocked channel that resulted from the assertion error and prevent the exception from bubbling up.. And although the asynchronous connection example doesn't hang since it simply doesn't do anything that interferes with the processing of that exception, the channel state would still be messed up - in particular, I expectself._blocking to remain set and the redundant callbacks to continue to be registered.. @lukebakken, I haven't forgotten - will catch up in the next couple of days.. Don't try it out just yet, I missed something there (and the fix in the PR did, too, I think). @lukebakken:\n\n\nThe if acceptable_replies: block sets self._blocking and also registers a number of callbacks. If self._send_method(method) fails while encoding frames (as in this case), we really don't want any of those registered callbacks to remain registered either.\nSince a successful self._send_method(method) call will ultimately just enqueue some data on the output write buffer, it should be possible to move the if acceptable_replies: block after self._send_method(method). This way, if any of the marshaling in self._send_method(method) fails, then nothing needs to be undone.\n\n\nFurthermore, to ensure that an incomplete message doesn't get placed in the output buffer (due to marshaling failure of one of its subframes), Connection._send_message() needs to be modified to pre-marshal all of its parts and then append them to the outbound buffer deque only after all marshaling is done, updating the stats and finally calling self._flush_outbound() and self._detect_backpressure() like Connection._send_frame(). To this end,  Connection._send_message() and Connection._send_frame() should share a method (e.g.,  Connection._output_marshaled_frame() that updates self.bytes_sent and self.frames_sent and appends the marshaled frame data to self.outbound_buffer .\n\n\nAnd code comments should provide some insight into each of these, so that a future contribution won't refactor or reorder things in a way that's incompatible with the solution.. @lukebakken, I renamed this PR \"Request marshaling error should not corrupt a channel\", which reflects issues #990 and #912 more accurately. . On broker's Channel.Close, the draining is necessary because ANQP says to\nignore all incoming  requests after  channel is closed except\nChannel.Close. So, draining in that case helps break the gridlock.\nHowever, in the case the client is closing the channel with some blocking\nrequests still pending normally, we have a perfectly healthy channel and\nnothing special is needed. The normal course of events will see it through.\nOn Tue, Apr 10, 2018, 5:00 AM Luke Bakken notifications@github.com wrote:\n\n@lukebakken commented on this pull request.\nIn pika/channel.py\nhttps://github.com/pika/pika/pull/991#discussion_r180392933:\n\n@@ -1327,9 +1327,10 @@ def _on_synchronous_complete(self, _method_frame_unused):\n         while self._blocked and self._blocking is None:\n             self._rpc(*self._blocked.popleft())\n\n\ndef _drain_blocked_methods_on_remote_close(self):\n\nIf we think we need an \"emergency channel-close\" method that purges\nqueued-up requests (I don't think we do\nDraining blocked methods on a broker-initiated close was introduced in\n957 https://github.com/pika/pika/pull/957 - please check that PR out\nagain. I still think it's necessary.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/991#discussion_r180392933, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KigMj1hv6PIavaR70oqFZk9LqB0iks5tnJ7igaJpZM4Sg71J\n.\n. @lukebakken, is this PR ready for re-review?. I think I might not have ended that review ??\n\nOn Mon, Apr 16, 2018, 6:22 AM Luke Bakken notifications@github.com wrote:\n\n@vitaly-krugl https://github.com/vitaly-krugl if I re-select your name\nin the \"Reviewers\" dropdown, the status icon changes back to an orange disk\n... do you not get a new email saying I re-requested a review? I assumed\nthat you did. If you don't get an email, I can @-mention you in a comment.\nThanks for the re-re-reviews \ud83d\ude04\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/991#issuecomment-381597285, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KmC9Xn-wJn2vurcgpPYZe9kJuVEtks5tpJsrgaJpZM4Sg71J\n.\n. > I tried to reproduce what was reported in #993, but couldn't\n\n@daryasary, could you please provide a simple example that reproduces the issue that you tried to fix with #993?. @lukebakken, could you please take a look at the software installation failures that caused a number of the build jobs to fail? This may be related to the changes in PR #984. Perhaps software installations could use some hardening or a faster source repository.. > It looks like Erlang Solutions' package repository is slow\nIs there a cache site that might be faster, more reliable?. After the initial socket connection establishment in pika, the connection's socket is set to be non-blocking. Non-blocking sockets don't pay attention to socket timeout. So, I am guessing that this issue concerns only the TCP connection-establishment phase, right?. @lukebakken - I implemented a new test that detected unwanted/dangerous behavior in BlockingConnection concerning leaking exceptions back into the I/O loop/SelectConnection.\nAs the result, this PR's tests are now failing due to ChannelClosed exception raised by BlockingChannel._on_channel_closed().\nMore info: BlockingConnection's underlying asynchronous connection adapter (SelectConnection) uses callbacks to communicate with its user (BlockingConnection in this case). If BlockingConnection leaks exceptions back into the I/O loop or the asynchronous connection adapter, it interrupts their normal workflow and introduces a high likelihood of state inconsistency within those components.\nI noticed this issue while working on another pull request that implements a transport abstraction, including non-blocking connection establishment.. Ready for review.\ncc @lukebakken . Had a transient timeout running \"Create and delete and exchange (AsyncioConnection)\" test over SSL and python 3.7-dev on travis-ci: https://travis-ci.org/pika/pika/jobs/354112927. Thanks @lukebakken!. cc @lukebakken . ### STATUS UPDATE:\n\nPasses almost all existing tests. Waiting for resolution of PR #996 which should facilitate the passing of the 5 remaining blocking connection acceptance tests that are presently failing.\nThree unit tests have been skipped, need to be fixed:\ntests/unit/blocking_connection_tests.py:    @unittest.skip('TODO FIX ME BEFORE MERGING IN MASTER (HANGS)')\ntests/unit/blocking_connection_tests.py:    @unittest.skip('TODO FIX ME BEFORE MERGING IN MASTER')\ntests/unit/connection_tests.py:    @unittest.skip('TODO FIX ME BEFORE MERGING IN MASTER')\n\n\nNeed to complete acceptance test for async services adapters.\nNeed to implement specific tests for\n_AsyncPlaintextTransport\n_AsyncSSLTransport\n_AsyncSocketConnector\n_AsyncStreamConnector\nbase_connection._TransportManager\n\n\nNot sure what to do with TwistedProtocolConnection, since one of its main reasons for existence is to provide non-blocking connection establishment in Twisted, which this pull request accomplishes for all adapters universally. That and the fact that it changes the connection and channel interfaces (returns deferreds) without providing anything close to sufficient test coverage makes me want to remove it from pika for v1.0.0, leaving TwistedConnection as the only twisted connection adapter, which also sports an overridden API that makes use of deferreds.\nNeed to add a \"channel ready\" callback to Connection.channel().\n\ncc @lukebakken . \nFOOD FOR THOUGHT:\n\nBoth twisted and asyncio (which borrows from twisted) frameworks have the concept of Protocol and Transport which I like. This PR makes this split internally, but I am beginning to think that it might be good to formalizing as part of the API such that any pika asynchronous connection is a Protocol that can be mated with an externally-created Transport.\nThis facilitates less error-prone AMQP connection setup - presently, we reuse the same Connection instance when socket or SSL connection attempts fail. This approach relies on resetting the connection's properties to the pristine state, which is error-prone and and doesn't have test coverage. Ideally, a user requests a Connection to be created and gets a Future (or another native construct) which notifies the user when the Connection has been set up and provides it to the user.\nMakes it more natural for users to implement their own connection workflows, if desired.\nMakes it cleaner to integrate with the native Transport/Protocol relationship of the given asynchronous framework.\n\n\n\nThe current callback-based ConnectionAPI is clumsy when it comes to integrating with Future-based frameworks (e.g., asyncio, twisted). The user can certainly wrap the callbacks in native Futures and the like, but it's not clean. Wouldn't it be nifty if pika core integrated naturally into the native I/O framework (returning Futures and the like) without having to create shims that override the many of the Connection and Channel methods? And if we could test them generically. E.g., presently, TwistedProtocolConnection attempts to alter the Connection/Channel interface to integrate natively with twisted using Defferreds, but there is no test coverage for it whatsoever, while the rest of the callback-based adapters are getting tested generically via a single shared async test suite.\n. ### UPDATE:\n\n\nAll existing unit/acceptance tests are now passing once again.\n\nThe three skipped tests cited in the prior update have been addressed\nPR #996 has been accepted in master and merged into this PR\nNeed to complete acceptance test for async services adapters.\nNeed to implement specific tests for\n_AsyncPlaintextTransport\n_AsyncSSLTransport\n_AsyncSocketConnector\n_AsyncStreamConnector\nbase_connection._TransportManager\n\n\nNot sure what to do with TwistedProtocolConnection, since one of its main reasons for existence is to provide non-blocking connection establishment in Twisted, which this pull request accomplishes for all adapters universally. That and the fact that it changes the connection and channel interfaces (returns deferreds) without providing anything close to sufficient test coverage makes me want to remove it from pika for v1.0.0, leaving TwistedConnection as the only twisted connection adapter, which also sports an overridden API that makes use of deferreds.\nNeed to add a \"channel ready\" callback to Connection.channel().\n. @lukebakken, in examining the build/test output log, I noticed the following message about erlang crash at the very end of the log. See in https://travis-ci.org/pika/pika/jobs/356647867, for example:\n\nDone. Your build exited with 0.\nerl_child_setup closed\nCrash dump is being written to: /home/travis/build/pika/pika/rabbitmq_server-3.7.4/var/log/rabbitmq/erl_crash.dump.... @lukebakken, I committed more tests and a bug fix or two and updated the commit comments with summaries of new features and backward-incompatible improvements.. @lukebakken: I am not planning any more commits at this time, so ready for review.. I know what you mean :)\nOn Thu, Apr 19, 2018, 9:40 PM Luke Bakken notifications@github.com wrote:\n\n@vitaly-krugl https://github.com/vitaly-krugl I'll try to find a nice\nchunk of time to look at this tomorrow. Thanks!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/1002#issuecomment-382973668, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9Kqti3ouO3yIx6w3sxwCL5_Nv9GJeks5tqWa6gaJpZM4Sy9uO\n.\n. @lukebakken, I just made a couple of API cleanup changes and rebased TwistedProtocolConnection on pika.connection.Connection instead of BaseConnection, since that makes a lot of sense.\n\nOf course, since TwistedProtocolConnection's authors included no tests, we have no idea whether it ever worked and whether it still works.\nReview away... :)\nThe changes are in separate commits in case you already started with the review.. @lukebakken, I fixed resource leaks in pika tests. Also, updated README.rst.. @hairyhum \n\nis there any usage examples in examples directory and docs in docs directory which should be updated after this change?\n\nYes, good point. I will look through the examples and update the ones that are affected.. @lukebakken, I will follow up on TODOs and @hairyhum's feedback from home.. @lukebakken @hairyhum: I updated the usage examples via commit 8f90e34c0609b4255a117b0b960fbc4d86974fb6.. @lukebakken, regarding \n\nShould the TODO items be taken care of?\n\nThere is nothing pressing about the new TODOs. Just food for thought at this time. If you're happy with the state of things, please go ahead and approve this pull request and we'll get it merged. Thanks!. @lukebakken, thanks for reviewing - I know it was a job and one half :). cc @lukebakken . @masell, the AMQP 0.9.1 specification defines the following types, but there is no X/x. Could you please attach a URL to the applicable AMQP specification that defines x?\nfield-value = 't' boolean\n / 'b' short-short-int\n / 'B' short-short-uint\n / 'U' short-int\n / 'u' short-uint\n / 'I' long-int\n / 'i' long-uint\n / 'L' long-long-int\n / 'l' long-long-uint\n / 'f' float\n / 'd' double\n / 'D' decimal-value\n / 's' short-string\nAdvanced Message Queuing Protocol Specification v0-9-1 Page 31 of 39\n Copyright (c) 2006-2008. All rights reserved. See Notice and License. Technical Specifications\n / 'S' long-string\n / 'A' field-array\n / 'T' timestamp\n / 'F' field-table\n / 'V' ; no field. @lukebakken, so what's the semantic difference between byte[] as represented by 'x' and raw byte[] as represented by 'S' ?\nIs it lack of implicit conversion to Unicode?. @lukebakken, would you mind completing this review? Thx!. @lukebakken  - done!. @lukebakken #956 (add_callback_threadsafe() support) is likely to be a valuable change for the 0.12, especially since there is not specific timeframe for v1.0.0 release. The PR itself doesn't seem to be affected by too much v1.0.0-related API changes, so it looks manageable.. Oh man, that's a pain :(. By the way, have you come across these?\n https://askubuntu.com/questions/329450/e-some-index-files-failed-to-download-they-have-been-ignored-or-old-ones-used \n https://ubuntuforums.org/showthread.php?t=1654068\nPossibly corrupted lists file in Travis-CI's ubuntu?. @lukebakken, would this help: https://www.rabbitmq.com/install-debian.html#bintray-apt-repo ?. @CharlesCCC: thank you for your contribution, but the example code in head of pika master branch matches Channel.basic_consume() args in head of pika master branch. The pika APIs in master are getting normalized for the v1.0.0 release.\nYou need to use examples from the release label that is compatible with the version of pika that you're using.. @lukebakken - I missed this doc update in my mega PR that you just merged.. @lukebakken, it's ready for review once again: I resolved merge conflicts against latest master, made BlockingChannel context manager compatible with the channel exception changes, and added corresponding BlockingChannel acceptance tests.. Thanks @michaelklishin . @hairyhum, good stuff!  If it's not too much to ask, a couple of additional examples would be useful in separate pull request to demonstrate the new support for multiple connection parameters per https://github.com/pika/pika#multiple-connection-parameters:\n\nUsing BlockingConnection - see example from acceptance test\nUsing a non-blocking connection via the create_connection() static method - see example from acceptance tet. Just one example file that combines recovery and multiple configs sounds\ngood. Thanks!\n\nOn Wed, May 2, 2018, 4:00 AM Daniil Fedotov notifications@github.com\nwrote:\n\n@hairyhum commented on this pull request.\nIn docs/examples/blocking_consume_recover_multiple_hosts.rst\nhttps://github.com/pika/pika/pull/1028#discussion_r185459909:\n\n\nimport pika\nimport random\n+\ndef on_message(channel, method_frame, header_frame, body):\nprint(method_frame.delivery_tag)\nprint(body)\nprint()\nchannel.basic_ack(delivery_tag=method_frame.delivery_tag)\n+\n\nAssuming there are three hosts: host1, host2, and host3\n\nparams1 = pika.URLParameters('amqp://host1')\nparams2 = pika.URLParameters('amqp://host2')\nparams3 = pika.URLParameters('amqp://host3')\nparams_all = [params1, params2, params3]\n+\nwhile(True):\n\n\nWhat do you think of combining multiple hosts and connection recovery\nexamples and having just one example file for both?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/1028#discussion_r185459909, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KjM5Wrc---BRm1_kqE9ALzDgnbcLks5tuZG_gaJpZM4TqoTn\n.\n. Resolved merge conflicts with master branch.. Thank you!\n. See also issue #1035.. cc @lukebakken . cc @Zephor5, @wulczer, @cellscape. Yes, it does.\n\nOn Mon, Aug 27, 2018, 2:55 AM Glyph notifications@github.com wrote:\n\nDoes #1069 https://github.com/pika/pika/pull/1069 cover the code that\nresolves this?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/1031#issuecomment-416176629, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9Ktgk4u5D0i94AYsWDuqiw21sJxXKks5uU8ICgaJpZM4Tvoi_\n.\n. Rebased against master. Hi @lukebakken, will you have some time to review #1034 and #1029? Also, I think we may be pretty close to another v1.0.0 candidate - need to take care of  issues #1036, #1031, and #1005.. @maggyero, given your expertise in contributing #1030, would you mind tackling this issue too? Thanks in advance.. Hi Michael - I have some ideas for testing it that I want to try first,\nfalling back to your suggestion if needed.\n\nOn Fri, May 4, 2018, 7:56 PM Michael Klishin notifications@github.com\nwrote:\n\nIn my experience this is a royal PITA to automate testing of peer\nverification since standard TLS peer verification usually involves hostname\ncomparison. How about we introduce some examples that use tls-gen\nhttps://github.com/michaelklishin/tls-gen instead and learn/document\nthe behavior that way?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/1035#issuecomment-386774276, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KpJLAHT7JCbw3weCzQVuCQDXpG6dks5tvRTdgaJpZM4TxexH\n.\n. Thanks @maggyero!. @michaelklishin , are you using the code base from current pika master?. I see now, it's my bug. I renamed that member to _heartbeat_checker in Connection, but failed to update the code in HeartbeatChecker class. Would be nice to add a heartbeat checker test method that would detect it before fixing.. Once a connection is created in the new test in  acceptance,\nasync_adapters_test.py, simply access the HeartbeatChecker.active property\nvia connection.heartbeat_checker.active. it should not crash and should\nreturn true. As simple as that :)\n. I'll do it separately, since the bug is actually at the async layer (which is also used by BlockingConnection). @michaelklishin - see pull request #1041 that fixes this. Looks normal to me. Pika gets both IPv6 and IPv4 addresses from DNS address resolution. Your RabbitMQ broker is most likely not listening on IPv6, so pika was unable to connect to it.\n\nPlease submit a pika pull request that changes the logging level of that message from warning to info.\nCheers!. @ajayimade - there is not enough traceback to understand which of the functions your app called threw the error.\nAlso, this sounds like something on the broker side. Take a look at the broker's (RabbitMQ?) logs.. @ajayimade , a traceback usually consists of multiple lines. Post the entire traceback - i.e., all the frames.\nAlso, turn on pika debug logging in your app like this: \nimport logging, sys; l = logging.getLogger('pika'); l.setLevel(logging.DEBUG); l.addHandler(logging.StreamHandler(sys.stderr)) \nand post the entire log.. This is a feature of RabbitMQ that deviates from AMQP 0.9.1. See http://www.rabbitmq.com/specification.html. cc @lukebakken, @michaelklishin . @jon-courtney - definitely DEBUG-level logs would be very helpful. \nLuke, et al.,\nthe default timeout in select poller is just a timeout in case no I/O event of interest occurs. The poller is configured  (or should be configured) to always wait on poll events from an \"interrupt\" file descriptor  which is used by add_callback_threadsafe() to wake up the poller (unblock its poll/select call).\nSo the value of the default timeout shouldn't matter at all - the implementation doesn't (shouldn't) rely on it for I/O.\nIf you run blocking adapter acceptance tests, they test publishing triggered via add_callback_threadsafe(), and I've never seen that sort of delay when running those tests in pika master. If you run those same blocking adapter acceptance tests in the 0.12 candidate branch and see performance issues, then it might indicate that something went wrong with the cherrypicking when creating the 0.12 branch.\nI will investigate this and https://groups.google.com/d/topic/pika-python/_laEMl-8y0g/discussion soon.\n. You can turn on pika debug logging like this:\npython\nimport logging, sys; l = logging.getLogger('pika'); l.setLevel(logging.DEBUG); l.addHandler(logging.StreamHandler(sys.stderr))\nConfigure logging format to output timestamps in each log line (I don't remember off the top of my head, so it's on you, but should be easy :) ), so that we can see how long things take and what may be responsible for the delay.\nAlso, which version of python are you using when running into this issue?\n. Hi @lukebakken, would you mind using commit-sha-relative URLs for code references? Because as soon as we merge something to master with a change in that file, the lines in the URL no longer match.. I click on History, then click on the latest commit, then view that file in the commit's context, and get its URL and line number. With that, the code URLs in issues/pull requests continue to make sense long-term.. @lukebakken: https://github.com/pika/pika/issues/1044#issuecomment-388488527 should never happen, because _r_interrupt file descriptor is now always registered for READ. And another reason it doesn't happen with BlockingConnection is that the connection's socket file descriptor is also always registered, at least for READ. In other words, there should never be a time with 0 file descriptors in the poller. With a connected BlockingConnection there should always be two active file descriptors.. @jon-courtney, please post complete minimum code that can be used to reproduce this issue.. > That's simple, just hit the y key when viewing a file\nThat's cool, @lukebakken! I wish I found out a long time ago.. @jon-courtney - thanks for following up. I didn't notice the attachment initially. And definitely, DEBUG+ level logs should help. Pika is instrumented pretty well with logging. That's the first thing users should be looking at when trying to debug.. ## Eurika!\n@jon-courtney and @lukebakken: I took a quick look at the attached code snippet and spotted the pattern in user code that's triggers this undesirable side-effect as well as another problem that violates the single-threaded access constraint (remember, add_callback_threadsafe() is the only thread-safe API method in BlockingConnection()). It's now going to take much longer to describe everything.\nRoot cause\nFirst, the issue that's causing the slowdown:  the user script in threadsafe_callback.py.txt instantiates BlockingConnection on one thread, then passes it to another thread that calls channel.start_consuming() (which runs the underlying event loop). select_connection.IOLoop() has an optimization (that I borrowed from another framework) such that it caches the threadID of the thread on which the loop runs the first time; subsequently, if add_callback_threadsafe() is called from another thread (whose threadID differs from the cached threadID), IOLoop sends a byte via the poller's _w_interrupt file descriptor which incurs overhead of system call(s) to send and then additional system call overhead on the receiving side when _r_interrupt file descriptor wakes up the poller and causes read(s) from _r_interrupt. On the other hand, if add_callback_threadsafe() is called from the same thread as the thread on which the underlying IOLoop instance ran for the first time, then the sending of the byte is bypassed (it's not necessary when the IOLoop and caller of add_callback_threadsafe() are really running on the same thread).\nYour application unknowingly violates the assumption of the optimization by creating the connection on the main thread, thus activating its IOLoop for the first time from the main thread (whereby it caches the main thread's threadID), and then passing the connection to the other thread to run channel.start_consuming() (which implicitly runs the IOLoop()). Now, when you call add_callback_threadsafe() from the main thread, the IOLoop notices that the caller's threadID matches its cached threadID and bypasses the byte-sending logic (even though they are now running on different threads), thus the poller doesn't wake up until the default timeout expires and you experience the unexpected delay.\nSo, if you refactor your code such that the main thread creates the connection, channel, and then runs channel.start_consuming(), while the background thread calls add_callback_threadsafe(), you should see the unexpected delay disappear.\nNot your app's fault\nThat said, the problem is not with your code, rather the optimization in IOLoop is more trouble than it's worth. I think it will be better to remove this optimization from select_connection.IOLoop (along with caching of threadID) and add a blurb in add_callback_threadsafe()'s docstring that in the case where you need to request a callback knowing that the requester is running on the same thread as the connection, it's more efficient to use add_timeout() with a delay of 0 than add_callback_threadsafe().\nCall to action\n@jon-courtney - now that you know as much as anyone about the issue in select_connection.IOLoop and how to remedy it, would you mind creating a simple pull request that removes that optimization in IOLoop as I described (including update to the add_callback_threadsafe docstring in BlockingConnection and in select_connection.IOLoop)? @lukebakken could then cherrypick your change from master branch and apply it to the next 0.12.x release candidate (or the other way around). You might also need to fix up some unit tests that could break as the result of the change (perhaps), but that should be simple. The optimization itself that you want to remove is this if statement, so self._poller.wake_threadsafe() will be called unconditionally (and the comment would become # Wake up the IOLoop possibly running in another thread); also get rid of self._thread_id. Easy, right? This way you will be able to confirm that the problem experienced by your app goes away without having to refactor your app, making it a good way to confirm the fix. Keep in mind that master branch has changed the order of some arguments in the connection methods that may or may not affect your script, so there is a small chance you might need to flip some args in your script.  \nThe unrelated problem in your user code\nIn the beginning, I mentioned that there is also a problem within your sample app threadsafe_callback.py.txt, although it has nothing to do with the callback latency. Since your background thread is running the connection via channel.start_consuming() the call from the main thread to channel.stop_consuming() triggers a race condition (recall that only add_callback_threadsafe() is thread-safe, the other API methods are not thread-safe for a given connection instance, including its channels). The safe way to do that would be: connection.add_callback_threadsafe(lamda: channel.stop_consuming() if channel.is_open else None) and then call th.join([timeout]) (with optional timeout arg) to wait for the thread that was running channel.start_consuming() to exit.. Regarding\n\ndo you think we should use thread IDs to print warning statements if the user is violating the expectations that we're assuming?\n\nHi @lukebakken,\nWhich specific scenarios are you thinking can benefit from this type of\nwarning?\nAfter making the wakeup in add_callback_threadsafe unconditional and adding\nthe recommended blurb in the docstrings, there won't be a need to warn the\nuser in this api method, since it should work in all scenarios after this\nchange.\nAlso, users can configure logging themselves to have python logging automatically add\nthread IDs to existing logging output in a systematic way.\n. @lukebakken - I followed up on https://groups.google.com/d/topic/pika-python/_laEMl-8y0g/discussion. It appears to be user error. The user is accessing a connection (and/or its channels) from different threads, which is not supported.. @lukebakken: The thing to keep in mind is that the default timeout is arbitrary. It could just as well be infinite. Pika's select_connection design/performance shouldn't rely on any specific value of _MAX_POLL_TIMEOUT. Of note is that making this timeout too small will unnecessarily drive up CPU utilization. select_connection.IOLoop's pollers are driven by file descriptors, timers, and scheduled callbacks that are added via the new method add_callback_threadsafe().. You're welcome. Cheers!. @lukebakken:\nregarding time.sleep() in select_connection's poller code...\n\nIt seems like 5 seconds is too long for a sleep, though, since it will block all fd activity, right?\n\nselect_connection.IOLoop's asynchronous behavior is governed only by 1. timers; 2. thread-safe callbacks; and 3. file descriptors. No other behaviors are possible with this IOLoop. If no file descriptors are registered, it leaves only timers and thread-safe callbacks, in which case the next time that the I/O loop would need to wake up is governed only by self._get_max_wait().\nSince asynchronous apps rely on the event loop for running their code, once control returns to the event loop, apps expect to be called for ready file descriptors of interest, expired timers, and thread-safe callbacks. No other behaviors are defined or expected by the apps.\nSelectPoller - which uses select.select() for polling - cannot call select.select() without file descriptors on Windows (windows raises an error in this case, although linux/unix do not). Functionally, calling time.sleep(self._get_max_wait()) is equivalent to calling select.slect([], [], [], self._get_max_wait()).\nThe other pollers that are based on poll(), epoll(), and OS X kernel queues don't have the Windows problem and act equivalently to time.sleep() when no file descriptors are registered.\n. @lukebakken: I believe it would be good to change the implementation of select_connection.IOLoop.call_later() to make use of time.monotonic() instead of time.time() on Python3, but outside the scope of this pull request (let's create a new issue for it):\nPython 3 supports time.monotonic() which is better suited for timers (\"a clock that cannot go backwards. ... not affected by system clock updates.\"). @sw360cab:\nRegarding your application that is already in Production: in your production environment, is your pika application on the same server as the RabbitMQ broker?\nHere is some additional food for thought: when something goes wrong on the network that severs the TCP connection abruptly such that FIN (or RST) is not received by the consumer, the consuming end running pika (with heartbeats disabled) has no way to find out that the connection was severed. I have seen this happen, for example, when the remote server dies suddenly; also have seen this when some load balancer drops the connection after a period of inactivity.\nIf the hang is really due to the lack of incoming FIN (or RST), then it would explain the appearance of a hang in BlockingConnection. It would also explain the lack of anything interesting in Pika DEBUG logs. If FIN didn't come in and no AMQP messages came in, then there would be no activity to log in Pika.\nHence, if a connection goes dead without receiving FIN (or RST), there is no way for an application or TCP/IP stack to find out. Unless, of course, you enable AMQP heartbeat! :).\nSince your sample code that reproduces the issue simply discards the messages without taking a long time to process them, go ahead and enable the heartbeat. You might see one of these new outcomes: 1. the connection no longer drops due to heartbeat traffic across some network element that used to sever the connection (not gracefully) when the connection became idle for some period of time; or 2. BlockingConnection with heartbeat enabled discovers the severed connection (no heartbeat or other traffic from RabbitMQ) and raises the expected exception to signal loss of connection.\nFinally, the reason that your app disables heartbeats is most likely because the long message processing time was causing heartbeat timeout and loss of connection. BlockingConnection in Pika v0.12.0 has a new feature to make it practical to defer processing of messages to another thread, while the BlockingConnection's thread continues the consumer event pump and processing of heartbeats; when the other thread is done with the message, it can use BlockingConnection's new method add_callback_threadsafe() to request that an ACK be sent in a thread-safe way. This feature was back-ported from the Pika 1.0 pre-release work and has an example in the README.rst. Look for add_callback_threadsafe in this README. This way, lengthy processing of incoming messages can coexist elegantly/efficiently with AMQP heartbeats, and the heartbeats will help detect adverse (not graceful) loss of TCP stream.. @sw360cab, does this help?. Thank you for the update. Please keep in mind\nthat add_callback_threadsafe() is not available on 0.11, only starting with\n0.12.0 release candidate and later. And that in order to keep the\nheartbeats going smoothly you need to process your message in the other\nthread while your BlockingConnection thread remains blocked in the\nstart-consuming() call or similar, and then use add_callback_threadsafe()\nfrom that other thread to dispatch the ack once you finish processing the\nmessage.\nAlso note that Blocking connection is built on top of SelectConnection, so\nswitching to SelectConnection directly might not be of much benefit, but\nfeel free to try if you like.\n. @lukebakken, would you mind including the pika version number in the log content when opening a connection? This would help get our bearings when examining logs. Thx!. cc @lukebakken \n@sw360cab - regarding:\n\nA first attempt adding an heatbeat timeout of 600s reproduces the issue. I pasted Pika log here: https://pastebin.com/K4cJwD3w\n\nWhich version of Pika are you using in this case?\nA couple of strange things in this log:\n1. @sw360cab - your log contains mixed messages from multiple connections running concurrently (either on multiple threads or processes).  This makes it too difficult to analyze. Have you tried making sense of this log yourself? :) If using threads, configure python logging to output the threadID and then filter your log by the thread id (see \"%(thread)d\" and \"%(threadName)s\") and attach only logging output from the pertinent BlockingConnection instance. If using processes, you can similarly configure python logging to emit process id via \"%(process)d\".\n2. An error is in the log that indicates that a callback function in your app is expecting a different number of parameters than it is passed. Could this have something to do with the problem that you're experience with Pika 0.12 prerelease? You probably need to debug/fix this exception in the app: 2018-05-28 14:18:27 DEBUG - workers.failurez.Failurez: [x] 'output_fail':'{\"errorDetails\": \"handleErrors() takes exactly 3 arguments (2 given)\", \"processedMsg\": {\"src\": \"/home/wimlabs/multiqueue-transcoder/videotest/download-s3/netbally.mp4\", \"destBucket\": \"net-cedeo-wimtv-transcoded-videos\", \"dest\": \"/home/wimlabs/multiqueue-transcoder/videotest/transcode/netbally.mp4\", \"contentId\": \"a_content_id\", \"sourceObjectName\": \"netbally.mp4\", \"sourceBucket\": \"net-cedeo-wimtv-original-videos\", \"srcType\": \"S3\", \"destObjectName\": \"netbally.mp4\"}, \"workerName\": \"workers.process_transfer_transcoder.TransferTranscorder\", \"errorName\": \"exceptions.TypeError\"}'\n. See also #1055. @lukebakken, I think the suggestion to use SelectConnection wouldn't improve the situation, just make his code more complicated due to \"callback hell\" :). Also, the suggestion to use add_timeout to delegate processing to the correct thread looks like a typo. If someone is using SelectConnection directly, and need to schedule a callback inside SelectConnection's I/O loop safely, they should call the add_callback_threadsafe() method of that SelectConnection's I/O loop directly - connection.ioloop.add_callback_threadsafe(...) - not add_timeout which isn't thread-safe.. @lukebakken: add_timeout() is the non-thread-safe timer API in 0.x.x\n(renamed to call_later() in master now because add_timeout is intended to\nbe used with absolute time while ours was accepting relative)\nTechnically, select_connection.IOLoop defines the alias add_callback that\nmaps to select_connection.IOLoop.add_callback_threadsafe. This was done\nbecause this IOLoop now shares some common Pika utilities with tornado's\nIOLoop class. select_connection's IOLoop API was historically modeled\non tornado's IOLoop which has add_callback(). But since we're talking\nabout using SelectConnection's IOLoop here, add_callback_threadsafe() is\nmore self-documenting, so I'd rather advertise it.\nOn Fri, Jun 8, 2018 at 2:46 PM, Luke Bakken notifications@github.com\nwrote:\n\nThanks @vitaly-krugl https://github.com/vitaly-krugl for some reason I\nthought add_timeout was the equivalent of BlockingConnection.add_\ncallback_threadsafe\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/1046#issuecomment-395899955, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KotPhVcmnTdxezR9iv-CWkrBbWIkks5t6vCygaJpZM4T_P56\n.\n. Closing - rabbitmq-users Google group is a more suitable forum for usage questions.. @fokhunov, which version of Pika are you using in this issue? Please collect, analyze, and attach DEBUG-level Pika logs from this issue beginning with connection establishment until ConnectionClosed.. @lukebakken, as far as I can tell, no change is necessary in Pika. Pika supports the max defined by AMQP for use with other brokers, but accepts a smaller value from the broker during connection tuning - see https://github.com/pika/pika/blob/61cbdbbd6a1d4fae19e486fe22a06c16c7821bdf/pika/connection.py#L2016-L2018.\n\nSo, if RabbitMQ passes a max of 2047, Pika will not exceed it.. Michael, but that's the whole point of the negotiation that picks the\nsmallest. Apps should be able to check the negotiated value after\nconnection establishment and take whatever action necessary. I don't see\nany benefit in crippling the client with specific limits from a specific\nversion of a specific broker implementation when the existing mechanism\nalready serves that purpose dynamically :)\nOn Mon, May 28, 2018, 2:54 PM Michael Klishin notifications@github.com\nwrote:\n\nWhile that is true, we try to reflect the default in client libraries as\nwell for two reasons:\n\nNewer clients can be used against older server versions. It would be\n   nice to have a safer default combination.\nDevelopers are likely to check the client's default when in doubt.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/1054#issuecomment-392608674, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9Kk2P9UB-oYf59sI6yTAXV7oR6cPpks5t3HIogaJpZM4UQk9j\n.\n. @mklishin, it still doesn't make sense to me, given that there is already a\nnegotiation mechanism that works. Given that, it wouldn't be right for me\nto accept such a PR. However, I won't object if Luke or someone else\naccepts it.\n. Please keep in mind that pika API will undergo additional tweaks on the way to 1.0 release. Cheers!. @abompard, thank you for the contribution. I haven't check, but just in case, please make sure that all the affected modules pass pylint (with pika's pylintrc) cleanly. Cheers!. @abompard  - Re.\nI have the following warnings, that I've disabled:\nW0703, line 124 & 182, \"Catching too general exception Exception (broad-except)\": we don't know what the exception can be in the wrapped function so I think it's OK to be broad.\nW0221, line 241, \"Parameters differ from overridden 'channel' method (arguments-differ)\": that's intended because the second argument is forced on the overridden method.\nBut that's only my opinion, I can adapt if you disagree.\n\nW0703: I don't know what else can be done differently for it, so just have to live with it.\nW0221: I understand that the native Twisted adaptation needs to have a different API because of Futures, for example. However, I think that the API overriding approach takem by the implementation of TwistedProtocolConnection (TPC after this) is suboptimal.\n Here, we can use the litmus test for subclassing versus containing: \"is a\" versus \"is implemented in terms of\". In my mind, TPC is not a Pika Connection Adapter in the sense that it's a Twisted Protocol Adapter and its ABI differs from Pika's Connection and BaseConnection. Just like Pika's BlockingConnection class that's implemented in terms of SelectConnection but doesn't subclass from it (see BlockingConnection._impl and BlockingChannel._impl), TPC should not subclass from Connection, but instead should contain an instance of an implementation class that derives from pika.Connection.\n And TPC should have its own explicitly-defined API without the *args/*kwargs funny business that falls short in terms of self-documenting code and when used with tab-completion-enabled IDE's. Looking at *args/*kwargs, I would have a bear of a time navigating/using this API as a developer.\nExample of what I have in mind; it should not be much effort to refactor current implementation, and partinioning of responsibilities would make the implementation more maintainable too:\n``\nclass _TwistedConnectionAdapter(pika.Connection):\n    # Here, implementpika.Connection's pure virtual methods - all that presently lives in \n    #TwistedProtocolConnection.\n    # Also implement whatever accessors are going to be needed by the new \n    #TwistedProtocolConnection` implementation\ndef __init__(...<whatever is needed by your implementation>...):\n    ....\n\n......\n\nclass TwistedProtocolConnection(object):\ndef __init__(...<whatever makes sense for twisted protocol adaptation>...):\n    \"\"\"\"\n    ....\n    :param <type> ...: ....\n    :param <type> ...: ....\n    \"\"\"\"\n    self._impl = _TwistedConnectionAdapter(...)\n    ....\n\ndef channel(......):\n        \"\"\"\"\n        :param  ...: ....\n        :param  ...: ....\n    :returns: <whatever makes sense for Twisted adaptation (Future probably in this case)>\n    :rtype: ....\n    \"\"\"\"\n    .....\n\ndef ....\n\nclass TwistedProtocolChannel(object):\n    def init(self, channel_impl, ):\n        self._impl = channel_impl\n        ....\ndef basic_get(<whatever makes sense for Twisted adaptation>):\n        \"\"\"\"\n    :param <type> ...: ....\n    :param <type> ...: ....\n\n    :returns: <whatever makes sense for Twisted adaptation>\n    :rtype: ....\n    \"\"\"\"\n    .....\n\n def ....\n\n```\n. I think it would be easier for developers to use if the methods we're all\nthere with explicit args. We're close to v1.0, so I don't anticipate method\nsignatures changing for a long time.\n@lukebakken, does this work for you?\nThank you,\nVitaly\nOn Fri, Jun 22, 2018, 1:52 AM Aur\u00e9lien Bompard notifications@github.com\nwrote:\n\n@abompard commented on this pull request.\nIn pika/adapters/twisted_connection.py\nhttps://github.com/pika/pika/pull/1069#discussion_r197382295:\n\n     return ret\n\n\n\ndef __clear_call(self, ret, d):\nself.__calls.discard(d)\ndef _clear_call(self, ret, d):\n\nself._calls.discard(d)\n         return ret\ndef getattr(self, name):\n\n\nI didn't think so, I thought you meant explicit signatures for the\nredefined methods only. But if you prefer not using getattr, I can\ncopy all the methods in WRAPPED_METHODS in this class.\nI'm just a bit worried it will be more maintenance in the long run, since\nthey will have to be adapted in this class too whenever they are changed in\nthe main Channel class. But I'll follow you if you think it's worthwhile.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/1069#discussion_r197382295, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KsHi5fja-visK_IVoe1YEOSf952Xks5t_LBlgaJpZM4UfHvF\n.\n. Hi Aur\u00e9lien,\n\nthe merge commit of your pull request is showing up for me in\nhttps://github.com/pika/pika/commits/master:\n====\nCommits on Aug 1, 2018\nMerge pull request #1108 from abompard/twisted-tests  \u2026\n@lukebakken\nlukebakken committed 4 days ago\n====\nAlso, this post provides some info about the name attribute:\nhttps://stackoverflow.com/questions/22204660/python-mock-wrapsf-problems\nBest,\nVitaly\nOn Wed, Aug 1, 2018 at 1:58 AM Aur\u00e9lien Bompard notifications@github.com\nwrote:\n\nOK I found why, it's because functools.wraps in Python2 requires the\nname attribute on functions and mock.Mock does not have that. In\nPython3, functools.wraps is protected against missing attributes with a\ntry..except block.\nI have a patch, I'll make a PR (with a couple minor fixes to the Twisted\nadapter) as soon as I can pull from master. At the moment Github does not\nshow me this PR as merged in master (a hiccup maybe?)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/1069#issuecomment-409503664, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KkM9ytHC9Osw6YtCfQ8crWBPNPpTks5uMW3HgaJpZM4UfHvF\n.\n. @lukebakken, let's work out one of these parallel pull requests first, then create the second one after that. . @lukebakken - It looks like RabbitMQ's heartbeat-emission functionality is broken as it's inconsistent with RabbitMQ's own documentation, and implements the heartbeat protocol asymmetrically. I posted on rabbitmq-users about it: https://groups.google.com/forum/#!topic/rabbitmq-users/Fmfeqe5ocTY.\n\nI recommend adopting the following strategy in Pika - use two timers in HeartbeatChecker:\n1. Heartbeat Transmit timer: fires every float(timeout) / 2 seconds just to send a heartbeat (note the float wrapped only around  timeout, not the entire quotient :) ). This is compatible with https://www.rabbitmq.com/heartbeats.html#heartbeats-timeout: \"Heartbeat frames are sent about every timeout / 2 seconds\"\n2. Heartbeat check timer: fires every float(timeout) * 3 / 2 seconds. Each time this timer fires, HertbeatChecker checks whether any data has been received since last time. This provides for sufficient additional time buffer to make sure we don't miss RabbitMQ's heartbeat to work around the noted flaw in RabbitMQ when there isn't any other data traffic from the broker without taking the full 2x time to detect dead connections. And a code comment attributing the workaround to RabbitMQ's errant behavior so that we'll later remember why it's being done.. Looks good. I wish RabbitMQ would change to a symmetric scheme. There would\nbe no harm if it sent heartbeats twice per negotiated heartbeat. :)\nOn Fri, Jun 22, 2018, 6:42 AM Luke Bakken notifications@github.com wrote:\n\nThanks @vitaly-krugl https://github.com/vitaly-krugl I think we came to\nthe right set of changes.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/pull/1072#issuecomment-399446844, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9Kgc4JBbTJsaoJd_bXgzQAAMXO4SNks5t_PRcgaJpZM4UhcAK\n.\n. Our python docstrings follow rst, and as someone has mentioned, Sphinx/rst\nis defacto standard for python shops. So, I vote for rst\n\nOn Thu, Oct 25, 2018, 12:27 PM Luke Bakken notifications@github.com wrote:\n\n@gmr https://github.com/gmr @vitaly-krugl\nhttps://github.com/vitaly-krugl do either of you have an opinion?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/1073#issuecomment-433175873, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABX9KhaHrHKPiepaonWbTb0vBDUtrav-ks5uohClgaJpZM4Umj8r\n.\n. Thanks for your report. It's not valid to pass None or any other value that was not returned from BlockingConnection.add_timeout() to BlockingConnection.remove_timeout(). I am not aware of too  many robust API's that allow you to pass bogus values. It's the responsibility of the application to check for None and not call remove_timeout() with that. I am closing this ticket.. Thank you for your contribution, but we don't want to accept bogus values passed to the APIs. If the application uses None as a sentinel value, it's up to the application to check for it and not call remove_timeout() if it's None. It's the application's responsibility to do so. It's bad precedent and a slippery slope for an API to accept bogus values. The public API doesn't promise to accept bogus values and ignore them. That would be a bad API. I am going ahead and reject this pull request since it's the application's responsibility to check for its own sentinel values.. @lukebakken, @michaelklishin   - this might be worth another look. It wold be cleaner if our stack avoided recursion.. @lukebakken - yeah, I think that raising an exception is the best way to go versus allowing the app to do something that isn't supported.\n\nSelectConnection ioloop API is based on (and is almost identical to) Tornado's ioloop API. Here is what tornado ioloop does the moment it enters its start() method:\ndef start(self):\n        if self._running:\n            raise RuntimeError(\"IOLoop is already running\")\nWe should do the same thing. This can be easily accomplished in select_connection._PollerBase.start():\n```\n    def start(self):\n        \"\"\"Start the main poller loop. It will loop until requested to exit\n    \"\"\"\n    if self._start_nesting_levels > 0:\n        raise RuntimeError(\"IOLoop is already running\")\n\n    self._start_nesting_levels += 1\n\n    . . .\n\n```\nIn fact, after this change, self._start_nesting_levels can be replaced with a simple flag, such as Tornado  ioloop's self._running, to avoid confusion in the logic regarding what \"nesting levels\" implies.\nOf course, select_connection.IOLoop.start() should be documented as not reentrant and that it raises RuntimeError upon attempt to reenter it.\nI see that you've already fixed pika/examples/asynchronous_consumer_example.py not to reenter iollop's start(), which is great!  I noticed a couple of comments/log messages that are now out of place after your change:\n1. https://github.com/pika/pika/blob/d7ed3e79a0df4bf8f316b3085154f29a879a4c3d/examples/asynchronous_consumer_example.py#L105-L106 - ExampleConsumer.reconnect() is no longer invoked by ioloop timer, right?\n2. https://github.com/pika/pika/blob/d7ed3e79a0df4bf8f316b3085154f29a879a4c3d/examples/asynchronous_consumer_example.py#L100 - it's no longer 5 seconds. @lukebakken - Shouldn't the following raise ValueError instead of just logging a warning? If I misconfigure something, I would hope to catch that right away via the exception , which is typical for Python code, instead of pretty much hiding it which is what just logging it amounts to:\n https://github.com/pika/pika/pull/1132/files#diff-ab2a915baf59d292f456225a7431ea11R999\n https://github.com/pika/pika/pull/1132/files#diff-ab2a915baf59d292f456225a7431ea11R1009\n* https://github.com/pika/pika/pull/1132/files#diff-ab2a915baf59d292f456225a7431ea11R1018\n. @michaelklishin , I strongly disagree with your statement\n\n\"There are over 2K LOC of acceptance tests for BlockingConnection. I suggest that we remove all mocked tests as they are a waste of time (sorry not sorry). If we are lacking tests in some areas, more integration tests on more platforms is what I'd do.\"\n\nMock-based unit tests are great at testing logic branches that are mostly impossible to test using system/integration tests. They are also necessary for Dynamic programming languages like Python which don't have the benefit of compile-time error-checks and strong typing. So please, don't remove them. Thanks!\nCase in point... when this pull request removed the conflict assertion from the poller's close() routine, it broke the API and jeopardized stability by no longer enforcing the restriction on closing the poller while it's still running. The unit test (that you suggested to remove) actually caught that API breakage by failing. However, instead of fixing the broken API call, this pull request removed the smoking gun - i.e., it removed the unit test that actually detected and signaled the API breakage.. @wjps, a partial write in this scenario is not abnormal at all, so let's not log it as a warning. debug level would be more appropriate, if it's necessary to log it at all.\n. @wjps, what impact will this change have on the SSL connection?\n. Let's delete this dead code\n. For a blocking socket, a write timeout is to be expected and not a cause for warning. @wjps, the reason that I am pointing these out is that it's difficult to debug real problems when the log is filled with warning messages that result from normal outcomes.\n. @wjps, what is the purpose of this new setblocking(0) call here if your change in super BaseConnection._adapter_connect() already does the same?\n. Consider skipping object allocation if the entire frame was written. The check should be cheaper than an allocation unless partial frame writes will be the norm. \n. I think I now understand better what you are trying to accomplish with the placement of _handle_write() call here: try to write immediately, and if the socket buffer fills up leaving some frames unwritten, then have the ioloop trigger writing of the remaining frames when the socket becomes writable. If that's the case, we have to be careful to avoid a lot of unnecessary costly kernel calls. Let's say that an app is sending a large number of messages back-to-back and the socket buffer fills up; then, many subsequent calls to _flush_outbound() will result in many unnecessary, expensive kernel calls from unproductive _handle_write() attempts. These unnecessary kernel calls can be avoided by checking if the connection was already waiting to write before making the _handle_write() call here. I think this could be something like:\n```\nif not self.event_state & self.WRITE:\n    self._handle_write()\nself._manage_event_state()\n```\nWhat do you think?\n. Previously, socket.timeout was re-raised in both _handle_read() and _handle_write(). This PR makes them inconsistent: re-raised in _handle_read(), but suppressed in _handle_write().\n. If socket.timeout exception is going to be suppressed in _handle_write(), it necessitates clean-up in BlockingConnection._flush_outbound(), which still expects to handle socket.timeout from BaseConnection._handle_write(), and BlockingConnection._flush_outbound's handling of socket.timeout becomes dead code with this change.\nThen there are also BlockingConnection._handle_timeout() and BlockingConnection._socket_timeouts logic bits that need to be considered. I think that some of that logic becomes inconsistent as the result of this PR.\nFinally, this changes the semantics of pika.connection.Properties.socket_timeout; at least as far as BlockingConnection is concerned.\n. Personally, I like the proposed change in _handle_write(), but I also wonder what was pika author's intention behind pika.connection.Properties.socket_timeout and the corresponding handling and re-raising of the socket.timeout exceptions in the original read and write paths. Was pika's author perhaps concerned about stalled connections, relying on socket.timeout exceptions to detect stalls and abort stalled connections? @gmr, would you mind chiming in on this? Thx!\n. @wjps, this is not an adequate fix because the raised exception would not be consistent with the state of the connection - i.e., connection.is_closed would still evaluate to False after this. It's the same problem as with the AttributeError exception handler that this PR removed.\nThe issue is reported in #412 and I believe that PRs #533 and #542 together address this issue completely, while at the same time maintaining consistent connection state. I didn't remove the AttributeError block in those PRs, though.\n@wjps, would you mind trying out the combination of the PRs #533 and #542 against your use case and posting your findings here? If it doesn't do the right thing, please describe the exact steps to reproduce and details of the incorrect outcome, including the traceback, if any. Many thanks in advance!\n. @wjps, I agree that 100% non-blocking would be best. From my experience in developing 100% non-blocking I/O integration with OpenSSL a few years ago (in C), I recall that there were a number of gotchas. At the time, I captured the gotchas that I discovered in code comments in https://github.com/openwebos/libpalmsocket/blob/master/src/psl_channel_fsm_crypto.c.\n. Is the exception being intentionally suppressed here? Please add a comment explaining why the exception is being suppressed and how user code might become informed of the failure.\n. The title of the PR refers to ProbableAuthenticationError, but the exception being trapped here is more general.\n. I can see how an exception here is a problem for frameworks like Tornado and Twisted that rely on continuation-passing style. I am not aware of an ideal solution for this in the existing pika code. Does TornadoConnection's stop_ioloop_on_close=True cause the ioloop to stop in the #525 scenario? I see that there are other types of exceptions that would be problematic for these frameworks, such as exceptions.ProtocolVersionMismatch.\n. I wonder if the following might do the trick for you (in addition to suppressing AMQPConnectionError in tornado_connection.py):\n1. Add try/finally in BaseConnection._handle_disconnect():\ndef _handle_disconnect(self):\n         \"\"\"Called internally when the socket is disconnected already\n         \"\"\"\n         try:\n             self._adapter_disconnect()\n         finally:\n             self._on_connection_closed(None, True)\n2. Pass on_close_callback in addition to on_open_error_callback callback function to the TornadoConnection constructor.\nI think this will assure that your on_close_callback will be called if AMQP handshake fails. The on_open_error_callback callback should be called if connection setup (TCP/IP connection or SSL handshake) fails prior to AMQP handshake.\non_open_error_callback is defined in pika as \"Method to call if the connection cant be opened\", which makes it seem like it's intended to report errors in TCP/IP connection establishment; whereas on_close_callback is intended for the AMQP handshake phase and later, since on_close_callback may be given the reason_code and  reason_text args that can only come from the broker.\n. @awelzel, the reason I proposed the try/finally BaseConnection._handle_disconnect() fix above is that it doesn't alter the documented semantics of on_close_callback and on_open_error_callback, shouldn't negatively impact any other connection classes, and is also deterministic versus \"If the connection is neither open, not closed, it probably was never successfully established\".\nCC @gmr \n. Hi @wjps, just to clarify. I think that my two PRs (#533 and #542) make this \"AttributeError\" issue go away altogether. I could no longer reproduce it with those PRs in place.\n. When tornado ioloop calls BaseConnection._handle_events and READ is indicated, it calls BaseConnection._handle_read, which will attempt to read by default at most pika.spec.FRAME_MAX_SIZE = 131072 bytes from the socket and pass that to Connection._on_data_available. The pika overhead of processing this data should be pretty minimal, so processing time of incoming messages largely depends on processing by user code.\n. @wjps, thanks for the explanation/analysis\n. @wjps, according to python doc, select.select() on Windows only works with sockets; see: https://docs.python.org/2/library/select.html\n\nOn Windows, the underlying select() function is provided by the WinSock library, and does not handle file descriptors that don\u2019t originate from WinSock\n\nSo, you will need to create a socket pair instead of a pipe for this in order to be compatible with all supported OS's. But again there is a caveat: you can't use socket.socketpair() for this on Windows. rabbitpy solved it like this using its own _socketpair() method: https://github.com/gmr/rabbitpy/commit/314924c9485195be20129ecfe2710980e7a01a10#diff-278cd36141e045c14fba20692cbdd05fR460\n. misspelling: *_interupt should be *_interrupt\n. @wjps, I found the name self.filenos to detract from code readability. Perhaps something like self._handlers or self._fileno_to_handler_map would make it easier to follow the logic in the code.\n. @wjps, would you mind documenting the params here?\n. Also, the arg name c is non-standard. Would you mind changing it to fileno to follow the convention in other methods here? I was scratching my head when I saw it and had to trace the call back to its origin to figure out what c really was. Also, events might be more consistent with other code here instead of e. Thx.\n. @wjps, KQueuePoller.start() still references self.open in while self.open:, which no one sets or even initializes any longer. Should this be while not self.stopping: instead, like in the other pollers?\n. Is there a possibility that this might get an OSError exception with EWOULDBLOCK? Should there be an exception handler that suppresses it just in case?\n. @wjps, it's conceivable that an earlier handler callback in this loop's cycle might call something that removes one of the other handlers, so this code will break. for fileno in self.filenos might break, too, if self.filenos were modified during iteration. Same applies to similar loops in the other pollers in this module. I think that you need to deal with it as follows, just to be on the safe side:\nThe problem might be more tricky if filenos are not sockets, but actual integer file descriptors, in which case closing one and opening another might re-use the same file descriptor number, so the check fileno not in self.filenos might not be enough.\nAnother complication might occur if an earlier handler callback (in the same loop cycle) modifies the events of interest for another fileno that was already registered in self.filenos, and the event that triggered is no longer of interest.\n- in __init__(): self._processing_filenos = []  # need a better name?\n- poll()'s processing loop:\n```\nself._processing_filenos[:] = self.filenos.keys()\nwhile self._processing_filenos:\n    fileno = self._processing_filenos.pop()\nif fileno in read:\n    events |= READ\nif fileno in write:\n    events |= WRITE\nif fileno in error:\n    events |= ERROR\n\n# TODO: also check if the detected event(s) are still of \n# interest to user before calling the handler; skip if not of interest;\n# the previous callback might have called update_handler to update\n# this filino's events such that the current event might not be of\n#  interest any longer\n\nif events:\n    handler = self.filenos[fileno]\n    handler(fileno, events, write_only=write_only)\n\n``\n- Inremove_handler(self, fileno)`:\ntry:\n   self._processing_filenos.remove(fileno)\nexcept ValueError:\n    pass\n. @wjps, shouldn't all the member variables here be designated as \"private\" or \"protected\" using a preceding underscore? I.e., self._w_fds, self._e_fds, self._filenos, self._stopping, etc.\n. @wjps, there needs to be some way for r_interupt and self.w_interupt to be closed. Otherwise, an app that creates/destroys many SelectConnection instances over its lifetime will end up leaking lots of kernel resources. The pollers should probably have a close() method that should be called reliably; perhaps from SelectConnection._adapter_disconnect():\ndef _adapter_disconnect(self):\n        \"\"\"Disconnect from the RabbitMQ broker\"\"\"\n        try:\n            if self.socket:\n                self.ioloop.remove_handler(self.socket.fileno())\n            super(SelectConnection, self)._adapter_disconnect()\n        finally:\n            if self.ioloop:\n                self.ioloop.close() # and it's implementation calls the underlying poller's `close()` method\n. @wjps, there is no remove_handler in self.ioloop. Try running this code through pylint to see what else it might uncover.\n. This is not enough. fileno might still be in self.r_fds, etc. Need something like this:\nself.update_handler(fileno, 0)\ndel self.filenos[fileno]\n. Not sure if it's an issue overall, but thought I'd mention it: in Mac OS select, the 3rd fd set arg is for errors, but on linux and unix (and windows?) it's for \"exceptfds\", which are for out-of-band data in case of TCP/IP (but not errors)\n. No point for as err, since err is not used\n. @gmr, I pulled the code in this module from my own https://github.com/vitaly-krugl/inetpy repo that presently uses an \"MIT License\". Do I need to put any sort of copyright header in this file? Thanks, Vitaly\n. @wjps, this is a bug; the code needs to use except NameError: explicitly here. except: by itself is almost always problematic for the following reasons:\n1. It prevents system-level exceptions, such as KeyboardInterrupt (based on BaseException, not on Exception), from working properly. Note that Ctl-C as well as SIGINT both trigger the KeyboardInterrupt exception by default.\n2. It's too broad and will catch unintended exceptions.\n. @wjps, this may be a bug. My understanding is that UDP does not guarantee packet delivery, possibly not even on the loopback interface.\n. @wjps, this may be a bug. Since get_interrupt_pair() configured the socket pair as non-blocking, OSError exception with errno EAGAIN or EWOULDBLOCK may be a possibility\n. @wjps, is socket.error with errno EAGAIN or EWOULDBLOCK a possibility here?\nI am also curious why you chose to use os.read() on self._r_interrupt, but self._w_interrupt .send() socket method here? Should they both use send/recv or os.read/os.write for consistency? There is a difference in error-handling, too: the os call can raise OSError, while the socket call would raise socket.error\n. @wjps, this change may be leaking UDP sockets, as there is no code that explicitly closes these two sockets self._r_interrupt, self._w_interrupt\n. @wjps, would the following recursive flow be possible/problematic for this logic?\n1. This logic is executing and a handler is invoked with the READ event and data representing an incoming message or some other event.\n2. The handler calls a user's callback (such as a basic_consume callback)\n3. A user's callback calls some pika API, which in turn ends up in poll() and subsequently in _process_fd_events\nI am trying to remember what in pika would prevent re-entry here?\n. @wjps, in a default SelectConnection, it's ioloop per connection. Some apps create many short-lived connections over their execution lifetime.\nThe reason that I pointed this out is that there was a recent PR (from you or someone else) that added a sock.shutdown() somewhere in pika, because of uncertainty when a socket would be garbage-collected.\n. Thanks @wjps, I can't say I feel very strongly about it, but I find that, in general, code is easier to understand and maintain when things are consistent. :)\n. @wjps, Would you mind catching a specific exception (socket.error in this case) and re-raising the exception if it's not err.errno == errno.EWOULDBLOCK? I am worried that this handler will make it more difficult to detect other unexpected errors. Thanks!\n. @wjps, this call will crash. Also, this flush_outbound() method didn't exist prior to this PR and appears to not be used by anything internally, so should be safe to just delete this method.\n. My feedback here is based entirely on code review, which anticipates that there might be a problem. By the way, I don't see how we would drop out of the outer/inner loops: fd_event_map.keys() returns a python list instance that will not be affected by overwriting of self._processing_fd_event_map, so the loops will just continue executing.\n```\n        self._processing_fd_event_map = fd_event_map\n    for fileno in fd_event_map.keys():\n\n```\n. Also, regarding\n\nWhen/if #556 is merged this nasty recursive behaviour goes away\n\nSorry, if you wouldn't mind explaining it to me, I don't see how #556 makes the recursive behavior go away.\n. Thanks @wjps for sharing the info about garbage-collection of the sockets in this case. The problem with relying on garbage collection exclusively is that if something else happens to retain a living reference to the un-closed socket, than that kernel  resource would leak. Given your verification, let's leave it be for now.\nRegarding a default loop, I think that with your latest changes (you can pass an existing loop to SelectConnection constructor), an app can do that if it wanted to. So, let's leave it as is, too.\n. @wjps, I think it would be better to simply delete this unnecessary method from SelectPoller. I believe that nothing is using it and I think that it was added anew in your recent PR.\n. Got it! Thank you.\n. @wjps, this triggers a bug in BaseConnection._handle_ioloop_stop() when it attempts to call self.ioloop.stop() here https://github.com/pika/pika/commit/e03c7adbb609f91e7523057a33c05817f096acad#diff-1a5308d8b684fdafac1f89e376e8933dR209\nIf confirmed, then in addition to the fix, it would be great to have a test for this code path.\nFYI, I logged this as issue #566\n. @wjps, this will crash, because SelectPoller doesn't have _manage_event_state as a member.\nIf confirmed, this code path could also benefit from a test.\nOh yeah, I checked again, and nothing accesses _flush_outbound() on any of the pollers, so it should be okay to just delete SelectPoller._flush_outbound rather than bother with the fix:\n$ git grep _flush_outbound\npika/adapters/base_connection.py:    def _flush_outbound(self):\npika/adapters/blocking_connection.py:        self._flush_outbound()\npika/adapters/blocking_connection.py:    def _flush_outbound(self):\npika/adapters/select_connection.py:    def _flush_outbound(self):\npika/adapters/select_connection.py:    def _flush_outbound(self):\npika/adapters/twisted_connection.py:        self._flush_outbound call that was in _send_frame which previously\npika/connection.py:    def _flush_outbound(self):\npika/connection.py:        self._flush_outbound()\npika/connection.py:            self._flush_outbound()\ntests/unit/connection_tests.py:        self.connection._flush_outbound = mock.Mock()\ntests/unit/connection_tests.py:        self.connection._flush_outbound = mock.Mock()\nFYI, I logged this as issue #565\n. @wjps, but this _flush_outbound is inside the SelectPoller class, which is not subclassed from BaseConnection. I think that you may have accidentally pasted _flush_outbound() into SelectPoller class. The legitimate _flush_outbound is in SelectConnection.\n. @wjps, interestingly, after all these changes, it seems like IOLoop class is no longer needed, and could be replaced by a function that simply creates the appropriate poller.\n. This looks pretty similar to Connection._send_message. Is there any reason why a single implementation cannot be shared?\nCC @wjps \n. Why import math here? The one at the top of the file is the right and only place for it.\n. @everilae: Instead of copying this function from blocking_adapter_tests.py, let's have a single shared version instead. Perhaps it could be a function in the pika.compat module?\n. Good point, I took another look at as_bytes, and it makes sense to get rid of _p3_as_bytes and use as_bytes instead because of the bytes/str equivalence in python 2.x.\n. @everilae: the asymmetry still doesn't make sense to me. It's unintuitive that I may pass a string for q_name, but it would come back as bytes in the result and that the two would not compare equal in py3.x. I understand that bytes and str are not equivalent in 3.x, but this asymmetry in pika seems rather error-prone to me. For the life of me, I can't seem to find the definitions of basic AMQP types such as shortstr. Would you mind pointing me to a document on the web where they are defined? Is shortstr defined as UTF8 or just binary bytes in AMQP?\n. @everilae: according to the RabbitMQ folks on this page https://www.rabbitmq.com/blog/2014/04/02/breaking-things-with-rabbitmq-3-3/:\n\nThe AMQP 'shortstr' data type (which is used for things like exchange and queue names, routing keys and so on) is defined by the spec as being in UTF-8 format.\n\nIf they are correct, it means that all of these values should be going in as UTF-8 strings (str would be okay in python2.x, too) and combing back as UTF-8 strings, but not as bytes. This should be similar to how the json module handles strings: accepts UTF-8  (and/or str in python 2.x) for encoding, but returns UTF-8 strings by default when decoding. In python3, this would be both UTF-8 in and out, since the string type is UTF-8 in 3.x.\nCC @gmr \n. @everilae: regarding\n\nmethod is a protocol class instance and never available for users\n\nThis statement is incorrect. A number of pika functions and callbacks return the method to the user. For example, basic_consume's consumer callback is explicitly called with the args channel, method, properties, body. Same is true for basic_get, etc.\n. Thanks @everilae: I was looking for shortstr in that doc, and came up empty-handed. this is perfect! Since shortstr are UTF-8 and objects such as queue names, exchange names, routing keys are defined as shortstr, pika should accept strings for any shortstr fields from the user and return unicode strings for shortsr fields back to the user. There is not need for the bytes there at all, right?\nCC @gmr \n. Thanks for entertaining this thought. It should be much more robust to deal with all shortstr types as strings (or unicode strings). This would make equality tests work intuitively for those fields. (http://stackoverflow.com/questions/1864701/convert-utf-8-octets-to-unicode-code-points)\nOnly longstr should be bytes per AMQP definition that you sited. I am guessing that body is longstr, right?\n. \"content-type\" is for application use. Pika probably shouldn't get involved with that level of encoding/decoding ? Dealing with message \"body\" as bytes seems to be the correct level there.\n. To enhance debugging, I think it would be good to print the first few hundred bytes of the value at args[0], e.g.,\nreturn 'AMQP Short String can contain up to 255 bytes: %.300s' % self.args[0] or\nreturn 'AMQP Short String can contain up to 255 bytes: %.300s' % self.args (simple and safe, in case no arg was passed to constructor)\n(working on my first code review from 35000 feet, compliments of JetBlue Beta WiFly :) )\n. ~~Was the old code machine-generated? If it was, then future re-generation will result in loss of mods.~~ Just realized that you updated codegen.py - that's great!\n. If exiting due to SystemExit, KeyboardInterrupt, or other non-Exception-derived exceptions, we most likely don't want to perform anything graceful that slows down the exit of the app. Anyone want to chime in?\n. BUG: This is the wrong return value for __exit__. Returning anything that evaluates to True (like self in this case) will suppress the exception. We don't want to suppress the exception. To fix, remove the return statement from __exit__. See http://effbot.org/zone/python-with-statement.htm\n. @reddec: on second thought, let's keep them all the same; let's just call self.cose() in all cases. Sorry for the initial feedback - I was thinking out loud.\n. @reddec: we don't want the return isinstance(... statement here at all. This is because we don't want to suppress any of these exceptions in pika. This would prevent CTL-C from stopping the program. If user's code wants to suppress them, then user's code is free to do that on its own, but we can't do that in the library.\nLet's have just this in __exit__:\ndef __exit__(self, *_args):\n    self.close()\nand add the test:\n```\nclass TestConnectionContextManagerClosesConnectionAndPassesSystemException(BlockingTestCaseBase):\n    def test(self):\n        \"\"\"BlockingConnection: connection context manager closes connection and passes system exception\"\"\"\n        with self.assertRaises(SystemExit):\n            with self._connect() as connection:\n                self.assertTrue(connection.is_open)\n                raise SystemExit()\n    self.assertTrue(connection.is_closed)\n\n```\n. @shinji-s: also consider the possibility that one timer callback can unregister additional timers. If timer A callback unregisters timers B and C that were also ready, then B and C callbacks should not be called. So you need something like this to really fix the bug:\nfor t in sorted(to_run, key=itemgetter('deadline')):\n    k = hash(frozenset(t.items()))\n    if k in self._timeouts:\n        try:\n            t['callback']()\n        finally:\n            self._timeouts.pop(k, None)\n. Another surprising aspect of the legacy implementation is the unnecessary re-computation of the key. The hash key is available in self._timeouts, so the to_run = ... statement could also save the key along with timer as a tuple in order to avoid the re-computation here. No need to do anything about it in this PR, but might be a nice clean-up change in a separate PR later.\n. We shouldn't need to inject test code into production code.\n. Please check your changes with pylint and fix findings.\n. Need a short docstring that describes what this is testing. \"test_timer_for_suicide\" is not meaningful to other people. Also, see my comment on test_timers_for_murder about using longer, more descriptive method names.\n. The use of \"suicide\" and \"murder\" in method names is a bit unpleasant. Could you use longer, more descriptive function names that don't involve those ominous words? In unittest, it's preferable to use longer, descriptive method names for test methods. For example: test_timer_handler_deletes_own_timer and test_timer_handler_deletes_another_ready_timer and similar for the timer handler names.\n. Let's revert this change (callback ...) in start. It's not necessary, and breaks the intended behavior of raise AssertionError('Test timed out') when the test times out. I will explain why it's not necessary in other comments in this code.\n. Please get rid of SelectPoller.is_ready_to_fire from SelectPoller class, this patch, and the \"This patch delays the execution...\" comment above. They are not needed to test this functionality. The following should suffice to ensure that the timers will be ready in the same invocation of SelectPoller.process_timeouts():\n1. Initialize self.deleted_another_timer = False\n2. Create timer 1: 0.01 seconds: callback is closure (function in function) on_timer_that_deletes_another_ready_timer. This callback should delete timer 2 and leave a breadcrumb in self (e.g., self.deleted_another_timer = True)\n3. Create timer 2: 0.02 seconds: callback is closure on_deleted_timer. This callback should call self.fail('deleted timer's callback called')\n4. Create timer 3: 0.03 seconds: callback is closure on_conclude_another_timer_deletion. This callback checks self.assertTrue(self.deleted_another_timer), checks self.assertNotIn(timer2_id, self.ioloop._timeouts), and calls self.ioloop.stop().\n5. Call time.sleep(0.03). This ensures that timers 1 and 2 will be ready when SelectPoller.process_timeouts() is called.\n6. Call self.start() to start the test\nFor example:\n```\ndef test_timer_handler_deletes_another_ready_timer(self):\n    self.deleted_another_timer = False\ndef on_timer_that_deletes_another_ready_timer():\n    self.ioloop.remove_timeout(timer_2)\n    self.deleted_another_timer = True\n\n    def on_conclude_another_timer_deletion():\n        self.assertTrue(self.deleted_another_timer)\n        self.assertNotIn(timer_2, self.ioloop._timeouts)\n        self.ioloop.stop()\n\n    self.ioloop.add_timeout(\n        0.03,\n        on_conclude_another_timer_deletion)\n\nself.ioloop.add_timeout(\n    0.01, \n    on_timer_that_deletes_another_ready_timer)\n\ndef on_deleted_timer()\n    self.fail('deleted timer's callback called')\n\ntimer_2 = self.ioloop.add_timeout(\n    0.02,\n    on_deleted_timer)\n\n# This sleep ensures that timers 1 and 2 will be both ready when \n# `SelectPoller.process_timeouts()` is called.\ntime.sleep(0.03)\n\nself.start()\n\n``\n. Change toself.start(). Deletecheck_none_is_left- this timer callback is not needed any longer\n. Bug:self._next_timeout = Noneused to be inside thefor t in sorted ...loop. It's a bug to have it here outside of the loop, because we don't know whetherto_runlist is empty or not at this point. Make it the first statement in your newfor k, timer ...loop to make sure that it will be executed ifto_run` is not empty:\n```\nfor k, timer in to_run:\n    self._next_timeout = None\nif k not in self._timeouts:\n    # Prior invocation must have deleted this timer\n    continue\n...\n\n``\n. Remove theif self.is_ready_to_fire(to_run):check; it pollutes the production code, and it's not necessary for making the unit tests work. See my comment ontest_timers_for_murderthat describes how to implement the test without this.\n. Please run pylint and fix problems introduced by your change in both scripts. For example, since you replaced the only user ofitemgetter, pylint will complain that import ofitemgetteris unused.\n. @shinji-s: Something like:W0201: Attribute 'timer_stack' defined outside initshould be disabled globally in this script. Withunittest, you typically don't declare your owninit` in the test case classes. Since this is normal behavior for unittest class implementations, disable this warning globally in the specific script. I tend to put the disable-warning pragmas between the module's docstring and the first import for visibility:\nUsing this script as an example:\n```\n-- coding: utf8 --\n\"\"\"\nTests for SelectConnection IOLoops\n\"\"\"\npylint: disable=W0201\nimport logging\n...\n```\nPlease note that it is generally bad practice to disable pylint warnings globally like this (okay in this case, though), because it could hide potential bugs. In most cases, you need to fix the code; in very special cases, where you really need to disable a pylint warning for a good reason, you would disable it on a case-by-case basis; you can put the pragma at the end of a line to disable the warning on a specific line of code;  you can also use disable/enable pragmas around a block of code, if necessary.\n. @shinji-s:\n\nOr the comment \"Please check your changes with pylint and fix findings.\" isn't necessarily directed to the diff that the comment is attached to?\n\nThis comment was attached to the change:\n+        self.handle = self.ioloop.add_timeout(\n+            0.1, partial(self.on_timer_suicide,handle_holder))\nI noticed the missing space after the comma here: self.on_timer_suicide,handle_holder\n. @shinji-s: I am not sure what you're asking in this question:\n\nAnother thing is I don't see pylint complain about the use of assertEqual().\n\nWhy would you expect pylint to coplain about assertEqual().?\n. Thanks for adding the comments to explain the pylint pragmas.\n. Unnecessary overhead: no need for the list [(k, timer) .... sorted accepts generators/iterators, too. So, this would produce the equivalent result without the unnecessary overhead of instantiating/populating  the list:\nto_run = sorted(((k, timer) for (k, timer) in self._timeouts.items()\n                 if timer['deadline'] <= now),\n                key=lambda item: item[1]['deadline'])\nThe original code had this problem, too, so fixing it in your PR is technically out of scope.\n. Spelling: test_timer_delete_anothoer --> test_timer_delete_another\n. Hmmm... interesting and somewhat unintuitive/unexpected outcome :)\n. Do the changes pass all pylint checks?\n. The use of spaces is non-standard. I would expect to see something like:\nselect.error: lambda e: e.args[0] == errno.EINTR,\n    OSError: lambda e: e.errno == errno.EINTR,\n    IOError: lambda e: e.errno == errno.EINTR,\n. This could benefit from a docstring, taking care to document the arg; something like\n```\n    \"\"\"\"\n:param exc: exception; must be of one of the exception classes in \n    _SELECT_ERRORS\n\"\"\"\"\n\n``\n. Why are you printing the result of basic_ack? Does it return anything interesting? No. That's not what the original code did.\n. Should beimport osand place it beforeimport sys ...(alphabetically)\n. Should beos.nameinstead ofplatform.os.name`\n. Interesting: On Python 3.x (tested on ~~3.5~~ 3.4), they are all aliases of OSError:\nIn [12]: _SELECT_ERROR_CHECKERS = {\n    select.error : \"select.error\",\n    OSError      : \"OSError\",\n    IOError      : \"IOError\"\n    }\nIn [13]: _SELECT_ERROR_CHECKERS\nOut[13]: {OSError: 'IOError'}\n. Bug: To guarantee consistency, _SELECT_ERRORS should be based on _SELECT_ERROR_CHECKERS. This way, it can also pick up InterruptedError, which it's presently missing. E.g.,\n_SELECT_ERRORS = pika.compat.dictkeys(_SELECT_ERROR_CHECKERS)\nDo this after _SELECT_ERROR_CHECKERS[InterruptedError] = lambda e: True\n. Need to put all these pylint pragmas on the same line as the imports, so they don't disable checks on all the other imports; e.g.,\nimport unittest2 as unittest  #pylint: disable=F0401\n. Don't need any of this remove_timeout logic here at all. Instead, use addCleanup to register cleanup call after creating fail timer in start(). This make the code more readable. See my comment in start()\n. Instead of the messy remove_timeout logic in tearDown, do this in start() instead:\nfail_timer = self.ioloop.add_timeout(self.TIMEOUT, self.on_timeout)\n        self.addCleanup(self.ioloop.remove_timeout, fail_timer)\nfail_timer can now be a local variable and there is no need to have the instance member self.fail_timer\n. Thanks for adding the docstrings\n. Use a #pylint: disable=... pragma on the def line instead for silencing pylint.\n. Use a #pylint: disable=... pragma on the def line instead for silencing pylint. Same in the other cases where you added more code for silencing pylint due to unused args. We want to keep the function's implementation cleaner.\n. This appears to implement the same functionality as threading.Timer class. Use threading.Timer instead of implementing this class. See https://docs.python.org/2/library/threading.html#timer-objects\n. Why derive from IOLoopBaseTest? This class is not using any of the core functionality from IOLoopBaseTest except the value of TIMEOUT, so it's not a \"IOLoopBaseTest\". Just derive from unittest.TestCase and reference IOLoopBaseTest.TIMEOUT where you need to use the same TIMEOUT.\n. It's more obvious and reliable to schedule cleanup right after registering the signal instead of having the cleanup code somewhere else. It's better for code readability, too:\noriginal_signal_handler = signal.signal(\n    signal.SIGUSR1,\n    self.signal_handler)\nself.addCleanup(signal.signal, signal.SIGUSR1, original_signal_handler)\n. Use threading.Timer instead and no need for the join calls in finally:\nkiller = threading.Timer(0.1, lambda: os.kill(os.getpid(), signal.SIGUSR1))\nself.addCleanup(killer.cancel)\nwriter = threading.Timer(0.2, lambda: sockpair[1].send(self.MSG_CONTENT))\nself.addCleanup(writer.cancel)\nkiller.start()\nwriter.start()\n. Use the following standard unittest functionality instead of raising AssertionError explicitly: self.fail('Eintr-test timed out')\n. Don't use the pylint pragma globally, because it disables the checks in other code. Use it on the affected functions or code lines directly instead.\n. Use unittest.skipUnless decorator instead of this check/return. It will automatically print a nice message instead of quietly skipping the test:\n@unittest.skipUnless(pika.compat.HAVE_SIGNAL,\n                     'This platform doesn't support posix signals')\n@PATCH_('pika.adapters.select_connection._is_resumable')\ndef test_eintr(\n    self,\n    is_resumable_mock,\n    is_resumable_raw=pika.adapters.select_connection._is_resumable):\n. @shinji-s: sorry for the confusion. What I was concerned about is what would happen on python 3.x, where all those different error classes are reduced to the single OSError class. I don't want select.error : lambda e: e.args[0] == errno.EINTR to win and be the only one left at the end, because we should be looking at e.errno in OSError, not e.args[0]. I am hoping that python evaluates the dict properties in order so that in our case, the last one in _SELECT_ERROR_CHECKERS would win out.\nThe experiment in my comment was performed via https://www.python.org/shell/ (python 3.4.0) to demonstrate the reduction of all those classes to OSError, and which property wins out.\nIf python guarantees the processing of the dict properties in the specified order, then we should probably have OSError      : lambda e: e.errno == errno.EINTR at the end to make that explicit and also add a comment in _SELECT_ERROR_CHECKERS that explains that \"all these error classes are reduced to the single OSError class in 3.x\"\n. @shinji-s: Regarding\n\nAnd we are supposed to get no EINTR exceptions raised in Python 3.5 as the standard library performs retries. Some of the testes here should fail.\n\nLet's add the appropriate unittest.skip* decorator to your test(s) to make sure that the one(s) that are not appropriate for Python 3.5+ will be skipped on 3.5 and later.\n. @shinji-s: t__ is not a good idea; it's not conventional. If you really needed a longer variable, it would have been better to change it to timer in this case. However, since the variable in this case is only used to start the timer, you could get rid of it altogether: threading.Timer(0.1, self.ioloop.stop).start() is all you need here\n. Thanks @shinji-s. I stand corrected. I also discovered that with my pylint version 1.4.4, pylint is smart about the ImportError exception. In the following code block, it only complains F0401 about import halbhalbhalb, but doesn't complain about import blahblahblah\n```\ntry:\n  import blahblahblah\nexcept ImportError:\n  import datetime\nimport halbhalbhalb\n```\n. @shinji-s: without our discussion, I don't think I would understand the implication of this comment. Perhaps this comment is not needed.\nOn 3.x, the number of elements will get reduced automatically, because _SELECT_ERROR_CHECKERS will get collapsed to just the two properties: OSError and InterruptedError\n. @shinji-s: \nIn [2]: isinstance(InterruptedError(), OSError)\nOut[2]: True\nIn [3]: InterruptedError.__bases__\nOut[3]: (OSError,)\nDo we need InterruptedError at all?\n. good point!\n. Need to stay within 80 characters per line. This looks like it could be longer. Check with pylint to see what else it might uncover.\n. @gst, we do unit/mock tests in the tests/unit/ directory. The acceptance tests are performed with real connections or connection attempts.  We already had a similar acceptance test TestConnectWithDownedBroker. I extended it to support 1 and 2 connection attempts: https://github.com/vitaly-krugl/pika/commit/81cbba61f4d8db1fd6c1db1650fbfe3acb0390df. I also documented the specific exception in BlockingConnection.__init__. Please feel free to merge my acceptance test change into this PR.\n. This method is small and the new _check_open_error method is only used from within this method. The code would be more readable to leave the old logic in place and simply make the call to _flush_output conditional on self._open_error_result.ready, like this:\n```\nif not self._open_error_result.ready:\n    self._flush_output(self._opened_result.is_ready, ...\nif self._open_error_result.ready:\n    exception_or_message = self._open_error_result.value.error\n    ...\n```\n. A commonly-used practice is to group imports as follows for readability:\n1. Imports of builtins\n2. Imports of 3rd party modules\n3. Imports of our own modules\nSo, this new pika.exceptions import belongs with the other pika imports below\nAlso, all other code in this test suite accesses pika exceptions relative to their namespace, so to follow the established convention, your code would access it as pika.exceptions.ConnectionClosed. However, we still need an explicit import import pika.exceptions after from pika.adapters import .... The original implementation forgot to add that import and got by implicitly.\n. It's easier on the eyes (for some developers, at least :) ) if the imports within each group are sorted, so in this case, you might have:\nimport errno\nimport socket\n. This comment seems incomplete: to be able ... to do what?\n. This relies on actually being able to connect to a live broker. Unit tests are not allowed to do that. All external networking needs to be mocked out as in the other unit tests. Unit tests are expected to work even when the external servers are not running.\nI think that one of the challenges here is that the fix is in pika.connection.Connection, but this PR is trying to unit-test that fix from several layers higher in pika.adapters.BlockingConnection unit tests. It would be more natural to implement a test in https://github.com/pika/pika/blob/master/tests/unit/connection_tests.py against Connection.channel.\nIn unit tests, knowledge should be constrained at the interface to the layer immediately below the layer that you're testing. Unit tests get implemented at each layer, so the test of each higher layer may trust and treat the lower layer(s) as black boxes.\nI was surprised that the acceptance test https://github.com/pika/pika/blob/94f86187bf8cbd6cdc18a52f17c564fbb2c78169/tests/acceptance/blocking_adapter_test.py#L196 was not reproducing the KeyError exception, and was hoping to find some way to make it do so. However, it turns out that once the connection is closed on the \"remote end\", it often takes more than one send request on the \"local end\" to get the exception from the send call.\n. We can probably handle mock testing at each layer without introducing a new dependency. It doesn't seem to be documented or even very active. I don't  I don't know whether Gavin (@gmr) would agree to this dependency. You might want to check with him before investing the time.\n. Which platforms don't have SO_LINGER?\n. @gst, this is not used now, please remove _check_open_error.\n. @gst, thank you for pointing this out. I had no idea about this change in Python v3.x.\nIdeally, I could use six.add_metaclass or six.with_metaclass, but pika has traditionally resisted external dependencies in its core code.\n@gmr: is dependency on six off-limits in pika?\n. @gst, I fixed this using exec on python 3. If Gavin allows dependency on six, then I will do it the six way.\n. @gmr, in order to make use of abc.ABCMeta in Python 3, we need to use the new Python 3 syntax that's not supported in Python 2. That new syntax (MyClass(metaclass=abc.ABCMeta)) prevents the module from loading when running in Python 2. To make it work without six, I had to wrap the Python 3 variant of the code in the exec() function so it won't break loads under Python 2, which isn't ideal (https://github.com/vitaly-krugl/pika/blob/897624da1fa6c9aeeb854aca3a9abf5dd4dcf148/pika/adapters/select_connection.py#L170). I am not sure how else to overcome this compatibility issue.\n. @gst, this line will likely have a minor merge conflict with the same line in your PR #678. I needed this here in order to get a clean build.\n. @quantum5, I found a builtin module that may provide the same functionality in a portable way without necessitating any pika.compat logic:\n```\nimport numbers\nif not isinstance(delivery_tag, numbers.Integral):\n    . . .\n```\nI think this is good enough without duplicating the functionality via pika.compat.is_integer()\n. @quantum5, without your register example, I get False on python 2.7.10 and 3.5. Why would someone do numbers.Integral.register(type(None))? Isn't it self-destructive or malicious?\n2.7.10 (default, Jul 14 2015, 19:46:27) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)]\nPython Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nimport numbers\nnumbers.Integral\n<class 'numbers.Integral'>\nisinstance(None, numbers.Integral)\nFalse\n. Thx\n. @awelzel, thanks for the feedback; implemented.\n. Couple of problems here:\n1. in python, it's a \"dict\", not \"json object\"\n2. \"json object\" is two words; only one word types are allowed here per rst syntax.\n. How would someone who needs URLParameters make use of this new feature?\n. What about integration with the Parameters class, as is the case for all other args, including DEFAULT_* and _validate_*?\n. Moved remove_timeout ahead of the callback to make sure it gets cleaned up, even if callback raises; eliminates need for try/finally\n. this old code could lead to the same timer ending up multiple times on self._stopped_timers deque if user removed it multiple times or if user removed it after the timer callback was called (which also removes it)\n. Assertion here doesn't help, since ioloops in some adapters, such as LibevConnection, suppress the exception. PR #678 will handle test timeout in a robust way, once it's merged.\n. @gst, please be careful to preserve these changes in on_timeout, too, when merging #678 that modifies the same method. I asked @gmr to merge this PR #683 ahead of the other PRs so that the other PRs may be re-merged with master after #683 to verify clean builds in all of them.\n. @gst: I meant that if github forces you to merge, we will need both: your changes from #678 and my changes from this PR.\n. This should probably use modern-style class declaration and be derived from object.\n. If this class could be made compatible with the other async adapters, such as SelectConnection, TornadoConnection, and LibevConnection, then it could easily integrate with and take advantage of the existing acceptance testing logic in tests/acceptance/async_adapter_tests.py\n. @gst: this merge went bad. We need this line on entry to on_timeout(self), right after the logger statement:\nself.timeout = None  # the dispatcher should have removed it\n. @gst, let's have a single error log message that gives us useful information for debugging; e.g.,\nself.logger.error('%s timed out; on_timeout called at %s', \n            self, datetime.utcnow())\nand get rid of self.logger.error('on_timeout') below\nSo, together with the above comment concerning self.timeout = None, the new on_timeout(self) code might look like below. Note that my earlier merged PR also removed raise AssertionError('Test timed out') for several reasons: 1. some adapters (e.g., Twisted adapter) suppresses exceptions on callbacks; 2. It circumvents your new logic flow for detecting timed-out tests:\n```\n    def on_timeout(self):\n        \"\"\"called when the test times out\"\"\"\n        self.logger.error('%s timed out; on_timeout called at %s', \n            self, datetime.utcnow())\n    self.timeout = None # the dispatcher should have removed it\n    self._timed_out = True\n\n    # Initiate cleanup\n    self.stop()\n\n``\n. Theself.assertDictEqualcall below is unreachable becauseblocking_connection.BlockingConnection()raises an exception with the given setup. Same applies totest_client_properties_override.\n. BlockingConnection doesn't have the_client_propertiesmember, since it's not subclassed from theConnectionhierarchy. The Connection hierarchy is reachable viaconn._impl. Same applies totest_client_properties_override.\n. I think that it might suffice only to test that the expectedclient_propertiesvalue was passed to the next layer (SelectConnection constructor). With unit tests, it might be better to isolate testing to the given layer/function. For this, as intest_sleep, simply patchSelectConnectionand_process_io_for_connection_setup, then callblocking_connection.BlockingConnection(), and finally useselect_connection_class_mock.assert_called_once_with()with the appropriate args to verify that the expectedclient_properitesvalue was passed to SelectConnection (it would suffice to usemock.ANYfor all args exceptclient_properties`, if that makes it easier).\nSame applies to test_client_properties_override\nWe need to have a version of this really simple unit test in each of the other adapter layers affected by this PR: SelectConnection, BaseConnection, TornadoConnection, LibevConneciton, and TwistedConnection.\n. Similarly to my blocking_connection_test feedback, this layer's unit test should avoid duplicating the logic from connection_test by simply testing to make sure that the expected connection_properties value was passed to the next layer (BaseConnection constructor)\n. Interestingly, this seems somewhat messed-up for our purposes, too:\nisinstance(True, int)\nTrue\nBut I wouldn't worry about it.\n. @gmr, I am not sure whether I understand your question correctly: I considered abstracting this repr to the Connection class, but then realized that the stack is designed such that Connection doesn't know anything about sockets.\nThe second point regarding\n\nif it matches the format in the RabbitMQ management plugin.\n\nI think you're recommending to check how the management plugin represents connections visually and match their format, right?\n. @gmr, I considered removing the BlockingChannel.__int__ (and Channel.__int__?) methods. Should I worry about breaking user code that relied on this shorthand way of getting the channel number for \"%i\" or similar in logs, exception messages, etc.?\n. @gmr: I removed the parameters from the ConnectionParameters class docstring, because the exact same information was in the constructor's docstring in ConnectionParameters.__init__. It was the usual pain point of having to maintain the same content in multiple places. What is the benefit of having the exact same information in both the class and constructor docstrings?\n. REVIEWME - removed the deprecated type arg\n. REVIEWME above clarification of callback behavior.\n. REVIEWME basic_cancel changes below\n. REVIEWME Channel.close changes below\n. REVIEWME Do you agree with this TODO?\n. REVIEWME _on_close changes below\n. REVIEWME new dedicated function for use by Connection class when connection is lost abruptly and channels remain that need to be cleaned up.\n. REVIEWME passes cached reply_code/reply_text to callback.\n. REVIEWME removed \"pending deliveries\" support that was in support of legacy BlockingConnection and is no longer needed.\n. REVIEWME change in _on_openok corresponding to this comment about suppressing.\n. REVIEWME Do you agree with the concern in the above TODO? It seems like pika should raise an exception if the callback is None. \n. REVIEWME Any comment on this TODO?\n. REVIEWME Added state names for use in __repr__\n. REVIEWME Initialize attributes in constructor that were previously initialized only elsewhere.\n. REVIEWME do not allow channel to be created on connection that hasn't completed transition to OPEN state to avoid race condition with connection setup.\n. REVIEWME Now also initiates closing of channels that are in OPENING state (in addition to those in OPEN state). Also doesn't force-close channels that are in CLOSING state, allowing them to finish closing. \nThe legacy behavior lead to a race condition whereby frames could arrive on a channel that was prematurely removed by this function.\n. REVIEWME There should be no such thing as \"out-of-band delivery\". if it happens, something is seriously wrong with either the broker's or pika's implementation.\n. REVIEWME _handle_content_frame doesn't return anything explicit, and neither does _deliver_frame_to_channel\n. REVIEWME logic in this block makes sure that all channels completed closing before continuing with closing of the connection.\n. REVIEWME now calls the dedicated channel method _on_close_meta instead of overloading the logic of _on_close, which should be dedicated to handling Channel.Close frames from broker.\n. REVIEWME use more meaningful variable names; also makes pylint happy\n. REVIEWME removed write-locking, since Connection is not thread-safe by design, and a number of other accesses to the output buffer weren't guarded anyway.\n. \"pending\" functionality was removed by this PR\n. the deprecated type arg was removed by this PR\n. corresponding functionality was removed by this PR\n. corresponding functionality was removed by this PR\n. Moved to a sorted position in script, similar to most other methods here.\n. Prior to this PR, there was a discrepancy between the old value (32768) of this constant and the hard-coded value (65535) used by the old \"channels_max\" validation logic in connection.Parameters.\n. RabbitMQ actually interprets the heartbeat value that we pass it in Connection.Tune as a timeout rather than an interval between heartbeats. RabbitMQ will send several Heartbeat frames during each heartbeat-timeout period.\n. @gmr, regarding\n\nWhy the tuple and trailing comma?\n\nin raise exceptions.ChannelClosed('Already closed: %s' % (self,))\nIt's a good question - I agree that unnecessary data structures are undesirable. It's an automatic habit that I got into owing to this construct's error-prone nature. For example,\n```\nobj = [1,9]\nraise Exception('the list %s is too short; length=%d', (obj))\n```\nIn this case, I intended to have (obj, len(obj)), but \"accidentally forgot\" to add len(obj). Because obj is a list, this omission will not be caught by pylint and the interpreter. When I was first learning Python, someone recommended always making it a tuple to avoid this type of error. Since things like exceptions are, well, exceptional, performance is not an issue.\nPersonally, I now prefer the format construct - e.g., raise Exception('the list {} is too short; length={}'.format(obj, len(obj)), which I think eliminates the error-prone situation above (pylint should detect it unambiguously in such cases). Would you object if we started using '...'.format(...) instead of '...' % (...) in future changes?\nOf course, this should not be a problem with self in pika classes, because they are not iterables, so I will push a new commit to replace uses of (self,) with self\n. @gmr, regarding\n\nIIRC _rpc doesn't handle the full Basic.GetOk method, header, and body frames.\n\nGood point about how pika's _rpc implementation works at present. However, since Basic.Get is defined by AMQP as synchronous, it should be treated like all the other synchronous requests and not get ahead of the proverbial queue. I had a long back-and-forth about the semantics of Basic.Get with @michaelklishin (pivotal/RabbitMQ), and he beat into me that we should treat Basic.Get like all the other synchronous requests, and made a believer out of me.\nAn additional important benefit of treating Basic.Get synchronously is that pika would then be able to allow multiple outstanding Basic.Get requests against different or same queue on the same channel and be able to do the right thing. Pika's present non-synchronous implementation of Basic.Get breaks when you attempt to make an additional basic_get call before the prior one completes (even if the second request is against a different queue). This is because there is only one Channel._on_getok_callback instance member, and Channel can't properly handle multiple outstanding Basic.Get requests this way.\n. @gmr, regarding\n\nShouldn't this come after the check on 1031?\n\nNo, it should not. Otherwise we would be subject to the race condition described in the code comment immediately following if self.is_closing: on 1031. Per AMQP 0.9.1, each peer must respond to the other peer's Channel.Close. This is what this specific compound scenario looks like:\npika: sends Channel.Close to broker\nbroker: sends Channel.Close to pika client at about the same time (e.g., in response to error from prior client request on this channel)\npika gets Channel.Close from broker and is required to reply to broker  with Channel.CloseOk per AMQP 0.9.1.\nbroker gets Channel.Close from pika client and is required to reply with Channel.Close.Ok (even if the broker already sent Channel.Close for some reason) \nSince pika client sent Channel.Close earlier, it should expect to get for Channel.CloseOk from the broker per AMQP 0.9.1, so pika client still needs to wait for Channel.CloseOk from the broker in order to avoid the race condition that is described in the code comment below.\nThat's why self._send_method(spec.Channel.CloseOk()) needs to come before the check if self.is_closing on 1031.\n. > Style nit: redundant parentheses\nWill be addressed in subsequent commit into this PR\n. @gmr, regarding\n\nHmm I seem to remember something about consuming with noack=True with messages in the socket recv buffer prior to the Channel.Close being issued. Even with a high QoS prefetch, could you not have frames that have yet to be decoded when you go to close a channel?\n\nThis would indicate protocol error either in client or in broker. Here is why: value.channel_number in self._channels should continue to evaluate to TRUE until the channel is fully closed: Channel.Close to broker and then matching Chanel.CloseOk from broker or an unsolicited Channel.Close from broker. In either case, there should be no more messages destined to the channel after the channel receives either  Channel.CloseOk or Channel.Close from the broker, because the Channel.CloseOk or Channel.Close from the broker effectively flush the channel.\nIt turns out that there used to be a bug in pika that caused pika to remove a Channel prematurely in one condition, allowing such a race condition as you described to occur. I fixed that bug in this PR. The bug was in Connection._close_channels. There is a REVIEWME-tagged comment in this PR that describes the problem and the fix. Duplicating it here for completeness:\nRegarding fix in Connection._close_channels:\n\nNow also initiates closing of channels that are in OPENING state (in addition to those in OPEN state). Also doesn't force-close channels that are in CLOSING state, allowing them to finish closing.\nThe legacy behavior lead to a race condition whereby frames could arrive on a channel that was prematurely removed by this function.\n. @gmr, regarding\nIIRC the original implementation was fairly agnostic to which side of the conversation it was on.\n\nDo you mean that the original implementation implemented both Client and Broker?\n. @gmr, regarding\n\nJust to reiterate, I think there is a case where you could have a pile of frames on the stack that are unprocessed when a Channel.Close is emitted. My thinking is that issuing the close should be similar to Basic.Cancel. I guess one could make the argument that both should not be respected until their respective Ok frames are received though.\n\nIn response to your other comment, I provided a detailed explanation of why that should not be possible with Channel.Close and also described a race condition bug that I fixed in this PR that used to allow for that protocol breach in pika. As a quick summary for completeness here, pika will not remove the channel number from Connection._channels until it receives the proper acknowledgement from the broker. Therefore, it would be impossible to receive frames for that channel after it's removed from Connection._channels - the broker will not send any more messages on the channel after sending Basic.CloseOk or Basic.Close to the client on that channel. It would be a violation of the AMQP protocol if it sent more frames after sending Basic.CloseOk or Basic.Close on the same channel.\nRegarding your comparison with Basic.Cancel, there is one important difference between it and Channel.Close: Channel.Close is only supported in synchronous mode - i.e., the broker must respond with Channel.CloseOk according to AMQ 0.9.1. On the other hand: although Basic.Cancel is defined a synchronous, it may be used asynchronously by setting nowait=True (unfortunate in my opinion), therefore eliminating Basic.CancelOk, so there is no simple way to tell when the Consumer is fully closed. As the result, pika.Channel ends up leaking some data structures (in Channel._cancelled, I think). In any case, Channel.Close flow doesn't have the problem that Basic.Cancel has. (with Consumers, we could probably safely clean up the leaked consumer-related object(s) after the next syncronous Channel._rpc request completes on that channel, but that's another story for another day :) )\n. @gmr, regarding\n\nIIRC xrange has some portability issues -- pypy? I don't recall exactly what the issue was that had me go back to it.\n\nxrange is used in a number of places in pika via pika.compat.xrange. See from pika.compat import (xrange, ... at the top of the file.\n. @gmr, regarding \n\nAt this point, why not from pika import compat? I don't consider this a blocker, just more of a thought about the coding style of importing a bunch of methods like that.\n\nI agree that this would be better (using the compat namespace), but I stuck to the established precedent in this file. See on previous line: from pika.compat import xrange, basestring, url_unquote, dictkeys. I felt that I should either change all the existing use here or stick to the established precedent for consistency. I chose the latter (perhaps unwisely?)\n. @gmr, regarding backpressure_detection\n\nI wonder if we can/should remove this soon given connection.blocked has been around for some time at this point.\n\nI agree. I am adding DEPRECATED comments on it in ConnectionParameters and URLParameters and also in docs/version_history.rst. I am tagging on this minor change to PR #712 that we're still discussing.\n. @dschep This is a unit test that uses mock, not an acceptance test. Therefore it belongs under tests/unit/, but not under tests/acceptance/, which establish live connections with the broker. Also, there is an existing unit test script for Channel unit tests, where these things belong: https://github.com/pika/pika/blob/master/tests/unit/channel_tests.py\n. @dschep: there is no need to check if self._on_getok_callback is not None before setting self._on_getok_callback = None. The check is redundant. Just setting self._on_getok_callback = None would suffice.\n. BlockingConnection is synchronous, so I wouldn't expect that it should ever see exceptions.ChannelAlreadyClosing, which is transient, percolating to this level. How are you able to reproduce exceptions.ChannelAlreadyClosing here?\n. It may be confusing/misleading to have something here that's not supposed to happen. Error-handling code, like other code, often gets copied, and unnecessary stuff (if indeed unnecessary) tends to spread around and add to confusion.\n. The combination of the word \"with\" with the other words in these new test names results in somewhat confusing names. For example, \"test_with_close\" raises the question \"test what with close?\" \ud83d\ude03 \nIt's desirable to use descriptive names, since that's what will show up in the test logs. So, the more information in the test name, the better. In tests, it's acceptable to use very long names, when needed. For example, test_with_close, might be more clear if named something like test_context_manager_exit_with_closed_channel. test_with_raise could be test_context_manager_does_not_suppress_exception, and test_with could be test_context_manager\n. These tests were renamed, updated to call Connection._on_connect_timer instead of Connection.connect, and moved here to follow the scripts (somewhat inconsistent now) test method sort order.\n. Is it practical to create an adapter that wraps around the new Asyncio adapter and presents an API that is compatible with SelectConnection, which could then be plugged into the existing async acceptance test suite? If not, then a dedicated comprehensive test suite is needed for this adapter, just as there is one for BlockingConnection. The comprehensive test suite needs to include both acceptance and unit tests with good quality coverage. This is necessary for maintainability of the Asyncio adapter.\n. What are you trying to accomplish? This is not an adequate explanation.\n. How does this help it not wait forever? This won't help the while loop exit, will it?\n. This change also makes it so that there is a single code path for connection-establishment logic and the corresponding success/error-reporting logic, regardless of whether it's the initial connection attempt or a reconnection attempt. Fewer code paths makes the code less error-prone and simplifies testing.\n. @mosquito, please model the constructor args on TornadoConnection/SelectConnection for consistency. This should also facilitate easy integration into the async adapter acceptance test framework. See \n- https://github.com/pika/pika/blob/43936088e1183ec5902f2ba2992c902e87f5460b/pika/adapters/tornado_connection.py#L30-L35\n- https://github.com/pika/pika/blob/43936088e1183ec5902f2ba2992c902e87f5460b/pika/adapters/select_connection.py#L68-L73\n. The code in base connection never calls _create_tcp_connection_socket without sock_type and sock_proto args, so don't need the default arg values. Please get rid of them.\n. It's confusing to have default values when the code has no intention of relying on them.\n. Pls add param description in the docstring for the new args.\n. Conceptually, this change makes sense. I think the same was done in Haigha some time ago, too.\n. I think there is more to it than this. stop_consuming sends a message to the broker and expects a response callback. If you stop the ioloop now, the response won't arrive and the response callback won't be handled. More needs to be done to make these states work correctly. See https://github.com/pika/pika/pull/710/files for an example of cleanup in the Async Publisher code, which is quite similar to the consumer code.\n. LGTM. Thanks. @lukebakken - WARNING this assignment will break application code that has its own check and workaround for missing socket.socketpair, particularly when the application needs to create a socket pair with parameters not supported by this limited implementation. \nThis is why we originally contained that logic inside the internal method _get_interrupt_pair and didn't set attributes in the socket module.. @lukebakken - WARNING The original code returned a non-blocking socket pair, but the pair is blocking after this PR. The change from non-blocking to blocking can lead to a deadlock. The implementation was designed for non-blocking model - see https://github.com/pika/pika/blob/2b145f4e40e4fa2dda3b5e94dba8a86fbaf83319/pika/adapters/select_connection.py#L539 and https://github.com/pika/pika/blob/2b145f4e40e4fa2dda3b5e94dba8a86fbaf83319/pika/adapters/select_connection.py#L651.\nA simple/pathological case that would reproduce the deadlock is calling stop enough times from code executing within the event loop itself to fill the socket's transmit buffer.\nA more complex - albeit still possibly pathological - case is where a background thread or many background threads call stop, while the event loop code is blocked waiting for something from the thread that happens to make the stop call that would exceed the _w_interrupt socket's transmit buffer.\nIs there any reason for removing the logic that made the sockets non-blocking?. My 2 cents: this function's name is exactly like the real deal in socket module which returns blocking sockets; if some other part of pika or pika's user use it, they might expect that it behaves just like the one in socket. For this reason, I would add the logic that makes the sockets non-blocking in the specialized method _get_interrupt_pair augmented with a comment, such as \"make sockets non-blocking to avoid potential deadlock\".. Add a function comment that explains the reasons for having this method - Windows doesn't provide socket.socketpair.. +1. Hi @gmr, thanks for chiming in. I remember that too. That was before BlockingConnection was rebased on SelectConnection. That legacy blocking socket implementation used to periodically try to read incoming data in the midsts of queuing of outbound message frames which could inject frames (e.g., heartbeat ack), thus corrupting the message being sent. I fixed that as part of the big refactoring effort a few years ago.. @cainbit -- The name libev_test is already used by the test function above. Should consumer_cb be just callback for consistency with the naming convention being implemented in this PR?. Also:\nadd_timeout(self, deadline, callback_method): callback_method needs to be renamed callback\npika.Channel. add_callback(self, callback, replies, one_shot=True)\nIn call to `self._impl.basic_get(callback=get_ok_result.set_value_once, queue=queue, no_ack=no_ack), consider moving callback arg to the end for consistency.\nIn call to self._impl.queue_unbind(callback=unbind_ok_result.set_value_once,\n                                    queue=queue,\n                                    exchange=exchange,\n                                    routing_key=routing_key,\n                                    arguments=arguments)\nIn call to self._impl.basic_recover(callback=recover_ok_result.signal_once, ....\nin call to self._impl.flow(callback=flow_ok_result.set_value_once, ...\ncallback.CallbackManager. add(self, prefix, key, callback, one_shot=True, only_caller=None, arguments=None)?\nOne consideration is ease of use. For example, moving callback to the end of callback.CallbackManager. add could make it more cumbersome for clients if prefix, key, callback are the only typically used args.. Should we stick to the new convention and pass callback at the end in _validate_callback_and_nowait? :). this rewrapping makes it more difficult for me to discern that the tail if nowait is False... is part of [(spec.Basic.CancelOk.... And if nowait is False might be better off as if not nowait just like the one in self._on_cancelok if not nowait .... Need ValueError if callback is None, since this API doesn't make sense without a callback. Am I mistaken?\n _validate_channel_and_callback only checks that it's callable if not None.. Should callback arg to to the end of the _rpc method signature too?. Is callback with value None meaningful in this method? (I don't know the answer off the top of my head). If not meaningful, then we need an explicit check for None and raise ValueError, since _validate_channel_and_callback won't do that.. Is callback with value None meaningful in this method? (I don't know the answer off the top of my head). If not meaningful, then we need an explicit check for None and raise ValueError, since _validate_channel_and_callback won't do that.\nIf it's okay to have callback be None here, then it should be documented as optional.. Just an observation: This signature confused me for a bit, as it seemed like the nowait/callback both had to do with notifying client of completion of Confirm.Select rather than the callback serving as vehicle for subsequent ACKs and NACKs. nowait/callback are often related in other APIs, but not here.. What is \"NB\"?. The callback part of _validate_channel_and_callback is redundant here. Perhaps we should just have _validate_channel for cases like this?. It is strange to me to see the required arg callback have the appearance of an optional arg via callback=None. Another school of thought might dictate required args at the beginning and optional args (like nowait in this case) at the end? And callback shouldn't default to None as this sends the wrong message to the user.. Yeah, it's okay to have callback be None in basic_recover.. yeah, it's okay to have callback=None in basic_qos.. Callback validation part of _validate_channel_and_callback is redundant in exchange_declare, since it's superseded by _validate_callback_and_nowait. Based on the foo, bar, baz pattern used in the other related tests, I surmise that None was intended to be the callback and should be moved to the end and be a kwarg callback=None. Looks like forgot to get rid of mock_callback arg and variable after changing to pass callback=None. Since callback is optional in flow and is now at the end, why not give it a default value - callback=None?. document that callback is optional and may be None. The None maps to the old callback arg and should be moved to the end and use kwarg. Move callback=self.on_queue_declared to the end to match the new arg order convention (even if kwarg works around the ordering issue) ? Elsewhere in this file too. Test code snippets get copied to apps and other tests.. callback at the end?. callback=None in the function signature suggests that callback is optional. However, the code and semantics dictate that it's required. This is likely confusing to the API user.. callback=None in the function signature suggests that callback is optional. However, the code and semantics dictate that it's required. This is likely confusing to the API user.. callback=None in the function signature suggests that callback is optional. However, the semantics dictate that it's required as it doesn't make sense to call without one.. This is likely confusing to the API user.. In both of these tests, you need to validate that the parameters of the exception are what you expected not just the exception type. See example in https://github.com/pika/pika/blob/c0d0b3a2a69bdf394cd20d367a10a375e716e7b0/tests/unit/blocking_connection_tests.py#L149-L152. And you don't have to verify the verbatim exception text, but just some substring, such as self.assertIn('prefetch_count', cm.exception.args[0]) (ValueError will have just the first string arg). Unrelated to this PR, but might be good to clarify in \"The prefetch-count is ignored if the no-ack option is set.\" above that this applies to consumers. Perhaps word it as follows: \"The prefetch-count is ignored by consumers with the no-ack option turned on.\". I think a different name for the callback arg would make the API less confusing, so the user won't think the callback is for signaling acknowledgment of Confirm.Select. Perhaps something like delivery_conf_cb?. The other thing that's unusual (defective) about Channel.confirm_delivery is that, unlike most APIs with nowait=False capability, it doesn't support the passing of a completion callback . Thus, BlockingChannel.confirm_delivery needs to work around this deficiency and dabble in some internal knowhow by using the lower-level API Channel.add_callback. What's the benefit of having confirm_delivery be inconsistent with all other AMQP synchronous method APIs?\nI think I would prefer to have this signature instead: def confirm_delivery(self, delivery_conf_cb, completion_cb=None). Ever wonder what benefit pika.utils.is_callable serves if python already provides the built-in function callable? I would prefer to get rid of our proprietary is_callable in favor of the built-in instead of changing more code to use the proprietary one.. @lukebakken - \"or if nowait is False and callback is not None and is not callable\" does not describe what the code is actually doing.\nThe code actually does this: \"or if nowait is False and callback is either None or is not callable: \nif callback is None or not is_callable(callback):\n    raise ValueError(...\nThe current description is what the original code (before this PR) allowed, but the code in this method doesn't match that behavior. See also my PR comment about consolidating nowait and callback, if the current implementation's tight coupling is intended.. My understanding is that the convention requires the indentation of args to follow all the way through. So, either\nself._rpc(spec.Basic.Cancel(consumer_tag=consumer_tag, nowait=nowait),\n          self._on_cancelok if not nowait else None, ...\nor\nself._rpc(\n    spec.Basic.Cancel(consumer_tag=consumer_tag, nowait=nowait),\n    self._on_cancelok if not nowait else None, ...\n. @lukebakken: we're making a fair amount of functional (API) changes in this PR already. The reviewing of the code is made more difficult by the presence of code reformatting changes. I recommend holding off on reformatting changes in this situation. I think that the generally accepted practice is to refrain from cosmetic reformatting as much as possible when making wide-spread functional changes, deferring cosmetic stuff to a separate PR.. It's probably more confusing than helpful here, I think. It had me puzzled and will probably have many others wondering unnecessarily too :). Since we have other types of callbacks besides the nowait/completion callbacks, I recommend renaming this method to be more specific; e.g., _validate_rpc_completion_callback. Remove references to non-existent param nowait here and elsewhere in the PR.. \"MUST be None when nowait=True.is no longer applicable here and similarly elsewhere in the PR. It needs to be an identity check against None, because python allows __bool__ to be overridden in unpredictable ways. So,if callback is not None:is preferred..if callback is not None:is preferred due to the possibility thatboolmight be overridden in unpredictable way..if not nowait else ...is preferred.W0612, 902: 8 - Unused variable 'nowait' (unused-variable). Run through pylint to see what else it might find.. It could be helpful to the user (and maintainers) to see what was actually passed in the event the check raises an exception. How about this for the entire implementation:\n```\ndef _require_callback(self, callback):\n    if not callable(callback):\n        raise ValueError('Callback must be callable, but got %r' % (callback,))\n```. By the way, wouldTypeErrorbe more appropriate? I think it might be, because we're looking for callback to be of callable type?. The new call signature forpika.connection.channelisdef channel(self, channel_number=None, callback=None). However, theopen_channelfunction in the asynchronous consumer example calls it using an incorrect signature:self._connection.channel(on_open_callback=self.on_channel_open). There are a number of others like it:\n```\n$ git grep 'on_open_callback='\ndocs/examples/asynchronous_consumer_example.rst:            self._connection.channel(on_open_callback=self.on_channel_open)\ndocs/examples/asynchronous_publisher_example.rst:            self._connection.channel(on_open_callback=self.on_channel_open)\ndocs/examples/asyncio_consumer.rst:            self._connection.channel(on_open_callback=self.on_channel_open)\ndocs/examples/tornado_consumer.rst:            self._connection.channel(on_open_callback=self.on_channel_open)\nexamples/asynchronous_consumer_example.py:        self._connection.channel(on_open_callback=self.on_channel_open)\nexamples/asynchronous_publisher_example.py:        self._connection.channel(on_open_callback=self.on_channel_open)\nexamples/confirmation.py:                                   on_open_callback=on_open)\n```\n. in docs/examples/comparing_publishing_sync_async.rst, asynchronousconnection.channelcall doesn't match the newConnection.channelsignature. it's called simply asconnection.channel(on_channel_open), which ends up passing the callback as first arg, which is nowchannel_number.. Why is queue param defaulted to''in this commit? The queue name is not optional, right?. What's the point of default value on thequeuearg? Queue name is required, isn't it?.queueshouldn't have a default value either, since you always need to pass a queue name, right?.queueshouldn't have a default value either, since you always need to pass the name of the queue to be consued, right?. Just curious -- how common of usage is it to have the queue auto-named by RabbitMQ?.queueshouldn't have a default value here, since you always need to name a queue that you're deleting..queueshouldn't have a default value since you always have to name the queue to be purged.queueshouldn't have a default value since you always have to name the queue to be unbound. Even if it's callable, the callback could be some strange user-supplied callable object, where the user overrodeboolto do something funky, soif callbackcould evaluate to false even when callback is not None. The only safe thing to do isif callback is not None:.. Even if it's callable, the callback could be some strange user-supplied callable object, where the user overrodeboolto do something funky, soif callbackcould evaluate to false even when callback is not None. The only safe thing to do isif callback is not None:.. Sorry, my mistake - that should be a % sign instead of comma after the string:'Callback must be callable, but got %r' % (callback,). Relying on it seems like a bug waiting to happen :). I wonder if, from the client API design point of view, it wouldn't be cleaner to get rid of these defaults? I don't know the answer, just wondering.. The last sentence about empty string is a proprietary feature of RabbitMQ, right? If so, need to make that clear or remove the sentence.. docs/examples/tornado_consumer.rst has an example with incorrectly ordered args that won't work:self._channel.queue_declare(self.on_queue_declareok, queue_name). Not that it hurts anything, but just wondering why you changed passing of the callback to use a kwarg in this call toadd_on_cancel_callback` which only takes one arg. \nAlso, wondering same about the _impl.add_callback calls below. And, are you considering moving the callback arg to be after replies (but before the optional one_shot) in Channel.add_callback to follow the new convention? Although, if you do, please do it in a new PR, since this one is pretty big already.. No point in creating a new variable consumer_tag just so you only use it for del in the very next line. It's not typical for python scripts, unless composing it is non-trivial. As a reviewer/maintainer, it (mis)leads me into thinking that this value is used elsewhere in the function and forces me to waste effort/time inspecting the rest of the function to see how/where else it might be used.. It might be helpful to give a hint to the user that in the case of server-generated queue name, the generated name may be extracted from spec.Queue.DeclareOk within the returned method frame.. Is it just me, or does the parameter name deadline in pika's add_timeout methods make one conjure up an epoch time value rather than a delay value relative to the current time? What Pika's add_timeout does is typically named call_later in other async packages.. [nit] please remove \"NB:\" from code. It's not a commonly-known/accepted abbreviation/acronym in computing. If you need to draw user's attention, \"NOTE\" is far more common. In this case, just # Disable too-many-lines message would suffice. This should be a @staticmethod. This should be a @staticmethod. Since you're unsuppressing this pylint check, would you mind converting several of the private methods that pylint exposes now as the result of this change to @staticmethod? It would be methods like _on_eventok that presently simply log some info from the args.. Are you planning to remove that sentence or add a reference to RabbitMQ? Ditto in basic_get and wherever else this is mentioned.. Sorry, never mind - it appears that this is a feature of the AMQP 0.9.1 specification, so not specific to RabbitMQ after all. Since callback being None or not None now determines the value of nowait, technically callback if not nowait else None can be reduced to just callback. Did you intend this to do something inside ack_nack_callback?. got it, thanks!. Since BaseConnection. _adapter_connect is documented to return None on success, it might be prudent to check for error is None instead.. Interesting, if there is an error reported to _handle_error, but there is no socket presently, the error is just suppressed? This doesn't sound right. If there is a legitimate reason for it, then a code comment is in order.\nAnd again, should be checking self.socket is not None as objects may override __bool__ and give it some unpredictable semantic.. I made the args explicit intentionally to make it very clear what's being tested and would prefer to keep them that way if possible. Ditto in the other tests where this change was made.\nIf we need tests that make sure that the defaults are as expected, then we should have separate unit tests for that.. Not sure about the purpose of the arg order change, since _basic_consume_impl has arguments before on_message_callback. probably change callback in signature to on_message_callback ?. Do you prefer the long form? Otherwise, I would suggest rpc_callback = self._on_eventok if callback is None else callback or rpc_callback = callback if callback is not None else self._on_eventok.. The comment was originally above def on_consume(... because it documented the code block that creates the consumer which includes the on_consume function.. The PR adds consumer_tag = in several places in the test file, but doesn't reference consumer_tag afterwards. Please run pylint on the changed files to catch them all.. Let's have a separate test with None completion callback which makes sure that self.obj._on_eventok is called.. This change and several others trigger \"Wrong continued indentation\" pylint warnings. Per pylint: \"Undefined variable 'callback'\". Sounds like we are missing a unit test for not callable(on_message_callback).. Bad vim, very bad vim! :). Something doesn't look right with the method name - the word \"raises\" before \"_prefetch_size\". Did you mean to name it test_basic_qos_*invalid*_prefetch_size_raises_error?. Something doesn't look right with the method name - the word \"raises\" before \"_prefetch_count\". Did you mean to name it test_basic_qos_*invalid*_prefetch_count_raises_error?. This and all similar changes in this pull request make the API very error-prone. In particular, a user can misspell a kwarg, and it will be silently ignored. For examples, if a user passes aguments=dict(...) (note the missing \"r\" in the arg name), it will end up inkwargs and not trigger any visible response from the python runtime.. Since no_ack is no longer documented, it's no longer part of the API, and users wouldn't really know to use it, nor should they. Applications must code to the library's API, not to its implementation. If this PR really needs to happen, then I see no benefit in keeping no_ack for the following reasons:\n1. Since it's not documented, no one should be using it\n2. It doesn't make sense to keep it for backward compatibility, since a couple of recently-merged pull requests already made a number of non-backward-compatible arg position/name changes in the library's APIs. Another user error that's silently hidden here is if the user inadvertently passes both auto_ack and no_ack with different values.. Also need test with auto_ack=False?. @michaelklishin, most of the code snippets from the web where callbacks are passed are now going to break due to position and/or name change of the callback args, including the ones for basic_consume and basic_get (due to position change). The key takeaway is that since no special accommodation is made for the backward compatibility of the callback arg changes (which will break existing code snippets from the web), why should backward compatibility of no_ack be any different? If not for lack of no_ack support, they will break due to the callback change anyway. Either of the breaks will cause developers to look at documentation and fix the code.\nThat said, if the above rationale is not convincing, I don't view having the no_ack support in as a huge issue and am content to move forward despite it :) . @michaelklishin, if we're leaving in no_ack for backward compatibility, we should at least emit a deprecation warning either via logger or https://docs.python.org/2/library/warnings.html. @michaelklishin, I think that the common approach is to pop the expected args from kwargs and then raise an exception if kwargs is not empty. I would also get rid of the bool typecast since we don't apply it to auto_ack and we didn't have it for no_ack either. For example:\n```\nif 'no_ack' in kwargs:\n    auto_ack = kwargs.pop('no_ack')\nif kwargs: # [pythonic way to check for non-empty container]\n    # [TypeError because the number/names of args is key to a function's signature\n    # which is part of its type]\n    raise TypeError(\n        'basic_consume called with {} unexpected argument(s): {}'.format(\n            len(kwargs),\n            compat.dictkeys(kwargs)))\n``. Kids do the darndest things. I have teenagers, so I should know :). How is this change relevant to the subject of the pull request? WasWARN_ABOUT_IOLOOPa nuisance? . Please implement test to exercise this logic. Please add code comment describing what could possibly causeself._blockingto revert to non-None and why we abort draining if we detect that it became non-None.. Wouldisinstancecheck be more pythonic in this case (versus name comparison)?. Please add a code comment explaining what prevents the blockedChannel.Closefrom becoming unblocked and sent to broker on an open connection so that it must be worked around here by discarding it manually. \n. \ud83d\udc4d . @lukebakken - that comment doesn't explain what is being asked here.. Optional: Wherever we document theauto_ackarg, it might be helpful to note that it correlates to AMQP no-ack. I don't think we need this test any more since legacy keyword is gone and this tests looks pretty much like the one above it. There should never be more than onespec.Channel.Closeinself._blockedbecauseChannel.closesets theCLOSINGstate and will raise an exception if called again in that state. So can probably break out of the loop immediately after callingself._on_close_meta. Might be good to include an explanation to this effect as code commend before breaking out.. Was this test reproducing the failure prior to your fix?. It's misleading to say \"Channel.Close method\\*s\\*\" and \"ensures  their callbacks are processed\", because there may be at most 1Channel.Closemethod in the blocked method queue. This unnecessarily confuses reviewers and maintainers into wondering how such a happenstance might come into being.. Unlike_on_synchronous_complete, which needs to stop calling_rpcuponself._blocking, we shouldn't care about the valueself._blockinghere, since the goal here is to purge all fromself._blocked, right? Why wouldself._blocked` matter?\nIn fact, would it not be better to leave the channel's self._blocking value alone (i.e., remove self._blocking = None above and remove the check for self._blocking is None from the while control statement), so that the callbacks would not inadvertently cause _rpc to send on this channel which we know to be closed?. > We do not want to send this method, but process it as though it had been sent\nJust FYI: technically, there would be no harm in sending the blocked Channel.Close to the server, since AMQP 0.9.1 specification obliges the server to respond with Close-OK in this case, while dropping all other incoming methods, save for Close-OK. Sorry, no -- it explains the side-effect, but not the root cause of the problem that we're working around with this code, which prevents reviewers/maintainers from gaining a valuable insight.\nHere is what I consider to be the root cause of what this PR is fixing, which would be helpful to explain in a comment above the call to self._drain_blocked_methods_on_remote_close:\nThe server closes the channel before responding to an outstanding synchronous method (that possibly triggered the channel error and which isn't Channel.Close). AMQP 0.9.1 obliges the server to drop all methods arriving on a closed channel other than Close-OK and Channel.Close. Since the response to that synchronous method never arrives, the channel never becomes unblocked, and the Channel.Close, if any, in the blocked queue has no opportunity to be sent, and thus it's completion callback of the Channel.Close would never be called.. > What do you think about continuing the loop just so that any other blocked methods are debug-logged?\nMay be marginal, because: 1. if channel errors out, the developer should reasonably expect that some initiated requests would not complete (also since they didn't get completion callbacks); and 2. The one(s) that made it into the transport buffer and/or socket would not show up in these debug statements.. \ud83d\udc4d . Should be \"but does *not* send\". Corrections:\n    The broker may close the channel before responding to outstanding in-transit\n    synchronous methods, or even before these methods have\n    been sent to the broker. AMQP 0.9.1 obliges the server to drop all\n    methods arriving on a closed channel other than Channel.CloseOk and\n    Channel.Close. Since the response to a synchronous method that blocked\n    the channel never arrives, the channel never becomes unblocked, and the\n    Channel.Close, if any, in the blocked queue has no opportunity to\n    be sent, and thus its completion callback would never be called.. @lukebakken - we're almost there. Just comment changes remain.. @lukebakken - good points. I will follow up in a couple of days. Thanks!. @lukebakken:\n\n\nI didn't find the equivalent to ioloop.close() in twisted reactor. And the Twisted connection adapter is not integrated into the async acceptance test framework. In fact, the Twisted adapter has very little, if any, testing in pika (I would be surprised if we didn't have an open issue for it already, but anything is possible).\nI added an assertion in select_connection's ioloop close() code path to detect if it's being called before the call(s) to start() fully unwind. On the Tornado adapter side of things, I added a docstring to advise users not to call it until start unwinds - however, enforcement, if any, in the tornado case should be up to tornado's implementation as our use is not different from anyone else's use of tornado.. @lukebakken, I added some more tests. The pull request is ready for resuming code review. Thanks!. I also discovered an existing bug in the process of implementing new tests that I filed as issue #970.. To avoid duplicating validation logic that's already in Parameters.heartbeat setter (type and minimum), just raise TypeError here if user's callback returned a callable or None, otherwise just assign the returned value to self.params.heartbeat to let it do the rest of the validation.. In this docstring, we should make clear what the new API is. Namely that an integer value specified by app or returned from callback will be used to override the server's value. None specified by app will be used to adopt the server's value.. we should go further and say that if it's an integer value, it will be used to override the server's value.. \"then be negotiated\" - instead, we should say that the callback return value must be a non-negative integer that will be used to override the server's value during negotiation.. Did you intend not to allow the callback to return None? (having the same meaning as if None were assigned to hearbeat directly by user code)?. This 0 and None sentence should be moved before the \"If set and is a callable...\" sentence, since the callable is not allowed to return None, thus making this misleading.. As far as I know, this legacy wording is incorrect: \"Specify the number of seconds between heartbeat frames\". Multiple heartbeat frames could be sent during this time (I think RabbitMQ sends 2). The heartbeat value is really a timeout value. Absence of incoming traffic during this period signals that the other end may be dead.  \n\nIt's better to get rid of this misleading legacy description and borrow the wording from Parameters.heartbeat setter, but only the non-negative integer part and the special meaning attributed to the value of zero.\nAnd I don't think that you can encode None in a URL parameter, can you?. I don't think that you can encode a callable in a URL parameter, can you?. Server's value is 10, not 60, right? See method_frame.method.heartbeat = 10 below. Should also assert that pika passed 10 from server's method_frame.method.heartbeat: add self.assertEqual(val, 10) before return in choose_max(). Please add a test for passing callable heartbeat using ConnectionParameters class to the appropriate section in test/unit/connection_parameters_tests.py. I don't understand the meaning of \"passed the server's proposal\". I think you intended to say \"its return value will be used to override the server's proposal\".. Please add a test for passing callable heartbeat using URLParameters class to the appropriate section in test/unit/connection_parameters_tests.py. Please add a test for passing callable heartbeat using Parameters class to the appropriate section in test/unit/connection_parameters_tests.py. @darcoli - If something ever changes, I hate fixing the same logic in multiple places. What I meant is the following:\n```\nret_heartbeat = self.params.heartbeat(method_frame.method.heartbeat)\nif ret_heartbeat is None or callable(ret_heartbeat):\n    # Enforce callback-specific restrictions on callback's return value\n    raise TypeError('heartbeat callback must must not return None or callable, but got %r' % (ret_heartbeat,))\nLet hearbeat setter deal with the rest of the validation, so as not to duplicate the additional validation logic\nself.params.heartbeat = ret_heartbeat\n```. @darcoli If we leave it as \"desired connection heartbeat timeout for negotiation\", it's not clear to the user that an integer value will actually override the server's value, since \"negotiation\" could mean anything.\nHow about the following here and in ConnectionParameters.__init__() docstring? I am going to borrow some of your new text from heartbeat documentation in theConnectionParameters constructor\n:param int|None|callable value: Controls AMQP heartbeat timeout negotiation\n    during connection tuning. An integer value always overrides the value\n    proposed by broker. Use 0 to deactivate heartbeats and None to always accept the\n    broker's proposal. If a callable is given, it will be given the heartbeat timeout int proposed\n    by broker as its only argument and must return a non-negative integer that will be used to\n    override the broker's proposal.. @darcoli, while making the above documentation recommendation, I realized that we also need to pass the reference to the connection instance in this callback as the first arg. So, it would be called like this from Connection:  callback(connection, broker_value). Use self.assertIs(params.heartbeat, heartbeat_callback). Also need a ConnectionParameters test for passing callable heartbeat arg via ConnectionParameters constructor arg under ConnectionParametersTests. Were you seeing this fail too?. Should fix erlang and elixir at particular version or always get the latest stable?. Not opposed to shutting RabbitMQ down, but wondering why it's necessary. Won't the VM restart next time anyway? Preferably, each build/test cycle should start with pristine RabbitMQ data files/databases.\nIf leaving this in, please add a comment explaining why it's needed.. Please add a comment explaining why turning off the firewall is desirable, which problem it solves.. Just wondering about this TODO comment which I added a couple of years ago. Since Pika is a pure-python package, would it ever make sense to have a deployment from AppVeyor? If not, please replace the comment with a mention that pika is pure-python and gets deployed by the travis-ci build already.. It looks like we're good for the next 10 years - woohoo!. Why wouldn't ERLANG_HOME be set? If it isn't, it would seem to be an error in our AppVeyor script, and we are better off fixing that rather than hardcoding the location in multiple places. If ERLANG_HOME isn't set up, let's cause the build to fail with a helpful error message.. In appveyor.yml, we have a reference toerlang_19.3.exe presumubly, 19.3 is the version number of erlang we want. What's the 8.3 in erts-8.3\\bin\\epmd.exe? I am not familiar with the erlang artifacts, so this looks like a version mismatch to me.. There is also a hard-coded reference to cmd /c \"C:\\Program Files\\RabbitMQ Server\\rabbitmq_server-3.7.3\\sbin\\ in appveyor.yml. This includes both installation location and version-specific information. Let's not duplicate this knowledge across scripts. Please export an environment variable with the appropriate path from appveyor.yaml and use the variable instead of hard-coding the path.. Please make should_test_tls and ssl_options available to other tests. Here is a proposal:\n\nUse an environment variable (e.g., PIKA_TEST_SSL_OPTIONS_FILE) to store the path of the JSON file containing the settings for assigning to self.parameters.ssl_options. This way, developers can set this up on their test environments as needed.\nExpose a function via test_utils.py with the functionality of should_test_tls.\nExpose a function via test_utils.py for loading the ssl_options from the file referenced by the corresponding (e.g., PIKA_TEST_SSL_OPTIONS_FILE) environment variable.\nUse both functions in this test instead of hard-coding the values.. @lukebakken: Just to be clear regarding my feedback items asking for comments to be added, I meant adding the comments in the code so that other maintainers and reviewers wouldn't be wondering why we're doing these things.. @lukebakken, I was hoping to extend SSL testing to BlockingConnection and the tests for the new SSL/transport refactoring work that I am doing. In addition, my transport tests will need the location of the server key file. That said, we can wait to add this functionality in the next \"sprint\".. ssl.SSLSocket supports the important server_hostname arg that ssl.wrap_socket doesn't.. I think it may be better to use more descriptive test names instead of suffixing with 1,2,3,4. It seems like the variability is upper case vs lower case encoding and terminating slash versus no terminating slash.. @lukebakken, here I renamed the method to make it explicit/self-documenting. The old name _validate_channel sounded more like a consistency-checker for the channel.. I changed these to lambda when I discovered that tests that fail with timeout also fail during cleanup - exploding the nosetests log size and creating unnecessary confusion during failure analysis - since the timeout timer in these addCleanup self._connect() calls also starts ticking at the same time as the connection used in each test.. The programmatically-generated code in pika/spec.py uses the AMQP no_ack versus the new name auto_ack that we adopted recently in the connection adapters.. Need to set these up since Channel. _closing_code_and_text now defaults to None. Majority of the one-liner changes in this test script are for this reason.. Channel.close() member function now saves the passed-in values in _closing_code_and_text, so comparing against the bogus (0, '') no longer works.. These changes are temporary, will be reverted. They help me get a better picture of what is failing.. The change in this method (including its name) is not necessary for fixing the issue and breaks normal functionality, please revert the change in this method entirely including the name. Here is why: the user was and should be able to make a blocking legitimate request or a series of them followed by Channel.Close and not have those legitimate requests preempted. Preempting them would be overly-opinionated :). If we think we need an \"emergency channel-close\" method that purges queued-up requests (I don't think we do), then we should either have a different appropriately-named method or an optional arg that makes that explicit.. We don't want/need this if self.is_closing and self._blocking code block. It prevents the user from being able to queue up several blocking requests followed by channel.close() and having those requests complete normally.. I don't think the except Exception as err: block adds any value. The user should be able to glean the same information from the traceback produced by the failed self._send_method().. It probably doesn't matter given my preceding comment, but if self._blocking was True, we wouldn't have made it  this far. . and even more importantly prevent self._blocking from being erroneously set too (which was causing the deadlock).. ... \"and update stats\". This sequence of calls (as well as the one below) still permits a partial request to be placed on the output buffer, thus corrupting the output stream. What needs to happen is all the marshaled frames need to be appended to a list. Only when the entire message has been successfully marshaled, we go ahead and place the marshaled frames on the output buffer.\n\nAlso, I find that the single-use intermediate variables as used here detract from code readability in Python. They have their place when the value being computed would make an all-in-one statement difficult to read, but I don't think it's the case here. Same in _send_frame(). Here is an example of what I have in mind:\n```\nmarshaled_frames = []\nmarshaled_frames.append(\n    frame.Method(channel_number, method_frame).marshal())\nmarshaled_frames.append(\n    frame.Header(channel_number, length, content[0]).marshal())\nAnd below...\nif content[1]:\n    # ...\n    for chunk in xrange(0, chunks):\n        # ...\n        marshaled_frames.append(\n            frame.Body(channel_number, content[1][start:end]).marshal())\nself._output_marshaled_frames(marshaled_frames)\n``. Oh yeah, as you can see from my code snippet above, I now think it's better to have a_output_marshaled_frames()(note plural) method instead of_output_marshaled_frame()in order to avoid_detect_backpressure()calls on per-frame basis._send_frame()can just wrap its single frame in alistwhen calling_output_marshaled_frames().. Incidentally, themethod_frameparameter to_send_message()is misnamed in this legacy code. What's passed in should be calledmethod. The method_frame is created inside the function when it constructsframe.Method.. I think it's better to have a_output_marshaled_frames()(note plural) method instead of_output_marshaled_frame()in order to avoid_detect_backpressure()calls on per-frame basis. So, insted of accepting a singlemarshaled_frame , it should accept a sequence ofmarshaled_frames. And_send_frame()can just wrap its single frame in alistwhen calling_output_marshaled_frames().. Users have been asking for this for a while. This PR makes it easy to provide such functionality.. @lukebakken, we really don't want this. If there is a blocking command, then Chanel.Close will get appended toself._blockedjust like any other blocked command. When the command(s) that blocked complete,_on_synchronous_completethat is separately registered as callback in_rpc()will allow the Channel.Close to do its work. Thisself._on_synchronous_complete(None)call here breaks the synchronous RPC workflow by allowing a new RPC method to be sent before the previous one completes.. A specialized deque class isn't needed here. A simplelistwill do. Themarshaled_body_frames.popleft()` logic in the loop at the bottom adds no value and is less efficient than a simple loop iteration. Keep in mind that when the list goes out of scope, python will automatically deref the list and all its items inside C code versus popping here one at a time.  . Per my previous comment, the popping is unnecessary and is adds inefficiencies. The following is preferred:\nfor chunk in marshaled_body_frames:\n    self._output_marshaled_frame(chunk)\nHowever, it's better to do this instead in order to avoid unnecessary per-chunk overhead inside self._output_marshaled_frame(), namely self._flush_outbound() and self._detect_backpressure():\nself._output_marshaled_frames(marshaled_body_frames)\n_output_marshaled_frames() should append all the chunks to self.outbound_buffer while updating self.bytes_sent and self.frames_sent (again no need to pop, see the for chunk in marshaled_body_frames: example above), and then perform self._flush_outbound() and if self.params.backpressure_detection: self._detect_backpressure() just once outside the loop at the end of the _output_marshaled_frames() method.. The problem we're trying to fix is \"Request marshaling error should not corrupt a channel\". The only reason the channel becomes corrupted is the marshaling issue, which the rest of this PR addresses.. Changing all the existing lines makes it difficult for a reviewer to check what changed and what didn't.. Where is apt-get update called?. Have you tried with apt: false? Just wondering if there is a bug with caching apt repos.. I tried apt: false and removing the apt line from cache, but it didn't help.. @lukebakken, we need to be policing test coverage more effectively. For example, the Twisted connection adapter was accepted without test coverage in 2011 and still has no meaningful test coverage in 2018. Over the years it has been anywhere between partially broken and completely broken. My preference in general is not to accept/release code without meaningful test coverage.. The context manager entry points got moved to a more natural location near other __zzzzz__ methods.. Added the self.is_open check because BlockingConnection.close() now raises the \"wrong state\" exception if it's not open.. We are now able to glean whether it's user-initiated from the exception class.. These got moved up closer to the constructor.. RecursionError conflicted with the builtin namesake.. This PR implements a common protocol/transport model for all supported non-blocking adapters as well as non-blocking connection establishment. The TwistedConnection class above already benefits from this. Because of this and lack of tests, I would prefer to get rid of TwistedProtocolConnection. If can't remove TwistedProtocolConnection class, then someone would need to implement a test suite for it and debug.. Moved here to be close to constructor to make it easier to correlate member  variables.. Replaced by non-blocking connection workflow initiated via _adapter_connect_stack(). Now handled entirely in transport. Moved to be near the constructor.. This turns expected exceptions in asynchronously-executing code into graceful on_open_error_callback() calls.. Orthogonally, only do one or the other callback depending on state before error/closing, but not both.. Go with latest twisted for testing (just like the other I/O frameworks) since we no longer support pre-2.7 pythons.. This fixes a nasty bug that would cause tests to pass when the connection establishment failed.. \"test\" in the beginning of method name allows py.test and builtin unittest runners to discover test methods without customization. Also, it's common practice that functions should be verbs.. One of the benefits of the refactoring is that we can now extensively test all the ioloop adapters generically using the same test code.. Blocking connection now makes use of external connection workflow via AMQPConnectionWorkflow to manage retries, etc.. The builtin unittest on python 3.5 doesn't have assert_called_once(). (earlier and later versions do, though). Oh, this looks like printer's nightmare :). socket connection timeouts are no longer implemented inside connection classes.. @lukebakken, as mentioned at the top of the script in its docstring, I modeled the interface on asyncio. create_streaming_connection() is a subset of the behavior of asyncio's AbstractEventLoop.create_connection(). The rationale is that a protocol (AMQP connection in our case) needs a stream. The stream might be TCP or SSL/TLS on top of TCP. The protocol doesn't care which one. It's up to the transport to abstract away the differences. Pika's new AbstractIOServices.create_streaming_connection() takes care of the details, just as the comparable method in asyncio.. \"expected connection\" ==> \"expected exception\". Surprising that the underlying implementation raises AssertionError instead of TypeError/ValueError. But no need to fix that.. ... Method, Header, and Content objects, .... pls fix indentation. I wonder if it's possible to instruct the documentation system to pull the source code directly out of examples/consume_recover.py instead of maintaining duplicates?. I am not finding a builtin retry decorator. Where is the one you're importing coming from?. Have you considered making use of the blocking connection as context manager:\nwith pika.BlockingConnection(parameters) as connection:\n    channel = ...\n    .... using ==> use. Recover is useful for publishers also, since connection may be lost or fail to become established due to transient network conditions, broker failure, broker upgrades, etc. So, this paragraph might be redundant.. \"Different connection adapters\" ==> \"Different types of connection adapters\". This code snippet doesn't really demonstrate any recovery logic.\nIn general, adding duplicates of the code in the README may be counterproductive for maintenance of the software as there will be 3 places to keep in sync in the future - 1. under docs; 2. under examples; 3. in README.\nI would prefer to have just references to them, accompanied by very short description(s).. @lukebakken - do you know whether sphynx supports pulling code directly from pika's examples .py files instead of maintaining duplicates in the .rst files under docs as well as the .py files under examples?. Recover is useful for publishers also, since connection may be lost or fail to become established due to transient network conditions, broker failure, broker upgrades, etc.. Wouldn't you want to exit the loop upon KeyboardInterrupt?. Rather than duplicating the recovery logic of blocking_consume_recover.rst, I would prefer to see a simple example demonstrating passing multiple params to the BlockingConnection constructor and a blurb that explains that this may be combined with the recovery logic along with reference to blocking_consume_recover.rst.\nI am trying to reduce the amount of code/content duplication since duplication adds unnecessary overhead to the maintenance of the software.\nIn this example, the following would suffice along with reference that this may be combined with connection recovery and with sphinx reference to the connection recovery example. Also, renaming of this .rst to blocking_connection_multiple_hosts.rst.  And same in the pika/examples/ directory.\nimport pika\nimport random\n\n## Assuming there are three hosts: host1, host2, and host3\nparams1 = pika.URLParameters('amqp://host1')\nparams2 = pika.URLParameters('amqp://host2')\nparams3 = pika.URLParameters('amqp://host3')\nparams_all = [params1, params2, params3]\n\n# Shuffling the hosts list can help balance connections\nrandom.shuffle(params_all)\nconnection = pika.BlockingConnection(params_all)\n\n. Probably want to exit the loop upon KeyboardInterrupt.. Change \"::\" to \":\" at the end. Also useful for publishers as mentioned in other review feedback.. Perhaps add to the description an example of installing the retry decorator. Is it pip install retry?. pls rename the .py file to match the corresponding .rst file to make it clear that this is for blocking connections and to make it easier for developers to correlate the two.\nAlso see feedback in corresponding .rst.. pls rename the .py file to match the corresponding .rst file to make it clear that this is for blocking connections and to make it easier for developers to correlate the two.\nAlso see feedback in corresponding .rst.. Add terminating newline. A single-use variable containing a simple value is unnecessary. Use 'example.com' directly in the pika.SSLOptions() instantiation below.. @lukebakken or @michaelklishin : need you to review the rabbitmq sample config change here.. Please apply the suggested changes in this function as well: https://github.com/pika/pika/blob/9fb234c0e91b709b5935f4d97432c5991597c61f/tests/acceptance/async_test_base.py#L120. Nice!. Please make sure these are consistent with https://github.com/pika/pika/blob/master/testdata/rabbitmq.conf.in. @maggyero, sorry, I should have been more specific - I meant the properties being set, not the values (e.g., file paths) themselves. Thank you.. @michaelklishin - The heartbeat unit tests had logic to test this, but the mock spec was not sufficiently detailed to allow the access to the bogus attribute heartbeat to be detected. Even the test itself was accessing the wrong attributes and succeeding.. => # Wake up the IOLoop *potentially* running in another thread. I would prefer to leave this time.sleep() code in place. The reason being that the poller should be able to work without file descriptors. The fact that we presently always have at least one file descriptor is an artifact of the current design. In particular, pushing the running of event loop and implementation of wakeup all the way down to the poller layer is arguably misplaced. As the result, there are several up/down delegations of API calls that don't belong. I am planning to refactor such that running of the event loop and wakeup logic is contained in the IOLoop class, while pollers are only responsible for the polling abstraction, which I believe to be more orthogonal. In that case, poller's implementation won't be guaranteed that it will always be used with >= 1 fd.. Also need to add the efficiency blurb in IOLoop.add_callback_threadsafe() docstring (just like the one you added in BlockingConnection.add_callback_threadsafe()). Except that in IOLoop, it's call_later() instead of add_timeout() and delay instead of deadline.. By the way, my understanding is that RuntimeError is intended to be used as the base class of the Python runtime errors, not our errors encountered during runtime.. @lukebakken: Sure, let's rename add_timeout(self, deadline, callback) to call_later(self, delay, callback) for v1.0. You will need to fix tests and example code too.\nRationale:\nThis is what this functionality of calling back in a number of seconds relative to current time is called in most frameworks, including Tornado, on which select_connection.IOLoop's API is based.\nAlso, some other code in pika (adapters/utils/selector_ioloop_adapter.py) that wraps around both select_connection.IOLoop and tornado.IOLoop expects call_later().\nMost frameworks offer several variants, including methods that accept an absolute time. add_timeout with the deadline arg is better suited for the absolute time variant, which we really don't need in select_connection.IOLoop and BlockingConnection (no one ever asked for it).\nPika's original use of the name add_timeout/deadline  for a current-time-relative callback is misleading and a misnomer, I believe.. Hi @lukebakken, this TODO didn't communicate the goal sufficiently clear. The goal was to remove the abstract method from pika.Connection class and remove it from all derived classes except pika.adapters.SelectConnection. _adapter_get_write_buffer_size is provided strictly for the benefit of BlockingConnection, which is implemented only on top of SelectConnection. Some native Transport implementations, like the one in Twisted don't even expose this info.. @lukebakken, yeah replacing interval and idle_count with a single timeout arg is what I had in mind, so that Connection would simply pass the negotiated heartbeat timeout value to this constructor and this class would encapsulate all the other knowledge. There is no need for defaults here because Connection would always pass an explicit value. This construction would never be used without an explicit timeout value being passed. Pika by default uses pika.connection.DEFAULT_HEARTBEAT_TIMEOUT = None, which causes it to accept the server's value, which sounds perfectly sane.\nAfter these changes, the constructor signature would be def __init__(self, connection, timeout): (no default, since its user always passes in the negotiated value, so no need for arg defaults of any kind here). interval is the wrong name for the arg anyway as it doesn't convey the meaning of timeout.. @lukebakken,\nAfter removing the idle_count arg and renaming interval arg to timeout (which makes more sense for what this is; also updating the docstring description and interval arg param section), and keeping MAX_IDLE_COUNT as is, this should be\nself._interval = float(timeout) / MAX_IDLE_COUNT\nfloat(timeout) is necessary for the sake of python 2.x; for example, float(3/2) = 1.0, but we really want 1.5.. @abompard  disable-msg is deprecated as of pylint 0.21.0. Please update it to just \"disable\"  everywhere in this PR.. ==> \"Sends heartbeats to the broker and checks to make sure that broker's heartbeat is received before the expected timeout expires.\" ?. _CONNECTION_FORCED is not used.. What made you switch from 2 to 4? Not saying it's wrong, just curious? I thought that RabbitMQ sends heartbeats only twice per negotiated timeout window? Or did I get that wrong?. This description sounds wrong. It should send a heartbeat multiple times within the timeout window. And it will also be checking for timeout of heartbeats from the broker.. The math counts don't work out with this PR: division by 2 versus _MAX_IDLE_COUNT. self._timeout is just timeout without any division. Cast it to a float, though, to make division correct on python 2.x later when calculating the delay value to pass to _adapter_add_timeout(). In other words: \nself._timeout = float(timeout)\nOr, if you want to store pre-computed \"check interval delay\", then \nself._check_interval = float(timeout) / HeartbeatChecker._MAX_IDLE_COUNT\nNote that having float wrapping timeout turns the division into floating point op on python 2.x too.\nIn subsequent comments, I will assume you are using self._check_interval. Pass self._check_interval instead of self._timeout.. Don't need this since will be using self._check_interval in _adapter_add_timeout() below.. Need to fix up tests to match the corrected implementation.. @lukebakken, I just re-read the heartbeat field description in https://www.rabbitmq.com/amqp-0-9-1-reference.html and there appears to be inconsistency between the spec and RabbitMQ documentation. Here is what this spec https://www.rabbitmq.com/amqp-0-9-1-reference.html says:\n\nThe delay, in seconds, of the connection heartbeat that the server wants. Zero means the server does not want a heartbeat\n\nAnd here is what rabbitmq doc https://www.rabbitmq.com/heartbeats.html says:\n\nThe heartbeat timeout value defines after what period of time the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries. This value is negotiated between the client and RabbitMQ server at the time of connection ...\n\nand \n\nHeartbeat frames are sent about every timeout / 2 seconds. After two missed heartbeats, the peer is considered to be unreachable.\n\nSo, AMQP spec appears to be interpreting the negotiated heartbeat value as a \"delay\" between sending heartbeats, while RabbitMQ doc is interpreting the same value as the absolute timeout (which makes more sense)\nSo, the behavior that you observed about RabbitMQ with 60-sec negotiated heartbeat:\n\nA 60 second negotiated timeout will result in RabbitMQ sending heartbeats every 60 seconds\"\n\nIs inconsistent with RabbitMQ doc statement:\n\nHeartbeat frames are sent about every timeout / 2 seconds. After two missed heartbeats, the peer is considered to be unreachable.\n\n@michaelklishin - can you chime in about the latter?. > except maybe having old version next to the new one in a table, perhaps\n@lukebakken - this sounds reasonable. Is name-mangling (two leading underscores notation) here used because there is a strong likelihood that TwistedChannel would be subclassed? The reason I ask is that the name-mangling makes it more challenging to reference those members from unit tests. . I think we're on a good path with this API change.. I think it would be easier for developers to use the API if it spelled out all methods and all args explicitly. When using PyCharm or similar, tab-completion and smart arg completion don't work. It's really difficult to figure out what the API of this thing is.\nI like what you did with basic_consume(). Can the rest be modified as well? I think that this would mostly be copying function declarations and docstrings from pika.channel with some painstaking massaging of the arg lists and docstrings.. @abompard, I am not well-versed in Twisted, but I am surprised to see a callback passed to the constructor. Should the user instead be able to pass a deferred that would be signaled? Or perhaps, just like self.ready = defer.Deferred(), there might be self.connection_closed = defer.Deferred() as well?. Does it not make sense to allow the user to pass a custom reactor to this constructor so that it would, in turn, pass it to the _TwistedConnectionAdapter's constructor?. There is no need for all the None default values since this is a private class used by TwistedProtocolConnection and the latter is going to pass all these arguments. Unnecessary stuff just adds a little unnecessary confusion. :). @lukebakken, this has been bugging me for some time in Pika core - it provides a seemingly unnecessary arg channel_number that lets the user specify his own channel_number instead of letting pika do it automagically. Is there a real use case for this? Do any of the other clients in RabbitMQ echo-system let you do that?. @lukebakken - this sounds a little complicated and is inconsistent with the RabbitMQ documentation https://www.rabbitmq.com/heartbeats.html#heartbeats-timeout:\n\nThe heartbeat timeout value defines after what period of time the peer TCP connection should be considered unreachable (down) by RabbitMQ and client libraries.\n\nThe documentation makes it pretty clear that if we negotiate a timeout of 60 seconds and nothing has been received from the broker in 60 seconds, then the connection should be considered dead and the client should close the socket/connection. The broker does the same: if it doesn't get anything from the client in 60 seconds, it will also close the connection. \nBased on your changes and the negotiated example timeout of 60 seconds, pika will send heartbeats every 30 seconds, but wait for double the negotiated timeout - 120 seconds of idleness from the broker - to conclude that the connection has died. So, if a user of Pika negotiates a 5-minute timeout, pika will end up waiting 10 minutes on a dead connection. This just doesn't make any sense to me at all and is unnecessarily complicated to explain to the user as exemplified by this long comment here :)\nIt also makes text = HeartbeatChecker._STALE_CONNECTION % self._timeout produce an incorrect message.. @lukebakken, please see my previous feedback about placing the float \"cast\" :). @lukebakken - help, I am confused! :) We have been discussing this for some time and agreed to rename the arg interval to timeout, and it's now back to interval. Interval is meaningless here because we shouldn't use it as interval between anything, and timeout has the correct semantics.. send_hearbeat and check_heartbeat should be private methods with leading underscore in the names.. There is unnecessary complication from the super-granular separation between _start_check_timer/_setup_check_timer and _start_send_timer/_setup_send_timer as well as _setup_timers. Let's just have _start_check_timer and _start_send_timer and no self.active check (as explained in the following paragraph)\nAnd the self.active checks are unnecessary since Connection calls the heartbeat's stop() method when it's not needed any more, and the stop() cancels the timeouts. So, let's get rid of the redundant HearbeatChecker.active property as well.\nThese unnecessary legacy things are an unwelcome distraction for maintainers. :). There is no use for _MAX_IDLE_COUNT any longer since we configure our check interval as self._check_interval = (float(timeout) * 3) / 2 (i.e.   timeout * 1.5) to account for latencies already. Otherwise, with _MAX_IDLE_COUNT of 2, a negotiated timeout of 10 minutes becomes a 30-minute timeout (10 * 1.5 * 2), a bit much, right? If you're an app developer and you limited timeout to x seconds to detect dead connections in some reasonable time, but pike obliged 3x later, wouldn't you and your SLA be fuming right now? :) So, no more _MAX_IDLE_COUNT.. Not sure why I originally suggested the * 3 / 2 math. My bad. It would be more readable as timeout * 1.5 and floating point math would be happy on both 2.x and 3.x too.. channel_closed() appears to be for internal use, so should be named with a leading underscore as a private method. @abompard, are you planning to add the remaining explicit Channel methods?. @michaelklishin, do you agree?. @lukebakken - this comment is confusing. We were using ssl.SSLSocket previously, not wrap_socket. Did you mean that ssl.SSLSocket was deprecated?. Should raise ValueError exception instead.. Should raise ValueError exception instead.. Should raise ValueError exception instead.. @lukebakken: Need to raise an exception if opts contains anything else that isn't supported. Otherwise, this change is very error-prone.. This is great @lukebakken, I've been wanting to have this change for some time!. @lukebakken - ctx is the standard abbreviation for context in computing, not cxt as far as I know :). Why remove IPV6 tests? It feels like taking a step back when we remove tests from the test suite that were testing legitimate scenarios.. ",
    "lukebakken": "@galindro - please give Pika 0.12.0b2 a try, which contains #956. It can be installed with this command:\npip install pika --pre\nThank you!. Passing user data would also address #898 . Both functools.partial and a closure are good solutions to this issue.. Re-opening. I noticed that this library supports this feature. Pika should too.. @vitaly-krugl has suggested that functools.partial be the accepted way to do this here and I agree. I have updated some of the code in examples/ to demonstrate this.. > maybe RabbitMQ isn't listening to your ipv6 loopback?\nlocalhost isn't correctly set up in /etc/hosts or in some other part of that person's system. They also filed #1151 with the same question.\nChanging localhost to 127.0.0.1 should work.. @vitaly-krugl could you provide the code you were using to reproduce this? I assume it must have disabled heartbeats, correct?. Thanks! I have verified #1 using the attached code, and you can see in the transcript that this issue is fixed.\nproducer.py.txt\ntranscript.txt\n. @lokeshh did you call connection.connect() first? Please see example code here. @radzhome if you can provide code to reproduce, that would be great. Please note that you must start a completely new Pika connection and set of channels per multiprocessing process. You can't share anything between processes.. Thanks for noticing this open issue @abompard . @jeremycline - any chance you could help out with PR #1180 ? I have done some work to eliminate pylint warnings for the examples/twisted_service.py file but the last few are a bit baffling to me.. Resolved by #888. Resolved by #805 . @anthonyserious this is resolved in #966 and will be released in version 1.0.0. @fscnick that may work. Give it a try and let us know. If you have code to share, even better - I can add it to our examples.. Thanks @vitaly-krugl - I updated your changes and included in #840 . We can fix this TODO once SSL tests are in place. It's related to this python SSL issue. This has effectively been in place for a while now ever since TLS/SSL was enabled on Travis, and @vitaly-krugl did the big refactor of connection workflow.. It is in place, first the test suite is run without TLS, then TLS is enabled on all connections and the suite is re-run. RabbitMQ is set up to accept TLS listeners on port 5671:\nhttps://github.com/pika/pika/blob/master/.travis.yml#L55-L61\nhttps://github.com/pika/pika/blob/master/.travis.yml#L45\nhttps://github.com/pika/pika/blob/master/testdata/rabbitmq.conf.in. Something like that! Or maybe a Travis issue?. I did all of the following investigation using Pika at tag 0.10.0, Python 2.7.13 and RabbitMQ 3.6.10\nThe provided code to reproduce this issue does indeed show it on my machine. Note: you don't need a 10 minute sleep to reproduce this - anything over 30 seconds works.\nDuring the time.sleep before sending the acknowledgement, there is a TCP send waiting in RMQ that times out, because the default TCP send timeout is 30 seconds.  At this point in time I'm not sure what part of the AMQP protocol RabbitMQ is sending to the client. More investigation is needed.\nSo, to address this issue, you have the following options:\n\nUse _channel.basic_qos(prefetch_count=1) - this does resolve the issue in my environment, probably because it consumes whatever data RabbitMQ is sending that would normally time out.\nAck the message immediately, then do your work. Of course this means that an error during processing will cause that message to be lost.\n\nPR #843 does not appear to resolve this issue.. I am going to close this as everything is working as expected -\nThe reproducer code opens a connection and channel to RabbitMQ, but does not specify basic.qos. This indicates to RabbitMQ that there should be no limit to the number of messages it can send to this client without acknowledgement. In addition, no_ack is False (the default setting) meaning that RabbitMQ will expect acknowledgement of every message delivered before de-queuing it.\nIn this scenario, the TCP connection can be thought of as a big pipe with no limits other than it's size (i.e. buffer sizes) on how much data can be pushed through it. So, RabbitMQ keeps sending data to the client while it sleeps after receiving the first message. Depending on message size and total count, the TCP \"pipe\" is big enough to hold all of these interim messages so RabbitMQ does not time out during the send operations. When message size and count is big enough to fill all buffers, RabbitMQ blocks on the send and eventually times out after 30 seconds which is what you see in the logs ({writer,send_failed,{error,timeout}}). The connection is closed and RabbitMQ re-tries sending all of those messages because none were acknowledged prior to the timeout.\nThis is one reason why it is critical to use channel.basic_qos(prefetch_count=N). That will limit the number of unacknowledged messages RabbitMQ will send to that particular client and will prevent the TCP send timeouts. This is also why @eandersson's comment about other client libraries applies since this not limited to Pika.\nbasic.qos docs. > we have not addressed why the timeout settings do not seem to be working with anyone\nIt has been long enough since I've responded that I don't remember the particulars for this issue. What timeout settings are you talking about? Heartbeats? Socket read/write?\nProviding details and (especially) configuration and code to demonstrate your issue will get the fastest response. Please remember that Pika's maintainers do so on a volunteer basis so anything you can do to help remove \"guesswork\" is greatly appreciated.. Using nowait=True, callback=None as parameters is the correct way to enable nowait without a callback.\nCallback validation could be changed to take nowait into account, and a PR providing that function with tests would be appreciated.. > Did you close it by accident?\nNo because it appears that Pika is working by design. Using nowait=True, callback=None as parameters is the correct way to enable nowait without a callback method.. @mathieulongtin - I decided that returning (None, None, None) would be less surprising behavior to users based on this comment.. When reporting an issue, please try to be as descriptive as possible which helps Pika developers resolve issues quickly.\n\nInclude the operating system, broker version (RabbitMQ version), Python version\nInclude the full set of working code necessary to reproduce the issue\nInclude a clear statement of what you expect to see and what you saw\nIf you include a stack trace, include it as a text file attachment, not in-line in the issue\n\nIf you can provide the above information, please do so and re-open this issue. Thank you!. Closing because I can't seem to reproduce:\n\nStart RabbitMQ 3.7.2 / Erlang 20.2.2\nStart consumer.py - consumer.py.txt (Python 3.6.4, master Pika code 1.0.0b1)\nBlock port 5672 - sufo nft -f ./nftables-block-5672.conf - nftables-block-5672.conf.txt\nObserve output - transcript.txt\n\nDEBUG      2018-01-31 13:35:20,472 pika.callback                  process                              220 : Processing 0:_on_connection_closed\nDEBUG      2018-01-31 13:35:20,473 pika.callback                  process                              234 : Calling <bound method ExampleConsumer.on_connection_closed of <__main__.ExampleConsumer object at 0x7fb1679578d0>> for \"0:_on_connection_closed\"\nWARNING    2018-01-31 13:35:20,473 __main__                       on_connection_closed                 90  : Connection closed, reopening in 5 seconds: (-1) ConnectionResetError(104, 'Connection reset by peer'). Please provide a complete set of code that the Pika team can use to reproduce and diagnose this issue. We do not have the time to guess and try to re-create it. If you do so, attach the code to this issue in a text file and re-open it. Thanks!. Closing this PR as your commits are included in #805 - thank you!. Thank you very much for the code to reproduce this issue.\n\nOn another note, when there is some error in the expected format of a message header, surely pika should acknowledge it before raising an exception?\n\nDoing so could result in a lost message, so we won't be changing that behavior.\nI'm investigating the decode issue now.. Here is how the RabbitMQ Java client writes a shortstr and reads a shortstr. It appears that the data is assumed to be UTF-8.. @arthur-tacca - the PR isn't ideal, but it solves the issue you raise. Thank you again for providing the report and code.. @guysoft - since this issue is closed and the last response from 2016, please open a new issue with a description of what you are trying to accomplish and what you observe. Also, provide a runnable code sample that demonstrates it. That will be the most help to Pika's maintainers, who are all volunteers. Thank you!. There should be more information provided by ConnectionClosed like error code. If you can provide more information, please re-open this issue. Thank you.. Please try this again using the latest Pika code (ideally the upcoming 0.11.0 release or the master branch). Feel free to re-open this issue if you are able to reproduce your problem.. @nicgirault - do you also see a message similar to this in the RabbitMQ logs?\n=WARNING REPORT==== 26-Oct-2016::20:53:23 ===\nclosing AMQP connection <0.4144.1> ([::1]:48194 -> [::1]:5672):\nclient unexpectedly closed TCP connection. @nicgirault is there a load balancer between your application and RabbitMQ? That is what first comes to mind when I see both sides of a connection unexpectedly close like this.. Superceded by #847 . We are working on 0.11.0. Thanks for your interest in Pika.. The tls_v1,enum_to_oid issue is a known issue. Please see this issue for more context and a resolution: ruby-amqp/bunny#303. scmb.py is not part of Pika, and we don't have the resources to address issues with 3rd-party software.. I don't see a reason for limiting this to str, and there is no mention in the commit where the change is made. If you'd like to provide a PR with tests, that would be very appreciated!. Thanks @everilae . I believe #1011 addresses this issue well enough (as pointed out by @masell), and I will backport it to 0.12. Backported in bafbf2a1f004cd6e47e11c2950ae2fb33778dcbb. Thank you! Superceded by #846. Hello! This PR is missing the referenced files. Please re-submit if you have them.\nexamples/direct_reply_to\nexamples/heartbeat_and_blocked_timeouts. Superceded by #845. Thanks!. Thank you! I have included your changes in #842 and added a test.. Please provide a PR that includes tests and reference this issue if you'd like this change implemented. It would be breaking change to the Pika API so we would have to take that into consideration.. By using nowait=True, subsequent method calls probably execute before the queue even exists. You should use the callback argument in this case to ensure that the queue exists before proceeding.. When reporting an issue, please try to be as descriptive as possible which helps Pika developers resolve issues quickly.\n\nInclude the operating system, broker version (RabbitMQ version), Python version\nInclude code to reproduce the issue\nInclude a clear statement of what you expect to see and what you saw\nIf you include a stack trace, include it as a text file attachment, not in-line in the issue\n\nIf you can provide the above information, please do so and re-open this issue.\nIn this case, I do not see AttributeError: 'NoneType' object has no attribute 'basic_publish' in the stack trace.. We should review how Twisted deals with this issue. I installed Python 3.6.3 on Windows 8.1 using chocolatey, which uses the official installer. In an interactive session, I do not see differences between errno values, which leads me to believe that Python is built with HAVE_ERRNO_H defined:\n```\nPython 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)] on win\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport errno\nerrno.EWOULDBLOCK\n10035\nerrno.WSAEWOULDBLOCK\n10035\nerrno.WSAECONNRESET\n10054\nerrno.ECONNRESET\n10054\nerrno.WSAECONNABORTED\n10053\nerrno.ECONNABORTED\n10053\nerrno.WSAEWOULDBLOCK\n10035\nerrno.EWOULDBLOCK\n10035\n``. Closing as this is a very old version of RabbitMQ. If this can be reproduced using the latest version (3.6.10), please re-open.. @hexadite-grishak - I believe pull request #886 addressed this issue. If not, please re-open this issue. Thanks!. @westphahl I can't reproduce the error you show while testing against RabbitMQ3.6.10stable` branch. I've attached the script as well as output from runs using python 2 and 3.\nrepro-py3.txt\nrepro.py.txt\nrepro.txt\n\n\n\n. @westphahl - I am re-trying on Arch Linux today.. Couldn't reproduce on Arch Linux. I opened #843 with a change that ensures the value returned by _acquire_event_dispatch() is valid throughout its use. See this discussion. There is not enough information to debug this issue. If you are able to provide code that reliably reproduces this issue, please do so and re-open this issue. Thanks.. Please follow issue #464. At this time, you must implement your own verification.. Yes, those look to be the correct properties to check if Connection and Channel objects are open.. Resolved by #818, thanks!. @behconsci - here's the problem. The default parameters for basic_cancel will cause that exception to happen every time, because nowait defaults to False which requires a callback, but that parameter defaults to None. asgi_rabbitmq has already addressed this here, so you need to upgrade asgi_rabbitmq.\nWe may consider changing the method signature to prevent this from happening in the future.. @behconsci thank you for letting me know that fixed your issue.. Yep, resolved by #941. There is what appears to be a pretty complete code example here. If that doesn't demonstrate what you are looking for, feel free to re-open this issue. Thanks!. One option is to stop and restart your broker. If you'd like to discuss this further, I suggest the rabbitmq-users mailing list. Thanks!. One suggestion - to figure out what command to run tests, I had to look at .travis.yml. The test suite ran successfully using python 2 and 3 \ud83d\udc4d . > I get the above error message\nWhere / how do you get that error message?\nCould you please provide more details on how to reproduce this? Providing code that reproduces this issue reliably would help us diagnose and fix this issue quickly.. What would be most helpful is a complete set of code that reproduces the issue - both sending and receiving. People have limited time to spend on Pika support so anything that can be done to shorten the diagnosis process is greatly appreciated.. Thanks! No hurry at all. I'll see what I can figure out on my end as well when I have time.. I can't reproduce this using RabbitMQ 3.6.10, Arch Linux, python 2.7.13 and 3.6.2 using the attached code.\nrepro.py.txt\n. Thanks for the additional information. There was no mention of Docker in your original report. I have saved your stack trace in a text file and attached it to your comment to keep from cluttering this page up.. I tried reproducing this issue using Docker without success. Here is my environment:\n```\n$ docker version\nClient:\n Version:      17.10.0-ce\n API version:  1.33\n Go version:   go1.9.1\n Git commit:   f4ffd2511c\n Built:        Wed Oct 18 23:08:56 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      17.10.0-ce\n API version:  1.33 (minimum version 1.12)\n Go version:   go1.9.1\n Git commit:   f4ffd2511c\n Built:        Wed Oct 18 23:09:11 2017\n OS/Arch:      linux/amd64\n Experimental: false\n```\nI have attached a transcript showing debug output and successful reconnection, even if I use docker stop and docker start on the image. I suspect this issue could be related to Docker on OS X.\ntranscript.txt\n. Closing as issue is discussed in this rabbitmq-users thread.. @mvallebr - please feel free to re-open this issue if you are able to provide code that reliably reproduces your issue with pika and RabbitMQ logs included. Thanks!. Thanks! Link it to this issue and re-open.. Thanks @LukaAndrojna \n@Panda-Master - Pika uses Python's standard logging module, so you can also find information in the docs and by searching online.. I can't reproduce this using RabbitMQ 3.6.10, Arch Linux, python 2.7.13 and 3.6.2. You don't specify your environment or provide server logs, both of which would be helpful.\nI added a couple print statements to your code:\n```python\nfrom pika import BlockingConnection, ConnectionParameters\ndef callback(channel, methods, props, body):\n    print(\"callback start\")\n    channel.basic_publish(\n        'exchange_not_exist',\n        routing_key='123',\n        body='Nope this is wrong'\n    )\n    print(\"callback exit\")\ndef main():\n    conn = BlockingConnection(\n        ConnectionParameters('localhost')\n    )\n    ch = conn.channel()\n    ch.exchange_declare('egress', exchange_type='topic', durable=True)\n    q = ch.queue_declare(exclusive=True).method.queue\n    ch.queue_bind(q, 'egress', 'logs.#')\n    ch.basic_consume(callback, q, True)\n    print(\"Starts consuming.\")\n    ch.start_consuming()\n    print(\"I'm done.\")\nif name == 'main':\n    main()\n```\nOutput:\nsh\n$ python ~/Projects/issues/pika/gh-841/repro.py \nStarts consuming.\ncallback start\ncallback exit. Thank you for the additional information. I understand what you were first reporting now and will take a look at it.. @streamliner18 I am re-visiting this issue. I am seeing this behavior in 0.10.0 as well as the latest code. Not sure why at this point.. @vitaly-krugl does this PR resolve the issue for you as well?. Finally, a build passed. Let me open a PR with those Travis CI changes.. @oleynikandrey - I have a quick question - are you using the connection_attempts parameter to set a value greater than 1 (the default)?. Thanks. If you have time, could you please try setting connection_attempts to 10 or so? I don't think it'll make a difference but I'd like to check.. @kmonson you're more than welcome to give the cainbit:gevent_connection_adapter branch a try. If you have success it will give us more confidence in the changes. I haven't had time to review them or add this adapter to the test suite.. @13steinj as you can see, there are no updates. We would appreciate tests being added for this code, so feel free to add them. Thanks.. @13steinj - feel free to fork this repository and merge in cainbit:gevent_connection_adapter or start from scratch. Open a PR from your fork & branch and we'll review it. Thanks!. * What version of Pika are you using?\n What version of Python are you using?\n What operating system and version are you using?\nThis is probably the same issue as #640. Thank you for the additional information.. @lanzz can you provide code to reproduce this or instructions on how to reproduce? Thanks.. Thank you. @huntercatalan - please refer to the settimeout documentation. Passing 0 has the same effect as putting the socket into non-blocking mode (i.e, no timeouts). You should use None to actually disable timeouts.. Thank you for the PR and tests! I have opened #880 which includes your commits. I'm going to add a couple other changes there.. Thanks!. Thanks!. Thank you!. @adriancarayol - you appear to be sending invalid data to RabbitMQ, possibly by re-using a connection.\nWe can't diagnose this issue without code to reproduce it. If you can provide complete code to reproduce this issue and instructions on how to do so, please attach it to this issue and re-open it. Thanks!@. Hi @apisarek -\nThis behavior has existed for 2 years and ensures that sane defaults are used for new connections. I'm not exactly sure what issue you're running into so providing example code showing that would be very helpful.\nI will close this issue now but please feel free to re-open it if you can provide code showing your use case.. @mosquito @eldardamari\nCould you please provide a complete set of code that reproduces this issue? That would speed up diagnosis and a resolution. Thanks!. @mosquito @eldardamari - I will close this issue but please re-open it if you can provide code to reproduce.. @jbfondo thanks for the script, I will give it a try.. @jbfondo - you absolutely can't do what you're doing in that code. Channels are not thread safe.\nI am going to do more research into mosquito/aio-pika#54, though nobody has provided code to reproduce this issue.. @alfredodeza I don't see how that error is similar to what else is reported here. If you can provide a runnable code sample that I can use to reproduce this, please do so and open a new issue. The Pika maintainers do so on their free time so anything that can help us out is much appreciated.. It's no problem! Thanks for clarifying and a standalone app will help me out immensely to fix the issue since I won't have to spend time guessing how to reproduce it.. Hello @zhangzx7054 -\nSince you are using a BlockingConnection, heartbeats will only be processed if you actually do something with the channel. You can see this behavior with the following code:\n```\nimport logging\nimport pika \nimport time\nlogging.basicConfig(level=logging.DEBUG)\ncreds = pika.PlainCredentials(username='guest', password='guest')\nparams = pika.ConnectionParameters(host='localhost', credentials=creds,\n                                   heartbeat_interval=10, blocked_connection_timeout=10)\nconnection = pika.BlockingConnection(params)\nchannel = connection.channel()\nwhile True:\n     print(time.strftime('%c', time.localtime()))\n     channel.basic_publish('', 'my-alphabet-queue', 'abc')\n     connect_close = connection.is_closed\n     connect_open = connection.is_open\n     channel_close = channel.is_closed\n     channel_open = channel.is_open\n     print(\"connection is_closed \", connect_close)\n     print(\"connection is_open \", connect_open)\n     print(\"channel is_closed \", channel_close)\n     print(\"channel is_open \", channel_open)\n     print(\"\")\n     time.sleep(5)\n```\nIf you use one of the asynchronous adapters for your client code, you will also see the heartbeat mechanism detect lost connections. Please refer to the example code.. Hello, thank you for your report. Please upgrade to Pika 0.11.0 to see if that resolves your issue. If it does not, feel free to re-open this issue. Thanks!. @raidoz - I am unsure how to reproduce this. I ran this example code using twistd -ny ./twisted_service.py, published a message to the \"task\" queue, then killed beam.smp via kill -9. I have attached the output as a file.\nI will close this issue for now but if you can provide a code sample that I can use to reproduce I am happy to fix the underlying problem. Thanks.\ntwisted-log.txt\n. @adriancarayol - the project you provided does not use the Pika module. I am unable to continue:\n```\n$ git remote -v\norigin  https://github.com/adriancarayol/issue-871 (fetch)\norigin  https://github.com/adriancarayol/issue-871 (push)\n$ git grep -i pika\nrequeriments.txt:2:pika==0.10.0\n```\nAlso see this search on GitHub: https://github.com/adriancarayol/issue-871/search?q=pika\nIf you update your project, let me know by re-opening this issue. Thanks!. @adriancarayol I have attached a transcript of trying to reproduce the issue.\nPlease note that I am using Pika 0.11.0 as that is the current version\nWhen I browse to localhost:8000/example, I get the following exception:\n2017-10-10 20:28:37,338 - INFO - server - Listening on endpoint tcp:port=8000:interface=127.0.0.1\nException in thread Thread-3:\nTraceback (most recent call last):\n  File \"/home/lbakken/.pyenv/versions/2.7.14/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n    self.run()\n  File \"/home/lbakken/.pyenv/versions/2.7.14/lib/python2.7/site-packages/channels/management/commands/runserver.py\", line 176, in run\n    worker.run()\n  File \"/home/lbakken/.pyenv/versions/2.7.14/lib/python2.7/site-packages/channels/worker.py\", line 87, in run\n    channel, content = self.channel_layer.receive_many(channels, block=True)\n  File \"/home/lbakken/.pyenv/versions/2.7.14/lib/python2.7/site-packages/asgiref/base_layer.py\", line 43, in receive_many\n    return self.receive(channels, block)\n  File \"/home/lbakken/.pyenv/versions/2.7.14/lib/python2.7/site-packages/asgi_rabbitmq/core.py\", line 822, in receive\n    return future.result()\n  File \"/home/lbakken/.pyenv/versions/2.7.14/lib/python2.7/site-packages/concurrent/futures/_base.py\", line 429, in result\n    return self.__get_result()\n  File \"/home/lbakken/.pyenv/versions/2.7.14/lib/python2.7/site-packages/concurrent/futures/_base.py\", line 381, in __get_result\n    raise exception_type, self._exception, self._traceback\nValueError: Must have completion callback with nowait=False\ntranscript.txt\n. @adriancarayol if I use Pika 0.10.0 your example works. I suspect there is a change in 0.11.0 that asgi_rabbitmq is not handling. I'll see what I can do. There's no sense in reproducing or fixing this for an old version of Pika.. That may be the same cause. Framing errors can have many different causes.. @kfrendrich can you provide code to reproduce this? Can you reprodue this using Pika version 0.11.0?\nI edited your comment because it was too long and did not provide valuable information. In the future, please save long output in a text file and attach it to your comments. Thanks!. @kfrendrich - that is not the same problem. That ValueError is due to a change in the Pika API that asgi_rabbitmq must take into account.\n@proofit404 I am going to open a PR to integrate Pika 0.11.0 into your project sometime today - have you also started work on that?. @adriancarayol - when you say \"problem solved\" does that mean the \"frame error\" (i.e. the original problem reported in this issue) is solved as well?\nI've made the same change and am running the test suite now. @proofit404 - PR on the way.. Resolved by proofit404/asgi_rabbitmq#14. @marijngiesen - I can see that you're using Python 3.6.1 on OS X, and your code is using the multiprocessing module. I'm wondering if you're running into this scenario.\nIf you can provide code that reliably reproduces this issue, feel free to re-open this issue and I'll take a look. Thanks!. @radzhome could you please provide a complete code sample that I can run? Thanks. I am currently investigating #841 but I see the same behavior in 0.10.0 as the current code in that issue.. @streamliner18 - please give these changes a try. Thanks!. @radzhome - if you have a chance to check out this PR, I would appreciate it. Thanks!. I'm not thrilled with catching struct.error but I expect this to be a rare occasion and it seems preferable to making this comparison every time - -2147483648 <= value <= 2147483647. Hi Alex,\nAll connection adapters use a blocking connect at this time, including SelectConnection.\nAs you can see in #100, this is a long-standing issue. I also have found some historical information in this thread.\nAt this time I can only recommend setting an appropriate timeout value.\n@gmr @michaelklishin what do you think about addressing this in 0.12.0?. > Would really appreciate an update on this\nIs there something else you need? It may be a while before this gets addressed as Pika's maintainers (myself included) spend most of their time working other projects (like RabbitMQ). I have tried to remain responsive to issues and pull requests, and a PR to address this issue would be much appreciated. Thanks again!. Hi Bogdan -\nSorry it has taken a while to review how you've modified Pika to suit your needs. If you would like to submit a pull request with proposed changes (like this suggestion), I would be happy to review them.\nThanks! \ud83c\udf84 . Hello!\nYour error contains the string tls_v1,enum_to_oid,[28] which is a common error when you use an old Erlang version. You need to use a more recent version, preferably Erlang 19.3 or 20.1.\nIf you upgrade and that does not resolve the issue, please re-open this issue. Thanks!. @calJ2016 without more information I can't help. Please provide a complete code sample that reproduces this and re-open this issue. Thanks.. Could you see if there are other Python projects that have this issue on GCP and what their workaround is?. Thanks everyone for the PR and the input.. Hello -\nRetrieving the unacked message count is not part of the AMQP specification. You can, however, query the RabbitMQ REST API to retrieve the count. This is not a feature that Pika will support.\nMore info here: https://stackoverflow.com/q/13721066. @jdavid please check out issue #158 where a suggestion to pass arbitrary user data to callbacks is suggested.. This will be resolved by #158. \"just following the tutorial\" - could you please link to the tutorial you are using? It may be out of date.. @bartverstrynge - the value returned by consume is a generator, which means you must iterate over it and not try to unpack it as though the value is a tuple. This is why the example here uses a for loop.\nThis code may make it a little clearer:\n```python\nimport pika\nimport pprint\nconnection = pika.BlockingConnection()\nchannel = connection.channel()\nresult = channel.consume('test', inactivity_timeout=1)\npprint.pprint(result)\ntry:\n    a, b, c = result\nexcept ValueError as err:\n    pprint.pprint(err)\n    for val in result:\n        pprint.pprint(val)\n```. It's no problem, I'm glad to help out. Frequently when I don't understand how an API is supposed to work I'll check out the test suite, too. Have a good weekend \ud83d\ude3a . Hello - thanks for providing code. Reading your description it's not exactly clear what works and what doesn't, because I'm not exactly sure what \"comment out the def callback\" means.\nIf you could provide two receive.py files - one working, and one not, that would be great.\nAlso, Pika is not tested using Cygwin. Could you try the native Python 2.7 build for Windows?. Thanks again for those files. I am unable to reproduce your issue using Python 2.7 on Windows 8.1 (the VM I have available). Here is what I tried:\n\nInstalled RabbitMQ 3.7.0 on the VM\nInstalled Python 2.7.14 from here using the MSI installer.\nModified your code to connect using this URI: amqp://guest:guest@127.0.0.1/%2F (%2F is the default vhost / url-encoded)\nSet the PATH to include the values you see in the screenshot.\nUse pip to install pika\nRun the program. As you can see, it waits for messages until I enter CTRL-C:\n\n\nAt this point I'm not sure what is the issue in your environment. I would make sure that you are using the python executable that you expect, or give it a try on a different Windows machine/VM.\nHere is the exact code that I ran -  notice that I removed the comment from the queue_declare method call:\n```python\nprint ('importing pika')\nimport pika\nprint \"started\"\nconnection = pika.BlockingConnection(pika.URLParameters('amqp://guest:guest@127.0.0.1/%2F'))\nchannel = connection.channel()\nprint \"hey there\"\nchannel.queue_declare(queue='hello')\ndef callback(ch, method, properties, body):\n    print \" [x] Received %r\" % body\nchannel.basic_consume(callback, queue='hello', no_ack=True)\nprint ' [*] Waiting for messages. To exit press CTRL+C'\nprint \"waiting\"\nchannel.start_consuming()\nprint \"done consuming\"\n``. Oh, OK, that's what wasn't clear. When I asked about Cygwin, I thought you were using *both* the Cygwinbashshell and Cygwin-installed Python. Yes, if you mix Windows-native Python with the Cygwinbash` shell I'm sure weird things happen.. @vitaly-krugl thank you for adding your input.. Hello!\nStarting in Python 3.3, the exception raised is OSError. I will incorporate this fix into a new PR today. Thanks!. socket.error was replaced by OSError in Python 3.3, but some instances of socket.error remain.. Thank you! I have opened #908 which contains a similar fix.. This is a Python related question, not Pika. Pika is only a library for communicating with AMQP brokers. You must use another library or software package (like Celery) to create a worker pool.. @michaelklishin this commit has the changes, and the next has changes due to YAPF formatting as required.. @zjj - if you could review this, that would be great. Thanks!. Tested on Windows 8.1 VM using Python 2 and RabbitMQ 3.7.2. @ccp-codex - Hello! Are you working on finishing up #860?. It would be great to have help with #860 if you have time. Thanks!. Thank you!. Could you please provide the complete set of code you're using? That would make it faster and easier for me to reproduce. What version of Erlang are you using and on which operating system? Thanks!. Thanks, I'll check it out today.. @sbidin - I have an environment set up and have run this a couple times. In order to determine if the ack/nack issue has happened, are you searching the output of the program? If so, could you modify the code to \"keep track\" and more verbosely indicate when this situation occurs? This will, again, assist me in determining what is happening.\nI'm assuming this situation only manifests when the consuming process is unable to keep up with the producer.. Thanks for the updated script. I'm not seeing the got retroactive nack message in the output, however. Could you give me a few more details?\n\nAre you running the producer/consumer processes on the same machine as RabbitMQ? (I'm assuming so...)\nWhat version of Python are you using?\n\nWhat are your machine specs?. Thanks, that's what I'm looking for. One last question - this isn't a cloud environment, right? I'm just trying to figure out what could be different between your environment and mine.. Removing this from the 1.0.0 milestone as I still can't reproduce.. @vitaly-krugl that makes sense to me. Since I can't reproduce it, I'll close it. @sbidin if you can try out the suggested code, that would be great.. @vitaly-krugl - PR #991 addresses this issue and #990. I had only linked it to #990, not realizing it addresses this issue as well.. Thanks for the info. I have some follow-up questions:\n\n\nHave you tried Erlang 20.2 and newer? 19.3?\n\nWhat version of Python are you using?\nWhat operating system and version is running RabbitMQ and your Python code?\nCould you describe your environment? What are the specs of the machines running RabbitMQ, and your producers & consumers?\nCould you share code to reproduce this issue?. Thanks for all of the information. Python 2.7.5 is very old - I wonder if using 2.7.14 or the latest 3.6 version would work. If possible, could you try that out?\n\nWhen this condition happens, do you notice anything in the RabbitMQ Management UI with regard to the exchange and queue being used? I assume you see consuming drop to zero and queued messages increase.\n  . >  I did however go back to default max_frame and heartbeat settings\nThanks - I have some questions:\n\nWhat value were you using for max_frame?\nYou have always been using Erlang 20.1.7.1 for all of your tests, correct?\nYou have always had issues with RabbitMQ 3.7.2 no matter the max_frame setting, correct?\nRabbitMQ 3.7.0 was only stable once you used the default max_frame setting, correct?\n\nI wonder if, somehow, this is related to rabbitmq/rabbitmq-java-client#341. Thank you for the update! I will close this issue.. > 'A non-string value was supplied for self.routing_key'\nThis means that the value of properties.reply_to was not a string, which is required.\nYou don't check to see if a queue exists - just declare it and if it doesn't exist it will be created.. Working on those conflicts now. OK, I'll take the implementation and make it internal to pika. From what I found, the version we have is what ships in recent Python versions. Thanks for the heads-up.. @vitaly-krugl thanks for reviewing our recently merged PRs \ud83d\ude04 . @vitaly-krugl  - I'm not sure. I think it has to do with test coverage specific to the changes in this PR. I'm not too concerned since everything else passes. What do you think?. Thanks for the PR.. Thanks! I think that may be the last remaining use of type that doesn't have a Pika version check.. We would be glad to accept a PR to address this, thanks!. You are connecting to port 5672 which is the default, non-TLS/SSL port for RabbitMQ. I can reproduce this error by connecting to that port. Be sure that you are connecting to the TLS/SSL port of your RabbitMQ server. If you are using the configuration from that example, the port is 5671. Thank you! I have opened #924 which includes your commit as well as one small change.. @hugovk - we would be happy to review a pull request with that change.. This is the default for the following methods as well\npika/channel.py:202:    def basic_cancel(self, callback=None, consumer_tag='', nowait=False):\npika/channel.py:542:    def confirm_delivery(self, callback=None, nowait=False):\npika/channel.py:844:    def queue_purge(self, callback=None, queue='', nowait=False):. Re-opening because other methods that default nowait=False do not check their callback parameter.. See #688, #759 . Resolved by #941 . Pika's maintainers would rather not use GitHub issues for diagnosis. Could you please post this to rabbitmq-users? If you are using a proxy, please indicate that, as well as provide the following information:\n\nRabbitMQ and Erlang version\nLibrary versions (like you provided here)\nOperating system and version\nVirtual environment, if used (cloud vendor, VirtualBox, etc)\n\nIf, after discussion on the mailing list, an actual issue is found, we can open it here. Thanks.. #752 is the closed issue. I deleted your comment there.\nCan you please provide a self-contained program that demonstrates this issue?. Fixed by #956 . Thanks. Is there any way to reproduce this?. We would be happy to accept a pull request with this change. Thanks!. Resolved by #932 . Thanks @gmr . Hello Dimitry,\nI see that this issue is a duplicate of the question that you asked on Stack Overflow. Pika's maintainers prefer to not use GitHub issues to answer questions or do issue diagnosis. Once we confirm that an actual bug is present, an issue is appropriate.\nIn your case, you must run self.process in such a way as to not block SelectConnection. This is a general Python programming question and not specific to Pika.. @gbartl - don't worry so much about 2.6. We need to drop support for that ancient version anyway. If anything, detect 2.6 and disable the changes you're making.. We may want to consider dropping pyev support since it is basically unmaintained. @gmr what do you think?. We'll get this once #937 is merged.. @hugovk looks like pinning awscli to version 1.11.18 could work. Waiting on builds now.. Well, 3.7.-dev is fixed, but now there's this - @gmr is this due to the older awscli version?\n$ aws s3 cp --recursive s3://com-gavinroy-travis/pika/$TRAVIS_BUILD_NUMBER/ coverage\nfatal error: Unable to locate credentials\nThe command \"aws s3 cp --recursive s3://com-gavinroy-travis/pika/$TRAVIS_BUILD_NUMBER/ coverage\" exited with 1.. Those are arguments to RabbitMQ, not to your callback. Please see #158 for solutions to what you're trying to do.. We would gladly accept a pull request with suggested changes. Thanks!. @vitaly-krugl thanks for pointing out how those libs use *args, **kwargs - I assume that is what you'd like to see this project implement?\nPlease feel free to rename other parameters you think are confusing and push to the pika-925 branch.. @vitaly-krugl thanks for the info. Should I modify the two documents that mention yapf?. @vitaly-krugl - This should be ready for a final round of review. I won't push anything else to this branch until I hear back. After that, I will update the changelog with details about what has changed in the API.. > Did examples and docs not need to be changed as the result of arg reorder in basic_consume?\nI will re-review both, as well as get the change log next.. What version of RabbitMQ are you using?. @jacky1193610322 - reject-publish is supported starting with RabbitMQ 3.7.0. If you can reproduce this issue using the latest version of RabbitMQ (3.7.3)  re-open this ticket.. We would welcome a PR to change this, thanks!. We would be happy to accept a PR to address this. Please include tests.. If you can provide a self-contained code sample and instructions to reproduce, that would help out. I'm not sure what you mean by \"passively declare\", for instance.\nIf you find that WARNING to be annoying, please provide a PR to change or remove it.. @sjlongland I can reproduce this issue using your code and only 3 channels. Everything exits as expected when 2 channels are used.. @sjlongland could you please edit your original description as it not the issue that your test code demonstrates? Another option would be to open a new GitHub issue for the second 'channel.open' seen issue if you can provide code that reproduces it.. This is a tricky one, but I do see some cases with three channels where it runs successfully, and cases where it doesn't, with packet captures for each. If you're interested, I forked your test code here and addressed one bug in it (self._all_channels_opened never reset to False) and _stop_test won't allow itself to runmultple times. Still, I see the issue on occasion.. @sjlongland if you'd like to test out my fix in the pika-945 branch, I would appreciate it.. That's strange. What commands are you running?. @sjlongland since I am not seeing this error, nor do we see it on the build servers, I can't diagnose unless I know exactly what commands you're running in your environment. If you can provide those, great, otherwise there should be a beta release of 1.0.0 this week. Thanks!. @sjlongland - could you add pika.compat to this line, re-build your package, and let me know if that resolves the issue? Thanks. Thanks for letting me know, I will address the setup.py issue.. We had a discussion about no_ack in issue #951 and decided that since we're working on 1.0.0 we would not be concerned about backwards compatibility. If you'd like to chime in, see issue #977.. PS @sjlongland thank you for all the testing you have been doing.. @vitaly-krugl I'll get it today. I didn't notice that \"DO NOT MERGE\" had been removed from the title.. @vitaly-krugl - this should be all set. I will address #951 next.. @vitaly-krugl - just FYI, this is ready for your review. Thanks!. Hi Ronald -\nWithout a lot of demand or community help in implementing AMQP 1.0, I doubt that Pika will support it. AMQP 1.0 is a brand-new protocol and is in no way a successor to AMQP 0.9.1.\nYou may find this list helpful to track down a Python client you can use.\nI'm going to close this issue, but @gmr may wish to add input.. IMHO we should use the same name as the other clients listed.. OK, if we decide to rename this, we could use a deprecation scheme like bunny. @vitaly-krugl what do you think? I'll be honest, I had to re-read how no_ack is used a couple times to \"get it\" at first but I have no idea how much this affects the typical first-time Pika user.. OK, I read @gmr and @vitaly-krugl's comments as \"leave it alone\". Good enough for me! Thanks for chiming in.. Thanks!. @vitaly-krugl I'm wondering about how I can replace the existing behavior with regard to consumer tag with a server-generated one, in the case where a completion callback is provided. As you can see, the code remembers the Pika-generated consumer tag for use later on. I think I should be able to generate a \"temporary\" consumer tag and then replace that with the server-generated one when the basic.consume-ok message comes back, but I thought I'd ask in case you can think of issues that could cause.\nI will also check to see what other libraries do like bunny or the RMQ Java Client ... they may always use client-generated consumer tags.. > Still, continuing to generate the consumer tag in pika seems like the safest and simplest approach with no apparent downside\nAgreed, I'll do that.. @vitaly-krugl - what do you think of naming callback parameters in the same style as in asyncio? For instance, instead of a generic callback parameter on method completion, it would be called method_completed (i.e. for basic.consume-ok). Likewise, ack_nack_callback would be split up into message_acked and message_nacked, in in the basic.consume case, we could call it message_delivered.\nThis would be done in a separate PR, of course.. > I wanted to push a small PR to your fork/branch, but couldn't find the source fork and branch of your PR, and couldn't find how to do it by googling. Do you know how to discover it in github?\n@vitaly-krugl - I don't use a fork, and just push branches to the github.com/pika/pika repository. Always feel free to make changes to my branches if you'd like. This branch is pika-953. I don't see a reason to use a fork if I have push access to a repository is all \ud83d\ude38 . Well, there is that. You used to not be able to add commits to branches on forks unless you collaborated on the forked repo, either, but that has changed. Maybe next PR.. @vitaly-krugl - all set, thanks!. My two cents -\n\nI think that supporting the old argument is counter-productive\n\nIt is a 1.0.0 release so not supporting the old argument is to be expected, I think.\n\nNot to be disrespectful, but I question the value of this proposition. I haven't experienced confusion over the use of no-ack, which is at least consistent with the AMQP spec, and haven't encountered user confusion over it (nothing that really stood out, anyway), having thoroughly scrubbed pika issues a couple of years ago\n\nIf I were a new user of this library, I would have to look up what no_ack or auto_ack means anyway. Maybe I could guess what the latter means from its name, but I would have to be familiar with how acknowledgements work with AMQP to begin with.\n\nIs no_ack the only significant way that Pika args differ from the other AMQP clients under the RabbitMQ umbrella?\n\nProbably (?) I did a quick comparison for basic.consume, basic.get and queue.declare between Pika and the .NET client.. I'm glad we received feedback from a couple users - thanks @kmonson and @shinji-s \nIf we assert on no_ack, then we ought to do the same for other parameters that have changed.. @shinji-s pointed it out -\nThe command \"mkdir coverage\" exited with 0.\n0.49s$ aws s3 cp --recursive s3://com-gavinroy-travis/pika/$TRAVIS_BUILD_NUMBER/ coverage\nfatal error: Unable to locate credentials\n@gmr - would you have time to address this? Thanks!!!. Reviewing now. Great work, this will be very useful for Pika users. @vitaly-krugl - all set for review 2, thanks. @vitaly-krugl - all set for review 3 \ud83c\udfa3 . @vitaly-krugl thanks for providing the comment you would like to see. I added it to the docstring for _drain_blocked_methods_on_remote_close. Let me know what you think.. @vitaly-krugl OK comments addressed. Thank you for the thorough review.. \ud83c\udf89 . Thanks!. \ud83d\udc4d from me. Reviewing this today. > Are you going to include it in 0.11.3 and what do you expect to be the release date?\nWe're working on 1.0.0. There is no definitive release date at this point. Once we announce beta versions of 1.0.0. we would really appreciate it if you test them. If this issue continues to persist, please-re open this GitHub issue (#963). I will be reviewing #956 soon (today is a US holiday). At that point you can give master a try. Thanks!. @adah1972 \nThanks for providing your test code. We'll check out basic_cancel.\n\nThe change is neither backward-compatible nor forward-compatible. \n\nDoing a major version increment allows this. Someone upgrading from Pika 0.11 should expect incompatibilities, they should review their code and they should run tests before considering deployment into a production environment. As @vitaly-krugl pointed out, there are many inconsistencies in Pika's API that we intend to address in the 1.0.0 release.\nThere were places in Pika's API where the callback preceded other arguments, and places where the callback followed other arguments. We have decided that callbacks should be at the end of the parameter list.\nIn the case of basic_consume and other, similar API changes we can check the value of queue to see if the user has passed-in a callable and raise a more helpful exception.\nThere is a lot of work to be done for 1.0.0 and complete documentation of the API changes is planned as part of it.. @stugots - yes it applies to all operations.\nPlease give Pika 0.12.0b2 a try, which contains #956 and allows methods to be executed on the correct thread via add_callback_threadsafe.\nPika 0.12.0b2 can be installed with this command:\npip install pika --pre\nThank you!. @ehp006 -\nhttps://groups.google.com/d/topic/pika-python/2pyZG-cB5_g/discussion\nhttps://groups.google.com/d/topic/rabbitmq-users/Ch4EYLm5RSE/discussion\nhttps://twitter.com/RabbitMQ/status/1009129541472645120. Reviewing this now :smiley_cat: . Hello, please see the discussion in #636, #666, #874 and the comment made in 3027890081adaa067268aa4839638a32734c263f by @michaelklishin.\nFinally, @vitaly-krugl made this comment which applies as well. If you sleep longer than the heartbeat interval while using BlockingConnection, you will see exactly the issue that you report. You should use BlockingConnection.sleep or BlockingConnection.process_data_events. Re-opening to address the misleading comment.. I'll let @vitaly-krugl and @michaelklishin comment on the decision that was made.. Fixed in #966 . Please see the comment I added for #965. Certainly, please add the verbiage that @vitaly-krugl requested and a test, if possible.\nThere is also this comment to address.. Do you think it would be possible to automate the selection of the correct connection class based on what library is available and / or what IOLoop may already be running? The Cassandra python driver does something like that.. Hello,\nPlease see this code which works correctly. I tested it using Python 3.6.4 and the master branch of Pika, which will become version 1.0.0 within the next few weeks.\nNote that it is unusual to close a connection without having done anything with it, like start a consumer.. > Side note: I noticed that stop_ioloop_on_close constructor parameter has been removed. Has that functionality been deprecated or is there a new API?\nYes, please note that I tested using master which will become 1.0.0. There are quite a few API changes that will be documented.. Thanks @vitaly-krugl !!!. At first glance no issues are immediately apparent. I assume you based your code on this example code.\nPlease provide a complete working set of code that executes AMQPconsume. In addition, let us know what version of RabbitMQ and Erlang you are using, what operating system, and the full logs from RabbitMQ. Once provided, please re-open this issue. Thanks.. I can't reproduce the issue using python 2.7.14 or 3.6.4. I used a slightly modified version of your code.\nSomething is closing your connection between your Python code and RabbitMQ. If you have a proxy, firewall or other network device, I would start looking there.. We've had one user (@adah1972) characterize the proposed API changes as \"dangerous\". #951 contains discussion with regard to the merits of backwards-compatibility and the amount of hand-holding that Pika will provide with regard to the rename of no_ack to auto_ack.. @vitaly-krugl apologies that I missed your question and thank you @anton-ryzhov for answering it.. I doubt Pika will add packages in the future. The issue around compat was all my mistake in thinking it had to be a package.. I'm working on this now. It looks like Travis CI runs RabbitMQ on an ancient version of Erlang (R16B03), so this will not be a trivial thing to implement.. I've got TLS testing working on my pika-981 branch as you can see here. It seems like a new test is failing randomly on pypy. I should be wrapping up Windows today as well.. Yep! It'll be back soon(ish). @vitaly-krugl you may want to review and ignore white space changes by adding ?w=1. All expected builds pass (coverage fails on PRs still). Thanks @vitaly-krugl. @vitaly-krugl good find, this commit will address it: https://github.com/pika/pika/pull/987/commits/3f0e73951ae3d3b8d71dafb609ef04ae5e7bc5f7. Thanks for the report. I can reproduce with the attached file.\nrepro.py.txt\n@vitaly-krugl thoughts? Some operations (like consume) have an inactivity timeout.\nI don't think this is a high-priority issue to fix at the moment.\n. @vitaly-krugl the debug log shows the TCP connection is established, and the AMQP0091 message is sent (I can see that in Wireshark). There is no response and the handshake never completes. Redis never shuts down the TCP socket, and Pika doesn't have a timeout for the handshake completion, so the connection just stays open forever.\nI'll attach the debug log and Wireshark trace when I can.. @vitaly-krugl done! Thanks.. @quozd when it is done. There is no ETA at this time. You could start testing the master branch and we would be very appreciative. Please note that there will be incompatible API changes in version 1.0.0.. @vitaly-krugl awesome, I'll review today or tomorrow. That's what I'm planning, if you still think the idea is a good one.. And if you'd like to keep things the way they are now, that's great too and I'll begin documenting the API changes.. Great, I'll start on some API notes and will make sure all the docstrings are accurate.. This has been supported since this commit - https://github.com/pika/pika/commit/5dc90ad8bce20687009d9e9ed358c84c4666d3f1. We would be happy to accept a pull request that improves the documentation and examples. Thanks!. Thanks for the report. For what it's worth, this also reproduces the issue:\n```python\nimport logging\nimport pika\nlogging.basicConfig(level=logging.DEBUG)\nc = pika.BlockingConnection()\ntry:\n    ch = c.channel()\n    ch.queue_declare(queue=[1, 2, 3])\nfinally:\n    c.close()\n```\nI am testing a fix now.. This applies to BlockingConnection: any time self._blocking is set to a method name in Channel's _rpc method and an exception is thrown by subsequent code, an attempt to close the connection after will fail because the Channel.Close method will be blocked by the previous method (that threw the exception). So, everything comes to a grinding halt. When using the above code, you can see execution continue when CTRL-C is entered.. One second and I'll have that information.. > Does this issue not exist in an asynchronous connection?\nNo. Modify the async consumer example so that QUEUE is [1, 2, 3] and you'll get this output - \nasynchronous_consumer_example.py-out.txt\nHere is the debug output when using BlockingConnection - \nrepro.py-out.txt\nHere is the code I used to get the above output -\n```python\nimport logging\nimport pika\nlogging.basicConfig(level=logging.DEBUG)\nwith pika.BlockingConnection() as connection:\n    channel = connection.channel()\n    channel.queue_declare(queue=[1, 2, 3])\n```\n. @vitaly-krugl no hurry at all! Thanks again.. @vitaly-krugl the queue.declare method never makes it to RabbitMQ. An AssertionError is thrown here which gums up the works when the with clause tries to exit.. I'll try that out.. @vitaly-krugl - I have merged in the tests you provided and this is ready for another review. Thanks!. @vitaly-krugl if I re-select your name in the \"Reviewers\" dropdown, the status icon changes back to an orange disk ... do you not get a new email saying I re-requested a review? I assumed that you did. If you don't get an email, I can @-mention you in a comment. Thanks for the re-re-reviews \ud83d\ude04 . @vitaly-krugl thanks! Sorry I missed the previous comment about that test.. @vitaly-krugl I made your latest suggested changes and rebased against master. I'm waiting on the builds at the moment. Thanks.. Thanks @vitaly-krugl . >  ran a publish inside the new process where the connection is defined in the master process\nYou can't share anything Pika-related between processes. In your new process, open a new connection and channel(s).. If this can be reliably reproduced, please provide code to do so. Thanks!. I can't reproduce the list index out of range error. If you could please provide the Python and Pika version you're using, as well as a way to reproduce this error, that would be appreciated.. It looks like Erlang Solutions' package repository is slow.. I'll try using the default Erlang package.. The default Erlang package for Ubuntu Trusty is too old to use (R16B03) with the latest RabbitMQ.. > DEFAULT_SOCKET_TIMEOUT is set to 250ms which is non-intuitive.\nWhy? What is a better default? Could you provide examples from other popular open-source projects? Simply opening a GitHub issue with an observation isn't very constructive. Even better would be to provide a pull request with your suggested changes and research, data or even anecdotes to back up the change.. @vitaly-krugl I'll take a look again today. Thanks!. Please be sure you are connecting to the correct RabbitMQ port which, by default, is 5672. If you can provide code to reproduce this issue, please attach it here and re-open this GitHub issue (#997). Are you using threads or the multiprocessing module?. Pika is not thread safe and what you're seeing with framing errors is a symptom. Without your code I can't suggest anything other than to open a new connection per thread. It won't affect performance (it might even help performance).. You need to add a user to RabbitMQ, set the user's password, and ensure they have the correct permissions. Please see the RabbitMQ documentation. If you have further problems, please ask on the mailing list and not via GitHub issues.. That is not enough information to work with here. \"Connection reset by peer\" means that either RabbitMQ terminated the TCP connection unexpectedly (a crash, maybe?) or something between your application and RabbitMQ terminated it (like a proxy - this is more likely).\nPlease post this to the RabbitMQ users mailing list, along with the following information:\n\nTimestamp of this event\nRabbitMQ, Erlang and operating system version\nEnvironment details (cloud provider, proxy between application and RabbitMQ, etc)\nComplete log files from RabbitMQ. This is due to the fact that Pika's current version is 0.11.2 and readthedocs.org uses a sort that thinks 0.10 > 0.11.2. This will be remedied when Pika 1.0.0 is relased, which should be \"soon\".\n\nI recommend bookmarking this link: http://pika.readthedocs.io/en/latest/\nIf you'd like to ask the RTD maintainers, I'd be interested in why this is. Thanks for using Pika!. @vitaly-krugl I'll try to find a nice chunk of time to look at this tomorrow. Thanks!. Nice work @vitaly-krugl . @stuartspotlight that was fixed by https://github.com/pika/pika/commit/973ebc3a9191214b0c500a7332d50876f8e48cde. For what it's worth, the Pika mailing list and the RabbitMQ mailing list are more appropriate places to ask questions like this.\nThe Pika team is comprised of volunteers like @vitaly-krugl as well as members of the RabbitMQ core team (like myself) who work on the project when time allows. Currently, a lot of work is being done on the 1.0.0 release and there is no estimated delivery date.\nIf someone were to put together a 0.12 release PR, branched from the latest release tag and containing cherry-picked commits from master that should be included in 0.12, I would be happy to review it. Thanks!. Pika 0.12.0 was released yesterday. Please give it a try!. Please provide code to reproduce. Thanks!. Hi @vitaly-krugl - I have been on vacation and will be re-visiting Pika issues and pull requests this week (April 9th - 13th). @masell - yep, don't worry about that! I'll review this PR within a week. Thanks again.. @vitaly-krugl - good point. @masell links to this document which mentions this non-standard extension:\n\nThe sole reason for the existence of this class is to permit encoding of byte[] as 'x' in AMQP field tables, an extension to the specification that is part of the tentative JMS mapping implemented by QPid\n\n@masell also says that the sending application he is using is a \"java spring application\". It looks as though the RabbitMQ Java client supports the x encoding as well: link\nType x is also supported in the Ruby amq-protocol project: link\nIt seems valid to add it to Pika as well, IMHO.. @vitaly-krugl I'll get it today. I've had a cold this week so things are going slowly.. @vitaly-krugl :+1: from me, could you update your review?. Alright, best of luck to them. Thanks for pointing that out.. You must use basic_cancel.\nIn the future, please use either the rabbitmq-users or pika mailing list to ask questions instead of GitHub issues.. PR #967 fixed this issue. I will evaluate that fix for inclusion into the 0.12 release. Thanks!. Please give the 0.12.0b2 release a test. Packages are available on pypi, and can be installed using this command\npip install pika --pre. Thank you for letting us know.. OK @vitaly-krugl I'll give it a try. Probably not in this PR, but a separate one.. @vitaly-krugl I did see something similar but I'm not sure we can do anything about it. I'll add those links to  travis-ci/travis-ci#6302. If you're using the GitHub UI, change the version in the dropdown to the tag that matches the release you are using, then navigate to the example file:\nhttps://github.com/pika/pika/blob/0.11.2/examples/asynchronous_consumer_example.py#L216-L229. Resolved by #1020. Queue length is available via the HTTP API.\nhttps://cdn.rawgit.com/rabbitmq/rabbitmq-management/v3.7.4/priv/www/api/index.html\nhttps://stackoverflow.com/q/24402399. @bhack since doing a passive queue declare accomplishes what you need, you should do that. Pika is a pretty low-level library that in general does not support superfluous convenience features -\n```\nRe-declare the queue with passive flag\nres = channel.queue_declare(\n        callback=on_callback,\n        queue=\"test\",\n        durable=True,\n        exclusive=False,\n        auto_delete=False,\n        passive=True\n    )\nprint 'Messages in queue %d' % res.method.message_count\n``. Are you testing themaster` branch of Pika?\nIf you'd like to open a PR, please do so. Thanks!. No problem!. > I see this in my rabbitmq management dashboard\nYou did not provide information or a screenshot of what you see in the management interface.\n\nI\u2019m using docker\n\nCan you reproduce this without using docker?\nIf you use ss or netstat, can you confirm that the TCP connection is still active?\nPlease post your original question as well as answers to my requests for information to the rabbitmq-users mailing list.\nPika's maintainers prefer to only use GitHub issues for items that can be worked on, not for investigations which tend to drag on and on.. > ConnectionClosed: Connection to server.ip.address.and:5672 failed: timeout\nYou need to diagnose why your connection isn't succeeding: https://www.rabbitmq.com/troubleshooting-networking.html. There's not enough information to diagnose this. If you can provide the following, re-open this issue.\n\nWhat version of Pika and Python?\nWhat version of RabbitMQ and Erlang?\nWhat operating system?\nCan you provide reproduction steps? This is not an issue that has been reported before and is probably specific to your environment.. Please use a newer version of RabbitMQ and Erlang.. I'll get it this week. I just got back from vacation.. @guillaume-michel what version of Python and setuptools are you using? This has never been a problem before. We need to make sure this doesn't affect older Python version.. @vitaly-krugl I'm back from vacation and will catch up with what has been going on with Pika this week. Thanks!. @jon-courtney - I suspect this message reveals the root cause of this issue. What do you think? (cc @vitaly-krugl). @jon-courtney - are you using Windows by any chance?. @jon-courtney OK that's interesting. If you enable debug logging, I would be interested to see if Using SelectPoller is logged.. I believe this line is the culprit:\n\nhttps://github.com/pika/pika/blob/67ba812c25ed505316e41fcb801360e07cd43084/pika/adapters/select_connection.py#L943\nThis will always sleep when the passed in set of descriptors are all empty arrays (though I don't know how often this happens in practice).. Sure, I just copied what the github url is when I browse that file. I'll see if there's an easy way to get the url with the sha instead.. That's simple, just hit the y key when viewing a file. I'm working on this today, thanks @vitaly-krugl . @vitaly-krugl do you think we should use thread IDs to print warning statements if the user is violating the expectations that we're assuming?. @vitaly-krugl sounds good. I was thinking about more common scenarios that we see where BlockingConnection is instantiated in one thread, and then operations like acknowledgements are called from other threads without using add_callback_threadsafe. That really is out of the scope of a fix for this though.. Thanks @vitaly-krugl I saw that. I had just updated their example code to work with master and was going to move on to scenario #1 to duplicate what they report.. The Pika project is maintained by volunteers who do not have time to try to guess what your your environment is and code is to reproduce an issue.\nHelp us help you by providing the working code you are using to reproduce this, exact steps to reproduce it, full logs from your Pika application as well as from RabbitMQ. We need to know what version of Erlang and RabbitMQ you are using.\nIf this is provided, I will re-open this issue. Thanks.. > I supposed (since I have used for years) that Pika has to be considered the \"official\" (state-of-the-art) RabbitMQ Python client\nPika is the Python AMQP client that is maintained, in part, by RabbitMQ team members, but it is still officially a community project.\n\nBTW I would be happy to know in which environment you feel confident your Consumer Example, works with the condition I described.\n\nAs far as I know, this is the first time this issue (long-idle connection) has been reported. If you can provide the information I requested in this comment, I can try to reproduce the issue.\nSince you say there is nothing logged by Pika at the debug level, it may be most helpful for you to reproduce the issue while capturing TCP traffic to/from port 5672 with Wireshark. That will show exactly what is happening.. It's worth a try to use SelectConnection but really BlockingConnection should work fine - it should continue to send heartbeats and not close the connection as you see in the logs. In your environment, I assume that your Pika program is running on one server, RabbitMQ on another, and there is no firewall involved.. @sw360cab could you please reproduce this while capturing traffic to port 5672 on your server? You can run the packet capture on the server. This will help us out to diagnose the issue. I will see if I can reproduce it using my home network.\n\nThis application is a crucial piece in our company and in this unstable situation sooner or later I would have to drop Pika and RabbitMQ in flavour of something else, and this would be sad and disappointing. Not only for the need of rewriting classes of code.\n\nCan you downgrade Pika to 0.10 until we figure this out? That would fix your issue, correct?. > I am doing already it, as long as I stuck with Pika 0.10 and with RabbitMQ and application on the same machine, the issue is not occuring.\nJust to clarify, if you use Pika 0.10 and RabbitMQ on different machines, you also do not see the issue. Is that correct?. As expected, I can't reproduce this issue when heartbeats are used. I tested with Pika 0.11.2 and RabbitMQ 3.7.5 with a simple script that sets up a BlockingConnection and a consumer. I published several messages and then let the application sit idle for several hours. Debug logs, as expected, showed heartbeats being sent. Subsequent publishes after the \"idle\" time succeeded.\nI'll test this again using Pika 0.12.0.b3 with disabled heartbeats (which works correctly in that version, see #1014).. I have had a test running for 8 hours with the attached code, using Pika 0.12.0b3 with heartbeats disabled. RabbitMQ is running on a separate machine from the test program. I just published 5 messages which were consumed as expected.\nconsume.py.txt\nI will leave this running all night and will publish messages in the morning.. I had no problems after leaving the test program running all night. Publishing several messages this morning worked as expected. You can see in the packet capture where TCP keepalives were sent at the default 2 hour interval overnight.\nGoing forward, please be sure to run Pika in your environment with heartbeats enabled. Version 0.12 and 1.0.0 support the add_callback_threadsafe method on BlockingConnection that can be used to schedule an acknowledgment from another thread.\nOr, a better solution would be to use SelectConnection and an async producer/consumer as shown in the examples. Threads that do work in your application can use the connection.ioloop.add_callback_threadsafe(...) method to schedule an callback to do the acknowledgement on the correct thread.\n@sw360cab if you wouldn't mind reporting what you find I would appreciate it. I'm going to close this issue but will keep an eye out for your response and I will re-open this issue if necessary.\ngh-1046-capture-5672.pcapng.gz\n. Thanks @vitaly-krugl for some reason I thought add_timeout was the equivalent of BlockingConnection.add_callback_threadsafe. @sw360cab thank you very much for following up -\n\nheartbeating of Pika 0.12b2 is working perfectly on MacOs, whereas on Ubuntu it sends heartbeat every heartbeat/2 seconds (e.g heartbeat=600, will send heartbeats every 300 seconds). May this be related to #1055 \n\nYes, Pika 0.12 will send heartbeats using an interval that is calculated by taking the negotiated connection timeout value (600 seconds in your case) and dividing it by 2. This will happen no matter the operating system. This change is to make Pika's behavior the same as the RabbitMQ Java client as well as Bunny (the Ruby client) and will help prevent scenarios where RabbitMQ thinks the connection is dead due to lack of activity.\n\nI forgot to mention that I was using (also at the time I opened the issue) in the production stage Ubuntu 18.04. Is it possible that I opted for this version (18) of Ubuntu too early?\n\nI doubt it as Python is Python ... I haven't seen reports of distribution-specific issues. I suppose it's possible but it seems that your issues were probably caused by a bug in Pika or your code.\nI just released Pika 0.12 yesterday - give it a try! Thank you again for keeping us informed.\nhttps://groups.google.com/forum/#!topic/pika-python/2pyZG-cB5_g. ERROR:pika.adapters.base_connection:Read empty data, calling disconnect\nSomething between your Pika program and RabbitMQ is disconnecting the socket (like a firewall), or perhaps RabbitMQ isn't actually listening on port 5672. Have you checked with netstat?\nIf you need further assistance, please follow up on a mailing list. We prefer to only use GitHub issues for actual bugs or feature requests, not root cause analysis.\nhttps://groups.google.com/forum/#!forum/pika-python\nhttps://groups.google.com/forum/#!forum/rabbitmq-users. Please use the mailing list(s) for questions like this:\nhttps://groups.google.com/forum/#!forum/pika-python\nhttps://groups.google.com/forum/#!forum/rabbitmq-users. RabbitMQ and Pika are behaving exactly as they should. None gets serialized to an invalid routing key, which RabbitMQ correctly interprets as an error and closes the channel.. Hello! Please share the code you are using. I will re-open this issue if I can reproduce. Thanks!. @benizri-ofir - please provide code that can reproduce this, or at the very least, a working set of code or steps that might reproduce the issue.. @fokhunov I have some questions -\n\nCan you provide all of the code you are using? It helps us a lot if we don't have to guess what code we need to write.\nCan you reproduce this outside of docker?\nWhat version of Erlang and RabbitMQ are used in the docker container?. Thanks for providing all that information. This issue may be related to #1055.\n\n\nI tried to run my script with RabbitMQ which is not in Docker container. Result: everything works well. \n\nFixing issues that only arise in docker are not as high priority as other issues. I suggest searching docker issues or the internet for other abrupt connection closure issues - maybe there is a docker setting that needs to be adjusted?\nJust as a reminder, Pika is maintained by volunteers on their spare time.. > I already posted my Producer code (see above)\nHi @fokhunov - in general, it is best to provide running code that reproduces an issue when requested. I have to read and understand what you are doing before I can put together my own set of code to try to reproduce this issue. This takes away from time I could be using to get to the bottom of the issue.\nI am planning on starting to look at this in a couple hours today (2018-06-04). If you have time to provide running code, that would be great, otherwise I will take time to distill what you have provided into something I can test with.. > I tried run code in Golang and auto-close connection not happened\nCould you please provide the golang code you're using?\nIf you'd like me to try to reproduce this using Docker, please let me know exactly how to set up Docker to match your environment. Thanks.. @farrukh-okhunov thanks for that information. I suspect the golang code runs successfully because it is not disabling heartbeats. I just now noticed that you are using ?heartbeat=0 - why is that? Disabling heartbeats is generally never a good idea. In addition, Pika 0.11.2 is affected by issue #1014 so RabbitMQ will still expect heartbeats from your application.\nIf you would like to double-check the code I'm using and Dockerfile, see this repository.\nI have never had a need to use Docker so please pardon my new-user questions -\n\nI had to modify producer.py for it to work by adding import pika at the top. Is there anything else missing?\nHow do you retrieve logs from your RabbitMQ docker image?\nIf I create a Dockerfile for the python program I can see requirements installed and the container start up, however it does not remain started. What can I do to diagnose?\nIf you re-run your test with heartbeats enabled, do you see the same issue?\nIf you absolutely must disable heartbeats, could you please test using version 0.12.0b3? This version includes a fix for issue #1014 which will tell RabbitMQ to disable heartbeats if the value is set to 0.. @farrukh-okhunov I am able to reproduce what you describe without using docker.\n\nPlease see the changes I made in this commit which shows how to ensure heartbeats are processed while using BlockingConnection.\nIn your code, you should enable heartbeats, and use process_data_events() periodically to ensure they are processed. Or, you could use the asynchronous publisher code here as a starting point, as it will periodically send heartbeats.\nHere is a summary of what I think is affecting you:\n\nYou attempt to disable heartbeats but they are not actually disabled due to the bug described in #1014. This is fixed in 0.12.0b3, so I would like you to test with that version.\nRather than disabling heartbeats, you should either use SelectConnection or modify your code in a manner to what I have done here to ensure that heartbeats are processed.\nYou did not see this issue using the golang code as it will process heartbeats asynchronously by default.\n\nI am going to close this issue now but please feel free to comment and re-open it if you have further questions or find something new. Thanks again for patiently providing the information I requested.. @vitaly-krugl thanks for investigating that for me!. Since Pika will use 2047 from RabbitMQ if a user does not provide a value for channel_max or if the user provides a value greater than 2047 (or 0), I think we're safe. If we decrease pika.channels.MAX_CHANNELS then validation here won't technically be correct anymore since the AMQP spec allows up to 65K channels.\nIn other words, the only way a Pika user can increase channel_max past 2047 is to do so via explicitly setting it in RabbitMQ.. Just for kicks I will see what the Java client does with regard to heartbeat interval.. Yep, the Java client divides the heartbeat interval by 2 - code. Here is where a missed server heartbeat exception is determined.. Good catch! Thanks.. @vitaly-krugl if you have a moment I'd like to see what you think. Thanks!. Does this require changes to this codebase?. Seems fine to me - @vitaly-krugl @gmr any opinions?. @waseem18 since there hasn't been feedback this should be OK.. Resolved by #1063 . Thanks!. @michaelklishin stable doesn't exist as a branch in your local repo since I bet you've never used it. Try this instead:\n$ git checkout -b lukebakken-pika-1060-version-logging origin/stable. Thanks @michaelklishin . Thank you @michaelklishin . Thank you @michaelklishin . @abompard thanks a lot! I have taken the liberty of removing TwistedConnection and pushed those updates to your branch. I'm running tests and reviewing all this now.. @abompard yep, we're aware of that, but thanks for noticing.. @vitaly-krugl I can do some long-running tests with code that uses this PR, but for a deep dive someone more familiar with twisted should check this out, or perhaps @gmr if he has time. We can also alert the twisted community when 1.0.0b1 is released with this code to test it out.\nI will take care of _adapter_get_write_buffer_size in a different PR - you answered a question that I was about to ask soon anyway :smile: . @abompard this hopefully will be the only time this needs to be done. One goal of 1.0.0 is to get as close to a final API as possible. Thank you for all your hard work, this is great!. I'll get back to working on Pika next week, I think. Have a good weekend.. I was out on vacation last week, I'll see if I can find time for Pika this week.. @abompard I merged this PR but noticed that tests fail on Python 2.7.15. Would you please address this and open a new PR? I couldn't figure out why this is happening, nor a fix. Transcript is attached here:\ngh-1069-test-transcript.txt\nThanks in advance!. Does the example code not work with b'Test Message'?. Using b'Test Message', u'Test Message' or just 'Test Message' sends the same 12 bytes to RabbitMQ. I'm going to close this for now but please reply with the reason you think this should be changed.. @vitaly-krugl this should be all set. I tried to fix this pylint error but the hassle involved in getting the mocks fixed for the tests is taking too much time -\n$ pylint pika/heartbeat.py\nUsing config file /home/lbakken/development/pika/pika/pylintrc\n************* Module pika.heartbeat\nW0212,  61:15 - Access to a protected member _heartbeat_checker of a client class (protected-access). @michaelklishin I think I know what's up. There's a slim chance that due to timing Pika will think it's missed an heartbeat from RMQ when the heartbeat was just about to arrive. I'll see what I can do.. @michaelklishin this is the code in the Java client that detects missed server heartbeats. Note that the interval used is twice the negotiated timeout value (\"two complete heartbeat setting intervals\"). So, my latest commit matches this behavior. If a user sets a 10 minute heartbeat timeout with this change a dead connection won't be detected for 20 minutes (just as the Java client does).\n@vitaly-krugl has commented that this behavior is a bit undesirable. Another alternative is, instead of doubling the negotiated value, we just add a fixed amount like 5 seconds. I tested this and it resolves the issue reported here.\nWe definitely have to increase the idle check interval by some amount or we are guaranteed to mis-identify a dead connection.. @michaelklishin well at least I'm no longer confused by the timeout vs interval debate \ud83d\ude04 . @michaelklishin -\n\nBunny adds a fixed number...\n\nIt looks like Bunny multiplies the negotiated heartbeat timeout by 2.2 here to set the socket read timeout value, which is pretty much equivalent to the change I have proposed in this PR.. Thanks @vitaly-krugl I think we came to the right set of changes.. Of course, we review all PRs as time allows. Thank you for offering! Please base your work on the stable branch and we can forward-port it to master. It's no problem, we're not in a hurry. I will check out toctrees ... I've never heard of them.. @romantolkachyov the plan is to only use markdown. rst isn't as familiar to this project's maintainers (myself included) and there are zero volunteers to help when rst issues come up.. @gmr @vitaly-krugl do either of you have an opinion?. Alright, I'll just ping one of you when rst drives me crazy next time \ud83d\ude03 . Hello!\nIn the future, please use the pika-python for questions like these. Another option is to use rabbitmq-users - the same RabbitMQ core team members try to keep up with both.\nYou need to customize the example code to do what you'd like. Creating an ExamplePublisher class for each message would be extremely inefficient because it would create a new connection and channel for every message.\nBasically, you need to implement your own means to publish messages -\nhttps://github.com/pika/pika/blob/0.11.2/examples/asynchronous_publisher_example.py#L259-L289\nFeel free to follow up on one of those mailing lists if you have questions.. Hi @bolshoydi - you are reading the README.rst file that corresponds to the master branch of Pika, which will become version 1.0.0. That code example is correct for the in-progress development of Pika. You will have to adjust your code to use ConnectionClosed - the README is correct.\nIf you view the version of README.rst that corresponds to the version of Pika that you are using, you will notice that example isn't there (file).. Searching the issues in this repository for readthedocs would have directed you to issue #1000 where I explained that there is nothing we can do about that behavior.. Fixed in #1102. Thank you!. Hello! Please help us help you out.\n\nCould you please provide all of your code?\nWhat AMQP broker are you using?\nIf RabbitMQ, what version and what version of Erlang?\nWhat operating system for your application and for your broker?\nDo you have any custom broker configuration?\n\nI will keep this issue open for 24 hours to wait for a response.. Thanks a lot. You're starting your connection in one process, and then consuming in sub-process, and then handling the messages in the parent process. I think you can see why this isn't supported :smile:\nWhat you need to do is ensure that your connection and channel instances are running in the same process and same thread within Python. You can run the entire Pika code in your subprocess and append messages to your buf, but realize that you will have to communicate with the subprocess (maybe via another Queue) to send the acks.\nIf you'd like I could help out refactoring your code.. @elbaro I suggest using SelectConnection as described in this example.\nI took 5 minutes to start refactoring in this gist. You will want to combine the behavior of your RabbitConsumer class and the ExampleConsumer class. Note that I haven't tested a single thing so there are bound to be issues.\nPlease note that you can multi-ack messages in this manner. This will remove the need for your \"batched\" ack code, I think.\nLet me know if you have issues with continuing with what I have started.. Thank you for following up.. This is duplicate of issues #1076 and #1022.\nPlease note that, when viewing code through GitHub, everything is shown on the master branch. The master branch is going to become Pika version 1.0.0 which has API changes compared with previous versions. As such, the code examples have already been updated to reflect these changes.\nIf you wish to see code examples for your Pika version, please remember to select the appropriate tag from the dropdown first, then navigate to the code sample. From the command line, you would clone this repository and then use git checkout 0.12.0 to switch to that Pika version.. This is duplicate of issues #1076 and #1022.\nPlease note that, when viewing code through GitHub, everything is shown on the master branch. The master branch is going to become Pika version 1.0.0 which has API changes compared with previous versions. As such, the code examples have already been updated to reflect these changes.\nIf you wish to see code examples for your Pika version, please remember to select the appropriate tag from the dropdown first, then navigate to the code sample. From the command line, you would clone this repository and then use git checkout 0.12.0 to switch to that Pika version.. Hello! Please help us help you out.\n\nYou say \"larger Messages\" but then fail to say just how much more large. At what size does this issue appear?\nCould you please provide all of your code, or at least a minimum reproducer that works out-of-the-box? Based on the very small and incomplete code snippet you provide I suspect you may be trying to re-use a connection or channel instance or are somehow not closing them correctly. But, without seeing more, I would be guessing which is not productive.\nWhat AMQP broker are you using?\nIf RabbitMQ, what version and what version of Erlang?\nWhat operating system for your application and for your broker?\nDo you have any custom broker configuration?\n\nSetting the socket to blocking is not the correct fix if there is in fact an issue.\nI will keep this issue open for 24 hours to wait for a response.. Thank you very much, I will see if I can reproduce this.. FYI you won't be able to disable SSL in Pika without also disabling SSL in RabbitMQ. That is why you see the SSLWantWriteError. You could also connect to the non-SSL RabbitMQ port if you have it configured.. @tazgithubmaster could you please share your entire RabbitMQ configuration? I can use my own SSL certs. Thanks.. Well, I have tried to reproduce this locally on my Arch Linux workstation using RabbitMQ 3.6.16, Erlang 19.3, Pika 0.11.0 and Python 3.5.4 without any luck. I have tried with SSL enabled (client cert auth just as you have done), and without SSL.\nNotice that I am using slightly newer versions of both RabbitMQ and Erlang. I know that there are changes to Erlang between version 19 and 19.3 that could affect you. Or, you could upgrade to the latest version in the 20.X release series (https://www.rabbitmq.com/install-debian.html)\nWe need to determine if this is due to TLS/SSL in your environment or not. Could you please change your test script to not use SSL and to connect to port 5672? Then, re-run your test to see if you get the error.\nAlso, please note that Pika 0.12.0 is available. I doubt that upgrading will address this, but that is another consideration.. @tazgithubmaster you mention using Python 3.5.3 but that last stack trace indicates Python 3.6.X (/home/ak/.local/lib/python3.6/...). Could you please run python --version as well as pip list in that environment? Thanks.. That's kind of surprising, but I'll leave it at that. Thanks for following up.. Hello!\nbasic_consume already supports arguments. This should accomplish what you want. Give it a try and let me know if it doesn't work and I will re-open this issue.. This is a RTD issue, not Pika's. Please see issue #1077 as well as the other issues linked to there.. Fixed in #1102 . Please use either the rabbitmq-users or the Pika mailing list for questions.\nYour question does not have enough information to work with. What are you trying to accomplish, what have you tried, and what are you observing? What version of Pika and Python are you using?. Thanks! I will merge once the builds finish.. Thank you! I have included your commit in #1089 which will be brought into master in #1090.. Thanks @michaelklishin . @sky-code I apologize, I forgot that libev support was removed in 3ce691b01bc567531aee96c0fd55d94bc3226061\nIf there is enough interest, we would consider re-adding it to Pika 1.0.0 and would welcome a PR to do so.. Hello -\nThanks for using Pika. The team prefers to not use GitHub issues for discussions. Please post this to the pika-python mailing list. Be sure to provide the following information:\n\nPython version\nPika version\nOperating system\nWhat you have tried so far\nWhat you observed\nWhat you expected to observe. Thank you for the complete report, I will check this out.\n\nBut, a question - what do you think happens when your receive_message blocks the thread by calling sleep? How are heartbeats supposed to be processed?. > I have long-running processes for which I do not want the server to tear down the connection while they're doing something.\nThat's exactly why add_callback_threadsafe was added to Pika 0.12.0 - it is even called out in the release notes. This is what you should be using to send acks when your long-running process completes.\nHere's what I see in my environment when I use a slightly modified version of your code, which I have attached here: repro.py.txt. I am testing with RabbitMQ 3.7.7 and Erlang 21.0.2, Python 3.6.5 and Pika 0.12.0. Notice that I am using a fresh virtualenv:\n```\nlbakken@shostakovich ~/issues/pika/gh-1093 (master %)\n$ source venv/3.6.5/bin/activate\n(3.6.5) lbakken@shostakovich ~/issues/pika/gh-1093 (master %)\n$ pip list\nPackage    Version\n\npika       0.12.0 \npip        10.0.1 \nsetuptools 39.2.0 \nwheel      0.31.1 \n```\nHEARTBEAT=0 correctly disables heartbeats. Using a 50 second wait time the channel and connection are not closed abruptly. Here is what I see in the console:\n$ python ./repro.py \nConnecting: heartbeat=0\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nI ran PerfTest with the following arguments to publish messages:\n--queue gh-1093 --predeclared --consumers 0 --publishing-interval 10\nYou can see the packet capture here: repro-heartbeat-0-wait-50.pcapng.gz\nOlder versions of Pika did not / could not disable heartbeats... maybe there's something amiss in your environment?. @carlosefr please provide a packet capture from port 5672 using HEARTBEAT=0 and WAIT_TIME=50. That will be the fastest way for me to see differences between your environment and mine.. One second, I'm re-checking this.. There may be an issue in the Pika logic when it comes to using the server vs client heartbeat value. It'll take a while to get to the bottom of it, and I'm multitasking with this at the moment.. Nope, everything appears to be correctly negotiated in your packet captures as well as mine. In your latest capture, you can see RabbitMQ suggest an 87 second heartbeat in packet 41, while Pika responds with 0 (as expected) in packet 42.\nAgain, I can't reproduce your issue locally. I have configured RMQ to request a 90 second heartbeat. Here's the output from my consumer (I am using your publisher code as well):\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nAcknowledged message! Timeout NOT triggered.\nGot message! Sleeping 50 seconds\nWhat is in your RabbitMQ log? If the connection is dropped unexpectedly, it will be logged.. Can you describe your VM more? Have you tried reproducing this without running RabbitMQ in a VM? That is one difference between your environment and mine - I am running RMQ and the test applications on my Arch Linux workstation.. @carlosefr - thank you for providing that log message. It explains the TCP window full message I see in your Wireshark logs, as well as the behavior you're In my environment, the TCP buffer sizes for RabbitMQ, the test apps, combined with the larger MTU for localhost all provide enough buffering for the issue to not present itself as RabbitMQ is able to write data to the socket. Had something filled up sooner, I would see the same thing. Re-visiting #753 and my comment I guess I already said the same thing.\nIf you'd like to open a pull request with that documentation change, that would be great.. Thank you for the complete report. I'll try to get to this when I can, but I would be thrilled to get a PR to fix this. Thanks.. @Jacobh2 IMHO this falls into the category of \"don't do that\".\nI am able to reproduce what you report by setting 0 as the argument to call_later. I'm not sure how feasible or worthwhile it would be to guard against the recursion that happens in this reconnect scenario. @vitaly-krugl and @michaelklishin what do you think?\n1167 adds a backoff to reconnection attempts.. @Jacobh2 - three days spent trying to reconnect? Is that how you originally found this issue?. OK. I would expect an eventual success to unwind the stack, but maybe that's not the case. I'll investigate.. @Jacobh2 please see PR #1173, which fixes the reconnect example.\nIn short, calling back into the consumer class via call_later is fraught with the potential for a stack overflow. There is no way around this, of course, because the interaction between the ioloop and callbacks will always result in this scenario if call_later is used.. reconnect suffered from this issue because failed reconnect attempts would then call back into the same ExampleConsumer instance over and over.\nSo, It all depends on how add_callback_threadsafe is used. 99% of the time this method will be used to acknowledge a message and nothing else, so when the callback is done, it will return and be done.\nIf you have a specific example you'd like me to review I would be happy to. I will review all of the example code to ensure it does not suffer from the issue reported here.. @vitaly-krugl I don't think there is a way for Pika to avoid this particular scenario. We could detect it, and throw an exception earlier perhaps. Is that what you're thinking?. @vitaly-krugl got it, thanks. Let's not have an explicit prefetch recommendation, and we could link to my comment here or expand on that comment and add it to the Pika / RabbitMQ docs. What do you think @michaelklishin ?. This is a consequence of the documentation being updated for the master branch of this project. When you visit this link, you are viewing the docs for Pika 1.0.0, which is yet to be released, and what the master branch represents.\nYou should instead view the stable version of the documentation on that site, which corresponds to version 0.12.0.\nOr, better yet, just choose the 0.12.0 tag on GitHub and read the docs here.\nreadthedocs.io has been a real pain in our side. I have yet to find the time to figure out why it \"works\" the way it does. I have considered dropping it to use GitHub pages or just markdown files in this repo.\nI'll close this but if you have issues, follow up here.. You are probably using example code that is written for the upcoming Pika 1.0.0 release, where the argument order has changed for several API calls, like queue_bind.\nPlease select the correct tag from the GitHub dropdown, then navigate to the appropriate example code. For instance, for version 0.12.0, use the code at this link:\nhttps://github.com/pika/pika/tree/0.12.0/examples. > use openssl connect to broker is success\nNo, that isn't true. When openssl connects successfully to RabbitMQ via TLS, and completes the TLS handshake, this is the output:\n```\n$ openssl s_client -connect localhost:5671 -CAfile /home/lbakken/development/michaelklishin/tls-gen/basic/result/ca_certificate.pem\nCONNECTED(00000003)\ndepth=1 CN = TLSGenSelfSignedtRootCA, L = $$$$\nverify return:1\ndepth=0 CN = shostakovich, O = server\nverify return:1\n140576545571264:error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure:ssl/record/rec_layer_s3.c:1407:SSL alert number 40\n\nCertificate chain\n 0 s:/CN=shostakovich/O=server\n   i:/CN=TLSGenSelfSignedtRootCA/L=$$$$\n 1 s:/CN=TLSGenSelfSignedtRootCA/L=$$$$\n   i:/CN=TLSGenSelfSignedtRootCA/L=$$$$\n\nServer certificate\n-----BEGIN CERTIFICATE-----\nMIIDJzCCAg+gAwIBAgIBATANBgkqhkiG9w0BAQsFADAxMSAwHgYDVQQDDBdUTFNH\nZW5TZWxmU2lnbmVkdFJvb3RDQTENMAsGA1UEBwwEJCQkJDAeFw0xODA3MTAxNzUz\nMzhaFw0yODA3MDcxNzUzMzhaMCgxFTATBgNVBAMMDHNob3N0YWtvdmljaDEPMA0G\nA1UECgwGc2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAnD6C\nctyI4SkB34avDU9oR/ohieElcwj12fCMEMx24NEgW8+9kwzwh43OeIL3D16FJYH5\n6ryq+HpnGBGTnEcEQYpGBCSJ3cVUxRG52wZd71MM8A00qBx5q3JNJhcI6RpVljL+\nf7aRCvjsftKVBf+q+XnQwPxFAlbD6iGIbGzsoA7rVgqqbPaJlfHQudWDvb2M3Btn\nbXLFxCh5QE3qH9mtrf/fA6N2ts9fx1AWxmsm4upxOwFz4cmJLD1BuYDkLI+D91Pz\nTjS2G+i2k/+aCzoci+Ck3CqUYCvzLMp+iDN7wwuM6LNxNogZ0MRJPJg8XfjlVjyR\nl2CtGyoLdTeBiYwGgwIDAQABo1MwUTAJBgNVHRMEAjAAMAsGA1UdDwQEAwIFoDAT\nBgNVHSUEDDAKBggrBgEFBQcDATAiBgNVHREEGzAZggxzaG9zdGFrb3ZpY2iCCWxv\nY2FsaG9zdDANBgkqhkiG9w0BAQsFAAOCAQEAViYkgeIXs0m54Y91SqT/zlPbGqng\n6bZtzN/5iLHFAFhdHcjfZZ3wDGya2TJDiBmJvXj8n/t9JFYdgJAOA4c/UmJOKCxj\n5iMuEAZC3H8w4JYtMv8eVGtuvzj4l58kaPhUUYUEfZjX4jwC9WSw9zWHpya2x8MC\naBSXRhAx/Faz4Z0zVRTpfms64dQE6lG5mHOvcoTQRUn1PrLGxR15C7QlP6Lj05qF\nJRfU3fBrP0XQeiWYPcp+kDpzerJfF8N4saFH2bQZSJ1xvsmQn+jffJvXSdjJX0l0\nCVpITK72zIH6Bkb0yH3U/h7usHwPE6GlKfndCv4xUXnBK9MHPcJ3Nh60IQ==\n-----END CERTIFICATE-----\nsubject=/CN=shostakovich/O=server\nissuer=/CN=TLSGenSelfSignedtRootCA/L=$$$$\n---\nAcceptable client certificate CA names\n/CN=TLSGenSelfSignedtRootCA/L=$$$$\nClient Certificate Types: ECDSA sign, RSA sign, DSA sign\nRequested Signature Algorithms: ECDSA+SHA512:RSA+SHA512:ECDSA+SHA384:RSA+SHA384:ECDSA+SHA256:RSA+SHA256:ECDSA+SHA224:RSA+SHA224:ECDSA+SHA1:RSA+SHA1:DSA+SHA1\nShared Requested Signature Algorithms: ECDSA+SHA512:RSA+SHA512:ECDSA+SHA384:RSA+SHA384:ECDSA+SHA256:RSA+SHA256:ECDSA+SHA224:RSA+SHA224:ECDSA+SHA1:RSA+SHA1:DSA+SHA1\nPeer signing digest: SHA512\nServer Temp Key: ECDH, P-256, 256 bits\n---\nSSL handshake has read 2141 bytes and written 314 bytes\nVerification: OK\n---\nNew, TLSv1.2, Cipher is ECDHE-RSA-AES256-GCM-SHA384\nServer public key is 2048 bit\nSecure Renegotiation IS supported\nCompression: NONE\nExpansion: NONE\nNo ALPN negotiated\nSSL-Session:\n    Protocol  : TLSv1.2\n    Cipher    : ECDHE-RSA-AES256-GCM-SHA384\n    Session-ID: 94AC404AAF9FCCA199918B11FF84A903BD911B17303E28E02EC0D581D0154A75\n    Session-ID-ctx: \n    Master-Key: 029BBA17C9AFEF83CA95E40C55793D6E3F24C1780D87CD5A1F5A09647CD7A2BDC2D4A33A8839DB00B7ED0BD4AC17F1FD\n    PSK identity: None\n    PSK identity hint: None\n    SRP username: None\n    Start Time: 1532987184\n    Timeout   : 7200 (sec)\n    Verify return code: 0 (ok)\n    Extended master secret: no\n---\n```\nI don't see anything in your RabbitMQ logs indicating that a connection attempt was made. The TLS handshake is not completing which means you are probably having certificate or connection issues, this isn't a Pika issue. I suggest reading these documents:\n\nhttps://www.rabbitmq.com/troubleshooting-networking.html\nhttps://www.rabbitmq.com/troubleshooting-ssl.html. ConnectionClosedByBroker does not exist in 0.12.0. Please be sure you are selecting the correct tag in GitHub before browsing code, otherwise you are looking at master, which will become 1.0.0. @gmr - thank you! I think I tried to do that at one point but couldn't figure it out or didn't have permissions. Is managing readthedocs.io something you have to do?. I have been out on vacation and will follow up more tomorrow. Prior to 0.12.0 there were issues around trying to disable heartbeats. You can read more about all of this in the issue and PR history for this project, and I have followed up on previous reports just like this one already. Please take the time to do a bit of searching.\n\n@chnandu - please provide your code so I can help out.\nIn short, you should be using Pika 0.12.0. If your message processing takes a long time before calling basic_ack, you should run the processing in another thread and then use add_callback_threadsafe to schedule the basic_ack call.\nPlease see this example.. I'm assuming that 0.12.0 will resolve your issues. If not, please add a comment here.. @chnandu your code doesn't work without modification using Python 2.7.15 due to missing symbols and such. I'm working on my own version here: https://gist.github.com/lukebakken/b52bf5023bfcb78208a02f56e942a011\nI'm pretty confident that this loop is blocking the SelectConnection's ioloop: https://gist.github.com/lukebakken/b52bf5023bfcb78208a02f56e942a011#file-zpika-py-L615-L639\nHow do I know? The very first Basic.Publish AMQP message is never even sent when tracing port 5672 in Wireshark.\nAs of now, there is no indication of a Pika bug.. @chnandu I have made some basic changes in my gist that keep the SelectConnection ioloop from being blocked. Here is the gist link again:\nhttps://gist.github.com/lukebakken/b52bf5023bfcb78208a02f56e942a011\nSince you are polling the message queue in the same thread that is running the SelectConnection, the queue polling loop will block the SelectConnection's ioloop! If you do this, messages aren't published, heartbeats aren't, sent, and RabbitMQ will close the connection.\nLet me know if you have additional questions, but I will close this issue as there is no bug.\n@yifeikong you may be running into the same thing.. @Jacobh2 - please don't respond to a closed issue. Instead, provide information on the pika-python mailing list, including the following:\n\nBroker, broker version, if RabbitMQ then Erlang version\nPika version\nPython version\nCode to reproduce the issue\n\nThe two small pieces of information you have provided suggest an issue in your code. Thanks!. Hello!\nI don't like to guess, so could you please provide the following information:\n\nHow long is \"some time working\"?\nCan you please provide your source code in rabbit.py? I need this code to be able to reproduce your issue. There is a chance you are not using SelectConnection correctly.\nIt looks like you're using Python 3.7.0, is that true?\nWhat version(s) of Linux and OS X were tried? Did you also use Python 3.7.0?\n\nThanks!. Thanks for that information. You need to be certain that when methods are called on instances of objects defined in rabbit.py that the call is being made on the same thread as the ioloop is running.\nFor instance, I suspect that the code running a task is probably executed in a different thread than the one running the Pika ioloop, which means that this call to ack the message isn't happening on the correct thread.\nI don't have time to debug this for you but I suspect that is what is going on.\nTo resolve it, use the add_callback_threadsafe method to schedule the basic_ack call.. There were changes to how the ioloop works between 0.10 and 0.12 that probably caused this issue. Let me know if you have any other Pika issues, or questions. Thanks!. Thanks again, @abompard . I recommend not sharing connections. Use a thread per consumer - it's the simplest way.\nIn the future, please ask questions on either the pika-python or rabbitmq-users mailing list. Thanks.. It's no problem at all! Thanks for using Pika and don't hesitate to ask questions on either mailing list. Have a great day :smile: . Hi G\u00e9ry -\nThanks for giving the latest Pika code a try. Please provide the code you're using to demonstrate this. By \"freezes\" I'm assuming you mean that no exceptions are raised, correct? What would you expect to happen in the above situations?\nI see that in several of the scenarios a ChannelClosed exception is raised, which is the expected result.\nAgain, please provide the code you're using. I'm interested to see what connection type you're using. Thanks - Luke. > queue_declare() -> raises TypeError: queue_declare() missing 1 required positional argument: 'queue' in Pika 1.0.0b1\nThis is an expected API change. If you want a server-named queue, you must use the empty string ''\n\nqueue_declare(queue=None) -> Unexpected, freezes instead of creating a queue with a unique name.\n\n1.0.0b2 -> throws ValueError, as do (almost all) the rest of the \"Unexpected\" scenarios. I had to update validation for exchange_bind. Thanks for testing, and please feel free to re-test when the next beta is available.. @maggyero I will make those changes, thanks.. We're good! I was just waiting on the tests.. It would help a lot if you could provide code to reproduce this. At first glance I'm surprised that this happens as Pika does not re-try connections as your log shows.. Thank you! I'll be sure to include this in the next release.. Hello,\nI used the following script to try to reproduce: repro.py.txt\nHere is the transcript when trying to connect to IPv4 localhost and IPv6. I can't seem to reproduce what you report: transcript.txt\n. Hello - this isn't a library issue. Please provide all of the code you are using, or similar code that can reliably reproduce the issue. That is by far the most efficient way to determine what is happening.. Hello, and thanks for using Pika.\nIt isn't helpful, ever, to dump only code and a stack trace into a GitHub issue. At the very least, you should explain what you expected to happen, and what actually happened, in plain words.\nDid you open this issue because you expect to be able to pass a tuple to BlockingConnection, or because this case raises a TypeError? I'm assuming the latter based on the subject of the issue.. Hello,\nI suspect that you must sent messages in a certain format into celery and that the json document you provide is not correct. This isn't a Pika issue - the message is being sent, just not parsed on the receiving side. I suggest you investigate what the expected message format is in a celery-based system and send messages in that format.\nThanks.. no_ack=True indicates that the message does not require an acknowledgement and will be instantly acknowledged on delivery. You should use no_ack=False if you need to manually ack messages.\nconfirm_delivery turns on publisher confirms, which is a different feature.\nPlease see the documentation here:\nhttps://www.rabbitmq.com/confirms.html. OK, so you're saying that with this code the message is auto-acknowledged:\nself._channel.basic_get(queue=self._queue, no_ack=False)\nWhat version of Pika, Python, RabbitMQ and Erlang are you using?\nFor what it's worth, you should prefer using basic_consume anyway instead of basic_get.. @ProPheT777 - here is the code I am using to test: blocking_basic_get.py.txt\nDuring the time.sleep(30) call, the message is in the \"Unacked\" category in RabbitMQ. Then, when the application exits, the channel and connection are closed which indicates to RabbitMQ that the message should be re-en-queued. I suspect your application is doing something similar.. Hello, thanks for using Pika.\nI don't have access to \"openSUSE build service\", nor do I know what it is. You can see that Pika passes tests on Travis CI using several different Python versions.\nI searched the build log and can't find failureException within it or any details about this particular issue.\nYou say this test fails from \"time to time\", it must be timing related. Does this build service use very slow VMs?. What is my action item here?\nAs I mentioned, Pika passes on Travis CI using several different versions of Python. The build log you gave me does not show the error (I can't find FAIL in the file). It shows that Python 2.7.15 is being used while 2.7.14 is used on Travis.. If you can provide the log of the failed run, maybe that will provide more information. There is a good chance that increasing this value to 0.10 or 0.25 will fix this issue.\nDo you have the ability to run this process using a branch from GitHub?. You can view the diff here:\nhttps://github.com/pika/pika/compare/pika-1119-experiment\nI believe the following is in the correct patch format. I generated it with git diff 0.12.0... within the checked out pika-1119-experiment branch:\npika-1119-experiment.patch.txt\n. That's a different error at least, let me check it.. Try this updated patch:\npika-1119-experiment.patch.txt\n. Have you encountered the issue again? I'd like to verify before committing the fix.. @mcepl thanks!. I can't merge this until #1107 is done. I haven't had time to work on 3.7 support and would gladly review a PR that added it.. > This exception cannot be caught\nWhy do you say that? Can you not try ... except around start_consuming ?\nIf you could provide a simple set of working code that I can download and run to reproduce this issue I can take a look. I don't have time at the moment to write something from scratch.. > I could make it work with wrapping try around start_consuming\nThat is what you will have to do for the time being. I may be able to change that behavior for Pika 1.0.0, but that does not have a firm release date.\nPika is maintained by community volunteers as well as some members of the RabbitMQ core team (myself included) as time allows. Thanks again.. @kamil765 - I suggest using a technique like this: repro.py.txt\nNote: I am using Pika 1.0.0 API calls.\nSince a duplicate ack is a channel exception in RabbitMQ, your code must re-open the channel and re-consume. You really shouldn't ack the same delivery_tag twice. Let me know if you have any more questions.. Hello, thanks for using Pika. This is behaving as designed - you can only connect to one broker at a time. If you would like to use a different node upon an error or other condition, you will have to implement that yourself.. > So I want to find a way to use host list in pika, and it would connect other host when current is down.\nYou must fix your haproxy installation, or you have to code a \"host list\" yourself. This would involve catching exceptions and reacting to them by closing the current set of channels and connections and re-opening them to the next host in the list.. Thank you for using Pika!\nWhile this PR is interesting, it also represents a lot of extra code we would have to maintain. I would rather see this distilled into a markdown document with small code snippets demonstrating each RabbitMQ \"extension\".. Thank you, we will review that PR as time allows.. Hello,\nThanks for using Pika! Pika's maintainers prefer that users ask questions via the pika-python or rabbitmq-users mailing lists. Just FYI for the future.\nTo answer your question, delivery_tag is sent by the broker, so don't worry about it. Long-running processes will work fine without any need to manage that value.\nThanks -\nLuke. Please ensure that this code will be tested. Thanks.. This sounds like a great idea if you'd like to submit a PR. Thanks!. Target master (which will become version 1.0.0) and maybe we'll back-port to version 0.13. Thanks again.. Fixed by #1138 . Hello, thanks for using Pika!\nGoing forward, please use one of these mailing lists to ask questions:\n\nhttps://groups.google.com/forum/#!forum/rabbitmq-users\nhttps://groups.google.com/forum/#!forum/pika-python\n\nPika isn't compatible with the multiprocessing library. It might be possible to make it compatible but it would involve a lot of research and work. At this point, we recommend that each sub-process establish its own set of connections and channels.\nThanks. No problem! It's not very obvious. I don't mind answering here with a re-direct \ud83d\ude04 . It would be extremely helpful to have a set of code that reproduces this issue every time it is run. That will lead to the fastest fix.. @octohedron thank you for the code.\nThis is almost certainly a conflict between Pika's I/O loop and how scrapy uses twisted. I recommend switching this code to using Pika's twisted connection adapter. There is some related example code here.\nIf changing the code doesn't resolve your issue, please update your example repo and make a note on this issue. Thanks!. @michaelklishin this is fine, we've switched other examples to the new API. @claws thanks for noticing this.. @claws please do that and use stable as the base branch for that pull request. The problem with those auto-formatters is that they change far more than the original intent of the pull requests.. I am revisiting all of these stale PRs today.. Thank you!. If you are using Pika 0.12.0 (the current released version), please be sure to choose that tag before you copy the example code. Otherwise, you'll be using examples for master which has API changes.\nIf that's not the issue, and you are using 1.0.0b1, please see #1135 . Hello!\nThanks for using Pika and (presumably) RabbitMQ. We prefer to keep GitHub issues free of discussion and only use them for tracking actionable work. In the future, please post questions to either the rabbitmq-users or pika-python mailing lists.\nYour understanding in your second comment is correct, that's how multiple works (docs).. Thanks for your input @gmr \n@claws we will be sticking with yapf / pep8 and I will re-visit this PR on Monday. Several contributors have used yapf like you did (and as part of the instructions) only to re-format parts of the code not related to their contribution.. In the future please use the pika-python mailing list for questions.\n1.0.0b1 is an unannounced BETA at the moment.\nPlease use 0.12.0 or give 0.13.0b1 a try.. Thanks for using Pika. In the future, please use the pika-python or rabbitmq-users mailing list for questions instead of GitHub issues. We use issues to track verified bugs and features to work on only, not discussions.. Thanks for using Pika. In the future, please use the pika-python or rabbitmq-users mailing list for questions instead of GitHub issues. We use issues to track verified bugs and features to work on only, not discussions. I intend to add issue templates saying this but haven't had time, yet.\nAnyway, on to your question:\n\nWhen I set a prefetch_count to n (where n is smaller than the number of messages in the broker), .consume will only yield n messages and then block forever.\n\nYou must acknowledge the messages to receive more messages.. Yes, fanout is supported. Please ask questions on the rabbitmq-users or pika-python mailing list.. Hello and thanks for using Pika. There isn't enough information for this to be useful.\n\nWhat version of Pika are you using?\nWhat Python version and on what OS?\nWhat AMQP broker are you using, and what version? If RabbitMQ, what Erlang version?\nCan you please provide steps to reproduce this?. Does this issue happen reliably? Can you provide a small, working code sample to reproduce it? If you can, this issue will be fixed very quickly.\n\nThis output suggests that your application tried to call close on an already closed connection:\nBlockingConnection.close(200, 'Normal shutdown') called on closed connection\nCan you confirm that? Can you provide the RabbitMQ log file from the same day that this exception happened?. Closing due to inactivity.. Thank you for the information!. We could consider changing this for 1.0.0. Do you use this feature?. There's not enough information to work with. Are you trying to publish from multiple threads to the same channel? Can you provide a small code sample to reproduce this? Is there anything in your AMQP broker's log file? I will consider re-opening if an actual issue can be determined.. Thank you. I didn't notice the script name in the title of this issue.. Since you are using Pika 0.12.0, did you use the example code from the correct tag? This is the correct file:\nhttps://github.com/pika/pika/blob/0.12.0/examples/twisted_service.py\nWhen I run that code, I don't see the error you report.\nIf you can, please test my pull request which has a small fix for that example, and uses a more recent twisted version. Here's how to test:\ngit clone https://github.com/pika/pika.git\ncd pika\ngit checkout lrb-fix-twisted-service-example\nvirtualenv venv/gh-1147\nsource venv/gh-1147/bin/activate\npython setup.py develop\npip install -r test-requirements.txt\npip install pyOpenSSL\npip install service_identity\ntwistd -ny ./examples/twisted_service.py\nThen, in the Management interface, publish a message to the foobar exchange using request1 as the routing key. You will see it end up in the request1 queue.. Thanks @michaelklishin . > It figures out basic_publish cann't take byte stream as its body parameter\nYes it can - body is a str which can be a sequence of bytes:\nhttps://github.com/pika/pika/blob/0.12.0/pika/channel.py#L394-L397. @caibojun you should ask on a Python-specific help forum how to convert binary data correctly. Since you haven't shared any code, I can't help.. Hello, Pika's maintainers use english to communicate. I can't help out here.. @aqparks I should have mentioned that I did the same thing, and the user's message is still not helpful. @wxyhcr - the only suggestion I can make is to provide code to reproduce what you are seeing, and post it as a message (in English, if possible) to the pika-python or rabbitmq-users mailing list. Please let us know what version of RabbitMQ, Python and Pika you are using, and please attach your RabbitMQ logs.. @wxyhcr - There is a chance that studying this example code may show you how to correctly acknowledge a message after a long-running task:\nhttps://github.com/pika/pika/blob/0.12.0/examples/basic_consumer_threaded.py. You need to configure DNS or /etc/hosts on your workstation. The hostname localhost isn't resolving, which is indicative of a serious problem.\nPlease post questions to the mailing list next time. Thanks.. If you are on a system with an /etc/hosts file, what is its contents?. What is happening is that by using localhost Pika is trying to connect over IPv6, which isn't correctly configured on your system. You should change /etc/hosts so that the second line is this:\n::1 localhost6 ip6-localhost ip6-loopback\nThen, using localhost will use the IPv4 address.. I don't have access to any AIX systems to even test a PR. Quoting the link you provided ...\n\nAIX is dead anyway\n\n... if a user must use Pika on AIX, they will have to modify the library on their own as you have done.. Hello, thank you for using Pika and RabbitMQ.\nPlease note the \"if possible\" in the documentation. You shouldn't depend on the position at which a message is re-enqueued.\nSince you are consuming messages without any prefetch setting, by the time you nack message a messages b and c have already been de-queued and are probably already on the network or in your client application's TCP buffers.\nTry setting channel.basic_qos(prefetch_count=1) to see if the behavior is different.\nFinally, GitHub issues are used by the Pika team to track work. If you'd like to ask questions like this one, please use the rabbitmq-users mailing list.. I can't reproduce this using 1.0.0b and your code. Heartbeats are sent from Pika to RabbitMQ every 5 seconds.\npika-1.0.0b1-trace.pcapng.gz\n. I am not responsible for pika-dev. If you'd like to install the correct version of 1.0.0b1 from pypi, use the instructions here:\npip install pika==1.0.0b1. @jstasiak - would you mind deleting the pika-dev project on PyPi as it has caused confusion for @maggyero (and maybe other people)?\nI hope to get a b2 release out by February 1, 2019.. Please re-read the commentary on #1154. pika-dev is in no way associated with this project.. > since i'm requeing, i exepct the generator to always return the same message, over and over.\nWhy would you expect that? Networks aren't infinitely fast, and neither is your workstation, nor the TCP stack, interrupts, etc etc.\n\nis this a bug, or am i missing something?\n\nIt is working exactly as documented. Depending on the timing of many different things, you may or may not have a full message to deliver to your code. You must take the (None, None, None) case into account.. > I thought it has something to do with calling process_events internally with 0 as param\nCould you elaborate on this?. > I didn't keep digging, but my gut feeling was that when I pass 0 to process_data_events() It doesn't actually process the frame of the message being available again\nIn the case when zero is used as inactivity_timeout, the entire frame is more likely to not be available immediately, and you are more likely to hit the (None, None, None) case.\ninactivity_timeout exists so that users can do useful work during the consume loop. In any case, the (None, None, None) return value is normal and should be handled.\nThanks!. It would be great if we didn't have to enable sudo, though. Builds are a lot faster without it.. @michaelklishin I will check the Travis docs, because they do allow some apt changes ... maybe you can add other sources? I'll let you know. It is a great idea.. @michaelklishin thank you for the reviews!. I'd rather not have a parameter ending in underscore, but I don't care that much ... @decaz if you'd like to submit a PR to change it to global_ please feel free.. Thank you, I removed all use of :py:class:... in the README. Thank you for using Pika.\nWhen you click \"New Issue\", there is text that people are expected to read, like this:\n\nGitHub issues are strictly used for actionable work and pull\nrequests.\n\nPlease direct your question to either the pika-python or rabbitmq-users mailing list:\n\nhttps://groups.google.com/forum/#!forum/pika-python\nhttps://groups.google.com/forum/#!forum/rabbitmq-users. Thank you @michaelklishin for QAing this.. Thank you. @vitaly-krugl there's a failing test that I'm not even sure what it's testing:\n\nhttps://github.com/pika/pika/blob/master/tests/unit/blocking_connection_tests.py#L63-L86\nBasically the problem is that .close is called here without first calling .start. I realize changing the assertion test from nesting levels to self._running == True isn't quite the same thing so I'm wondering what this failure reveals.. @jeremycline THANK YOU. The first two were driving me nuts and I couldn't figure it out via the docs, either.. Merging since these are just cosmetic changes to example code.. Merging since all tests passed and these should be non-functional changes. \ud83d\udc4d thanks @michaelklishin . Thank you!. Please, at the very least let us know what software versions you're using. Python, Pika, Dramatiq, RabbitMQ, Erlang, etc. Thanks.\nWithout a way to reliably reproduce this there is very little I can do. Pika's maintainers simply do not have free time to try to come up with ways to reproduce issues like these.\nI suggest doing a bit of debugging yourself. If you can provide exact steps or a script to reproduce this it would be very appreciated, and I will re-open this issue if provided.\nI reviewed how dramatiq uses Pika and it appears correct on first glance.. > happening pretty regularly when running flask in uwsgi with threads enabled\nThis points to mis-use of the Pika library with regard to threading.\nAgain, providing something I can clone and run will expedite assistance. I'll leave this open until the end of the week.. Seems like code to reproduce this issue using flask might be the best bet.. Thanks. I don't use docker but this will get me started.. @Nizebulous I can reproduce this with the following settings in uwsgi.ini:\nenable-threads=true\nprocesses=4\nHowever, everything works fine with these settings:\nenable-threads=false\nprocesses=1\nPika is not a thread-safe library at this point in time, and connections/channels certainly can't be shared among OS processes. More than likely, a thread different than the one that created the RabbitmqBroker instance is accessing the object instance.\nWhen I run with threads enabled and 4 workers, I see only one connection and channel open to RabbitMQ which is a bit surprising. Do those 4 workers attempt to use the same connection/channel? That would cause this issue.\nI recommend stripping away layers to see what is actually causing this issue. Can this be reproduced with just dramatiq and Pika? (I suspect not). What happens when flask is in the mix, but run outside of uwsgi?. Closing since this is most likely due to incorrect use of Pika - I don't see evidence otherwise.. No problem. At some point we're going to have to add thread safety to this library, it just won't make 1.0.0. Fixed in #1193. The spec documents heartbeat to be a short. It's not clear whether it's signed or unsigned but, either way, the upper limit is far larger than any reasonable heartbeat timeout.\nWe try to not go overboard with validation in this library, so I think we'll skip validating this until at least one other person notices. Thanks.. Thanks for the report @Avivsalem . Calling inactivity_timeout=0.0 is allowed but doesn't really make sense. I'll either disallow it or figure out why this error happens.\n\ni noticed that at the master branch, the lines i mentioned above, are not existent. is there a release coming soon with the new code?\n\nPlease give 1.0.0b2 a try in your environment. The 1.0.0 release will happen once I get more feedback about the beta release. I don't expect there to be any more API changes for 1.0.0 so you won't be wasting time by changing your code to accomodate them. All of the code in the examples/ directory has already been updated for master.\nNOTE: the 1.0.0 release has API changes that are detailed here,. @Avivsalem I also suggest joining the pika-python mailing list if you haven't already. Thanks!. I can't reproduce this using Python 3.7.2 or 2.7.15 using 0.13.0b1 and inactivity_timeout=0.0 (code).\nIf you can think of what might be different in your environment, I'd appreciate it. Thanks! I will re-open this issue if new information allows me to reproduce it.. Wow, thank you for the detailed analysis! I'll check it out today. Have a great weekend!. @Avivsalem what heartbeat timeout settings have you tried to reproduce this issue?\n@vitaly-krugl - if you have a few minutes to look at this I would appreciate it. It looks as though when inactivity_timeout=0.0 is used, this code won't ever get called, and if a queue is empty heartbeats may not be sent to RabbitMQ because there.. See #1103. @gmr - is there something I can do or modify the build so that manual intervention isn't required?. Once we get to this line, the yield in _acquire_event_dispatch() has been exited and we no longer hold the resource, so the boolean value in common_terminator is possibly invalid. The if / else statements must be indented to be part of the with block.\n@vitaly-krugl if the if / else statement is in the same block as the with statement, is this change valid?. Right, but then the yield exits, hitting the finally block, which decrements _event_dispatch_suspend_depth. I realize that this is all happening via a single event loop with callbacks. I assume part of the reason for this code is so that later calls don't try to acquire the same resource. In addition, nowhere else in the code is _acquire_event_dispatch called and the value used outside of the yield.. What does this change do?. This is super nit-picky, but typical Python style for parameter default assignment is to not use spaces.. Please use Pika instead of PIKA. This is a change from current behavior, though I'm not all that concerned about it. @michaelklishin - thoughts?. Nope, I just missed it in the change. Addressing it now.. Remove version 3.3. Could these changes be part of a separate PR? They aren't directly related to dropping 2.6 and 3.3 support, right?. See my comment about making these changes in a separate PR. Please use a separate PR for formatting changes.. Should 3.7-dev be included as well as in #934?. Sounds good. Ah I thought I had reverted those. I will simplify that.. Thanks for the feedback, I was wondering about that.. A habit of mine - https://en.wikipedia.org/wiki/Nota_bene. The test is checking _rpc, and the last two arguments are callback and acceptable_replies, in that order.. Yeah I'm trying to revert any of the formatting changes I've done, I'll get this one too.. In this case _validate_callback has made sure that it's callable or None which should evaluate correctly here. I can always change it to if callable(callback) too. RabbitMQ \"remembers\" and uses the last auto-named queue used for a channel if you pass the empty string for an operation.. I don't know how common an auto-named queue is, but it is supported and I suspect is why queue defaults to the empty string in the code. I personally think that may be more confusing to a new user than having them pass the empty string explicitly if they want an auto-named queue. I'll see what the bunny library does.. Well that's interesting, thanks!. I wanted to make sure all queue args were the same and defaulted to the empty string, which is supported by RabbitMQ. Basically, the empty string means \"use the last auto-named queue\". I don't particularly care for this default, to be honest.. bunny does not use the empty string as the default, which I like better. @vitaly-krugl what do you think?. This should be all set now. I kept the name on_open_callback for both connections and channels to be consistent.. That was probably an old change I can revert.. This is something we can consider changing.. I'll put that back, I don't remember why I removed it. We can do pylint changes in another PR like you suggested.. Nope, we're requiring a callable for the ack_nack_callback parameter so I did it this way.. Sure, I can revert this.. OK, getting that now. Blame vim!!! \ud83d\ude09 . Sure did, good catch. Good catch, I wouldn't have thought to even look for that.. I was just trying to think of a way to do that \ud83d\ude16 . Yes, the person who reported this issue mentions it specifically:\n\nI'll also point out that the \"Connection is closed but not stopping IOLoop\" is annoying; the IOLoop does not belong to pika, so it isn't pika's place to say whether the IOLoop should be running or stopped.\n\nIt could be changed to a debug message, but I don't see what value it adds since the message isn't used for other IO loop types.. Ah, yes it would. How about this comment?\nhttps://github.com/pika/pika/blob/pika-945/pika/channel.py#L1286-L1289. Sure does, I'll add a comment about it to clarify, since it doesn't really look like it tests anything.. What do you think about continuing the loop just so that any other blocked methods are debug-logged?. It sure doesn't, because it's the wrong set of lines! \ud83d\ude04 I meant to link to this:\nhttps://github.com/pika/pika/blob/pika-945/pika/channel.py#L1292-L1295\nI'll move the comment and make it clearer what the scenario is.. That's interesting, because that's not what happens with RabbitMQ. What I saw is 504 error response where RabbitMQ says it expects a channel.open. Maybe because in that case the test used the same channel number? I'll re-test it.. Should all of the connection adapters implement close? It looks like the Twisted connection IOLoopReactorAdapter does not, and the Tornado docs state that stop must be called before close which isn't enforced in our code. There's also a good chance I'm mis-understanding how these adapters work. Great! I'll get on it today.. I suspect it is due to this bug which has been resolved for a long time.. Yes, since the timer class implements value equality this would evaluate to equal as both timers have 0 as their deadline.. > Please make should_test_tls and ssl_options available to other tests\nThere aren't other tests that need them at this point, right? I don't really want to expand this configuration beyond what is needed by our continuous integration environments in this PR. I will, however, add a section in the README giving instructions on how to use the included certs if someone wants to run TLS tests themselves. Since we include a rabbitmq.conf.in file it is pretty easy to get going.. It's just how the internal version works. 20.2 is erts-9.2, I believe. Not sure why it works that way but that's how it works on Unix and Windows.. That's probably not necessary. I added it to try and figure out why I was having issues with the updated build process. Turned out to be slow starts in RabbitMQ and epmd.. I figure this way we might find incompatibilities sooner. We know that Pika works with 19.3, for instance.. Oh, I meant that I'm going to remove Set-NetFirewallProfile from the build :smile: . OK let's do it as part of that work.. We're keeping SSLOptions around just for server_hostname. We could get rid of this class and add ssl_server_hostname to ConnectionParameters. Just an idea.. One less TODO :+1: . > If we think we need an \"emergency channel-close\" method that purges queued-up requests (I don't think we do\nDraining blocked methods on a broker-initiated close was introduced in #957 - please check that PR out again. I still think it's necessary.. We don't need it now as we're not installing anything with these changes and using the already-installed Erlang and RabbitMQ. I was surprised that a method called create_streaming_connection is described with \"Perform SSL session establishment\". Thanks for clarifying.. I referred to this documentation. It seemed like the best choice.. Before 1.0.0 is released should these three methods all have the same name and signature?. It seems like 5 seconds is too long for a sleep, though, since it will block all fd activity, right?. @vitaly-krugl I didn't want to do too many changes to this class but we could remove these two parameters and use default values within the class if you prefer.. It makes sense to use the same comparison logic for connections that RabbitMQ does.. Oh, right. Hmm it would be nice to have access to the ephemeral ports in use as that would identify the connection. That seems like the correct way to compare two connections.. Here's what I observed. A 60 second negotiated timeout will result in RabbitMQ sending heartbeats every 60 seconds, and RMQ will expect at least one heartbeat from a client within a 60 second time period. Since we changed Pika to send heartbeats every timeout / 2 seconds (to prevent edge cases that can end up with RMQ closing a connection), heartbeats will be sent to RMQ every 30 seconds. We also check for an idle connection in send_and_check in Pika, so we'll be checking twice as often. Once, when testing my change, the timing was just right that the idle count reached 2 and Pika erroneously thought the connection was idle. So, I think it makes sense to basically double the idle count to go with halving the heartbeat timeout. For a 60 second timeout, this means that the idle timeout is still 120 seconds (4 * 30), which is what it used to be (and should be).. Great, _check_interval is a much better name.. @vitaly-krugl how would you like API changes documented? I can't really think of a good way, except maybe having old version next to the new one in a table, perhaps.. I will re-run my test with Wireshark, but I'm pretty sure my observation is correct when a 60 second heartbeat is used with an app that uses the Java client.. I understand what the docs say, but I am trying to match what the Java client does.. I will update the documentation. We are sending heartbeats at a specified interval (half of what's passed in) so I think that's an appropriate name for it.. @michaelklishin updated and pushed, let me know what you think. I already have a link to that mailing list thread in a comment as well.. OK, send_and_check used to be public, so I kept it that way.. Aha! Thank you.. I log a warning because the previous code did not raise an exception in this case. I will take these comments into account for version 1.0.0. If I remember correctly, you don't see an error / exception until later with the old \"create a socket to evaluate ssl options\" method.. The ssl.SSLSocket ctor we were using is deprecated and the old wrap_socket, too.. Will this work with Python 2?. pika/spec.py is auto-generated. Please re-generate this file using the changes you made in utils/codegen.py. Thanks.. To remove the need for sudo which would make builds a lot faster. This PR is more of an experiment than anything, because we can't use custom RabbitMQ config when the package is used without also using sudo. ",
    "invisibleroads": "Instructions:\nRun consumer.py, then run producer.py in a separate terminal.\nResults:\n- In pika==0.9.5, the consumer will print a message every three seconds.\n- In pika==dev, the consumer will print a batch of messages only after the user terminates the producer.\nconsumer.py\n``` python\nfrom pika import SelectConnection, ConnectionParameters\nCHANNEL = None\nQUEUE = 'xxx'\ndef on_connection_open(connection):\n    connection.channel(on_channel_open)\ndef on_channel_open(channel):\n    global CHANNEL\n    CHANNEL = channel\n    CHANNEL.queue_declare(queue=QUEUE, callback=on_queue_declared)\ndef on_queue_declared(frame):\n    CHANNEL.basic_consume(queue=QUEUE, consumer_callback=consume)\ndef consume(channel, method, header, body):\n    print body\n    CHANNEL.basic_ack(delivery_tag=method.delivery_tag)\nparameters = ConnectionParameters()\nconnection = SelectConnection(parameters, on_open_callback=on_connection_open)\ntry:\n    connection.ioloop.start()\nexcept KeyboardInterrupt:\n    connection.close()\n    connection.ioloop.start()\n```\nproducer.py\n``` python\nfrom pika import SelectConnection, ConnectionParameters\nfrom time import sleep\nCHANNEL = None\nQUEUE = 'xxx'\ndef on_connection_open(connection):\n    connection.channel(on_channel_open)\ndef on_channel_open(channel):\n    global CHANNEL\n    CHANNEL = channel\n    CHANNEL.queue_declare(queue=QUEUE, callback=on_queue_declared)\ndef on_queue_declared(frame):\n    count = 0\n    while True:\n        CHANNEL.basic_publish(\n            exchange='',\n            routing_key=QUEUE,\n            body=str(count))\n        sleep(3)\n        count += 1\nparameters = ConnectionParameters()\nconnection = SelectConnection(parameters, on_open_callback=on_connection_open)\ntry:\n    connection.ioloop.start()\nexcept KeyboardInterrupt:\n    connection.close()\n    connection.ioloop.start()\n```\n. ",
    "doubleukay": "Removal of flush_outbound() from send_frame() also has the side effect that frames are flushed only every 1 second instead of immediately.\n. ",
    "Jonty": "Are you sure you're using TwistedConnection in production and not TwistedProtocolConnection? The TwistedConnection example doesn't even function without 44f89e9 because the arguments are missing on _adapter_connect.\nI can attempt to knock up a testcase tomorrow replicating the write loop issue. (Note: The loop doesn't block anything, it just results in continually writing nothing to the socket and hammering the CPU)\n. ",
    "spanezz": "Commenting out \"self.ioloop.remove_handler(self.socket.fileno())\" in PatchedTornadoConnection._handle_disconnect actually makes it works as expected (modulo the logic of enqueuing messages instead of sending them if the connection is down, which is beyond the scope of this test code)\n. On Fri, Feb 10, 2012 at 02:27:54PM -0800, Gavin M. Roy wrote:\n\nSo originally Pika did not stop the event loop, then it was requested that it did. I think the right solution is a flag in the Tornado adapter to tell it to stop the ioloop on disconnect.\nAny disagreement with that?\n\nThanks, sounds fine to me.\nCiao,\nEnrico\n\nGPG key: 4096R/E7AD5568 2009-05-08 Enrico Zini enrico@enricozini.org\n. ",
    "bergundy": "I'd also encountered this problem and I think the flag is a good solution.\n. Great, let me know if I can help.\n. Accidently recreated the issue, sorry\n. ",
    "pksinghus": "Oops, I just created an issue similar to this. Looks like it's been around for a while.\n. I am using this version of PatchedTornadoConnection. It definitely keeps tornado up, not sure about other implications.\n``` python\n    def _adapter_disconnect(self):\n     self.stop_poller()\n\n    # Close our socket\n    try:\n        self.socket.shutdown(socket.SHUT_RDWR)\n    except:\n        logging.error(traceback.format_exc())\n\n    try:\n        self.socket.close()\n    except:\n        logging.error(traceback.format_exc())\n\n    # Check our state on disconnect\n    self._check_state_on_disconnect()\n\n```\n. ",
    "lockonoko": "Hey. The problem was solved. It just needs an exception catch at the statement connection.ioloop.start():\nthe file d2lAgentConsume.py has been changed into this:\n```\ndef rough_connect(parameters,on_connected,srs):\n    connection = SelectConnection(parameters, on_open_callback=on_connected, reconnection_strategy=srs)\n    try:\n        # Loop so we can communicate with RabbitMQ\n        connection.ioloop.start()\n    except KeyboardInterrupt:\n        # Gracefully close the connection\n        connection.close()\n        # Loop until we're fully closed, will stop on its own\n        connection.ioloop.start()\n    except Exception as inst:\n        print \"Encounter exception \" + str(inst) + \" create another connection\"\n        rough_connect(parameters,on_connected,srs)\ndef fetch_job(d2l_agent):\n    global agent\n    agent = d2l_agent\n    agent.log.info('post_job(): MQ Parameter setup')\n    credentials = pika.PlainCredentials( agent.mq_username, agent.mq_password)\nparameters = pika.ConnectionParameters(\n    host = agent.mq_hostname,\n    credentials = credentials,\n    virtual_host = agent.mq_vhost)\n    #heartbeat = True\n\nagent.log.info('post_job(): Setting-up connection, host: ' + agent.mq_hostname )\n\n# Step #1: Connect to RabbitMQ\nsrs = SimpleReconnectionStrategy()\nrough_connect(parameters,on_connected,srs)\n\n```\n. ",
    "lsowen": "I see now that the pollers call _manage_event_state(), so that isn't my underlying problem.  Closing the pull request.\n. I am also using SelectConnection and was experiencing the exception.  I'm okay with the exception as long as it is the desired response.  It appears to be thrown because the flow is as follows:\n1) Server sends Connection.Close\n2) Client closes connection\n3) Client attempts to send Close.Ok from each channel\nI believe the flow needs to be as follows:\n1) Server sends Connection.Close\n2) Client sends Close.Ok from each channel\n3) Client closes connection.\n. ",
    "jacksonofalltrades": "This is still broken in version 0.9.13 which is apparently the latest distro on pip.\n. Copy/pasted the following code into my test.py:\nimport pika\ndef on_open(connection):\n    # Invoked when the connection is open\n    pass\nCreate our connection object, passing in the on_open method\nconnection = pika.SelectConnection(on_open)\ntry:\n    # Loop so we can communicate with RabbitMQ\n    connection.ioloop.start()\nexcept KeyboardInterrupt:\n    # Gracefully close the connection\n    connection.close()\n    # Loop until we're fully closed, will stop on its own\n    connection.ioloop.start()\nThen, got this:\nTraceback (most recent call last):\n  File \"test.py\", line 8, in \n    connection = pika.SelectConnection(on_open)\n  File\n\"/usr/local/lib/python2.7/dist-packages/pika/adapters/select_connection.py\",\nline 51, in init\n    stop_ioloop_on_close)\n  File\n\"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\",\nline 50, in init\n    if parameters and parameters.ssl and not ssl:\nAttributeError: 'function' object has no attribute 'ssl'\nOn Thu, Oct 24, 2013 at 1:17 PM, Gavin M. Roy notifications@github.comwrote:\n\nPlease paste the traceback.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/128#issuecomment-27026126\n.\n. It's all good, I found a workaround for this.\n\nOn Fri, Apr 11, 2014 at 5:18 AM, iainelder notifications@github.com wrote:\n\nEither the package or the documentation is broken.\njacksonofalltrades's repro is copied straight from the Introduction to\nPika http://pika.readthedocs.org/en/latest/intro.html documentation.\n\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/128#issuecomment-40197305\n.\n. \n",
    "iainelder": "Either the package or the documentation is broken.\njacksonofalltrades's repro is copied straight from the Introduction to Pika documentation.\n. Glad you found a fix. Can you share it with us? Maybe we can use it to fix the documentation.\nAfter hitting that error I abandoned the introductory guide and just adapted someone else's script to do what I needed.\nDisappointing first-time user experience!\n. > This has been fixed somewhere along the line, indirectly.\nWhich version fixed the issue?\nVersion 0.9.13 was published to Pypi on 2013-05-16.\nLooks like the problem has regressed here, so we should reopen the issue.\n. ",
    "daharon": "Great.  I think I encountered this problem just yesterday when RabbitMQ became overloaded.\nMy Python consumers' memory usage went from a normal ~12MB to 500MB+.\nI had also just made a significant change in the program, so I thought that I had caused some kind of memory leak somewhere.\nBut I do remember a ton of Pika messages regarding pending frames (or many 'x frames behind').\n. ",
    "ecerulm": "pika.exceptions.AMQPConnectionError for those that land here from google while searching for \"pika reconnect\"\n. ",
    "rowanu": "+1. \nIs sanitize intentionally missing? It's called in pika/adapters/blocking_connection.py and tests/unit/callback_test.py...\nThanks for monkey patch @seriyps.\n. ",
    "ramdyne": "Not knowing anything about AMQP or pika internals I did an educated guess and added the following to decode_value() in data.py:\nelif kind == 'V':\n    value = None\nThis seems to have fixed the missing field exception and I'm seeing other fields still containing sensible data.\n. ",
    "shreddd": "Has this issue been addressed? I am seeing similar behavior:\nimport pika\nfrom ssl import CERT_REQUIRED, PROTOCOL_SSLv3, CERT_NONE  \nssl_options = {\"ca_certs\": \"/Users/dev/testca/cacert.pem\",\n    \"certfile\": \"/Users/dev/testca/client/cert.pem\",\n    \"keyfile\": \"/Users/dev/testca/client/key.pem\",\n    \"cert_reqs\": CERT_REQUIRED} \nparams = pika.ConnectionParameters(host=\"127.0.0.1\", port=5671, ssl=True, ssl_options=ssl_options)\nconn = pika.BlockingConnection(params)\nThe last line returns \npika.exceptions.AMQPConnectionError: (0, '')\nDigging deeper with pdb, I am seeing the root cause comes from:\n```\npika/adapters/base_connection.py\", line 302, in _handle_write\nSSLError(8, '_ssl.c:1237: EOF occurred in violation of protocol')\n```\nAny updates would be appreciated.\n. Looks good. Thanks! Any sense for when 0.9.6 will make it into pypi?\n. Not sure if this is related but I seem to be seeing something strange with closed connections. \nEven though I am not explicitly closing my connection, I seem to be seeing this, when I try to reuse an existing connection:\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 244, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/connection.py\", line 1304, in _send_frame\n    raise exceptions.ConnectionClosed\nShould I open a new ticket or is this part of the same issue?\n. Here you go::\n``` python\nTraceback (most recent call last):\nFile \"/usr/local/lib/python2.7/dist-packages/django/utils/decorators.py\", line 25, in _wrapper\n    return bound_func(args, *kwargs)\nFile \"/kb/deployment/services/cluster_service/cluster_service/decorators.py\", line 54, in _wrapped_view\n    return view_func(request, args, *kwargs)\nFile \"/usr/local/lib/python2.7/dist-packages/django/utils/decorators.py\", line 21, in bound_func\n    return func(self, args2, *kwargs2)\nFile \"/kb/deployment/services/cluster_service/cluster_service/job_service/handlers.py\", line 162, in create\n    body=app_info_send)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 355, in basic_publish\n    (properties, body), False)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 755, in _send_method\n    self.connection.send_method(self.channel_number, method_frame, content)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 119, in send_method\n    self._send_method(channel_number, method_frame, content)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/connection.py\", line 1324, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 244, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/connection.py\", line 1311, in _send_frame\n    self._flush_outbound()\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 204, in _flush_outbound\n    if self._handle_write():\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/base_connection.py\", line 320, in _handle_write\n    return self._handle_error(error)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/base_connection.py\", line 264, in _handle_error\n    self._handle_disconnect()\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 180, in _handle_disconnect\n    self._on_connection_closed(None, True)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 231, in _on_connection_closed\n    self._channels[channel]._on_close(method_frame)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 670, in _on_close\n    self._send_method(spec.Channel.CloseOk(), None, False)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 755, in _send_method\n    self.connection.send_method(self.channel_number, method_frame, content)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 119, in send_method\n    self._send_method(channel_number, method_frame, content)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/connection.py\", line 1324, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/adapters/blocking_connection.py\", line 244, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\nFile \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre6-py2.7.egg/pika/connection.py\", line 1304, in _send_frame\n    raise exceptions.ConnectionClosed\nConnectionClosed\n```\n. Couple of additional details - I only see this for SSL connections. Non-SSL connections work fine. Also it seems to work for the first 2 channel.basic_publish() commands and fails on the 3rd.\n. I will play around with different timeout values. The odd things is that 1. I only see the error for SSL connections. 2. I am not explicitly setting the timeout anywhere. Wondering if something in the SSL code is dropping the connection. Worst case, I can always explicitly set up and tear down the connection before each publish, but I was hoping to avoid the overhead.\n. Thanks for the explanation. I guess the real question then is this - is there a recommended way (or perhaps an example) for yielding control back to the ioloop? And if this is not a recommended pattern is there an advantage to use the SelectConnection over a BlockingConnection?\n. ",
    "ghost": "I hope this will be fixed!!\nWith the code from master i'm trying\nconnection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost', port=5671, ssl=True))\n...\nthe bottom of the error i get is:\n  File \"/usr/local/lib/python2.7/dist-packages/pika-0.9.6_pre1-py2.7.egg/pika/adapters/blocking_connection.py\", line 214, in _on_connection_closed\n    raise exceptions.AMQPConnectionError(*self.closing)\npika.exceptions.AMQPConnectionError: (0, '')\n. That's awesome. thanks.\n. Duplicate of #236 \n. I know. I described the problem.\n\nWhen an auto-delete exchange is created, it will go away as soon as any bound queues are removed.\n\nA sender doesn't know about it.It continues to send messages. I restarted a receiver. It doesn't get messages.\n. No. I just wanted to send status of a client repeatedly. One receiver (server) and some senders (clients). I planned to remove an exchange when all connections will be closed.\nMaybe It's an error of RabbitMQ.\n. Yes, this is still a problem. \nPull request was closed because I wasn't sure of the process for submitting patches (github noob).\nSo if you can fix this it would be great - I found the problem very easy to reproduce using the sample code in my post.\nThanks,\nMark\n. I think it should be fairly straightforward to replace the following:\ntimeout_id = '%.8f' % time.time()\nwith a uuid\n. Excellent. Thanks.\n. I've seen this in the past - rabbit was killing the connection because the application was not sending amqp heartbeats frequently enough.\nPeriodically calling process_data_events() when the connection was idle fixed this for me (from memory the default heartbeat interval is 600s so RabbitMQ will kill an idle connection after 20 minutes if you don't send any traffic and don't call process_data_events).\n. Traceback (most recent call last):\nFile \"/usr/lib/python2.6/threading.py\", line 532, in __bootstrap_inner\nself.run()\nFile \"./lspollserve.py\", line 153, in run\nconnector.change_handler(Action.replace, self._query_name, c_item)\nFile \"./lspollserve.py\", line 274, in change_handler\nprint(\"Change Handler Connection Error\", e)\nFile \"/usr/local/lib/python2.6/dist-packages/pika-0.9.14p0-py2.6.egg/pika/exceptions.py\", line 56, in __repr__\nreturn 'The AMQP connection was closed (%s) %s' % (self.args[0],\nIndexError: tuple index out of range\nI get the same error. I added a try-except to reconnect in the case of errors only to find that an error is raised in the error handler.\n. I am also able to reproduce the issue. Unfortunately, I can't provide any example code but I will try to describe the circumstances and precisely as possible:\nI am using pika 0.10.0 installed via pip with python 2.7.9 on Debian jessie x86_64 and recently came across a use case that required using several connections to rabbitmq in a single application. The code is mostly based on the rabbitmq tutorial found on https://www.rabbitmq.com/tutorials/tutorial-one-python.html . Since I need to open multiple connections, I'm starting the channel.start_consuming() method in a separate thread.\nThis way I create several (might be as little as 3, using more just makes the issue more likely to occur) connections to the rabbitmq server. My example code then just uses time.sleep to wait a few seconds and then begins closing all the connections again with connection.close(). This works most of the time but (with 3 connections) approximately every second time I run the code, one of the connections fails to close.\nI dug through the code for several hours and only got as far as the I/O loop. In the connections that fail to shut down is_done() always returns False so the completion condition is never satisfied and the loop keeps running until some terminates the application manually. I don't understand why that happens but my guess would be that the issue is with the last condition in the lambda respectively the fact that ready() keeps returning False.\n. @hoverzheng @vmarkovtsev It might be interesting to compare the circumstances in which you and me ran into this issue. Are you using threads in any way or are you creating and closing the connection directly from your main thread?\n. @vmarkovtsev: You might also want  to look into the twisted connection adapter. At least for my use case it turned out that it could be much easier (and probably also more efficiently) be solved by using twisted instead of starting dozens of threads with one connection each.\n. ",
    "bartekgorny": "Sure - I modified the demo_twisted.py in my fork to show the idea (and the way I use it). Have a look. If the same can be done in a better way (I'm not sure if I can use consumer_tag for this) plz advise.\n. On Fri, 27 Apr 2012 07:52:49 -0700\nJan Urba\u0144ski\nreply@reply.github.com\nwrote:\n\nI'm still not sure I understand why do you have to keep your custom\nfunction for handling data attached to the Pika Channel object.\n\nI don't \"have to\", I just didn't notice the channel_number attribute,\nthis serves the purpose equally well without the need to change the\npika's code. Thanks for your advice, and reject the pull request.\nBartek\n\nSince you control the implementation of the function processing the\ndata (that's handle_payload in demo_twisted.py), you can call\nwhatever code you want - and if you want it to behave differently\ndepending on the channel, just make it dispatch on something like\nchannel.channel_number.\nThis means you could for instance to something like that:\n``` python\ndef got_channel(self, channel):\n    self.channel = channel\n    HANDLERS[channel.channel_number] = my_channel_handler\ndef handle_payload(self, channel, method_frame, header_frame, body):\n    HANDLERSchannel.channel_number\n```\nI agree it's annoying that the object returned from\nconnection.channel() is not the same as the object you get from the\nqueue returned by basic_consume, but I don't see how this would be\neasily fixable...\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/pika/pika/pull/144#issuecomment-5382973\n\n\nMam wiele powod\u00f3w \u017ceby ci\u0105gle i\u015b\u0107 do przodu trzy z nich witam od progu\ngdy wracam do domu... (Abradab/O.S.T.R.)\n. ",
    "bra-fsn": "Also hit me, latest release (0.9.5) leaks memory unbounded... Please pull this and release a new version is possible.\nThanks,\n. ",
    "dpetzold": "I also ran into this. It breaks the blocking example scripts.\n. ",
    "khomenko": "Anyone? Is pika development effectively dead right now? I tried to solve some issues in 0.9.5 by upgrading to master, but with these pull requests languishing around master is next to useless for me.\n. ",
    "lpefferkorn": "You can try to check if the connection is still opened before sending a message: (check connection.is_open value):\n``` python\n!/usr/bin/python\nimport pika\nimport time\ncredentials = pika.credentials.PlainCredentials('guest', 'guest')\nparameters = pika.ConnectionParameters(\n    host=\"172.16.1.29\",\n    credentials=credentials,\n    virtual_host=\"/\")\nconnection = pika.BlockingConnection(parameters)\nchannel = connection.channel()\nwhile True:\n    if connection.is_open:\n        print \"sending...\"\n        channel.basic_publish(\n            exchange=\"\",\n            body=\"test\",\n            routing_key=\"\",\n            properties=pika.BasicProperties(\n                content_type=\"text/plain\",\n                delivery_mode=2))\n        time.sleep(1)\n    else:\n        print 'connection closed'\n        break\n```\nThe connection close event could also be handled by a custom function, with Connection.add_on_close_callback(callback)\n. ",
    "wwoods": "We had a similar issue with Pika - our fix is to modify pika.adapters.blocking_connection.BlockingConnection._flush_outbound to return immediately if self.outbound_buffer.size (is non zero).\nThe problem, in short, was that if you call the body of _flush_outbound with a zero outbound buffer, it thinks that the socket successfully sent a message, and resets self._socket_timeouts to 0, but sending zero bytes always works through the python API.  So, since _socket_timeouts is always being reset to 0, the receive timeouts never actually reach the SOCKET_TIMEOUT_THRESHOLD parameter specified at the top of the file.\nWe also changed SOCKET_TIMEOUT_THRESHOLD to 10 instead of 100...  100 is super unresponsive for a blocking connection.\n. Might as well complete the comment with some code; we fixed the issue by issuing the following commands after initializing a BlockingConnection into self.connection:\nimport pika.adapters.blocking_connection as bc\n    bc.SOCKET_TIMEOUT_THRESHOLD = 3\n    oldFlush = self.connection._flush_outbound\n    def newFlush(*args, **kwargs):\n        if self.connection.outbound_buffer.size:\n            return oldFlush(*args, **kwargs)\n    self.connection._flush_outbound = newFlush\n. Sweet, thanks!\nOn Thu, Sep 27, 2012 at 8:01 PM, Gavin M. Roy notifications@github.comwrote:\n\nThis is fixed/addressed in 64d7ddchttps://github.com/pika/pika/commit/64d7ddc5f261a2f9aa\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/150#issuecomment-8962274.\n. \n",
    "lbolla": "This is fixed on tip (but other things are broken, for which I've submitted a pull request).\n. The fix is in the Tornado connection object, which is unable to handle \"None\" errors. When a disconnect happens, the idea is to ignore \"None\" errors and let the reconnection module do its job. Have you tried the latest version of pika?\n. Submitted a patch in https://github.com/pika/pika/pull/155\n. ",
    "shylent": "Ok, but what is the current workaround? I mean, every time a connection is closed while it has at least one channel active, ioloop crashes taking everything else (not just pika!) with it. This is rather.. problematic. Surely there is some way to work around this?\n. You mean \"tip\" as in 0.9.6pre? No, not really, since it will eventually have to be used in production and I have no idea when it will be released. Maybe I should try and backport whatever fixes there are to 0.9.5.\nAnyway, I'll take a look at the latest version of pika.\n. ",
    "thezerobit": "How about a closure?\n```\ndef temp_callback(frame): on_queue_declare(user_data, frame)\nchannel.queue_declare(\n    callback = temp_callback,\n    queue = \"whatever\",\n    auto_delete = True)\ndef on_queue_declare(host, frame):\n    # Do my funky thing\n```\n. It would be really great to get a new pika version out, 0.9.5 is basically unusable. We can't open source a library we have unless a new version of pika is on PyPI. Since it's been 1 1/2 years since the last release, I've been considering forking and putting an alternate version up on PyPI just to get things working sanely.\n. ",
    "art049": "That's still an issue. ",
    "pnathan": "Found the problem after cloning git and digging around the source...\nDocs are wrong. basic_get doesn't take a callback, so ticket was being given a method... wouldn't encode to integer. :o\nSnippet from the docs.\n\nbasic_get(callback, ticket=0, queue='', no_ack=False)\nGet a single message from the AMQP broker. The callback method signature should have 3 parameters: The method frame, header frame and the body, like the consumer callback for Basic.Consume. If you want to be notified of Basic.GetEmpty, use the Channel.add_callback method adding your Basic.GetEmpty callback which should expect only one parameter, frame. For more information on basic_get and its parameters, see:\n. ",
    "ShilpaKarkera": "why is this closed. I'm still facing this issue. Can i understand the solution?\n. Inspite of setting a callback to reconnect on disconnection, it keeps happening on high speeds of messaging & pops up this error. As per as my understanding, its an interlocking issue, reads faster than it writes sometimes. \nHowever yes, the callback prevents from the program to stop. \n. ",
    "gvsmirnov": "Thanks for getting back to me. I had this reproduced on python <= 2.6.5\n. ",
    "c00kiemonster": "So if I want to install the most up-to-date version which version would that be then? It seems 0.9.5 is too old, and pika2 seems too far away. \n. Am I supposed to simply do this?\npika_logger = logging.getLogger('pika')\npika_logger.setLevel(logging.CRITICAL)\n. Is this a new feature, a setup issue somehow? It worked completely fine sending non-unicode binary data in 0.9.5. Isn't RabbitMQ supposed to be indifferent to what goes into the body of a message?\n. Sorry I misunderstood your comment, please disregard my comment. I just checked your commit, and the error is gone now. Thanks much.\n. ",
    "bgoldfedder": "I agree that we really need a 0.9.6 or something even if its just a patch since some significant bugs have been fixed\n. can we tag 170fce6  as a new release 0.9.5b or something so we can have a stable point to work from?\n. Really what we need is 0.9.6 though. Wondering what the right path to get there is or if we should be moving away from the pika tool\n. ",
    "patricklaw": "I agree that we desperately need a bugfix release.  For now I am using 170fce6de8e4f25f891ec8640e19c479601e886c rather than 0.9.5.  It seems to have the bugfixes that I need as well as support for a custom ioloop added in fa4173c2935f36440bd303712d2c158d83bacb23.  I would just use the head of master, but the pull request 787b95bac3ae0df4cc3003ed764b80fd8bda48ee obviously completely breaks master.\n. I would tend to be in favor of that, but it is notable that a portion of the test suite fails (for me, anyway) on 170fce6 that does not fail on v0.9.5.  In particular:\nconfirm_select_test.TestPublisherConfirms.test_publisher_confirms\nsend_consume_test.TestSendConsume.test_send_and_consume\nsend_get_reject_get_test.TestSendGetRejectGet.test_send_and_get\ntimers_test.TestAdapters.test_kqueue_connection\nall fail with:\nTimeExpired: Time limit (2) exceeded.\nI haven't looked into what actually causes the tests to fail.  I'll probably run a git bisect on it later tonight and report further.\n. Yes, when running automated tests, a new IOLoop is spun up for each test instance.  The Tornado documentation on its testing framework describes this use case: http://www.tornadoweb.org/documentation/testing.html\nSubclassing the connection class will be my alternative.  I feel like it is a brittle solution, since it requires that I override internal methods of TornadoConnection that seem to change relatively frequently.  I first wanted to confirm that I will have to do so in 0.9.6.\n. Thanks for continuing to support this!\n. ",
    "raphdg": "I've got code almost ready for this. What behavior would you expect from the uri parameters on other parameters ?\nI suggest that anything parsed from the uri parameter override any other parameter provided (according to the spec you provided of course).\nFor example:\npython\ncp = ConnectionParameters(uri='amqp://user:pass@somehost/vhost', virtual_host='myvhost')\ncp.virtual_host value should be vhost.\nWhat do you think ?\n. It still happens. The exception occurs when rabbitmq is back online.\nTo reproduce:\n- Start the script\n- service rabbitmq-server restart\n``` python\nimport logging\nimport pika\nLOG_FORMAT = ('%(levelname) -10s %(asctime)s %(name) -30s %(funcName) '\n              '-35s %(lineno) -5d: %(message)s')\nLOGGER = logging.getLogger(name)\nclass ExampleConsumer(object):\n    \"\"\"This is an example consumer that will handle unexpected interactions\n    with RabbitMQ such as channel and connection closures.\nIf RabbitMQ closes the connection, it will reopen it. You should\nlook at the output, as there are limited reasons why the connection may\nbe closed, which usually are tied to permission related issues or\nsocket timeouts.\n\nIf the channel is closed, it will indicate a problem with one of the\ncommands that were issued and that should surface in the output as well.\n\n\"\"\"\nEXCHANGE = 'message'\nEXCHANGE_TYPE = 'topic'\nQUEUE = 'text'\nROUTING_KEY = 'example.text'\n\ndef __init__(self, amqp_url):\n    \"\"\"Create a new instance of the consumer class, passing in the AMQP\n    URL used to connect to RabbitMQ.\n\n    :param str amqp_url: The AMQP url to connect with\n\n    \"\"\"\n    self._connection = None\n    self._channel = None\n    self._consumer_tag = None\n    self._url = amqp_url\n\ndef connect(self):\n    \"\"\"This method connects to RabbitMQ, returning the connection handle.\n    When the connection is established, the on_connection_open method\n    will be invoked by pika.\n\n    :rtype: pika.SelectConnection\n\n    \"\"\"\n    LOGGER.info('Connecting to %s', self._url)\n    return pika.SelectConnection(pika.URLParameters(self._url),\n                                 self.on_connection_open)\n\ndef close_connection(self):\n    \"\"\"This method closes the connection to RabbitMQ.\"\"\"\n    LOGGER.info('Closing connection')\n    self._connection.close()\n\ndef add_on_connection_close_callback(self):\n    \"\"\"This method adds an on close callback that will be invoked by pika\n    when RabbitMQ closes the connection to the publisher unexpectedly.\n\n    \"\"\"\n    LOGGER.info('Adding connection close callback')\n    self._connection.add_on_close_callback(self.on_connection_closed)\n\ndef on_connection_closed(self, method_frame):\n    \"\"\"This method is invoked by pika when the connection to RabbitMQ is\n    closed unexpectedly. Since it is unexpected, we will reconnect to\n    RabbitMQ if it disconnects.\n\n    :param pika.frame.Method method_frame: The method frame from RabbitMQ\n\n    \"\"\"\n    #LOGGER.warning('Server closed connection, reopening: (%s) %s',\n    #               method_frame.method.reply_code,\n    #               method_frame.method.reply_text)\n    self._channel = None\n    self._connection = self.connect()\n\ndef on_connection_open(self, unused_connection):\n    \"\"\"This method is called by pika once the connection to RabbitMQ has\n    been established. It passes the handle to the connection object in\n    case we need it, but in this case, we'll just mark it unused.\n\n    :type unused_connection: pika.SelectConnection\n\n    \"\"\"\n    LOGGER.info('Connection opened')\n    self.add_on_connection_close_callback()\n    self.open_channel()\n\ndef add_on_channel_close_callback(self):\n    \"\"\"This method tells pika to call the on_channel_closed method if\n    RabbitMQ unexpectedly closes the channel.\n\n    \"\"\"\n    LOGGER.info('Adding channel close callback')\n    self._channel.add_on_close_callback(self.on_channel_closed)\n\ndef on_channel_closed(self, method_frame):\n    \"\"\"Invoked by pika when RabbitMQ unexpectedly closes the channel.\n    Channels are usually closed if you attempt to do something that\n    violates the protocol, such as redeclare an exchange or queue with\n    different paramters. In this case, we'll close the connection\n    to shutdown the object.\n\n    :param pika.frame.Method method_frame: The Channel.Close method frame\n\n    \"\"\"\n    LOGGER.warning('Channel was closed: (%s) %s',\n                   method_frame.method.reply_code,\n                   method_frame.method.reply_text)\n    self._connection.close()\n\ndef on_channel_open(self, channel):\n    \"\"\"This method is invoked by pika when the channel has been opened.\n    The channel object is passed in so we can make use of it.\n\n    Since the channel is now open, we'll declare the exchange to use.\n\n    :param pika.channel.Channel channel: The channel object\n\n    \"\"\"\n    LOGGER.info('Channel opened')\n    self._channel = channel\n    self.add_on_channel_close_callback()\n    self.setup_exchange(self.EXCHANGE)\n\ndef setup_exchange(self, exchange_name):\n    \"\"\"Setup the exchange on RabbitMQ by invoking the Exchange.Declare RPC\n    command. When it is complete, the on_exchange_declareok method will\n    be invoked by pika.\n\n    :param str|unicode exchange_name: The name of the exchange to declare\n\n    \"\"\"\n    LOGGER.info('Declaring exchange %s', exchange_name)\n    self._channel.exchange_declare(self.on_exchange_declareok,\n                                   exchange_name,\n                                   self.EXCHANGE_TYPE)\n\ndef on_exchange_declareok(self, unused_frame):\n    \"\"\"Invoked by pika when RabbitMQ has finished the Exchange.Declare RPC\n    command.\n\n    :param pika.Frame.Method unused_frame: Exchange.DeclareOk response frame\n\n    \"\"\"\n    LOGGER.info('Exchange declared')\n    self.setup_queue(self.QUEUE)\n\ndef setup_queue(self, queue_name):\n    \"\"\"Setup the queue on RabbitMQ by invoking the Queue.Declare RPC\n    command. When it is complete, the on_queue_declareok method will\n    be invoked by pika.\n\n    :param str|unicode queue_name: The name of the queue to declare.\n\n    \"\"\"\n    LOGGER.info('Declaring queue %s', queue_name)\n    self._channel.queue_declare(self.on_queue_declareok, queue_name)\n\ndef on_queue_declareok(self, method_frame):\n    \"\"\"Method invoked by pika when the Queue.Declare RPC call made in\n    setup_queue has completed. In this method we will bind the queue\n    and exchange together with the routing key by issuing the Queue.Bind\n    RPC command. When this command is complete, the on_bindok method will\n    be invoked by pika.\n\n    :param pika.frame.Method method_frame: The Queue.DeclareOk frame\n\n    \"\"\"\n    LOGGER.info('Binding %s to %s with %s',\n                self.EXCHANGE, self.QUEUE, self.ROUTING_KEY)\n    self._channel.queue_bind(self.on_bindok, self.QUEUE,\n                             self.EXCHANGE, self.ROUTING_KEY)\n\ndef add_on_cancel_callback(self):\n    \"\"\"Add a callback that will be invoked if RabbitMQ cancels the consumer\n    for some reason. If RabbitMQ does cancel the consumer,\n    on_consumer_cancelled will be invoked by pika.\n\n    \"\"\"\n    LOGGER.info('Adding consumer cancellation callback')\n    self._channel.add_on_cancel_callback(self.on_consumer_cancelled)\n\ndef on_consumer_cancelled(self, method_frame):\n    \"\"\"Invoked by pika when RabbitMQ sends a Basic.Cancel for a consumer\n    receiving messages.\n\n    :param pika.frame.Method method_frame: The Basic.Cancel frame\n\n    \"\"\"\n    LOGGER.info('Consumer was cancelled remotely, shutting down: %r',\n                method_frame)\n    self._channel.close()\n\ndef acknowledge_message(self, delivery_tag):\n    \"\"\"Acknowledge the message delivery from RabbitMQ by sending a\n    Basic.Ack RPC method for the delivery tag.\n\n    :param int delivery_tag: The delivery tag from the Basic.Deliver frame\n\n    \"\"\"\n    LOGGER.info('Acknowledging message %s', delivery_tag)\n    self._channel.basic_ack(delivery_tag)\n\ndef on_message(self, unused_channel, basic_deliver, properties, body):\n    \"\"\"Invoked by pika when a message is delivered from RabbitMQ. The\n    channel is passed for your convenience. The basic_deliver object that\n    is passed in carries the exchange, routing key, delivery tag and\n    a redelivered flag for the message. The properties passed in is an\n    instance of BasicProperties with the message properties and the body\n    is the message that was sent.\n\n    :param pika.channel.Channel unused_channel: The channel object\n    :param pika.Spec.Basic.Deliver: basic_deliver method\n    :param pika.Spec.BasicProperties: properties\n    :param str|unicode body: The message body\n\n    \"\"\"\n    LOGGER.info('Received message # %s from %s: %s',\n                basic_deliver.delivery_tag, properties.app_id, body)\n    self.acknowledge_message(basic_deliver.delivery_tag)\n\ndef on_cancelok(self, unused_frame):\n    \"\"\"This method is invoked by pika when RabbitMQ acknowledges the\n    cancellation of a consumer. At this point we will close the connection\n    which will automatically close the channel if it's open.\n\n    :param pika.frame.Method unused_frame: The Basic.CancelOk frame\n\n    \"\"\"\n    LOGGER.info('RabbitMQ acknowledged the cancellation of the consumer')\n    self.close_connection()\n\ndef stop_consuming(self):\n    \"\"\"Tell RabbitMQ that you would like to stop consuming by sending the\n    Basic.Cancel RPC command.\n\n    \"\"\"\n    LOGGER.info('Sending a Basic.Cancel RPC command to RabbitMQ')\n    self._channel.basic_cancel(self.on_cancelok, self._consumer_tag)\n\ndef start_consuming(self):\n    \"\"\"This method sets up the consumer by first calling\n    add_on_cancel_callback so that the object is notified if RabbitMQ\n    cancels the consumer. It then issues the Basic.Consume RPC command\n    which returns the consumer tag that is used to uniquely identify the\n    consumer with RabbitMQ. We keep the value to use it when we want to\n    cancel consuming. The on_message method is passed in as a callback pika\n    will invoke when a message is fully received.\n\n    \"\"\"\n    LOGGER.info('Issuing consumer related RPC commands')\n    self.add_on_cancel_callback()\n    self._consumer_tag = self._channel.basic_consume(self.on_message,\n                                                     self.QUEUE)\n\ndef on_bindok(self, unused_frame):\n    \"\"\"Invoked by pika when the Queue.Bind method has completed. At this\n    point we will start consuming messages by calling start_consuming\n    which will invoke the needed RPC commands to start the process.\n\n    :param pika.frame.Method unused_frame: The Queue.BindOk response frame\n\n    \"\"\"\n    LOGGER.info('Queue bound')\n    self.start_consuming()\n\ndef close_channel(self):\n    \"\"\"Call to close the channel with RabbitMQ cleanly by issuing the\n    Channel.Close RPC command.\n\n    \"\"\"\n    LOGGER.info('Closing the channel')\n    self._channel.close()\n\ndef open_channel(self):\n    \"\"\"Open a new channel with RabbitMQ by issuing the Channel.Open RPC\n    command. When RabbitMQ responds that the channel is open, the\n    on_channel_open callback will be invoked by pika.\n\n    \"\"\"\n    LOGGER.info('Creating a new channel')\n    self._connection.channel(on_open_callback=self.on_channel_open)\n\ndef run(self):\n    \"\"\"Run the example consumer by connecting to RabbitMQ and then\n    starting the IOLoop to block and allow the SelectConnection to operate.\n\n    \"\"\"\n    self._connection = self.connect()\n    self._connection.ioloop.start()\n\ndef stop(self):\n    \"\"\"Cleanly shutdown the connection to RabbitMQ by stopping the consumer\n    with RabbitMQ. When RabbitMQ confirms the cancellation, on_cancelok\n    will be invoked by pika, which will then closing the channel and\n    connection. The IOLoop is started again because this method is invoked\n    when CTRL-C is pressed raising a KeyboardInterrupt exception. This\n    exception stops the IOLoop which needs to be running for pika to\n    communicate with RabbitMQ. All of the commands issued prior to starting\n    the IOLoop will be buffered but not processed.\n\n    \"\"\"\n    LOGGER.info('Stopping')\n    self.stop_consuming()\n    self._connection.ioloop.start()\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT)\n    example = ExampleConsumer('amqp://guest:guest@localhost:5672/%2F?connection_attempts=50')\n    try:\n        example.run()\n    except KeyboardInterrupt:\n        example.stop()\nif name == 'main':\n    main()\n```\nINFO       2013-02-06 15:44:58,428 __main__                       connect                              47  : Connecting to amqp://guest:guest@localhost:5672/%2F?connection_attempts=50\nDEBUG      2013-02-06 15:44:58,429 pika.callback                  add                                  161 : Added: {'callback': <bound method ExampleConsumer.on_connection_open of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2013-02-06 15:44:58,430 pika.callback                  add                                  161 : Added: {'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,430 pika.connection                _connect                             805 : Attempting connection\nDEBUG      2013-02-06 15:44:58,430 pika.adapters.base_connection  _adapter_connect                     98  : Connecting the adapter to the remote host\nDEBUG      2013-02-06 15:44:58,430 pika.adapters.base_connection  _create_and_connect_to_socket        153 : Creating the socket\nINFO       2013-02-06 15:44:58,431 pika.adapters.base_connection  _create_and_connect_to_socket        163 : Connecting fd 3 to localhost:5672\nDEBUG      2013-02-06 15:44:58,432 pika.adapters.select_connection start_poller                         112 : Starting the Poller\nDEBUG      2013-02-06 15:44:58,432 pika.adapters.select_connection start_poller                         121 : Using EPollPoller\nDEBUG      2013-02-06 15:44:58,432 pika.connection                _send_frame                          1319: Frame: <ProtocolHeader(['frame_type=-1', 'major=0', 'minor=9', 'revision=1'])>\nDEBUG      2013-02-06 15:44:58,432 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,432 pika.connection                _connect                             808 : Connected\nDEBUG      2013-02-06 15:44:58,432 pika.adapters.select_connection start                                98  : Starting IOLoop\nDEBUG      2013-02-06 15:44:58,433 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,433 pika.connection                _process_frame                       1184: Frame: <METHOD(['frame_type=1', 'channel_number=0', 'method=<Connection.Start([\"server_properties={\\'information\\': \\'Licensed under the MPL.  See http://www.rabbitmq.com/\\', \\'product\\': \\'RabbitMQ\\', \\'copyright\\': \\'Copyright (C) 2007-2011 VMware, Inc.\\', \\'capabilities\\': {\\'exchange_exchange_bindings\\': True, \\'consumer_cancel_notify\\': True, \\'publisher_confirms\\': True, \\'basic.nack\\': True}, \\'platform\\': \\'Erlang/OTP\\', \\'version\\': \\'2.6.1\\'}\", \\'version_minor=9\\', \\'mechanisms=PLAIN AMQPLAIN\\', \\'locales=en_US\\', \\'version_major=0\\'])>'])>\nDEBUG      2013-02-06 15:44:58,433 pika.callback                  process                              217 : Processing 0:Connection.Start\nDEBUG      2013-02-06 15:44:58,433 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,433 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,433 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,434 pika.callback                  process                              231 : Calling <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x857192c>> for \"0:Connection.Start\"\nDEBUG      2013-02-06 15:44:58,434 pika.callback                  add                                  161 : Added: {'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,434 pika.connection                _send_frame                          1319: Frame: <METHOD(['frame_type=1', 'channel_number=0', 'method=<Connection.StartOk([\\'locale=en_US\\', \\'mechanism=PLAIN\\', \"client_properties={\\'platform\\': \\'Python 2.7.3\\', \\'product\\': \\'Pika Python Client Library\\', \\'version\\': \\'0.9.9\\', \\'capabilities\\': {\\'consumer_cancel_notify\\': True, \\'publisher_confirms\\': True, \\'basic.nack\\': True}, \\'information\\': \\'See http://pika.github.com\\'}\", \\'response=\\\\x00guest\\\\x00guest\\'])>'])>\nDEBUG      2013-02-06 15:44:58,434 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,434 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,435 pika.connection                _process_frame                       1184: Frame: <METHOD(['frame_type=1', 'channel_number=0', \"method=<Connection.Tune(['frame_max=131072', 'channel_max=0', 'heartbeat=0'])>\"])>\nDEBUG      2013-02-06 15:44:58,435 pika.callback                  process                              217 : Processing 0:Connection.Tune\nDEBUG      2013-02-06 15:44:58,435 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,435 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,435 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,435 pika.callback                  process                              231 : Calling <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x857192c>> for \"0:Connection.Tune\"\nDEBUG      2013-02-06 15:44:58,435 pika.connection                _send_frame                          1319: Frame: <METHOD(['frame_type=1', 'channel_number=0', \"method=<Connection.TuneOk(['frame_max=131072', 'channel_max=0', 'heartbeat=0'])>\"])>\nDEBUG      2013-02-06 15:44:58,435 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,436 pika.callback                  add                                  161 : Added: {'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,436 pika.connection                _send_frame                          1319: Frame: <METHOD(['frame_type=1', 'channel_number=0', \"method=<Connection.Open(['insist=True', 'capabilities=', 'virtual_host=/'])>\"])>\nDEBUG      2013-02-06 15:44:58,436 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,476 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,476 pika.connection                _process_frame                       1184: Frame: <METHOD(['frame_type=1', 'channel_number=0', \"method=<Connection.OpenOk(['known_hosts='])>\"])>\nDEBUG      2013-02-06 15:44:58,477 pika.callback                  process                              217 : Processing 0:Connection.OpenOk\nDEBUG      2013-02-06 15:44:58,477 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,477 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,478 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,478 pika.callback                  process                              231 : Calling <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x857192c>> for \"0:Connection.OpenOk\"\nDEBUG      2013-02-06 15:44:58,478 pika.callback                  add                                  161 : Added: {'callback': <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,479 pika.callback                  process                              217 : Processing 0:_on_connection_open\nDEBUG      2013-02-06 15:44:58,479 pika.callback                  process                              231 : Calling <bound method ExampleConsumer.on_connection_open of <__main__.ExampleConsumer object at 0x8571d4c>> for \"0:_on_connection_open\"\nINFO       2013-02-06 15:44:58,479 __main__                       on_connection_open                   86  : Connection opened\nINFO       2013-02-06 15:44:58,479 __main__                       add_on_connection_close_callback     61  : Adding connection close callback\nDEBUG      2013-02-06 15:44:58,480 pika.callback                  add                                  161 : Added: {'callback': <bound method ExampleConsumer.on_connection_closed of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': False, 'arguments': None}\nINFO       2013-02-06 15:44:58,480 __main__                       open_channel                         283 : Creating a new channel\nDEBUG      2013-02-06 15:44:58,480 pika.callback                  add                                  161 : Added: {'callback': <bound method SelectConnection._on_channel_closeok of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,481 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_getempty of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2013-02-06 15:44:58,481 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_cancel of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2013-02-06 15:44:58,481 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_flow of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2013-02-06 15:44:58,481 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_close of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2013-02-06 15:44:58,482 pika.channel                   _rpc                                 1000: Adding in on_synchronous_complete callback\nDEBUG      2013-02-06 15:44:58,482 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,482 pika.channel                   _rpc                                 1005: Adding passed in callback\nDEBUG      2013-02-06 15:44:58,482 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,483 pika.connection                _send_frame                          1319: Frame: <METHOD(['frame_type=1', 'channel_number=1', \"method=<Channel.Open(['out_of_band='])>\"])>\nDEBUG      2013-02-06 15:44:58,483 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,484 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,484 pika.connection                _process_frame                       1184: Frame: <METHOD(['frame_type=1', 'channel_number=1', \"method=<Channel.OpenOk(['channel_id='])>\"])>\nDEBUG      2013-02-06 15:44:58,485 pika.callback                  process                              217 : Processing 1:Channel.OpenOk\nDEBUG      2013-02-06 15:44:58,485 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,485 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,485 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,486 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,486 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,486 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,486 pika.callback                  process                              231 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>> for \"1:Channel.OpenOk\"\nDEBUG      2013-02-06 15:44:58,486 pika.channel                   _on_synchronous_complete             957 : 0 blocked frames\nDEBUG      2013-02-06 15:44:58,487 pika.callback                  process                              231 : Calling <bound method Channel._on_openok of <pika.channel.Channel object at 0x857956c>> for \"1:Channel.OpenOk\"\nINFO       2013-02-06 15:44:58,487 __main__                       on_channel_open                      122 : Channel opened\nINFO       2013-02-06 15:44:58,487 __main__                       add_on_channel_close_callback        95  : Adding channel close callback\nDEBUG      2013-02-06 15:44:58,487 pika.callback                  add                                  161 : Added: {'callback': <bound method ExampleConsumer.on_channel_closed of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': False, 'arguments': None}\nINFO       2013-02-06 15:44:58,487 __main__                       setup_exchange                       135 : Declaring exchange message\nDEBUG      2013-02-06 15:44:58,488 pika.channel                   _rpc                                 1000: Adding in on_synchronous_complete callback\nDEBUG      2013-02-06 15:44:58,488 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,488 pika.channel                   _rpc                                 1005: Adding passed in callback\nDEBUG      2013-02-06 15:44:58,489 pika.callback                  add                                  161 : Added: {'callback': <bound method ExampleConsumer.on_exchange_declareok of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,489 pika.connection                _send_frame                          1319: Frame: <METHOD(['frame_type=1', 'channel_number=1', \"method=<Exchange.Declare(['nowait=False', 'exchange=message', 'durable=False', 'passive=False', 'internal=False', 'arguments={}', 'ticket=0', 'type=topic', 'auto_delete=False'])>\"])>\nDEBUG      2013-02-06 15:44:58,490 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,491 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,491 pika.connection                _process_frame                       1184: Frame: <METHOD(['frame_type=1', 'channel_number=1', 'method=<Exchange.DeclareOk>'])>\nDEBUG      2013-02-06 15:44:58,492 pika.callback                  process                              217 : Processing 1:Exchange.DeclareOk\nDEBUG      2013-02-06 15:44:58,492 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,492 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,493 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,493 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,493 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,494 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method ExampleConsumer.on_exchange_declareok of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,494 pika.callback                  process                              231 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>> for \"1:Exchange.DeclareOk\"\nDEBUG      2013-02-06 15:44:58,494 pika.channel                   _on_synchronous_complete             957 : 0 blocked frames\nDEBUG      2013-02-06 15:44:58,495 pika.callback                  process                              231 : Calling <bound method ExampleConsumer.on_exchange_declareok of <__main__.ExampleConsumer object at 0x8571d4c>> for \"1:Exchange.DeclareOk\"\nINFO       2013-02-06 15:44:58,495 __main__                       on_exchange_declareok                147 : Exchange declared\nINFO       2013-02-06 15:44:58,495 __main__                       setup_queue                          158 : Declaring queue text\nDEBUG      2013-02-06 15:44:58,496 pika.channel                   _rpc                                 1000: Adding in on_synchronous_complete callback\nDEBUG      2013-02-06 15:44:58,496 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': {'queue': 'text'}, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,496 pika.channel                   _rpc                                 1005: Adding passed in callback\nDEBUG      2013-02-06 15:44:58,496 pika.callback                  add                                  161 : Added: {'callback': <bound method ExampleConsumer.on_queue_declareok of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': True, 'arguments': {'queue': 'text'}, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,497 pika.connection                _send_frame                          1319: Frame: <METHOD(['frame_type=1', 'channel_number=1', \"method=<Queue.Declare(['passive=False', 'nowait=False', 'exclusive=False', 'durable=False', 'queue=text', 'arguments={}', 'ticket=0', 'auto_delete=False'])>\"])>\nDEBUG      2013-02-06 15:44:58,497 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,500 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,500 pika.connection                _process_frame                       1184: Frame: <METHOD(['frame_type=1', 'channel_number=1', \"method=<Queue.DeclareOk(['queue=text', 'message_count=0', 'consumer_count=0'])>\"])>\nDEBUG      2013-02-06 15:44:58,501 pika.callback                  process                              217 : Processing 1:Queue.DeclareOk\nDEBUG      2013-02-06 15:44:58,501 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,501 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,502 pika.callback                  _dict_arguments_match                337 : Comparing {'queue': 'text'} to {'queue': 'text'}\nDEBUG      2013-02-06 15:44:58,502 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': {'queue': 'text'}, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,502 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,503 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,503 pika.callback                  _dict_arguments_match                337 : Comparing {'queue': 'text'} to {'queue': 'text'}\nDEBUG      2013-02-06 15:44:58,503 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method ExampleConsumer.on_queue_declareok of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': True, 'arguments': {'queue': 'text'}, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,504 pika.callback                  process                              231 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>> for \"1:Queue.DeclareOk\"\nDEBUG      2013-02-06 15:44:58,504 pika.channel                   _on_synchronous_complete             957 : 0 blocked frames\nDEBUG      2013-02-06 15:44:58,504 pika.callback                  process                              231 : Calling <bound method ExampleConsumer.on_queue_declareok of <__main__.ExampleConsumer object at 0x8571d4c>> for \"1:Queue.DeclareOk\"\nINFO       2013-02-06 15:44:58,504 __main__                       on_queue_declareok                   172 : Binding message to text with example.text\nDEBUG      2013-02-06 15:44:58,505 pika.channel                   _rpc                                 1000: Adding in on_synchronous_complete callback\nDEBUG      2013-02-06 15:44:58,506 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,506 pika.channel                   _rpc                                 1005: Adding passed in callback\nDEBUG      2013-02-06 15:44:58,507 pika.callback                  add                                  161 : Added: {'callback': <bound method ExampleConsumer.on_bindok of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,507 pika.connection                _send_frame                          1319: Frame: <METHOD(['frame_type=1', 'channel_number=1', \"method=<Queue.Bind(['nowait=False', 'exchange=message', 'routing_key=example.text', 'queue=text', 'arguments={}', 'ticket=0'])>\"])>\nDEBUG      2013-02-06 15:44:58,507 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,509 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,509 pika.connection                _process_frame                       1184: Frame: <METHOD(['frame_type=1', 'channel_number=1', 'method=<Queue.BindOk>'])>\nDEBUG      2013-02-06 15:44:58,509 pika.callback                  process                              217 : Processing 1:Queue.BindOk\nDEBUG      2013-02-06 15:44:58,510 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,510 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,510 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,510 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,510 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,511 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method ExampleConsumer.on_bindok of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,511 pika.callback                  process                              231 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>> for \"1:Queue.BindOk\"\nDEBUG      2013-02-06 15:44:58,511 pika.channel                   _on_synchronous_complete             957 : 0 blocked frames\nDEBUG      2013-02-06 15:44:58,511 pika.callback                  process                              231 : Calling <bound method ExampleConsumer.on_bindok of <__main__.ExampleConsumer object at 0x8571d4c>> for \"1:Queue.BindOk\"\nINFO       2013-02-06 15:44:58,511 __main__                       on_bindok                            266 : Queue bound\nINFO       2013-02-06 15:44:58,511 __main__                       start_consuming                      253 : Issuing consumer related RPC commands\nINFO       2013-02-06 15:44:58,512 __main__                       add_on_cancel_callback               182 : Adding consumer cancellation callback\nDEBUG      2013-02-06 15:44:58,512 pika.callback                  add                                  161 : Added: {'callback': <bound method ExampleConsumer.on_consumer_cancelled of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2013-02-06 15:44:58,512 pika.channel                   _rpc                                 1000: Adding in on_synchronous_complete callback\nDEBUG      2013-02-06 15:44:58,512 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.0'}, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,512 pika.channel                   _rpc                                 1005: Adding passed in callback\nDEBUG      2013-02-06 15:44:58,513 pika.callback                  add                                  161 : Added: {'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.0'}, 'calls': 1}\nDEBUG      2013-02-06 15:44:58,513 pika.connection                _send_frame                          1319: Frame: <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Consume(['exclusive=False', 'nowait=False', 'no_local=False', 'consumer_tag=ctag1.0', 'queue=text', 'arguments={}', 'ticket=0', 'no_ack=False'])>\"])>\nDEBUG      2013-02-06 15:44:58,513 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,513 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:58,513 pika.connection                _process_frame                       1184: Frame: <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.0'])>\"])>\nDEBUG      2013-02-06 15:44:58,514 pika.callback                  process                              217 : Processing 1:Basic.ConsumeOk\nDEBUG      2013-02-06 15:44:58,514 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,514 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,514 pika.callback                  _dict_arguments_match                337 : Comparing {'consumer_tag': 'ctag1.0'} to {'consumer_tag': 'ctag1.0'}\nDEBUG      2013-02-06 15:44:58,514 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.0'}, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,519 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:58,519 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:58,519 pika.callback                  _dict_arguments_match                337 : Comparing {'consumer_tag': 'ctag1.0'} to {'consumer_tag': 'ctag1.0'}\nDEBUG      2013-02-06 15:44:58,519 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x857956c>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.0'}, 'calls': 0}\nDEBUG      2013-02-06 15:44:58,519 pika.callback                  process                              231 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x857956c>> for \"1:Basic.ConsumeOk\"\nDEBUG      2013-02-06 15:44:58,519 pika.channel                   _on_synchronous_complete             957 : 0 blocked frames\nDEBUG      2013-02-06 15:44:58,519 pika.callback                  process                              231 : Calling <bound method Channel._on_eventok of <pika.channel.Channel object at 0x857956c>> for \"1:Basic.ConsumeOk\"\nDEBUG      2013-02-06 15:44:58,520 pika.channel                   _on_eventok                          860 : Discarding frame <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.0'])>\"])>\nDEBUG      2013-02-06 15:44:59,856 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>\nDEBUG      2013-02-06 15:44:59,856 pika.connection                _process_frame                       1184: Frame: <METHOD(['frame_type=1', 'channel_number=0', 'method=<Connection.Close([\\'class_id=0\\', \\'method_id=0\\', \\'reply_code=320\\', \"reply_text=CONNECTION_FORCED - broker forced connection closure with reason \\'shutdown\\'\"])>'])>\nDEBUG      2013-02-06 15:44:59,857 pika.callback                  process                              217 : Processing 0:Connection.Close\nDEBUG      2013-02-06 15:44:59,857 pika.callback                  _use_one_shot_callback               390 : Processing use of oneshot callback\nDEBUG      2013-02-06 15:44:59,857 pika.callback                  _use_one_shot_callback               392 : 0 registered uses left\nDEBUG      2013-02-06 15:44:59,857 pika.callback                  remove                               260 : Removing callback #0: {'callback': <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x857192c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2013-02-06 15:44:59,857 pika.callback                  process                              231 : Calling <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x857192c>> for \"0:Connection.Close\"\nWARNING    2013-02-06 15:44:59,858 pika.connection                _on_connection_closed                1056: Disconnected from RabbitMQ at localhost:5672 (320): CONNECTION_FORCED - broker forced connection closure with reason 'shutdown'\nWARNING    2013-02-06 15:44:59,858 pika.adapters.base_connection  _check_state_on_disconnect           149 : Unknown state on disconnect: 5\nDEBUG      2013-02-06 15:44:59,858 pika.adapters.select_connection stop                                 136 : Stopping the poller event loop\nWARNING    2013-02-06 15:44:59,858 pika.channel                   _on_close                            820 : Received remote Channel.Close (320): CONNECTION_FORCED - broker forced connection closure with reason 'shutdown'\nINFO       2013-02-06 15:44:59,858 pika.callback                  cleanup                              178 : Clearing out '1' from the stack\nDEBUG      2013-02-06 15:44:59,859 pika.callback                  process                              217 : Processing 0:_on_connection_closed\nDEBUG      2013-02-06 15:44:59,859 pika.callback                  process                              231 : Calling <bound method ExampleConsumer.on_connection_closed of <__main__.ExampleConsumer object at 0x8571d4c>> for \"0:_on_connection_closed\"\nINFO       2013-02-06 15:44:59,859 __main__                       connect                              47  : Connecting to amqp://guest:guest@localhost:5672/%2F?connection_attempts=50\nDEBUG      2013-02-06 15:44:59,859 pika.callback                  add                                  161 : Added: {'callback': <bound method ExampleConsumer.on_connection_open of <__main__.ExampleConsumer object at 0x8571d4c>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2013-02-06 15:44:59,860 pika.callback                  add                                  161 : Added: {'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x857964c>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2013-02-06 15:44:59,860 pika.connection                _connect                             805 : Attempting connection\nDEBUG      2013-02-06 15:44:59,860 pika.adapters.base_connection  _adapter_connect                     98  : Connecting the adapter to the remote host\nDEBUG      2013-02-06 15:44:59,860 pika.adapters.base_connection  _create_and_connect_to_socket        153 : Creating the socket\nINFO       2013-02-06 15:44:59,861 pika.adapters.base_connection  _create_and_connect_to_socket        163 : Connecting fd 3 to localhost:5672\nERROR      2013-02-06 15:44:59,861 pika.adapters.base_connection  _adapter_connect                     109 : socket error: Connection refused\nWARNING    2013-02-06 15:44:59,861 pika.adapters.base_connection  _adapter_connect                     115 : Could not connect due to \"Connection refused,\" retrying in 2 sec\nDEBUG      2013-02-06 15:45:01,863 pika.adapters.base_connection  _create_and_connect_to_socket        153 : Creating the socket\nINFO       2013-02-06 15:45:01,863 pika.adapters.base_connection  _create_and_connect_to_socket        163 : Connecting fd 3 to localhost:5672\nERROR      2013-02-06 15:45:01,863 pika.adapters.base_connection  _adapter_connect                     109 : socket error: Connection refused\nWARNING    2013-02-06 15:45:01,864 pika.adapters.base_connection  _adapter_connect                     115 : Could not connect due to \"Connection refused,\" retrying in 2 sec\nDEBUG      2013-02-06 15:45:03,866 pika.adapters.base_connection  _create_and_connect_to_socket        153 : Creating the socket\nINFO       2013-02-06 15:45:03,866 pika.adapters.base_connection  _create_and_connect_to_socket        163 : Connecting fd 3 to localhost:5672\nDEBUG      2013-02-06 15:45:03,866 pika.adapters.select_connection start_poller                         112 : Starting the Poller\nDEBUG      2013-02-06 15:45:03,867 pika.adapters.select_connection start_poller                         121 : Using EPollPoller\nDEBUG      2013-02-06 15:45:03,867 pika.connection                _send_frame                          1319: Frame: <ProtocolHeader(['frame_type=-1', 'major=0', 'minor=9', 'revision=1'])>\nDEBUG      2013-02-06 15:45:03,867 pika.adapters.select_connection poll                                 438 : Calling <bound method SelectConnection._handle_events of <pika.adapters.select_connection.SelectConnection object at 0x857964c>>\nDEBUG      2013-02-06 15:45:03,867 pika.connection                _connect                             808 : Connected\nINFO       2013-02-06 15:45:03,868 pika.adapters.select_connection start                                391 : Unregistering poller on fd 3\nDEBUG      2013-02-06 15:45:03,868 pika.adapters.select_connection start                                395 : Got IOError while shutting down poller: [Errno 2] No such file or directory\nAnd it exits.\n. Ok no problem, thank you for your work !\n. ",
    "petere": "I think conflicting or redundant arguments should lead to an error.  This would be consistent with, for example, repeating the virtual_host argument more than once, which is also an error.\nIt would be different, for example, if I were to design a command-line interface around this, then later options would override earlier ones.\n. ",
    "Pankrat": "+1. Tests are broken because of this. Also note that it should be possible to set float timeouts (e.g. 0.5) - not just ints.\nWhy not remove the buggy validation of socket_timeout altogether? It's not very pythonic anyway.\n. It should also be possible to set float timeouts:\nhttp://docs.python.org/library/socket.html?highlight=settimeout#socket.socket.settimeout\n. ",
    "lgiordani": "This is a simple fix which is working\n-----------------------------------------8<------------------------------------------------\ndiff --git a/pika/adapters/blocking_connection.py\nb/pika/adapters/blocking_connection.py\nindex 879f589..26d1eb0 100644\n--- a/pika/adapters/blocking_connection.py\n+++ b/pika/adapters/blocking_connection.py\n@@ -39,7 +39,7 @@ class BlockingConnection(base_connection.BaseConnection):\n\"\"\"\n     timeout_id = '%.8f' % time.time()\n-        self._timeouts[timeout_id] = {'deadline': deadline,\n-        self._timeouts[timeout_id] = {'deadline': time.time() + deadline,\n                                     'handler': callback_method}\n       return timeout_id\n-----------------------------------------8<------------------------------------------------\n2012/10/1 Gavin M. Roy notifications@github.com\n\nThanks for the bug report, fix coming in shortly.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/176#issuecomment-9044135.\n\n\nLeonardo Giordani\n. I'm going to create a test to expose the problem, thank you. I'll\ninvestigate about auto_delete, too.\n2013/1/30 Gavin M. Roy notifications@github.com\n\nWithout looking at your publishing code, it's hard to say, but your code\nshould work fine with the blocking_connection adapter.\nPerhaps because auto_delete=True, your queue is being deleted before your\npublisher is publishing?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/260#issuecomment-12870464.\n\n\nLeonardo Giordani\n. This is my test code.\nThe publisher sends a message that makes the consumer declare and bind a queue.\nThen after 5 seconds the publisher sends a text message to the newly created group and after 5 seconds a text message to the standard queue.\nThe group message is not received by the consumer.\nThis behaviour is the same with or without auto_delete=True.\n(Sorry for the low quality of the code, it is just a test to expose the issue I'm experiencing)\nconsume.py\n``` python\nimport pika\ndef on_message(channel, method_frame, header_frame, body):\n    channel.queue_declare(queue=body, auto_delete=True)\nif body.startswith(\"queue:\"):\n    queue = body.replace(\"queue:\", \"\")\n    key = body + \"_key\"\n    print \"Declaring queue %s bound with key %s\" %(queue, key)\n    channel.queue_declare(queue=queue, auto_delete=True)\n    channel.queue_bind(queue=queue, exchange=\"test_exchange\", routing_key=key)\nelse:\n    print \"Message body\", body\n\nchannel.basic_ack(delivery_tag=method_frame.delivery_tag)\n\ncredentials = pika.PlainCredentials('guest', 'guest')\nparameters =  pika.ConnectionParameters('localhost', credentials=credentials)\nconnection = pika.BlockingConnection(parameters)\nchannel = connection.channel()\nchannel.exchange_declare(exchange=\"test_exchange\", exchange_type=\"direct\", passive=False, durable=True, auto_delete=False)\nchannel.queue_declare(queue=\"standard\", auto_delete=True)\nchannel.queue_bind(queue=\"standard\", exchange=\"test_exchange\", routing_key=\"standard_key\")\nchannel.basic_qos(prefetch_count=1)\nchannel.basic_consume(on_message, 'standard')\ntry:\n    channel.start_consuming()\nexcept KeyboardInterrupt:\n    channel.stop_consuming()\nconnection.close()\n```\npublish.py\n``` python\nimport pika\nimport time\ncredentials = pika.PlainCredentials('guest', 'guest')\nparameters =  pika.ConnectionParameters('localhost', credentials=credentials)\nconnection = pika.BlockingConnection(parameters)\nchannel = connection.channel()\nchannel.exchange_declare(exchange=\"test_exchange\", exchange_type=\"direct\", passive=False, durable=True, auto_delete=False)\nprint \"Sending message to create a queue\"\nchannel.basic_publish('test_exchange', 'standard_key', 'queue:group',\n                      pika.BasicProperties(content_type='text/plain',\n                                           delivery_mode=1))\ntime.sleep(5)\nprint \"Sending text message to group\"\nchannel.basic_publish('test_exchange', 'group_key', 'Message to group_key',\n                      pika.BasicProperties(content_type='text/plain',\n                                           delivery_mode=1))\ntime.sleep(5)\nprint \"Sending text message\"\nchannel.basic_publish('test_exchange', 'standard_key', 'Message to standard_key',\n                      pika.BasicProperties(content_type='text/plain',\n                                           delivery_mode=1))\nconnection.close()\n```\n. ",
    "michaelplaing": "Verification is a good thing! :) ml\nOn Mon, Oct 8, 2012 at 7:26 PM, Gavin M. Roy notifications@github.comwrote:\n\nThanks, need to verify, Channel is the class I've not had a chance to\nclean up as much as I've had liked.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/pull/183#issuecomment-9244058.\n. Hmmm. Is there a reliable way to 'wait' only as long as needed?\n\nOn Sun, Nov 4, 2012 at 8:55 AM, Gavin M. Roy notifications@github.comwrote:\n\nIt does, it expects you to wait for a synchronous call to finish before\ncalling another. I'm thinking about ways to make this easier to deal with.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/211#issuecomment-10050668.\n. I am wait'ing as described. I do not see that 'nowait=True' is a valid\nparameter for basic_consume.\n\nOn Sun, Nov 4, 2012 at 9:22 AM, Gavin M. Roy notifications@github.comwrote:\n\nThe example at pika/pika#204https://github.com/pika/pika/issues/204#issuecomment-9924606shows one wait, the other is to pass in \"nowait=True\" as one of the\narguments.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/211#issuecomment-10050935.\n. Actually there should probably be a 'basic_consume_ok' callback. As a fix\nfor now one could perhaps use Channel.add_callback(...).\n\nOn Sun, Nov 4, 2012 at 9:32 AM, Michael Laing mlaing@post.harvard.eduwrote:\n\nI am wait'ing as described. I do not see that 'nowait=True' is a valid\nparameter for basic_consume.\nOn Sun, Nov 4, 2012 at 9:22 AM, Gavin M. Roy notifications@github.comwrote:\n\nThe example at pika/pika#204https://github.com/pika/pika/issues/204#issuecomment-9924606shows one wait, the other is to pass in \"nowait=True\" as one of the\narguments.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/211#issuecomment-10050935.\n. Yes that works for me. gigimon's gist could be rewritten something like this:\n\n\n``` python\n...\ndef send_auth(self, frame):\n    def initial_consume_ok(self, frame):\n        self.chan.basic_consume(consumer_callback=self.authorize_scalarizr, queue='handshake', no_ack=False)\nbody = json.dumps({'server_id': self.id, 'crypto_key': self.crypto_key})\nself.chan.basic_publish(exchange='',\n                   routing_key='handshake', body=body,\n                   properties=pika.BasicProperties(reply_to=frame.method.queue,)\n)\nself.chan.add_callback(initial_consume_ok, [pika.spec.Basic.ConsumeOk])\nself.chan.basic_consume(consumer_callback=self.wait_credentials, queue=frame.method.queue, exclusive=True)\n\n...\n```\n. Thanks Gavin.\nOn Wed, Mar 13, 2013 at 7:42 PM, Gavin M. Roy notifications@github.comwrote:\n\nFound the problem, was not the header values themselves, but the keys.\nFixed for 0.9.10.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/280#issuecomment-14875916\n.\n. I used the 9.9 adapter, select connection. I'll see if I can send a\nbreaking example. -ml\n\nOn Tue, Feb 26, 2013 at 12:48 AM, Gavin M. Roy notifications@github.comwrote:\n\nThat's pretty odd -- I send unicode all the time. Which version and\nconnection adapter? Post 0.9.6(?) Unicode handling was cleaned up quite a\nbit.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/282#issuecomment-14096887\n.\n. Yes - unless 'debug=True' is set in the io_loop, pyev absorbs signals... However, when running in that debug mode, signals can take an unacceptably long time to propagate in my integration testing with multiple apps running - minutes, sometimes many minutes. In fact there are no guarantees from libev about when a signal will be delivered in that case, except that it will be after the signal is triggered(!). So this seemed the best solution, and creates a predictable environment - my standard callback stops the ioloop and causes the server to break out of its loop.\n. If someone does signal management, they may want to set 'debug=True' in a\ncustom io_loop. Then pyev will propagate signals, subject to the caveats I\nmentioned previously. They do not need to provide a callback.\n\nIf they do not provide an on_signal_callback, no signal watchers are\ncreated by the adapter and their signal 'experience' is basically the\nnative one provided by pyev and libev.\nIf they do provide a callback, the can reraise either signal in their\ncallback, of course, but must stop the io_loop so it will make it through\npyev, or insure that 'debug=True' is set on the io_loop.\nWith regard to that latter point, the default_loop in libev is shared, and\nthe first invocation sets its parameters, which cannot be overridden, hence\neven if an app sets 'debug=True' it may or may not take effect - it will\ngenerate a warning.\nOn Fri, Aug 9, 2013 at 7:40 PM, Gavin M. Roy notifications@github.comwrote:\n\nI'm a bit concerned about registering for signals here and responding to\nthem. What about people who use Pika in apps that do signal management?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/pull/388#issuecomment-22429379\n.\n. Great! I hope to work on examples this week.\n\nOn Mon, May 5, 2014 at 11:59 PM, Gavin M. Roy notifications@github.comwrote:\n\nMerged #466 https://github.com/pika/pika/pull/466.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/pull/466#event-118100202\n.\n. Hmm - yes it should work, that's how I run it...\n\nI just uninstalled and reinstalled libev using homebrew and uninstalled and\nreinstalled pyev with pip. Then I did 'git pull upstream' to bring in the\ncurrent pika master.\nI just ran 'nosetests --verbosity=3\ntests/acceptance/libev_adapter_tests.py' and it worked.\nMy apps are working OK too.\nI'm running Xcode 5.1.1 and OSX 10.9.2.\n:-/\nml\nOn Tue, May 6, 2014 at 10:49 AM, Gavin M. Roy notifications@github.comwrote:\n\nCool, I am getting errors with libev in my dev environment, do you know if\nthe homebrew libev+pyev should work?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/pull/466#issuecomment-42312040\n.\n. My python is:\n\nPython 2.7.6 (default, Apr  9 2014, 11:48:52)\n[GCC 4.2.1 Compatible Apple LLVM 5.1 (clang-503.0.38)] on darwin\npyev is: pyev-0.9.0\nlibev is: libev-4.15\nOn Tue, May 6, 2014 at 2:26 PM, Laing, Michael michael.laing@nytimes.comwrote:\n\nHmm - yes it should work, that's how I run it...\nI just uninstalled and reinstalled libev using homebrew and uninstalled\nand reinstalled pyev with pip. Then I did 'git pull upstream' to bring in\nthe current pika master.\nI just ran 'nosetests --verbosity=3\ntests/acceptance/libev_adapter_tests.py' and it worked.\nMy apps are working OK too.\nI'm running Xcode 5.1.1 and OSX 10.9.2.\n:-/\nml\nOn Tue, May 6, 2014 at 10:49 AM, Gavin M. Roy notifications@github.comwrote:\n\nCool, I am getting errors with libev in my dev environment, do you know\nif the homebrew libev+pyev should work?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/pull/466#issuecomment-42312040\n.\n. Removing force_binary breaks my production code - investigating...\n. yes - your commit looks good.\n\n\nOn Fri, May 9, 2014 at 12:15 PM, Samuel Kirton notifications@github.comwrote:\n\nPlease see (efea53dhttps://github.com/pika/pika/commit/efea53d109352400830609ecdca86707782fb822)\nforce_binary was already removed from channel.py. The latest build will\nbreak your production code also.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/pull/469#issuecomment-42684073\n.\n. Looks to me like force_binary is gone for good. Fine with me as we only use\nbinary message bodies.\n\nI'll just have to be careful with the next upgrade and synchronize removal\nfrom my library code.\nOn Fri, May 9, 2014 at 12:35 PM, Samuel Kirton notifications@github.comwrote:\n\nMy guess is that the owner has removed force_binary for now and will merge\nit as part of a release\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/pull/469#issuecomment-42686279\n.\n. Arrghh - forgot to squash into a branch - perhaps you can delete this?\n. Since the 'examples' are rather hefty, I could also make a NYTimes public project out of 'rabbit_helpers' and put them there instead. What do you think? I may do that anyway, using an expanded and refactored version of 'rabbit_helpers' that we will target for our own production use - that may take a while though.\n. Actually, I think I will add a more extensive set of examples and benchmarks and put these in a 'rabbit_helpers' project under the NYTimes. We can reference from here.\n. \n",
    "jameskeane": "I tested as far back as rabbitMQ 2.6, and they all leak channels without a\ncloseOK.\nAlso rabbitMQ 2.8.7, accepts the closeOK and releases the channel.\nI've talked with the rabbitMQ devs, and they said closeOK on remote channel\nclose is proper.  And that the reference java client does this.\nOn Oct 11, 2012 1:33 PM, \"Gavin M. Roy\" notifications@github.com wrote:\n\nWhich version of RabbitMQ? I had this implemented and just accepted a\npatch to remove it because the author claimed that RabbitMQ was sending an\nerror on the CloseOk.\npika/pika#183 https://github.com/pika/pika/issues/183\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/185#issuecomment-9350163.\n. Thanks, Gavin! Your speed is impressive :+1:\n. Hey Gavin,\n\nIt is a blocking connection, is the performance hit unavoidable, or is there a way to configure it?\n. ",
    "senseysensor": "Thank you!\n. I have exactly\n'0.9.6-pre5'\nmaybe I did something implicitly? I have already some pika related code in the project, before I tried to use pika 0.9.5, but have never seen such warnings before\n. Can you please specify, so here is nothing wrong with pika code and it's my fault - I try attach some callback to the same event twice? But why that started since pika update? Before I didn't have such warnings. And why in this case issue is not closed?\n. thanks for reply. And also thanks for your hard work, waiting for release (for now library is very hardly usable)\n. As I replied in another issue, my version is '0.9.6-pre5'\ninterval between opening channels is about several seconds (so to reach limit 1000 channels - about 10 minutes is passed). It is not case when I open / close channels very quickly.\nIn any moment I cannot see more then 1 opening channel in the connection (I monitor that via web-interface)\n. It's a bit weird, but nothing special. Just creating object to communicate with rabbit (my own class that uses pika) and calling its method to send request and obtain result (all this in that single method).\nSo that I wonder why traceback is so short, where no any communications with pika at all.\nUpdated full traceback (no PyCharm debugging, from console) is:\nNo handlers could be found for logger \"pika.adapters.base_connection\"\nTraceback (most recent call last):\n  File \"utilites/manager.py\", line 126, in \n    main()\n  File \"utilites/manager.py\", line 114, in main\n    '\\nException type:\\t%s; args:\\t%s' % (sys.exc_info(), e.args)+colF.RESET)\n  File \"/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pika/exceptions.py\", line 48, in repr\n    return 'The AMQP connection was closed (%s) %s' % (self.args[0],\nIndexError: tuple index out of range\nps - There are no passive queue declaration. Will make additional researches to narrow issue. Maybe there is some fault in my side.\n. Found source of issue!\nThe exception was raised on queue_delete. Exception type - ConnectionClosed. Probably backtrace was cut because deletion was in finally statement.\nSo I catch now that exception ConnectionClosed and my problem seems to be resolved (but still not sure why connection was closed by itself - maybe because I interrupt channel.start_consuming() with timeout_limiter - http://pguides.net/python-tutorial/python-timeout-a-function/). But before I had not such an issue.\nOn the other side, I believe you definitely have problem with that IndexError in repr, I cannot use exc_info() function because of that.\n. well, any time estimations...? don't pushing anyone just would like know what expect.\nthanks\n. +1\n. Probably you need more info? BTW, I have the same issue on new fresh system (after reinstalling)\n. ",
    "akorobov": "This issue seems is still broken for ssl connections in latest master:\n\nbaikal:ssl ak$ python ssl_receive.py test100\nINFO       2012-10-19 10:32:11,819 pika.adapters.base_connection       _create_and_connect_to_socket   163 : Connecting fd 3 to 127.0.0.1:5671 with SSL\ndemo_receive: Connected to RabbitMQ\ndemo_receive: Received our Channel\ndemo_receive: Queue Declared \"])>\nreceived 5000 messages\nreceived 10000 messages\nreceived 15000 messages\nreceived 20000 messages\nreceived 25000 messages\nTraceback (most recent call last):\n  File \"ssl_receive.py\", line 92, in \n    connection.ioloop.start()\n  File \"build/bdist.macosx-10.6-universal/egg/pika/adapters/select_connection.py\", line 102, in start\n  File \"build/bdist.macosx-10.6-universal/egg/pika/adapters/select_connection.py\", line 325, in start\n  File \"build/bdist.macosx-10.6-universal/egg/pika/adapters/select_connection.py\", line 350, in poll\n  File \"build/bdist.macosx-10.6-universal/egg/pika/adapters/base_connection.py\", line 279, in _handle_events\n  File \"build/bdist.macosx-10.6-universal/egg/pika/adapters/base_connection.py\", line 307, in _handle_read\n  File \"build/bdist.macosx-10.6-universal/egg/pika/connection.py\", line 1130, in _on_data_available\n  File \"build/bdist.macosx-10.6-universal/egg/pika/connection.py\", line 1184, in _process_frame\n  File \"build/bdist.macosx-10.6-universal/egg/pika/connection.py\", line 850, in _deliver_frame_to_channel\n  File \"build/bdist.macosx-10.6-universal/egg/pika/channel.py\", line 534, in handle_content_frames\n  File \"build/bdist.macosx-10.6-universal/egg/pika/channel.py\", line 803, in _on_basic_deliver\n  File \"ssl_receive.py\", line 62, in handle_delivery\n    channel.basic_ack(delivery_tag=method_frame.delivery_tag)\n  File \"build/bdist.macosx-10.6-universal/egg/pika/channel.py\", line 138, in basic_ack\n  File \"build/bdist.macosx-10.6-universal/egg/pika/channel.py\", line 963, in _rpc\n  File \"build/bdist.macosx-10.6-universal/egg/pika/channel.py\", line 974, in _send_method\n  File \"build/bdist.macosx-10.6-universal/egg/pika/connection.py\", line 1323, in _send_method\n  File \"build/bdist.macosx-10.6-universal/egg/pika/connection.py\", line 1310, in _send_frame\n  File \"build/bdist.macosx-10.6-universal/egg/pika/adapters/select_connection.py\", line 46, in _flush_outbound\n  File \"build/bdist.macosx-10.6-universal/egg/pika/adapters/select_connection.py\", line 350, in poll\n  File \"build/bdist.macosx-10.6-universal/egg/pika/adapters/base_connection.py\", line 286, in _handle_events\n  File \"build/bdist.macosx-10.6-universal/egg/pika/adapters/base_connection.py\", line 319, in _handle_write\n  File \"build/bdist.macosx-10.6-universal/egg/pika/adapters/base_connection.py\", line 233, in _handle_error\nsocket.timeout\n\nrabbitmq is 2.8.7, pika is latest master c3bb166, ssl_receive.py code is https://gist.github.com/3919513.\n. Modified ssl_receive.py(gist above)  fails with the same message. To clarify, here's the test case that results in the error:\n1. run ssl_send.py to put 100k messages in queue\n2. run ssl_receive.py to drain those messages.\nThis only happens when I use ssl connection in ssl_receive. If I switch to plain 5762 it successfully drains queue, which made me think it's related to #188.  Still figuring out pika's async/event control flows.\nHere's debug log before it fails - https://gist.github.com/3920818\n. Both AsyncoreConnection and TornadoConnection adapters work fine using ssl connection.\n. Yes, it's osx 1.6.8 with default python 2.6.1. Rerun this test on linux box with python 2.7.3 and openssl 1.0 and SelectConnection +ssl works fine, it also works fine on osx with freshly installed python 2.7.3. So it seems my default python install is acting up. Apologies for false alarm and thank you for your time!\n. ",
    "shanyu": "Roy,\nAha, that explains why! I wasn\u2019t actually using time.sleep() in my application, I just created a connection at the beginning and tried to use it periodically without calling the connection.process_data_events() function, sometimes it doesn\u2019t work because the time interval exceeds 3 * the heartbeat.\nLooks like I should switch to the io and event looping instead of using the block connection.\nThanks a lot!\nShanyu\nFrom: Gavin M. Roy [mailto:notifications@github.com]\nSent: Monday, October 22, 2012 8:07 AM\nTo: pika/pika\nCc: Zhao, Shanyu\nSubject: Re: [pika] Failure to publish delayed message on a connection with heartbeat enabled (#196)\nI added a new method to handle this. When you set a heartbeat, you're defining a contract with the broker. When you actually sleep for 9 seconds, you're violating the contract and RabbitMQ is disconnecting you. By using connection.sleep() you'll allow pika to continue to process events from RabbitMQ such as heartbeat frames violating the Heartbeat contract.\nimport pika\nconnection = pika.BlockingConnection(pika.ConnectionParameters(heartbeat_interval=3))\nchannel = connection.channel()\nchannel.queue_declare(queue='hello')\nconnection.sleep(9)\nchannel.basic_publish(exchange='',\n```\n                             routing_key='hello',\n                         body='Hello world')\n\n```\nprint \" [x] Sent 'Hellow world'\"\nconnection.close()\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/196#issuecomment-9667220.\n. ",
    "johnpneumann": "Awesome. Thanks for the update. Much appreciated.\n. When I set the queues to be exclusive, I don't receive the error message, but I also can't consume any messages either. They simply sit on the queue.\nAlso, if the consumer only subscribes to one queue, then the issue doesn't persist either. \n. Got it. Makes sense now. I was watching output trying to step through it with pdb, but just couldn't figure out the issue. Thanks a ton.\n. ",
    "rboulton": "Is anyone currently working on / making progress on this?\n. great, thanks.\n. ",
    "gigimon": "Hello, I'm have this problem too. My code is: https://gist.github.com/4001899\nAnd error:\n``` python\n/Users/gigimon/workspace/python/scalr/lib/python2.7/site-packages/pika/connection.pyc in _process_frame(self, frame_value)\n   1187         # If the frame has a channel number beyond the base channel, deliver it\n   1188         elif frame_value.channel_number > 0:\n-> 1189             self._deliver_frame_to_channel(frame_value)\n   1190 \n   1191     def _read_frame(self):\n/Users/gigimon/workspace/python/scalr/lib/python2.7/site-packages/pika/connection.pyc in _deliver_frame_to_channel(self, value)\n    839                                value, value.channel_number)\n    840             return\n--> 841         return self._channels[value.channel_number]._handle_content_frame(value)\n    842 \n    843     def _detect_backpressure(self):\n/Users/gigimon/workspace/python/scalr/lib/python2.7/site-packages/pika/channel.pyc in _handle_content_frame(self, frame_value)\n    727 \n    728         \"\"\"\n--> 729         response = self.frame_dispatcher.process(frame_value)\n    730         if response:\n    731             if isinstance(response[0].method, spec.Basic.Deliver):\n/Users/gigimon/workspace/python/scalr/lib/python2.7/site-packages/pika/channel.pyc in process(self, frame_value)\n   1021             return self._handle_body_frame(frame_value)\n   1022         else:\n-> 1023             raise exceptions.UnexpectedFrameError(frame_value)\n   1024 \n   1025     def _finish(self):\nUnexpectedFrameError: \"])>\n```\n. ",
    "krisneuharth": "Please see: https://gist.github.com/3990001\nIt was my thought that I could just call basic_nack as you suggested but that seems to not be present in BlockingChannel. I was able to get reject which works in my case for what I need, but the more general basic_nack would be nice as well.\nI started looking through the pika master branch trying to see if there was an error subclassing the base Channel which I suspect to be the cause of this method not being callable.\nWhat do you recommend? Do you need a full code example?\nThanks!\n. I am also getting this exception when stopping the queue attempting to invoke the retry behavior when running the Asynchronous consumer example.\n. ",
    "flopezluis": "Here it is (with the log set to DEBUG):\nException in thread Thread-12:\nTraceback (most recent call last):\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py\", line 552, in __bootstrap_inner\n    self.run()\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py\", line 756, in run\n    self.function(_self.args, *_self.kwargs)\n  File \"test.py\", line 57, in process\n    self.channel.basic_ack(delivery_tag=msg[0].delivery_tag)\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/channel.py\", line 136, in basic_ack\n    raise exceptions.ChannelClosed()\nChannelClosed\nTraceback (most recent call last):\n  File \"test.py\", line 71, in \n    connection.ioloop.start()\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/adapters/select_connection.py\", line 102, in start\n    self.poller.start()\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/adapters/select_connection.py\", line 325, in start\n    self.poll()\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/adapters/select_connection.py\", line 350, in poll\n    self._handler(self.fileno, events, write_only=write_only)\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 280, in _handle_events\n    self._handle_read()\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 308, in _handle_read\n    self._on_data_available(data)\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/connection.py\", line 1135, in _on_data_available\n    self._process_frame(frame_value)\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/connection.py\", line 1176, in _process_frame\n    if self._process_callbacks(frame_value):\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/connection.py\", line 1150, in _process_callbacks\n    frame_value)                 # Args\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/callback.py\", line 62, in wrapper\n    return function(_tuple(args), _kwargs)\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/callback.py\", line 93, in wrapper\n    return function(_args, _kwargs)\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/callback.py\", line 218, in process\n    callback(_args, *_keywords)\n  File \"/Users/flopez/.virtualenvs/testing/lib/python2.7/site-packages/pika/connection.py\", line 1054, in _on_connection_closed\n    self._channels[channel].on_remote_close(method_frame)\nAttributeError: 'Channel' object has no attribute 'on_remote_close'\n. No, in fact if you put a pdb after self.channel.basic_ack(delivery_tag=msg[0].delivery_tag)\nyou can see that after the first execution of that, self.channel.is_closed is True but no exception is raised\n. I've setup the logging to DEBUG (I should made a mistake when I did it before) and this is the output:\nDEBUG:pika.connection:Added 21 bytes to the outbound buffer\nDEBUG:pika.adapters.select_connection:Calling >(4)\nDEBUG:pika.adapters.select_connection:Calling >(4)\nDEBUG:pika.adapters.select_connection:Calling >(4)\n\n/Users/flopez/Dev/migration/Connectors/plaxoContacts/test.py(57)process()\n-> import pdb;pdb.set_trace()\n(Pdb) DEBUG:pika.adapters.select_connection:Calling >(5)\nDEBUG:pika.callback:Args: (, 1, ), kwargs: {}\nDEBUG:pika.callback:Args: (, 1, , , \"])>), kwargs: {}\nDEBUG:pika.callback:Processing 1:Channel.Close\nDEBUG:pika.callback:Calling > for \"1:Channel.Close\"\nWARNING:pika.channel:Received remote Channel.Close (406): PRECONDITION_FAILED - unknown delivery tag 6\nDEBUG:pika.connection:Added 12 bytes to the outbound buffer\nDEBUG:pika.adapters.select_connection:Calling >(4)\nDEBUG:pika.adapters.select_connection:Calling >(4)\nDEBUG:pika.adapters.select_connection:Calling >(4\n. yes, more or less it is. I want to do the ack one to one because I want to asure that all processed messages have sent the ack. I'm trying to avoid that if my consumer fails any ack is lost. \n\nBTW, nice example, thanks!!\n. I've tried to do it with your example, but the result was the same. It seems that is not calling to on_connection_closed. Here is the code https://gist.github.com/3988798\nAnd the traceback:\nINFO       2012-10-31 19:26:11,462 main                       connect                              51  : Connecting to amqp://guest:guest@localhost:5672/%2F\nINFO       2012-10-31 19:26:11,462 pika.adapters.base_connection  _create_and_connect_to_socket        164 : Connecting fd 3 to localhost:5672\nINFO       2012-10-31 19:26:11,466 main                       on_connection_open                   91  : Connection opened\nINFO       2012-10-31 19:26:11,466 main                       add_on_connection_close_callback     65  : Adding connection close callback\nINFO       2012-10-31 19:26:11,466 main                       open_channel                         306 : Creating a new channel\nINFO       2012-10-31 19:26:11,467 main                       on_channel_open                      127 : Channel opened\nINFO       2012-10-31 19:26:11,467 main                       add_on_channel_close_callback        100 : Adding channel close callback\nINFO       2012-10-31 19:26:11,467 main                       setup_exchange                       140 : Declaring exchange proban\nINFO       2012-10-31 19:26:11,468 main                       on_exchange_declareok                152 : Exchange declared\nINFO       2012-10-31 19:26:11,468 main                       setup_queue                          163 : Declaring queue proban\nINFO       2012-10-31 19:26:11,468 main                       on_queue_declareok                   177 : Binding proban to proban with proba\nINFO       2012-10-31 19:26:11,469 main                       on_bindok                            289 : Queue bound\nINFO       2012-10-31 19:26:11,469 main                       start_consuming                      276 : Issuing consumer related RPC commands\nINFO       2012-10-31 19:26:11,469 main                       add_on_cancel_callback               187 : Adding consumer cancellation callback\nINFO       2012-10-31 19:26:11,470 main                       on_message                           226 : Received message # 1 from None: Message\nINFO       2012-10-31 19:26:11,471 main                       on_message                           226 : Received message # 2 from None: Message\nINFO       2012-10-31 19:26:11,471 main                       on_message                           226 : Received message # 3 from None: Message\nINFO       2012-10-31 19:26:11,472 main                       on_message                           226 : Received message # 4 from None: Message\nINFO       2012-10-31 19:26:21,473 main                       acknowledge_message                  208 : Acknowledging message 4\nINFO       2012-10-31 19:26:21,474 main                       acknowledge_message                  208 : Acknowledging message 3\nINFO       2012-10-31 19:26:21,474 main                       acknowledge_message                  208 : Acknowledging message 2\nWARNING    2012-10-31 19:26:21,475 pika.channel                   _on_close                            779 : Received remote Channel.Close (406): PRECONDITION_FAILED - unknown delivery tag 4\nINFO       2012-10-31 19:26:21,475 main                       acknowledge_message                  208 : Acknowledging message 1\nWARNING    2012-10-31 19:26:21,476 main                       on_channel_closed                    115 : Channel was closed: (406) PRECONDITION_FAILED - unknown delivery tag 4\nINFO       2012-10-31 19:26:21,476 pika.connection                close                                585 : Closing connection (200): Normal shutdown\n. This is my enviroment:\nRabbitMQ 2.8.6 on Erlang R15B01. Also tried on RabbitMQ 2.8.7 on Erlang R15B01\nPython 2.7.2\nPython enviroment:\npika==0.9.6\nDateUtils==0.5.2\nDjango==1.4.2\njsonpickle==0.4.0\nlxml==3.0.1\nmockito==0.5.1\npython-dateutil==2.1\npytz==2012h\nrequests==0.14.2\nsix==1.2.0\nvobject==0.8.1c\nwsgiref==0.1.2\n. Thanks!! I really appreciate the dedication\n. Here is the whole log:\nINFO       2012-10-31 20:26:41,572 main                       connect                              53  : Connecting to amqp://guest:guest@localhost:5672/%2F\nDEBUG      2012-10-31 20:26:41,572 pika.callback                  wrapper                              49  : Args: (, 0, '_on_connection_open', >, False), kwargs: {}\nDEBUG      2012-10-31 20:26:41,572 pika.callback                  add                                  148 : Added \"0:_on_connection_open\" with callback: >\nDEBUG      2012-10-31 20:26:41,572 pika.callback                  wrapper                              49  : Args: (, 0, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,572 pika.callback                  add                                  148 : Added \"0:Connection.Start\" with callback: >\nDEBUG      2012-10-31 20:26:41,572 pika.connection                _connect                             800 : Attempting connection\nDEBUG      2012-10-31 20:26:41,572 pika.adapters.base_connection  _adapter_connect                     98  : Connecting the adapter to the remote host\nDEBUG      2012-10-31 20:26:41,573 pika.adapters.base_connection  _create_and_connect_to_socket        154 : Creating the socket\nINFO       2012-10-31 20:26:41,573 pika.adapters.base_connection  _create_and_connect_to_socket        164 : Connecting fd 3 to localhost:5672\nDEBUG      2012-10-31 20:26:41,573 pika.adapters.select_connection start_poller                         113 : Starting the Poller\nDEBUG      2012-10-31 20:26:41,574 pika.adapters.select_connection start_poller                         127 : Using KQueuePoller\nDEBUG      2012-10-31 20:26:41,574 pika.connection                _send_frame                          1314: Added 8 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:41,574 pika.adapters.select_connection poll                                 349 : Calling >(4)\nDEBUG      2012-10-31 20:26:41,574 pika.connection                _connect                             803 : Connected\nDEBUG      2012-10-31 20:26:41,574 pika.adapters.select_connection start                                99  : Starting IOLoop\nDEBUG      2012-10-31 20:26:41,575 pika.adapters.select_connection poll                                 349 : Calling >(1)\nDEBUG      2012-10-31 20:26:41,575 pika.callback                  wrapper                              49  : Args: (, 0, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,575 pika.callback                  wrapper                              49  : Args: (, 0, , , '])>), kwargs: {}\nDEBUG      2012-10-31 20:26:41,575 pika.callback                  process                              200 : Processing 0:Connection.Start\nDEBUG      2012-10-31 20:26:41,576 pika.callback                  wrapper                              49  : Args: (, '0', 'Connection.Start', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,576 pika.callback                  process                              217 : Calling > for \"0:Connection.Start\"\nDEBUG      2012-10-31 20:26:41,576 pika.callback                  wrapper                              49  : Args: (, 0, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,576 pika.callback                  add                                  148 : Added \"0:Connection.Tune\" with callback: >\nDEBUG      2012-10-31 20:26:41,576 pika.connection                _send_frame                          1314: Added 247 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:41,577 pika.adapters.select_connection poll                                 349 : Calling >(4)\nDEBUG      2012-10-31 20:26:41,577 pika.adapters.select_connection poll                                 349 : Calling >(1)\nDEBUG      2012-10-31 20:26:41,577 pika.callback                  wrapper                              49  : Args: (, 0, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,577 pika.callback                  wrapper                              49  : Args: (, 0, , , \"])>), kwargs: {}\nDEBUG      2012-10-31 20:26:41,577 pika.callback                  process                              200 : Processing 0:Connection.Tune\nDEBUG      2012-10-31 20:26:41,577 pika.callback                  wrapper                              49  : Args: (, '0', 'Connection.Tune', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,578 pika.callback                  process                              217 : Calling > for \"0:Connection.Tune\"\nDEBUG      2012-10-31 20:26:41,578 pika.connection                _send_frame                          1314: Added 20 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:41,578 pika.adapters.select_connection poll                                 349 : Calling >(4)\nDEBUG      2012-10-31 20:26:41,578 pika.callback                  wrapper                              49  : Args: (, 0, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,578 pika.callback                  add                                  148 : Added \"0:Connection.OpenOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,578 pika.connection                _send_frame                          1314: Added 16 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:41,579 pika.adapters.select_connection poll                                 349 : Calling >(4)\nDEBUG      2012-10-31 20:26:41,579 pika.adapters.select_connection poll                                 349 : Calling >(1)\nDEBUG      2012-10-31 20:26:41,579 pika.callback                  wrapper                              49  : Args: (, 0, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,579 pika.callback                  wrapper                              49  : Args: (, 0, , , \"])>), kwargs: {}\nDEBUG      2012-10-31 20:26:41,579 pika.callback                  process                              200 : Processing 0:Connection.OpenOk\nDEBUG      2012-10-31 20:26:41,579 pika.callback                  wrapper                              49  : Args: (, '0', 'Connection.OpenOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,580 pika.callback                  process                              217 : Calling > for \"0:Connection.OpenOk\"\nDEBUG      2012-10-31 20:26:41,580 pika.callback                  wrapper                              49  : Args: (, 0, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,580 pika.callback                  add                                  148 : Added \"0:Connection.Close\" with callback: >\nDEBUG      2012-10-31 20:26:41,580 pika.callback                  wrapper                              49  : Args: (, 0, '_on_connection_open', , ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,580 pika.callback                  process                              200 : Processing 0:_on_connection_open\nDEBUG      2012-10-31 20:26:41,580 pika.callback                  process                              217 : Calling > for \"0:_on_connection_open\"\nINFO       2012-10-31 20:26:41,580 main                       on_connection_open                   92  : Connection opened\nINFO       2012-10-31 20:26:41,580 main                       add_on_connection_close_callback     67  : Adding connection close callback\nDEBUG      2012-10-31 20:26:41,580 pika.callback                  wrapper                              49  : Args: (, 0, '_on_connection_closed', >, False), kwargs: {}\nDEBUG      2012-10-31 20:26:41,581 pika.callback                  add                                  148 : Added \"0:_on_connection_closed\" with callback: >\nINFO       2012-10-31 20:26:41,581 main                       open_channel                         307 : Creating a new channel\nDEBUG      2012-10-31 20:26:41,581 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,581 pika.callback                  add                                  148 : Added \"1:Channel.CloseOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,581 pika.callback                  wrapper                              49  : Args: (, 1, , >, False), kwargs: {}\nDEBUG      2012-10-31 20:26:41,581 pika.callback                  add                                  148 : Added \"1:Basic.GetEmpty\" with callback: >\nDEBUG      2012-10-31 20:26:41,581 pika.callback                  wrapper                              49  : Args: (, 1, , >, False), kwargs: {}\nDEBUG      2012-10-31 20:26:41,581 pika.callback                  add                                  148 : Added \"1:Basic.Cancel\" with callback: >\nDEBUG      2012-10-31 20:26:41,581 pika.callback                  wrapper                              49  : Args: (, 1, , >, False), kwargs: {}\nDEBUG      2012-10-31 20:26:41,582 pika.callback                  add                                  148 : Added \"1:Channel.Flow\" with callback: >\nDEBUG      2012-10-31 20:26:41,582 pika.callback                  wrapper                              49  : Args: (, 1, , >, False), kwargs: {}\nDEBUG      2012-10-31 20:26:41,582 pika.callback                  add                                  148 : Added \"1:Channel.Close\" with callback: >\nDEBUG      2012-10-31 20:26:41,582 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,582 pika.callback                  add                                  148 : Added \"1:Channel.OpenOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,582 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,582 pika.callback                  add                                  148 : Added \"1:Channel.OpenOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,582 pika.connection                _send_frame                          1314: Added 13 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:41,582 pika.adapters.select_connection poll                                 349 : Calling >(4)\nDEBUG      2012-10-31 20:26:41,583 pika.adapters.select_connection poll                                 349 : Calling >(1)\nDEBUG      2012-10-31 20:26:41,583 pika.callback                  wrapper                              49  : Args: (, 1, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,583 pika.callback                  wrapper                              49  : Args: (, 1, , , \"])>), kwargs: {}\nDEBUG      2012-10-31 20:26:41,584 pika.callback                  process                              200 : Processing 1:Channel.OpenOk\nDEBUG      2012-10-31 20:26:41,584 pika.callback                  wrapper                              49  : Args: (, '1', 'Channel.OpenOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,584 pika.callback                  wrapper                              49  : Args: (, '1', 'Channel.OpenOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,584 pika.callback                  process                              217 : Calling > for \"1:Channel.OpenOk\"\nDEBUG      2012-10-31 20:26:41,584 pika.channel                   _on_synchronous_complete             910 : 0 blocked frames\nDEBUG      2012-10-31 20:26:41,584 pika.callback                  process                              217 : Calling > for \"1:Channel.OpenOk\"\nINFO       2012-10-31 20:26:41,584 main                       on_channel_open                      128 : Channel opened\nINFO       2012-10-31 20:26:41,584 main                       add_on_channel_close_callback        101 : Adding channel close callback\nDEBUG      2012-10-31 20:26:41,584 pika.callback                  wrapper                              49  : Args: (, 1, , >, False), kwargs: {}\nDEBUG      2012-10-31 20:26:41,585 pika.callback                  add                                  148 : Added \"1:Channel.Close\" with callback: >\nINFO       2012-10-31 20:26:41,585 main                       setup_exchange                       141 : Declaring exchange proban\nDEBUG      2012-10-31 20:26:41,585 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,585 pika.callback                  add                                  148 : Added \"1:Exchange.DeclareOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,585 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,585 pika.callback                  add                                  148 : Added \"1:Exchange.DeclareOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,585 pika.connection                _send_frame                          1314: Added 33 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:41,586 pika.adapters.select_connection poll                                 349 : Calling >(4)\nDEBUG      2012-10-31 20:26:41,586 pika.adapters.select_connection poll                                 349 : Calling >(1)\nDEBUG      2012-10-31 20:26:41,586 pika.callback                  wrapper                              49  : Args: (, 1, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,586 pika.callback                  wrapper                              49  : Args: (, 1, , , '])>), kwargs: {}\nDEBUG      2012-10-31 20:26:41,586 pika.callback                  process                              200 : Processing 1:Exchange.DeclareOk\nDEBUG      2012-10-31 20:26:41,586 pika.callback                  wrapper                              49  : Args: (, '1', 'Exchange.DeclareOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,587 pika.callback                  wrapper                              49  : Args: (, '1', 'Exchange.DeclareOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,587 pika.callback                  process                              217 : Calling > for \"1:Exchange.DeclareOk\"\nDEBUG      2012-10-31 20:26:41,587 pika.channel                   _on_synchronous_complete             910 : 0 blocked frames\nDEBUG      2012-10-31 20:26:41,587 pika.callback                  process                              217 : Calling > for \"1:Exchange.DeclareOk\"\nINFO       2012-10-31 20:26:41,587 main                       on_exchange_declareok                153 : Exchange declared\nINFO       2012-10-31 20:26:41,587 main                       setup_queue                          164 : Declaring queue proban\nDEBUG      2012-10-31 20:26:41,587 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,587 pika.callback                  add                                  148 : Added \"1:Queue.DeclareOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,587 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,587 pika.callback                  add                                  148 : Added \"1:Queue.DeclareOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,587 pika.connection                _send_frame                          1314: Added 26 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:41,587 pika.adapters.select_connection poll                                 349 : Calling >(4)\nDEBUG      2012-10-31 20:26:41,588 pika.adapters.select_connection poll                                 349 : Calling >(1)\nDEBUG      2012-10-31 20:26:41,588 pika.callback                  wrapper                              49  : Args: (, 1, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,588 pika.callback                  wrapper                              49  : Args: (, 1, , , \"])>), kwargs: {}\nDEBUG      2012-10-31 20:26:41,588 pika.callback                  process                              200 : Processing 1:Queue.DeclareOk\nDEBUG      2012-10-31 20:26:41,588 pika.callback                  wrapper                              49  : Args: (, '1', 'Queue.DeclareOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,588 pika.callback                  wrapper                              49  : Args: (, '1', 'Queue.DeclareOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,589 pika.callback                  process                              217 : Calling > for \"1:Queue.DeclareOk\"\nDEBUG      2012-10-31 20:26:41,589 pika.channel                   _on_synchronous_complete             910 : 0 blocked frames\nDEBUG      2012-10-31 20:26:41,589 pika.callback                  process                              217 : Calling > for \"1:Queue.DeclareOk\"\nINFO       2012-10-31 20:26:41,589 main                       on_queue_declareok                   178 : Binding proban to proban with proba\nDEBUG      2012-10-31 20:26:41,589 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,589 pika.callback                  add                                  148 : Added \"1:Queue.BindOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,589 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,589 pika.callback                  add                                  148 : Added \"1:Queue.BindOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,590 pika.connection                _send_frame                          1314: Added 39 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:41,590 pika.adapters.select_connection poll                                 349 : Calling >(4)\nDEBUG      2012-10-31 20:26:41,590 pika.adapters.select_connection poll                                 349 : Calling >(1)\nDEBUG      2012-10-31 20:26:41,591 pika.callback                  wrapper                              49  : Args: (, 1, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,591 pika.callback                  wrapper                              49  : Args: (, 1, , , '])>), kwargs: {}\nDEBUG      2012-10-31 20:26:41,591 pika.callback                  process                              200 : Processing 1:Queue.BindOk\nDEBUG      2012-10-31 20:26:41,591 pika.callback                  wrapper                              49  : Args: (, '1', 'Queue.BindOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,591 pika.callback                  wrapper                              49  : Args: (, '1', 'Queue.BindOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,591 pika.callback                  process                              217 : Calling > for \"1:Queue.BindOk\"\nDEBUG      2012-10-31 20:26:41,591 pika.channel                   _on_synchronous_complete             910 : 0 blocked frames\nDEBUG      2012-10-31 20:26:41,591 pika.callback                  process                              217 : Calling > for \"1:Queue.BindOk\"\nINFO       2012-10-31 20:26:41,591 main                       on_bindok                            290 : Queue bound\nINFO       2012-10-31 20:26:41,591 main                       start_consuming                      277 : Issuing consumer related RPC commands\nINFO       2012-10-31 20:26:41,591 main                       add_on_cancel_callback               188 : Adding consumer cancellation callback\nDEBUG      2012-10-31 20:26:41,592 pika.callback                  wrapper                              49  : Args: (, 1, , >, False), kwargs: {}\nDEBUG      2012-10-31 20:26:41,592 pika.callback                  add                                  148 : Added \"1:Basic.Cancel\" with callback: >\nDEBUG      2012-10-31 20:26:41,592 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,592 pika.callback                  add                                  148 : Added \"1:Basic.ConsumeOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,592 pika.callback                  wrapper                              49  : Args: (, 1, , >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,592 pika.callback                  add                                  148 : Added \"1:Basic.ConsumeOk\" with callback: >\nDEBUG      2012-10-31 20:26:41,592 pika.connection                _send_frame                          1314: Added 34 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:41,592 pika.adapters.select_connection poll                                 349 : Calling >(4)\nDEBUG      2012-10-31 20:26:41,593 pika.adapters.select_connection poll                                 349 : Calling >(1)\nDEBUG      2012-10-31 20:26:41,593 pika.callback                  wrapper                              49  : Args: (, 1, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,593 pika.callback                  wrapper                              49  : Args: (, 1, , , \"])>), kwargs: {}\nDEBUG      2012-10-31 20:26:41,593 pika.callback                  process                              200 : Processing 1:Basic.ConsumeOk\nDEBUG      2012-10-31 20:26:41,593 pika.callback                  wrapper                              49  : Args: (, '1', 'Basic.ConsumeOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,594 pika.callback                  wrapper                              49  : Args: (, '1', 'Basic.ConsumeOk', >), kwargs: {}\nDEBUG      2012-10-31 20:26:41,594 pika.callback                  process                              217 : Calling > for \"1:Basic.ConsumeOk\"\nDEBUG      2012-10-31 20:26:41,594 pika.channel                   _on_synchronous_complete             910 : 0 blocked frames\nDEBUG      2012-10-31 20:26:41,594 pika.callback                  process                              217 : Calling > for \"1:Basic.ConsumeOk\"\nDEBUG      2012-10-31 20:26:41,594 pika.channel                   _on_eventok                          816 : Discarding frame \"])>\nDEBUG      2012-10-31 20:26:41,594 pika.callback                  wrapper                              49  : Args: (, 1, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,594 pika.channel                   _on_deliver                          793 : Called with \"])>, \", 'body_size=7'])>, 'Message'\nINFO       2012-10-31 20:26:41,595 main                       on_message                           227 : Received message # 1 from None: Message\nDEBUG      2012-10-31 20:26:41,595 pika.callback                  wrapper                              49  : Args: (, 1, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,595 pika.channel                   _on_deliver                          793 : Called with \"])>, \", 'body_size=7'])>, 'Message'\nINFO       2012-10-31 20:26:41,596 main                       on_message                           227 : Received message # 2 from None: Message\nDEBUG      2012-10-31 20:26:41,596 pika.callback                  wrapper                              49  : Args: (, 1, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,596 pika.channel                   _on_deliver                          793 : Called with \"])>, \", 'body_size=7'])>, 'Message'\nINFO       2012-10-31 20:26:41,596 main                       on_message                           227 : Received message # 3 from None: Message\nDEBUG      2012-10-31 20:26:41,597 pika.callback                  wrapper                              49  : Args: (, 1, ), kwargs: {}\nDEBUG      2012-10-31 20:26:41,597 pika.channel                   _on_deliver                          793 : Called with \"])>, \", 'body_size=7'])>, 'Message'\nINFO       2012-10-31 20:26:41,598 main                       on_message                           227 : Received message # 4 from None: Message\nINFO       2012-10-31 20:26:51,599 main                       acknowledge_message                  209 : Acknowledging message 4\nDEBUG      2012-10-31 20:26:51,599 pika.connection                _send_frame                          1314: Added 21 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:51,600 pika.adapters.select_connection poll                                 349 : Calling >(4)\nDEBUG      2012-10-31 20:26:51,600 pika.adapters.select_connection poll                                 349 : Calling >(4)\nINFO       2012-10-31 20:26:51,601 main                       acknowledge_message                  209 : Acknowledging message 3\nDEBUG      2012-10-31 20:26:51,601 pika.connection                _send_frame                          1314: Added 21 bytes to the outbound buffer\nINFO       2012-10-31 20:26:51,602 main                       acknowledge_message                  209 : Acknowledging message 2\nDEBUG      2012-10-31 20:26:51,602 pika.connection                _send_frame                          1314: Added 21 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:51,602 pika.adapters.select_connection poll                                 349 : Calling >(5)\nINFO       2012-10-31 20:26:51,602 main                       acknowledge_message                  209 : Acknowledging message 1\nDEBUG      2012-10-31 20:26:51,603 pika.connection                _send_frame                          1314: Added 21 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:51,603 pika.adapters.select_connection poll                                 349 : Calling >(1)\nDEBUG      2012-10-31 20:26:51,603 pika.adapters.select_connection poll                                 349 : Calling >(1)\nDEBUG      2012-10-31 20:26:51,604 pika.callback                  wrapper                              49  : Args: (, 1, ), kwargs: {}\nDEBUG      2012-10-31 20:26:51,604 pika.callback                  wrapper                              49  : Args: (, 1, , , \"])>), kwargs: {}\nDEBUG      2012-10-31 20:26:51,604 pika.callback                  process                              200 : Processing 1:Channel.Close\nDEBUG      2012-10-31 20:26:51,605 pika.callback                  process                              217 : Calling > for \"1:Channel.Close\"\nWARNING    2012-10-31 20:26:51,605 pika.channel                   _on_close                            779 : Received remote Channel.Close (406): PRECONDITION_FAILED - unknown delivery tag 4\nDEBUG      2012-10-31 20:26:51,605 pika.connection                _send_frame                          1314: Added 12 bytes to the outbound buffer\nDEBUG      2012-10-31 20:26:52,606 pika.callback                  process                              217 : Calling > for \"1:Channel.Close\"\nWARNING    2012-10-31 20:26:52,607 main                       on_channel_closed                    116 : Channel was closed: (406) PRECONDITION_FAILED - unknown delivery tag 4\nINFO       2012-10-31 20:26:52,607 pika.connection                close                                585 : Closing connection (200): Normal shutdown\nDEBUG      2012-10-31 20:26:52,607 pika.adapters.select_connection stop                                 137 : Stopping the poller event loop\n. I didn't know that tool!\nI think I'm going to change my solution and use the multiple ack as you say \nThanks!! Awesome answer\n. I'm on Ubuntu 12.04 and I have a problem similar:\n```\n2013-07-30 10:38:03,450 [WARNING] consumer: Channel '])> was closed: (None) None\n2013-07-30 10:51:41,153 [WARNING] consumer: Connection closed, reopening in 5 seconds: (None) None\n2013-07-30 10:51:41,154 [ERROR] pika.adapters.base_connection: Error event 25, None\n2013-07-30 10:51:41,154 [CRITICAL] pika.adapters.base_connection: Tried to handle an error where no error existed\n2013-07-30 10:51:41,154 [WARNING] consumer: Connection closed, reopening in 5 seconds: (None) None\n2013-07-30 10:51:41,154 [WARNING] consumer: Connection closed, reopening in 5 seconds: (None) None\n2013-07-30 10:51:41,154 [WARNING] consumer: Connection closed, reopening in 5 seconds: (None) None\n2013-07-30 10:51:41,154 [ERROR] pika.adapters.base_connection: Error event 9, None\n2013-07-30 10:51:41,155 [CRITICAL] pika.adapters.base_connection: Tried to handle an error where no error existed\nAfter that never reconnect to the broker.\n```\nHere is the stacktrace:\nFile \"/home/ubuntu/endpoints/testing/env/src/consumer/consumer/__init__.py\", line 261, in acknowledge_message\n self._channel.basic_ack(delivery_tag)\nFile \"/home/ubuntu/endpoints/testing/env/local/lib/python2.7/site-packages/pika/channel.py\", line 146, in basic_ack\n return self._send_method(spec.Basic.Ack(delivery_tag, multiple))\nFile \"/home/ubuntu/endpoints/testing/env/local/lib/python2.7/site-packages/pika/channel.py\", line 1055, in _send_method\n self.connection._send_method(self.channel_number, method_frame, content)\nFile \"/home/ubuntu/endpoints/testing/env/local/lib/python2.7/site-packages/pika/connection.py\", line 1366, in _send_method\n self._send_frame(frame.Method(channel_number, method_frame))\nFile \"/home/ubuntu/endpoints/testing/env/local/lib/python2.7/site-packages/pika/connection.py\", line 1353, in _send_frame\n self._flush_outbound()\nFile \"/home/ubuntu/endpoints/testing/env/local/lib/python2.7/site-packages/pika/adapters/select_connection.py\", line 45, in _flush_outbound\n self.ioloop.poller.poll(write_only=True)\nFile \"/home/ubuntu/endpoints/testing/env/local/lib/python2.7/site-packages/pika/adapters/select_connection.py\", line 445, in poll\n self._handler(fileno, event, write_only=write_only)\nFile \"/home/ubuntu/endpoints/testing/env/local/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 300, in _handle_events\n self._handle_error(error)\nFile \"/home/ubuntu/endpoints/testing/env/local/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 248, in _handle_error\n. ",
    "damzam": "I'll give you a pass on the saucy rhetorical question. But let me be clear:\nThe following code did NOT hang in Pika version 0.9.5:\n```\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\nconnection.socket.settimeout(10000)\nchannel = connection.channel()\nchannel.queue_declare(queue=queue_name, durable=True)\nchannel.basic_publish(exchange = '',\n                  routing_key = queue_name,\n                  body = msg,\n                  properties = pika.BasicProperties(\n                     delivery_mode = 2,\n                  ))\n```\nIt DOES hang with Pika 0.9.6. Why the change in behavior?\n. It does not seem intuitive at all to me that setting a connection's socket timeout would result in it blocking for the duration specified.\ni.e. if I were to do the following\nimport socket; socket.setdefaulttimeout(1000)\nI would SHOCKED if this meant that every connection would block for 1000 seconds.\n. Forgive me if I've come off as abrupt here.\nWas only trying to ensure that the connections wouldn't timeout. The change in behavior was surprising and not intuitive.\nWe're grateful for the work that you all have done on this project.\n. ",
    "quiver": "Thank you for your great work, Gavin!\n. ",
    "pspierce": "Any word on this one?  I'm trying to work through the tutorial and seeing this behavior in \"Routing\" with the receive_logs_direct.py file hanging at line 11, \"result = channel.queue_declare(exclusive=True)\".  It occurred in the previous tutorial as well but I just moved on.  It would seem to be related to this issue.\n. ",
    "jonathanhogg": "Same here. A simple fix appears to be to swap the 'ioloop.remove_handler' line and the 'super' call line in _adapter_disconnect().\n. ",
    "sn4kebite": "This is the error I see:\n=ERROR REPORT==== 27-Nov-2012::17:38:50 ===\nAMQP connection <0.4222.11> (running), channel 1 - error:\n{amqp_error,channel_error,\"expected 'channel.open'\",'channel.close'}\nEDIT: I'm also seeing this instead sometimes, but not until after I stop my script:\nclosing AMQP connection <0.4357.11> (*snip*):\nconnection_closed_abruptly\nEDIT: Actually, please disregard that first error, my bad.\n. I have a set of consumers with their own queues, bound twice to the same exchange with the queue name and an empty string as routing key, so that I can publish to either a specific consumer or broadcast to all of them. I could of course use something other than an empty string, but it hasn't really occurred to me that this might be invalid, since this worked fine with 0.9.5.\n. The presence-exchange plugin uses empty messages, which is where I encountered this issue.\n. Forgot to mention that my closeok callback is never called, meaning I'm stuck with a dead consumer here.\n. Seems commit bf2d526 fixed this issue, as I no longer see any warnings and all callbacks works as expected.\nconnecting\nconnected\nchannel opened\nconsuming from foo\nconsuming from bar\nconsumer canceled\nchannel closed (Channel.CloseOk)\nreopening channel\nchannel opened\nconsuming from foo\nconsuming from bar\nconsumer canceled\nchannel closed (Channel.CloseOk)\nreopening channel\nchannel opened\nconsuming from foo\nconsuming from bar\n. Same example with added logging, using latest master:\n$ python2 pika-blocking-consumer.py 10\nINFO:pika.adapters.base_connection:Connecting to ::1:5672\nbasic_consume\nReceived: 1\nReceived: 2\nReceived: 3\nReceived: 4\nReceived: 5\nReceived: 6\nReceived: 7\nReceived: 8\nReceived: 9\nReceived: 10\n^CINFO:pika.adapters.blocking_connection:Channel.close(0, Normal Shutdown)\nINFO:pika.adapters.blocking_connection:Closing connection (200): Normal shutdown\nWARNING:pika.adapters.base_connection:Unknown state on disconnect: 0\nWARNING:pika.adapters.base_connection:Unknown state on disconnect: 0\nAnd here's the same as above with the logging level set to DEBUG:\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingConnection._on_connection_error of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingConnection._on_connection_start of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nINFO:pika.adapters.base_connection:Connecting to ::1:5672\nDEBUG:pika.callback:Processing 0:Connection.Start\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': <bound method BlockingConnection._on_connection_start of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling <bound method BlockingConnection._on_connection_start of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>> for \"0:Connection.Start\"\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingConnection._on_connection_tune of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Processing 0:Connection.Tune\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': <bound method BlockingConnection._on_connection_tune of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling <bound method BlockingConnection._on_connection_tune of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>> for \"0:Connection.Tune\"\nDEBUG:pika.connection:Creating a HeartbeatChecker: 600\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingConnection._on_connection_open of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Processing 0:Connection.OpenOk\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': <bound method BlockingConnection._on_connection_open of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling <bound method BlockingConnection._on_connection_open of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>> for \"0:Connection.OpenOk\"\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingConnection._on_connection_closed of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.adapters.blocking_connection:Opening channel 1\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingChannel._on_close of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingChannel._on_getempty of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingChannel._on_cancel of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Processing 1:Channel.OpenOk\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>> for \"1:Channel.OpenOk\"\nbasic_consume\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.0'}, 'calls': 1}\nDEBUG:pika.callback:Processing 1:Basic.ConsumeOk\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Comparing {'consumer_tag': 'ctag1.0'} to {'consumer_tag': 'ctag1.0'}\nDEBUG:pika.callback:Removing callback #0: {'callback': <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.0'}, 'calls': 0}\nDEBUG:pika.callback:Calling <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>> for \"1:Basic.ConsumeOk\"\nDEBUG:pika.channel:Called with <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Deliver(['consumer_tag=ctag1.0', 'redelivered=False', 'routing_key=foo', 'delivery_tag=1', 'exchange='])>\"])>, <Header(['frame_type=2', 'channel_number=1', 'properties=<BasicProperties>', 'body_size=1'])>, '1'\nReceived: 1\nDEBUG:pika.channel:Called with <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Deliver(['consumer_tag=ctag1.0', 'redelivered=False', 'routing_key=foo', 'delivery_tag=2', 'exchange='])>\"])>, <Header(['frame_type=2', 'channel_number=1', 'properties=<BasicProperties>', 'body_size=1'])>, '2'\nReceived: 2\nDEBUG:pika.channel:Called with <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Deliver(['consumer_tag=ctag1.0', 'redelivered=False', 'routing_key=foo', 'delivery_tag=3', 'exchange='])>\"])>, <Header(['frame_type=2', 'channel_number=1', 'properties=<BasicProperties>', 'body_size=1'])>, '3'\nReceived: 3\nDEBUG:pika.channel:Called with <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Deliver(['consumer_tag=ctag1.0', 'redelivered=False', 'routing_key=foo', 'delivery_tag=4', 'exchange='])>\"])>, <Header(['frame_type=2', 'channel_number=1', 'properties=<BasicProperties>', 'body_size=1'])>, '4'\nReceived: 4\nDEBUG:pika.channel:Called with <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Deliver(['consumer_tag=ctag1.0', 'redelivered=False', 'routing_key=foo', 'delivery_tag=5', 'exchange='])>\"])>, <Header(['frame_type=2', 'channel_number=1', 'properties=<BasicProperties>', 'body_size=1'])>, '5'\nReceived: 5\nDEBUG:pika.channel:Called with <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Deliver(['consumer_tag=ctag1.0', 'redelivered=False', 'routing_key=foo', 'delivery_tag=6', 'exchange='])>\"])>, <Header(['frame_type=2', 'channel_number=1', 'properties=<BasicProperties>', 'body_size=1'])>, '6'\nReceived: 6\nDEBUG:pika.channel:Called with <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Deliver(['consumer_tag=ctag1.0', 'redelivered=False', 'routing_key=foo', 'delivery_tag=7', 'exchange='])>\"])>, <Header(['frame_type=2', 'channel_number=1', 'properties=<BasicProperties>', 'body_size=1'])>, '7'\nReceived: 7\nDEBUG:pika.channel:Called with <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Deliver(['consumer_tag=ctag1.0', 'redelivered=False', 'routing_key=foo', 'delivery_tag=8', 'exchange='])>\"])>, <Header(['frame_type=2', 'channel_number=1', 'properties=<BasicProperties>', 'body_size=1'])>, '8'\nReceived: 8\nDEBUG:pika.channel:Called with <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Deliver(['consumer_tag=ctag1.0', 'redelivered=False', 'routing_key=foo', 'delivery_tag=9', 'exchange='])>\"])>, <Header(['frame_type=2', 'channel_number=1', 'properties=<BasicProperties>', 'body_size=1'])>, '9'\nReceived: 9\nDEBUG:pika.channel:Called with <METHOD(['frame_type=1', 'channel_number=1', \"method=<Basic.Deliver(['consumer_tag=ctag1.0', 'redelivered=False', 'routing_key=foo', 'delivery_tag=10', 'exchange='])>\"])>, <Header(['frame_type=2', 'channel_number=1', 'properties=<BasicProperties>', 'body_size=2'])>, '10'\nReceived: 10\n^CDEBUG:pika.callback:Added: {'callback': <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.0'}, 'calls': 1}\nDEBUG:pika.callback:Processing 1:Basic.CancelOk\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Comparing {'consumer_tag': 'ctag1.0'} to {'consumer_tag': 'ctag1.0'}\nDEBUG:pika.callback:Removing callback #0: {'callback': <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.0'}, 'calls': 0}\nDEBUG:pika.callback:Calling <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>> for \"1:Basic.CancelOk\"\nINFO:pika.adapters.blocking_connection:Channel.close(0, Normal Shutdown)\nDEBUG:pika.callback:Incremented callback reference counter: {'callback': <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\nDEBUG:pika.callback:Processing 1:Channel.CloseOk\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:1 registered uses left\nDEBUG:pika.callback:Calling <bound method BlockingChannel._on_rpc_complete of <pika.adapters.blocking_connection.BlockingChannel object at 0x7fcca345f4d0>> for \"1:Channel.CloseOk\"\nDEBUG:pika.callback:Clearing out '1' from the stack\nINFO:pika.adapters.blocking_connection:Closing connection (200): Normal shutdown\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingConnection._on_connection_closed of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Processing 0:Connection.CloseOk\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': <bound method BlockingConnection._on_connection_closed of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling <bound method BlockingConnection._on_connection_closed of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>> for \"0:Connection.CloseOk\"\nWARNING:pika.adapters.base_connection:Unknown state on disconnect: 0\nDEBUG:pika.callback:Added: {'callback': <bound method BlockingConnection._on_connection_start of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nWARNING:pika.adapters.base_connection:Unknown state on disconnect: 0\nDEBUG:pika.callback:Incremented callback reference counter: {'callback': <bound method BlockingConnection._on_connection_start of <pika.adapters.blocking_connection.BlockingConnection object at 0x7fcca345f090>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\n. ",
    "hvnsweeting": "solved problem:\nThis was happened because I installed devstack on my computer and it cause some unknown behaviors. Remove keystone, rabbitmq-server then reinstall rabbitmq-server make those examples work.\n. ",
    "aswinnair": "I had the same issue (had devstack installed), but turns out that the real issue was that the credentials were not setup correctly. \ntail -100  /var/log/rabbitmq/rabbit@ubuntu.log  reveled the following \n=ERROR REPORT==== 16-Feb-2013::17:42:32 ===\nexception on TCP connection <0.364.0> from 192.168.50.1:55486\n{channel0_error,starting,\n                {amqp_error,access_refused,\n                            \"PLAIN login refused: user 'guest' - invalid credentials\",\n                            'connection.start_ok'}}\nAs per http://comments.gmane.org/gmane.comp.networking.rabbitmq.general/12916 I did the following and things were fine \nrabbitmqctl change_password guest guest\nNote : guest/guest is what is used by default as per  PlainCredentials documentation . \n. ",
    "dlzh": "Hi Aswinnair,\nThanks. That works for me. \n. ",
    "marioluan": "+1 @aswinnair \n. ",
    "irodushka": "I want to add - this error is due to unicode hostname representation. Casting it to str(hostname) works fine in my case.\n. I remember that I saw just the same behavoiur while assigning pika.BasicProperties.headers, when header name was in unicode. Maybe it is more common.\nThanks.\n. Well, I understand it. But maybe it will be better approach to execute\ntimeout handler AFTER previous event handler finishes? Without such a\ncollision cause it may make the code execution inpredictable. Why don't you\nmake a event queue for ioloop, when event fires it puts handler pointer\ninto this queue, and ioloop processes this queue one-by-one fifo. Just like\nmswindows message loop:)\n2013/1/30 Gavin M. Roy notifications@github.com\n\nThe main reason you see this is Python is not a real-time system and\noperates in a single thread. In async development, you do not have hard (or\neven) soft guarantees for timers firing when you want them to. The timeouts\nare the first time pika can fire the event due to other processing in the\nPython interpreter.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/234#issuecomment-12870847.\n. Sure. zip will be ok?\n\n. \n",
    "cmiller-github": "sorry, pebcak\n. ",
    "richardhills": "This behaviour was introduced with commit https://github.com/pika/pika/commit/da5411dac4d17c85b81e60c33dacc09acf83e9d8.\nFiring any kind of callback between sending frames is not good. There is a \"write-only\" check on the poll method to prevent callbacks triggered by the arrival of new messages. So why fire timeout callbacks?\nLooking at tornadoconnection, this doesn't seem to suffer from the same problem.\n. ",
    "pbacci": "I have been having the same issue and commenting out the process_timeouts() call in flush_outbounds seems to resolve it for me.\n. ",
    "datakurre": "It's asynchronous, but single-threaded: there's no parallel execution. In theory, you could spawn worker threads from callbacks, but best would be just to design your messaging in a way that you could run multiple single-threaded clients consuming the messages in parallel.\n. ",
    "rikhul": "An updated graph, for what it is worth...\nnote that time to make a single message publish once the connection has been established has gone down (the colors are not the same as in the graph above)\n\n. It is a mix of Blocking and Select in that graph. 4-8 connections made per second\nPython 2.6\nRabbitMQ 2.8.7 on Erlang R14B04\nWindows web servers, ubuntu running a two-node rabbit cluster (all queues are on one node and not mirrored atm)\nPlease ask if I can assist with any other info!\n. We do not measure that atm, let me implement that and get get back :)\n. StatsD into Graphite, cannot recommend it enough!\nhttps://github.com/etsy/statsd\n. I see, yes it is a manual instrumentation from the places in the code base that sets up a connection, nothing fancy.\nIt looks like most, if not all, connections are made with BlockingConnection after all. Can we do anything about its connection time, or should we refactor those parts to use SelectConnection instead?\n. I have not :(\nMoving to SelectConnection is the way to go I guess, but its a somewhat large codebase to go through and update if we have to do that.\nWe use CGI and a new process is spun up for each request, so no easy way to share a connection across requests, so as of now we slow down the requests that sends rabbit msgs.\n. Very cool! I will try it first thing tomorrow\n. The fix lowered our time to connect, get channel, send msg and close connection again to 100 ms (from 350). Great work and thank you vary much!\n. lol fixed typo with a typo... go ahead and deny this pull req\n. Don't know what this is worth but for me this works well enough\npython\nlogging.getLogger('pika').setLevel(logging.INFO)\nI thought the dots in the logger name constructed a hierarchy so that you could actually do something like\npython\nlogging.getLogger('pika').setLevel(logging.INFO)\nlogging.getLogger('pika.frame').setLevel(logging.DEBUG)\nif you wanted INFO level from pika in general but DEBUG level logging from the sub-package frame (if there is such a package, I haven't checked)\n. ",
    "simon-liu": "hi, rikhul. how did you fix this problem.\n. ",
    "brettlangdon": "@gmr any idea when 0.9.9 will be available?\n. That is awesome, thanks!\n. ",
    "dcrosta": "Hi @gmr -- any idea when 0.9.9 will be ready? Can we help at all with testing or preparing the release?\n. ",
    "base698": "when I try to set heartbeat on SelectConnection I get:  TypeError: init() got an unexpected keyword argument 'heartbeat_interval'\n. I see nothing referencing heartbeat in the sourcecode for selectconnection either...\n. I think this is what I'm seeing in this question:  http://stackoverflow.com/questions/33739147/never-ending-message-loop-same-message-redelivered-in-python-rabbitmq-consumer   Any work arounds?\n. @vitaly-krugl  I can't see anything on that page you posted about heartbeats in python.  I'm using SelectConnection and can't find anything related to timeouts.  Printing dir(_connection) shows something called heartbeat but it's not a method.  I see references online about heatbeat and heartbeat_interval neither are params allowed for select connection.\n. ",
    "hoverzheng": "Think you !\nI solved it, I closed the connect twice, and modified it.\n. Thank you @vitaly-krugl ! \nI try traceback firstly!\n. Thank you ,I used threads. but used two connection, every thread had its connection.\nI found it blocking connection.close() function.\nI solve it using: remove connection.close() function, And When exception happend, I close the connection. \n. ",
    "bdeeney": "I believe I'm running into the same issue while using the SelectConnection adapter as shown in asynchronous_consumer_example.  Traceback: https://gist.github.com/4503584\n. Nevermind...I see this is fixed in master by 1799798d92cb1324864f6e29292922abcd0e3ab2.\n. ",
    "samrudh": "In general, what is the callback for sender to know that exchange is deleted?\nBy callback, I mean asynchronous callback, and not some flag to be checked before publishing.\n. ",
    "briancline": "Sure thing. Thanks for the quick fix and for squeezing it into 0.9.9! Will give it a go tomorrow morning.\n. ",
    "atatsu": "It only means you haven't configured pika's loggers. It isn't hiding errors from you, though. It's merely informing you that there's log output you aren't going to see until the logger is configured. If there were an exception happening you'd still see it raised. \nIf you wanted to see all of pika's log output you could add this to the top of your file:\nimport logging\nlogging.getLogger('pika').setLevel(logging.DEBUG)\nNote: there's a TON of output if you having logging set to DEBUG\nLong story short, you can ignore that message if you want to.\n. How are you using your channels? Are you using the same channel to do everything? As in declaring queues/exchanges, setting up bindings, consuming queues? \n. From my experience you can save yourself a lot of headaches if you use a new channel for each new operation you wish to perform. The number of channel's you can have open at one time is really only limited by the resources of the box running RabbitMQ. We have thousands of active channels and the box we're running Rabbit on still has plenty to give. \nThere are a number of reasons to not use the same channel for multiple operations (i.e. queue_declare, exchange_declare, basic_consume) IMO. Say for example you've instructed a channel to consume ten different queues. Now pretend there's a channel error, you tried declaring a queue that already existed with different properties. RabbitMQ will close the channel. Your ten consumers are no longer consuming. You'll have to handle that. Setup all ten consumers again, assuming you know which ten were impacted. \nAnother problem you run into when reusing the same channel is RabbitMQ expecting a certain response from the channel and your code trying to do something else too soon resulting in an unexpected response on your end which can lead to RabbitMQ closing your connection (which I'm guessing is the problem you're experiencing).\nIn order to reuse the same channel you have to setup callbacks to know when Rabbit is finished doing its thing. So if you want to call channel.queue_declare five times consecutively you need to call it once, and then in the callback you can call it again. Rinse and repeat three more times.\nPersonally I find it easier to just use a separate channel for everything and then just close it when it isn't needed anymore. Does this make sense?\n. Ha. Yeah I was writing my notes and saw your commit. No worries. :)\n. What version of pika are you using?\n. The fix for this made it in right after 0.9.9 was released\n. I have a fix for this and will get it in tomorrow night hopefully\n. Can this be closed?\n. ",
    "vgoklani": "Yes, I am using the default channel for everything. (presumably this is bad)\nMy base code is here: https://gist.github.com/4669710\nI followed the RabbitMQ tutorial. Could you please point me to a better example. \nMy callback function is passed in, and the messages are a list of JSON objects that are produced by the producer and consumed by the consumer.\nThanks!\nOn Jan 29, 2013, at 8:07 PM, Nathan Lundquist notifications@github.com wrote:\n\nHow are you using your channels? Are you using the same channel to do everything? As in declaring queues/exchanges, setting up bindings, consuming queues?\n\u2014\nReply to this email directly or view it on GitHub.\n. There is no full traceback from pika, I only see these messages:\n\nERROR:pika.adapters.base_connection:Socket Error on fd 4: 104 \nAre you referring to the RabbitMQ logs?\nOn Jan 29, 2013, at 8:07 PM, Nathan Lundquist notifications@github.com wrote:\n\nHow are you using your channels? Are you using the same channel to do everything? As in declaring queues/exchanges, setting up bindings, consuming queues?\n\u2014\nReply to this email directly or view it on GitHub.\n. I created a try / catch block:\n\ntry:\n    queue.publish(messages)\nexcept Exception as e:\n    print e.message\nand got this:  'ConnectionClosed' object has no attribute 'messages'. I don't see any other traceback.\nSo it's throwing a ConnectionClosed exception.  The solution would be to catch the exception and just open another connection. The bigger question is why is the connection closing, and is the correct solution to simply open another connection (?)\nThis is a rough sketch of my code:\n1. open connection to queue\n2. call producer and put messages into queue\n3. sleep for 20min\n4. goto 2\nBut it seems now that the connection is closing after step 3.\nIs it possible that the 20min delay is closing the connection? \nThanks for all the help!\nBest,\nVishal\nOn Jan 29, 2013, at 8:33 PM, Gavin M. Roy notifications@github.com wrote:\n\nNo, I am referring to the python traceback when the exception is raised in your application. That error tells us the problem but not where it is occurring or why. The Python traceback when the app breaks is more useful. \nOn Tuesday, January 29, 2013 at 8:22 PM, Vishal Goklani wrote: \n\nThere is no full traceback from pika, I only see these messages: \nERROR:pika.adapters.base_connection:Socket Error on fd 4: 104 \nAre you referring to the RabbitMQ logs? \nOn Jan 29, 2013, at 8:07 PM, Nathan Lundquist  wrote: \n\nHow are you using your channels? Are you using the same channel to do everything? As in declaring queues/exchanges, setting up bindings, consuming queues? \n\u2014 \nReply to this email directly or view it on GitHub. \n\n\u2014 \nReply to this email directly or view it on GitHub (https://github.com/pika/pika/issues/266#issuecomment-12868900). \n\u2014\nReply to this email directly or view it on GitHub.\n. is it normal to get messages like this (from the RabbitMQ log):\n\n\n=WARNING REPORT==== 30-Jan-2013::03:52:16 ===\nclosing AMQP connection <0.466.0> (10.169.2.187:35609 -> 10.169.2.187:5672):\nconnection_closed_abruptly\n=INFO REPORT==== 30-Jan-2013::03:52:21 ===\naccepting AMQP connection <0.515.0> (10.169.2.187:35624 -> 10.169.2.187:5672)\n=INFO REPORT==== 30-Jan-2013::03:52:38 ===\naccepting AMQP connection <0.527.0> (10.169.2.187:35630 -> 10.169.2.187:5672)\n=ERROR REPORT==== 30-Jan-2013::04:32:21 ===\nclosing AMQP connection <0.515.0> (10.169.2.187:35624 -> 10.169.2.187:5672):\n{heartbeat_timeout,running}\n=INFO REPORT==== 30-Jan-2013::04:34:32 ===\naccepting AMQP connection <0.802.0> (10.169.2.187:37799 -> 10.169.2.187:5672)\nHow would I debug this further?\nOn Jan 29, 2013, at 10:34 PM, Gavin M. Roy notifications@github.com wrote:\n\nIf you're not getting the error message from: \ntry: \nfoo do whatever here\ncatch ConnectionClosed as error: \nprint 'Connection was closed due to: %s' % error \nThen the RabbitMQ logs are the next place to look. \nOn Tue, Jan 29, 2013 at 10:28 PM, Vishal Goklani \nnotifications@github.comwrote: \n\nI created a try / catch block: \ntry: \nqueue.publish(messages) \nexcept Exception as e: \nprint e.message \nand got this: 'ConnectionClosed' object has no attribute 'messages'. I \ndon't see any other traceback. \nSo it's throwing a ConnectionClosed exception. The solution would be to \ncatch the exception and just open another connection. The bigger question \nis why is the connection closing, and is the correct solution to simply \nopen another connection (?) \nThis is a rough sketch of my code: \n1. open connection to queue \n2. call producer and put messages into queue \n3. sleep for 20min \n4. goto 2 \nBut it seems now that the connection is closing after step 3. \nIs it possible that the 20min delay is closing the connection? \nThanks for all the help! \nBest, \nVishal \nOn Jan 29, 2013, at 8:33 PM, Gavin M. Roy notifications@github.com \nwrote: \n\nNo, I am referring to the python traceback when the exception is raised \nin your application. That error tells us the problem but not where it is \noccurring or why. The Python traceback when the app breaks is more useful. \nOn Tuesday, January 29, 2013 at 8:22 PM, Vishal Goklani wrote: \n\nThere is no full traceback from pika, I only see these messages: \nERROR:pika.adapters.base_connection:Socket Error on fd 4: 104 \nAre you referring to the RabbitMQ logs? \nOn Jan 29, 2013, at 8:07 PM, Nathan Lundquist < \nnotifications@github.com (mailto:notifications@github.com)> wrote: \n\nHow are you using your channels? Are you using the same channel to \ndo everything? As in declaring queues/exchanges, setting up bindings, \nconsuming queues? \n\u2014 \nReply to this email directly or view it on GitHub. \n\n\u2014 \nReply to this email directly or view it on GitHub ( \nhttps://github.com/pika/pika/issues/266#issuecomment-12868900). \n\u2014 \nReply to this email directly or view it on GitHub. \n\n\n\u2014 \nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/266#issuecomment-12872498. \n\n\nGavin M. Roy \nChief Technology Officer \nhttp://www.meetme.com/ \n100 Union Square Drive \nNew Hope, PA 18938 \np. +1.215.862.1162 x263 \nf. +1.215.862.0465 \nhttps://www.facebook.com/pages/MeetMe/21931227129 \nhttps://twitter.com/meetme \nhttp://www.youtube.com/user/MeetMeVideos \nThe public market leader in social discovery. (NYSE MKT: MEET)\n\u2014\nReply to this email directly or view it on GitHub.\n. yes, I am using time.sleep to take 30min breaks in-between requests - time.sleep(60 * 30).\n\nWhat's the proper way of pausing the producer/consumer, should I switch to connection.sleep(60*30)?\nThanks again for all the help!\nOn Jan 30, 2013, at 1:06 AM, Gavin M. Roy notifications@github.com wrote:\n\nIt appears that you are blocking in your consumer for longer than Pika has to respond to timeouts. The telling line is:\n=ERROR REPORT==== 30-Jan-2013::04:32:21 === \nclosing AMQP connection <0.515.0> (10.169.2.187:35624 -> 10.169.2.187:5672): \n{heartbeat_timeout,running} \nYou can either turn them off or make them much longer. In pika it's heartbeat_interval=0 to turn them off or heartbeat_interval={seconds} to set how many seconds you want them run. My guess is your consumer is blocking in Python processing for a fair amount of time if this is happening.\nAre you using time.sleep or any such thing?\n\u2014\nReply to this email directly or view it on GitHub.\n. I'm getting the same error, what's the time-frame for the 0.9.14 release? This is running in production, so I don't want to install a dev build.\n. \n",
    "ilfa": "We use BlockingConnection, when catch this problem.\nThere is code of connection:\ncredentials = pika.PlainCredentials(user, password)\nparameters = pika.ConnectionParameters(credentials=credentials, host=host, virtual_host=vhost)\nconnection = pika.BlockingConnection(parameters)\nchannel = connection.channel()\nchannel.queue_declare(queue=queue_name, durable=True)\nchannel.basic_consume(callback, queue=queue_name)\nchannel.start_consuming()\n. ",
    "sooshie": "I'm also experiencing this same issue. I upgraded from 0.9.8 to 0.9.9 last night and noticed this same issue. I reproduced it both with the default Ubuntu 10.04 LTS Python 2.6.5 and a built 2.7.3 on the same system.\n. Patch fixed the issue for me. Thanks!\n. ",
    "rafaelmagu": "Any ETAs to getting this into a release?\n. When is this fix making it into a proper version? 0.9.13 does not have it and I had to manually patch the file on a number of servers.\n. Still seeing system load high when using 0.9.13 (which I've verified to have the above fix). Diagnosed on EC2 instances to it being caused by AWS stealing CPU time due to a process consuming too much of it (my pika-based worker).\nReverting to 0.9.8 fixed it.\n. @gmr Fair enough. I can try and put rabbitpy to work for me in my workers, but beaver (a logstash shipper) still relies on pika for the rabbitmq transport, so I'm stuck with it for the time being.\n. ",
    "mk270": "+1 on getting this into a release; it's not that easy to replace the PyPI version of pika with the git version\n. This is a stability bug affecting all the other processes on a machine, pika or otherwise; is it not possible to get out a point release that people using requirements.txt files can target, without waiting for the other features in 0.9.10?\n. ",
    "AndyNewlands": "I thought I'd included that (sorry - I didn't).\nThe version is 0.9.9\n. ",
    "ChrisPappalardo": "routing_key is an optional argument.\nIt appears that this issue is caused by two different default values for routing_key in the bind vs. the unbind methods.  In the bind, routing_key defaults to None, while in the unbind, routing_key defaults to ''.  When binding the queue with a routing_key value of '', ch.queue_unbind(queue='fooQ', exchange='fooEx') works.\n. ",
    "patcpsc": "Tracked it down to patch (74929841).  Reverting this patch \"fixes\" pika in the sense of making this script work.\n. Let m have a look today ...\n. yeah sorry it was fixed a couple of patches back, forgot to update the comment\n. ",
    "amyxzhang": "I am having this problem still with the latest master installed. I just have a basic consumer looking much like the rabbitmq tutorials and I have ~19 million items in my queue. Soon after I start my script, the following gets called recursively:\nch.basic_ack(delivery_tag = method.delivery_tag)\nFile \"build/bdist.linux-x86_64/egg/pika/channel.py\", line 146, in basic_ack\nFile \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 1050, in _send_method\nFile \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 227, in send_method\nFile \"build/bdist.linux-x86_64/egg/pika/connection.py\", line 1433, in _send_method\nFile \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 370, in _send_frame\nFile \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 193, in process_data_events\nFile \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 306, in _handle_read\nFile \"build/bdist.linux-x86_64/egg/pika/adapters/base_connection.py\", line 336, in _handle_read\nFile \"build/bdist.linux-x86_64/egg/pika/connection.py\", line 1215, in _on_data_available\nFile \"build/bdist.linux-x86_64/egg/pika/connection.py\", line 1295, in _process_frame\nFile \"build/bdist.linux-x86_64/egg/pika/connection.py\", line 908, in _deliver_frame_to_channel\nFile \"build/bdist.linux-x86_64/egg/pika/channel.py\", line 790, in _handle_content_frame\nFile \"build/bdist.linux-x86_64/egg/pika/channel.py\", line 885, in _on_deliver\nFile \"get_archive_image_consumer.py\", line 34, in callback\nbefore I get a maximum recursion depth exceeded error. With the latest master, this happens quite soon after starting my script. In 0.9.12, the script would run longer - an hour or more before erroring.\n. ",
    "rca": "I'm also running into this with version 0.9.13 on Ubuntu 13.04.\nI'm not sure if this is a red herring or not, but I have been able to reproduce this processing a queue with 30k messages; it usually fails after processing 659 [1].\nBelow is the pika-related logic in the program.\nThanks!\n[1] The data below is attempting to process a queue containing ~30,000 messages.  Each number here represents running a python script and failing after the given number of messages.  The message count was calculated by running sudo rabbitmqctl report and taking the difference of len before and after the program ran:\n668 659 659 659 659 659 659 659 659 677 659 573 659 659 659 659 659 582 659 659 676 659 659 676 659 659 659 573 659 659 659 671 676 668 659 659 659 659 659 4943\n``` python\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\nchannel = connection.channel()\nDeclare the queue\nchannel.queue_declare(queue=\"foo\", durable=True, exclusive=False, auto_delete=False)\nchannel.queue_bind(exchange='amq.fanout', queue='foo')\nMethod that will receive our messages and stop consuming after 10\ndef _on_message(channel, method, header, body):\n    body_split = body.split()\n[...]\n\n# Acknowledge message receipt\nchannel.basic_ack(method.delivery_tag)\n\nThis is blocking until channel.stop_consuming is called and will allow us to receive messages\ntry:\n    # Setup up our consumer callback\n    channel.basic_consume(_on_message, queue=\"foo\")\nchannel.start_consuming()\n\nexcept KeyboardInterrupt:\n    channel.stop_consuming()\n```\n. ",
    "linbo": "I saw this issue again\n1. CentOS release 6.3 (Final)\n2. Python 2.6.5\n3. Pika 0.9.13\n. After this issue happen, I restart consumer script, but script won't consumer data from rabbitmq any more.\n. @eandersson\nYes, I close it.\n``` python\ndef close():\n    close_mq()\n    sys.exit(1)\ndef consume(callback, queue_name):\n    ensure_channel()\n    try:\n        channel.basic_consume(callback,\n                              queue=queue_name)\n        channel.start_consuming()\n    except Exception, e:\n        rb_logger.exception(\"Consumer rabbmitmq erro %s\" % str(e))\n        close()\n```\nAnd I meet an strange issue, only 2 or 3 consumer script works fine, other consumer scrpit is running, but can't get data from rabbitmq\n. Figure out the issue, I have two node for mirror queue, but one node has bad IO performance, that's why mirror queue hit the issue. After change bad IO node, the issue solved.  \nDetails refer to maillist discussion\nClose thie issue\n. ",
    "liangrubo": "I also saw this issue.\nCode to reproduce the problem(assume there are enough messages in queue 'hello'):\n``` python\nimport pika\nconnection = pika.BlockingConnection(pika.ConnectionParameters(host='xxx'))\nchannel = connection.channel()\nchannel.queue_declare(queue='hello')\ndef c(ch, method, properties, body):\n    ch.basic_ack(delivery_tag=method.delivery_tag)\nchannel.basic_consume(c,\n                      queue='hello',\n                      no_ack=False)\nchannel.start_consuming()\n```\nI got the \"maximum recursion depth exceeded\" error after consuming about 660 messages with current master while I got the same error after consuming about 66250 messages with version 0.9.8\n. ",
    "eandersson": "@linbo Are the messages stuck as unacked? Have you tried catching the exit signal and closing the connection connection.close()?\nIf you exit your script using Ctrl+C, you could catch KeyboardInterrupt and close the connection.\ntry:\n    ......\n    channel.start_consuming()\nexcept KeyboardInterrupt:\n    connection.close()\n. I am testing out a couple of changes on my own branch of pika that I think at least partly fixed the recursion issue. I renamed it pypika, but this branch is only really intended for personal use/testing.\nhttps://github.com/eandersson/pypika\nHowever, if you guys can help confirm that the patches works I'll come up with a proper solution and submit them to the pika repo. At least until gmr can finish his re-factoring of the BlockingConnection.\n. @moeffju \nhttps://github.com/eandersson/pika/commit/f73c1410964c836e78df1d7ab463087fa63df575\nReally though this would only be a work-around, and not sure if it works without the many changes I have done on my other branch.\n. Let me know if you guys are still experiencing this issue after #440.\n. I am glad the fix worked.\nGmr is skipping 0.9.14 and going straight for 0.10.0. Last I heard it was about a month away.\n. @moeffju Isn't that patch for the select_connection only? It looks to me like he is using 0.9.13 and the Blocking Connection.\n. This is fixed in the upcoming 0.9.14 update. If you are still running 0.9.13, you will have to wait for the next update, or download the master branch and use that.\n. I don't think there is an ETA unfortunately, but hopefully soon.\n. I cleaned up the code, and added another minor fix causing logging spam in one of my applications.\n. I think this goes beyond the missing size attribute, or the behavior is different in 0.9.13. Even after adding a new implementation for size, I was still unable to get back-pressure working as excepted. \n. Have you tried running this against the latest master?\n. The master is stable afaik. Been testing it for a week or two now.\n. @Daniel you could also try rabbitpy or amqp-storm. Both should be rocksolid and are still maintained.\n. @thomdixon Are you running one pika connection per thread?\n. This was fixed with pull request #340\n. This is a known issue with 0.9.13. This was fixed with pull request #340 for the upcoming 0.9.14.\n. Not sure how to reproduce this in a testable manner, but might be worth double checking the getaddrinfo related changes.\n. IPv6 should is disabled, and nothing out of the ordinary.\nYea, I have port=5672 set in the ConnectionParameters. It is odd as I have multiple connections running with identical parameters at any given time, and I have only seen it happen to one of those connections, never multiple at the same time.\n. I am now able to reproduce it on my VM running CentOS 6.4. Here is the stacktrace.\n*** print_exception:\nTraceback (most recent call last):\n  File \"test.py\", line 37, in process_task\n    r.connect(str('localhost'))\n  File \"/home/eandersson/test_project/lib/connectors/rabbitmqbase.py\", line 22, in connect\n    host=host, port=5672))\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 107, in __init__\n    super(BlockingConnection, self).__init__(parameters, None, False)\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/base_connection.py\", line 62, in __init__\n    on_close_callback)\n  File \"build/bdist.linux-x86_64/egg/pika/connection.py\", line 590, in __init__\n    self.connect()\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 206, in connect\n    if not self._adapter_connect():\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 274, in _adapter_connect\n    if not super(BlockingConnection, self)._adapter_connect():\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/base_connection.py\", line 105, in _adapter_connect\n    0, 0, socket.getprotobyname(\"tcp\"))\ngaierror: [Errno -8] Servname not supported for ai_socktype\n*** format_exception:\n['Traceback (most recent call last):\\n', '  File \"test.py\", line 37, in process_task\\n    r.connect(str(\\'localhost\\'))\\n', '  File \"/home/eandersson/test_project/lib/connectors/rabbitmqbase.py\", line 22, in connect\\n    host=host, port=5672))\\n', '  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 107, in __init__\\n    super(BlockingConnection, self).__init__(parameters, None, False)\\n', '  File \"build/bdist.linux-x86_64/egg/pika/adapters/base_connection.py\", line 62, in __init__\\n    on_close_callback)\\n', '  File \"build/bdist.linux-x86_64/egg/pika/connection.py\", line 590, in __init__\\n    self.connect()\\n', '  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 206, in connect\\n    if not self._adapter_connect():\\n', '  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 274, in _adapter_connect\\n    if not super(BlockingConnection, self)._adapter_connect():\\n', '  File \"build/bdist.linux-x86_64/egg/pika/adapters/base_connection.py\", line 105, in _adapter_connect\\n    0, 0, socket.getprotobyname(\"tcp\"))\\n', 'gaierror: [Errno -8] Servname not supported for ai_socktype\\n']\nAll I do to reproduce this is open and close a large amount of connections. Eventually I will run into this exception on my CentOS boxes. \n. 1. I double checked and IPv6 is enabled on my VM.\n2. \n127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4\n::1         localhost localhost.localdomain localhost6 localhost6.localdomain6\nIt is indeed not a major issue, since I can simply catch it and open a new connection, but I have reproduced this on multiple servers. The real odd thing is that it isn't happening on earlier versions.\nIt does also not make any difference if I use localhost, 127.0.0.1 or the actual IP address of the blade.\n. It does indeed look like removing getprotobyname resolved this.\n. In the current master it looks like we get stuck in an endless loop, starting with _send_method from blocking_connection.py\nwhile wait and not self._received_response:\n    try:\n        self.connection.process_data_events()\n    except exceptions.AMQPConnectionError:\n        break\n. You need to add channel.confirm_delivery() when setting up the channel. \n. That might be something worth looking into as I would also expect a proper Exception to be sent. I'll see what I can find out, but Gavin should be able to provide us with a better insight into this.\n. I'll submit a fix in a couple of minutes.\nFalse\nsend 1 messages\nFalse\nsend 2 messages\nFalse\nsend 3 messages\nTraceback (most recent call last):\n  File \"/home/eandersson/repo/x/t2.py\", line 25, in <module>\n    connection.close()\n  File \"/usr/local/lib/python2.7/dist-packages/pika-0.9.13-py2.7.egg/pika/adapters/blocking_connection.py\", line 197, in close\n    self.process_data_events()\n  File \"/usr/local/lib/python2.7/dist-packages/pika-0.9.13-py2.7.egg/pika/adapters/blocking_connection.py\", line 221, in process_data_events\n    raise exceptions.ConnectionClosed()\npika.exceptions.ConnectionClosed\n. This looks like a simple unable to connect to RabbitMQ error. Are you sure that RabbitMQ is running on your localhost?\nsudo /etc/init.d/rabbitmq-server start\nAs for the logging error, you should be able to resolve that by simply adding\nimport logging\nlogging.basicConfig(format='%(levelname)s:%(message)s', level=logging.CRITICAL)```\n. There is most likely an issue with your RabbitMQ configuration.\n. Take a look at the management module.\nhttp://localhost:55672/#/queues\nYou may need to enable it manually -- http://www.rabbitmq.com/management.html\nUse the management plug-in to manually create a queue, and then a message to see if it works as intended.\n. It has been in the works for a long time, there is unfortunately no easy way to convert an application that needs to keep backwards compatibility with Python 2.6+. The latest version of Pika introduced a lot of issues, and we have to prioritize on fixing those first.\ntl;dr It is a lot of work, so when it's done\u2122 no time estimate available yet\n. I managed to fix this on Jython by using the inbuilt compatible_select function.\nfrom select import cpython_compatible_select as select\nAnd by catching the error with socket.SQL_TCP and replacing it with the equivalent for Jython.\ntry:\n    SOL_TCP = socket.SOL_TCP\nexcept AttributeError:\n    SOL_TCP = 6\n@alswl\nTry this build out\nhttps://github.com/eandersson/pika/commit/256ed3d0b9bc95e751a3236222371c6ac10a091d\n. There is unfortunately still an issue with Jython related to this bug http://bugs.jython.org/issue1949\nI am looking into a couple of solutions, but not sure which is better. We could simply using dequeue(list()) for Jython by catching the exception, or manually checking the length of the queue in code.\nThe first option could cause unexpected behavior under Jython, will the second one will make the code more difficult to maintain. \n. This was fixed with pull request #346.\n. Unless you have a specific reasons to run 2.8.6 I would recommend that you update to 3.X. Technically 2.8.3 should work fine with Pika, but there has been many bug fixes released since 2.8.6.\nIn addition you also have the new monitoring features in 3.1.0 which are amazing. ;)\n. This is indeed fixed in 0.9.14 (Master). It should hopefully be released in a few weeks.\n. @gmr I am pretty sure this only happens when the socket was unexpectedly closed. I have some code somewhere that reproduces this issue.\nFound it.\nimport pika\nconnection = pika.BlockingConnection()\nchannel = connection.channel()\nconnection.socket.close()\nchannel.basic_publish('', 'test', 'hello_world')\nconnection.close()\nTraceback\nTraceback (most recent call last):\n  File \"C:/Users/eandersson/PycharmProjects/untitled1/world.py\", line 8, in <module>\n    channel.basic_publish('', 'test', 'hello_world')\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 521, in basic_publish\n    (properties, body))\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 1104, in _rpc\n    self._wait_on_response(method_frame))\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 1121, in _send_method\n    self.connection.send_method(self.channel_number, method_frame, content)\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 249, in send_method\n    self._send_method(channel_number, method_frame, content)\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\connection.py\", line 1489, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 388, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\connection.py\", line 1476, in _send_frame\n    self._flush_outbound()\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 348, in _flush_outbound\n    if self._handle_write():\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\adapters\\base_connection.py\", line 338, in _handle_write\n    return self._handle_error(error)\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\adapters\\base_connection.py\", line 282, in _handle_error\n    self._handle_disconnect()\n  File \"C:\\Python26\\Lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 318, in _handle_disconnect\n    self.disconnect()\nAttributeError: 'BlockingConnection' object has no attribute 'disconnect'\n. @gmr Testing on master as well now, but pretty sure it only happens on 0.9.13.\nUnless you are referring to the disconnection, and not the AttributeError.\nI do believe that the source of the error is like @vfilimonov mentioned caused by the socket closing unexpectedly. At least that has been my experience so far.\n. If you send a lot of messages you can keep a counter when publishing and periodically call process_data_events when you call publish.\nDepending on the rate you publish messages call process_data_events every 100 messages or so.\n\nYou could either use a counter, if you are consistently sending messages.\nmessage_sent_counter = 0\nwhile True:\n    channel.basic_publish('', 'test', 'hello_world')\n    message_sent_counter += 1\n    if message_sent_counter % 100 == True:\n        connection.process_data_events()\nOr you could use a timer if you send messages irregularly.\nlast_process_event_call = 0\nwhile True:\n    channel.basic_publish('', 'test', 'hello_world')\n    if (time.time() - last_process_event_call) > 6:\n        connection.process_data_events()\n        last_process_event_call = time.time()\n. Are you running 0.9.13 or Master?\n. Could you try out the latest master and see if you still experience the same issue?\n. A critical socket bug is the first thing that comes to mind. \n. This is caused by a bug in the socket code. This has already been fixed in this https://github.com/pika/pika/commit/abf9fa80d8a661e265a26438722d783c1f130ac0 commit for the upcoming release.\n. I agree that there has been too long, but the problem with an automatic build is that if there is a bug introduced, it would get re-distributed to a lot of people, and could cause a lot of problem. \nKeep in mind that there is nothing stopping you from downloading the master and using that instead of pip, or creating a quick RPM and re-building it whenever an important fix is released. It is not as easy as installing it with pip, but it involves a lot less risk than an automated build system.\n. I agree with you completely @tiagoboldt, but depending on the application it isn't always as simple as it seems.\nA great example is the socket bug introduced in 0.9.13, this bug is only triggered when you try to send large messages over VLANs with a specific configuration. I know that gmr tested this out before releasing 0.9.13, but as this requires a very specific scenario, and it is very difficult to test every conceivable scenario. \nIn addition it would require a lot more time to properly review all the pull requests, improve the unit-test coverage, and finally set up a proper testing platform that can cover everything. The question is really if gmr has the time required to make such an commitment.\nEdit: \nThe worst thing with the bug I mentioned above is that when the engineers work on their application and test it on their development platform it will most likely work, but when they move it to production with federation, multiple regions etc they will run into issues. This is why these bugs are so dangerous, and makes production applications very difficult to troubleshoot. \n. In additional process_data_events in _send_frame seems to be the cause of the recursion issue.\nI added something similar to this in my own branch of pika and since I have not been able to reproduce the recursion issue.\nif self._frames_written_without_read >= self.WRITE_TO_READ_RATIO:\n    if not isinstance(frame_value, frame.Method):\n        self._frames_written_without_read = 0\n        self.process_data_events()\n. Thanks. I made a new pika repo and made my modifications there. I'll use that until 0.10.0 comes out for my projects that are relying on pika, instead of rabbitpy.\n. I might be wrong here, but adding confirm_delivery normally prevents multiple messages from being sent without an exception being thrown on a bad connection.\nchannel.confirm_delivery()\n. I can't remember exactly how it is implemented, but I the key is the rpc request that needs to be sent to confirm that the delivery was made successfully. Without that confirmation it will not publish the next message.\nThat is also why enabling the confirm_delivery flag has a big impact on the number of messages per second you can publish.\nIn addition confirm_delivery will return True or False depending if the message was delivered successfully to RabbitMQ. This can be helpful to catch scenarios where RabbitMQ is experiencing issues.\n. I don't know exactly how heartbeat behaves, but I think the problem here is that you are not calling process_data_events. As the IO handling is not handled Async, it wont handle the heartbeats properly.\nTry calling con.process_data_events() every 5 seconds or so.\n. You can create your own IO thread that calls process_data_events() ever so often, but it is important that you add a lock. If you try to publish, at the same time as you are running process_data_events you will run into problems.\n```\ninternal_lock = threading.Lock()\nwith self.internal_lock:\n    self.channel.process_data_events()\nwith self.internal_lock:\n    self.channel.basic_publish(..........)\n```\n. That shouldn't be much trouble. As long as you keep a lock between each thread. I simply have one pika connection per wsgi process. Then keep a lock on only the Pika calls. In fact I am running something like mentioned above on my flask application.\n. My flask application is using only Rpc calls. I have a IO thread that checks for responses. I can easily host 200 threads per process and still see decent performance. I only send and receive messages in the 1 to 100kb range, but never had a problem.\nIt might be worth to keep a small delay (sleep) on your process_data_events background loop as it will be fighting for the lock if you have a lot of threads publishing. \nBasically the call is not completely blocking. It will block only while there are messages to consume.  That is why you need to loop it when consuming.\n. @joekarl I uploaded a simple example on how to do rpc request with Flask. https://github.com/eandersson/python-rabbitmq-examples/blob/master/Flask-examples/pika_async_rpc_example.py\nFeel free to test it out. I use this type of implementation myself on some pretty hefty rpc calls.\n. I ran extensive tests over multiple days using multiple RabbitMQ servers to test the recursive fix and not had a single issue.\nThe only issue encountered was towards my server in Tokyo, but that was due to an insufficient socket timeout. I experienced the same problem in the current master. So not related to these fixes.\nIn fact I would recommend that we increase the default timeout.\n. Looks good thanks.\n. That is a big addition for such small change. \n. I think that would make more sense. If @gmr has time I am sure he will try to implement a fix for 0.10.0.\n. Gmr is the decision maker here. I am mostly here to support. \nYou can reset your fork of pika and manually reapply the intended changes. That should allow you to turn this into a nice and clean pull request.\n. You can add channel.confirm_delivery(). This way pika will confirm that the message had been published before moving on.\n. It's already been released.\n. You are not actually applying the parameters to the connection.\nconn=pika.BlockingConnection(parameters=para)\n. > i have the same problem.\n\nlog:\nTraceback (most recent call last):\nFile \"manual_payment_processing.py\", line 70, in send_pay\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\nFile \"/usr/local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 130, in init\nsuper(BlockingConnection, self).init(parameters, None, False)\nFile \"/usr/local/lib/python3.6/site-packages/pika/adapters/base_connection.py\", line 72, in init\non_close_callback)\nFile \"/usr/local/lib/python3.6/site-packages/pika/connection.py\", line 596, in init\nself.connect()\nFile \"/usr/local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 230, in connect\nerror = self._adapter_connect()\nFile \"/usr/local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 301, in _adapter_connect\nraise exceptions.AMQPConnectionError(error)\npika.exceptions.AMQPConnectionError: Connection to ::1:5672 failed: [Errno 99] Address not available\ncode:\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\ncan some one help me?\n\nTry changing localhost to 127.0.0.1, maybe RabbitMQ isn't listening to your ipv6 loopback?. This is mostly likely indicating that the socket was closed due to the connection being closed forcefully.\nDoes the RabbitMQ logs say anything?\n. Is this with the latest RabbitMQ version (3.6.2?)\n. I am seeing this in other AMQP libraries as well.\n. ",
    "duylong": "Hi,\nI have a similar problem. I use pika 0.9.13 with RabbitMQ 3.1.3.\nMy code :\n``` python\nimport pika\nimport sys\ncredentials = pika.PlainCredentials(\"test\", \"test\")\nparameters = pika.ConnectionParameters(\n        host = \"localhost\",\n        port = 5671,\n        virtual_host = \"/test\",\n        credentials = credentials,\n        ssl = True\n)\nconnection = pika.BlockingConnection(parameters)\nchannel1 = connection.channel(1)\nchannel2 = connection.channel(2)\nchannel2.confirm_delivery()\ndef c(ch, method, properties, body):\n    channel2.basic_publish(\n            exchange = \"amq.direct\",\n            routing_key = \"route.test\",\n            body = body\n    )\n\n    ch.basic_ack(delivery_tag=method.delivery_tag)\n\ntry:\n        channel1.basic_consume(c,\n                      queue='queue.test',\n                      no_ack = False)\n    channel1.start_consuming()\n\nexcept KeyboardInterrupt:\n        connection.close()\n```\nError 1 : RuntimeError: maximum recursion depth exceeded while calling a Python object\nFile \"test.py\", line 27, in c\n    body = body\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 521, in basic_publish\n    (properties, body))\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 1104, in _rpc\n    self._wait_on_response(method_frame))\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 1124, in _send_method\n    self.connection.process_data_events()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 215, in process_data_events\n    if self._handle_read():\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 328, in _handle_read\n    super(BlockingConnection, self)._handle_read()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/base_connection.py\", line 326, in _handle_read\n    self._on_data_available(data)\n  File \"/usr/lib/python2.6/site-packages/pika/connection.py\", line 1271, in _on_data_available\n    self._process_frame(frame_value)\n  File \"/usr/lib/python2.6/site-packages/pika/connection.py\", line 1351, in _process_frame\n    self._deliver_frame_to_channel(frame_value)\n  File \"/usr/lib/python2.6/site-packages/pika/connection.py\", line 963, in _deliver_frame_to_channel\n    return self._channels[value.channel_number]._handle_content_frame(value)\n  File \"/usr/lib/python2.6/site-packages/pika/channel.py\", line 791, in _handle_content_frame\n    self._on_deliver(*response)\n  File \"/usr/lib/python2.6/site-packages/pika/channel.py\", line 886, in _on_deliver\n    body)\n  File \"test.py\", line 27, in c\n    body = body\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 521, in basic_publish\n    (properties, body))\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 1104, in _rpc\n    self._wait_on_response(method_frame))\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 1124, in _send_method\n    self.connection.process_data_events()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 215, in process_data_events\n    if self._handle_read():\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 328, in _handle_read\n    super(BlockingConnection, self)._handle_read()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/base_connection.py\", line 326, in _handle_read\n    self._on_data_available(data)\n  File \"/usr/lib/python2.6/site-packages/pika/connection.py\", line 1267, in _on_data_available\n    consumed_count, frame_value = self._read_frame()\n  File \"/usr/lib/python2.6/site-packages/pika/connection.py\", line 1359, in _read_frame\n    return frame.decode_frame(self._frame_buffer)\n  File \"/usr/lib/python2.6/site-packages/pika/frame.py\", line 243, in decode_frame\n    return frame_end, Method(channel_number, method)\n  File \"/usr/lib/python2.6/site-packages/pika/frame.py\", line 65, in __init__\n    Frame.__init__(self, spec.FRAME_METHOD, channel_number)\nRuntimeError: maximum recursion depth exceeded while calling a Python object\nError 2 : And when I don't have the last error. I get a other after some sent events: \nERROR:Read empty data, calling disconnect\nERROR:Read empty data, calling disconnect\nERROR:Read empty data, calling disconnect\nERROR:Read empty data, calling disconnect\nERROR:Read empty data, calling disconnect\nTraceback (most recent call last):\n  File \"test.py\", line 46, in <module>\n    queue='queue.test')\n  File \"/usr/lib/python2.6/site-packages/pika/channel.py\", line 220, in basic_consume\n    {'consumer_tag': consumer_tag})])\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 1106, in _rpc\n    self.connection.process_data_events()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 218, in process_data_events\n    raise exceptions.ConnectionClosed()\npika.exceptions.ConnectionClosed\nRabbitMQ log : \n{amqp_error,unexpected_frame,\n            \"expected content header for class 60, got non content header frame instead\",\n            'basic.publish'}\nThere are a problem in my example ?\n. Same problem since update to 0.9.14 with RabbitMQ 3.2.3: \nERROR: Socket Error on fd 5: 104\nWARNING: Socket closed when connection was open\nERROR: 'NoneType' object has no attribute 'sendall'\nI use \"blockingconnection\" adapter, \"consume\" and \"basic_publish\" methods without ack (consume) and confirm_delivery (publish). \nStrangely when I restart, I must use confirm_delivery to true, otherwise it does not work.\n. ",
    "ivandigiusto": "Happening for me as well.  I use pika 0.9.13 with RabbitMQ 3.1.5 on Ubuntu 12.04.  Queue connected to a fanout exchange, receiving messages with delivery_mode = 2.\nHeader frame:  \nMethod frame: \nDefinitely a problem with:\nRuntimeError: maximum recursion depth exceeded while calling a Python object\nHappens to me when there are over ~3,000 packets in the queue and I start consuming then with channel.start_consuming().\nFor example, if I start the consumer when there are 5,000 messages in the queue, I may be left with ~4,000 messages that are unacked.  The consumer will then keep consuming all the new packets coming in but old unacked packets will remain in the queue.  If I then stop the consumer, unacked messages go back to ready state.  If I start up the consumer now, I will end up with ~3,000 unacked messages in the queue.  The numbers are rough, but this is what ends up happening.\nThis did not happen with the old pika 0.9.5 which I was using for quite a while (that version had a different bug, but I digress).\nI don't have any additional info to submit than what previous commenters have submit already.\n. ",
    "dcbarans": "+1\nExperiencing this exact issue.\n- Queue with 27k items\n- Items no longer than 100 characters\n- only reading from queue\n- attempting to basic_ack and after a while python throws recursion error\n- little less than 1000 items get removed from the queue (it varies)\n. ",
    "agnivade": "+1 happening to me as well.\nQueue has over 1 lakh objects. Pika version 0.9.13.\n. Yes, please sooner the better ! I am currently using a workaround in production environment :)\n. Good find @susam ! I will try it out. Have you played with the prefetch_size param ? Any recommendations on that ?\n. Hmm this seems to be a nagging issue for long .. :(\n. Awesome, eagerly awaiting for the release.\n. ",
    "susam": "I am unable to use channel.basic_ack due to pika/pika#286 (this issue) and pika/pika#410 (consumer receiving messages from one queue only). I was able to reproduce this issue in the following two environments:\n1. pika 0.9.13, Python 2.7.3, RabbitMQ 3.1.5 and Windows 7 Enterprise Service Pack 1.\n2. pika 0.9.13, Python 2.6.6, RabbitMQ 3.0.1 and CentOS release 6.4.\nHere is the producer and consumer code that reproduces the issue.\nsend.py\n```\nimport pika\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\ndef print_data(channel, method, properties, body):\n    channel.basic_ack(delivery_tag = method.delivery_tag)\nparameters = pika.ConnectionParameters('localhost')\nconnection = pika.BlockingConnection(parameters)\nchannel = connection.channel()\nchannel.queue_declare(queue='q1')\nchannel.basic_consume(print_data, queue='q1')\nchannel.start_consuming()\nconnection.close()\n```\nrecv.py\n```\nimport pika\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nparameters = pika.ConnectionParameters('localhost')\nconnection = pika.BlockingConnection(parameters)\nchannel = connection.channel()\nDeclare the queue for the consumer\nchannel.queue_declare(queue='q1')\nPublish messages\nfor i in range(1, 20001):\n    if i % 5000 == 0:\n        print 'Published %d messages' % i\nmessage = 'message ' + str(i)\nchannel.basic_publish(exchange='', routing_key='q1', body=message)\n\nconnection.close()\n```\nRunning recv.py after send.py results in RuntimeError: maximum recursion depth exceeded.\n```\nC:\\pika\\recursion-issue>python send.py 2> send.txt\nPublished 5000 messages\nPublished 10000 messages\nPublished 15000 messages\nPublished 20000 messages\nC:\\pika\\recursion-issue>python recv.py 2> recv.txt\nC:\\pika\\recursion-issue>tail recv.txt\n    None, sys.stderr)\n  File \"C:\\Python27\\lib\\traceback.py\", line 125, in print_exception\n    print_tb(tb, limit, file)\n  File \"C:\\Python27\\lib\\traceback.py\", line 69, in print_tb\n    line = linecache.getline(filename, lineno, f.f_globals)\n  File \"C:\\Python27\\lib\\linecache.py\", line 14, in getline\n    lines = getlines(filename, module_globals)\n  File \"C:\\Python27\\lib\\linecache.py\", line 40, in getlines\n    return updatecache(filename, module_globals)\nRuntimeError: maximum recursion depth exceeded\n```\nSee https://gist.github.com/susam/7173596 for the complete consumer logs. This issue does not occur if I do not use channel.basic_ack(delivery_tag = method.delivery_tag).\n. Here is a workaround that works for me. I have tested this workaround with 100,000 messages and 1,000,000 messages in the queue. The workaround is to set a small prefetch_count.\nchannel.basic_qos(prefetch_count=4)\nSmaller prefetch_count values like 1, 2 and 3 work as well. I have not seen larger values like 11, 12, etc. to be working consistently. These larger values sometimes leads to RuntimeError: maximum recursion depth exceeded sometimes and not at other times.\nHere is a complete code that shows my recv.py code from the previous code with the workaround added.\n```\nimport pika\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\ndef print_data(channel, method, properties, body):\n    channel.basic_ack(delivery_tag = method.delivery_tag)\nparameters = pika.ConnectionParameters('localhost')\nconnection = pika.BlockingConnection(parameters)\nchannel = connection.channel()\nchannel.queue_declare(queue='q1')\nchannel.basic_qos(prefetch_count=4) # This is the workaround\nchannel.basic_consume(print_data, queue='q1')\nchannel.start_consuming()\nconnection.close()\n```\n. Some more information about this issue. This issue does not occur if the following statement is removed from recv.py provided in the previous comment.\nchannel.basic_ack(delivery_tag = method.delivery_tag)\nThis shows that the usage of the channel.basic_ack is related to this issue. However, removing channel.basic_ack without setting no_ack=True for the channel.basic_consume calls would mean that if the consumer script quits and starts again, all messages would be redelivered to it again and again, as can be seen below.\n```\nC:\\pika\\one-queue-issue>python send.py 2> out.txt\nPublished message 0 with routing keys q1 and q2\nPublished message 1 with routing keys q1 and q2\nPublished message 2 with routing keys q1 and q2\nC:\\pika\\one-queue-issue>python recv.py 2> out.txt\nbody: q1: message 0\nbody: q1: message 1\nbody: q1: message 2\nbody: q2: message 0\nbody: q2: message 1\nbody: q2: message 2\nC:\\pika\\one-queue-issue>python recv.py 2> out.txt\nbody: q1: message 0\nbody: q1: message 1\nbody: q1: message 2\nbody: q2: message 0\nbody: q2: message 1\nbody: q2: message 2\n```\nTherefore, now we need to modify the channel.basic_consume calls to use no_ack=True.\nchannel.basic_consume(print_data, queue='q1', no_ack=True)\nchannel.basic_consume(print_data, queue='q2', no_ack=True)\nNow, there is no re-delivery of messages and we are able to read from both queues.\n```\nC:\\pika\\one-queue-issue>python send.py 2> out.txt\nPublished message 0 with routing keys q1 and q2\nPublished message 1 with routing keys q1 and q2\nPublished message 2 with routing keys q1 and q2\nC:\\pika\\one-queue-issue>python recv.py 2> out.txt\nbody: q1: message 0\nbody: q1: message 1\nbody: q1: message 2\nbody: q2: message 0\nbody: q2: message 1\nbody: q2: message 2\nC:\\pika\\one-queue-issue>python recv.py 2> out.txt\n```\nHowever, we are losing the benefits of ack, i.e. the guarantee that any unacknowledged messages would be re-delivered.\n. ",
    "moeffju": "Just to say we\u2019re running into the same problem with 319 messages_ready. On basic_nack\u2019ing one, pika runs into an infinite recursion. Traceback: https://gist.github.com/moeffju/ae09de3f1d707bbf0f7c\nThe prefetch_count workaround does not make a difference for us.\n. @eandersson It would\u2019ve been useful if you forked the Github repository so we could do a simple cross-repo comparison of your changes. Can you extract a patch vs. pika master or something?\n. @YEXINGZHE54 Have you also tried the basic_ack callback method with the patch from @vermoudakias? While a workaround is good, fixing the underlying issue is better.\n. @eandersson @YEXINGZHE54 Whoops, sorry, I meant the fix from #440.\n. ",
    "andrej-peterka": "I am working on a project using RabbitMQ and was testing how Rabbit behaves with 1 million+ messages in a queue.\nI let the producer fill up the queue in a course of a day or so.\nToday I tried de-queueing them all one by one with a Python consumer, using pika and BlockingConnection.\nThe callback was nothing more than getting messages (a short JSON message, nothing fancy) and printing them to the console, then acking them.\nUsing pika version 0.9.13 from PyPI I got the error described in this issue within a couple of seconds after running the consumer.\nUsing master, everything works as expected! :smile: \nWhat is the release schedule for pika (a.k.a, when will the master be on PyPI)?\nNot really a pressing issue as I can use the master, but just curious.\n. ",
    "ejarendt": "Any update on when this fix might be released?\n. ",
    "CharlesLovely": "@ejarendt Sounds like we might not see a formal release to PyPi until 0.10:\nhttp://gavinroy.com/posts/pika-0-9-14-and-0-10-plams.html\nIn the meantime, you can install pika directly from this github repo to get the fix.\n. ",
    "YEXINGZHE54": "the same problem, in version 0.9.13\n. Hi, I found a solution:  Do not basic_consume data in the way illustrated by the officail site http://www.rabbitmq.com/tutorials/tutorial-two-python.html. Yes, I mean : do not ack in callbacker of basic_consume. When you call basic_ack in callbacker with blocking connection channel, it will lead to process read events (about 1 in ten times), which in turn calls your consume callbacker again, and then you call ack ack again ... this is the recursive cycle. \nBetter way is use channel.consume instead: \n                    for method, properties, body in channel.consume(queue_name):\n                        print body\n                        channel.basic_ack(delivery_tag=method.delivery_tag)\n. ",
    "khj1218": "I have the same issue. It seems like I'm having the error in case of the socket error (104, 'Connection reset by peer')\n. ",
    "markunsworth": "We're still seeing this in 0.9.12 and 0.9.13\n2013-05-17 14:45:26,737 default-worker-0     ERROR   'BlockingConnection' object has no attribute 'disconnect'\n.....\n..... \n.....\n  File \"/home/dotcloud/.env/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 540, in basic_publish\n    (properties, body), False)\n  File \"/home/dotcloud/.env/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 1121, in _send_method\n    self.connection.send_method(self.channel_number, method_frame, content)\n  File \"/home/dotcloud/.env/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 249, in send_method\n    self._send_method(channel_number, method_frame, content)\n  File \"/home/dotcloud/.env/local/lib/python2.7/site-packages/pika/connection.py\", line 1489, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"/home/dotcloud/.env/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 388, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\n  File \"/home/dotcloud/.env/local/lib/python2.7/site-packages/pika/connection.py\", line 1476, in _send_frame\n    self._flush_outbound()\n  File \"/home/dotcloud/.env/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 348, in _flush_outbound\n    if self._handle_write():\n  File \"/home/dotcloud/.env/local/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 338, in _handle_write\n    return self._handle_error(error)\n  File \"/home/dotcloud/.env/local/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 282, in _handle_error\n    self._handle_disconnect()\n  File \"/home/dotcloud/.env/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 318, in _handle_disconnect\n    self.disconnect()\nAttributeError: 'BlockingConnection' object has no attribute 'disconnect'\n. not the same issues actually, but this pull request should fix it.\n. The issue is that in the BlockingConnection class the  _adapter_disconnect method calls the overridden _check_state_on_disconnect method.\nIn this method a ConnectionClosed exception is raised if the connection is marked as open, which means the _init_connection_state method is never called (which recreates the socket).\n@jamutton Is it really necessary to raise the ConnectionClosed exception here? This was added in your commit 6c93b38f8369328a9b21d5d7f8c8224842091fb0\n. I think this is a useful option. I do agree, as @mffrench suggests, that the application is the client here, but it does make sense to hold environment properties (python version, pika version) alongside application specific properties.\nThe AMQP spec defines:\n\nThe properties SHOULD contain at least these fields: \"product\", giving the name of the client product, \"version\", giving the name of the client version, \"platform\", giving the name of the operating system, \"copyright\", if appropriate, and \"information\", giving other general information.\n\nWhat about defining a structure that meets the specification for client properties but that allows application defined settings as well as pika defined ones as well. \n{\n    \"product\": \"Pika\",\n    \"version\": \"0.10.0.b\",\n    \"platform\": \"Python 2.7.10\",\n    \"copyright\": \"\",\n    \"information\": {\n        \"pika_version\": \"0.10.0.b\",\n        \"python_version\": \"2.7.10\",\n        \"process_id\": 237,\n        \"hostname\": \"my-server.eu.aws, \n        ...                            \n    },\n}\nBy default we set the product, version and platform as we do currently and also set them in the information. If a client specifies the product, version or platform they should override the defaults. We then don't lose information, but it gives app developers flexibility\n\nWe've been setting the hostname by default on all connections, just added as #628 as it's helps with debugging as we run multiple instances of each service in our system.\n\n. We just wanted to know which connections were coming from which servers as the ip addresses shown in the name don't seem to match.\n. @vitaly-krugl yes, it's limited for now, maybe pull request #571 by @mffrench is a better place to do this, we just need to agree on an approach\n. I think this is/was down to the network topology. We recently moved from internally managed NAT servers to AWS Managed NAT gateways which would pretty much coincide with this problem starting. The network setup:\nservice (private subnet) -> nat gateway -> ELB -> rabbitmq cluster (public subnet)\nIn an attempt to resolve this I decided to try moving the rabbitmq cluster inside the private subnet so the setup is now:\nservice (private subnet) -> internal ELB -> rabbitmq cluster (private subnet)\nThis seems to have solved the issue we were seeing, but i'm now trying to investigate with AWS if the NAT Gateway was the root cause of this, and if so why.\nIn answer to your other questions:\n1) Ubuntu 14.04 (64 bit)\n3/4) I will create some gists with these too.\n. Yeah, we've been running RabbitMQ behind ELBs for a couple of years and dealt with that by increasing the ELB timeout to be greater than the heartbeat value set on rabbit (600s for ELB & 580s for rabbit)\n. Yes, I was aware of that. We deploy rabbit with chef so have configuration overrides on place.\nThe strangest thing for me was the fact it was over 15mins (almost always around 15m 30s) before the publish actually timed out as that's a value we can't find in any configuration\n. Is there a way to set a publish timeout on the pika client? So if there's no response from the broker we throw timeout/disconnect exception?\nIt would seem that behind the scene the system is retrying the send \n. Just an FYI here. I'm no longer at the company where this was an issue so won't be able to feedback on anything. @alvises may be able to help though.. This isn't the place for support requests. Please post them in the pika google group. You should direct usage questions at the google group, this is just for bugs.\nhttps://groups.google.com/forum/#!forum/pika-python. ",
    "texens": "I'm still getting the same error every once in a while. Unfortunately I don't have a stack trace right now. But the error message that I got is : \"pika.adapters.base_connection:Socket Error on fd 604: 10054\n(, AttributeError(\"'BlockingConnection' object has no attribute 'disconnect'\",), )\"\nAm I missing or overlooking something ?\npika version : 0.9.13\nTIA\n. ",
    "cenkalti": "Why don't you use Travis-CI? It's hosted and free for open source projects. It also supports RabbitMQ.\n. :+1: \n. When will 0.9.14 be released?\n. I can't see it. In which line is the syscall going to be retried?\n. I understand now. Thanks for the explanation.\n. I can't reproduce it easily. It is a race condition in my case.\n. ",
    "puentesarrin": "Using pika 0.9.9, when I add two parameters more(reply_code, reply_text); a exception's raised:\nERROR:root:Exception in I/O handler for fd 7\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 327, in start\n    self._handlers[fd](fd, events)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\", line 279, in _handle_events\n    self._handle_read()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\", line 299, in _handle_read\n    return self._handle_error(error)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\", line 263, in _handle_error\n    self._handle_disconnect()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\", line 213, in _handle_disconnect\n    self._on_connection_closed(None, True)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/connection.py\", line 1065, in _on_connection_closed\n    self._process_connection_closed_callbacks()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/connection.py\", line 1169, in _process_connection_closed_callbacks\n    self.callbacks.process(0, '_on_connection_closed', self, self)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/callback.py\", line 61, in wrapper\n    return function(*tuple(args), **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/callback.py\", line 92, in wrapper\n    return function(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/callback.py\", line 232, in process\n    callback(*args, **keywords)\nTypeError: _on_connection_closed() takes exactly 4 arguments (2 given)\n. Thanks a lot, I will try cloning the repository directly.\n. ",
    "tiadobatima": "Python 2.7.3\n. Awesome! Thx!\n. ",
    "chuckhacker": "If I hit CTRL+C while it is in this unresponsive state, I get the following traceback:\n^CTraceback (most recent call last):\n  File \"ffa_service_rabbit_mq.py\", line 79, in \n    main()\n  File \"ffa_service_rabbit_mq.py\", line 74, in main\n    channel.start_consuming()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 775, in start_consuming\n    self.connection.process_data_events()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 130, in process_data_events\n    if self._handle_read():\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 232, in _handle_read\n    if self._read_poller.ready():\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 54, in ready\n    events = self.poller.poll(self.poll_timeout)\nKeyboardInterrupt\n. Blocking heartbeats from processing makes sense that it would cause this problem. What confuses me is that the default heart beat interval in the connection parameters section of the docs is said to already be zero (source: https://pika.readthedocs.org/en/latest/connection.html). Nonetheless, I'm going to try running it with it explicitly set to zero, and I will write back with whether this solves the issue. On the other hand, I'd like to avoid setting it to an arbitrarily large number. That feels like a hack to me. If I switched to SelectConnection, would this allow my worker client to respond to heartbeats?\n. I explicitly set heartbeat interval to zero, ran my worker overnight, and it still stopped responding to messages by morning. I am using CloudAMQP as my message broker. Two things I will try are 1.) running my own local broker and 2.) switching to SelectConnection.\n. Setting heartbeat interval to zero wasn't enough. I had to switch to SelectConnection async and upgrade to latest RabbitMQ 3.0.3, then I stopped getting disconnected\n. ",
    "ntom": "I had exactly the same issue, again and again, I switched to using the async example form here, so far seems okay, will report back if no issues http://pika.readthedocs.io/en/0.10.0/examples/asynchronous_consumer_example.html . ",
    "therve": "The default behavior of assuming that the body is utf-8 seems wrong, though. Nowhere in the spec something like that is proposed, AFAIK. Could the current behavior be deprecated? At least, there should be a way to ask pika to always get strings. It doesn't seem really hard to have a custom ContentFrameDispatcher, but the _finish method being private makes it a bit nasty.\n. I'm relatively confused by your response. First the native string types are not meaningful for the body? Then, pika right now is using UTF-8, but you're saying making it UTF-8 would break things? The spec mentions it's a binary payload, so bytes, so with Python2 it's a str. It's not confusing. The problem is that pika sometimes give you unicode, sometimes str, depending on the fact that it managed to decode it as utf-8 or not.\n. Thanks, it's more clear indeed.\nPika doesn't really try to determine if the body type is UTF-8 though, it just tries to decode and hope for the best.\nPython 2 has a native byte type, it's str. I agree you can't change the default behavior, but I'd like to be able to configure it to always get str, because it's cleaner, and that's what pika is getting already after all.\n. It seems it has been broken in commit 61ecac445bf5a71a5f88381585b76246d50558a5\n. ",
    "soniaseguz": "Great to hear it's patched. \nCould force_binary also be exposed to connection.channel() method? As suggested in the docs, we don't create a channel directly, instead we call the connection\u2019s channel() method.\n. ",
    "sanjioh": "Hi, +1 for force_binary exposed to connection.channel() method.\n. ",
    "thomie": "Now the channels won't get closed in Connection._close_channels().\n. Sorry for bothering you again. This issue is still not properly addressed. The following test fails.\n```\nimport pika\nconnection = pika.BlockingConnection()\nchannel = connection.channel()\nchannel.close()\nassert not connection._has_open_channels\n```\n. For your information: the test from my last comment still doesn't pass.\n. I'm sorry, you are right. Please see issue #310.\n. ",
    "maitreyavvm": "Thanks Gavin. Just wanted to add that php-amqplib client could ask for 3600s of heatbeat_interval and the RMQ server 3.0.3 agreed to it. I am just guessing there is a change in the handshaking protocol for heartbeat with RMQ 3.0. Can you suggest any temporary workaround for this behavior?\n. I think the heartbeat interval should be flexible enough for different type of workers. For example, we have a combination of long-running tasks and short-running tasks. The short running tasks range from 1 to 10 seconds. The long running tasks can take anytime depending on the input video size. So, for long running tasks, if the heartbeat interval set is 10 minutes, the client will not respond to the server's heartbeat for a long time which will result in the server disconnecting the client. Now the problem for disabling the heartbeat is the network infrastructure (like EC2) will kill any connections which are idle for more than about a day(dont know the exact time). So if the heartbeat is disabled then the connections are gone for good. An application can have a variety of load patterns, mine being short bursts with long idle time. So I think the heartbeat should be flexible enough to support a variety of consumers and different network infrastructures.\n. Gavin, I am new to mailing lists. Can I create an account for the mailing list or just reply to public mailing list ? Please let me know so that I can describe my usecase to Matthias. Thanks.\n. ",
    "carlhoerberg": "This is a bug. Especially as BlockingConnection doesn't handle heartbeats in the background in a separate thread, something that at least should be documented.\nAll other official clients can override the server suggested heartbeat today, this is the first client i've seen that can't do it. \n. ",
    "rcarz": "Well it's broken as it is. Would you accept a pull request to change the signature in the Twisted adapter?\n. Thanks!\n. ",
    "sethbollinger": "Thanks very much!\nOn Sun, Mar 17, 2013 at 9:51 PM, Gavin M. Roy notifications@github.comwrote:\n\nGood catch, sorry for the trouble. Will push 0.9.11 out tonight.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/306#issuecomment-15036954\n.\n. \n",
    "alex88": "Looking at tcpdump it does only an A query for dns not a AAAA\n. Uh OK thank you very much!\n. Seems that just using\nif (socket.has_ipv6):\n        self.socket = socket.socket(socket.AF_INET6, socket.SOCK_STREAM, 0)\n    else:\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)\nin base connection works, should I create a pull request?\n. I've added the PR, lets move the discussione there\n. A next step would be to loop trough all the returned address found by getaddrinfo and try to connect to each one\n. Thanks!\n. Just seen the code, seems perfect now ;) Awesome!\n. ",
    "alexchandel": "That works, is pamqp stable enough to begin? I can see which parts will get torn out.\nI think it'd be worthwhile to add some 3.x support in the interim. Most of the changes are idempotent. The only thing that didn't compile in 2.x was the urlparse->urllib.parse change, and I just pushed a fix for that.\nAlso what do you mean by sync up? I forked the current version of the master branch. I couldn't find anything more recent.\n. I think the key thing is that this commit isn't as large as the number would make it seem. The vast majority of changes are print statements and basestring --> str, and those will be numerically great no matter how much you talk about them.\n. The biggest change I see is spread over the second and third commit, where BytesIO is try-imported in addition to the original StringIO and cStringIO.\n. Sorry, I meant to have the second commit as a separate pull request.\nIt's succinct though. Methods that were removed in Python 3, like dict.iterkeys, are simply swapped for backwards-compatible ones, like dict.keys.\nWhere necessary, the output is wrapped in a list, as in return list(dict.keys()).\n. Anybody seen this?\n. Yeah it doesn't fix any bugs under Python 2.7. It just adds support for Python 3.\nI have a series of non-breaking Python 3 compatibility fixes to contribute, but I'd like to merge them one at a time. Would you rather I expand this pull request with several small compatibility commits, or wait for this to be merged and file several new pull requests?\n. Any progress on merging?\n. ",
    "niwinz": "That is status of it? Python3 support would be awesome...\n. ",
    "hayd": "This PR does some strange things with unicode (I don't think you can simply replace it with str and assume that this works in both python 2 and 3, surely it doesn't) :s \n@gmr to \"sync up\", is there a branch for py3? Is there a road-map (is making pika python 2 and 3 compatible the objective?). Is it worth running futurize -1 on the code-base and trying to get the tests to pass?\n. Since Python 2.5 is no longer supported, (exception) syntax is not a problem i.e. you can rewrite to py2 and 3 compatible code. My suggestion would be:\n1. run futurize stage 1, and fix up imports etc. to work for python 2 and 3.\n2. add python 3.3, 3.4 as allowable failures to travis. (commit to master)\n3. get the tests passing on python 3. (hardest part, may take a while, no need to be done singlehanded)\nPersonally I'm happy to help/PR on the first two points... if it was likely work in this direction would be accepted. Then things can get started.\n\"When it's done\" means never unless there's a plan which can be worked towards/tests that can be chipped away at. At the moment it seems there isn't a plan ?\n(#312 was rejected, though to be fair, I think this broke py2...)\n. @robertlagrant It's merged in #581, so this issue can be closed now?\n. ",
    "nolink": "That's great, the other day I was doing some search about twisted pika example, but without any luck, so I wrote one, anyway, it would be really helpful with example in docs...\n. ",
    "wedaly": "Hi Gavin,\nI'm still getting the error.  I tried uninstalling/re-installing with pip (version 0.9.10p0).  I also tried pulling from master and installing the latest version (0.9.13p1).  I checked that these were the installed versions when I ran my tests by using the python interpreter (pika.version)\nIn both cases, I get the same error described above.\nHere is the git log for the last build I tried:\ncommit 7c4cc75c6f57031507bf00cb3e18af41a7641931\nAuthor: Gavin M. Roy gmr@meetme.com\nDate:   Sun Apr 7 21:29:39 2013 -0400\nClarify the docstring in the on_cancelok method (#308)\ncommit 3d86c6221ea7f07398c28cc090e754ae0a7bc21a\nAuthor: Gavin M. Roy gmr@meetme.com\nDate:   Sun Apr 7 19:51:38 2013 -0400\nUpdate docs to p13r1, add twisted info\ncommit 6abc38e4030e9c69b7356aade2526986c34f831b\nAuthor: Gavin M. Roy gmr@meetme.com\nDate:   Sun Apr 7 19:36:53 2013 -0400\nOops forgot to add the on_close callback.\ncommit 4fd07e07e2e6088d670ea28f8f30658a63137071\nAuthor: Gavin M. Roy gmr@meetme.com\nDate:   Sun Apr 7 19:30:42 2013 -0400\nFix the spacing issue to make the list render in docs\ncommit c6143f90a20c0a5f5d0976cea288cc1b72c63a1c\nAuthor: Gavin M. Roy gmr@meetme.com\nDate:   Sun Apr 7 19:26:39 2013 -0400\nRefactor BlockingConnection.Close to not raise an exception when called and to go through all of the proper steps\ncommit 052a1152397d5be742ef6a3cf9c24e0872fa46a6\nAuthor: Gavin M. Roy gmr@meetme.com\nDate:   Sun Apr 7 19:26:13 2013 -0400\nInvoke HeartbeatChecker.stop when the connection is closed\ncommit e265e1be81efab413db737be0ef4bcf29cc1d63f\nAuthor: Gavin M. Roy gmr@meetme.com\nDate:   Sun Apr 7 19:24:26 2013 -0400\nAdd stop method to stop the heartbeat checker when disconnected (#307)\ncommit dd46c58930a994adb4cfd3fcf4c74d7019f41bd5\nAuthor: Gavin M. Roy gmr@meetme.com\nDate:   Sun Apr 7 18:42:30 2013 -0400\nDocumentation updates adding a Twisted example and release notes for 0.9.13\nThanks!\n-- Will\n. Narrowed the issue down: it seems to occur only when using multiprocessing.Process.  The code example below recreates the problem:\n```\nimport multiprocessing                                                           \nimport pika                                                                        \nclass Worker(multiprocessing.Process):                                           \n    def init(self):                                                          \n        super(Worker, self).init()                                             \n    credentials = pika.PlainCredentials('guest', 'guest')\n\n    parameters = pika.ConnectionParameters(heartbeat_interval=5,               \n                                           credentials=credentials,            \n                                           host='localhost')\n\n    self.connection = pika.SelectConnection(parameters, self.on_connected)\n\ndef on_connected(self, connection):                                            \n    print \"connected\"\n\ndef run(self):                                                                 \n    self.connection.ioloop.start()\n\nif name == 'main':                                                       \n    worker = Worker()                                                            \n    worker.start()                                                               \n    worker.join() \n```\nI know this is probably not cleanest implementation, but this is based on code that I inherited and that is currently running in production without errors.  Any ideas?\n. ",
    "Ralithune": "I'm getting this error too.  Did this ever get figured out?\n. @vitaly-krugl I can go through my notes, but I don't think I took note of how this got fixed for me.  I think I remember it being something really dumb that I was doing, but I can't be sure.  In any event it apparently wasn't a big enough deal for me to remember it, so maybe it was something simple like blocked ports or bad credentials? Sorry I'm not more help.\n. ",
    "raissanucci": "I'm getting the error when I try to create more than 900 child processes, each of them declaring a queue\n. ",
    "vaishalipavashe": "Hi,\nI have \"pika (0.10.0)\" version installed and I am getting same error:\nERROR:pika.adapters.base_connection:Socket Error: 54\nERROR:pika.adapters.base_connection:Incompatible Protocol Versions\n.\n.\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 154, in _adapter_disconnect\n    self._check_state_on_disconnect()\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 169, in _check_state_on_disconnect\n    raise exceptions.IncompatibleProtocolError\npika.exceptions.IncompatibleProtocolError\nI am trying to use it with HP OneView RabbitMQ: http://h17007.www1.hpe.com/docs/enterprise/servers/oneviewhelp/oneviewRESTAPI/content/s_SCMB-python-fusion.html\n. ",
    "eddwardo": "Im quite sure. This is the exact code that im using : https://gist.github.com/eddwardo/5347176\nScenario:\n1. rmq server is working, i can publish messages correctly\n2. _basic_publish returns None\n3. i killed rmq server with -9\n3. _basic_publish still retuns None, messages are gone.\n. PS. Im printing msg_body in errback to make sure that basic publish don't raise any Exception during rmq-server failure.\n. wulczer: with new (patched) version of pika everthing seems to look fine. thx for quick a response :) \n. Waiting! :D\n. ",
    "Kuv": "version is 0.9.12\n. Sorry, but i have this error on my pika 0.9.13\nself._queue_declare(queue=queue)\nFile \"core/network.py\", line 67, in _queue_declare\n    self._channel.queue_declare(queue, durable=True)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 867, in queue_declare\n    None, replies)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 1104, in _rpc\n    self._wait_on_response(method_frame))\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 1121, in _send_method\n    self.connection.send_method(self.channel_number, method_frame, content)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 249, in send_method\n    self._send_method(channel_number, method_frame, content)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/connection.py\", line 1489, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 388, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/connection.py\", line 1476, in _send_frame\n    self._flush_outbound()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 348, in _flush_outbound\n    if self._handle_write():\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\", line 338, in _handle_write\n    return self._handle_error(error)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\", line 282, in _handle_error\n    self._handle_disconnect()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 318, in _handle_disconnect\n    self.disconnect()\nAttributeError: 'BlockingConnection' object has no attribute 'disconnect'\n\n\n\nimport pika\npika.file\n'/usr/local/lib/python2.7/dist-packages/pika/init.pyc'\npika.version\n'0.9.13'\n. \n\n\n",
    "wmslei": "But I haven't basic_ack the message, where the message? Shouldn't it in the queue? And if the queue have messages, the queue shouldn't be auto deleted.\n. When I basic_nack the message, I set the requeue=True, so the message should be requeued. On the other side, I create a queue set auto_delete=True, then I publish a message to it, finally I close the channel and connection. This moment, the queue is also exist and the message in it.\n. ",
    "CrackerJackMack": "``` python\nimport pika\nconn = pika.BlockingConnection(pika.ConnectionParameters(,\n    virtual_host='vhost', connection_attempts=2, heartbeat_interval=20,\n))\nmessages_per_run = 10000\nwhile True:\n    fetched = 0\n# Create a new channel to set a different QoS\nchannel = conn.channel()\nchannel.basic_qos(prefetch_count=1000)\n\ntry:\n    messages = channel.consume(queue_name)\n    for method_frame, properties, body in messages:\n        if fetched >= messages_per_run:\n            channel.cancel()\n            break\n        # do stuff\n        channel.basic_ack(method_frame.delivery_tag)\nfinally:\n    if channel.is_open:\n        # FIXME: channels remain on the connection as well, smaller memory leak\n        channel.close()\n    # XXX: removes all references, fixing the memory creep problem\n    channel._generator_messages = []\n\n```\n. ",
    "arachnotron": "Sure.\n``` Python\nclass SM(object):\n    @defer.inlineCallbacks\n    def run(self, connection):\n        try:\n            #Set up connection\n            chan = yield connection.channel()\n            #Declare queue\n            queue = yield chan.queue_declare(queue=\"test_queue\", durable=True, exclusive=False, auto_delete=False)\n            exchange = yield chan.exchange_declare(exchange=\"test_exchange\", type=\"direct\", durable=True)\n            yield chan.queue_bind(queue=\"test_queue\", exchange=\"test_exchange\", routing_key=\"test_key\")\n            yield chan.basic_qos(prefetch_count=1)\n            queue_obj,consumer_tag = yield chan.basic_consume(queue='test_queue',\n                                    no_ack=True, consumer_tag='test_consumer')\n            l = task.LoopingCall(self.test, queue_obj)\n            l.start(0.01)\n        except Exception, e:\n            print \"ERROR: %s\" % str(e)\n            connection.close()\n            reactor.stop()\n@defer.inlineCallbacks\ndef test(self, queue):\n    print \"test\"\n    ch,method,properties,body = yield queue.get()\n    print \"test1\"\n\nif name == 'main':\n    #logging.basicConfig(level=logging.DEBUG)\n    parameters = ConnectionParameters()\n    cc = protocol.ClientCreator(reactor, twisted_connection.TwistedProtocolConnection, parameters)\n    d = cc.connectTCP('localhost', 5672)\n    d.addCallback(lambda protocol: protocol.ready)\n    d.addCallback(SM().run)\nreactor.run()\n\n```\nPrints \"test\" and then hangs.\n. Then perhaps the issue is with my publishing code? It seems fine to me but I'm not sure now.\n``` Python\nfrom twisted.internet.defer import inlineCallbacks\nfrom twisted.internet import reactor, task, protocol\nfrom twisted.internet.protocol import ClientCreator\nfrom twisted.python import log\nfrom pika.adapters import twisted_connection\nfrom pika import ConnectionParameters, BasicProperties\n@inlineCallbacks\ndef do_msg(connection):\n    try:\n    msg = content\n        print \"Connected to broker.\"\n        chan = yield connection.channel()\n        #Declare queue\n        print \"Channel constructed.\"\n        yield chan.queue_declare(queue=\"test_queue\", durable=True, exclusive=False)\n        print \"Queue opened.\"\n    print \"Sending message: %s\" % msg\n    yield chan.basic_publish(exchange=\"test_exchange\", body=msg, routing_key=\"test_key\",\n                             properties=BasicProperties(delivery_mode = 2))\n\nexcept Exception, e:\n    print \"ERROR: %s\" % str(e)\nfinally:\n    yield connection.close()\n    reactor.stop()\n\nif name == \"main\":\n    import sys\n    if len(sys.argv) < 2:\n        print \"%s host port vhost username password path_to_spec content [count]\" % sys.argv[0]\n        sys.exit(1)\n#host,port,vhost,username,password = (\"localhost\",5672,\"/\",\"guest\",\"guest\")\ncontent = sys.argv[1]\n\nparameters = ConnectionParameters()\ncc = protocol.ClientCreator(reactor, twisted_connection.TwistedProtocolConnection, parameters)\nd = cc.connectTCP('localhost', 5672)\nd.addCallback(lambda protocol: protocol.ready)\nd.addCallback(do_msg)\n\nreactor.run()\n\n```\nOkay I guess the actual problem was closing the connection and stopping the reactor too quickly, but I'm not sure how to go about slowing that down enough.\nThis issue can be closed, then.\n. ",
    "garthwilliamson": "I didn't check the tests - do you want me to send in a patch for those also?\n. ",
    "nyren": "It is useful in unittests, e.g. start IOloop, send some messages, stop IOloop, verify desired states were reached.\nThe tornado IOLoop support this for example, see the documentation of the stop() function [1].\n[1] http://www.tornadoweb.org/en/stable/ioloop.html\n. Of course, thanks for looking into this.\nI have another unittest-related oneliner-fix coming as well, need to do some more testing first though. It looks like it self._adapter_disconnect() should be called in BaseConnection.close() before stopping the ioloop.\n. ",
    "fat-crocodile": "Workaround, that works:\nclass MyConnection(BlockingConnection):\n....def init(self, _args, _nargs):\n........BlockingConnection.init(self, _args, _nargs)\n....def _send_method(self, channel_number, method_frame, content=None):\n........BaseConnection._send_method(self, channel_number, method_frame, content)\n........if self._frames_written_without_read >= self.WRITE_TO_READ_RATIO:\n............self._frames_written_without_read = 0\n............self.process_data_events()\n....def _send_frame(self, frame_value):\n........BaseConnection._send_frame(self, frame_value)\n........self._frames_written_without_read += 1\n. Do you mean https://github.com/pika/pika/commit/abf9fa80d8a661e265a26438722d783c1f130ac0 ?\nNo, for my problem this patch does not work.\nAnd, in any case, server says that it have got unexpected frame, not bad frame.\nMy tests:\n======= Send: ==========\n!/usr/bin/env python\nimport pika\ninput_exchange_name  = \"test.inputexchange.123456\"\ninput_queue_name  = \"test.inputqueue.123456\"\nconnection = pika.BlockingConnection()\nc = connection.channel()\nc.exchange_declare(exchange=input_exchange_name, type='fanout')\nc.queue_declare(queue = input_queue_name)\nc.queue_bind(exchange=input_exchange_name, queue=input_queue_name, routing_key='')\nfor x in range(1000):\n....c.basic_publish(exchange=input_exchange_name, routing_key='', body=\"%d\" % x)\n======= Receive: ============\n!/usr/bin/env python\nfrom pika import BlockingConnection, BaseConnection\nimport logging\nlogger = logging.getLogger(name)\nlogging.getLogger('pika').addHandler(logging.StreamHandler())\noutput_exchange_name = \"test.outputexchange.123456\"\noutput_queue_name = \"test.outputqueue.123456\"\ninput_queue_name  = \"test.inputqueue.123456\"\nconnection = BlockingConnection()\nc = connection.channel()\nc.exchange_declare(exchange=output_exchange_name, type='fanout')\nc.queue_declare(queue = output_queue_name)\nc.queue_bind(exchange=output_exchange_name, queue=output_queue_name, routing_key='')\ndef callback(ch, method, properties, body):\n....c.basic_publish(exchange=output_exchange_name, routing_key='', body='xxx')\n....c.basic_ack(method.delivery_tag)\nc.basic_consume(callback, queue=input_queue_name)\nc.start_consuming()\n\nI run \"send\", wait until it finished, and then run \"recv\".\n. A little bit easier: bug reproduce without acking. My server -- RabbitMQ 3.1.1\nFrom AMPQ 0.9.1 specification (section 4.2.6, page 35):\n\n\nContent frames on a specific channel are strictly sequential. That is, they may be mixed with frames for\nother channels, but no two content frames from the same channel may be mixed or overlapped, nor may\ncontent frames for a single content be mixed with method frames on the same channel.\n\n\nSo...\n. Previous workaround cause problems with too deep recursion. My next try, seems to work \nhttps://github.com/fat-crocodile/heap/blob/master/sync_connection.py\nMay it is interesting for you. Or may be you will find some bugs, that I've missed.\n. ",
    "samirbaaz": "I'm facing the same issue, has there been any progress on this?\nI meet the same condition in the _send_frame function where self._frames_written_without_read ends up equaling the WRITE_TO_READ_RATIO in the middle of a message. Pika then throws a socket error and closes the connection and the message never gets sent. My current workaround is to just set the WRITE_TO_READ_RATIO to 2000 instead of 1000. \nThe reason we're hitting this limit is because one consuming channel generates messages to send to another queue. Depending on the message, it can create a pretty high volume of messages to send to the other queue. \nWhat i've now done is moved the check against the ratio into the basic_publish function and given appropriate access to the variables that are needed. I know it's been a while since you've looked at this but do you have any comments/ideas about this?\ndef _send_frame(self, frame_value):\n....super(BlockingConnection, self)._send_frame(frame_value)\n....self._frames_written_without_read += 1\n....if self._frames_written_without_read == self.WRITE_TO_READ_RATIO:\n........self._frames_written_without_read = 0\n........self.process_data_events()\n. ",
    "cooper6581": "Thanks for the quick response!  Client side, I was able to duplicate this issue with CentOS 5.5 (Python 2.6.5), and Ubuntu 13.04 (Python 2.7.4).  I wasn't able to duplicate this issue when connecting to a Rabbit server running on localhost (Ubuntu 13.04 Erlang 15B01).  When connecting to our Rabbit servers over the internal network (Centos 5.9 Erlang 15B) I was running into this problem with both clients.\n. Looks good so far!  Thanks for the quick turn-around.  I'll try and get some more testing in this weekend.\n. ",
    "wichert": "Downgrading to pika 0.9.9 seems to solve this.\n. I might be able to try this later today. Is the relevant change pull #345 ?\n. Using master as of 42a878790dfeff54fa0e07dd750b52d922ce9679 I now get this error instead:\nAMQP connection <0.19853.3> (running), channel 1 - error:\n{amqp_error,unexpected_frame,\n            \"expected content body, got non content body frame instead\",\n            'basic.publish'}\n. @jwestfall69 That is an excellent analysis! That also explains why I ran into this so often: my messages are typically large and the AMQP server is on another continent.\n. And 0.01 milliseconds comes down to not waiting at all, which is what makes this a busy-loop with pika essentially doing polling at an insanely high frequency.\n. @gmr Am I guessing correctly that development of pika has been stalled in favour of gmr/rabbitpy ?\n. This is likely to be a duplicate of #361.\n. ",
    "unthingable": "Downgrading to 0.9.9 does indeed help. Waiting for a fix, can't really test the latest master...\n. This works for me, no more growing stack:\n``` python\nclass Consumer(object):\n...\n    def on_connection_closed(self, connection, reply_code, reply_text):\n        self._subscription_channel = None\n        if self._closing:\n            self._connection.ioloop.stop()\n        else:\n            logger.warning('Connection closed, reopening in %d seconds: (%s) %s',\n                           self._reconnect_wait, reply_code, reply_text)\n            self._connection.add_timeout(5, self.reconnect)\ndef reconnect(self):\n    raise ReconnectionException()\n\ndef run(self):\n    while not self._closing:\n        try:\n            # Create a new connection\n            self._connection = self.connect()\n\n            # There is now a new connection, needs a new ioloop to run\n            self._connection.ioloop.start()\n\n        except ReconnectionException:\n            self._connection.ioloop.stop()\n\ndef stop(self):\n    logger.info('Stopping')\n    self._closing = True\n    self.stop_consuming()\n    self._connection.ioloop.start()\n    logger.info('Stopped')\n\n```\n. ",
    "gkappel": "Any news on 0.9.14 release?  Running into frame issue with 0.9.13\n. ",
    "chenyf": "what's the status of this issue?  I've reproduce this issue with 0.9.13\n. I test with the master, this issue has been fixed.\n. ",
    "rhtyd": "I see this issue with RabbitMQ 3.2.2 and pika 0.9.13 but not with RabbitMQ 3.2.1\n. Hi Gavin, thanks for replying. No, I just checked AMQP 1.0 plugin was not enabled. Please note that downgrading to pika 0.9.9 worked and did not cause any error so probably it's something with 0.9.13 version and RabbitMQ 3.2.2?\nFrom server logs --\nServer startup complete; 7 plugins started.\n- amqp_client\n- mochiweb\n- rabbitmq_management\n- rabbitmq_management_agent\n- rabbitmq_management_visualiser\n- rabbitmq_web_dispatch\n- webmachine\nErrors are like this:\n=ERROR REPORT==== 16-Dec-2013::12:51:30 ===\nAMQP connection <0.1389.0> (running), channel 1 - error:\n{amqp_error,frame_error,\n            \"type 3, first 16 octets = <<\\\"{\\\"ua\\\":\\\"loader.io\\\">>: {invalid_frame_end_marker,\\n                                                      34}\",\n            none}\n. Hi again Gavin,\nI think you missed by point twice now: With pika 0.9.12 and 0.9.9 I'm able to publish message to the broker but not with 0.9.13;\nIn case that confused you I'm pushing newline separated json message to rabbitmq and pulling it out from a consumer written in Python/pika 0.9.13. The json message contains ua (user agent) etc. Please, don't get confused by it, what RabbitMQ is reporting is that the message published to the broker from the consumer has some protocol issues. Hope this clearly explains you the case, let me know if you have further question that may help you nail down this issue. Thanks and regards.\n. In my use case I push data from Redis to RabbitMQ which is the producer, you may see the code here: https://github.com/wingify/agentredrabbit The code is opensource.\nThis file (based on the async publisher example) is responsible for transporting data from Redis to RabbitMQ and it is responsible for producing those errors when use with pika 0.9.13 but not with 0.9.12 or 0.9.9: https://github.com/wingify/agentredrabbit/blob/master/agentredrabbit/transport.py\n. Also to note; the errors happen only when used with RabbitMQ 3.2.2 but not with RabbitMQ 3.2.1 using pika 0.9.13\n. ",
    "jwestfall69": "Hi\nWe ran into this issue too using pika master from 2014/03/10 and rabbitmq 3.2.4.  I was able to capture a tcpdump of the bad frame, examining the dump shows that a partial frame is written then another frame is getting written.  This is what caused rabbitmq to throw the FRAME_ERROR because of a invalid_frame_end_marker.\nI was able to track down the bug to the following code in pika/adapters/base_connection.py\npython\n   def _handle_write(self):\n        \"\"\"Handle any outbound buffer writes that need to take place.\"\"\"\n        total_written = 0\n        if self.outbound_buffer:\n            frame = self.outbound_buffer.popleft()\n            while total_written < len(frame):\n                try:\n                    total_written += self.socket.send(frame[total_written:])\n                except socket.timeout:\n                    raise\n                except socket.error as error:\n                    return self._handle_error(error)\n        return total_written\nThere are actually multiple bugs with this code that writes data to the socket.  (For folks that don't know pika has a default socket timeout of 250ms)\n1.  On a blocking socket the socket buffer could be full or near full causing the send() to block, but then you hit the 250ms timeout.  This results in only part of the frame being written to the socket, then raises the socket.timeout (which is caught within pika and somewhat ignored).  If there are more frames that need to be written pika will continue to call _handle_write(), each time trying to write out the next frame.  However rabbitmq is going to kill the connection from the partially written frame.\n2.  On a non-blocking socket the socket buffer could be full or near full causing the send() to return an EAGAIN or EWOULDBLOCK socket error to let you know.  However for any socket error code its doing a 'return self._handle_error(error)'.  This puts you in the same boat as the blocking socket with a partially written frame.  The _handle_error() function has code to ignore these non-error socket errors, but its pointless since its doing a 'return' on the function call.\nI believe both of these can also result in a stalled connection that we have seen a couple times.  If you think of the situation where the last body frame is the one that not fully written. rabbitmq is waiting on the rest of the last body frame to come over the wire, but pika thinks it\nalready sent it and is waiting on rabbitmq to reply about the message as a whole.\nI think all adapters except for the evp one use a blocking socket.  \nThe blocking adapter is most prone to the error since its trying to send frames as fast as possible. While select/tornado/twisted adapters will at least verify the socket it writable before trying to send a frame.  However the socket being writable doesn't guarantee you can send() an entire frame without blocking.\nKnowing the issue makes is pretty trivial to reproduce.  Have your pika client on a different machine from the rabbitmq server, setup a network rate limit to restrict the throughput between them, then attempt to send a large message using the blocking adapter.\nOne thing folks can do as a stop gap until a fix is made it to pass in 'socket_timeout=10' on your pika.ConnectionParameters().  This will allow the send() to block for up to 10 seconds before triggering the timeout.   At least for us if rabbitmq can't make space in its tcp buffer in that long something is likely truly wrong.\n. There is a bug with the patch.\nThe resulting code is the following \npython\ndef _handle_write(self):\n        \"\"\"Handle any outbound buffer writes that need to take place.\"\"\"\n        if self.outbound_buffer:\n            frame = self.outbound_buffer.popleft()\n            try:\n                bytes_written = self.socket.send(frame)\n                if bytes_written < len(frame):\n                    self.outbound_buffer.appendleft(frame[bytes_written:])\n            except socket.timeout:\n                raise\n            except socket.error as error:\n                return self._handle_error(error)\n        return bytes_written\nIn the case where _handle_write() is called and self.outbound_buffer is empty, you are doing a 'return bytes_written'.  In that code path bytes_written has never been defined and will result in a python error.\nAlso I am pretty sure the tornado connection is using a blocking socket, so I am unsure how you would ever get a WOULDBLOCK.  I think its more likely you were seeing a socket timeout.  However since tornado adapter wont call _handle_write() unless the socket is writable you should be able to at least make one call to socket.send(), which is what your patch is doing by removing the while loop.\nI think your patch is a step in the right direction for fixing the partial frame sending issue #349 \n. ",
    "jjmalina": "Any news on when this fix will be released? Ran into this issue and had to downgrade to Pika 0.9.9 to get around it. Or could I install off of master if it's considered stable?\n. Does anyone have sample code that can reliably reproduce the issue? I'd be interested in taking a stab at writing a fix because I've been stuck on 0.9.9 for a while and want to use Tornado but 0.9.9's Tornado version is like 3 years old.\n. ",
    "nithril": "I meet the same issue with the 0.9.14 \nINFO 2014-08-29 11:29:35,399 __main__ on_message 319 : New channel 'rpc.request' OK\nWARNING 2014-08-29 11:29:39,399 pika.adapters.blocking_connection _on_connection_closed 398 : Disconnected from RabbitMQ at W.X.Y.Z:5672 (501): FRAME_ERROR - type 3, first 16 octets = <<\"XXXXXXXXXXXXXXXX\">>: {invalid_frame_end_marker, 88}\nThe payload has a size of 100MB (full of X character). I'm not yet able to provide a test case\n. ",
    "danielfaust": "I'm having the same issue\n~~\nAMQP connection <0.4834.0> (running), channel 1 - error:\n{amqp_error,unexpected_frame,\n            \"expected content body, got non content body frame instead\",\n            'basic.publish'}\n~~\nThis shows up when publishing back from a consumer's callback.\nI downgraded from 0.9.14 to 0.9.9, and the problem dissappeared. Not only that, but the performance is much better now.\nThe following is when publishing in the consumer's callback is off:\nI'm using a twisted reactor to obtain a loopingcall. In there I publish events to a channel, so I'm able to control how many messages get send.\nWhen attempting to send 200msg/s rabbitmq was only receiving 125msg/s, the entire publishing app only managed to send 125msg/s, no messages got lost. With 0.9.9 I'm geting the full 200msg/s\nWhen attempting to send 100, only 77 could be sent.\nIn a while loop I reaced a limit of 350msg/s. With version 0.9.9 I'm able to send 1520 msg/s.  Rabbitmq is running on olinuxino lime (Allwinner A10 @ 1GHz, 512MB RAM), tested with on a raspberry pi as well. At 1520 msg/s the system can't keep up, so the consumer starts getting the messages delayed. At a rate of 400msg/s the system is stable, what gets in, gets out and arrives immediately.\nAt 300msg/s I'm able to start publishing in the consumer's callback, so that rabbitmq has 590 publish events and 590msg/s deliver events, since one consumer \"replies\" with a publish event which gets consumed by some other non-publishing consumer. 350msg/s is not stable, there I get the same error I mentioned above. It's just that with 0.9.14 I'm getting that error at >50msg/sec.\nNow I checked the version. The problem starts with 0.9.13. There, when I try to push 300msg/s, the publisher gets throttled down to 156 msg/s. And the consumer which publishes back crashes with that error after about a sec. At 100msg/sec the publisher manages to push 76 msg/s, the consumer crashes after a sec. At 50msg/s everything is stable.\nThis is with pika installed via pip\nBTW, @ 300msg/sec (which get magnified to 600msg/s published and 600msg/s delivered) an Allwinner A10 chip uses 64% CPU\nThe payload is just a short json string containing a timestamp, msg count number, and board ID.\nThe problem also exists on the master branch.\n. I moved over to py-amqp, then to txamqp. I think I'm staying with txamqp.\nBoth work well.\n. Try txamqp, I think it's no longer mantained, and requires a little bit of tweaking, but it is fast and rock-solid.\n. ",
    "hupantingxue": "so, everybody, how to solve it?\n. My pika version is 0.9.13 and met the same question too.\n. I met the same question , the pika version is 0.9.13\n\n\n\nimport pika\npika.version\n'0.9.13' \n\n\n\nand the traceback is below:\n\nTraceback (most recent call last):\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 521, in basic_publish\n    (properties, body))\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 1107, in _rpc\n    return self._process_replies(replies, callback)\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 1061, in _process_replies\n    frame_value = self._frames[reply]\nKeyError: 'Basic.Ack'                                                                                         \n. Upgrade the version\n. \n",
    "jkehler": "I have also encountered the same problem as described by @danielfaust. I am publishing back from a consumer callback using a very basic JSON body and I encounter this error constantly. I am not sure what is causing it as the JSON body is not malformed when checking it.\n. ",
    "fordnox": "Having same issue here with RabbitMQ 3.3.5 and pika 0.9.14 \nDowngrading to pika 0.9.9 helped\n. ",
    "fake-name": "Why is this issue closed? I have recently encountered it using 0.9.14 (both this, and the python3-pika fork).\nMore critically, I am specifically acking after I publish, and the issue is still present:\n```\n    msgProps = pika.BasicProperties(correlation_id = props.correlation_id)\n    ch.basic_publish(exchange='',\n                     routing_key=props.reply_to,\n                     properties=msgProps,\n                     body=response)\nself.log.info(\"Message published. Acking.\")\n\n# Ack has to be after the payload response has been published (and the requisite delay),\n# or pika will shit a brick.\n# see https://github.com/pika/pika/issues/349\nch.basic_ack(delivery_tag = method.delivery_tag)\n\n```\nI moved the ack after the publish, and the issue still continued to happen.\nLike the others, downgrading to 0.9.9 fixes this issue.\nFWIW, I'm pushing around very large messages (10-500 MBytes), which I think probably considerably exacerbates the underlying metastability, but it should work for any message size.\n. This issue, and the complete lack of response have lead to me completely abandoning pika alltogether.\nI'd recommended just using bare amqp until the dev's actually deign to respond.\n. ",
    "lenolib": "I am also having the same problems, running RabbitMQ 3.4.4 and pika 0.9.14\nSeem to occur randomly when I send large messages.\n. ",
    "thomdixon": "Same problem with 0.9.14. I'll attempt the downgrade to 0.9.9 as others have done to see if that temporarily addresses the issue.\n. @jjmalina Running the code from this repository gives me the following output:\nvagrant@localhost:/vagrant$ python main.py\nPUBLISH: blah\nCONSUME: blah\nPUBLISH: test\nPUBLISH: test2\nCONSUME: test\nPUBLISH: test\nPUBLISH: test2\nCONSUME: test2\nCONSUME: test\nPUBLISH: test\nPUBLISH: test2\nCONSUME: test2\nPUBLISH: test\nPUBLISH: test2\nPUBLISH: test\nPUBLISH: test2\nProcess Process-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n    self._target(*self._args, **self._kwargs)\n  File \"main.py\", line 50, in run\n    self.consumer_channel.start_consuming()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 955, in start_consuming\n    self.connection.process_data_events()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 243, in process_data_events\n    raise exceptions.ConnectionClosed()\nConnectionClosed\nand looking at tail /var/log/rabbitmq/rabbit@localhost.log gives:\n```\naccepting AMQP connection <0.420.0> ([::1]:54685 -> [::1]:5672)\n=ERROR REPORT==== 2-Apr-2015::10:27:15 ===\nAMQP connection <0.420.0> (running), channel 2 - error:\n{amqp_error,unexpected_frame,\n            \"expected content body, got non content body frame instead\",\n            'basic.publish'}\n=INFO REPORT==== 2-Apr-2015::10:27:15 ===\nclosing AMQP connection <0.420.0> ([::1]:54685 -> [::1]:5672)\n```\nDoes this help?\n. @eandersson One connection with two channels per process (using multiprocessing, as opposed to threads).\n. @vitaly-krugl I've updated the repository to no longer use multiprocessing at all. One process, one thread, one connection, two channels. The following still reproduces the problem:\n```\nvagrant@localhost:/vagrant$ python main.py &\n[1] 8174\nvagrant@localhost:/vagrant$ ./rabbitmqadmin publish exchange=issue349 routing_key=test.issue349 payload=\"blah\"\nCONSUME: blah\nPUBLISH: test\nPUBLISH: test2\nCONSUME: test\nPUBLISH: test\nPUBLISH: test2\nCONSUME: test2\nCONSUME: test\nPUBLISH: test\nPUBLISH: test2\nCONSUME: test2\nPUBLISH: test\nPUBLISH: test2\nPUBLISH: test\nPUBLISH: test2\nTraceback (most recent call last):\n  File \"main.py\", line 53, in \n    consumer.run()\n  File \"main.py\", line 49, in run\n    self.consumer_channel.start_consuming()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 955, in start_consuming\n    Message published\nself.connection.process_data_events()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 243, in process_data_events\n    raise exceptions.ConnectionClosed()\npika.exceptions.ConnectionClosed\n[1]+  Exit 1                  python main.py\nvagrant@localhost:/vagrant$ tail /var/log/rabbitmq/rabbit@localhost.log\naccepting AMQP connection <0.392.0> ([::1]:54697 -> [::1]:5672)\n=ERROR REPORT==== 2-Apr-2015::12:41:32 ===\nAMQP connection <0.392.0> (running), channel 2 - error:\n{amqp_error,unexpected_frame,\n            \"expected content body, got non content body frame instead\",\n            'basic.publish'}\n=INFO REPORT==== 2-Apr-2015::12:41:32 ===\nclosing AMQP connection <0.392.0> ([::1]:54697 -> [::1]:5672)\n``\n. @vitaly-krugl Thank you for taking the time to test. I guess we'll update from 0.9.9 to 0.9.15 (or 0.10) when it comes out to skip this issue.\n. @gmr Ran into this today. I recommend passingsys.exc_info()[2]as the third argument toraise` so the traceback more accurately reflects the source of the error until your more permanent solution is in place:\npython\nraise exceptions.ConnectionClosed(), None, sys.exc_info()[2]\n. ",
    "pat1": "I do not need it installed, but i need it in source tar ball; I am trying to build an updated rpm for Fedora system and I need a full source tar ball. I try to get it with python setup.py sdist ...\n. Thanks for your replay.\nmy os is Linux  (Fedora 20 386)\nrpm -q python    python-2.7.5-16.fc20.i686\npika git master \nrpm -q rabbitmq-server rabbitmq-server-3.6.2-1.noarch\nMy app is not accessing the same pika connection (and/or its channels) from more than one thread.\ntest program:\nhttps://gist.github.com/pat1/4017d6565501b657731560af3d2e0b9e\nThe log:\n```\n [*] Waiting for messages. To exit press CTRL+C\nINFO 2016-05-27 18:53:54,438 main connect  56  : Connecting to localhost\nINFO 2016-05-27 18:53:54,440 pika.adapters.base_connection _create_and_connect_to_socket  216 : Connecting to ::1:5672\nINFO 2016-05-27 18:53:54,445 main on_connection_open  72  : Connection opened\nINFO 2016-05-27 18:53:54,445 main add_on_connection_close_callback  81  : Adding connection close callback\nINFO 2016-05-27 18:53:54,446 main open_channel  124 : Creating a new channel\nINFO 2016-05-27 18:53:54,448 main on_channel_open  136 : Channel opened\nINFO 2016-05-27 18:53:54,448 main add_on_channel_close_callback  148 : Adding channel close callback\nINFO 2016-05-27 18:53:54,448 main start_consuming  178 : Issuing consumer related RPC commands\nINFO 2016-05-27 18:53:54,448 main add_on_cancel_callback  188 : Adding consumer cancellation callback\nINFO 2016-05-27 18:53:54,463 main on_message  218 : Received message # 1 from None\nINFO 2016-05-27 19:03:54,556 main acknowledge_message  243 : Acknowledging message 1\nERROR 2016-05-27 19:03:54,557 pika.adapters.base_connection _handle_error  362 : Socket Error: 104\nINFO 2016-05-27 19:03:54,558 pika.connection _on_terminate  1891: Disconnected from RabbitMQ at localhost:5672 (-1): error(104, 'Connection reset by peer')\nWARNING 2016-05-27 19:03:54,559 main on_channel_closed  164 : Channel 1 was closed: (-1) error(104, 'Connection reset by peer')\nWARNING 2016-05-27 19:03:54,559 pika.connection close  1135: Suppressing close request on >\nWARNING 2016-05-27 19:03:54,560 main on_connection_closed  99  : Connection closed, reopening in 5 seconds: (-1) error(104, 'Connection reset by peer')\nINFO 2016-05-27 19:03:59,563 main connect  56  : Connecting to localhost\n```\n. Adding socket_timeout=1200 in ConnectionParameters do not solve the problem.\nOn the server:\nsysctl -a |grep net.ipv4.tcp_keepalive\nnet.ipv4.tcp_keepalive_intvl = 75\nnet.ipv4.tcp_keepalive_probes = 9\nnet.ipv4.tcp_keepalive_time = 7200\n. I have no proxy, I access rabbitmq via localhost (or public IP, do not change the result).\nThe rabbitmq log looks like:\n```\n=INFO REPORT==== 31-May-2016::19:41:30 ===\naccepting AMQP connection <0.32634.2> ([::1]:50677 -> [::1]:5672)\n=ERROR REPORT==== 31-May-2016::19:42:00 ===\nclosing AMQP connection <0.32634.2> ([::1]:50677 -> [::1]:5672):\n{writer,send_failed,{error,timeout}}\n```\nSetting {heartbeat, 0} in /etc/rabbitmq/rabbitmq.config do not change the behavior.\n. looking at:\nhttp://stackoverflow.com/questions/35438843/rabbitmq-error-timeout\nadding \nself._channel.basic_qos(prefetch_count=1)\nbefore\nself._consumer_tag = self._channel.basic_consume(self.on_message,queue)\nsolve the problem.\nNote that I have more than 1000 messages in queue and some are not little.\nI think this configuration/example should be published somewhere as template for a very common user case application.\nAlso a ioloop heartbeat poll method should be useful for do not disable heartbeat and execute only consumer task in thread.\nYou can evaluate to close this issue.\n. To be clear this do not work:\nhttps://gist.github.com/pat1/4017d6565501b657731560af3d2e0b9e/3dc43a75c874d76ecdd1412f7b91c06febd848e1\nthis work:\nhttps://gist.github.com/pat1/4017d6565501b657731560af3d2e0b9e/7246b17d7b4b883d9ff68e53dc65657cdd2eedde\n@vitaly-krugl  I do not have a script to populates the queue, I think you can fill it with  1000 messages of 100k size\n. As written some time ago the use of \nself._channel.basic_qos(prefetch_count=1)\nshould be published somewhere as template for a very common user case application.\nAlso a ioloop heartbeat poll method should be useful for do not disable heartbeat and execute only consumer task in thread.. ",
    "jeffwrule": "Still seeing this error, any word on the upcoming 9.0.14?  This seems like quite an old issue at this time.\n. ",
    "johnnyrubin": "Ok thanks I will test this tomorrow I hope this resolve all warning messages\n. So i have test, and now first warning messages is cleared but i have problem enough because when i stop my program receive i have this error:\nFile \"receive_log_direct.py\", line 34, in <module>\n  channel.start_consuming()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 814, in start_consuming\n    self.connection.process_data_events()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 168, in process_data_events\n    if self._handle_read():\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 271, in _handle_read\n    if self._read_poller.ready():\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 54, in ready\n    events = self.poller.poll(self.poll_timeout)\nand when my receive program stop i have this error:\nNo handlers could be found for logger \"pika.connection\"\nWhereas my message is send, i don't understand....\n. ok i understand for the warning message, i put my code here:\n```\n!/usr/bin/env python\nimport pika\nimport sys\nconnection = pika.BlockingConnection(pika.ConnectionParameters(\n        host='10.185.104.192'))\nchannel = connection.channel()\nchannel.exchange_declare(exchange='direct_logs',\n                         exchange_type='direct')\nresult = channel.queue_declare(exclusive=True)\nqueue_name = result.method.queue\nseverities = sys.argv[1:]\nif not severities:\n    print >> sys.stderr, \"Usage: %s [info] [warning] [error]\" % (sys.argv[0],)\n    sys.exit(1)\nfor severity in severities:\n    channel.queue_bind(exchange='direct_logs',\n                       queue=queue_name,\n                       routing_key=severity)\nprint ' [*] Waiting for logs. To exit press CTRL+C'\ndef callback(ch, method, properties, body):\n    print \" [x] %r:%r\" % (method.routing_key, body,)\nchannel.basic_consume(callback,\n                      queue=queue_name,\n                      no_ack=True)\nchannel.start_consuming()\n```\nThanks to help me\n. Really thanks, it works. Thanks vermoudakias :)\n. ",
    "SYNchroACK": "Same problem here. :+1: \n. I add in my code:\n```\n    def disconnect(self):\n      pass\nBlockingConnection.disconnect = disconnect\n\n```\nNow, I have received the following error:\nFile \"lib/bot.py\", line 42, in start\n    self.process()\n  File \"/opt/bots/feed.py\", line 12, in process\n    self.send_message(report)\n  File \"lib/bot.py\", line 132, in send_message\n    self.pipeline.send(message)\n  File \"lib/pipeline.py\", line 60, in send\n    self.destination_channel.basic_publish(exchange=self.destination_exchange, routing_key='', body=unicode(message))\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 540, in basic_publish\n    (properties, body), False)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 1121, in _send_method\n    self.connection.send_method(self.channel_number, method_frame, content)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 249, in send_method\n    self._send_method(channel_number, method_frame, content)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/connection.py\", line 1496, in _send_method\n    self._send_frame(frame.Header(channel_number, length, content[0]))\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 388, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/connection.py\", line 1476, in _send_frame\n    self._flush_outbound()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 348, in _flush_outbound\n    if self._handle_write():\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\", line 338, in _handle_write\n    return self._handle_error(error)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\", line 282, in _handle_error\n    self._handle_disconnect()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 319, in _handle_disconnect\n    self._on_connection_closed(None, True)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 375, in _on_connection_closed\n    self._channels[channel]._on_close(method_frame)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 1018, in _on_close\n    raise exceptions.ChannelClosed(0, 'Not specified')\nChannelClosed: (0, 'Not specified')\n. ",
    "rioh": "Me too.\n. ",
    "qwang2505": "Thanks! But I got another problem. When I added channel.confirm_delivery(), I got following exception:\nTraceback (most recent call last):\n  File \"send.py\", line 21, in \n    connection.close()\n  File \"/home/baina/pikatest/pika/adapters/blocking_connection.py\", line 200, in close\n    self._send_connection_close(reply_code, reply_text)\n  File \"/home/baina/pikatest/pika/connection.py\", line 1438, in _send_connection_close\n    self._on_connection_closed, [spec.Connection.CloseOk])\n  File \"/home/baina/pikatest/pika/connection.py\", line 1428, in _rpc\n    self._send_method(channel_number, method_frame)\n  File \"/home/baina/pikatest/pika/connection.py\", line 1492, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"/home/baina/pikatest/pika/adapters/blocking_connection.py\", line 396, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\n  File \"/home/baina/pikatest/pika/connection.py\", line 1479, in _send_frame\n    self._flush_outbound()\n  File \"/home/baina/pikatest/pika/adapters/blocking_connection.py\", line 356, in _flush_outbound\n    if self._handle_write():\n  File \"/home/baina/pikatest/pika/adapters/base_connection.py\", line 338, in _handle_write\n    total_written += self.socket.send(frame[total_written:])\nAttributeError: 'NoneType' object has no attribute 'send'\nI'm hoping it would raise a ConnectionError or ChannelError, but why this? Do I miss something? Or it should be like this?\n. Thanks a lot! I will handle the AtrributeError in my code, but it should be a proper exception. I will leave this open to track this improper exception problem.\n. Thanks for help!\n. ",
    "William-Kong": "Was wondering whether this issue has been fixed or not? \nI copied the posted code, and added the line \"channel.confirm_delivery()\", But for me, the basic_publish functions hangs there. \nDone think it matters, but my os: Ubuntu 10.04.4 LTS\nThank you in advace\n. After I hit ctrl-c, I see this: \nFile \"./test1.py\", line 21, in \n    connection.close()\n  File \"/usr/local/lib/python2.6/dist-packages/pika-0.9.14p0-py2.6.egg/pika/adapters/blocking_connection.py\", line 211, in close\n    self.process_data_events()\n  File \"/usr/local/lib/python2.6/dist-packages/pika-0.9.14p0-py2.6.egg/pika/adapters/blocking_connection.py\", line 233, in process_data_events\n    if self._handle_read():\n  File \"/usr/local/lib/python2.6/dist-packages/pika-0.9.14p0-py2.6.egg/pika/adapters/blocking_connection.py\", line 345, in _handle_read\n    if self._read_poller.ready():\n  File \"/usr/local/lib/python2.6/dist-packages/pika-0.9.14p0-py2.6.egg/pika/adapters/blocking_connection.py\", line 36, in inner\n    return f(_args, *_kwargs)\n  File \"/usr/local/lib/python2.6/dist-packages/pika-0.9.14p0-py2.6.egg/pika/adapters/blocking_connection.py\", line 78, in ready\n    events = self.poller.poll(self.poll_timeout)\nThanks\n. Please ignore the previous post, regarding the ctrl-c, the code was trying to close the connection to the server, which is not accessible anymore.\n. Hi Gavin,\nThat is what i am expecting too.\nBut it seems that the time out for tcp connection, by default, is like 2\nhours. Could this be the cause of the issue?\nThanks\nWilliam\nOn Sep 25, 2013 6:10 PM, \"Gavin M. Roy\" notifications@github.com wrote:\n\nThe behavior I would expect should the socket connection be disrupted is\nthat the socket would time out on a read, returning an OS level error on\nthe socket that pika would catch. I'll have to play around to figure out\nexactly what is happening.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/400#issuecomment-25129058\n.\n. No, I don't set anything, just use the default setting, which seems to be 2\nhours.\nOn Sep 25, 2013 8:59 PM, \"Gavin M. Roy\" notifications@github.com wrote:\nThat would be an issue, is that what you're setting it to?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/issues/400#issuecomment-25136918\n.\n. \n",
    "andhadley": "We experienced this same issue when using blocking connections in pika 0.9.9 as well. Setting the timeout value to 10 instead of 0.01 dramatically decreased CPU usage with no noticeable impact on performance. The comment on this line of code states:\nA poller that will check to see if data is ready on the socket using\n    very short timeouts to avoid having to read on the socket, speeding up the\n    BlockingConnection._handle_read() method.\nI am not sure how having such a short timeout offers any benefit at all, especially on a blocking connection. Can anyone clarify?\n. I would agree that timeouts of more than a second could degrade performance but as noted by wichert above the timeout specified is in milliseconds, not seconds. From the python 2.6 docs:\npoll.poll([timeout])\n...If timeout is given, it specifies the length of time in milliseconds which the system will \nwait for events before returning. If timeout is omitted, negative, or None, the call will block \nuntil there is an event for this poll object.\nSurely having a timeout of 10 milliseconds should not effect write performance. The current value of 0.01 leads me to believe the dev thought the timeout was in seconds, not milliseconds. \n. Any update on this?\n. ",
    "julio-vaz": "Any update on this matter?\n. ",
    "alvinkatojr": "Thank you @eandersson the logging error is now fixed and even then RabbitMQ was already running in the background. I now have a new error and my send.py file is exactly as it is in the RabbitMQ tutorial 01 what could be the problem here?\n\nFile \"send.py\", line 7, in \n    connection=pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n  File \"/home/alvin/.virtualenvs/sparkplug/local/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 61, in init\n    super(BaseConnection, self).init(parameters, on_open_callback)\n  File \"/home/alvin/.virtualenvs/sparkplug/local/lib/python2.7/site-packages/pika/connection.py\", line 513, in init\n    self._connect()\n  File \"/home/alvin/.virtualenvs/sparkplug/local/lib/python2.7/site-packages/pika/connection.py\", line 804, in _connect\n    self._adapter_connect()\n  File \"/home/alvin/.virtualenvs/sparkplug/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 146, in _adapter_connect\n    self.process_data_events()\n  File \"/home/alvin/.virtualenvs/sparkplug/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 88, in process_data_events\n    if self._handle_read():\n  File \"/home/alvin/.virtualenvs/sparkplug/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 184, in _handle_read\n    super(BlockingConnection, self)._handle_read()\n  File \"/home/alvin/.virtualenvs/sparkplug/local/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 300, in _handle_read\n    return self._handle_error(error)\n  File \"/home/alvin/.virtualenvs/sparkplug/local/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 264, in _handle_error\n    self._handle_disconnect()\n  File \"/home/alvin/.virtualenvs/sparkplug/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 181, in _handle_disconnect\n    self._on_connection_closed(None, True)\n  File \"/home/alvin/.virtualenvs/sparkplug/local/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 235, in _on_connection_closed\n    raise exceptions.AMQPConnectionError(*self.closing)\npika.exceptions.AMQPConnectionError: (0, '')\n\n. My code already had that implementation here is the link to the code: https://github.com/alvinkatojr/rabbitmq-tutorial.git  Please take a look and tell me if there is anything wrong there.\nI can actually run RabbitMQ server when I run the command:\n   \nI also set a config file for Rabbit so basically I have no idea why this is n't working when I followed the instructions on the site when installing. \n. Can't believe its been a year since this happened. Thanks for the update @JamiesonWarner will try it out next time when am playing with Rabbit. \n. ",
    "JamiesonWarner": "If anyone else new to RabbitMQ gets stuck here, I solved these problems by:\n1. Run the following before the example code:\nimport logging\n   logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.CRITICAL)\n2. Add credentials like so:\n```\n   import pika\ncredentials = pika.PlainCredentials('', '')\n   connection = pika.BlockingConnection(pika.ConnectionParameters(\n                  'localhost', 5672, '/', credentials))\n   channel = connection.channel()\n   ```\n. ",
    "cuiyz": "Have you changed the default username and password \"guest/guest\" for RabbitMQ? That will cause what you have seen.\n. ",
    "nqphuong": "It's ok for me @JamiesonWarner. Thanks.\n. ",
    "abranches": "I also have this issue. Is this the expected behavior? I so, it isn't very clear at the documentation.\nI had to put the basic_consume running in a separate thread, which may give some problems any day soon.\n. Any change of including an implementation of OrderedDict for older versions of Python?\nhttp://code.activestate.com/recipes/576693/\nThis recipe looks stable and someone already turned it into a pip package.\n. I understand that.\nWhat about making _timeouts a list?\nThe only downfall, would be that remove_timeout() would become O(n) instead of O(1).\nprocess_timeouts() really just iterates over the structure...\nI believe it would pay of to have ordered timeouts. For me it's being crucial right now, as I'm scheduling sending of messages in a thread other than the main one (in which the order matters :)\n. I can also submit my fix as a new pull request so you can review it and check if it's ok. It's a simple fix.\n. Sorry all the messed up commits. Do you prefer me to fill a new clean pull request?\n. ",
    "majecty": "First, I'm sorry for my poor english.\nI found the reason of this issue. It's because neted call of  _rpc logic. When basic_consume is called, server send consum_ok and queued messages. So consume_callback is called, and It call basic_ack which is another rpc call.\nBlocking rpc is implemented by boolean flag, whose name is received_response. So when nested rpc is called, received_response is not correctly setted. The last rpc set  received_response be false when quit it's wait loop.\nI think it need  rpc count instead of just boolean flag.\n. ",
    "timeemit": "@soichih if you look at the references immediately before your message, you'll see that a patch was merged on October 18th, 2013.\n. I had run into the same issue.\nImplemented the following workaround:\n``` python\nimport pika\nimport time\nimport logging\nlogger = logging.getLogger()\ndef attempt_connection():\n    _connecting = True\n    _connection = connect()\n    while not _connection.is_open:\n        logger.info('Connection still closed')\n        try:\n            # Succeeds once pika's internal poller has started\n            logger.info('Invoking IO Loop')\n            _connection.ioloop.start()\n        except AttributeError:\n            # If the poller has not started, ioloop is None\n            logger.info('IO Loop not started.  Reconnection status is: {}'.format(_connecting))\n            if not _connecting:\n                logger.info('Retry Count: {}'.format(retry_count))\n                if retry_count > 0:\n                    sleep_time = 2  retry_count\n                    logger.info('Sleeping {} seconds'.format(sleep_time))\n                    time.sleep(2retry_count) # Expontential backoff\n                logger.info('Issuing reconnect attempt')\n                _connecting = True\n                _connection.connect()\ndef connect():\n    logger.info('Connecting to %s', _url)\n    return pika.SelectConnection(\n        pika.URLParameters(_url),\n        on_connection_open,\n        on_connection_error,\n        on_connection_close,\n        stop_ioloop_on_close=False\n    )\ndef on_connection_open(connection):\n    logger.info('Connected to {} successfully!'.format(_url))\n    retry_count = 0\n    _connecting = False\n    logger.info('Creating a new channel')\n    _connection.channel(on_channel_open)\ndef on_connection_error(args, *kwargs):\n    logger.warning('Connection to {} resulted in an error. Initiating new connection.'.format(_url))\n    retry_count += 1\n    _connecting = False\ndef on_connection_close(connection, reply_code, reply_text):\n    logger.info('Connection closed')\n    close_channel()\n    _channel = None\n    if _closing:\n        # Connection close was intended, so stop the ioloop.\n        logger.info('Stopping IO loop')\n        _connection.ioloop.stop()\n        logger.info('IO loop stopped')\n    else:\n        logger.warning('Unexpected connection close: (%s) %s', reply_code, reply_text)\n        check_to_clear_queue()\n        reconnect()\n```\n. ",
    "anton-ryzhov": "I'm using pika's build-in error handler. It will skip EINTR error, and selected (BlockingConnection in your case) adapter should retry syscall.\nTry to reproduce your error with my patch if you can.\nIf it solves your bug, my solution preferable over your, cause it fixes not only for blocking connection and select.select().\n. Does this patch solve bug in your case?\n. Related to #365.\nI've fixed this on lower level.\n. @gmr, I doesn't understand you, what signature?\npika.TornadoConnection will be the function that returns object instance. We can use it transparently to create object instances.\nBut I've just realized that it couldn't be used in isinstance(). This is not good. There should be solution, I'll work more about this question.\n. I've totally changed mechanism (old version \u2014 anton-ryzhov/pika@19f341627e48db57112cbb8aa0bdb1b4e7790f9e). Now almost all classes is defined in common way. But implementations in all three files differs.\nTornadoConnection imports tornado only in one place, pretty simple.\nLibevConnection appends self class on first init\nTwisted* has module-level loader, which is called from all constructors.\nOnly API change \u2014 pretty private class ClosableDeferredQueue is created only after explicit call of _loader. If someone inherits this class \u2014 he has to call _loader before.\n. This is bug I've faced with tonight and been going to debug it.\nThank you done it earlier.\n. This is not only about Tornado \u2014 I've tested it on it, but looks like it affects all adapters.\nYour code:\npython\n    def close(self, reply_code=200, reply_text='Normal shutdown'):\n        # Do *schedule* sending `Channel.Close`s and `Connection.Close`\n        super(BaseConnection, self).close(reply_code, reply_text)\n        # Stop IOLoop right now without any chances scheduled packets ever be sent.\n        self._handle_ioloop_stop()\nThere is no graceful disconnecting, it looks like timeout or socket reset from RabbitMQ server.\nIf we remove this code, super().close() function will schedule packets, IOLoop sends it and receives answer. _handle_ioloop_stop will be called as response of Connection.CloseOk packet and stops IOLoop right after graceful disconnecting.\nLook at current pikas packet exchange in packet tracer, you will see this issue.\n. Interesting \u2014 I cannot reproduce it now, but there definitely was this issue 5 month ago.\nI'll check this later.\n. You are right, it was mistake in my old code.\nSorry for troubling you.\n. I've faced with this bug too. Hope this fix will be in release soon.\nReproduction is very simple \u2014 use 3.5+ and send one big message (100k in my case for real network, may be bigger for localhost). If this message will be bigger than underlying socket buffer \u2014 you'll get the error.\nAlso doc says that both recv and send may cause both SSLWantReadError or SSLWantWriteError, but pika now handles only one.. @vitaly-krugl because distutils installs only packages listed under packages keyword in setup.py. compat.py was a part of pika package, but compat/__init__.py is a new pika.compat package, different than just pika.\nIf you add new [sub]packages, you must list it in setup.py. Or use http://setuptools.readthedocs.io/en/latest/setuptools.html#using-find-packages. Yes, sure, like this:\npython\nsetuptools.setup(\n    \u2026\n    packages=['pika', 'pika.adapters', 'pika.compat'],\n    \u2026\n)\nor even better (autodiscovery):\n```python\nfrom setuptools import find_packages\nsetuptools.setup(\n    \u2026\n    packages=find_packages(exclude=['.tests', '.tests.', 'tests.', 'tests']),\n    \u2026\n)\n```. ",
    "toabctl": "Hm. To eary in the morning. The tags are there. Sorry for the noise.\n. ",
    "Caligatio": "So someone already did the legwork (https://github.com/hollobon/pika-python3) to get Python3 working with Pika as of 0.9.8  I am willing to catch it up to 0.9.13 if it would somehow be used.  Is there no interest in getting Pika 0.9.X working with Python3?\n. ",
    "stleon": "Yeah, when?\n. ",
    "alxpy": "we are looking forward to the support of python 3 =)\n. ",
    "poncki": "Any news about python3 compatibility? \n. ",
    "opyate": "@gmr \n\nThere is interest and a roadmap to get there. The pika-python3 port doesn't follow the same direction of pulling out the core AMQP marshaling and demarshaling and moving it to the pamqp library (https://github.com/gmr/pamqp).\n\nJust checking if there's a pika branch which uses pamqp which we can contribute to, or if pamqp is somehow meant to replace pika in another way?\n. @ztane from my understanding, nothing if the presumed upgrade path is a brute-force conversion (which it is not -- see @gmr's comment).\n. ",
    "ztane": "what exactly there is that is so difficult to make Python 2.6+/Python 3.4 polyglot?\n. This is exactly the same kind of bs that kept boto from supporting Python 3, as \"botocore and boto 3 would come really soon now.\n. I am almost done with part 3 for the git tip (took 3 hours) there was really no futurize needed, yet there is still some confusion with\na) how long is handled,\nb) how the field names are actually supposed to be handled\nI made a compat module for the case a) and added a class long(int) so that the code can deduce whenever to use long serialization and whenever to use int serialization; for the case b) the thing is that it is not very clear from especially the tests which strings are supposed to genuinely be bytes and which should be text; the pitfall here is that some bytestrings are used as dictionary keys etc, and these would not be matched with Python 3 str (whereas they'd work as is with python 2 unicode/str if they just were ascii).\nAs for breaking Py2, every single test is still happily passing on Py 2.7 (and presumably 2.6, as changes were also 2.6 compatible, though I didn't test them yet). I will upload my branch shortly to Github.\n. The truth is I do not really know anything about AMQP, so I am not even sure if the int/long distinction needs to be there; e.g. must some numbers even how small be encoded into 'l' and not 'I', or only those which do not fit in 4 bytes; I assumed the former, based on how the test and serialization code was written, but if the latter is the case, the code could be streamlined a bit.\n. the problem is there is still some unicode/bytes confusion in the code, it is outputting some \"b''\" stuff in acceptance tests @Conan-Kudo.\n. @Conan-Kudo published here, we have now 3 devs sorting out the remaining issues - the issues really are only about \"this should be byte data\" vs \"this should be text data\", because pika seems to throw them both very carelessly and encoding something as needed to one or another.\n. Pull request for the new flat patch, the changes are quite nicely viewable at the GitHub Pull request files changed tab\n. It seems that the tox.ini is not used by the travis; need to install ordereddict for the Python 2.6 test\n. https://github.com/pika/pika/pull/569 here as a completely flat commit\n. @analytik good point, seems that there is no test to even cover that if. \n. I made a flat commit of this into https://github.com/pika/pika/pull/569 - and fixed the issue by @analytik there.\n. @vitaly-krugl ah closed\n. Yeah, I am not too sure about it; it can very well be a problem with this patch. Though I'd consider the fact that the first message gets through a great success as such ;)\n. No assumptions, could very well still be a problem with the patch itself :D\n. @gmr the latest merges to the pika master broke something (did not merge cleanly); we'd try to sort them out now in the following hours.\n. @vitaly-krugl the only problem is that the merge is not successful. When the merge is successful and there is no regression to Python 2 (everything does not need to work in Python 3 on all platforms yet, though we'll ofc make sure that the Python 3 tests passes on whatever platform we can try before merging) then IMHO the pull request should be merged, we also have other things to do besides trying to keep the PR up-to-date.\nIt is easier to squash the bugs against master then.\n. Superseded by a new\u00a0pull request\n. Isn't this the old Python 3 compatibility library, not the new Python 3 integration?\n. You could try running against the standard pika Git HEAD, aka this repository\n. it is surprising that the test suite actually does not test these.\n. Ironically this bug was introduced by @wjps in an effort to make the SelectPoller to actually work\n. 604 is still buggy, needs fix :D\n. I guess one should be really using EnvironmentError which is the superclass of all errno errors. Except of course in Python 2.6, 2.7 the select.error is not a subclass of EnvironmentError nor IOError, and might not even have .errno... though CPython indeed does set it to have .errno always I guess...\n. Welcome to hell. Python 2:\n```\n\n\n\nimport select\ntry:\n...     select.select([42], [], [], 1)\n... except select.error as e:\n...     pass\n... \ne\nerror(9, 'Bad file descriptor')\ne.errno\nTraceback (most recent call last):\n  File \"\", line 1, in \nAttributeError: 'error' object has no attribute 'errno'\n```\n\n\n\nThe select.error will be created in Python 2.6, 2.7 with arguments errno, message, but there is no errno property; instead the errno value will be in args[0], and this should be compared against the EINTR.\n. get_hex() is the internal method for hex property. It was removed in Python 3\n. Add OrderedDict support for Python 2.6 to make the one test fully compatible with all 5 Python flavors supported.\n. This was using keys() needlessly for key in dict test.\n. no % formatting for bytes in Python 3.0-3.4 (Python 3.5 has it)\n. This way of accessing the components was removed in Python 3.\n. The 1-slice here is to work around the difference that in Python 2 indexing a single character in str still yields str; in Python 3, indexing a single byte yields an integer, that is the value of the byte. But 1-slice of bytes is still bytes in Python 3\n. test for the canonical str which in Python 2 is an str in Python 3, also an str\n. The encoded table contents are reordered to match the order of insertion to the OrderedDict\n. With the change to OrderedDict this method does not fail on PyPy.\n. ",
    "Conan-Kudo": "@ztane Have you published your code yet?\n. @gmr Is there an issue where this is being tracked?\n. ",
    "robertlagrant": "@ztane thanks for cleaning up the code and converting it! @bkjones have you had a chance to test?\n. @hayd okay, that's cool. Just saw that pika was still Python 2-only on Pypi, so I thought it wasn't finalised.\n. ",
    "flying-sheep": "yeah, updating PyPI seems to be the only thing left yay!\n. ",
    "alswl": "Jython cannot use BlockingConnection, it will raise exception\nself.poller.register(self.fd, self.poll_events)\n  File \"/duitang/dist/sys/jython/Lib/select.py\", line 108, in register\n    raise _map_exception(jlx)\nselect.error: (20000, 'socket must be in non-blocking mode')\nI'm try to figure it out.\n. And you should see this https://github.com/pika/pika/issues/67\n. Thanks very much, it solved my problem.\n. ",
    "jimbaker": "Although commenting on closed bugs may be a bad idea, it seems better in this case to provide some completeness. So with that in mind, I just opened a bug in Jython for the missing socket.SOL_TCP support (which I got some useful details about by reviewing this specific issue.) This does not change the fact that it makes more sense to use socket.IPPROTO_TCP.\nPlease note that we completely revamped socket support as of Jython 2.7.0, with it now being based on Netty. So far it's looking pretty good, but like any rewrites there is the possibility of regressions. Feel free to open bugs as they come up at http://bugs.jython.org.\n. ",
    "marcoavalero": "Confirmed. Just tested it\n. ",
    "kivsiak": "+1  Same situation there - quite hard to debug. At least you it should wrap existing exception\n. ",
    "kmelnikov": "rabbitmq_server-2.8.6 \nThe reason I think the problem is in pika, because BlockingConnection is working fine.\nDo you think the upgrade of Rabbit could be useful?\n. Ok. I\u00b4ll update Rabbit today to check if the problem goes away =)\n. After the upgrade the issue is gone. \n. Experience the similar problem with pika==0.9.13. \nRabbitMQ 3.2.3, Erlang R14B04\nUpdate to the latest master didn't help. Is there some work around?\nStacktrace:\nFile \"C:\\Users\\kmelnikov\\Projects\\ROCS2\\rocs2\\common\\broker.py\", line 154, in set_up_queue_for_response\n    self._channel.queue_declare(queue=back_q_name, exclusive=True, durable=False)).method.queue\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 867, in queue_declare\n    None, replies)\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 1104, in _rpc\n    self._wait_on_response(method_frame))\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 1121, in _send_method\n    self.connection.send_method(self.channel_number, method_frame, content)\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 249, in send_method\n    self._send_method(channel_number, method_frame, content)\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\connection.py\", line 1489, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 388, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\connection.py\", line 1476, in _send_frame\n    self._flush_outbound()\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 348, in _flush_outbound\n    if self._handle_write():\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\base_connection.py\", line 338, in _handle_write\n    return self._handle_error(error)\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\base_connection.py\", line 282, in _handle_error\n    self._handle_disconnect()\n  File \"C:\\PL\\Python27\\lib\\site-packages\\pika\\adapters\\blocking_connection.py\", line 318, in _handle_disconnect\n    self.disconnect()\nAttributeError: 'BlockingConnection' object has no attribute 'disconnect'\n. Any clues for possible roots of the error? \n. The problem was with the hartbits. It was set to 5 and after a while the channel was closed automatically. After removing this parameter the problem gone.\n. ",
    "y4n9squared": "If you are not using a custom ioloop, you can just remove the stop and start commands since TornadoConnection will simply use tornado.ioloop.IOLoop.instance(). Calling Tornado's ioloop.stop and ioloop.start does not have the same effect as a \"resume\" and will cause the process to terminate as you described.\n. ",
    "simmel": "That works, thanks @y4n9squared!\nI remember that there was a special case where it would reconnect \"clients\" at the same time though (from the same consumer.py). But I can't reproduce that now, so I guess this works.\nHow would I do it if I wanted to use my own ioloop? E.g. for doing stuff async in the consumer.\n. Sweet @vitaly-krugl. I can't test it I'm afraid, but I'll take your word for it.\n. ",
    "timxxx": "I should have noted, server is rabbitmq-server latest release (3.1.4) and both server and client are running on ubuntu server 12.04, python 2.7.3.\n. ",
    "vibragiel": "I'm having the same issue. This is also happening with SelectConnections. It fails depending on the body size. This snippet of code fails to deliver the message:\n``` python\nimport pika\ndef on_open(connection):\nconnection.channel(on_channel_open)\n\ndef on_channel_open(channel):\nchannel.confirm_delivery()\nchannel.basic_publish(exchange='', routing_key=\"my_queue\", body=\"x\"*14473,\n                      properties=pika.BasicProperties(delivery_mode=1),\n                      mandatory=True)\nconnection.close()\n\nparameters = pika.URLParameters(\"my_url\")\nconnection = pika.SelectConnection(parameters=parameters,\n                                   on_open_callback=on_open)\ntry:\n    connection.ioloop.start()\nexcept KeyboardInterrupt:\n    connection.close()\n    connection.ioloop.start()\n```\nThe body size (\"x\"*14473) is significative. Any size lower than that does get delivered.\n. ",
    "rvoicilas": "I'm seeing the same issue while trying to upgrade some code from 0.9.5 to 0.9.13\nIs there any fix that went to master for this ? I'm testing with RabbitMQ server 3.1.5\n. Seems it has been fixed on master ... see _handle_disconnect in BlockingConnection\n. @vfilimonov : probably not what you want to hear, but downgrading to 0.9.12 fixed it for me\n. +1 to this - we're seeing this one too with 0.9.13. Also, maybe it would be nice to have some comments around that try: except AttributeError block. I wondered what's the code path that takes you into that situation. Thanks!\n. This is pretty easy to reproduce with:\npython\nimport sys\nfrom pika.exceptions import ConnectionClosed\ntry:\n    raise ConnectionClosed()\nexcept:\n    print('%s' % (sys.exc_info(),)\nAnd a fix would be to change the __repr__ method of the ConnectionClosed exception to something like:\npython\nexception_message = 'The AMQP connection was closed'                                                                                                                                                                                                \nif len(self.args) == 2:\n    return '%s (%s) %s' % (exception_message, self.args[0], self.args[1])                                                                                                                                                        \nelse:\n    return exception_message\n. ",
    "HWiese1980": "Any news about this? Struggling with the same error. \n//edit: appears to be solved in pika-0.9.14p0\n. ",
    "cdelfosse": "Got the same issue there with pika 0.9.13.\nIt happens in our case only when using RabbitMQ behind HAProxy.\nCurrent master (0.9.14p1) fixes it.\n. ",
    "qmor": "C:\\Users\\qmor\\Documents\\Visual Studio 2008\\Projects\\simulator2k14>send.py\nTraceback (most recent call last):\n  File \"C:\\Users\\qmor\\Documents\\Visual Studio 2008\\Projects\\simulator2k14\\send.p\ny\", line 5, in <module>\n    channel=connection.channel()\n  File \"build\\bdist.win32\\egg\\pika\\adapters\\blocking_connection.py\", line 188, i\nn channel\n  File \"build\\bdist.win32\\egg\\pika\\adapters\\blocking_connection.py\", line 453, i\nn __init__\n  File \"build\\bdist.win32\\egg\\pika\\adapters\\blocking_connection.py\", line 836, i\nn open\n  File \"build\\bdist.win32\\egg\\pika\\adapters\\blocking_connection.py\", line 1122,\nin _rpc\n  File \"build\\bdist.win32\\egg\\pika\\adapters\\blocking_connection.py\", line 1142,\nin _send_method\n  File \"build\\bdist.win32\\egg\\pika\\adapters\\blocking_connection.py\", line 233, i\nn process_data_events\n  File \"build\\bdist.win32\\egg\\pika\\adapters\\blocking_connection.py\", line 346, i\nn _handle_read\n  File \"build\\bdist.win32\\egg\\pika\\adapters\\base_connection.py\", line 331, in _h\nandle_read\n  File \"build\\bdist.win32\\egg\\pika\\connection.py\", line 1272, in _on_data_availa\nble\n  File \"build\\bdist.win32\\egg\\pika\\connection.py\", line 1364, in _read_frame\n  File \"build\\bdist.win32\\egg\\pika\\frame.py\", line 240, in decode_frame\n  File \"build\\bdist.win32\\egg\\pika\\spec.py\", line 549, in decode\n  File \"C:\\Python27\\lib\\encodings\\utf_8.py\", line 16, in decode\n    return codecs.utf_8_decode(input, errors, True)\nUnicodeDecodeError: 'utf8' codec can't decode byte 0xfa in position 0: invalid s\ntart byte\n. It looks like the problem is gone after i've commented line 549 in pika\\spec.py.\n. ",
    "kjbeeman": "I realize this thread is several months old, but just want to add that I too am seeing this issue with pika connecting to qpid-broker-0.16.  (version shipping with Kaazing)\nFor now I've commented out the offending line. \n. ",
    "qfan": "The same test script also reports event 25, no error object, on CentOS 6.4. pika version 0.9.12\n. ",
    "stanBienaives": "Hi @mfyang, \nI'm having the same problems with the ruby client bunny. \nDid you find what causes thoses 505 errors. \nThanks for you help. \n. ",
    "bjoernhaeuser": "Any chance to get a bugfix-release for this? :)\n. Is there a plan to release 0.9.14 any time soon?\n. The connection pushes several messages/sec to rabbit.\n. Which code parts would normally call this method? Should I do this? Or just handle the exception?\n. Hi,\nI added the code which calls \"connection.process_data_events\" every 100 messages. We are still getting these errors.\nThanks\n. Additionaly this is in the log:\n=ERROR REPORT==== 15-Mar-2014::08:49:42 ===\nAMQP connection <0.29128.89> (running), channel 1 - error:\n{amqp_error,unexpected_frame,\n            \"expected method frame, got non method frame instead\",none}\n. 0.9.13\n. What should have changed between 0.9.13 and current master?\n. The most likely thing is that rabbitmq closes the connection because flow controls kick in. You can see that in the web management interface by looking at the connections tab. Rabbit does not keep historical data for this.\n. That also could be an issue. Are you calling, as suggested, process_data_events() every now and than?\nWe enabled publisher confirmations and we also switched to an AsnyConnection. We are currently able to publish 3 million messages in just one connection.\n. I think you wont be able to use a BlockingConnection and those long waiting times, without manually house keeping. The BlockingConnection is not able to handle timeouts / connection closed stuff by itself, because it blocks on your / the next operation. \nYou could use a AsyncConnection and an appropiate heartbeat interval to handle the ConnectionClosed exceptions. In your \"normal\" code you could wait on some condition to be fulfilled, e.g. that an object arrived in an internal python queue. \nYour application code could look just like this:\npython\ndef foo(self):\n    self.queue.put(message)\n    ret = self.answer_queue.get(True, 60)\nThe code which wraps the pika Connection than could check every x-seconds that some obj arrived in the queue and publish it to rabbitmq. \n. With 0.9.14 and a SelectConnection we dont experience this issue anymore. Thanks!\n. ",
    "mark-adams": "It looks like this was fixed with https://github.com/pika/pika/issues/351 but is waiting on the 0.9.14 release.\n. @gmr Surely a hack with one less bug is more desirable than a hack with one more bug, right? I'm still experiencing a production issue with this and its becoming problematic to manually patch the files with each deploymetn.\n. ",
    "vfilimonov": "While there's no fix yet: could you please suggest any workaround of the bug?\nI'm using BlockingConnection and basic_consume method with ACK. Because of this 'AttributeError' when processing of the message takes a while, connection is dropped and consuming function die with exception and can not acknowledge message, which results in dispatching the same messages again and again to new consumers.\n. @rvoicilas thanks for the suggestion! \nAfter downgrading instead of this exception I have\npika.exceptions.ChannelClosed: (0, 'Not specified')\nException pika.exceptions.ChannelClosed: The channel was remotely closed (0) Not specified in <bound method RabbitStore.__del__ of <batch.RabbitStore object at 0x2b7b7d021c90>> ignored\nat the exactly same line as former AttributeError: 'BlockingConnection' object has no attribute 'disconnect'\nThis makes me think that it is not the issue of pika library, but server closes connection that does not respond for a while (idle time is about 1 hour). In RabbitMQ logs these connections are marked as\n=ERROR REPORT==== 2-Jan-2014::22:21:55 ===\nclosing AMQP connection <0.18899.2> (x.x.x.x:50538 -> x.x.x.x:5672):\n{heartbeat_timeout,running}\ntcpdump also showed no communication between server and client while processing (i.e. no heartbeats send)\nSo some questions about heartbeats (sorry, I did not find it in docs):\n- does pika sends them automatically?\n- when does it send them? (perhaps, after calling callback for consumer?)\n- can I send them manually inside time-consuming callback method?\n. Just a quick follow-up: are plans still to release 0.9.14 as a bugfix soon and major 0.10 later?\n. ",
    "mateo41": "Hi folks,\nI'm also seeing this exception.  Are there plans to release version 0.9.14 anytime soon? Thanks!\n. Hello,\nI appreciate all of the work you have done with the pika library. I am very interested in a new release that fixes the BlockingConnection issue. If there is anything that I can do that will enable you to release 0.9.14(or 0.10.0) sooner, please let me know. \n. ",
    "telsacolton": "I'm experiencing a similar problem on Master, where if I send a message a ConnectionClosed exception gets thrown.  If I catch that and re-establish my connection as a pika.BlockingConnection(), somehow another ConnectionClosed gets thrown.  I have to hide part of my traceback for confidentiality, but I'm calling pika.BlockingConnection() in my self.connect method.\nself.connect()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 568, in basic_publish\n    (properties, body), False)\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 1155, in _send_method\n    self.connection.send_method(self.channel_number, method_frame, content)\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 274, in send_method\n    self._send_method(channel_number, method_frame, content)\n  File \"/usr/lib/python2.6/site-packages/pika/connection.py\", line 1507, in _send_method\n    self._send_frame(frame.Header(channel_number, length, content[0]))\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 418, in _send_frame\n    self.process_data_events()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 243, in process_data_events\n    raise exceptions.ConnectionClosed()\nConnectionClosed\n. Hi,\nWe've decided to fork Pika as we deal with this issue.  The branch I am working in is called long_running_dc, you can view it here:  https://github.com/telsacolton/pika/tree/long_running_dc   currently the only change is to comment out some lines so that blocking_connection won't swallow part of the traceback.\n```\ndef process_data_events(self):\n    \"\"\"Will make sure that data events are processed. Your app can\n    block on this method.\n\"\"\"\ntry:\n    if self._handle_read():\n        self._socket_timeouts = 0\n#Don't swallow this Exception so that we can see what's really happening\n#except AttributeError:\n#    raise exceptions.ConnectionClosed()\nexcept socket.timeout:\n    self._handle_timeout()\nself._flush_outbound()\nself.process_timeouts()\n\n```\n. After my change, an even weirder traceback:\nself.connect()\nFile \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 568, in basic_publish\n    else:\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 1155, in _send_method\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 274, in send_method\n    \"\"\"\n  File \"/usr/lib/python2.6/site-packages/pika/connection.py\", line 1516, in _send_method\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 418, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 243, in process_data_events\n    #except AttributeError:\nConnectionClosed\n. We're not publishing enough messages for flow control to be activated. I believe it's an issue with heartbeats not being sent by Pika that causes the connection to close and I'm not sure why it's failing when I try to establish a new connection, like in the traceback above.\n. I'm not calling process_data_events manually, I can't because I'm blocking waiting to consume (I consume a message and then send one with my processed result). I don't process messages very often and will sometimes not see any for 20 or 30 minutes, so calling every X messages isn't an option.  I would be OK with catching the first exception about the connection being closed and re-establishing one, except that doesn't work for whatever reason.  I supposed I could re-factor my whole thing to call basic_get instead of blocking in basic_consume, and then call process_data_events every so often.\n. ",
    "TeddyHartanto": "I had some tests that I want to run. Everything is fine when I run it in my localhost. But, when I run the tests in a server, I had exactly the same problem and situation as this one. I checked my pika version, it's 0.9.13. I'm using BlockingConnection too, by the way. \nFirst, I tried to call process_data_events() periodically. It didn't work.\nSecond, I tried to open a new connection and channel whenever I publish a message or open a new queue. It didn't work.\nThird, I tried to upgrade pika to 0.9.14. It didn't work.\nSomebody on the other thread here: https://github.com/pika/pika/issues/397 mentioned the possibility of a socket bug. So, I checked the python version, hypothesizing that maybe there IS a bug, and it was fixed in the later version of python. In the server, the python version is 2.7.3, and in my localhost it's 2.7.12. To test if python version is really the problem, I installed conda and created an environment with python version 2.7.3. I ran the tests and it passed (I can't reproduce the issue).\nAfter all of the above attempts, I came up with another hypothesis that maybe it's a bug in my server's rabbitmq-server. I compared the versions: in localhost it's the latest (3.6.5), in my server it's 2.8.4. To verify that this is the actual problem, I ran the tests, but I used a remote rabbitmq with a later version. Everything worked fine. So, I upgraded the rabbitmq-server and, lo and behold, the problem disappears!\nTL;DR:\nThe solution is to upgrade your rabbitmq-server!\n. I had some tests that I want to run. Everything is fine when I run it in my localhost. But, when I run the tests in a server, I had exactly the same problem and situation as this one. I checked my pika version, it's 0.9.13. I'm using BlockingConnection too, by the way. \nFirst, I tried to call process_data_events() periodically. It didn't work.\nSecond, I tried to open a new connection and channel whenever I publish a message or open a new queue. It didn't work.\nThird, I tried to upgrade pika to 0.9.14. It didn't work.\nSomebody on the other thread here: https://github.com/pika/pika/issues/397 mentioned the possibility of a socket bug. So, I checked the python version, hypothesizing that maybe there IS a bug, and it was fixed in the later version of python. In the server, the python version is 2.7.3, and in my localhost it's 2.7.12. To test if python version is really the problem, I installed conda and created an environment with python version 2.7.3. I ran the tests and it passed (I can't reproduce the issue).\nAfter all of the above attempts, I came up with another hypothesis that maybe it's a bug in my server's rabbitmq-server. I compared the versions: in localhost it's the latest (3.6.5), in my server it's 2.8.4. To verify that this is the actual problem, I ran the tests, but I used a remote rabbitmq with a later version. Everything worked fine. So, I upgraded the rabbitmq-server and, lo and behold, the problem disappears!\nTL;DR:\nThe solution is to upgrade your rabbitmq-server!\n. ",
    "eschnou": "It crashes after ~300 attempts. Since I attempts at 5 seconds delay this means ~30 minutes. My current fix is simply to increase the delay but that ain't a real fix :-)\nWhat is the reason behind stopping the poller when connection is stopped?\n. ",
    "badray": "Hey!\nI didn't notice, but i was accessing a channel in another thread - according to documentation i shouldn't :), so I think we could close the issue.\n. ",
    "vsudilov": "Dealt with this yesterday and came to the same conclusion. I suspect that the issue is clear enough, but here's some code that reproduces this bug:\n```\nimport pika\nclass GenericWorker:\n  def init(self,url='amqp://admin:password@localhost:5672/%2F'):\n    self.connection = pika.BlockingConnection(pika.URLParameters(url))\n    self.channel = self.connection.channel()\n    self.channel.exchange_declare(exchange='TestExchange',exchange_type='direct',passive=False,durable=False)\n    self.channel.queue_declare(queue='TestQueue')\n    self.channel.queue_bind(queue='TestQueue',exchange='TestExchange',routing_key='TestQueueRoute')\ndef run(self):\n    self.channel.basic_consume(self.callback,queue='TestQueue')\n    self.channel.start_consuming()\ndef callback(self, channel, method_frame, header_frame, body):\n    print \"Entered Callback with body=\", body\n    body.foobar\n    print \"Exited Callback gracefully\"\ndef publish(self,payload):\n    self.channel.basic_publish('TestExchange','TestQueueRoute',payload)\nW = GenericWorker()\nW.publish('fubar') #Message is in the queue\nprints \"Entered Callback with body= fubar\", and then blocks indefinitely.\nW.run()\n```\n. ",
    "thisloginalsotaken": "it is not MAXINT, channel number is passed to server as short, so is is\ncapped at 2_16.\ni have a situation where i have a running server that sometimes open new\nchannels. and sometimes closes old channels. closed channels typically have\nlower channel numbers. so after some time next proposed channel number\nexceeds 65535 and I_ get an error, event if a number of simultaneously\nopened channels is relatively small.\nso suppose I opened channel number 1, then number 2, then number 3 and then\nclosed number 1\nnext channel number  is going to be 4, even there are only two opened\nchannels. after some time this number grows to 65536 and I get an error.\n2013/10/21 Gavin M. Roy notifications@github.com\n\nWhat is your workflow that you have > MAXINT channels in a single\nconnection? Seems like an odd condition to run into.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/pika/pika/pull/404#issuecomment-26734138\n.\n. \n",
    "skygon": "Thanks for your replay. The issue has been resolved.\nThe fd(file description) of queue msg has been reused by another process which will write huge information into the msg.\nThis will cause the frame too big error.\n\u00d4\u00da 2013-10-22 00:42:08\u00a3\u00ac\"Gavin M. Roy\" notifications@github.com \u00d0\u00b4\u00b5\u00c0\u00a3\u00ba\nWhat version?\n\u00a1\u00aa\nReply to this email directly or view it on GitHub.\n. ",
    "jonhol": "I also have this issue (verified with 0.9.13 and 0.9.12, python 2.7.3 on ubuntu). I believe what happens is that consuming starts immediately when the first consumer is added (before start_consuming() is called), and then blocks waiting for more messages when the messages already on the queue is consumed.\nExample producer:\n``` python\n!/usr/bin/env python\nimport pika\nimport sys\ndef producer(number_of_messages):\n    for _ in range(number_of_messages):\n        connection = pika.BlockingConnection(pika.ConnectionParameters(\n                host='localhost'))\n        channel = connection.channel()\n        channel.queue_declare(queue='test')\n        channel.basic_publish(exchange='',\n                              routing_key='test',\n                              body='Hello World!')\nif name == 'main':\n    if len(sys.argv) != 2:\n        print (\"Usage: %s \" % sys.argv[1])\n        exit(1)\n    print (\"Producer. Pika version: %s\" % pika.version)\n    number_of_messages = int(sys.argv[1])\n    producer(number_of_messages)\n```\nConsumer:\n``` python\n!/usr/bin/env python\nimport pika\nimport sys\ndef callback(channel, method, properties, body):\n    print \" [x] Received %r\" % (body,)\n    channel.basic_ack(delivery_tag = method.delivery_tag)\ndef consumer():\n    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\n    channel = connection.channel()\n    channel.queue_declare(queue='test')\n    channel.basic_consume(callback,\n                          queue='test',\n                          no_ack=False)\n    print (\"Not reached\")\nif name == 'main':\n    print (\"Consumer. Pika version: %s\" % pika.version)\n    consumer()\n```\nIf we first run the producer and then the consumer we see that the line \npython\n    print (\"Not reached\")\nis not reached:\n<whatever>$ ./producer.py 5\nProducer. Pika version: 0.9.13\n<whatever>$ ./consumer.py \nConsumer. Pika version: 0.9.13\n [x] Received 'Hello World!'\n [x] Received 'Hello World!'\n [x] Received 'Hello World!'\n [x] Received 'Hello World!'\n [x] Received 'Hello World!'\n. ",
    "maziyarpanahi": "I started to see if I can listen to multiple queues and I notice the exact same situation. If there are messages in those queues only one will be used and the other not at all. But if the publisher starts after the receiver has been running it's all good.\nIs there any new information about this?\nThanks\n. ",
    "joekarl": "I'm seeing the same thing, even when specifying heartbeat_interval in my connection params.\n. I should have mentioned that yes, this is in a BlockingConnection.\n. How does pika handle a confirm_delivery message when a connection fails in the middle of a publish? Does it resend the message when the connection comes back? If the connection is gone gone (ie, won't ever come back) is there a timeout exception thrown or something?\n. Ended up implementing with confirm_delivery which will at least give feedback if the message didn't make it to the broker. \nOne other question, does heartbeat_interval work as documented for blocking connections? From what I'm seeing, if I do the following, as far as I can tell my blocking connection doesn't heartbeat and gets closed. Thoughts?\ncon = pika.BlockingConnection({\n    'heartbeat_interval': 5,\n    'other connection params...': ...\n})\nch = con.channel()\ntime.sleep(30)\nch.basic_publish(...) # disconnected when I reach here\n. Ok. So this is essentially the same issue as #397. If I don't call process data events then heartbeats don't get replied to and the connection gets killed by rabbitmq. \nSo I know there's a big warning about not using a connection from multiple threads. Is that still an issue if I have a background thread that is calling process events every so often? \nKarl\nSent from my iPhone\n\nOn Feb 27, 2014, at 4:14 AM, Erik Olof Gunnar Andersson notifications@github.com wrote:\nI don't know exactly how heartbeat behaves, but I think the problem here is that you are not calling process_data_events. As the IO handling is not handled Async, it wont handle the heartbeats properly.\nTry calling con.process_data_events() every 5 seconds or so.\n\u2014\nReply to this email directly or view it on GitHub.\n. Sounds good. Yeah to complicate things, this is all stuck inside of a flask app trying to manage resources amongst wsgi threads. Right now I'm pooling connections and checking them out per thread, thus avoiding the threading issues, but what I'd really like to do is to pool channels on a single connection. Sounds like to do that I'd need to lock around each channel action (publish, ack, etc) as the connection doesn't synchronize channel events internally... \n\nSent from my iPhone\n\nOn Feb 27, 2014, at 7:47 AM, Erik Olof Gunnar Andersson notifications@github.com wrote:\nYou can create your own IO thread that calls process_data_events() ever so often, but it is important that you add a lock. If you try to publish, as the same time as you are running process_data_events you will run into problems.\nwith self.internal_lock:\n    self.channel.process_data_events()\nwith self.internal_lock:\n    self.channel.basic_publish(..........)\n\u2014\nReply to this email directly or view it on GitHub.\n. I would assume that this would still have issues if one is doing blocking reads (i.e. rpc type things) as the connection would be locked until the read completes (and could timeout if the read takes too long\u2026).\n\nI suppose if I really want to do dynamic stuff like that then I\u2019d really need to look into an async connection.\nKarl\nOn Feb 27, 2014, at 10:51 AM, Erik Olof Gunnar Andersson notifications@github.com wrote:\n\nThat shouldn't be much trouble. As long as you keep a lock between each thread. I simply have one pika connection per wsgi process. Then keep a lock on only the Pika calls. In fact am running something like mentioned above on my flask application.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "modeyang": "@vitaly-krugl  can you give me example for this situation?  using mutli-threads, one for real worker, another for events? \n. ",
    "GWSzeto": "I'm starting to get the same error even when I adjust the heartbeat connection parameter.\nMy tasks take usually around 5 mins, but even then, when I set the heartbeat to both 0 and some very large number (ie: 60000)\nMy code Is essentially identical to the one above and the error I receive is this:\nTraceback (most recent call last):\n  File \"instagram_worker.py\", line 43, in <module>\n    channel.start_consuming()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 1780, in start_consuming\n    self.connection.process_data_events(time_limit=None)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 716, in process_data_events\n    self._dispatch_channel_events()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 518, in _dispatch_channel_events\n    impl_channel._get_cookie()._dispatch_events()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 1403, in _dispatch_events\n    evt.body)\n  File \"instagram_worker.py\", line 37, in dataHandler\n    channel.basic_ack(delivery_tag=method.delivery_tag)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 1988, in basic_ack\n    self._flush_output()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 1250, in _flush_output\n    *waiters)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 474, in _flush_output\n    result.reason_text)\npika.exceptions.ConnectionClosed: (-1, \"error(104, 'Connection reset by peer')\")\n . I'm using version 0.11.2\nAnd after reading your comment from the other issue, I just wanted to clarify if it is like this example from the pika Doumentation http://pika.readthedocs.io/en/0.10.0/examples/connecting_async.html. Since I am fairly new to this, correct me if I am wrong, but from what I understand, the IO Loop is what handles taking in tasks and publishing them into the event loop\nIf what I said above is true, then I have my publisher and my worker in 2 separate files, where I just ran the publisher once and published all the tasks to the queue and had it stop completely.\nThen I had my worker go and consume from the queue after. I'm going to post my code here just in case it's something external to pika,\n```\nfrom google.cloud import bigquery\nimport pandas as pd\nimport pika\nimport re\nimport json\nfrom unidecode import unidecode\nfrom time import sleep\nclient = bigquery.Client(project=\"some_project\")\nparams = pika.ConnectionParameters(host='localhost', heartbeat=600000)\nconnection = pika.BlockingConnection(params)\nchannel = connection.channel()\nchannel.queue_declare(queue=\"some_queue\", durable=True)\ndef dataHandler(channel, method, properties, body):\n    body = json.loads(body)\n    csvURL = body[\"csvURL\"]\n    tablename = body[\"tablename\"]\n    print(\"recieved csv batch for %s\" % tablename)\ndf = pd.read_csv(csvURL, encoding=\"utf-8\")\nprint(\"read in the csv\")\ncolumns = list(df)\nascii_only_name = [unidecode(name) for name in columns]\ncleaned_column_names = [re.sub(\"[^a-zA-Z0-9_ ]\", \"\", name) for name in ascii_only_name]\nunderscored_names = [name.replace(\" \", \"_\") for name in cleaned_column_names]\nvalid_gbq_tablename = \"test.\" + tablename\ndf.columns = underscored_names\n\n# try:\ndf.to_gbq(valid_gbq_tablename, \"some_project\", if_exists=\"append\", verbose=True, chunksize=10000)\n# print(\"Finished Exporting\")\n# except Exception as error:\n#     print(\"unable to export due to: \")\n#     print(error)\n#     print()\nchannel.basic_ack(delivery_tag=method.delivery_tag)\n\nchannel.basic_qos(prefetch_count=1)\nchannel.basic_consume(dataHandler, queue=\"some_queue\")\ntry:\n    channel.start_consuming()\nexcept KeyboardInterrupt:\n    channel.stop_consuming()\nchannel.close()\n```. It works now, thank you so much for your help. ",
    "marxin": "Error:\n(505, 'UNEXPECTED_FRAME - expected content header for class 60, got non content header frame instead')\n(501, 'FRAME_ERROR - type 143, all octets = <<>>: {frame_too_large,2379267024,131064}')\n. ",
    "vuori777": "_handle_write in base_connection.py in 0.9.13 never checks that the whole frame was sent. Looks like it has been fixed in git (0.9.14).\n. ",
    "JuhaS": "Any hints on the next release? Or if the HEAD is stable?\n0.9.13 is also missing #371.\n. ",
    "hangtwenty": ":+1: \n. Yes, please! Anything we can do?\n. ",
    "itsadok": "At the time of this writing, the master branch is at 0.9.14p1, and the version on pypi is still 0.9.13. I think this issue was closed prematurely.\nIs there anything we can do to help push the version out?\n. ",
    "fantamp": "Yes, please! We want 0.9.14 very much\nThanx!\n. ",
    "dangra": "+1 to 0.9.14\n. ",
    "skillflip": "Until this is added you can do it by directly manipulating the right member variable (not nice, but it works):\n```\nconnection = pika.BlockingConnection(pika.ConnectionParameters(self.host))\nchannel = connection.channel()\nBug workaround - https://github.com/pika/pika/issues/428\nchannel.frame_dispatcher.force_binary = True\n```\n. ",
    "dmoore19": "My bad.  Total user error.  Case closed.\n. ",
    "tiagoboldt": "@eandersson that's the sole purpose of unit testing. Big companies (facebook, twitter, github, etc) deploy to production dozens of times per day using such philosophy. If git is correctly used, with features developed in branches, merge to master based on reviewed pull requests that pass all tests, that \"problem\" disappears. \nI accept your argument, though, as I am not aware of the test coverage in pika and the practices of its collaborators; although, if that is an issue, both could also be considered for improvement, resolving in an increased quality of the project.\n. ",
    "corford": "Re-opening as I'm getting the same IndexError if I purposefully use invalid credentials to connect to the server (this is with a BlockingConnection, haven't tried it with anything else).\nThe ConnectionClosed exception is triggered but this is what's raised:\npython\nNo handlers could be found for logger \"pika.adapters.base_connection\"\nTraceback (most recent call last):\n  File \"/srv/python/2.7.6/lib/python2.7/logging/__init__.py\", line 851, in emit\n    msg = self.format(record)\n  File \"/srv/python/2.7.6/lib/python2.7/logging/__init__.py\", line 724, in format\n    return fmt.format(record)\n  File \"/srv/python/2.7.6/lib/python2.7/logging/__init__.py\", line 464, in format\n    record.message = record.getMessage()\n  File \"/srv/python/2.7.6/lib/python2.7/logging/__init__.py\", line 328, in getMessage\n    msg = msg % self.args\n  File \"/home/corford/venvs/lchargent/lchargent_ci_worker/lib/python2.7/site-packages/pika/exceptions.py\", line 53, in __repr__\n    return 'The AMQP connection was closed (%s) %s' % (self.args[0],\nIndexError: tuple index out of range\nI'm using sys.exc_info() to pull the exception info\n. ",
    "Sesshomurai": "I have this problem too. Let us know when its fixed. It's a showstopper. Thank you!\nNot sure if the above hack would introduce a memory leak situation since the disconnect command is not being issued correctly.\n. Here is my stack trace if it helps any.\n[2014-02-10 17:16:25,189: ERROR/Worker-20] Socket Error on fd 9: 32\n[2014-02-10 17:16:25,190: WARNING/Worker-20] Traceback (most recent call last):\n.....\nFile \"/home/ubuntu/software/code/gridwave/matrix/bundles/workflow/celery/init.py\", line 920, in construct\nself.workflow_broker.publish(wfdoc)\nFile \"/home/ubuntu/software/code/gridwave/matrix/bundles/messaging/rabbitmq/init.py\", line 131, in publish\nself.channel.basic_publish(exchange=self.exchange,routing_key=self.key,body=message)\nFile \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 540, in basic_publish\n(properties, body), False)\nFile \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 1121, in _send_method\nself.connection.send_method(self.channel_number, method_frame, content)\nFile \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 249, in send_method\nself._send_method(channel_number, method_frame, content)\nFile \"/usr/local/lib/python2.7/dist-packages/pika/connection.py\", line 1489, in _send_method\nself._send_frame(frame.Method(channel_number, method_frame))\nFile \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 388, in _send_frame\nsuper(BlockingConnection, self)._send_frame(frame_value)\nFile \"/usr/local/lib/python2.7/dist-packages/pika/connection.py\", line 1476, in _send_frame\nself._flush_outbound()\nFile \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 348, in _flush_outbound\nif self._handle_write():\nFile \"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\", line 338, in _handle_write\nreturn self._handle_error(error)\nFile \"/usr/local/lib/python2.7/dist-packages/pika/adapters/base_connection.py\", line 282, in _handle_error\nself._handle_disconnect()\nFile \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 318, in _handle_disconnect\nself.disconnect()\nAttributeError: 'BlockingConnection' object has no attribute 'disconnect'\n. ",
    "cjschroeder": "When can we expect 0.9.14?\n. ",
    "wmiltenburg": "I think it makes more sense to throw an error when you use BlockingConnection and the socket closes. In my case, I want to be sure everything is added in an ordered sequence, so that's why I use BlockingConnection. The confirm_delivery solution might work, but it will slow down the overall application when you wait for the confirm_delivery (to be sure everything has been added in an ordered sequence). It would make more sense to not use the confirm_delivery and just throw an error.\n. The recommendation from gmr didn't work (the callback mechanism). Since I use a BlockingConnection (and it isn't possible to add a on_close_callback there) I had to do it on the channel level. So I created the channel added an on_close_callback, but my method was never invoked. So isn't the best solution to throw an error in the case of using BlockingConnection and the socket has been closed?\nThis is the output of my logfile, you don't see the line 'CLOSE_CALLBACK invoked' which you should see if the method has been invoked.\nText\n2014-07-08 12:37:41,764 [pika.connection] [CRITICAL]  Attempted to send frame when closed\n2014-07-08 12:37:41,764 [pika.connection] [CRITICAL]  Attempted to send frame when closed\n2014-07-08 12:37:41,764 [pika.connection] [CRITICAL]  Attempted to send frame when closed\n2014-07-08 12:37:51,765 [pika.adapters.base_connection] [INFO]  Connecting to 192.168.10.2:5672\n2014-07-08 12:37:51,765 [pika.adapters.base_connection] [WARNING]  Connection to 192.168.10.2:5672 failed: [Errno 111] Connection refused\n2014-07-08 12:37:51,766 [pika.adapters.base_connection] [INFO]  Connecting to 192.168.10.2:5672\n2014-07-08 12:37:51,766 [pika.adapters.base_connection] [WARNING]  Connection to 192.168.10.2:5672 failed: [Errno 111] Connection refused\n2014-07-08 12:37:51,766 [pika.adapters.base_connection] [INFO]  Connecting to 192.168.10.2:5672\n2014-07-08 12:37:51,766 [pika.adapters.base_connection] [WARNING]  Connection to 192.168.10.2:5672 failed: [Errno 111] Connection refused\n2014-07-08 12:38:01,767 [pika.adapters.base_connection] [INFO]  Connecting to 192.168.10.2:5672\n2014-07-08 12:38:01,795 [LocalSaveManager] [INFO]  Line to send 3351\n. ",
    "Red-Lv": "What I can say... If the pika module don't handle this properly, then we have to deal with it with ourselves. I don't think pika module is a suitable choice in production enviroment.\n. @gmr \n. Same question?\n. @nrolans I am testing. If really as you said, then the connection thread safety is really a bug in pika.\n. ",
    "purpleP": "Um... Hello? Anyone there?\n. ok\n. ",
    "ajkavanagh": "Yes, I'm wondering this too.\n. Thanks for the update.  Looking forward to both 0.9.14 and 0.10.x.\n. ",
    "techzhou": "plz release 0.9.14\nI got #371 \nsad\n. ",
    "oxtopus": "Great.  Thanks for the update!\nI won't be making PyCon this year, but if you need any help, let me know.\n. ",
    "julianhille": "i would also love to hear what you have planed about the next release.\n. ",
    "kaos": "perhaps close, as it has been released now.. ?\n. ",
    "segedunum": "\nYes, I get that.\nYes, I get that but the performance penalty only comes about in combination with asynchronous publishing and publisher confirms. Perhaps I didn't make that clear?\nThis isn't relevant to my query. I don't want a blocking connection, but I'm querying why the blocking connection is so much faster in this case and doesn't yield such a dramatic drop in performance.\nLogically, this isn't an under-provisioned server problem because this only happens when I turn publisher confirms on with the asynchronous adapter and in combination with HA and delivery mode 2. The blocking connection does not yield this problem with or without HA and with or without delivery mode 2. That's my problem.\n. 1. Provision your IO subsystem accordingly......\n\n...and I'm sorry but I didn't give you any information about my 'IO subsystem' nor is there anything wrong with it regarding a blocking connection.\nThis took a month to reply to?\n. ",
    "wjps": "@Caligatio, this should be fixed following the merge of #578 \n. Having been bitten by this, I wonder why is AttributeError being treated specially here?\nI assume it's trying to catch some specific condition when something hasn't been assigned in BlockingConnection (I'm guessing _read_poller?) In which case isn't the correct fix to detect that specific case and avoid catching all AttributeErrors raised in user code?\n. Yup, it will fix #538 and various other intermittent failures that people who publish large volumes of messages will see. The existing code was basically giving up on a socket.timeout on write when this is actually very possible if you're generating messages faster than the network can send. \n. I would imagine SelectConnection is picking the KQueuePoller on OSX whereas it'll be picking EPoll on Linux and the code is quite different.\nPR #549 contains a fix if you're interested. \n. Thanks @vitaly-krugl. No, unfortunately I've not used python 3 much at all and know nothing about selectors. I'll take a look if I get a moment but personally I'd be reluctant to add a dependency on an external package for 2.x. However the SelectConnection code is actually pretty simple and with a bit more cleaning up will be in reasonable shape. Do you know what the general thoughts are about all the various adapters? There seem to be rather a lot of them and they seem to have quite different behaviours, 'fixes' have obviously been applied to one but not identical code in others. I was going down the path of making them all present a standard ioloop like interface but can't help thinking I might be wasting my time with some of them.\n. @vitaly-krugl, that's great news. That's exactly how I think the BlockingConnection should be written and one of the things I was hoping to get to once I had the async stuff working fully non-blocking with a standardised ioloop interface. \nMy thought was that the blocking connection should be possible to implement on top of any of the async ioloops/connections. It hadn't progressed past an idle thought though, and I'm not even sure whether it's something you'd want to be able do. You're right, the current performance of BlockingConnection is shocking for some things, there's no reason why it shouldn't be comparable to the async adapaters as you've shown.\nI presume you're looking to completely replace BlockingConnection? In which case you'd want to keep the interface backwards compatible. No harm in extending it though.\n. @vitaly-krugl, thanks for the review, been away for the last week but I'll take a look over the next couple of days.\n. replaced with #555\n. @gmr, @vitaly-krugl, my pleasure, thanks for your reviews.\n. Hmm, not sure what's going on there. The tests run fine here, will take a look.\n. @gmr, don't know if you have any ideas? It seems to have failed on py2.6 but I've run the tests on python2.6 over and over and cannot reproduce, I've also now set-up travis on my own repo and that's passed the same code!?\nI suspect it's some timing related issue with the test and may have been triggered by the removal of the _flush_outbound in SelectConnection but, not being able to reproduce, it's hard to say either way. I can kill this PR and re-push but I'd rather not spam you with PRs either. \n. ok there's definitely some sort of race in there, I can reproduce the failure on both master and my PR for both TestZ_PublishAndConsume and TestZ_PublishAndConsumeBig (more easily on the latter).\nSometimes it seems to take several thousand iterations to do so though, loading the machine up does appear to make it more easy to trigger..\n. Ok the problem was with the tests, they were using a value in seconds rather than ms as the x-expires argument to the queue.declare. Sometimes the queue was getting deleted before the rest of the test could run. Fixed in #558\n. @gmr, any thoughts on merging this?\n. sure np.\nOn Mon, 18 May 2015 at 17:38 Gavin M. Roy notifications@github.com wrote:\n\nThere are a lot of commits in this and I'd rather not review the whole\nchain, any chance of a rebase flattening the PR\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/556#issuecomment-103121585.\n. sent a new PR #578\n. @vitaly-krugl, just seen a test failure of one of these tests on one of my travis builds and I think it's more likely the tests than the code. Care to take a look?\n\nFull logs are here\nhttps://travis-ci.org/wjps/pika/jobs/63049021\nStack trace...\n```\nBlockingConnection to downed broker results in AMQPConnectionError ... ok\nBlockingConnection TCP/IP connection loss in CONNECTION_PROTOCOL ... ok\nBlockingConnection TCP/IP connection loss in CONNECTION_START ... ERROR\nBlockingConnection TCP/IP connection loss in CONNECTION_TUNE ... ok\nBlockingConnection reconnect with downed broker ... ok\nBlockingConnection resets properly on TCP/IP drop during channel() ... ok\n======================================================================\nERROR: BlockingConnection TCP/IP connection loss in CONNECTION_START\n\nTraceback (most recent call last):\n  File \"/home/travis/build/wjps/pika/tests/acceptance/blocking_adapter_test.py\", line 155, in start_test\n    self.connection.connect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 234, in connect\n    self._adapter_connect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 310, in _adapter_connect\n    self.process_data_events()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 242, in process_data_events\n    if self._handle_read():\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 358, in _handle_read\n    super(BlockingConnection, self)._handle_read()\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 389, in _handle_read\n    self._on_data_available(data)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1334, in _on_data_available\n    self._process_frame(frame_value)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1401, in _process_frame\n    if self._process_callbacks(frame_value):\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1371, in _process_callbacks\n    frame_value)  # Args\n  File \"/home/travis/build/wjps/pika/pika/callback.py\", line 62, in wrapper\n    return function(tuple(args), kwargs)\n  File \"/home/travis/build/wjps/pika/pika/callback.py\", line 94, in wrapper\n    return function(*args, kwargs)\n  File \"/home/travis/build/wjps/pika/pika/callback.py\", line 238, in process\n    callback(args, **keywords)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1319, in _on_connection_tune\n    self._send_connection_open()\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1504, in _send_connection_open\n    self._on_connection_open, [spec.Connection.OpenOk])\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1488, in _rpc\n    self._send_method(channel_number, method_frame)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1554, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 423, in _send_frame\n    super(BlockingConnection, self)._send_frame(frame_value)\n  File \"/home/travis/build/wjps/pika/pika/connection.py\", line 1539, in _send_frame\n    self._flush_outbound()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 377, in _flush_outbound\n    if self._handle_write():\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 417, in _handle_write\n    return self._handle_error(error)\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 323, in _handle_error\n    self._handle_disconnect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 273, in _handle_disconnect\n    self._adapter_disconnect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/blocking_connection.py\", line 325, in _adapter_disconnect\n    self._check_state_on_disconnect()\n  File \"/home/travis/build/wjps/pika/pika/adapters/base_connection.py\", line 160, in _check_state_on_disconnect\n    raise exceptions.ProbableAccessDeniedError\nProbableAccessDeniedError: \n-------------------- >> begin captured logging << --------------------\n...\n```\n. No it was a one off failure, exactly the same push passed on the official\npika setup minutes later. I suspect there's either another timing related\nissue in there or something environmental. Like an idiot I've just told\ntravis to re-run the tests now, not realising it would trash the old logs,\nsorry. It was getting 'connection refused' on a high port number though if\nthat's any use.\nOn Mon, 18 May 2015 at 19:44 vitaly-krugl notifications@github.com wrote:\n\n@wjps https://github.com/wjps, thanks for the report. Is it\nreproducible on your travis setup? It seems to be passing on the official\npika setup. I see that the tests were running at the latest cd8c9b0\nhttps://github.com/pika/pika/commit/cd8c9b08e8a121acfb8b53344292d7b2ff4eecc6.\nI will take a look after work.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/557#issuecomment-103168345.\n. Having said that I can reproduce it in a few dozen iterations with\n\n$ true; while [ $? == 0 ]; do  nosetests -x -verbosity=3 tests/acceptance/blocking_adapter_test.py; done\n. Hmm, I have ssl.WantReadError in my python 2.7.9,it must have been included\nwhen SSL was backported for POODLE. It's not in < 2.7.9 though.\nOn Tue, 19 May 2015 at 06:39 vitaly-krugl notifications@github.com wrote:\n\n@wjps https://github.com/wjps, it looks like you added\nssl.SSLWantReadError exception handler in the \"write starvation\" PR. It\nturns out that this exception was added in python 3.3, so it's not\ncompatible with 2.x and is causing failures on 2.x:\nPer https://docs.python.org/3/library/ssl.html:\nexception ssl.SSLWantReadError\nA subclass of SSLError raised by a non-blocking SSL socket when trying to read or write data, but more data needs to be received on the underlying TCP transport before the request can be fulfilled.\nNew in version 3.3.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/557#issuecomment-103345405.\n. Hi Seung,\n\nAs a test, could you try wrapping your on_request handler in a try/catch\nand see whether any exception is being thrown whilst processing a message.\nOr try running against git:master.\nThanks\nWill\nOn Thu, 7 May 2015 at 00:49 Seung-Jin Sul notifications@github.com wrote:\n\nHi,\nI'm using 0.9.14 version pika on RabbitMQ 3.1.5. My application has a\nclient which distributes a set of tasks so that many workers process the\ntasks. The issue I've been seeing for a long time is the worker always lose\nit's connection esp. when there are multiple workers are running. Please\nlet me know if this has already been discussed. Here comes the code and the\ntrace.\nThanks,\nSeung\ndef on_request(ch, method, props, body):\n...\nlog.info(\"Return queue name = %s\", props.reply_to)\nch.basic_publish(exchange='',\nrouting_key=props.reply_to,\nproperties=pika.BasicProperties(\ndelivery_mode = 2,\ncorrelation_id = props.correlation_id),\nbody=response)\nlogger.info(\"Sent the result back to the client via '%s' queue.\" % (props.reply_to) )\nch.basic_ack(delivery_tag=method.delivery_tag)\ndef main(argv):\n...\nch.basic_qos(prefetch_count=1)\nch.basic_consume(on_request, queue=taskQueueName, no_ack=False)\nch.start_consuming()\n...\n2015-05-06 12:29:34,029 | tfmq-worker | on_request | DEBUG : Return queue\nname = after_20150201.csv.tfmq.task.lst_RESULT_QUEUE\nTraceback (most recent call last):\nFile \"/usr/common/jgi/utilities/tfmq/dev-v2.2-76-g862bd95/tfmq-worker\",\nline 555, in\nsys.exit(main(sys.argv))\nFile \"/usr/common/jgi/utilities/tfmq/dev-v2.2-76-g862bd95/tfmq-worker\",\nline 545, in main\nch.start_consuming()\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\",\nline 955, in start_consuming\nself.connection.process_data_events()\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\",\nline 240, in process_data_events\nif self._handle_read():\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\",\nline 348, in _handle_read\nsuper(BlockingConnection, self)._handle_read()\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/base_connection.py\",\nline 351, in _handle_read\nself._on_data_available(data)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/connection.py\",\nline 1285, in _on_data_available\nself._process_frame(frame_value)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/connection.py\",\nline 1365, in _process_frame\nself._deliver_frame_to_channel(frame_value)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/connection.py\",\nline 976, in _deliver_frame_to_channel\nreturn self._channels[value.channel_number]._handle_content_frame(value)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/channel.py\",\nline 792, in _handle_content_frame\nself._on_deliver(*response)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/channel.py\",\nline 886, in _on_deliver\nbody)\nFile \"/usr/common/jgi/utilities/tfmq/dev-v2.2-76-g862bd95/tfmq-worker\",\nline 191, in on_request\nbody=response)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\",\nline 572, in basic_publish\n(properties, body), False)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\",\nline 1159, in _send_method\nself.connection.send_method(self.channel_number, method_frame, content)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\",\nline 274, in send_method\nself._send_method(channel_number, method_frame, content)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/connection.py\",\nline 1503, in _send_method\nself._send_frame(frame.Method(channel_number, method_frame))\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\",\nline 417, in _send_frame\nsuper(BlockingConnection, self)._send_frame(frame_value)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/connection.py\",\nline 1490, in _send_frame\nself._flush_outbound()\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\",\nline 377, in _flush_outbound\nif self._handle_write():\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/base_connection.py\",\nline 365, in _handle_write\nreturn self._handle_error(error)\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/base_connection.py\",\nline 302, in _handle_error\nself._handle_disconnect()\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/base_connection.py\",\nline 248, in _handle_disconnect\nself._adapter_disconnect()\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\",\nline 318, in _adapter_disconnect\nself._check_state_on_disconnect()\nFile\n\"/global/dna/projectdirs/PI/qc/people/sulsj/packages/genepool/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\",\nline 371, in _check_state_on_disconnect\nraise exceptions.ConnectionClosed()\npika.exceptions.ConnectionClosed\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/issues/564.\n. Based on the line numbers that looks like it's against master.\n\nA test case would be idea, failing that a bit more of the logs with pika\nlogging at debug might prove useful.\nOn Thu, 7 May 2015 at 22:53 vitaly-krugl notifications@github.com wrote:\n\nNoneType would be from data = self.socket.recv(self._buffer_size) in\n_handle_read. self.socket gets set to None when the connection is closed.\n@sulsj https://github.com/sulsj, the best way to get help is to write a\nsmall, simple program that reproduces your failure and post it here. Also,\nplease wrap your code snippets and tracebacks in github markdown for code:\n3 back-ticks before and 3 back-ticks after.\ndef foo():\ndo_stuff()\n. . .\nMake sure there are no spaces between the back-ticks.\nAlso, does your code work okay against pika master? Please post the\nresults of that, too.\nFinally, is your app sharing the same pika connection among multiple\nthreads?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/issues/564#issuecomment-100031146.\n. Fixed in #578 \n. Do you have a test case? Or do you just think there might be an issue?\n\nioloop.stop() is pushed through to the same method on the poller.\nOn Thu, 7 May 2015 at 09:20 vitaly-krugl notifications@github.com wrote:\n\nCC @wjps https://github.com/wjps\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/issues/566#issuecomment-99767240.\n. @vitaly-krugl, Thanks very much. Yes I don't have access to an OSX box atm,\nwill have to try and sort that out if I'm planning to break some more stuff\nin here.\n\nOn Thu, 7 May 2015 at 11:21 vitaly-krugl notifications@github.com wrote:\n\nI thought I disabled the trailing white space eliminator on my editor, but\nsomehow they still got eliminated, making the diff a little more involved\nthan I intended, but not too bad, so I'm keeping it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/568#issuecomment-99805454.\n. @analytik, if you have a moment could you try with PR#556, the behaviour you're seeing sounds very similar. Thanks.\n. Great news.\n\nOn Mon, 18 May 2015 at 08:43 Milan B notifications@github.com wrote:\n\n@wjps https://github.com/wjps - thanks! On Python 3.4.1, Windows, with\nflatpatch+write-starvation-fixes, both Tornado and Select adapters work as\nexpected. No write buffer warnings anymore. :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/569#issuecomment-102952213.\n. Hi @vitaly-krugl, I'm having problems repeating your issue here against either pika:master or  7f91a68. Does it take many iterations to occur for you?\n\nI'm also slightly confused by the second call to connection.ioloop.start() in onChannelOpen, was that intentional?\n. Ok, got it. You need that recursive call to ioloop.start() to trigger but found at least one bug as a result. The fix looks simple but just want to make sure I understand exactly what's going on.\n. Fixed in PR #577, the code was obviously wrong but it does require quite an specific sequence of events to trigger. Trying to think how to create a simple test case that's not dependent on network timings and the order dictionary keys are returned, but it's hurting my head.\n. Great thanks, you going to close the issue?\n. FWIW I'd like to see this or something similar merged, it would be useful\nto us. I also think the product and version fields should be settable by\nthe application, I'd be inclined to put 'pika' and it's version in a new\nfield.\nOn Fri, 29 May 2015 at 07:44 Mathilde Ffrench notifications@github.com\nwrote:\n\nDear Pika project collaborators,\nAs there may be here a disagreement around RabbitMQ client properties\nusage I would like to know the final status of this pull req (rejected or\naccepted) ? If rejected I will have to maintain a specific fork to fit my\nneeds.\nBest regards,\nMathilde\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/571#issuecomment-106711060.\n. @vitaly-krugl, thanks for that, always nice to see some numbers.\n. Yup, fair enough.\n. The whole connect sequence including SSL handshake is still done with a blocking socket. I have a separate sequence of patches that makes the whole lot non-blocking (which I think is cleaner) but it requires quite a lot of fixing to Select and Asyncore Connections.\n. Will do\n. sure, np\n. Sorry, should have gone when I moved the changes to base_connection\n. I actually benchmarked that bit and couldn't see any appreciable difference, and it made the code a little neater like this.\n. Yeah, you could do, though I'd be surprised if that overhead would really be noticeable compared to everything else we're doing in python. Also if we haven't dropped back into the ioloop in a while the kernel may have cleared some space in the buffer in the meantime, and in cases where we're consuming a full queue of small messages we can go quite a long time before dropping into the ioloop (it's what lead me to take a look at this in the first place). So I'd prefer to keep it as is, might not be the absolutely most efficient but it will keep the output buffer full and avoid stalling the pipeline. \n. @vitaly-krugl, thanks. Though I think a lot of your gotchas will have to have been handled at the python wrapper layer, the interface exposed to python is reasonably straightforward. We'll see how we go.\n. @vitaly-krugl, not sure I agree on that one, it fixes the issue it sets out\nto fix and I don't think it introduces any new ones. It wasn't aiming to\naddress any broader issues in BlockingConnection which are the subject of\nyour PRs.\n\nOn Mon, 13 Apr 2015 at 18:40 vitaly-krugl notifications@github.com wrote:\n\nIn pika/adapters/blocking_connection.py\nhttps://github.com/pika/pika/pull/546#discussion_r28260003:\n\n@@ -344,6 +342,8 @@ def _handle_read(self):\n         keeps track of how many frames have been written since the last read.\n\"\"\"\n-        if not hasattr(self, '_read_poller'):\n-            raise exceptions.ConnectionClosed()\n\n@wjps https://github.com/wjps, this is not the right fix because the\nraised exception would not be consistent with the state of the connection -\ni.e., connection.is_closed would still evaluate to False after this. It's\nthe same problem as with the AttributeError exception handler that this\nPR removed.\nThe issue is reported in #412 https://github.com/pika/pika/issues/412\nand I believe that PRs #533 https://github.com/pika/pika/pull/533 and\n542 https://github.com/pika/pika/pull/542 together address this issue,\nwhile at the same time maintaining consistent connection state. I didn't\nremove the AttributeError block in those PRs, though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/546/files#r28260003.\n. @vitaly-krugl, I think we're talking about different things. I think I've tested your PRs and assuming I've pulled them correctly I don't think they address the issue.\n\nTest case below....\n``` python\nimport pika\nimport logging\nclass PubSub:\ndef init(self, url='amqp://guest:guest@localhost:5672/%2F'):\n    self.conn = pika.BlockingConnection(pika.URLParameters(url))\n    self.chan = self.conn.channel()\n    # works more reliably if we make sure the q is empty\n    self.chan.queue_delete(queue='testq') \n    self.chan.queue_declare(queue='testq')\n    self.chan.queue_bind(queue='testq',exchange='amq.direct',routing_key='test')\n    self.chan.basic_consume(self.callback,queue='testq')\n    self.count = 0\ndef run(self):\n    while True:\n        try:\n            logging.info(\"Start consuming.....\")\n            self.chan.start_consuming()\n    except pika.exceptions.ConnectionClosed:\n        logging.error(\"WTF?!, my connection got closed\")\n        raise\n\n    except Exception as e:\n        logging.error(\"start_consuming raised: %s\", e)\n\ndef callback(self, cha, meth, header, body):\n    if self.count == 0:\n        self.count += 1\n        logging.info(\"In callback, about to generate some generic exception\")\n        a = undefined_var\n    else:\n        logging.info(\"In callback, about to generate an AttributeError\")\n        a = body.some_random_attribute\ndef publish(self, body):\n    self.chan.basic_publish('amq.direct','test', body)\nlogging.basicConfig(level=logging.INFO)\np = PubSub()\np.publish('message1') \np.publish('message2') \np.run()\n```\nWith the broken code, output is...\nINFO:pika.adapters.base_connection:Connecting to ::1:5672\nINFO:root:Start consuming.....\nINFO:root:In callback, about to generate some generic exception\nERROR:root:start_consuming raised: global name 'undefined_var' is not defined\nINFO:root:Start consuming.....\nINFO:root:In callback, about to generate an AttributeError\nERROR:root:WTF?!, my connection got closed\nTraceback (most recent call last):\n  File \"attribute_error_test.py\", line 47, in <module>\n    p.run()\n  File \"attribute_error_test.py\", line 20, in run\n    self.chan.start_consuming()\n  File \"/home/will/git/vitaly-krugl-pika/pika/adapters/blocking_connection.py\", line 965, in start_consuming\n    self.connection.process_data_events()\n  File \"/home/will/git/vitaly-krugl-pika/pika/adapters/blocking_connection.py\", line 243, in process_data_events\n    raise exceptions.ConnectionClosed()\npika.exceptions.ConnectionClosed\nShould be...\nINFO:pika.adapters.base_connection:Connecting to ::1:5672\nINFO:root:Start consuming.....\nINFO:root:In callback, about to generate some generic exception\nERROR:root:start_consuming raised: global name 'undefined_var' is not defined\nINFO:root:Start consuming.....\nINFO:root:In callback, about to generate an AttributeError\nERROR:root:start_consuming raised: 'str' object has no attribute 'some_random_attribute'\nINFO:root:Start consuming.....\n. Yup sure, I'm not casting any aspersions in pika's _handle_read overhead.\nPerhaps my original comment was too flippant. I am actually extremely interested in the performance of pika, but more than that, I'm interested in getting behaviour that is reliable, consistent and what a user might reasonably expect. I think this PR gets us a step closer to that goal, AND it is actually significantly more efficient than the existing code.\nLet's keep in mind that we're only talking about a situation here where the publisher is consistently generating messages faster than the network can send them, in this situation we would actually be pushing back using the backpressure detection, if it worked (I have a commit for that too).  In such a situation, taken to extreme, we're talking about swapping and potentially memory exhaustion, IMO a small additional overhead (1.4us* on my laptop) is probably not the first thing to worry about.\nFurthermore, scenarios such as the following continue to break down...\nImagine you have an application that is reading a large amount of data from a non rabbit source (say a large XML document retrieved over http, data from some sort of DB or even generated internally), as it processes that data it publishes a large number of messages. In this situation the code might well not drop into the ioloop until it's completed processing of all the data. On pika/master it would never attempt to send a single message until processing of the full document was complete whereas this PR would keep putting messages in the socket buffer whenever space was available. With your suggestion we would fill the socket buffer once and then never send another message until we finished processing the document.\nMy aim with this PR is to get the data on the wire ASAP to minimise latency and maximise opportunities for parallel-isation.\n*1.4us is the overhead on my laptop of attempting a 1byte write on a blocked socket in python, including catching and handling the generated exception.\n0.5us is the overhead of an equivalent write on an unblocked socket.\n4.3us is the overhead of dropping into the tornado ioloop and getting triggered when the socket becomes writeable as the existing code does, individually for EVERY frame sent.\n. Sure, np. Is probably an edge case but easily fixed.\n On 4 May 2015 22:13, \"vitaly-krugl\" notifications@github.com wrote:\n\nIn pika/adapters/select_connection.py\nhttps://github.com/pika/pika/pull/555#discussion_r29624303:\n\n\nself._timeouts = {}\nself._next_timeout = None\nself._processing_fd_event_map = {}\nself._r_interrupt, self._w_interrupt = self.get_interrupt_pair()\nself.add_handler(self._r_interrupt.fileno(), self.read_interrupt, READ)\n  +\ndef get_interrupt_pair(self):\n\"\"\" Use a socketpair to be able to interrupt the ioloop if called\nfrom another thread. Socketpair() is not supported on some OS (Win)\nso use a pair of simple UDP sockets instead. The sockets will be\nclosed and garbage collected by python when the ioloop itself is.\n\"\"\"\ntry:\nreturn socket.socketpair()\n  +\nexcept:\n\n\n@wjps https://github.com/wjps, this is a bug; the code needs to use except\nNameError: explicitly here. except: by itself is almost always\nproblematic for the following reasons:\n1. It prevents system-level exceptions, such as KeyboardInterrupt (based\non BaseException, not on Exception), from working properly. Note that Ctl-C\nas well as SIGINT both trigger the KeyboardInterrupt exception by default.\n2. It's too broad and will catch unintended exceptions.\nSee except NameError\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/555/files#r29624303.\n. No, it doesn't. But then the socketpair is basically an optimisation to\ndrop out of the ioloop immediately when triggered from elsewhere. It will\nwork for 99.99% of cases and, as long as the default timeout is kept at\naround 5 secs, it's unlikely to be an issue there. I didn't want to use a\ntcp socket because then we have to handle the connect/accept cycle and the\npotential to block or fail there. I'd rather we just lose the optimisation.\nOn 4 May 2015 22:34, \"vitaly-krugl\" notifications@github.com wrote:\nIn pika/adapters/select_connection.py\nhttps://github.com/pika/pika/pull/555#discussion_r29626106:\n\n\nself._processing_fd_event_map = {}\nself._r_interrupt, self._w_interrupt = self.get_interrupt_pair()\nself.add_handler(self._r_interrupt.fileno(), self.read_interrupt, READ)\n  +\ndef get_interrupt_pair(self):\n\"\"\" Use a socketpair to be able to interrupt the ioloop if called\nfrom another thread. Socketpair() is not supported on some OS (Win)\nso use a pair of simple UDP sockets instead. The sockets will be\nclosed and garbage collected by python when the ioloop itself is.\n\"\"\"\ntry:\nreturn socket.socketpair()\n  +\nexcept:\nLOGGER.debug(\"Using custom socketpair for interrupt\")\nread_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n\n\n@wjps https://github.com/wjps, this may be a bug. My understanding is\nthat UDP does not guarantee packet delivery, possibly not even on the\nloopback interface.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/555/files#r29626106.\n. Well, if the socket has been triggered as readable, there should be at\nleast one byte to read. Theoretically I guess it could happen but then it\nwould either be a bug in our code or the OS.\nOn 4 May 2015 22:36, \"vitaly-krugl\" notifications@github.com wrote:\nIn pika/adapters/select_connection.py\nhttps://github.com/pika/pika/pull/555#discussion_r29626337:\n\n\nread_sock.bind(('localhost', 0))\nwrite_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\nwrite_sock.setblocking(0)\nwrite_sock.connect(read_sock.getsockname())\nreturn read_sock, write_sock\n  +\ndef read_interrupt(self, interrupt_sock, events, write_only):\n\"\"\" Read the interrupt byte(s). We ignore the event mask and write_only\nflag as we can ony get here if there's data to be read on our fd.\n  +\n:param int interrupt_sock: The file descriptor to read from\n:param int events: (unused) The events generated for this fd\n:param bool write_only: (unused) True if poll was called to trigger a\nwrite\n\"\"\"\nos.read(interrupt_sock, 512)\n\n\n@wjps https://github.com/wjps, this may be a bug. Since\nget_interrupt_pair() configured the socket pair as non-blocking, OSError\nexception with errno EAGAIN or EWOULDBLOCK may be a possibility\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/555/files#r29626337.\n. No, the socket should be garbage collected by python.\n\nAlso remember we're talking about one pair ioloop, not per connection and\nusually you'd only ever want one ioloop for the lifetime of the process.\nOn 4 May 2015 22:44, \"vitaly-krugl\" notifications@github.com wrote:\n\nIn pika/adapters/select_connection.py\nhttps://github.com/pika/pika/pull/555#discussion_r29627028:\n\n\"\"\"\n-        self.fileno = fileno\n-        self.events = events\n-        self.open = True\n-        self._handler = handler\n-        self._timeouts = []\n-        self._manage_event_state = state_manager\n-        self._fd_handlers = dict()\n-        self._fd_events = {READ: set(), WRITE: set(), ERROR: set()}\n-        self._stopping = False\n-        self._timeouts = {}\n-        self._next_timeout = None\n-        self._processing_fd_event_map = {}\n-        self._r_interrupt, self._w_interrupt = self.get_interrupt_pair()\n\n@wjps https://github.com/wjps, this change may be leaking UDP sockets,\nas there is no code that explicitly closes these two sockets self._r_interrupt,\nself._w_interrupt\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/555/files#r29627028.\n. Yes, again a fairly theoretical one, but we're still catching it. If we do get EWOULDBLOCK then it means we've sent an awful lot of unread interrupt bytes so there's not much we can do except log it.\n\nWRT send/write. The ioloop works in terms of filedescriptors so the code is a lot cleaner simply doing read/write on those. This could be changed to os.write(self._w_interrupt.fileno(), 'X') but I don't think it adds anything. Happy to change if you feel strongly otherwise.\n. So, as it stands, a read can trigger a write which could recursively drop into poll with the write_only flag set. When/if #556 is merged this nasty recursive behaviour goes away. Until then, whilst being ugly, does it cause a problem?\nWe'd end up overwriting self._processing_fd_event_map which would mean we'd drop out of the outer loop immediately after the inner. We'd then pop straight out of poll again with any events that hadn't been triggered. I guess if you had a fd that was always readable you could starve events on other descriptors?\nHopefully #556 will get merged shortly, if not I'll think about it some more.\n. Whoops, that was search and replace gone awry.\n. Do you think it would be preferable to have a default ioloop that was re-used in that case? It would also mean that two SelectConnection instances would automatically use the same ioloop without having to extract it from one and pass into the other.\nWasn't from me, but I don't think you need a shutdown() here. The only references to these sockets are held by the ioloop and we don't care about potentially losing data in the socket buffer. In my tests the sockets get correctly garbage collected when the object is de-allocated.\nIf we add a close function then we'll need to handle situations where somebody attempts to re-use a closed ioloop.\n. WRT the fd_event_map, you're right. I was thinking we removed fds from the map as we went through, but it's the lists we modify so should be fine.\n556 removes SelectConnection._flush_outbound() which causes the recursive call.\n. No, that was existing code, the bug was that I'd accidentally renamed it in my original patch. It is removed by #556, but the other changes in there are required in order to do so and not create performance problems.\n. Err, _manage_event_state() is defined is BaseConnection()\nOn Thu, 7 May 2015 at 08:45 vitaly-krugl notifications@github.com wrote:\n\nIn pika/adapters/select_connection.py\nhttps://github.com/pika/pika/pull/559#discussion_r29830763:\n\n\"\"\"Call the state manager who will figure out that we need to write\n         then call the poller's poll function to force it to process events.\n     \"\"\"\n-        self._()\n-        self._manage_event_state()\n\n@wjps https://github.com/wjps, this will crash, because SelectPoller\ndoesn't have _manage_event_state as a member.\nIf confirmed, this code path could also benefit from a test.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/559/files#r29830763.\n. Ah right, apologies, yes that is garbage. Never gets called and should just\nbe deleted.\n\nOn Thu, 7 May 2015 at 10:31 vitaly-krugl notifications@github.com wrote:\n\nIn pika/adapters/select_connection.py\nhttps://github.com/pika/pika/pull/559#discussion_r29836935:\n\n\"\"\"Call the state manager who will figure out that we need to write\n         then call the poller's poll function to force it to process events.\n     \"\"\"\n-        self._()\n-        self._manage_event_state()\n\n@wjps https://github.com/wjps, but this _flush_outbound is inside\nSelectPoller, which is not subclassed from BaseConnection. I think that\nyou may have accidentally pasted _flush_outbound() into SelectPoller\nclass. The legitimate _flush_outbound is in SelectConnection.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/559/files#r29836935.\n. Yeah, I did toy with that idea but decided to keep it as is in the end. Still working on trying to get all the ioloops behaving exactly the same and I think it makes sense to maintain an IOLoop class in each at that point. Not had so much time lately though.\n. \n",
    "vak": "this is the example content of self._frames at the moment when KeyError is about to be raised:\n{'Basic.ConsumeOk': \n   <METHOD(['frame_type=1', \n            'channel_number=1', \n            \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.0'])>\"])>}\n. ",
    "anuragpatibandla": "I am having the same error in Pika 0.9.14. What is the work around?\n. ",
    "pfreixes": "Hi guys,\nI came across with this issue a few days ago, I'm using BlockingConnection and the Pika version 0.9.14. The code looks like that:\n``` python:\n    connection = pika.BlockingConnection(pika.ConnectionParameters(\n        host='localhost'))\n    channel = connection.channel()\n# Declare and bind the queue to consume the messages\nresult = channel.queue_declare()\nqueue = result.method.queue\nchannel.queue_bind(exchange='miimetiq', queue=queue,\n                   routing_key='sometopic')\n\ndef callback(channel, method, properties, message):\n    queue_response = routing_key = properties.correlation_id.replace('-','')\n    channel.queue_declare(queue=queue_response, auto_delete=True)\n    channel.queue_bind(exchange='results', queue=queue_response, routing_key=routing_key)\n    channel.basic_publish(\n        exchange='results',\n        routing_key=properties.correlation_id.replace('-', ''),\n        body=json.dumps(miimetiq_response_payload),\n        properties=pika.BasicProperties(content_type='application/json'))\n\nchannel.basic_consume(callback, queue=queue)\nchannel.start_consuming()\n\n```\nAs you can see the design pattern looks like the snippet of code submit by @vak. After a while of time I found out the root of the issue, it is triggered because of the handling of frames belonging to other amqp messages, let me show you.\nWhen a new amqp message get the callback  function it runs until the call to the channel.queue_declare is executed, this function [1] prepare the rpc to declare a new queue and register a callback to save the response as a frame in a global variable called self._frames. To get these frames the blocking channel calls to process_data_events [2] guessing that when it comes back the new frames will be available on the internal self._frames.\nMeanwhile another amqp messages has arrived and the calling frames are waiting to be dispatched, the connection process all data available in the socket connection, looking first the frames of the new message arrived. Therefore first is called the callback related with this message, putting in hold the frame that responses to the new queue request belonging to the first amqp messages.\nThis new request also gets the queue_declare step and reproducing again the whole steps already mentioned before and also trying to register the properly callback for his queue_declare function.\nAt last the callback of the first connection is triggered and the replies are processed here [3], removing his replies and ALSO the replies that don't belong to its.\nWhen the last request tries to process the replies it raises one exception because of the reply was removed before by the first request. \nI've made a draft patch that hash the replies by the name of the message plus the id or name of the type object, i.e \"Queue.DeclareOk+queue_name\", and add some needed arguments to retrieve the context of each message. \nWhat do you think about that ?\n[1] https://github.com/pika/pika/blob/master/pika/adapters/blocking_connection.py#L878\n[2] https://github.com/pika/pika/blob/master/pika/adapters/blocking_connection.py#L1142\n[3] https://github.com/pika/pika/blob/master/pika/adapters/blocking_connection.py#L1102\n. There is a workaround to figure it out while the blocking connection adapter is not patched, using the basic_qos [1] set with prefetch_count argument as 1 avoids handle new messages until the current one is not ack, obviously it doesn't work if no_ack was set to True to consume the messages. \nTo enable the prefetch_count set the qos before the start_consuming is called as the next snippet shows, also the ack of the message has to be done as a last step.\n``` python\n    def callback(channel, method, properties, message):\n        queue_response = routing_key = properties.correlation_id.replace('-','')\n        channel.queue_declare(queue=queue_response, auto_delete=True)\n        channel.queue_bind(exchange='results', queue=queue_response, routing_key=routing_key)\n        channel.basic_publish(\n            exchange='results',\n            routing_key=properties.correlation_id.replace('-', ''),\n            body=json.dumps(miimetiq_response_payload),\n            properties=pika.BasicProperties(content_type='application/json'))\n        channel.basic_ack(delivery_tag=method.delivery_tag)\nchannel = connection.channel()\nchannel.queue_declare(queue=\"test\", auto_delete=True)\nchannel.queue_bind(exchange=\"test\", queue=\"test\")\nchannel.basic_qos(prefetch_size=0, prefetch_count=1, all_channels=True)\nchannel.start_consuming()\n\n```\n[1] http://pika.readthedocs.org/en/latest/modules/adapters/blocking.html#pika.adapters.blocking_connection.BlockingChannel.basic_qos\n. There is no a good way to patch the blocking connection adapter, it lacks of stack frame up such as asyncronous adapter. The only way to figure it out for all RPC commands - i.e declare queue, bind queue, create exchange - is modify the core adding a new keyword to stack up other incomming frames until the _rpc is full finished. Obviusly this patch could be seen as a bit dumb because with the workarround that I posted before setting the prefetch_count=1 gets the same behaviour. IMHO it shouldn't be patched but some kind of warning could be displayed if the user tries to run one RPC command in the middle of the callback when prefetch_count is greater than 1.\nAnother solution for that is implement a threaded adapter for the blocking connection to overcome this issue and allowing prefetch_count value greater than 1.\n. In fact I got the master yesterday and this is the version used  where I got this issue\nbash\n$ pip freeze\npika===0.10.0p0\n. @vitaly-krugl it helps a lot. Thanks. \n. @gmr obviously Python interpreter do almost nothing when the sleep is called but I disagree with one thing, that I was looking for using this sleep was trigger the timeouts related with the buffers beyond the user land. \nThink for example with the tcp_nodelay [1] option or other optmitzations that could leave some packages still to be sent. \nIn fact there is no reason to think that exit of the program without make the channel.close() should get a different behavior. IIRC when the file descriptors are closed before that the buffer is flushed automatically. \n[1] http://aboutsimon.com/2012/07/27/python-tcp-socket-performance-tweak-on-linux/\n. Ahhh I thought that publish was syncron as well, that was my mistake. I will try to remember that for future uses.\n. @gmr I was thinking at socket level about async and sync, not in use transactions or confirmation delivery at amqp protocol level.\n@vitaly-krugl obvsiously the code was only a test and the sleep was putted there to reach some kind of flush, and the most important it worked for that test. \nThe point here is that I used te script made by @vitaly-krugl and I ran the same test, and it appears to have the same issue, when the channel is not closed explicitly the messages already sent aren't the 1K messages that we should expect. Have a look to that script just a bit modified to fit into my test environment\n``` python\n from pika import BlockingConnection\ndef main():\n  ch = BlockingConnection().channel()\nfor i in xrange(1000):\n    ch.basic_publish(\"test.concurrence\", routing_key=\"test.{}\".format(i+1), body=\"message-%s\" % (i+1,))\nif name == \"main\":\n  main()\n```\nAnd here different executions of this test\nbash\n$ python publisher_2.py\n$ ./rabbitmqadmin.py list queues name messages | grep \"test.\" | grep \" 0 \" | wc -l\n221\n$ ./purge.sh\n$ ./rabbitmqadmin.py list queues name messages | grep \"test.\" | grep \" 0 \" | wc -l\n1000\n$ ./rabbitmqadmin.py list queues name messages | grep \"test.\" | grep \" 0 \" | wc -l\n503\nMay be I'm missing something but the point here is the buffers are not already flushed or something like that.\n. @vitaly-krugl and  @gmr finnally I've found out the issue, this is a kind of side effect because of rabbitmq. I dont know if it is a really bug but the behavior is a bit weird.\nWhen the channel is not properly closed by the client, Rabbitmq might drop the last packages sent by the snippet used in these tests. I realized now about that because, first I can confirm that all tcp packets are already sent by the client until the last FIN tcp package and lowering the number of messages to send until 200 hundred I saw that the messages that always can be missed are the last ones. For example:\nbash\n| test.189 | 0        |\n| test.190 | 0        |\n| test.191 | 0        |\n| test.192 | 0        |\n| test.193 | 0        |\n| test.194 | 0        |\n| test.195 | 0        |\n| test.196 | 0        |\n| test.197 | 0        |\n| test.198 | 0        |\n| test.199 | 0        |\n| test.200 | 0        |\nRemember that the script send each message in a ordred way, starting from 1 until N - where for this lasts tests I used 200. Threfore only the last messages are affected.\nThis is a random behavior, and it explains why the sleep worked to remove this side-effect. Rabbitmq looks to publish the messages in a kind of burst way, and when the time elapsed between the last messages and the disconnection of the client - without the properly close - the chances to get this side-effect are inversely proportional.\nIMHO the destructor of the channel could deal with those situations where the user has not close the channel \"by hand\".\n. @vitaly-krugl done, even you should consider to make the close of the channel by the destructor to avoid this behavior. \n. @vitaly-krugl Just to give more information and close it at last. Rabbitmq guys says that the only way to make sure that the tcp buffers are properly read is using the properly channel close, follow this thread [1].\nTherefore the close of the channel operation have to be considered to put it into the destructor to avoid this side-effect.\n[1] https://groups.google.com/forum/#!topic/rabbitmq-users/7lMUnkS1ssU\n. As I can read, I did not realize about this situation before you explained it, leaving aside the none deterministic use of the __del__,  the main issue can be overcome just using weak references. Expained also in the same stackoverfow thread [1] by the author of the question.\n[1] http://eli.thegreenplace.net/2009/06/12/safely-using-destructors-in-python/ \n. ",
    "jonkirsch": "@jwestfall69 : Thanks for catching the failure-to-initialize bug.  I fixed it in my branch.  I think you are correct about the timeout vs. WOULDBLOCK issue, although in either case I think the bug results in the same \"partial sending\" behavior, as the remainder of the frame is never put back onto the outbuffer.  Your reference to issue #349 is also in line with the types of errors we were seeing in our log files.\n. ",
    "jamutton": "Proposed solution is in PR(https://github.com/pika/pika/pull/450)\nAdd a log entry to base_connection. _handle_error to note this specific case\npython\n        elif error_code == errno.EPIPE:\n            # Broken pipe, happens when connection reset\n            LOGGER.error(\"Socket connection was broken\")\nand put in some blocking_connection specific handling because I'm not certain if it makes sense in the other connection types:\npython\n    def _check_state_on_disconnect(self):\n        \"\"\"Checks closing corner cases to see why we were disconnected and if we should\n        raise exceptions for the anticipated exception types.\n        \"\"\"\n        super(BlockingConnection, self)._check_state_on_disconnect()\n        if self.is_open:\n            # already logged a warning in the base class, now fire an exception\n            raise exceptions.ConnectionClosed()\n. channel.confirm_delivery() will force a select.error on publish by wrapping _send_method in _rpc and adding callbacks to trigger a read from the socket which indirectly fires the select.error.  It works, but in a very round-about way and the approach feels backwards, hide the error by default unless asked to show just opens the door for implementation bugs.\nMostly though, I'd assert that it's not consistent with other behaviors in the module to swallow the error like it does now.  If the intention is to swallow all errors then why allow the select.error to propagate when connection.close() is called or when the receiver is waiting.\nThe BlockingConnection class already follows an error path, resets its state and None's the socket, yet it tells the caller everything was fine.  It continues to do so if you keep trying to publish even though the object knows nothing is going through.  If the PR is applied, the caller can duplicate the current behavior of swallowing the error by putting basic_publish in a try/except which requires them to ack and pass the error.  Much less likely to inadvertently drop/lose messages.\n. ",
    "Neetuj": "btw the consumer is asynchronous\n. but the queue is not declared by me in my application  ...I assume the client of my application will create it ..i just want to verify that the queue exists. \n. who need to use a callback ? the creater of the queue or the consumer? as a consumer i am giving  it a callback on_queue_declareok .but it  goes there only after i start consuming .. channel.basic_consume(queue=queuename, consumer_callback=message )\ni want to check if queue exists before i start consuming .is there a way to do that ? \n..am new to rabbit so may be i am missing somehting \n. ",
    "MartyMacGyver": "Note: I initially posed the question on StackOverflow to rule out operator error on my part. Another user (dano) was able to provide much more detail on what's broken here (specifically, AsyncoreConnection.adapter_connect isn't returning a value explicitly so it's returning _None rather than True).\nhttp://stackoverflow.com/questions/23537466/amqpconnectionerror-using-pika-and-rabbitmq-with-asyncore-why/23550594?noredirect=1#23550594\n. Note: this may have been fixed by the code in commit https://github.com/pika/pika/commit/1f9e72b91e88860de4b9dec445976901757a9e20\n. Per my comment and the thread, this does appear to be fixed on master (though the builds were broken at the time I was trying it, using the last buildable version succeeded). It wasn't clear that this was a known issue when I opened this bug, but it's good to see it was fixed as part of the error propagation change. Eagerly awaiting 0.9.14.\n. I see it - thank you!\n. ",
    "samkirton": "Please see (https://github.com/pika/pika/commit/efea53d109352400830609ecdca86707782fb822) force_binary was already removed from channel.py. The latest build will break your production code also.\n. My guess is that the owner has removed force_binary for now and will merge it as part of a release\n. ",
    "coveralls": "\nCoverage decreased (-0.39%) when pulling afbc9e01e5eefd59e27a0c5974caecd4ceaa1704 on michaelplaing:reset_io_watcher into 24332a2fc5e8de961d208dc4b439e82c977c011e on pika:master.\n. \nCoverage decreased (-0.14%) when pulling b1109b6605cf24af6642a2d46ae91422df59231e on michaelplaing:libev_examples into 4adc88be55969b203471f506b9f373673367b46b on pika:master.\n. \nCoverage decreased (-0.14%) when pulling fae328ec17bbaf933120267f928e45ec4979b2fe on jfw:master into 4adc88be55969b203471f506b9f373673367b46b on pika:master.\n. \nCoverage decreased (-0.13%) when pulling 01710ad900e49577f575a93de92c72e46d8f8e11 on reywood:master into 4adc88be55969b203471f506b9f373673367b46b on pika:master.\n. \nCoverage decreased (-3.48%) when pulling f4faf0ac9ed29182d3031d313556db6f899ccd06 on entropiae:master into 4adc88be55969b203471f506b9f373673367b46b on pika:master.\n. \nCoverage decreased (-0.14%) when pulling 57fe43e78a4f92a45cdcbfdeb2af5a1fb41d4797 on michaelplaing:fix-test_next_channel_number_returns_lowest_unused into 4adc88be55969b203471f506b9f373673367b46b on pika:master.\n. \nCoverage decreased (-0.21%) when pulling 0d2452709d55298d2178b6065dc5e53a262ceff4 on husio:no_sendall into 4adc88be55969b203471f506b9f373673367b46b on pika:master.\n. \nCoverage decreased (-0.17%) when pulling 0d2452709d55298d2178b6065dc5e53a262ceff4 on husio:no_sendall into 4adc88be55969b203471f506b9f373673367b46b on pika:master.\n. \nCoverage increased (+0.01%) when pulling 96592c6c64e39906e40d4c0ce8adfa6088abc19e on anjensan:master into d1876b0cb5ad9cacb67d0579c49ef1e8a97c0afb on pika:master.\n. \nCoverage remained the same when pulling 6cc22a5997e63f5c714c87771a177234d26ad877 on bra-fsn:patch-1 into f8c263f234cca6b0f573ac63268a4034e32bd3eb on pika:master.\n. \nCoverage increased (+0.11%) when pulling b5f52fb8284bb5f751664e10c7181e7fc354ef5f on aeirola:master into f8c263f234cca6b0f573ac63268a4034e32bd3eb on pika:master.\n. \nCoverage remained the same when pulling c1234725e42fb70a880e96fbd83857e303c19711 on robertodecurnex:master into f8c263f234cca6b0f573ac63268a4034e32bd3eb on pika:master.\n. \nCoverage decreased (-0.17%) when pulling e81c0f1f6bc77aa4d8200be6410b30c114682405 on shiroyuki:master into f8c263f234cca6b0f573ac63268a4034e32bd3eb on pika:master.\n. \nCoverage decreased (-0.37%) when pulling e2c98599d225d75a296768a7376a3038e09f878c on shiroyuki:master into f8c263f234cca6b0f573ac63268a4034e32bd3eb on pika:master.\n. \nCoverage decreased (-0.18%) when pulling c4a2cf722e03e6050fe4f8e03b94fad368c4c638 on shiroyuki:master into f8c263f234cca6b0f573ac63268a4034e32bd3eb on pika:master.\n. \nCoverage increased (+0.13%) when pulling 8dab34a4f28d65997ef83f11bd775f148dfbfd29 on kherrala:master into f8c263f234cca6b0f573ac63268a4034e32bd3eb on pika:master.\n. \nCoverage increased (+0.08%) when pulling c3f390c30e8ef7bb815ad6b1976f423c7a68f12a on kherrala:master into e27b537a723ed0693e9d2294e633c66f54e4eef5 on pika:master.\n. \nCoverage increased (+0.07%) when pulling 8fe6bddb6832361d25e5b494c893c3bbbbf126c9 on kherrala:heartbeat-fix into e27b537a723ed0693e9d2294e633c66f54e4eef5 on pika:master.\n. \nCoverage increased (+0.16%) when pulling 4584329dfbad5753e3e894e223f0b3b3a557afad on kherrala:socket-fix into e27b537a723ed0693e9d2294e633c66f54e4eef5 on pika:master.\n. \nCoverage remained the same when pulling 2f209da79d3027b5d8b0039c8c9d0a04bd6b7918 on lclementi:master into 28c221402a15a00f4321a05009eadf62825f149f on pika:master.\n. \nCoverage remained the same when pulling 54987dc6530abe9394b3d59a729fb44a9ea41d37 on zbenjamin:fix-sync-methods into 28c221402a15a00f4321a05009eadf62825f149f on pika:master.\n. \nCoverage decreased (-0.07%) to 72.55% when pulling 28d30ab343e4a0390ecdfcba54aa22c9ac0ba4b9 on zbenjamin:fix-sync-methods into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage remained the same when pulling 7e50302bf317ae605321019d207f4e4a7eb84562 on ei-grad:master into 28c221402a15a00f4321a05009eadf62825f149f on pika:master.\n. \nCoverage decreased (-0.21%) when pulling a66e7a0168b64887df7c1c14996f0c2629daa2f4 on gitter-badger:gitter-badge into 28c221402a15a00f4321a05009eadf62825f149f on pika:master.\n. \nCoverage increased (+3.3%) when pulling aa65fcb0b4b705acb55057ccc5ee14650b75f47b on jimhorng:feature/skip_IncompatibleProtocolError into 44291104c0d2565ee5a45436191a5d7330d8611f on pika:master.\n. \nCoverage increased (+3.65%) when pulling 2d75088941ca2deaa38fd35932d946daa674959e on jimhorng:feature/skip_IncompatibleProtocolError into 44291104c0d2565ee5a45436191a5d7330d8611f on pika:master.\n. \nCoverage increased (+3.39%) when pulling 0ab5a6018e36bb4656b8b24ba82cd79e07e5636b on cato-:master into 44291104c0d2565ee5a45436191a5d7330d8611f on pika:master.\n. \nCoverage increased (+3.39%) when pulling 361c0ad756f783dfd7c5c658ea9a15e635d4ba02 on cato-:master into 44291104c0d2565ee5a45436191a5d7330d8611f on pika:master.\n. \nCoverage remained the same when pulling 452995cf6aba3dca279fc23a974204bf5050d606 on Scorpil:patch-1 into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage remained the same when pulling bb9c9aa3b3f938802376810cd44dc420a3be2be1 on jpzk:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.04%) when pulling 562aa15bafa7be84a67d0496c4bc741d8a7bc3c5 on sjlongland:callback_exception into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.04%) when pulling 562aa15bafa7be84a67d0496c4bc741d8a7bc3c5 on sjlongland:callback_exception into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-3.36%) to 69.26% when pulling 18de5f8fcf1c25877522e3ed37404d13795fbc60 on awelzel:errorcb into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.02%) to 72.64% when pulling 18de5f8fcf1c25877522e3ed37404d13795fbc60 on awelzel:errorcb into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.02%) to 72.64% when pulling 18de5f8fcf1c25877522e3ed37404d13795fbc60 on awelzel:errorcb into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.01%) to 72.63% when pulling c8ae0ec798edea93517705cd4d02541b285c76c9 on thomdixon:issue403 into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.01%) to 72.63% when pulling c8ae0ec798edea93517705cd4d02541b285c76c9 on thomdixon:issue403 into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.25%) to 72.87% when pulling 6052ecfce63ffb230d9f0795e5fff14ad5ee62e8 on vitaly-krugl:pika532-handle-timeout-bug into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.25%) to 72.87% when pulling 8c08f931f05f3b5e45aa154d429bec6ccebc3435 on vitaly-krugl:pika532-handle-timeout-bug into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.23%) to 72.39% when pulling 71bc0eb76b48b5726301020ee9485917bdaad3d1 on vitaly-krugl:removed-unused-self.fd into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-3.47%) to 69.15% when pulling 09d2ddd66577dedc7161d008327226a3407e03b7 on vitaly-krugl:fix-closing-args into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.09%) to 72.53% when pulling 09d2ddd66577dedc7161d008327226a3407e03b7 on vitaly-krugl:fix-closing-args into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-3.38%) to 69.24% when pulling 0c9be998538296129d009be14df479bce751ed1c on vitaly-krugl:fix-closing-args into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.01%) to 72.63% when pulling 0c9be998538296129d009be14df479bce751ed1c on vitaly-krugl:fix-closing-args into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.01%) to 72.63% when pulling 0c9be998538296129d009be14df479bce751ed1c on vitaly-krugl:fix-closing-args into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.01%) to 72.63% when pulling 0c9be998538296129d009be14df479bce751ed1c on vitaly-krugl:fix-closing-args into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.1%) to 72.52% when pulling 63d29510ce4be81534985f41d553a93c468684ea on vitaly-krugl:pika540-connectionclosed-on-lost-connection into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.18%) to 72.44% when pulling d11e73fc4003d61dd67997f747b4642b89eeef67 on vitaly-krugl:pika540-connectionclosed-on-lost-connection into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.15%) to 72.47% when pulling 576c1f09f5758a56965375167ae6944376ae735f on wjps:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-3.47%) to 69.15% when pulling f2dc43056ff77e829094184916960d0e6888574d on wjps:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.52%) to 72.1% when pulling 7b474bb8ef1b624e6d615bdbc843bb59aa2becbc on wjps:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.52%) to 72.1% when pulling 7b474bb8ef1b624e6d615bdbc843bb59aa2becbc on wjps:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.08%) to 72.54% when pulling 7d457f5e3871fb5050028d4130f04c8631a96ecd on wjps:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.08%) to 72.54% when pulling 7d457f5e3871fb5050028d4130f04c8631a96ecd on wjps:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.13%) to 72.49% when pulling 635957f8a780814cb6eaa91c886a891b31518662 on wjps:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage remained the same at 72.62% when pulling 2dd23aeeaba27a28ce63f3a9314e1115dd663abe on wjps:issue403 into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage remained the same at 72.62% when pulling 2dd23aeeaba27a28ce63f3a9314e1115dd663abe on wjps:issue403 into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-0.19%) to 72.43% when pulling 1583537f2e77dba55d12b3d0ed581983773635b8 on markcf:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.21%) to 72.83% when pulling be06bf8b8f252f65fc69b6b965197f4d3801b5a6 on wjps:issue548 into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage increased (+0.21%) to 72.83% when pulling be06bf8b8f252f65fc69b6b965197f4d3801b5a6 on wjps:issue548 into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage decreased (-3.39%) to 69.23% when pulling 66372126f5a231c42a0a4d6141384512b19bf3e0 on bstemshorn:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage remained the same at 72.62% when pulling 66372126f5a231c42a0a4d6141384512b19bf3e0 on bstemshorn:master into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \nCoverage remained the same at 72.62% when pulling b80407e7a4920d2616243b1093caa3c286f31637 on andrewmichaelsmith:patch-1 into 409670bdbea13d11a43d3d02162d49a200e450a5 on pika:master.\n. \n\nChanges Unknown when pulling 94246d25fbc403dbd1f7bb3a8958a69ceaac7a06 on vitaly-krugl:basic_publish-result-compatibility into  on pika:master.\n. ",
    "reywood": "Regarding \"Coverage decreased (-0.13%)\", does coveralls need to be told to ignore the tests directory?\n. ",
    "husio": "I also asked for help on tornado mailing list (**) and it looks like socket.sendall should not be used with non blocking socket.\n** https://groups.google.com/forum/#!topic/python-tornado/hpFSq-kA1BU\n. I should write a test case for this, but I need some help with reproducing message being chunked by the system.\n. ",
    "Daemoneyes": "I've check the rabbit.js with node.js, the connection is fine with the same configuration.\n. ",
    "ilias1995": "i have the same problem.\nlog:\nTraceback (most recent call last):\n  File \"manual_payment_processing.py\", line 70, in send_pay\n    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n  File \"/usr/local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 130, in init\n    super(BlockingConnection, self).init(parameters, None, False)\n  File \"/usr/local/lib/python3.6/site-packages/pika/adapters/base_connection.py\", line 72, in init\n    on_close_callback)\n  File \"/usr/local/lib/python3.6/site-packages/pika/connection.py\", line 596, in init\n    self.connect()\n  File \"/usr/local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 230, in connect\n    error = self._adapter_connect()\n  File \"/usr/local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 301, in _adapter_connect\n    raise exceptions.AMQPConnectionError(error)\npika.exceptions.AMQPConnectionError: Connection to ::1:5672 failed: [Errno 99] Address not available\ncode:\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\ncan some one help me?\n. 127.0.0.1 localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n\nIf you are on a system with an /etc/hosts file, what is its contents?\n\n. ",
    "smurfix": "Hi,\nGavin M. Roy:\n\nOf course part of the problem is that amqplib and others that are not async or that do not run a background IO thread are not well behaved AMQP clients.\n\nI'm currently using amqp -- but I'm doing the background I/O thing, courtesy of gevent.\nActually using a proper gevent loop in the test (i.e. two context switches\nper RPC call) pushes the timing from 350\u00b5sec per loop to 450. Still more\nthan twice as fast as pika's 1050.\n\nI'd be curious how your test holds up against rabbitpy, which implements similar constructs to how I intend for BlockingConnection to work for 0.10.\nrabbitpy is much slower. It manages to accrue twice pika's user CPU time\n(and that was after I increased the terminally stupid 16-byte read buffer\nsize) and, worse, it has ten times the wall clock runtime.\n\nNo, I didn't try to find out where that comes from, but one reason is that\nrabbitpy busy-loops reading the event queue from its read task:\nwhile True:\n    self._check_for_exceptions()\n    try:\n        value = self._read_queue.get(True, 1)\n        self._read_queue.task_done()\n        return value\n    except queue.Empty:\n        pass\n    return None\nThis is broken (a) because generating a throwaway exception eats CPU, and\n(b) because Python's queue implementation doesn't really support that;\ninstead it sleeps 1\u202650ms between checks whether another task has queued\nanything.\nThe correct pattern is to enqueue an \"an exception has occurred,\nbreak out of your loop\" pseudo-datum.\nThis actually means that running rabbitpy with gevent is faster than using\nnative threads, but it's still dead slow. Wallclock time: 70 seconds vs.\n1.05sec with amqp vs. 1.75sec pika.\nAttached: code. Note that the rabbitpy example only does 100 iterations,\nnot 1000, otherwise I'd fall asleep while testing. :-/\n\n-- Matthias Urlichs\n. ",
    "andrewmichaelsmith": "I've raised #530 for this issue\n. ",
    "joeclarkia": "I think my formatting was less than stellar above, as the rx[\"channel\"] syntax is particular to my code... Further, to answer my own question, after some experimentation, I think the best \"workaround\" is to stop using the consume() function and instead use the more typical basic_consume() mechanism, and add a timeout that calls a callback periodically which performs any needed upkeep (e.g., connection.process_data_events()). This seems to be working for me....\n. ",
    "hbcheng": "I have experienced an issue related to this- see https://github.com/pika/pika/issues/349.\nIf this is your error, RabbitMQ should be reporting a malformed frame error, and upgrading to the version currently in master should fix it.\n. ",
    "umgeher": "Thanks @hbcheng.\n. ",
    "MaikuMori": "I'm getting this and ConnectionClosed exception which doesn't make sense since it's LAN connection and there is no packet loss or anything like that. I don't see anything that might be related to this in the rabbitmq logs or maybe my logging setup is not verbose enough. Other services don't have problems communicating with rabbitmq (they're not using pika).\nClearly after disconnect pika still tries to use the socket which is wrong, but it shouldn't disconnect in the first place.\nFollow up:\nRewrote the same thing with kombu (had it as dependency anyway) and everything is working.\n. ",
    "tesch1": "I get the same thing since upgrading to 0.9.4, happens after my app has been running for a while, maybe the rabbitmq connection times out at some point.\n. ",
    "danjamker": "Was wondering if this has been resolved as I am getting this issue. \n. ",
    "DanCech": "I ran into this issue last week, and after looking around at the code I don't see any way to safely raise an exception from within BlockingConnection._check_state_on_disconnect().\nI experimented with using try/finally in _adapter_disconnect() to make sure _init_connection_state() was always called then bubble the exception up the stack, but that causes a problem in _on_connection_closed() because it means that the code to handle channel and callback teardown isn't executed.\nIn the end I'm using this workaround for now to catch the exception and things are working the way I expect.\npython\nclass BlockingConnection(pika.BlockingConnection):\n    \"\"\"\n    This class works around a bug in pika.BlockingConnection's\n    _check_state_on_disconnect() method; it raises an exception which prevents\n    the connection state from being updated properly when the rabbit\n    connection is dropped.  We catch and ignore the exception to allow\n    _adapter_disconnect() to properly call _init_connection_state() and not\n    leave the client in an inconsistent state.\n    \"\"\"\n    def _check_state_on_disconnect(self):\n        try:\n            super(BlockingConnection, self)._check_state_on_disconnect()\n        except pika.exceptions.ConnectionClosed:\n            pass\n. ",
    "lovato": "Hi @gmr \nI an running into this issue, and already saw it is supposed to be fixed under master branch.\nBut master differs from 0.9.14 a lot, which (I guess) makes a cherry pick near impossible. I am correct?\nIf so, is there any \"official\" patch to fix this for 0.9.14? Or can you recommend a specific cherry pick?\nBest,\nLovato\n. ",
    "niteeshm": "@gmr ,\nI am using a single pika connection and channel (global) and publishing the message inside a Celery Task and when I execute this task multiple times. It still gives me this error : \n[2015-07-03 16:02:37,315: ERROR/MainProcess] Task celery_rabbit.add[cea48b24-236c-4777-a62e-64854ed91737] raised unexpected: OSError(9, 'Bad file descriptor')\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 240, in trace_task\n    R = retval = fun(_args, _kwargs)\n  File \"/Library/Python/2.7/site-packages/celery/app/trace.py\", line 438, in protected_call\n    return self.run(_args, _kwargs)\n  File \"/Users/8320/myntra/git/lgp-fumes/activity_engine/app/helpers/celery_rabbit.py\", line 15, in add\n    body='Hello World!')\n  File \"build/bdist.macosx-10.10-intel/egg/pika/adapters/blocking_connection.py\", line 1916, in basic_publish\n  File \"build/bdist.macosx-10.10-intel/egg/pika/adapters/blocking_connection.py\", line 2005, in publish\n  File \"build/bdist.macosx-10.10-intel/egg/pika/channel.py\", line 338, in basic_publish\n  File \"build/bdist.macosx-10.10-intel/egg/pika/channel.py\", line 1150, in _send_method\n  File \"build/bdist.macosx-10.10-intel/egg/pika/connection.py\", line 1571, in _send_method\n  File \"build/bdist.macosx-10.10-intel/egg/pika/connection.py\", line 1601, in _send_message\n  File \"build/bdist.macosx-10.10-intel/egg/pika/adapters/base_connection.py\", line 282, in _flush_outbound\n  File \"build/bdist.macosx-10.10-intel/egg/pika/adapters/base_connection.py\", line 452, in _handle_write\n  File \"build/bdist.macosx-10.10-intel/egg/pika/adapters/base_connection.py\", line 338, in _handle_error\n  File \"build/bdist.macosx-10.10-intel/egg/pika/adapters/base_connection.py\", line 288, in _handle_disconnect\n  File \"build/bdist.macosx-10.10-intel/egg/pika/adapters/select_connection.py\", line 95, in _adapter_disconnect\n  File \"build/bdist.macosx-10.10-intel/egg/pika/adapters/select_connection.py\", line 319, in remove_handler\n  File \"build/bdist.macosx-10.10-intel/egg/pika/adapters/select_connection.py\", line 480, in update_handler\nOSError: [Errno 9] Bad file descriptor\n. ",
    "chubakur": "Maybe you should use conn.sleep(1) instead of time.sleep(1)?\n. ",
    "Jiaion": "Hi, Thank you\u3002\n. ",
    "xintron": "For anyone else stumbling onto this issue; here's my workaround until a solution has been merged:\npython\nfrom pika.adapters.select_connection import SelectPoller\nSelectPoller.TIMEOUT = 0.01\n. ",
    "sjlongland": "I've been confused about this issue myself.  Either the documentation is wrong or the code is wrong.\nI thought I was doing something wrong in my code, and it's only when I went digging I found this pull request.\nIt is not a difficult to fix code that uses the old (broken?) behaviour:\ndef on_return(on_return_tuple):\n    (channel, basic_return, properties, body) = on_return_tuple\n    # ...\ncan be changed to:\ndef on_return(*on_return_tuple):\n    (channel, basic_return, properties, body) = on_return_tuple\n    # ...\nThe question remains, how many people would be inconvenienced by this, versus how many are being inconvenienced by on_return callbacks that don't work for mysterious reasons?\n. Okay, for others that are looking for this information, I've found out through experiment (hacking up the RPC example on the RabbitMQ site):\n- ch is indeed the channel created when connecting to the AMQP server\n- header / props is a pika.spec.BasicProperties object, which amongst other things has the following attributes (according to dir): 'app_id', 'cluster_id', 'content_encoding', 'content_type', 'correlation_id', 'decode', 'delivery_mode', 'encode', 'expiration', 'headers', 'message_id', 'priority', 'reply_to', 'timestamp', 'type', 'user_id'\n- method is a pika.spec.Deliver in this case, in amongst that is the routing_key.\n. I've occasionally butted heads with this exception myself.  Is there a workaround?  (Sometimes it isn't an authentication error, just the AMQP broker being slow.)\n. @dave-shawley: That indeed does cure this problem.  Thank-you.\n. Problem seems to have appeared between 0.9.14 and 0.10.0b2.  Our code has not changed between those two releases.. Okay, I have a simplified test case here:\nhttps://gist.github.com/sjlongland/a8acc05e399ec1700d61fe11aba2f0f4\nBasically, pypi releases 0.9.14 works on Python 2.7 only (not sure if it's Python 3 compatible, this could be a known issue); 0.10.0 works on both, 0.11.2 fails on both.  I'm not seeing the connection getting closed due to double-ups on channel.open like with my other code (which is proprietary sadly, otherwise I'd happily share it), but it shouldn't hang like this either.\nIn our code base, it could also be a race condition, which is a hard thing to reproduce.  This isn't the exact problem I saw in the original report, but on the other hand, when someone says close, they mean it.. Yeah, my other code base actually opens one channel per queue / exchange, an idea I got from QAMQP.  It's a device driver which creates an exchange per device\u2026 and in the situation where we had the problem, there's nearly 200 devices.. On 15/02/18 02:48, Luke Bakken wrote:\n\n@sjlongland https://github.com/sjlongland if you'd like to test out my\nfix in the |pika-945| branch\nhttps://github.com/pika/pika/tree/pika-945, I would appreciate it.\n\nYep, I'll see if I can get that bundled into a Debian package and try it\nout on the production instance affected.\nIt'll either work fine, or fail (safely), and in the latter case\nroll-back is easy; we just install the python-pika package from the\nofficial Debian repositories.\n-- \nStuart Longland (aka Redhatter, VK4MSL)\nI haven't lost my mind...\n  ...it's backed up on a tape somewhere.\n. I'm just trying that branch now\u2026 seems I get a 404 message looking for it.  I'll try the master branch instead and see how I go.. The build breaks:\nFile \"/usr/lib/python2.7/dist-packages/mm_da/amqp/amqpconnection.py\", line 6, in <module>\n    from pika.adapters import TornadoConnection\n  File \"/usr/lib/python2.7/dist-packages/pika/__init__.py\", line 9, in <module>\n    from pika.connection import ConnectionParameters\n  File \"/usr/lib/python2.7/dist-packages/pika/connection.py\", line 22, in <module>\n    from pika import callback as pika_callback\n  File \"/usr/lib/python2.7/dist-packages/pika/callback.py\", line 8, in <module>\n    from pika import frame\n  File \"/usr/lib/python2.7/dist-packages/pika/frame.py\", line 7, in <module>\n    from pika import spec\n  File \"/usr/lib/python2.7/dist-packages/pika/spec.py\", line 16, in <module>\n    from pika import data\n  File \"/usr/lib/python2.7/dist-packages/pika/data.py\", line 10, in <module>\n    from pika.compat import PY2, PY3\nImportError: No module named compat. Well, the offending line appears to be this:\nfrom pika.adapters import TornadoConnection\nIt doesn't get a chance to run anything else as it dies right there.. Okay, we actually build pika out of a forked repository on Github.  For this issue, I'm testing against https://github.com/vrtsystems/pika/tree/bugfix/WH-378-close-hang-fix where I have merged the latest master branch.\nThe diff between this branch; and current pika master is not huge:\n``\n$ git log --oneline upstream/master -1\n0e13941 (pika/master, origin/upstream/master, upstream/master) Merge pull request #947 from vitaly-krugl/fix-multiple-timers-same-deadline\n$ git log --oneline HEAD -3\nf127c53 (HEAD -> bugfix/WH-378-close-hang-fix, origin/bugfix/WH-378-close-hang-fix) WH-388: changelog: Version bump to 0.99.0\ncc93128 WH-388: Merge branch 'upstream/master' into bugfix/WH-378-close-hang-fix\n0e13941 (pika/master, origin/upstream/master, upstream/master) Merge pull request #947 from vitaly-krugl/fix-multiple-timers-same-deadline\n$ git diff upstream/master..HEAD\ndiff --git a/debian/changelog b/debian/changelog\nnew file mode 100644\nindex 0000000..aa591e0\n--- /dev/null\n+++ b/debian/changelog\n@@ -0,0 +1,16 @@\n+pika (0.99.0) unstable; urgency=low\n+  * Pull upstream release 1.0.0b1 to address issues with handling channel.close.\n+ -- Stuart Longland <stuartl@vrt.com.au>  Fri, 16 Feb 2018 15:22:01 +1000\n+\n+pika (0.10.0-1) unstable; urgency=low\n+\n+  * Weakly reference Connection from Channel objects to prevent garbage\n+    collection cycles.\n+\n+ -- Stuart Longland <stuartl@vrt.com.au>  Wed, 30 Nov 2016 09:20:12 +1000\n+\n+pika (0.10.0) unstable; urgency=low\n+\n+  * source package automatically created by stdeb 0.8.5\n+\n+ -- Stuart Longland <stuartl@vrt.com.au>  Thu, 17 Nov 2016 22:59:17 +0000\ndiff --git a/debian/compat b/debian/compat\nnew file mode 100644\nindex 0000000..ec63514\n--- /dev/null\n+++ b/debian/compat\n@@ -0,0 +1 @@\n+9\ndiff --git a/debian/control b/debian/control\nnew file mode 100644\nindex 0000000..aa10310\n--- /dev/null\n+++ b/debian/control\n@@ -0,0 +1,36 @@\n+Source: pika\n+Maintainer: Gavin M. Roy <gavinmroy@gmail.com>\n+Section: python\n+Priority: optional\n+Build-Depends: python-setuptools (>= 0.6b3), python-all (>= 2.6.6-3), debhelper (>= 7)\n+Standards-Version: 3.9.1\n+\n+\n+\n+Package: python-pika\n+Architecture: all\n+Depends: ${misc:Depends}, ${python:Depends}\n+Description: Pika Python AMQP Client Library\n+ Pika, an AMQP 0-9-1 client library for Python\n+ =============================================\n+ .\n+ |Version| |Downloads| |Status| |Coverage| |License|\n+ .\n+ Introduction\n+ -------------\n+ Pika is a pure-Python implementation of the AMQP 0-9-1 protocol that tries\n+ to stay fairly independent of the underlying network support library.\n+ .\n+ - Python 2.6+ and 3.3+ are supported.\n+ .\n+ - Since threads aren't appropriate to every situation, it doesn't\n+   require threads. It takes care not to forbid them, either. The same\n+   goes for greenlets, callbacks, continuations and generators. It is\n+   not necessarily thread-safe however, and your mileage will vary.\n+ .\n+ - People may be using direct sockets, plain oldselect()`,\n+   or any of the wide variety of ways of getting network events to and from a\n+   python application. Pika tries to stay compatible with all of these, and to\n+\n+\n+\ndiff --git a/debian/rules b/debian/rules\nnew file mode 100755\nindex 0000000..dbaa2c9\n--- /dev/null\n+++ b/debian/rules\n@@ -0,0 +1,31 @@\n+#!/usr/bin/make -f\n+\n+# This file was automatically generated by stdeb 0.8.5 at\n+# Thu, 17 Nov 2016 22:59:17 +0000\n+\n+%:\n+   dh $@ --with python2 --buildsystem=python_distutils\n+\n+\n+override_dh_auto_clean:\n+   python setup.py clean -a\n+   find . -name *.pyc -exec rm {} \\;\n+\n+\n+\n+override_dh_auto_build:\n+   python setup.py build --force\n+\n+\n+\n+override_dh_auto_install:\n+   python setup.py install --force --root=debian/python-pika --no-compile -O0 --install-layout=deb\n+\n+\n+\n+override_dh_python2:\n+   dh_python2 --no-guessing-versions\n+\n+\n+\n+\ndiff --git a/debian/source/format b/debian/source/format\nnew file mode 100644\nindex 0000000..163aaf8\n--- /dev/null\n+++ b/debian/source/format\n@@ -0,0 +1 @@\n+3.0 (quilt)\ndiff --git a/debian/source/options b/debian/source/options\nnew file mode 100644\nindex 0000000..bcc4bbb\n--- /dev/null\n+++ b/debian/source/options\n@@ -0,0 +1 @@\n+extend-diff-ignore=\".egg-info$\"\n\\ No newline at end of file\ndiff --git a/pika/channel.py b/pika/channel.py\nindex 7fc501a..63ed468 100644\n--- a/pika/channel.py\n+++ b/pika/channel.py\n@@ -8,6 +8,7 @@ implementing the methods and behaviors for an AMQP Channel.\n import collections\n import logging\n import uuid\n+import weakref\nimport pika.frame as frame\n import pika.exceptions as exceptions\n@@ -60,8 +61,7 @@ class Channel(object):\n         self._validate_rpc_completion_callback(on_open_callback)\n     self.channel_number = channel_number\n\n\nself.callbacks = connection.callbacks\nself.connection = connection\n\nself._connection = weakref.ref(connection)\n # Initially, flow is assumed to be active\n self.flow_active = True\n\n@@ -90,6 +90,27 @@ class Channel(object):\n     # opaque cookie value set by wrapper layer (e.g., BlockingConnection)\n     # via _set_cookie\n     self._cookie = None\n+        LOGGER.debug('Creating channel %d', self.channel_number)\n+\n+    def repr(self):\n+        return '<%s.%s #%d>' % (\n+                self.class.module,\n+                self.class.name,\n+                self.channel_number\n+        )\n+\n+    def del(self):\n+        LOGGER.debug('Destroying channel %d', self.channel_number)\n+\n+    @property\n+    def connection(self):\n+        \"\"\"Return the connection this channel belongs to.\"\"\"\n+        return self._connection()\n+\n+    @property\n+    def callbacks(self):\n+        \"\"\"Return the callback manager for this channel.\"\"\"\n+        return self.connection.callbacks\ndef int(self):\n     \"\"\"Return the channel object as its channel number\n@@ -977,6 +998,7 @@ class Channel(object):\ndef _cleanup(self):\n     \"\"\"Remove all consumers and any callbacks for the channel.\"\"\"\n+        LOGGER.debug('Cleaning up channel %d', self.channel_number)\n     self.callbacks.process(self.channel_number,\n                            self._ON_CHANNEL_CLEANUP_CB_KEY, self,\n                            self)\n```\n\n\nThe other changes there concern a memory leak we had with pika, in that it wouldn't clean up the channels properly after they were closed.\nThe above branch gets built as a Debian package dpkg-buildpackage -us -uc -b then the resulting package deployed.  When my Python code runs, it runs the following Python statement:\nfrom pika.adapters import TornadoConnection\nThat's just importing the Pika library, nothing else.  That's where it fails.. No worries, just doing a build now.. Okay, looks like there's now an issue to address regarding the parameters we supply to pika:\n2018-02-26 13:52:16,996 mm_da.amqp.amqpconnection.AMQPConnection[amqpconnection.py: 409] ERROR FAILED CONNECTION\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/mm_da/amqp/amqpconnection.py\", line 390, in _on_connect\n    backpressure_detection=None)\n  File \"/usr/lib/python2.7/dist-packages/pika/connection.py\", line 647, in __init__\n    self.backpressure_detection = backpressure_detection\n  File \"/usr/lib/python2.7/dist-packages/pika/connection.py\", line 209, in backpressure_detection\n    'but got %r' % (value,))\nTypeError: backpressure_detection must be a bool, but got None\nBut otherwise, we're over the hump caused by setup.py not including pika.compat. :-). I'll chase the above bug down in the morning, but I think now any further problems are small API changes that needs to be taken into account in my code.. Okay, so further testing today (on a dev instance, not production this time), it seems we're not out of the woods:\n2018-02-27 00:06:07,627 tornado.application[ioloop.py: 638] ERROR Exception in callback <functools.partial object at 0x7fe3ed84c2b8>\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/mm_da/amqp/amqpexchange.py\", line 238, in declare\n    self._channel_state_machine.declare_ev()\n  File \"/usr/lib/python2.7/dist-packages/fysom/__init__.py\", line 303, in fn\n    self.transition()\n  File \"/usr/lib/python2.7/dist-packages/fysom/__init__.py\", line 298, in _tran\n    self._after_event(e)\n  File \"/usr/lib/python2.7/dist-packages/fysom/__init__.py\", line 329, in _after_event\n    return getattr(self, fnname)(e)\n  File \"/usr/lib/python2.7/dist-packages/fysom/__init__.py\", line 89, in _callback\n    return func(obj, *args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/mm_da/amqp/amqpexchange.py\", line 58, in _on_declare\n    arguments=self._exchange_args)\nTypeError: exchange_declare() got multiple values for keyword argument 'passive'\n2018-02-27 00:06:07,628 tornado.application[ioloop.py: 638] ERROR Exception in callback <functools.partial object at 0x7fe3ed84c3c0>\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/tornado/ioloop.py\", line 605, in _run_callback\n    ret = callback()\n  File \"/usr/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/mm_da/amqp/amqpqueue.py\", line 177, in declare\n    self._channel_state_machine.declare_ev()\n  File \"/usr/lib/python2.7/dist-packages/fysom/__init__.py\", line 303, in fn\n    self.transition()\n  File \"/usr/lib/python2.7/dist-packages/fysom/__init__.py\", line 298, in _tran\n    self._after_event(e)\n  File \"/usr/lib/python2.7/dist-packages/fysom/__init__.py\", line 329, in _after_event\n    return getattr(self, fnname)(e)\n  File \"/usr/lib/python2.7/dist-packages/fysom/__init__.py\", line 89, in _callback\n    return func(obj, *args, **kwargs)\n  File \"/usr/lib/python2.7/dist-packages/mm_da/amqp/amqpqueue.py\", line 55, in _on_declare\n    arguments=self._queue_args)\nTypeError: queue_declare() got multiple values for keyword argument 'passive'\nIt seems declaring queues and exchanges is now broken because Python is confusing arguments.\namqpqueue.py:\ndef _on_declare(self, event):\n        self._log.debug('Declaring exchange %s', self._name)\n        self._channel.exchange_declare(\n                # callback\n                self._channel_state_machine.declared_ev,\n                # exchange name\n                self._name,\n                # exchange type\n                self._type,\n                # other args\n                passive=self._passive,\n                durable=self._durable,\n                auto_delete=self._auto_delete,\n                internal=self._internal,\n                arguments=self._exchange_args)\nand amqpexchange.py:\ndef _on_declare(self, event):\n        self._log.debug('Declaring exchange %s', self._name)\n        self._channel.exchange_declare(\n                # callback\n                self._channel_state_machine.declared_ev,\n                # exchange name\n                self._name,\n                # exchange type\n                self._type,\n                # other args\n                passive=self._passive,\n                durable=self._durable,\n                auto_delete=self._auto_delete,\n                internal=self._internal,\n                arguments=self._exchange_args)\nI get the same thing if all arguments are keyword arguments (which is what I originally had).. Okay, found that one easy enough\u2026 callback goes to the end.  Final trip-up was no_ack:\nFile \"/usr/lib/python2.7/dist-packages/mm_da/amqp/amqpqueue.py\", line 148, in _on_consume\n    **consumer_opts)\nTypeError: basic_consume() got an unexpected keyword argument 'no_ack'\nIt might be nice to have no_ack also be accepted in newer pika with perhaps a warning emitted.  I'm thinking something along the lines of:\n```\nimport warnings\n\u2026\nclass Channel(\u2026):\n    \u2026\n    def basic_consume(self, \u2026 auto_ack=False, **kwargs):\n        if 'no_ack' in kwargs:\n            warnings.warn('basic_consume no_ack is now called auto_ack')\n            auto_ack = kwargs.pop('no_ack')\n        \u2026\n```\nThen pika is backward compatible, otherwise this is a breaking change.. No problems there\u2026 it's a package that my workplace depends rather heavily on, so it's in my interest to get things right. :-). ",
    "robertodecurnex": "I am too!!! Weird error indeed. \nI'll work over it and check whats wrong.\n. ",
    "shiroyuki": "@gmr I currently updated the code to address the refactoring and the message. I'll address the point whether ready() should better return False once I get back home.\n. @gmr I tried to make ReadPoller.ready to return false but it still runs into the same RuntimeError: concurrent poll() invocation. Idea?\n``` pytb\n  File \"/home/jnopporn/Projects/demo/ws/amqp.py\", line 110, in run\n    ch.start_consuming()\n  File \"/home/jnopporn/Projects/pika/pika/adapters/blocking_connection.py\", line 969, in start_consuming\n    self.connection.process_data_events()\n  File \"/home/jnopporn/Projects/pika/pika/adapters/blocking_connection.py\", line 254, in process_data_events\n    if self._handle_read():\n  File \"/home/jnopporn/Projects/pika/pika/adapters/blocking_connection.py\", line 361, in _handle_read\n    if self._read_poller.ready():\n  File \"/home/jnopporn/Projects/pika/pika/adapters/blocking_connection.py\", line 43, in inner\n    return f(args, *kwargs)\n  File \"/home/jnopporn/Projects/pika/pika/adapters/blocking_connection.py\", line 98, in ready\n    events = self.poller.poll(self.poll_timeout)\n```\n. ",
    "earthquakesan": "I have the same problem in my case.\nWrapper: https://github.com/AKSW/CSV2RDF-WIKI/blob/master/csv2rdf/messaging/init.py\nListener: https://github.com/AKSW/CSV2RDF-WIKI/blob/master/csv2rdf/tabular/sparqlify_java_handler.py (this one got stuck)\nLooking in the docs right now to figure out what is the problem.\n. ",
    "happybob007": "I created versions of connection.send_method() and connection._send_frame() that buffer the frames for a method and then push them onto the outbound_buffer in an uninterupted operation, rather than with a calls to _flush_outbound() in between.  With this approach, I'm unable to reproduce the out-of-order frames experienced earlier.\nI have replaced the connection.send_method() method with a wrapper that just dispatches to the buffered or unbuffered (stock) version of connection.send_method().  If this approach works for more than just me, I'll make a patch that drops the unbuffered versions.  I've left them in because I don't know for sure that the buffering approach doesn't have some other side effect that breaks functionality for other people.\n. ```\ndiff -u pika-9-14-stock/lib/python2.7/site-packages/pika/connection.py pika9-14/lib/python2.7/site-packages/pika/connection.py\n--- pika-9-14-stock/lib/python2.7/site-packages/pika/connection.py  2014-10-02 01:22:15.000000000 -0700\n+++ pika9-14/lib/python2.7/site-packages/pika/connection.py 2014-10-02 01:20:48.217818286 -0700\n@@ -1491,7 +1491,8 @@\n         if self.params.backpressure_detection:\n             self._detect_backpressure()\n\ndef _send_method(self, channel_number, method_frame, content=None):\n+\ndef _send_method_unbuffered(self, channel_number, method_frame, content=None):\n         \"\"\"Constructs a RPC method frame and then sends it to the broker. :param int channel_number: The channel number for the frame\n\n@@ -1507,7 +1508,10 @@\n         return\n length = len(content[1])\n\n\nself._send_frame(frame.Header(channel_number, length, content[0]))\ntry:\nself._send_frame(frame.Header(channel_number, length, content[0]))\nexcept Exception, e:\nraise\n     if content[1]:\n         chunks = int(math.ceil(float(length) / self._body_max_length))\n         for chunk in range(0, chunks):\n@@ -1518,6 +1522,84 @@\n             self._send_frame(frame.Body(channel_number,\n                                         content[1][start:end]))\n\n\n\n+\n+    def _send_frame_buffered(self, frame_value, frame_acc):\n+        \"\"\"An implementation of _send_frame that does not touch outbound_buffer\n+        \"\"\"\n+        if frame_acc is None:\n+            frame_acc = []\n+        if self.is_closed:\n+            LOGGER.critical('Attempted to send frame when closed')\n+            return\n+        marshaled_frame = frame_value.marshal()\n+        self.bytes_sent += len(marshaled_frame)\n+        self.frames_sent += 1\n+        # Append to the accumulator instead of the outbound_buffer.\n+        frame_acc.append(marshaled_frame)\n+        # self.outbound_buffer.append(marshaled_frame)\n+        self._flush_outbound()\n+        if self.params.backpressure_detection:\n+            self._detect_backpressure()\n+        return frame_acc\n+\n+\n+    def _send_method_buffered(self, channel_number, method_frame, content=None):\n+        \"\"\"Constructs a RPC method frame and then sends it to the broker.\n+\n+        :param int channel_number: The channel number for the frame\n+        :param pika.object.Method method_frame: The method frame to send\n+        :param tuple content: If set, is a content frame, is tuple of\n+                              properties and body.\n+\n+        \"\"\"\n+        LOGGER.debug(\"\\n ===== Entered conn:_send_method_buffered()\")\n+        # Accumulate the frames for this method here.\n+        frames = []\n+        self._send_frame_buffered(frame.Method(channel_number, method_frame), frames)\n+\n+        # If it's not a tuple of Header, str|unicode then return\n+        if not isinstance(content, tuple):\n+            self._flush_buffered_frames(frames)\n+            return\n+\n+        length = len(content[1])\n+        try:\n+            self._send_frame_buffered(frame.Header(channel_number, length,\n+                content[0]), frames)\n+        except Exception, e:\n+            raise\n+        if content[1]:\n+            chunks = int(math.ceil(float(length) / self._body_max_length))\n+            LOGGER.debug(\"Number of body chunks: %d\", chunks)\n+            for chunk in range(0, chunks):\n+                start = chunk * self._body_max_length\n+                end = start + self._body_max_length\n+                if end > length:\n+                    end = length\n+                self._send_frame_buffered(frame.Body(channel_number,\n+                                            content[1][start:end]), frames)\n+        self._flush_buffered_frames(frames)\n+\n+\n+    def _flush_buffered_frames(self, frame_acc):\n+        \"\"\"Put accumulated marshaled frames onto the outbound buffer.\n+        This might still be subject to a race condition on access to the\n+        outbound_buffer, but I think it is significantly lessened.\n+        \"\"\"\n+        for marshaled_frame in frame_acc:\n+            self.bytes_sent += len(marshaled_frame)\n+            self.frames_sent += 1\n+            self.outbound_buffer.append(marshaled_frame)\n+            LOGGER.debug(\"added marshaled frame, %d bytes\", len(marshaled_frame))\n+        self._flush_outbound()\n+        if self.params.backpressure_detection:\n+            self._detect_backpressure()\n+\n+\n+    def _send_method(self, args):\n+        return self._send_method_buffered(args)\n+        # return self._send_method_unbuffered(*args)\n+\n     def _set_connection_state(self, connection_state):\n         \"\"\"Set the connection state.\n```\n. Once concern I have with this approach is that it introduces additional latency that is unnecessary in the case of large messages being published by calls to basic_publish() that are outside of subscriber callbacks.  Without inspecting the call stack on each call to basic_publish(), I don't see an obvious way to detect that the caller isn't coming from within a callback.\nFor smallish messages, the impact should be minimal.  However, if the message is large, then the time needed to completely marshal the message before its first frame can be delivered may have a substantial impact.\nPerhaps an optional parameter could be added to basic_publish(), indicating that it should use either the buffered or unbuffered path would be the best way to address the issue of increased latency due to buffering.  In that case, my preference would be for it to default to the safe/bufferred route.\n. Closing\n. ",
    "rajagopalans": "small correction.\nI found that method is pika.spec.Basic.Deliver\n. I clearly missed seeing it. Sorry\n On Nov 10, 2014 7:08 PM, \"Gavin M. Roy\" notifications@github.com wrote:\n\nWe'll add more documentation around this, but it's worth noting the\nexamples on http://pika.readthedocs.org generally have docstrings for\nthese methods like:\ndef on_message(self, channel, basic_deliver, properties, body):\n    \"\"\"Invoked by pika when a message is delivered from RabbitMQ. The        channel is passed for your convenience. The basic_deliver object that        is passed in carries the exchange, routing key, delivery tag and        a redelivered flag for the message. The properties passed in is an        instance of BasicProperties with the message properties and the body        is the message that was sent.        :param pika.channel.Channel channel: The channel object        :param pika.Spec.Basic.Deliver: basic_deliver method        :param pika.Spec.BasicProperties: properties        :param str|unicode body: The message body        \"\"\"\n    LOGGER.info('Received message # %s from %s: %s',\n                basic_deliver.delivery_tag, properties.app_id, body)\n    self.acknowledge_message(basic_deliver.delivery_tag)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/issues/510#issuecomment-62480084.\n. \n",
    "nrolans": "I had this exact problem when I tried using different channels of the same connection in several threads, then I read the FAQ...\n\nIs Pika thread safe?\nPika does not have any notion of threading in the code. If you want to use Pika with threading, make sure you have a Pika connection per thread, created in that thread. It is not safe to share one Pika connection across threads.\n. \n",
    "ttezel": ":+1: \n. ",
    "zbenjamin": "Ping on this PR\n. ",
    "jimhorng": "@gmr , thanks for comments, \n1) do you mean adding another error capturer around\npika/adapters/base_connection.py \ndef _handle_error(self, error_value):\nfor this kind of error? and set a flag to skip raising IncompatibleProtocolError\npika/adapters/base_connection.py\n_check_state_on_disconnect()\nso that self._on_connection_closed can be called to trigger connection.on_close() callback registered by client?\n2) or create a new callback listener like on_connection_error around \npika/adapters/base_connection.py \ndef _handle_error(self, error_value):\n?\n. @gmr , got it, thanks for taking time to look at this PR anyway.\nI'l try to draft more solid scenario for reproducing this problem in future since I can reproduce it easily :(\n. ",
    "ayounes": "I have one connection per thread, each thread uses the same style call I mentioned above, and the queries and processing time between calls to RMQ are about 20ms, so they are short.  There are no additional sleeps.  What is the limit on the period of time for it to block.  I had implemented this workaround to see if it helps, when I was batch sending a bunch of messages.  What I mean by that is that I would grab 1000 msgs, then send them all at once.  While I was building each message, I would call this code every 100 msgs:\n#Prevent UNEXPECTED_FRAME ERROR \n            #https://github.com/josegonzalez/python-beaver/issues/264\n            #https://github.com/pika/pika/issues/397\n            number_of_rows += 1\n            if number_of_rows % 100 == 0:\n                data_connection.process_data_events()\nBTW, the process data events would get called about every 2s based on my time measurements.  I removed it since I switched to sending each message at the moment I build it, and that is about 20ms between each message.\nThe problem is intermittent, it does not happen all the time, but it happens frequently enough that it affects the throughput of the thread I mentioned.  \nI am looking into reducing the message size to something smaller however, that is not the ideal solution I am looking for.\n. I stand corrected, i did  pip list and I am running pika (0.9.13).\nAnyway, I will try rabbitpy and let you know.  Hopefully it won't be a huge code change.\nThanks...Amro\n. I made three changes in my code that seem to have alleviated the issue. It only occurs rarely when I first start the program but it does not seem to happen again.  Need to soak the code to make sure.  None of the steps below worked alone.\n1.  Upgrade from 0.9.13 to 0.9.14\n2. Reduced the average size of my messages.  Rather than send everything every time, I make sure that for that type of data, only the fields that will be used on the other end are sent.\n3.  Imeplemented this after the basic_publish() call in my loop:\nlast_process_event_call = 0  \n           while(some_condition):\n               <....snipped...>\n               data_connection.basic_publish(<...snipped...>)\n               if (time.time() - last_process_event_call) > 6:\n                      data_connection.process_data_events()\n                      last_process_event_call = time.time()\n. It turns out there was a connectivity issue between the two.  A firewall in between was blocking the connection which led to PIKA reporting a protocol error.\n. ",
    "bigmstone": "Not that this exactly helps Pika, but for those searching for a solution to this (admittedly very specific) problem you can use RabbitPy with a fairly simple reconnect loop and it works like a champ. Luckily for us the AMQP connection was modularly built so it was easy to swap out and keep the pika code around if we need to swap back in the future.\nI'll leave this open if the pika developers want to use this to tie to an eventual fix. If not feel free to close.\n. As I outlined in my original post the issue is with how the library calls the socket after it's been disconnected. Short of rewriting that portion of the lib there won't be a way to get it to work on Windows. My solution was to use rabbitpy which worked well for us.\nHere is the bit of code being called:\nhttps://github.com/pika/pika/blob/master/pika/adapters/base_connection.py#L299\nHere is the Python documentation on that method:\nhttps://docs.python.org/2/library/socket.html#socket.socket.fileno\nFrom the docs:\n\"Under Windows the small integer returned by this method cannot be used where a file descriptor can be used (such as os.fdopen()). Unix does not have this limitation.\"\n. ",
    "wtarr": "I too am having this issue on windows using (v0.9.14) that same code example with a couple of modifications (passing parameters url and not using the reconnect code).  And that exception too always seems to occur on windows when I stop the rabbit broker service. \nThe pika exception logged is:\nERROR      2015-04-15 15:19:25,796 pika.adapters.base_connection  _handle_events                       330 : Error event 8, error(10038, 'An operation was attempted on something that is not a socket')\nThe exception I catch is:\nINFO       2015-04-15 15:19:25,796 main                       main                                 390 : Exception output 'NoneType' object has no attribute 'fileno'\nMy work around at the moment is to just wrap in while loop with a try catch (not pretty but I'm desperate)  and it will reconnect when I restart the service.\n'''python\ndef main():\n    logging.basicConfig(filename=\"async_consumer.log\", level=logging.INFO, format=LOG_FORMAT)\n```\ncredentials = pika.PlainCredentials(USERNAME, PASSWORD)\nparameters = pika.ConnectionParameters(host=HOST,\n                                       port=PORT,\n                                       virtual_host='/',\n                                       credentials=credentials,\n                                       connection_attempts=30,\n                                       retry_delay=5)\nrmqconsumer = ExampleConsumer(parameters)\nwhile True:\n    try:\n        rmqconsumer.run()\n        LOGGER.info('We have exited the run() method, will retry in 10 seconds')\n    except Exception as error:\n        LOGGER.info('Exception output %s an attempt to re-establish a connection will occur in 10 seconds', error)\n    time.sleep(10) # try try try again\n```\n''''\n. @vitaly-krugl, hey, I am currently testing connectivity issues by having rabbit running on a ubuntu machine hosted in a virtual machine which I then connect to from a windows guest via the script.\nTo break the connection, I simply stop the broker by running sudo service rabbitmq-server stop and that is when I get the error.\n\npython rabbit_events_to_couchbase_consumer.py\nTraceback (most recent call last):\n  File \"rabbit_events_to_couchbase_consumer.py\", line 393, in \n    main()\n  File \"rabbit_events_to_couchbase_consumer.py\", line 386, in main\n    rmqconsumer.run()\n  File \"rabbit_events_to_couchbase_consumer.py\", line 351, in run\n    self._connection.ioloop.start()\n  File \"C:\\Python27\\lib\\site-packages\\pika\\adapters\\select_connection.py\", line\n138, in start\n    self.poller.start()\n  File \"C:\\Python27\\lib\\site-packages\\pika\\adapters\\select_connection.py\", line\n302, in start\n    self.poll()\n  File \"C:\\Python27\\lib\\site-packages\\pika\\adapters\\select_connection.py\", line\n258, in poll\n    return self._handler(self.fileno, ERROR, error)\n  File \"C:\\Python27\\lib\\site-packages\\pika\\adapters\\base_connection.py\", line 33\n1, in _handle_events\n    self._handle_error(error)\n  File \"C:\\Python27\\lib\\site-packages\\pika\\adapters\\base_connection.py\", line 29\n9, in _handle_error\n    self.socket.fileno(), error_code)\nAttributeError: 'NoneType' object has no attribute 'fileno'\n. @vitaly-krugl This is a log where I connect and bind to an existing exchange, then stop the rabbit service.\n\nINFO       2015-04-16 08:00:46,614 pika.adapters.base_connection  _create_and_connect_to_socket        179 : Connecting to snipped\nINFO       2015-04-16 08:00:46,667 main                       on_connection_open                   114 : Connection opened\nINFO       2015-04-16 08:00:46,667 main                       add_on_connection_close_callback     86  : Adding connection close callback\nINFO       2015-04-16 08:00:46,667 main                       open_channel                         342 : Creating a new channel\nINFO       2015-04-16 08:00:46,667 main                       on_channel_open                      167 : Channel opened\nINFO       2015-04-16 08:00:46,667 main                       add_on_channel_close_callback        139 : Adding channel close callback\nINFO       2015-04-16 08:00:46,668 main                       setup_queue                          181 : Declaring queue snipped\nINFO       2015-04-16 08:00:46,668 main                       on_queue_declareok                   195 : Binding none to snipped with \nINFO       2015-04-16 08:00:46,670 main                       on_bindok                            325 : Queue bound\nINFO       2015-04-16 08:00:46,670 main                       start_consuming                      312 : Issuing consumer related RPC commands\nINFO       2015-04-16 08:00:46,670 main                       add_on_cancel_callback               205 : Adding consumer cancellation callback\nERROR      2015-04-16 08:00:57,502 pika.adapters.base_connection  _handle_error                        299 : Socket Error on fd 604: 10054\nWARNING    2015-04-16 08:00:57,502 pika.adapters.base_connection  _check_state_on_disconnect           160 : Socket closed when connection was open\nWARNING    2015-04-16 08:00:57,502 pika.connection                _on_disconnect                       1298: Disconnected from RabbitMQ at snipped (0): Not specified\nWARNING    2015-04-16 08:00:57,502 main                       on_connection_closed                 103 : Connection closed\nERROR      2015-04-16 08:00:57,503 pika.adapters.base_connection  _handle_events                       330 : Error event 8, error(10038, 'An operation was attempted on something that is not a socket')\n. ",
    "andrewelkins": "Can we get a 0.9.15 as well? \n. ",
    "awelzel": "This is with pika 0.9.14.\n. PR #672 fixes this.\n. #525 \n. > Have you examined the impact that your change to BaseConnection will have on all of the connection adapters?\nNot in too close detail as they all use _adapter_disconnect() from the base connection. I have tested the SelectConnection and confirmed it is still working and will indeed call on_open_error_callback when provided.\nA possibly user visible change is that for errors that happen after adapter_connect(), but before _on_connection_open(), a call to the provided on_open_error_callback will now be done for all adapters (see below for twisted). But, that seems to be the right thing to do and the reason for #525 to begin with.\nFurther, _handle_ioloop_stop and _init_connection_state will be properly called in above scenario.\nThe default implementation (_on_connection_error) will simply raise an exceptions.AMQPConnectionError as well, so this will keep the behavior.\nThe twisted connection uses its own _adapter_disconnect() so it's not affected. However, I think it has the same problem - on_open_error_callback won't be called for \"probably auth\" errors.\n. @vitaly-krugl , I have tested that PR #672 fixes the issue. This PR is no longer relevant.\n. @vitaly-krugl , ok, makes sense. Your commit changed this behavior though: Before it, it was possible to pass in a callback together with nowait=True - it was simply ignored.\nBut, I agree. It makes sense to bail out when passing in a callback and having nowait=True as this combination is invalid.\nThere are two unfortunate things:\n- callback for queue_declare() is a required parameter, so you have to pass None explicitly. Similar for queue_bind().\n- the exception message isn't very indicative about the issue.A populated callback must be accompanied by populated acceptable_replies certainly doesn't point to \"you should pass None for nowait=True\". Could check before calling _rpc()? Thoughts?\nAnyways, maybe just nitpicking ;)\n. @vitaly-krugl , I thought that simply putting into queue_declare()\nif nowait and callback is not None:\n   raise ValueError('Unexpected callback for nowait=True')\nwould be enough. And note it in the Changelog?\nHowever, given that it applies to more method with nowait and a callback, maybe _validate_channel_and_callback() should be extended with above condition and taking nowait=False as default, then modifying callers as appropriate?\n. Yes, thanks!\n\nOn 10 Jan 2016, at 20:48, vitaly-krugl notifications@github.com wrote:\n@awelzel, can this issue be closed?\n\u2014\nReply to this email directly or view it on GitHub.\n. @xiaopeng163 ,\n\nIIRC, pika nicely adheres to Python logging, so you may use either of the following:\n```\nreduce log level\nlogging.getLogger(\"pika\").setLevel(logging.WARNING)\nor, disable propagation\nlogging.getLogger(\"pika\").propagate = False\n```\nThe first one reduces the log level of the logger used by pika to WARNING. The second one disables propagation of log messages to the root logger. You should probably use the first one as it will still show error and critical/fatal logs from pika.\nThe python logging docs are very helpful:\nhttps://docs.python.org/2/library/logging.html\nHTH,\n   Arne\n. @vitaly-krugl ,\n\n[...]I simply change the log level for my own app and leave everyone else's alone like this: logging.getLogger(\"taurus\").setLevel(logging.DEBUG)\n\n...and you attach a handler to the taurus logger, not the root logger? Otherwise I don't understand it ;)\n. > Okay, here is an actual example via 2.7.10 python shell; observe that other_logger.debug(...) doesn't produce anything, as intended.\n@vitaly-krugl Thanks, indeed this works. Double checked the docs and it makes sense. cool!\nhttps://docs.python.org/2/howto/logging.html#logging-flow\n. @vitaly-krugl , thanks for checking. The commit message had the reason:\n\ntornado_connection: silence AMQPConnectionError in _adapter_disconnect\nIt's not possible to handle this exception anywhere and it's super noisy.\n18de5f8fcf1c25877522e3ed37404d13795fbc60\n\nAFAICT, if a AMQPConnectionError happens at that point inside a tornado callback, you as a user can't catch it anywhere reasonably well and it.\nHowever - I recently came across the following messages which seem to be caused by this change (in the scenario of #525 ):\nCRITICAL:pika.adapters.base_connection:Tried to handle an error where no error existed\nSeems some other code is relying for the exception to happen so it's not executed... :-/\n. > The title of the PR refers to ProbableAuthenticationError, but the exception being trapped here is more general.\nYes, that is true. I observed the problem only with the ProbableAuthenticationError. IMO, however, the on_open_error_callback should be called for any connection errors in cases where the connection has not been fully opened.\n. For torando I think only after applying b999636b4f1371268896840090a635a32fa4f980 - the code of the finally clause was skipped previously.\n. I used on_open_error_callback because at that point, the state of the connection is not in CONNECTION_OPEN yet. Also, I think you do not get a reason/message from the broker. The connection essentially times out. That's why ProbableAuthenticationError.\nI leave this decision to @gmr .\n. @vitaly-krugl , works for me - the following may be more succinct, YMMV.\nUnexpected callback for asynchronous (nowait) operation.\n. ",
    "rmb938": "~~It also seems like the callback for connection.add_timeout is not always called~~\nIf x-cancel-on-ha-failover is given to a consumer and add_on_cancel_callback is set. The callback is not called on failover\n. 0.9.14 pika with the Select connection adapter\n3.4.3 RabbitMQ\n. That is my understanding and testing it now I was actually doing something completely wrong so there isn't an issue.\n. I think it just cancels the consumer I don't think it actually closes the connection or channel.\n. Well the cancel on ha failover is called when the queue you're consuming on moves to another node due to failover. So if you have a cluster of node A,B and C and your client is connected to C but the queue is \"living\" on A, if node A fails the queue will get moved to B or C. When that happens node C will tell the client that the queue has failed over. \nHowever if the client is connected to C and that node fails it now has to manually reconnect to another node. The Java client will just try and connect to the next server in the list,reopen the connections (not the channels) and call the consumer cancel event to let the programmer to decide what to do at that point.\nSince pika doesn't do this natively I just keep my own list of servers and when a consumer throws a connection exception I just have it try and open a connection to a server that is next in my list. So this feature is not really a large priority but it would be nice to keep in line on the way the official java client does things.\n. Yea no problem. The code the it is part of is specific to my system so if the bellow code is not enough to figure it out please let me know.\n``` python\nwhile not self.stopped:\n            if self.connection is not None:\n                self.logger.debug(\"Restarting IO Loop\")\n                if not self.connection.is_closed and not self.connection.is_closing:\n                    self.connection.close()\n            try:\n                if self.connection is None:\n                    self.connection = self.connect()\n                else:\n                    self.connection = self.connect(True)\n            except AMQPConnectionError as e:\n                self.logger.error(\"Rabbitmq Connection error \" + e.message)\n                continue\n            if self.queue.startswith(\"amq.\"):\n                self.queue = \"\"\n            self.connection.ioloop.start()\ndef connect(self, reconnect=False):\n        self.logger.debug(\"Creating connection\")\n        connection = self.rabbitmq.asyncconnection(self.on_connection_open, reconnect)\n        connection.add_on_open_error_callback(self.on_connection_open_error)\n        self.logger.debug(\"Connection created\")\n        return connection\ndef asyncconnection(self, callback, reconnect=False):\n        if reconnect:\n            self.paramIndex += 1\n            if self.paramIndex >= len(self.connectionParams):\n                self.paramIndex = 0\n        connection = pika.SelectConnection(parameters=self.connectionParams[self.paramIndex],\n                                           on_open_callback=callback, stop_ioloop_on_close=False)\n        return connection\n```\nconnectionParams in asyncconnection is a list of pika.ConnectionParameters\n. Actually seems like it was my fault. Was mixing channels from the SelectConnection\n. Calling ioloop.stop() in another thread.\n. ```\nimport pika\nimport threading\ndef main():\n        credentials = pika.PlainCredentials(username=\"root\", password=\"asdasd\")\n        params = pika.ConnectionParameters(host=\"database\", port=5672, virtual_host=\"/\", credentials=credentials)\n        c = pika.SelectConnection(parameters=params, on_open_callback=(lambda c: c.channel(lambda ch: None)))\n        t = threading.Timer(5, c.ioloop.stop)\n        t.start()\n        c.ioloop.start()\n        c.close()\nmain()\n```\nTraceback (most recent call last):\n  File \"test\", line 13, in <module>\n    main()\n  File \"test\", line 11, in main\n    c.close()\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 95, in close\n    super(BaseConnection, self).close(reply_code, reply_text)\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/connection.py\", line 690, in close\n    self._close_channels(reply_code, reply_text)\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/connection.py\", line 910, in _close_channels\n    self._channels[channel_number].close(reply_code, reply_text)\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/channel.py\", line 386, in close\n    self._on_closeok, [spec.Channel.CloseOk])\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/channel.py\", line 1046, in _rpc\n    self._send_method(method_frame)\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/channel.py\", line 1057, in _send_method\n    self.connection._send_method(self.channel_number, method_frame, content)\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/connection.py\", line 1503, in _send_method\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/connection.py\", line 1490, in _send_frame\n    self._flush_outbound()\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/adapters/select_connection.py\", line 73, in _flush_outbound\n    self.ioloop.poller._manage_event_state()\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/adapters/base_connection.py\", line 390, in _manage_event_state\n    self.event_state)\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/adapters/select_connection.py\", line 183, in update_handler\n    self.poller.update_handler(fileno, events)\n  File \"/pub/cmt/venv/lib/python2.7/site-packages/pika/adapters/select_connection.py\", line 427, in update_handler\n    self._poll.modify(fileno, self.events)\nIOError: [Errno 2] No such file or directory\nPlatform is Redhat 7\n. The issue does not seem to happen on OSX 10.10.* so it could just be a rhel or linux issue.\n. That would make sense. Thanks for making a fix.\n. +1 been waiting for this for a while\n. ",
    "pmphnd": "Having multiple hosts in connection would be extremely useful for failover (in case loadbalancer is not an option). Looking forward.\n. ",
    "awasilyev": "@rmb938, could you please share your code for reconnecting to the list of rabbitmq hosts?\n. ",
    "luiscoms": "How can I use multiple hosts on SelectConnection?\nLike:\npython\npika.SelectConnection(configs, self.on_connection_open). ",
    "lokeshh": "@vitaly-krugl I did like you said. Following is my configuration:\n```\n        conns = []\n        credentials = pika.PlainCredentials(self.username, self.password)\n        ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLSv1)\n        ssl_context.verify_mode = ssl.CERT_REQUIRED\n        ssl_context.load_cert_chain(certfile=\"cert.pem\", keyfile=\"key.pem\")\n        ssl_context.load_verify_locations(cafile=\"cacert.pem\")\n        for node_with_port in self.nodes.split(','):\n            node = node_with_port.split(':')[0]\n            parameters = pika.ConnectionParameters(\n                ssl_options=pika.SSLOptions(ssl_context),\n                credentials=credentials,\n                host=node,\n                port=5671\n            )\n            conns.append(parameters)\n    LOGGER.info('Connecting to rabbit')\n\n    connection = pika.SelectConnection.create_connection(conns, self.on_connection_open)\n\n``\nThe above code works but when I doconnection.ioloop.start()` I get following error:\n```\nAttributeError: 'AMQPConnectionWorkflow' object has no attribute 'ioloop'\n```. ",
    "robochat": "There's also another problem with this part of the code though. I always see a warning message from the BaseConnection._adapter_disconnect() method (actually  _check_state_on_disconnect()), that says\nWARNING -  ... -  Unknown state on disconnect: 6\nconnection_state = 6 meaning CONNECTION_CLOSING which makes sense since  _adapter_disconnect() is called before Connection._on_disconnect() (which sets the connection_state as closed). It almost seems like in _on_connection_closed() we should be disconnecting the adapter after we run _on_disconnect() ie.\n```\n        # Invoke a method frame neutral close\n        self._on_disconnect(self.closing[0], self.closing[1])\n    # If this did not come from the connection adapter, close the socket\n    if not from_adapter:\n        self._adapter_disconnect()\n\n```\nBut there are probably issues with swapping the order of the calls that I'm not aware of.\n. If I understand correctly, the key line is in base_connection._check_state_on_disconnect(self)\n    elif not self.is_closed and not self.is_closing:\nYes, I think that should be enough. I will try to test it soon. Together with your change to the passing of (reply_code, reply_text) values: https://github.com/pika/pika/pull/537, we could start getting the proper shutdown messages now.\nThanks\n. ",
    "markfennema": "Hello,\nThat's essentially what I'm thinking.\nNormally I'd agree with you and use an event driven system. \nFor this specific project, performance isn't a huge concern. Especially since the messages in Rabbit tend to be a bit larger, so the time spent polling an in memory list is minimal compared to the time spent processing them.\nThe priorities of this system are high reliability and easy maintenance. Polling avoids a lot of potential pitfalls (timing bugs come to mind) and is much simpler to debug if something does come up. In addition, it makes log messages easier to understand and makes the system generally simpler. \nFor this project both options were evaluated, and polling came out on top.\nI understand this is not the general case, and if the general opinion is that such a feature is too specific too include that seems reasonable.\n. I'm a little bit confused about how my changes hurt coveralls testing of init.py, but I don't think it's a problem.\n. Yes. the code for option 2 is actually all in blocking_connection.py, since that's how the blocking_connection generator works, so I could just copy and paste out of there. \nThis issue isn't particularly important for me, since I can do that or use one of a few other solutions. I just thought it would be nice for it to be included in Pika, so I tossed up a pull request.\nIt seems strange to me to have a generator with no way to check if there's anything in it, especially since calling it when there's nothing in it results in a tight polling loop. This function allows a user to define their own polling loop, which can be designed with their use case in mind, without them having to copy and paste significant amounts of code out of Pika or having to write their own.\n. Fair. I was operating under the (perhaps false) assumption that the generator was unlikely to change in the future. If you think it has potential to change then it's probably not worth adding the function.\nThanks.\n. ",
    "radzhome": "I'm seeing the error when using blocking connection with multiprocessing in pika 0.11.2. After a few mins of running ok I get:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/pipeline/python/daemons/split_files.py\", line 126, in mp_file\n    queue_compare_tickets(broker_data, diag, rows, filename)\n  File \"/pipeline/python/daemons/../lib/queue_helpers.py\", line 51, in queue_compare_tickets\n    'tickets': tix,\n  File \"/usr/local/virtualenvs/pipeline_py3/src/fanx-service-clients/serviceclients/queue/rabbit.py\", line 127, in publish\n    self._channel.basic_publish(exchange=exchange, routing_key=exchange, body=data, properties=props)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 2077, in basic_publish\n    mandatory, immediate)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 2164, in publish\n    self._flush_output()\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 1250, in _flush_output\n    *waiters)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 456, in _flush_output\n    self._impl.ioloop.process_timeouts()\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/select_connection.py\", line 224, in process_timeouts\n    self._poller.process_timeouts()\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/select_connection.py\", line 366, in process_timeouts\n    timer['callback']()\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/heartbeat.py\", line 88, in send_and_check\n    return self._close_connection()\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/heartbeat.py\", line 125, in _close_connection\n    text)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/connection.py\", line 1974, in _on_terminate\n    self._adapter_disconnect()\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/select_connection.py\", line 110, in _adapter_disconnect\n    self.ioloop.remove_handler(self.socket.fileno())\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/select_connection.py\", line 202, in remove_handler\n    self._poller.remove_handler(fileno)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/select_connection.py\", line 418, in remove_handler\n    self._unregister_fd(fileno, events_to_clear=events_cleared)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/select_connection.py\", line 983, in _unregister_fd\n    self._poll.unregister(fileno)\nFileNotFoundError: [Errno 2] No such file or directory\nI'll see if ignoring the error for now is ok.. I think my issue is related to this one - https://github.com/pika/pika/issues/885  . I see start_consuming() doesn't raise an exception even though basic_cancel was called anymore. This seems to be fine in pika 0.10 though.. I have no issues with 0.10.0 and my code sample, let me try latest build and see if its resolved. If not I'll give you an updated code sample.. Have you tried Erlang 20.2 and newer? 19.3?\nWhat version of Python are you using?\n OS CentOS Linux release 7.4.1708 (Core)\n Python 2.7.5 in virtualenv\n I have an override service file:\n[Service]\nLimitNOFILE=65536\nmy rabbit config is:\n```\n%% -- mode: erlang -*-\n%% ----------------------------------------------------------------------------\n%% RabbitMQ  Configuration File. /etc/rabbitmq/\n%% Source https://github.com/rabbitmq/rabbitmq-server/blob/master/docs/rabbitmq.config.example\n%% See https://www.rabbitmq.com/configure.html#configuration-file for details.\n%% ----------------------------------------------------------------------------\n[\n  {\n    rabbit, [\n      {loopback_users, []},\n      {log_levels, [{connection, error}, {channel, error}]},\n      {disk_free_limit, {mem_relative, 0.1}},\n      {tcp_listeners, [5672]},\n      {heartbeat, 0},\n      {frame_max, 0},\n      {vm_memory_high_watermark, 0.6},\n      {rates_mode, none},\n      {collect_statistics, none}\n    ]\n  }\n].\n```\nrabbit admin enabled. Here is my whole ansible provision:\n```\nImport PGP Key for rabbitmq repo\n\nname: Import new RPM key for rabbitmq repo\n  rpm_key: key=https://packagecloud.io/rabbitmq/erlang/gpgkey state=present\n  ignore_errors: yes\n  tags:\nrabbitmq\n\n\n\nEnable repo for later updates\n\nname: enable erlang repo for rabbitmq\n  template:\n    src: rabbitmq_erlang.repo\n    dest: /etc/yum.repos.d/rabbitmq_erlang.repo\n    owner: root\n    group: root\n    mode: 0644\n  tags:\nrabbitmq\n\n\n\nRabbit 3.6.15+ requires erlang 19.3.x, zero dependency erlang: https://github.com/rabbitmq/erlang-rpm/releases\nRabbit 3.7.2 issues started along with 20.1.7.1, same ioloop issues seem to exist on 3.7.0\n\n\nname: yum list version of installed erlang\n  shell: yum list installed erlang | grep erlang | awk '{print $2}' | cut -d'-' -f1 warn=no\n  register: erlang_version\n  tags:\n\nrabbitmq\n\n\n\nname: yum remove erlang\n  yum: pkg=erlang* state=absent update_cache=yes\n  when: erlang_version['stdout'] != '20.1.7'  # Reverted from 20.1.7.1 to 20.1.7\n  tags:\n\nrabbitmq\n\n\n\nBacked up at https://mirror.fanxchange.com/centos/rabbit/erlang-20.1.7.1-1.el7.centos.x86_64.rpm\nTrying https://github.com/rabbitmq/erlang-rpm/releases/download/v20.1.7/erlang-20.1.7-1.el7.centos.x86_64.rpm\n\n\nname: install zero dependency erlang for rabbit\n  yum:\n    name: https://github.com/rabbitmq/erlang-rpm/releases/download/v20.1.7/erlang-20.1.7-1.el7.centos.x86_64.rpm\n    state: present\n    update_cache: yes\n  when: erlang_version['stdout'] != '20.1.7'\n  tags:\n\nrabbitmq\n\n\n\nname: yum list version of installed rabbitmq-server\n  shell: yum list installed rabbitmq-server | grep rabbitmq-server | awk '{print $2}' | cut -d'-' -f1 warn=no\n  register: rabbit_version\n  tags:\n\nrabbitmq\n\n\n\nname: yum remove rabbitmq-server\n  yum: pkg=rabbitmq-server state=absent\n  when: rabbit_version['stdout'] != '3.7.0'  # Reverted from 3.7.2 due to problems\n  tags:\n\nrabbitmq\n\n\n\nBackup is at http://mirror.int.fanxchange.com/centos/rabbit/rabbitmq-server-3.7.2-1.el7.noarch.rpm\nTrying old https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.0/rabbitmq-server-3.7.0-1.el7.noarch.rpm\n\nname: install rabbitmq-server from rpm\n  yum:\n    name: https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.0/rabbitmq-server-3.7.0-1.el7.noarch.rpm\n    state: present\n    update_cache: yes\n  when: rabbit_version['stdout'] != '3.7.0'\n  tags:\nrabbitmq\n\n\n\nDir for socket descriptors override\n\nname: create /etc/systemd/system/rabbitmq-server.service.d/ dir\n  file: path=/etc/systemd/system/rabbitmq-server.service.d state=directory\n  tags:\nrabbitmq\n\n\n\nSetup socket descriptors, see rabbitmqctl status\n\n\nname: /etc/systemd/system/rabbitmq-server.service.d/override.conf\n  template: src=service.d.override.conf dest=/etc/systemd/system/rabbitmq-server.service.d/override.conf\n            owner=root group=root mode=644\n  notify: reload systemd\n  tags:\n\nrabbitmq\n\n\n\nname: /etc/rabbitmq/rabbitmq.config\n  copy: src=../files/etc/rabbitmq/rabbitmq.config dest=/etc/rabbitmq/rabbitmq.config mode=444 owner=root group=root\n  tags:\n\nrabbitmq\n\n\n\nUsing default config here for now\nDefault location of rabbitmq.config and rabbitmq-env.conf is /etc/rabbitmq/\nhttps://www.rabbitmq.com/configure.html#configuration-file\n- name: /etc/rabbitmq/rabbitmq-env.conf\ncopy: src=../files/etc/rabbitmq/rabbitmq-env.conf dest=/etc/rabbitmq/rabbitmq-env.conf mode=444 owner=root group=root\ntags:\n- rabbitmq\n\n\nname: rabbitmq-server service\n  systemd: name=rabbitmq-server state=restarted enabled=true daemon_reload=yes\n  tags:\n\nrabbitmq\n\n\n\nname: /etc/sudoers.d/queue-wheel\n  copy: src=../files/etc/sudoers.d/queue-wheel dest=/etc/sudoers.d/queue-wheel owner=root group=root mode=444\n  tags:\n\nrabbitmq\n\n\n\nname: enable rabbitmq management ui\n  rabbitmq_plugin: names=rabbitmq_management state=enabled\n  tags:\n\nrabbitmq\n\n\n\nname: /usr/local/bin/check_rabbitmq.py\n  copy: src=../files/usr/local/bin/check_rabbitmq.py dest=/usr/local/bin/check_rabbitmq.py mode=555 owner=root group=root\n  tags:\n\nrabbitmq\n```\n\n\n\nThe consumers and publishers both running on t2.large, rabbit is also running on a t2.large aws instance, no credit issues on both.\nThe code is exactly a copy of the async consumer example. The publisher code is working properly the problem is lockup on the ioloop. This is the basically the code for the breaking consumer - http://pika.readthedocs.io/en/0.10.0/examples/asynchronous_consumer_example.html\nMy messages can get quite large, not sure if that has anything to do with it. I have to play around with it more. I can reproduce the issue when I pump prod data through it, but haven't been able to isolate the exact problem in my dev environment.\nI haven't tried erlang 20.2 or newer and haven't tried erlang 19.3. I am using rabbits 0 dependency rabbit.\n. I just had the condition happen on erlang 20.1.7 and rabbit 3.7.0. I ran this without problems for weeks so not sure what it is. I'll run the consumer with trace on to see where it breaks. I'll close the issue for now.. My bad.. for some reason the provision ran and updated the rabbit version to 3.7.2 and erlang 20.1.7.1 and that's when it broke again.\nI'm running a trace now with the current config to see what happens. Will report back.. Everything goes idle. In my consumer function at the end of the callback I use procname to update the process name to say \"worker (waiting)\". I see that its waiting and not consuming any more messages and that makes sense with what I see in the rabbitmq admin. You can see what the strace info gives, not very useful at least to me.\nI'm running two workers using prod data with trace on so I can see everything being executed. Unfortunately with trace on (python -m trace --trace test.py) its running very slow and not being able to keep up with the publishers so messages are queueing up quickly.\nUpdate: Yes I will try to update to latest python2.\nUpdate: With 1 worker without trace the consumer can eat 100msgs/sec. With two workers with trace set, it's going at ~0msgs/sec. Might take some time to break these.\n  . So I finally got a traceback from one of the workers. But not sure if this is the actual problem since there was a traceback this time rather than the whole process hanging without any logging.\nhttps://gist.github.com/radzhome/b1fa5de488d7f4332a5ceece5a854402\nLooks like it dies on reconnect. But I never saw any such traceback before when starting the consumer via systemd script.\n  . Here is my consumer code based off that example:\n```\nclass AsyncConsumer(object):  # pragma: no cover\n    \"\"\"\n    Uses select connection instead of blocking to consume messages\n    quickly without needing to send ack/no ack signals.\n    Non-blocking connection allows for concurrency, fire and forget\n    \"\"\"\nRECONNECT_SLEEP_SECS = 4\n\n# TODO: add option to direct declare on init\ndef __init__(self, queue, routing_key, exchange, exchange_type,\n             message_callback, **kwargs):\n    self.config = kwargs.get('config', MNG.QUEUE_CONN_PARAMS)\n    self.connection = None\n    self.msg_callback = message_callback\n    self._host = self.config.get('host', '')\n    self._port = self.config.get('port', 0)\n    self._channel = None\n    self._closing = False  # Used to signal shutdown\n    self._consumer_tag = None\n    self._queue_name = queue\n    self._routing_key = routing_key\n    self._exchange = exchange\n    self._exchange_type = exchange_type\n\ndef connect(self):\n    \"\"\"\n    This method connects to RabbitMQ, returning the connection handle.\n    When the connection is established, the on_connection_open method\n    will be invoked by pika.\n\n    :rtype: pika.SelectConnection\n    \"\"\"\n    params = pika.ConnectionParameters(host=self._host, port=self._port, heartbeat_interval=0)\n\n    return pika.SelectConnection(params, self.on_connection_open, stop_ioloop_on_close=False)\n\ndef reconnect(self):\n    \"\"\"\n    Will be invoked by the IOLoop timer if the connection is\n    closed. See the on_connection_closed method.\n    \"\"\"\n    # This is the old connection IOLoop instance, stop its ioloop\n    self.connection.ioloop.stop()\n\n    if not self._closing:\n\n        # Create a new connection\n        self.connection = self.connect()\n\n        # There is now a new connection, needs a new ioloop to run\n        self.connection.ioloop.start()\n\ndef on_channel_open(self, channel):\n    \"\"\"\n    This method is invoked by pika when the channel has been opened.\n    The channel object is passed in so we can make use of it.\n\n    Since the channel is now open, we'll declare the exchange to use.\n\n    :param pika.channel.Channel channel: The channel object\n    \"\"\"\n    self._channel = channel\n    logging.debug(\"AsyncConsumer.on_channel_open Channel opened, adding channel close callback\")\n\n    self._channel.add_on_close_callback(self.on_channel_closed)\n\n    # self.setup_exchange()\n    self.start_consuming()\n\ndef on_connection_open(self, unused_connection):\n    logging.debug(\"AsyncConsumer.on_connection_open Adding connection close callback\")\n    self.connection.add_on_close_callback(self.on_connection_closed)\n    self.open_channel()\n\ndef on_connection_closed(self, connection, reply_code, reply_text):\n    \"\"\"\n    Added as callback 'add_on_close_callback' in  on_connection_open\n    Invoked when the connection to RabbitMQ is\n    closed either due to shutdown or unexpectedly.\n    If unexpected, we will reconnect to RabbitMQ if it disconnects.\n    :param pika.connection.Connection connection: The closed connection obj\n    :param int reply_code: The server provided reply_code if given\n    :param str reply_text: The server provided reply_text if given\n    \"\"\"\n    self._channel = None\n    if self._closing:\n        # Shutdown invoked\n        self.connection.ioloop.stop()\n    else:\n        # Connection closed unexpectedly\n        logging.warning(\"AsyncConsumer.on_connection_closed closed with reply code '{}', reply text '{}', \"\n                        \"re-opening connection\".format(reply_code, reply_text))\n        self.connection.add_timeout(self.RECONNECT_SLEEP_SECS, self.reconnect)\n\ndef on_channel_closed(self, channel, reply_code, reply_text):\n    \"\"\"\n    Added as callback 'add_on_close_callback' in on_channel_open\n    Invoked by pika when RabbitMQ unexpectedly closes the channel.\n    Channels are usually closed if you attempt to do something that\n    violates the protocol, such as re-declare an exchange or queue with\n    different parameters. In this case, we'll close the connection\n    to shutdown the object.\n\n    :param pika.channel.Channel: The closed channel\n    :param int reply_code: The numeric reason the channel was closed\n    :param str reply_text: The text reason the channel was closed\n    \"\"\"\n    logging.warning(\n        \"AsyncConsumer.on_channel_closed Channel {} was closed: ({}) {}\".format(channel, reply_code, reply_text))\n    self.connection.close()\n\ndef open_channel(self):\n    \"\"\"Open a new channel with RabbitMQ by issuing the Channel.Open RPC\n    command. When RabbitMQ responds that the channel is open, the\n    on_channel_open callback will be invoked by pika.\n    \"\"\"\n    logging.debug(\"AsyncConsumer.open_channel Creating a new channel\")\n    self.connection.channel(on_open_callback=self.on_channel_open)\n\ndef start_consuming(self):\n    \"\"\"\n    This method sets up the consumer by first calling\n    add_on_cancel_callback so that the object is notified if RabbitMQ\n    cancels the consumer. It then issues the Basic.Consume RPC command\n    which returns the consumer tag that is used to uniquely identify the\n    consumer with RabbitMQ. We keep the value to use it when we want to\n    cancel consuming. The on_message method is passed in as a callback pika\n    will invoke when a message is fully received.\n    \"\"\"\n    logging.debug(\"AsyncConsumer.start_consuming Adding consumer cancellation callback\")\n    self._channel.add_on_cancel_callback(self.on_consumer_cancelled)\n    self._channel.basic_qos(prefetch_count=1)\n    self._consumer_tag = self._channel.basic_consume(self.on_message, self._queue_name, no_ack=True)\n\ndef stop_consuming(self):\n    \"\"\"\n    Tell RabbitMQ that you would like to stop consuming by sending the\n    Basic.Cancel RPC command.\n    \"\"\"\n    if self._channel:\n        logging.debug('Sending a Basic.Cancel RPC command to RabbitMQ')\n        self._channel.basic_cancel(self.on_cancelok, self._consumer_tag)\n\ndef on_consumer_cancelled(self, method_frame):\n    \"\"\"\n    Invoked by pika when RabbitMQ sends a Basic.Cancel for a consumer\n    receiving messages.\n\n    :param pika.frame.Method method_frame: The Basic.Cancel frame\n\n    \"\"\"\n    logging.debug(\n        \"AsyncConsumer.on_consumer_cancelled Consumer was cancelled, shutting down: {}\".format(method_frame))\n    if self._channel:\n        self._channel.close()\n\ndef on_cancelok(self, unused_frame):\n    \"\"\"\n    This method is invoked by pika when RabbitMQ acknowledges the\n    cancellation of a consumer. At this point we will close the channel.\n    This will invoke the on_channel_closed method once the channel has been\n    closed, which will in-turn close the connection.\n\n    :param pika.frame.Method unused_frame: The Basic.CancelOk frame\n    \"\"\"\n    logging.debug(\n        \"AsyncConsumer.on_cancelok RabbitMQ acknowledged the cancellation of the consumer, closing the channel\")\n    self._channel.close()\n\ndef on_message(self, unused_channel, basic_deliver, properties, body):\n    \"\"\"\n    Invoked by pika when a message is delivered from RabbitMQ. The\n    channel is passed for your convenience. The basic_deliver object that\n    is passed in carries the exchange, routing key, delivery tag and\n    a redelivered flag for the message. The properties passed in is an\n    instance of BasicProperties with the message properties and the body\n    is the message that was sent.\n\n    :param pika.channel.Channel unused_channel: The channel object\n    :param pika.Spec.Basic.Deliver: basic_deliver method\n    :param pika.Spec.BasicProperties: properties\n    :param str|unicode body: The message body\n    \"\"\"\n    # logging.debug(\"AsyncConsumer.on_message Received message # {} from {}\"\n    #               \"\".format(basic_deliver.delivery_tag, properties.app_id))\n    self.msg_callback(body)\n\ndef run(self):\n    \"\"\"\n    Run the consumer loop by connecting to RabbitMQ and then\n    starting the IOLoop to block and allow the SelectConnection to operate.\n\n    \"\"\"\n    self.connection = self.connect()\n    self.connection.ioloop.start()\n\ndef shutdown(self, signum, stack):\n    \"\"\"\n    Stops all the things\n    \"\"\"\n    logging.info('AsyncConsumer.shutdown Stopping')\n    self._closing = True\n    self.stop_consuming()\n    self.connection.ioloop.start()\n    logging.info('AsyncConsumer.shutdown Stopped')\n\n```\nand its started  this way:\n```\nif name == 'main':\n    logging.info(\"split_files.py (initializing)\")\n    procname.setprocname(\"split_files.py (initializing)\")\n    declare_all_queues()\n    WORKER = queue.AsyncConsumer(DEQUEUE_NAME, DEQUEUE_ROUTING_KEY,\n                                 DEQUEUE_EXCHANGE, DMN.DIRECT_EXCHANGE,\n                                 process_split_files)\n# setup signal handling\nsignal.signal(signal.SIGTERM, WORKER.shutdown)\nsignal.signal(signal.SIGQUIT, WORKER.shutdown)\nsignal.signal(signal.SIGINT, WORKER.shutdown)\n\ntry:\n    WORKER.run()\nexcept (IOError, KeyboardInterrupt):\n    # This is the exception thrown on shutdown, it's all good\n    pass\n\n```. Tried restarting the rabbit sever while running rabbit 3.7.2 and erlang 20.1.7.1 which seems to be the latest 0 dependency rabbitmq erlang release. All consumers died again, so reverted back again to old versions rabbit 3.7.0 on Erlang 20.1.7. . Things have been very stable on rabbitmq 3.7.0 on Erlang 20.1.7. I updated my config to new format as well not that really makes a difference. I did however go back to default max_frame and heartbeat settings.\n```\n======================================\nRabbbitMQ conf - sysctl format\n======================================\nRelated doc guide: http://rabbitmq.com/configure.html. See\nhttp://rabbitmq.com/documentation.html for documentation ToC.\nSample config: https://raw.githubusercontent.com/rabbitmq/rabbitmq-server/v3.7.x/docs/rabbitmq.conf.example\n\nLogging: https://www.rabbitmq.com/logging.html\nrabbitmqcntl: https://www.rabbitmq.com/rabbitmqctl.8.html\nAdvanced config: https://github.com/rabbitmq/rabbitmq-server/blob/master/docs/advanced.config.example\nNetworking\n====================\n\nRelated doc guide: http://rabbitmq.com/networking.html.\n\nBy default, RabbitMQ will listen on all interfaces, using\nthe standard (reserved) AMQP 0-9-1 and 1.0 port.\n\nlisteners.tcp.default = 5672  # Default port\nThe default \"guest\" user is only permitted to access the server\nvia a loopback interface (e.g. localhost).\n{loopback_users, [<<\"guest\">>]},\n\nloopback_users = none  # Remove guest as loopback only user, without it guest auth won't work unless localhost\nMemory-based Flow Control threshold.\n\nvm_memory_high_watermark.relative = 0.65  # % of total mem usage\nOne of 'basic', 'detailed' or 'none'. See\nhttp://rabbitmq.com/management.html#fine-stats for more details.\nShow ack, incoming, get rates in admin\n\nmanagement.rates_mode = none\nStat collection primarily for admin\n\ncollect_statistics = none\nLog level for file logging\n\nlog.connection.level = error\nlog.channel.level = error\nSet the default AMQP 0-9-1 heartbeat interval (in seconds). Default 600.\nRelated doc guides:\n\n* http://rabbitmq.com/heartbeats.html\n* http://rabbitmq.com/networking.html\n\nheartbeat = 0\nSet the max permissible size of an AMQP frame (in bytes). Default 131072.\n\nframe_max = 0\nSet the max frame size the server will accept before connection\ntuning occurs\n\ninitial_frame_max = 4096\n```\nI also updated to python 2.7.14 but still haven't ran the latest version of rabbit against it.. 1) I was using max_frame = 0 and heartbeat = 0\n2) I used 20.1.7.1 as well as 20.1.7. I reverted back to 20.1.7 and rabbit 3.7.0. Not sure if I tried rabbit 3.7.0 on erlang 20.1.7.1.\n3) No with rabbit 3.7.2 I only used my classic settings which I posted:\n```\n%% -- mode: erlang --\n%% ----------------------------------------------------------------------------\n%% RabbitMQ  Configuration File. /etc/rabbitmq/\n%% Source https://github.com/rabbitmq/rabbitmq-server/blob/master/docs/rabbitmq.config.example\n%% See https://www.rabbitmq.com/configure.html#configuration-file for details.\n%% ----------------------------------------------------------------------------\n[\n  {\n    rabbit, [\n      {loopback_users, []},\n      {log_levels, [{connection, error}, {channel, error}]},\n      {disk_free_limit, {mem_relative, 0.1}},\n      {tcp_listeners, [5672]},\n      {heartbeat, 0},\n      {frame_max, 0},\n      {vm_memory_high_watermark, 0.6},\n      {rates_mode, none},\n      {collect_statistics, none}\n    ]\n  }\n].\n```\nOn 3.7.0 consumer didn't die but I had rabbit crashing a few times a day so I went to the new settings above with a higher watermark as well of 0.65. Sorry to confuse, rabbit crashing from time to time was a distinct issue. Not related to the consumer breaking.. Haven't had a problem after upgrading to latest rabbit 3.7.3 on Erlang 20.2.3. Working well with same config on both python 3.6.4 and 2.7.14 .\nSo may have had something to do with that particular release only.. That's no good... thought it was thread-safe now. So basically have to open a new connection in each process in the pool which is created and destroyed using multiprocessing hmm. I created the connection within the new process but same issue\n```\nraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(args, *kwds))\n  File \"/pipeline/python/daemons/split_files.py\", line 161, in process_local_file\n  File \"/usr/local/lib/python3.6/csv.py\", line 112, in next\n    row = next(self.reader)\n  File \"/usr/local/virtualenvs/pipeline_py3/src/fanx-service-clients/serviceclients/queue/rabbit.py\", line 502, in shutdown\n    self.stop_consuming()\n  File \"/usr/local/virtualenvs/pipeline_py3/src/fanx-service-clients/serviceclients/queue/rabbit.py\", line 439, in stop_consuming\n    self._channel.basic_cancel(self.on_cancelok, self._consumer_tag)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/channel.py\", line 267, in basic_cancel\n    nowait is False else [])\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/channel.py\", line 1316, in _rpc\n    self._send_method(method)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/channel.py\", line 1328, in _send_method\n    self.connection._send_method(self.channel_number, method, content)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/connection.py\", line 2200, in _send_method\n    self._send_frame(frame.Method(channel_number, method))\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/connection.py\", line 2184, in _send_frame\n    self._flush_outbound()\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/base_connection.py\", line 315, in _flush_outbound\n    self._manage_event_state()\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/base_connection.py\", line 511, in _manage_event_state\n    self.event_state)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/select_connection.py\", line 194, in update_handler\n    self._poller.update_handler(fileno, events)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/select_connection.py\", line 401, in update_handler\n    events_to_set=events_set)\n  File \"/usr/local/virtualenvs/pipeline_py3/lib/python3.6/site-packages/pika/adapters/select_connection.py\", line 972, in _modify_fd_events\n    self._poll.modify(fileno, events)\nFileNotFoundError: [Errno 2] No such file or directory\n\"\"\"\nThe above exception was the direct cause of the following exception:\n```. ",
    "brnrc": "@gmr thanks for your response.\nWhat makes me think that it could be related to the AMQP protocol is the fact that all HTTP requests work flawlessly.\nI'll try to check what's happening on the rabbit end.\n. Passing the proxy explicitly to the python application works, but with the transparent proxy it doesn't.\nChecking with wireshark I could see that pika is using AMQP over HTTP (although it could be something from my side that is doing it)... I'll try to isolate the environment from any proxy to check what does pika actually does.\n. ",
    "jan1206": "Looks like this was an issue with the code that tried to determine the cause of a connection failure and subsequently wouldn't propagate the reasons to the caller.\nThis has been fixed for me on the master branch: https://github.com/pika/pika/commit/43936088e1183ec5902f2ba2992c902e87f5460b\n. ",
    "everilae": "Had pyev tests pass on 3.4. Interesting that they should fail on similar setup elsewhere.\n. I'm not terribly proud of the \"quick and dirty\" str(...).encode() mantras in tests just for byte  stringing the results in PY3. Should've wrapped in a function. TODO? :)\n. Missing Travis yml stuff for 2.6? :)\n. The problems boil down to the good old strings vs. bytes issues. As was aptly put in the original pull request discussion, the PY3 port is very brute force and it does not in any way help.\nSo the issue is: when you publish strings in PY3, you get bytes out on the consumer side. When you declare queues with routing_keys, the routing_keys get converted to bytes. The tests were for example timing out because of this.\nThis is rather inconvenient, but we've learned to live with it for the time being. It's that or no pika at all for us, since we're committed to using PY3.\nI'll create a patch on top of your work.\n. Nah, it's not broken. It is just that \"\\\"string\\\"\" literal in python2 is actually bytes, when in python3 it is text/string/what have you. So for the time being the way to canonicalise tests is to use b\"\"literals. The true fix would be to canonicalise those bytes and strings in the library so that if you pass text/strings in python3, it does-the-right-thing.\nKeep in mind that if you use pika with python2 only as a User you will not see the difference that your tests brought up. Everything just works, because everything has and will be byte \"strings\". It is with python3 that you do have to actually know your data type (which is a good thing, in the long run).\n. I think @ztane has been doing some thinking on the whole bytes/string canonicalisation thingie, but hasn't yet been able to get to it.\nAnd yeah, no re-encoding bytes, if given. It's up to the User to know what they're up to at that point, because the library cannot (is this latin-1? utf-8? wtf-...?).\n. The consumer side receiving issue is a bit trickier though, if you want to be able to send and receive text in python3. As it is now, pika does encoding when sending, but no decoding when consuming. We've sort of \"worked around\" this by accepting that we must talk to pika in bytes in python3 too. Still better than not having amqp at all :).\n. I reckon that this issue should be closed. Author seems to have resolved the issue with latest version, or at least there's no traceback in the logs after upgrade.\n. @vitaly-krugl: added short-strings-as-text-in-python3, python2-does-what-it-did-before. How does it look? Moved some data handling from codegen.py/spec.py to pika/data.py. Seemed logical.\n. Will have a go at the exception str handling + data table keys decoding with short string decoder -> text keys (this PR is getting a bit out of hand...)\n. Rewriting pika/data.py at datafixes, but pushed that stuff to a separate branch in order to avoid bloating this PR even more. Can push here too, if you accept the ideas in it.\n. History rewritten, lots of painful mistakes made. Apparently should not have merged before rebase etc...\n. Followed basic squashing/rebasing instructions, except I had merged latest changes from master before that. Turned out to be a disaster, so had to undo that merge. From there on it turned out ok. I guess it's ok to do this kind of history rewriting since nobody else has based any work on top of this (I hope).\nHad to rebase own work though...\n. Well, it is a bit hard to produce since pika as it is now doesn't actually support sending floats... but if pika is supposed to be \"an AMQP 0-9-1 client library for Python\", it isn't that far fetched to imagine some other client publishing data with floats that pika then would consume and cast to integer. Just take a gander at the linked piece of code:\npython\n    value = long(struct.unpack_from('>f', encoded, offset)[0])\nIt does the right-thing-tm up until the long() wrapping.\n. Both https://www.rabbitmq.com/amqp-0-9-1-errata.html#section_3 and\n4.2.1 Formal Protocol Grammar\n...\nfield-value = 't' boolean\n              ...\n              / 'f' float\n              / 'd' double\n              ...\nfloat = 4*OCTET ; IEEE-754\ndouble  = 8*OCTET  ; rfc1832 XDR double\ndefine 'f' and 'd' field values.\nPika also tries to support these field values, but casts them to integers. Simple as that.\n. https://github.com/pika/pika/blob/master/pika/data.py#L238 and https://github.com/pika/pika/blob/master/pika/data.py#L243, still long() casts.\n. Aye, been waiting for news on that. gmr/pamqp seemed nice, with a few warts of its own ;)\n. For a moment thought that long long integers were mixed as well, but read RabbitMQ errata again and saw that there's a conflict between AMQP0-9-1 and RabbitMQ there.\n. Actually, the encoding / decoding of long long integers is inconsistent:\n- encoding is done according to RabbitMQ\n- decoding is done according to AMQP 0-9-1\n. The byte thing:\npython\n    >>> pika.data.decode_value(struct.pack('>cb', b'b', -128), 0)\n    (128, 2)\n    >>> pika.data.decode_value(struct.pack('>cb', b'b', -1), 0)\n    (255, 2)\n'b' is \"signed byte\" for RabbitMQ, AMQP 0-9-1 and Python struct format string. Correct results would be (-128, 2) and (-1, 2) respectively. Again, this is a bit hard to test, since pika doesn't seem to publish bytes, but tries to support decoding them.\nThus the short-short-integer unsigned vs. signed is an issue whether or not we speak to RabbitMQ, if the errata is to be trusted. The long-long-integer is another can of worms.\n. @beruic could you check what version of pika you are using, or describe how you installed it.\nOne way to check would be\npython\n    >>> import pika\n    >>> help(pika)\nand look for VERSION. I second @vitaly-krugl that prolly you have an old version of pika which lacks PY3 support.\nYou could also verify against the latest from master using a virtualenv:\nbash\n% virtualenv -p python3 venv\n% source venv/bin/activate\n% pip install git+https://github.com/pika/pika.git#egg=pika\nand then run your tests.\n. Success! Aka had it again. And here's how:\nbash\n% i=0; while [ $? -eq 0 ]; \\\ndo \\\necho $(( ++i )); \\\nnosetests -c nose.cfg --tests=tests/acceptance/blocking_adapter_test.py:TestBasicCancelWithNonAckableConsumer; \\\ndone\n...and the result:\n```\n806\nnose.config: INFO: Ignoring files matching ['^\\.', '^_', '^setup\\.py$']\nBlockingChannel user cancels non-ackable consumer via basic_cancel ... FAIL\n======================================================================\nFAIL: BlockingChannel user cancels non-ackable consumer via basic_cancel\n\nTraceback (most recent call last):\n  File \"/home/saarni/Work/liilak/pika/tests/acceptance/blocking_adapter_test.py\", line 1489, in test\n    self.assertEqual(len(messages), 2)\nAssertionError: 0 != 2\n-------------------- >> begin captured logging << --------------------\npika.adapters.select_connection: DEBUG: Using EPollPoller\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.adapters.base_connection: INFO: Connecting to 127.0.0.1:5672\npika.callback: DEBUG: Processing 0:Connection.Start\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.Start\"\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Processing 0:Connection.Tune\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.Tune\"\npika.connection: DEBUG: Creating a HeartbeatChecker: 580\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Processing 0:Connection.OpenOk\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.OpenOk\"\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Processing 0:_on_connection_open\npika.callback: DEBUG: Calling > for \"0:_on_connection_open\"\npika.connection: DEBUG: Creating channel 1\npika.callback: DEBUG: Added: {'callback': >, 'only': , 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.channel: DEBUG: Adding in on_synchronous_complete callback\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.channel: DEBUG: Adding passed in callback\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.adapters.blocking_connection: INFO: Created channel=1\npika.callback: DEBUG: Processing 1:Channel.OpenOk\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"1:Channel.OpenOk\"\npika.channel: DEBUG: 0 blocked frames\npika.callback: DEBUG: Calling > for \"1:Channel.OpenOk\"\npika.channel: DEBUG: Adding in on_synchronous_complete callback\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'queue': 'TestBasicCancelWithNonAckableConsumer_qe0f350341b1311e5897db4b676cff4c7'}, 'calls': 1}\npika.channel: DEBUG: Adding passed in callback\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'queue': 'TestBasicCancelWithNonAckableConsumer_qe0f350341b1311e5897db4b676cff4c7'}, 'calls': 1}\npika.callback: DEBUG: Processing 1:Queue.DeclareOk\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Comparing {'queue': 'TestBasicCancelWithNonAckableConsumer_qe0f350341b1311e5897db4b676cff4c7'} to {'queue': 'TestBasicCancelWithNonAckableConsumer_qe0f350341b1311e5897db4b676cff4c7'}\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'queue': 'TestBasicCancelWithNonAckableConsumer_qe0f350341b1311e5897db4b676cff4c7'}, 'calls': 0}\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Comparing {'queue': 'TestBasicCancelWithNonAckableConsumer_qe0f350341b1311e5897db4b676cff4c7'} to {'queue': 'TestBasicCancelWithNonAckableConsumer_qe0f350341b1311e5897db4b676cff4c7'}\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'queue': 'TestBasicCancelWithNonAckableConsumer_qe0f350341b1311e5897db4b676cff4c7'}, 'calls': 0}\npika.callback: DEBUG: Calling > for \"1:Queue.DeclareOk\"\npika.channel: DEBUG: 0 blocked frames\npika.callback: DEBUG: Calling > for \"1:Queue.DeclareOk\"\npika.adapters.select_connection: DEBUG: Using EPollPoller\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.adapters.base_connection: INFO: Connecting to 127.0.0.1:5672\npika.callback: DEBUG: Processing 0:Connection.Start\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.Start\"\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Processing 0:Connection.Tune\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.Tune\"\npika.connection: DEBUG: Creating a HeartbeatChecker: 580\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Processing 0:Connection.OpenOk\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"0:Connection.OpenOk\"\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Processing 0:_on_connection_open\npika.callback: DEBUG: Calling > for \"0:_on_connection_open\"\npika.connection: DEBUG: Creating channel 1\npika.callback: DEBUG: Added: {'callback': >, 'only': , 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.channel: DEBUG: Adding in on_synchronous_complete callback\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.channel: DEBUG: Adding passed in callback\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.adapters.blocking_connection: INFO: Created channel=1\npika.callback: DEBUG: Processing 1:Channel.OpenOk\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Calling > for \"1:Channel.OpenOk\"\npika.channel: DEBUG: 0 blocked frames\npika.callback: DEBUG: Calling > for \"1:Channel.OpenOk\"\npika.channel: DEBUG: Adding in on_synchronous_complete callback\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}, 'calls': 1}\npika.channel: DEBUG: Adding passed in callback\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}, 'calls': 1}\npika.callback: DEBUG: Processing 1:Basic.ConsumeOk\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Comparing {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'} to {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}\npika.callback: DEBUG: Removing callback #1: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}, 'calls': 0}\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Comparing {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'} to {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}\npika.callback: DEBUG: Removing callback #1: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}, 'calls': 0}\npika.callback: DEBUG: Calling > for \"1:Basic.ConsumeOk\"\npika.callback: DEBUG: Calling > for \"1:Basic.ConsumeOk\"\npika.channel: DEBUG: 0 blocked frames\npika.callback: DEBUG: Calling > for \"1:Basic.ConsumeOk\"\npika.channel: DEBUG: Discarding frame \"])>\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.channel: DEBUG: Adding in on_synchronous_complete callback\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}, 'calls': 1}\npika.channel: DEBUG: Adding passed in callback\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}, 'calls': 1}\npika.callback: DEBUG: Processing 1:Basic.CancelOk\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Comparing {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'} to {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}, 'calls': 0}\npika.callback: DEBUG: Processing use of oneshot callback\npika.callback: DEBUG: 0 registered uses left\npika.callback: DEBUG: Comparing {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'} to {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}\npika.callback: DEBUG: Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.46337800851a41d8bf659e246fc34723'}, 'calls': 0}\npika.callback: DEBUG: Calling > for \"1:Basic.CancelOk\"\npika.callback: DEBUG: Calling > for \"1:Basic.CancelOk\"\npika.channel: DEBUG: 0 blocked frames\npika.callback: DEBUG: Calling > for \"1:Basic.CancelOk\"\n--------------------- >> end captured logging << ---------------------\n\nRan 1 test in 0.029s\nFAILED (failures=1)\n```\n. A bit faster way to run multiple iterations of tests:\nbash\n% for test in `grep 'class Test' tests/acceptance/blocking_adapter_test.py | sed -r 's|class ([^(]+).*|\\1|'`; do \\\nnosetests --processes 4 -c nose.cfg --tests=\"$(yes tests/acceptance/blocking_adapter_test.py:$test, | head -n 800)\" || break; \\\ndone\n. I'm thinking this might be a race in the test itself:\n``` python\n        # Publish two messages to the queue by way of default exchange\n        ch.publish(exchange='', routing_key=q_name, body=body1)\n        ch.publish(exchange='', routing_key=q_name, body=body2)\n    # Create a non-ackable consumer\n    consumer_tag = ch.basic_consume(lambda *x: None, q_name, no_ack=True,\n                                    exclusive=False, arguments=None)\n\n    # Cancel the consumer\n    messages = ch.basic_cancel(consumer_tag)\n\n    # Both messages should have been on their way when we cancelled\n    self.assertEqual(len(messages), 2)\n\n```\nTo me it seems that there's a race between the basic_consume finishing and consuming 1 or 2 of the published messages before basic_cancel kicks in, but I'm not familiar enough with the system to be sure.\n. And it is exactly that. Adding a trace to Channel._on_deliver and running a few thousand iterations of the test yields:\n```\n % nosetests -q -c nose.cfg --tests=\"$(yes tests/acceptance/blocking_adapter_test.py:TestBasicCancelWithNonAckableConsumer, | head -n 1000)\"\n\n/home/saarni/Work/liilak/pika/pika/channel.py(962)_on_deliver()\n-> if self.is_open and consumer_tag not in self._consumers_with_noack:\n(Pdb) list\n957\n958             \"\"\"\n959             consumer_tag = method_frame.method.consumer_tag\n960             if consumer_tag in self._cancelled:\n961                 from nose.tools import set_trace; set_trace()\n962  ->             if self.is_open and consumer_tag not in self._consumers_with_noack:\n963                     self.basic_reject(method_frame.method.delivery_tag)\n964                 return\n965             if consumer_tag not in self._consumers:\n966                 return self._add_pending_msg(consumer_tag, method_frame,\n967                                              header_frame, body)\n(Pdb) method_frame\n\"])>\n(Pdb) method_frame.method\n\n(Pdb) consumer_tag\n'ctag1.5d3e3d9738ee4ef796462da8b799d274'\n(Pdb) \n```\n\nIn this light and from reading the doc string of Channel.basic_cancel I think this test is quite broken, because\n\nThe client may receive an arbitrary number of messages in between sending the cancel method and receiving the cancel-ok reply.\n. Ran some 255000 iterations, no issues. Closing.\n. Have not made a pull req as of yet, but would https://github.com/everilae/pika/blob/datafixes/pika/data.py#L124 address the issue? The whole branch would require some major eyeballing, since it rewrites a lot of data handling, but is based on the current master of pika/pika pretty much.\n\nThe implementation does not allow explicitly choosing the type of integer representation in a data table, but it's hard with current impl. too.\n. @lukebakken About \"It appears that the data is assumed to be UTF-8\", that's how the AMQP 0-9-1 spec defines short-strings in 4.2.5.3  Strings.. @lukebakken About the commit, if I remember correctly the reasoning was that even though long-strings may hold any data (4.2.5.3 Strings), it would have been surprising in Python 3 that sending str results in bytes at the receiving end, because if sending any byte data were allowed, there could be no automatic decoding. Of course it could have been left to the users to both encode and decode when using long strings (and in hindsight that might have been a good idea), but since it is a \"string\", mapping it to bytes in Python 3 could have been awkward.\nIn Python 2 the situation was different since a str was just bytes to begin with, even though actually decode_value() implicitly decodes from UTF-8 in both Py 2 & 3 and before that commit was made. That automatic decoding also pretty much disallows random bytes as long-strings in Py 3. It would also blow up in Py 2, but we didn't touch it as it was already there.. True, though actually the method in itself is a bit pointless, since as_bytes does what it does and as such the whole _p3_as_bytes -> as_bytes. I'll replace.\n. Here there is no visible asymmetry: method is a protocol class instance and never available for users (as much as anything can be made unavailable in python). This test asserts that the implementation detail of method containing attributes with certain values holds.\nI confess not being well-versed in AMQP either, but: Pika did unicode to utf-8 bytes conversions in pika/spec.py before we started with the py3 changes. This is why as_bytes does make sense here.\nPlease have a look at how utils/codegen.py generates \"string handling\" for protocol methods. It was and still is unicode aware.\n. From AMQP 0-9-1:\n4.2.5.3 Strings\nAMQP strings are variable length and represented by an integer length followed by zero or more octets of data. AMQP defines two native string types:\n- Short strings, stored as an 8-bit unsigned integer length followed by zero or more octets of data. Short strings can carry up to 255 octets of UTF-8 data, but may not contain binary zero octets.\n- Long strings, stored as a 32-bit unsigned integer length followed by zero or more octets of data. Long strings can contain any data.\n. Yeah, forgot those callbacks receiving method. That's a bit evil, I agree.\nStrings are certainly not \"UTF-8\" in python3. They are unicode codepoints. The internal encoding varies on string to string basis and cannot be relied upon / used in any meaningful manner.\n\"Strings\" in python2 are byte data with unknown encoding. Usually it's ASCII.\nThere is no \"utf-8 string\" in either python 2 or 3.\n. That should be possible.\n. From what I've been able to learn up till now, I think it is and as such your statement should be correct. There is one interesting thing though: pika seems to be able to pass properties in the message header for the body, such as \"content-type\". Now if one were to choose a standard encoding for unicode and send this information as the content-type property...\n. True, though would it be possible to have it in a double role of sorts, something like\n- bytes -> do what it does at the moment\n- something with application provided properties -> do what it does at the moment\n- text (unicode codepoints) given, no application provided properties -> encode and provide proper content-type allowing magic decoding at consumer end\nJust a thought, might as well keep it as is, or even require that body must be \"opaque binary payload\" (as the AMQP spec calls it).\n. Naturally all such ideas should be backwards compatible, which is easy, since no python3 support and text vs. binary data issues have existed (sort of), since in python2 people usually pass around mystery binary blobs. In such environment py2 would continue to handle short strings and long strings as binary data and py3 would handle text to data conversions.\n. ",
    "analytik": "Hi! Thanks to all of you for working on this.\n@ztane - in blocking_connection.py@554, if isinstance(body, unicode): also needs to be replaced with if isinstance(body, unicode_type):, plus import added. I'm just starting experimenting with RabbitMQ, Python 3.4 and pika, so I don't have any large project to test this on, but after this one fix, it works correctly as far as I can see.\n. I'm using the latest ztane/pika@flatpatch with Python 3.4.1 on Windows, and I was wondering if this was related.\nI have a simple REST application that saves some data into DB and then pushes it to RabbitMQ. But TornadoConnection sends only the first messages to Rabbit, and then just queues the rest in Connection.outbound_buffer ~~and never sends it, even if I queue hundreds of messages. (Unless I put breakpoints somewhere in _flush_outbound,~~ [[see my following comment]] then and only then I get something like \nWARNING:pika.connection:Pika: Write buffer exceeded warning threshold at 205 bytes and an estimated 24 frames behind\nand then it's sent out. However, I suspect that is a Tornado problem, as it's not officially supported on Windows.\n. OK, So I've tested this on Windows with TornadoConnection in\nPython 2 + master\nPython 2 + flatpatch\nPython 3 + flatpatch\nIt behaves the same, except it does get sent eventually on all 3 combinations (I falsely reported it doesn't get sent). The first message gets sent immediately, the next ones are sent all at once after about 30 seconds.\nThe only difference is that Python 3 + flatpatch reports Write buffer exceeded warning treshold at xx bytes... warnings. So I assume it's just a limitation of how pika+tornado behaves on Windows.\nSample code here. Most of it is copied from pika documentation. (Please forgive me the Borg pattern.)\n. @wjps - thanks! On Python 3.4.1, Windows, with flatpatch+write-starvation-fixes, both Tornado and Select adapters work as expected. No write buffer warnings anymore. :)\n. Got fixed in the meantime by 8654fbcb4672ef327a9689e58f724d80f3e2943e\n. Thanks to everyone involved! This is wonderful.\n. ",
    "paulinus": "As i understand it (i'm new to this),  server sends its preferences on with the Tune message, pika's Connection class \"combine's\" it with its own params, and then these are sent back to the server when sending the TuneOK message.\nI've tested that this works on my setup using rabbitMQ, and it is in accordance to rabbit's doc. But i could be missing something.\n. Thanks for all the work! Using pika a lot here, and it is working great.\n. You are right. The problem is solved in current master. Thanks @vitaly-krugl !\n. ",
    "sulsj": "Hi Will\nThanks for the comment. I'm running with git:master. I've tried to catch any exception following your advice and found the below. Any help will be appreciated.\nBest,\nSeung\n\n2015-05-07 13:16:37,861 | tfmq-worker | on_request | CRITICAL : Something wrong in on_request(): Socket connection lost\nTraceback (most recent call last):\n  File \"/.../tfmq-worker\", line 557, in \nFile \"/.../tfmq-worker\", line 547, in main\n    ch.basic_qos(prefetch_count=1) \n  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 990, in start_consuming\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 242, in process_data_events\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/blocking_connection.py\", line 358, in _handle_read\n  File \"build/bdist.linux-x86_64/egg/pika/adapters/base_connection.py\", line 356, in _handle_read\nAttributeError: 'NoneType' object has no attribute 'recv'\n. Thanks for the comment, @vitaly-krugl @wjps \nI'll try to make a test code to reproduce it.\nYes, I'm running it against master.\nThere are multiple threads but no connection is shared. There is a single client which disperse tasks to many workers via a pika connection. Each pair of client and worker has two threads for heartbeating and housekeeping and both threads have their own pika connections.\nIf I have 10,000 tasks to process and two workers are created, everything goes fine until around 1500~2000 tasks are completed. After that I lost the connection like the traceback. I might need to set the \"heartbeat_interval\" or \"socket_timeout\" in my URLParam.\nI'm testing more now. I'll keep you updated. \nThanks, \nSeung\n. Once a blocking connection is opened, what could be the best way to prevent it from closing. I've found I lost the socket connection when a long running worker finally completes and tries to send the result back to the client using the connection from which it received its task.\nFor your example above, what if \nprint >> sys.stderr, \".\",\ntakes an hour to complete? \nI've tried to do \nparams = pika.ConnectionParameters(credentials=creds, host=RMQ_HOST, virtual_host=RMQ_VHOST, heartbeat_interval=10)\nbut still got the lost connection.\nThanks, \nSeung\n. @vitaly-krugl Thank you for your help and info. \nBest, \nSeung\n. @vitaly-krugl Nop. Please close this. Thanks a lot again.\n. ",
    "mffrench": "Hello guys,\nplease tell me if there is any other blocking points for that pull request...\nCheers,\nMathilde\n. About nuking properties from pika : I reproduced behaviour of the Java RabbitMQ lib. \nAs a former MoM OPS I think the product connecting to RabbitMQ is not the libraries (Java, Python or whatever) but the application calling this lib. Some time you can have a lot of connections from many different apps into the same MoM and knowing the apps connecting on it is much more relevant than knowing the lib used by the app... So I think the java lib on this point has the good behaviour by allowing client properties redefinition.\nIf you want to keep Pika reference on the client properties I personally would prefer to add a new client property like library=Pika 0.9.1.\nAbout the contructor : do you prefer to add client_props setter/getter instead of passing through the ConnectionParameters constructor ? Or do you have other solution in mind ?\nCheers\n. Dear Pika project collaborators,\nAs there may be here a disagreement around RabbitMQ client properties usage I would like to know the final status of this pull req (rejected or accepted) ? If rejected I will have to maintain a specific fork to fit my needs. \nBest regards,\nMathilde\n. Dear Pika contributors,\ncurrently waiting the status of this request. Meanwhile I pushed my own distribs adding this functionality to Pika 0.9.14 : \nfor master : https://github.com/echinopsii/net.echinopsii.3rdparty.pika/tree/tune_client_props\nfor last release on python 2 (0.9.14) : https://github.com/echinopsii/net.echinopsii.3rdparty.pika/tree/tune_client_props_0.9.14\npip install epika-python2\nfor last release on python 3 (0.9.14) : \nhttps://github.com/echinopsii/net.echinopsii.3rdparty.pika/tree/tune_client_props_p3_0.9.14\npip3 install epika-python3 \nLet me know when this feature will be merged or if you plan to develop something similar, I will then stop maintaining these branches.\nBest regards,\nMathilde\n. Dear Pika contributors,\nsince pika 0.9.14 we do not have any clear answer regarding this pull req. I'm looking to your last version and asking my self if I will report my changes in a separate packaging like I did with pika 0.9.14 ( https://github.com/echinopsii/net.echinopsii.3rdparty.pika/tree/tune_client_props).\nSo could you please tell us if you plan to merge this pull req and if yes when ?\nThank you\n. Don't have time to fulfill your complains now. If you're interested by this feature I'm sure you'll find the time to complete this PR the way you think it should be. \nCheers\n(FYI : next time I'm working on this subject - if this PR has not been merged - I will just report this PR on the last pika tag to update our current pika fork.)\n. Don't worry I was not upset.  You're right to push your comments for code quality/stability but I really don't have time to make changes now - even to look forward what should be done. \nCurrently we're using echinopsii's pika branch which is working well for our current needs and I even don't know when I will upgrade it... \nSo if you(*)'re interested on this functionality, feel free to complete this PR following the recommendations you made.\nCheers,\n(*) 'you' means you (@vitaly-krugi) or anybody else interested on this functionality and wanting to go ahead on this topic...\n. Hello @markunsworth,\ninteresting solution but what if you have several connections from same client (which could happen as pika connections are not really thread safe) ?\nCurrently still waiting this pull request or some other solutions : \nhttps://github.com/pika/pika/pull/571\nCheers,\nMathilde\n. ",
    "barryib": "+1 for this pull request.\nIn our case, we wanted to use client properties for debug and operational purposes. In fact, our rabbitmq clusters are behind load balancers, so the rabbitmq admin console shows the load balancer's ip address for connections, instead of the original client ip. To solve that problem, we wanted to add the client ip address or hostname within the client properties. \nFor the product field in client properties, I think that it should refer to the application name instead of the used library. An additional filed can be added for library.\n. ",
    "tonyennis": "What's the best way to provide the info? Dump it into this page?\n. I am incorrect. It is indeed the channel that's the issue.  Thanks for the help.  If it turns out to be more subtle, I'll open another issue and include full information.\n. ",
    "andreas-kopecky": "Hello,\nEven tho this is already merged i wanted to post some feedback here too: \nI am well aware that master is unstable - i use it still because in 0.9.14 the maximum publish rate seems to be about 300 messages/s due to some issues that have been fixed in master.\nNow with this current master there is at least a few issues that come to mind:\n- how does blocking connection work now? i get from the pull request description, that one is supposed to use \"read_events\" now since there is no start_consuming or the likes anymore... is that the only canoncial way now?\n- how does one declare queues? or to be more precise: how does on REFERENCE declared queues since \"declare_queue\" no longer returns a queue object that allows to access the automatically generated name - it returns None to be exact.\n. @vitaly-krugl Thanks a lot for the quick reply. I have personally not contributed to this project so far but if i can be of any assistance please say so.\n. @vitaly-krugl \nHi - i was away over the weekend too. I am not sure if i should provide tests since in don't know the codebase all that well yet (just used it up until now) but i will read myself into it and keep this thread updated should i implement a few of those tests.\n. Actually I thought it an enticing idea to have only one canonical way of\nconsuming... Maybe at another time :)\nAm 08.06.2015 20:20 schrieb \"Gavin M. Roy\" notifications@github.com:\n\n[image: :+1:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/pull/591#issuecomment-110097068.\n. \n",
    "gaochunzy": "@vitaly-krugl thanks for your work, I will check it later.\n. ok, i see.\n. Thanks for your review and it is my mistake to re-import here.\n. Connection._send_message will lead a 'nonetype' error when you send a message to rabbitmq using twisted_server.py in example folder.\n. ",
    "pmav99": "Thanks for the response, I just installed with \n```\n$ pip install git+https://github.com/pika/pika.git\n$ pip freeze | grep pika\npika===0.10.0p0\n```\nThis is what I get from consumer.py\n$ python consumer.py\nINFO       2015-06-10 16:04:46,209 __main__                       connect                              49  : Connecting to amqp://guest:guest@localhost:5672/%2F\nINFO       2015-06-10 16:04:46,217 pika.adapters.base_connection  _create_and_connect_to_socket        198 : Connecting to ::1:5672\nWARNING    2015-06-10 16:04:46,217 pika.adapters.base_connection  _create_and_connect_to_socket        213 : Connection to ::1:5672 failed: [Errno 111] Connection refused\nINFO       2015-06-10 16:04:46,217 pika.adapters.base_connection  _create_and_connect_to_socket        198 : Connecting to 127.0.0.1:5672\nINFO       2015-06-10 16:04:46,225 __main__                       on_connection_open                   95  : Connection opened\nINFO       2015-06-10 16:04:46,225 __main__                       add_on_connection_close_callback     66  : Adding connection close callback\nINFO       2015-06-10 16:04:46,226 __main__                       open_channel                         312 : Creating a new channel\nINFO       2015-06-10 16:04:46,229 __main__                       on_channel_open                      148 : Channel opened\nINFO       2015-06-10 16:04:46,229 __main__                       add_on_channel_close_callback        120 : Adding channel close callback\nINFO       2015-06-10 16:04:46,229 __main__                       setup_exchange                       161 : Declaring exchange message\nINFO       2015-06-10 16:04:46,231 __main__                       on_exchange_declareok                173 : Exchange declared\nINFO       2015-06-10 16:04:46,231 __main__                       setup_queue                          184 : Declaring queue text\nAnd this is what I get from publisher.py\n$ python publisher.py\nINFO       2015-06-10 16:04:51,454 __main__                       connect                              57  : Connecting to amqp://guest:guest@localhost:5672/%2F?connection_attempts=3&heartbeat_interval=3600\nDEBUG      2015-06-10 16:04:51,457 pika.adapters.select_connection _get_poller                          105 : Using EPollPoller\nDEBUG      2015-06-10 16:04:51,457 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': False, 'callback': <bound method SelectConnection._on_connection_error of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>>, 'only': None}\nDEBUG      2015-06-10 16:04:51,458 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': False, 'callback': <bound method ExamplePublisher.on_connection_open of <__main__.ExamplePublisher object at 0x7f82a6c0ef98>>, 'only': None}\nDEBUG      2015-06-10 16:04:51,458 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': True, 'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>>, 'calls': 1, 'only': None}\nINFO       2015-06-10 16:04:51,463 pika.adapters.base_connection  _create_and_connect_to_socket        198 : Connecting to ::1:5672\nWARNING    2015-06-10 16:04:51,463 pika.adapters.base_connection  _create_and_connect_to_socket        213 : Connection to ::1:5672 failed: [Errno 111] Connection refused\nINFO       2015-06-10 16:04:51,463 pika.adapters.base_connection  _create_and_connect_to_socket        198 : Connecting to 127.0.0.1:5672\nDEBUG      2015-06-10 16:04:51,464 pika.adapters.select_connection start                                295 : Starting IOLoop\nDEBUG      2015-06-10 16:04:51,466 pika.callback                  process                              220 : Processing 0:Connection.Start\nDEBUG      2015-06-10 16:04:51,466 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-06-10 16:04:51,466 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-06-10 16:04:51,467 pika.callback                  remove                               269 : Removing callback #0: {'arguments': None, 'one_shot': True, 'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>>, 'calls': 0, 'only': None}\nDEBUG      2015-06-10 16:04:51,467 pika.callback                  process                              234 : Calling <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>> for \"0:Connection.Start\"\nDEBUG      2015-06-10 16:04:51,467 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': True, 'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,468 pika.callback                  process                              220 : Processing 0:Connection.Tune\nDEBUG      2015-06-10 16:04:51,469 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-06-10 16:04:51,469 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-06-10 16:04:51,469 pika.callback                  remove                               269 : Removing callback #0: {'arguments': None, 'one_shot': True, 'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>>, 'calls': 0, 'only': None}\nDEBUG      2015-06-10 16:04:51,469 pika.callback                  process                              234 : Calling <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>> for \"0:Connection.Tune\"\nDEBUG      2015-06-10 16:04:51,470 pika.connection                _create_heartbeat_checker            998 : Creating a HeartbeatChecker: 580\nDEBUG      2015-06-10 16:04:51,470 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': True, 'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,471 pika.callback                  process                              220 : Processing 0:Connection.OpenOk\nDEBUG      2015-06-10 16:04:51,471 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-06-10 16:04:51,472 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-06-10 16:04:51,472 pika.callback                  remove                               269 : Removing callback #0: {'arguments': None, 'one_shot': True, 'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>>, 'calls': 0, 'only': None}\nDEBUG      2015-06-10 16:04:51,472 pika.callback                  process                              234 : Calling <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>> for \"0:Connection.OpenOk\"\nDEBUG      2015-06-10 16:04:51,472 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': True, 'callback': <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,473 pika.callback                  process                              220 : Processing 0:_on_connection_open\nDEBUG      2015-06-10 16:04:51,473 pika.callback                  process                              234 : Calling <bound method ExamplePublisher.on_connection_open of <__main__.ExamplePublisher object at 0x7f82a6c0ef98>> for \"0:_on_connection_open\"\nINFO       2015-06-10 16:04:51,473 __main__                       on_connection_open                   104 : Connection opened\nINFO       2015-06-10 16:04:51,473 __main__                       add_on_connection_close_callback     75  : Adding connection close callback\nDEBUG      2015-06-10 16:04:51,473 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': False, 'callback': <bound method ExamplePublisher.on_connection_closed of <__main__.ExamplePublisher object at 0x7f82a6c0ef98>>, 'only': None}\nINFO       2015-06-10 16:04:51,474 __main__                       open_channel                         325 : Creating a new channel\nDEBUG      2015-06-10 16:04:51,474 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': True, 'callback': <bound method SelectConnection._on_channel_closeok of <pika.adapters.select_connection.SelectConnection object at 0x7f82a82e7f60>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,474 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': False, 'callback': <bound method Channel._on_getempty of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'only': None}\nDEBUG      2015-06-10 16:04:51,474 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': False, 'callback': <bound method Channel._on_cancel of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'only': None}\nDEBUG      2015-06-10 16:04:51,474 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': False, 'callback': <bound method Channel._on_flow of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'only': None}\nDEBUG      2015-06-10 16:04:51,475 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': True, 'callback': <bound method Channel._on_close of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,475 pika.channel                   _rpc                                 1069: Adding in on_synchronous_complete callback\nDEBUG      2015-06-10 16:04:51,475 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': True, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,476 pika.channel                   _rpc                                 1074: Adding passed in callback\nDEBUG      2015-06-10 16:04:51,476 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': True, 'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,478 pika.callback                  process                              220 : Processing 1:Channel.OpenOk\nDEBUG      2015-06-10 16:04:51,479 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-06-10 16:04:51,479 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-06-10 16:04:51,479 pika.callback                  remove                               269 : Removing callback #0: {'arguments': None, 'one_shot': True, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'calls': 0, 'only': None}\nDEBUG      2015-06-10 16:04:51,479 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-06-10 16:04:51,479 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-06-10 16:04:51,480 pika.callback                  remove                               269 : Removing callback #0: {'arguments': None, 'one_shot': True, 'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'calls': 0, 'only': None}\nDEBUG      2015-06-10 16:04:51,480 pika.callback                  process                              234 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x7f82a62f3e80>> for \"1:Channel.OpenOk\"\nDEBUG      2015-06-10 16:04:51,480 pika.channel                   _on_synchronous_complete             1027: 0 blocked frames\nDEBUG      2015-06-10 16:04:51,480 pika.callback                  process                              234 : Calling <bound method Channel._on_openok of <pika.channel.Channel object at 0x7f82a62f3e80>> for \"1:Channel.OpenOk\"\nINFO       2015-06-10 16:04:51,480 __main__                       on_channel_open                      155 : Channel opened\nINFO       2015-06-10 16:04:51,480 __main__                       add_on_channel_close_callback        127 : Adding channel close callback\nDEBUG      2015-06-10 16:04:51,481 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': False, 'callback': <bound method ExamplePublisher.on_channel_closed of <__main__.ExamplePublisher object at 0x7f82a6c0ef98>>, 'only': <pika.channel.Channel object at 0x7f82a62f3e80>}\nINFO       2015-06-10 16:04:51,481 __main__                       setup_exchange                       168 : Declaring exchange message\nDEBUG      2015-06-10 16:04:51,481 pika.channel                   _rpc                                 1069: Adding in on_synchronous_complete callback\nDEBUG      2015-06-10 16:04:51,482 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': True, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,482 pika.channel                   _rpc                                 1074: Adding passed in callback\nDEBUG      2015-06-10 16:04:51,482 pika.callback                  add                                  164 : Added: {'arguments': None, 'one_shot': True, 'callback': <bound method ExamplePublisher.on_exchange_declareok of <__main__.ExamplePublisher object at 0x7f82a6c0ef98>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,484 pika.callback                  process                              220 : Processing 1:Exchange.DeclareOk\nDEBUG      2015-06-10 16:04:51,484 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-06-10 16:04:51,484 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-06-10 16:04:51,485 pika.callback                  remove                               269 : Removing callback #0: {'arguments': None, 'one_shot': True, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'calls': 0, 'only': None}\nDEBUG      2015-06-10 16:04:51,485 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-06-10 16:04:51,485 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-06-10 16:04:51,486 pika.callback                  remove                               269 : Removing callback #0: {'arguments': None, 'one_shot': True, 'callback': <bound method ExamplePublisher.on_exchange_declareok of <__main__.ExamplePublisher object at 0x7f82a6c0ef98>>, 'calls': 0, 'only': None}\nDEBUG      2015-06-10 16:04:51,486 pika.callback                  process                              234 : Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x7f82a62f3e80>> for \"1:Exchange.DeclareOk\"\nDEBUG      2015-06-10 16:04:51,486 pika.channel                   _on_synchronous_complete             1027: 0 blocked frames\nDEBUG      2015-06-10 16:04:51,486 pika.callback                  process                              234 : Calling <bound method ExamplePublisher.on_exchange_declareok of <__main__.ExamplePublisher object at 0x7f82a6c0ef98>> for \"1:Exchange.DeclareOk\"\nINFO       2015-06-10 16:04:51,486 __main__                       on_exchange_declareok                180 : Exchange declared\nINFO       2015-06-10 16:04:51,487 __main__                       setup_queue                          191 : Declaring queue text\nDEBUG      2015-06-10 16:04:51,487 pika.channel                   _rpc                                 1069: Adding in on_synchronous_complete callback\nDEBUG      2015-06-10 16:04:51,487 pika.callback                  add                                  164 : Added: {'arguments': {'queue': 'text'}, 'one_shot': True, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,488 pika.channel                   _rpc                                 1074: Adding passed in callback\nDEBUG      2015-06-10 16:04:51,488 pika.callback                  add                                  164 : Added: {'arguments': {'queue': 'text'}, 'one_shot': True, 'callback': <bound method ExamplePublisher.on_queue_declareok of <__main__.ExamplePublisher object at 0x7f82a6c0ef98>>, 'calls': 1, 'only': None}\nDEBUG      2015-06-10 16:04:51,490 pika.callback                  process                              220 : Processing 1:Queue.DeclareOk\nDEBUG      2015-06-10 16:04:51,490 pika.callback                  _obj_arguments_match                 374 : Values in <class 'pika.spec.Queue.DeclareOk'> do not match for queue\nDEBUG      2015-06-10 16:04:51,490 pika.callback                  _should_process_callback             389 : Arguments do not match for {'arguments': {'queue': 'text'}, 'one_shot': True, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x7f82a62f3e80>>, 'calls': 1, 'only': None}, [<METHOD(['channel_number=1', 'frame_type=1', 'method=<Queue.DeclareOk([\\'consumer_count=0\\', \\'message_count=0\\', \"queue=b\\'text\\'\"])>'])>]\nDEBUG      2015-06-10 16:04:51,490 pika.callback                  _obj_arguments_match                 374 : Values in <class 'pika.spec.Queue.DeclareOk'> do not match for queue\nDEBUG      2015-06-10 16:04:51,490 pika.callback                  _should_process_callback             389 : Arguments do not match for {'arguments': {'queue': 'text'}, 'one_shot': True, 'callback': <bound method ExamplePublisher.on_queue_declareok of <__main__.ExamplePublisher object at 0x7f82a6c0ef98>>, 'calls': 1, 'only': None}, [<METHOD(['channel_number=1', 'frame_type=1', 'method=<Queue.DeclareOk([\\'consumer_count=0\\', \\'message_count=0\\', \"queue=b\\'text\\'\"])>'])>]\n. ",
    "rajesh-kanakabandi": "I was able to fix it by just moving the basic_qos call to set the prefetch count to 1 on_channel_create call back method.\nfor some reason setting prefetch value to 1 just before the basic_consume is not good enough. must be something to do with the pika's io loop.\nthank you.\n. thank you Vitaly, My Consumer looks very similar to the async Consumer example. but I am trying to consume from 2 different queues with different callback methods using the same connection and channel. is there any example code for that?\n. ",
    "fgrim": "Yeah. I am using 0.9.14. But the bug is also resident in master (i just verified). It is odd.  The problem goes away by switching to the SelectConnection in the same example code. I poked around the code trying to figure out what is going on but it looks like everything is going through the same base class in base_connection.py.  But as it stands it looks like the AsyncoreConnection just doesn't work with rabbit 3.5.1 at least\n. By the way, this issue aside, pika is a really nice interface to rmq.\n. ",
    "beruic": "This is during actual use. This is suficient to trigger it:\n```\n!/usr/bin/env python3\nimport pika\nif name == 'main':\nparameters = pika.ConnectionParameters(\"localhost\")\n\n```\nI'm on Ubuntu 15.04 using python 3.4.\n. Tell me what to do.\n. Running the above script, the output is:\nberuic@Skrumpbuntu:~$ ./test.py\nTraceback (most recent call last):\n  File \"./test.py\", line 7, in <module>\n    parameters = pika.ConnectionParameters(\"localhost\")\n  File \"/usr/local/lib/python3.4/dist-packages/pika/connection.py\", line 361, in __init__\n    if host and self._validate_host(host):\n  File \"/usr/local/lib/python3.4/dist-packages/pika/connection.py\", line 207, in _validate_host\n    if not isinstance(host, basestring):\nNameError: name 'basestring' is not defined\n. The pika package I get throgh pip is version 0.9.14. I installed it in a virtualenv through pycharm. I'll try with the latest version.\n. The issue is the version. It works with the newest version.\n. ",
    "Dmitry-N-Medvedev": "pika version: pika==0.10.0\npython --version: Python 3.5.1\nboth are used from virtual environment.\nthe error persists:\nTraceback (most recent call last):\n  File \"/home/dmitry/Documents/projects/cmi-services/tests/common/rmq/infra/test_rmq_infra.py\", line 42, in test_recreate\n    infra.recreate()\n  File \"/home/dmitry/Documents/projects/cmi-services/common/rmq/infra/__init__.py\", line 141, in recreate\n    self._connection = self.connect()\n  File \"/home/dmitry/Documents/projects/cmi-services/common/rmq/infra/__init__.py\", line 96, in connect\n    pika.ConnectionParameters(self.connectionParameters),\n  File \"/home/dmitry/Documents/projects/cmi-services/venv/lib/python3.5/site-packages/pika/connection.py\", line 652, in __init__\n    self.host = host\n  File \"/home/dmitry/Documents/projects/cmi-services/venv/lib/python3.5/site-packages/pika/connection.py\", line 392, in host\n    (value,))\nTypeError: host must be a str or unicode str, but got <ConnectionParameters host=127.0.0.1 port=5672 virtual_host=cmi ssl=False>\n. ",
    "abompard": "I think this is fixed by #1069.. I'll try to write some tests, since I need that component for my new project. But don't wait on me to release 1.0.0.. I have the following warnings, that I've disabled:\n W0703, line 124 & 182, \"Catching too general exception Exception (broad-except)\": we don't know what the exception can be in the wrapped function so I think it's OK to be broad.\n W0221, line 241, \"Parameters differ from overridden 'channel' method (arguments-differ)\": that's intended because the second argument is forced on the overridden method.\nBut that's only my opinion, I can adapt if you disagree.. Oh by the way, now that you've removed TwistedConnection, you can also remove _TwistedIOServicesAdapter and _TwistedDeferredIOReference as they were only used by TwistedConnection.. Thanks for your suggestions @vitaly-krugl . I tried to do this refactoring in the last commit, please tell me if you like it better that way.. The coverage test fails but the reason seems unrelated (AWS credentials).. Thanks a lot for your in-depth review @vitaly-krugl ! I've implemented the proposed changes in the last three commits. It seems to be working, Pylint is silent and coverage is as before (99% because of the two unimplemented methods).. I'm currently copying the other methods and I realize I should amend my last comment: all the public methods in pika.Channel must be copied, not only the redefined or wrapped ones. TwistedChannel only wraps the methods that have a callback, but without __getattr__ even methods such as basic_ack must be redefined, only to return self._channel.<parent method>.\nAs I said I don't mind doing it, copying is easy, I'm just worried about the maintenance. Changes in the docstrings will have to be propagated too.. OK, I've made the changes to the class. It's a big commit and it also adds missing features that I added while I was checking the public Channel methods. The commit message has details about what's inside.\nI will be offline starting Wed 27th for two weeks until Wed 11th included, so I tried to have a commit I could show you for review before I leave. I know it's pretty big, sorry for that.\nThanks a lot for your advice and guidance.. I'm back! Eager to see and implement your future suggestions :-). Hey @lukebakken , did you get a chance to work on this? I don't want to be pushy, I understand other things can get in the way, it's just a friendly reminder :-). I'll check it out, thanks for the merge!. OK I found why, it's because functools.wraps in Python2 requires the __name__ attribute on functions and mock.Mock does not have that. In Python3, functools.wraps is protected against missing attributes with a try..except block.\nI have a patch, I'll make a PR (with a couple minor fixes to the Twisted adapter) as soon as I can pull from master. At the moment Github does not show me this PR in the master commits (a hiccup maybe?). Or maybe the code wasn't merged because the tests didn't pass?. I'm not sure why the tests failed on Python 2.7, it seems unrelated.. I'm not the original author of that but if I understand correctly the point was to avoid conflicting with one of the wrapped method (since __getattr__ is subclassed here). However none of the wrapped method start with an underscore and I don't think we will want to wrap one that does in the future, so a single underscore prefix should be sufficient. I also don't expect the TwistedChannel class to be subclassed. I'll change it.. Sure, I'll do it, but we must be careful to update TwistedChannel's wrapper methods whenever we update the main Channel's methods. I guess that's a trade-off in favor of more advanced editors :-). You're right. I'll implement a mechanism similar to the ready attribute.. Sure, will do.. Sure!. Right, I'll fix this.. I didn't think so, I thought you meant explicit signatures for the redefined methods only. But if you prefer not using __getattr__, I can copy all the methods in WRAPPED_METHODS in this class.\nI'm just a bit worried it will be more maintenance in the long run, since they will have to be adapted in this class too whenever they are changed in the main Channel class. But I'll follow you if you think it's worthwhile.. ",
    "mpolk": "Yes I know. But does not it mean that:\n1) since ioloop.start() blocks the calling thread\n2) and since any other thread should not even touch this ioloop,\ntherefore, any pika consumer should live forever?\nI understand, some ways to stop this eternal consuming still exist:\n1) for example, send \"Terminate now!\" message from the main thread to the specially arranged exchange on the RabbitMQ, so that RabbitMQ will return it to the consuming thread; it will obey and terminate\n2) apply \"kill -9\" to entire process with all its threads, consumers and ioloops\n3) unplug the server from the outlet\n4)...\nAny of this methods will probably stop the consumer, but fixing ioloop.stop() seems to be a better way. This call is obviously designed to serve this purpose, and some recent attempts to make it work may be found right in this repository.\n. Thank you, I will try it.\n2016-01-13 0:26 GMT+02:00 vitaly-krugl notifications@github.com:\n\n@mpolk https://github.com/mpolk, SelectConnection's ioloop.stop is\nthread-safe as of pika v0.10.0. Please verify.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/pika/pika/issues/620#issuecomment-171079997.\n\n\n\u0421 \u0443\u0432\u0430\u0436\u0435\u043d\u0438\u0435\u043c,\n\u0414\u043c.\u0421\u0442\u0435\u043f\u0430\u043d\u0435\u043d\u043a\u043e, \u0441\u0438\u0441\u0442.\u0430\u0434\u043c\u0438\u043d. \"\u0421\u0430\u0442\u0435\u043b\u043b\u0438\u0442 \u0421\u0435\u0440\u0432\u0438\u0441\"\nICQ: 112689047 \u0442\u0435\u043b.: +38(050)47-32-118\n. ",
    "xmrdsc": "@vitaly-krugl: Thanks for the merge! To answer your question: which files would need to be tested? I don't have any experience in writing tests for Twisted (or unittests in general). I can give it a try.\n. ",
    "edmcman": "I just installed from pip... 0.9.14\n. Ok, will try it now\n. Seems to work, thanks.  I did notice that specifying a heartbeat_interval higher than the server's recommended value results in the server's value being used.  Is this correct?  (Doesn't effect me either way -- I just thought it was odd)\n. ",
    "reddec": "@vitaly-krugl thanks for review. I will fix it. Are you sure, that connection have not to be closed for system exception? At least like this:\npython\nself._cancel_all_consumers()\nself._cleanup()\nwithout any notification message (that may be slow)\n. Thank you for help!\nDoes it look good now?\n. Done\n. Done \n. Thank you for your help!\n. ",
    "shinji-s": "@vitaly-krugl \nThank you for your valuable feedback. I thought I've sent a PR for the timer-fix only but it seems the EINTR patch was piggy backed. Sorry for the extra review work that has caused. Following your advice I'll first work on renewing PR for the timer issue.\nTo me it seems forbidding timer removal within a timeout handler altogether  is a cleaner solution, which may be too big a change to bring in now and/or too difficult/costly to implement.  With that said, I would take your patch as is for the imminent fix.\nAs for EINTR issue, raised exception class varies depending on the polling mechanism being used. Looking at selectmodule.c in python 2.7.3, the mapping is as follows;\nselect => SelectError\npoll => SelectError\nepoll => IOError\nkevent => OSError\nkqueue => IOError\nIt seems the exception handler need to be prepared for more than one kind of exception class even though we hypothetically target 2.7.3 only. I'll work on it after preparing a renewed PR for the timer.\n. @vitaly-krugl \nWill you please have a look on the new patch below before my sending in another PR and tell me if it's good enough for submission? (Key recomplutation optimization seemed less complex than I had thought. I bit the bullet. Please advise if you think that is too much to carry in the patch.)\nhttps://github.com/shinji-s/pika/commit/e1d287bfc05da79c44aadfeb04843e03dd9fbaee\nAs I have not dealt with unittest beffore and  I don't understand how the test cases are organized in pika, I might be doing something terribly wrong. Please holler if I did.\n. Thank you for the poiner to Travis-CI. The issue with Python3 should have been resolved.\nFor unit testing I need to have two timeouts fired in the same invocation of process_timeouts(). \nWhat do you have to say about wrapping the expression below in a method and patching it when testing?\n      [(k,timer) for (k,timer) in self._timeouts.items() if timer['deadline'] <= now]\n. @gmr , @vitaly-krugl \nWill you please give another consideration to the merge?\n. @vitaly-krugl Thank you very much for your kind inspection. The typo is now fixed. The timer sorting code is left as it is.\n. @gmr, @vitaly-krugl Thank you guys for kindly guiding a github newbie.\n. @gmr\nThanks to @vitaly-krugl 's direction, I've managed to cook up small code that demonstrate the issue.\nUnfortunately, however, I'm on a ferry to another island to start my vacation there. I'm afraid I'm not going to make much progress in coming two weeks.\nThe output from the script is as follows. I think the issue has more to do with implementatiion select.epoll in python 2.7 than with ubuntu.\nTesting\nTraceback (most recent call last):\nFile \"test.py\", line 70, in run_test_on\npoll_using(poller)\nFile \"test.py\", line 35, in poll_using\npoller(sock)\nFile \"test.py\", line 40, in select_poller\nselect.select([sock], [], []) \\\nerror: (4, 'Interrupted system call')\nTesting\nTraceback (most recent call last):\nFile \"test.py\", line 70, in run_test_on\npoll_using(poller)\nFile \"test.py\", line 35, in poll_using\npoller(sock)\nFile \"test.py\", line 46, in epoll_poller\nep.poll()\nIOError: [Errno 4] Interrupted system call\nHere is the test code.\n`````` python\nimport threading, socket, time, signal, sys, select, os, traceback\nPORT = 8181\nSIGNAL_TO_USE = signal.SIGUSR1\nclass KillerThread(threading.Thread):\n    def init(self, initial_wait, *args, kw):\n        self.initial_wait = initial_wait\n        super(KillerThread, self).init(*args, kw)\ndef run(self):\n    time.sleep(self.initial_wait)\n    os.kill(os.getpid(), SIGNAL_TO_USE)\n\nclass ServerThread(threading.Thread):\n    def run(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        sock.bind(('localhost', PORT))\n        sock.listen(8)\n        sk, addr = sock.accept()\n        try:\n            while 1:\n                b = sk.recv(4096)\n                if not b:\n                    break\n        finally:\n            sk.close()\ndef poll_using(poller):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        sock.connect(('localhost', PORT))\n        poller(sock)\n    finally:\n        sock.close()\ndef select_poller(sock):\n    select.select([sock], [], [])\ndef epoll_poller(sock):\n    ep = select.epoll()\n    ep.register(sock, select.EPOLLIN)\n    ep.poll()\ndef signal_handler(signum, interrupted_stack):\n    # print 'Got signal-%s. Ignoiring' % signum\n    pass\ndef run_test_on(poller):\n    th_list = []\n    try:\n        k = KillerThread(0.2)\n        k.start()\n        th_list.append(k)\n        s = ServerThread()\n        s.start()\n        th_list.append(s)\n    time.sleep(0.1)\n    old_handler = signal.signal(SIGNAL_TO_USE, signal_handler)\n    try:\n        print 'Testing %s' % poller\n        try:\n            poll_using(poller)\n        except KeyboardInterrupt, SystemExit:\n            raise\n        except:\n            traceback.print_exc()\n            print\n            print\n    finally:\n        signal.signal(SIGNAL_TO_USE, old_handler)\nfinally:\n    for th in th_list:\n        th.join()\n\nif name=='main':\n    run_test_on( select_poller )\n    run_test_on( epoll_poller )\n```python\n``````\n. Output was messed up.\n```\nTesting \nTraceback (most recent call last):\n  File \"test.py\", line 70, in run_test_on\n    poll_using(poller)\n  File \"test.py\", line 35, in poll_using\n    poller(sock)\n  File \"test.py\", line 40, in select_poller\n    select.select([sock], [], [])\nerror: (4, 'Interrupted system call')\nTesting \nTraceback (most recent call last):\n  File \"test.py\", line 70, in run_test_on\n    poll_using(poller)\n  File \"test.py\", line 35, in poll_using\n    poller(sock)\n  File \"test.py\", line 46, in epoll_poller\n    ep.poll()\nIOError: [Errno 4] Interrupted system call\n```\n. Although I have pylint related fixes and test cases tha need a little brushing up pending in my workspace, I don't mind the fix above to be merged now if you guys sees it's ok. +1 from me. I'll commit&push pending changes and send another PR later (Perhaps comming Mon or Tue?).\n. @gmr @vitaly-krugl  I will commit pending test cases asap when I get back home on Sep. 9. I may need your help to get tests to be Windows 'compatible'.\n. Sorry, pushed the last commit too early. Now fixing the problem.\n. @gmr, @vitaly-krugl Turned out that InterruptedError as well needs to be handled in PY3. The test case should be ready for your review. Don't know yet how it behaves in  Windows envrionment though.\nPlease note that 300be19 is a fundamental change that requires special attention.\n. @vitaly-krugl Thank you for your feedback. I think I've applied most of the fixes you suggested except one on changing _SELECT_ERROR_CHECKER and one on test class derivation. Please let me know if there are some that I missed.\n. @vitaly-krugl Thank you for your directions.\nI got new commit message for  'git rebase -i HEAD~15, sqush on all lines except the first and git puph'  but the old commit messages too are still in 'git log'. Did I do something wrong? Maybe it's better if I fork from pika/pika again and apply the diffs?\n. The culprit must be the 'git pull' that I had to perform (without thinking about what it would do) in order to have push accepted. Should I perform 'git push --force' after rebase?\n. I think I've manged to fix the history.\n. > git rebase -i master\n\n@vitaly-krugl  That looks more reliable than rebase -i HEAD~n.\nThank you for the tip and review of the PR.\n. I guess the cause is python 3.5 absorbing EINTR and retries the system call within the library without raising an exception to the application. I'll have a closer look later today or tomorrow.\n. There was a typo in a Python3.5 specific portion of the code.\nI've sent a new PR with the fix. Please review.\n. @chengchengpei, if you mean IOError(Errno=4) by \"Errno=4\" you've likely hit the issue patched by this; \nhttps://github.com/pika/pika/commit/59dfa06cb18ce53e12979547312941eba04b0a6d\n. I think you have hit a bug that has been patched but not in the version of pika you are using.\nSee https://github.com/pika/pika/issues/664.\n. Would\nassert 'no_ack' not in kwargs, \"'no_ack' is obsolete. Use 'auto_ack' instead.\"\nbe a too much hand-holding?\n. The attempt to populate coverage/ directory from s3 storage is failing because credential is not installed/found. Sorry I don't know how to fix though.. Regarding the coverage test failure, my guess below may be well off the mark but I'd try.\n\nThe credential is given here in .travis.yml\n\n- secure:  {actual-encrypted-string-is-omitted}\n  - secure:  {actual-encrypted-string-is-omitted}\n\nHowever, https://docs.travis-ci.com/user/encryption-keys/ has this to say:\n\nPlease note that encrypted environment variables are not available for pull requests from forks.\n\nThat is why the credential is not available in the above build?\n. Forgive me for chipping in to hazard a wild guess. Is there chance network latency/bandwidth is involved here?. How should I deal with the warnings like one below?\n  Attribute 'timer_stack' defined outside init (attribute-defined-outside-init)\nIf we initialize all attributes, that are need only temporarily during execution of certain test(s), then init method gets too messy. Any suggestions?\nAnother thing is I don't see pylint complain about the use of assertEqual(). Will you please show me how you run pylint? Or the comment \"Please check your changes with pylint and fix findings.\" isn't necessarily directed to the diff that the comment is attached to?\n. @vitaly-krugl Thank you very much for your replies. I'll follow your advice of selectively disabling the warningings thought rather than using setattr() instead of regular assignment. As for my question on pylint complaining about assertEqual, it was my misunderstanding. My apology.\n. Thanks for pointing out sorted() can take a generator, which I was not aware of.\nI think the code is better left as it is for now because it seems generator is not necessarily faster (especially because sorted() materialize generator as a list anyway?).\nFor our amusement, the script below generate following output. (The left side number is for generator and the right is for list.)\nWith threshold==0.01 (selectivity approx 1%)\nTotal = 0.4483 / 0.0092\nTotal = 0.3955 / 0.0099\nWith threshold==1.00 (selectivity approx 100%)\nTotal = 4.7233 / 2.2647\nTotal = 4.7220 / 2.2071\n```\nimport time, random\ndef bench(l):\n    start = time.time()\n    sorted(l, key = lambda item: item[1]['k'])\n    elapsed = time.time() - start\n    return elapsed\ndef run(threshold):\n    accum = {}\n    for i in range(10):\n        source = dict(map(lambda x: (random.random(), {'k':random.random()}),\n                          range(0, 65536)))\n        accum[1] = accum.get(1, 0.0) + \\\n            bench( (k,v) for (k,v) in source.items() if k<threshold )\n        accum[2] = accum.get(2, 0.0) + \\\n            bench( [(k,v) for (k,v) in source.items() if k<threshold] )\n    print 'Total = %.4f / %.4f' % (accum[1], accum[2])\naccum = {}\nfor i in range(10):\n    source = dict(map(lambda x: (random.random(), {'k':random.random()}),\n                      range(0, 65536)))\n    accum[2] = accum.get(2, 0.0) + \\\n        bench( [(k,v) for (k,v) in source.items() if k<threshold] )\n    accum[1] = accum.get(1, 0.0) + \\\n        bench( (k,v) for (k,v) in source.items() if k<threshold )\nprint 'Total = %.4f / %.4f' % (accum[1], accum[2])\n\nif name=='main':\n    print 'With threshold==0.01 (selectivity approx 1%)'\n    run(0.01)\n    print 'With threshold==1.00 (selectivity approx 100%)'\n    run(1.0)\n```\n. And we are supposed to get no EINTR exceptions raised in Python 3.5 as the standard library performs retries. Some of the testes here should fail.\nI'm afraid I don't get what changes you are suggesting here.\nSince _SELECT_ERRORS_CHECKERS is indexed with most-derived class of a caught exception and on some platform the classes are different, we can't reduce _SELECT_ERROR_CHECKERS to the one-element dictionary. Also the values of _SELECT_ERROR_CHECKERS should be callable.\n. Thanks. Since I made the last push I realized that not all classes in _SELECT_ERROR_CHECKS needs to be in _SELECT_ERRORS if one is a subclass of another, only most generic ones need to be caught. I'll leave the optimization out for now and follow your suggestion.\n. I understand that pylint error/warning suppression is effective only in the block it appears.\nAnd I think it's ok or desirable to ignore warnings about failed imports after a failed import.\n. Thanks. This made the code much lean and clean.\n. It depends on IOLoopBaseTest.setUp() to have self.ioloop created based upon the mechanism specifid by self.SELECT_TYPE.\n. Thank you for the detailed explanation. Now I get what you meant and see that it is a valid concern.\nI'll use a sequence of statement to initialize the dict so that precedence is more obvious.\n. I guess we do.\nIn python3.4 on Windows;\n```\n\n\n\ne = InterruptedError()\ntype(e)\n\nisinstance(e, OSError)\nTrue\n```\n\n\n\nAlso please search for nose.proxy.KeyError: <class 'InterruptedError'> in the failed logs at\nhttps://travis-ci.org/pika/pika/builds/79073409\n. Thanks for pointing it out. I've left the comment half cooked. I'll make it more detailed.\n. This discussion lead me to notice a bug in _is_resumable() implementation. It is possible that an exception which is a subclass of a classe in _SELECT_ERRORS but whose actual class is not listed in _SELECT_ERROR_CHECKERS. That exception needs to be reraised rather than causing KeyError when _SELECT_ERROR_CHECKERS is indexed.\n. Changed the name to timer and installed a cleanup handler.\n. As for python 3.5 compatibility, the test (I wrote 'tests' before but there is only one test that exercise EINTR handling. my bad) should run except that EINTR exception does not get handled. cf. https://www.python.org/dev/peps/pep-0475/\nI introduced pika.compat.EINTR_IS_EXPOSED flag and asserts on _is_resumable_mock.call_count accordingly. \n. ",
    "bobcyw": "@gmr ,I run rabbitmq also in a dock container. It looks like that\ndocker run -p 5672:5672 -d --hostname my-rabbit --name some-rabbit rabbitmq\nit is a little different from office command line (https://hub.docker.com/_/rabbitmq/)\ndocker run -d --hostname my-rabbit --name some-rabbit rabbitmq:3-management\nand this is container description\ne4b4034dcd84    rabbitmq    \"/docker-entrypoint.   7 hours ago    Up 7 hours    0.0.0.0:5672->5672/tcp  some-rabbit\nIs port mapping cause this exception? Because there's no 5672/UDP port mapping.\n@vitaly-krugl thank you for your suggestion, I'll try pip install pika --pre.\n. @gmr The same module is ok on my MacBook with the same RabbitMQ IP addr. 5 workers are running over 12 hours. And it's also ok when type python3 -m maintenance.rss.fenci_worker in the dock container's interactive console with /bin/bash. This timeout exception only occurs when put python3 -m maintenance.rss.fenci_worker in dockerfile like CMD python3 -m maintenance.rss.fenci_worker.\n. It seems ok when container's host os is Ubuntu 14.04. This exception only occours when container's host os is Ubuntu 15.04. One workaround way in Ubuntu 15.04 is running the worker module in container's console manually instead of putting this command in dockerfile. \nBy the way, both Ubuntu 14.04 and Ubuntu 15.04 are running in VirtualBox. The real host os is OSX. Is that a problem?\n. ",
    "prawn-cake": "+1\nstill reproduced\n. Not production-ready workaround, if you need to stay performant you have to reuse your connection --> heartbeats are required\n. Indeed in most of cases keeping single connection as long as possible better than open/close it - it's not reasonable, but affordable in some cases.\nExceptions can be if connections are limited (hardware or server limitations) or you wanna to hide stuff - shady business.\n@nam4dev this is reasonable and for HTTP as well (hereby I mean rich web apps mostly) - keep as less connections as possible and as long as possible and this is one of the drawbacks for HTTP 1/X with its workarounds with \"Keep-Alive\" HTTP headers, and this is one of the bullet points of HTTP 2, cuz it obviously increasing performance and getting rid of this handshake hell.\nSo yes, we have to have working heartbeats =)\n. ",
    "nam4dev": "I've actually found a workaround, as I bumped into that bug as well.\nThis is only a workaround\n- I initialize a new connection each time I need to produce (instead of keeping connection alive).\n- I then, close the connection when data is produced.\nI did not reproduce the bug since then!\nHope it helps :)\n. @prawn-cake \nDepends on your design and needs indeed :)\nI agree with you heartbeats are required!!\nBut just a thought, \n- AMQP is just a protocol defined over TCP, right?\n- HTTP is a protocol over TCP too, right?\nTherefore, when you use an HTTP client, say for instance requests do you really need to keep your connection alive when sending a POST request ?\nI mean, in term of Producer, is that such a big deal to open a connection, send data on the wire, then close?\nEven in production for some cases at least, I think this is an acceptable workaround, though, I agree not the best!\n. Ok thx!\n. :+1: \n. ",
    "KenjiTakahashi": "Thanks guys for quick respones!\n@gmr: That might not be so easy, because real code is inside celery worker, which messes with logging a bit. But will try to make them appear somehow. Anyway, will let you know if I get something.\n@vitaly-krugl: That's pika 0.10.0b2 (as stated in the report :-)) running on latest Debian Stable. The call about exceptions might be good, we are currently catching UnroutableError and NackError on publish. Is there anything else that publish method might throw?\n. Uh, sorry for the delay.\nThanks @vitaly-krugl for a detailed explanation, definitely useful to improve our error handling.\nIn the meanwhile, I've found the cause of the problem. It turned out that a different part of our system, that operates on a threadpool, was given the same pika connection instance as the one were the error occurred. Hence there were two different threads trying to publish on the same connection and as pika connection is not threadsafe, it exploded like above.\nIt makes sense and I have not seen the error after fixing that some time ago, so I'll call that closed.\n. ",
    "tamjd1": "Will give it a go and share results. The only issue is, it happens randomly after weeks of stability and I can't seem to figure out how to recreate it. Nonetheless, I'll give pika 0.10.0 a test drive for some time and report back.\nThanks,\nT\n. Just reporting back... The pre-release version of the code has been in our UAT environment for over a month now and no issues so far!\n. ",
    "steveYeah": "OK, thanks. I haven't worked with codegen before, so not sure what to update, but will take a look and add another PR soon\n. ",
    "ulyktey": "This situation is happened when RabbitMQ server does not have free file/socket descriptors. Please a look on screenshot above. In this case BlockingConnection._adapter_connect can not finish loop: \nwhile not self.is_open:\n   self.process_data_events()\nTo reproduce, please insert line \"ulimit -n 115\"  in file /etc/rabbitmq/rabbitmq-env.conf.\nAfter that please restart the RabbitMQ server: /sbin/service rabbitmq-server restart\n. ok, thanks\n. Pika version 0.9.14\nMy script:\n  1 import pika\n  2 import logging\n  3\n  4 logging.basicConfig(level=logging.DEBUG)\n  5\n  6 credentials = pika.PlainCredentials('guest', 'guest1')\n  7 parameters = pika.ConnectionParameters(host='localhost', port=5672, credentials=credentials)\n  8 conn = pika.BlockingConnection(parameters=parameters)\nPika raise an error after 40 seconds.\nExample of pika log on DEBUG\npython test_pika.py\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nINFO:pika.adapters.base_connection:Connecting to 127.0.0.1:5672\nERROR:pika.adapters.base_connection:Socket Error on fd 3: 104\nERROR:pika.adapters.base_connection:Incompatible Protocol Versions\nTraceback (most recent call last):\n  File \"test_pika.py\", line 8, in \n    conn = pika.BlockingConnection(parameters=parameters)\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 130, in init\n    super(BlockingConnection, self).init(parameters, None, False)\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/base_connection.py\", line 72, in init\n    on_close_callback)\n  File \"/usr/lib/python2.6/site-packages/pika/connection.py\", line 600, in init\n    self.connect()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 230, in connect\n    error = self._adapter_connect()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 309, in _adapter_connect\n    self.process_data_events()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 240, in process_data_events\n    if self._handle_read():\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 348, in _handle_read\n    super(BlockingConnection, self)._handle_read()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/base_connection.py\", line 343, in _handle_read\n    return self._handle_error(error)\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/base_connection.py\", line 302, in _handle_error\n    self._handle_disconnect()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/base_connection.py\", line 248, in _handle_disconnect\n    self._adapter_disconnect()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 318, in _adapter_disconnect\n    self._check_state_on_disconnect()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/blocking_connection.py\", line 368, in _check_state_on_disconnect\n    super(BlockingConnection, self)._check_state_on_disconnect()\n  File \"/usr/lib/python2.6/site-packages/pika/adapters/base_connection.py\", line 149, in _check_state_on_disconnect\n    raise exceptions.IncompatibleProtocolError\npika.exceptions.IncompatibleProtocolError\n\nTest 2\nPika version 0.10\npython pika_test.py\nDEBUG:pika.adapters.select_connection:Using KQueuePoller\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nINFO:pika.adapters.base_connection:Connecting to 127.0.0.1:5672\nERROR:pika.adapters.base_connection:Socket Error: 54\nERROR:pika.adapters.base_connection:Incompatible Protocol Versions\nDEBUG:pika.callback:Incremented callback reference counter: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\nTraceback (most recent call last):\n  File \"pika_test.py\", line 8, in \n    conn = pika.BlockingConnection(parameters=parameters)\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/blocking_connection.py\", line 339, in init\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/blocking_connection.py\", line 374, in _process_io_for_connection_setup\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/blocking_connection.py\", line 410, in _flush_output\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/select_connection.py\", line 528, in poll\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/select_connection.py\", line 443, in _process_fd_events\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/base_connection.py\", line 364, in _handle_events\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/base_connection.py\", line 407, in _handle_read\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/base_connection.py\", line 338, in _handle_error\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/base_connection.py\", line 288, in _handle_disconnect\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/select_connection.py\", line 95, in _adapter_disconnect\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/base_connection.py\", line 154, in _adapter_disconnect\n  File \"build/bdist.freebsd-7-amd64/egg/pika/adapters/base_connection.py\", line 169, in _check_state_on_disconnect\npika.exceptions.IncompatibleProtocolError\n. Hi guys,\nI did small fix in code for version pika 0.9.14. I added \"timeout\" option in pika.BlockingConnection and added check in this loop:\nwhile not self.is_open and time.time() > time.time() - timeout:\nself.process_data_events()\nThis fix works for me, but it would be better when pika will contain addition timeout option.\nThanks,\nYurii\n. ",
    "gst": "@vitaly-krugl \nYou say this is a malicious case, and I tend to agree with you, but malicious cases are not impossible cases..  shit happens, Murphy's law you know :)\nSo I agree with @ulyktey, his proposed solution (or something like that) seems to be what needs to be done.. / what's missing against this specific case.. imo.\nnb: I've been testing pika these last 2 days and it's the kind of case that I also want to test/check and make sure that pika is safe/reliable/good against.\n. problem confirmed on my side.\n. > Now, if I set a connection_attemps > 1 to the BlockingConnection parameters then I get well the socket.error timeout exception..\nwell.. what I get is exactly AMQPConnectionError (but with the socket timeout error text as argument so it's ok for me).\n. Have maybe a patch.. PR should arrive..\n. In this regards, I find that it should be usefull / good to have a connect_timeout parameter, so to be used instead of socket_timeout only in the _create_and_connect_to_socket().. once the connection is established then the socket_timeout would be applied.. what do you think ?\n. damn, apparently it's not good (enough), now getting some AssertionError when using connection_attemps > 1..\nEDIT: Found & Fixed.\n. There it is.\n. NB: actually I'm not completly sure this is the best way / place to handle this specific case..\n. damn, 4 green on the 5 travis configurations.. and the one which fail is python3.4 but I can't reproduce it on my side..\n. @vitaly-krugl \ncould please restart the travis build on this : https://travis-ci.org/pika/pika/jobs/98017989  ?\nwant to see if it reproduces itself..\n. damn, worse, and on my side I have all tests (455) passing under python2.7 as well 3.4 and 3.5 :s \n. @vitaly-krugl \nsorry didn't see directly your comments yesterday.\nAlready I could reproduce some of the travis failures (simply by running the test with nosetest instead of pytest..).\nAh, and you have apparently a fix for that .. ? What I'm not sure is if it's related to the specific problem I'm trying to get fixed here. If not related: will you open a separate PR with it ? or else I'm not against doing it..\nAnd, if I get all things correct, about the AsyncTestCase adaptation I made it's better to have it separately as well.. I'm gonna do that no problem :)\n. > I'm gonna do that no problem :)\njust waiting confirmation..\n. For point 4, I'm not sure what to do finally. I do think that my test case is better because I also assert that the socket.connect() is actually called the number of times desired (\"connect_attempts\" times) (and normally with the expected arguments though that's less \"important\" in this situation) + also, and that's the more important, that the raised exception is actually the timeout one..\n. @vitaly-krugl  I'm not sure what must/should be done finally here.. :?\n. both I guess ;)\n. done and rebased. \n. see https://github.com/pika/pika/issues/659 for more details\n. Hi @vitaly-krugl \nwith python3.4(.3), I just do : \n(env) \u2718 ~/work/public/python/pika [master|\u2026746]\ngreg@hal  $ python -c 'import time, pika; print(pika); c = pika.BlockingConnection(); time.sleep(5) ; c.channel()'\n<module 'pika' from '/mnt/DATA/greg/work/public/python/pika/pika/__init__.py'>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/mnt/DATA/greg/work/public/python/pika/pika/adapters/blocking_connection.py\", line 698, in channel\n    channel_number=channel_number)\n  File \"/mnt/DATA/greg/work/public/python/pika/pika/connection.py\", line 733, in channel\n    return self._channels[channel_number]\nKeyError: 1\n(env) \u2718 ~/work/public/python/pika [master|\u2026746]\ngreg@hal  $\nduring the time.sleep(5) I shutdown rabbitmq server (with sudo service rabbitmq-server stop).\nI'm on master:\n```\n$ git log -1\ncommit d8a782d97579cd96ed67ccfb55f63ca8fdafa199\nMerge: 208b8b5 3ef1a16\nAuthor: Gavin M. Roy gavinmroy@gmail.com\nDate:   Mon Dec 7 18:24:57 2015 -0500\nMerge pull request #672 from dave-shawley/call-open-error-cb\n\nCall on open error callback for probable auth/access failures\n\n```\n. I'm on ubuntu 15.04 and its his system Python3.4(.3).\nwith my python2.7 I also get that:\ngreg@hal  $ python2.7 -c 'import pika; print(pika); c = pika.BlockingConnection(); import subprocess; subprocess.check_call([\"sudo\", \"rabbitmqctl\", \"stop\"]); c.channel()' \n<module 'pika' from 'pika/__init__.pyc'>\nStopping and halting node rabbit@hal ...\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"pika/adapters/blocking_connection.py\", line 698, in channel\n    channel_number=channel_number)\n  File \"pika/connection.py\", line 733, in channel\n    return self._channels[channel_number]\nKeyError: 1\nSame with python3.5 (have to restart the server each time before):\n```\n/opt/python/3.5.0/bin/python3 -c 'import pika; print(pika); c = pika.BlockingConnection(); import subprocess; subprocess.check_call([\"sudo\", \"rabbitmqctl\", \"stop\"]); c.channel()'\n\nStopping and halting node rabbit@hal ...\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/mnt/DATA/greg/work/public/python/pika/pika/adapters/blocking_connection.py\", line 698, in channel\n    channel_number=channel_number)\n  File \"/mnt/DATA/greg/work/public/python/pika/pika/connection.py\", line 733, in channel\n    return self._channels[channel_number]\nKeyError: 1\n```\n. Here it is : \ngreg@hal  $ sudo service rabbitmq-server start\n(env) \u2714 ~/work/public/python/pika [master|\u2026831]\ngreg@hal  $ python -c \"import logging ; LOG_FORMAT = ('%(levelname) -10s %(asctime)s %(name) -30s %(funcName) -35s %(lineno) -5d: %(message)s') ; logging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT); import pika; print(pika); c = pika.BlockingConnection(); import subprocess; subprocess.check_call(['sudo', 'rabbitmqctl', 'stop']); c.channel()\"\n<module 'pika' from '/mnt/DATA/greg/work/public/python/pika/pika/__init__.py'>\nDEBUG      2015-12-20 15:53:19,331 pika.adapters.select_connection _get_poller                          137 : Using EPollPoller\nDEBUG      2015-12-20 15:53:19,331 pika.callback                  add                                  164 : Added: {'one_shot': False, 'only': None, 'callback': <bound method _CallbackResult.set_value_once of <pika.adapters.blocking_connection._CallbackResult object at 0x7f6ffb834cc8>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:19,331 pika.callback                  add                                  164 : Added: {'one_shot': False, 'only': None, 'callback': <bound method _CallbackResult.set_value_once of <pika.adapters.blocking_connection._CallbackResult object at 0x7f6ffb834ec8>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:19,331 pika.callback                  add                                  164 : Added: {'one_shot': False, 'only': None, 'callback': <bound method _CallbackResult.set_value_once of <pika.adapters.blocking_connection._CallbackResult object at 0x7f6ffa0e3948>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:19,331 pika.callback                  add                                  164 : Added: {'one_shot': True, 'calls': 1, 'only': None, 'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>>, 'arguments': None}\nINFO       2015-12-20 15:53:19,333 pika.adapters.base_connection  _create_and_connect_to_socket        212 : Connecting to 127.0.0.1:5672\nDEBUG      2015-12-20 15:53:19,334 pika.callback                  process                              220 : Processing 0:Connection.Start\nDEBUG      2015-12-20 15:53:19,334 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-20 15:53:19,334 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-20 15:53:19,335 pika.callback                  remove                               269 : Removing callback #0: {'one_shot': True, 'calls': 0, 'only': None, 'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:19,335 pika.callback                  process                              234 : Calling <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>> for \"0:Connection.Start\"\nDEBUG      2015-12-20 15:53:19,335 pika.callback                  add                                  164 : Added: {'one_shot': True, 'calls': 1, 'only': None, 'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:19,335 pika.callback                  process                              220 : Processing 0:Connection.Tune\nDEBUG      2015-12-20 15:53:19,335 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-20 15:53:19,335 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-20 15:53:19,335 pika.callback                  remove                               269 : Removing callback #0: {'one_shot': True, 'calls': 0, 'only': None, 'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:19,335 pika.callback                  process                              234 : Calling <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>> for \"0:Connection.Tune\"\nDEBUG      2015-12-20 15:53:19,336 pika.connection                _create_heartbeat_checker            1025: Creating a HeartbeatChecker: 580\nDEBUG      2015-12-20 15:53:19,336 pika.callback                  add                                  164 : Added: {'one_shot': True, 'calls': 1, 'only': None, 'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:19,336 pika.callback                  process                              220 : Processing 0:Connection.OpenOk\nDEBUG      2015-12-20 15:53:19,336 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-20 15:53:19,336 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-20 15:53:19,336 pika.callback                  remove                               269 : Removing callback #0: {'one_shot': True, 'calls': 0, 'only': None, 'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:19,336 pika.callback                  process                              234 : Calling <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>> for \"0:Connection.OpenOk\"\nDEBUG      2015-12-20 15:53:19,336 pika.callback                  add                                  164 : Added: {'one_shot': True, 'calls': 1, 'only': None, 'callback': <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:19,337 pika.callback                  process                              220 : Processing 0:_on_connection_open\nDEBUG      2015-12-20 15:53:19,337 pika.callback                  process                              234 : Calling <bound method _CallbackResult.set_value_once of <pika.adapters.blocking_connection._CallbackResult object at 0x7f6ffb834ec8>> for \"0:_on_connection_open\"\nStopping and halting node rabbit@hal ...\nDEBUG      2015-12-20 15:53:20,703 pika.connection                _create_channel                      1013: Creating channel 1\nDEBUG      2015-12-20 15:53:20,704 pika.callback                  add                                  164 : Added: {'one_shot': True, 'calls': 1, 'only': <pika.channel.Channel object at 0x7f6ff9c1fd30>, 'callback': <bound method SelectConnection._on_channel_cleanup of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:20,704 pika.callback                  add                                  164 : Added: {'one_shot': False, 'only': None, 'callback': <bound method Channel._on_getempty of <pika.channel.Channel object at 0x7f6ff9c1fd30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:20,704 pika.callback                  add                                  164 : Added: {'one_shot': False, 'only': None, 'callback': <bound method Channel._on_cancel of <pika.channel.Channel object at 0x7f6ff9c1fd30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:20,704 pika.callback                  add                                  164 : Added: {'one_shot': False, 'only': None, 'callback': <bound method Channel._on_flow of <pika.channel.Channel object at 0x7f6ff9c1fd30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:20,705 pika.callback                  add                                  164 : Added: {'one_shot': True, 'calls': 1, 'only': None, 'callback': <bound method Channel._on_close of <pika.channel.Channel object at 0x7f6ff9c1fd30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:20,705 pika.channel                   _rpc                                 1130: Adding in on_synchronous_complete callback\nDEBUG      2015-12-20 15:53:20,705 pika.callback                  add                                  164 : Added: {'one_shot': True, 'calls': 1, 'only': None, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x7f6ff9c1fd30>>, 'arguments': None}\nDEBUG      2015-12-20 15:53:20,705 pika.channel                   _rpc                                 1135: Adding passed in callback\nDEBUG      2015-12-20 15:53:20,706 pika.callback                  add                                  164 : Added: {'one_shot': True, 'calls': 1, 'only': None, 'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x7f6ff9c1fd30>>, 'arguments': None}\nERROR      2015-12-20 15:53:20,706 pika.adapters.base_connection  _handle_error                        344 : Socket Error: 104\nDEBUG      2015-12-20 15:53:20,706 pika.heartbeat                 stop                                 108 : Removing timeout for next heartbeat interval\nWARNING    2015-12-20 15:53:20,706 pika.adapters.base_connection  _check_state_on_disconnect           180 : Socket closed when connection was open\nDEBUG      2015-12-20 15:53:20,706 pika.callback                  add                                  164 : Added: {'one_shot': True, 'calls': 1, 'only': None, 'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f6ffd177d30>>, 'arguments': None}\nWARNING    2015-12-20 15:53:20,707 pika.connection                _on_disconnect                       1382: Disconnected from RabbitMQ at localhost:5672 (0): Not specified\nDEBUG      2015-12-20 15:53:20,707 pika.callback                  process                              220 : Processing 0:_on_connection_closed\nDEBUG      2015-12-20 15:53:20,707 pika.callback                  process                              234 : Calling <bound method _CallbackResult.set_value_once of <pika.adapters.blocking_connection._CallbackResult object at 0x7f6ffa0e3948>> for \"0:_on_connection_closed\"\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/mnt/DATA/greg/work/public/python/pika/pika/adapters/blocking_connection.py\", line 698, in channel\n    channel_number=channel_number)\n  File \"/mnt/DATA/greg/work/public/python/pika/pika/connection.py\", line 733, in channel\n    return self._channels[channel_number]\nKeyError: 1\n. with python2.7 : \n$ python2.7 -c \"import logging ; LOG_FORMAT = ('%(levelname) -10s %(asctime)s %(name) -30s %(funcName) -35s %(lineno) -5d: %(message)s') ; logging.basicConfig(level=logging.DEBUG, format=LOG_FORMAT); import pika; print(pika); c = pika.BlockingConnection(); import subprocess; subprocess.check_call(['sudo', 'rabbitmqctl', 'stop']); c.channel()\"\n<module 'pika' from 'pika/__init__.pyc'>\nDEBUG      2015-12-20 15:54:31,916 pika.adapters.select_connection _get_poller                          137 : Using EPollPoller\nDEBUG      2015-12-20 15:54:31,916 pika.callback                  add                                  164 : Added: {'callback': <bound method _CallbackResult.set_value_once of <pika.adapters.blocking_connection._CallbackResult object at 0x7f4cdba50f80>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-20 15:54:31,916 pika.callback                  add                                  164 : Added: {'callback': <bound method _CallbackResult.set_value_once of <pika.adapters.blocking_connection._CallbackResult object at 0x7f4cdd677098>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-20 15:54:31,916 pika.callback                  add                                  164 : Added: {'callback': <bound method _CallbackResult.set_value_once of <pika.adapters.blocking_connection._CallbackResult object at 0x7f4cdbeac320>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-20 15:54:31,917 pika.callback                  add                                  164 : Added: {'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nINFO       2015-12-20 15:54:31,917 pika.adapters.base_connection  _create_and_connect_to_socket        212 : Connecting to 127.0.0.1:5672\nDEBUG      2015-12-20 15:54:31,918 pika.callback                  process                              220 : Processing 0:Connection.Start\nDEBUG      2015-12-20 15:54:31,918 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-20 15:54:31,918 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-20 15:54:31,918 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2015-12-20 15:54:31,919 pika.callback                  process                              234 : Calling <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>> for \"0:Connection.Start\"\nDEBUG      2015-12-20 15:54:31,919 pika.callback                  add                                  164 : Added: {'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-20 15:54:31,919 pika.callback                  process                              220 : Processing 0:Connection.Tune\nDEBUG      2015-12-20 15:54:31,919 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-20 15:54:31,919 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-20 15:54:31,919 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2015-12-20 15:54:31,920 pika.callback                  process                              234 : Calling <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>> for \"0:Connection.Tune\"\nDEBUG      2015-12-20 15:54:31,920 pika.connection                _create_heartbeat_checker            1025: Creating a HeartbeatChecker: 580\nDEBUG      2015-12-20 15:54:31,920 pika.callback                  add                                  164 : Added: {'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-20 15:54:31,920 pika.callback                  process                              220 : Processing 0:Connection.OpenOk\nDEBUG      2015-12-20 15:54:31,920 pika.callback                  _use_one_shot_callback               404 : Processing use of oneshot callback\nDEBUG      2015-12-20 15:54:31,920 pika.callback                  _use_one_shot_callback               406 : 0 registered uses left\nDEBUG      2015-12-20 15:54:31,920 pika.callback                  remove                               269 : Removing callback #0: {'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG      2015-12-20 15:54:31,920 pika.callback                  process                              234 : Calling <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>> for \"0:Connection.OpenOk\"\nDEBUG      2015-12-20 15:54:31,921 pika.callback                  add                                  164 : Added: {'callback': <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-20 15:54:31,921 pika.callback                  process                              220 : Processing 0:_on_connection_open\nDEBUG      2015-12-20 15:54:31,921 pika.callback                  process                              234 : Calling <bound method _CallbackResult.set_value_once of <pika.adapters.blocking_connection._CallbackResult object at 0x7f4cdd677098>> for \"0:_on_connection_open\"\nStopping and halting node rabbit@hal ...\nDEBUG      2015-12-20 15:54:33,282 pika.connection                _create_channel                      1013: Creating channel 1\nDEBUG      2015-12-20 15:54:33,283 pika.callback                  add                                  164 : Added: {'callback': <bound method SelectConnection._on_channel_cleanup of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>>, 'only': <pika.channel.Channel object at 0x7f4cdd742750>, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-20 15:54:33,283 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_getempty of <pika.channel.Channel object at 0x7f4cdd742750>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-20 15:54:33,284 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_cancel of <pika.channel.Channel object at 0x7f4cdd742750>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-20 15:54:33,284 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_flow of <pika.channel.Channel object at 0x7f4cdd742750>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG      2015-12-20 15:54:33,284 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_close of <pika.channel.Channel object at 0x7f4cdd742750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-20 15:54:33,285 pika.channel                   _rpc                                 1130: Adding in on_synchronous_complete callback\nDEBUG      2015-12-20 15:54:33,285 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x7f4cdd742750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG      2015-12-20 15:54:33,285 pika.channel                   _rpc                                 1135: Adding passed in callback\nDEBUG      2015-12-20 15:54:33,285 pika.callback                  add                                  164 : Added: {'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x7f4cdd742750>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nERROR      2015-12-20 15:54:33,285 pika.adapters.base_connection  _handle_error                        344 : Socket Error: 104\nDEBUG      2015-12-20 15:54:33,285 pika.heartbeat                 stop                                 108 : Removing timeout for next heartbeat interval\nWARNING    2015-12-20 15:54:33,286 pika.adapters.base_connection  _check_state_on_disconnect           180 : Socket closed when connection was open\nDEBUG      2015-12-20 15:54:33,286 pika.callback                  add                                  164 : Added: {'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x7f4cdbeaabd0>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nWARNING    2015-12-20 15:54:33,286 pika.connection                _on_disconnect                       1382: Disconnected from RabbitMQ at localhost:5672 (0): Not specified\nDEBUG      2015-12-20 15:54:33,286 pika.callback                  process                              220 : Processing 0:_on_connection_closed\nDEBUG      2015-12-20 15:54:33,286 pika.callback                  process                              234 : Calling <bound method _CallbackResult.set_value_once of <pika.adapters.blocking_connection._CallbackResult object at 0x7f4cdbeac320>> for \"0:_on_connection_closed\"\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"pika/adapters/blocking_connection.py\", line 698, in channel\n    channel_number=channel_number)\n  File \"pika/connection.py\", line 733, in channel\n    return self._channels[channel_number]\nKeyError: 1\n. > I do wish that we had a test that could reproduce the issue in pika test suite.\nthere I added one. I tried to use mock to trigger the send() to raise socket.error but couldn't succeed.. after 2 minutes I got this self-made solution.. don't know if it's ok to you.\nI also added comment within the channel method to let the reader know..\n. by the way my test function was even not testing the actual exception that I receive in real situation.. the actual error received (in SelectConnection._handle_write()) is ECONNRESET (104) .. which makes this code path, in _handle_error(), to actually be covered:\npython\n        else:\n            # Haven't run into this one yet, log it.\n            LOGGER.error(\"Socket Error: %s\", error_code)\nI'll review all that..  though now I'm unsure the python-mocket lib would make my life easier here.. have to give it a try..\n. I have a test case always reproducing the problem, it's not using a real server but a simple threaded tcpserver (launched from within the test) which upon connection from the client I then write & read the expected packets to the client (amqp hanshake and so on), syncing between the client and server with thread events (so to be sure to have the right sequence of actions ongoing).. \nif you will I clean & submit it  ?\nI do think the same could be used to reproduce quite a lot of others problems of the same kind..\n. > I have a test case always reproducing the problem\nI thought it was always but apparently not.. it depends on I don't know (sometimes the write succeed (after the connection was closed :s) and I get read to return 0 data, sometimes I get write fails with ECONNRESET, etc..).. by also using a proxy object over the client socket and forcing the right exception for the send() implied in channel() creation now it's so really always reproducing. \n. there it is.\n. @vitaly-krugl  :  OK I, more or less, get what you've discovered / are explaining. \nGonna close this PR so, no problem.\nand giving the #679 a review.. (but it will be more a diagonal one, the PR not being a little one ;)\n. rebased for #683 merge conflict now resolved.\n. there, not 200% sure about how I fixed the merge conflict.. can you double-check it ? @vitaly-krugl \n. here we go\n. having a look into that..\n. can't reproduce directly on my side..\n. the only error I can trigger (in python2.6 but I think it's the same with others versions), is this one : \n```\nTest that poll() is properly restarted after receiving EINTR error. ... FAIL\n======================================================================\nFAIL: Test that poll() is properly restarted after receiving EINTR error.\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/mock.py\", line 1201, in patched\n    return func(args, *keywargs)\n  File \"/home/gstarck/work/public/python/pika/tests/unit/select_connection_ioloop_tests.py\", line 396, in test_eintr\n    self.assertEqual(is_resumable_mock.call_count, 1)\nAssertionError: 0 != 1\n-------------------- >> begin captured logging << --------------------\npika.adapters.select_connection: DEBUG: Using SelectPoller\npika.adapters.select_connection: DEBUG: Using SelectPoller\npika.adapters.select_connection: DEBUG: Starting IOLoop\npika.adapters.select_connection: DEBUG: Stopping IOLoop\n--------------------- >> end captured logging << ---------------------\n```\nI can trigger it, \"simply\" by executing many instances of the test at the same time, with something like this : \n$ for i in $(seq 100) ; do ( nosetests -x tests/acceptance/blocking_adapter_test.py:TestUnroutableMessagesReturnedInNonPubackMode  &>/tmp/res$i || echo res$i failed) & done\nthis also triggers quite a lot of Timed out errors which is expected given the load generated by the execution of so many tests in // ..\n. strange, when you give a look at the debug output : \nroot: DEBUG: ZZZ self.connection.callbacks.process.call_args_list: [call(0, '_on_connection_error', <pika.connection.Connection object at 0x2f92dd0>, <pika.connection.Connection object at 0x2f92dd0>, Client was disconnected at a connection stage indicating a probable denial of access to the specified virtual host: (1, 'error text')),\nyou see the mock has well been called with the, as far as i see, expected arguments.. \nstrange..\n. For refs/archive the actual error:\n```\nFAIL: on_disconnect invokes ON_CONNECTION_ERROR with ProbableAccessDeniedError and ON_CONNECTION_CLOSED callbacks\nTraceback (most recent call last):\n  File \"/home/travis/build/pika/pika/tests/unit/connection_tests.py\", line 228, in test_on_disconnect_invokes_access_on_connection_error_and_closed\n    mock.ANY)\n  File \"/home/travis/virtualenv/python2.6.9/lib/python2.6/site-packages/mock/mock.py\", line 999, in assert_any_call\n    ), cause)\n  File \"/home/travis/virtualenv/python2.6.9/lib/python2.6/site-packages/six.py\", line 718, in raise_from\n    raise value\nAssertionError: mock(0, '_on_connection_error', , , ) call not found\n-------------------- >> begin captured logging << --------------------\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.callback: DEBUG: Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\npika.connection: WARNING: Disconnected from RabbitMQ at localhost:5672 from_adapter=True (1): error text\npika.connection: ERROR: Socket closed while tuning the connection indicating a probable permission error when accessing a virtual host\npika.connection: ERROR: Connection setup failed due to Client was disconnected at a connection stage indicating a probable denial of access to the specified virtual host: (1, 'error text')\npika.callback: DEBUG: Incremented callback reference counter: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\npika.callback: DEBUG: Incremented callback reference counter: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\nroot: DEBUG: ZZZ self.connection.callbacks.process.call_args_list: [call(0, '_on_connection_error', , , Client was disconnected at a connection stage indicating a probable denial of access to the specified virtual host: (1, 'error text')),\n call(0, '_on_connection_closed', , , 1, 'error text')]\n--------------------- >> end captured logging << ---------------------\n```\ndoes it reproduce on travis ?\n. could be due to mock version.. \n. @vitaly-krugl \nI'd like to know the mock version used by travis, how can I know ?\nin fact: it would be good if before the execution of the tests there was a \"pip freeze\" executed, so that we can know all versions which are in use by travis..\n. @vitaly-krugl : good to know about the reproduce :)\nand yes for having pip freeze output it's effectively within .travis.yml ; could be put after the - pip install -r test-requirements.txt in install section..\n. here it is.. https://github.com/pika/pika/pull/689\n. effectively ;) the next comment (not far away) resumes this one..\n. yeah, that was the easier solution I found actually.\nI agree with most of your comments/remarks but otherwise I would have to handle a full socket(.socket) mock which was harder to me. I just found a lib to do that, giving it a look right now.. (https://github.com/mocketize/python-mocket).. would it makes the job so here ?\n. I agree it's not good ;) a simple tcpserver would do the trick..\n. non-POSIX ones I guess ?\nI made that because I'm not sure if it's available on every possible platform out there.. but if you think it's not necessary to test for its presence then I can unconditionnaly use it, no problem..\n. > but if you think it's not necessary to test for its presence then I can unconditionnaly use it, no problem..\nthat would remove the need (though, I do think it's better to use it, even always) of the Proxy() \"mock\" needed to be sure to trigger exactly the expected error/exception..\n. I forgot it, will remove it asap\n. this (_metaclass definition/assignment in class body) is py2 only\n. EDIT: if you put it in a module on its own, say pika.compat._py3_only(.py), and doing the import here that would solve the problem without requiring the exec.. I think/am mostly sure (already done in the past)\n. heuh yes.. I had done this #678 totally based on your input if I remember correctly ;)\n. ",
    "morden2k": "Sorry I do not understand you. You just write what I write. What do you mean on \"add the socket exception message in the reply_text variable in the callback though.\"? As I said reply_text is 'Not specified' not \u201cSocket closed when connection was open' and reply_code=0. Yes, I know that on_close_callback uses for process close codes from RabbitMQ level, not socket. But I did not find way how to retrive any error information about closing with any errors so I \"supposed\" that \"on close\" is for all close situations: I will have error message/code in any case in onclose, but NO, it is - 'Not specified' and no any other onclose callbacks.\nFor example, in WinAPI we have function GetLastError which allow to get last error code in last call. And it is not depends in which level error occurred. It just return last error and allow to know what is going on on call. E.g. when you open sharing file this is can be FS/HDD level problem, this is can be CIFS level problem and this is can be socket level problem since you uses network for open file. In any case you will have error information.\nBut how I see (as I said may be I missed something) in pika it is not possible to know reason what is going on unexpected close with error. No any errors, no any exceptions :-( I just can detect error using my additional internal variable like \u201cthis is manual close\u201d (if false) and set is true if this is was manual/expected close. But this is do not resolve problem \u2013 I do not have reason what is wrong. What I found that is you just display warning \"WARNING:pika.adapters.base_connection:Socket closed when connection was open\". But hooking and analyze logger this is idiocy. This is only one way to know error which I know on current moment.\nDo you really not return or not save in variable any error information in library (in case if this is not RabitMQ level)? If no will and when this possible in future? Could you propose workaround for retrieve error info (I do not specialist in python, I just uses it for make some control/testing scripts for my services)?\nIf you do not want to have/process \"error codes\" in your lib, some function like get_last_error_string or raise exception with error string, or some callback with error string (I mean ANY way for know error information) will enough. Or you can also add special internal code (not RabbitMQ) for on_close_callback which calls with 0 'Not specified' in curent moment. But please return this info to outside application which uses your lib. Which way for this (callback or last error string or  last error code or raise exception in ioloop.start() or  ...) - it is not important.\n. P.S. IMHO 'Socket closed when connection was open is not correct message (not gracefully message) when you unplug ethernet or have crash of app on outside endpoint. How I remember must be like \"Connection reset by peer\" or \"Connection reset by server\" or something this (do not remember exactly strings in linux/windows socket errors) which usualy uses in sockets on this case (when TCP RST packet received without FIN-ACK). You can retrieve this string and error code from socket library. I did not work with sockets in python but I'm sure that python socket library allow to retrive error information and error codes (more than confident, since these errors processes in OS level TCP/IP driver/lib, not python adapter library for sockets).\n. ",
    "KartikKannapur": "Thanks @gmr Will check on that.\n. ",
    "jackeisenbach": "Hi gmr, \nCould you please specify steps on how to start up the RabbitMQ server. It's not in the tutorial. Thanks!\n. ",
    "RONNCC": "@KartikKannapur @gmr \nCan you elaborate on how to check from the pika side if a connection is ok?\ne.g.. I have \nif self.connection.is_closed:\n        #reconnect\n        self.connection = pika.BlockingConnection(connectionParams)\n        self.channel = self.connection.channel()\nbefore my send (a basic publish), and that doesn't seem to be enough as I still get ConnectionClosed errors sporadically\n. ",
    "antime": "When I use haproxy to connection the rabbitmq ,I got the same error after running script. But without haproxy ,It is run well.\n. ",
    "fannyhub": "@antime and for anyone who has this problem - make sure you have your rabbitmq-server installed correctly. I have noticed I had some problem with my installation and was getting the problem mentioned above\n. ",
    "devpmn": "I am getting the same error.\nI have my RabbitMQ Server running on AWS EC2\nI am getting this same error on this line:\nconnection = pika.BlockingConnection(pika.ConnectionParameters('xx.xx.xx.xx',5672,'/',credentials))where xx.xx.xx.xx : public ip address of my instance\nPls tell me,  if I am using the correct parameters. What should be the IP address , virtual hostname.\nI have checked  the credentials , the user that I am using exists and it has the rights to access '/' virtual host. ",
    "mivade": "@vitaly-krugl: At least on the legacy Python builds I have tried, the print function Just Works:\n```\nPython 2.7.9 (default, Mar  1 2015, 12:57:24) \n[GCC 4.9.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nprint('test')\ntest\n```\n\n\n\n```\nPython 2.6.9 |Continuum Analytics, Inc.| (unknown, Aug 21 2014, 18:28:52) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nprint('test')\ntest\n```\n\n\n\nThis is despite the documentation seemingly saying that you must add from __future__ import print_statement. Anecdotally, I seem to recall in earlier builds of 2.7 explicitly having to use this, but I don't remember the details of what specific version it was.\n. Thanks. Fixed.\n. ",
    "replay": "Thanks for all the thoughts, this makes much more sense now.\nWhat we are trying to achieve is very similar to the RPC Client in the example in the RabbitMQ documentation (https://www.rabbitmq.com/tutorials/tutorial-six-python.html). The main difference is that we want to let the user choose whether he wants to use synchronous or asynchronous interface, in the case of the synchronous interface I'd need to use .consume() instead of relying on the callback while continuously calling process_data_events() like they do in the example. \nSo that's where the problem happens, the RPC Client publishes a job into the exchange, then calls process_data_events() once, and then uses .consume() to wait for the reply.\nMy plan was to just raise an exception in on_blocked_connection_callback so the user of the RPC Client can deal with it.\nI understand your arguments that a user could potentially end up in an endless recursion when he does certain things inside the callback. Do you think it would be a possible solution for the BlockingConnection adapter to setup it's own internal callback for the ConnectionBlocked and ConnectionUnblocked events, and then in this internal callback it just sets some flag like self.blocked = True. Then the chan.consume() method could raise some exception to the user saying that this connection got blocked, which would be consistent with the expectation of a synchronous interface. \nI'll test the confirm_delivery() and then report back here.\n. @vitaly-krugl you were right, chan.confirm_delivery() hangs exactly the same way as chan.consume() does because it's waiting for the OK message.\n. All your points make sense, especially that it's important to take care that we do not modify the interface and raise unexpected Exceptions.\nAs a near term solution, which would require documentation explaining the restrictions, what do you think about this? I also added a test which fails without the patch due to time out, but succeeds with the patch. This does not include anything like returning promises from the API yet:\nhttps://github.com/pika/pika/compare/master...NetAccessCorp:first_try\n. @vitaly-krugl I have pushed an updated and improved version: https://github.com/pika/pika/compare/master...NetAccessCorp:blocked_event_callback\nCurrently I'm still not satisfied with the way the Timeouts work in my branch, because in the case of the timeouts it's still an issue to resume to the waiting state after a Timeout exception has been raised, so please just ignore this part for now. In fact i'm not sure if it makes sense to use exceptions for timeouts at all, because their case is quite different and will always require reentry at the point where the exception has been raised.\nRegarding your two points to consider:\n1. With this patch, the state of connections and channels gets cleaned up on ConnectionBlockedException, so the channel and connections are not left in an inconsistent state. Since RabbitMQ simply ignores frames that have been sent on a blocked connection it should be safe to just clean the state up on client side. \n   The limitations are similar as for the add_timeout method where the docs say:\n   NOTE: the timer callbacks are dispatched only in the scope of specially-designated methods: see BlockingConnection.process_data_events and BlockingChannel.start_consuming.\n2. In the case of basic_publish the only way to be sure that the msg has been delivered is by turning confirm_delivery on, so that's the same as it is now. In the case of basic_get RabbitMQ won't send another msg if the connection is blocked, so it's not destructive. This should also make the promises unnecessary.\nThis is some test code:\n```\nimport pika\nimport random\nconnection_parameters = pika.ConnectionParameters(\n        host='127.0.0.1',\n        port=5672,\n        credentials=pika.PlainCredentials('guest', 'guest'),\n)\nconn = pika.BlockingConnection(connection_parameters, exception_on_event=True)\nchan = conn.channel()\nchan.confirm_delivery()\nwhile True:\n    try:\n        print(\"publishing\")\n        chan.basic_publish(\n            exchange='test_exchange',\n            routing_key='key',\n            body=str(random.random()),\n        )\n        conn.sleep(1)\n        chan.basic_get(\n            queue='test_queue'\n        )\n    except pika.exceptions.ConnectionBlockedEvent:\n        print(\"blocked\")\n        while True:\n            try:\n                print(\"sleeping\")\n                conn.sleep(3)\n            except pika.exceptions.ConnectionUnblockedEvent:\n                print(\"unblocked\")\n                break\n```\nWhen I Block/Unblock this connection, while the script is running, I see that it correctly detects and handles the events.\nI'd be happy to hear your thoughts on this.\n. ",
    "krishna-kashyap": "+1\n. If there is a interface, set of methods to be implemented in a adapter so as to be able to plug it in Pika, I can try writing the adapter for asyncio. From source code it wasn't all that evident (I compared multiple of the adapters to see if there are common methods which are being implemented)\n. ",
    "leifurhauks": "Are there concrete plans to be adding support for asyncio, or is it currently just an an item on the wishlist?\n. Would it be mostly just a matter of implementing those methods from pika.connection.Connection that raise NotImplementedError?\nFor example, for add_timeout(self, deadline, callback_method), it looks to me like it would just need to pass the parameters to the call_later method on the asyncio event loop.\nLike @krishna-kashyap , I might be able to contribute if someone is prepared to review PRs for this.\n. ",
    "zloidemon": "Hey, when are you planing to release it?\n. ",
    "thedodd": "+1 this would be super awesome!\n. ",
    "appetito": "Hi!\nHere is preview version of asyncio adapter - just clone of Twisted adapter\nhttps://github.com/appetito/pika\nIt is not tested well but seems working, you cat try.\n. This adapter is for python3 only - all classes are modern-style)\n. This adapter use different approach then  SelectConnection or TornadoConnection, so i suppose existing acceptance testing logic may not fit to him\n. ",
    "povilasb": "+1 @appetito what about a pull request? :)\n. ",
    "mosquito": "Hi there! I made PR #768 now it works.\n. Actually you should try to use aio-pika. This is not a fork. This module extends pika interface and using futures instead callbaks. . > Hi Mosquito, thank you for submitting this PR. I took a brief look, and a few things jumped out right away that are lacking:\n\n\nDocumentation (helpful docstrings)\nUsage examples\nTests\n\n\nDone \ud83d\ude0a\n. @zyp thank you for you changes. The @decaz add your changes to the aio-pika. So @decaz already add you as a contributor to the \"thank's to\" section \ud83d\ude00. Hi @michaelklishin could you please upload a new version to the pypi?. @jbfondo can you provide simple script for reproduce this bug. If you could do it,\nit should be exclude possible bugs in aio-pika. Pure-implementation (without any wrapper) should be better.. @decaz I have no any tests which might reproduce it, but when version 0.10 has been frozen the problem was disappear.. ",
    "vbogretsov": "+1\n. ",
    "kotyara1005": "+1. ",
    "pquentin": "For reference, PR #805 improves #768.. For what it's worth, the tests pass on OS X on my laptop, and Travis is only red because it can't setup the OS X environment correctly. Thanks @zyp! We're getting closer with each new pull request. :)\n(The coverage is not too bad, and if I understand correctly, twisted coverage is currently around 30%. Which is ironical because until this pull request is merged, the best alternative is twisted.). ",
    "jeremycline": "I have been interested in using Pika with Twisted's endpoints API, but I've run into trouble. The script I've tried is:\n```\nfrom pika.adapters import twisted_connection\nfrom twisted.internet import endpoints, protocol, reactor\nfrom twisted.application import service\nfrom twisted.application.internet import ClientService\namqp_endpoint = endpoints.clientFromString(reactor, 'tcp:host=127.0.0.1:port=5672:timeout=15')\namqp_factory = protocol.Factory.forProtocol(\n    twisted_connection.TwistedProtocolConnection)\namqp_service = ClientService(amqp_endpoint, amqp_factory)\napplication = service.Application('Twisted Pika Client')\nservice_collection = service.IServiceCollection(application)\namqp_service.setServiceParent(service_collection)\n```\nThe problem is, Twisted keeps reconnecting:\njcline\u00a0\ue0b0\u00a0\u24d4\u00a0\u00a0twisted\u00a0\ue0b0\u00a0~\u00a0\ue0b1\u00a0devel\u00a0\ue0b1\u00a0playpen\u00a0\ue0b0\u00a0twistd -n -y pika_twisted.py \n2016-10-30T10:54:37-0400 [-] Loading pika_twisted.py...\n2016-10-30T10:54:37-0400 [-] Loaded.\n2016-10-30T10:54:37-0400 [twisted.scripts._twistd_unix.UnixAppLogger#info] twistd 16.4.1 (/home/jcline/.virtualenvs/twisted/bin/python 2.7.12) starting up.\n2016-10-30T10:54:37-0400 [twisted.scripts._twistd_unix.UnixAppLogger#info] reactor class: twisted.internet.epollreactor.EPollReactor.\n2016-10-30T10:54:37-0400 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7fce27cbf710>\n2016-10-30T10:54:37-0400 [twisted.application.internet.ClientService#info] Scheduling retry 1 to connect <twisted.internet.endpoints.TCP4ClientEndpoint object at 0x7fce27d06150> in 1.69262447035 seconds.\n2016-10-30T10:54:37-0400 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7fce27cbf710>\n2016-10-30T10:54:39-0400 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7fce27cbf710>\n2016-10-30T10:54:39-0400 [twisted.application.internet.ClientService#info] Scheduling retry 2 to connect <twisted.internet.endpoints.TCP4ClientEndpoint object at 0x7fce27d06150> in 2.96890088608 seconds.\n2016-10-30T10:54:39-0400 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7fce27cbf710>\n^C2016-10-30T10:54:41-0400 [-] Received SIGINT, shutting down.\n2016-10-30T10:54:41-0400 [-] Main loop terminated.\n2016-10-30T10:54:41-0400 [twisted.scripts._twistd_unix.UnixAppLogger#info] Server Shut Down.\nWhen I tcpdump the conversation, I see something rather unexpected. The handshake goes fine, instantly after Twisted finishes the handshake it sends a FIN and the connection is reset. The logs aren't very informative about what went wrong, so I looked at the process with a debugger and Twisted is raising a Timeout error, even though the timeout was not hit at all.\nThis is with pika-0.10.0 and twisted-16.4.1. I'm at a bit of a loss since I don't know the Twisted internals that well, so I can't say whether this is a problem with Pika or with Twisted. @glyph does this ring any bells?\n. @glyph Using different timeout values (I tried 30 and 60) or removing the timeout entirely does not change the behaviour.\nSwitching to amqp_endpoint = endpoints.HostnameEndpoint(reactor, 'localhost', 5672) changed the log output to\n2016-10-31T14:12:01+0000 [-] Loading pika_twisted.py...\n2016-10-31T14:12:01+0000 [-] Loaded.\n2016-10-31T14:12:01+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] twistd 16.5.0 (/home/vagrant/.virtualenvs/pika/bin/python2 2.7.12) starting up.\n2016-10-31T14:12:01+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] reactor class: twisted.internet.epollreactor.EPollReactor.\n2016-10-31T14:12:01+0000 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:01+0000 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:02+0000 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:02+0000 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:02+0000 [twisted.application.internet.ClientService#info] Scheduling retry 1 to connect <twisted.internet.endpoints.HostnameEndpoint object at 0x7f4c62110450> in 2.12706868345 seconds.\n2016-10-31T14:12:04+0000 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:04+0000 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:04+0000 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:04+0000 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:05+0000 [twisted.application.internet.ClientService#info] Scheduling retry 2 to connect <twisted.internet.endpoints.HostnameEndpoint object at 0x7f4c62110450> in 2.55572051894 seconds.\n2016-10-31T14:12:07+0000 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:07+0000 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:07+0000 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:07+0000 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7f4c620baea8>\n2016-10-31T14:12:08+0000 [twisted.application.internet.ClientService#info] Scheduling retry 3 to connect <twisted.internet.endpoints.HostnameEndpoint object at 0x7f4c62110450> in 4.11712648038 seconds.\nIt turns out this is because it's trying ::1 and then 127.0.0.1, both of which are doing the handshake followed immediately by FIN.\nI set up a duplicate experiment using the mail client example[0] as a base, and that did not exhibit the same behaviour. The only way this differs from the pika example I posted is the protocol class I'm providing to forProtocol:\n```\nfrom twisted.internet import endpoints, protocol, reactor\nfrom twisted.application import service\nfrom twisted.application.internet import ClientService\nfrom twisted.mail import imap4\nemail_endpoint = endpoints.clientFromString(reactor, 'tcp:host=127.0.0.1:port=2525:timeout=15')\nemail_factory = protocol.Factory.forProtocol(imap4.IMAP4Client)\nemail_service = ClientService(email_endpoint, email_factory)\napplication = service.Application('Twisted Pika Client')\nservice_collection = service.IServiceCollection(application)\nemail_service.setServiceParent(service_collection)\n```\nThe logs from running this:\n2016-10-31T14:21:03+0000 [-] Loading email.py...\n2016-10-31T14:21:03+0000 [-] Loaded.\n2016-10-31T14:21:03+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] twistd 16.5.0 (/home/vagrant/.virtualenvs/pika/bin/python2 2.7.12) starting up.\n2016-10-31T14:21:03+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] reactor class: twisted.internet.epollreactor.EPollReactor.\n2016-10-31T14:21:03+0000 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7f49407847e8>\n^C2016-10-31T14:22:18+0000 [-] Received SIGINT, shutting down.\n2016-10-31T14:22:18+0000 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7f49407847e8>\n2016-10-31T14:22:18+0000 [-] Main loop terminated.\n2016-10-31T14:22:18+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] Server Shut Down.\nTo take Rabbit out of the equation I just started listening on a socket with nc -l. In the email example, the connection in left open and nc continues to listen, but in the pika example nc immediately exits when it closes the connection.\nI'm speculating wildly here, since I don't know much about the internals of either project (yet), but could it be that Twisted hands the established connection over to Pika to do any initial setup it needs to do and pika is somehow indicating (incorrectly) to Twisted that it couldn't do its setup, so Twisted is cleaning the TCP connection up and starting over?\n[0] http://twistedmatrix.com/trac/#imap4client\n. @glyph Fantastic, altering twisted_connection.TwistedProtocolConnection to not require parameters got this working. I'll file the issue for the error-reporting problem in Twisted. \nUnfortunately, I still can't get this to behave like I'd expect. If I try to get the active client with ClientService.whenConnected(), it works just fine the first time around, but if RabbitMQ goes down, calling this gets me the same TwistedProtocolConnection instance, which is disconnected (this is run with the patch from https://github.com/pika/pika/pull/780):\n```\nfrom pika.adapters import twisted_connection\nfrom twisted.internet import defer, endpoints, protocol, reactor\nfrom twisted.application import service\nfrom twisted.application.internet import ClientService\namqp_endpoint = endpoints.clientFromString(\n    reactor, 'tcp:host=localhost:port=5672:timeout=15')\namqp_factory = protocol.Factory.forProtocol(\n    twisted_connection.TwistedProtocolConnection)\namqp_service = ClientService(amqp_endpoint, amqp_factory)\napplication = service.Application('Twisted Pika client')\nservice_collection = service.IServiceCollection(application)\namqp_service.setServiceParent(service_collection)\n@defer.inlineCallbacks\ndef setup(amqp_service):\n    \"\"\"Returns a Deferred that fires with the queue and consumer tag when it's ready\"\"\"\n    connection = yield amqp_service.whenConnected()\n    yield connection.ready\n    channel = yield connection.channel()\n    yield channel.exchange_declare(exchange='topic_link', type='topic')\n    yield channel.queue_declare(\n        queue='hello', auto_delete=False, exclusive=False)\n    yield channel.queue_bind(\n        exchange='topic_link', queue='hello', routing_key='hello.world')\n    yield channel.basic_qos(prefetch_count=1)\n    queue_object, consumer_tag = yield channel.basic_consume(\n        queue='hello', no_ack=False)\ndefer.returnValue((queue_object, consumer_tag))\n\n@defer.inlineCallbacks\ndef read(amqp_service):\n    \"\"\"Consumes messages forever.\nThis checks for an active connection each time, so you can stop and start\nthe AMQP server and this will continue to work.\n\"\"\"\nqueue_object, consumer_tag = yield setup(amqp_service)\nwhile True:\n    # Get a single message and print it\n    try:\n        channel, method, properties, body = yield queue_object.get()\n        if body:\n            print(body)\n        yield channel.basic_ack(delivery_tag=method.delivery_tag)\n    except Exception as e:\n        print(repr(e))\n        queue_object, consumer_tag = yield setup(amqp_service)\n\nread(amqp_service)\n```\nThis script results in the following when RabbitMQ is stopped and restarted:\n2016-11-01T19:54:30+0000 [stdout#info] The channel was closed (320) CONNECTION_FORCED - broker forced connection closure with reason 'shutdown'\n2016-11-01T19:54:30+0000 [twisted.internet.defer#critical] Unhandled error in Deferred:\n2016-11-01T19:54:30+0000 [twisted.internet.defer#critical] \n        Traceback (most recent call last):\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1258, in _inlineCallbacks\n            result = result.throwExceptionIntoGenerator(g)\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/twisted/python/failure.py\", line 389, in throwExceptionIntoGenerator\n            return g.throw(self.type, self.value, self.tb)\n          File \"test.py\", line 60, in read\n            queue_object, consumer_tag = yield setup(amqp_service)\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1406, in unwindGenerator\n            return _inlineCallbacks(None, gen, Deferred())\n        --- <exception caught here> ---\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/twisted/internet/defer.py\", line 1260, in _inlineCallbacks\n            result = g.send(result)\n          File \"test.py\", line 29, in setup\n            channel = yield connection.channel()\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/pika/adapters/twisted_connection.py\", line 416, in channel\n            base_connection.BaseConnection.channel(self, d.callback, channel_number)\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/pika/connection.py\", line 710, in channel\n            self._channels[channel_number].open()\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/pika/channel.py\", line 635, in open\n            self._rpc(spec.Channel.Open(), self._on_openok, [spec.Channel.OpenOk])\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/pika/channel.py\", line 1139, in _rpc\n            self._send_method(method_frame)\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/pika/channel.py\", line 1150, in _send_method\n            self.connection._send_method(self.channel_number, method_frame, content)\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/pika/connection.py\", line 1569, in _send_method\n            self._send_frame(frame.Method(channel_number, method_frame))\n          File \"/home/vagrant/.virtualenvs/pika/lib/python2.7/site-packages/pika/connection.py\", line 1548, in _send_frame\n            raise exceptions.ConnectionClosed\n        pika.exceptions.ConnectionClosed: \n2016-11-01T19:54:30+0000 [twisted.application.internet.ClientService#info] Scheduling retry 1 to connect <twisted.internet.endpoints.TCP4ClientEndpoint object at 0x7f07a05603d0> in 2.15385893877 seconds.\n2016-11-01T19:54:30+0000 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7f07a0508b48>\n2016-11-01T19:54:32+0000 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7f07a0508b48>\n2016-11-01T19:54:32+0000 [twisted.application.internet.ClientService#info] Scheduling retry 2 to connect <twisted.internet.endpoints.TCP4ClientEndpoint object at 0x7f07a05603d0> in 3.20834667501 seconds.\n2016-11-01T19:54:32+0000 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7f07a0508b48>\n2016-11-01T19:54:35+0000 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7f07a0508b48>\n2016-11-01T19:54:35+0000 [twisted.application.internet.ClientService#info] Scheduling retry 3 to connect <twisted.internet.endpoints.TCP4ClientEndpoint object at 0x7f07a05603d0> in 3.5323085557 seconds.\n2016-11-01T19:54:35+0000 [twisted.internet.protocol.Factory#info] Stopping factory <twisted.internet.protocol.Factory instance at 0x7f07a0508b48>\n2016-11-01T19:54:39+0000 [twisted.internet.protocol.Factory#info] Starting factory <twisted.internet.protocol.Factory instance at 0x7f07a0508b48>\nBasically, Pika is upset for trying to get a channel from a connection that's closed. Am I mis-reading how whenConnected should work?\n. Ahh, I lost track of this. It looks like @lukebakken added a note on setting up x509-based authentication (thanks!). Is there anything else you'd like included here?. Hi @lukebakken,\nI took a look at the warnings for examples/twisted_service.py as requested. I should say I never use pylint myself since I disagree with it so regularly, so... take my feedback with a grain of salt, I guess:\n************* Module twisted_service\nE1101,  60:19 - Module 'twisted.application.internet' has no 'SSLClient' member (no-member)\nE1101,  66:19 - Module 'twisted.application.internet' has no 'TCPClient' member (no-member)\nW0201, 200: 8 - Attribute 'factory' defined outside __init__ (attribute-defined-outside-init)\nW0221, 204: 4 - Parameters differ from overridden 'clientConnectionLost' method (arguments-differ)\nW0201,  69: 8 - Attribute 'service' defined outside __init__ (attribute-defined-outside-init)\nR0201, 239: 4 - Method could be a function (no-self-use)\nE1111, 254: 8 - Assigning result of a function call, where the function has no return (assignment-from-no-return)\nE1121, 254:23 - Too many positional arguments for method call (too-many-function-args)\nW0201, 255: 8 - Attribute 'amqp' defined outside __init__ (attribute-defined-outside-init)\nThe first two are wrong and can be safely ignored, but to be fair to pylint Twisted is dynamically generating the classes and that's... unusual.\nW0201, 200: 8 - Attribute 'factory' defined outside __init__ (attribute-defined-outside-init)\nW0201,  69: 8 - Attribute 'service' defined outside __init__ (attribute-defined-outside-init)\nThese two can be fixed by defining \"factory\" in the PikaProtocol.__init__ method and \"service\" in the PikaFactory.__init__ method respectively. Since PikaProtocol doesn't have its own __init__ at the moment, it's tough to say whether fixing that warning is worth the additional boilerplate code. This is much more a style issue where everyone has their own opinion. \nW0221, 204: 4 - Parameters differ from overridden 'clientConnectionLost' method (arguments-differ)\nThis one, in my opinion, is silly. It's upset that the argument names differ - the parent class uses \"unused_reason\" because it doesn't use the reason, but the overridden method does and calls the argument \"reason\".\nR0201, 239: 4 - Method could be a function (no-self-use)\nYeah, the task method could be a function outside the class. Seems fine to me as it is since it's just an example, but :man_shrugging:.\nE1111, 254: 8 - Assigning result of a function call, where the function has no return (assignment-from-no-return)\nE1121, 254:23 - Too many positional arguments for method call (too-many-function-args)\nI think this is Twisted being too clever for pylint again, as the function in question returns a service and takes one argument.\nW0201, 255: 8 - Attribute 'amqp' defined outside __init__ (attribute-defined-outside-init)\nThis could be fixed by adding an __init__ to the TestService and setting amqp to None.\nI'm happy to help by fixing these and sending in a PR if you'd prefer that to fixing them here.. No problem, happy to help!. ",
    "glyph": "@jeremycline Can you create a minimal reproducer of this example?  One without using Pika, for example? :).  I don't know much about how twisted_connection works, but this shouldn't be happening; at the very least it should be logging more information about why it was disconnected.\n. @jeremycline If you use a different timeout value, does it work? Alternately; if you eliminate the timeout entirely, does that work? And finally, how about HostnameEndpoint with localhost?\n. This\u2026 might just be an error-reporting bug.\nprotocol.Factory.forProtocol, formally, takes a 0-argument callable that returns an IProtocol provider; idiomatically, that's a Protocol subclass with a constructor that takes no argument.\nWhat you're passing, TwistedChannel, appears to have a totally different interface; for starters, it takes a required argument.  That alone will make it blow up during buildProtocol.\nThis is a bug in Twisted: your logs ought to be clearly showing a traceback!  Please feel free to file this at https://twistedmatrix.com/\nHowever, the fix for you should be simple.  Write a function that takes no arguments and returns a TwistedProtocolConnection (which is a subclass of Protocol, although its constructor still requires a \"parameters\" argument) where you're passing TwistedChannel right now.\nDoes that make sense?\n. I don't know how Pika is attempting to keep track of connection state, but with simpler tests I can't reproduce this.  whenConnected seems to do roughly the right thing and reconnect.  Can you boil this down to a more minimal (hopefully no pika) reproducer?\n. Does #1069 cover the code that resolves this?. Thanks so much to @abompard for keeping the Twisted dream alive! \ud83c\udf89 . ",
    "vmarkovtsev": "This happens to me as well.\nconnection.close()\n  File \"pika/adapters/blocking_connection.py\", line 629, in close\n    self._flush_output(self._closed_result.is_ready)\n  File \"pika/adapters/blocking_connection.py\", line 410, in _flush_output\n    self._impl.ioloop.poll()\n  File \"pika/adapters/select_connection.py\", line 590, in poll\n    events = self._poll.poll(self.get_next_deadline())\nKeyboardInterrupt\n. @heg-hpo I am using a separate thread to basic_consume() and close connection from the main thread.\n. Ok, thanks for pointing this out. Is there any way to forcefully close connection from the right thread? I mean, basic_consume() blocks the thread, so there is no direct way to call close() in it...\n. Thanks for advice, but nah, I am using Django.\n. I've used gmr/rabbitpy and it appeared to be almost a drop-in replacement. It does not have this issue (it is you who run the event loop thread)\n. ",
    "dwt": "I think it quite likely that this is the same issue reported by another user: https://github.com/mher/tornado-celery/issues/56\n. @vitaly-krugl: Sorry, but I don't think I understand enough of pika / rabbitmq to deliver such a standalone app (or rather, don't have the time to investigate it this fully). I will try to provide you as much answers to your questions as I can later though.\n. To answer your questions:\n1. we've seen the problem with rabbitmq 3.5.3 and 3.1.5\n2. Our app is built using tornado and is most definitely completely single threaded. Especially where Pika is used.\n3.  I'm not entirely sure I understood your request for logging correctly. As far as I can see, the snippet you sent (that is also mentioned in your documentation) just enables general logging, nothing for pika specifically. So I'm not sure the output can help you, but here you go. Oh, and there's some logging from our celery worker too - but I removed the output from our app.\n2015-11-30 17:17:11,129 root - INFO - dispatch_async_or_local_call 9: Dispatching background worker call to callable <@task: our.background.task of our.celery:0x10430af10>([14, 200, {'user_id': 32}])\n[2015-11-30 17:17:11,132: INFO/MainProcess] Received task: our.background.task[18018ae3-f217-4e5d-b023-589b44a5702e]\n[2015-11-30 17:17:11,504: INFO/MainProcess] Task our.background.task[18018ae3-f217-4e5d-b023-589b44a5702e] succeeded in 0.371212509999s: []\n. @vitaly-krugl I'll see if we have some other code that configures logging. I configured this logging in the main() method of the tornado app, but maybe that's too late? I'll go check.\n@gmr: I think you misunderstood us earlier, the background celery worker is blocking for 4 seconds while it fetches the data. The tornado app does (as far as I know) not block at all.\n. Sorry, but I couldn't get pika logging to work, even after sinking way too much time into this. \nTo enable pika logging, I also had to enable it like this, to see any output at all: logger.getLogger('pika').setLevel(logging.DEBUG) Just the snippet you sent me doesn't do anything at all. (It does increase the log level on the root logger, I can verify that though).\nWhatever is happening, something seems to disable pika logging after some time\u00bf\u00bf\u00bf\nSome things about the environment I'm testing this in:\n- pika (0.10.0)\n- tornado (4.1)\n- tornado-celery (0.3.4)\n. @gmr: I'm pretty positive that our celery workers are indeed their own processes and are not called inline in the tornado app - or do you mean something else? Of course, all blocking is blocking, thats the exact reason we use celery to offload blocking api calls (e.g. long running SQL accesses in this case) into another process.\n. ",
    "rbu": "@vitaly-krugl, thank you for the detailed response. Your analysis is pretty much correct. Concerning some of the points:\n(2) I am not aware of the requests's TTL. Is it relevant? The request is definitely reaching the worker.\n(3) Correct. The total roundtrip from the first message reaching the rabbitmq and the worker's resonse being visible in the response queue that TornadoConnection should be waiting on is in the realm of 5-10 seconds.\n(4) The response TTL is 30 seconds.\n(5) Correct. We can see the message sitting unacknowledged for 30 seconds and then disappear.\n. ",
    "Strawhatfy": "@vitaly-krugl  @rbu @gmr @dwt I read the code of celery,there may be some problems with nowait=True(channel.queue_declare or channel.basic_cancel, not sure).\nI wrote some testing code nowait_test.txt, notice the 42th line: \nself._channel.basic_cancel(consumer_tag=consumer_tag, nowait=True)  \nqueue 4,5  on_queue_declared would not be called. But if nowait=True, it would be ok.\n. ",
    "chengchengpei": "@vitaly-krugl Linux mypc 3.16.0-30-generic #40~14.04.1-Ubuntu SMP Thu Jan 15 17:43:14 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n. @shinji-s Is this patch in 0.10.0?\n. @vitaly-krugl OK, I will test it today or tomorrow...\n. I downloaded the source code, and run \"python setup.py build; python setup.py install.\" start_consuming() can not be interupted by TERM, USR1, and USR2.\n. @vitaly-krugl I have a question. I downloaded the source code, and run \"python setup.py build; python setup.py install.\"  But I have no idea where pika is installed. and when I run my program, I am not sure which pika is used. I tried to find the new versrion of select_connnection.py, but failed in the path printed out when installing it.\n. @vitaly-krugl my output is >>> import pika\n\n\n\npika.version\n'0.10.0'\npika\n\n\n\n\n/home/myadmin/.virtualenvs/porj-dev/local/lib/python2.7/site-packages/pika/adapters/select_connection.py looks like:\nREAD = 0x0001\nWRITE = 0x0004\nERROR = 0x0008\nif pika.compat.PY2:\n    _SELECT_ERROR = select.error\nelse:\n    # select.error was deprecated and replaced by OSError in python 3.3\n    _SELECT_ERROR = OSError\nIt seems that the patch is not in my used pika....But when I installed pika, the output is:\nRemoving /home/myadmin/.virtualenvs/proj-dev/lib/python2.7/site-packages/pika-0.10.0-py2.7.egg\nCopying pika-0.10.0-py2.7.egg to /home/myadmin/.virtualenvs/proj-dev/lib/python2.7/site-packages\npika 0.10.0 is already the active version in easy-install.pth\nInstalled /home/myadmin/.virtualenvs/proj-dev/lib/python2.7/site-packages/pika-0.10.0-py2.7.egg\n. @vitaly-krugl I am new to pika. I have two questions. (1) How to check whether one virtual host exists or not? (2) How to create it? I did not find hints. In most examples, vhost already exists.. Are there any APIs to do them?\n. @vitaly-krugl pika/pika can not check or create vhost?\n. ",
    "hiqsol": "I also have run into this problem on Ubuntu 16.04 with pika 0.10.0\nAfter installing pika from master branch everyting is ok!\nSo, release new version please :)\n. ",
    "nickcash": "After looking more closely at the Java client's code that MK linked to, I'm wondering if this change goes far enough. In addition to the heartbeat, the Java client sets the channel max and frame max as the max of the client request and server response, whereas pika is using the min for those as well.\nSo maybe a better change would be to switch _combine() to use max instead? If matching the Java behavior is desired, that is. \n. I fully intended to write tests, but between holidays and workload I haven't had much time. I'll get to them ASAP!\n. @vitaly-krugl I've merged your pull request to my fork. Thanks for doing all the actual work while I was being lazy :smile: \n. @vitaly-krugl, looks good to me. Thanks again!\n. ",
    "avihoo": "Is it possible that even though the PR has merged it can't be found in current version 0.10.0?. So, any estimation on a new version to include that fix? Really needs it.. I'm using this in a production environment so pip install directly from master is probably not the greatest idea for now. Is there anyone that might know when the next version is to be released?. ",
    "anthonyserious": "I'm on 0.11.2 (via pip), but I'm still seeing the server default chosen over mine.  Has there been any progress getting this into a release?  Or am I possibly facing a different problem?\nself.conn = pika.BlockingConnection(pika.ConnectionParameters(\n        host=host,\n        port=port,\n        blocked_connection_timeout=360,\n        heartbeat=600\n    ))\nYet on the server, missed heartbeats from client, timeout: 60s.  Also passing heartbeat=0 has no effect - I would expect heartbeating to be disabled, but I still get the server-logged 60s timeout message plus a connectionClosed exception:\nFile \"/usr/local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 2077, in basic_publish\n    mandatory, immediate)\n  File \"/usr/local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 2164, in publish\n    self._flush_output()\n  File \"/usr/local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 1250, in _flush_output\n    *waiters)\n  File \"/usr/local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 474, in _flush_output\n    result.reason_text)\npika.exceptions.ConnectionClosed: (-1, 'EOF')\n. @lukebakken thanks for the quick response.  I can override the server-side heartbeat for now as a workaround (dedicated instance for 1 or 2 queues).. ",
    "Zephor5": "@gmr Thx a lot to wake me up.\nHere I have another question that I can't use one channel to get the message published by that channel.\nThe method basic_publish just flush data and return, the publisher don't care if the publish is success ?\n. these commits are not neat, close this and reopen. Thx for the information, it's ok for me. Maybe I'll get something done in the future.. ",
    "taoqf": "I use two connections, one for consuming, and the other one for publishing\nI do not reuse the channel,, so when i want to publish a message ,i create a channel, when i get the replay, i close the channel.\nBut i find the channel will close when it is not be using for some minutes.so when i need publish again, it will throw a pika.exceptions.ConnectionClosed error.\nWhat is the best way to use pika? i am new to it.\n. Should i connect and disconnect from rabbitmq each time i want publish?\n. ",
    "JetDrag": "Thanks for reply. I'll try to use Pika by ignoring some small defects not effect the main function first. On the other hand, do you have any suggestion for the AMQP client on Windows? most of them are optimizing for linux.\n. Sorry @vitaly-krugl , Outlook mobile's relay gateways are blocked in China(the fu*king GFW), so I cant receive any notification from Github till now. \nI've try the new example and it works fine. But is there any other way to  completely avoid the problem such as set the connection error event priority highest?\nBTW, i found something wrong maybe. If the server has the same existing exchange,queue,bind relationship Pika said connection is closed during those things declaring. But the new user guide says that it will skip over.\n. It seems to be my fault that I forgot to set 'durable' to the exchange which i did before.\nAbout the event priority, My thought is that adding a arg to the timeout callback to indicate whether the timer callback should delay when some reconnect event happen. If the arg is True, the timeout callback will invoked after the  reconnect event. It's very useful when I do some periodic but not time-accurate tasks.\nAnd I found a situation that when the eventloop is blocked e.g blocked on comsume, the heartbeat won't send normal even using select adapter. Is that a bug\uff1f\n. @vitaly-krugl Many Thanks to your contribution to Pika. I'm now using the similar solutions to slove the heartbeat problem that use a thread to run the tasks reveived in basic_consume and ack or nack the message according to the result the thread return. It have the same restrictions.\nIt seems that the async adapter don't async the code even just IO block in the callback?\n.  @vitaly-krugl Thanks\uff0cI'm take care of it.\n. @vitaly-krugl Hi, vitaly, I still have some questions here. Will the selection adapter support double consumer runing on separated channel? And how will it behave if events arrive at the same time?\nI'm a new coder ,so I consult you about that.\nMany Thanks\n. ",
    "bertrandhaut": "Dear,\nI'm facing the same issue as JetDrag (OSError: [Errno 9] Bad file descriptor on Windows).\nI've copy/adapt/paste the following code hoping to have automatic reconnection trial each 5 seconds.\n```\ndef init(self, amqp_url, logger):\n    \"\"\"Create a new instance of the consumer class, passing in the AMQP\n    URL used to connect to RabbitMQ.\n:param str amqp_url: The AMQP url to connect with\n\n\"\"\"\nself._connection = None\nself._channel = None\nself._closing = False\nself._consumer_tag = None\nself._url = amqp_url\nself.logger = logger\n\ndef connect(self):\n    \"\"\"This method connects to RabbitMQ, returning the connection handle.\n    When the connection is established, the on_connection_open method\n    will be invoked by pika.\n:rtype: pika.SelectConnection\n\n\"\"\"\nself.logger.info('Connecting to %s', self._url)\nreturn pika.SelectConnection(pika.URLParameters(self._url),\n                             self.on_connection_open,\n                             stop_ioloop_on_close=False)\n\ndef on_connection_open(self, unused_connection):\n    \"\"\"This method is called by pika once the connection to RabbitMQ has\n    been established. It passes the handle to the connection object in\n    case we need it, but in this case, we'll just mark it unused.\n:type unused_connection: pika.SelectConnection\n\n\"\"\"\nself.logger.info('Connection opened')\nself.add_on_connection_close_callback()\nself.open_channel()\n\ndef add_on_connection_close_callback(self):\n    \"\"\"This method adds an on close callback that will be invoked by pika\n    when RabbitMQ closes the connection to the publisher unexpectedly.\n\"\"\"\nself.logger.info('Adding connection close callback')\nself._connection.add_on_close_callback(self.on_connection_closed)\n\ndef on_connection_closed(self, connection, reply_code, reply_text):\n    \"\"\"This method is invoked by pika when the connection to RabbitMQ is\n    closed unexpectedly. Since it is unexpected, we will reconnect to\n    RabbitMQ if it disconnects.\n:param pika.connection.Connection connection: The closed connection obj\n:param int reply_code: The server provided reply_code if given\n:param str reply_text: The server provided reply_text if given\n\n\"\"\"\nself._channel = None\nif self._closing:\n    self._connection.ioloop.stop()\nelse:\n    self.logger.warning('Connection closed, reopening in 5 seconds: (%s) %s',\n                   reply_code, reply_text)\n    self._connection.add_timeout(5, self.reconnect)\n\ndef reconnect(self):\n    \"\"\"Will be invoked by the IOLoop timer if the connection is\n    closed. See the on_connection_closed method.\n\"\"\"\n# This is the old connection IOLoop instance, stop its ioloop\nself._connection.ioloop.stop()\n\nif not self._closing:\n\n    # Create a new connection\n    self._connection = self.connect()\n\n    # There is now a new connection, needs a new ioloop to run\n    self._connection.ioloop.start()\n\ndef open_channel(self):\n    \"\"\"Open a new channel with RabbitMQ by issuing the Channel.Open RPC\n    command. When RabbitMQ responds that the channel is open, the\n    on_channel_open callback will be invoked by pika.\n\"\"\"\nself.logger.info('Creating a new channel')\nself._connection.channel(on_open_callback=self.on_channel_open)\n\ndef on_channel_open(self, channel):\n    \"\"\"This method is invoked by pika when the channel has been opened.\n    The channel object is passed in so we can make use of it.\nSince the channel is now open, we'll declare the exchange to use.\n\n:param pika.channel.Channel channel: The channel object\n\n\"\"\"\nself.logger.info('Channel opened')\nself._channel = channel\n\nif True:\n    self.logger.info('Purging existing queue messages')\n    try:\n        self._channel.queue_purge(queue=self.QUEUE)\n    except pika.exceptions.ChannelClosed as e:\n        self.logger.info('nothing to delete: {} '.format(e))\n    self.logger.info('purge done')\n\nself.add_on_channel_close_callback()\nself.setup_exchange(self.EXCHANGE)\n\ndef add_on_channel_close_callback(self):\n    \"\"\"This method tells pika to call the on_channel_closed method if\n    RabbitMQ unexpectedly closes the channel.\n\"\"\"\nself.logger.info('Adding channel close callback')\nself._channel.add_on_close_callback(self.on_channel_closed)\n\ndef on_channel_closed(self, channel, reply_code, reply_text):\n    \"\"\"Invoked by pika when RabbitMQ unexpectedly closes the channel.\n    Channels are usually closed if you attempt to do something that\n    violates the protocol, such as re-declare an exchange or queue with\n    different parameters. In this case, we'll close the connection\n    to shutdown the object.\n:param pika.channel.Channel: The closed channel\n:param int reply_code: The numeric reason the channel was closed\n:param str reply_text: The text reason the channel was closed\n\n\"\"\"\nself.logger.warning('Channel %i was closed: (%s) %s',\n               channel, reply_code, reply_text)\nself._connection.close()\n\ndef setup_exchange(self, exchange_name):\n    \"\"\"Setup the exchange on RabbitMQ by invoking the Exchange.Declare RPC\n    command. When it is complete, the on_exchange_declareok method will\n    be invoked by pika.\n:param str|unicode exchange_name: The name of the exchange to declare\n\n\"\"\"\nself.logger.info('Declaring exchange %s', exchange_name)\nself._channel.exchange_declare(self.on_exchange_declareok,\n                               exchange_name,\n                               self.EXCHANGE_TYPE)\n\ndef on_exchange_declareok(self, unused_frame):\n    \"\"\"Invoked by pika when RabbitMQ has finished the Exchange.Declare RPC\n    command.\n:param pika.Frame.Method unused_frame: Exchange.DeclareOk response frame\n\n\"\"\"\nself.logger.info('Exchange declared')\nself.setup_queue(self.QUEUE)\n\ndef setup_queue(self, queue_name):\n    \"\"\"Setup the queue on RabbitMQ by invoking the Queue.Declare RPC\n    command. When it is complete, the on_queue_declareok method will\n    be invoked by pika.\n:param str|unicode queue_name: The name of the queue to declare.\n\n\"\"\"\nself.logger.info('Declaring queue %s', queue_name)\nself._channel.queue_declare(self.on_queue_declareok, queue_name)\n\ndef on_queue_declareok(self, method_frame):\n    \"\"\"Method invoked by pika when the Queue.Declare RPC call made in\n    setup_queue has completed. In this method we will bind the queue\n    and exchange together with the routing key by issuing the Queue.Bind\n    RPC command. When this command is complete, the on_bindok method will\n    be invoked by pika.\n:param pika.frame.Method method_frame: The Queue.DeclareOk frame\n\n\"\"\"\nself.logger.info('Binding %s to %s with %s',\n            self.EXCHANGE, self.QUEUE, self.ROUTING_KEY)\nself._channel.queue_bind(self.on_bindok, self.QUEUE,\n                         self.EXCHANGE, self.ROUTING_KEY)\n\ndef on_bindok(self, unused_frame):\n    \"\"\"Invoked by pika when the Queue.Bind method has completed. At this\n    point we will start consuming messages by calling start_consuming\n    which will invoke the needed RPC commands to start the process.\n:param pika.frame.Method unused_frame: The Queue.BindOk response frame\n\n\"\"\"\nself.logger.info('Queue bound')\nself.start_consuming()\n\ndef start_consuming(self):\n    \"\"\"This method sets up the consumer by first calling\n    add_on_cancel_callback so that the object is notified if RabbitMQ\n    cancels the consumer. It then issues the Basic.Consume RPC command\n    which returns the consumer tag that is used to uniquely identify the\n    consumer with RabbitMQ. We keep the value to use it when we want to\n    cancel consuming. The on_message method is passed in as a callback pika\n    will invoke when a message is fully received.\n\"\"\"\nself.logger.info('Issuing consumer related RPC commands')\nself.add_on_cancel_callback()\nself.logger.info('Starting basic_consume')\nself._consumer_tag = self._channel.basic_consume(self.on_message,\n                                                 self.QUEUE)\n\ndef add_on_cancel_callback(self):\n    \"\"\"Add a callback that will be invoked if RabbitMQ cancels the consumer\n    for some reason. If RabbitMQ does cancel the consumer,\n    on_consumer_cancelled will be invoked by pika.\n\"\"\"\nself.logger.info('Adding consumer cancellation callback')\nself._channel.add_on_cancel_callback(self.on_consumer_cancelled)\n\ndef on_consumer_cancelled(self, method_frame):\n    \"\"\"Invoked by pika when RabbitMQ sends a Basic.Cancel for a consumer\n    receiving messages.\n:param pika.frame.Method method_frame: The Basic.Cancel frame\n\n\"\"\"\nself.logger.info('Consumer was cancelled remotely, shutting down: %r',\n            method_frame)\nif self._channel:\n    self._channel.close()\n\ndef on_message(self, ch, basic_deliver, properties, body):\n    \"\"\"Invoked by pika when a message is delivered from RabbitMQ. The\n    channel is passed for your convenience. The basic_deliver object that\n    is passed in carries the exchange, routing key, delivery tag and\n    a redelivered flag for the message. The properties passed in is an\n    instance of BasicProperties with the message properties and the body\n    is the message that was sent.\n:param pika.channel.Channel ch: The channel object\n:param pika.Spec.Basic.Deliver: basic_deliver method\n:param pika.Spec.BasicProperties: properties\n:param str|unicode body: The message body\n\n\"\"\"\nself.logger.info('Received message # %s from %s',\n            basic_deliver.delivery_tag, properties.app_id)\n\n# do complex things with data\n# to get back results\n\n# send back results\nch.basic_publish(exchange='',\n                 routing_key=properties.reply_to,\n                 properties=pika.BasicProperties(correlation_id= \\\n                                                     properties.correlation_id),\n                 body=results)\n\nself.acknowledge_message(basic_deliver.delivery_tag)\n\ndef acknowledge_message(self, delivery_tag):\n    \"\"\"Acknowledge the message delivery from RabbitMQ by sending a\n    Basic.Ack RPC method for the delivery tag.\n:param int delivery_tag: The delivery tag from the Basic.Deliver frame\n\n\"\"\"\nself.logger.info('Acknowledging message %s', delivery_tag)\nself._channel.basic_ack(delivery_tag)\n\ndef stop_consuming(self):\n    \"\"\"Tell RabbitMQ that you would like to stop consuming by sending the\n    Basic.Cancel RPC command.\n\"\"\"\nif self._channel:\n    self.logger.info('Sending a Basic.Cancel RPC command to RabbitMQ')\n    self._channel.basic_cancel(self.on_cancelok, self._consumer_tag)\n\ndef on_cancelok(self, unused_frame):\n    \"\"\"This method is invoked by pika when RabbitMQ acknowledges the\n    cancellation of a consumer. At this point we will close the channel.\n    This will invoke the on_channel_closed method once the channel has been\n    closed, which will in-turn close the connection.\n:param pika.frame.Method unused_frame: The Basic.CancelOk frame\n\n\"\"\"\nself.logger.info('RabbitMQ acknowledged the cancellation of the consumer')\nself.close_channel()\n\ndef close_channel(self):\n    \"\"\"Call to close the channel with RabbitMQ cleanly by issuing the\n    Channel.Close RPC command.\n\"\"\"\nself.logger.info('Closing the channel')\nself._channel.close()\n\ndef run(self):\n    \"\"\"Run the example consumer by connecting to RabbitMQ and then\n    starting the IOLoop to block and allow the SelectConnection to operate.\n\"\"\"\nself._connection = self.connect()\nself._connection.ioloop.start()\n\ndef stop(self):\n    \"\"\"Cleanly shutdown the connection to RabbitMQ by stopping the consumer\n    with RabbitMQ. When RabbitMQ confirms the cancellation, on_cancelok\n    will be invoked by pika, which will then closing the channel and\n    connection. The IOLoop is started again because this method is invoked\n    when CTRL-C is pressed raising a KeyboardInterrupt exception. This\n    exception stops the IOLoop which needs to be running for pika to\n    communicate with RabbitMQ. All of the commands issued prior to starting\n    the IOLoop will be buffered but not processed.\n\"\"\"\nself.logger.info('Stopping')\nself._closing = True\nself.stop_consuming()\nself._connection.ioloop.start()\nself.logger.info('Stopped')\n\ndef close_connection(self):\n    \"\"\"This method closes the connection to RabbitMQ.\"\"\"\n    self.logger.info('Closing connection')\n    self._connection.close()\n```\nWhen I face a network loss, I get the following messages: \n2016-04-26 08:41:16,963 - pika.adapters.select_connection - DEBUG - Using SelectPoller\n2016-04-26 08:41:16,963 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': False, 'arguments': None, 'callback': <bound method SelectConnection._on_connection_error of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>>}\n2016-04-26 08:41:16,963 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': False, 'arguments': None, 'callback': <bound method SmartWaterAsyncConsumer.on_connection_open of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>>}\n2016-04-26 08:41:16,963 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>>, 'calls': 1}\n2016-04-26 08:41:16,983 - pika.adapters.base_connection - INFO - Connecting to 146.148.23.73:5672\n2016-04-26 08:41:17,003 - pika.adapters.select_connection - DEBUG - Starting IOLoop\n2016-04-26 08:41:17,003 - pika.adapters.select_connection - DEBUG - Using custom socketpair for interrupt\n2016-04-26 08:41:17,026 - pika.callback - DEBUG - Processing 0:Connection.Start\n2016-04-26 08:41:17,026 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,026 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,026 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>>, 'calls': 0}\n2016-04-26 08:41:17,026 - pika.callback - DEBUG - Calling <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>> for \"0:Connection.Start\"\n2016-04-26 08:41:17,026 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>>, 'calls': 1}\n2016-04-26 08:41:17,047 - pika.callback - DEBUG - Processing 0:Connection.Tune\n2016-04-26 08:41:17,047 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,047 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,047 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>>, 'calls': 0}\n2016-04-26 08:41:17,047 - pika.callback - DEBUG - Calling <bound method SelectConnection._on_connection_tune of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>> for \"0:Connection.Tune\"\n2016-04-26 08:41:17,047 - pika.connection - DEBUG - Creating a HeartbeatChecker: 580\n2016-04-26 08:41:17,047 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>>, 'calls': 1}\n2016-04-26 08:41:17,071 - pika.callback - DEBUG - Processing 0:Connection.OpenOk\n2016-04-26 08:41:17,071 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,071 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,071 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>>, 'calls': 0}\n2016-04-26 08:41:17,072 - pika.callback - DEBUG - Calling <bound method SelectConnection._on_connection_open of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>> for \"0:Connection.OpenOk\"\n2016-04-26 08:41:17,072 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SelectConnection._on_connection_closed of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>>, 'calls': 1}\n2016-04-26 08:41:17,072 - pika.callback - DEBUG - Processing 0:_on_connection_open\n2016-04-26 08:41:17,072 - pika.callback - DEBUG - Calling <bound method SmartWaterAsyncConsumer.on_connection_open of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>> for \"0:_on_connection_open\"\n2016-04-26 08:41:17,072 - root - INFO - Connection opened\n2016-04-26 08:41:17,072 - root - INFO - Adding connection close callback\n2016-04-26 08:41:17,072 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': False, 'arguments': None, 'callback': <bound method SmartWaterAsyncConsumer.on_connection_closed of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>>}\n2016-04-26 08:41:17,072 - root - INFO - Creating a new channel\n2016-04-26 08:41:17,072 - pika.connection - DEBUG - Creating channel 1\n2016-04-26 08:41:17,072 - pika.callback - DEBUG - Added: {'only': <pika.channel.Channel object at 0x0000000006DCB2E8>, 'one_shot': True, 'arguments': None, 'callback': <bound method SelectConnection._on_channel_cleanup of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>>, 'calls': 1}\n2016-04-26 08:41:17,072 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': False, 'arguments': None, 'callback': <bound method Channel._on_getempty of <pika.channel.Channel object at 0x0000000006DCB2E8>>}\n2016-04-26 08:41:17,072 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': False, 'arguments': None, 'callback': <bound method Channel._on_cancel of <pika.channel.Channel object at 0x0000000006DCB2E8>>}\n2016-04-26 08:41:17,073 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': False, 'arguments': None, 'callback': <bound method Channel._on_flow of <pika.channel.Channel object at 0x0000000006DCB2E8>>}\n2016-04-26 08:41:17,073 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_close of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 1}\n2016-04-26 08:41:17,073 - pika.channel - DEBUG - Adding in on_synchronous_complete callback\n2016-04-26 08:41:17,073 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 1}\n2016-04-26 08:41:17,073 - pika.channel - DEBUG - Adding passed in callback\n2016-04-26 08:41:17,073 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 1}\n2016-04-26 08:41:17,091 - pika.callback - DEBUG - Processing 1:Channel.OpenOk\n2016-04-26 08:41:17,091 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,091 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,091 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 0}\n2016-04-26 08:41:17,092 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,092 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,092 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_openok of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 0}\n2016-04-26 08:41:17,092 - pika.callback - DEBUG - Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>> for \"1:Channel.OpenOk\"\n2016-04-26 08:41:17,092 - pika.channel - DEBUG - 0 blocked frames\n2016-04-26 08:41:17,092 - pika.callback - DEBUG - Calling <bound method Channel._on_openok of <pika.channel.Channel object at 0x0000000006DCB2E8>> for \"1:Channel.OpenOk\"\n2016-04-26 08:41:17,092 - root - INFO - Channel opened\n2016-04-26 08:41:17,092 - root - INFO - Purging existing queue messages\n2016-04-26 08:41:17,092 - pika.channel - DEBUG - Adding in on_synchronous_complete callback\n2016-04-26 08:41:17,092 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 1}\n2016-04-26 08:41:17,092 - root - INFO - purge done\n2016-04-26 08:41:17,092 - root - INFO - Adding channel close callback\n2016-04-26 08:41:17,093 - pika.callback - DEBUG - Added: {'only': <pika.channel.Channel object at 0x0000000006DCB2E8>, 'one_shot': False, 'arguments': None, 'callback': <bound method SmartWaterAsyncConsumer.on_channel_closed of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>>}\n2016-04-26 08:41:17,093 - root - INFO - Declaring exchange revenues_estimation\n2016-04-26 08:41:17,111 - pika.callback - DEBUG - Processing 1:Queue.PurgeOk\n2016-04-26 08:41:17,111 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,111 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,112 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 0}\n2016-04-26 08:41:17,112 - pika.callback - DEBUG - Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>> for \"1:Queue.PurgeOk\"\n2016-04-26 08:41:17,112 - pika.channel - DEBUG - 1 blocked frames\n2016-04-26 08:41:17,112 - pika.channel - DEBUG - Adding in on_synchronous_complete callback\n2016-04-26 08:41:17,112 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 1}\n2016-04-26 08:41:17,112 - pika.channel - DEBUG - Adding passed in callback\n2016-04-26 08:41:17,112 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SmartWaterAsyncConsumer.on_exchange_declareok of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>>, 'calls': 1}\n2016-04-26 08:41:17,131 - pika.callback - DEBUG - Processing 1:Exchange.DeclareOk\n2016-04-26 08:41:17,131 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,131 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,131 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 0}\n2016-04-26 08:41:17,131 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,131 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,131 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SmartWaterAsyncConsumer.on_exchange_declareok of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>>, 'calls': 0}\n2016-04-26 08:41:17,131 - pika.callback - DEBUG - Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>> for \"1:Exchange.DeclareOk\"\n2016-04-26 08:41:17,132 - pika.channel - DEBUG - 0 blocked frames\n2016-04-26 08:41:17,132 - pika.callback - DEBUG - Calling <bound method SmartWaterAsyncConsumer.on_exchange_declareok of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>> for \"1:Exchange.DeclareOk\"\n2016-04-26 08:41:17,132 - root - INFO - Exchange declared\n2016-04-26 08:41:17,132 - root - INFO - Declaring queue revenues_estimation\n2016-04-26 08:41:17,132 - pika.channel - DEBUG - Adding in on_synchronous_complete callback\n2016-04-26 08:41:17,132 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': {'queue': 'revenues_estimation'}, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 1}\n2016-04-26 08:41:17,132 - pika.channel - DEBUG - Adding passed in callback\n2016-04-26 08:41:17,132 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': {'queue': 'revenues_estimation'}, 'callback': <bound method SmartWaterAsyncConsumer.on_queue_declareok of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>>, 'calls': 1}\n2016-04-26 08:41:17,152 - pika.callback - DEBUG - Processing 1:Queue.DeclareOk\n2016-04-26 08:41:17,152 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,152 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,152 - pika.callback - DEBUG - Comparing {'queue': 'revenues_estimation'} to {'queue': 'revenues_estimation'}\n2016-04-26 08:41:17,152 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': {'queue': 'revenues_estimation'}, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 0}\n2016-04-26 08:41:17,152 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,152 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,152 - pika.callback - DEBUG - Comparing {'queue': 'revenues_estimation'} to {'queue': 'revenues_estimation'}\n2016-04-26 08:41:17,152 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': {'queue': 'revenues_estimation'}, 'callback': <bound method SmartWaterAsyncConsumer.on_queue_declareok of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>>, 'calls': 0}\n2016-04-26 08:41:17,152 - pika.callback - DEBUG - Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>> for \"1:Queue.DeclareOk\"\n2016-04-26 08:41:17,152 - pika.channel - DEBUG - 0 blocked frames\n2016-04-26 08:41:17,153 - pika.callback - DEBUG - Calling <bound method SmartWaterAsyncConsumer.on_queue_declareok of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>> for \"1:Queue.DeclareOk\"\n2016-04-26 08:41:17,153 - root - INFO - Binding revenues_estimation to revenues_estimation with *\n2016-04-26 08:41:17,153 - pika.channel - DEBUG - Adding in on_synchronous_complete callback\n2016-04-26 08:41:17,153 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 1}\n2016-04-26 08:41:17,153 - pika.channel - DEBUG - Adding passed in callback\n2016-04-26 08:41:17,153 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SmartWaterAsyncConsumer.on_bindok of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>>, 'calls': 1}\n2016-04-26 08:41:17,172 - pika.callback - DEBUG - Processing 1:Queue.BindOk\n2016-04-26 08:41:17,172 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,172 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,172 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 0}\n2016-04-26 08:41:17,172 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,172 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,172 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SmartWaterAsyncConsumer.on_bindok of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>>, 'calls': 0}\n2016-04-26 08:41:17,173 - pika.callback - DEBUG - Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>> for \"1:Queue.BindOk\"\n2016-04-26 08:41:17,173 - pika.channel - DEBUG - 0 blocked frames\n2016-04-26 08:41:17,173 - pika.callback - DEBUG - Calling <bound method SmartWaterAsyncConsumer.on_bindok of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>> for \"1:Queue.BindOk\"\n2016-04-26 08:41:17,173 - root - INFO - Queue bound\n2016-04-26 08:41:17,173 - root - INFO - Issuing consumer related RPC commands\n2016-04-26 08:41:17,173 - root - INFO - Adding consumer cancellation callback\n2016-04-26 08:41:17,173 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': False, 'arguments': None, 'callback': <bound method SmartWaterAsyncConsumer.on_consumer_cancelled of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>>}\n2016-04-26 08:41:17,173 - root - INFO - Starting basic_consume\n2016-04-26 08:41:17,173 - pika.channel - DEBUG - Adding in on_synchronous_complete callback\n2016-04-26 08:41:17,173 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.839c0b002d854118b763425fa99bd86a'}, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 1}\n2016-04-26 08:41:17,173 - pika.channel - DEBUG - Adding passed in callback\n2016-04-26 08:41:17,173 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.839c0b002d854118b763425fa99bd86a'}, 'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 1}\n2016-04-26 08:41:17,193 - pika.callback - DEBUG - Processing 1:Basic.ConsumeOk\n2016-04-26 08:41:17,193 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,193 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,193 - pika.callback - DEBUG - Comparing {'consumer_tag': 'ctag1.839c0b002d854118b763425fa99bd86a'} to {'consumer_tag': 'ctag1.839c0b002d854118b763425fa99bd86a'}\n2016-04-26 08:41:17,194 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.839c0b002d854118b763425fa99bd86a'}, 'callback': <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 0}\n2016-04-26 08:41:17,194 - pika.callback - DEBUG - Processing use of oneshot callback\n2016-04-26 08:41:17,194 - pika.callback - DEBUG - 0 registered uses left\n2016-04-26 08:41:17,194 - pika.callback - DEBUG - Comparing {'consumer_tag': 'ctag1.839c0b002d854118b763425fa99bd86a'} to {'consumer_tag': 'ctag1.839c0b002d854118b763425fa99bd86a'}\n2016-04-26 08:41:17,194 - pika.callback - DEBUG - Removing callback #0: {'only': None, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag1.839c0b002d854118b763425fa99bd86a'}, 'callback': <bound method Channel._on_eventok of <pika.channel.Channel object at 0x0000000006DCB2E8>>, 'calls': 0}\n2016-04-26 08:41:17,194 - pika.callback - DEBUG - Calling <bound method Channel._on_synchronous_complete of <pika.channel.Channel object at 0x0000000006DCB2E8>> for \"1:Basic.ConsumeOk\"\n2016-04-26 08:41:17,194 - pika.channel - DEBUG - 0 blocked frames\n2016-04-26 08:41:17,194 - pika.callback - DEBUG - Calling <bound method Channel._on_eventok of <pika.channel.Channel object at 0x0000000006DCB2E8>> for \"1:Basic.ConsumeOk\"\n2016-04-26 08:41:17,194 - pika.channel - DEBUG - Discarding frame <METHOD(['channel_number=1', 'frame_type=1', \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.839c0b002d854118b763425fa99bd86a'])>\"])>\n2016-04-26 08:41:22,538 - pika.adapters.base_connection - ERROR - Socket Error: 10054\n2016-04-26 08:41:22,538 - pika.heartbeat - DEBUG - Removing timeout for next heartbeat interval\n2016-04-26 08:41:22,538 - pika.adapters.base_connection - WARNING - Socket closed when connection was open\n2016-04-26 08:41:22,539 - pika.callback - DEBUG - Added: {'only': None, 'one_shot': True, 'arguments': None, 'callback': <bound method SelectConnection._on_connection_start of <pika.adapters.select_connection.SelectConnection object at 0x000000000630F828>>, 'calls': 1}\n2016-04-26 08:41:22,539 - pika.connection - WARNING - Disconnected from RabbitMQ at spotted-monkey.rmq.cloudamqp.com:5672 (0): Not specified\n2016-04-26 08:41:22,539 - pika.callback - DEBUG - Processing 0:_on_connection_closed\n2016-04-26 08:41:22,539 - pika.callback - DEBUG - Calling <bound method SmartWaterAsyncConsumer.on_connection_closed of <__main__.SmartWaterAsyncConsumer object at 0x0000000002CCF128>> for \"0:_on_connection_closed\"\n2016-04-26 08:41:22,539 - root - WARNING - Connection closed, reopening in 5 seconds: (0) Not specified\n2016-04-26 08:41:27,540 - pika.adapters.select_connection - DEBUG - Stopping IOLoop\nTraceback (most recent call last):\n  File \"C:/data/Laborelec/Smartwater/SmartWater/smartwater/revenues_evaluation_async_server.py\", line 420, in <module>\n    example.run()\n  File \"C:/data/Laborelec/Smartwater/SmartWater/smartwater/revenues_evaluation_async_server.py\", line 381, in run\n    self._connection.ioloop.start()\n  File \"C:\\Anaconda3\\envs\\SmartWater\\lib\\site-packages\\pika\\adapters\\select_connection.py\", line 355, in start\n    self.process_timeouts()\n  File \"C:\\Anaconda3\\envs\\SmartWater\\lib\\site-packages\\pika\\adapters\\select_connection.py\", line 283, in process_timeouts\n    timer['callback']()\n  File \"C:/data/Laborelec/Smartwater/SmartWater/smartwater/revenues_evaluation_async_server.py\", line 112, in reconnect\n    self._connection.ioloop.stop()\n  File \"C:\\Anaconda3\\envs\\SmartWater\\lib\\site-packages\\pika\\adapters\\select_connection.py\", line 379, in stop\n    os.write(self._w_interrupt.fileno(), b'X')\nOSError: [Errno 9] Bad file descriptor\nI'm using pika 0.10.0, python 3.4.4 distributed by Anaconda. Single threaded application.\n. ",
    "atuljangra": "Reestablishing the connection normally. Like you would create the normal one.\nI understand that we should not use the channels from previously closed connection. But that should not be allowed by design.\nAccessing a channel when the underlying connection is closed should throw a good descriptive exception.\n. Yes. This should resolve the issue.\nThanks @vitaly-krugl \nClosing.\n. ",
    "dave-shawley": "I think that this might fix #649 as well\n. ",
    "adrianheron": "I'm afraid that the full stack trace doesn't provide anything other than I've already posted:\npython\n<class '_socket.timeout'>\nTraceback (most recent call last):\n  File \"...\", line 101, in <my_main_function>()\n  File \"...\", line 221, in <my_io_loop>()\n  File \"/usr/lib/pypy/lib_pypy/_functools.py\", line 22, in __call__\n    return self.func(*(self.args + fargs), **fkeywords)\n  File \"...\", line 124, in fire_fd_handler\n    self._fd_handlers[fileno](fileno, event)\n  File \".../pypy/site-packages/pika/adapters/base_connection.py\", line 364, in _handle_events\n    self._handle_read()\n  File \".../pypy/site-packages/pika/adapters/base_connection.py\", line 407, in _handle_read\n    return self._handle_error(error)\n  File \".../pypy/site-packages/pika/adapters/base_connection.py\", line 309, in _handle_error\n    raise socket.timeout\ntimeout\nPerhaps the one strange thing is that it is running through pypy, but I'm not sure how that would relate to this specific error.\nThe pika version is 0.10.0.\nI am unable to reproduce the error (it has happened three times over the last several weeks that I'm aware of) so I cannot generate debug-level pika logs.\nOne thing to note is that the original exception generated in _handle_read was not a socket.timeout exception, it is _handle_error that explicitly raises a socket.timeout exception in response to the original exception.  I'm not sure if that sheds any light on this.\n. Excellent! I will upgrade to the latest version.\n. ",
    "boyxuper": "Hi, @vitaly-krugl \nSorry for posting a question here, I thought it's an issue, and it need to be fixed, in my first opinion.\nI thought consume() loop should handles Basic.CancelOk. But maybe I should consider another approach.\n1. Adding a timeout would certainly not be signal safe\n   I'm not familiar to asynchronous-signal-safe things also, can u provide some references links?\n2. raise an exception from your SIGTERM handler\n   This will randomly interrupt consumer execution, and it's why I try to handle SIGTERM. \n3. BlockingChannel.consume API has a new parameter inactivity_timeout\n   I'm already using Pika 0.10.0, maybe this is only solution here.\n4. you should not be accessing connection._impl for anything\n   I use this for scheduling some healthy check tasks, and I want to keep it single-threaded.\n. @vitaly-krugl  \nThanks very much, I must missed Connection.add_timeout\n2. But we should try to shutdown gracefully, and yes, we MUST able to deal with the worst cases.\nthx again, closing this non-issue\n. ",
    "xiaopeng163": "part of my code:\n``` python\nimport logging\nimport traceback\nimport pika\nLOG = logging.getLogger(name)\nclass Publisher(object):\n    def init(self, host='localhost', port=5672, userid='guest', password='guest'):\n    self.parameters = pika.ConnectionParameters(\n        host=host, port=port,\n        credentials=pika.PlainCredentials(userid, password))\n\n    self._connection = None\n    self._channel = None\n\ndef connect(self):\n    \"\"\"This method connects to RabbitMQ, returning the connection handle.\n\n    :rtype: pika.BlockingConnection\n\n    \"\"\"\n    LOG.debug('Connecting to rabbitmq server')\n    try:\n        # Open a connection to RabbitMQ on localhost using all default parameters\n        self._connection = pika.BlockingConnection(self.parameters)\n\n        # Open the channel\n        self._channel = self._connection.channel()\n        return True\n    except Exception as e:\n        LOG.error(e)\n        LOG.debug(traceback.format_exc())\n        return False\n\n```\nwhen I set the level to DEBUG, i will get message like this:\nbash\n016-01-13 15:55:13,055.055 47795 DEBUG pika.callback add 161 [-] Added: {'callback': <bound method BlockingConnection._on_connection_error of <pika.adapters.blocking_connection.BlockingConnection object at 0x10f616d10>>, 'only': None, 'one_shot': False, 'arguments': None}\n2016-01-13 15:55:13,056.056 47795 DEBUG pika.callback add 161 [-] Added: {'callback': <bound method BlockingConnection._on_connection_start of <pika.adapters.blocking_connection.BlockingConnection object at 0x10f616d10>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2016-01-13 15:55:13,056.056 47795 INFO pika.adapters.base_connection _create_and_connect_to_socket 179 [-] Connecting to 10.75.44.10:5672\n2016-01-13 15:55:13,061.061 47795 DEBUG pika.callback process 217 [-] Processing 0:Connection.Start\n2016-01-13 15:55:13,061.061 47795 DEBUG pika.callback _use_one_shot_callback 393 [-] Processing use of oneshot callback\n2016-01-13 15:55:13,061.061 47795 DEBUG pika.callback _use_one_shot_callback 395 [-] 0 registered uses left\nBut I don't want this message. how can i disable that\n. python\n    _EARLY_LOG_HANDLER = logging.StreamHandler(sys.stderr)\n    log = logging.getLogger()\n    log.addHandler(_EARLY_LOG_HANDLER)\n    if level is not None:\n        log.setLevel(level)\n. anyone know that?\n. @vitaly-krugl yes, thanks!\n. Hi, the same issue, have you resolved that?. OK, got it. maybe I should add some code to do auto reconnection when the connection is refused.. ",
    "mattboston": "Just to add a little clarity, we're using this code to decide when it's safe to power off our dev message queue servers at the end of the day, but the code said the queues were clear and powered off the server when there might have been unack'd messages\n. ",
    "sashas-lb": "what is the status of this change ? is it going to enter the master branch soon ? \n. ",
    "codecov-io": "Current coverage is 79.19% (diff: 26.11%)\n\nNo coverage report found for master at bd33443.\nPowered by Codecov. Last update bd33443...4e42cb7\n. ## Current coverage is 82.54%\nMerging #718 into master will not change coverage\n\ndiff\n@@           master    #718   diff @@\n=====================================\n  Files          19      19          \n  Lines        3409    3409          \n  Methods         0       0          \n  Branches      536     536          \n=====================================\n  Hits         2814    2814          \n  Misses        463     463          \n  Partials      132     132\n\nPowered by Codecov. Last updated by 9f62cbe...5d37c24\n. ## Current coverage is 82.60%\nMerging #750 into master will decrease coverage by 1.08%\n\ndiff\n@@           master       #750   diff @@\n========================================\n  Files          19         19          \n  Lines        3574       3574          \n  Methods         0          0          \n  Branches      534        534          \n========================================\n- Hits         2991       2952    -39   \n- Misses        454        491    +37   \n- Partials      129        131     +2\n\nPowered by Codecov. Last updated by 51fd186...ad91e17\n. ## Current coverage is 83.55%\nMerging #754 into master will decrease coverage by 0.08%\n\ndiff\n@@           master       #754   diff @@\n========================================\n  Files          19         19          \n  Lines        3574       3574          \n  Methods         0          0          \n  Branches      534        538     +4   \n========================================\n- Hits         2991       2986     -5   \n- Misses        454        456     +2   \n- Partials      129        132     +3\n\nPowered by Codecov. Last updated by 51fd186...d4263bf\n. ## Current coverage is 82.99% (diff: 40.74%)\nNo coverage report found for master at 51fd186.\nPowered by Codecov. Last update 51fd186...bb19d48\n. ## Current coverage is 83.63% (diff: 100%)\nNo coverage report found for master at 51fd186.\nPowered by Codecov. Last update 51fd186...394e21b\n. ## Current coverage is 82.51% (diff: 100%)\nMerging #780 into master will not change coverage\n\ndiff\n@@             master       #780   diff @@\n==========================================\n  Files            19         19          \n  Lines          3574       3574          \n  Methods           0          0          \n  Messages          0          0          \n  Branches        538        538          \n==========================================\n  Hits           2949       2949          \n  Misses          491        491          \n  Partials        134        134\n\nPowered by Codecov. Last update fe99f35...a8417fd\n. ## Current coverage is 82.51% (diff: 100%)\nMerging #781 into master will not change coverage\n\ndiff\n@@             master       #781   diff @@\n==========================================\n  Files            19         19          \n  Lines          3574       3574          \n  Methods           0          0          \n  Messages          0          0          \n  Branches        538        538          \n==========================================\n  Hits           2949       2949          \n  Misses          491        491          \n  Partials        134        134\n\nPowered by Codecov. Last update fe99f35...02a5d40\n. ## Current coverage is 83.54% (diff: 100%)\nMerging #786 into master will increase coverage by 1.03%\n\ndiff\n@@             master       #786   diff @@\n==========================================\n  Files            19         19          \n  Lines          3574       3574          \n  Methods           0          0          \n  Messages          0          0          \n  Branches        538        538          \n==========================================\n+ Hits           2949       2986    +37   \n+ Misses          491        456    -35   \n+ Partials        134        132     -2\n\nPowered by Codecov. Last update fe99f35...5b9a105. ## Current coverage is 83.54% (diff: 100%)\nMerging #787 into master will increase coverage by 1.03%\n\ndiff\n@@             master       #787   diff @@\n==========================================\n  Files            19         19          \n  Lines          3574       3574          \n  Methods           0          0          \n  Messages          0          0          \n  Branches        538        538          \n==========================================\n+ Hits           2949       2986    +37   \n+ Misses          491        456    -35   \n+ Partials        134        132     -2\n\nPowered by Codecov. Last update fe99f35...1c43802. ## Current coverage is 82.46% (diff: 100%)\nMerging #788 into master will decrease coverage by 0.04%\n\ndiff\n@@             master       #788   diff @@\n==========================================\n  Files            19         19          \n  Lines          3574       3571     -3   \n  Methods           0          0          \n  Messages          0          0          \n  Branches        538        527    -11   \n==========================================\n- Hits           2949       2945     -4   \n- Misses          491        493     +2   \n+ Partials        134        133     -1\n\nPowered by Codecov. Last update fe99f35...9dcf00d. ## Current coverage is 82.48% (diff: 100%)\nMerging #789 into master will decrease coverage by 0.02%\n\ndiff\n@@             master       #789   diff @@\n==========================================\n  Files            19         19          \n  Lines          3574       3574          \n  Methods           0          0          \n  Messages          0          0          \n  Branches        538        528    -10   \n==========================================\n- Hits           2949       2948     -1   \n- Misses          491        493     +2   \n+ Partials        134        133     -1\n\nPowered by Codecov. Last update fe99f35...7c332e5. ## Current coverage is 82.48% (diff: 100%)\nMerging #791 into master will decrease coverage by 0.02%\n\ndiff\n@@             master       #791   diff @@\n==========================================\n  Files            19         19          \n  Lines          3574       3574          \n  Methods           0          0          \n  Messages          0          0          \n  Branches        538        528    -10   \n==========================================\n- Hits           2949       2948     -1   \n- Misses          491        493     +2   \n+ Partials        134        133     -1\n\nPowered by Codecov. Last update fe99f35...890f198. # Codecov Report\nMerging #802 into master will decrease coverage by 0.02%.\nThe diff coverage is n/a.\n\n```diff\n@@            Coverage Diff             @@\nmaster     #802      +/-\n==========================================\n- Coverage   82.51%   82.48%   -0.03%   \n==========================================\n  Files          19       19            \n  Lines        3574     3574            \n  Branches      538      528      -10   \n==========================================\n- Hits         2949     2948       -1   \n- Misses        491      493       +2   \n+ Partials      134      133       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 66.27% <0%> (-0.4%) | :x: |\n| pika/adapters/select_connection.py | 78.57% <0%> (-0.27%) | :x: |\n| pika/adapters/blocking_connection.py | 84.64% <0%> (+0.13%) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fe99f35...c8e43b9. Read the comment docs.. # Codecov Report\nMerging #803 into master will decrease coverage by 0.02%.\nThe diff coverage is 100%.\n\n```diff\n@@            Coverage Diff             @@\nmaster     #803      +/-\n==========================================\n- Coverage   82.51%   82.48%   -0.03%   \n==========================================\n  Files          19       19            \n  Lines        3574     3575       +1   \n  Branches      538      528      -10   \n==========================================\n  Hits         2949     2949            \n- Misses        491      493       +2   \n+ Partials      134      133       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 84.66% <100%> (+0.15%) | :white_check_mark: |\n| pika/adapters/base_connection.py | 66.27% <0%> (-0.4%) | :x: |\n| pika/adapters/select_connection.py | 78.57% <0%> (-0.27%) | :x: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fe99f35...5ef33be. Read the comment docs.. # Codecov Report\nMerging #805 into master will decrease coverage by 0.51%.\nThe diff coverage is 60.49%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #805      +/-\n==========================================\n- Coverage   82.51%   81.99%   -0.52%   \n==========================================\n  Files          19       20       +1   \n  Lines        3574     3655      +81   \n  Branches      538      544       +6   \n==========================================\n+ Hits         2949     2997      +48   \n- Misses        491      511      +20   \n- Partials      134      147      +13\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/__init__.py | 77.27% <100%> (+5.05%) | :arrow_up: |\n| pika/adapters/asyncio_connection.py | 58.44% <58.44%> (\u00f8) | |\n| pika/adapters/base_connection.py | 66.27% <0%> (-0.4%) | :arrow_down: |\n| pika/adapters/select_connection.py | 78.57% <0%> (-0.27%) | :arrow_down: |\n| pika/adapters/blocking_connection.py | 84.64% <0%> (+0.13%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fe99f35...c7ba3c1. Read the comment docs.\n. # Codecov Report\nMerging #807 into master will increase coverage by 1.03%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #807      +/-\n==========================================\n+ Coverage   82.51%   83.54%   +1.03%   \n==========================================\n  Files          19       19            \n  Lines        3574     3574            \n  Branches      538      528      -10   \n==========================================\n+ Hits         2949     2986      +37   \n+ Misses        491      457      -34   \n+ Partials      134      131       -3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 84.64% <0%> (+0.13%) | :arrow_up: |\n| pika/adapters/base_connection.py | 67.05% <0%> (+0.39%) | :arrow_up: |\n| pika/adapters/select_connection.py | 88.09% <0%> (+9.25%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fe99f35...26e6d59. Read the comment docs.\n. # Codecov Report\nMerging #815 into master will decrease coverage by 0.02%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #815      +/-\n==========================================\n- Coverage   82.51%   82.48%   -0.03%   \n==========================================\n  Files          19       19            \n  Lines        3574     3574            \n  Branches      538      528      -10   \n==========================================\n- Hits         2949     2948       -1   \n- Misses        491      493       +2   \n+ Partials      134      133       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 66.27% <0%> (-0.4%) | :arrow_down: |\n| pika/adapters/select_connection.py | 78.57% <0%> (-0.27%) | :arrow_down: |\n| pika/adapters/blocking_connection.py | 84.64% <0%> (+0.13%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fe99f35...29b51b5. Read the comment docs.\n. \n",
    "carlosefr": "We're using this adapter out-of-tree and haven't had any issues up to now. I've been looking around a bit, mostly to get a feeling of how much work would be involved in properly testing this to maybe help it move forward.\nI've reached two conclusions:\n1. The amount of work involved would be enormous. Even if there is a way to wrap this adapter in something that can be integrated into the existing test suite, it's very hard to even begin because what needs to be implemented for this to work is far from clear. Existing adapters don't seem to help much here.\n2. Being based on the Twisted adapter, it would seem that the better course of action would be to try using the same tests as it does. However, it looks like the Twisted adapter also has little to no tests.\nThis is a catch-22 situation: The adapter doesn't get included because it would require \"a comprehensive test suite\" but that's so much work that the chances of it happening are close to none.\nOn the other hand, including it would be the only way of giving it a fighting chance of getting a minimum amount of tests. At least the same chance the Twisted adapter currently has.\nWhy not include it as experimental with no maintenance guarantees? Right now it pays off to fix it out-of-tree if it breaks. At least this would give some incentive to fix it in a centralized location.\n. I'm not advocating against tests, far from it. Nor am I advocating that it's somehow unreasonable to request that contributions be accompanied with proper tests. But, as with everything, context matters and Pika doesn't provide the proper context for this to be a feasible requirement.\nAllow me to elaborate on what I mean by this and I apologise beforehand if it happens to read a bit too harsh or if I repeat bit of my previous comment.\nPika is sufficiently documented if you're consuming its user-facing API, but is seriously lacking if you're looking to extend it (i.e. there is no documentation at all). I've spent a number of hours browsing adapter code and tests and got nowhere. I can't tell what's the interface adapters are supposed to be implementing, nor what a \"complete\" adapter is supposed to look like. The actual interface is whatever the adapters want it to be, it seems.\nProper testing in this scenario means either a full test suite for each adapter or understanding the core bits that are already being tested otherwise. The first option requires a deep level of knowledge in AMQP while both options require a core-developer knowledge of Pika. There's no way to justify the time investment, mostly because there's no way to estimate an upper bound for that investment and trying to estimate it is already a significant investment by itself.\nOther projects enforcing proper test requirements don't have such a wide gap between the effort of contributing something and building the tests to go along with it. This makes it an impossible proposition for outside contributions as most of them will be of the \"scratch one's own itch\" kind.\nPer my experience, most contributors to open-source projects are \"drive-by shooters\" that won't even reply after the initial patch or pull-request. A small percentage will be willing to add the extra effort to clean the patch up and write tests but, if that requires more than twice the effort, you'll find that no one is willing to do it. And I'm being very optimistic, I think.\n\nTL;DR: Poorly tested code clearly advertised as such is better than no code at all. It doesn't even have to work in the future, it can just sit there harming no one. The simple fact that it's there will attract random fixes from whoever is using it despite the warnings (like myself, which looked into this because of a bug we were having that turned out to be our own code's fault). This alone is worth it, IMHO.\n. I understand that when receive_message blocks the thread, heartbeats are not processed at all, which is why I wanted to change the default heartbeat interval (or, in this case, timeout).\nI have long-running processes for which I do not want the server to tear down the connection while they're doing something.. The results I posted in the initial comments are what I'm seeing. The publisher I'm using is below, for completeness. I've since upgraded RabbitMQ to 3.7.7 in my test VM. I'm also (was already) using a fresh virtualenv:\n```\n$ pip list\nPackage    Version\n\npika       0.12.0 \npip        10.0.1 \nsetuptools 39.0.1 \n```\nPerhaps is something that only happens with the specific settings I'm using?\nI'm unable to modify the application to have all RabbitMQ-related things in a separate thread. In either case, if the heartbeat is ignored (along with the proposed value from the server), then something's not right.\nTest Publisher\n```python3\n!/usr/bin/env python3\n-- coding: utf-8 --\nimport time\nimport pika\ncs = pika.PlainCredentials(\"guest\", \"guest\")\nps = pika.ConnectionParameters(host=\"127.0.0.1\", credentials=cs)\nconn = pika.BlockingConnection(ps)\nchan = conn.channel()\ntry:\n    count = 1\n    while True:\n        print(\"Publishing... %d\" % count)\n        chan.basic_publish(exchange=\"example_exchange\", routing_key=\"example.key\", body=\"%d\" % count)\n        count += 1\n    time.sleep(1)\n\nexcept KeyboardInterrupt:\n    conn.close()\n``. Here's a capture done withsudo tcpdump -i lo tcp port 5672 -w pika-gh-1093.pcap`:\npika-gh-1093.pcap.gz\n. The code I'm using is what I've pasted above. Both for the publisher and the subscriber. No other clients are using this RabbitMQ server (this is all running inside a test VM).\nThe 90 second heartbeat also matches what I've pasted above: it's probably the heartbeat set in my rabbitmq.config file.. I've changed the default heartbeat in RabbitMQ to 87. Here's another capture:\npika-gh-1093.pcap.gz\n. I'm running CentOS 7.5 in VirtualBox 5.2.12 set up by Vagrant 2.1.2 on macOS 10.12.6. The base box is this one: https://app.vagrantup.com/centos/boxes/7/versions/1804.02\n$ rpm -qa | egrep 'erlang|rabbitmq'\nrabbitmq-server-3.7.7-1.el7.noarch\nerlang-21.0.2-1.el7.centos.x86_64\nUnfortunately I don't have any physical Linux machine to try. But I can try other distros in a VM, if needed.\nRabbitMQ log:\n2018-07-07 20:15:46.601 [info] <0.619.0> accepting AMQP connection <0.619.0> (127.0.0.1:52308 -> 127.0.0.1:5672)\n2018-07-07 20:15:46.604 [info] <0.619.0> connection <0.619.0> (127.0.0.1:52308 -> 127.0.0.1:5672): user 'guest' authenticated and granted access to vhost '/'\n2018-07-07 20:16:17.091 [warning] <0.619.0> closing AMQP connection <0.619.0> (127.0.0.1:52308 -> 127.0.0.1:5672):\n{writer,send_failed,{error,timeout}}. It's strange that you can't reproduce the problem, but I think I found it. It's the same as issue #753: the server times out the connection while trying to send more messages to the client.\nAdding chan.basic_qos(prefetch_count=1) to the above test case works.\nAlthough this isn't an actual bug, it still is kind of surprising and I think it would be worth adding a note in the documentation about it. Here's my suggestion:\n\nAMQP connection heartbeat timeout value for negotiation during connection tuning or callable which is invoked during connection tuning. None to accept broker\u2019s value. 0 turns heartbeat off. Defaults to DEFAULT_HEARTBEAT_TIMEOUT.\nNote: The connection may still be timed out by the broker, regardless of the heartbeat setting, if the client takes too long to accept new messages. Setting prefetch_count to 1 on the channel is one way to prevent this from happening.. There is such a thing as explaining in too much detail in the wrong place, that's why my suggestion (which is what this is) for the documentation says \"one way to prevent this from happening\", meaning it's a warning and not an explanation. More detailed information should be elsewhere, IMHO.\n\nMy point is that something is needed so that users of the API don't spend too much time running around this problem thinking heartbeats are not working, as I did (and others did as well, judging from the issues).. ",
    "janLo": "What is the status of this PR? Is it going to get merged anytime soon? Or is there another option available to use pika with asyncio?. ",
    "michaelklishin": "Note that asyncio adapter was also contributed in #768 and later #805, the latter of which was merged.. @daoiqi I'm not sure why you expect a Kombu question to be answered here. I believe recent Kombu versions implement heartbeats but I'd nonetheless recommend Pika any day of the week.\n. Note that the prefetch value is very likely a total red herring. It all comes down to how a server-initiated connection closure is handled.\n. Please post server logs. The object that's None seems to be connection socket.\n. We will apply this change with slightly different constant names manually. Thank you, @eshizhan!. @lukebakken should it go into pika.channel then?. @lukebakken and I agreed to keep the constants in pika.spec as they are, well, defined in the AMQP 0-9-1 spec :). See #839.. It's not actually necessary to cancel consumers when closing a channel (the server will discard them all anyway) but it's a decent start.. @lukebakken I added a section about how to run tests.. @rmk135 hi Roman. We are aware of community interest in a new release.\nThis library is undergoing a transition from one extremely busy maintainer to a group of \"just\" busy ones. It takes a while to make sure everyone agrees on what needs to be done before we can produce a release. Please be patient.\nThe best thing the community can do to help is going through the pull requests, trying them out and reviewing them.. @westphahl thank you for confirming!. @lukebakken thanks for spotting the typo, ready for another round.. Pika does not support automatic connection recovery the same way Bunny, Java, .NET clients do. This is nothing new in 0.11.. Filed https://github.com/pika/pika/issues/858. This is not a small undertaking, although with at least 6 clients in 3 different languages using roughly the same recovery algorithm, there's plenty of prior art to learn from.. Fair enough. There are now doc examples that demonstrate recovery.. @cainbit sorry, any updates on this?. @cainbit if you have no plans to finish this PR that's fine but please let us know so that another contributor or one of RabbitMQ engineers can step up. Thank you.. Should this PR be resurrected, we would consider it as long as @vitaly-krugl's comments are addressed.. This problem is generally present in every (or at least many) client in a dynamically typed language. amq-protocol used by Ruby clients serialises numbers as longs for this reason IIRC.. This is generally in line with what Bunny and Java client do (https://github.com/ruby-amqp/bunny/blob/master/lib/bunny/session.rb#L1172, https://github.com/ruby-amqp/bunny/blob/master/lib/bunny/session.rb#L1259) but I'd like to take a closer look.\nFor example, when only one of the peers disables heartbeats, Bunny will use the non-zero value of the two.. I confirmed that Java client does the same thing (which is not surprising given that with Bunny we tried to copy most of those decisions).\nTherefore Pika should, too.. @zjj I don't think \"flexibility\" is a good enough justification for inconsistent behaviour between clients when it comes to heartbeat negotiation. Or perhaps I misunderstood your comment.\nI pushed an update that changes heartbeat negotiation logic to be the same as in Java client, which accomplishes the same goal: the smallest value of the two will be used when both client and server provide non-zero values.. Releasing this in 0.11.1 seems OK: for the majority of users nothing will change. In other cases a lower value will be used, which is safe unless the values are really low (< 5 seconds is known to produce false positives often enough). Arguably this is a bug fix: the logic is now consistent with that of multiple  other widely used clients (Java, .NET, Bunny) and it's closer to \"doing the right thing\" if you consider what the goal of having heartbeats in the protocol is.. Wow, I stopped receiving notifications about this repo in my general GitHub feed :/. We are going to release 0.11.2 in a bit.. Release announcement.. Thread safety is a property of an algorithm (a sequence of steps). You haven't mentioned what specific scenario you are trying to make safer, so we can't know whether locking in that one function is enough.\nLocking around I/O reads should be done with care. The same can be said for writes but in practice most if not all clients do it and combined with heartbeats it works OK. It can have devastating effect on throughput, as demonstrated by recent .NET client changes (there were multiple ones, both for publishers and consumers, so I'm not linking to a specific one). Were any benchmarks conducted to compare the difference for the \"no concurrent access\" (only one thread) scenario, which is likely to be the most common one?. It would help a lot if you explained what problem we are trying to solve here. How can it be reproduced?. A suggestion from the sidelines if I may: a traffic dump would greatly help here. If this is primarily about the lack of a timeout for basic.consume operations, it'd still be great to have some evidence of what the client does before an operation blocks.. @lukebakken @gmr fine with you to merge?\n. I'm hardly a Python expert but looks like distutils version parser does not expect the version this 3.7 package uses?. Looks like this has been addressed in https://github.com/pika/pika/commit/1b0f440010e062921ca74f0a9b502a4f88d05778.. We can use manual_ack but it would mean more changes for those upgrading (the value would have to be negated).. @gmr we've been there with Bunny, Java and C# clients, a lot of users do not read the docs and a clearer named option reduces the probability of potentially costly mistakes. In fact, IIRC for Bunny and the C# client the renaming was suggested by the community.. FTR, @lukebakken convinced me to drop support for no_ack. Oh the thousands of code examples on the Web that will be copied only to immediately fail\u2026. I strongly disagree with the premise that \"all API changes are bad\". Breaking API changes are unavoidable for any project, in particular those that have been around for years. They let the API eventually shape into something better than whatever the author originally came up with.\n\"Sticking with the first idea you have\" ends up badly more often than not.\nInability to make sensible, discussed and communicated API changes is the absolute hell for a library maintainer. This is how we end up with many year old libraries with horrible, confusing, ad-hoc APIs. Most programming languages have a standard library corner (or ten) which look like that.\nAs of #977 there will be helpful exceptions raised in most obvious cases. The changes resulted in a minor and soon a major API version bump. Those changes should collectively be enough to\ncommunicate the changes to those who pay attention and quite closely follow a pretty well established set of practices in the industry (minor and major version bumps, change logs, dependency version pinning on the consuming side).. According to RabbitMQ logs, all connections are closed cleanly by the client.. The supervisor reports are just noise and cannot affect the client. Next thing I'd check is how Pika handles socket reads that return no data (which can be a natural course of events on a closing connection).. My 2\u00a2: handshake timeouts are present in all official and most popular clients.. @vitaly-krugl yes, since any default library maintainers pick is going to be wrong ;) Java and .NET clients use the same value of 10 seconds.. Bunny doesn't have a handshake timeout per se but it does have a TCP connection timeout, socket operation timeouts and channel continuation timeouts, so collectively that's sufficient but produces slightly different error messages.. Makes sense to me.. @vitaly-krugl thank you for your ongoing (and seemingly never ending :)) contributions!. Shovel, Federation and some clients use URI parameters for a subset of connection options. It's not strictly a requirement.. A somewhat related question, should non-string queue names even be allowed by the API?. Bunny uses 30 second timeouts. In general any value under the heartbeat timeout makes sense.\n5-15 seconds is the range I'd start with.. @vitaly-krugl that's my reading of the 500 Mile Email story (I wish all users collected data to report first, like that statistics department head!). The idea makes sense. Good catch, @vitaly-krugl!. I tested and polished the examples as well as did an editing pass. Thank you, @hairyhum. @vitaly-krugl if you have more tweaks in mind, feel free to submit PRs or file issues :). Thank you!. Thank you!. Thank you!. In my experience this is a royal PITA to automate testing of peer verification since standard TLS peer verification usually involves hostname comparison. How about we introduce some examples that use tls-gen instead and learn/document the behavior that way?. I suggest that Pika adopts tls-gen's basic profile (possibly with some tweaks and improvements) for tests instead of reinventing the wheel :) We already do that in one doc guide, in fact.. @maggyero thanks good to know. That deserves a couple of sections in RabbitMQ's TLS and Management plugin guides, in fact.. @vitaly-krugl FTR, I was using the tip of master + a PR by @hairyhum.. @vitaly-krugl so should this be addressed in a separate PR or #1034? As our experience with other clients suggests, automatically testing heartbeat implementations beyond a couple of very common scenarios is hard. More often than not we modify the server e.g. to not send heartbeat frames to imitate certain failures. So I'd apply a fix and test with the recovery example we now have in the docs.. See server logs and if you can reproduce, take a traffic dump. Very likely it's an environment-specific issue.. Two hypotheses that immediately come to mind:\n\nA socket is no longer in a functional/connected state but heartbeats were not enabled or the interval is too high\nClient is out of file descriptors\n\nBoth should be reasonably quick to verify.. I'm curious why the implementation we had did install successfully from git via pip?. The queue.delete and exchange.delete behavior was changed in 3.0 IIRC.. What exactly does \"simply hanging\" mean? Are there no deliveries? Have you checked server logs to verify that there are no client connection loss events or missed heartbeats?. While that is true, we try to reflect the default in client libraries as well for two reasons:\n\nNewer clients can be used against older server versions. It would be nice to have a safer default combination.\nDevelopers are likely to check the client's default when in doubt.. @vitaly-krugl we just want to use a safer default regardless of the server/client combination. It is definitely common to see clients upgrade before the server they connect to. This only changes the default and I don't see how it \"cripples the client\".. @vitaly-krugl would you object if I submit a PR that changes the default to 2047?. I'm getting a conflict:\n\ngit checkout -b lukebakken-pika-1060-version-logging stable\ngit pull https://github.com/lukebakken/pika.git pika-1060-version-logging\nfatal: 'stable' is not a commit and a branch 'lukebakken-pika-1060-version-logging' cannot be created from it\nFrom https://github.com/lukebakken/pika\n * branch            pika-1060-version-logging -> FETCH_HEAD\nFirst, rewinding head to replay your work on top of it...\nApplying: Implemented BlockingConnection test TestViabilityOfMultipleTimeoutsWithSameDeadlineAndCallback\nUsing index info to reconstruct a base tree...\nM   tests/acceptance/blocking_adapter_test.py\nFalling back to patching base and 3-way merge...\nAuto-merging tests/acceptance/blocking_adapter_test.py\nCONFLICT (content): Merge conflict in tests/acceptance/blocking_adapter_test.py\nRecorded preimage for 'tests/acceptance/blocking_adapter_test.py'\nerror: Failed to merge in the changes.\nPatch failed at 0001 Implemented BlockingConnection test TestViabilityOfMultipleTimeoutsWithSameDeadlineAndCallback\nSomehow the conflicting revision is e9f4304883595d7254ab642dd6d72e69e60cea03 from February :/. This rabbitmq-users thread is worth mentioning.. The changes move Pika much closer to the Java client and look reasonable. I tried the following script, which is a modification of tutorial 1's consumer:\n``` python\n!/usr/bin/env python\nimport pika\nconnection = pika.BlockingConnection(pika.ConnectionParameters(\n        host='localhost', heartbeat = 10))\nchannel = connection.channel()\nchannel.queue_declare(queue='hello')\ndef callback(ch, method, properties, body):\n    print(\" [x] Received %r\" % body)\nchannel.basic_consume('hello', callback, auto_ack = True)\nprint(' [*] Waiting for messages. To exit press CTRL+C')\nchannel.start_consuming()\n```\nConnection heartbeat timeout is negotiated as 10 but after 40 to 60 seconds I reliably get a missed heartbeat exception from Pika:\nPYTHONPATH=. ./debug/idle_consumer.py                                                                                                                                                                 1 \u21b5\n [*] Waiting for messages. To exit press CTRL+C\nTraceback (most recent call last):\n  File \"./debug/idle_consumer.py\", line 17, in <module>\n    channel.start_consuming()\n  File \"/Users/antares/Development/RabbitMQ/pika.git/pika/adapters/blocking_connection.py\", line 1886, in start_consuming\n    self._process_data_events(time_limit=None)\n  File \"/Users/antares/Development/RabbitMQ/pika.git/pika/adapters/blocking_connection.py\", line 2048, in _process_data_events\n    self.connection.process_data_events(time_limit=time_limit)\n  File \"/Users/antares/Development/RabbitMQ/pika.git/pika/adapters/blocking_connection.py\", line 840, in process_data_events\n    self._flush_output(common_terminator)\n  File \"/Users/antares/Development/RabbitMQ/pika.git/pika/adapters/blocking_connection.py\", line 533, in _flush_output\n    raise self._closed_result.value.error\npika.exceptions.AMQPHeartbeatTimeout: No activity or too many missed meartbeats in the last 10 seconds\nHere's a capture of the heartbeat frames sent on the connection in text form.. My point was that both server and Pika had heartbeats enabled and for several timeout periods the connection was considered to be alive even without any activity (it definitely was fine). Bunny adds a fixed number and does not allow heartbeat timeout to be configured < ~ 1 second. It also can run for as long as I don't shut it down with a similar test.. @tazgithubmaster please upgrade to RabbitMQ 3.7.6 ASAP. 3.5.x has been out of support for at least a year now. You are many releases behind.. I'm not sure what the intent of your code is but remove_timeout assumes its argument is a timer. I somewhat doubt it is a part of the public API. Consider removing that line in your code or explaining why exactly you need it.. I now see in #1086 that the same practice that's used for all RabbitMQ distributions subprojects and some clients has been adopted for Pika: questions belong to the mailing list, either rabbitmq-users or pika-python.\nI do not see any examples that use remove_timeout. The most logical explanation I can think of is: it's not a part of the public API, even if the function happens to be public.. @sky-code that's a little light on details. I don't see LibevConnection mentioned anywhere in Pika master. Other adaptors don't seem to use async for attribute name or similar. Please provide a way to reproduce and save the maintainers some time.. If retry attempts do not use back-off then the interval of 0 effectively creates an infinite loop.\nA minimal non-zero recovery interval makes sense for many reasons: TCP connection, Pika's/AMQP 0-9-1 connection and RabbitMQ's view of both of those have state and it takes time to reset. Clients cannot synchronize this with the server and the remote host's OS. Failure conditions also do not usually disappear immediately so recovery retries are a very common occurrence, which should be taken into account when picking a retry interval value.\nI think Pika should reject zero values in this case as invalid.. Closing since there is no consensus about this specific wording. We'd consider an alternative PR, of course.. @lukebakken I'm not sure what's the status of this. Do we expect any more changes?. @gmr WDYT?. This is a good catch, we should rename it to global and de-emphasize in the docs (like we will do for other clients). The reason being is that quorum queues coming in 3.8 won't support every single QoS/prefetch feature classic queues have accumulated over the years.. I don't think we can provision packages without sudo, so this is probably a no-go then.. Thank you.\nThis looks good at first glance. Is it expected that GitHub does not render the :py:class: domains? Is the goal here to improve the README on PyPI?. The consumer recovers from connection failure as expected \ud83d\udc4d. The test (and most of its peers in BlockingConnectionTests) uses mocks exclusively. I find such tests to be very fragile as they heavily depend on the smallest implementation detail changes. Of course, my general opinion that mock tests are of very little use for projects such as Pika (data service clients) doesn't help here :). There are over 2K LOC of acceptance tests forBlockingConnection`. I suggest that we remove all mocked tests as they are a waste of time (sorry not sorry). If we are lacking tests in some areas, more integration tests on more platforms is what I'd do.. I disagree with various lint tools myself on a regular basis but they are still worth using in almost every case.\nThank you for the detailed explanation, @jeremycline!. Let's rename this to TRANSIENT_DELIVERY_MODE.. Let's rename this to PERSISTENT_DELIVERY_MODE. Err, looks like an Emacs artefact. Will fix.. I'd go further and not mention the \"no-ack option\" alone but say \"manual acknowledgements\" and then explain that the no-ack (which should be renamed to auto-ack) controls it. And perhaps link to the RabbitMQ guide on acknowledgements.. You ignore the fact that there are 10 years worth of Web materials that do use no_ack.. Other breaking changes do not matter for this decision. Most developers do not read documentation until things break. Copying code from examples on the Web is extremely common and a lot of those examples use no_ack, as does probably every app that uses Pika out there. It is trivial to support both options, so it's the right thing to do.. The probability of which I estimate to be effectively 0.. Happy to add another test.. Should no_ack be a kwarg in the function signatures then? I'm not sure how kwarg aliasing is typically handled in the Python community, sorry.. Sure but isn't passing two conflicting options a textbook example of user error? We can log a warning if both keys are present.. I agree, that's how we've done it in Bunny. Will look into it.. I agree with @maggyero.. Can we change this to just \"HTTPS\" (this is not a music album cover ;)) and add a comment that says \"See http://www.rabbitmq.com/ssl.html to learn more\"? The rest looks reasonable to me.. Haha. Mock tests are a never ending source of entertainment :). It's been 60 seconds by default in the broker and most clients for a good couple of years now.. Can we make this say \"node\"? \"AMQP\" is used to refer to two protocols and the target could be a proxy or load balancer, too.. RabbitMQ identifies connections using a 4-tuple: client hostname, client port, server hostname, server port. Should the same logic be applied here? Of course it would assume that the connection has or has had an open socket.. Since the doc string says \"timeout\" and not \"interval\" and interval is a half of the timeout value per spec (2 missed heartbeats are considered to be a timeout), should we rename the argument and properties?. @vitaly-krugl . RabbitMQ server will treat the negotiated period of time as a timeout (2 intervals), regardless of what it's named in the client.. I'm afraid I don't. Client responsiveness is not tied to a particular prefetch setting and 1 is a very conservative value. Again, I primarily dislike the incredible generality of the language here.. A quick REPL experiment suggests that it will, at least if the body data type is compatible with %s.. ",
    "Ignalion": "Hi, @vitaly-krugl \nNo, work with pika is being carried out only from one thread.\nRegarding the logs. In fact there is not much logs to provide such as pika logging only its initialization.\nI mean, you can't see any logs covering the issue time because pika does not do them. I did a bit of debugging so I can tell you what I'm seeing is.\nActually, I'm attaching startup pika log, I hope you found anything interesting there:\nhttps://gist.github.com/Ignalion/dac49e941d246f6bacdc\n. @vitaly-krugl This is almost what I did for debugging it.\nI got the logs you need especially for the message affected. There is some issue with loggers so I couldn't set a formatter for root logger unfortunately however I think the issue is pretty obvious here.\nhttps://gist.github.com/Ignalion/4097c322a07bb65f8443\nAs you can see, all the data was received at one frame, the size of body matches.\nThere is 5 sec delay before _dispatch_channel_events method and I know exactly what it did during that time (I've seen it). It waits timeout for events = self._poll.poll(self.get_next_deadline() line.\n. @vitaly-krugl yes, sure. It just didn't go under that if.\nRegarding the code sample. Actually, I can provide you with the code however issue appears not every time so I can't tell exact circumstances for it to raise.\nHere the class that handles requests by RabbitMQ:\nhttps://gist.github.com/Ignalion/7c991cba7c18ecc8b938\n. > From which thread/context will process_response be called? I suspect that it's going to be called from a different thread\nNo, the communication in this thread is doing only through queues. process_response will be called by retry function, here is it's code:\n@inlineCallbacks\ndef retry(func, exc, interval=0.1, timeout=30):\n    max_time = time() + timeout\n    while time() <= max_time:\n        try:\n            result = func()\n            returnValue(result)\n        except exc:\n            yield sleep(interval)\n    else:\n        raise TimeoutError\nAnd if you interested, there is the code of sleep function as follows:\ndef sleep(t):\n    awaken_later = Deferred()\n    reactor.callLater(t, awaken_later.callback, None)\n    return awaken_later\nI guess what I'm trying to say is that the code of this snippet would be called only from current thread and it's absolutely thread safe.\n. @vitaly-krugl more logs for the logs god\nhttps://gist.github.com/Ignalion/cd2de7ae4576eb613429\n. @vitaly-krugl Unfortunately I can't run your snippet in my environment because I can't block app where this code is running. It explains the different thread.\nI tried though just run your code snippet and it just works w\\o any exceptions. I ran it even during reproducing the issue with my code but it doesn't reproduce that way which is expected.\nIn fact, I see this issue as race condition which is not easily reproducible. In your example you constantly sending two messages so issue most likely won't appear because it would be no timeout because there is always system events coming.\nAs far as I understand for the issue you should get only one message and then don't have any for timeout to starts.\nIn fact, the question here is a bit different. The question is why pika checking poll event AFTER it receives data. As for me this is the root cause.\nThat pika calling poll() when all system events where already fired.\nLike, we have data we need and then we're trying to check whether there is any more data. If there is no system events, we have to wait 5 secs anyway before timeout is fired.\nUPD: I managed to reproduce the issue on my env even with running your script in parallel mode. It's very weird actually.\n. @vitaly-krugl like I running the provided script and my script in different consoles:)\n. @vitaly-krugl There is some misunderstanding.\nYour script doesn't reproduce the issue. And in fact I have no idea how to adapt it for using in my env (actually I have but i'm thinking of trying to resolve the issue for starting get rid of multithreading).\nIn fact, I'd use Twisted Connection with async Twisted sockets but I can't do it because our app uses its own main loop so I can't just block it with reactor.run() call. So, I have to do some workarounds here.\n. Well, I get rid of threads here. I left blocking connection but simply get rid of threading and the issue seems to be resolved. I still don't understand how it works because I call and init pika and do all the communication with pika only from one different thread.\nI think the socket and select operations in pika are not thread safe. And I think the possible explanation is that main thread also uses some poll() methods and because of this it's blocking pika's one.\n. ",
    "yangyuliang1981": "@vitaly-krugl \nThanks a lot for your help.\nNow our project is on requirements phase, I'll forward a response when we get a final solution in 2016.\n. Using the latest code from github, it works ok, while the code from pypi is not work.\nThe former fix some bugs with connection close.\nNot test backpressure yet, another issue.\n. @vitaly-krugl, It's ok to add the case as an unit test.\nShould I add it, or you will do it??\nI use the latest code from github, then the connection is not closed strangely. But it takes 5 seconds to send and recv a message. It introduce another bug obviously.\n. @vitaly-krugl thanks, you explained it very clearly. But I don't familiy with the mock things and I don't know how to submit a PR. Can you help me to do these things?\nI created the 5 seconds issue, and you have closed it as pika not support thread.\nThen I am confused about how to schedule the publishing process.\nThe following code is a segment of a pika publishing example.\nDoes it means I should publish the message with following mode?\nFirst, create a global message queue,\nSecond, start a loop to checking the queue until it is not empty,  and add a timeout callback function\nThird, run the basic_publish function to send the message\nForth, go back to second step for the checking loop.\nIs the above mode correct?\nI am worried about the message queue checking loop will take too much CPU resources.\n```\ndef schedule_next_message(self):\n        \"\"\"If we are not closing our connection to RabbitMQ, schedule another\n        message to be delivered in PUBLISH_INTERVAL seconds.\n    \"\"\"\n    if self._stopping:\n        return\n    LOGGER.info('Scheduling next message for %0.1f seconds',\n                self.PUBLISH_INTERVAL)\n    self._connection.add_timeout(self.PUBLISH_INTERVAL,\n                                 self.publish_message)\n\ndef publish_message(self):     \n        if self._stopping:\n            return\n    message = {u'\u0645\u0641\u062a\u0627\u062d': u' \u0642\u064a\u0645\u0629',\n               u'\u952e': u'\u503c',\n               u'\u30ad\u30fc': u'\u5024'}\n    properties = pika.BasicProperties(app_id='example-publisher',\n                                      content_type='application/json',\n                                      headers=message)\n\n    self._channel.basic_publish(self.EXCHANGE, self.ROUTING_KEY,\n                                json.dumps(message, ensure_ascii=False),\n                                properties)\n\n    self._message_number += 1\n    self._deliveries.append(self._message_number)\n    LOGGER.info('Published message # %i', self._message_number)\n    self.schedule_next_message()\n\n```\n. ok, I'll run it later!\nAnd I think you should add your explanation about how to use pika into the official docs.\nThen the fresh guys, like me, can learn how to use pika quickly.\n. @vitaly-krugl \nYes, when cpu is busy, and in some other situations, the udp package will be lost.\nAnd if I want to use UDP things,  that means, I have to modify several functions in pika.\nUDP works well with pika.\nAnd I  tried to use TCP to do the work, but not works by now. I will try to figure it out later.\nI test _backpressure_detection _issue again, it works well by now.\nThanks for your help.\n. Using the latest code from github, it works ok, while the code from pypi is not work.\nThe former fix some bugs with connection close.\nDone!\n. on_open_error_callback is a registered callback function.\nWhen it is called, it will not continue the run the IO loop things.\nso the on_open_error_callback function should jump to run the reconnect function, then everything will be OK.\nI think the question should be closed.\n. ",
    "fscnick": "If I use add_callback_threadsafe() to send messages to another channel, is that OK?. @lukebakken Here is the snippet of the concept. If the source channel  (self.chan_src) consumes a message from queue, it would create a thread to handle the message. Emit the callback at the end of thread.\n```python\ndef thread_task( pika_manager, method_frame, msg):\n    logging.info(\"In thread. msg: %s\", msg)\n    res = int(msg) + 1\n    pika_manager.add_threadsafe_callback(method_frame, method_frame, str(res))\nclass PikaManager(object):\n     ...\n    def on_message(self, channel, method_frame, header_frame, body):\n        logging.info(\"Get msg %s\", body)\n        t = threading.Thread(\n            target = functools.partial(thread_task, self, method_frame, body))\n        t.start()\ndef send_result(self, method_frame, msg):\n    self.chan_dst.basic_publish(exchange='', routing_key='dst', body=msg)\n    logging.info(\"Send result.\")\n\n    if not NO_ACK:\n        self.chan_src.basic_ack(delivery_tag=method_frame.delivery_tag)\n        logging.info(\"Send ack.\")\n\ndef add_threadsafe_callback(self, cb, method_frame, msg):\n    self.conn.add_callback_threadsafe(\n        functools.partial(self.send_result, method_frame, msg)\n    )\n\n```\nTest 80000 messages with NO_ACK is True a few times.\nTest 80000 messages with NO_ACK is False a few times.\nThe entire example is here. Feel free to use it, if nothing goes wrong. . ",
    "deadtrickster": "@vitaly-krugl \nWell I'm not that good with Python, also having this wide adoption the last thing I want is to break stuff.\nI came up with this question while researching for my client implementation in Common Lisp. I just wanted to make sure I'm doing everything right. I wrote SSL layer and I ran test for 24 hrs without rehandshake. So actually the first question here is how to properly trigger rehandshake manually.\n. ",
    "txomon": "My mistake, it should be decoded in python3, I will add an if clause for datatype for compatibility\n. ",
    "irshad-qb": "My Progarm logic is : \n```\nwhile True:\n        result = channel.basic_get(queue=request_id, no_ack=False)\n        if  not result[0]:\n\n\n                break\n\n        if PacketQueueHandler.message_belongs_to_request(result[2],request_id):\n            channel.basic_ack(result[0].delivery_tag)\n           #handles the message\n        else:\n            channel.basic_nack(delivery_tag=result[0].delivery_tag)\n\n```\n. Thanks @vitaly-krugl : I tried multiple connection with pika-pool . But in that case also no luck. Is this also expected behaviour ? Is there any option for me to avoid / reduce new connection making burden ?\n. ",
    "appleyuchi": "I have already solved this ,the solution is:\nthreads = []\nfor num in nums:\n    threads.append(MyThread(center.request, num))\nfor thread in threads:\n    thread.start()\n    thread.join()\n\nDo NOT write code like this:\nfor thread in threads:\n    thread.start()\nfor thread in threads:\n    thread.join(). ",
    "jamestbrown": ":+1: \n. ",
    "dschep": "I agree that typically basic_consume should be used, I was using basic_get to keep my HTTP service stateless and just pop 1 message off the queue on HTTP GET(in hindsight, basic_qos + only ACKing when the message is fetched via HTTP would work better). Regardless, I'd expect basic_get functionality should be callable more than once or unless it's document that it can't.\n. @vitaly-krugl Basic.GetEmpty case fixed and added an acceptance test.\n. ",
    "Johnathon332": "1) I am able to get it running on the main process.\n2) I have taken your advice and instantiated the connection object within the process.\n3) I have turned on debugging however it hangs on this INFO       2016-03-08 08:43:07,991 __main__                       connect                              447 : Connecting to amqp://guest:guest@localhost:5672/%2F\nI am using pika version 0.10.0 and python2.7.  The operating system is running CentOS 6\n. ",
    "GayatriNittala": "Sorry If this should not be posted here under the issues section. Posting this again under rabbitmq users group. \n. Thanks a lot for the quick response.\nI shall add logging as suggested, but I did print the arguments passed to select.select\nprint self._fd_events[READ], self._fd_events[WRITE], self._fd_events[ERROR],self.get_next_deadline()\ngives :\nset([]) set([]) set([]) 2.99799990654\nPython documentation states that, on Windows, select call raises an error if all the first arguments were empty. I did not understand why are they empty in this case.\nWhat I actually would like to implement is :\nI would like to make connection attempts to RMQ server - If the server is down. I would like my script to retry the connection until the server is up.Though I could not see a parameter wherein I could retry forever, I thought of trying it atleast for a predefined number of attempts. For this, I used connection_attempts and retry_delay connection parameters.\nThat resulted in the above select error when RMQ server is down.\nAm I missing anything?\n. ",
    "tripleee": "In case you still need to use pika 0.10.0 (Debian Squeeze!) here's a workaround.  It took me a while to figure out where exactly to add the filter.\n```\nimport logging\nfrom pika.connection import LOGGER as pika_logger\nclass LoggerFilterNormalCloseIsFine (logging.Filter):\n    def filter (self, record):\n        return not record.getMessage().endswith('(200): Normal shutdown')\npika_logger.addFilter(LoggerFilterNormalCloseIsFine())\n```\n. ",
    "Giri-Chintala": "@vitaly-krugl i have use-case with similar requirements. \nWe are using AWS Lambda to put/get messages from RabbitMQ. Our security policy restricts us saving Certificates to disk on EC2 where Lambda is hosted. So, we are thinking about store certificates in S3 and retrieve certs in Lambda Function and pass cert as string in ssl_options.\n. @vitaly-krugl i will try FUSE. Thanks for great suggestions! \n. @vitaly-krugl\ni tried using fuse in my local, but unfortunately i am unable to use in my AWS Lambda as i don't get root access.\nhttps://www.stavros.io/posts/python-fuse-filesystem/\n. ",
    "dukhlov": "@vitaly-krugl, Hi. I little bit of context. I try to use single connection for whole application. For this case I created my own implementation of blocking connection which works over SelectConnection. Then I can create a lot of channels per thread and perform request via single connection. Processing ioloop I do in dedicated thread. It works but my blocking connection's basic_ack returns when data actually is not sent to the socket and it makes my connection not fully blocked :). Original Blocking connection wait until whole outbound buffer become empty, but in my case a couple of threads may write simultaneously and append buffer. And I can wait much longer than it is needed. \nBTW: I see that a lot of bugs fixed in master. When do you plan new release?\n. @vitaly-krugl, Thank you for information! I reviewed your implementation and see that you also implemented callbacks for message sending via socket. So I think implementing this by this patch is possibly small part to make you thread branch closer to upstream master. You ThreadingConnection implementation looks like a bit complicated to me. Because it looks like that we could use SelectConnection as is for sending messages from different threads (it have self.write_lock to handle correct frame ordering and appending outbound buffer) send and read operations are performed by ioloop only and should be executed in single thread. So it is not clear to me why we need a lot of proxies and gateways. It is probably because 10.0 release and current master code are different and it didn't work in described way previously. Anyway for now it is necessary for me to understand may this patch get merged soon? How long should I wait for the next pika release? because I need to plan my futher activities and work.\nProbably @gmr could provide more info.\n. @vitaly-krugl, Yes it is open source https://review.openstack.org/#/c/285239/44/oslo_messaging/_drivers/pika_driver/pika_connection.py\nI know that Select connection has problems with synchronisation, only writing is synchronised but there a lot of problems with connection state handling, here is a lot of logic like, if channel.is_open do something with channel. it is not synchronised and if ioloop thread detect connection problem and close it after if channel.is_open but before actual working with channel, errors like NoneType don't have some method could appears. In my patch ThreadSafeSelect connection is needed just for fixing such problems, BTW, a lot of them are already fixed in master. and Blocking connection is just tiny wrapper with waiting executing select connection method and waiting for callback.\nProbably this code is less mature then your but I think that it should be so simple. And select connection should be adapted to be able to work in multithreaded environment correctly.\nI am newcomer to pika, so if I am missing something please guide me:)\nThank you!\n. @vitaly-krugl,\nYes I saw it but thought that threading support will be completed soon. It is interesting why do you want to remove it? Is overhead for locking in so significant?\n. ",
    "abakhru": "i'm using latest master from this repo\n. ",
    "jaygorrell": "@vitaly-krugl \nFile \"ijm_subscriber.py\", line 332, in stop\nself._connection.ioloop.start()\nFile \"/home/andy/projects/python/ijm/.env/lib/python3.5/site-packages/tornado/ioloop.py\", line 748, in start\nraise RuntimeError(\"IOLoop is already running\")\nWith that said, the Tornado Consumer example's run() method does actually call IOLoop.start(), which I'd think is a mistake since it should be running already at that point.\n. I have thrown together a somewhat minimal test-case to show the problem. I know this has some extras but I tried to keep it relatively clean for investigation.\nhttps://github.com/jaygorrell/pika-tornado-leak\nedit I just stripped out the callback functionality to set up everything in Pika to show that this is strictly about the connection itself. If you'd like to see it before that change, just take a look at the previous commit.\n. If you can cross-reference this implementation with one you know works and point out any flaws, I'd owe you a big one. I've been banging my head on this for some time now. The part killing me is that I don't actually have to use Pika/RMQ messaging at all -- it's the HTTP requests over Tornado that seem to be leaking, but only once I've used a Pika Tornado connection.\nLet me know if the test-case project above causes any trouble. If you don't want to use Docker, you'd just need to adjust the hostname and set up the pythonpath for /lib. Everything else should be good.\n. @vitaly-krugl Please see the linked example project above: https://github.com/jaygorrell/pika-tornado-leak You can see the run method is already modified to not start the ioloop as you suggested. It simply connects with the Tornado adapter. And yes, this is a default ioloop instance and run is only called once. All of this is in the example project to show a repeatable test-case.\n@mleva-bnet Have you verified that the timeouts get removed after one hour? I thought for sure some of my tests went much longer but I may be mistaken.\n. In the real use-case, these callbacks were definitely set to an object and not None, so they weren't cleaned up. If that's not the case in the example project above then I've accidentally taken something out that's needed to show this. :(\n. You may be correct, it's been a bit since looking.\nIn that case, when you see the _timeouts get reset to None, does the memory in use get cleared up as well? I was definitely seeing a steady leak of memory over time, based on HTTP requests.\n. Awesome, nice catch. Still not updated in the latest AMI, either: https://aws.amazon.com/amazon-linux-ami/2016.03-packages/\n. ",
    "ziXiong": "It turned out be the problem of multi-process mode of uwsgi.  Uwsgi spawns workers that share file descriptors.  That is  worse than multi-threads, because even lock won't help.\nAny way, the solution is to set  lazy-apps = true in uwsgi ini file, which makes wokers not share file descriptors.\nWish this could help those who have the same problem with me.\n. ",
    "Akanoa": "You've saved my evening! many thanks :) @ziXiong . ",
    "ZMI-BalajiThiruvengadam": "any luck Gavin ?\n. ",
    "mleva-bnet": "i think were still looking into this a little deeper, but what i see is the following:\nwhen the pika consumer connects to the rabbit server, if no heartbeat interval is explicitly set on the consumer, it will get it from the rabbit server. in rabbit v3.5.5+, this interval is 60 seconds; in versions prior, it is 580 seconds. so the the pika consumer (at app start time) adds a timeout on the tornado ioloop for 580 seconds from now.\nas requests come in, tornado adds timeouts of 3600 seconds (1 hour) to the ioloop for idle connection timeouts. however, as the http requests are served, tornado removes these timeouts. removing a timeout actually just sets its callback to None, so the timeout still exists in tornados timeout list.\nfor every event on the ioloop, tornado checks the first item in the timeout list. if it has expired or its callback is set to None, it will pop it off the list and proceed to the next timeout in the list. otherwise, it stops there and will check again on the next ioloop event. so what we see happening is requests 'leak' for 580 seconds. then, once the heartbeat timeout expires, tornado removes all the idle request timeouts. at least, thats what i saw in my small tests. maybe there is something else going on when this happens at scale, but we are still looking here and elsewhere in our code.\nlet us know if you find anything @gmr \n. @jaygorrell as soon as the http requests are served, tornado removes their timeout, which just sets their callback to None (they are still in the timeout list, but callback=None). so once the pika heartbeat timeout fires (every ~580 seconds), tornado handles the pika heartbeat timeout, then proceeds down the ioloop timeout list removing/handling all timeouts that have either expired or have callback set to None. in my small scale tests, the request timeouts were removed as soon as the pika heartbeat timeout fired. you can test this by setting the heartbeat timeout on the consumer in your TornadoPikaPublisher init:\nself.amqp_url = amqp_url + '?heartbeat_interval=30'\nyoull see the timeouts list empty out every 30 seconds\n. the ioloop's _timeouts list contains _Timeout objects. _Timeout objects have deadline, callback, and tiebreaker attributes. the _timeouts list will definitely contain non-None objects, but those objects (in the case of the http request 1hr timeout) will have their callback attribute set to None once the request is served. are you sure youre not thinking about the _Timeout object?\n. in theory, they shouldve been cleaned up anyway since the callback was set to None, thus releasing the reference. True, the _Timeout objects would temporarily leak, but not the HTTP1Connection objects. i didnt look into the memory side of things much; just tried to understand why the timeouts list was growing. jg is delving deeper now.\n. jacob found the leak. it was in the old version (7.40) of libcurl available in the amazon linux ami\nhttps://github.com/curl/curl/issues/147\nhttps://aws.amazon.com/amazon-linux-ami/2015.09-packages/\nunless there are still suspicions, you can close this issue.\n. ",
    "holljen": "Version 0.10.0.\nAnd you're quite right about the time.sleep() it was just a quick way of triggering the problem with the example code :) Our own code uses timeouts and exhibits the same behaviour.\nThe pika.exceptions.IncompatibleProtocolError is being raised when rabbitmq is back up again.\n. @vitaly-krugl What exactly do you mean by a new connection instance? If you look a the example code above I create a new TornadoConnection for every connection retry which I considered to be a new connection instance, but I might be mistaken. \n. I'm having success with pika master. Error is similar, but at least I can catch the exception now and retry the connect. I'll do some more testing and come back with the results.\nI noticed however, that master is backwards incompatible with 0.10.0. The type parameter to Channel.exchange_declare was changed into exchange_type. Is this intentional?\n. With master I get both a on_open_error AND a connection_closed callback when this connection issue occurs. (Now using my production code). \nOther than that it seems to me that the difference between master and 0.10 is that an exception was being raised somewhere where I couldn't get at it while in Master the callbacks are called as expected.\n[I 160414 09:19:54 pikaclient:78] Starting PIKA connection\n[I 160414 09:19:54 base_connection:216] Connecting to 127.0.0.1:13372\n[E 160414 09:19:54 base_connection:348] Socket Error: 104\n[I 160414 09:19:54 connection:1891] Disconnected from RabbitMQ at localhost:13372 (-1): ConnectionResetError(104, 'Connection reset by peer')\n[W 160414 09:19:54 base_connection:309] Connection is closed but not stopping IOLoop\n[E 160414 09:19:54 connection:1919] Incompatible Protocol Versions\n[E 160414 09:19:54 connection:1954] Connection setup failed due to The protocol returned by the server is not supported: (-1, \"ConnectionResetError(104, 'Connection reset by peer')\")\n[W 160414 09:19:54 pikaclient:122] Pika open failed: (-1, \"ConnectionResetError(104, 'Connection reset by peer')\")\n[W 160414 09:19:54 pikaclient:175] PIKA Connection closed unexpectedly. Reply (-1) ConnectionResetError(104, 'Connection reset by peer')\n. I guess so. Thanks for already fixing this issue and rapid response! :+1: \n. ",
    "pfhayes": "great that's what I was looking for - thanks!\n. I notice that the BlockingChannel does not have an enter/exit - would that be an appropriate addition vs. closing the BlockingChannel manually?\n. Methods have been renamed and commit squashed\n. I'm not able to reproduce it but it seemed like this would be safest - I'm happy to remove this though\n. okay, I've removed this check\n. ",
    "madirey": "Any updates on this? Fix works for me.. ",
    "AdamMiltonBarker": "Thanks for the reply there are no errors in the logs as if it never made it to the broker. I have written a more detailed description here:\nhttps://groups.google.com/forum/#!topic/rabbitmq-users/u9hpB1ewDjQ\n. Ok thank you checking this all out again tonight.\n. ",
    "hasB4K": "Hi @vitaly-krugl,\nFor now, the hostname of the server certificate is not checked in both cases (like described by you in #464). Is there a way to do this easily with the current implementation of pika? Or should I create a class that inherit of BlockingConnection and do a workaround in a overrided method?\nIf a workaround is needed, I will be glad to suggest a pull request to a review when I will need to add SSL in pika (which is hopefully soon enough).\nI think that a even simpler workaround is to provide only your CA in ca_certs...\nMaybe you should add a notification regarding this issue in your examples :)\nRegards,\n. Because of #765, the tests, maybe, should be also made on Centos :)\n. ",
    "mishas": "Forgot to mention, very important note:\nWe have a context manager, where enter() creates the BlockingConnection and the channels, and the exit() calls BlockingConnection.stop(). It seems to fail only when there's an exception in the with block (but not always).\nOperating System where pika was used: Mac/Ubuntu14.04/Debian8\npika version (or whether you're using pika from pika master branch): latest 0.10.0\nrabbitmq version: 3.6.1 (Erlang 18.2)\nDEBUG-level log from pika for the affected period: attached in the end.\nrabbitmq log for the affected period: attached in the end.\nAbout hanging for quiet a while:\nSometimes it finishes normally after a minute or two,\nSometimes it fails on timeout of heartbeat (as in the log in the bottom),\nSometimes it doesn't finish until I kill it manually.\nAbout pipes being clogged: In my tests, I only send one message that makes the system stop (the handle function throws an intentional exception, which makes the with block exit, which calls stop). So - there should be no more than at most 1 message unprocessed.\nI tried canceling and then closing each connection one by one, and the same happens. It seems to get stuck on \"cancel\" of one of the connections (not necessarily the first one).\n(Actually the traceback above is from just such a test).\nIMPORTANT NOTE:\nI might have made a huge mistake! As I said, the code is very complex, and it uses threads in some areas. I just noticed that it actually does one of the consumes in a separate thread. It might be the case that if that thread still consumes, we get the problem.\nSorry for wasting your time :(.\nPika DEBUG log:\n\"\"\"\nI0426 06:33:15.443935 15553 pika/adapters/blocking_connection.py:616] Closing connection (200): Normal shutdown\nI0426 06:33:15.444247 15553 pika/adapters/blocking_connection.py:1330] Channel.close(200, Normal shutdown)\nD0426 06:33:15.444429 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': None}\nI0426 06:33:15.444664 15553 pika/channel.py:422] Channel.close(200, Normal shutdown)\nD0426 06:33:15.444913 15553 pika/channel.py:1130] Adding in on_synchronous_complete callback\nD0426 06:33:15.445104 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': None}\nD0426 06:33:15.445281 15553 pika/channel.py:1135] Adding passed in callback\nD0426 06:33:15.445432 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': None}\nD0426 06:33:15.446360 15553 pika/callback.py:220] Processing 1:Channel.CloseOk\nD0426 06:33:15.446560 15553 pika/callback.py:404] Processing use of oneshot callback\nD0426 06:33:15.446718 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:15.446897 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': None, 'calls': 0, 'one_shot': True, 'arguments': None}\nD0426 06:33:15.447071 15553 pika/callback.py:404] Processing use of oneshot callback\nD0426 06:33:15.447218 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:15.447385 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': None, 'calls': 0, 'one_shot': True, 'arguments': None}\nD0426 06:33:15.447595 15553 pika/callback.py:404] Processing use of oneshot callback\nD0426 06:33:15.447769 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:15.447985 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': None, 'calls': 0, 'one_shot': True, 'arguments': None}\nD0426 06:33:15.448245 15553 pika/callback.py:234] Calling > for \"1:Channel.CloseOk\"\nD0426 06:33:15.448503 15553 pika/callback.py:234] Calling > for \"1:Channel.CloseOk\"\nD0426 06:33:15.448688 15553 pika/channel.py:1088] 0 blocked frames\nD0426 06:33:15.448974 15553 pika/callback.py:234] Calling > for \"1:Channel.CloseOk\"\nD0426 06:33:15.449196 15553 pika/callback.py:220] Processing 1:_on_channel_cleanup\nD0426 06:33:15.449373 15553 pika/callback.py:404] Processing use of oneshot callback\nD0426 06:33:15.449587 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:15.449807 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': , 'calls': 0, 'one_shot': True, 'arguments': None}\nD0426 06:33:15.450010 15553 pika/callback.py:234] Calling > for \"1:_on_channel_cleanup\"\nD0426 06:33:15.450160 15553 pika/connection.py:1203] Removed channel 1\nD0426 06:33:15.450367 15553 pika/callback.py:181] Clearing out '1' from the stack\nI0426 06:33:20.450986 15553 pika/adapters/blocking_connection.py:1330] Channel.close(200, Normal shutdown)\nD0426 06:33:20.451329 15553 pika/adapters/blocking_connection.py:1288] Cancelling 1 consumers\nD0426 06:33:20.451611 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': None}\nD0426 06:33:20.451971 15553 pika/channel.py:1130] Adding in on_synchronous_complete callback\nD0426 06:33:20.452234 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag2.a2b3fcbb38d0445fb0f6dfbb849d06a6'}}\nD0426 06:33:20.452481 15553 pika/channel.py:1135] Adding passed in callback\nD0426 06:33:20.452662 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag2.a2b3fcbb38d0445fb0f6dfbb849d06a6'}}\nD0426 06:33:20.453696 15553 pika/callback.py:220] Processing 2:Basic.CancelOk\nD0426 06:33:20.453924 15553 pika/callback.py:404] Processing use of oneshot callback \nD0426 06:33:20.454105 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:20.454314 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': None, 'calls': 0, 'one_shot': True, 'arguments': None}\nD0426 06:33:20.454524 15553 pika/callback.py:404] Processing use of oneshot callback \nD0426 06:33:20.454696 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:20.454895 15553 pika/callback.py:350] Comparing {'consumer_tag': 'ctag2.a2b3fcbb38d0445fb0f6dfbb849d06a6'} to {'consumer_tag': 'ctag2.a2b3fcbb38d0445fb0f6dfbb849d06a6'}\nD0426 06:33:20.455216 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': None, 'calls': 0, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag2.a2b3fcbb38d0445fb0f6dfbb849d06a6'}}\nD0426 06:33:20.455432 15553 pika/callback.py:404] Processing use of oneshot callback\nD0426 06:33:20.455602 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:20.455795 15553 pika/callback.py:350] Comparing {'consumer_tag': 'ctag2.a2b3fcbb38d0445fb0f6dfbb849d06a6'} to {'consumer_tag': 'ctag2.a2b3fcbb38d0445fb0f6dfbb849d06a6'}\nD0426 06:33:20.455980 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': None, 'calls': 0, 'one_shot': True, 'arguments': {'consumer_tag': 'ctag2.a2b3fcbb38d0445fb0f6dfbb849d06a6'}}\nD0426 06:33:20.456148 15553 pika/callback.py:234] Calling > for \"2:Basic.CancelOk\"\nD0426 06:33:20.456301 15553 pika/callback.py:234] Calling > for \"2:Basic.CancelOk\"\nD0426 06:33:20.456448 15553 pika/channel.py:1088] 0 blocked frames\nD0426 06:33:20.456596 15553 pika/callback.py:234] Calling > for \"2:Basic.CancelOk\"\nD0426 06:33:25.458982 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': None}\nI0426 06:33:25.459490 15553 pika/channel.py:422] Channel.close(200, Normal shutdown)\nD0426 06:33:25.459808 15553 pika/channel.py:1130] Adding in on_synchronous_complete callback\nD0426 06:33:25.460108 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': None}\nD0426 06:33:25.460507 15553 pika/channel.py:1135] Adding passed in callback\nD0426 06:33:25.460766 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': None}\nD0426 06:33:25.461918 15553 pika/callback.py:220] Processing 2:Channel.CloseOk\nD0426 06:33:25.462225 15553 pika/callback.py:404] Processing use of oneshot callback\nD0426 06:33:25.462506 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:25.462714 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': None, 'calls': 0, 'one_shot': True, 'arguments': None}\nD0426 06:33:25.462896 15553 pika/callback.py:404] Processing use of oneshot callback\nD0426 06:33:25.463044 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:25.463214 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': None, 'calls': 0, 'one_shot': True, 'arguments': None}\nD0426 06:33:25.463384 15553 pika/callback.py:404] Processing use of oneshot callback\nD0426 06:33:25.463526 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:25.463692 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': None, 'calls': 0, 'one_shot': True, 'arguments': None}\nD0426 06:33:25.463856 15553 pika/callback.py:234] Calling > for \"2:Channel.CloseOk\"\nD0426 06:33:25.464008 15553 pika/callback.py:234] Calling > for \"2:Channel.CloseOk\"\nD0426 06:33:25.464154 15553 pika/channel.py:1088] 0 blocked frames\nD0426 06:33:25.464302 15553 pika/callback.py:234] Calling > for \"2:Channel.CloseOk\"\nD0426 06:33:25.464479 15553 pika/callback.py:220] Processing 2:_on_channel_cleanup\nD0426 06:33:25.464627 15553 pika/callback.py:404] Processing use of oneshot callback\nD0426 06:33:25.464767 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:25.464916 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': , 'calls': 0, 'one_shot': True, 'arguments': None}\nD0426 06:33:25.465069 15553 pika/callback.py:234] Calling > for \"2:_on_channel_cleanup\"\nD0426 06:33:25.465206 15553 pika/connection.py:1203] Removed channel 2\nD0426 06:33:25.465352 15553 pika/callback.py:181] Clearing out '2' from the stack\nI0426 06:33:30.466241 15553 pika/connection.py:731] Closing connection (200): Normal shutdown\nD0426 06:33:30.466788 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': None}\nD0426 06:33:30.470046 15553 pika/callback.py:220] Processing 0:Connection.CloseOk\nD0426 06:33:30.470364 15553 pika/callback.py:404] Processing use of oneshot callback\nD0426 06:33:30.470584 15553 pika/callback.py:406] 0 registered uses left\nD0426 06:33:30.470823 15553 pika/callback.py:269] Removing callback #0: {'callback': >, 'only': None, 'calls': 0, 'one_shot': True, 'arguments': None}\nD0426 06:33:30.471062 15553 pika/callback.py:234] Calling > for \"0:Connection.CloseOk\"\nD0426 06:33:30.471275 15553 pika/heartbeat.py:108] Removing timeout for next heartbeat interval\nD0426 06:33:30.471660 15553 pika/callback.py:164] Added: {'callback': >, 'only': None, 'calls': 1, 'one_shot': True, 'arguments': None}\nW0426 06:33:30.471938 15553 pika/connection.py:1360] Disconnected from RabbitMQ at 192.168.99.100:5672 (200): Normal shutdown\nD0426 06:33:30.472259 15553 pika/callback.py:220] Processing 0:_on_connection_closed\nD0426 06:33:30.472552 15553 pika/callback.py:234] Calling > for \"0:_on_connection_closed\"\nI0426 06:33:30.472954 15553 pika/adapters/blocking_connection.py:428] Connection closed; result=BlockingConnection__OnClosedArgs(connection=, reason_code=200, reason_text='Normal shutdown')\n\"\"\"\nTraceback after when I did Ctrl+C after ~10 minutes:\n\"\"\"\nTraceback (most recent call last):\n  [ ... ]\n  File \".../rmq/connector.py\", line 68, in close\n    self._connection.close()\n  File \"pika/adapters/blocking_connection.py\", line 629, in close\n    self._flush_output(self._closed_result.is_ready)\n  File \"pika/adapters/blocking_connection.py\", line 410, in _flush_output\n    self._impl.ioloop.poll()\n  File \"pika/adapters/select_connection.py\", line 515, in poll\n    self.get_next_deadline())\nKeyboardInterrupt\n\"\"\"\nRabbitMQ log:\n\"\"\"\n=INFO REPORT==== 26-Apr-2016::06:33:10 ===\naccepting AMQP connection <0.2404.0> (192.168.99.1:51158 -> 172.22.0.2:5672)\n=INFO REPORT==== 26-Apr-2016::06:33:13 ===\naccepting AMQP connection <0.2457.0> (192.168.99.1:51159 -> 172.22.0.2:5672)\n=INFO REPORT==== 26-Apr-2016::06:33:13 ===\nclosing AMQP connection <0.2457.0> (192.168.99.1:51159 -> 172.22.0.2:5672)\n=INFO REPORT==== 26-Apr-2016::06:33:28 ===\nclosing AMQP connection <0.2404.0> (192.168.99.1:51158 -> 172.22.0.2:5672)\n\"\"\"\nThanks again.\n. ",
    "Silver90": "Ok I followed the steps that you have requested:\n- Rabbitmq 3.6.1, ubuntu 14.04.4, pika 0.10.0\n- Now use BlockingConnection\nAnd this is the result of logging:\n```\n=ERROR REPORT==== 4-May-2016::09:03:06 ===\nclosing AMQP connection <0.395.0> (172.18.0.3:49855 -> 172.18.0.4:5672):{writer,send_failed,{error,timeout}}\nERROR:pika.adapters.base_connection:Error event 25, None\nCRITICAL:pika.adapters.base_connection:Tried to handle an error where no error existed\nDEBUG:pika.heartbeat:Received 2 heartbeat frames, sent 2\nDEBUG:pika.heartbeat:Sending heartbeat frame\nERROR:pika.adapters.base_connection:Socket Error: 104\nDEBUG:pika.heartbeat:Removing timeout for next heartbeat interval\nWARNING:pika.adapters.base_connection:Socket closed when connection was open\nDEBUG:pika.callback:Added: {'only': None, 'calls': 1, 'arguments': None, 'one_shot': True, 'callback': >}\nWARNING:pika.connection:Disconnected from RabbitMQ at olorabbitmq:5672 (0): Not specified\nDEBUG:pika.callback:Processing 0:_on_connection_closed\nDEBUG:pika.callback:Calling > for \"0:_on_connection_closed\"\nCRITICAL:pika.adapters.blocking_connection:Connection close detected\nTraceback (most recent call last):\nFile \"code/server.py\", line 201, in \nchannel.start_consuming()\nFile \"/usr/local/lib/python3.4/site-packages/pika/adapters/blocking_connection.py\", line 1681, in start_consuming\nself.connection.process_data_events(time_limit=None)\nFile \"/usr/local/lib/python3.4/site-packages/pika/adapters/blocking_connection.py\", line 647, in process_data_events\nself._flush_output(common_terminator)\nFile \"/usr/local/lib/python3.4/site-packages/pika/adapters/blocking_connection.py\", line 426, in _flush_output\nraise exceptions.ConnectionClosed()\npika.exceptions.ConnectionClosed\n```\n. Yes, sorry if I answered late. I have partially solved by setting heartbeat to 0 (Consumer), restart always on docker-compose file and set message durable (from client). \nHowever, this problem occurs only if I use vagrant in ubuntu does not happen.\n. ",
    "SamuelRamond": "Great, this worked using channel.confirm_delivery.\nFor the record do you have any explanation on why it failed silently ? From a rapid point of view on tcp networking connection i would have assumed that the socket.sendall should have failed in the first place.\n. ",
    "corpulent": "I resolved this by stopping the logging.\n. ",
    "Zhomart": "I've created a class RpcServer\n```\nclass RpcServer:\n  def init(self):\n    self.connection = pika.BlockingConnection(...)\n    self.channel = self.connection.channel()\ndef add(self, name, callable):\n    pass\ndef run(self):\n    \"This method blocks thread\"\n    self.channel.start_consuming()\ndef stop(self):\n    self.channel.stop_consuming()\n    self.channel.close()\n```\nNow I want to test it:\n```\nclass TestRpcServer:\n  def setUp(self):\n    self.rpc_server = RpcServer()\n    self.thread = Thread(target=self.rpc_server.run)\n    self.thread.start()\ndef tearDown(self):\n    self.rpc_server.stop()\n    self.thread.join() # make sure that thread is stopped\ndef test_something(self):\n    assert True\n```\nWhen I run test suite, it goes first setUp, then test_something and then finally tearDown.\nIt correctly removes consumers, I've debugged source code.\nBut self.thread is not stopped, because the loop inside start_consuming is not finished.\nstop_consuming correctly removes consumers from self._consumer_infos.\nThen I opened local source code of pika from my ~/.virtualenv and changed following line:\n```\nself.connection.process_data_events(time_limit=None)\nto\nself.connection.process_data_events(time_limit=0.5) # worked well\n```\nSecond line worked and my test suite finished.\nI really don't know how amqp works, but it looks like if time_limit is None, then process_data_events waits data from rabbitmq server forever.\nBut when I'm cancelling all subscribers, it doesn't get any data from server and\nthat is why process_data_events waits for data forever.\n. Thank you @vitaly-krugl . Your suggestion works perfectly. I understood that problems is I'm running start_consuming from one thread and stop_consuming from another.\nI'll close this issue.\n. ",
    "daoiqi": "I have some  long time I/O. (more than 10 min ) (compute large data)\nWhen recive data, consumer do something, but it is long time task.  When  consumer has done, then ack to MQ, and publish new data.\nIf all in one thread, consumer doing task  with block, then the heartbeats not any thread to deal with.\nOr I'm using ack mechanism error?   \nwhich is  better?\n1. revice data then ack to MQ  \n2. do long time.         ----- if server is crash, heartbeats is not significance.\n3. publish new data.\nor\n1. recive data\n2. do long time and hearbeat check    --- if crash, mq will resend message\n3. publish new data and ack to MQ.\nCan I start thread to do I/O task, and another thread do BlockingConnection.process_data_events? \nOr , Should I use LibevConnection or SelectConnection?\n. kombu.connection.Connection.heartbeat_check\nDoes the kombu solve the heartbeat ?\n. ",
    "tcwalther": "@vitaly-krugl Thanks a lot for your explanations, this is very helpful. Did you happen to make any progress on that threaded prototype you mentioned?\n. I switched away from Kombu because of too much magic. With amqpy backend, it seems to be able to support heartbeats, but not with librabbitmq if I remember correctly. Whether they then happen in a background thread, I don't know.\nFor now I've solved this by calling process_data_events regularly as suggested, and it works perfectly fine. I kind of like this approach the more I think about it; if you keep everything in one thread you don't have to worry about a situation where your worker thread died or got stuck but your messaging thread still acknowledges hearbeats.\n. ",
    "barroca": "Hello, is there any update regarding this issue? We started getting this error this week, but we didn't change anything except for the environment we run our code.\nThanks. moving from time.sleep() to the busy wait:\n```\ndef _safe_sleep(duration):\n    deadline = time() + duration\n    time_limit = duration\nwhile True:\n    time_limit = deadline - time()\n\n    if time_limit <= 0:\n        break\n\n```\nworked for me :). ",
    "jmartinez-jobsity": "I'm also having this issue, even with the prefetch_count=1 solution @pat1 suggested. I'm not using any sleeps, all the time is being spent in actual message processing, which might include heavy SQL calls and a lot of requests calls among others. ",
    "rogamba": "@pat1  Been struggling with this for days. This solved my problem, you're the man! . ",
    "NavalKK": "@lukebakken I'm also having this issue, even with the prefetch_count=1 options. In my rabbitmq consumer actual parsing of a rabbitmq message involves many mongo queries, So it may take around 10-15 minutes, but i'm using heartbeat_interval:90. Is there any chance that i can use heartbeat interval value other than 0.\nRabbitmq log look like this.- \n`=INFO REPORT==== 26-Sep-2017::13:14:32 ===\naccepting AMQP connection <0.889.0> (127.0.0.1:55214 -> 127.0.0.1:5672)\n=INFO REPORT==== 26-Sep-2017::13:14:32 ===\nconnection <0.889.0> (127.0.0.1:55214 -> 127.0.0.1:5672): user 'guest' authenticated and granted access to vhost '/'\n=INFO REPORT==== 26-Sep-2017::13:14:32 ===\nconnection <0.892.0> (127.0.0.1:55216 -> 127.0.0.1:5672): user 'guest' authenticated and granted access to vhost '/'\n=ERROR REPORT==== 26-Sep-2017::13:19:02 ===\nclosing AMQP connection <0.892.0> (127.0.0.1:55216 -> 127.0.0.1:5672):\nmissed heartbeats from client, timeout: 90s`\nCan you please suggest what i'm doing wrong. code for the consumer is same as of @pat1.\ncode link\n. ",
    "scherma": "I am also having this issue, and I don't think it is anything to do with having a wait before message acknowledgement (receipt to ack is less than 1 sec, however the task after the ack does take 3-4 minutes).\nMy relevant functions are here and here. Using prefetch_count=1 makes no difference. Calling stop_consuming prior to running the task and start_consuming after makes no difference.. ",
    "TimZaman": "Although @lukebakken 's response is accurate, we have not addressed why the timeout settings do not seem to be working with anyone.. ",
    "adamchainz": "Better late than never!. ",
    "antoineleclair": "Same problem here. I'm not sure if it's a bug or if I'm missing something.\nI ended up with something like this to reconnect on any error.\n``` python\nclass Producer:\n    \"\"\"Message producer.\nThis class is used to send message to RabbitMQ.\n\n\"\"\"\ndef __init__(self):\n    log.debug('Initializting MQ producer')\n    self._connect()\n\ndef _connect(self):\n    params = pika.URLParameters(config.get('rabbitmq.url'))\n    self.connection = pika.BlockingConnection(params)\n    self.channel = self.connection.channel()\n    common.declare(self.channel)\n\ndef publish(self, queue, msg):\n    \"\"\"Publishes a message to RabbitMQ.\n\n    This is not thread safe.\n\n    \"\"\"\n    log.debug('Publishing message to queue %s: %s', queue, msg)\n    try:\n        self._publish(queue, msg)\n    except:\n        log.info('Got exception when trying to publish MQ message, '\n                 'reconnecting')\n        self._connect()\n        self._publish(queue, msg)\n\ndef _publish(self, queue, msg):\n    self.channel.basic_publish(exchange='',\n                               routing_key=queue,\n                               body=msg,\n                               properties=pika.BasicProperties(\n                                  delivery_mode = 2, # persistent\n                               ))\n\ndef close(self):\n    try:\n        self.connection.close()\n    except pika.exceptions.ConnectionClosed:\n        pass\n\n```\nI don't mind reconnecting on errors, but it would be great to not have to silent all errors like this. I'm not sure if I missed something (because the doc is unclear) or if it's an actual issue.\n. Thanks @vitaly-krugl, that did it. Much cleaner with BlockingConnection.add_timeout. Thanks a lot!\nAlso, if my opinion can be useful, I would have expected, from an API design perspective, BlockingChannel.stop_consuming to be signal-safe and to work even when no messages are being received.\n. ",
    "reallistic": "The latest code still has some bugs so I am closing this.\n. ",
    "Serafean": "IMHO not only adding the example, but also modifying the existing one to discourage the assumption that consume() always yields a 3-tuple.\n. ",
    "dunk": "This just bit me. Yes, it is in some production code which is a pity because clearly the correct way to do this is to yield (None, None, None). +1 for making that the default (at some point)\n. While it might require changes in all clients it's only going to be a change that everyone wants to do - i.e. to remove that mandatory special-casing code.\n. ",
    "mathieulongtin": "Instead of special casing the iterator, why not just stop the consumer when it times out?\nThe above code would become:\nwhile True:\n    for method, properties, body in channel.consume('queue', inactivity_timeout=5):\n        print body\n        channel.basic_ack(method.delivery_tag)\n    print \"got inactivity timeout\"\n. ",
    "LPiner": "+1 @mathieulongtin \nRaising an exception for this is just strange.. ",
    "joshpurvis": "Nope, sorry. I ended up moving all instances off of AWS us-east-1a to a less noisy datacenter and added alerts to detect when queues silently drop (using sensu). I've been experiencing the problem much less frequently, and sensu alerts me when it happens.. @vitaly-krugl \nI'm slightly confused by your last message. Does it imply that the consumer can't be fixed as you did for the publisher examples?\nI actually ran into this problem again earlier today. We have some long running python processes that use pika (in a consumer style setup). The queues for a number of these vanished from the Rabbitmq panel, but the processes are still hanging without errors. I suspect this was caused by a network glitch, but oddly not all of the processes failed. If I restart these, they'll be fine.\nAs an aside: I have AMQP heartbeats set to 60 and TCP keep alive is 7200. It's been longer than 2 hours and they're still hanging. So these settings don't seem to help the situation.. ",
    "mlmarius": "I second this. How can it be fixed when using the async consumer example ?. I would like to close this issue. When specifying a number of reconnect attempts (amqp://guest:guest@127.0.0.1:5672/%2F?connection_attempts=3600&heartbeat_interval=30&retry_delay=10 )pika seems to work as expected. However, if during the reconnection attempting period i restart the rabbitmq server, pika fails with an exception and the program stops. I will open a new issue for this.. I have upgraded to python 2.7.13 and refreshed my rabbitmq server image, reinstalled pika from the github repo and then:\n- started the docker container running rabbitmq\n- manually created the \"test\" queue on the default exchange\n- ran the repo.py.txt script. The script connected and started consuming. Dispatching messages to the queue worked as expected\n- shut down the docker container running rabbitmq. this is important. Just closing a port to the server in the firewall will not trigger this problem. The reconnection attempt needs to happen while the server is actually starting.\n- the script detected this and started reconnection attempts\n- after some seconds i restarted the docker container running the rabbitmq server.\n- the blow-up happens every time and the error looks like this:\nstacktrace.txt\n. Yes, the app is using more than one thread. It is using one thread for the publisher and one for the consumer, each with its own connection. But I think I may have stumbled upon what was wrong with my app. For my publisher I was using the example from here: \nhttps://github.com/pika/pika/pull/710/files\nIn that publisher example the publish is performed like so:\n```\n       properties = pika.BasicProperties(app_id='example-publisher',\n                                          content_type='application/json',\n                                          headers=message)\n    self._channel.basic_publish(self.EXCHANGE, self.ROUTING_KEY,\n                                json.dumps(message, ensure_ascii=False),\n                                properties)\n    self._message_number += 1\n    self._deliveries.append(self._message_number)\n    LOGGER.info('Published message # %i', self._message_number)\n    self.schedule_next_message()\n\n```\nI think the problem is in the example, here: \nproperties = pika.BasicProperties(app_id='example-publisher',\n                                          content_type='application/json',\n                                          headers=message)\nI think headers=message makes the thread die when I attempt to publish big messages. After removing the heades=message (what should go in headers anyway?) part the publisher works as expected, I can publish big messages to RabbitMQ and the thread is not killed anymore.\n. ",
    "osteenbergen": "I have the same issue but using a different test method. Running netcat (nc -k -l 5672) and pointing pika to it results in a hanging process during connection. Stack trace is the same.\nSetting timeouts doesn't work as the connection is established, but the server doesn't respond.\n. ",
    "mnoky": "Thanks Vitaly. I tried out Pika from the master and set blocked_connection_timeout. It worked like a charm! Debug messages indicate that the server does indeed send back a reason why the operation failed:\nWARNING:pika.connection:Received <METHOD(['channel_number=0', 'frame_type=1', \"method=<Connection.Blocked(['reason=low on disk'])>\"])> from broker\nOne followup question: Is an official new release due anytime soon?\n. Do you have a sense if the master branch is fairly stable? If so, it might be worthwhile for us to use that. Thanks again!\n. ",
    "fengyiyingdong": "Thanks!  My problem have been sorted.\nMy server certificate (the one in ca/server/server.crt) uses md5WithRSAEncryption as the signature algorithm. That is going to be a problem. open ssl  states that \"Verification of signatures using the MD5 hash algorithm is disabled in Red Hat Enterprise Linux 7 due to insufficient strength of this algorithm. Always use strong algorithms such as SHA256.\"\n. ",
    "JonathanBrannock": "From the RabbitMQ Management web page, all of the connections are still there, but they have no channels associated with them. \n. You are correct, I am trying to figure out a better way to do this. I am only using the channel to send the ACK/NACK for the message. For everything else the child opens its own channel. I am hesitant to ACK the message before the worker is complete. \nThis may be the cause, but this only happens every few days to several times a day at the most. So  its not consistently reproducible.\nI started down this rabbit hole because I occasionally have processes that take longer than the rabbitmq 60 second heartbeat to finish. \n. ",
    "Michael754267513": "thanks a lot for your suggestions !  i track this agent!\n. ",
    "nicbou": "When you find the solution to your problem, don't leave everyone else hanging, Michael.\nhttps://xkcd.com/979/. ",
    "guysoft": "In what scope does the callback in BlockingConnection.add_timeout run? There is something I saw it mentioned in the documentation but I see no example.\n\nNOTE: the timer callbacks are dispatched only in the scope of specially-designated methods: see BlockingConnection.process_data_events and BlockingChannel.start_consuming.\n\nI have the BlockingConnection.add_timeout running as part of a class, and I want to access attributes of that class (something like RpcServer.is_running() ? ). But it seems like the variables do not update in this scope.\n. Ok, I fixed my issue (using basic_consume and handling it myself). But basically the issue was this:\nWhen running the add_timeout callback you can't interact with part of the python variables, even though its same thread.\nI can't manage to rip example code, I might have time to solve this later on. Thanks for the reply though and well done!. ",
    "hodgestar": "Thank you for the link to the documentation, but I have already read it. As described in the docs you link to, I have a connection per thread and would like to notify the connection in the thread that it needs to wake up so that it can publish an event. Pika already handles this case when stopping a connection in a thread.\n. Correct.\n. ",
    "kirkdsayre": "@vitaly-krugl, it looks like I may need to pull Pika directly from GitHub rather than using the Ubuntu 14.04 python-pika package. The Pika version provided by python-pika does not have the _PollerBase class, _uninit_poller() method, etc. I'll get the most recent Pika version from GitHub and run the following test code, which I expect will show the FD problem in the earlier version is fixed.\n`import pika \nimport psutil\nproc = psutil.Process()\nprint \"# Open Files: \" + str(proc.get_num_fds())\nconnection1 = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\nconnection1.close()\nprint \"# Open Files: \" + str(proc.get_num_fds())`\n. Ok, I cloned the most recent Pika code, ran the above code snippet, and got the expected answer:\n`\nOpen Files: 7\nOpen Files: 7\n`\nIt looks like this issue was a false alarm. I'll close the issue.\n. ",
    "ricoxor": "Here is the logs before the error messages : \n2016-10-09 20:43:49 [pika.adapters.base_connection] ERROR: Socket Error: 104\n2016-10-09 20:43:49 [pika.adapters.base_connection] WARNING: Socket closed when connection was open\n2016-10-09 20:43:49 [pika.connection] WARNING: Disconnected from RabbitMQ at localhost:5672 (0): Not specified\nThank's for your help\n. Do you know why the connection disconnects ?\nAnd how fix that ?\n. ",
    "anthok": "The problem doesn't start happening until there is over 2000 messages in total. We are just using the async consumer to process multiple queues (we don't do anything else for threading).\n. We ended up redoing our queuing system and it all works great now. Thanks for the help!. ",
    "lexdene": "@vitaly-krugl, I created this pr because I was confused with the hard code sock_type and sock_proto param inside _create_tcp_connection_socket function.\n. @vitaly-krugl, I pushed new commits to get rid of the default arg values and add docstring.\n. ",
    "NicolasDuran": "It was a problem on my side, a bad network configuration on my cloud provider.\n. ",
    "thasler": "@NicolasDuran can you provide more details about the network configuration issue? which cloud provider do you use?. ",
    "nicgirault": "In my case here is the logs I get:\n[ERROR] Error event 25, None\n[CRITICAL] Tried to handle an error where no error existed\n[ERROR] Socket Error: 104\n[INFO] Disconnected from RabbitMQ at blablabla.com:5672 (-1): ConnectionResetError(104, 'Connection reset by peer')\n[ERROR] Connection close detected; result=BlockingConnection__OnClosedArgs(connection=<SelectConnection CLOSED socket=None params=<ConnectionParameters host=blablabla.com port=5672 virtual_host=/ ssl=False>>, reason_code=-1, reason_text=\"ConnectionResetError(104, 'Connection reset by peer')\")\nTraceback (most recent call last):\n  File \"/xxx/worker.py\", line 91, in <module>\n    channel.start_consuming()\n  File \"/xxx/env/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 1780, in start_consuming\n    self.connection.process_data_events(time_limit=None)\n  File \"/xxx/env/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 707, in process_data_events\n    self._flush_output(common_terminator)\n  File \"/xxx/env/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 474, in _flush_output\n    result.reason_text)\npika.exceptions.ConnectionClosed: (-1, \"ConnectionResetError(104, 'Connection reset by peer')\")\nI'm using AWS with 2 EC2 instances. The worker is not on the same instance than the RabbitMQ server. @NicolasDuran I would be interested to understand what king of network configuration I should write. I didn't do anything specific. @lukebakken yes I get this kind of logs. I'm not sure to understand what it means. Are the sides telling that the other side closed unexpectedly?. @lukebakken no, there is no load balancer.\nMy setup is the following:\n\n\nI have a first EC2 instance running RabbitMQ in a docker container (https://hub.docker.com/_/rabbitmq/) without any customisation (other than custom username/password). Port 5672 is open on this instance.\n\n\non the same instance is running a Flask application. A HTTP POST on a route executes this code publishing to rabbitmq:\n\n\n```python\nconnection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost', credentials=RABBITMQ_CREDENTIALS))\nchannel = connection.channel()\nchannel.queue_declare(queue=COMPUTE_THETAS_QUEUE)\nchannel.basic_publish(exchange='', routing_key=COMPUTE_THETAS_QUEUE, body=str(student_id))\n```\nI just noticed here that I never close the connexion so a new connexion was created but not closed each time this code was triggered. This might be the cause of my issue (I'm following this lead right now and will let you know if it fix my issue).\n\nOn another EC2 instance, supervisor is handling a python script:\n\npython\nconnection = pika.BlockingConnection(\n    pika.ConnectionParameters(\n        socket_timeout=10,\n        host=RABBITMQ_HOST, # this host is a public address to the other EC2 instance\n        credentials=RABBITMQ_CREDENTIALS\n    )\n)\nchannel = connection.channel()\nchannel.queue_declare(queue=COMPUTE_THETAS_QUEUE)\nchannel.basic_consume(compute_thetas_receiver, queue=COMPUTE_THETAS_QUEUE, no_ack=True)\nlogging.info('[*] compute_thetas waiting for messages.')\nchannel.start_consuming()\nconnection.close(). I don't get the error since I close the connexion of the publisher :-). Thanks for your reactivity.. ",
    "wood-j": "maybe you should use self.connection if in a class, so that could prevent connection got cleaned by gc.. ",
    "danslapman": "Really need blocked_connection_timeout support! . ",
    "hassonofer": "Can anyone please comment on this. ",
    "dacox": "I am also curious about this - I see there have been some bug fixes and was wondering if there is a reason that no new release has been uploaded to PyPi.. ",
    "ecederstrand": "Ping? We're hitting #659 and would be glad for a minor release :-). ",
    "eldhart": "Also looking forward to a new release, glad to see I'm not alone :). ",
    "bobintornado": "pointing to master branch in my requirements.txt already and can't get my stuff working without socket_timeout. (my network is just a bit slow). ",
    "chaozmc": "Hi,\ni tried with this example (docs/examples/tls_server_uathentication.rst) above, but I also stuck with pika just quitting with the exception\nTraceback (most recent call last):\n  File \"tls_example.py\", line 13, in \n    conn = pika.BlockingConnection(cp)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 339, in init\n    self._process_io_for_connection_setup()\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 374, in _process_io_for_connection_setup\n    self._open_error_result.is_ready)\n  File \"/usr/local/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 395, in _flush_output\n    raise exceptions.ConnectionClosed()\npika.exceptions.ConnectionClosed\nI just had to add port=5671 in the pika.ConnectionParameters, otherwise it tried to connect on 5672 which is my plain port.\ni've tried with python2 and python3 - same. \nAlso i tried out \"openssl s_client -connect localhost:5671\" which works fine. \nLooking on the server with \"netstat -apn | grep :5671\" I can see that pika is opening the connection and leaves it then in the CLOSE_WAIT state when crashing.\nDoes anybody have an idea? I'm trying since 4 hours and no progress :(\nThank you!. ",
    "masell": "This should fix it, right?\nhttps://github.com/pika/pika/pull/1011. @lukebakken Added test case, and encode also (Needed for test.. :) ). @lukebakken The coverage job is failing due to some credentials stuff... I can't fix that i assume?. @vitaly-krugl reverted the tbl_encoded.. Please note that in python 2.X, a byte (type X) is a str as they do not distinguish the types. while in 3.X its as bytes b\"\". This the reason for checking if its PY3 in the test.\n```\nPython 2.7.14 (default, Sep 23 2017, 22:06:14) \n[GCC 7.2.0] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nb\"a\" == u\"a\"\nTrue\n```\n\n\n\n```\nPython 3.6.3 (default, Oct  3 2017, 21:45:48) \n[GCC 7.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nb\"a\" == u\"a\"\nFalse\n```. @hairyhum Same test, different fixture for 2.7\n\n\n\nhttps://github.com/pika/pika/pull/1011/files#diff-6d92c2a692fc77fd16dcd1bc9b922392R35\nMeans,that in 2.7 i do not change behavior, and type X will not do anything, as the type \"S\" already do it.\n. @hairyhum \nAdded new test for decoding.\nhttps://github.com/pika/pika/pull/1011/commits/fca2ec9b6a04273a3a1d7102d2d3c36d100b7281. @vitaly-krugl \n\"S\" is string.\n\"x\" is byte[].\n\"S\" is byte[] on the wire, that is unicode encoded. Decoded would be str() in python3/2, System.String in java.\n\"x\" is byte[] with no encoding, a data blob. Can even be a JPEG if you wish. Decoded would be a bytes() in python3, a byte[] in java. Python2 lacks a byte representation AFAIK, so its the same as str in python2.. I dont know what this is.... ",
    "xvsfekcn": "when i stop the consume program that i can get the real message_count of the queue in produce use queue_declare. sorry i get the reason about this problem that when we want control the speed of consume need set the basic_qos function. ",
    "NohSeho": "I check that _on_close had with method frame.\nclose the comment.. ",
    "cessor": "I just realised that this is mentioned in the comment, and apoligize for causing work! \n\nThe IOLoop is started again because this method is invoked when CTRL-C is pressed raising a KeyboardInterrupt exception. This exception stops the IOLoop which needs to be running for pika to communicate with RabbitMQ. All of the commands issued prior to starting the IOLoop will be buffered but not processed.\n. \n",
    "westphahl": "The problem can be reproduce with the following setup:\n```python\nimport logging\nimport pika\nclass Consumer:\ndef run(self, queue):\n    self.connection = connection = pika.BlockingConnection()\n    channel = connection.channel()\n\n    channel.basic_qos(prefetch_count=1, all_channels=True)\n    channel.queue_declare(queue=queue, durable=False,\n                          exclusive=False, auto_delete=False)\n    channel.basic_consume(self.on_message, queue)\n    connection.add_timeout(1, self.on_timeout)\n    try:\n        channel.start_consuming()\n    except KeyboardInterrupt:\n        channel.stop_consuming()\n\ndef on_timeout(self):\n    self.connection.add_timeout(1, self.on_timeout)\n\ndef on_message(self, channel, method, properties, body):\n    self.connection.sleep(200)\n    channel.basic_ack(method.delivery_tag)\n\nif name == \"main\":\n    logging.basicConfig(level=logging.INFO)\n    consumer = Consumer()\n    consumer.run(\"foobar\")\n```\nWhich will give you the following traceback when the on_message() tries to ack the message.\nINFO:pika.adapters.base_connection:Connecting to ::1:5672\nINFO:pika.adapters.blocking_connection:Created channel=1\nERROR:pika.adapters.base_connection:Socket Error: 104\nINFO:pika.connection:Disconnected from RabbitMQ at localhost:5672 (-1): ConnectionResetError(104, 'Connection reset by peer')\nERROR:pika.adapters.blocking_connection:Connection close detected; result=BlockingConnection__OnClosedArgs(connection=<SelectConnection CLOSED socket=None params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>, reason_code=-1, reason_text=\"ConnectionResetError(104, 'Connection reset by peer')\")\nTraceback (most recent call last):\n  File \"/opt/src/bug_demo.py\", line 32, in <module>\n    consumer.run(\"foobar\")\n  File \"/opt/src/bug_demo.py\", line 17, in run\n    channel.start_consuming()\n  File \"/opt/src/pika/pika/adapters/blocking_connection.py\", line 1759, in start_consuming\n    self.connection.process_data_events(time_limit=None)\n  File \"/opt/src/pika/pika/adapters/blocking_connection.py\", line 719, in process_data_events\n    self._dispatch_channel_events()\n  File \"/opt/src/pika/pika/adapters/blocking_connection.py\", line 518, in _dispatch_channel_events\n    impl_channel._get_cookie()._dispatch_events()\n  File \"/opt/src/pika/pika/adapters/blocking_connection.py\", line 1388, in _dispatch_events\n    evt.body)\n  File \"/opt/src/bug_demo.py\", line 26, in on_message\n    channel.basic_ack(method.delivery_tag)\n  File \"/opt/src/pika/pika/adapters/blocking_connection.py\", line 1967, in basic_ack\n    self._flush_output()\n  File \"/opt/src/pika/pika/adapters/blocking_connection.py\", line 1254, in _flush_output\n    *waiters)\n  File \"/opt/src/pika/pika/adapters/blocking_connection.py\", line 474, in _flush_output\n    result.reason_text)\npika.exceptions.ConnectionClosed: (-1, \"ConnectionResetError(104, 'Connection reset by peer')\"). Unfortunately I also can't reproduce the problem anymore. Only difference I saw is that you probably tested it on OS X (KQueuePoller) where I used Linux. Maybe it was the RabbitMQ version.\nThanks anyways for checking the PR!. I was again able to reproduce the problem (after waiting long enough). The issue  is more obvious when enabling debug logging. Then there is a flood of messages like:\nDEBUG:pika.adapters.select_connection:add_timeout: added timeout 1106198814716520823; deadline=0.001033782958984375 at 1501048203.384139\nDEBUG:pika.adapters.select_connection:remove_timeout: removed 1106198814716520823\nDEBUG:pika.adapters.select_connection:add_timeout: added timeout 7612051997220596087; deadline=0.0009546279907226562 at 1501048203.3841395\nDEBUG:pika.adapters.select_connection:remove_timeout: removed 7612051997220596087\nDEBUG:pika.adapters.select_connection:add_timeout: added timeout 8739932674260658551; deadline=0.0008752346038818359 at 1501048203.3841393\nThis PR fixes the issue for me :+1:. The context manager _aquire_event_dispatch() will yield True in case the the event dispatcher could be acquired.. https://github.com/pika/pika/blob/master/pika/adapters/blocking_connection.py#L388. ",
    "szepeviktor": "@vitaly-krugl Thank you!\nThe short answer is\nbash\nhttp://127.0.0.1:15672/api/aliveness-test/%2F\nhttp://127.0.0.1:15672/api/healthchecks/node. The sad new is there is no usable response without authentication.\nI am interested in the AMQP answer also as the above answer is on an HTTP management port.. ",
    "ThreeWalnuts": "@vitaly-krugl , Yes, my app is multi-threaded, two threads, one thread is used to push messages into a queue. The other is used to receive messages from another queue. From the beginning, I used only one connection for both threads. Later, I changed my app to use different connections respectively. But no matter which pattern, I still get above errors. \nWhat should I do next? I'm stuck here now.. By the way, my rabbit MQ is not down. So it's very strange to have that error.. @wligang ,Yes. I found that the connection is closed for any unknown reason.\nI need to reconnect to MQ.\nRabbitMQ doesn't support long-lived connection?. ",
    "wligang": "python \uff1f. ",
    "behconsci": "I am still getting this error with version 0.11.2\n```\nFile \"/usr/local/lib/python3.6/site-packages/pika/channel.py\", line 1125, in _on_deliver\n    header_frame.properties, body)\n  File \"/usr/local/lib/python3.6/site-packages/asgi_rabbitmq/core.py\", line 195, in consume_message\n    amqp_channel.basic_cancel(consumer_tag=tag)\n  File \"/usr/local/lib/python3.6/site-packages/pika/channel.py\", line 231, in basic_cancel\n    'Must have completion callback with nowait=False')\nValueError: Must have completion callback with nowait=False\namqp_channel.basic_cancel(consumer_tag=tag)\n\nFile \"/usr/local/lib/python3.6/site-packages/pika/channel.py\", line 231, in basic_cancel\n    'Must have completion callback with nowait=False')\nValueError: Must have completion callback with nowait=False\n```\n. thanks @lukebakken! I upgraded asgi_rabbitmq and it worked again. . ",
    "codecov[bot]": "Codecov Report\n\nMerging #822 into master will increase coverage by 1.25%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #822      +/-\n==========================================\n+ Coverage   82.51%   83.77%   +1.25%   \n==========================================\n  Files          19       19            \n  Lines        3574     3574            \n  Branches      538      528      -10   \n==========================================\n+ Hits         2949     2994      +45   \n+ Misses        491      452      -39   \n+ Partials      134      128       -6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 84.64% <0%> (+0.13%) | :arrow_up: |\n| pika/adapters/base_connection.py | 70.19% <0%> (+3.52%) | :arrow_up: |\n| pika/adapters/select_connection.py | 88.09% <0%> (+9.25%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fe99f35...58f76cc. Read the comment docs.\n. # Codecov Report\nMerging #827 into master will decrease coverage by 0.97%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #827      +/-\n==========================================\n- Coverage   83.54%   82.56%   -0.98%   \n==========================================\n  Files          19       19            \n  Lines        3574     3574            \n  Branches      528      528            \n==========================================\n- Hits         2986     2951      -35   \n- Misses        457      490      +33   \n- Partials      131      133       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/twisted_connection.py | 33.51% <100%> (+1.59%) | :arrow_up: |\n| pika/adapters/select_connection.py | 78.57% <0%> (-9.53%) | :arrow_down: |\n| pika/adapters/base_connection.py | 66.27% <0%> (-0.79%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 64f8f12...9550709. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@dd6c8dd). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #833   +/-\n=========================================\n  Coverage          ?   83.66%         \n=========================================\n  Files             ?       19         \n  Lines             ?     3575         \n  Branches          ?      529         \n=========================================\n  Hits              ?     2991         \n  Misses            ?      452         \n  Partials          ?      132\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/channel.py | 94.02% <\u00f8> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dd6c8dd...f48250f. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@f3e129d). Click here to learn what that means.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #842   +/-\n=========================================\n  Coverage          ?   82.19%         \n=========================================\n  Files             ?       20         \n  Lines             ?     3656         \n  Branches          ?      545         \n=========================================\n  Hits              ?     3005         \n  Misses            ?      503         \n  Partials          ?      148\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 88.62% <100%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f3e129d...dca0f4d. Read the comment docs.\n. # Codecov Report\nMerging #843 into master will increase coverage by 0.29%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #843      +/-\n==========================================\n+ Coverage   82.19%   82.48%   +0.29%   \n==========================================\n  Files          20       19       -1   \n  Lines        3656     3575      -81   \n  Branches      545      528      -17   \n==========================================\n- Hits         3005     2949      -56   \n+ Misses        503      493      -10   \n+ Partials      148      133      -15\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 84.66% <100%> (+0.02%) | :arrow_up: |\n| pika/adapters/__init__.py | 72.22% <0%> (-5.06%) | :arrow_down: |\n| pika/adapters/twisted_connection.py | 31.91% <0%> (-1.6%) | :arrow_down: |\n| pika/connection.py | 88.06% <0%> (-0.56%) | :arrow_down: |\n| pika/adapters/asyncio_connection.py | | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5fd432f...d82badd. Read the comment docs.\n. # Codecov Report\nMerging #843 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #843      +/-\n=========================================\n+ Coverage   82.19%   82.2%   +0.01%   \n=========================================\n  Files          20      20            \n  Lines        3656    3664       +8   \n  Branches      545     547       +2   \n=========================================\n+ Hits         3005    3012       +7   \n  Misses        503     503            \n- Partials      148     149       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 84.66% <100%> (+0.02%) | :arrow_up: |\n| pika/connection.py | 88.59% <0%> (-0.03%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5fd432f...d82badd. Read the comment docs.\n. # Codecov Report\nMerging #844 into master will increase coverage by 0.81%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #844      +/-\n==========================================\n+ Coverage    82.1%   82.91%   +0.81%   \n==========================================\n  Files          20       20            \n  Lines        3666     4174     +508   \n  Branches      546      650     +104   \n==========================================\n+ Hits         3010     3461     +451   \n- Misses        505      553      +48   \n- Partials      151      160       +9\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/select_connection.py | 79.33% <0%> (+1.02%) | :arrow_up: |\n| pika/adapters/blocking_connection.py | 86.87% <0%> (+2.5%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3f472a6...e5267c4. Read the comment docs.\n. # Codecov Report\nMerging #845 into master will decrease coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #845      +/-\n==========================================\n- Coverage   82.19%   82.17%   -0.02%   \n==========================================\n  Files          20       20            \n  Lines        3656     3653       -3   \n  Branches      545      544       -1   \n==========================================\n- Hits         3005     3002       -3   \n  Misses        503      503            \n  Partials      148      148\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/select_connection.py | 78.4% <100%> (-0.18%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5fd432f...49869a5. Read the comment docs.\n. # Codecov Report\nMerging #846 into master will increase coverage by <.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #846      +/-\n=========================================\n+ Coverage   82.19%   82.2%   +<.01%   \n=========================================\n  Files          20      20            \n  Lines        3656    3663       +7   \n  Branches      545     547       +2   \n=========================================\n+ Hits         3005    3011       +6   \n  Misses        503     503            \n- Partials      148     149       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 88.59% <0%> (-0.03%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5fd432f...a292185. Read the comment docs.\n. # Codecov Report\nMerging #848 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #848   +/-\n=======================================\n  Coverage   82.09%   82.09%         \n=======================================\n  Files          20       20         \n  Lines        3664     3664         \n  Branches      545      545         \n=======================================\n  Hits         3008     3008         \n  Misses        505      505         \n  Partials      151      151\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f205c7c...929ddab. Read the comment docs.\n. # Codecov Report\nMerging #849 into master will increase coverage by 1.03%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #849      +/-\n==========================================\n+ Coverage   82.09%   83.13%   +1.03%   \n==========================================\n  Files          20       20            \n  Lines        3664     3664            \n  Branches      545      545            \n==========================================\n+ Hits         3008     3046      +38   \n+ Misses        505      469      -36   \n+ Partials      151      149       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 67.05% <0%> (+0.78%) | :arrow_up: |\n| pika/adapters/select_connection.py | 87.73% <0%> (+9.59%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f205c7c...961a1fa. Read the comment docs.\n. # Codecov Report\nMerging #851 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #851   +/-\n=======================================\n  Coverage   82.09%   82.09%         \n=======================================\n  Files          20       20         \n  Lines        3664     3664         \n  Branches      545      545         \n=======================================\n  Hits         3008     3008         \n  Misses        505      505         \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/channel.py | 94.02% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/blocking_connection.py | 84.39% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f73f9bb...8e7264f. Read the comment docs.\n. # Codecov Report\nMerging #853 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #853   +/-\n=======================================\n  Coverage   83.13%   83.13%         \n=======================================\n  Files          20       20         \n  Lines        3664     3664         \n  Branches      545      545         \n=======================================\n  Hits         3046     3046         \n  Misses        469      469         \n  Partials      149      149\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ab560a6...f409d68. Read the comment docs.\n. # Codecov Report\nMerging #855 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #855   +/-\n=======================================\n  Coverage   83.13%   83.13%         \n=======================================\n  Files          20       20         \n  Lines        3664     3664         \n  Branches      545      545         \n=======================================\n  Hits         3046     3046         \n  Misses        469      469         \n  Partials      149      149\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 535c5a5...ff2d8cb. Read the comment docs.\n. # Codecov Report\nMerging #857 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #857   +/-\n=======================================\n  Coverage   82.09%   82.09%         \n=======================================\n  Files          20       20         \n  Lines        3664     3664         \n  Branches      545      545         \n=======================================\n  Hits         3008     3008         \n  Misses        505      505         \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/channel.py | 94.02% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 54fee19...7d76efd. Read the comment docs.\n. # Codecov Report\nMerging #860 into master will decrease coverage by 0.77%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #860      +/-\n==========================================\n- Coverage   82.09%   81.31%   -0.78%   \n==========================================\n  Files          20       21       +1   \n  Lines        3664     3699      +35   \n  Branches      545      548       +3   \n==========================================\n  Hits         3008     3008            \n- Misses        505      540      +35   \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/gevent_connection.py | 0% <0%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 01adac6...5f60c5d. Read the comment docs.\n. # Codecov Report\nMerging #862 into master will decrease coverage by 0.1%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #862      +/-\n==========================================\n- Coverage   82.09%   81.98%   -0.11%   \n==========================================\n  Files          20       20            \n  Lines        3664     3664            \n  Branches      545      545            \n==========================================\n- Hits         3008     3004       -4   \n- Misses        505      513       +8   \n+ Partials      151      147       -4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 64.7% <0%> (-1.57%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 01adac6...62e2ac3. Read the comment docs.\n. # Codecov Report\nMerging #864 into master will increase coverage by 0.09%.\nThe diff coverage is 90.47%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #864      +/-\n==========================================\n+ Coverage   81.98%   82.08%   +0.09%   \n==========================================\n  Files          20       20            \n  Lines        3664     3706      +42   \n  Branches      545      552       +7   \n==========================================\n+ Hits         3004     3042      +38   \n- Misses        513      515       +2   \n- Partials      147      149       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 88.57% <100%> (+0.19%) | :arrow_up: |\n| pika/compat.py | 96.55% <85.71%> (-1.49%) | :arrow_down: |\n| pika/adapters/base_connection.py | 66.42% <86.36%> (+1.72%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fdd6c8f...430ce10. Read the comment docs.\n. # Codecov Report\nMerging #865 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #865   +/-\n=======================================\n  Coverage   81.98%   81.98%         \n=======================================\n  Files          20       20         \n  Lines        3664     3664         \n  Branches      545      545         \n=======================================\n  Hits         3004     3004         \n  Misses        513      513         \n  Partials      147      147\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 84.39% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a95668a...9b7ca19. Read the comment docs.\n. # Codecov Report\nMerging #867 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #867   +/-\n=======================================\n  Coverage   81.98%   81.98%         \n=======================================\n  Files          20       20         \n  Lines        3664     3664         \n  Branches      545      545         \n=======================================\n  Hits         3004     3004         \n  Misses        513      513         \n  Partials      147      147\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a07690...ddc8705. Read the comment docs.\n. # Codecov Report\nMerging #868 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #868   +/-\n=======================================\n  Coverage   81.98%   81.98%         \n=======================================\n  Files          20       20         \n  Lines        3664     3664         \n  Branches      545      545         \n=======================================\n  Hits         3004     3004         \n  Misses        513      513         \n  Partials      147      147\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 84.39% <\u00f8> (\u00f8) | :arrow_up: |\n| pika/connection.py | 88.37% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a07690...edc26ec. Read the comment docs.\n. # Codecov Report\nMerging #869 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #869   +/-\n=======================================\n  Coverage   81.98%   81.98%         \n=======================================\n  Files          20       20         \n  Lines        3664     3664         \n  Branches      545      545         \n=======================================\n  Hits         3004     3004         \n  Misses        513      513         \n  Partials      147      147\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a07690...3873f10. Read the comment docs.\n. # Codecov Report\nMerging #870 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #870      +/-\n==========================================\n+ Coverage   81.98%   81.99%   +<.01%   \n==========================================\n  Files          20       20            \n  Lines        3664     3665       +1   \n  Branches      545      544       -1   \n==========================================\n+ Hits         3004     3005       +1   \n  Misses        513      513            \n  Partials      147      147\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 88.39% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fdd6c8f...3c3a0a3. Read the comment docs.\n. # Codecov Report\nMerging #874 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #874   +/-\n=======================================\n  Coverage   81.98%   81.98%         \n=======================================\n  Files          20       20         \n  Lines        3664     3664         \n  Branches      545      545         \n=======================================\n  Hits         3004     3004         \n  Misses        513      513         \n  Partials      147      147\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 88.37% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f643d5c...7c13a57. Read the comment docs.\n. # Codecov Report\nMerging #880 into master will increase coverage by 0.1%.\nThe diff coverage is 86.36%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #880     +/-\n=========================================\n+ Coverage   81.99%   82.09%   +0.1%   \n=========================================\n  Files          20       21      +1   \n  Lines        3665     3719     +54   \n  Branches      544      552      +8   \n=========================================\n+ Hits         3005     3053     +48   \n- Misses        513      517      +4   \n- Partials      147      149      +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 88.58% <100%> (+0.19%) | :arrow_up: |\n| pika/adapters/base_connection.py | 64.98% <76.92%> (+0.27%) | :arrow_up: |\n| pika/tcp_socket_opts.py | 84.37% <84.37%> (\u00f8) | |\n| pika/compat.py | 96.55% <87.5%> (-1.49%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 73c72e3...8257a8e. Read the comment docs.\n. # Codecov Report\nMerging #883 into master will increase coverage by 0.02%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #883      +/-\n==========================================\n+ Coverage      82%   82.03%   +0.02%   \n==========================================\n  Files          21       21            \n  Lines        3724     3724            \n  Branches      554      554            \n==========================================\n+ Hits         3054     3055       +1   \n  Misses        519      519            \n+ Partials      151      150       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 88.27% <0%> (+0.12%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6baad8e...2b071b2. Read the comment docs.\n. # Codecov Report\nMerging #886 into master will decrease coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #886      +/-\n==========================================\n- Coverage      82%   81.93%   -0.07%   \n==========================================\n  Files          21       21            \n  Lines        3724     3732       +8   \n  Branches      554      554            \n==========================================\n+ Hits         3054     3058       +4   \n- Misses        519      522       +3   \n- Partials      151      152       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/channel.py | 94.02% <\u00f8> (\u00f8) | :arrow_up: |\n| pika/adapters/blocking_connection.py | 84.01% <100%> (-0.38%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6baad8e...399bcfc. Read the comment docs.\n. # Codecov Report\nMerging #887 into master will decrease coverage by 0.01%.\nThe diff coverage is 50%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #887      +/-\n==========================================\n- Coverage   81.96%   81.94%   -0.02%   \n==========================================\n  Files          21       21            \n  Lines        3732     3734       +2   \n  Branches      554      554            \n==========================================\n+ Hits         3059     3060       +1   \n- Misses        522      523       +1   \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 83.92% <50%> (-0.1%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1a7ecc4...d6397cf. Read the comment docs.\n. # Codecov Report\nMerging #888 into master will increase coverage by 0.04%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #888      +/-\n==========================================\n+ Coverage   81.94%   81.99%   +0.04%   \n==========================================\n  Files          21       21            \n  Lines        3734     3743       +9   \n  Branches      554      554            \n==========================================\n+ Hits         3060     3069       +9   \n  Misses        523      523            \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/data.py | 82.85% <100%> (+0.92%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7022aa9...b98e085. Read the comment docs.\n. # Codecov Report\nMerging #890 into master will decrease coverage by 0.06%.\nThe diff coverage is 66.66%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #890      +/-\n==========================================\n- Coverage   81.94%   81.88%   -0.07%   \n==========================================\n  Files          21       21            \n  Lines        3734     3743       +9   \n  Branches      554      554            \n==========================================\n+ Hits         3060     3065       +5   \n- Misses        523      527       +4   \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/data.py | 80.57% <66.66%> (-1.36%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7022aa9...ca5bf0a. Read the comment docs.\n. # Codecov Report\nMerging #891 into master will not change coverage.\nThe diff coverage is 33.33%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #891   +/-\n=======================================\n  Coverage   81.92%   81.92%         \n=======================================\n  Files          21       21         \n  Lines        3752     3752         \n  Branches      554      554         \n=======================================\n  Hits         3074     3074         \n  Misses        527      527         \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 83.92% <33.33%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a978965...0dc020f. Read the comment docs.\n. # Codecov Report\nMerging #895 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #895      +/-\n==========================================\n+ Coverage   81.92%   81.96%   +0.03%   \n==========================================\n  Files          21       21            \n  Lines        3752     3759       +7   \n  Branches      554      555       +1   \n==========================================\n+ Hits         3074     3081       +7   \n  Misses        527      527            \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/compat.py | 96.92% <100%> (+0.37%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9a03d44...d339793. Read the comment docs.\n. # Codecov Report\nMerging #901 into master will increase coverage by 0.18%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #901      +/-\n==========================================\n+ Coverage   81.96%   82.14%   +0.18%   \n==========================================\n  Files          21       21            \n  Lines        3759     3759            \n  Branches      555      555            \n==========================================\n+ Hits         3081     3088       +7   \n+ Misses        527      517      -10   \n- Partials      151      154       +3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/__init__.py | 100% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/blocking_connection.py | 84.87% <100%> (+0.95%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2ee4f0e...a8754d7. Read the comment docs.\n. # Codecov Report\nMerging #902 into master will increase coverage by 0.01%.\nThe diff coverage is 91.3%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #902      +/-\n==========================================\n+ Coverage   81.96%   81.97%   +0.01%   \n==========================================\n  Files          21       21            \n  Lines        3759     3762       +3   \n  Branches      555      555            \n==========================================\n+ Hits         3081     3084       +3   \n  Misses        527      527            \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 83.98% <91.3%> (+0.06%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2ee4f0e...0a7d463. Read the comment docs.\n. # Codecov Report\nMerging #903 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #903   +/-\n=======================================\n  Coverage   81.96%   81.96%         \n=======================================\n  Files          21       21         \n  Lines        3759     3759         \n  Branches      555      555         \n=======================================\n  Hits         3081     3081         \n  Misses        527      527         \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/select_connection.py | 78.13% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2ee4f0e...e09e08a. Read the comment docs.\n. # Codecov Report\nMerging #904 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #904   +/-\n=======================================\n  Coverage   81.96%   81.96%         \n=======================================\n  Files          21       21         \n  Lines        3759     3759         \n  Branches      555      555         \n=======================================\n  Hits         3081     3081         \n  Misses        527      527         \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/select_connection.py | 78.13% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2ee4f0e...e09e08a. Read the comment docs.\n. # Codecov Report\nMerging #905 into master will decrease coverage by 0.06%.\nThe diff coverage is 12.5%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #905      +/-\n==========================================\n- Coverage   81.96%   81.89%   -0.07%   \n==========================================\n  Files          21       21            \n  Lines        3759     3762       +3   \n  Branches      555      555            \n==========================================\n  Hits         3081     3081            \n- Misses        527      530       +3   \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/select_connection.py | 77.51% <12.5%> (-0.63%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2ee4f0e...1f5c382. Read the comment docs.\n. # Codecov Report\nMerging #907 into master will increase coverage by 0.05%.\nThe diff coverage is 62.16%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #907      +/-\n=========================================\n+ Coverage   82.14%   82.2%   +0.05%   \n=========================================\n  Files          21      21            \n  Lines        3759    3759            \n  Branches      555     555            \n=========================================\n+ Hits         3088    3090       +2   \n+ Misses        517     515       -2   \n  Partials      154     154\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/tcp_socket_opts.py | 89.28% <100%> (+4.91%) | :arrow_up: |\n| pika/adapters/select_connection.py | 78.19% <50%> (+0.05%) | :arrow_up: |\n| pika/adapters/base_connection.py | 65.07% <66.66%> (+0.09%) | :arrow_up: |\n| pika/compat.py | 94.52% <75%> (-2.41%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1a33aed...d6721d2. Read the comment docs.\n. # Codecov Report\nMerging #908 into master will decrease coverage by 0.67%.\nThe diff coverage is 11.9%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #908      +/-\n==========================================\n- Coverage    82.2%   81.52%   -0.68%   \n==========================================\n  Files          21       21            \n  Lines        3759     3789      +30   \n  Branches      555      563       +8   \n==========================================\n- Hits         3090     3089       -1   \n- Misses        515      545      +30   \n- Partials      154      155       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/select_connection.py | 79.23% <100%> (+1.04%) | :arrow_up: |\n| pika/compat/__init__.py | 63.71% <9.75%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8f2f874...6aaa006. Read the comment docs.\n. # Codecov Report\nMerging #915 into master will decrease coverage by 0.89%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #915     +/-\n=========================================\n- Coverage   82.45%   81.55%   -0.9%   \n=========================================\n  Files          20       21      +1   \n  Lines        3670     3784    +114   \n  Branches      546      561     +15   \n=========================================\n+ Hits         3026     3086     +60   \n- Misses        498      544     +46   \n- Partials      146      154      +8\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 88.45% <100%> (-0.06%) | :arrow_down: |\n| pika/compat/__init__.py | 63.71% <0%> (-20.6%) | :arrow_down: |\n| pika/adapters/asyncio_connection.py | 58.44% <0%> (-1.3%) | :arrow_down: |\n| pika/adapters/base_connection.py | 65.07% <0%> (-0.84%) | :arrow_down: |\n| pika/__init__.py | 100% <0%> (\u00f8) | :arrow_up: |\n| pika/adapters/libev_connection.py | 73.43% <0%> (\u00f8) | |\n| pika/adapters/select_connection.py | 79.23% <0%> (+0.11%) | :arrow_up: |\n| pika/adapters/blocking_connection.py | 84.87% <0%> (+0.13%) | :arrow_up: |\n| pika/adapters/__init__.py | 77.27% <0%> (+5.05%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9088b23...930d8bc. Read the comment docs.\n. # Codecov Report\nMerging #918 into master will increase coverage by 0.59%.\nThe diff coverage is 51.72%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #918      +/-\n==========================================\n+ Coverage   81.52%   82.12%   +0.59%   \n==========================================\n  Files          21       21            \n  Lines        3789     3776      -13   \n  Branches      563      560       -3   \n==========================================\n+ Hits         3089     3101      +12   \n+ Misses        545      517      -28   \n- Partials      155      158       +3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/select_connection.py | 79.12% <50%> (-0.12%) | :arrow_down: |\n| pika/compat/__init__.py | 84.31% <52%> (+20.59%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4433edf...1e2eccb. Read the comment docs.\n. # Codecov Report\nMerging #923 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #923   +/-\n=======================================\n  Coverage   82.12%   82.12%         \n=======================================\n  Files          21       21         \n  Lines        3776     3776         \n  Branches      560      560         \n=======================================\n  Hits         3101     3101         \n  Misses        517      517         \n  Partials      158      158\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/libev_connection.py | 73.43% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6638774...3d60e54. Read the comment docs.\n. # Codecov Report\nMerging #924 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #924   +/-\n=======================================\n  Coverage   82.12%   82.12%         \n=======================================\n  Files          21       21         \n  Lines        3776     3776         \n  Branches      560      560         \n=======================================\n  Hits         3101     3101         \n  Misses        517      517         \n  Partials      158      158\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/libev_connection.py | 73.43% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6638774...3580e5b. Read the comment docs.\n. # Codecov Report\nMerging #928 into master will decrease coverage by 0.41%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #928      +/-\n==========================================\n- Coverage   82.42%   82.01%   -0.42%   \n==========================================\n  Files          20       21       +1   \n  Lines        3636     3781     +145   \n  Branches      540      562      +22   \n==========================================\n+ Hits         2997     3101     +104   \n- Misses        494      521      +27   \n- Partials      145      159      +14\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 63.81% <0%> (-1.79%) | :arrow_down: |\n| pika/adapters/asyncio_connection.py | 58.44% <0%> (-1.3%) | :arrow_down: |\n| pika/__init__.py | 100% <0%> (\u00f8) | :arrow_up: |\n| pika/adapters/libev_connection.py | 73.43% <0%> (\u00f8) | |\n| pika/adapters/blocking_connection.py | 84.87% <0%> (+0.13%) | :arrow_up: |\n| pika/adapters/__init__.py | 77.27% <0%> (+5.05%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 35169f8...32723b8. Read the comment docs.\n. # Codecov Report\nMerging #932 into master will decrease coverage by 0.16%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #932      +/-\n==========================================\n- Coverage   82.42%   82.26%   -0.17%   \n==========================================\n  Files          20       21       +1   \n  Lines        3636     3805     +169   \n  Branches      540      564      +24   \n==========================================\n+ Hits         2997     3130     +133   \n- Misses        494      517      +23   \n- Partials      145      158      +13\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 66.66% <100%> (+1.06%) | :arrow_up: |\n| pika/connection.py | 88.51% <100%> (+0.23%) | :arrow_up: |\n| pika/__init__.py | 100% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/asyncio_connection.py | 58.44% <0%> (-1.3%) | :arrow_down: |\n| pika/adapters/libev_connection.py | 73.43% <0%> (\u00f8) | |\n| pika/adapters/blocking_connection.py | 84.87% <0%> (+0.13%) | :arrow_up: |\n| pika/adapters/__init__.py | 77.27% <0%> (+5.05%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 35169f8...0fa7208. Read the comment docs.\n. # Codecov Report\nMerging #933 into master will decrease coverage by 0.17%.\nThe diff coverage is 83.33%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #933      +/-\n==========================================\n- Coverage   82.26%   82.09%   -0.18%   \n==========================================\n  Files          20       21       +1   \n  Lines        3643     3769     +126   \n  Branches      541      559      +18   \n==========================================\n+ Hits         2997     3094      +97   \n- Misses        499      518      +19   \n- Partials      147      157      +10\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/exceptions.py | 72.11% <0%> (\u00f8) | :arrow_up: |\n| pika/compat/__init__.py | 84.31% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/blocking_connection.py | 84.87% <100%> (+0.13%) | :arrow_up: |\n| pika/__init__.py | 100% <100%> (+22.22%) | :arrow_up: |\n| pika/adapters/base_connection.py | 65.59% <100%> (+0.52%) | :arrow_up: |\n| pika/adapters/__init__.py | 68.18% <0%> (-4.05%) | :arrow_down: |\n| pika/adapters/libev_connection.py | 73.43% <0%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 03275ed...68cee87. Read the comment docs.\n. # Codecov Report\nMerging #934 into master will decrease coverage by 0.3%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #934      +/-\n==========================================\n- Coverage   82.42%   82.12%   -0.31%   \n==========================================\n  Files          20       21       +1   \n  Lines        3636     3776     +140   \n  Branches      540      560      +20   \n==========================================\n+ Hits         2997     3101     +104   \n- Misses        494      517      +23   \n- Partials      145      158      +13\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/asyncio_connection.py | 58.44% <0%> (-1.3%) | :arrow_down: |\n| pika/adapters/base_connection.py | 65.07% <0%> (-0.53%) | :arrow_down: |\n| pika/__init__.py | 100% <0%> (\u00f8) | :arrow_up: |\n| pika/adapters/libev_connection.py | 73.43% <0%> (\u00f8) | |\n| pika/adapters/blocking_connection.py | 84.87% <0%> (+0.13%) | :arrow_up: |\n| pika/adapters/__init__.py | 77.27% <0%> (+5.05%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 35169f8...d1e18c4. Read the comment docs.\n. # Codecov Report\nMerging #937 into master will decrease coverage by 2.59%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #937     +/-\n=========================================\n- Coverage   82.12%   79.52%   -2.6%   \n=========================================\n  Files          21       21           \n  Lines        3776     3776           \n  Branches      560      560           \n=========================================\n- Hits         3101     3003     -98   \n- Misses        517      626    +109   \n+ Partials      158      147     -11\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/libev_connection.py | 0.78% <0%> (-72.66%) | :arrow_down: |\n| pika/__init__.py | 78.94% <0%> (-21.06%) | :arrow_down: |\n| pika/adapters/blocking_connection.py | 84.74% <0%> (-0.14%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ef157cb...06f294c. Read the comment docs.\n. # Codecov Report\nMerging #938 into master will increase coverage by 2.73%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #938      +/-\n==========================================\n+ Coverage   79.52%   82.26%   +2.73%   \n==========================================\n  Files          21       20       -1   \n  Lines        3776     3643     -133   \n  Branches      560      541      -19   \n==========================================\n- Hits         3003     2997       -6   \n+ Misses        626      499     -127   \n  Partials      147      147\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/__init__.py | 77.77% <\u00f8> (-1.17%) | :arrow_down: |\n| pika/adapters/__init__.py | 72.22% <100%> (-5.06%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 44ff74f...758e0e9. Read the comment docs.\n. # Codecov Report\nMerging #941 into master will decrease coverage by 0.2%.\nThe diff coverage is 83.33%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #941      +/-\n==========================================\n- Coverage   82.48%   82.27%   -0.21%   \n==========================================\n  Files          20       19       -1   \n  Lines        3665     3690      +25   \n  Branches      544      548       +4   \n==========================================\n+ Hits         3023     3036      +13   \n- Misses        497      503       +6   \n- Partials      145      151       +6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/tcp_socket_opts.py | 89.28% <\u00f8> (\u00f8) | :arrow_up: |\n| pika/adapters/tornado_connection.py | 95.83% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/select_connection.py | 79.12% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/asyncio_connection.py | 59.74% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/base_connection.py | 65.91% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/twisted_connection.py | 34.04% <40%> (\u00f8) | :arrow_up: |\n| pika/connection.py | 87.34% <60%> (-1.35%) | :arrow_down: |\n| pika/adapters/blocking_connection.py | 84.72% <84%> (-0.03%) | :arrow_down: |\n| pika/channel.py | 94.29% <97.14%> (+0.26%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a7fed21...75bf414. Read the comment docs.\n. # Codecov Report\nMerging #948 into master will increase coverage by 1.66%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #948      +/-\n==========================================\n+ Coverage   82.33%   83.99%   +1.66%   \n==========================================\n  Files          19       19            \n  Lines        3690     4298     +608   \n  Branches      548      659     +111   \n==========================================\n+ Hits         3038     3610     +572   \n- Misses        502      524      +22   \n- Partials      150      164      +14\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/channel.py | 95.03% <100%> (+0.74%) | :arrow_up: |\n| pika/adapters/blocking_connection.py | 87.21% <0%> (+2.49%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3d3b95d...6065dba. Read the comment docs.\n. # Codecov Report\nMerging #954 into master will increase coverage by <.01%.\nThe diff coverage is 75%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #954      +/-\n==========================================\n+ Coverage   82.33%   82.34%   +<.01%   \n==========================================\n  Files          19       19            \n  Lines        3690     3692       +2   \n  Branches      548      548            \n==========================================\n+ Hits         3038     3040       +2   \n  Misses        502      502            \n  Partials      150      150\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/channel.py | 94.32% <100%> (+0.02%) | :arrow_up: |\n| pika/adapters/blocking_connection.py | 84.72% <62.5%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3d3b95d...a46c320. Read the comment docs.\n. # Codecov Report\nMerging #955 into master will decrease coverage by 0.05%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #955      +/-\n==========================================\n- Coverage   82.36%   82.31%   -0.06%   \n==========================================\n  Files          19       19            \n  Lines        3698     3698            \n  Branches      549      549            \n==========================================\n- Hits         3046     3044       -2   \n- Misses        502      503       +1   \n- Partials      150      151       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 84.72% <100%> (\u00f8) | :arrow_up: |\n| pika/channel.py | 94.4% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/base_connection.py | 65.91% <0%> (-0.75%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9acd768...751364b. Read the comment docs.\n. # Codecov Report\nMerging #957 into master will increase coverage by 1.19%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #957      +/-\n==========================================\n+ Coverage   82.36%   83.56%   +1.19%   \n==========================================\n  Files          19       19            \n  Lines        3698     4094     +396   \n  Branches      549      647      +98   \n==========================================\n+ Hits         3046     3421     +375   \n- Misses        502      509       +7   \n- Partials      150      164      +14\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/tornado_connection.py | 95.65% <\u00f8> (-0.19%) | :arrow_down: |\n| pika/adapters/base_connection.py | 65.53% <\u00f8> (-1.14%) | :arrow_down: |\n| pika/channel.py | 95.87% <100%> (+1.46%) | :arrow_up: |\n| pika/__init__.py | 100% <0%> (\u00f8) | :arrow_up: |\n| pika/adapters/blocking_connection.py | 85.71% <0%> (+0.99%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9acd768...e00c5b9. Read the comment docs.\n. # Codecov Report\nMerging #967 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #967   +/-\n=======================================\n  Coverage   82.45%   82.45%         \n=======================================\n  Files          19       19         \n  Lines        3734     3734         \n  Branches      555      555         \n=======================================\n  Hits         3079     3079         \n  Misses        504      504         \n  Partials      151      151\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 87.34% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0e13941...f29d904. Read the comment docs.\n. # Codecov Report\nMerging #978 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster    #978   +/-\n======================================\n  Coverage    82.5%   82.5%         \n======================================\n  Files          19      19         \n  Lines        3779    3779         \n  Branches      567     567         \n======================================\n  Hits         3118    3118         \n  Misses        508     508         \n  Partials      153     153\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/compat.py | 84.31% <\u00f8> (\u00f8) | |\n| pika/adapters/blocking_connection.py | 85.75% <0%> (-0.28%) | :arrow_down: |\n| pika/adapters/base_connection.py | 64.86% <0%> (+0.77%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 173ce21...48d8339. Read the comment docs.\n. # Codecov Report\nMerging #983 into master will increase coverage by 0.52%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #983      +/-\n==========================================\n+ Coverage   82.59%   83.12%   +0.52%   \n==========================================\n  Files          19       19            \n  Lines        3775     3775            \n  Branches      567      567            \n==========================================\n+ Hits         3118     3138      +20   \n+ Misses        504      483      -21   \n- Partials      153      154       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 73.79% <0%> (+8.06%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7f81bc7...3167ba4. Read the comment docs.\n. # Codecov Report\nMerging #1007 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1007   +/-\n=======================================\n  Coverage   83.15%   83.15%         \n=======================================\n  Files          19       19         \n  Lines        3775     3775         \n  Branches      563      563         \n=======================================\n  Hits         3139     3139         \n  Misses        481      481         \n  Partials      155      155\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 86.73% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3a3baa4...0e4acb8. Read the comment docs.\n. # Codecov Report\nMerging #1015 into stable will increase coverage by 0.97%.\nThe diff coverage is 82.55%.\n\n\n```diff\n@@            Coverage Diff             @@\nstable    #1015      +/-\n==========================================\n+ Coverage   81.96%   82.93%   +0.97%   \n==========================================\n  Files          21       20       -1   \n  Lines        3759     3757       -2   \n  Branches      555      565      +10   \n==========================================\n+ Hits         3081     3116      +35   \n+ Misses        527      488      -39   \n- Partials      151      153       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/exceptions.py | 72.11% <0%> (\u00f8) | :arrow_up: |\n| pika/adapters/__init__.py | 72.22% <100%> (-5.06%) | :arrow_down: |\n| pika/__init__.py | 100% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/asyncio_connection.py | 61.72% <100%> (+3.28%) | :arrow_up: |\n| pika/tcp_socket_opts.py | 89.28% <100%> (+4.91%) | :arrow_up: |\n| pika/adapters/tornado_connection.py | 89.28% <50%> (-6.55%) | :arrow_down: |\n| pika/adapters/twisted_connection.py | 34.21% <50%> (+0.16%) | :arrow_up: |\n| pika/compat.py | 84.31% <63.15%> (-12.61%) | :arrow_down: |\n| pika/adapters/base_connection.py | 72.32% <80.55%> (+7.34%) | :arrow_up: |\n| pika/adapters/select_connection.py | 79.76% <84.26%> (+1.63%) | :arrow_up: |\n| ... and 6 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2ee4f0e...88d65d8. Read the comment docs.\n. # Codecov Report\nMerging #1016 into master will decrease coverage by 0.47%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1016      +/-\n==========================================\n- Coverage    83.2%   82.72%   -0.48%   \n==========================================\n  Files          19       19            \n  Lines        3775     3775            \n  Branches      563      563            \n==========================================\n- Hits         3141     3123      -18   \n- Misses        480      500      +20   \n+ Partials      154      152       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 65.38% <0%> (-7.7%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0793350...d2e007a. Read the comment docs.\n. # Codecov Report\nMerging #1018 into master will increase coverage by 0.47%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1018      +/-\n==========================================\n+ Coverage   82.67%   83.15%   +0.47%   \n==========================================\n  Files          19       19            \n  Lines        3775     3775            \n  Branches      563      563            \n==========================================\n+ Hits         3121     3139      +18   \n+ Misses        501      481      -20   \n- Partials      153      155       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 72.22% <0%> (+7.69%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 75fd11d...dd6f9a1. Read the comment docs.\n. # Codecov Report\nMerging #1020 into 0.12-release-2 will decrease coverage by 0.01%.\nThe diff coverage is 85.78%.\n\n\n```diff\n@@                Coverage Diff                 @@\n0.12-release-2    #1020      +/-\n==================================================\n- Coverage           82.95%   82.93%   -0.02%   \n==================================================\n  Files                  20       20            \n  Lines                3672     3757      +85   \n  Branches              546      565      +19   \n==================================================\n+ Hits                 3046     3116      +70   \n- Misses                478      488      +10   \n- Partials              148      153       +5\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/asyncio_connection.py | 61.72% <100%> (+1.98%) | :arrow_up: |\n| pika/connection.py | 88.55% <100%> (+0.01%) | :arrow_up: |\n| pika/adapters/tornado_connection.py | 89.28% <50%> (-6.55%) | :arrow_down: |\n| pika/adapters/twisted_connection.py | 34.21% <50%> (+0.16%) | :arrow_up: |\n| pika/adapters/base_connection.py | 72.32% <50%> (-0.34%) | :arrow_down: |\n| pika/adapters/blocking_connection.py | 84.95% <85.71%> (+0.21%) | :arrow_up: |\n| pika/adapters/select_connection.py | 79.76% <87.42%> (+0.64%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7b6d798...42f3d5b. Read the comment docs.\n. # Codecov Report\nMerging #1049 into stable will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nstable    #1049      +/-\n==========================================\n+ Coverage   82.93%   82.96%   +0.02%   \n==========================================\n  Files          20       20            \n  Lines        3757     3751       -6   \n  Branches      565      563       -2   \n==========================================\n- Hits         3116     3112       -4   \n+ Misses        488      487       -1   \n+ Partials      153      152       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 84.95% <\u00f8> (\u00f8) | :arrow_up: |\n| pika/adapters/select_connection.py | 79.95% <100%> (+0.18%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8a617ad...92fb598. Read the comment docs.\n. # Codecov Report\nMerging #1050 into stable will increase coverage by 0.07%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nstable   #1050      +/-\n=========================================\n+ Coverage   82.93%     83%   +0.07%   \n=========================================\n  Files          20      20            \n  Lines        3757    3761       +4   \n  Branches      565     565            \n=========================================\n+ Hits         3116    3122       +6   \n+ Misses        488     487       -1   \n+ Partials      153     152       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/blocking_connection.py | 84.95% <\u00f8> (\u00f8) | :arrow_up: |\n| pika/adapters/select_connection.py | 80% <100%> (+0.23%) | :arrow_up: |\n| pika/__init__.py | 100% <100%> (\u00f8) | :arrow_up: |\n| pika/data.py | 82.38% <100%> (+0.86%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8a617ad...0adbe3a. Read the comment docs.\n. # Codecov Report\nMerging #1065 into stable will increase coverage by 0.18%.\nThe diff coverage is 81.12%.\n\n\n```diff\n@@            Coverage Diff             @@\nstable    #1065      +/-\n==========================================\n+ Coverage   83.01%   83.19%   +0.18%   \n==========================================\n  Files          20       24       +4   \n  Lines        3762     5011    +1249   \n  Branches      565      671     +106   \n==========================================\n+ Hits         3123     4169    +1046   \n- Misses        487      653     +166   \n- Partials      152      189      +37\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/tcp_socket_opts.py | 89.28% <\u00f8> (\u00f8) | :arrow_up: |\n| pika/compat.py | 85.32% <100%> (+1%) | :arrow_up: |\n| pika/__init__.py | 100% <100%> (\u00f8) | :arrow_up: |\n| pika/heartbeat.py | 98.46% <100%> (+0.02%) | :arrow_up: |\n| pika/data.py | 82.38% <100%> (\u00f8) | :arrow_up: |\n| pika/diagnostic_utils.py | 100% <100%> (\u00f8) | |\n| pika/adapters/utils/nbio_interface.py | 58.92% <58.92%> (\u00f8) | |\n| pika/adapters/utils/io_services_utils.py | 69.14% <69.14%> (\u00f8) | |\n| pika/exceptions.py | 82.67% <71.42%> (+10.56%) | :arrow_up: |\n| pika/adapters/twisted_connection.py | 58.99% <77.56%> (+24.78%) | :arrow_up: |\n| ... and 23 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 44c0a99...8e633c0. Read the comment docs.\n. # Codecov Report\nMerging #1090 into master will decrease coverage by 0.27%.\nThe diff coverage is 87.74%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1090      +/-\n==========================================\n- Coverage    83.3%   83.02%   -0.28%   \n==========================================\n  Files          24       20       -4   \n  Lines        5019     3771    -1248   \n  Branches      672      566     -106   \n==========================================\n- Hits         4181     3131    -1050   \n+ Misses        649      487     -162   \n+ Partials      189      153      -36\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/compat.py | 84.31% <\u00f8> (-1.01%) | :arrow_down: |\n| pika/__init__.py | 100% <100%> (\u00f8) | :arrow_up: |\n| pika/data.py | 82.38% <100%> (\u00f8) | :arrow_up: |\n| pika/adapters/asyncio_connection.py | 61.72% <100%> (-29.71%) | :arrow_down: |\n| pika/adapters/tornado_connection.py | 89.28% <50%> (-0.19%) | :arrow_down: |\n| pika/adapters/twisted_connection.py | 34.21% <50%> (-24.79%) | :arrow_down: |\n| pika/adapters/base_connection.py | 73.16% <71.42%> (-22.08%) | :arrow_down: |\n| pika/adapters/blocking_connection.py | 84.95% <85.71%> (-2.03%) | :arrow_down: |\n| pika/connection.py | 88.55% <86.66%> (+2.73%) | :arrow_up: |\n| pika/adapters/select_connection.py | 80% <88.27%> (+0.27%) | :arrow_up: |\n| ... and 21 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 03542ef...fb71b55. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@e58f3a1). Click here to learn what that means.\nThe diff coverage is 41.66%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1132   +/-\n=========================================\n  Coverage          ?   85.26%         \n=========================================\n  Files             ?       24         \n  Lines             ?     5096         \n  Branches          ?      697         \n=========================================\n  Hits              ?     4345         \n  Misses            ?      551         \n  Partials          ?      200\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/connection.py | 84.43% <41.66%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e58f3a1...1e6023e. Read the comment docs.\n. # Codecov Report\nMerging #1147 into stable will decrease coverage by 0.05%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nstable   #1147      +/-\n=========================================\n- Coverage   81.95%   81.9%   -0.06%   \n=========================================\n  Files          20      20            \n  Lines        3780    3780            \n  Branches      575     575            \n=========================================\n- Hits         3098    3096       -2   \n- Misses        525     527       +2   \n  Partials      157     157\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 69.48% <0%> (-0.74%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 160caa4...0ff275e. Read the comment docs.\n. # Codecov Report\nMerging #1147 into stable will decrease coverage by 0.05%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nstable   #1147      +/-\n=========================================\n- Coverage   81.95%   81.9%   -0.06%   \n=========================================\n  Files          20      20            \n  Lines        3780    3780            \n  Branches      575     575            \n=========================================\n- Hits         3098    3096       -2   \n- Misses        525     527       +2   \n  Partials      157     157\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 69.48% <0%> (-0.74%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 160caa4...0ff275e. Read the comment docs.\n. # Codecov Report\nMerging #1148 into stable will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nstable   #1148   +/-\n======================================\n  Coverage    81.9%   81.9%         \n======================================\n  Files          20      20         \n  Lines        3780    3780         \n  Branches      575     575         \n======================================\n  Hits         3096    3096         \n+ Misses        527     526    -1   \n- Partials      157     158    +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/adapters/base_connection.py | 69.48% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2f6c19f...aa6d9b1. Read the comment docs.\n. # Codecov Report\nMerging #1148 into stable will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nstable   #1148   +/-\n======================================\n  Coverage    81.9%   81.9%         \n======================================\n  Files          20      20         \n  Lines        3780    3780         \n  Branches      575     575         \n======================================\n  Hits         3096    3096         \n  Misses        527     527         \n  Partials      157     157\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2f6c19f...56147a7. Read the comment docs.\n. # Codecov Report\nMerging #1162 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1162   +/-\n=======================================\n  Coverage   85.56%   85.56%         \n=======================================\n  Files          25       25         \n  Lines        5093     5093         \n  Branches      686      686         \n=======================================\n  Hits         4358     4358         \n  Misses        551      551         \n  Partials      184      184\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| pika/channel.py | 94.54% <\u00f8> (\u00f8) | :arrow_up: |\n| pika/adapters/twisted_connection.py | 93.06% <\u00f8> (\u00f8) | :arrow_up: |\n| pika/adapters/blocking_connection.py | 87.59% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b58ff2e...c5f4f07. Read the comment docs.\n. \n",
    "hannuvisti": "Can I email you instead of posting the code here?  \nThe code may be unusable without the publisher code, but most errors have occurred overnight when there is no activity on any of the channels, so it might be worthwhile just running the receiver and seeing what happens.  . I'll see what I can do but this will be next week. I need to sanitise the code first to remove any proprietary elements while keeping it still fully functional. This is also a part of a larger production system, with an external configuration server containing routing keys to be listened to, and I need to import this part to publisher and subscriber with static data.  \nI will let you know if I manage to actually do all that. \nI understand your resources are limited, thanks for replying so quickly.  In my particular case, I have built a mechanism to restart the listener as soon as it exits for any reason. It will do. I just wanted to report this as I use Pika a lot and this is the only time I have encountered any stability issues. . This may actually be a threading issue. I thought each thread had its own Pika connection but it appears one of them is sharing with the main thread.  \nFeel free to close this, the explanation seems plausible.  . ",
    "amulyas": "I think its bug in heartbeat with Blocking connection but I will post on google groups. ",
    "nanvel": "https://www.rabbitmq.com/tutorials/amqp-concepts.html\n\nFor applications that use multiple threads/processes for processing, it is very common to open a new channel per thread/process and not share channels between them.\n\nIt sounds like open one connection and have a channel per thread.. ",
    "mvallebr": "I am not using threads, it's a single threaded application. I have, however, 2 instances of Blocking connection, 1 to consume (I consume using channel.consume() method which returns a generator) and 1 to publish to a different exchange. \nI will try to create an example project, publish on github and post the link here. The problem is - the error happens just after some time, it will be hard to reproduce on purpose. I will try though.. Luke, I think we found the problem and instead we would like to provide you a pull request, I am just trying to get permission from my company first. . ",
    "LukaAndrojna": "import logging\nlogging.getLogger('pika').setLevel(logging.WARNING). ",
    "rmk135": "Hello @dumbbell, @gmr, @tonyg, @vitaly-krugl,\nCan I, please, ask you to pay some attention on this issue?\nThanks,\nRoman. Hello @michaelklishin ,\nThanks in advance for giving some understanding on what's going on. However, I need to ask if you see any light in the end of the tunnel and have any kind of target date, when this release could be seen on PyPi?\nI'm really sorry for disturbing you, guys, but I need to know this because it affects my business.\nRoman. Thanks, guys\nWill wait until you will release it.. ",
    "streamliner18": "But you just reproduced the problem!\nAfter your publishing call, the broker should raise an exception stating that a channel exception occurred (due to the nonexistent queue), and closes the channel. You can verify from the management console that the channel and all of its associated consumer tags are gone. However,\n\nNo exception raised from the callback (which seems to be expected)\nstart_consuming did not raise an exception, but its channel is dead\nstart_consuming did not return, despite this in the documentation:\n\n\nBlockingChannel.start_consuming()\nProcesses I/O events and dispatches timers and basic_consume callbacks until all consumers are cancelled.\n\nIn fact all the consumers are cancelled. But the function weren't returning.\nFyi, my platform is python (miniconda3) 3.6, pika latest, macOS 10.12.2. I retested this on my linux box (same python and pika, Ubuntu 16.04 x64), same problem. Both using a docker version of rabbitmq:3.6.10 with manually enabled management plugin.. ",
    "oleynikandrey": "I thought that it will keep trying to connect to the brocker every 5 seconds and will continue to handle messages when broker become available. \nI don't know how does it work in 0.10.0.  . What is the correct way to get pika reconnecting say every 5 seconds while RabbitMQ is down?. @lukebakken, I have tried with default value for connection_attempts. ",
    "hairyhum": "It does not look like this client should necessary have that kind of feature. Effectively for non-blocking connections the synchronous_publisher_example.html example is demonstrating a reasonable approach to reconnections and promotes better code quality by separating consumer and producer to make recoverable connection.\nBlocking connection can be recovered in exception handling style (similar to php client). For example:\n```python\nwhile True:\n    connection = pika.BlockingConnection(parameters)\nchannel = connection.channel()\nchannel.basic_consume(on_message, 'queue')\n\ntry:\n    channel.start_consuming()\nexcept pika.exceptions.ConnectionClosed:\n    LOGGER.info('Connection closed. Recovering')\n    continue\nexcept KeyboardInterrupt:\n    channel.stop_consuming()\n\nconnection.close()\nbreak\n\n``. @vitaly-krugl is there any usage examples inexamplesdirectory and docs indocsdirectory which should be updated after this change?. @masell it looks like there is no test to check thextype parsing with 2.7. Can you add one?. @masell yes, but it uses typeS, notx`. Or do I miss something?. I can do that. A I understand it should make the code safer to copy-paste. Is my understanding correct?. What do you think of combining multiple hosts and connection recovery examples and having just one example file for both?. ",
    "prasenforu": "What I trying to do instead of hostname & port I want to use url as a endpoint.. Thanks,\nDoes it without port like below.\nBasically my case its a http endpoint (http://rabbitmqservices.test.in)\namqp://username:password@rabbitmqservices.test.in/[?query-string]. ",
    "kmonson": "How is this coming along? \nThe VOLTTRON project is exploring RabbitMQ as an alternative to ZMQ going forward. gevent support is required for this to happen. \nWe really like what rabbit offers. It would allow us to move away from much (all?) of our custom routing and authorization solution. \nI cannot commit to anything, but we may be able to help out with this if needed.. Could the monkey patching you are doing in the test be causing problems?. How is this adapter cooperative? I don't see anywhere where you are releasing control to other greenlets in your code. You are still just using the normal stdlib socket after setting it to block.. @vitaly-krugl That was my thought as well. In this case I think you would only need to patch socket. patch_all() is kind of a shotgun approach that is usually not advised.\nIt seems like this could be more elegantly fixed by overriding _create_tcp_connection_socket to return a gevent socket. Then no patching is required.. > Would\nassert 'no_ack' not in kwargs, \"'no_ack' is obsolete. Use 'auto_ack' instead.\"\nbe a too much hand-holding?\nPersonally I love it when library developers do this kind of hand holding. It saves me time and saves them user help requests. . ",
    "cainbit": "There is still 4 cases fail, I can't point out the reasons, I had wrote some code, not using unittest, to test the same cases on my local env, it runs well. . @kmonson I also think this is the most probable cause, I'm trying to find a way to fix the test to work with gevent.. OK, I will fix this and add test in next commit.. ",
    "13steinj": "any updates on this?. @lukebakken more than just that, I'd like to change the implementation slightly. \nI haven't used timers and as such would need to look into whether or not it's okay for them to just be noop, but I don't like the fact that the ioloop is a mocked object, and the connection internally starts reading the loop in a separate greenlet.\nIMO the ioloop start/stop should be used, keeping a greenlet in memory and then killing it when necessary on stop (documenting that since this is using greenlets and communicating across separate threads that the IOLoop stop does not guarantee that stop will stop the io loop in its tracks but rather send a signal notifying the greenlet to die, and in theory in that small window another cycle of the loop may be run-- it's a race condition that can't really be avoided so long as reading the loop is on a separate green thread). @Kampii I myself don't have a vested interest as I don't personally use pika right now in a non company setting, and the issue at work has been put on the backburner (because it was surprisingly easier to just use gevent + a blocking connection + reset the connection upon failure until the rewrite of the internals were done). \nI'll take a look at said rewrite when home and if it wasn't too major as to also require major changes in a custom adapter I wrote I'll open a PR, otherwise you'll have to wait until the weekend.. ",
    "Kampii": "@13steinj I know that you are busy, but can you take a look at this? We left this issue hanging in our app and still hoping it can be fixed.. ",
    "lanzz": "pika v0.10.0\nPython 2.7.6\nUbuntu 14.04.4\nIt is similar to #640, but is not specific to Python 3; rather it is caused by the assumption that the int type will be 32-bit, which is not true on 64-bit platforms.. This should be enough to trigger it on a 64-bit platform (assumes you already have a channel open):\npython\nchannel.queue_declare('queue_name', arguments={'x-message-ttl': 2592000000})\nThis is a 30-day TTL, might seem a lot but in our use cases we route failed tasks to a separate queue where we can inspect and requeue them manually if needed.\nThis can be worked around by explicitly casting the value to long, but that should not be necessary:\npython\nchannel.queue_declare('queue_name', arguments={'x-message-ttl': long(2592000000)}). The code I initially highlighted should be fairly easy to change to do the right thing:\nhttps://github.com/pika/pika/blob/master/pika/data.py#L118-L123\nRight now it chooses the output size based on the input Python type, which does not align with reality. It should explicitly check if the input value will fit into 32 bits and choose 64-bit output if it doesn't, regardless of the Python type. Alternatively, it could always choose to encode integers as 64-bit values I guess. Arguably, it should also raise a ValueError if given a value that doesn't fit in 64 bits (as a cursory glance seems to indicate 64-bit integers are the largest integer type supported in AMQP).. ",
    "katajakasa": "Squashed, documentation updated. Ready.. You are enabling delivery confirmations after every message publish. You only need to do it once, after creating the channel. Now you have a duplicate on_delivery_confirmation callback after the second publish call.. Interesting, I expected the release to return a clean version string since that is what it shows in python documentation (https://docs.python.org/3/library/platform.html#platform.release).\nAs the LINUX_VERSION is only checked if python does not natively support TCP_USER_TIMEOUT, it might be bet to just wrap the contents of the function to a try-except and catch ValueError, then return None on exception. This might make old kernels with weird release strings disable the TCP_USER_TIMEOUT support, but that is probably preferable to completely blowing up pika with new kernels.\nI'd offer a patch but I'm currently away from my dev machine, sorry ;). Actually, looks like py-amqp has a solution for this: https://github.com/celery/py-amqp/blob/master/amqp/platform.py#L24\nEdit: Also, py-amqp has had the exact same problem: https://github.com/celery/py-amqp/issues/119. ",
    "Jimexist": "sorry i didn't see the comment. ",
    "adriancarayol": "@gmr any ideas? I think the django requests are mixed with pika. @lukebakken I have created a project with the configuration of the project: \nhttps://github.com/adriancarayol/issue-871\nSteps:\n\ninstall requeriments.txt\nmakemigrations + migrate\nrunserver\nenter the URL: 127.0.0.1:8000/example\nUpload a file larger than 30 MB\n\nin the rabbitmq log file I also found this:\n```\n=ERROR REPORT==== 6-Sep-2017::00:34:55 ===\nError on AMQP connection <0.2183.0> (127.0.0.1:36466 -> 127.0.0.1:5672, vhost: '/', user: 'guest', state: running), channel 2:\noperation none caused a connection exception frame_error: \"type 3, first 16 octets = \u00ab132,2,158,13,120,86,89,177,63,47,150,109,77,252,\\n                            102,160\u00bb: {invalid_frame_end_marker,83}\"\n=ERROR REPORT==== 6-Sep-2017::00:34:58 ===\nclosing AMQP connection <0.2183.0> (127.0.0.1:36466 -> 127.0.0.1:5672):\nfatal_frame_error\n```\nWireshark captures:\nhttp://imgur.com/fxoKunol.png\nhttp://imgur.com/dyjfivll.png\nhttp://imgur.com/R3QYOS8l.png. @lukebakken - I already use pika in the project, through \"asgi_rabbitmq\" package.. @lukebakken same as: @ #349 . @kfrendrich, @lukebakken, problem solved for me if use: https://github.com/adriancarayol/asgi_rabbitmq/tree/patch-1 (and pika 0.11.0). @lukebakken yes, frame error has been solved, we are waiting for profit404 to do the pull request.. ",
    "apisarek": "IMPORTANT EDIT (at the end of comment): There is a better solution for that problem which is not exponential. My problem disappeared but I still doubt that there is need for having your own implementation of default argument.\nLet's assume that _DEFAULT is not for internal use. Then we can write something like this\ndef get_connection_parameters(\n        host: str,\n        port: Optional[int] = None,\n        credentials: Optional[PlainCredentials] = None\n) -> ConnectionParameters:\n    default = ConnectionParameters._DEFAULT\n    parameters = {\n        'host': host,\n        'port': port or default,\n        'credentials': credentials or default\n    }\n    return ConnectionParameters(**parameters)\nNow let's assume that we want to be good engineers and don't look into places that we were told explicitly not to look: Designates default parameter value; internal use. Then our previous example must turn into:\ndef get_connection_parameters(\n        host: str,\n        port: Optional[int] = None,\n        credentials: Optional[PlainCredentials] = None,\n) -> ConnectionParameters:\n    parameters = {}\n    if port is None and credentials is None:\n        parameters = {\n            'host': host,\n        }\n    elif port is None and credentials is not None:\n        parameters = {\n            'host': host,\n            'credentials': credentials\n        }\n    elif port is not None and credentials is None:\n        parameters = {\n            'host': host,\n            'port': port,\n        }\n    elif port is not None and credentials is not None:\n        parameters = {\n            'host': host,\n            'port': port,\n            'credentials': credentials\n        }\n    return ConnectionParameters(**parameters)\nI haven't reduced boolean expressions on purpose so you can get the idea better. Now port and credentials are optional, so there are 4 options. If the function had let's say 10 optional parameters then we would get 1024 conditional branches.\nAnd as we can see in:\ndef __init__(self,  # pylint: disable=R0913,R0914,R0912\n                 host=_DEFAULT,\n                 port=_DEFAULT,\n                 virtual_host=_DEFAULT,\n                 credentials=_DEFAULT,\n                 channel_max=_DEFAULT,\n                 frame_max=_DEFAULT,\n                 heartbeat=_DEFAULT,\n                 ssl=_DEFAULT,\n                 ssl_options=_DEFAULT,\n                 connection_attempts=_DEFAULT,\n                 retry_delay=_DEFAULT,\n                 socket_timeout=_DEFAULT,\n                 locale=_DEFAULT,\n                 backpressure_detection=_DEFAULT,\n                 blocked_connection_timeout=_DEFAULT,\n                 client_properties=_DEFAULT,\n                 **kwargs):\nthere are 16 optional arguments, which gives us 2^16 = 65536 conditional branches. If you are paid per LOC then it's really good idea to spend time!\nThere exists simple solution to this problem which is backward compatible. One may change\nif heartbeat is not self._DEFAULT:\n            self.heartbeat = heartbeat\ninto\nif not self.is_default(heartbeat):def get_connection_parameters2(\n        host: str,\n        port: Optional[int] = None,\n        credentials: Optional[PlainCredentials] = None,\n) -> ConnectionParameters:\n    parameters = {}\n    if port is None and credentials is None:\n        parameters = {\n            'host': host,\n        }\n    elif port is None and credentials is not None:\n        parameters = {\n            'host': host,\n            'credentials': credentials\n        }\n    elif port is not None and credentials is None:\n        parameters = {\n            'host': host,\n            'port': port,\n        }\n    elif port is not None and credentials is not None:\n        parameters = {\n            'host': host,\n            'port': port,\n            'credentials': credentials\n        }\n    return ConnectionParameters(**parameters)\n            self.heartbeat = heartbeat\nwhere is_default(self, parameter) is defined as:\ndef is_default(self, parameter):\n    return parameter is self._DEFAULT or parameter is None\nand do it for every parameter in constructor.\nBETTER SOLUTION: One can create dict with all parameters and then go through dict and delete those which have value equal None.\n. ",
    "zjj": "@michaelklishin  this pull request seems a bit ... well, but the current heartbeat policy doesn't seem good. please, check this simple patch. thx. @michaelklishin leave some flexibility to users .... @michaelklishin your patch is ok, but I think it's not good for pika to have its own cleaver policy to choose a value. it's a user's duty ???  . @jordanburke  see Michael's commit 3027890081adaa067268aa4839638a32734c263f. we overload BlockingConnection _flush_output, with Rlock to fix this, but not sure if just flush_output with lock is enough for threading safe. pull request #902 . is the lock for _flush_output enough ???. @michaelklishin  see issue #892. @michaelklishin  some tests are not covered.. python2 socket.err\npython3 either OSError or IOError works.. this patch shall be merge after #904,  We found STREAM is much stabler than DGRAM  on Windows while large amount of msgs.. ",
    "jordanburke": "Is 0 supposed to still turn off heartbeats? If so it looks like there's still an issue related to recent changes in the 11.1 or 11.2 release:\n```\n    @staticmethod\n    def _negotiate_integer_value(client_value, server_value):\n        \"\"\"Negotiates two values. If either of them is 0 or None,\n        returns the other one. If both are positive integers, returns the\n        smallest one.\n    :param int client_value: The client value\n    :param int server_value: The server value\n    :rtype: int\n\n    \"\"\"\n    if client_value == None:\n        client_value = 0\n    if server_value == None:\n        server_value = 0\n\n    # this is consistent with how Java client and Bunny\n    # perform negotiation, see pika/pika#874\n    if client_value == 0 or server_value == 0:\n        val = max(client_value, server_value)\n    else:\n        val = min(client_value, server_value)\n\n    return val\n\n```\nWith this there's no way 0 can turn off heartbeats, unless the server is set to 0 as well which invalidates the purpose of the 0 switch. Else if the functionality has changed the doc should be updated to indicate that 0 no longer switches off heartbeats. . ",
    "jbfondo": "Exactly this issue happened to me recently. \nI'll try to post some code when i'm be able to reproduce it with a simple snippet.\nScenario:\n- Several processes pubish in queue1.\n- ProcessX listen from queue1, does some stuff and publish to queue2.\n- ProcessX use a Threadpool to execute the function that process every message received. Just after this call, ProcessX acks this message (channel.basic_ack).\n- The function that process the message, at the end, publish another message in queue2.\nWhen using Pika 0.10.0 all work fine.\nWhen I upgrade to pika 0.11.0 the issue opened by @mosquito  happened.\nSolution 1: Remove threadpool and process the message in the same thread/process\nSolution 2: Remove ack. I don't need ack messages.\nSolution 3: Keep using pika 0.10.0\n. @mosquito @lukebakken,  I modified the asynchronous_consumer_example according to my last post.\nIf you let it run for a while consuming messages sent by the asynchronous_publisher_example, you can see, in a variable amount of time (maybe some minutes), that the consumer stops consuming until rabbitMQ server closes the connection.\nThe modified lines are marked by a comment \"# +++++\"\nI know that using threads is not a good practice when using pika, but it's curious the difference in operation between versions.\nasynchronous_consumer_example_modified.zip\n. ",
    "decaz": "@mosquito is this issue should be reopened? Is something need to be done from the aio-pika side to resolve such an issue or it's fully depends on pika?. @mosquito you mean that reverting to 0.10 helped? Or there is no problem now and aio-pika can depend on 0.11?. Found the solution: channel.confirm_delivery() had to be used.. Thanks! Currently I don't use this feature, just noticed this when researched.. @lukebakken for consistency it could be called global_ according to https://github.com/pika/pika/blob/e76e2eaeb1ea8b903934dc45bb843ff0fd315fc7/pika/spec.py#L1309-L1311. @lukebakken done - #1166.. ",
    "finetuned89": "Is there any progress on this? I have a situation where I need both aio-pika and pika==0.11. Is there something I can do to help? I have tried to replicate whatever behavior is occurring and cannot do so using the script provided by @jbfondo.. ",
    "alfredodeza": "@lukebakken we are hitting this on a small plugin for an IRC bot that consumes messages and then dispatches them. The code is almost a 1:1 from the pika docs: https://github.com/alfredodeza/helga-pika/blob/master/helga_pika/plugin.py\nThe error is mostly the same as others have reported here:\nUnhandled Error\nTraceback (most recent call last):\n  File \"/Users/alfredo/python/helga/helga/bin/helga.py\", line 59, in main\n    run()\n  File \"/Users/alfredo/python/helga/helga/bin/helga.py\", line 37, in run\n    reactor.run()\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/Twisted-17.9.0-py2.7-macosx-10.11-x86_64.egg/twisted/internet/base.py\", line 1243, in run\n    self.mainLoop()\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/Twisted-17.9.0-py2.7-macosx-10.11-x86_64.egg/twisted/internet/base.py\", line 1252, in mainLoop\n    self.runUntilCurrent()\n--- <exception caught here> ---\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/Twisted-17.9.0-py2.7-macosx-10.11-x86_64.egg/twisted/internet/base.py\", line 878, in runUntilCurrent\n    call.func(*call.args, **call.kw)\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/pika-0.11.2-py2.7.egg/pika/heartbeat.py\", line 88, in send_and_check\n    return self._close_connection()\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/pika-0.11.2-py2.7.egg/pika/heartbeat.py\", line 125, in _close_connection\n    text)\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/pika-0.11.2-py2.7.egg/pika/connection.py\", line 1959, in _on_terminate\n    self._remove_heartbeat()\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/pika-0.11.2-py2.7.egg/pika/connection.py\", line 1461, in _remove_heartbeat\n    self.heartbeat.stop()\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/pika-0.11.2-py2.7.egg/pika/heartbeat.py\", line 109, in stop\n    self._connection.remove_timeout(self._timer)\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/pika-0.11.2-py2.7.egg/pika/adapters/base_connection.py\", line 144, in remove_timeout\n    self.ioloop.remove_timeout(timeout_id)\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/pika-0.11.2-py2.7.egg/pika/adapters/twisted_connection.py\", line 226, in remove_timeout\n    call.cancel()\n  File \"/Users/alfredo/.virtualenvs/helga-pika/lib/python2.7/site-packages/Twisted-17.9.0-py2.7-macosx-10.11-x86_64.egg/twisted/internet/base.py\", line 99, in cancel\n    raise error.AlreadyCalled\ntwisted.internet.error.AlreadyCalled: Tried to cancel an already-called event.\n\nWe are hitting this fairly consistently with incoming messages, but I can't tell for sure what is the exact situation that creates this. We are using version 0.11.2 Would be happy to provide any info that may produce a fix.\nIn the meantime, we are trying to not do any acks and see if that helps.. @lukebakken apologies, got completely confused, I came from issue #881 and assumed this was related. Should've read the comments more thoroughly.\nWill try to get a standalone/vanilla way to reproduce this. In the meantime, the removal of acks on every message has made this issue go away for us. Thanks again for helping out. ",
    "austinnichols101": "@lukebakken Is there a way to directly trigger a heartbeat send_and_check with a  BlockingConnection?  \nMy current workaround to \"do something with the channel\" is to check for the existence of an exchange, but I would prefer to simply force a heartbeat to show RMQ that my process is still alive. \nself._channel.exchange_declare(exchange=self.EXCHANGE, passive=True)\nMore specifically, I would like to be able to manually send a heartbeat frame from a BlockingConnection.\nself._connection._send_frame(self._new_heartbeat_frame())\n. ",
    "pehrsoderman": "Will try with 0.11.0 and see if it goes away!. ",
    "binarytrails": "It does go away on 0.11.0, cheers \ud83d\udc19 . ",
    "raidoz": "Perhaps similar to https://github.com/pika/pika/issues/875 ?. ",
    "proofit404": "Hi, guys.\nI'm asgi_rabbitmq developer. I will try to upgrade my library to recent pika and check it against given project.\nHope I will have time to do it soon.. ",
    "kfrendrich": "Hi\nI have the same problem with pika. I tried @adriancarayol 's example and got the some error.. @lukebakken  thanks.\nI used the same code that @adriancarayol posted, because it is hard to minify our application to demostrate it.\nRabbitMQ server 3.6.6.\nasgi-rabbitmq (0.5.3)\nasgiref (1.1.2)\nchannels (1.1.8)\nDjango (1.11.6)\npika (0.10.0)\nwith pika 0.11.0 I experienced the same problem:\nValueError: Must have completion callback with nowait=False\n. Thank you guys. ;)\nI am going to test it tomorrow morning.. @lukebakken @adriancarayol  the upgrade solved the problems. Thank you.. ",
    "desertkun": "Thank for for fast response.\nWould really appreciate an update on this.. Yeah, I would also appreciate a couple hints on how I can fix it myself.. @lukebakken @vitaly-krugl thank you very much guys, been waiting for this.. ",
    "Bogdanp": "@vitaly-krugl great! I've scanned the PR, but I don't have the pika implementation in my head atm. I assume this line ensures the polling mechanism's loop is interrupted and the callback is run immediately in the connection's thread, correct? If so, that looks great! Hope it gets merged. :D. Yup, I got that from the PR. My question was more about whether or not the select/poll/epoll/kqueue calls would keep blocking until the idle timeout or if they would be interrupted and thereby the callback would run immediately. Looking at this test it looks like they do get interrupted so that's great! Thanks!. ",
    "calJ2016": "@lukebakken \nhello, when I called  the publish message interface of my tornado application,  got a log  as below:\n2017-11-23 09:14:42,452 [INFO] Connection opened\n2017-11-23 09:14:42,453 [INFO] Adding connection close callback\n2017-11-23 09:14:42,453 [INFO] Creating a new channel\n2017-11-23 09:14:42,455 [INFO] Channel opened\n2017-11-23 09:14:42,457 [INFO] Adding channel close callback\n2017-11-23 09:14:42,457 [INFO] Declaring exchange dad\n2017-11-23 09:14:42,460 [INFO] Exchange declared\n2017-11-23 09:14:42,460 [INFO] Declaring queue service\n2017-11-23 09:14:42,463 [INFO] Binding dad to service with io\n2017-11-23 09:14:42,464 [INFO] Queue bound\n2017-11-23 09:14:45,963 [INFO] Issuing consumer related RPC commands\n2017-11-23 09:14:45,963 [INFO] Published message # 1\n2017-11-23 09:14:45,964 [INFO] Issuing Confirm.Select RPC command\n[I 171123 09:14:45 web:2063] 200 POST /pub (::1) 3.06ms\n2017-11-23 09:14:46,662 [INFO] Issuing consumer related RPC commands\n2017-11-23 09:14:46,663 [INFO] Published message # 2\n2017-11-23 09:14:46,663 [INFO] Issuing Confirm.Select RPC command\n[W 171123 09:14:46 callback:157] Duplicate callback found for \"1:Basic.Ack\"\n[W 171123 09:14:46 callback:157] Duplicate callback found for \"1:Basic.Nack\"\nI am not sure what this \"Duplicate callback found for \" means and is it bug or not?\nenvironment \uff1a\ntornado4.5 +pika0.11\nrelated code blocks:\nhandler of tornado\nclass publishDataHandler(tornado.web.RequestHandler):\n    def post(self):\n        rr = RestResponse()\n        mq = self.application.mq\n        r_data = json.loads(self.request.body.decode('utf-8'))\n        mq.start_publishing(r_data['data'])   #call this function to publish msg.\n        print(\" [x] Sent {}\".format(r_data['data']))\n        rr.success(\"ok!\")\n        self.write(rr.get_json())\n\nmq_processor.py  :Used to send and receive rabbitmq messages asynchronously\n```\nimport json\nfrom pika import adapters\nimport pika\nimport logging\nLOGGER = logging.getLogger('mq.service')\nclass MqProcessor(object):\nEXCHANGE = 'dad'\nEXCHANGE_TYPE = 'direct'\nQUEUE = 'service'\nROUTING_KEY = 'io'\nPUBLISH_INTERVAL = 1\n\ndef __init__(self, amqp_url):\n    \"\"\"Create a new instance of the consumer class, passing in the AMQP\n    URL used to connect to RabbitMQ.\n\n    :param str amqp_url: The AMQP url to connect with\n\n    \"\"\"\n    self._connection = None\n    self._channel = None\n    self._closing = False\n    self._consumer_tag = None\n    self._url = amqp_url\n    self._deliveries = None\n    self._message_number = None\n\n    self._deliveries = []\n    self._acked = 0\n    self._nacked = 0\n    self._message_number = 0\n\ndef connect(self):\n    LOGGER.info('Connecting to %s', self._url)\n    return adapters.TornadoConnection(pika.URLParameters(self._url),\n                                      self.on_connection_open)\n\ndef close_connection(self):\n    \"\"\"This method closes the connection to RabbitMQ.\"\"\"\n    LOGGER.info('Closing connection')\n    self._connection.close()\n\ndef add_on_connection_close_callback(self):\n    LOGGER.info('Adding connection close callback')\n    self._connection.add_on_close_callback(self.on_connection_closed)\n\ndef on_connection_closed(self, connection, reply_code, reply_text):\n    self._channel = None\n    if self._closing:\n        self._connection.ioloop.stop()\n    else:\n        LOGGER.warning('Connection closed, reopening in 5 seconds: (%s) %s',\n                       reply_code, reply_text)\n        self._connection.add_timeout(5, self.reconnect)\n\ndef on_connection_open(self, unused_connection):\n    LOGGER.info('Connection opened')\n    self.add_on_connection_close_callback()\n    self.open_channel()\n\ndef reconnect(self):\n    if not self._closing:\n        # Create a new connection\n        self._connection = self.connect()\n\ndef add_on_channel_close_callback(self):\n\n    LOGGER.info('Adding channel close callback')\n    self._channel.add_on_close_callback(self.on_channel_closed)\n\ndef on_channel_closed(self, channel, reply_code, reply_text):\n\n    LOGGER.warning('Channel %i was closed: (%s) %s',\n                   channel, reply_code, reply_text)\n    self._connection.close()\n\ndef on_channel_open(self, channel):\n    LOGGER.info('Channel opened')\n    self._channel = channel\n    self.add_on_channel_close_callback()\n    self.setup_exchange(self.EXCHANGE)\n\ndef setup_exchange(self, exchange_name):\n    LOGGER.info('Declaring exchange %s', exchange_name)\n    self._channel.exchange_declare(self.on_exchange_declareok,\n                                   exchange_name,\n                                   self.EXCHANGE_TYPE,\n                                   durable=True)\n\ndef on_exchange_declareok(self, unused_frame):\n    LOGGER.info('Exchange declared')\n    self.setup_queue(self.QUEUE)\n\ndef setup_queue(self, queue_name):\n    LOGGER.info('Declaring queue %s', queue_name)\n    self._channel.queue_declare(self.on_queue_declareok, queue_name, durable=True)\n\ndef on_queue_declareok(self, method_frame):\n    LOGGER.info('Binding %s to %s with %s',\n                self.EXCHANGE, self.QUEUE, self.ROUTING_KEY)\n    self._channel.queue_bind(self.on_bindok, self.QUEUE,\n                             self.EXCHANGE, self.ROUTING_KEY)\n\ndef add_on_cancel_callback(self):\n    LOGGER.info('Adding consumer cancellation callback')\n    self._channel.add_on_cancel_callback(self.on_consumer_cancelled)\n\ndef on_consumer_cancelled(self, method_frame):\n    LOGGER.info('Consumer was cancelled remotely, shutting down: %r',\n                method_frame)\n    if self._channel:\n        self._channel.close()\n\ndef acknowledge_message(self, delivery_tag):\n    LOGGER.info('Acknowledging message %s', delivery_tag)\n    self._channel.basic_ack(delivery_tag)\n\ndef on_message(self, unused_channel, basic_deliver, properties, body):\n    LOGGER.info('Received message # %s from %s: %s',\n                basic_deliver.delivery_tag, properties.app_id, body)\n    self.acknowledge_message(basic_deliver.delivery_tag)\n\ndef on_cancelok(self, unused_frame):\n    LOGGER.info('RabbitMQ acknowledged the cancellation of the consumer')\n    self.close_channel()\n\ndef stop_consuming(self):\n    if self._channel:\n        LOGGER.info('Sending a Basic.Cancel RPC command to RabbitMQ')\n        self._channel.basic_cancel(self.on_cancelok, self._consumer_tag)\n\ndef start_consuming(self):\n    LOGGER.info('Issuing consumer related RPC commands')\n    self.add_on_cancel_callback()\n    self._channel.basic_qos(prefetch_count=1)\n\n    self._consumer_tag = self._channel.basic_consume(self.on_message,\n                                                     self.QUEUE)\n\ndef start_publishing(self, data):\n    LOGGER.info('Issuing consumer related RPC commands')\n    self.publish_message(data)\n    self.enable_delivery_confirmations()\n\ndef enable_delivery_confirmations(self):\n    LOGGER.info('Issuing Confirm.Select RPC command')\n    self._channel.confirm_delivery(self.on_delivery_confirmation)\n\ndef on_delivery_confirmation(self, method_frame):\n    confirmation_type = method_frame.method.NAME.split('.')[1].lower()\n    LOGGER.info('Received %s for delivery tag: %i',\n                confirmation_type,\n                method_frame.method.delivery_tag)\n    if confirmation_type == 'ack':\n        self._acked += 1\n    elif confirmation_type == 'nack':\n        self._nacked += 1\n    self._deliveries.remove(method_frame.method.delivery_tag)\n    LOGGER.info('Published %i messages, %i have yet to be confirmed, '\n                '%i were acked and %i were nacked',\n                self._message_number, len(self._deliveries),\n                self._acked, self._nacked)\n\ndef publish_message(self,data):\n    if self._channel is None or not self._channel.is_open:\n        return\n\n    properties = pika.BasicProperties(app_id='example-publisher',\n                                      content_type='application/json',\n                                      delivery_mode=2)\n\n    self._channel.basic_publish(self.EXCHANGE, self.ROUTING_KEY,\n                                json.dumps(data, ensure_ascii=False),\n                                properties)\n    self._message_number += 1\n    self._deliveries.append(self._message_number)\n    LOGGER.info('Published message # %i', self._message_number)\n    # self.schedule_next_message()\n\n\ndef schedule_next_message(self):\n\n    LOGGER.info('Scheduling next message for %0.1f seconds',\n                self.PUBLISH_INTERVAL)\n    self._connection.add_timeout(self.PUBLISH_INTERVAL,\n                                 self.publish_message)\n\ndef on_bindok(self, unused_frame):\n\n    LOGGER.info('Queue bound')\n\n\ndef close_channel(self):\n\n    LOGGER.info('Closing the channel')\n    self._channel.close()\n\ndef open_channel(self):\n\n    LOGGER.info('Creating a new channel')\n    self._connection.channel(on_open_callback=self.on_channel_open)\n\ndef run(self):\n\n    self._connection = self.connect()\n    self._connection.ioloop.start()\n\ndef stop(self):\n    LOGGER.info('Stopping')\n    self._closing = True\n    self.stop_consuming()\n    self._connection.ioloop.start()\n    LOGGER.info('Stopped')\n\n```\ntornado app.py\n```\nmqp = MqProcessor('amqp://admin:dTvnNfA8@localhost:5672/')\ndad_application = tornado.web.Application(router.router_list, settings, db=mongodb)\ndad_application.mq = mqp\ndef main():\n    tornado.options.parse_command_line()\n    config.dictConfig(yaml.load(open('logging.yaml', 'r')))\n    http_server = tornado.httpserver.HTTPServer(dad_application, xheaders=True)\n    http_server.listen(options.port)\n    try:\n        mqp.run()\n    except KeyboardInterrupt:\n        mqp.stop()\nif name == \"main\":\n    main()\n```\n\ncould you help me ? thanks!. ",
    "avinassh": "@lukebakken I don't know of any packages which rely on platform info. Do you happen to know any? If so, I will start from there. . Updated PR, added a test for the same.. @katajakasa Thank you! I have updated the PR + added some more tests . ",
    "ofpiyush": "Our workaround was to clean it ourselves and monkey patch this line on platform just in case something else assumed platform.release() to be in X.X.X-something format.\nI am looking for the source of why platform.release() is assumed to be in that format. \n@katajakasa any ideas?. ",
    "linemos": "I am waiting for this as well.. ",
    "trenton42": "In the mean time, I changed my requirements to git+https://github.com/pika/pika.git@387cb322885cfcf54f9f14b63b03fac6c2ce0ca5#egg=pika (pinned to the specific commit with the fix instead of master in case some breaking changes are committed). @michaelklishin Thanks!\nand Thanks @lukebakken for the version fix. ",
    "jdavid": "That would be cool. Right now I am using closures for the callbacks passed to exchange_declare, queue_declare, queue_bind and basic_consume.\nGuess we can close this issue as a dup.. ",
    "bloodybeet": "So what I did was follow http://pika.readthedocs.io/en/0.11.2/examples/blocking_basic_get.html and get a basic queue producer and consumer up and running, which works. I had my basic_get with a sleep of 10 seconds if it saw a None. To make consumption faster, I then looked at actual blocking calls with time-outs, and moved to the consume functionality, looking at http://pika.readthedocs.io/en/0.11.2/examples/blocking_consumer_generator.html combined with http://pika.readthedocs.io/en/0.11.0/modules/adapters/blocking.html which states\n\ninactivity_timeout (float) \u2013 if a number is given (in seconds), will cause the method to yield None after the given period of inactivity; this permits for pseudo-regular maintenance activities to be carried out by the user while waiting for messages to arrive. If None is given (default), then the method blocks until the next event arrives. NOTE that timing granularity is limited by the timer resolution of the underlying implementation. NEW in pika 0.10.0.\n\nAs such, I tried the code in my first post, which gives the error indicated. . This makes a LOT of sense - thanks for your explanation. Sorry for this faulty ticket :). ",
    "fornof": "Hello , thanks for responding. \n in receive.py  working: https://gist.github.com/fornof/8624c476f2adadd73c1200e1f119dd9d#file-receive-py-working , I commented out those lines and it works meaning it prints out lines. When those lines that are necessary are left uncommented, Pika/python hangs. \nthe other receive.py is https://gist.github.com/fornof/8624c476f2adadd73c1200e1f119dd9d#file-receive-py and does not work (it hangs) \nI am running native python2.7 build for windows (located at c:\\Python27) as I was doing earlier. \n. Thank you so much for those steps and for your help. I tried all those steps and in CMD it works. when I tried running it in Cygwin, it still bombs. Cygwin is the issue. I learned something new about using cygwin today. I will transition my workflow to using CMD and powershell as pika runs great on those. \nThanks again! . ",
    "ccp-codex": "My mistake. Too much egg-nog this season. :). Hi @lukebakken I'm just reviewing the patch in my own fork for now but I will certainly commit back any changes if they will move the pull request forward. I'm pretty certain you won't want the custom stackless implementation I'll definitely be working on. ;). ",
    "sbidin": "I'm using Erlang 20.2 on an up-to-date Arch Linux.\nI'll provide a minimal working code example today.\n  . Here's the code. Run the same script twice, once for the consumer and once for the producer. The issue becomes visible only when a larger amount of messages is published (e.g. 10k). Please let me know if there's any way I can help.\n  . Yes, I'm searching the output to determine when it happens.\nI've updated the script to log those moments with a special message: it'll print out \"got retroactive nack?\" after every such bad confirmation.\nNote that the queue is limited to only 10 messages (on purpose, to test publisher confirms). The nacks in question are messages sent by the server to the producer, telling it that the queue has reached capacity.\nThanks for looking into this!\n  . Absolutely:\n\nYes, running both producer and consumer on the same machine.\nUsing Python 3.6.4.\nWhat exactly are you interested in? Hopefully this helps.. Nope, no cloud environment. This is my personal machine.\n\nAgain, thanks a lot for looking into this!. ",
    "tobiasfielitz": "Needs to be exchange_type.. ",
    "hvico": "Thanks lukebakken, you're absolutely right, it was as simple as that! I now see I checked the connection using openssl -s_client on port 5671 which gave me an OK message. Thank you very much.. ",
    "hugovk": "How about also testing on \"3.7-dev\" with Travis CI to make sure this really is fixed and remains fixed?\n\nhttps://github.com/pika/pika/blob/master/.travis.yml#L9\nhttps://docs.travis-ci.com/user/languages/python/#Specifying-Python-versions\nhttps://snarky.ca/how-to-use-your-project-travis-to-help-test-python-itself/. @lukebakken Sure, there you go! https://github.com/pika/pika/pull/934. When merged, the 2.6 skips in https://github.com/pika/pika/pull/932 can be removed.. Some of the changes are only possible after dropping 2.6 (formatters/set/dict), but sure, I can move all those to another PR! . This PR updated to only include dropping the EOL versions. Will create another for the others.\n\nThe coverage build job fails because:\n0.54s$ aws s3 cp --recursive s3://com-gavinroy-travis/pika/$TRAVIS_BUILD_NUMBER/ coverage\nfatal error: Unable to locate credentials\nThis is unrelated, and should be disabled from PRs and forks.. I think the problem is in pyev, it compares two StrictVersions, which don't work with the 3.7 dev version. Something like LooseVersion might work. \nThe last pyev release is this 0.9.0, from 2013. . Rebased, and now PyYAML fails to build on 3.7: \n```\n  ext/_yaml.c: In function \u2018__Pyx__ExceptionSave\u2019:\n  ext/_yaml.c:24143:19: error: \u2018PyThreadState\u2019 has no member named \u2018exc_type\u2019\n       type = tstate->exc_type;\n                     ^\n  ext/_yaml.c:24144:20: error: \u2018PyThreadState\u2019 has no member named \u2018exc_value\u2019\n       value = tstate->exc_value;\n                      ^\n  ext/_yaml.c:24145:17: error: \u2018PyThreadState\u2019 has no member named \u2018exc_traceback\u2019\n       *tb = tstate->exc_traceback;\n                   ^\n  ext/_yaml.c: In function \u2018__Pyx__ExceptionReset\u2019:\n  ext/_yaml.c:24152:22: error: \u2018PyThreadState\u2019 has no member named \u2018exc_type\u2019\n     member named \u2018exc_traceback\u2019\n       tstate->exc_traceback = local_tb;\n             ^\n  error: command 'gcc' failed with exit status 1\n\nFailed building wheel for PyYAML\n  Running setup.py clean for PyYAML\nFailed to build PyYAML\n```\nhttps://travis-ci.org/pika/pika/jobs/335516418. I've reported this to PyYAML: https://github.com/yaml/pyyaml/issues/126.. @lukebakken That'll be https://github.com/pika/pika/pull/937#issuecomment-361953868. . Good news, PyYAML 4.1 has just been released which now supports Python 3.7: https://github.com/yaml/pyyaml/issues/126.\nWhen https://github.com/aws/aws-cli/pull/3414 is merged to upgrade the PyYAML used by awscli, the awscli version pinning from this PR can be removed.. The coverage build job fails on PRs (eg. https://github.com/pika/pika/pull/933) and forks because:\n0.54s$ aws s3 cp --recursive s3://com-gavinroy-travis/pika/$TRAVIS_BUILD_NUMBER/ coverage\nfatal error: Unable to locate credentials\nThis should be disabled from PRs and forks.. And this PR:\nhttps://travis-ci.org/pika/pika/builds/397389341\nRan for 19 min 22 sec. Will do! . ",
    "stuartspotlight": "Part of the problem is that the problem only rears its head quite rarely, as in when I try to ingest and process a large number of files (e.g. 100,000 tweets) very quickly, up to 30 files a second. The code I'm using can be found at on the spotlight data github repo here.\n. I've simplified the code so that the library that handles pika is here:-\n```\n!/usr/bin/env python3\n\"\"\"\nProvides a bind function to plugins so they can simply bind a function to a queue.\n\"\"\"\nimport logging\nimport json\nfrom os import environ\nimport threading\nimport time\nimport pika\nimport datetime\nset up the logger globally\nlogger = logging.getLogger(\"nanowire-plugin\")\nclass heart_runner():\ndef __init__(self, connection):\n\n    if \"blockingconnection\" not in str(connection).lower() and \"mock\" not in str(connection).lower():\n        raise Exception(\"Heartbeat runner requires a connection to rabbitmq as connection, actually has %s, a %s\"%(str(connection), type(connection)))\n\n    if connection.is_open == False:\n        raise Exception(\"Heart runner's connection to rabbitmq should be open, is actually closed\")\n\n    self.connection = connection\n    self.internal_lock = threading.Lock()\n\n\ndef _process_data_events(self):\n        \"\"\"Check for incoming data events.\n        We do this on a thread to allow the flask instance to send\n        asynchronous requests.\n        It is important that we lock the thread each time we check for events.\n        \"\"\"\n\n        while True:\n            with self.internal_lock:\n                #This is a command to run a heartbeat. It has a limit of 10\n                #seconds becuase it kept hanging here due to pika not being fully thread\n                #safe\n                self.connection.process_data_events()\n                #This is how often to run the pacemaker\n                time.sleep(0.1)\n\ncreate a class so we can feed things into the on_request function\nclass on_request_class():\ndef __init__(self, function, name, output_channel):\n\n\n\n    self.name = name\n    self.function = function\n    self.output_channel = output_channel\n\n\ndef on_request(self, ch, method, props, body):\n\n    #check the channel is open\n    if not ch.is_open:\n        raise Exception(\"Input channel is closed\")\n\n\n    #check the body is a byte string\n    if not isinstance(body, bytes):\n        raise Exception(\"The body data should be a byte stream, it is actually %s, %s\"%(body, type(body)))\n\n\n\n    #set up logging inside the server functions\n    logger.setLevel(logging.DEBUG)\n\n    data = body.decode(\"utf-8\")\n\n    payload = json.loads(data)\n\n    returned = send(self.name, payload, ch, self.output_channel, method, props, self.function)\n\n\n    logger.info(\"Finished running user code at %s\"%str(datetime.datetime.now()))\n    logger.info(\"returned, %s\"%returned)\n    logger.info(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n\ndef failed_to_grab():\nraise Exception(\"REACHED TIMEOUT, ETHER BECAUSE THERE'S NOTHING ON THE QUEUE OR BECAUSE THE CONNECTION WAS HANGING AGAIN\")\n\nlogger.info(\"Failed to grab a message\")\n\ndef bind(function, name, version=\"1.0.0\", pulserate=30):\n    \"\"\"binds a function to the input message queue\"\"\"\n#set up the logging\nlogger.setLevel(logging.DEBUG)\n\nlogger.info(\"Running with pika version %s\"%str(pika.__version__))\n\n#write to screen to ensure logging is working ok\n#print \"Initialising nanowire lib, this is a print\"\nlogger.info(\"initialising nanowire lib\")\n\nlogger.info(\"initialising plugin: %s\"%name)\n\n#set the parameters for pika\nparameters = pika.ConnectionParameters(\n    host=environ[\"AMQP_HOST\"],\n    port=int(environ[\"AMQP_PORT\"]),\n    credentials=pika.PlainCredentials(environ[\"AMQP_USER\"], environ[\"AMQP_PASS\"]),\n    heartbeat=pulserate,\n    socket_timeout=10,\n    blocked_connection_timeout=120)\n\n#set up pika connection channels between rabbitmq and python\nconnection = pika.BlockingConnection(parameters)\n\n#This is an attempt to fix the problem with basic_consume hanging on consume sometimes. THIS\n#IS AN EXPEREMENT AND MAY WELL NOT WORK!!!!!\nconnection.add_timeout(900, failed_to_grab)\n\n#add something to stop the connection hanging when it's supposed to be grabbing. This does not work\nconnection.add_on_connection_blocked_callback(failed_to_grab)\ninput_channel = connection.channel()\noutput_channel = connection.channel()\n\n#The confirm delivery on the input channel is an attempt to fix the hanging problem. IT MIGHT NOT WORK!!!\ninput_channel.confirm_delivery()\noutput_channel.confirm_delivery()\n\n#setup the pacemaker\npacemaker = heart_runner(connection)\n\n#set up this deamon thread in order to avoid everything dying due to a heartbeat timeout\nthread = threading.Thread(target=pacemaker._process_data_events)\nthread.setDaemon(True)\nthread.start()\n\n\nlogger.info(\"initialised nanowire lib\", extra={\n    \"rabbit\": environ[\"AMQP_HOST\"]\n})\n\nlogger.info(\"rabbit: %s\"%environ[\"AMQP_HOST\"])\n\n\nlogger.info(\"consuming from %s\"%name)\n\ninput_channel.queue_declare(name, durable=True)\n\n#all the stuff that needs to be passed into the callback function is stored\n#in this object so that it can be easily passed through\nrequester = on_request_class(function, name, output_channel)\n\n#set the queue length to one\ninput_channel.basic_qos(prefetch_count=1)\n\n#The inactivity timeout might cause the pod to die and restart every 15 minutes when the queue is empty. This is\n#an attempt to fix the problem with the system hanging on basic_consume THIS IS EXPEREMENTAL AT THE MOMENT, IT MIGHT\n#NOT FIX THE PROBLEM!!!!\ninput_channel.basic_consume(requester.on_request, queue=name, no_ack=False)\n\nlogger.info(\"Created basic consumer\")\n\ninput_channel.start_consuming()\n\nlogger.info(\"Past start consuming, not sure whats going on...\")\n\ndef get_this_plugin(this_plugin, workflow):\n    \"\"\"ensures the current plugin is present in the workflow\"\"\"\n#perform type checking\nif not isinstance(this_plugin, str):\n    raise Exception(\"Plugin name must be a string\")\n\nif not isinstance(workflow, list):\n    raise Exception(\"Workflow must be a list\")\n\nif len(workflow) == 0:\n    raise Exception(\"Workflow is empty, something is wrong\")\n\n\nlogger.info(\"this_plugin: %s\"%this_plugin)\n\nfor i, workpipe in enumerate(workflow):\n    if workpipe[\"config\"][\"name\"] == this_plugin:\n        return i\nreturn -1\n\ndef send(name, payload, next_plugin, input_channel, output_channel, method, properties, function):\n    #Sends the result to the next queue in the pipeline\n#log some info about what the send function has been given\nlogger.info(\"consumed message\")\nlogger.info(\"channel %s\"%input_channel)\nlogger.info(\"method %s\"%method)\nlogger.info(\"properties %s\"%properties)\n\nresult = function(payload[\"jsonld\"])\n\n\n#send the info from this plugin to the next one in the pipeline\nsend_to_next_queue(next_plugin, result, output_channel)\n\n#Let the frontend know that we're done\ninput_channel.basic_ack(method.delivery_tag)\n\nreturn {}\n\ndef send_to_next_queue(next_queue, payload, output_channel):\nif next_queue != None:\n\n    #declare a queue for outputing the results to the next plugin\n    output_channel.queue_declare(\n        next_queue,\n        durable=True\n        )\n\n    #send the result from this plugin to the next plugin in the pipeline\n    _ = output_channel.basic_publish(\"\", next_queue, json.dumps(payload), pika.BasicProperties(content_type='text/plain', delivery_mode=2))\n\n```\nThis library can then be used like this:-\n```\nexample main function\nfrom example_rabbit_library import bind\ndef main(jsonld):\nif \"text\" in jsonld.keys():\n\n    jsonld[\"cat-finder\"] = {}\n    jsonld[\"cat-finder\"][\"@type\"] = \"PropertyValue\"\n    jsonld[\"cat-finder\"][\"measurementTechnique\"] = \"basic-cat-finding\"\n    jsonld[\"variableMeasured\"] = []\n\n    if \"cat\" in jsonld[\"text\"]:\n\n        result_dict = {}\n        result_dict[\"@type\"] = \"PropertyValue\"\n        result_dict[\"name\"] = \"found-cat\"\n        result_dict[\"value\"] = \"1\"\n\n\nreturn jsonld\n\nbind(main, \"example-function\")\n```\nThe function detailed is a something simple designed to pass data through quickly to give a better chance of replicating the intermittent bug. It might be worth trying to pass this through repeatedly to try and recreate the problem:-\n```\n{\n    \"@type\": \"SocialMediaPosting\",\n    \"@id\": \"955640572072652800\",\n    \"author\": {\n        \"@type\": \"Person\",\n        \"@id\": \"954949663173567891\",\n        \"name\": \"Anon\"\n    },\n    \"publisher\": {\n        \"@type\": \"Organization\",\n        \"name\": \"Twitter\"\n    },\n    \"dateCreated\": \"2018-01-23T07:17:22.253Z\",\n    \"datePublished\": \"2018-01-23T07:17:21.000Z\",\n    \"text\": Here is some example text\",\n    \"keywords\": [\n        \"example tweet\"\n    ],\n    \"interactionStatistic\": [\n        {\n            \"interactionType\": \"http://schema.org/CommentAction\",\n            \"@type\": \"InteractionCounter\",\n            \"userInteractionCount\": 0\n        },\n        {\n            \"interactionType\": \"http://schema.org/ReplyAction\",\n            \"@type\": \"InteractionCounter\",\n            \"userInteractionCount\": 0\n        },\n        {\n            \"interactionType\": \"http://schema.org/ShareAction\",\n            \"@type\": \"InteractionCounter\",\n            \"userInteractionCount\": 0\n        },\n        {\n            \"interactionType\": \"http://schema.org/FollowAction\",\n            \"@type\": \"InteractionCounter\",\n            \"userInteractionCount\": 0\n        }\n    ],\n    \"sharedContent\": [],\n    \"provider\": {\n        \"@type\": \"Organization\",\n        \"name\": \"Twitter for Android\"\n    },\n    \"mentions\": [],\n    \"@graph\": [\n    ]\n}\n```\nGiven that the problem arises when using a pacemaker and the exception references the process_data_events function could it be that if heartbeats are sent from separate threads the system hangs? Why does the introduction of a timeout to the process_data_events function produce an error? \nThanks for the help. I've found that waiting to start the pacemaker allows you to use the time_limit parameter on process_data_events. The code for the pacemaker is now:-\n```\nclass heart_runner():\ndef __init__(self, connection):\n\n    if \"blockingconnection\" not in str(connection).lower() and \"mock\" not in str(connection).lower():\n        raise Exception(\"Heartbeat runner requires a connection to rabbitmq as connection, actually has %s, a %s\"%(str(connection), type(connection)))\n\n    if connection.is_open == False:\n        raise Exception(\"Heart runner's connection to rabbitmq should be open, is actually closed\")\n\n    self.connection = connection\n    self.internal_lock = threading.Lock()\n\n\ndef _process_data_events(self):\n        \"\"\"Check for incoming data events.\n        We do this on a thread to allow the flask instance to send\n        asynchronous requests.\n        It is important that we lock the thread each time we check for events.\n        \"\"\"\n        time.sleep(1)\n\n        while True:\n            with self.internal_lock:\n                #This is a command to run a heartbeat. It has a limit of 10\n                #seconds becuase it kept hanging here due to pika not being fully thread\n                #safe\n\n                self.connection.process_data_events(time_limit=1)\n                time.sleep(1)\n                #This is how often to run the pacemaker\n\n```\nThis seems to have made the hanging problem significantly worse. I've made an effort to simplyfy the issue. I tried using the signal library to create a timeout for the pacemaker but that failed since signal cannot be used on a deamon thread so I'm going to post a simplified version of the full issue in case someone has a way of adding a timeout to the pacemaker on a second thread. A simplified version of the code is shown below:-\n```\n-- coding: utf-8 --\n\"\"\"\nCreated on Wed Oct 25 11:30:46 2017\n@author: stuart\n\"\"\"\n!/usr/bin/env python3\n\"\"\"\nProvides a bind function to plugins so they can simply bind a function to a queue.\n\"\"\"\nimport logging\nimport json\nfrom os import environ\nimport threading\nimport signal\nimport time\nimport pika\nset up the logger globally\nlogger = logging.getLogger(\"nanowire-plugin\")\nclass pacemaker_timeout:\ndef __init__(self, seconds=1, error_message='Timeout'):\n\n    self.seconds = seconds\n    self.error_message = error_message\n\ndef handle_timeout(self, signum, frame):\n\n    raise TimeoutError(self.error_message)\n\ndef __enter__(self):\n    signal.signal(signal.SIGALRM, self.handle_timeout)\n    signal.alarm(self.seconds)\n\ndef __exit__(self, type, value, traceback):\n    signal.alarm(0)\n\nclass heart_runner():\ndef __init__(self, connection):\n\n    if \"blockingconnection\" not in str(connection).lower() and \"mock\" not in str(connection).lower():\n        raise Exception(\"Heartbeat runner requires a connection to rabbitmq as connection, actually has %s, a %s\"%(str(connection), type(connection)))\n\n    if connection.is_open == False:\n        raise Exception(\"Heart runner's connection to rabbitmq should be open, is actually closed\")\n\n    self.connection = connection\n    self.internal_lock = threading.Lock()\n\n\ndef _process_data_events(self):\n        \"\"\"Check for incoming data events.\n        We do this on a thread to allow the flask instance to send\n        asynchronous requests.\n        It is important that we lock the thread each time we check for events.\n        \"\"\"\n\n        while True:\n            with self.internal_lock:\n                #This is a command to run a heartbeat. I have used the signal library\n                #to add a 10 second timeout because it kept hanging.\n                with pacemaker_timeout(seconds=10, error_message=\"Pacemaker has timed out\"):\n\n                    #This is the section causing the trouble. Since it hangs occasionally and I can't add a timeout using\n                    #the signal library in python 2\n                    self.connection.process_data_events()\n\n                time.sleep(1)\n                #This is how often to run the pacemaker\n\ncreate a class so we can feed things into the on_request function\nclass on_request_class():\ndef __init__(self, function, name, output_channel):\n\n\n    self.name = name\n    self.function = function\n    self.output_channel = output_channel\n\n\ndef on_request(self, ch, method, props, body):\n\n    #when a piece of data is found on the queue it is picked off and converted into a dictionary\n    payload = json.loads(body)\n\n\n    returned = send(self.name, payload, ch, self.output_channel, method, props, self.function)\n\ndef bind(function, name, version=\"1.0.0\", pulserate=30):\n    \"\"\"binds a function to the input message queue\"\"\"\n#set the parameters for pika, here we are setting the pika host and details\nparameters = pika.ConnectionParameters(\n    host=environ[\"AMQP_HOST\"],\n    port=int(environ[\"AMQP_PORT\"]),\n    credentials=pika.PlainCredentials(environ[\"AMQP_USER\"], environ[\"AMQP_PASS\"]),\n    heartbeat=pulserate)\n\n#set up pika connection channels between rabbitmq and python\nconnection = pika.BlockingConnection(parameters)\n\n\n#add something to stop the connection hanging when it's supposed to be grabbing. This does not work\n#connection.add_on_connection_blocked_callback(failed_to_grab)\ninput_channel = connection.channel()\noutput_channel = connection.channel()\n\n#The confirm delivery on the input channel is an attempt to fix the hanging problem. IT MIGHT NOT WORK!!!\ninput_channel.confirm_delivery()\noutput_channel.confirm_delivery()\n\n#setup the pacemaker\npacemaker = heart_runner(connection)\n\n#set up this deamon thread in order to avoid everything dying due to a heartbeat timeout\nthread = threading.Thread(target=pacemaker._process_data_events)\nthread.setDaemon(True)\nthread.start()\n\n\n#make sure the queue we want to injest from exists\ninput_channel.queue_declare(name, durable=True)\n\n#all the stuff that needs to be passed into the callback function is stored\n#in this object so that it can be easily passed through\nrequester = on_request_class(function, name, output_channel)\n\n#set the queue length to one\ninput_channel.basic_qos(prefetch_count=1)\n\n#create the consumer\ninput_channel.basic_consume(requester.on_request, queue=name, no_ack=False)\n\n#here is where the basic consumer is created on the main thread\ninput_channel.start_consuming()\n\ndef send(name, payload, input_channel, output_channel, method, properties, function, next_queue):\n    \"\"\"unwraps a message and calls the user function\"\"\"  \nsending_payload = function(payload)\n\n#send the info from this plugin to the next one in the pipeline\nsend_result = output_channel.basic_publish(\"\", next_queue, json.dumps(sending_payload), pika.BasicProperties(content_type='text/plain', delivery_mode=2))\n\n\n#Let the rabbitmq know we're done with this one\ninput_channel.basic_ack(method.delivery_tag)\n\n\nEnd of rabbitMQ interface functions\n\ndef example_function(payload):\ntext = payload['text']\n\nlast_word = text.split(' ')[-1]\n\npayload[\"last_word\"] = last_word\n\nreturn payload\n\nThis is an example of how the bind function is used. What it is doing in this\nexample is irrelevant\nbind(example_function, 'example-function')\n```\nThe problem is on line 85 where I attempt to perform an artificial heartbeat, using process_data_events(), to rabbitMQ. This has a habbit of causing the system to hang producing no errors randomly and if you use the timeout parameter for process_data_events() this will cause the artificial heartbeat to hang every time it is called. Does anyone know a thread safe way to add a timeout to this separate thread that will kill python allowing the automated recovery system to restart the program thereby clearing the hanging problem. Please note that this method must work in both python 2 and python 3?\nThank you. \n\nHere are two screenshots from wireshark. They show that before the hang there are no heartbeats but as soon as the heartbeats start all other traffic stops. I've tried git cloning and installing using setup.py on 32723b8 and I get the error:-\nNo module named 'pika.compat'\nIs this a known issue with that commit?. I've git cloned version 1.0.0b1 and that does seem to have fixed the issue. large messages sent over ssl also appear to be working. ",
    "skizunov": "@lukebakken : I saw this on Windows 7 on a lower spec'ed machine. Using Python 3.6.4, and  aBlockingConnection on an SSL connection, just keep writing data using Channel.publish in a tight loop. With the previous code, it would disconnect. You won't see the issue if you put a small time.sleep between calls to Channel.publish.. ",
    "gbartl": "Closed untill I can sort out python 2.6 compat issues . ",
    "northtree": "Thanks @lukebakken \nBTW, I suggest to update the document as it's quite confusing.\n\narguments (dict) \u2013 Custom key/value pair arguments for the consumer. \n",
    "jacky1193610322": "3.6.10. ",
    "ronaldddilley": "Thanks for your quick response.. ",
    "adah1972": "Renaming no_ack to auto_ack makes sense. However, should we, maybe in a limited transitional period, use **kwargs to map no_ack to auto_ack, and (optionally) give a warning?. Oops, it seems I misinterpreted the scope of https://www.rabbitmq.com/api-guide.html. I found it by searching, which said:\n\nWhen manual acknowledgements are used, it is important to consider what thread does the acknowledgement. If it's different from the thread that received the delivery (e.g. Consumer#handleDelivery delegated delivery handling to a different thread), acknowledging with the multiple parameter set to true is unsafe and will result in double-acknowledgements, and therefore a channel-level protocol exception that closes the channel. Acknowledging a single message at a time can be safe.\n\nBut it is irrelevant, as the title is really \"Java Client API Guide\", not the generic cross-language API guide. So it does not include Pika\u2026\nGood to know there is a solution. Are you going to include it in 0.11.3 and what do you expect to be the release date?\nAs of the current version, can I achieve the same purpose on a different thread? Or am I only allowed to acknowledge the message in the callback thread (i.e. the main thread in my case)?. Where should I download the beta version and how can I get informed? Would you post the information here?. By the way, is there a best known method to handle late acknowledgement in the current version? I tried some methods, but nothing is satisfactory so far... Mainly because when there is no new message coming in, I cannot execute anything in the same thread as the consumer callback.. The generic Timer does not seem to work, as my test shows that the function run by the timer is in a different thread (on Mac/Linux at least). With your hint, I have found the method BlockingConnection.add_timeout, which seems to serve the purpose. Thank your for the help. (Of course, I am still looking forward to the nicer solution.)\nIn case someone else might be interested, I am posting my test code here (one interesting thing that I do not quite understand is: the code here is able to achieve higher throughput rate than the immediate-acknowledgement solution!):\n```python\n!/usr/bin/env python3\nimport json\nimport pika\nimport queue\nimport signal\nimport threading\nconnection = None\nack_queue = None\nsend_queue = None\nsend_channel = None\nconsumer_tag = None\nend_event = threading.Event()\nexit_event = threading.Event()\ntotal_count = 50000\nrabbitmq_credentials = pika.PlainCredentials('guest', 'guest')\nrabbitmq_host = 'localhost'\nrabbitmq_port = 5672\nrabbitmq_queue = 'test_queue'\ndef stop_consume():\n    global consumer_tag\n    if consumer_tag is not None:\n        send_channel.basic_cancel(consumer_tag)\n        consumer_tag = None\ndef send_from_queue():\n    while not exit_event.is_set():\n        count = 0\n        ack_list = []\n        while count < 100 and not send_queue.empty():\n            message = send_queue.get(block=False)\n            ack_list.append(message['delivery_tag'])\n            count += 1\n            global total_count\n            total_count -= 1\n    if ack_list:\n        print('Acking messages')\n        for tag in ack_list:\n            print('Acking', tag)\n            ack_queue.put(tag)\n        print('Acking done')\n\n    if send_queue.empty():\n        if total_count <= 0:\n            exit_event.set()\n            stop_consume()\n        exit_event.wait(0.25)\n\nend_event.set()\nprint('Ending')\n\ndef ack_processed(ch):\n    while not ack_queue.empty():\n        tag = ack_queue.get(block=False)\n        ch.basic_ack(delivery_tag=tag)\ndef ack_processed_periodically():\n    ack_processed(send_channel)\n    connection.add_timeout(0.5, ack_processed_periodically)\ndef rabbitmq_callback(ch, method, properties, body):\n    try:\n        message = json.loads(body.decode())\n        message['delivery_tag'] = method.delivery_tag\n        while not exit_event.is_set():\n            try:\n                send_queue.put(message, block=False)\n                break\n            except queue.Full:\n                ack_processed(ch)\n                exit_event.wait(0.0625)\n    except ValueError:\n        print('Non-JSON message encountered:', body)\n        ch.basic_ack(delivery_tag=method.delivery_tag)\ndef handle_signal(signum, stack):\n    exit_event.set()\nglobal consumer_tag\nif consumer_tag is not None:\n    print('Signal', signum,\n          'is received.  Stopped receiving requests...')\n    stop_consume()\n\ndef repeat_send(channel, tries):\n    message = {\n        \"id\": 0,\n        \"content\": \"\"\n    }\n    for i in range(tries):\n        message['id'] = i\n        message['content'] = 'Hello World' + str(i)\n        channel.basic_publish(exchange='',\n                              routing_key=rabbitmq_queue,\n                              body=json.dumps(message),\n                              properties=pika.BasicProperties(\n                                  delivery_mode=2,  # make message persistent\n                              ))\ndef main():\n    signal.signal(signal.SIGTERM, handle_signal)\nglobal ack_queue\nglobal connection\nglobal send_queue\nglobal send_channel\nglobal consumer_tag\n\nconnection = pika.BlockingConnection(pika.ConnectionParameters(\n    rabbitmq_host, rabbitmq_port, '/', rabbitmq_credentials))\nsend_channel = connection.channel()\nsend_channel.queue_declare(queue=rabbitmq_queue,\n                           durable=True)\n\nrepeat_send(send_channel, total_count)\n\nack_queue = queue.Queue(10000)\nsend_queue = queue.Queue(10000)\nsend_thread = threading.Thread(target=send_from_queue)\nsend_thread.start()\n\nsend_channel.basic_qos(prefetch_count=10000)\nconsumer_tag = send_channel.basic_consume(\n    rabbitmq_callback, queue=rabbitmq_queue)\nack_processed_periodically()\nsend_channel.start_consuming()\n\nwhile not end_event.is_set():\n    ack_processed(send_channel)\n    end_event.wait(0.25)\nack_processed(send_channel)\n\nsignal.signal(signal.SIGINT, handle_signal)\nif name == \"main\":\n    main()\n```. I am trying the master version now. I had a few problems.\n\nsetup.py seems to have missed 'pika.compat' in packages.\nAfter fixing the problem above and running python3 setup.py install, my original test failed with the following error:\n\n$ ./test_pika.py\nTraceback (most recent call last):\n  File \"./test_pika.py\", line 119, in <module>\n    main()\n  File \"./test_pika.py\", line 112, in main\n    rabbitmq_callback, queue=rabbitmq_queue)\nTypeError: basic_consume() got multiple values for argument 'queue'\nI am really surprised that you changed both the order and names of arguments of basic_consume! :-(\nIn 0.11.2:\npython\n    def basic_consume(self,\n                      consumer_callback,\n                      queue,\n                      no_ack=False,\n                      exclusive=False,\n                      consumer_tag=None,\n                      arguments=None):\nIn current master:\npython\n    def basic_consume(self,\n                      queue,\n                      on_message_callback,\n                      auto_ack=False,\n                      exclusive=False,\n                      consumer_tag=None,\n                      arguments=None):\nIt means all code calling basic_consume has to be changed for your 1.0. It looks very dangerous to me. It does not look to me a necessary change (unlike the change from no_ack to auto_ack, which eliminates the unclear negative). I do not feel there are issues with the original order or name.\nOnly after fixing the above two issues did I have a smooth run to enjoy add_callback_threadsafe. I did not find a problem in it per se.. For completeness, my current test code:\n```python\n!/usr/bin/env python3\nimport json\nimport pika\nimport queue\nimport signal\nimport threading\nconnection = None\nsend_queue = None\nsend_channel = None\nconsumer_tag = None\nexit_event = threading.Event()\ntotal_count = 10000\nrabbitmq_credentials = pika.PlainCredentials('guest', 'guest')\nrabbitmq_host = 'localhost'\nrabbitmq_port = 5672\nrabbitmq_queue = 'test_queue'\ndef stop_consume():\n    global consumer_tag\n    if consumer_tag is not None:\n        send_channel.basic_cancel(consumer_tag)\n        consumer_tag = None\ndef ack_func(tag):\n    def inner_func():\n        send_channel.basic_ack(delivery_tag=tag)\nreturn inner_func\n\ndef send_from_queue():\n    while not exit_event.is_set():\n        count = 0\n        ack_list = []\n        while count < 100 and not send_queue.empty():\n            message = send_queue.get(block=False)\n            ack_list.append(message['delivery_tag'])\n            count += 1\n            global total_count\n            total_count -= 1\n    if ack_list:\n        print('Acking messages')\n        for tag in ack_list:\n            print('Acking', tag)\n            connection.add_callback_threadsafe(ack_func(tag))\n        print('Acking done')\n\n    if send_queue.empty():\n        if total_count <= 0:\n            exit_event.set()\n            stop_consume()\n        exit_event.wait(0.25)\n\ndef rabbitmq_callback(ch, method, properties, body):\n    try:\n        message = json.loads(body.decode())\n        message['delivery_tag'] = method.delivery_tag\n        send_queue.put(message)\n    except ValueError:\n        print('Non-JSON message encountered:', body)\n        ch.basic_ack(delivery_tag=method.delivery_tag)\ndef handle_signal(signum, stack):\n    exit_event.set()\nglobal consumer_tag\nif consumer_tag is not None:\n    print('Signal', signum,\n          'is received.  Stopped receiving requests...')\n    stop_consume()\n\ndef repeat_send(channel, tries):\n    message = {\n        \"id\": 0,\n        \"content\": \"\"\n    }\n    for i in range(tries):\n        message['id'] = i\n        message['content'] = 'Hello World' + str(i)\n        channel.basic_publish(exchange='',\n                              routing_key=rabbitmq_queue,\n                              body=json.dumps(message),\n                              properties=pika.BasicProperties(\n                                  delivery_mode=2,  # make message persistent\n                              ))\ndef main():\n    signal.signal(signal.SIGTERM, handle_signal)\nglobal connection\nglobal send_queue\nglobal send_channel\nglobal consumer_tag\n\nconnection = pika.BlockingConnection(pika.ConnectionParameters(\n    rabbitmq_host, rabbitmq_port, '/', rabbitmq_credentials))\nsend_channel = connection.channel()\nsend_channel.queue_declare(queue=rabbitmq_queue,\n                           durable=True)\n\nrepeat_send(send_channel, total_count)\n\nsend_queue = queue.Queue(10000)\nsend_thread = threading.Thread(target=send_from_queue)\nsend_thread.start()\n\nsend_channel.basic_qos(prefetch_count=1000)\nconsumer_tag = send_channel.basic_consume(\n    on_message_callback=rabbitmq_callback, queue=rabbitmq_queue)\nsend_channel.start_consuming()\n\nsignal.signal(signal.SIGINT, handle_signal)\nif name == \"main\":\n    main()\n``. @vitaly-krugl More careful testing with the code above revealed a few problems. Some messages remain unacknowledged on program exit. I need to changestop_consume()toconnection.add_callback_threadsafe(stop_consume)insend_from_queue`.\nIs this the proper way? Is basic_cancel thread-safe or not? If it is, should there be something like basic_cancel_soft?. @lukebakken Let me be very specific about the API change. Hopefully we can argue over it for once and all.\n0: Any API change is bad (or, do not punish your users).\nI suppose this is self-evident. API change breaks existing code, and invalidates documentation. Your users will need to re-read the documentation (if it is up to date), and change the current code.\nGranted, some API might not be originally designed very well. We are human, and we all err. No code is perfect, and no code can be (anyway, non-trivial code can't be). Sometimes we need to live with imperfections. Over-pursuit of perfection may even be harmful.\n1: When change is necessary, the benefits should be big enough to overcome the harm.\nSometimes we just have to change. If software does not change, in most cases it is dead. It does not conflict with point 0, but amends it. Just be sure the trade-off is good overall.\nThe change from basic_consume(self, consumer_callback, queue, \u2026) to basic_consume(self, queue, on_message_callback, \u2026) looks a bit arbitrary, especially the order change. It does not look to me beneficial enough to justify the API change.\n2: When change is necessary, provide a smooth path for the upgrade.\nIt is important not to make API users err easily. The common practice in kernel develop is always to use a new syscall number for new/changed APIs, and obsolete syscall numbers are never reused. Libtool provides shared library versioning to make sure when a library is upgraded, existing binaries can either link to an old version or abort on running. Compiled languages usually ensure that changed API will cause build-time failures (by using different types of arguments, or even changing the function name).\nRegretfully, nothing like this happens for the basic_consume API change. Programs just break at runtime. The change is neither backward-compatible nor forward-compatible. A breakdown may happen when someone accidentally updates a package. This is very dangerous and very bad.\nWhat is worse, there is no way to write code that can run on both 0.11.2 and 1.0.0b, excepting the ugliness as follows:\npython\nfrom packaging import version\n\u2026\nif version.parse(pika.__version__) < version.parse('0.99'):\n    channel.basic_consume(rabbitmq_callback, rabbitmq_queue)\nelse:\n    channel.basic_consume(rabbitmq_queue, rabbitmq_callback)\nIn summary, I do not think this API change is worthwhile. I would suggest keeping the new name but revert the order (basic_consume(self, on_message_callback, queue, \u2026) is acceptable to me).. @vitaly-krugl Currently, calling basic_cancel from another thread seems to be the most straightforward way to stop consuming (excepting the add_timeout workaround)... I will change to the new way when the new version is released.. I understand fully most of your pro-change arguments. But there is one gotcha. When there are API changes, traditional libraries often will have a separate release to ensure compatibility is not broken, and those releases can be installed and used side by side without conflicts. I suppose Pika will remain \u2018pika\u2019 in PyPI, right? In that case, people may incidentally install the wrong version\u2026\nSo a helpful exception will definitely help. (Unless you want to change to a new package name in PyPI.). ",
    "RasmusWL": "closing and reopening to make Travis rerun the tests.\nThe travis build step 1255.8 (coverage) failed to do aws s3 cp --recursive s3://com-gavinroy-travis/pika/$TRAVIS_BUILD_NUMBER/ coverage. Okay, coverage step failed again on Travis. Hopefully you guys can help out :)\nLooking at the log again, I can see that the real error is probably the credentials:\n$ aws s3 cp --recursive s3://com-gavinroy-travis/pika/$TRAVIS_BUILD_NUMBER/ coverage\nfatal error: Unable to locate credentials. My bad. I was only look at open issues. Too bad that RTD are using this weird sorting behaviour :(. ",
    "ehp006": "@vitaly-krugl  Are the changes for #956 only on master or are they in a pika release yet ? If not, is there a target date to have #956 in a release ? \n--thanks. Hi all\nIs there a plan to release 0.12.0 ?\nOn Thu, May 10, 2018 at 5:50 AM vitaly-krugl notifications@github.com\nwrote:\n\nClosed #963 https://github.com/pika/pika/issues/963.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/pika/pika/issues/963#event-1619443700, or mute the\nthread\nhttps://github.com/notifications/unsubscribe-auth/AQQmLU_dQ3t5b4YbreAfuhuqhgz0D8nQks5txA1sgaJpZM4SJOOo\n.\n\n\n-- \nHemang Patel\n. ",
    "stugots": "Does the requirement to execute within the same thread also apply to Channel.basic_nack?. ",
    "darcoli": "The time.sleep() function was used instead of BlockingConnection.sleep() to deliberately incur the issue. \nThe bunny client referenced in #874 (mentioned above)\n https://github.com/ruby-amqp/bunny/blob/master/lib/bunny/session.rb#L1172 also provides a way for clients to disable heartbeats. In my opinion, if the amqp specification allows for the heartbeats to be disabled, clients should have a way to do so.. @vitaly-krugl I agree although if clients are disabling heartbeats then one would assume that they understand the consequences. I would \"protect\" the clients by providing sensible defaults (ie by enabling heartbeats or accepting server setting) if nothing is specified explicitly. But in my opinion giving a warning as well is unnecessary. This is analogous to disabling publish confirmations - in which case no warning is given but still there are consequences which the publisher must be well aware of. . Ah ok then I fully agree - I thought you meant warning emitted by code ;). maybe this can be reconsidered given the discussion in #965 . Hi @lukebakken. Yes i am implementing the suggested changes. Quick question, I'd like to reuse the CallbackManager instance in Connection to implement the suggested callback for heartbeat tuning. However, I am missing if there is a way to get the return value of the user's callback after invoking self.callbacks.process(..., ON_CONNECTION_TUNE,...). In CallbackManager I do not see the return value of the callbacks being saved. Is this the correct way or should this be done differently?. I thought it might help to keep all callbacks handled consistently instead of reimplementing something that might already exist. I quickly skimmed through the code of CallbackManager and saw that it does some checking as well. But of course it may not apply here so I will take your advise and implement as suggested ;). I thought about reusing the setter logic but now I do not want the callback to return another callable (in this fix I changed the setter to allow callable) - so essentially it is not duplicating the same logic because the logic of the check changes when then TUNE connection state is reached. I thought about making the logic in heartbeat's setter in Parameters depend on the Connection state (ie. do not allow callable during TUNE state) but I thought doing so is not a clean solution (would then need to add another property inside Parameters for the connection state).. Yes not allowing the callback return None was actually on purpose. The rationale behind it is that if the client passes a callback heartbeat, then an intention to control the heartbeat is shown and the client then needs to specify it. Of course, if the client wants to accept the server's proposal, it should just return with the same value that was passed as an argument to the callback. But, of course, I can change it to allow None to be returned if you think it makes more sense to do so ;). I meant passed the server's proposal as an argument to the callback. I will change the wording ;). No URLParameter's actually cannot have callable. Will fix the docstring.. Agreed. URL parameter heartbeat can only have an int value.. Instead of using None, it should just be omitted from the URL.. done. done. yes, fixed. changed, pls review ;). changed, pls review. changed pls review. Hehe ok ... still not completely untangled but for sure its better than my version :D . Yes good point. Will change and will update the relevant documentation as well. ",
    "banjoh": "Your approach of closing the connection when stop is called isn't thread safe. I know pika is not thread safe, but this particular use case allows crossing the thread boundary according to this issue.\nWorth noting that self._connection.ioloop.add_callback_threadsafe(self._connection.ioloop.stop) does not solve my problem either. It's a new method in v1.0.0.. > Note that it is unusual to close a connection without having done anything with it, like start a consumer.\nI stripped off my business logic not to have this issue convoluted. I noticed this in a full-fledged application that's creating queues, binding to queues, publishing to exchanges... the full works.. Side note: I noticed that stop_ioloop_on_close constructor parameter has been removed. Has that functionality been deprecated or is there a new API?\nPardon my ignorance if it's an obvious question. I've not taken a look at the new changes coming up but noticed your change.. ",
    "nskalis": "thanks a lot for looking into it. please find below the requested information:\n\nmessaging.py\n```\nimport logging\nlogger = logging.getLogger(\"ipacc\")\n\nimport os\nimport socket\nimport json\nimport importlib\nimport pika\ndef import_mod(mod_name):\n    try:\n        mod = importlib.import_module(mod_name)\n        return mod\n    except Exception as e:\n        logger.error('importing {} caused {}'.format(mod_name,repr(e)), exc_info=True)\n    return\nclass AMQPconsume(object):\nEXCHANGE        = None\nEXCHANGE_TYPE   = None\nQUEUE           = None\nROUTING_KEY     = None\n\nCACHE           = None\nLOOKUP          = None\n\nPLUGIN          = None\nLABEL           = None\n\ndef __init__(self, amqp_url):\n    self._connection = None\n    self._channel = None\n    self._closing = False\n    self._consumer_tag = None\n    self._url = amqp_url\n\ndef connect(self):\n    logger.info('Connecting to %s', self._url)\n    return pika.SelectConnection(pika.URLParameters(self._url), self.on_connection_open, stop_ioloop_on_close=False)\n\ndef on_connection_open(self, unused_connection):\n    logger.info('Connection opened')\n    self.add_on_connection_close_callback()\n    self.open_channel()\n\ndef add_on_connection_close_callback(self):\n    logger.info('Adding connection close callback')\n    self._connection.add_on_close_callback(self.on_connection_closed)\n\ndef on_connection_closed(self, connection, reply_code, reply_text):\n    self._channel = None\n    if self._closing:\n        self._connection.ioloop.stop()\n    else:\n        logger.warning('Connection closed, reopening in 5 seconds: (%s) %s', reply_code, reply_text)\n        self._connection.add_timeout(5, self.reconnect)\n\ndef reconnect(self):\n    self._connection.ioloop.stop()\n    if not self._closing:\n        self._connection = self.connect()\n        self._connection.ioloop.start()\n\ndef open_channel(self):\n    logger.info('Creating a new channel')\n    self._connection.channel(on_open_callback=self.on_channel_open)\n\ndef on_channel_open(self, channel):\n    logger.info('Channel opened')\n    self._channel = channel\n    self.add_on_channel_close_callback()\n    self.setup_exchange(self.EXCHANGE)\n\ndef add_on_channel_close_callback(self):\n    logger.info('Adding channel close callback')\n    self._channel.add_on_close_callback(self.on_channel_closed)\n\ndef on_channel_closed(self, channel, reply_code, reply_text):\n    logger.warning('Channel %i was closed: (%s) %s', channel, reply_code, reply_text)\n    self._connection.close()\n\ndef setup_exchange(self, exchange_name):\n    logger.info('Declaring exchange %s', exchange_name)\n    self._channel.exchange_declare(self.on_exchange_declareok, exchange_name, self.EXCHANGE_TYPE)\n\ndef on_exchange_declareok(self, unused_frame):\n    logger.info('Exchange declared')\n    self.setup_queue(self.QUEUE)\n\ndef setup_queue(self, queue_name):\n    logger.info('Declaring queue %s', queue_name)\n    self._channel.queue_declare(self.on_queue_declareok, queue_name)\n\ndef on_queue_declareok(self, method_frame):\n    logger.info('Binding %s to %s with %s', self.EXCHANGE, self.QUEUE, self.ROUTING_KEY)\n    self._channel.queue_bind(self.on_bindok, self.QUEUE, self.EXCHANGE, self.ROUTING_KEY)\n\ndef on_bindok(self, unused_frame):\n    logger.info('Queue bound')\n    self.start_consuming()\n\ndef start_consuming(self):\n    logger.info('Issuing consumer related RPC commands')\n    self.add_on_cancel_callback()\n    self._channel.basic_qos(prefetch_count=1)\n    self._consumer_tag = self._channel.basic_consume(self.on_message, self.QUEUE)\n\ndef add_on_cancel_callback(self):\n    logger.info('Adding consumer cancellation callback')\n    self._channel.add_on_cancel_callback(self.on_consumer_cancelled)\n\ndef on_consumer_cancelled(self, method_frame):\n    logger.info('Consumer was cancelled remotely, shutting down: %r', method_frame)\n    if self._channel:\n        self._channel.close()\n\ndef on_message(self, unused_channel, basic_deliver, properties, body):\n    logger.info('#%s received message %s from %s', self.LABEL, basic_deliver.delivery_tag, properties.app_id)\n\n    try:\n        body = json.loads(body.decode(\"utf-8\"))\n        self.PLUGIN.stash( *self.PLUGIN.calculate(body['identifier'], body['timestamp'], body['message']) )\n    except Exception as e:\n        logger.error('#{} processing message `{}` at `{}` caused `{}`'.format(self.LABEL,info['identifier'],info['timestamp'],repr(e)), exc_info=True)\n\n    self.acknowledge_message(basic_deliver.delivery_tag)\n\ndef acknowledge_message(self, delivery_tag):\n    logger.info('#%s acknowledging message %s', self.LABEL, delivery_tag)\n    self._channel.basic_ack(delivery_tag)\n\ndef stop_consuming(self):\n    if self._channel:\n        logger.info('Sending a Basic.Cancel RPC command to RabbitMQ')\n        self._channel.basic_cancel(self.on_cancelok, self._consumer_tag)\n\ndef on_cancelok(self, unused_frame):\n    logger.info('RabbitMQ acknowledged the cancellation of the consumer')\n    self.close_channel()\n\ndef close_channel(self):\n    logger.info('Closing the channel')\n    self._channel.close()\n\ndef run(self):\n    self._connection = self.connect()\n    self._connection.ioloop.start()\n\ndef stop(self):\n    logger.info('Stopping')\n    self._closing = True\n    self.stop_consuming()\n    self._connection.ioloop.start()\n    logger.info('Stopped')\n\ndef close_connection(self):\n    logger.info('Closing connection')\n    self._connection.close()\n\ndef run(RABBITMQ, customize):\n    worker = AMQPconsume('amqp://{}:{}@{}:{}/{}'.format(os.environ['USR_NETOPS'],os.environ['PASS_NETOPS'],RABBITMQ['ipaddr'],RABBITMQ['port'],RABBITMQ['vhost']))\n    worker.EXCHANGE     = RABBITMQ['exchange']\n    worker.EXCHANGE_TYPE= RABBITMQ['exchange_type']\n    worker.ROUTING_KEY  = RABBITMQ['routing_key']\n    worker.QUEUE        = customize\n    worker.PLUGIN       = import_mod('app.ipacc.plugin.{}'.format(customize))\n    worker.LABEL        = customize\n    if worker.PLUGIN is None:\n        return\n    try:\n        worker.run()\n    except Exception as e:\n        logger.error('consuming pmacct caused {}'.format(repr(e)), exc_info=True)\n        worker.stop()\n```\n\n\nrabbitmq version\n3.7.3\n\n\nerlang version\n20.2.3\n\n\nos details\n$ hostnamectl \n   Static hostname: xxx\n         Icon name: computer-server\n           Chassis: server\n        Machine ID: 892774a867e540f3a4208afa312cbfec\n           Boot ID: 1bf82670b74e4bdb92462f02bca91207\n  Operating System: CentOS Linux 7 (Core)\n       CPE OS Name: cpe:/o:centos:centos:7\n            Kernel: Linux 3.10.0-693.17.1.el7.x86_64\n      Architecture: x86-64\n\n\nrabbitmq server logs\n```\n\n\ntail -n 100 rabbit@server-name.log rabbit@server-name_upgrade.log log/crash.log.4\n==> rabbit@server-name.log <==\n2018-02-26 11:31:32.259 [info] <0.1957.105> closing AMQP connection <0.1957.105> (server-ip:25151 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:32.516 [info] <0.2009.105> accepting AMQP connection <0.2009.105> (server-ip:25153 -> server-ip:5673)\n2018-02-26 11:31:32.553 [info] <0.2009.105> connection <0.2009.105> (server-ip:25153 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:32.561 [info] <0.1992.105> accepting AMQP connection <0.1992.105> (server-ip:25155 -> server-ip:5673)\n2018-02-26 11:31:32.576 [info] <0.2028.105> accepting AMQP connection <0.2028.105> (server-ip:25157 -> server-ip:5673)\n2018-02-26 11:31:32.596 [info] <0.1992.105> connection <0.1992.105> (server-ip:25155 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:32.597 [info] <0.2028.105> connection <0.2028.105> (server-ip:25157 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:32.623 [info] <0.2009.105> closing AMQP connection <0.2009.105> (server-ip:25153 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:32.654 [info] <0.2028.105> closing AMQP connection <0.2028.105> (server-ip:25157 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:32.669 [info] <0.2014.105> accepting AMQP connection <0.2014.105> (server-ip:25159 -> server-ip:5673)\n2018-02-26 11:31:32.677 [info] <0.1992.105> closing AMQP connection <0.1992.105> (server-ip:25155 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:32.707 [info] <0.2014.105> connection <0.2014.105> (server-ip:25159 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:32.778 [info] <0.2014.105> closing AMQP connection <0.2014.105> (server-ip:25159 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:32.807 [info] <0.2049.105> accepting AMQP connection <0.2049.105> (server-ip:25161 -> server-ip:5673)\n2018-02-26 11:31:32.843 [info] <0.2049.105> connection <0.2049.105> (server-ip:25161 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:32.908 [info] <0.2049.105> closing AMQP connection <0.2049.105> (server-ip:25161 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:32.980 [info] <0.2091.105> accepting AMQP connection <0.2091.105> (server-ip:25163 -> server-ip:5673)\n2018-02-26 11:31:32.987 [info] <0.2104.105> accepting AMQP connection <0.2104.105> (server-ip:25165 -> server-ip:5673)\n2018-02-26 11:31:33.016 [info] <0.2091.105> connection <0.2091.105> (server-ip:25163 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:33.016 [info] <0.2104.105> connection <0.2104.105> (server-ip:25165 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:33.087 [info] <0.2076.105> accepting AMQP connection <0.2076.105> (server-ip:25167 -> server-ip:5673)\n2018-02-26 11:31:33.096 [info] <0.2104.105> closing AMQP connection <0.2104.105> (server-ip:25165 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:33.111 [info] <0.2091.105> closing AMQP connection <0.2091.105> (server-ip:25163 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:33.128 [info] <0.2076.105> connection <0.2076.105> (server-ip:25167 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:33.184 [info] <0.2076.105> closing AMQP connection <0.2076.105> (server-ip:25167 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:33.237 [info] <0.2133.105> accepting AMQP connection <0.2133.105> (server-ip:25169 -> server-ip:5673)\n2018-02-26 11:31:33.272 [info] <0.2133.105> connection <0.2133.105> (server-ip:25169 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:33.368 [info] <0.2133.105> closing AMQP connection <0.2133.105> (server-ip:25169 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:33.414 [info] <0.2144.105> accepting AMQP connection <0.2144.105> (server-ip:25171 -> server-ip:5673)\n2018-02-26 11:31:33.448 [info] <0.2144.105> connection <0.2144.105> (server-ip:25171 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:33.511 [info] <0.2127.105> accepting AMQP connection <0.2127.105> (server-ip:25173 -> server-ip:5673)\n2018-02-26 11:31:33.526 [info] <0.2144.105> closing AMQP connection <0.2144.105> (server-ip:25171 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:33.546 [info] <0.2127.105> connection <0.2127.105> (server-ip:25173 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:33.564 [info] <0.2137.105> accepting AMQP connection <0.2137.105> (server-ip:25175 -> server-ip:5673)\n2018-02-26 11:31:33.601 [info] <0.2137.105> connection <0.2137.105> (server-ip:25175 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:33.602 [info] <0.2127.105> closing AMQP connection <0.2127.105> (server-ip:25173 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:33.651 [info] <0.2137.105> closing AMQP connection <0.2137.105> (server-ip:25175 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:33.744 [info] <0.2189.105> accepting AMQP connection <0.2189.105> (server-ip:25177 -> server-ip:5673)\n2018-02-26 11:31:33.781 [info] <0.2189.105> connection <0.2189.105> (server-ip:25177 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:33.800 [info] <0.2171.105> accepting AMQP connection <0.2171.105> (server-ip:25179 -> server-ip:5673)\n2018-02-26 11:31:33.835 [info] <0.2171.105> connection <0.2171.105> (server-ip:25179 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:33.839 [info] <0.2189.105> closing AMQP connection <0.2189.105> (server-ip:25177 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:33.891 [info] <0.2171.105> closing AMQP connection <0.2171.105> (server-ip:25179 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:33.893 [info] <0.2219.105> accepting AMQP connection <0.2219.105> (server-ip:25181 -> server-ip:5673)\n2018-02-26 11:31:33.929 [info] <0.2219.105> connection <0.2219.105> (server-ip:25181 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:34.038 [info] <0.2219.105> closing AMQP connection <0.2219.105> (server-ip:25181 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:34.133 [info] <0.2238.105> accepting AMQP connection <0.2238.105> (server-ip:25183 -> server-ip:5673)\n2018-02-26 11:31:34.167 [info] <0.2238.105> connection <0.2238.105> (server-ip:25183 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:34.253 [info] <0.2238.105> closing AMQP connection <0.2238.105> (server-ip:25183 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:34.263 [info] <0.2249.105> accepting AMQP connection <0.2249.105> (server-ip:25185 -> server-ip:5673)\n2018-02-26 11:31:34.291 [info] <0.2256.105> accepting AMQP connection <0.2256.105> (server-ip:25187 -> server-ip:5673)\n2018-02-26 11:31:34.298 [info] <0.2256.105> connection <0.2256.105> (server-ip:25187 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:34.298 [info] <0.2249.105> connection <0.2249.105> (server-ip:25185 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:34.365 [info] <0.2256.105> closing AMQP connection <0.2256.105> (server-ip:25187 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:34.379 [info] <0.2249.105> closing AMQP connection <0.2249.105> (server-ip:25185 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:34.528 [info] <0.2257.105> accepting AMQP connection <0.2257.105> (server-ip:25189 -> server-ip:5673)\n2018-02-26 11:31:34.564 [info] <0.2257.105> connection <0.2257.105> (server-ip:25189 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:34.617 [info] <0.2257.105> closing AMQP connection <0.2257.105> (server-ip:25189 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:34.678 [info] <0.2268.105> accepting AMQP connection <0.2268.105> (server-ip:25191 -> server-ip:5673)\n2018-02-26 11:31:34.725 [info] <0.2292.105> accepting AMQP connection <0.2292.105> (server-ip:25193 -> server-ip:5673)\n2018-02-26 11:31:34.734 [info] <0.2292.105> connection <0.2292.105> (server-ip:25193 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:34.735 [info] <0.2268.105> connection <0.2268.105> (server-ip:25191 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:34.794 [info] <0.2268.105> closing AMQP connection <0.2268.105> (server-ip:25191 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:34.796 [info] <0.2292.105> closing AMQP connection <0.2292.105> (server-ip:25193 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:34.962 [info] <0.2314.105> accepting AMQP connection <0.2314.105> (server-ip:25195 -> server-ip:5673)\n2018-02-26 11:31:35.000 [info] <0.2314.105> connection <0.2314.105> (server-ip:25195 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.048 [info] <0.2314.105> closing AMQP connection <0.2314.105> (server-ip:25195 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.117 [info] <0.2315.105> accepting AMQP connection <0.2315.105> (server-ip:25197 -> server-ip:5673)\n2018-02-26 11:31:35.147 [info] <0.2340.105> accepting AMQP connection <0.2340.105> (server-ip:25199 -> server-ip:5673)\n2018-02-26 11:31:35.151 [info] <0.2340.105> connection <0.2340.105> (server-ip:25199 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.151 [info] <0.2315.105> connection <0.2315.105> (server-ip:25197 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.156 [info] <0.2342.105> accepting AMQP connection <0.2342.105> (server-ip:25201 -> server-ip:5673)\n2018-02-26 11:31:35.190 [info] <0.2342.105> connection <0.2342.105> (server-ip:25201 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.201 [info] <0.2340.105> closing AMQP connection <0.2340.105> (server-ip:25199 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.220 [info] <0.2315.105> closing AMQP connection <0.2315.105> (server-ip:25197 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.258 [info] <0.2342.105> closing AMQP connection <0.2342.105> (server-ip:25201 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.319 [info] <0.2374.105> accepting AMQP connection <0.2374.105> (server-ip:25203 -> server-ip:5673)\n2018-02-26 11:31:35.358 [info] <0.2374.105> connection <0.2374.105> (server-ip:25203 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.424 [info] <0.2374.105> closing AMQP connection <0.2374.105> (server-ip:25203 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.443 [info] <0.2370.105> accepting AMQP connection <0.2370.105> (server-ip:25205 -> server-ip:5673)\n2018-02-26 11:31:35.478 [info] <0.2370.105> connection <0.2370.105> (server-ip:25205 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.538 [info] <0.2370.105> closing AMQP connection <0.2370.105> (server-ip:25205 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.586 [info] <0.2404.105> accepting AMQP connection <0.2404.105> (server-ip:25207 -> server-ip:5673)\n2018-02-26 11:31:35.615 [info] <0.2434.105> accepting AMQP connection <0.2434.105> (server-ip:25209 -> server-ip:5673)\n2018-02-26 11:31:35.619 [info] <0.2434.105> connection <0.2434.105> (server-ip:25209 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.619 [info] <0.2404.105> connection <0.2404.105> (server-ip:25207 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.671 [info] <0.2434.105> closing AMQP connection <0.2434.105> (server-ip:25209 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.683 [info] <0.2404.105> closing AMQP connection <0.2404.105> (server-ip:25207 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.690 [info] <0.2413.105> accepting AMQP connection <0.2413.105> (server-ip:25211 -> server-ip:5673)\n2018-02-26 11:31:35.727 [info] <0.2413.105> connection <0.2413.105> (server-ip:25211 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.770 [info] <0.2412.105> accepting AMQP connection <0.2412.105> (server-ip:25213 -> server-ip:5673)\n2018-02-26 11:31:35.793 [info] <0.2413.105> closing AMQP connection <0.2413.105> (server-ip:25211 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.804 [info] <0.2412.105> connection <0.2412.105> (server-ip:25213 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.829 [info] <0.2443.105> accepting AMQP connection <0.2443.105> (server-ip:25215 -> server-ip:5673)\n2018-02-26 11:31:35.863 [info] <0.2443.105> connection <0.2443.105> (server-ip:25215 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:35.866 [info] <0.2412.105> closing AMQP connection <0.2412.105> (server-ip:25213 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.928 [info] <0.2443.105> closing AMQP connection <0.2443.105> (server-ip:25215 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n2018-02-26 11:31:35.948 [info] <0.2473.105> accepting AMQP connection <0.2473.105> (server-ip:25217 -> server-ip:5673)\n2018-02-26 11:31:35.983 [info] <0.2473.105> connection <0.2473.105> (server-ip:25217 -> server-ip:5673): user 'user-name' authenticated and granted access to vhost 'ipacc'\n2018-02-26 11:31:36.040 [info] <0.2473.105> closing AMQP connection <0.2473.105> (server-ip:25217 -> server-ip:5673, vhost: 'ipacc', user: 'user-name')\n==> rabbit@server-name_upgrade.log <==\n==> log/crash.log.4 <==\n2018-02-18 10:02:36 =SUPERVISOR REPORT====\n     Supervisor: {<0.4445.2>,rabbit_channel_sup_sup}\n     Context:    shutdown_error\n     Reason:     shutdown\n     Offender:   [{nb_children,1},{name,channel_sup},{mfargs,{rabbit_channel_sup,start_link,[]}},{restart_type,temporary},{shutdown,infinity},{child_type,supervisor}]\n2018-02-18 10:18:04 =SUPERVISOR REPORT====\n     Supervisor: {<0.12651.2>,rabbit_channel_sup_sup}\n     Context:    shutdown_error\n     Reason:     shutdown\n     Offender:   [{nb_children,1},{name,channel_sup},{mfargs,{rabbit_channel_sup,start_link,[]}},{restart_type,temporary},{shutdown,infinity},{child_type,supervisor}]\n2018-02-18 13:33:15 =SUPERVISOR REPORT====\n     Supervisor: {<0.23211.4>,rabbit_channel_sup}\n     Context:    shutdown_error\n     Reason:     noproc\n     Offender:   [{pid,<0.23214.4>},{name,channel},{mfargs,{rabbit_channel,start_link,[1,<0.23205.4>,<0.23212.4>,<0.23205.4>,<<\"server-ip:22350 -> server-ip:5673\">>,rabbit_framing_amqp_0_9_1,{user,<<\"user-name\">>,[administrator],[{rabbit_auth_backend_internal,none}]},<<\"ipacc\">>,[{<<\"publisher_confirms\">>,bool,true},{<<\"connection.blocked\">>,bool,true},{<<\"basic.nack\">>,bool,true},{<<\"authentication_failure_close\">>,bool,true},{<<\"consumer_cancel_notify\">>,bool,true}],<0.23206.4>,<0.23213.4>]}},{restart_type,intrinsic},{shutdown,70000},{child_type,worker}]\n2018-02-18 14:07:13 =SUPERVISOR REPORT====\n     Supervisor: {<0.4942.5>,rabbit_channel_sup_sup}\n     Context:    shutdown_error\n     Reason:     noproc\n     Offender:   [{nb_children,1},{name,channel_sup},{mfargs,{rabbit_channel_sup,start_link,[]}},{restart_type,temporary},{shutdown,infinity},{child_type,supervisor}]\n2018-02-18 15:52:56 =SUPERVISOR REPORT====\n     Supervisor: {<0.19097.6>,rabbit_channel_sup_sup}\n     Context:    shutdown_error\n     Reason:     shutdown\n     Offender:   [{nb_children,1},{name,channel_sup},{mfargs,{rabbit_channel_sup,start_link,[]}},{restart_type,temporary},{shutdown,infinity},{child_type,supervisor}]\n2018-02-18 16:19:51 =SUPERVISOR REPORT====\n     Supervisor: {<0.15033.2>,rabbit_channel_sup_sup}\n     Context:    shutdown_error\n     Reason:     shutdown\n     Offender:   [{nb_children,1},{name,channel_sup},{mfargs,{rabbit_channel_sup,start_link,[]}},{restart_type,temporary},{shutdown,infinity},{child_type,supervisor}]\n```\n\npika client logs\n2018-02-26 10:53:56,476 | ERROR | 31230 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 10:53:56,477 | WARNING | 31230 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 10:53:56,477 | WARNING | 31230 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 10:53:56,477 | WARNING | 31230 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 10:54:41,789 | ERROR | 29516 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 10:54:41,790 | WARNING | 29516 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 10:54:41,790 | WARNING | 29516 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 10:54:41,790 | WARNING | 29516 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:03:23,536 | ERROR | 31702 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:03:23,537 | WARNING | 31702 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:03:23,537 | WARNING | 31702 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:03:23,537 | WARNING | 31702 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:08:43,306 | ERROR | 28749 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:08:43,307 | WARNING | 28749 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:08:43,307 | WARNING | 28749 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:08:43,308 | WARNING | 28749 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:09:40,886 | ERROR | 29693 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:09:40,886 | WARNING | 29693 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:09:40,886 | WARNING | 29693 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:09:40,887 | WARNING | 29693 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:13:35,387 | ERROR | 29811 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:13:35,388 | WARNING | 29811 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:13:35,388 | WARNING | 29811 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:13:35,388 | WARNING | 29811 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:16:24,094 | ERROR | 30698 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:16:24,095 | WARNING | 30698 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:16:24,095 | WARNING | 30698 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:16:24,096 | WARNING | 30698 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:18:46,448 | ERROR | 29339 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:18:46,449 | WARNING | 29339 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:18:46,449 | WARNING | 29339 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:18:46,449 | WARNING | 29339 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:19:35,847 | ERROR | 31230 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:19:35,847 | WARNING | 31230 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:19:35,847 | WARNING | 31230 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:19:35,848 | WARNING | 31230 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:21:33,096 | ERROR | 28985 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:21:33,097 | WARNING | 28985 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:21:33,097 | WARNING | 28985 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:21:33,097 | WARNING | 28985 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:24:29,999 | ERROR | 31348 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:24:29,999 | WARNING | 31348 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:24:29,999 | WARNING | 31348 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:24:30,000 | WARNING | 31348 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:24:35,887 | ERROR | 28867 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:24:35,887 | WARNING | 28867 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:24:35,887 | WARNING | 28867 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:24:35,888 | WARNING | 28867 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:25:19,924 | ERROR | 29398 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:25:19,925 | WARNING | 29398 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:25:19,925 | WARNING | 29398 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:25:19,925 | WARNING | 29398 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:26:25,457 | ERROR | 30343 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:26:25,457 | WARNING | 30343 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:26:25,458 | WARNING | 30343 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:26:25,458 | WARNING | 30343 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:26:36,722 | ERROR | 28395 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:26:36,723 | WARNING | 28395 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:26:36,723 | WARNING | 28395 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:26:36,724 | WARNING | 28395 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:33:14,303 | ERROR | 29339 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:33:14,304 | WARNING | 29339 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:33:14,304 | WARNING | 29339 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:33:14,304 | WARNING | 29339 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:33:31,457 | ERROR | 29162 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:33:31,458 | WARNING | 29162 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:33:31,458 | WARNING | 29162 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:33:31,459 | WARNING | 29162 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF\n2018-02-26 11:37:28,516 | ERROR | 30224 | base_connection | _handle_read | Read empty data, calling disconnect\n2018-02-26 11:37:28,517 | WARNING | 30224 | messaging | on_channel_closed | Channel 1 was closed: (-1) EOF\n2018-02-26 11:37:28,517 | WARNING | 30224 | connection | close | Suppressing close request on <SelectConnection CLOSED socket=None params=<URLParameters host=172.31.101.97 port=5673 virtual_host=ipacc ssl=False>>\n2018-02-26 11:37:28,518 | WARNING | 30224 | messaging | on_connection_closed | Connection closed, reopening in 5 seconds: (-1) EOF. ps. would you be so kind please to reopen the issue (I cannot do this by myself, since @lukebakken closed, @lukebakken has to re-open it). \n",
    "anapsix": "from Setting up Highly Available RMQ cluster on AWS Medium article:\n\nELB resets the connection with the application servers periodically. Thus, it is possible that your producers send a task to the ELB which gets lost because of this resetting of connection between the ELB and the nodes.\nIn such situation, we will need to enable confirmation of the tasks published. The solution discussed here is specific to Celery based integration with RMQ where backend is built on Django. The following needs to be set in settings.py in such case:\nBROKER_TRANSPORT_OPTIONS = {'confirm_publish': True}\nThis way, your publisher will wait for the RMQ to send an ack before sending any further tasks to the same queue.\n\nIf that's the case, I suspect, solution suggested in the article will work for other Load Balancers / Proxies. ",
    "quozd": "\ud83d\udc4d\ud83d\udc4d when this expected to be released?\nSent from my iPhone\n\nOn Apr 26, 2018, at 5:59 PM, Luke Bakken notifications@github.com wrote:\nClosed #985 via #1002.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "C-Duv": "Great. Sorry I didn't found it in the documentation. Thanks :)\nHere is an example for anyone stepping here and as lost as I was:\nPython\nparameters = pika.ConnectionParameters(\n    'amqp.example.com',\n    5672,\n    virtual_host='/',\n    client_properties={\n        'connection_name': 'My foobar client',\n    },\n)\nconnection = pika.BlockingConnection(parameters). ",
    "vahidmohsseni": "yes i'm sure about connecting to the right port. also in my first execution of my script it works good. but in continuous unexpectedly this error happens:\n505, 'UNEXPECTED_FRAME - expected method frame, got non method frame instead'\nI`m using bad solution now that i try to close previous connection and make new one. but this is not good solution for a pika error!\n. yes. i'm using threads. but for each thread i have Lock object on sending message. . Ok then. I try to use connections per thread.\nI will give thanks to you if you give me some information about what happenning exactly that this error occurs.. ",
    "hustshawn": "```\n$ pip freeze | grep -i pika\npika==0.11.2\n$ python --version\nPython 3.6.1\n```\n. Thank you and it fixed by add user to RabbitMQ. ",
    "timcroydon": "Ha! Amazing.\nThanks for clarifying the issue. Will have a closer look at RTD itself later. Cheers.. ",
    "will3942": "This appears to have fixed it (for now?):\nfrom gevent import monkey\nmonkey.patch_all(). cc: @stuartspotlight. ",
    "jstasiak": "I believe this is fixed in https://github.com/pika/pika/commit/32723b851c966042fac6304a6e6c79480c5e298e (not released yet).\nBe advised that gevent monkey patching is absolutely not an answer here, it has significant consequences other than silencing this issue (it does so because gevent's ssl socket write() implementation retries internally).. Thank you for the response, appreciate it!. ",
    "backloger": "Looks like it makes no sense anymore, because aio-pika vendored pika package and now planning to create own amqp driver: https://github.com/mosquito/aio-pika/issues/132#issuecomment-403459429. ",
    "levsa": "pip install pika --pre didn't pick the beta release but pip install pika==0.12.0b2 worked fine.\nIt seems to work better, thanks!. > Apart from big headaches, I would like to point out that in pika 0.11.2 this problem should not be present, but another takes places. When operating very long tasks connection is dropped as well.\nIs this due to that heartbeat response times out since the thread is occupied handling the previous message? That happened to me and that is why I turned of heartbeats.. I'm seeing something similar. I'm running without heartbeats and after some time of inactivity, messages end up queued in unacked state without being yielded from the channel.consume() call I'm using (see code below). After I restart the process and connection is made, consuming proceeds as normal.\nMy listen-loop looks as follows:\n```\n        connection = None\n        try:\n            connection = self._connection_factory.create()  # BlockingConnection\n            channel = connection.channel()\n            channel.queue_declare('incoming', durable=True)\n            logging.info(\"Waiting for RabbitMQ messages\")\n            generator = channel.consume('incoming', inactivity_timeout=60)\n            for message_tuple in generator:\n                if message_tuple is not None:  # None returned after inactivity timeout\n                    # tuple(spec.Basic.Deliver, spec.BasicProperties, str or unicode)\n                    method, properties, body = message_tuple\n                    if method is not None and body is not None:\n                        self._handle(channel, method, properties, body)\n            channel.cancel()\n        except Exception:\n            logging.exception(\"Exception in message receiver loop\")\n        if connection is not None:\n            connection.close()\n```\n. ",
    "bhack": "I meant if we can automate message count with res.method.message_count like in https://stackoverflow.com/a/13629296. Other API has http://pyrabbit.readthedocs.io/en/latest/api.html#pyrabbit.api.Client.get_queue_depth. ",
    "nikolaevigor": "You are right, I am using 0.11.2 version, which documentation is correct. Sorry for bothering :). ",
    "vitalibr": "more informations:\nmy publisher:\nhttps://gist.github.com/vitalibr/56ff3371d0af8e1b81039d571011412b\nmy consumer:\nhttps://gist.github.com/vitalibr/b2aee8c0fb7de888152453ae609b8660\nlogs from publisher:\n\n2018-04-21 14:48:01,750 - publisher - INFO - Connecting to amqp://user:pass@rabbitmq:5672/vhost\n2018-04-21 14:48:01,760 - publisher - INFO - Connection opened\n2018-04-21 14:48:01,760 - publisher - INFO - Creating a new channel\n2018-04-21 14:48:01,762 - publisher - INFO - Channel opened\n2018-04-21 14:48:01,762 - publisher - INFO - Adding channel close callback\n2018-04-21 14:48:01,763 - publisher - INFO - Declaring exchange message\n2018-04-21 14:48:01,765 - publisher - INFO - Exchange declared\n2018-04-21 14:48:01,765 - publisher - INFO - Declaring queue notifications\n2018-04-21 14:48:01,766 - publisher - INFO - Binding message to notifications with notificationsRountingKey\n2018-04-21 14:48:01,767 - publisher - INFO - Queue bound\n2018-04-21 14:48:01,768 - publisher - INFO - Issuing consumer related RPC commands\n2018-04-21 14:48:01,768 - publisher - INFO - Issuing Confirm.Select RPC command\n2018-04-21 15:52:07,041 - publisher - INFO - Received ack for delivery tag: 1\n2018-04-21 15:52:07,041 - publisher - INFO - Published 1 messages, 0 have yet to be confirmed, 1 were acked and 0 were nacked\n2018-04-21 15:52:07,041 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n2018-04-21 17:01:41,324 - publisher - INFO - Received ack for delivery tag: 2\n2018-04-21 17:01:41,325 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n2018-04-21 19:52:23,224 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n2018-04-21 20:00:32,669 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n2018-04-21 20:00:53,145 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n2018-04-22 03:00:12,356 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n2018-04-22 08:12:55,683 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n2018-04-22 11:11:49,733 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n2018-04-22 11:59:08,889 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n2018-04-22 12:00:40,934 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n2018-04-22 12:04:47,031 - publisher - INFO - Published message # {\"message\": \"xxxx\", \"source_msg_id\": 0}\n\nlogs from consumer:\n\n2018-04-20 22:41:52,813 - consumer - INFO - Connecting to amqp://user:pass@rabbitmq:5672/vhost\n2018-04-20 22:41:52,822 - consumer - INFO - Connection opened\n2018-04-20 22:41:52,822 - consumer - INFO - Creating a new channel\n2018-04-20 22:41:52,824 - consumer - INFO - Channel opened\n2018-04-20 22:41:52,824 - consumer - INFO - Adding channel close callback\n2018-04-20 22:41:52,824 - consumer - INFO - Declaring exchange: message\n2018-04-20 22:41:52,826 - consumer - INFO - Exchange declared: message\n2018-04-20 22:41:52,826 - consumer - INFO - Declaring queue notifications\n2018-04-20 22:41:52,827 - consumer - INFO - Binding message to notifications with notificationsRountingKey\n2018-04-20 22:41:52,832 - consumer - INFO - Queue bound: notifications\n2018-04-20 22:41:52,832 - consumer - INFO - Issuing consumer related RPC commands\n2018-04-20 22:41:52,832 - consumer - INFO - Adding consumer cancellation callback\n2018-04-21 15:52:01,799 - consumer - INFO - Received message # 1 from None: b'{\"message\": \"xxxx\", \"source_msg_id\": 0}'\n2018-04-21 15:52:01,800 - consumer - INFO - Acknowledging message 1\n2018-04-21 15:52:42,878 - consumer - INFO - Received message # 2 from None: b'{\"message\": \"xxxx\", \"source_msg_id\": 0}'\n2018-04-21 15:52:42,879 - consumer - INFO - Acknowledging message 2\n2018-04-21 17:01:41,320 - consumer - INFO - Received message # 3 from None: b'{\"xxxx\", \"source_msg_id\": 0}'\n2018-04-21 17:01:41,320 - consumer - INFO - Acknowledging message 3\n2018-04-21 19:52:14,987 - consumer - INFO - Received message # 4 from None: b'{\"message\": \"xxxx\", \"source_msg_id\": 0}'\n2018-04-21 19:52:14,987 - consumer - INFO - Acknowledging message 4\n2018-04-21 19:55:32,924 - consumer - INFO - Received message # 5 from None: b'{\"message\": \"xxxx\", \"source_msg_id\": 0}'\n2018-04-21  @19:55:32,925 - consumer - INFO - Acknowledging message 5\n\nthere are no error messages in the logs, or that the connection was closed, I do not understand what is going wrong.. ",
    "ajayimade": "Python version: 2.7.14\nPika:  0.11.0\nRabbitMQ: 3.2.4\nErlang: R16B03\nOS-  Ubuntu 14.04\nRabbitmq server is running. I can see its UI but whenever I tried to connect using above script it throws above error.\nIt was previously working. \nI tried to debug. I kept debugger loc before connection & tried connection code it works :( \nEven I tried to keep sleep before connection but wont succeed . When i tried to create object of above class, it throws error at BlockingConnection line\n    producer = Producer(QUEUE_NAMES[queue_name], **RABBITMQ).. ",
    "cellscape": "I no longer use pika with twisted, removing is fine by me.. ",
    "maggyero": "Hi @vitaly-krugl and @michaelklishin. I'm not a T.L.S. specialist but maybe I could help.\nFirst I noticed that you rely on the commonName field of the Subject section to set the domain name of the X.509 certificates in the pika/testdata/certs directory.\nRFC 2818 (published in May 2000) describes two methods to match a domain name against a certificate:\n\nIf a subjectAltName extension of type dNSName is present, that MUST be used as the identity. Otherwise, the (most specific) Common Name field in the Subject field of the certificate MUST be used. Although the use of the Common Name is existing practice, it is deprecated and Certification Authorities are encouraged to use the dNSName instead.\n\nSo the new standard is to set the domain names in the DNS or IP fields of the X509v3 Subject Alternative Name (SAN) section. Since the version 58, Google Chrome no longer falls back to the commonName field for server certificates lacking a  DNS or IP field (it shows an ERR_CERT_COMMON_NAME_INVALID error): see the bug report (2013) and the release (2017). I have indeed got this error while trying to connect to the RabbitMQ Management plugin in H.T.T.P.S. from Google Chrome using a server certificate lacking a proper SAN section.\nIn order to generate a C.A.-signed certificate with a SAN section, the two key things are:\n\nsetting the SAN section in the req_extensions section of the openssl.cnf file for generating the certificate signing request (e.g., subjectAltName = DNS:localhost, IP:127.0.0.1, or using an environment variable, subjectAltName = $ENV::SAN);\nactivating the copy of request extensions in the default_ca section of the openssl.cnf file for generating the C.A.-signed certificate from the certificate signing request (copy_extensions = copy).\n\nThis is the best openssl.cnf example file (with all the openssl commands in comments) that I've found so far.\n\n\nIn my experience this is a royal PITA to automate testing of peer verification since standard TLS peer verification usually involves hostname comparison.\n\n@michaelklishin The good thing with the SAN section is that, contrary to the Subject section, you can specify multiple domain names (using multiple DNS or IP fields).. @lukebakken:\n\nThank you. Please provide tests that fail prior to these changes and succeed after applying them.\n\nDone. I restricted the comparison of the pika.Parameters instances to the server _hostand _port attributes. If the client attributes are needed, they can be easily added.. @lukebakken With the last update from the master branch (previously a non PR-related test on Python 2 failed), all tests have passed. So I think you can re-accept this PR now.. Hi @lukebakken,\n\nThanks for giving the latest Pika code a try.\n\nYou are welcome. Note that the freezing problem affects Pika 0.13.0 as well (not only Pika 1.0.0b1).\n\nBy \"freezes\" I'm assuming you mean that no exceptions are raised, correct? What would you expect to happen in the above situations?\nAgain, please provide the code you're using. I'm interested to see what connection type you're using.\n\nI have updated my post. I hope it answers your questions.\n\nI see that in several of the scenarios a ChannelClosed exception is raised, which is the expected result.\n\nYes, this is the expected result. I just mentioned it for exhaustivity.. @lukebakken Okay thanks. So if I understood well you added validation for addressing the case:\n\nexchange_bind(destination=None, source=None, routing_key=None)\n\nThat way instead of freezing the function validators.require_string will raise:\n\nTypeError: destination must be a str or unicode str, but got 'None'\n\nwhich addresses one of the six unexpected behaviours that I listed. (And in Pika 1.0.0b2 the function validators.require_string will raise a ValueError instead of the current TypeError.)\nI see also that on 22 January 2019 you addressed three other cases by adding the same validation:\n\nqueue_declare(queue=None)\nqueue_bind(queue=None, exchange=None, routing_key=None)\nexchange_declare(exchange=None)\n\nSo there remains two cases to address:\n\nexchange_declare()\nexchange_bind()\n\nThey can be addressed by simply removing the default None arguments in the definitions:\ndef exchange_declare(self,\n                     exchange=None,  # to be replaced by: exchange\n                     exchange_type='direct',\n                     passive=False,\n                     durable=False,\n                     auto_delete=False,\n                     internal=False,\n                     arguments=None):\nand\ndef exchange_bind(self,\n                  destination=None,  # to be replaced by: destination\n                  source=None,       # to be replaced by: source\n                  routing_key='',\n                  arguments=None):\nThat way they the call exchange_declare() would raise\n\nTypeError: exchange_declare() missing 1 required positional argument: 'exchange'\n\nand the call exchange_bind() would raise:\n\nTypeError: exchange_bind() missing 2 required positional arguments: 'destination' and 'source'\n\nwhich is consistent with the symmetric calls queue_declare() and queue_bind() which already raise these TypeError.. Hello @michaelklishin. I don't think you should close this issue immediately as PR #1163 addresses only part of it (see my previous answer) and @lukebakken has not read my previous answer yet.. @lukebakken Thank you.. Thank you @lukebakken! With channel.basic_qos(prefetch_count=1) I get the documented behaviour:\n\nINFO:root:Produced message 'a' with routing key 'foobar'\nINFO:root:Produced message 'b' with routing key 'foobar'\nINFO:root:Produced message 'c' with routing key 'foobar'\nINFO:root:Consumed message 'a' from queue 'foobar'\nINFO:root:Consumed message 'a' from queue 'foobar'\nINFO:root:Consumed message 'a' from queue 'foobar'\nINFO:root:Consumed message 'a' from queue 'foobar'\nINFO:root:Consumed message 'a' from queue 'foobar'\nINFO:root:Consumed message 'a' from queue 'foobar'\nINFO:root:Consumed message 'a' from queue 'foobar'\nINFO:root:Consumed message 'a' from queue 'foobar'\nINFO:root:Consumed message 'a' from queue 'foobar'. @lukebakken Okay I have found the issue: the version Pika 1.0.0b1 on PyPI that you get when you type:\npip install pika-dev\n\nis from 21 March 2018, and I see on Github that the same version was released on 28 Sep 2018! So there is a version mismatch with PyPI. Could you upload the real Pika 1.0.0b1 version on PyPI?\nAnd it makes sense that I have this heartbeat timeout issue with the PyPI version since it was actually solved on 6 June 2018.. @lukebakken Thank you very much, it works like a charm now!. @michaelklishin @lukebakken Actually I did not introduce :py:class: domains, they were already used in the README. I just added the missing ones. So should we remove them all instead?. @michaelklishin @lukebakken Thank you for the review.. Okay, thanks for the review. Done.. Done.. You mean the paths of the certificates and private keys (I had kept the ones used in the old version of the example)? Done.. To me the other properties in the suggested rabbitmq.conf.in (listeners.tcp.default = 5672, num_acceptors.tcp = 10, num_acceptors.ssl = 10, reverse_dns_lookups = false, loopback_users.guest = true, log.console = false, log.console.level = debug) are distracting for the reader who would expect only the necessary parameters for setting up TLS in an example like this.. From Wikipedia:\n\nIn English, abbreviations have traditionally been written with a full stop/period/point in place of the deleted part to show the ellipsis of letters\u2014although the colon and apostrophe have also had this role\u2014and with a space after full stops (e.g. \"A. D.\"). In the case of most acronyms, each letter is an abbreviation of a separate word and, in theory, should get its own termination mark. Such punctuation is diminishing with the belief that the presence of all-capital letters is sufficient to indicate that the word is an abbreviation.\nSome influential style guides, such as that of the BBC, no longer require punctuation to show ellipsis; some even proscribe it. Larry Trask, American author of The Penguin Guide to Punctuation, states categorically that, in British English, \"this tiresome and unnecessary practice is now obsolete\".\nNevertheless, some influential style guides, many of them American, still require periods in certain instances. For example, The New York Times' guide recommends following each segment with a period when the letters are pronounced individually, as in K.G.B., but not when pronounced as a word, as in NATO. The logic of this style is that the pronunciation is reflected graphically by the punctuation scheme.\n\nBut since periods are disappearing, I\u2019ve just removed them (just note that they\u2019re not wrong). Thanks for the review. Done.. > add a comment that says \"See http://www.rabbitmq.com/ssl.html to learn more\"\nIt\u2019s actually already there at the beginning of the example:\n\nSee https://www.rabbitmq.com/ssl.html for certificate generation and RabbitMQ TLS configuration.. @michaelklishin @lukebakken Sorry I misread. I am not sure I understand now. Which connection parameters do you suggest we compare exactly? You are talking about client and server hostnames and ports, but as far as I know we only have access to the server hostnames and ports (self._host and self._port).. \n",
    "guillaume-michel": "I don't know, maybe you use editable mode?\nThanks for merging the branch so quickly.. ",
    "9dogs": "Oh, I've missed that, thanks!. ",
    "jon-courtney": "Nope, I'm on a macOS 10.13.4 .\nYeah, I saw that message and wondered if it was related.  I haven't dug into it though.. @vitaly-krugl : I'm not sure that I can simplify the code much beyond the attachment that I originally provided, besides removing print statements.  The multiple threads are necessary to reproduce the problem.\nI am running python 3.6.\nI will provide logging output when I have a moment.. @lukebakken, @vitaly-krugl : I just wanted to say thanks for jumping on this bug!  You resolved it before I even had time to start looking at the details.  Please let me know if you need anything else from me.. ",
    "sw360cab": "Dear @lukebakken thank you for your comment.\nI supposed (since I have used for years) that Pika has to be considered the \"official\" (state-of-the-art) RabbitMQ Python client.\nBTW I would be happy to know in which environment you feel confident your Consumer Example, works with the condition I described.\nOtherwise we can state that the issue is simply there whatever environment I run pika.\n. I would be happy to give you the information you are asking.\nBy tomorrow I can provide them.\nI just go for days through all the issues and I found something like that, as I said looks that in 11.02 this was solved. It looks to me like a regression.\nMay I ask you, is this kind of \"idle hang\" problem something that could be related with \"Blocking Connection\"? In other terms, do you think that this can be more easily avoided employing \"SelectConnection\" (staying with the fact the we don't know wheter this is an issue)\n. I will give a try to \n\nSelectConnection\n\nNo, my environment is on a server running Pika, contacting another server running RabbitMQ.\nWhen I start the program it works smootly, then after a while (suppose an hour) if I try to publish message to RabbitMQ and it works (test app communicating and publishing on rabbit queue), instead the consumer app (as the one in the example) on the remote server is still up and idle (the process is still working), but it is simply hanging.\n. I would like to share what is happening to me.\nI have an application based on Pika and employing RabbitMq. \n The application is made of serveral \"blocks\" in a sort of pipeline. \n Each block will:\n  * consume message from the previous block\n  * produce a message to the next block\n the initial block is triggered from an external event, meaning that external applications will publish messages onto the channel/queue where the initial block is consuming\n each block will be spawned in a thread, but everything regarding Pika connection for the same block will stay in the same thread\n each block will consume a message from a queue and produce message to another (shared with the following block in the pipeline)\n Heartbeats have been disabled as they usually drop connections\nThis application has already been in production (using Pika 0.10) for at least 2 years, having no issue on Pika. Issue started when application and RabbitMQ server (v. 3.6.6) have to be deployed on different networks / servers. As long as they remained on the same machine (precisely a docker network), the issue was not there.\nThe relevant part of the application (that consumes messages) has been coded exactly as this repo example:\n```` \ndef configure(self):\n    exchange_name = exchanger['exchange_name']\n    hostname = self.read_config_value('host','localhost')\n    username = self.read_config_value('username','guest')\n    passwd =  self.read_config_value('passwd','guest')\n    try:\n      # Basic Connection configurtion\n      credentials = pika.PlainCredentials(username, passwd)\n      connParams = pika.ConnectionParameters(host=str(hostname), credentials=credentials, \n      heartbeat_interval=0, connection_attempts=5, retry_delay=1)\n      connection = pika.BlockingConnection(connParams)\n      channel = connection.channel()\n      channel.exchange_declare(exchange=exchange_name,\n                               exchange_type='direct')\n      return connection, channel\n    except pika.exceptions.AMQPConnectionError:\n      self.logger.warn(\"RabbitMQ may have not been started...\")\n      import sys\n      sys.exit(1)\ndef bind(self, channel):\n    \"\"\"\n      Binding\n    \"\"\"\n    # Named Queue\n    if self.hasNamedQueue():\n      channel.basic_qos(prefetch_count=1)\n      result = channel.queue_declare(queue=self.named_queue_name, durable=True)\n    else:\n      result = channel.queue_declare(exclusive=True)\n    queue_name = result.method.queue\n    routing_key = self.inputTier['routing_key']\n    channel.queue_bind(exchange=self.inputTier['exchange_name'],\n                        queue=queue_name,\n                        routing_key = routing_key)\n    return queue_name\ndef consume(self):\n    \"\"\"\n      Consuming incoming messages.\n      Emit errors to failureWorker.\n    \"\"\"\n    def processMsg(ch, method, properties, body):\n      self.logger.debug(\"[x] %r:%r\" % (method.routing_key, body,))\n      ch.basic_ack(delivery_tag=method.delivery_tag)\nwhile(True):\n  try:\n    # Connect & Bind Queue\n    connection, channel = self.configure()\n    queue_name = self.bind(channel)\n\n    channel.basic_consume(processMsg,\n            queue=queue_name,\n            no_ack=False)\n    channel.start_consuming()\n  # Do not recover if connection was closed by broker\n  except pika.exceptions.ConnectionClosed:\n    break\n  # Do not recover on channel errors\n  except pika.exceptions.AMQPChannelError:\n    break\n  except KeyboardInterrupt:\n    channel.stop_consuming()\n    break\n  # Recover on all other connection errors\n  except pika.exceptions.AMQPConnectionError:\n    continue\nconnection.close()\n\n````\nHere is how I could reproduce the issue:\n1. Start RabbitMQ and application (@ 5pm, timing is relevant)\n2. Run test script (that publish messages). The application works smootly and can do is job as well as RabbitMQ.\n3. Run the test again and again and no issue at all\n4. Going home resting from headaches\n5. Getting back in the morning and audit logs. The issue occurred. \nSee what i found the following day (I am using a \"pipeline\" of 2 \"blocks\" respect to what I described above):\n* at midnight (I suppose that) blocks simply reconnected to RabbitMQ:\nRabbitMQ:\n````\n=INFO REPORT==== 16-May-2018::00:00:01 ===\naccepting AMQP connection <0.2383.0> (192.168.1.1:44530 -> 192.168.1.100:5672)\n=INFO REPORT==== 16-May-2018::00:00:01 ===\naccepting AMQP connection <0.2386.0> (192.168.1.1:44532 -> 192.168.1.100:5672)\n````\nPIka:\n2018-05-16 00:00:01 DEBUG - pika.callback: Calling <bound method Channel._on_synchronous_complete of <Channel number=1 OPEN conn=<SelectConnection OPEN socket=('192.168.1.226', 44530)->('130.0.131.174', 5672) params=<ConnectionParameters host=peer.wim.tv port=5672 virtual_host=/ ssl=False>>>> for \"1:Basic.ConsumeOk\"\n2018-05-16 00:00:01 DEBUG - pika.callback: Calling <bound method Channel._on_synchronous_complete of <Channel number=1 OPEN conn=<SelectConnection OPEN socket=('192.168.1.226', 44534)->('130.0.131.174', 5672) params=<ConnectionParameters host=peer.wim.tv port=5672 virtual_host=/ ssl=False>>>> for \"1:Basic.ConsumeOk\"\n2018-05-16 00:00:01 DEBUG - pika.channel: 0 blocked frames\n2018-05-16 00:00:01 DEBUG - pika.channel: 0 blocked frames\n2018-05-16 00:00:01 DEBUG - pika.callback: Calling <bound method Channel._on_eventok of <Channel number=1 OPEN conn=<SelectConnection OPEN socket=('192.168.1.226', 44530)->('130.0.131.174', 5672) params=<ConnectionParameters host=peer.wim.tv port=5672 virtual_host=/ ssl=False>>>> for \"1:Basic.ConsumeOk\"\n2018-05-16 00:00:01 DEBUG - pika.callback: Calling <bound method Channel._on_eventok of <Channel number=1 OPEN conn=<SelectConnection OPEN socket=('192.168.1.226', 44534)->('130.0.131.174', 5672) params=<ConnectionParameters host=peer.wim.tv port=5672 virtual_host=/ ssl=False>>>> for \"1:Basic.ConsumeOk\"\n2018-05-16 00:00:01 DEBUG - pika.channel: Discarding frame <METHOD(['channel_number=1', 'frame_type=1', \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.a50a662ba47944929f287e448ebab64a'])>\"])>\n2018-05-16 00:00:01 DEBUG - pika.channel: Discarding frame <METHOD(['channel_number=1', 'frame_type=1', \"method=<Basic.ConsumeOk(['consumer_tag=ctag1.057fa9bff5484e9ab0ceac20051a930a'])>\"])>\n\n@ 10 am the day after, I run again the test, which publish a message:\n\nRabbitMQ:\n````\n=INFO REPORT==== 16-May-2018::10:04:18 ===\naccepting AMQP connection <0.6235.0> (192.168.1.1:49338 -> 192.168.1.100:5672)\n=WARNING REPORT==== 16-May-2018::10:04:18 ===\nclosing AMQP connection <0.238.0> (192.168.1.1:49338 -> 192.168.1.100:5672):\nconnection_closed_abruptly\n````\nfor the message published by the test.\nBut the application does not start consuming, moreover Pika is not printing anything.\nAs long as OS (Ubuntu) is saying the application process is still UP.\nHere is the strange, RabbitMQ logs:\n````\n=WARNING REPORT==== 16-May-2018::10:04:18 ===\nclosing AMQP connection <0.2383.0> (192.168.1.1:44530 -> 192.168.1.100:5672):\nconnection_closed_abruptly\n=WARNING REPORT==== 16-May-2018::10:04:18 ===\nclosing AMQP connection <0.2389.0> (192.168.1.1:44534 -> 192.168.1.100:5672):\nconnection_closed_abruptly\n````\nBTW the application would stop or exit, but from the Pika point of view it would simply stay and hang.\nLast but not least if I force kill application and restart it, It consumes the message published by the test and never consumed before.\nI can state that the same happens if I wait a couple hours. For sake of simplicity I tried waiting longer, so all night.\nI just want to point out again that as today this application and its architecture are already in production, and this kind of issue does not appear. They do appear where I move RabbitMQ to an address different than \"localhost\".\nBasically I work with Docker. I made these attempt in a \"native\" environment to avoid that something was related to Docker. As the issue occurred the same I suppose no.\nI would be glad if I can cooperate to find and solve the issue. Or if you have any suggestion to replace Blocking Connection with something more reliable. Also heartbeats are a long chapter, at the end disabling them has always solved troubles but I can be work or using a na\u00eff approach.\nThis application is a crucial piece in our company and in this unstable situation sooner or later I would have to drop Pika and RabbitMQ in flavour of something else, and this would be sad and disappointing. Not only for the need of rewriting classes of code.\n. @vitaly-krugl\nThank you for raising so many arguments, they are all strongly related to my scenario description.\n\nRegarding your application that is already in Production: in your production environment, is your pika application on the same server as the RabbitMQ broker?\n\nThat's something I want to point.\nIn production application and RabbitMQ live on the some host.\nThe issues started occurring when RabbitMQ and application are living on different hosts, even if on the same or on different networks.\n\nSince your sample code that reproduces the issue simply discards the messages without taking a long time to process them, go ahead and enable the heartbeat. You might see one of these new outcomes: 1. the connection no longer drops due to heartbeat traffic across some network element that used to sever the connection (not gracefully) when the connection became idle for some period of time; or 2. BlockingConnection with heartbeat enabled discovers the severed connection (no heartbeat or other traffic from RabbitMQ) and raises the expected exception to signal loss of connection.\n\nAbout this part (and I thank you for the full and deep explaination), I used to disable heartbeats because they gave extra pain to my application. And since disabling them allowed the application to work (in local and production environment, on a single host), I too naively reached to the conclusion that application can stand with them disabled.\nBut once I encountered the issue of course I started to attempt dealing with heartbeats. At first I didn't have an exception handler as satisfing as I suppose to have now, so I didn't go in deep for this path. On the other hand Pika 0.11.2 seems to have solved perfectly any issue, but as my application may perform very long tasks (I skipped this partein the snippet for sake of smooth explaination), I encountered many problems in \"acking\" back messages on \"dead\" channel connection (as you figured out correctly).\nStrangely although pika 0.11.2 seamed to deal perfectly with connection status, it didn't with acking messages after very long processing. Whereas Pika 0.12pre looks like doing exactly the opposite, it dealed the problem of \"long task\", but fails connection soon.\nEventually I felt myself resting in a \"cul-de-sac\" / dead end...\nNow after your suggestion, I am working on 3 different attempts to allow us investigating more in deep into the problem:\nattempt # 1\nI will enable heartbeats and see whether exception are raised by Pika and handled correctly by the application\nattempt # 2\nI will try to move the callback \"ack\" part into a add_callback_threadsafe() block, eventually re-switching back to Pika 0.11.2\nattempt # 3\nI will rewrite part of the application to work with SelectConncetion with is a little more complex to be handled but probably more reliable and promising\n\nAs long as I end up with these attempts I will be happy to report to you and see what happened. \nHope that by the end of the week I can share any result.\n. A first attempt adding an heatbeat timeout of 600s reproduces the issue.\nI pasted Pika log here:\nhttps://pastebin.com/K4cJwD3w\nThere could be some extra log.... But the attempt was, at 10.40 a message is received and processed, the same message is sent 4 hours later and never received.\nCould it be related with previous message handling? I remeber that when I attempted to reproduce the issue earlier I used to:\n1. run the application \n2. produce & process several messages\n3. wait all night\nAnd then the day after producing further message would leave the application in hanging state.\nIn this case I ran the application, but I did not produce any message, I just left it up all night.\nStarting producing messages the day after works. Waiting 3 hours and producing further messages seem to hang the whole connection (as stated before).\nAnd I repeat that the application thread never catched any error on connection. It is still running.\n. > @sw360cab could you please reproduce this while capturing traffic to port 5672 on your server? You can run the packet capture on the server. This will help us out to diagnose the issue. I will see if I can reproduce it using my home network.\nYes I'll try\n\nCan you downgrade Pika to 0.10 until we figure this out? That would fix your issue, correct?\n\nI am doing already it, as long as I stuck with Pika 0.10 and with  RabbitMQ and application on the same machine, the issue is not occuring.\n@lukebakken  Thank you for supporting.\n. @lukebakken I think the issue is there even with 0.10, or similar issue was there.\nThe motivation of moving to new version of Pika were issues on 0.10 version while changing the architecture of application (rabbit and app on different hosts).\nSince I have attempted hundreds of combinations of configurations I don't remeber exactly.\nI can try again this scenario too.\n. @lukebakken \nI am doing my tests with 0.12.0.b2 and heartbeats at 600s\nWhile testing I am capturing packets (Wireshark). Everything is working smoothly since this morning in the scenario.\nI will see if by tomorrow the connection is still available leaving it idle all night long.\nIf this works I would repeat the tests with:\n- pika 0.12.0.b3\n- dockerized version of RabbitMq\n. @lukebakken and @vitaly-krugl thank you for your support. I have changed my code (as I stated in my previous comment) and put it in deep testing employing hearthbeats and Wireshark to inspect packets flowing in the net and I definitely upgraded Pika to version 0.12b2.\nI don't want to scream it loud yet, but after a week of testing it sems to be working back smootly even with the production scenario. I hope it will keep going like that \ud83d\udc6f \nI would like to add more:\n- I usually develop on Mac Os (10.12 + Python 2.7), and now I deploy on Ubuntu (right now, 18.04).\n- heartbeating of Pika 0.12b2 is working perfectly on MacOs, whereas on Ubuntu it sends heartbeat every heartbeat/2 seconds (e.g heartbeat=600, will send heartbeats every 300 seconds). May this be related to #1055 ?\n- I forgot to mention that I was using (also at the time I opened the issue) in the production stage Ubuntu 18.04. Is it possible that I opted for this version (18) of Ubuntu too early? \nI state that because after upgrading system and reinstalling Pika (0.12b2), it seems definitely stable. I haven't had any specific edit on my code since that big ones done before my last comment to the issue.\nBut some small problems I was encountering disappered simply by upgrading. \nWhat is your feeling on that?\n- I will plan to pass to Pika 0.12b4 or any other stable one in the near future. I will stay tuned on the follow ups of Pika.\nFinally thank you for supporting and for being patience and trying the issue environment.\n@lukebakken I hope I can keep the issue close \ud83d\ude38 \n. ",
    "fl0wx": "@lukebakken Thanks a lot, i will try my luck at the google groups. ",
    "blint587": "I posted the issue on the mailing list, but I would argue if this is a bug or not. Just simply chaining  to default exchange in the example code brakes it. While connecting to the same server/vhost/queue with with the same credentials works.. ",
    "yuxifu": "It's not always reproducible. Basically this is what I do:\npython\ndef consume_until_empty_or_expire(self, channel, callback=None, expires_sec=0, echo=False):\n        \"\"\"\n        consume the messages in the queue until no message available\n        callback: func(event_data)\n        expires_sec: quit consuming after certain time. <=0, never expires.\n        \"\"\"\n        start_time = time.time()\n        if callback is None:\n            callback = self._default_consume_until_empty_callback\n        total = 0\n        processed = 0\n        returned = 0\n        skipped = 0\n        message_count = 0\n        while True:\n            elapsed_time = time.time() - start_time\n            if expires_sec > 0 and elapsed_time >= expires_sec:\n                return {\n                    'success': True,\n                    'total': total,\n                    'processed': processed,\n                    'returned': returned,\n                    'skipped': skipped,\n                    'message_count': message_count,\n                    'elapsed_time_sec': elapsed_time\n                }\n            method_frame, header_frame, body = channel.basic_get(\n                queue=self.queue_name)\n            if method_frame is None or method_frame.NAME == 'Basic.GetEmpty':\n                return {\n                    'success': True,\n                    'total': total,\n                    'processed': processed,\n                    'returned': returned,\n                    'skipped': skipped,\n                    'message_count': message_count,\n                    'elapsed_time_sec': elapsed_time\n                }\n            else:\n                event_data = {}\n                event_data['frame'] = vars(method_frame)\n                event_data['header'] = vars(header_frame)\n                event_data['body'] = json.loads(body)\n                message_count = event_data['frame']['message_count']\n                total += 1\n                result = callback(event_data)\n                if result == EventProcessingResult.PROCESSED:\n                    channel.basic_ack(delivery_tag=method_frame.delivery_tag)\n                    processed += 1\n                if result == EventProcessingResult.SKIPPED:\n                    channel.basic_ack(delivery_tag=method_frame.delivery_tag)\n                    skipped += 1\n                if result == EventProcessingResult.RETURED:\n                    returned += 1\n                if echo:\n                    print event_data['frame']['routing_key'] + ': ' + result\nNothing really fancy.  Probably this is related:\npython\nif instance.queue_ttl_min > 0:\n                queue_args = {\"x-expires\": instance.queue_ttl_min * 60000}\n            channel.queue_declare(\n                instance.queue_name,\n                durable=instance.queue_durable,\n                arguments=queue_args)\nI did not really see the problem until started to use \"x-expires\". Could be a coincidence, though.. ",
    "benizri-ofir": "Hi,\nI'm getting the same messages :\nDuplicate callback found for \"0:Connection.Unblocked\"\ndid you solve this?. ",
    "fokhunov": "Hello @vitaly-krugl, thanks for reply. \nAs I can see from logs there is Creating a HeartbeatChecker: 60 and RabbitMQ closes connection after 3 minutes. My script tries to publish in RabbitMQ and Exception throwing that connection closed.\nPika version: \n__version__ = '0.11.2'\nHere is RabbitMq container:\n2018-05-28 21:47:09.408 [info] <0.3732.0> accepting AMQP connection <0.3732.0> (172.18.0.1:58404 -> 172.18.0.2:5672)\n2018-05-28 21:47:09.425 [info] <0.3732.0> connection <0.3732.0> (172.18.0.1:58404 -> 172.18.0.2:5672): user 'guest' authenticated and granted access to vhost '/'\n2018-05-28 21:50:07.698 [warning] <0.3732.0> closing AMQP connection <0.3732.0> (172.18.0.1:58404 -> 172.18.0.2:5672):\nmissed heartbeats from client, timeout: 60s \nHere is my script output:\n```\n\nStarting service Mon May 28 23:47:01 2018\nINFO:pika.adapters.base_connection:Connecting to ::1:5672\nDEBUG:pika.callback:Processing 0:Connection.Start\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': ('::1', 5672, 0, 0) params=>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling ('::1', 5672, 0, 0) params=>> for \"0:Connection.Start\"\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Processing 0:Connection.Tune\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': ('::1', 5672, 0, 0) params=>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling ('::1', 5672, 0, 0) params=>> for \"0:Connection.Tune\"\nDEBUG:pika.connection:Creating a HeartbeatChecker: 60\nDEBUG:pika.adapters.select_connection:add_timeout: added timeout -6377231062257697481; deadline=60 at 1527544081.27\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Processing 0:Connection.OpenOk\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': ('::1', 5672, 0, 0) params=>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling ('::1', 5672, 0, 0) params=>> for \"0:Connection.OpenOk\"\nDEBUG:pika.callback:Processing 0:_on_connection_open\nDEBUG:pika.callback:Calling > for \"0:_on_connection_open\"\nDEBUG:pika.connection:Creating channel 1\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>, 'only': ('::1', 5672, 0, 0) params=>>, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.channel:Entering blocking state on frame ; acceptable_replies=[]\nDEBUG:pika.channel:Adding on_synchronous_complete callback\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.channel:Adding passed-in callback\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>>>, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\nINFO:pika.adapters.blocking_connection:Created channel=1\nDEBUG:pika.callback:Processing 1:Channel.OpenOk\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': ('::1', 5672, 0, 0) params=>>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': ('::1', 5672, 0, 0) params=>>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling ('::1', 5672, 0, 0) params=>>> for \"1:Channel.OpenOk\"\nDEBUG:pika.channel:0 blocked frames\nDEBUG:pika.callback:Calling ('::1', 5672, 0, 0) params=>>> for \"1:Channel.OpenOk\"\nDEBUG:pika.channel:Entering blocking state on frame ; acceptable_replies=[]\nDEBUG:pika.channel:Adding on_synchronous_complete callback\nDEBUG:pika.callback:Added: {'callback': ('::1', 5672, 0, 0) params=>>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.channel:Adding passed-in callback\nDEBUG:pika.callback:Added: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Processing 1:Exchange.DeclareOk\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': ('::1', 5672, 0, 0) params=>>>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': >, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling ('::1', 5672, 0, 0) params=>>> for \"1:Exchange.DeclareOk\"\nDEBUG:pika.channel:0 blocked frames\nDEBUG:pika.callback:Calling > for \"1:Exchange.DeclareOk\"\nDEBUG:pika.heartbeat:Received heartbeat frame\nDEBUG:pika.heartbeat:Received heartbeat frame\nDEBUG:pika.heartbeat:Received 2 heartbeat frames, sent 0\nDEBUG:pika.heartbeat:Sending heartbeat frame\nDEBUG:pika.adapters.select_connection:add_timeout: added timeout -4136541231859227008; deadline=60 at 1527544408.75\nERROR:pika.adapters.base_connection:Read empty data, calling disconnect\nINFO:pika.connection:Disconnected from RabbitMQ at localhost:5672 (-1): EOF\nDEBUG:pika.heartbeat:Removing timeout for next heartbeat interval\nDEBUG:pika.adapters.select_connection:remove_timeout: removed -4136541231859227008\nDEBUG:pika.channel:Handling meta-close on >>\nDEBUG:pika.callback:Processing 1:_on_channel_cleanup\nDEBUG:pika.callback:Processing use of oneshot callback\nDEBUG:pika.callback:0 registered uses left\nDEBUG:pika.callback:Removing callback #0: {'callback': >>, 'only': >>, 'one_shot': True, 'arguments': None, 'calls': 0}\nDEBUG:pika.callback:Calling >> for \"1:_on_channel_cleanup\"\nDEBUG:pika.connection:Removed channel 1\nDEBUG:pika.callback:Clearing out '1' from the stack\nDEBUG:pika.callback:Processing 0:_on_connection_closed\nDEBUG:pika.callback:Calling > for \"0:_on_connection_closed\"\nDEBUG:pika.callback:Added: {'callback': >>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\nDEBUG:pika.callback:Incremented callback reference counter: {'callback': >>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\nERROR:pika.adapters.blocking_connection:Connection close detected; result=BlockingConnection__OnClosedArgs(connection=>, reason_code=-1, reason_text='EOF')\nTraceback (most recent call last):\n  File \"/Users/fokhunov/develop/python/build4use/parser/src/parser.py\", line 1552, in \n    start()\n  File \"/Users/fokhunov/develop/python/build4use/parser/src/parser.py\", line 1525, in start\n    rmq_info(\"[ Tajikistan ] parsing started\", tj_tag)\n  File \"/Users/fokhunov/develop/python/build4use/parser/src/parser.py\", line 151, in rmq_info\n    rqm_log(LL_INFO, msg, tag)\n  File \"/Users/fokhunov/develop/python/build4use/parser/src/parser.py\", line 139, in rqm_log\n    rmq_publish(rmq_route_logs, data)\n  File \"/Users/fokhunov/develop/python/build4use/parser/src/parser.py\", line 124, in rmq_publish\n    rmq_client.publish(route, json.dumps(data))\n  File \"/Users/fokhunov/develop/python/build4use/parser/src/producer.py\", line 26, in publish\n    self._channel.basic_publish(exchange=self._exchange, routing_key=route, body=data)\n  File \"/Library/Python/2.7/site-packages/pika/adapters/blocking_connection.py\", line 2077, in basic_publish\n    mandatory, immediate)\n  File \"/Library/Python/2.7/site-packages/pika/adapters/blocking_connection.py\", line 2164, in publish\n    self._flush_output()\n  File \"/Library/Python/2.7/site-packages/pika/adapters/blocking_connection.py\", line 1250, in _flush_output\n    *waiters)\n  File \"/Library/Python/2.7/site-packages/pika/adapters/blocking_connection.py\", line 474, in _flush_output\n    result.reason_text)\npika.exceptions.ConnectionClosed: (-1, 'EOF')\n```. @lukebakken \n\nI already posted my Producer code (see above). \nHere is some part of code (which still reproduces this issue).\n\n```\n!/usr/bin/env python\n-- coding: utf-8 --\nimport argparse\nimport json\nimport signal\nimport sys\nimport time\nimport producer\nprint(\"______\")\nprint(\"Starting service %s\" % time.strftime(\"%c\"))\ngeneral\nservice_name = \"parser\"\narguments\nparser = argparse.ArgumentParser(prog=service_name, description='parses web pages and calls API to collect exchange rates')\nparser.add_argument('-rmq',      default='localhost',   help='rabbitmq host')\nparser.add_argument('-interval', default='300',         help='parsing interval')\nargs = parser.parse_args()\nRabbitMQ\nrmq_url = 'amqp://guest:guest@'+ args.rmq +':5672/%2F?heartbeat=0'\nrmq_exchange = \"erp\"\nrmq_route_logs = \"logs\"\nrmq_client = producer.BlockingProducer(rmq_url, rmq_exchange)\ndef rqm_send(msg):\n    data = {\n        \"service\": service_name,\n        \"msg\": msg\n    }\n    rmq_client.publish(rmq_route_logs, json.dumps(data))\n    print(data)\n\"\"\"\n    Entry point to parser\n\"\"\"\ndef finish(signal_num, stack_frame):\n    print(\"Terminating service %s\" % time.strftime(\"%c\"))\n    rqm_send(\"Terminating service %s\" % time.strftime(\"%c\"))\n    rmq_client.close()\n    sys.exit()\nhandle Termination signal\nsignal.signal(signal.SIGTERM, finish)\nif name == \"main\":\n    rqm_send(\"Starting service with \" + str(args))\n    counter = 0\nwhile True:\n    rqm_send(\"Executing #\" + str(counter))\n    counter += 1\n    time.sleep(float(args.interval))\n\n```\n2.  I tried to run my script with RabbitMQ which is not in Docker container. Result: everything works well. There are no connection closed, I waited more than 10 minutes for it.\nHere is RabbitMQ status output:\n```\n/usr/local/sbin>rabbitmqctl status\nStatus of node rabbit@localhost ...\n[{pid,11668},\n {running_applications,\n     [{rabbitmq_stomp,\"RabbitMQ STOMP plugin\",\"3.7.5\"},\n      {rabbitmq_mqtt,\"RabbitMQ MQTT Adapter\",\"3.7.5\"},\n      {rabbitmq_management,\"RabbitMQ Management Console\",\"3.7.5\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.7.5\"},\n      {rabbitmq_amqp1_0,\"AMQP 1.0 support for RabbitMQ\",\"3.7.5\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.7.5\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.7.5\"},\n      {rabbit,\"RabbitMQ\",\"3.7.5\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.15.3\"},\n      {rabbit_common,\n          \"Modules shared by rabbitmq-server and rabbitmq-erlang-client\",\n          \"3.7.5\"},\n      {ranch_proxy_protocol,\"Ranch Proxy Protocol Transport\",\"1.5.0\"},\n      {cowboy,\"Small, fast, modern HTTP server.\",\"2.2.2\"},\n      {ranch,\"Socket acceptor pool for TCP protocols.\",\"1.5.0\"},\n      {ssl,\"Erlang/OTP SSL application\",\"8.2.6\"},\n      {public_key,\"Public key infrastructure\",\"1.5.2\"},\n      {asn1,\"The Erlang ASN1 compiler version 5.0.5\",\"5.0.5\"},\n      {jsx,\"a streaming, evented json parsing toolkit\",\"2.8.2\"},\n      {amqp10_common,\n          \"Modules shared by rabbitmq-amqp1.0 and rabbitmq-amqp1.0-client\",\n          \"3.7.5\"},\n      {cowlib,\"Support library for manipulating Web protocols.\",\"2.1.0\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.4.4\"},\n      {crypto,\"CRYPTO\",\"4.2.2\"},\n      {xmerl,\"XML parser\",\"1.3.16\"},\n      {recon,\"Diagnostic tools for production use\",\"2.3.2\"},\n      {sasl,\"SASL  CXC 138 11\",\"3.1.2\"},\n      {inets,\"INETS  CXC 138 49\",\"6.5.1\"},\n      {lager,\"Erlang logging framework\",\"3.5.1\"},\n      {goldrush,\"Erlang event stream processor\",\"0.1.9\"},\n      {compiler,\"ERTS  CXC 138 10\",\"7.1.5\"},\n      {syntax_tools,\"Syntax tools\",\"2.1.4\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"3.4.5\"},\n      {kernel,\"ERTS  CXC 138 10\",\"5.4.3\"}]},\n {os,{unix,darwin}},\n {erlang_version,\n     \"Erlang/OTP 20 [erts-9.3.1] [source] [64-bit] [smp:4:4] [ds:4:4:10] [async-threads:64] [hipe] [kernel-poll:true] [dtrace]\\n\"},\n {memory,\n     [{connection_readers,61896},\n      {connection_writers,3120},\n      {connection_channels,9624},\n      {connection_other,135640},\n      {queue_procs,17608},\n      {queue_slave_procs,0},\n      {plugins,2377352},\n      {other_proc,22575424},\n      {metrics,208280},\n      {mgmt_db,236352},\n      {mnesia,79152},\n      {other_ets,2484328},\n      {binary,1752464},\n      {msg_index,30064},\n      {code,29314811},\n      {atom,1131721},\n      {other_system,14086956},\n      {allocated_unused,17671592},\n      {reserved_unallocated,0},\n      {strategy,rss},\n      {total,[{erlang,74504792},{rss,59625472},{allocated,92176384}]}]},\n {alarms,[]},\n {listeners,\n     [{clustering,25672,\"::\"},\n      {amqp,5672,\"127.0.0.1\"},\n      {http,15672,\"::\"},\n      {mqtt,1883,\"::\"},\n      {stomp,61613,\"::\"}]},\n {vm_memory_calculation_strategy,rss},\n {vm_memory_high_watermark,0.4},\n {vm_memory_limit,3435973836},\n {disk_free_limit,50000000},\n {disk_free,146002087936},\n {file_descriptors,\n     [{total_limit,156},{total_used,4},{sockets_limit,138},{sockets_used,2}]},\n {processes,[{limit,1048576},{used,433}]},\n {run_queue,0},\n {uptime,976},\n {kernel,{net_ticktime,60}}]\n```\n\nThis is from RabbitMQ container:\n\n```\nroot@rabbitmq:/# rabbitmqctl status\nStatus of node rabbit@rabbitmq ...\n[{pid,127},\n {running_applications,\n     [{rabbitmq_management,\"RabbitMQ Management Console\",\"3.7.5\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.7.5\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.7.5\"},\n      {rabbit,\"RabbitMQ\",\"3.7.5\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.7.5\"},\n      {rabbit_common,\n          \"Modules shared by rabbitmq-server and rabbitmq-erlang-client\",\n          \"3.7.5\"},\n      {ranch_proxy_protocol,\"Ranch Proxy Protocol Transport\",\"1.5.0\"},\n      {cowboy,\"Small, fast, modern HTTP server.\",\"2.2.2\"},\n      {ranch,\"Socket acceptor pool for TCP protocols.\",\"1.5.0\"},\n      {ssl,\"Erlang/OTP SSL application\",\"8.2.6\"},\n      {public_key,\"Public key infrastructure\",\"1.5.2\"},\n      {asn1,\"The Erlang ASN1 compiler version 5.0.5\",\"5.0.5\"},\n      {recon,\"Diagnostic tools for production use\",\"2.3.2\"},\n      {jsx,\"a streaming, evented json parsing toolkit\",\"2.8.2\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.4.4\"},\n      {cowlib,\"Support library for manipulating Web protocols.\",\"2.1.0\"},\n      {xmerl,\"XML parser\",\"1.3.16\"},\n      {crypto,\"CRYPTO\",\"4.2.1\"},\n      {inets,\"INETS  CXC 138 49\",\"6.5.1\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.15.3\"},\n      {lager,\"Erlang logging framework\",\"3.5.1\"},\n      {goldrush,\"Erlang event stream processor\",\"0.1.9\"},\n      {compiler,\"ERTS  CXC 138 10\",\"7.1.5\"},\n      {syntax_tools,\"Syntax tools\",\"2.1.4\"},\n      {sasl,\"SASL  CXC 138 11\",\"3.1.2\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"3.4.5\"},\n      {kernel,\"ERTS  CXC 138 10\",\"5.4.3\"}]},\n {os,{unix,linux}},\n {erlang_version,\n     \"Erlang/OTP 20 [erts-9.3.1] [source] [64-bit] [smp:2:2] [ds:2:2:10] [async-threads:64] [hipe] [kernel-poll:true]\\n\"},\n {memory,\n     [{connection_readers,0},\n      {connection_writers,0},\n      {connection_channels,0},\n      {connection_other,2840},\n      {queue_procs,15752},\n      {queue_slave_procs,0},\n      {plugins,1780728},\n      {other_proc,25547032},\n      {metrics,197264},\n      {mgmt_db,332040},\n      {mnesia,74976},\n      {other_ets,2174528},\n      {binary,1215472},\n      {msg_index,28848},\n      {code,28437512},\n      {atom,1123529},\n      {other_system,22197343},\n      {allocated_unused,21848520},\n      {reserved_unallocated,0},\n      {strategy,rss},\n      {total,[{erlang,83127864},{rss,90521600},{allocated,104976384}]}]},\n {alarms,[]},\n {listeners,[{clustering,25672,\"::\"},{amqp,5672,\"::\"},{http,15672,\"::\"}]},\n {vm_memory_calculation_strategy,rss},\n {vm_memory_high_watermark,0.4},\n {vm_memory_limit,838426624},\n {disk_free_limit,50000000},\n {disk_free,57450815488},\n {file_descriptors,\n     [{total_limit,1048476},\n      {total_used,2},\n      {sockets_limit,943626},\n      {sockets_used,0}]},\n {processes,[{limit,1048576},{used,369}]},\n {run_queue,0},\n {uptime,64093},\n {kernel,{net_ticktime,60}}]\n```\nAny ideas ??. For sure problem in pika library. I tried run code in Golang and auto-close connection not happened.\nThanks.. ",
    "farrukh-okhunov": "@lukebakken Hi, \n1. Actually I provided all code. On my first post you can see class Producer... which I store in separate file and import it in main file as import producer.\n\n\nFor Go code I just used example from RabbitMQ Go routing\n\n\nMy docker env:\n```\n\n\ndocker network create erp_network\ndocker run -d --hostname rabbitmq --name rabbitmq --network erp_network -p 5672:5672 rabbitmq:3.7.5\ndocker build -t parser .\ndocker run -d --name parser --network erp_network parser -rmq=rabbitmq\n```\nHere is my app Dockerfile\n```\nFROM python:2.7.15-alpine3.7\nCopy and install dependencies\nCOPY requirements.txt /\nRUN pip install -r /requirements.txt\nSet workdir. Script should be mounted.\nWORKDIR /app\nMove app files inside image\nCOPY ./src/parser.py ./src/producer.py ./\nRun container\nENTRYPOINT [ \"python\", \"-u\", \"parser.py\" ]\nrabbitmq host based on environment (docker tip: use custom network)\ninterval in seconds\nCMD [ \"-rmq=localhost\", \"-interval=300\" ]\n```\nHere is my requirements.txt\nbeautifulsoup4==4.6.0\nrequests==2.18.4\npika==0.11.2\npytz==2013.7\nThat's all ;). ",
    "waseem18": "No @lukebakken.  The stubs go into https://github.com/python/typeshed. To create stubs for a third party package, the policy is to get permission of the package owner first. That's why I created this issue.. Thanks for the information @vitaly-krugl \nI guess I'll wait till the changes happen so that the stubs doesn't break for folks who try to use it.. ",
    "giovana-morais": "can I help with this? . sorry it took so long. I finished everything, except for toctrees. do you have any tips for handling these? :smile: . yeah, I've never heard of them before too, but them I found it at index.rst.\n```\nUsing Pika\n\n.. toctree::\n   :glob:\n   :maxdepth: 1\nintro\n   modules/index\n   examples\n   faq\n   contributors\n   version_history\n```\nit links to other files.   \nI thought about doing relative links, but I'm still testing this. . ",
    "romantolkachyov": "It is wrong to use markdown with Sphinx. Don't touch docs/ :-) \nIt is harder to maintain markdown with rst inclusions (in this case you must know both syntaxes and know how to use them together) rather than native rst.. @lukebakken I can help you with rst. Just mention me.\nYou will have more issues when you try use rst syntax (directives) in markdown if you want extract docs from sources or make other advanced things. sphinx/rst is de facto standard in python, I think we must follow it.\nJust for example (quick googling): http://ericholscher.com/blog/2016/mar/15/dont-use-markdown-for-technical-docs/ And there was a lot more pain when I've tried it last time (2-3 years ago) but I didn't remember exactly reason. \nIt just a recommendation anyway.. ",
    "beyondasce": "Ok,thank you.. ",
    "elbaro": "Env: Ubuntu 18.04, rabbitmq-server 3.6.10-1, Erlang/OTP 20 [erls-9.2]\nCode : https://gist.github.com/elbaro/77e669306b847e31566315d688c17757\nConsumer: \npy\nimport time\nq = RabbitConsumer('test_q')\nfor (batch,ack) in q.iter_batch(100):\n    assert len(batch) == len(ack)\n    print(batch)\n    q.ack(ack)\n    time.sleep(1)\nProducer\npy\nimport time\nq = RabbitProducer('test_q')\ni = 0\nwhile True:\n    i += 1\n    print(i)\n    q.send('random ' + str(i))\n    time.sleep(3)\nI just started to use RabbitMQ so the bug may be in my code. but I couldn't see any heartbeat config is required in tutorials.\n. Oh, ack() is run on the main thread.\nI'd appreciate your help. I am figuring out how one subprocess deals with a blocking channel.start_consuming() and at the same time sends ack as well.\n. Thanks. I rewrote the class and it's working now!\nAsyncioConnection exposes ioloop, so I expected this trick to work:\n```py\nclass AsyncConsumer():\n    ...\n# used by main thread\ndef ack(self, ack_tag):\n    self.conn.ioloop.call_soon_threadsafe(self._ack, args=(\nack_tag,))\n\n# used by ioloop\ndef _ack(self, ack_tag):\n    self.channel.basic_ack(ack_tag)\n\n```\nHowever this gets\nAttributeError: 'IOLoopAdapter' object has no attribute 'call_soon_threadsafe'\nThis is how I ended up in 0.12:\npy\nself.conn.loop.call_soon_threadsafe(lambda: self._ack(ack_tags))\n. ",
    "AstraSerg": "Hello\nI didn't change the code. Only the documentation. Why travis failed?. I didn't change the code. Only the documentation. Why travis failed?. ",
    "tazgithubmaster": "\nYou say \"larger Messages\" but then fail to say just how much more large. At what size does this issue appear?\n\nwithout the change i could only send 20KB messages. Increasing it to 200KB i got the problem.\n\nCould you please provide all of your code, or at least a minimum reproducer that works out-of-the-box? Based on the very small and incomplete code snippet you provide I suspect you may be trying to re-use a connection or channel instance or are somehow not closing them correctly. But, without seeing more, I would be guessing which is not productive.\n\n\n#!/usr/bin/env python3\n\nimport pika\nfrom pika import ConnectionParameters\nfrom pika.credentials import ExternalCredentials\nfrom ssl import CERT_REQUIRED\n\n\n\nssl_options = dict(\n    certfile = \"amqp/amqp.crt\",\n    keyfile = \"amqp/amqp.key\",\n    ca_certs = \"amqp/cacert.crt\",\n    cert_reqs = CERT_REQUIRED )\n\n\nparameters=ConnectionParameters(\n    host = \"amqp.taz.de\",\n    port = 5673,\n    virtual_host = \"spielwiese\",\n    credentials = ExternalCredentials(),\n    ssl = True,\n    ssl_options = ssl_options)\n\nprops = pika.BasicProperties(delivery_mode = 2)\n\n\nwith open(\"foo\",\"rb\") as fd:\n    body = fd.read()\n    print(len(body))\n    con=pika.BlockingConnection(parameters)\n    ch=con.channel()\n    ch.basic_publish(exchange=\"spielwiese\",\n                     routing_key=\"spielroute\",\n                     properties=props,\n                     body=body)\n\n    ch.close()\n    con.close()\n\n\nWhat AMQP broker are you using?\nIf RabbitMQ, what version and what version of Erlang?\n\nthe problem happens with rabbitmq 3.4.3-1 (Erlang R15B01) and with rabbitmq 3.6.15-1 (Erlang/OTP 19 [erts-8.2.1] )\n\nWhat operating system for your application and for your broker?\n\nProblem happens on debian stretch and on arch-linux. server is running debian stretch (rabbitmq 3.6.15) or debian wheezy (rabbitmq 3.4.3). The publisher runs on debian stretch and on arch-linux. in all cases the client breaks and the server reports in the logfile:\n\n=WARNING REPORT==== 21-Jun-2018::16:03:32 ===\nclosing AMQP connection <0.18155.112> (gw.taz.de:33090 -> amqp.taz.de:5673):\nconnection_closed_abruptly\n\n\nDo you have any custom broker configuration\n\nyes, we use client certificates for authentification. and we are running a cluster with two nodes. when disable ssl i see pika.exceptions.ChannelClosed: Already closed: <Channel number=1 CLOSED conn=<SelectConnection CLOSED socket=None params=<ConnectionParameters host=amqp.taz.de port=5673 virtual_host=spielwiese ssl=True>>> if ssl is enabled i get \n\ndd if=/dev/urandom of=foo bs=200k count=1\n1+0 records in\n1+0 records out\n204800 bytes (205 kB, 200 KiB) copied, 0.00198611 s, 103 MB/s\n[ak@bounty src]$ ./min.py\n204800\nTraceback (most recent call last):\n  File \"./min.py\", line 36, in \n    body=body)\n  File \"/home/ak/.local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 2077, in basic_publish\n    mandatory, immediate)\n  File \"/home/ak/.local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 2164, in publish\n    self._flush_output()\n  File \"/home/ak/.local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 1250, in _flush_output\n    *waiters)\n  File \"/home/ak/.local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 474, in _flush_output\n    result.reason_text)\npika.exceptions.ConnectionClosed: (-1, \"SSLWantWriteError(3, 'The operation did not complete (write) (_ssl.c:2130)')\")\n\n. thanks for the help. Setting the connection into blocking is only a workaround I'm aware of that but i lack the skill to solve the problem an other way :). rabbitmq 3.4 on debian wheezy\n\ncat /etc/rabbitmq/rabbitmq.config\n[\n  {\n    rabbit, [\n      { ssl_allow_poodle_attack, true},\n      { cluster_nodes, {['rabbit@havelock.taz.de','rabbit@drumknott.taz.de'], disc}},\n      { auth_mechanisms, ['PLAIN', 'AMQPLAIN', 'EXTERNAL'] },\n      { ssl_cert_login_from, common_name },\n      { loopback_users, [ \"apache-loopback\", <<\"guest\">> ] },\n      { tcp_listeners, [ { \"127.0.0.1\",5672 } ]},\n      { reverse_dns_lookups, true },\n      { heartbeat, 30 },\n      { tcp_listen_options, [binary,\n                     {packet, raw},\n                     {reuseaddr, true},\n                     {backlog, 128},\n                     {nodelay, true},\n                     {exit_on_close, false},\n                     {keepalive, true}]},\n      { ssl_listeners, [ 5673]},\n      { ssl_options, [\n        { cacertfile,\"/etc/rabbitmq/ssl/cacert.crt\" },\n        { certfile,\"/etc/rabbitmq/ssl/drumknott-amqp.crt\" },\n        { keyfile,\"/etc/rabbitmq/ssl/drumknott-amqp.key\" },\n        { verify,verify_peer },\n        { fail_if_no_peer_cert,true },\n        { depth, 1} ]\n      }\n    ]\n  },\n  {\n    rabbitmq_management, [\n      {  listener, [\n        { port, 15672 },\n        { ssl, true },\n        { ssl_opts, [\n          { cacertfile, \"/etc/rabbitmq/ssl/cacert.crt\" },\n          { certfile,   \"/etc/rabbitmq/ssl/drumknott-amqp.crt\" },\n          { keyfile,    \"/etc/rabbitmq/ssl/drumknott-amqp.key\" }\n        ]}\n     ]}\n  ]}\n].\n. i did setup an ssh-tunnel with ssh -L 5672:localhost:5672 havelock.taz.de to one of the nodes and used\n\nparameters=ConnectionParameters(\n    host = \"localhost\",\n    port = 5672,\n    virtual_host = \"spielwiese\")\n\nI didn't get any error for filesizes upto 200MB (didn't try larger files)\n\nForgive me for chipping in to hazard a wild guess. Is there chance network latency/bandwidth is involved here?\n\ni don't think so - the same script (min.py with ssl) is working with python2 and 200MB files\n\n[ak@bounty src]$ python2 ./min.py \n209715200\n[ak@bounty src]$ python3 ./min.py \n209715200\nTraceback (most recent call last):\n  File \"./min.py\", line 36, in \n    body=body)\n  File \"/home/ak/.local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 2077, in basic_publish\n    mandatory, immediate)\n  File \"/home/ak/.local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 2164, in publish\n    self._flush_output()\n  File \"/home/ak/.local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 1250, in _flush_output\n    *waiters)\n  File \"/home/ak/.local/lib/python3.6/site-packages/pika/adapters/blocking_connection.py\", line 474, in _flush_output\n    result.reason_text)\npika.exceptions.ConnectionClosed: (-1, \"SSLWantWriteError(3, 'The operation did not complete (write) (_ssl.c:2130)')\")\n. sure - it's my devel machine\n\n[ak@bounty src]$ python --version\nPython 3.6.5\n[ak@bounty src]$ python2 --version\nPython 2.7.15\n[ak@bounty src]$ pip list\nPackage                      Version  \n\naiodns                       1.1.1  \nansible                      2.5.4  \nappdirs                      1.4.3  \nasn1crypto                   0.24.0 \nbackcall                     0.1.0  \nbcrypt                       3.1.4  \nBeaker                       1.10.0 \nblessings                    1.6.1  \nbpython                      0.17.1 \nBrlapi                       0.6.7  \nbtrfsutil                    1.0.0  \nCacheControl                 0.12.5 \ncatfish                      1.4.5  \ncffi                         1.11.5 \nchardet                      3.0.4  \nclick                        6.7    \ncolorama                     0.3.9  \nconfigobj                    5.0.6  \ncryptography                 2.2.2  \ncupshelpers                  1.0    \ncurtsies                     0.3.0  \ncycler                       0.10.0 \nCython                       0.28.3 \ndecorator                    4.3.0  \ndistlib                      0.2.7  \ndistro                       1.3.0  \ndocutils                     0.14   \nelasticsearch                5.1.0  \nentrypoints                  0.2.3  \nFlask                        1.0.2  \ngajim                        1.0.3  \ngreenlet                     0.4.13 \ngufw                         18.4.0 \nhtml5lib                     1.0.1  \nidna                         2.6    \nimutils                      0.4.5  \niotop                        0.6    \nipython                      6.3.1  \nipython-genutils             0.1.0  \nisc                          2.0    \nitsdangerous                 0.24   \njedi                         0.12.0 \njeepney                      0.3.1  \nJinja2                       2.10   \nkeyring                      12.2.1 \nlensfun                      0.3.2  \nlightdm-gtk-greeter-settings 1.2.2  \nlockfile                     0.12.2 \nlouis                        3.5.0  \nMako                         1.0.7  \nMarkupSafe                   1.0    \nmatplotlib                   2.1.1  \nmsgpack                      0.5.6  \nmysql-connector-python       8.0.11 \nnbxmpp                       0.6.6  \nnetworkx                     2.0    \nnpyscreen                    4.10.5 \nnumpy                        1.13.3 \npackaging                    17.1   \npacman-mirrors               4.10.1 \npandas                       0.23.0 \nparamiko                     2.4.1  \nparso                        0.2.1  \npexpect                      4.6.0  \npickleshare                  0.7.4  \npika                         0.11.2 \nPillow                       5.0.0  \npip                          10.0.1 \nply                          3.11   \nprogress                     1.3    \nprompt-toolkit               1.0.15 \npsutil                       5.4.6  \nptyprocess                   0.5.2  \npyasn1                       0.3.7  \npyasn1-modules               0.1.5  \nPyBluez                      0.22   \npycairo                      1.17.0 \npycares                      2.3.0  \npycparser                    2.18   \npycrypto                     2.6.1  \npycups                       1.9.73 \npycurl                       7.43.0.2 \nPygments                     2.2.0  \npygobject                    3.28.3 \nPyNaCl                       1.2.1  \npyOpenSSL                    18.0.0 \npyparsing                    2.2.0  \npyparted                     3.11.1 \npython-daemon                2.1.2  \npython-dateutil              2.6.1  \npython-libtorrent            1.1.7  \npytoml                       0.1.16 \npytz                         2017.3 \nPyWavelets                   0.5.2  \npyxdg                        0.26   \nPyYAML                       3.12   \nranger-fm                    1.9.1  \nRecoll                       1.0    \nrequests                     2.18.4 \nretrying                     1.3.3  \nscikit-image                 0.13.1 \nscipy                        1.0.0  \nSecretStorage                3.0.1  \nsetuptools                   39.2.0 \nsimplegeneric                0.8.1  \nsix                          1.11.0 \nsleekxmpp                    1.3.3  \nteam                         1.0    \ntraitlets                    4.3.2  \nufw                          0.35   \nurllib3                      1.23   \nvirtualenv                   15.1.0 \nwcwidth                      0.1.7  \nwebencodings                 0.5.1  \nWerkzeug                     0.14.1 \nxmltodict                    0.11.0 \nyoutube-dl                   2018.5.18\n[ak@bounty src]$ pip2 list\nPackage                            Version\n\nappdirs                            1.4.3\nasn1crypto                         0.24.0 \nbackports.shutil-get-terminal-size 1.0.0\nbcrypt                             3.1.4\nBeaker                             1.10.0 \nCacheControl                       0.12.5 \ncffi                               1.11.5 \nchardet                            3.0.4\ncolorama                           0.3.9\ncryptography                       2.2.2\ndecorator                          4.3.0\ndistlib                            0.2.7\ndistro                             1.3.0\ndnet                               1.12 \ndocutils                           0.14 \nduplicity                          0.7.17 \necdsa                              0.13 \nenum34                             1.1.6\nfasteners                          0.14.1 \nfuncsigs                           1.0.2\ngunicorn                           19.0.0 \nhtml5lib                           1.0.1\nidna                               2.6  \nipaddress                          1.0.22 \nipython                            5.6.0\nipython-genutils                   0.1.0\nJinja2                             2.10 \nlockfile                           0.12.2 \nlouis                              3.5.0\nMako                               1.0.7\nMarkupSafe                         1.0  \nmonotonic                          1.5  \nmsgpack                            0.5.6\nmutagen                            1.40.0 \nMySQL-python                       1.2.5\nnbxmpp                             0.6.6\nnetifaces                          0.10.7 \nnetsnmp-python                     1.0a1\npackaging                          17.1 \nparamiko                           2.4.1\npath.py                            8.2.1\npathlib                            1.0.1\npexpect                            4.6.0\npickleshare                        0.7.4\npika                               0.11.2 \npip                                10.0.1 \nply                                3.11 \nprogress                           1.3  \nprompt-toolkit                     1.0.15 \npsutil                             5.4.6\nptyprocess                         0.5.2\npyasn1                             0.4.2\npyasn1-modules                     0.2.1\nPyBluez                            0.22 \npycairo                            1.17.0 \npycparser                          2.18 \npycrypto                           2.6.1\nPygments                           2.2.0\npygobject                          3.28.3 \nPyNaCl                             1.2.1\npyOpenSSL                          18.0.0 \npyparsing                          2.2.0\npython-daemon                      2.1.2\npython-ldap                        3.1.0\npython-libtorrent                  1.1.7\npython-musicbrainz                 0.0.0\npytoml                             0.1.16 \npyxdg                              0.26 \nPyYAML                             3.12 \nrequests                           2.18.4 \nretrying                           1.3.3\nsetuptools                         39.2.0 \nsimplegeneric                      0.8.1\nsix                                1.11.0 \nteam                               1.0  \nterminator                         1.91 \ntraitlets                          4.3.2\nurllib3                            1.23 \nwcwidth                            0.1.7\nwebencodings                       0.5.1\nzenmap                             7.70 \n. mhhh i just installed pika 0.12 and it looks like this is fixing the issue.\nsorry for the trouble and thanks again for your help.. ",
    "cs-akash-jarad": "closing. ",
    "sky-code": "this is because I install dramatiq library\npip install dramatiq[rabbitmq,watch]==1.2.0\n. where is this file ?\nhttps://github.com/pika/pika/commits/d4bdaaa75805cb7899ed53cf684bbaa8bf026449/pika/adapters/libev_connection.py\nwhy if i install clear pika package i not have this file in site-packages/pika/adapters but when pika installed with additional packages, this file appears from nowhere?. ",
    "Jacobh2": "@lukebakken & @michaelklishin Since I opened this issue, I have changed lib from pika partly because of this issue.\nI believe the setting I was using was the 5 seconds delay mentioned in the original post. Setting the value to zero was mearly to trigger this bug faster than the three or so days it took for me to find it in the first place!\nSounds like pika should not allow a zero value if that can trigger this problem, at the same time, will that cure the symptoms and not the cause? . I honestly don't remember the exact timeframe, but I remember it didn't happen directly. Instead I found it after at least a couple of days. \nThe original problem happened on a IoT device running with faulty configured network, where I set it to never stop trying to reconnect. The amount of times the reconnect had to be called until it crashed I don't know, so not completely sure of the timeframe, guessing it also have to do with what I set the delay to be. \nWhen I saw the logs (as posted above), I  narrow it down to the fact that I can very simply reproduce this with no server online at all and with zero as delay. . Cool! Just a quick follow-up question: If using call_later can cause stack overflow, does also add_callback_threadsafe suffer from the same issues? Since that function is this lib's recommended way of ACKing messages from another thread. Thank you @lukebakken, it was just a thought that poped up so I don't have a specific example at the time.. I think I'm also seeing a similar problem to this:\nI'm using the BlockingConnection and after some time of consuming messages I see a \npika.exceptions.ConnectionClosed: (-1, \"ConnectionResetError(104, 'Connection reset by peer')\")\nand the logs of RabbitMQ I see:\n2018-09-26 12:53:04.114 [warning] <0.8398.0> closing AMQP connection <0.8398.0> (client_sender_1.client_default:40236 -> dc000002c2:5672):\nmissed heartbeats from client, timeout: 60s\nHow can I ensure that the heartbeat is running?. ",
    "0x00evil": "Got the same problem.\nI think it's bug.\nMy solution is let the callback run in another thread and main thread send heartbeat every 5 secs.\n```python\nimport threading\nimport pika\nclass Rabbitmq:\n    def init(self, username, password, host, vhost):\n        credentials = pika.PlainCredentials(username=username, password=password)\n        connection_parameters = pika.ConnectionParameters(credentials=credentials, host=host, virtual_host=vhost)\n        connection = pika.BlockingConnection(connection_parameters)\n        channel = connection.channel()\n        channel.basic_qos(prefetch_count=1)\n        self.connection = connection\n        self.channel = channel\ndef receive(self, queue_name, callback=None):\n    \"\"\"rabbitmq receiver\"\"\"\n\n    def cb(ch, method, properties, body):\n        if callback is not None:\n            t = threading.Thread(target=callback, args=(body,))\n            t.daemon = True\n            t.start()\n\n            while t.is_alive():\n                print \"[INFO] heart beating\"\n                self.connection.process_data_events()\n                self.connection.sleep(5)\n\n        ch.basic_ack(delivery_tag=method.delivery_tag)\n\n    self.channel.basic_consume(cb, queue_name)\n    self.channel.start_consuming()\n\ndef send(self, message, exchange, routing_key, delivery_mode=2, priority=None):\n    \"\"\"rabbitmq sender\"\"\"\n    self.channel.basic_publish(exchange=exchange,\n                               routing_key=routing_key,\n                               body=message,\n                               properties=pika.BasicProperties(delivery_mode=delivery_mode, priority=priority))\n\ndef close(self):\n    self.connection.close()\n\n```\n. ",
    "chnandu": "@0x00evil Thanks. I can try implementing similar logic using SelectConnection adapter. As of now, we are trying to change our application logic to do the processing outside of sending thread for avoiding the idle periods in between. If it is a bug then it would be nice to have a fix for it.. Hi @lukebakken, Thanks for looking in to it. I see the 0.12.0 release notes has \"Heartbeats are now sent at an interval equal to 1/2 of the negotiated idle connection timeout\".  So I guess that resolves the issue because pika sending heartbeats to broker every 1/2 of negotiated idle connection timeout would definitely keep broker from closing the connection prematurely.\nI'll upload my code in a day or two. I need to test our code with 0.12.0 version and make a plan to upgrade pika in all our systems. After that I'll upload the code.\nWe have just upgraded our application to use pika 0.11.2 from 0.9.6, early June. I noticed the version 0.12.0 got released after we finished upgrading our Prod system or at about same time but we didn't think 0.11.2 would cause connections to be closed by broker prematurely. So didn't pay attention to the latest release.\nAlso, our application is failing even when publishing messages. We implemented logic to reconnect automatically in consumer but we didn't implement the same in publisher so the failures with publishing were obvious. And we haven't disabled the heartbeats.\n. Hi @lukebakken, Unfortunately I am still seeing the issue even with 0.12.0. I uploaded the code here: https://gist.github.com/chnandu/9d90e8b2cc46f762ee66d3db032832b0.\nWhen sending(or publishing) messages to Rabbit, if there is idle time(>30 secs) between sending messages then Rabbit is closing the connection prematurely.\nI also tried adding our send_msg() function as callback using add_threadsafe_callback() but it still not working.\nNot sure why Rabbit thinks the client is lost or why pika isn't sending heartbeats... We have worked around this by eliminating the processing of messages once the connection is opened for now, so it is not a pressing issue at the moment but would like to understand what is wrong and what to be changed..\nAppreciate your help.. Hi @lukebakken Thanks for looking in to it and for the solution. Didn't realize ioloop is blocked, now I got the answers I am looking for. Thanks again... ",
    "yifeikong": "same here, I'm using 0.12.0 and noticed the same issue.\nWhen I stopped publishing messages, and restart publishing messages, and got ChannelClosed Error. ",
    "moelius": "Hi, thanks for your reply.\nAnswers to your questions:\n\nHow long is \"some time working\"\n\n2-3 minutes, then application freezes for 2-4 minutes and then error appears\n\nCan you please provide your source code in rabbit.py? I need this code to be able to reproduce your issue. There is a chance you are not using SelectConnection correctly.\n\nthis is my project https://github.com/moelius/gromozeka (the very beginning of it).\nHere some documentation http://gromozeka.readthedocs.io/en/latest/.\nSee https://github.com/moelius/gromozeka/blob/master/gromozeka/brokers/rabbit.py - here I use pika. \nyou can install it with pip:\npip install gromozeka\nYou need to run redis also: You can use docker:\ndocker run --name gromozeka_redis -p6379:6379 -d redis \nBut this version without error (pika==0.10.0). If you change this to 0.12.0 ...\n\nIt looks like you're using Python 3.7.0, is that true?\n\n3.6.5 - 3.7.0 tested\n\nWhat version(s) of Linux and OS X were tried? Did you also use Python 3.7.0?\n\nmacOS - High Siera 10.13.6\nlinux - Ubuntu 16.04.5 LTS\nCode to reproduce error:\n```python\nimport time\nfrom random import random\nfrom gromozeka import Gromozeka, ThreadWorker\nfrom gromozeka import task\nfrom gromozeka.primitives.base import BrokerPoint\n@task(bind=True)\ndef test_func_one(self, sleep_time, word):\n    \"\"\"\nArgs:\n    self(gromozeka.primitives.Task):\n    sleep_time(int): Time to sleep\n    word(str): Word to print\n \"\"\"\nself.logger.info('start working')\nself.logger.info('Job is done. Word is %s', word)\nreturn random()\n\nif name == 'main':\n    app = Gromozeka().config_from_dict(\n        dict(broker_url='amqp://guest:guest@localhost:5672/%2F', backend_url='redis://localhost'))\nbroker_point_first = BrokerPoint(exchange='first_exchange', exchange_type='direct', queue='first_queue',\n                                 routing_key='first')\napp.register(task=test_func_one(), broker_point=broker_point_first, worker_class=ThreadWorker, max_workers=5)\n# Start application\napp.start()\n# Run tasks\ntime.sleep(1)\nfor i in range(100000):\n    test_func_one().apply_async(sleep_time=1, word=\"bobo\")\n\n```\n. I'll try to thank you, but here is one instance of the rabbit in one thread for the rabbit queue, and it communicates with other threads using a python queue. Strange thing why 0.10 works fine.. @lukebakken That helped. But it's strange. Thank you.\npython\ncallback = functools.partial(self.channel.basic_ack, delivery_tag)\nself.connection.ioloop.add_callback_threadsafe(callback). Thank you, sorry for the issue, did not find these links. ",
    "Nizebulous": "I'm running into this problem using dramatiq. And I just want to raise it here in case the problem is here and not how they are invoking things. Here is the stacktrace:\nFileNotFoundError: [Errno 2] No such file or directory\n  File \"flask/app.py\", line 2309, in __call__\n    return self.wsgi_app(environ, start_response)\n  File \"appoptics_apm/middleware.py\", line 142, in __call__\n    result = self.wrapped_app(environ, wrapped_start_response)\n  File \"werkzeug/contrib/fixers.py\", line 152, in __call__\n    return self.app(environ, start_response)\n  File \"raven/middleware.py\", line 100, in __call__\n    iterable = self.application(environ, start_response)\n  File \"flask/app.py\", line 2295, in wsgi_app\n    response = self.handle_exception(e)\n  File \"flask_restful/__init__.py\", line 273, in error_router\n    return original_handler(e)\n  File \"flask/app.py\", line 1741, in handle_exception\n    reraise(exc_type, exc_value, tb)\n  File \"flask/_compat.py\", line 35, in reraise\n    raise value\n  File \"flask/app.py\", line 2292, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"flask/app.py\", line 1815, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"flask_restful/__init__.py\", line 273, in error_router\n    return original_handler(e)\n  File \"flask/app.py\", line 1718, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"flask/_compat.py\", line 35, in reraise\n    raise value\n  File \"flask/app.py\", line 1813, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"flask/app.py\", line 1799, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"flask/views.py\", line 88, in view\n    return self.dispatch_request(*args, **kwargs)\n  File \"flask/views.py\", line 158, in dispatch_request\n    return meth(*args, **kwargs)\n  File \"flask_login/utils.py\", line 261, in decorated_view\n    return func(*args, **kwargs)\n  File \"sylo/permissions/access.py\", line 96, in wrapped\n    return f(*args, **kwargs)\n  File \"webargs/core.py\", line 447, in wrapper\n    return func(*new_args, **kwargs)\n  File \"<my-file>\", line 47, in wrapper\n    result = func(*args, **kwargs)\n  File \"<my-file>\", line 486, in func\n    <my_actor>.send(one_id, two_id)\n  File \"dramatiq/actor.py\", line 179, in send\n    return self.send_with_options(args=args, kwargs=kwargs)\n  File \"dramatiq/actor.py\", line 198, in send_with_options\n    return self.broker.enqueue(message, delay=delay)\n  File \"dramatiq/brokers/rabbitmq.py\", line 250, in enqueue\n    properties=properties,\n  File \"pika/adapters/blocking_connection.py\", line 2206, in publish\n    immediate=immediate)\n  File \"pika/channel.py\", line 425, in basic_publish\n    (properties, body))\n  File \"pika/channel.py\", line 1328, in _send_method\n    self.connection._send_method(self.channel_number, method, content)\n  File \"pika/connection.py\", line 2341, in _send_method\n    self._send_message(channel_number, method, content)\n  File \"pika/connection.py\", line 2355, in _send_message\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"pika/connection.py\", line 2327, in _send_frame\n    self._flush_outbound()\n  File \"pika/adapters/base_connection.py\", line 330, in _flush_outbound\n    self._manage_event_state()\n  File \"pika/adapters/base_connection.py\", line 536, in _manage_event_state\n    self.event_state)\n  File \"pika/adapters/select_connection.py\", line 446, in update_handler\n    self._poller.update_handler(fileno, events)\n  File \"pika/adapters/select_connection.py\", line 639, in update_handler\n    events_to_set=events_set)\n  File \"pika/adapters/select_connection.py\", line 1152, in _modify_fd_events\n    self._poll.modify(fileno, events). Yup. Sorry for not including more info.\nRunning in the official Python:3.7 docker container.\nPython 3.7\nFlask 1.0.2\nDramatiq 1.4.3\nPika 0.13.0\nRabbitMQ 3.6.10\nErlang 20.1\nI just started looking at this, so I will see what I can isolate in terms of reproducing the issue.\n. Yeah...if I can isolate it I will try to give you a minimal example.. OK. I have a minimal example that runs into the issue. Again, the issue is intermittent, but I was able to get it to occur.\nhttps://github.com/Nizebulous/pika_example\nI was running the Docker container built in the repo above on an EC2 instance:\nAWS AMI: ubuntu/images/hvm-ssd/ubuntu-xenial-16.04-amd64-server-20180727 (ami-ef151d90)\nuname -a Linux ip-XXX-XX-XX-XXX 4.4.0-1063-aws #72-Ubuntu SMP Fri Jul 13 07:23:34 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.5 LTS\nRelease:    16.04\nCodename:   xenial\nThe most reliable way I was able to reproduce the issue is start the docker container...let it run for a while....then run curl localhost:8080 a couple of times. It failed on the second try....and then you can get the stacktrace with the docker logs command:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 2292, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1815, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1718, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/lib/python3.7/site-packages/flask/_compat.py\", line 35, in reraise\n    raise value\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1813, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python3.7/site-packages/flask/app.py\", line 1799, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"./app.py\", line 25, in schedule\n    do_this_thing.send()\n  File \"/usr/local/lib/python3.7/site-packages/dramatiq/actor.py\", line 179, in send\n    return self.send_with_options(args=args, kwargs=kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/dramatiq/actor.py\", line 198, in send_with_options\n    return self.broker.enqueue(message, delay=delay)\n  File \"/usr/local/lib/python3.7/site-packages/dramatiq/brokers/rabbitmq.py\", line 250, in enqueue\n    properties=properties,\n  File \"/usr/local/lib/python3.7/site-packages/pika/adapters/blocking_connection.py\", line 2206, in publish\n    immediate=immediate)\n  File \"/usr/local/lib/python3.7/site-packages/pika/channel.py\", line 425, in basic_publish\n    (properties, body))\n  File \"/usr/local/lib/python3.7/site-packages/pika/channel.py\", line 1328, in _send_method\n    self.connection._send_method(self.channel_number, method, content)\n  File \"/usr/local/lib/python3.7/site-packages/pika/connection.py\", line 2341, in _send_method\n    self._send_message(channel_number, method, content)\n  File \"/usr/local/lib/python3.7/site-packages/pika/connection.py\", line 2355, in _send_message\n    self._send_frame(frame.Method(channel_number, method_frame))\n  File \"/usr/local/lib/python3.7/site-packages/pika/connection.py\", line 2327, in _send_frame\n    self._flush_outbound()\n  File \"/usr/local/lib/python3.7/site-packages/pika/adapters/base_connection.py\", line 330, in _flush_outbound\n    self._manage_event_state()\n  File \"/usr/local/lib/python3.7/site-packages/pika/adapters/base_connection.py\", line 536, in _manage_event_state\n    self.event_state)\n  File \"/usr/local/lib/python3.7/site-packages/pika/adapters/select_connection.py\", line 446, in update_handler\n    self._poller.update_handler(fileno, events)\n  File \"/usr/local/lib/python3.7/site-packages/pika/adapters/select_connection.py\", line 639, in update_handler\n    events_to_set=events_set)\n  File \"/usr/local/lib/python3.7/site-packages/pika/adapters/select_connection.py\", line 1152, in _modify_fd_events\n    self._poll.modify(fileno, events)\nFileNotFoundError: [Errno 2] No such file or directory. And just for further information...I was able to replicate this running the same docker image on my local mac.. @lukebakken thanks for the help. I definitely made the assumption that threads were being handled somewhere along the chain.. ",
    "merretbuurman": "Hi!\nHere's my code snippet:\n```\nimport pika\nimport sys\nimport logging\nroot = logging.getLogger()\nroot.setLevel(logging.DEBUG)\nha = logging.StreamHandler(sys.stdout)\nha.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\nroot.addHandler(ha)\ncreds = pika.PlainCredentials(\n    'foo',\n    'bar'\n)\nparams = pika.ConnectionParameters(\n    host='foobar',\n    port=5672,\n    virtual_host='baz',\n    ssl=True,\n    credentials=creds\n)\ntry:\n    connection = pika.BlockingConnection(params)\nexcept pika.exceptions.ConnectionClosed as e:\n    print('This is what pika tells me: %s' % e)\n```\nThe host has a RabbitMQ without SSL running on that port.\nThis is the ouput:\n2018-09-11 08:17:50,158 - pika.adapters.base_connection - ERROR - Connection to 999.999.22.1111:5672 failed: [SSL: UNKNOWN_PROTOCOL] unknown protocol (_ssl.c:661)\n2018-09-11 08:17:50,161 - pika.adapters.base_connection - ERROR - Connection to 2001:xxx.yyy.zzz:5672 failed: [Errno 101] Network is unreachable\n2018-09-11 08:17:50,162 - pika.adapters.blocking_connection - ERROR - Connection open failed - 'Connection to 2001:xxx.yyy.zzz:5672 failed: [Errno 101] Network is unreachable'\nThis is what pika tells me: Connection to 2001:xxx.yyy.zzz:5672 failed: [Errno 101] Network is unreachable\nMore verbose log:\n```\n2018-09-11 08:19:34,089 - pika.adapters.select_connection - DEBUG - Using EPollPoller\n2018-09-11 08:19:34,091 - pika.callback - DEBUG - Added: {'callback': >>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2018-09-11 08:19:34,092 - pika.callback - DEBUG - Added: {'callback': >>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 1}\n2018-09-11 08:19:34,093 - pika.callback - DEBUG - Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\n2018-09-11 08:19:34,093 - pika.callback - DEBUG - Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\n2018-09-11 08:19:34,094 - pika.callback - DEBUG - Added: {'callback': >, 'only': None, 'one_shot': False, 'arguments': None}\n2018-09-11 08:19:34,094 - pika.adapters.select_connection - DEBUG - call_later: added timeout  with deadline=1536646774.09437 and callback=>>; now=1536646774.09; delay=0\n2018-09-11 08:19:34,106 - pika.adapters.base_connection - INFO - Pika version 0.12.0 connecting to 999.999.22.111:5672 with SSL\n2018-09-11 08:19:34,184 - pika.adapters.base_connection - ERROR - Connection to 999.999.22.111:5672 failed: [SSL: UNKNOWN_PROTOCOL] unknown protocol (_ssl.c:661)\n2018-09-11 08:19:34,186 - pika.adapters.base_connection - INFO - Pika version 0.12.0 connecting to 2001:xxx:yyy:zzz:5672 with SSL\n2018-09-11 08:19:34,186 - pika.adapters.base_connection - ERROR - Connection to 2001:xxx:yyy:zzz:5672 failed: [Errno 101] Network is unreachable\n2018-09-11 08:19:34,187 - pika.connection - WARNING - Could not connect, 0 attempts left\n2018-09-11 08:19:34,187 - pika.callback - DEBUG - Processing 0:_on_connection_error\n2018-09-11 08:19:34,187 - pika.callback - DEBUG - Calling > for \"0:_on_connection_error\"\n2018-09-11 08:19:34,188 - pika.adapters.blocking_connection - ERROR - Connection open failed - 'Connection to 2001:xxx:yyy:zzz:5672 failed: [Errno 101] Network is unreachable'\nThis is what pika tells me: Connection to 2001:xxx:yyy:zzz:5672 failed: [Errno 101] Network is unreachable\n```. Cool, thanks!. ",
    "ProPheT777": "I already tried with False, it's the exactly same result on rabbitmq side. Consume with False have the expected behavior (Deliver manual_ack), that actually solve my problem.   . > self._channel.basic_get(queue=self._queue, no_ack=False)\nyes\n\nPika version : 0.12.0\nRabbitmq : v3.7.6\nErlang 20.1.7\nPython 3.6.6\n\n. ",
    "mcepl": "Sorry, Open Build Service is the system which produces packages for (among other) openSUSE Linux distribution. I am a general maintainer of Python packages there, which is the reason why I got in contact with pika package. The page of the package on OBS is https://build.opensuse.org/package/show/devel:languages:python/python-pika . Yes, I guess it uses VMs, but generally they shouldn't be too slow (we build all openSUSE packages there, and I don't think we have too many timeout-related problems).. I just hoped that perhaps you would have some idea what could be wrong. Apparently, you don't.. Well, I can add any patch to the package. And this is just a development repo, so I can play there with a package before pushing it to the distro.. OK, the patch applied, and no luck. I tried to run tests in for loop, but it hasn't passed even once. See the full log.\n. Seems, like this helps. Thank you.. Yup, seems to be more stable. Thank you.. ",
    "kamil765": "Hello,\nthanks for quick response, here is a self sustained example, just fill in proper plain credentials (with queue declare rights) and it will reproduce the problem.\nI could make it work with wrapping try around start_consuming, but at that moment all my processing context will be lost as it jumps out of consuming method, I thought maybe there is more systemic solution.\nVersion of pika is 0.12\n```\nimport pika\nfrom pika.credentials import PlainCredentials\nfrom pika.exceptions import ChannelClosed\nmy_credentials = PlainCredentials('tmp', 'tmp')\nconnection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost', credentials=my_credentials))\nchannel = connection.channel()\nchannel.queue_declare(queue='create.application', durable=True, exclusive=False, auto_delete=False)\nchannel.publish(exchange='',\n                    properties=pika.BasicProperties(content_type='application/json', delivery_mode=2),\n                    routing_key='create.application',\n                    body='just text')\ndef callback(ch, method, properties, body):\n    try:\n        # Happens sometimes during code execution\n        channel.basic_ack(delivery_tag=method.delivery_tag)\n    except Exception as e:\n        print(e)\n    finally:\n        try:\n            # Do this only if message has not been acknowledged yet\n            channel.basic_ack(delivery_tag=method.delivery_tag)\n        except ChannelClosed as e:\n            # Or catch an exception that it produces to prevent channel closing?\n            print(\"Inner Channel closed, %s\", e)\n        except Exception as e2:\n            print(\"Inner Generic %s\", e2)\nchannel.basic_consume(callback, queue=\"create.application\", no_ack=False)\nchannel.basic_qos(prefetch_count=1)\nprint(\" [*] Waiting for messages. To exit press CTRL+C\")\ntry:\n    channel.start_consuming()\nexcept Exception as e:\n    print(\"Outer:%s\", e)\nprint(\"Exit\")\n```. ",
    "guanzydev": "I have 3 nodes as rabbitmq cluster, pika use vip from haproxy to connect rabbitmq, but got error log as:\nERROR:pika.adapters.base_connection:Read empty data, calling disconnect\nINFO:pika.connection:Disconnected from RabbitMQ at 10.22.42.1:5672 (-1): EOF\nERROR:pika.connection:Incompatible Protocol Versions\nERROR:pika.connection:Connection setup failed due to The protocol returned by the server is not supported: (-1, 'EOF')\nDEBUG:pika.callback:Processing 0:_on_connection_error\nDEBUG:pika.callback:Calling > for \"0:_on_connection_error\"\nDEBUG:pika.callback:Processing 0:_on_connection_closed\nDEBUG:pika.callback:Calling > for \"0:_on_connection_closed\"\nDEBUG:pika.callback:Incremented callback reference counter: {'callback': >>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\nDEBUG:pika.callback:Incremented callback reference counter: {'callback': >>, 'only': None, 'one_shot': True, 'arguments': None, 'calls': 2}\nERROR:pika.adapters.blocking_connection:Connection open failed - The protocol returned by the server is not supported: (-1, 'EOF')\nTraceback (most recent call last):\n  File \"test.py\", line 34, in \n    connection = pika.BlockingConnection(node1)\n  File \"/usr/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 374, in init\n    self._process_io_for_connection_setup()\n  File \"/usr/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 414, in _process_io_for_connection_setup\n    self._open_error_result.is_ready)\n  File \"/usr/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 466, in _flush_output\n    raise maybe_exception\npika.exceptions.IncompatibleProtocolError: (-1, 'EOF')\nSo I want to find a way to use host list in pika, and it would connect other host when current is down.. ",
    "Str-Gen": "Unfortunately I currently don't have the time nor the familiarity with the project and its dependencies to provide the extensive testing code, requested by the authors. . ",
    "timabbott": "@lukebakken I guess the main potential concern with doing my proposal is that it's not completely backwards-compatible (users would need to update their imports).  Just wanted to check the maintainers for that before potentially doing a PR.. OK, I've now tested that it indeed solves the intended problem in Zulip with this 2-commit branch: https://github.com/timabbott/zulip/tree/pika-testing; with that, importing pika is just 35ms on my system.  \nhttps://github.com/timabbott/zulip/commit/014c261e4a64412c05885e2bbf236c55b31585ac is the part of that PR that is the migration for this change; the other commit is just reverting the hack I'd added to work around this issue in Zulip.\nStill haven't seen whether CI passes here, so I'm leaving the WIP tag on this PR.. OK, it looks like CI passed (the only failure looks like a coverage infrastructure flake), so this is ready for a review.. ",
    "jothan": "I seem to have duplicated existing code on master, sorry.\n. ",
    "clippered": "Thanks for your quick response @lukebakken . I'll remember to use the google groups next time.. ",
    "octohedron": "There you go @lukebakken https://github.com/octohedron/pikatest follow the instructions in the readme and let me know if you need anything else.\nTo see the issue, run the send.py (after running the spider) and notice that it doesn't run the first request, it waits until x number of messages were received to yield the request in the start_requests method.\nJust to give you an idea of why it's broken, scrapy's start_request method yields instantaneously when passing a list of urls, i.e. [\"http://example.com\"] so, it shouldn't wait until x number of messages are received to start yielding requests, something is stopping the function.. ",
    "claws": "The existing examples in the repo do not work with pika version 0.12.0 or 1.0.0. This change will make them at least work with the code in master.. I have removed the non-functional yapf related changes.\nShould a seperate issue be raised on the CONTRIBUTING.md instructions that ask contributors to format their modified files with yapf?. > @claws please do that and use stable as the base branch for that pull request. The problem with those auto-formatters is that they change far more than the original intent of the pull requests.\nAs requested I have raised #1139, which targets the stable branch, to remove the line requesting contributors use yapf to format files they have changed.. ",
    "Vanlightly": "I think I may have misunderstood how the multiple flag works. Let me know if this interpretation matches yours:\nIf we consider we have sent 10 messages which are pending, then we can think of those 10 messages existing in a set of pending messages. \nEach ack/nack we get back with a multiple=false, we simply remove that message from the pending set.\nWhen we get a multiple=true, then we remove all messages in the pending set with a sequence number equal or lower.\nSo, if the pending set starts as [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nack delivery tag 1, multiple false. [2, 3, 4, 5, 6, 7, 8, 9, 10]\nack delivery tag 3, multiple false. [2, 4, 5, 6, 7, 8, 9, 10]\nack delivery tag 5, multiple false. [2, 4, 6, 7, 8, 9, 10]\nack delivery tag 6, multiple true. [7, 8, 9, 10]\nack delivery tag 8, multiple false. [7, 9, 10]\nack delivery tag 10, multiple true. []\n. Ok, will do. Thanks.. ",
    "azmeuk": "Nevermind, BlockingChannel does have a return value.\nhttps://github.com/pika/pika/blob/master/pika/adapters/blocking_connection.py#L2223. Ok thanks!. ",
    "yutiansut": "\npika version : 1.0.0b1\npython 3.6.7X64  OS: win10\nRABBITMQ : 3.7.8\n\nusing 'fanout' exchangetype, and the error occur in subscriber( using Blocked Connection)\n. sorry for that, to much mesage..\ni just use try/except to catch this. ",
    "Rmkek": "Hello! Ran into this issue as well.\n\nPika 1.0.0b1\nPython 3.7.1, Linux 4.19.13-1-MANJARO, x64\nRabbitMQ, Erlang 21.2\n\nThe error was because of function call using asyncio where function was not annotated with @asyncio.coroutine.. ",
    "zkqiang": "I ran the unmodified example code (just used remote host)\nrabbitmq 3.7.8\npika 0.12\npython 3.6. THX. I'm glad you are paying attention to this issue.. Ok, it's my mistake.\nNo error when I use this code.\nThank you. ",
    "caibojun": "It seems some where is still wrong,\nch.basic_publish(exchange='',\n                     routing_key=props.reply_to,\n                     properties=pika.BasicProperties(correlation_id = props.correlation_id,\n                                                     headers = header_data),\n                     body=response) \nIn that code,response is a binary string converted by Excel file(*.xls) , and the code dosen't raise any wrong but I cann't get any  returns in my client .. ",
    "aqparks": "Via Google translate...\n\"After I finish executing a time-consuming task, I am ready to write data to rabbitmq, the queue can not write. When the task is finished, the data appears in the queue.\". ",
    "Avivsalem": "@lukebakken, Thank you for your response!\nThe only reason I thought it might be a bug or a race condition, was that setting the inactivity_timeout to 0.01 made it work like I expected, which is still faster than the network delay...\nI thought it has something to do with calling process_events internally with 0 as param... \nThanks anyway! . As I wrote on the original issue, when passing non-zero to inactivity_timeout, no matter how small, it works as I expected...\nSo I looked at the implementation of the generator, and looked for uses of inactivity_timeout. \nI saw it being passed to process_data_events() as a parameter.\nI didn't keep digging, but my gut feeling was that when I pass 0 to process_data_events() It doesn't actually process the frame of the message being available again, while when I pass a non-zero small number, I does process the frame... \nAgain...i didn't investigate deeply, just reporting on my results, and my hypothesis... . We're using 0.13.0 release from pypi (don't know the difference between that and 0.13.0b1)\nSecond, it happens sporadically on our most heavy consumers. \nWe use prefetch_count=1\nand we consume from mirrored queues if that matters... \nAlso, we have multiple consumer channels on same connection \n. And by multiple consumers, I mean in a one thread round Robin fashion... I'll try to post a gist later... . @lukebakken\nStill didn't manage to pull the code for a gist... But I have some thought about what might be happening...\nIf you set the heartbeat for a short enough time, and then run the code you wrote on an EMPTY queue, with inactivity_timeout=0.0, eventually, the error will occur (after heartbeat.)\nI think it's because when inactivity_timeout=0, the code doesn't process the heartbeats at all... Makes sense? . @lukebakken \nfollowed it more into the code...\nthe lines in questions are:\nhttps://github.com/pika/pika/blob/df10c154e7d1948e0943b0fe07a26021868e3d23/pika/adapters/blocking_connection.py#L1940 is called with time_limit=0\nthen https://github.com/pika/pika/blob/df10c154e7d1948e0943b0fe07a26021868e3d23/pika/adapters/blocking_connection.py#L752 is called, with a _IoloopTimerContext that has 0 as time_limit, \nand finally this https://github.com/pika/pika/blob/df10c154e7d1948e0943b0fe07a26021868e3d23/pika/adapters/blocking_connection.py#L457 doesn't enter the while loop at all... so \nself._impl.ioloop.poll()\n self._impl.ioloop.process_timeouts()\nnever gets called...\ni believe this is the main difference between 0.0 and 0.01... and the reason for the metioned error... (this, combined with the fact that the heartbeat interval has passed, and was not handled on an empty queue).\ni know this is a \"voodo\" scenario, but we actually encounter this.. You too! Thanks for your time. . We use heartbeat=300. ",
    "booleys1012": "I am including this note just to supplement the issue with additional information in case it is helpful.\npython2.7\npika==0.13.0\nRabbitMQ 3.6.10, Erlang R16B03-1\nI have a web application (django via uwsgi) and multiple python 2.7 microservices publishing to RabbitMQ using the same code (using pika). I am not using any python threading related to publishing or subscribing -- this includes not using threading in uwsgi. This has been operating for approximately 2 years without seeing the error logs mentioned above.\nDocker migration\nI recently migrated all services to container deployments (docker) as a proof-of-concept and I see this error now after a handful of minutes of successful publishing, but only on the webserver -- the microservices do not appear to exhibit this error\nNo code changes, just the migration to docker\nUpdate (2019-02-27)\nI am starting to think it was related to uwsgi and the --lazy-apps flag, which I did turn off as part of the above docker release (apologies). I am thinking the difference is creating a pika connection pre-fork in uwsgi versus post-fork. Creating the connection pre-fork gives any forked processes an (illegal) copy of the connection (file descriptor), which could explain the IOError. This would negate the implications that this is related to docker. I'll add further updates if I find anything more.\n. ",
    "adaptivelogic": "Being able to do this is the whole point of this pull request.\n. ",
    "quantum5": "I thought about using it, but decided against it because:\n``` py\n\n\n\nimport numbers\nnumbers.Integral.register(type(None))\nisinstance(None, numbers.Integral)\nTrue\n``\n. My point is that people can register any type to be anIntegral, and so I decided to not risk it.NoneTypewas just an extreme example.\n.TrueandFalseare basically1and0`. They even work in arithmetic:\n\n\n\n``` py\n\n\n\nTrue + True\n2\nTrue + False\n1\nFalse + False\n0\n5 + True\n6\nFalse - 100\n-100\n```\n. \n\n\n",
    "dmacnet": "This code fails to pass the passed-in asyncio loop to the Future() and ClosableQueue() calls, so it hangs when yielding from .ready if you're not using the default loop. It also fails to provide an AsyncioChannel.close() method and an AsyncioProtocolConnection.close() method.\n. "
}
{
    "lsegal": "Thanks for opening this. A discussion on Twitter with @jed was recently started on this very topic. I think it should be possible to support both the primitive node callback style and the promise objects together.\n. Thanks @mhart. I've updated the npm install snippet in @trevorrowe's comment. We plan on tackling the error object issue when we address #4, so let's follow that there.\n. I'm prototyping something based on AWS.Error right now, FYI.\n. See the node-errors branch for the code.\n. This is now supported by either globally setting:\njs\nAWS.Config.httpOptions = {timeout: 5000};\nOr per-service:\njs\ns3 = new AWS.S3({httpOptions: {timeout: 5000}});\nNote that all timeout values are in milliseconds.\n. I had updated the tags a couple of weeks ago but forgot to close this issue. I think adding the v prefix makes sense, thanks for the suggestion.\n. The example wasn't fully updated with the Api changes, but if you simply remove resp it should work.\n. What happens if you log only data? If an error occurred, the value may be null. You should always check err before using data.\n. What is the result of AWS.VERSION?\n. I see what the issue is-- you likely have not set your region or your credentials. We are working on making the error message more obvious for this one, but the solution is to set the region via AWS.config.update. This is documented at the top of the Examples section in the guides. Something like the following should work (assuming your credentials are set via envrionment variables):\njs\nvar AWS = require('aws-sdk');\nAWS.config.update({region: 'us-east-1'});\n. I've opened an issue for the unintuitive error at #10. I will also correct the example documentation.\n. This was previously fixed in 6417d9de1b3561defc0fa8dc335176cf8d122ddb but wasn't pushed out to the public documentation. That happened today, so everything should be in sync now.\nThanks for reporting!\n. We've decided to drop the promise API altogether and opt for a much simpler event based callback system which was just merged in via #22. The event based refactor allows for EventEmitter events registered on the request object. The initial goal here was to allow for success/fail callbacks, but this can now be achieved with something like .on('success', callback) instead.\nIf promises are needed, it makes much more sense to go to the many already-available libraries like promise.js or Q as mentioned in #2.\n. We already support callbacks as implemented in #2. The event based system is just extra sugar to allow for specific events that are not compatible with a callback/promise based system (like data streaming events). I don't think it's wise for us to make an opinionated decision on which promise library to depend on when there are so many options out there. Our current callback-style interface should allow any of those promise libraries to be plugged in and used if needed.\n. @ForbesLindesay, I am a little confused about the need to \"adopt a standard\" for promises. It seems like it should not be the concern of the AWS SDK to implement, return, or deal with promises in any way. It's very trivial to bake your favourite promise library into the API with a helper method, or even by extending the API:\njs\nAWS.Request.prototype.promise = function() {\n  var deferred = require('q').defer();\n  this.on('complete', function(resp) {\n    if (resp.error) deferred.reject(resp.error);\n    else deferred.resolve(resp.data);\n  });\n  this.send();\n  return deferred.promise;\n};\nYou can use this quite easily:\njs\ndb = new AWS.DynamoDB({region:'us-east-1'});\ndb.client.listTables().promise().\n  then(function(data) {\n    console.log(data);\n  }, function(err) {\n    console.log(err);\n  }).then(...);\nNow you have a \"real\" promise, and there's no need for the SDK to take on extra dependencies for users who have no need for a promise API. The above snippet could make a great aws-sdk-promises plugin, and would keep concerns of the core library separate from having to adopt any specific interface.\n. @ForbesLindesay:\n\nReturning an object that is a stream that fires 'httpData' instead of 'data' and 'httpDone' instead of 'end' is insane and doesn't inherit from stream is insane.\n\nThis is why we should not make the Request object a stream, because this would be insane. @trevorrowe pointed this out: Request objects are not, and should not, be Streams. Making them as such would make no conceptual sense. Requests contain context for a service request and are much more than just a handle to a socket to read and write from. Yes, they share similar concepts with Streams, but only through composition-- a request is composed of a Stream; it is not, itself, a Stream. Smashing these two objects into one would be confusing from an architectural point of view, and I think it would cause a lot of user confusion as well. The idea to createReadStream() in order to get back a proper Steam object makes sense and seems perfectly in line with how Node's fs module works. It also properly separates these composable objects and keeps two very separate and well encapsulated components.\nI also think it's important to take a step back and realize that the lifecycle of a request to AWS is much more than just an HTTP socket stream that you read and write from. There is also plenty of logic in building, signing, retrying and parsing data from responses that is baked into the whole request lifecycle that seems to be taken for granted here. These are interesting parts of a request lifecycle that we are trying to expose to users through events, and make the system much more composable and extensible. A promises interface is much too simplistic to answer these questions, this is why we created a much more robust event-based system. The more simplistic promises interface can be much more easily derived from the event-based system than the other way around.\nAs a sidenote, prefixing the HTTP portion of the lifecycle with \"http\" was done to conceptually separate these events for our users. Furthermore, emitting the \"end\" event for the end of the HTTP stream would be wrong, because the end of the HTTP request does not signify the end of the request (from a logical point of view, it's actually less than half of the full lifecycle). If anything, the \"end\" of the request is on \"complete\", but we explicitly decided against naming this end to avoid confusion with Streams (recall: Requests are not Streams). Ditto for \"data\", as there are actually two kinds of \"datas\" in a request (the raw HTTP data and then the parsed response XML/JSON data).\nI think looking at the bigger picture is important, here. Providing streaming operations and promise interfaces can be useful things, but there are better places for these things than at the low-level client APIs.\n. I understand that Streams are not inherently low level, but the Request object is inherently low level. Streaming parsed JSON is a completely orthogonal concern to the Request class, and can be implemented independently of the Request in higher level APIs (you would actually be operating entirely on the Response object).\nAs far as one or two stage: I don't see this as significant. I don't believe the value of Node is that you can do everything with a single one-step method call (and that is not even true). Separating concerns is a much more valuable concept, and I think even Node developers would agree that preferring composition over inheritance is ideal.\nFinally, initializing a request when a listener registers to the 'data' event simply cannot be done. This approach might work in the simple case of piping content directly to a stream, but it fails miserably in a more complex scenario, i.e., if multiple objects want to register for the 'data' event (one debug logger, one stream).\nHowever-- to clarify: we are in agreement that we should add better Streaming support. We just merged in #3 which addresses parts of these issues, and @trevorrowe's streaming snippet can be added to give better stories for streaming raw content. Streaming parsed XML/JSON will most likely be left up to higher level APIs, though.\n. Can you list concrete examples of where the SDK is not interoperable with \"the rest of node.js\"? I have shown that you can quite easily interop with the promise interface using a simple wrapper, and @trevorrowe has provided an easy adapter method that returns a stream from a request (something which we agree we would like to incorporate into the API). Are there any technical limitations with these solutions? It would be much more useful if we had examples of concrete cases where the solutions proposed do not work.\n. I don't consider the create[Read|Write]Stream() proposal to be a shim since the Request object is not actually a Stream object (and is not intended to be). From a technical perspective, there is no difference to calling stream.pipe(out) vs. stream = req.createReadStream().pipe(); it is simply a stylistic distinction.\nAs far as promises go, yes, this is a shim, but I don't believe this wrapper code belongs in the core library. Given that users who are interested in promises will already have a library of choice pulled into their applications, it is not correct for us to make assumptions about which promise library to use, especially since writing a wrapper is trivial and could be published as a separate package for those users who want a promise interface. It's also important to note that part of the reason promises were dropped from the SDK was the overwhelming feedback from users in #2 stating that promises were, in fact, no longer idiomatic in Node, and it was something they were simply not interested in seeing in the library. And you can always add it in if you really do want it.\n. @Raynos:\n\nyour custom events are not documented\n\nThese changes were just merged in yesterday-- the documentation on http://docs.amazonwebservices.com/AWSJavaScriptSDK/latest/frames.html represents the last release (v0.9.1-pre.2), not what is in GitHub. There will be published documentation for all of the events emitted by the request object come the next release.\n\nit is trivial to infer how to read your data if I inspect an AWSRequest instance and see a pipe method.\n\nThis is where things get complicated. How do you define \"the data\"? Only for streaming operations (a handful of methods on Amazon S3 and Glacier) is the raw HTTP body equivalent to \"the data\" (and even then, it's not quite the only data). In most other cases, the actual data will be post processed, will NOT be string/buffer data, and will be much more useful than the raw data that would be retrieved by pipe(). In the case of non-streaming operations (the vast majority of AWS operations), we do not want users using pipe() to stream raw data, as this short-circuits most of the valuable work the SDK does to extract data from responses. In other words, unless you're streaming files to/from S3, you probably don't want to be calling pipe() anyway.\n. @Termina1 @domenic @ForbesLindesay just to clarify, you can use the aws-sdk to stream PUT operations to S3, you simply need to pass a stream object as the body parameter:\njs\nvar fs = require('fs');\nvar stream = fs.createReadStream('/path/to/file');\ns3.putObject({Bucket:'bucket', Key:'key', Body:stream}, function() { ... });\nFor streams that are not FS objects, you will need to define a .length property on the stream object to tell the SDK how large the payload is, but we were just discussing exposing a ContentLength property in #94 to allow this as a separate parameter. That would make the story very similar to knox.\n. @mitar streams are supported for payloads in both directions. For getObject you would call .createReadStream() to return a stream wrapper for the request object like so:\njs\nvar fs = require('fs');\nvar writeStream = fs.createWriteStream('/path/to/outfile');\nvar readStream = s3.getObject(params).createReadStream();\nreadStream.pipe(writeStream);\nWe are working on revamping our guides to make these examples more prominent-- until then, you can see this in our API docs for the AWS.Request object.\n. @ForbesLindesay as Node moves to the new \"streams2\" API, it is no longer necessary for library writers to support back-pressure. This is something that is now supported by the underlying streams implementation with the new internal high/low water mark settings. Effectively, the stream API is  now poll instead of push, meaning the consumers now read whatever they can (and want to) instead of what they are forced to. Pause and resume are no longer used in the new API, so this simplification means we do not have to support back-pressure at the SDK level going forward. That's a great thing for us and all other library developers implementing custom streams or stream wrappers.\n. @ForbesLindesay I think the buffering issue you raised was an oversight of the implementation. I've just pushed a fix in 91e5964dc3f69384487be5136691fe26ef5d608a that addresses buffering. Previously we were taking data events right from our HttpClient stream which reverted it to the old-style stream. We just didn't get around to making the stream2 switch everywhere.\nThis change corrects that and respects the new readable event everywhere, so that our createReadStream() only calls read() on the underlying HTTP socket when you do. This should implicitly support the backpressure functionality you've been talking about.\nA few notes:\n\nYou don't get this for free if you wrap the underlying streams in something that looks like a stream but isn't, then convert that non-stream back into a stream.\n\nWe are not wrapping the stream in something that looks like a stream but isn't. The stream we are exposing is (and always was) a bona fide stream.Readable (or Stream before Node 0.10.0) that implements the proper read protocol. The problem was that prior to the above commit we were not fully respecting the streams2 protocol. Please correct me if I'm wrong, but the streams2 protocol _should hide all backpressure details from stream implementors, as the stream implementors never actually try to read anything; the consumers do. Therefore we do actually get backpressure for free by implementing this protocol properly. Basically, in a Node 0.10.x client, our streams do not buffer data-- we only read when the consumer asks for bytes through the readable event. If a starved process stops asking for bytes, we should stop trying to read from the underlying HTTP stream. \nLet us know if we're still missing anything here, otherwise I think this issue is set to be closed.\n. Thanks for the inline comments @ForbesLindesay, they look helpful. We will look through them to see what we can fix up and simplify. It's also good to see sign off from members of the community when we are doing things right, even if they might not immediately be perfect.\nAs for your examples, I'm not sure this is the right approach for our SDK long-term. I can think of a few problems with that API:\n1. Your proposed \"createReadStream()\" method operation does not map to a specific operation on the S3 service, which deviates from our explicit design of keeping the Service objects (previously \"Clients\") a 1:1 mapping of the actual service operations (you can see these service definitions in our lib/services/api/ directory which we use to build up-to-date service objects).\n2. Following from (1), it is non-obvious that \"createReadStream\" refers to \"getObject\". More importantly, a service might have multiple operations that can stream a payload of data (S3 and Glacier already do, if you include multi-part operations). In those cases, our API will be confusing to users trying to operate on those \"secondary\" streaming operations. In short, there is more than one kind of streaming operation that can exist per service, so it does not make sense to map this one stream call for the entire service. This goes similarly for createWriteStream and putObject.\n3. The proposal is specific to the S3 service (or specific to a service), which means we would have to opt-in to this kind of functionality for other services (like Glacier, which also supports streaming). The benefit of the createReadStream living on AWS.Request is that you can pipe HTTP data from any operation on any service without us having to explicitly support it. This is much more of a composable interface, IMO, and supports many more of AWS's use cases without any extra manual code. I think higher level APIs can do more to make these service objects easier to use, but those interfaces should live outside the S3 service object and be a composable interface in the same way the 'through' module or 'concat-stream' modules live outside stream and are composable.\nFinally, I don't know that our current interface is less node-like. It might be slightly more verbose, but not less node-like. If you are comparing it to 'fs', this might be a little unfair. Let me explain why:\nIf you think about the API in terms of creating streams from resources, createReadStream should give you a stream for a given resource. I think we both would agree on this interpretation (yes? no?). The difference here is that there is only one \"fs\" object, therefore it makes sense to say fs.createReadStream() and never instantiate a new fs (it's basically a singleton representation of your file system). Services, however, have multiple resources that can be turned into streams, specifically, all of their operations. In other words, we stream operations, not services. If it were possible to access multiple \"filesystems\" in the same way that we create operation requests, I think it would be completely idiomatic/node-like to say something like fs('/dev/sda1').createReadStream('/path/to/file'), but the lack of this functionality means there is no node-like example in the standard library, and for that reason, our API seems out of place.\nIs there something I'm missing here, or is it really just the fact that we are not creating a stream from a single service object (something that seems like a \"singleton\")?\n. Thanks @ForbesLindesay. All the feedback you've provided is certainly appreciated. I think at this point it seems like we've moved beyond the original issue that was reported, namely returning real streams. Although the API might have some rough edges for certain use cases, and there is some optimization / refactoring that can be done on the internals, it seems that we are indeed providing real stream objects that can be streamed from as normal. The internals can be refactored over time without affecting compatibility, and the rough edges can be worked down. I believe this issue can be closed (we've met this goal), but I also believe we should certainly keep iterating on this. At this point, I actually think the best way for you to help us iterate would be through pull requests that implement the modifications you are suggesting. That way we can compare the pros and cons with real examples rather than hypotheticals.\nOn a sidenote, I should point out that @trevorrowe and I have already discussed and taken serious looks at what it would take to split the SDK up into multiple packages. We've also heard that requests from others, and it's not lost on us. There are some pros and cons with multiple packages, but the option is still on the table. I don't think further discussion of that topic belongs in this issue, though.\nAgain, thank you to everyone who took part in this discussion!\n. The failure scenario with that change is expressed in a test added in the commit prior to that one. The problem is raised if you make assumptions about the data['Body'] value being a String object (like, calling .length on it). This matters, since that body should actually be a 0-length file, so it should be a string, not null.\n. This can be closed, as it was fixed by #16.\n. We absolutely do have plans to build up service support. The goal is to have the Node.js SDK covering as many services as our other SDKs.\n. This was added by #23\n. I can't seem to reproduce this in my testing. Can you provide details about your system? OS, node version? etc.\n. Note I'm not testing this within Meteor, but given the stack trace it seems like the issue is coming from the framework, specifically in \"app/packages/livedata/livedata_server.js:131:29\"\nI don't know what that file is, but given the function name in the trace, it looks like they might be extending the object with some circular references that make the JSON.stringify puke. Not even sure if it's the SDK code that is puking, though.\n. Can you reproduce this from a plain node console? Try the following:\njs\nAWS = require('aws-sdk');\ns3 = new AWS.S3({region: 'us-east-1'});\ns3.client.deleteObject({Bucket: 'yourbucket', Key: 'yourkey'}, function(err, data) {\n  console.log(err, data)\n});\n. Sidenote: we currently only test the SDK on and support Node 0.8.x -- that might be relevant here as well. I would also try the latest version of Node (v0.8.16) after testing the above snippet (regardless of the output).\n. Make sure to load credentials and set a region when using the SDK, that \"Not a string or buffer\" error usually occurs when credentials or region is not set. My snippet assumes you're loading credentials via environment variables, but if not you should load them manually with AWS.config.loadFromPath\n. We've made changes to the Request class API since 0.9.1.pre-2. The changes are currently only in master and have not yet been released, so there isn't much available documentation. This will change by the next release, but to quickly get you going, you can rewrite the above as:\njs\nvar stream = fs.createWriteStream(\"/tmp/test.jpg\", { flags: 'w', encoding: null, mode: 0666 });\ns3.client.getObject({ Bucket: bucket, Key: key }).on('httpData', function(chunk) {\n    stream.write(chunk);\n}).on('complete', function() {\n    stream.end();\n}).send();\nNote that we may be adding more streaming support before the next preview release, so the API might be simplified considerably to something like:\njs\nvar out = fs.createWriteStream(\"/tmp/test.jpg\", { flags: 'w', encoding: null, mode: 0666 });\ns3.client.getObject({ Bucket: bucket, Key: key }).createReadStream().pipe(out);\nI would suggest that you should run off of the npm package releases until the next preview is out unless you explicitly want to experiment with the current features that are in the works-- with the caveat that some documentation may not be in sync until we push out the next release.\n. You could apply this workaround without hacking source directly via some code in the header of your application:\njs\nvar operation = AWS.EC2.Client.prototype.api.operations.createTags;\noperation.i.m.Resources.m = {n: 'ResourceId'};\noperation.o = {};\nBut yes, this should not be necessary.\n. Thanks for reporting, this issue should be resolved by the above commit.\n. This can be closed along with #25, thanks again for the patch!\n. The above commit should fix the copyObject method failing outright; it does not address the other part of your issue, though (still working on that one).\n. We have cucumber level tests for SQS in https://github.com/aws/aws-sdk-js/tree/master/features/sqs\nHave you looked at these?\n. We are actually trying to move the S3 tests into cucumber, so those are a little stale. Reading through your tests, we probably have the same coverage in our cucumber tests for SQS. Sorry about the confusion! The contribution is very much appreciated though-- if you want to help improve our coverage in our cucumber suite, that would be great!\n. See #19 for a workaround and information on a fix that is currently available in the master branch and will be made available in the next preview release.\n. See #19 for a workaround and information on a fix that is currently available in the master branch and will be made available in the next preview release.\n. (closing since this is a duplicate of #19)\n. (closing since this is a duplicate of #19)\n. This patch actually causes our tests to fail (not related to the false positive from Travis CI's failed test above) because arrayEach is not explicitly meant just for arrays, but rather to iterate over properties with the property value as the first param of the callback (rather than the standard each, which passes the property name first). It's named arrayEach because it's more useful for array iteration (where you don't care about the property name, aka the index), but we actually use this in event_emitter.js for objects that can be arrays or standard objects. The related failing tests are failing when a non-array is passed into the above function.\nAll that said, there are two ways to solve this:\n1. We take a hard stance that arrayEach only allows actual Array objects as parameters. This would mean changing the above failing code to check Array vs non-array objects and choosing the appropriate looping function, or,\n2. We add the if (obj.hasOwnProperty(prop)) check that each uses to still handle both object types at a performance cost. \nI'm inclined to go with option 2, since I think a better policy for array-primitive iteration is to inline the for loops directly, ie., if we know we're always dealing with arrays, we should not be using the util function.\n. This patch actually causes our tests to fail (not related to the false positive from Travis CI's failed test above) because arrayEach is not explicitly meant just for arrays, but rather to iterate over properties with the property value as the first param of the callback (rather than the standard each, which passes the property name first). It's named arrayEach because it's more useful for array iteration (where you don't care about the property name, aka the index), but we actually use this in event_emitter.js for objects that can be arrays or standard objects. The related failing tests are failing when a non-array is passed into the above function.\nAll that said, there are two ways to solve this:\n1. We take a hard stance that arrayEach only allows actual Array objects as parameters. This would mean changing the above failing code to check Array vs non-array objects and choosing the appropriate looping function, or,\n2. We add the if (obj.hasOwnProperty(prop)) check that each uses to still handle both object types at a performance cost. \nI'm inclined to go with option 2, since I think a better policy for array-primitive iteration is to inline the for loops directly, ie., if we know we're always dealing with arrays, we should not be using the util function.\n. sidenote: along with option 2, we might want to rename the function to cause less confusion. Although the function is in our private API, I agree that the intent can be confusing.\n. sidenote: along with option 2, we might want to rename the function to cause less confusion. Although the function is in our private API, I agree that the intent can be confusing.\n. That would be great!\n. That would be great!\n. Just push to your master branch with the fix. GitHub should pick it up.\n. Just push to your master branch with the fix. GitHub should pick it up.\n. Thanks @corymsmith!\n. Thanks @corymsmith!\n. The entries should be listed as an array inside of the key \"Entries\", not \"DeleteMessageBatchRequestEntry\":\njs\nsqs.client.deleteMessageBatch({\n    QueueUrl: \"https://sqs.us-east-1.amazonaws.com/863688628359/Build-Output\",\n    Entries: [ { Id: \"foo\", ReceiptHandle: data.Messages[0].ReceiptHandle } ]\n}, function (err, data) {\n    console.log('deleteMessageBatch ' + JSON.stringify(err));\n    console.log('deleteMessageBatch ' + JSON.stringify(data));\n});\nDoes that work?\n. The entries should be listed as an array inside of the key \"Entries\", not \"DeleteMessageBatchRequestEntry\":\njs\nsqs.client.deleteMessageBatch({\n    QueueUrl: \"https://sqs.us-east-1.amazonaws.com/863688628359/Build-Output\",\n    Entries: [ { Id: \"foo\", ReceiptHandle: data.Messages[0].ReceiptHandle } ]\n}, function (err, data) {\n    console.log('deleteMessageBatch ' + JSON.stringify(err));\n    console.log('deleteMessageBatch ' + JSON.stringify(data));\n});\nDoes that work?\n. Can you explain what the confusion is in using accessKeyId versus AWSAccessKeyID? Was it not clear in the documentation where these key names were found? We could do a better job of exposing this.\nIt should be noted that accessKeyId is more consistent with many of the other SDKs supported by Amazon. Also, I personally think shorter is better when it comes to configuration names, and the \"AWS\" prefix would be a little redundant in the context of the AWS SDKs.\n. Can you explain what the confusion is in using accessKeyId versus AWSAccessKeyID? Was it not clear in the documentation where these key names were found? We could do a better job of exposing this.\nIt should be noted that accessKeyId is more consistent with many of the other SDKs supported by Amazon. Also, I personally think shorter is better when it comes to configuration names, and the \"AWS\" prefix would be a little redundant in the context of the AWS SDKs.\n. This is fixed in the master branch by issue #7.\n. This is fixed in the master branch by issue #7.\n. I believe this is a duplicate of #42 and should be fixed in the latest master branch.\n. I believe this is a duplicate of #42 and should be fixed in the latest master branch.\n. Good catch, thanks!\n. Good catch, thanks!\n. There will be another release including this fix. Until then, you can ignore this parser error and use the following workaround:\njs\ns3.client.getBucketPolicy(params, function() {\n  console.log(this.httpResponse.body.toString());\n});\n. There will be another release including this fix. Until then, you can ignore this parser error and use the following workaround:\njs\ns3.client.getBucketPolicy(params, function() {\n  console.log(this.httpResponse.body.toString());\n});\n. AWS_CONFIG_FILE is a different thing from using instance metadata, because it won't stop you from hardcoding the credentials on the machine. That said, you can always use AWS.config.loadFromPath(process.env.AWS_CONFIG_FILE) if you want this behaviour. Is AWS_CONFIG_FILE used in any other Amazon tools? If it is, it's likely that it's not in the same format as we would expect in the Node.js SDK, so supporting this out of the box might not work.\n. AWS_CONFIG_FILE is a different thing from using instance metadata, because it won't stop you from hardcoding the credentials on the machine. That said, you can always use AWS.config.loadFromPath(process.env.AWS_CONFIG_FILE) if you want this behaviour. Is AWS_CONFIG_FILE used in any other Amazon tools? If it is, it's likely that it's not in the same format as we would expect in the Node.js SDK, so supporting this out of the box might not work.\n. >  If IAM roles were to be implemented, would it be in a different section of the code? \nNo, it would be the next check in the chain after env vars. I actually just pushed the EC2 instance metadata branch, see #78. We don't check disk in the chain though, because as you pointed out, we don't use AWS_CONFIG_FILE as a standard mechanism for loading credentials. It seems that this variable is new to the AWS CLI tool, so we could add support for this, but note that it's not in a JSON format, so that might be something Node developers might not be used to.\n\nThis is of consequence only because there is no afaik no standard export for region defined,\n\nWe have AWS_REGION for a standard region, actually. You can use that.\n. >  If IAM roles were to be implemented, would it be in a different section of the code? \nNo, it would be the next check in the chain after env vars. I actually just pushed the EC2 instance metadata branch, see #78. We don't check disk in the chain though, because as you pointed out, we don't use AWS_CONFIG_FILE as a standard mechanism for loading credentials. It seems that this variable is new to the AWS CLI tool, so we could add support for this, but note that it's not in a JSON format, so that might be something Node developers might not be used to.\n\nThis is of consequence only because there is no afaik no standard export for region defined,\n\nWe have AWS_REGION for a standard region, actually. You can use that.\n. Roles on EC2 instances should now be transparently supported. Those interested in testing this out can pull down the master branch and give it a spin!\nNote that we currently do not handle invalidation of expired credentials, we will be adding this before the next release, and we are tracking that specific feature as #80\n. Roles on EC2 instances should now be transparently supported. Those interested in testing this out can pull down the master branch and give it a spin!\nNote that we currently do not handle invalidation of expired credentials, we will be adding this before the next release, and we are tracking that specific feature as #80\n. This looks like a bug. From my tests it looks like you are correctly able to put the metadata (which you seemed to notice as well), but getting it back out is the problem.\nA temporary workaround for this would be to manually grab the 'x-amz-meta-metainfo' header key from the httpResponse property directly:\njs\ns3.client.getObject(params, function() {\n  console.log(this.httpResponse.headers['x-amz-meta-metainfo']);\n});\nOr just print the headers to see the other metadata you may have set.\nAs a sidenote, the \"base64\" method you mentioned is no longer necessary. The SDK correctly supports Buffer objects in the latest version, so you can simply pass myAudioFile to Body in your putObject call, assuming it is a Buffer object.\n. This looks like a bug. From my tests it looks like you are correctly able to put the metadata (which you seemed to notice as well), but getting it back out is the problem.\nA temporary workaround for this would be to manually grab the 'x-amz-meta-metainfo' header key from the httpResponse property directly:\njs\ns3.client.getObject(params, function() {\n  console.log(this.httpResponse.headers['x-amz-meta-metainfo']);\n});\nOr just print the headers to see the other metadata you may have set.\nAs a sidenote, the \"base64\" method you mentioned is no longer necessary. The SDK correctly supports Buffer objects in the latest version, so you can simply pass myAudioFile to Body in your putObject call, assuming it is a Buffer object.\n. @BharatMeda: as of the above commit, Metadata should now be properly filled. If you aren't seeing the headers in your getObject requests, it's possible they are not being properly set in your putObject-- if you are using an older version of the SDK, I would upgrade and try again.\n. @BharatMeda: as of the above commit, Metadata should now be properly filled. If you aren't seeing the headers in your getObject requests, it's possible they are not being properly set in your putObject-- if you are using an older version of the SDK, I would upgrade and try again.\n. The previous fix to this was a manual change to our local docs that were (at the time) hardcoded in the SDK. We've since moved to pulling docs directly from an upstream source, and it looks like the docs have not been fixed there. I will re-open this and make sure this is fixed upstream.\n. The previous fix to this was a manual change to our local docs that were (at the time) hardcoded in the SDK. We've since moved to pulling docs directly from an upstream source, and it looks like the docs have not been fixed there. I will re-open this and make sure this is fixed upstream.\n. I am going to close this since the docs have been updated to indicate that NextMarker may not always be present, in which case the last key should be used.\n. I am going to close this since the docs have been updated to indicate that NextMarker may not always be present, in which case the last key should be used.\n. @lifeofzero how are you specifying your region? And which region are you specifying? It looks like you might be specifying the region as 'standard', which you should not be doing; us-east-1 is the \"US Standard\" region (or when no region is specified, as you noted).\n. I'm not sure if you are still having this issue, but I am still unable to reproduce it. \"us-standard\" is not a named endpoint for Amazon S3. You can get a list of endpoint names from http://docs.aws.amazon.com/general/latest/gr/rande.html. If you are still getting this issue, please re-open again.\n. I cannot reproduce this on my install. I did notice that you are using node 0.6.x, which we do not explicitly support. Can you try upgrading to the latest version of node (currently 0.8.18, though anything in 0.8.x should work) and try this again?\nNote that you should not have to explicitly keep the process alive with a setInterval call, nor should you need to explicitly call process.exit. This works over here:\n``` js\nvar AWS = require('aws-sdk');\nvar S3 = new AWS.S3();\nS3.client.listBuckets(function(err, ans) {\n  if (err) throw err;\n  console.dir(ans);\n});\n```\n. I guess we can rule out 0.6.x. Can you tell more about your environment / setup? (Approximately) how many buckets do you have in S3? 0? a few? a lot?\nAlso, can you try this without the setTimeout and process.exit calls?\n. I've managed to reproduce the issue on a Ubuntu machine. I'm investigating the issue now.\n. Thanks @mlin. Since 0.6.x is the default package in Ubuntu, and since the only outlying issue (for now) was lack of Buffer.concat, we've decided to backport concat functionality into the SDK. If we do run into more compat issues like these we may have to re-evaluate our ability to support 0.6.x.\nAs for reproducing this in 0.8.x, I haven't been able to reproduce any issues on Ubuntu with 0.8.19. However, we've added guards in the http* events that should now properly propagate errors instead of hang, so any failures should be easier to debug. If you can reproduce your failures in 0.8.x, feel free to open a new issue.\n. Good catch, this issue is caused by the '' character not being properly escaped. Can you confirm this by doing something like: SELECT someAttribute FROM mysimpledomain?\n. Thanks for playing with SimpleDB in the master branch and reporting this before the release! This last commit should resolve '' characters not being encoded in URI input for SimpleDB and similar services.\n. Let's re-open this until we've accepted #60 (if we accept it). I'm noticing some issues with the implementation that I will be able to do a little bit more research on tomorrow.\n. Specifically I'm concerned about concatenating the args on a new arguments array for every emit call, but I will wait until I have a chance to benchmark the performance hit before I draw any conclusions. Also, the patch is missing tests, which would be helpful to keep us from a regression.\n. @gramakri, I've based the fix on your initial patch and added the attribution to the commit message along with a test case. Thanks for the bug report and pull request, don't be afraid to keep on helping us improve the SDK! \n. This is a valid reproduction, but it would be much more useful as an automated test inside our test suite. If you could update your pull request to include the tests that prove your implementation works, that would be helpful. Otherwise I can get around to adding tests for this if we merge it.\nNote that my concerns about creating a new arguments array still stands, though.\n. We're working to add full support for all AWS services into the SDK. Your request for CloudFormation and CloudFront is noted, unfortunately I can't give exact dates on when that will happen. I will note that I do actually have some CloudFront code kicking around in a local branch. I'll keep this ticket open in the meantime until we have them both in master.\n. Just FYI, I've added CloudFormation today in 673ddffa131c58df80e5586625becd8085e26296\nI will keep this open until CloudFront is also added.\n. CloudFront was added by a recent commit, so they are now both supported by the SDK. They will be available in the next release (we cannot provide a date for this).\n. Hey @ChristopherBiscardi, can you repush your commit without all the extra whitespace / formatting noise? It looks like your editor is reformatting a lot of code and it's making it to the patch.\n. In what scenario is null being passed to the sslEnabled option? This patch seems like it fixes a symptom, not the underlying problem. We should never be passing null to sslEnabled.\n. I'm not entirely sure that we should be translating nulls as something other than a falsey. These settings are documented as Boolean values, and as such, nobody should ever be passing in null. If that is happening, it would seem more consistent with JS to treat it as a falsey in Boolean terms (Boolean(VALUE)) rather than allow nulls. \nIf you want your options to ignore the sslEnabled value and rely on the default, you should either be deleting the property from the hash, or, if that's not acceptable, pass in undefined as a worst case. This seems much more inline with the documented values accepted by Boolean configuration options and doesn't introduce any extra magic values.\n. I'm going to go ahead and close this PR since the option is documented as a boolean value. Passing a null instead of a boolean could possibly hide underlying problems with the SDK or other tooling, and given that there are other ways to force the default value (pass undefined or delete the property), I think relying on those methods is better.\nThanks for the pull request though! We definitely appreciate all the help we can get.\n. Thanks for the report! Turns out we were only calculating this for deleteObjects, which was required since this operation does not have a parameter for setting the Content-MD5 header value. In all other cases, we were incorrectly documenting the ContentMD5 parameter as optional, but this was only because we were expecting to build the MD5 on your behalf if it was not provided. This functionality was not implemented, but I've now added it and the parameter should now reflect the documentation as optional.\n. @shargors Yes\n. It was just added by the above commit that closed this issue (5eb285f17d8698785013bc8ef10506d19f0b6d69), see above. This change will be available in the next release.\n. The input parameter you are looking for is \"SelectExpression\". The next release will include parameter validation, which will provide a much more helpful error message in these cases. You can already try it out by installing the package in the master branch right now.\n. This was due to a recent change in xml2js which we are currently looking into on Leonidas-from-XIV/node-xml2js#76\nUntil it is resolved you should lock into xml2js version 0.2.4 in your package.json or install our npm package from master:\nnpm install git://github.com/aws/aws-sdk-js\n. Closing as this was fixed upstream in xml2js\n. Thanks for reporting this. This is a known issue that we are working on a fix for. We will track it here.\n. Thanks for the report! I've just fixed the issue in master with the above commit, and it should now work as advertised.\n. I would avoid using toString() on your image data. You want the image to remain a Buffer object throughout your pipeline since binary data should always be stored in a Buffer / typed array format inside of JavaScript. If I remove the toString('binary'), your code works for me.\n. Unfortunately it's not possible to throw exceptions from asynchronous callbacks in JavaScript. We are aware of the possibility of losing errors from inside callbacks, and we were experimenting with wrapping the callbacks in try/catches that would log these lost exceptions to stderr. I think this would be useful for debugging purposes, but this error log will not stop program flow, so it too can get lost in your output. Also, a library like aws-sdk should probably not try to take over your stderr pipe, so we would at least have to make this toggleable.\nAs you allude to in #75, domains are one of node's proposed solution to this overarching technical limitation of the language. Domains are also one way we would like to support this kind of a problem, but the API is not stable enough for us to support fully yet-- more in that issue.\n. @cjhanks the latest commit (above) should make it so exceptions now properly propagate out from the callback. You can now also use domains to manage these exceptions (see #75).\n. Unfortunately this change had to be reverted due to #131, since this original change caused requests to be retried when errors were thrown from callbacks; not what you would want to happen. I'm currently looking into a way to get the best of both worlds.\n. @joonas-fi just to update-- this issue is fairly old. We actually did end up re-introducing logic to throw errors out of the callback, first in 3cdd424ce8509e76ccfcbd47ffffec4df040b56f (which is shown in this issue above) and then in a more elegant way with the release of v2.0.5 as @mhart pointed at in #307.\nIn short, if you're on the latest version of the SDK your errors should be propagating out of the callback. If this is still happening to you, can you verify and report the version of the SDK you are using?\n. @joonas-fi do you have an example of code that swallows an error?\n. Ah, I can explain this. Errors will propagate, except if it happens to be the same error object the SDK gave you in the callback. Not saying that's how it should work, just that it's how it works right now:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/request.js#L29\nFor what it's worth, that check does seem a little odd. I just ran our tests with that e !== err check removed and everything passes, so we might be able to just rip that out and have things work normally. I'll do some more tests first to make sure nothing is relying on that behavior.\nThat said, the following does throw, which you could use in the meantime:\njs\ns3.putObject(params, function(err) { throw new Error(err) });\n. @bigeasy Storing the \"last error\" on the global object could potentially cause a memory leak if it wasn't properly cleaned up-- especially if at some point in the future we start referencing the related request object information in the error. More importantly, it's likely to cause async-based race conditions if a user launches two requests at once, or even does something in the callback that causes another AWS error to throw.\nI think the right solution here is to just drop that e !== err check if we can. I'll have to investigate why that code was added in the first place, but it seems like we could refactor that logic out.\nThanks for your feedback!\n. @bigeasy there would certainly be an issue with memory leaks if the reference was not properly cleaned up. As for the race condition issue, perhaps we're using different terminology-- the issue here is that multiple sync / async operations writing to the same object can cause one of the callbacks to read the property too late. It depends on how this is all implemented, obviously, but the initially proposed solution above seems like it would be less error prone.\nCan you elaborate more on where exactly those two blocks would be in the SDK? Perhaps I'm not following on some of the details.\n. Opened issue #392 to track this, @mhart, @bigeasy, @joonas-fi. Thanks for reporting and all the suggestions, they're all very helpful!\n. The full stack trace would actually be more useful. Are you calling something explicit on the request object? Requests should no longer extend from node's EventEmitter, as we are moving to more of a state machine based approach that can support asynchronous event transitions (something EventEmitter does not support). \nUnfortunately this means that we currently do not support domains in the AWS.Request object. If you think it's useful to have this support, it's something we can look into adding, but without a stable domains API, it might not happen right away. Fortunately Node.js v0.10.0 bumped the API from experimental to unstable, so it looks like things are moving in the right direction for us to have a locked down API-- one that we would happily support.\n. @vthunder thanks for the report. This latest change should add (basic) support for the new (now unstable) domain API in Node. This API might change in the future, so our support may break-- it also may not yet be complete. Test it out and let us know how it works for you.\n. @vthunder thanks for the report. This latest change should add (basic) support for the new (now unstable) domain API in Node. This API might change in the future, so our support may break-- it also may not yet be complete. Test it out and let us know how it works for you.\n. We've already been testing locally on 0.10.0, just didn't get around to adding it to the Travis matrix, so thanks!\n. We've already been testing locally on 0.10.0, just didn't get around to adding it to the Travis matrix, so thanks!\n. Can you provide more information about this error? A backtrace, perhaps? I am running our integration tests that call receiveMessage and am unable to reproduce this.\n. Can you provide more information about this error? A backtrace, perhaps? I am running our integration tests that call receiveMessage and am unable to reproduce this.\n. @danmilon this is not the same issue-- it seems very specific to using the repl module where fs.createReadStream() does not want to emit 'data' events. Streaming doesn't seem to come into play-- at least from my experiments.\n. @danmilon this is not the same issue-- it seems very specific to using the repl module where fs.createReadStream() does not want to emit 'data' events. Streaming doesn't seem to come into play-- at least from my experiments.\n. Can you provide an example of the code you provided and the error message?  UserData should be a documented parameter of the runInstances method, see http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EC2/Client.html#runInstances-property\nIt must be base64 encoded as per the documentation on EC2 (http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-RunInstances.html), but it seems this information is omitted from our API docs. This can be corrected.\n. Can you provide an example of the code you provided and the error message?  UserData should be a documented parameter of the runInstances method, see http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EC2/Client.html#runInstances-property\nIt must be base64 encoded as per the documentation on EC2 (http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-RunInstances.html), but it seems this information is omitted from our API docs. This can be corrected.\n. FYI I've updated SDK documentation for the UserData parameter to notify users that the value should be Base64-encoded. This update will go out with our next release. Thanks for reporting this!\n. FYI I've updated SDK documentation for the UserData parameter to notify users that the value should be Base64-encoded. This update will go out with our next release. Thanks for reporting this!\n. Thanks for reporting this. We've corrected the formatting in the source, and the docs will be updated in our next release. Sorry for the confusion!\n. Thanks for reporting this. We've corrected the formatting in the source, and the docs will be updated in our next release. Sorry for the confusion!\n. Can you provide the code you are using to reproduce this issue. It sounds like this might be similar to issues we've been seeing in the Ruby SDK (aws/aws-sdk-ruby#133 and aws/aws-sdk-ruby#172).\nYou may also want to try the master branch with the change listed above. It may solve your issue, but there are edge cases where it will not (see the above Ruby issues for more details on those edge cases).\n. Can you provide the code you are using to reproduce this issue. It sounds like this might be similar to issues we've been seeing in the Ruby SDK (aws/aws-sdk-ruby#133 and aws/aws-sdk-ruby#172).\nYou may also want to try the master branch with the change listed above. It may solve your issue, but there are edge cases where it will not (see the above Ruby issues for more details on those edge cases).\n. > Need to change permissions?\nYes, AccessDenied is more than likely a permission issue, but I am not sure why.\nTry the following modification, which should also list the payload body:\n``` js\nvar AWS = require('aws-sdk');\nAWS.config.update({accessKeyId:'something-here', secretAccessKey:'another-here'});\nAWS.config.update({region:'eu-west-1'});\nvar s3 = new AWS.S3();\ns3.client.createBucket({Bucket: 'myBucket'}, function() {\n    var data = {Bucket: 'myBucket', Key: 'myKey', Body: 'Hello!'};\n    s3.client.putObject(data, function(err, data) {\n        if (err) {\n            console.log(this.httpResponse.body.toString()); // print response body\n            console.log(\"Error uploading data: \", err);\n        } else {\n            console.log(\"Successfully uploaded data to myBucket/myKey\");\n        }\n    });\n});\n```\n. > Need to change permissions?\nYes, AccessDenied is more than likely a permission issue, but I am not sure why.\nTry the following modification, which should also list the payload body:\n``` js\nvar AWS = require('aws-sdk');\nAWS.config.update({accessKeyId:'something-here', secretAccessKey:'another-here'});\nAWS.config.update({region:'eu-west-1'});\nvar s3 = new AWS.S3();\ns3.client.createBucket({Bucket: 'myBucket'}, function() {\n    var data = {Bucket: 'myBucket', Key: 'myKey', Body: 'Hello!'};\n    s3.client.putObject(data, function(err, data) {\n        if (err) {\n            console.log(this.httpResponse.body.toString()); // print response body\n            console.log(\"Error uploading data: \", err);\n        } else {\n            console.log(\"Successfully uploaded data to myBucket/myKey\");\n        }\n    });\n});\n```\n. @igorissen Are you sure you're using v1.3.2? The above syntax should not be valid in the latest version of the SDK. We removed the .done and .fail methods a while back.\n. @igorissen Are you sure you're using v1.3.2? The above syntax should not be valid in the latest version of the SDK. We removed the .done and .fail methods a while back.\n. @worp1900 sorry for the late response on this... if you are trying to access a bucket you have to make sure you are accessing it from the same region it was created in. This means that if you created the bucket in, for example, eu-west-1, you would need to specify this region to the S3 service object making the request. You cannot access the bucket from another region. \nDo you know which region the bucket was created in? Note that the PermanentRedirect error is a little unhelpful, because it tells you to access the endpoint but does not tell you the actual endpoint. The endpoint is buried in the body of the response, which you should be able to access by doing:\njs\ns3bucket.putObject(params, function(err, data) {\n  if (err) {\n    console.log(this.httpResponse.body.toString());\n  }\n});\nThe endpoint should be in the body of that raw XML string. This would tell you what endpoint the bucket was created in.\n. @worp1900 sorry for the late response on this... if you are trying to access a bucket you have to make sure you are accessing it from the same region it was created in. This means that if you created the bucket in, for example, eu-west-1, you would need to specify this region to the S3 service object making the request. You cannot access the bucket from another region. \nDo you know which region the bucket was created in? Note that the PermanentRedirect error is a little unhelpful, because it tells you to access the endpoint but does not tell you the actual endpoint. The endpoint is buried in the body of the response, which you should be able to access by doing:\njs\ns3bucket.putObject(params, function(err, data) {\n  if (err) {\n    console.log(this.httpResponse.body.toString());\n  }\n});\nThe endpoint should be in the body of that raw XML string. This would tell you what endpoint the bucket was created in.\n. Thanks for clarifying, and glad you got it working!\nI am going to close this issue, but if anyone else has any related issues please feel free to open a new issue that describes the problem with an example to reproduce the issue.\n. Thanks for clarifying, and glad you got it working!\nI am going to close this issue, but if anyone else has any related issues please feel free to open a new issue that describes the problem with an example to reproduce the issue.\n. Thanks for the pull req. See the line comments above for my reservations about this. If we can solve these two issues it would look at lot more likely to land.\nAlso, if you could add tests, that would also be helpful for getting this into the codebase.\n. Thanks for the pull req. See the line comments above for my reservations about this. If we can solve these two issues it would look at lot more likely to land.\nAlso, if you could add tests, that would also be helpful for getting this into the codebase.\n. > Note: While I have attached an additional test function to util.spec.coffee, I do not know how to run these tests and I have never used coffeescript, it is being submitted as a different commit.\nIf you npm install within the project you should get all the dependencies needed to run tests. npm test should run the tests, or npm run-script unit.\n. > Note: While I have attached an additional test function to util.spec.coffee, I do not know how to run these tests and I have never used coffeescript, it is being submitted as a different commit.\nIf you npm install within the project you should get all the dependencies needed to run tests. npm test should run the tests, or npm run-script unit.\n. Can you provide more information about the intermittent failures? It's hard to reproduce this specific issue. If you can reproduce this and print the results of\nconsole.log(this.httpResponse)\nand\nconsole.log(this.request.httpRequest)\nThat would be helpful.\nFor example:\njs\nec2.client.copyImage(params, function (err, data) {\n  if (err) {\n    console.log(\"Got error:\", err.message);\n    console.log(\"Request:\");\n    console.log(this.request.httpRequest);\n    console.log(\"Response:\");\n    console.log(this.httpResponse);\n  }\n  // ...\n});\n. Can you provide more information about the intermittent failures? It's hard to reproduce this specific issue. If you can reproduce this and print the results of\nconsole.log(this.httpResponse)\nand\nconsole.log(this.request.httpRequest)\nThat would be helpful.\nFor example:\njs\nec2.client.copyImage(params, function (err, data) {\n  if (err) {\n    console.log(\"Got error:\", err.message);\n    console.log(\"Request:\");\n    console.log(this.request.httpRequest);\n    console.log(\"Response:\");\n    console.log(this.httpResponse);\n  }\n  // ...\n});\n. It looks like the request is being sent to the us-east-1 endpoint, which from your description is not what you are trying to do. Are you providing the correct region to either the global config or EC2 object?\nIf so, perhaps this is an issue only on retries, which could explain the \"randomness\". Adding a console.log line for this.retryCount (or even just this) would help to show if this was triggered by a retry or not.\n. It looks like the request is being sent to the us-east-1 endpoint, which from your description is not what you are trying to do. Are you providing the correct region to either the global config or EC2 object?\nIf so, perhaps this is an issue only on retries, which could explain the \"randomness\". Adding a console.log line for this.retryCount (or even just this) would help to show if this was triggered by a retry or not.\n. Closed by #107\n. Closed by #107\n. Seems much simpler, thanks for the report!\n. Seems much simpler, thanks for the report!\n. Great catch! I'm wondering if maybe we should just solve this by picking a more appropriate name for our internal domain flag. _domain would solve this, wouldn't it?\nOtherwise, SWF might not raise an exception, but it will still not function with domain support. Seems like we'd just be obscuring that problem with this patch.\n. Great catch! I'm wondering if maybe we should just solve this by picking a more appropriate name for our internal domain flag. _domain would solve this, wouldn't it?\nOtherwise, SWF might not raise an exception, but it will still not function with domain support. Seems like we'd just be obscuring that problem with this patch.\n. Thanks! I just did a minor refactor since we don't need to require('domain') a whole bunch of times, same thing though.\n. Thanks! I just did a minor refactor since we don't need to require('domain') a whole bunch of times, same thing though.\n. See http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/SQS/Client.html#constructor-property for options for the SQS Client constructor. The Service class constructors (SQS, S3, etc.) just delegate the options hash to an SQS.Client constructor when creating the .client property.\n. See http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/SQS/Client.html#constructor-property for options for the SQS Client constructor. The Service class constructors (SQS, S3, etc.) just delegate the options hash to an SQS.Client constructor when creating the .client property.\n. Also note that the options themselves are typically just the set of options available in AWS.Config, so you can look there for available options that can be passed around to service constructors.\nI agree that we should be doing a better job of communicating this, though. We are working on making this better.\n. Also note that the options themselves are typically just the set of options available in AWS.Config, so you can look there for available options that can be passed around to service constructors.\nI agree that we should be doing a better job of communicating this, though. We are working on making this better.\n. Thanks, this makes much more sense. I'll work on some integration tests and merge this if everything looks good.\n. Thanks, this makes much more sense. I'll work on some integration tests and merge this if everything looks good.\n. I merged this via eb3b8eabf91cc4bd01f7e2323a39b5703aec76ff and then added some tests and fixed a missing output change to getBucketCors in the above commit. Sorry about taking so long for this, had to find some time to write tests.\nThank you for the contribution!\n. I merged this via eb3b8eabf91cc4bd01f7e2323a39b5703aec76ff and then added some tests and fixed a missing output change to getBucketCors in the above commit. Sorry about taking so long for this, had to find some time to write tests.\nThank you for the contribution!\n. Closed by #96. Note that there are still some cases where we have incorrect Attribute map names. I'm going to go through and clean this up. Thanks for the fix!\n. Closed by #96. Note that there are still some cases where we have incorrect Attribute map names. I'm going to go through and clean this up. Thanks for the fix!\n. An easier workaround for this would be to set the .length property on the stream that you created, for instance:\n``` js\ngm(request('http://www.some-domain.com/image.jpg'), 'image.jpg').stream(function(err, stdout, stderr) {\n  var data, request;\n// set stream length\n  stdout.length = SIZE_HERE;\ndata = {\n    Bucket: 'my-bucket',\n    Key: 'image.jpg',\n    Body: stdout,\n    ContentType: mime.lookup('image.jpg')\n  };\n  s3.client.putObject(data, function(err, res) {\n   console.log('done');\n  });\n});\n```\nThe above should work as advertised. It also avoids messing around with extra stream specific logic (something we unfortunately have to do for fs streams) and makes use of a fairly standard JS convention.\nThough I do agree exposing ContentLength in S3 would be useful, but note that if it were added it would be available on a per-service basis only.\n. An easier workaround for this would be to set the .length property on the stream that you created, for instance:\n``` js\ngm(request('http://www.some-domain.com/image.jpg'), 'image.jpg').stream(function(err, stdout, stderr) {\n  var data, request;\n// set stream length\n  stdout.length = SIZE_HERE;\ndata = {\n    Bucket: 'my-bucket',\n    Key: 'image.jpg',\n    Body: stdout,\n    ContentType: mime.lookup('image.jpg')\n  };\n  s3.client.putObject(data, function(err, res) {\n   console.log('done');\n  });\n});\n```\nThe above should work as advertised. It also avoids messing around with extra stream specific logic (something we unfortunately have to do for fs streams) and makes use of a fairly standard JS convention.\nThough I do agree exposing ContentLength in S3 would be useful, but note that if it were added it would be available on a per-service basis only.\n. I'm not sure how gm() works, but you're likely going to have to ask the gm stream, not the request stream, how many bytes it will be generating. Even if request() gave you a number, it won't be the same number that your manipulated image is. It may not even be possible to get the size via streams with that library, in which case you would probably want to write to disk first (or an in-memory buffer) and then stream from there. I don't know for sure though; it would be best to check the docs linked from https://npmjs.org/package/gm\nI'm going to close this since it's specific to a third party library. S3 requires a Content-Length to be provided on all payload requests, so this is something that must be supported by whatever third party library you use.\n. I'm not sure how gm() works, but you're likely going to have to ask the gm stream, not the request stream, how many bytes it will be generating. Even if request() gave you a number, it won't be the same number that your manipulated image is. It may not even be possible to get the size via streams with that library, in which case you would probably want to write to disk first (or an in-memory buffer) and then stream from there. I don't know for sure though; it would be best to check the docs linked from https://npmjs.org/package/gm\nI'm going to close this since it's specific to a third party library. S3 requires a Content-Length to be provided on all payload requests, so this is something that must be supported by whatever third party library you use.\n. FWIW I see a \"filesize()\" property in the gm docs that might allow you to get this value: http://aheckmann.github.io/gm/docs.html#getters\n. FWIW I see a \"filesize()\" property in the gm docs that might allow you to get this value: http://aheckmann.github.io/gm/docs.html#getters\n. @terribleplan the SDK supports ReadableStream per the docs, and you do not need to specify a length with the stream. The issue is that the underlying service (S3) needs to know how many bytes are in your PUT request. Checking the .length property on the stream is just a convenience (since it is implemented in readable streams returned by the fs module)-- you can alternatively provide a regular vanilla stream and pass the byte length as the ContentLength parameter to the putObject operation. \nThere is not much the SDK can do about this limitation in S3, as needing to know the size of the payload is a requirement of the service. If the only way you can determine the size is to load large amounts of data into memory, this is not something the SDK could do much about-- again, the restriction comes from the service-- though I would strongly recommend buffering in chunks if you can so as to not load all data into memory at once (potentially writing out to disk for larger files if you're doing some kind of transform on the stream).\nThat said, thanks for the feedback. I agree that better documentation about S3's limitation could be useful here, I will add a note to look into making this limitation more explicit. I would also recommend visiting S3's forums to put in a feature request to remove the Content-Length restriction. The ability to support streaming payloads would be a huge benefit not just for the JS SDK, but other tools as well. Let them know you believe this would be an important improvement to the service!\n. @terribleplan the SDK supports ReadableStream per the docs, and you do not need to specify a length with the stream. The issue is that the underlying service (S3) needs to know how many bytes are in your PUT request. Checking the .length property on the stream is just a convenience (since it is implemented in readable streams returned by the fs module)-- you can alternatively provide a regular vanilla stream and pass the byte length as the ContentLength parameter to the putObject operation. \nThere is not much the SDK can do about this limitation in S3, as needing to know the size of the payload is a requirement of the service. If the only way you can determine the size is to load large amounts of data into memory, this is not something the SDK could do much about-- again, the restriction comes from the service-- though I would strongly recommend buffering in chunks if you can so as to not load all data into memory at once (potentially writing out to disk for larger files if you're doing some kind of transform on the stream).\nThat said, thanks for the feedback. I agree that better documentation about S3's limitation could be useful here, I will add a note to look into making this limitation more explicit. I would also recommend visiting S3's forums to put in a feature request to remove the Content-Length restriction. The ability to support streaming payloads would be a huge benefit not just for the JS SDK, but other tools as well. Let them know you believe this would be an important improvement to the service!\n. @nmccready you still don't need to use pipe, simply pass the stream object as the body parameter of the upload() call as shown in the doc examples. The stream object depends on the library you are using, but it's typically the thing you are trying to call .pipe on.\n. @nmccready you still don't need to use pipe, simply pass the stream object as the body parameter of the upload() call as shown in the doc examples. The stream object depends on the library you are using, but it's typically the thing you are trying to call .pipe on.\n. This is most likely a duplicate of #87. Can you try the SDK off of the master branch (npm install git://github.com/aws/aws-sdk-js) and see if that works for you?\n. I'm unable to reproduce this with err={}, but I was able to see an issue with parameter validation of unix timestamps. A quick workaround is to toggle paramValidation off:\njs\nvar swf = new AWS.SimpleWorkflow({paramValidation: false});\nOr you can use the master branch which contains a fix that I just pushed for this issue above. To install the npm from master, run:\nnpm install git://github.com/aws/aws-js-sdk\nThis fix will be included in the next release of the SDK.\n. @trevorrowe just pushed a related fix in #99 that allows you to pass a Date object directly as a parameter to timestamp fields in JSON based services: e579b41cc56e1cb4404b8364df7a6360d71a9296. This will also be in the next release, and means you won't have to worry about doing this conversion yourself. You could just pass endDate directly.\n. @whockey can you list a minimal case that shows retryCount not exceeding 1?\n. I'm going to close this because I cannot reproduce. Please feel free to re-open if you can provide more information that reproduces the issue. Thanks!\n. There should be support for this feature in S3.putBucketRequestPayment. Is this what you are looking for, or is it not working for you?\n. You are correct that getObject has no parameter to specify the 'x-amz-request-payer' header. That's something that can be easily added (I was writing this as you just did). In the meantime, you can also workaround this by adding the header to the request yourself:\njs\nvar req = s3.getObject(params);\nreq.httpRequest.headers['x-amz-request-payer'] = 'requester';\nreq.on('complete', function (resp) { console.log(resp.data) });\nreq.send();\n. Thanks for the pull request, but these examples are correct with respect to the latest version in master. We will be making a change in the next release that no longer relies on the client or Client properties. See 48ed485fa30a14e4d19ff975ce8f4ad17ae4dfca for the change.\n. I saw this post on the forums last night and I was getting around to responding to it. I will respond to it there since that is a better venue for these kinds of questions.\n. Unfortunately, the 2012-08-10 API of DynamoDB is not backwards-compatible with the older 2011-12-05 version. You can see the documentation for the current Query command in the DynamoDB documentation. You'll note that HashKeyValue is no longer in the API. We've also written a simple migrating guide in our release notes that covers some of the basic parameter changes.\nYou can either lock into the old DynamoDB API or migrate your query call to use the new API.\nTo lock into the old API, construct your DynamoDB object with:\njs\nvar db = new AWS.DynamoDB({apiVersion: '2011-12-05'});\nTo migrate your query you can change it to the following (note the YourPrimaryKeyId key):\njs\nvar params = {\n  TableName: table,\n  KeyConditions:{\n    'YourPrimaryKeyId': {\n      AttributeValueList: [{S: id}],\n      ComparisonOperator:'EQ'\n    }\n  }\n};\ndb.query(params, callback);\nThe 'YourPrimaryKeyId' value should be replaced with your table's actual hash key. Note that the new query interface is a lot more powerful, so I would suggest looking at the new documentation to see what new functionality you can take advantage of in the API.\n. Thanks for reporting the omission. It looks like the EC2 service documentation did indeed go missing somehow. I just pushed an update of the docs and you should now be able to find the EC2 documentation here. \nNote that we merged the Client and Service classes in our last release, so that specific URL you provided is no longer valid. Can you tell me where you got that URL from so we can track down these broken links? Thanks!\n. Make sure to refresh your browser cache (either holding Ctrl or Shift when refreshing, or restart your browser). I've verified the page on a handful of browsers, and I did have the same issue you described in Chrome until I restarted it. Did that help?\n. @ithkuil for reference, we have release notes that explain the changes made to Client and Service objects and how to migrate to the latest version. Even though the changes are backwards compatible, I recommend migrating your code to avoid more problems if we ever remove these deprecated features.\n. Please see our latest release notes for changes regarding the deprecated client property.\n. Thanks Carl!\n. Thank you for taking a deep look into this issue and fixing it. Your contribution is appreciated!\n. If your proxy being served over HTTP and not HTTPS, this might cause issues with signing. A quick way to verify would be to use sslEnabled: false or use your proxy over HTTPS.\nCan you confirm whether this is the issue?\n. @OferE what proxy software are you using?\n. I did some more digging into this and I am unable to reproduce this using tinyproxy with sslEnabled: false. Note that our implementation does not (yet) support CONNECT tunneling, so if the proxy server requires the use of CONNECT, the SDK will not be able to handle that (again: yet). There are other workarounds for CONNECT proxies, in the meantime, like using a node-tunnel agent, which can be done by constructing a tunnelingAgent object and specifying:\njs\nAWS.config.httpOptions = { agent: tunnelingAgent };\nOn another note, you mentioned you tried this in other proxies-- can you specify which so that I can try these too?\n. @bindugh the SDK uses SSL connections by default, which means you would have to use an HTTPS over HTTP tunnel. Have you tried tunnel.httpsOverHttp? You can also use new AWS.EC2({sslEnabled: false}) to disable SSL, but that is not recommended, in general. \n. @wabmca I'm not sure what the https_proxy or proxy environment variables that you are exporting are. Can you explain what environment / libraries you are using? Have you tried using the node-tunnel library mentioned above?\n. I'm going to mark this as a feature request, since the SDK is behaving as is but does not (currently) support CONNECT proxies. \nUnfortunately there are no current plans to add CONNECT proxy support to the SDK. I'm going to leave this feature request open for a bit in case anyone wants to pick this up with a pull request, but I think proxy support should be implemented by Node itself, not the SDK. It seems out of scope to implement the CONNECT proxy protocol and deal with all of the potential pitfalls of implementations that may not be to-spec. There's a great opportunity to build a generic HTTP proxy Node.js library here, and if there are extra things the SDK could do to tie into these HTTP proxies that would be something we could do, but I think this code would belong outside of the SDK itself.\n. This looks like the correct interpretation to me. Thanks for fixing this up with a test!\n. Hi @lancecarlson, you can install the npm from github in the meantime:\nnpm install git://github.com/aws/aws-js-sdk\n. Is there something incorrect about the current behavior, or is this just a refactor suggestion?\n. I'm going to close this since it does not provide any new functionality. Thanks for the helpful refactor though!\n. I don't think we will be accepting this patch, but I do believe there should be a way to get presigned URLs in the SDK. This code is causing build failures (please make sure to run tests before submitting pull requests) and has no extra tests, which a feature like this would definitely need. I'm also concerned that there seems to be a lot of unnecessary duplication in your patch that we probably don't want to have in the final implementation.\nI'm going to keep this issue open though-- I will mark this as a feature request and will close it when we have presigned URL support implemented.\n. I'm going to close this in favour of #115, which should implement pre-signed URLs for any operation, not just getObject. Thanks for contributing a patch though!\n. Are you testing this on the latest version of the SDK (v1.1.0)? There was a recent thread on the AWS developer forums about a related issue, but this was solved by the 1.1.0 release.\n. Amazon ElastiCache is protocol-compliant with Memcached, so once you have an endpoint to connect to, you can use any memcached library to add/retrieve cache items. In node, you can use something like the memcached module for this. There is some user documentation on connecting to a cluster in the ElastiCache user guide which may provide some extra examples for you.\nBasically, you only need to use the SDK if you are looking to automate provisioning of cache clusters. If you already have the clusters setup, you can use memcache compatible libraries directly. Hope that helps.\n. @ithkuil have you tried with just maxSockets? Rejecting unauthorized certificates is a big part of guaranteeing security when using SSL.\n. There are a couple of ways you can do this already without directly modifying the NodeHttpClient.sslAgent.\nFirst, you can pass your own agent to httpOptions:\n``` js\nvar https = require('https');\nvar agent = new https.Agent();\nagent.maxSockets = 500;\nagent.rejectUnauthorized = true;\nAWS.config.httpOptions = {agent: agent};\n```\nAlternatively, you can configure the global HTTPS agent to accept more concurrent connections and tell the SDK to use that:\njs\nvar https = require('https');\nhttps.globalAgent.maxSockets = 500;\nhttps.globalAgent.rejectUnauthorized = true;\nAWS.config.httpOptions = {agent: https.globalAgent};\nWe don't use the globalAgent by default because in 0.8.x, the rejectUnauthorized property on that agent instance is set to false, which is not a secure setting. Node 0.10.x changes this, so in the future we may begin to use the globalAgent, and this would mean doing everything in the above except for having to set the httpOptions.\nDo these options work for you?\n. Thanks for the report @gjersvik!\n. This was actually a documentation issue-- an unrelated issue cropped up with indentation and fixing our docs corrected the CidrIp to be a child of IpRanges: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EC2_20130201.html#authorizeSecurityGroupIngress-property\nDoes this solve the issue?\n. I am going to close this as resolved, but feel free to open if you notice any other similar documentation bugs.\n. In supporting pre-signed URLs we made a change to the signer that will sign the \"Expires\" parameter in place of the Date header, since it has a separate meaning in getSignedUrl(). The problem is that Expires as a parameter exists in other operations like putObject, which will cause our signer to try to incorrectly use that as the date. I've just pushed a fix for this above-- if you want to use it you can run:\nnpm install git://github.com/aws/aws-sdk-js\nWe will be doing a maintenance release to fix this issue in the npm package soon.\nThanks for reporting this issue!\n. You'll get a much better idea of where the error is coming from if you print err.stack instead of just err. What does that print?\n. Okay, I will close this. Let us know if you run into any other issues.\n. Thanks for the fix!\n. The SDK actually supports the very latest version of S3, and we often stay lockstep with any new service features on this service, as well as others. The latest version of the S3 API is in fact 2006-03-01, and this is the same API version used by all SDKs, as well as the API that is in the S3 documentation. \nThe reason that it is still marked as '2006-03-01' is because S3 has not made any changes in its 7 years to require a major version bump; every addition has been backwards compatible. In other words, the API version represents a major version of the API, not a timestamp of the last service update. You will see this same versioning scheme used in other AWS services. Typically, if a service makes a service addition that is backwards compatible, it will not require a change to the API version. Conversely, changing the API version usually implies a backwards incompatible change in the service, which is why we expose the ability to lock into specific API versions in the SDK.\nHope that helps!\n. No problem! :)\n. The AWS SDK for Node.js already supports the updateDistribution operation to update distribution configuration. Is this what you are looking for?\n. The updateStreamingDistribution method also exists, if that's what you are looking for. \n. The SDK documentation is not clear here. It's a little more clear on the ELB documentation for RegisterInstancesWithLoadBalancer. Basically, the array should be a list of structures with the InstanceId field set.\nTry:\njs\nparams = {\n  LoadBalancerName: lbName,\n  Instances: [{InstanceId: instanceId}]\n};\nDoes this work?\nI will look into seeing if there is a way we can make this more clear in our SDK documentation.\n. Noted. We will take this feedback and see what we can do about making these scenarios more clear. I think examples usually help to clarify these scenarios. Do you think that soliciting examples from the community would a good route here?\nI will go ahead and close this ticket since you seem to have it working. Don't be shy to raise more issues if things dont \"just work\" in the SDK!\n. I'm all for this. The Ruby SDK uses this file to leverage the endpoints API; we probably want something like that as well. Pulling it into AWS.endpoints might be a good first step though.\n. Sorry about the delay in responding to this issue!\nGiven the simple workaround above we've decided that we're going to leave this out of the SDK for now. Creating a third-party plugin via an external npm package might be the best way to support this in the community if users are still looking for this behavior. We'd be happy to see it turned into a third-party npm package. If it was popular enough, we could look at merging it in at that point.\n. @fastman can you try v1.3.2 released last night and see if this resolves the issue? We had a few open issues in #131 and #132 that seem related and were fixed.\n. This seems to be an issue with Node.js v0.8.x. I am not reproducing this in v0.10 and onwards. I will look into this issue and see if it is something that we can support or if this was a bug in v0.8.x of Node.\n. It looks like in Node v0.8, the 'timeout' event can get emitted multiple times in a timeout condition. The above commit simply ensures we only register for this event once. This fix will be available in the next release of the SDK. \nThanks for reporting this!\n. @psugihara a \"ReferenceError\" means that something went wrong in a bad way. Can you provide a stack trace?\nIn general, printing the err.stack is much more valuable than just the err object itself. In both of these cases, err.stack would be valuable to debugging this.\n. @kixorz can you provide the version of the SDK you are using?\n. Does this happen when you remove the request.send() line? Calling the describeInstances with a callback will automatically send the request.\n. Constructing the service object via new AWS.EC2() does not create a request object, it creates a service object. The semantics of callbacks and send() are documented in our Getting Started Guide. \nYou are free to exclusively use send(), but you should not mix the simplified callback interface and send() together. This behavior is unlikely to change as we move to V2. Note also that send() now takes an optional callback, so you could use send() via the following syntax, if you prefer it:\njs\nec2.describeInstances({...}).send(function(err, data) {\n  // handle response\n});\n. > If there is any exception/error in my code then I would like to handle it. But this doesn't happen in this particular case.\nYou should try/catch inside of your query callback and handle any exceptions there. \nI believe that we are retrying failures by calling the callback chain with the error you raised, which, when raised from the callback, seems to cause an infinite loop. At first glance, I would call this a bug, but if we do not re-raise the error it would get lost (callbacks can not throw errors up the call stack). Domains might be the right answer here, but they are still pretty new in node, so this isn't an answer for everybody.\nIn other words, throwing errors from your query callback should either:\n- Be dispatched to a domain that you have registered, or,\n- Get silently ignored, because we cannot re-throw an error to the callback that threw it,\n- Have its own try/catch block to catch and handle these errors so they don't get lost.\nYou probably don't want the latter, which means you either want to use domains or surround the body of your callback with try catch to do your handling there.\n. > So if there is an exception in my error callback , still its going to call it again and again?\nThis is the bug. We shouldn't be doing this. These exceptions will get lost, though-- there is not much else we can do (other than domain support, which should work, but the same problem applies there).\n. This was merged into master and was pushed out with the v1.3.2 release last night. Give that a shot and see if this fixes the issue.\n. No, it's only calling the callback a second time. This issue was reported a few days ago as #131.\n. I have a working implementation of this feature as AWS.Request.abort(). Providing access to the low level HTTP stream and allowing users to abort directly on that would not be sufficient, since that would actually trigger a retry and cause the error to propagate to your callback multiple times, which is probably not what you want. A full abort implementation has to also manage the event handlers registered to the request cycle and cancel those too. I will be pushing my implementation of this shortly.\n. If you npm install git://github.com/aws/aws-sdk-js you should be able to test out this feature. Sample usage would be:\n``` js\nvar s3 = new AWS.S3();\nvar params = {\n  Bucket: 'bucket', Key: 'key',\n  Body: new Buffer(1024 * 1024 * 5) // 5MB payload\n};\nvar request = s3.putObject(params);\nrequest.send(function (err, data) {\n  if (err) console.log(\"Error:\", err.code, err.message);\n  else console.log(data);\n});\n// abort request in 1 second\nsetTimeout(request.abort.bind(request), 1000);\n// prints \"Error: RequestAbortedError Request aborted by user\"\n```\n. Good question. We release fairly often, and this feature will be available in the next release, but I can't give an exact date on when this will happen. Using the above npm command should work until then.\n. I'll mark this as a feature suggestion for supporting the document service portion of Amazon CloudSearch's API.\n. The above release should now have support for search/upload in the AWS.CloudSearchDomain service object.\n. This is an interesting error. I'm having trouble reproducing it here locally, so I'm not sure exactly what is happening, but I am able to reproduce the fact that error events don't seem to be sent from the low level http stream (from http.request). If this was happening, your failing downloads would show up with error logging. \nNote that fixing the error event here won't stop your downloads from erroring out, it will just make the SDK more helpful when informing you that one of the downloads failed. Running parallel downloads will increase the chance of socket failures, which is why you're seeing it on async.each but not eachSeries.\nAs for fixing the error handling portion of this issue, I'm still investigating why the error is not coming out of the low level http.request stream. If I cut my network out mid-stream, I do not get an 'error' event from the request, which seems like a Node.js issue (same behavior on 0.8.x, 0.10.x and 0.11.x).\n. It does seem to be an issue with Node.js error handling in http.request streams. I ran the following code (modified from a nodejitsu tutorial on http.request):\n``` js\nvar http = require('http');\nvar options = {\n  host: 's3.amazonaws.com',\n  path: '/mybucket/5d63d1c77edbee73aa59dfbf74ab2dfea70dcd5f/X_0.csv.gz'\n};\ncallback = function(response) {\n  var str = '';\n//another chunk of data has been recieved, so append it to str\n  response.on('data', function (chunk) {\n    console.log(\"Got\", chunk);\n  });\n//the whole response has been recieved, so we just print it out here\n  response.on('end', function () {\n    console.log(\"Done\");\n  });\n//this should not actually fire, we just register it in case Node has\n  //undocumented error handling behavior.\n  response.on('error', function(err) {\n    console.log(\"Got error from resp\", err);\n  });\n}\nvar req = http.request(options, callback);\n// if we cut the connection, we should see this message\nreq.on('error', function(err) {\n  console.log(\"Got error from req\", err);\n});\nreq.end();\n```\nNeither error event propagated from the stream if I cut the connection (\"pulled the cable\") midway through. The error did display if I had no connectivity when the initial request was attempted, though. It seems like Node will not raise an error if your socket gets cut in the middle of a connection, which seems contrary to the documentation, and fairly odd.\n. What happens if you make the following adjustment to your script?\n``` js\nvar req = s3.getObject({Bucket:'mcmcResults', Key:filename});\nreq.on('send', function(resp) {\n  resp.request.httpRequest.stream.on('close', function() {\n    cb(new Error(\"Connection closed\"));\n  });\n});\nreq.createReadStream()\n    .pipe(zlib.createUnzip())\n    .pipe(writeStream);\n//...\n```\n. I'm having trouble reproducing your failure. Both your AWS and knox examples are working for me (*), but I don't doubt that they are failing for you, so I will keep trying to look into this-- it just may take a little more time. Is there anything else about your environment that might be relevant to reproducing this? What OS are you on? What is your location relative to the bucket location (is it in a relatively close region)? I've tried running this 10+ times on both Windows and OSX with Node v0.10.13.\n(*) I did manage to reproduce the assertion error once, but I was able to do so for both aws-sdk and knox by hammering S3 with requests (running multiple process of the above scripts in parallel across 5-6 processes). So when I was able to reproduce it, the aws-sdk was not alone in getting truncated data. Note that this is what I would expect to see when S3 cuts off the connection due to throttling or various socket congestion issues. Perhaps you're just hitting this scenario more often, though I'm surprised you haven't seen it at all with knox. What happens if you kick off 4x the requests, for instance (run the outer loop 4 times) using the s3knox.js script? Or just run it in parallel with a bunch of processes:\nfor x in 1 2 3 4 5 6 7 8 9 10 11; do node s3knox.js & done\nThat reliably fails the knox script for me. What about for you?\n. @justincy this does not sound like the same problem. This issue is specific to parallel downloads, not series (see discussion above). I would suggest opening a separate issue with more details and logs (including errors). Also please make sure to print error.stack in addition to just the error object when listing errors. Thanks.\n. @extrabacon can you provide some sample code that reproduces your issue with gunzip? I'm not exactly sure how you are piping your data through, so I can't reproduce this. Awesome username by the way!\n. @mike-spainhower why would you need intimidate? The SDK supports retry logic and exponential backoff built-in. It's one of our core features. S3's maxRetries is set to 3 by default, but if you're dealing with a lot of date, you can bump it up to whatever is reasonable.\n. @sposmen the pipe solution provided by @extrabacon above should work. Did it not work for you? If so, can you provide an error associated with the request?\n. @extrabacon @sposmen @abtris and even @andrewrk (since you do great work on node-s3-client), I've just pushed a very rough work in progress branch on some of the things we've been doing to improve the S3 interface in the SDK. You can find it in the s3-managed-upload branch and usage info in this commit: afbb9df4766105aba4db497c9ea0f1fe3395076c (using the scripts to build the browser version doesn't yet work though it does support browser uploads).\nI figured it would be interesting to look at since we support uploading arbitrarily sized streams (like gzipped files or data manipulated with image processing libraries etc.) using multiparts as well as uploading these parts concurrently.\nI'll be doing some refactoring and implementing tests over the next weeks, so reminder that this is rough, but take a look and see what you think.\n. @andrewrk looks fairly similar, though we don't buffer to temporary files (but we could support that).\n. Good catch @djechlin. This looks like an easy fix, I will take a look at getting this out for the next release.\n. Actually, are you sure you have a sessionToken in your configured credentials? When I configure a sessionToken manually in my AWS.config, the getSignedUrl method is properly adding the parameter:\njs\naws-sdk> AWS.config.credentials.sessionToken = 'SESSION'\naws-sdk> s3 = new AWS.S3();\naws-sdk> s3.getSignedUrl('getObject', {Bucket:'mybucket',Key:'foo'})\n'https://mybucket.s3.amazonaws.com/foo?AWSAccessKeyId=...&X-Amz-Security-Token=SESSION'\n. What happens if you use AWS.config.getCredentials before getting the URL? Like so:\njs\nAWS.config.getCredentials(function(err, creds) {\n  var s3 = new AWS.S3({credentials: creds});\n  s3.getSignedUrl(...);\n});\n. Oops, fixed the missing \"new\". \nThe reason you need to create a new object is because we only seed the credentials in AWS.config into the service object on construction. If you modify AWS.config's credentials after constructing the S3 object, the object will not see those changed credentials. It keeps a separate copy.\nOn that note, you should be able to drop the {credentials: creds} portion of my example, since the getCredentials callback will have already updated the AWS.config.credentials that the new AWS.S3 is sourcing from.\n. The callback form should fetch credentials. That's one of the reasons that the form exists. Is that not happening? ie., if you do,\njs\ns3.getSignedUrl(..., function (err, url) {\n  console.log(url);\n});\n. I should clarify here: the callback form fetches credentials from any of the builtin credential providers, which are:\n- Statically defined credentials (AWS.config.credentials)\n- Your environment (process.env)\n- EC2 IAM roles, if running on an EC2 instance\nThe default providers don't handle credentials loaded from STS. How are you loading your credentials from that?\n. @djechlin can you provide code that reproduces the issue for you, including how you are configuring your credentials from STS?\n. I cannot reproduce the error when running your gist. This is what I get when I run it on an EC2 instance with a configured IAM role:\n```\n[ec2-user@ip-*** 6057364]$ node s3\nBroke: https://s3.amazonaws.com/unrollme_node_unittest/item.txt?AWSAccessKeyId=ASIAIDPEL7NW75WG43KQ&Expires=1374527426&Signature=Q%2BGn1HfcRhv4TXHig87VRLbRy6s%3D&X-Amz-Security-Token=AQoDYXdzEK3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaoAIxMMds2cTs9Wvv4zTd%2FMnpX8SgxbJ21Kr7YFD1yfI%2Fof8Mt81nOOa3PAuYwTo3Rzv8PjcO5szMgvNqKNEiiv2vuZDOksXBfTdPXgXaMtqqA%2Bsh5BeOoLM8lYMDAu6gnYfjtdo4%2FuuhGs7mAz0nmj5coiZGPZtp1lzhzsouiqQhoLDyh5VgVmAiiEYc%2FHgsnW88GD60vB%2FMJD927%2FN6XN9adTSQ%2BvAfDP0I3R4ak%2F63uDhyJtctGY856gXcBmdFxZ2lrwJ1ATNuuQGH5ntoNw3seFk56nTU9ULg%2Bt%2BTnFFTk%2BLfKX%2Fqjie9J%2F0rGGdkgpeNPU6w9jnvNPqQuJfiuAAFq64BwfuKxPFNrPxt0V9leYaWbz%2BFXng%2BtRwTghImu6Ag%2FqS2jwU%3D\nFixed: https://s3.amazonaws.com/unrollme_node_unittest/item.txt?AWSAccessKeyId=ASIAIDPEL7NW75WG43KQ&Expires=1374527426&Signature=Q%2BGn1HfcRhv4TXHig87VRLbRy6s%3D&X-Amz-Security-Token=AQoDYXdzEK3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaoAIxMMds2cTs9Wvv4zTd%2FMnpX8SgxbJ21Kr7YFD1yfI%2Fof8Mt81nOOa3PAuYwTo3Rzv8PjcO5szMgvNqKNEiiv2vuZDOksXBfTdPXgXaMtqqA%2Bsh5BeOoLM8lYMDAu6gnYfjtdo4%2FuuhGs7mAz0nmj5coiZGPZtp1lzhzsouiqQhoLDyh5VgVmAiiEYc%2FHgsnW88GD60vB%2FMJD927%2FN6XN9adTSQ%2BvAfDP0I3R4ak%2F63uDhyJtctGY856gXcBmdFxZ2lrwJ1ATNuuQGH5ntoNw3seFk56nTU9ULg%2Bt%2BTnFFTk%2BLfKX%2Fqjie9J%2F0rGGdkgpeNPU6w9jnvNPqQuJfiuAAFq64BwfuKxPFNrPxt0V9leYaWbz%2BFXng%2BtRwTghImu6Ag%2FqS2jwU%3D&x-amz-security-token=AQoDYXdzEK3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaoAIxMMds2cTs9Wvv4zTd%2FMnpX8SgxbJ21Kr7YFD1yfI%2Fof8Mt81nOOa3PAuYwTo3Rzv8PjcO5szMgvNqKNEiiv2vuZDOksXBfTdPXgXaMtqqA%2Bsh5BeOoLM8lYMDAu6gnYfjtdo4%2FuuhGs7mAz0nmj5coiZGPZtp1lzhzsouiqQhoLDyh5VgVmAiiEYc%2FHgsnW88GD60vB%2FMJD927%2FN6XN9adTSQ%2BvAfDP0I3R4ak%2F63uDhyJtctGY856gXcBmdFxZ2lrwJ1ATNuuQGH5ntoNw3seFk56nTU9ULg%2Bt%2BTnFFTk%2BLfKX%2Fqjie9J%2F0rGGdkgpeNPU6w9jnvNPqQuJfiuAAFq64BwfuKxPFNrPxt0V9leYaWbz%2BFXng%2BtRwTghImu6Ag%2FqS2jwU%3D\n```\nNote that X-Amz-Security-Token is in the first URL.\nI've simplified the script to the following, which I am running on the same instance:\n``` js\nvar aws = require('aws-sdk');\nvar s3 = new aws.S3();\nvar params = {Bucket: 'foo', Key: 'bar'};\ns3.getSignedUrl('getObject', params, function (err, url) {\n  console.log(err, url);\n});\n```\nI get:\n[ec2-user@ip-*** ~]$ node test\nnull 'https://foo.s3.amazonaws.com/bar?AWSAccessKeyId=ASIAIDPEL7NW75WG43KQ&Expires=1374527180&Signature=qXIsN4%2Bbi9UxVVWjmA2Gq7a0zWY%3D&X-Amz-Security-Token=AQoDYXdzEK3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaoAIxMMds2cTs9Wvv4zTd%2FMnpX8SgxbJ21Kr7YFD1yfI%2Fof8Mt81nOOa3PAuYwTo3Rzv8PjcO5szMgvNqKNEiiv2vuZDOksXBfTdPXgXaMtqqA%2Bsh5BeOoLM8lYMDAu6gnYfjtdo4%2FuuhGs7mAz0nmj5coiZGPZtp1lzhzsouiqQhoLDyh5VgVmAiiEYc%2FHgsnW88GD60vB%2FMJD927%2FN6XN9adTSQ%2BvAfDP0I3R4ak%2F63uDhyJtctGY856gXcBmdFxZ2lrwJ1ATNuuQGH5ntoNw3seFk56nTU9ULg%2Bt%2BTnFFTk%2BLfKX%2Fqjie9J%2F0rGGdkgpeNPU6w9jnvNPqQuJfiuAAFq64BwfuKxPFNrPxt0V9leYaWbz%2BFXng%2BtRwTghImu6Ag%2FqS2jwU%3D'\nNote that the X-Amz-Security-Token is being set here too.\n. In that case, this is a much easier fix-- just a matter of making sure the x-amz-security-token is lower cased.\n. @djechlin run npm install git://github.com/aws/aws-sdk-js and let me know if that resolves the issue for you.\n. In what situation is Expires failing validation? We have various unit and integration tests for this with the Expires param that are currently passing. Can you provide some code that reproduces a validation error?\n. In what situation is Expires failing validation? We have various unit and integration tests for this with the Expires param that are currently passing. Can you provide some code that reproduces a validation error?\n. What is the contents of sign-fail.js?\n. What is the contents of sign-fail.js?\n. I am able to reproduce with your code.\nThis seems to be related to the use of bound parameters on the service object constructor. A temporary workaround would be to just avoid bound params when using presigned URLs. The following should work:\njs\nvar AWS = require('aws-sdk');\nAWS.config.loadFromPath(process.env.HOME + '/.aws-sdk-credentials.json');\nvar s3 = new AWS.S3();\nvar params = {Expires: 5000, Bucket: '...', Key: '...'};\ns3.getSignedUrl('getObject', params, function(err, url) {\n  if (err) { console.error(err); throw err; }\n});\n. I am able to reproduce with your code.\nThis seems to be related to the use of bound parameters on the service object constructor. A temporary workaround would be to just avoid bound params when using presigned URLs. The following should work:\njs\nvar AWS = require('aws-sdk');\nAWS.config.loadFromPath(process.env.HOME + '/.aws-sdk-credentials.json');\nvar s3 = new AWS.S3();\nvar params = {Expires: 5000, Bucket: '...', Key: '...'};\ns3.getSignedUrl('getObject', params, function(err, url) {\n  if (err) { console.error(err); throw err; }\n});\n. Looks good. I just added a test to verify that this does fix the behavior. Thanks for the contribution!\n. Looks good. I just added a test to verify that this does fix the behavior. Thanks for the contribution!\n. @muddydixon Can you elaborate on what benefit this provides? It looks like the patch does not actually provide the ability to use different hashing algorithms, so providing any authScheme other than the current hardcoded value should fail the request. Also, the authScheme probably does not belong as part of credentials object, it should be part of the base configuration object.\n. @Almad can you print the region you are using, as well as the result of \ncoffeescript\nconsole.log(this.httpRequest.endpoint)\nInside of the httpError event handler?\n. I am going to close this issue since there is not enough information to reproduce. Feel free to reopen if you can provide more details, specifically the endpoint used in your request, which should be printed by default in a NetworkingError as of v1.5.0 of the SDK.\n. The operation to create tags is AWS.EC2.createTags. You can use that to add tags to an instance. If you were using the EC2 CLI tools and not the new AWS CLI, it is likely that they call this extra operation on your behalf.\n. The API documentation comes from auto-generated service documentation, but I have added an example for this in the guides, which should help.\n. Looks good!\n. I can't reproduce a signature failure using the following code:\n``` js\n// region and credentials set by environment variables\nvar AWS = require('aws-sdk');\nvar elb = new AWS.ELB();\nelb.describeLoadBalancers(function(err, data) { console.log(err, data) });\n```\nThe error you are getting seems to indicate you are having an issue not with the signing mechanism, but with the timestamp that you are signing requests with. Errors like these are common when your machine's local time is out of step with the time calculated by the AWS service you are sending requests to. This is known as \"clock skew\". We have a few mechanisms for dealing with clock skew in the SDK, but I think in this case your best bet is to adjust your system's clock to match up with the correct time in your given timezone. Using NTP to synchronize your clock is recommended, but OS's like Windows and OSX also have built-in mechanisms to synchronize your system time.\nIf you try to synchronize your system time, does this resolve the issue?\n. Closing this issue since I am unable to reproduce. If you can provide more information about the problem, feel free to reopen.\n. > An additional problem is instead of returning a 409 as per the S3 FAQ, a 403 Access Denied is returned, e.g.\nThe documentation in the above FAQ actually states that the 409 error occurs only when trying to create a bucket that already exists, which correctly raises the BucketAlreadyExists error (409):\n\nIf you try to create a bucket with a name that already exists, you will get a 409 Conflict.\n\nIf you are doing something other than createBucket, you will get an acess denied error.\nI can look into adding a note to the guide that explains that the bucket parameter must be changed. Note that bucket names are globally unique, not just to your region. If you create a bucket in us-west-2, it will be available from any other region as well.\n. @d-smith the issue here is that you are not trapping the err in your first createBucket call, which has the 409 response:\njs\ns3bucket.createBucket(function(err1, data1) {\n  if (err1) { console.log(err1); }\n  else {\n    var data = {Key: 'myKey', Body: 'Hello!'};\n    s3bucket.putObject(data, function(err2, data2) {\n      ...\nYou should always have some sort of if (err) .. else check in all service calls or you may miss an important exception. Alternatively, you can use libraries like Q or async to simplify the callback chains.\nThat said, the SDK only returns exactly what the error from the service is, and in the case of 403s, there is little we can do to infer context around a failing request. If you believe that the errors you are getting from Amazon S3 are too vague, this would be something that you could suggest improving in the S3 forums. Hope that helps.\n. Thanks for bringing this up, I will close this because I've responded in the forums, which is a better place for questions like these.\n. Can you show which region and endpoint you are using to connect to? You can do so by also printing the following from the callback:\njs\nsqs.someOperation(function (err, data) {\n  console.log('region:', this.request.httpRequest.region);\n  console.log('endpoint:', this.request.httpRequest.endpoint.hostname);\n});\n. FYI the above commit adds the region and endpoint hostname to the NetworkingError object so that they will be immediately visible when debugging these sorts of errors in the future, which should make things a little less magical. This will be in the next release (you can also use it directly by typing npm install git://github.com/aws/aws-sdk-js).\n. @nodefourtytwo the EMFILE error certainly makes sense if it happens after a while. The EMFILE error means you are trying to open too many file handles, or, in this case, sockets, which can really only happen after a while. Googling for EMFILE shows a few StackOverflow questions with similar Node.js issues.\nAre you running SQS against a large number of concurrent items? Have you changed the Agent.maxSockets by any chance? That would certainly cause the EMFILE errors. ENOTFOUND can be a related error, but I'm not entirely sure why you are seeing that one.\n. > Does it mean we're getting close to the limits of what an SQS queue can take?\nNot necessarily, it means you are maxing out on your local system resources. If you were hitting SQS limits you would see throttling errors. See the links from the Google search linked above, or this specific StackOverflow question, for instance. You can potentially tune your OS to increase the file handle limits, but you should do this carefully, as increasing these limits can strain the I/O limits on your hardware and reduce overall performance (the ENOTFOUND error seems to imply this, but I cannot confirm without more information).\n. The fact that it happens on medium instances and no micro instances likely means that you have enough memory and processing power to actually saturate your open file limit-- the micros are probably just not reaching these limits. If you need to increase your file limit, you can look at bumping this, though you might also want to look at adding more instances to your cluster to process these messages.\nI am going to close this issue since it seems to be very much related to the environment, and there is little we can do in the SDK about the open file limits on your OS. Feel free to open a new issue if you conclude that the SDK is doing something it shouldn't be doing in order to cause this problem, such as leaking file handles.\n. Connections should be getting re-used. Can you provide information on how you are detecting that the connections are not getting re-used?\n. > I tried making 10 calls in sequential order. All are taking the same time.\nCan you provide code that reproduces and shows this behaviour?\n. Closing this issue since there is not enough information to reproduce the problem. Feel free to reopen if you can provide more details.\n. Thanks for reporting this issue. The above fix should go out with the next release.\n. Thanks for the patch. I'm a little concerned about checking the property with hasOwnProperty, because this still means we are clobbering the async property on functions that might rely on it, which we shouldn't be doing. I think the right thing here is to rename the property we are using completely.\n. At first glance this seems like it might be a regression in Node.js, if it worked in 0.10.16 and broke in 0.11.x. I have not yet tried to reproduce this in Windows, but I will take a look.\n. I was able to reproduce this on 0.11.5 on OSX and Windows, but this only seems to be an issue on that specific version. I tried also on 0.11.4 and 0.11.3 and both of those versions seem to work properly, so this looks like a regression in Node.js. The tests fail in a few places, so we will have to see what happens with future updates. If this issue persists in the next update, I will look into possibly writing a workaround, but we should not need to.\nI would recommend sticking to stable builds of Node.js, which currently means v0.10.x. The 0.11 series is considered unstable and may have regressions that make it out to releases; it may also change the API in ways that we do not yet support. I would also recommend opening an issue on http://github.com/joyent/node with the details of this bug, as it may be a legitimate regression.\n. I can't seem to reproduce this on a 100mb file in v0.10.16 or v0.11.3. Furthermore, we only have one call to process.nextTick in the entire SDK and it is only called once to start the process of createReadStream (which you are not using). \nCan you possibly provide the code used to reproduce this issue on your machine?\n. Great question-- getSignedUrl accepts any parameter that can be passed to the underlying operation, and putObject does support ContentMD5:\njs\nvar url = s3.getSignedUrl('putObject',{Bucket:'bucket',Key:'foo',ContentMD5:'...MD5...'});\n// 'https://bucket.s3.amazonaws.com/foo?AWSAccessKeyId=AKID&Content-MD5=abc&Expires=1377206528&Signature=YctACAvasw1YUC4LWE2SS%2B%2FtGqw%3D'\nThis would force S3 to validate the MD5 checksum sent along with the payload.\n. UserData must be specified as a Base64 encoded string. I just recently closed this related issue here: https://github.com/aws/aws-sdk-js/issues/81\nThe documentation is going to be updated in our next release.\n. Just FYI, we've updated the documentation to reflect the Base64-encoding on UserData.\n. > I'd love understand the correct way to use those tools with this SDK.\nYou are correct in associating the EC2 metadata service with IAM roles. The correct terminology should in fact be \"IAM roles for EC2 instances\"-- we should update the guide to reflect this.\nYou're also right that IAM roles should definitely be preferred over env vars or configuration files on disk. The verbage is a little inaccurate in the guide; the rationale for that order is that the SDK always looks for credentials from the EC2 instance metadata last, because this is a slow operation on non-EC2 instances, since it is making an HTTP request to a local IP that might not be present. We would also only be recommending that be used if the SDK is used on an EC2 instance, of course, which we should update the guide to reflect.\n. The feature is enabled by default. Sorry I forgot to mention this. The SDK works with \"zero configuration\" if you have IAM roles enabled on an instance.\n. @onetom \"zero configuration\" literally means that there is zero configuration for your application to use credentials from the instance metadata service. I'm not sure what your example shows?\nAs for questions:\n\nQ1: When using the EC2MetadataCredentials will it refresh the credentials when they expire?\n\nYes.\n\nQ2: Why is it necessary to specify the region? Shouldn't it be specified automatically using the metadata too?\n\nThe region that the EC2 instance is located is not necessarily the region in which your other resources live. It may be the case that this is true, but defaulting this value can lead to confusing behavior if a user forgets to configure their region. You can certainly setup your machine to export AWS_REGION from the instance metadata, but it should be something users explicitly choose to avoid confusion about what region the SDK is operating in.\n. Can you explain a little more about what you are trying to achieve? If your goal is to pass custom headers that are stored with your object, you can use the Metadata argument on putObject(). For example:\njs\ns3 = new AWS.S3({params: {Bucket: 'mybucket', Key: 'mykey'}});\ns3.putObject({Metadata: {customfield: 'customvalue'}}, function (err, data) { ... });\nIf you are simply trying to pass extra headers along with the request, you can build them up on the request object yourself like so:\njs\nvar req = s3.putObject(params);\nreq.httpRequest.headers['Custom-Header'] = 'custom-value';\nreq.send(function (err, data) { console.log(data) });\nDoes this answer your question?\n. Closing this stale issue. Feel free to comment or open another issue if you still have other questions.\n. I'm unable to reproduce this on my machine using node v0.10.16 and aws-sdk@1.5.2. What version of Node.js and the aws-sdk are you using?\n. @alexeypetrushin can you explain in what scenario the above code will load the entire stream into memory? Unless there is a bug in the current implementation, we should be calling pipe() on the stream object passed by the user, as per this line (body would be set here). Is this not happening properly?\nAs for length, it's already possible to provide that value through the ContentLength parameter of a putObject operation in cases where it's not available on the stream, though you can also set the .length property on the object passed into the body (which is the method used to implicitly determine the Content-Length for Buffer/String data types).\n. > 1. ... maybe it has something to do with GC and V8 memory management\nYes. V8 will pull larger chunks of data into memory until it has a reason to run a collection task. 30mb is a very small chunk, and likely won't get collected. Try uploading 500mb+. If we were loading the entire file into memory you should see your process spike to 500mb+ even before the HTTP request begins. The following would give you more data:\njs\nvar params { ..., Body: mystream };\nvar req = s3.putObject(params);\nreq.on('send', function() {\n  console.log(\"Starting request with memory state:\");\n  console.log(process.memoryUsage());\n});\nreq.send();\nIf the above does not show that the entire contents of your payload are in memory, then the data is in fact being streamed.\n\n\nI also found this test case ...\n\n\nThe above test case is for reading payloads out of responses. In that case, you are right, the SDK will by default load the entire payload into memory. If you want the response to stream, you would have to use the createReadStream method on Request; but this only applies for requests with response bodies; putObject has no response body.\nI'm going to close this since we do support streaming of payloads through requests, but if your experimentation with the above code shows that memory usage is unnecessarily high, feel free to re-open this and we can take another look at what is going on.\n. This looks like a duplicate of #150, which is not an issue in the SDK. Given that I was not able to reproduce this with v0.10.16 I would recommend upgrading Node.js, since it looks like a regression. I'm going to close this as a third-party issue.\n. @springmeyer can you provide a test case that reproduces the error for you?\n. @springmeyer I fixed the zero-buffer issue, investigating the second issue now.\n. @springmeyer I've just pushed a fix for #248 which should fix both issues in your gist. The process.nextTick issue probably will not be resolved by this, though.\n. Thanks for finding all of these issues, @springmeyer. I've just resolved the last one in the above commit.\n. Setting a highWaterMark seems like a reasonable and fairly simple thing to do on our part. Are there any downsides that I might be missing?\n. This is a good catch. We recently upgraded CloudFront to use the 2013-05-12 API version which uses SigV4 to sign requests. Unlike the previous signer, SigV4 signs with a region, causing the error. The problem here is since CloudFront has a single global endpoint, it should only sign with us-east-1. We should fix the SDK to force the us-east-1 region for this service when the global endpoint is used.\nNote that the older '2012-05-05' version of CloudFront is still available in the SDK, and you can use it by constructing the object with:\njs\ncloudfront = new AWS.CloudFront({apiVersion: '2012-05-05'});\nThat should be a temporary workaround. Keep in mind that by default the SDKs use the latest version available in each service, and updates might include backward incompatible changes (legitimate ones, not bugs like this), so it is wise to version lock when possible. Information on locking API versions can be found in the Configuration section of the guide, as well as in the Services section.\n. The above commit should fix the issue. If you want to test it out you can:\n$ npm install git://github.com/aws/aws-sdk-js\n. Thanks for reporting, by the way!\n. Hi George,\nSorry for the lack of response. Can you provide a complete working example of the code you are using? I'm noticing that the output does not quite match up with the code you have. I'm unable to reproduce timeouts with a regular upload (simply passing the createReadStream handle with no attached events). Note that for larger files you may want to look at the createMultipartUpload operation (and uploadPart / completeMultipartUpload calls) specifically because of the fact that larger downloads can occasionally fail. Splitting the data up into smaller chunks will mean less time spent recovering in such an event.\nThat said, it certainly should not happen every time.\n. Were you sending multiple files in parallel with the original implementation as well? If so, that is likely the reason that the uploads are occasionally failing. Amazon S3 does throttle connections, so if you send too many at a time it may abort some of them. Your multipart upload functionality should work just fine as long as you make sure not to send data too aggressively. \nWe recently had a similar issue reported on aws/aws-sdk-ruby#244 in which heavy use of parallel requests was also causing connection drops. If you reduce the amount of parallel requests, do you still have issues?\n. @GeorgePhillips I'm not sure if there is any detailed documentation on throttling in Amazon S3. I would suggest asking about this in the Amazon S3 forums. You may get a better / official answer there.\n. I'm going to close this issue but feel free to re-open if you think there is something the SDK is doing wrong.\n. Using IsTruncated is not a workaround, but rather the expected usage of that property. If you want to know whether there is more data, you should be checking IsTruncated. The fix you made in that related issue was the correct thing to do. I would suggest submitting feedback on Amazon S3's GET Bucket documentation page (feedback link) if you think Marker isn't properly documented, but note that they show examples on the page where Marker is empty (<Marker/>), and that would fail your check (an if (list.Marker) check will fail if Marker is ''). Also, Marker is only passed back if you pass it in.\nNote also that we have a pagination interface in the SDK. This might be of use to you if you're going to do a lot of object pagination, as it should be able to simplify quite a bit of your code.\n. > I was using this page as my reference and nothing is said about markers in there.\nI noticed that Marker output documentation is missing from our docs. We will look into fixing that one.\n\nif the marker has to be passed to be retrieved in the response, does that mean that it will be the same ...?\n\nYes, Marker should always match the input you passed in. Typically you would set Marker to the last object's name, unless you have a Delimiter and Prefix, at which point you get a NextMarker token. Note that this behavior is all implemented by the paginators in the SDK, and it can get complicated for S3, so I would suggest testing it out to see if it works for you. Note that the pagination APIs are currently experimental, but they should work. Feel free to report any bugs if you find any.\n. The documentation comes from the generalized Query documentation for Amazon DynamoDB. These docs use null in the language agnostic sense, and since services rarely ever actually send \"null\" down the wire as a response, can also mean undefined for languages that have such a construct. I would suggest submitting feedback to the documentation in the above link if you think that's a confusing description-- perhaps a better term to use in language agnostic docs would be to use the terms \"set\" and \"not set\", i.e., \n\nIf LastEvaluatedKey is not set, ...\n\nThat reflects much more closely to what is actually happening over the wire (it's not actually being sent).\nIn general, though, you should treat null and undefined values as the same in responses from the SDK.\n. Closing this since there was no action on the issue. Let us know if you still have questions about this.\n. I would try the package on github via npm install git://github.com/aws/aws-sdk-js. If this problem persists, this is probably an issue that should be opened against joyent/node, as it would be a regression in Node.js, not the SDK.\nNote however that I'm not able to reproduce a problem on OSX 10.7 with Node.js v0.10.19. Perhaps this is specific to your environment?\n. Thanks for following up @nathanpeck! Glad to see that the Node guys got a fix out fast, and that we have a reliable root cause for this issue. I will close this but if anyone opens a similar issue I will be sure to point them here.\n. Looks good, Ben. Thanks for the pull request!\n. Can you provide the error you are getting when using this with the unmodified SDK?\n. Understood, but do you have an stack trace or error that shows the problem? Can you describe a little bit more about which emulated service tools you are using? Is this DynamoDB Local by any chance?\nI ask because, typically, servers should not be caring about the port value in the host header in order to properly route a request. If this is causing a signing error, that's an issue we need to fix, but if your emulated server is choking on the missing port, that seems like an issue specific to the server itself.\nI should clarify that I'm fine with your pull request, and I think we should merge it to be consistent in the SDK, I just want to know what kind of use cases this issue affects so we can have the right changelog entry and communicate the right thing to our users.\n. > See section 14.23, which says that the Host header must contain the port.\nThanks for providing the link. I understand that we should be sending the port in the Host header when sending HTTP/1.1 requests. What I'm trying to verify is whether this is an issue that might affect functionality of existing tools; that's why I had asked if you could provide a stack trace for an existing tool to see in what way it was failing.\n\nI'm trying to write a fake SQS for testing. SQS has some APIs for getting queue urls, and we want to use the Host header to implement this.\n\nIf you're looking to emulate SQS, you should look at the QueueUrl parameter to determine the URL when responding to operations on queues rather than trying to read the Host header. The queue URL is always specified via that parameter in queue operations, and although the Host header might be set due to a requirement in the HTTP/1.1 specification, it is not the actual contract required by SQS. Note that HTTP/1.0 requests do not require the Host header, so tools (other than the SDK) that interact with SQS may be sending these requests. If you want your mock server to also be compatible with these tools, you should not rely strictly on the Host header, as it may not be present.\n. Closed by #167.\n. Hi,\nThe password provided by EC2 is encrypted using the private RSA key you got when you launched the instance. If you want to decrypt the password, you can use a package like ursa to load the PEM and decrypt the response value (note that it is base64 encoded).\nFor instance, the following code would be able to decrypt the data using my keypair called 'windows-keypair.pem' (assuming you have done an npm install ursa):\n``` js\nvar AWS = require('aws-sdk');\nvar fs = require('fs');\nvar ursa = require('ursa');\nvar pem = fs.readFileSync('/path/to/windows-keypair.pem');\nvar pkey = ursa.createPrivateKey(pem);\nvar ec2 = new AWS.EC2();\nec2.getPasswordData({InstanceId: 'i-abcdefg'}, function (err, data) {\n  if (data) {\n    var password = pkey.decrypt(data.PasswordData, 'base64', 'utf8', ursa.RSA_PKCS1_PADDING);\n    console.log(\"Password is\", password);\n  } else {\n    console.log(\"Could not get the encrypted password\");\n  }\n});\n```\nAnother way to decode the data would be to use the AWS CLI which has an easy to use high-level interface for this kind of out-of-band task called get-password-data. You can also decrypt the password by providing your private keypair in the EC2 console.\nDoes that help?\n. Glad it is working for you!\n. Thanks for the patch @nicks!\n. Can you provide more details on where this error is coming from? What service are you using? Code to reproduce this behavior would be helpful.\n. Are you using IAM roles for EC2 instances, or are you actually specifying credentials with the following as you showed in the sample?\njs\nAWS.config.update({\naccessKeyId: configApp().aws.accessKeyId,\nsecretAccessKey: configApp().aws.secretAccessKey\n});\nIf you are hardcoding credentials like the above, you shouldn't be having any expiration issues (since hardcoded credentials should not expire).\n. Are the credentials in the configuration file being rotated by any chance? If not, I'm not sure what the issue could be. We did change some semantics around credential management but nothing related to statically loaded credentials. Are you sure this was not happening before?\n. Note that we recently had a forum post about this issue but it was related to IAM roles for EC2 instances, not hardcoded credentials. \nUnfortunately I'm not sure how to narrow this down any further. If you can provide more information about the version of Node.js you are running, the server software, etc., that might help.\n. Good catch Michael! That is definitely the issue. We started coaching SigV4 requests for performance but are incorrectly caching the date. The fix for this is pretty easy. I will take a look in a bit.\n. Thanks for reporting this @fprivitera and @gsabena. I just pushed out a patchlevel release of aws-sdk v1.7.1 which you should be able to npm install. We've added regression tests above for this issue and will be much more vigilant about tracking changes like these in the future so that we don't introduce other similar bugs.\nAlso thanks to @mhart for the quick patch!\n. @zkimmel can you confirm that 1.7.1 and/or 1.8.0 resolved the issue for you? We have a similar issue (#171) being reported and it would be helpful to rule out some of the culprits. Thanks!\n. Re-opening because this is still an issue. We have a fix in the works that should correctly resolve.\n. Thanks Michael. I will add some regression tests for this one and refer #168.\n. Thanks for the patch!\n. If you can print dynamodb.config.credentials (please make sure to censor your secretAccessKey!), that would provide information on whether the SDK attempted to refresh the credentials.\n. It seems like this is not related to the midnight issue that was occurring in #168. Can you confirm that this request occurred at 2:54 PM GMT? It also seems like according to the credentials object it had a good 6 more hours before it was going to expire. Does that look correct as well?\n. In addition to the SigV4 caching, we also made some changes to pre-emptively expire EC2 role credentials in v1.7.0. It seems like this is not actually the problem (since we didn't hit the expire time yet), but we may have inadvertently broken how the SDK re-signs invalid credentials if credentials get swapped way before the expiry time, because the SDK should have attempted to retry. If you print this.retryCount, what is the value? Is the SDK attempting to retry the request?\nTwo questions to cover another possible base:\n1. Once this starts failing, does it continue to fail, or is this just temporal (further requests succeed)?\n2. Do you have any unicode in the body of these requests, by any chance?\n. this.retryCount should be printed from the callback function. The context of the callback function is the Response object returned by the SDK.\njs\nec2.describeRegions(params, function (err, data) {\n  if (err) {\n    console.log(err.stack);\n    console.log(\"Retries\", this.retryCount);\n  }\n});\nNote that you can also turn on request logging in 1.7.1 with\njs\nAWS.config.logger = process.stdout;\nWhich will print request time and retry count, along with other details about the request.\n. Thanks @mlogan. I just started running your reproduction code now and I will see what I get.\n. @mlogan I've been running your script since my last response (GitHub says \"9 hours ago\") and have not yet seen any failures. Is there a typical time when it starts to fail? N number of hours in, or at a specific time? I've already hit a date rollover in UTC and it still seems to be working.\n. Oh right, if it's related to the previous issue it would only occur on services using SigV4, which EC2 does not.\n. I'm testing this on DynamoDB right now. ListTables should be a sufficient request, which doesn't require any tables.\n. FYI I am testing with the following code:\n``` js\nvar aws = require('aws-sdk');\naws.config = {\n  region: 'us-west-2',\n  logger: process.stdout\n};\nvar db = new aws.DynamoDB();\nfunction doListTables() {\n  console.log(\"Sending request at \", new Date);\n  db.listTables(function (err, data) {\n    console.log(this.request.service.config.credentials);\n    //this.request.service.config.credentials.expireTime = new Date(0);\n    if (err) {\n      console.log('ERR:', err);\n      console.log(this);\n    }\n    setTimeout(doDescribeRegions, 1000 * 60 * 30);\n  }).on('retry', function() { console.log(\"RETRYING!\") });\n}\ndoListTables();\n```\nThis should print enough information, but note that this will print your secretAccessKey, so please make sure to sanitize the output before sharing any data from this snippet.\n. I've identified the issue and we will be putting out a fix shortly. I will keep everyone here posted.\n. I just pushed aws-sdk@1.8.1 which should resolve this issue. The root cause can be found in the above commit. In short, our performance optimization patches (which improved request performance by ~2x on subsequent calls) added caching, which was not correctly being invalidated when the expired access keys were changed. A patch in #168 fixed invalidation on date, but we missed invalidation on the access key ID.\n. Thanks @mlogan for providing the useful reproduction steps and data to find the root cause.\n. The SDK already retries timeout errors. Detailed discussion is in the forum thread.\n. This is the same issue reported in #150 and #158. Unfortunately the SDK never actually calls process.nextTick in the portion of code you are using, so it would most likely be a third party issue with another library (possibly the async module if you are using it) or Node.js itself. I would recommend trying this with the latest stable release of Node.js v0.10.20.\n. I would also recommend using multi part uploads (createMultipartUpload, uploadPart and completeMultipartUpload) for larger files to reduce the time spent recovering from failed uploads.\n. We publish release notes for each release at http://aws.amazon.com/releasenotes/SDK/JavaScript\nDoes that help? Perhaps we should call this out in the README.\n. We went back and forth on this a few times. Bubbling the exception up will actually cause it to get bubbled up to the send lifecycle event handler, which makes the request get retried on the service a number of times even if it is successful. We had turned on bubbling in #74 but had to revert due to #131.\nThe best way to handle these errors it to use domain support inside of nodejs and allow the domain handler to catch the exception.\n. Closing this due to age. We recommend using domains when trying to bubble up errors. If you are having issues getting domains to work properly, please open a new issue for that.\nThanks for reporting this.\n. Can you explain how you are using the SDK? Do you have code and error output that you can show for this issue?\n. Closing this. Feel free to re-open if you have a minimal code sample that can reproduce the issue.\n. We're looking to add an 'httpProgress' event in light of the new 2.0 release we just made that added browser support to the SDK-- it will definitely be useful there as well.\nIn Node.js environments you can upload a Stream object and attach a 'readable' event that should get triggered as contents get uploaded. You can potentially track progress there, though you will have to dig into the stream to get the actual amount of data sent over the wire.\n. If the goal is only to use this in Node, it's already possible to attach a handler to the stream object you are uploading and track progress like that, as mentioned above. This does not require any changes in the SDK; you can do the following with most versions of the SDK (using the through package to simplify the piping interface):\n``` js\nvar fs = require('fs');\nvar through = require('through');\nvar numBytes = 0;\nvar stream = fs.createReadStream('bigfile');\nstream.pipe(through(function(data) {\n  this.queue(data);\n  numBytes += data.length;\n  console.log(numBytes, 'bytes written');\n}));\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3({params: {Bucket: 'mybucket'}});\ns3.putObject({Key: 'bigfile', Body: stream}).send();\n```\n. @deth4uall I have a WIP branch called http-progress with the above change that adds an httpUploadProgress and httpDownloadProgress event. The event emits a structure that contains the .loaded and .total properties.\nCurrently this only works in the browser. This will be merged when it is ported over to work inside of Node.js as well.\n. PR #209 is now merged and httpUploadProgress / httpDownloadProgress events will be available for use in the next release (or immediately if you build/use from the master branch on GitHub). See description in #209 for example usage.\nThe feature is available for both the browser and Node.js, but note that only Node.js v0.10.x+ is supported.\n. @dhiraj72 what version of the SDK are you using? Is this in Node.js or the browser?\n. I would recommend upgrading to the latest version of the SDK: v2.0.8\n. We're now gzipping the JavaScript assets through the CloudFront distribution. Thanks for bringing this to our attention!\n. @unscriptable the desired structure would be AWS.S3, AWS.DynamoDB, etc. You can see this structure in our API documentation: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/frames.html -- We don't have any public APIs that are nested 2+ objects deep. Everything that users access are at the toplevel of AWS, similar to a standard exports \"bag\" form.\nPart of the issue I raised on Twitter stems from keeping a consistent API both in a CommonJS environment and in the browser. I had specifically asked how we could maintain the same API from the user's perspective. Although your change does bring the code in line with a standard CommonJS module (something we looked at early on), it means the usage would differ in a Node.js environment and in the browser. @substack's comments are a good example of that. One of the goals we tried to strive for when releasing this SDK was to make it so code is portable between environments. This is helpful because we want users to be able to take code they wrote in the browser to \"Just Work\" in Node or vice-versa, with minimal changes. Having to do var S3 = require('aws/services/s3') in one environment and AWS.S3 in another would be confusing (for docs, see below), and not very portable. \nWe also want a consistent story for documentation. Having two completely separate module schemes would mean having to maintain two completely separate sets of API documentation and guides (the latter is maintained completely by hand). If we're talking about maintainability, that's not a very maintainable route to take. Having one set of documentation is simpler for us and simpler for users as well.\nSo for that reason, the global AWS \"namespace\" needs to stick around. How we build up that namespace could change, but effectively the aws-sdk should be seen as a single module, to users. We want to keep the SDK to a single point of entry: var AWS = require('aws-sdk');-- this is much easier to use, much easier to grok, and much easier to maintain. Having users reach into files on disk (like require('aws-sdk/s3/...')) would be a maintenance nightmare, because we would never be able to rename a file on disk if users started doing that. Our public API would extend to the structure of files on disk, which I can attest from maintaining Ruby code over the years, is not fun.\nThere are certainly some stylistic uses that we could clean up, like calling service.defineService() instead of AWS.Service.defineService. We made those choices explicitly too, though, and this one was actually for better maintenance. I'd be happy to dive deeper into our reasoning, if you're interested. But there are other things like our util module, and other internal modules, which could very well be made into proper CommonJS modules and not punched onto the global AWS object. We could even do a better job of hiding private functions rather than punching them onto classes, which would likely yield better minification. All of this to say, there are certainly things we can make better in the codebase. I think that's true for most of the codebases I've worked on.\nFYI I definitely appreciate your input and eyes on this project, John. We definitely want feedback like this. I think these discussions are useful. They help us figure out what we can improve. I really like these conversations.\nAlso hello @substack! We love browserify! Thanks for making that awesome tool.\n. Closing this issue/pull request since it has gone stale. Thanks for the feedback and demonstration of modules, but without a way to provide a uniform interface to both Node and browser users alike, it will be difficult to support this kind of a organizational code change. One of the goals of the SDK is to make it as consistent to use across platforms as possible-- if it means bending some of the conventions around module formatting to get this consistency, I think it is worth it.\n. Thanks for the quick fix, @jbt!\n. Thanks for pointing this out!\n. Does your proxy use CONNECT tunneling? If so, this is unsupported; see #108 for more information.\n. We publish release notes at http://aws.amazon.com/releasenotes/SDK/JavaScript \nThe URL is linked from the README, also see #175.\n. This just looks like an unfortunate naming accident; we don't use the \"toJSON\" function name for any specific reason (besides describing what it is doing), so it can be renamed. /cc @trevorrowe \n. I can't seem to reproduce this on the latest version of the SDK. What version are you on? DryRun is a parameter that was added sometime after the initial launch of the SDK, so it's possible that you are using an older version. The latest version as of this posting is v1.12.0; I would recommend using that.\n. Are you sure that this is still occurring in the latest version of the SDK? Now that I can see the issue, I remember us resolving a similar issue. Can you try this on v1.12.0?\n. I just looked into this and it seems like nock does not support the new 0.10/0.11 \"streams2\" API. The SDK relies on the \"readable\" event in 0.10+, since this is how streams work in new versions. This seems like a missing feature in nock for not supporting this new API.\nFortunately there is an easy workaround to force our SDK to use the old streams API-- actually there are two ways:\n1. Use Node 0.8.x-- this should work for you there.\n2. Set AWS.HttpClient.streamsApiVersion = 1 in your tests. Don't do this on production code, but it should work in your tests.\nLet me know if that works for you. I would also open an issue with nock to get them working on the new readable event interface for streams.\n. Glad the workaround works for you. I'm going to close this since there's not much we can do for this in the SDK itself. For reference, here is some info on the \"streams2\" API (actually just called streams in Node, but it was originally advertised with that name):\nhttp://blog.nodejs.org/2012/12/20/streams2/\n. Kinesis support has been added via 4195c88a8a29d6a87c7f957cad7329784a1a2149 and will be available in the next release. Note that the API might be a little different from the 2013-11-04 model that @chrishamant was working on above, as they've made some changes since then.\n. What browser are you using to reproduce this?\n. If you look into your Firefox HTTP logs (via Firebug or what have you), can you list the Content-Type header that the browser is actually sending across the wire? We have some custom code for FF because it likes to append extra data not supplied by the user, and it might be doing that differently in 25.0.1.\n. Thanks. I've reproduced this and will be pushing a fix shortly.\n. Looks like Firefox does not actually look at the charset and always appends \"charset=UTF-8\". I tested this with an application/zip content-type and Firefox sent it down the wire as \"Content-Type\": \"application/zip; charset=UTF-8\". It may be possible for FF to change the charset, but I'm not sure what that logic is.\n. The ETag is sent back as a header, and S3 requires that you configure all exposed response headers via the CORS configuration. You can do this by adding\n<ExposeHeader>ETag</ExposeHeader>\nIn your CORS config. You would do this for any other header elements you want to access via CORS.\nDoes that help?\n. I believe we don't yet support this in the browser environment, I will put this on the to do list to add support for request.abort().\n. The SDK supports Elastic Beanstalk. Are you sure you're sending this request to the right region? Keep in mind that applications are scoped to a specific region, so you might be talking to the wrong one.\n. I tried creating a new application in us-west-2 and I was able to get a correct response from Beanstalk:\njs\n{ ResponseMetadata: { RequestId: 'b24b4431-53c0-11e3-aa51-55790b98d7bf' },\n  Applications: \n   [ { Versions: [Object],\n       ConfigurationTemplates: [],\n       ApplicationName: 'My First Elastic Beanstalk Application',\n       DateCreated: Fri Nov 22 2013 13:54:39 GMT-0800 (PST),\n       DateUpdated: Fri Nov 22 2013 13:54:39 GMT-0800 (PST) } ] }\nCan you verify that you have applications in us-west-2? Keep in mind that us-west-2 is the \"Oregon\" region in the console.\n. This would be an issue to open with the Elastic Beanstalk team on the forums. The SDK has no way of knowing why the service is returning what it is returning.\n. Thanks for reporting, this is an easy fix!\n. Fixed by #197 .\nThanks for submitting @seangarner!\n. If you can only login with admin it means that your application may be in sandbox mode and may be disabling login for those users. If that's not the case, I would open a thread on the IAM forums to ask about this, since they would have more information. \nI am going to close this since this is not an issue with the SDK. The SDK is sending the correct request to the service, so the service will be responsible for authenticating the users.\n. Yes, this is a CORS issue. EC2 does not currently support CORS. You can use EC2 from the browser or web enabled environments, but only environments that can disable the CORS restriction via some other permission mechanism (like Chrome extensions). See this forum post for more information.\n. I would suggest opening a thread on the EC2 forums so that the EC2 team knows about your interest for CORS support.\nClosing this issue here. Hope that helps.\n. @nkratzke Quick update in case you're still interested! Amazon EC2 has recently introduced CORS support and the AWS.EC2 service is now available in the hosted build of the AWS SDK for JavaScript in the browser:\nhttp://blogs.aws.amazon.com/javascript/post/Tx30V18CDJFNSB/Announcing-CORS-Support-for-Amazon-EC2\n. At a high level I actually think this might be a good idea. That said, I think there is value for \"going as fast as possible\". Perhaps having both eachPage and eachPageAsync calls would be useful.\n. Ah, good point. Then yes, if it checks the function arity, that's a very good way to go about it in a backward compatible way, then we wouldn't need the *Async option.\n. I finally had a chance to jump back on this. Just merged it via ef4e3e1d08c68bd80b38576a8b1ab464f34faa39. This should be in the next release.\nThanks for the contribution! Let me know if you ever get this working for eachItem. Pulling that in should be quicker.\n. It might be possible to make this change, but note that it won't work everywhere, specifically for REST based services, like S3, and in general this kind of a thing may not work in the future. Generally speaking, the path of an API call depends heavily on the protocol and serialization format used. For instance, in RPC style services (like EC2), everything should be sent to the root of the endpoint. It's possible to take this for granted, but it's not as possible for REST style services where the path is defined in the API configuration directly. It's not usually possible to guarantee that the SDK will work mounted in sub-paths of an endpoint.\nThat said, I think it would be safe to change this at least for JSON and Query style services.\n. @c4milo the API configurations are generated by respective SDKs. You can see similar configurations in PHP, Ruby, Python, and the CLI.\n. @c4milo the SDKs themselves are the source. You can find a fairly complete version of these configs in https://github.com/aws/aws-sdk-core-ruby/tree/master/apis/source\n. They are all canonical. Boto-core has similar configs: https://github.com/boto/botocore/tree/develop/services\n. @c4milo thanks for following up-- I had missed this comment. There is really nothing disingenuous going on here. The contracts are pulled directly from the services, so those JSON documents are basically the contracts. You can think of them similarly to a WSDL, except they are hosted primarily in the respective SDKs rather than a special endpoint. Does that help?\n. @c4milo we have had customer feedback for a Go SDK a few times before. I can tell you that we look at all SDK requests like these. I will definitely keep you posted if we ever do release something.\n. Can you provide more details on this example? Can you print the actual error and stack trace (err.code, err.stack) so that it's more clear what is going wrong? Also, is EC2 relevant here?\n. I see. It looks like the NetworkingError is triggering a success event to fire, but the request is also being retried. Thanks for reporting this, I can probably narrow down the cause with that info.\n. I've narrowed down the issue to the XHR event in Firefox emitting the readystatechange event on failures like these in addition to error, which causes us to emit both the httpDone and httpError events. I will have a fix for this shortly.\n. Thanks for reporting, the above commit should fix the issue and will be available in the next release.\n. Feel free to star our repository!\n. The OPTIONS error you are getting is due to CORS not being configured on your S3 bucket. S3 requires CORS to be manually configured for requests to access objects in a bucket. You can see this described in the Getting Started Guide of the AWS SDK for JavaScript. Hope that helps.\n. This is not going to be an SDK issue since the SDK has no impact on the OPTIONS request that is sent by the browser's security layer. The configuration you listed looks correct, but you might want to verify that you are operating on the correct bucket with that configuration. Without more information about this failure it is difficult to know exactly what is going wrong.\n. Closing this old issue. If you run into similar issues please feel to open a new issue with all the necessary information to reproduce the problem, but make sure to verify that the OPTIONS request sent by your browser is returning successfully to ensure that CORS is correctly configured. You can check this by looking at your browser network inspector tab -- it should list the OPTIONS request with either success or error.\n. Seems like the SDK timed out when attempting to get credentials from the metadata service, but the stack trace is a little unclear, since it's not a complete trace. We recently changed the way we retry failures from the metadata service in e2a733183cc14ff803a401755bbd5697c8829e3d and available in v1.15.0/v2.0.0-rc3 that should continue to retry if this timeout occurs. That said, without more information it's difficult to know if this is a bug in the SDK.\nCan you provide a reproduction case or more of the stack trace?\n. Closing this since it's unclear what the issue is. Please re-open or create a new issue if you can provide reproduction steps and we will be glad to look into it!\n. The current version of the SDK is v1.17.3. Can you confirm that you are using this version? Printing AWS.VERSION should tell you this.\n. We've not made any changes to the instance metadata service support since 1.15.0, I was just confirming that you were on a later version, since your post said 1.5, not 1.15.0.\nI think I might know what the issue is. I will be able to investigate this tomorrow.\n. Solving this is going to be a little more complicated, but to summarize the issue:\n- The SDK attempts to validate existence of credentials before sending a request, which means it tries to get creds from your metadata service and it fails (temporally).\n- Since the credentials turn up empty, validation fails.\n- The SDK currently does not retry a request if validation or building of a request fails, so this does not trigger any of the retry logic.\nThe fix to this would be to have all steps retry, but currently retry logic assumes the request is already built, an assumption that is not true if validation triggered a retry. I will be looking into solving this, but in the mean time, you can wrap these calls in your own checks to kick off the request again if it fails:\n``` js\nfunction makeRequest(req, callback) {\n  return req.on('error', function (err) {\n    if (err.name === 'CredentialsError') makeRequest(req, callback);\n  }).send(callback);\n}\nmakeRequest(s3.listBuckets(), function (err, data) {\n  console.log(\"Done\");\n});\n```\n. I am going to close this since there hasn't been much action in a while, and it seems as though the upgrade has fixed the issue for some users. If you are still having problems, feel free to re-open this or create a new issue.\n. You can do this by removing the signing and credential validation handlers:\n``` js\nfunction unauthenticatedRequest(operation, params, callback) {\n  var request = s3operation;\n  request.removeListener('validate', AWS.EventListeners.Core.VALIDATE_CREDENTIALS);\n  request.removeListener('sign', AWS.EventListeners.Core.SIGN);\n  request.send(callback);\n}\nunauthenticatedRequest('listObjects', {Bucket: 'mybucket'}, function (err, data) { ... });\n```\nAs for an unsigned getObject, you can just use a regular http GET to download public objects from S3.\n. I've publicly exposed the internal makeUnauthenticatedRequest operation we use for some unauthenticated operations, which you will be able to use in the next release of the SDK (you can already use this in the current SDK). See the above commit for a usage example.\n. Can you provide more information as to how you are verifying this? Can you print the ab.byteLength value?\n. It looks like upgrading browserify to 3.x resolves this issue, and also increases performance with the new native Buffer implementation. I see a much lower memory profile when building my own version off of 3.x. You can do this by following the \"Browser Building Guide\" but with the extra step of changing the browserify dependency in dist-tools/package.json to:\n\"browserify\": \"3.x\",\nI will do some more tests with this new version of browserify to make sure nothing else breaks, and if it looks good, the next RC of the SDK will be using this new Buffer implementation which should fix this memory issue.\nLet me know if you can reproduce better memory performance with the above fix.\n. I've just updated the repository with the above commit, so following the instructions now should already use browserify 3.x.\n. @ryan-digbil give this a shot: https://gist.github.com/lsegal/8445157/raw/fda47ce7b4d000ba02b0b055be0e9fa377007602/aws-sdk-v2.0.0-rc8.fix-gh-207.js\n. Glad to hear that it resolved the issue. I am going to mark this as fixed by 48ed006e0938b913bad0b2bcaa5beb688185d2a2. This will be available in the next official release.\n. 2.0.0-rc9 is now available and should resolve this issue:\nhttp://sdk.amazonaws.com/js/aws-sdk-2.0.0-rc9.min.js\n. See the forum for more information, but there is nothing we can fix in the SDK for this except improve documentation. The SDK is behaving correctly, but tier environments depend on a very specific set of inputs. I've contacts the Elastic Beanstalk team to follow up on that forum thread.\nI am going to close this issue in favor of the forums for this one. Feel free to re-open if you still think the SDK is doing something wrong.\nHope that helps.\n. Yes, it should work in 0.10.x+\n. Print the response body to confirm that the InternalFailure is coming from the service. In your cb, do:\nfunction mycallback(err, data) { if (err) console.log(this.httpResponse.body.toString()); }\nIf the \"InternalFailure\" is part of the body, then I would suggest opening a thread on the DynamoDB forums and following up there, as this would not be an SDK issue, and talking to the DynamoDB team would get you a much better answer much more quickly.\n. @mhart if the DynamoDB team hasn't yet seen your project I'll be sure to send it over to them when I'm back in the office next week. The list of issues in your README would be a good set of things for them to tackle. Thanks for linking this!\n. Thanks for reporting this. I think it makes sense to implicitly do conversion on strings to integers in JavaScript given the weak typing built into the language. The above commit should fix the issue using a solution similar to the one you provided and will go out with the next release. \n. Just an update on this: I just released v1.17.1 and v2.0.0-rc6 which include a fix for this issue. See:\nhttp://aws.amazon.com/releasenotes/SDK/JavaScript/6430217080426395\n. The above commit should fix the issue. I'm not around a machine to test this right now (doing this from my phone), so it would be great if someone can confirm that the above line actually fixes the issue. I can look into the root cause of this regression when I'm at a computer later today.\n. I will be putting out a patch release sometime tomorrow. Thanks for reporting this and confirming the fix.\n. Update: v1.17.2 and v2.0.0-rc7 are now released.\n. See #212. there was a regression in 1.17.1 / 2.0.0-rc6 that I have just pushed out a fix for in 1.17.2 / 2.0.0-rc7. Updating to this version should fix your issue.\n. Closing this as a duplicate of 212, but thank you for reporting.\n. This looks like a regression in v1.17.0. Using v1.16.x should emit an 'error' event. I will look into the root cause of this issue.\n. The above commit fixes the error and I've just put out v1.17.3 and v2.0.0-rc8 to fix this issue. Thanks for reporting the issue.\n. You can get the request ID from the headers of the response:\njavascript\ndynamodb.listTables(function(err, data) {\n  console.log(this.httpResponse.headers['x-amzn-requestid']);\n});\n. That is only for REST JSON services. DynamoDB happens to be an RPC JSON service, and not REST based. That said, it would be trivial to copy that same code over to json.rb, which is what the RPC JSON protocol uses.\n. Hi @jeevankkwy. I've made the above change that adds a AWS.Response.requestId property that should be accessible from all services. Note that the property is no longer modeled in the \"data\" bag, which makes it a breaking change (and will only be available in the 2.x releases).\nLet me know if you think the request ID is better off in the data property-- it's not too late to move this back, but putting it directly in the response should give it some better documentation visibility.\n. @ivanakimov That's correct. Currently the SDK only manages the control plane side of the API. We've had requests for supporting search, and there is an open issue for this feature request in #134, so I will close this issue in favor of that one. I will mark this question as a feature request, and you can track the request in that issue.\nThanks for the feature suggestion.\n. Thanks for reporting this. I am closing this as a duplicate of #207, since we are already tracking this there. We are investigating, but keep in mind that the code listed is part of a third-party dependency browserify (actually the native-buffer-browserify module), so it might be wise to report a similar issue about Buffer memory usage there. Any fixes we use would have to be pulled into browserify at some point anyway. \n. I'm unable to reproduce the failure with your code. What browser are you testing this in?\nTo answer your question, you can use the S3.getSignedUrl function to get URLs for images. If the image is public, you can just point directly to the endpoint: https://yourbucket.s3-us-west-2.amazonaws.com/yourimage.jpg.\n. The \"Key\" in the getSignedUrl params should be the key name of the image in your bucket, never your AWS credentials.\n. I've only been able to reproduce the toString error in Chrome when CORS is not correctly configured. Have you checked your console and network tab of your dev inspector to check if the OPTIONS request for CORS is correctly being sent? Have you enabled CORS for your bucket?\n. The above commit resolves an issue causing the callback event to fire multiple times if a networking error occurs when making the XHR connection in Chrome only. Note that this fix will make it more clear that a network error is happening (rather than the obscure toString error you see), but you would still see the network error returned.\nTo resolve the network error you must configure CORS on your bucket.\n. Configuring CORS is described in the Getting Started Guide for the SDK. I will resolve this issue since this seems to be related to CORS configuration. The toString error reported here is caused by an unrelated bug and is resolved by a8cab09e563938f3d0a633161d3ec4b3da16128a -- this will be available in the next release.\nFeel free to comment on here and re-open if you are still having issues.\n. Is the value missing from the SDK, or is it missing from the response that DynamoDB is sending? You can verify this by inspecting the output of this.httpResponse.body.toString() in your callback to check the raw payload sent back by the service. We can look at fixing this if the data is coming through but not available from the SDK, but unfortunately, if the result is missing from the raw response, there is not much the SDK can do to fill in this data.\n. Yes, I would recommend either posting on the Amazon DynamoDB forums or contacting AWS support with your issue.\nI am going to close this since there is nothing we can do from the SDK side, but thank you for bringing this to our attention.\n. @polythene1337 actually for what it's worth the low-level DynamoDB documentation does not make any mention of the \"NumberOfDecreasesToday\" being available as part of the GlobalSecondaryIndexes attribute in the response: \nhttp://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TableDescription.html#DDB-Type-TableDescription-GlobalSecondaryIndexes\nIt's possible that our documentation is out of date and/or incorrect with respect to the actual service response. I will look into this and respond back tomorrow. In any event, that data is either incorrectly missing or \"correctly\" omitting this data, so you can still feel free to create a post on the forums or contact support to request these values be made available if that is the correct behavior.\n. Thanks for following up @polythene1337. If it turns out that our docs are wrong I'll be sure to update.\n. Closing this as ProvisionedThroughput does indicate that the parameters should exist in DynamoDB's documentation, so this would be a service issue. You should follow up on the forums.\n. The SDK can attempt to handle redirected 307s (this is what the SDK is doing here) but it is not always reliable. The best way to handle this is to always set the correct region when working with buckets outside of the US standard region.\n. The SDK supports parallel requests. Everything in the SDK, and in fact, Node.js, is asynchronous, so you can dispatch multiple requests in parallel. If you can show some code reproducing your issue I can help you debug why the requests are getting sent serially. \nNote if requests are not being dispatched in parallel, this is possibly why you are also getting the \"Recursive nextTick\" warning. It would also explain why you are getting time skew errors. The SDK must send requests within 15 minutes of them being built. If the SDK is building all the requests up front in parallel but sending them serially, it's very likely the last of the requests will not send correctly.\n. Clients do not share any state between requests. Or should not, anyway. It would be very hard for them to do so, given that all request state is in the Request object.\n. I also am having trouble understanding, and executing, the code. When I run it, it only uploads the first 5 files and then stops.\nI rewrote your example using async.js, which is my preferred parallelization lib:\n``` javascript\nvar AWS = require('aws-sdk');\nvar async = require('async');\nAWS.config.logger = process.stdout;\nvar s3_client = new AWS.S3({params: {Bucket: 'bucket'}});\nfunction uploadFiles(files, maxUploads, done) {\n  async.eachLimit(files, maxUploads, function(file, next) {\n    s3_client.putObject({Key: 'key', Body: file}, next);\n  }, done);\n}\nvar files = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k'];\nuploadFiles(files, 5, function() { console.log(\"Done\"); });\n```\nIf you could reproduce the issue with the above code, that would certainly take out one variable.\nAs far as your code-- my original hunch stands: if Q is building the Request objects and only sending after, this would cause those old objects to be build too early.\nYou might also want to try increasing the maxSockets on your HTTP agent (default is 5). My guess is this is how the SDK could end up waiting on other files to be sent. You can do that like so:\n``` javascript\nvar AWS = require('aws-sdk');\nvar async = require('async');\nvar https = require('https');\nAWS.config.logger = process.stdout;\nvar agent = new https.Agent();\nagent.maxSockets = 20;\nvar s3_client = new AWS.S3({params: {Bucket: 'bucket'}, httpOptions: {agent: agent}});\nfunction uploadFiles(files, maxUploads, done) {\n  async.eachLimit(files, maxUploads, function(file, next) {\n    s3_client.putObject({Key: 'key', Body: file}, next);\n  }, done);\n}\nvar files = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k'];\nuploadFiles(files, 5, function() { console.log(\"Done\"); });\n```\n. @mhart and I are a hive-mind.\n. I would suggest that if you are going to send in parallel, you should make sure that maxSockets is set to at least the same size as the parallelization amount you are using. In this case, it should be at least 10 if you are uploading 10 at a time.\nI can look into other solutions in the SDK, but the only way we could fix this would be if we waited until the connection opened to sign the request. While this technically is a logical approach, it may be a backwards incompatible change. I will also talk to maintainers of our other SDKs to see how this issue might be handled there.\n. @my8bird just an FYI-- I just pushed a change that should allow the SDK to re-sign requests that were queued up for a long period of time. This should allow you to lower your maxSockets setting below your level of parallelism, if you wanted, and should generally provide a much more graceful experience with parallel requests.\n. I'm unable to reproduce this behavior. Using our test console (scripts/console in the SDK package):\njavascript\naws-sdk> rds.describeDBInstances()\n[AWS rds 200 0.466s 0 retries] describeDBInstances({})\n{ ResponseMetadata: { RequestId: 'f04abfbf-8931-11e3-922d-83efe754da79' },\n  DBInstances: [] }\nLooking at our description, this input parameter is not marked as required: \nhttps://github.com/aws/aws-sdk-js/blob/master/apis/rds-2013-09-09.json#L1518\nYou would see \"required\": true if it were.\n. Even though this is not a bug, I would be interested in knowing how you managed to produce this error, if you're willing to share code. It's possible that we can make it a little harder for users to shoot themselves in the foot here.\n. Thanks Brent. Not wasting our time at all. I actually think it could be useful to add the operation name to the error object we return. That would potentially make it easy to spot these errors. I will experiment a little bit with adding this in.\n. Can you provide more information about when you are getting this error? A stack trace would be helpful (printing err.stack).\n. Can you provide the stack trace for this exception?\n. I'm going to close this, then. If you find a way to provide steps to reproduce or an exception that is raised, feel free to re-open this or create a new issue. I'd be happy to look through and see what we could do about fixing it.\nGlad you got it working, though.\n. Unfortunately this patch won't actually work because we only ever create the sslAgent object one time on the first request. This means that if you adjust https.globalAgent.maxSockets mid-way through your program, you will get unexpected results-- that change will not be reflected in the SDK. I would rather not change the agent behavior to be less consistent. Fortunately there is a way to delegate the property with Object.defineProperty, so I will work on a patch based on yours that can properly delegate maxSockets (and other settings).\nThat said, note that you can configure maxSockets in the SDK, either by creating your own agent, or by passing the globalAgent into your client. The SDK has an httpOptions configuration value that accepts an agent object:\n``` js\n// use global agent\nAWS.config.httpOptions = { agent: require('https').globalAgent };\n// alternatively, create your own\nvar agent = new https.Agent();\nagent.maxSockets = 10;\n// optionally pass it into the client rather than globally\nvar s3 = new AWS.S3({httpOptions: {agent: agent}});\n```\nNote that there are reasons not to use the global agent, specifically in 0.8.x, where the rejectUnauthorized setting defaults to false, which opens you up to man-in-the-middle attacks. This is the reason the SDK does not use the globalAgent by default.\n. I added the above change to make the SDK respect the https.globalAgent.maxSockets setting through delegation so you should not have to pass in a custom agent at all. This will be available in the next 2.x RC release, but not 1.x, since this might not be backwards compatible.\n. Unfortunately this breaks our existing test suite. Can you explain a little more about the issue you are seeing? The SDK should already handle passing null through to ProviderId.\n. @seeekr thanks for the pull request illustrating the deficiency in parameter validation. I've taken your code and added some extra functionality to deal with null values passing through to serialization, and also supporting nulls on structs/maps/lists. In short, passing a null should now behave as if the property was not set, which is the intention of the guide documentation when it suggests setting ProviderId to null. \nNote that the current workaround would be to omit ProviderId entirely for Google+, or set it to undefined instead. We can also consider updating the documentation, if that makes it less confusing.\n. @seeekr keep in mind that for Google+ your WebIdentityToken has to use the authResponse.id_token, not the access_token like in other identity providers. Here's an example from fine-uploader: https://github.com/Widen/fine-uploader-examples/blob/master/src/s3-no-server/google-auth.js#L19-L22 -- that might be the error you were having.\n. I agree that we could be doing a better job documenting these integration points between identity providers. Improving this getting started experience is on our roadmap. Thanks for the feedback!\n. This does look like a regression. Thanks for pointing this out. In the meantime you can pass paramValidation: false to your global AWS.config or service configuration to workaround this error.\n. Thanks for reporting, @mlogan. The above fix will be part of the next release. Sorry for the headache!\n. The API definition lists Expected as a single structure element, not a list, so it looks like the definition is incorrect. It seems as though this issue is affecting our other SDKs too, see aws-sdk-ruby and aws-sdk-core-ruby. \n. On further inspection, it actually looks like the SimpleDB documentation might be wrong with respect to support for Expected (http://docs.aws.amazon.com/AmazonSimpleDB/latest/DeveloperGuide/SDB_API_PutAttributes.html). It looks like Expected does indeed only take 1 item as a structure, not a list. If I update the SDK to accept a list of Expected items, I get this response from the service:\n<Error>\n  <Code>MissingParameter</Code>\n  <Message>The request must contain the parameter Name</Message>\n  ...\n</Error>\nSo it seems like the API is described properly, but only supports a single Expected attribute. I will contact the SimpleDB team to confirm this and update their documentation if this is in fact the behavior they expose.\nFor reference, using a single Expected item works:\njs\naws-sdk> simpledb.putAttributes({DomainName:'domain',ItemName:'foo',Attributes: [{Name: 'x', Value: 'y'}], Expected: {Name: 'x', Exists: false}})\n[AWS simpledb 200 1.431s 0 retries] putAttributes({ DomainName: 'domain',\n  ItemName: 'foo',\n  Attributes: [ [Object], [length]: 1 ],\n  Expected: { Name: 'x', Exists: false } })\n{ ResponseMetadata: \n   { RequestId: '96eb4527-0bf8-aa9c-3100-5c970a3885be',\n     BoxUsage: '0.0000219909' } }\n. I agree with you there. Unfortunately there is not much we can do in the SDK about this, so I am going to close this here. If you are interested in this feature, I would recommend opening a thread in the Amazon SimpleDB forums to suggest an addition to the API.\n. I'm going to mark this as a duplicate of #134. Thanks for suggesting this. Follow along in the referenced issue for updates on improved CloudSearch support.\n. Thanks for bringing this up. As AWS services begin enabling support for CORS, these services (like CloudWatch) may be added to the default bundle. One thing we have to watch for is download size, so we evaluate each service as they enable CORS. We're also looking at an easier way to allow users to customize the services bundled in the SDK without affecting size. Watch this space.\n. http://sdk.amazonaws.com/js/aws-sdk-2.0.0-rc10.min.js should include Amazon Kinesis and Amazon CloudWatch. The way this is loaded in the final build will change slightly, but this should simplify things for now.\n. Thanks for this pull request. In the meantime, you can set the region after initialization as a workaround:\njs\nvar ses = new AWS.SES();\nses.config.region = 'eu-west-1';\n. Hey @jasonsims. I think the complication would come from the inability to cleanly make this work in the browser because of the private key signing, as well as the new crypto support required, as you point out in the link. We try to add features to the core SDK that could conceivably work both in the Node.js environment and in the browser, but there are some exceptions. This could be one of them.\nIn my opinion, there are two options here, saving the best for last:\n1. We add a getSignedUrl method for CloudFront and treat this as one of those \"exception\" cases that are not supported in the browser. There are arguments for and against, but if you were interested in packaging your code in the link up as a pull request, it looks like solid code we could use. If you do go this route, it would be great if you could remove the moment dependency (adding time delays in seconds should be easy enough to support without that lib).\n2. The easier path to get this out the door would be to take your code and package it up as a third party lib. This would allow you to: add features as you wish without being bound by our release cycle, use the dependencies you want (you could use moment if you wanted here), and finally document it as Node only without having to worry about browser support. There is a healthy ecosystem of small third-party utilities growing around the SDK (see here), and I think this would be a good addition. I also think this is a great way to experiment with new features without the burden of potentially having to support an API we don't like. And, if this library proves successful, stable, and heavily used, nothing is stopping us from merging it into the SDK at that point.\nEither one works, but I definitely think option 2 has some big wins. What do you think?\nPS. Thanks for the super detailed SO response!\n. This is actually related to #191. You can read there, but the summary is this is a limitation of CORS, and you must explicitly expose every header you want access to by name in your S3 CORS configuration. See that issue for how to do this. Unfortunately there isn't much we can do about this in the SDK, since this is a security policy enforced by the browser.\nWe could do a better job of documenting this, though.\n. @alexpearce92 you should not be using getObject() if you just want to access metadata. Use headObject() instead. Your data will be in the Metadata property of the response.\n. The SDK does not support IE9 or below. You can see the documentation here for a list of supported browsers.\nIt could be possible to work around that error by disabling parameter validation, which you can do by passing paramValidation: false to the service object constructor, but there will likely be other issues with the lack of full CORS support in IE versions prior to 10.\n. Can you provide code that reproduces this error? I am unable to reproduce with:\n``` js\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3({params: {Bucket: 'mybucket'}});\ns3.putObject({Key: 'foo.txt', Body: 'hello world'}, function() {\n  s3.deleteObject({Key: 'foo.txt'}, function(err,data) {\n    console.log(err, data);\n  });\n});\n```\nPrints:\nnull {}\n. I can't seem to reproduce your behavior with the exception being thrown out of the call. I run the following:\n``` js\ndelete process.env.AWS_ACCESS_KEY_ID;\ndelete process.env.AWS_SECRET_ACCESS_KEY;\nvar AWS = require('aws-sdk');\nconsole.log(AWS.VERSION);\nvar s3 = new AWS.S3();\ns3.listObjects({ Bucket: \"my-bucket\" }, function(err, objects){\n  console.log(err, objects);\n});\n```\nAnd I get this:\n2.0.0-rc11\n{ [TimeoutError: Could not load credentials from any providers]\n  message: 'Could not load credentials from any providers',\n  code: 'CredentialsError',\n  time: Tue Mar 11 2014 10:55:23 GMT-0700 (PDT),\n  originalError: \n   { message: 'Connection timed out after 1000ms',\n     code: 'TimeoutError',\n     time: Tue Mar 11 2014 10:55:23 GMT-0700 (PDT) },\n  _willRetry: false } null\nNote that it is being printed as part of the callback.\n. It looks like we are jumping to the uncaughtError state of our request lifecycle when an error pops up in the complete event. I will look into this.\n. Thanks for reporting this issue.\n. @danie11am I would recommend upgrading to the latest version of the SDK (rc20) as error handling has changed slightly to not throw these exceptions.\n. Thanks for bringing this up. Is there a specific issue with these dependency versions that is being reported? Unless there is a problem with one of these versions, it's unlikely that we will update. The last time we updated the xml2js dependency, we incurred a regression (#69), so we try to be conservative and only update when necessary.\n. This was acknowledged as a regression / breaking change in the API and a patch release was cut. Whether or not the library should be asynchronous is another discussion, but until that point it had not been. Feel free to read the root cause issue from that related ticket if you want to see the full details. For what it's worth, xml2js does no I/O, so it should not require asynchrony-- the actual \"asynchronous\" code that was added was a simple process.nextTick() deferral. As we move into building the browser version of the SDK, we want to avoid pulling in changes like process.nextTick that don't translate into the browser environment, so that would be an argument for not upgrading the xml2js library. We could switch over to parseStringSync, but right now our usage is not broken, and stability is our priority.\nI am going to close this for now. Unless there are issues with the versions that are causing bugs or performance issues that have been fixed in future releases, I don't see any advantage to upgrading either of these packages. Reading through the commits on both xml2js and xmlbuilder, I don't see any changes that affect our usage.\n. Is this inside of Firefox, by any chance? Or is this in Node?\n. Does this still happen if you leave \"charset=utf-8\" out of your ContentType parameter? Or perhaps if you use UTF-8 instead? Notice the case difference.\n. > But its strange that Firefox sending these headers works:\nThis is because we have special logic to handle the case for Firefox fixed in #190:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/services/s3.js#L62-L66\nIt looks like Chrome also has a special behavior in that it seems to only append charset if it is appended by the user.\nSomething you can do that will work across all browsers is avoid setting the charset, which actually should be set by the UserAgent anyway, not directly by the user.\n. Thanks for taking all this time to investigate the issue and open a PR. Unfortunately I seem to be getting very different results, and I'm unable to reproduce your behavior on any of the browsers I've tested. I wasn't able to test Android on my current setup, but I ran the following code across browsers on both Windows and OSX:\n``` html\n<!DOCTYPE html>\n\n\n\n",
    "mhart": "A big +1 from me and I agree with @jed and @mjijackson that there's no need to ship another promise implementation alongside the rest of the SDK. I've already run into some issues with the ways errors are transformed in the promise meaning there's no stack trace, etc.\nShould make your lives a lot easier not having to support it and being able to focus on awesome AWS features :smile_cat: \n. Woo! That was quick! And it fills me with happiness :koala: \n. Seems to work well in my testing (against DynamoDB).\nThe only thing I'd add is that the err objects should really be standard JS errors/exceptions instead of the {code: '', message: ''} structures they currently are (also mentioned in https://github.com/aws/aws-sdk-js/issues/4). Either the original exception as thrown/raised in the case it came from node.js or a 3rd party lib, or one using an Error constructor (or inherited) if you need to create it yourself.\nAlso, for those that don't know this, you can use npm to install this branch:\nnpm install [--save] 'git://github.com/aws/aws-sdk-js.git#node-callbacks'\n. :thumbsup: \n. I think this may have been closed a bit summarily. The code is still littered with usages of this custom {code: '', message: ''} structure and strings instead of real JavaScript exceptions (which makes it very hard to deal with errors when they arise - are they JavaScript Error objects, are they some custom object, are they strings, etc).\nPerhaps you want to extend the JS Error object and create your own AwsError or similar: http://www.devthought.com/2011/12/22/a-string-is-not-an-error/\nSome examples:\nhttps://github.com/aws/aws-sdk-js/blob/92c766fe7da7b3d8ebb3e219a014d98ce9c4d5dd/lib/xml/parser_xml2js.js#L56\nhttps://github.com/aws/aws-sdk-js/blob/92c766fe7da7b3d8ebb3e219a014d98ce9c4d5dd/lib/xml/parser_xml2js.js#L134\nhttps://github.com/aws/aws-sdk-js/blob/92c766fe7da7b3d8ebb3e219a014d98ce9c4d5dd/lib/xml/parser_xml2js.js#L166\nhttps://github.com/aws/aws-sdk-js/blob/4fa2d20b51656d5711abe4b1b72821ccd9d564b5/lib/query_client.js#L67\nhttps://github.com/aws/aws-sdk-js/blob/4fa2d20b51656d5711abe4b1b72821ccd9d564b5/lib/rest_xml_client.js#L107\nhttps://github.com/aws/aws-sdk-js/blob/8d32fd9240f408ca499a648dc885b1737d94901e/lib/services/ec2.js#L54\nhttps://github.com/aws/aws-sdk-js/blob/8d32fd9240f408ca499a648dc885b1737d94901e/lib/services/s3.js#L181\n. I think this may have been closed a bit summarily. The code is still littered with usages of this custom {code: '', message: ''} structure and strings instead of real JavaScript exceptions (which makes it very hard to deal with errors when they arise - are they JavaScript Error objects, are they some custom object, are they strings, etc).\nPerhaps you want to extend the JS Error object and create your own AwsError or similar: http://www.devthought.com/2011/12/22/a-string-is-not-an-error/\nSome examples:\nhttps://github.com/aws/aws-sdk-js/blob/92c766fe7da7b3d8ebb3e219a014d98ce9c4d5dd/lib/xml/parser_xml2js.js#L56\nhttps://github.com/aws/aws-sdk-js/blob/92c766fe7da7b3d8ebb3e219a014d98ce9c4d5dd/lib/xml/parser_xml2js.js#L134\nhttps://github.com/aws/aws-sdk-js/blob/92c766fe7da7b3d8ebb3e219a014d98ce9c4d5dd/lib/xml/parser_xml2js.js#L166\nhttps://github.com/aws/aws-sdk-js/blob/4fa2d20b51656d5711abe4b1b72821ccd9d564b5/lib/query_client.js#L67\nhttps://github.com/aws/aws-sdk-js/blob/4fa2d20b51656d5711abe4b1b72821ccd9d564b5/lib/rest_xml_client.js#L107\nhttps://github.com/aws/aws-sdk-js/blob/8d32fd9240f408ca499a648dc885b1737d94901e/lib/services/ec2.js#L54\nhttps://github.com/aws/aws-sdk-js/blob/8d32fd9240f408ca499a648dc885b1737d94901e/lib/services/s3.js#L181\n. Errors have been problematic for a while FWIW: See https://github.com/aws/aws-sdk-js/issues/4 and https://github.com/aws/aws-sdk-js/issues/307 (the discussions there may be germane to this issue)\n. Errors have been problematic for a while FWIW: See https://github.com/aws/aws-sdk-js/issues/4 and https://github.com/aws/aws-sdk-js/issues/307 (the discussions there may be germane to this issue)\n. @bigeasy I'm not quite sure why you need a global property set instead of just setting it on the error (ie, e._thrownByUser = true ... if (e._thrownByUser)?\nIn any case, I'm in agreement that the way aws-sdk handles exceptions is troublesome \u2013 but I've made my case on this before.\n. @bigeasy I'm not quite sure why you need a global property set instead of just setting it on the error (ie, e._thrownByUser = true ... if (e._thrownByUser)?\nIn any case, I'm in agreement that the way aws-sdk handles exceptions is troublesome \u2013 but I've made my case on this before.\n. Could it be that the date/times are out, messing up the signature? Make sure you've got your times sync'ed correctly on your system.\n. This looks like a likely culprit: https://github.com/aws/aws-sdk-js/commit/0f2e7232b92d15b6c4577e6cc3e5715387b4aa3f\n. (the datetime.substr(0, 8) call specifically - if that's cached, that will fail when the date changes - ie, at midnight)\n. Yeah - I think it's pretty straightforward - #169 should fix it\n. @bennett000 how are you instantiating your config? It's not a shell escaping issue is it? (ie, doing AWS_SECRET_ACCESS_KEY=asdf+asdf instead of AWS_SECRET_ACCESS_KEY=\"asdf+asdf\")\n. Just to see if it's an issue with aws-sdk or something in your setup, try using the aws4 lib as a sanity check.\nTry this (after an npm install aws4):\n``` js\nvar http = require('http'),\n    aws4 = require('aws4')\n// If you don't already have them in env variables:\nvar credentials = {accessKeyId: '', secretAccessKey: ''}\nvar opts = aws4.sign({\n  service: 'dynamodb',\n  headers: {\n    'Content-Type': 'application/x-amz-json-1.0',\n    'X-Amz-Target': 'DynamoDB_20120810.ListTables',\n  },\n  body: '{}'\n}, credentials)\nhttp.request(opts, function(res) { res.pipe(process.stdout) }).end(opts.body)\n```\nShould result in something like:\n{\"TableNames\":[]}\n(you can add a region param in the options you pass to aws4.sign if you want to use something other than us-east-1)\n. @bennett000 I suspect if it's happening with both the aws-sdk lib and the aws4 lib, then it's very likely to be something to do with your setup (which I guess you already suspect)...\nIf you ever figure out what it was, let us know!\n. Also check out whether dynalite works for you - if not, lemme know and I'll fix it up\n. Also check out whether dynalite works for you - if not, lemme know and I'll fix it up\n. Out of interest, do you get the error if you create the s3_client fresh for each putObject invocation?\nThat would at least clear up if there's some sort of state being held in it across requests.\n. Out of interest, do you get the error if you create the s3_client fresh for each putObject invocation?\nThat would at least clear up if there's some sort of state being held in it across requests.\n. You are making a new client? It looks from the code like you're reusing it.\n. I'm not sure I fully understand the code - I don't really know the Q library that well - but it sounds like it's got to do with the requests being created a while before they're sent (or at least, received by s3) - so I'm not sure if you're queueing them up too early, or if your bandwidth is really slow or something?\n. You could always test with another async library to see whether it's your usage of Q that might be getting in the way of things. Limited requests are pretty easy with the async module:\n``` js\nvar async = require('async')\nvar aws = require('aws-sdk')\nvar s3 = new aws.S3()\nvar fileBuffers = // ...\nvar maxUploads = // ...\nfunction putFile(file, cb) {\n  s3.putObject({Bucket: 'bucket', Body: file, Key: 'some random'}, cb)\n}\nasync.eachLimit(fileBuffers, maxUploads, putFile, console.error)\n``\n. Haha :link: \n. This is nodev0.10.29` btw\n. It's just very unusual in Node.js land for a client library to completely take over error handling like this. I certainly had no idea what was going on in my scripts for hours - they were just silently hanging and I had no idea why.\nI'm not sure why you're trying to do anything with thrown exceptions - shouldn't they just be allowed to bubble up like normal (unless the consumer has explicitly wrapped their code in a domain)? I'm guessing this has been discussed elsewhere perhaps?\n. Personally I think the best behaviour would be to not doing anything special with exceptions at all - as is pretty standard Node.js practice.\nThere's a good overview of best practices here:\nhttp://www.joyent.com/developers/node/design/errors\nUnless I'm missing something, the only exceptions you should be seeing thrown (apart from the obvious JSON.parse case) are as a result of programmer errors.\nIf you can point me to the relevant sections of the aws-sdk that are catching exceptions, and/or the discussions you've had around why there are problems with retries, I'll try to chime in with what might be a better way forward...?\nI can see on #176 you've said \"Bubbling the exception up will actually cause it to get bubbled up send lifecycle event handler, which makes the request get retried\" - so what happens if you just don't have this behaviour in the \"send lifecycle event handler\"? Just retry on callback errors, not thrown exceptions...? It seems very dangerous to be retrying on exceptions at all (ie, there could be resource leakage, etc).\nWadya reckon?\n. Argh, was just bitten by this again when I did ec2.waitFor('InstanceRunning', ... (as per the docs - will file a separate issue for this) and my whole app just silently froze. Looking into it, it seems to be aws-sdk throwing its own StateNotFoundError - but again, suuuuuuper hard to debug when the app just freezes with no output - no stack trace, nothing.\nAnyway, I'm obviously flogging a dead horse here \u2013 I know you're not 100% happy with the current situation either, but just want to reiterate my strong -1 to the current behaviour \u2013 especially for command-line apps which don't expect to run in domains or anything like that \u2013 it all just feels very un-Node-like.\n. I'm still unclear though... Why are operational errors being thrown? As opposed to just returned in callbacks?\n. \"it may be that plugin X throws a retryable error\"... Well that's just bad practice. Plugin X shouldn't be doing that, and shouldn't be allowed to do that. It should just be throwing for programmer errors.  \nSurely in these cases aws-sdk should be dictating how third-party plugins are expected to behave?\n. I'm not saying anything new here btw - this has been standard Node.js advice for quite a while now:\nhttps://groups.google.com/d/msg/nodejs/1ESsssIxrUU/5abyX25Dv2sJ\n. But JSON.parse is a pretty well known case that should be handled immediately by the consumer. I'd say that it should be doing this:\njs\nrequest.on('extractData', function(resp) {\n  try {\n    resp.data = JSON.parse(resp.httpResponse.body.toString());\n  } catch (e) {\n    request.emit('error', e) // Or give it more info\n  }\n});\n. I think it just comes down to me not understanding why aws-sdk is different from other Node.js modules in this regard? Especially other http client libraries (like request)?\nThe foolibrary example isn't particularly helpful because I don't really know what it is... Is it a module that aws-sdk is consuming? A well known module? Or is it a user-provided plugin?\nI'm just not sure what case you're trying to solve for here - have you got a more realistic example of the issues you've encountered?\n. > I'm not entirely sure why it matters what the library is? Is there a different expectation for \"well known\" modules? I would imagine that require('foo').parse(stuff) should have the exact same usage as JSON.parse(stuff) proper, from a functional perspective, well known or not. \nYeah, so JSON.parse is a function that is known to throw, and not just on programmer error \u2013 which is why best practice is to wrap it in try/catch. If another library has a well known use case like that (for example a parsing library), then I'd expect it to be wrapped in the same manner. To let these exceptions bubble up to some broader, higher level exception manager is just not how 99% of JavaScript libraries are written, so it's unexpected.\n\nHaving to know the entire closure of dependencies that each plugin uses in order to detect whether we need to wrap that plugin in a try/catch block would be an extremely high maintenance overhead\n\nI'm just having a hard time understanding this. aws-sdk has 3 external dependencies. Only 2 now that you've removed agentkeepalive. I've used xml2js quite extensively and it's very straightforward to manage parse errors with. So I can't imagine you're talking about the dependencies of aws-sdk... So what are you referring to? Plugins that are written for the aws-sdk? If that's the case, then aws-sdk completely dictates how they should be written... If people are writing plugins that just throw exceptions in random places, I don't understand why it would be the job of aws-sdk to manage them...?\nLet me try another tack: If you removed the current exception handling behaviour altogether in aws-sdk - if you just wrapped your JSON.parse and xml2js calls in try/catch blocks and returned or emitted the errors as is standard... what would happen? What would go wrong currently? What sort of issues would people post here on GitHub?\n. Well... I find that very unfortunate. I honestly just think you're making your lives a lot more complicated at the end of the day by trying to do things differently to everyone else. Having aws-sdk act differently to other Node.js modules just surprises the user.\nYou cut off my context with the fact that JSON.parse is known to throw and taking that to the nth degree - I said specifically not under programmer error \u2013 due to the fact that the parser is also acting as the validator. This whole idea you're pushing that JS libraries throw all these exceptions that are just operational errors just really doesn't hold water IMO. I certainly haven't seen them anyway - TBH synchronous parsers are the only case I can think of off hand.\nAgain, I really think we've got to move away from theoreticals and hypotheticals, it doesn't help much. So \u2013 has there actually been a case where a user plugin has caused issue here, or has the architecture been developed just in case that ever happens? If there were some concrete issues that have occurred, I really think it would make it easier to understand rather than arguments that boil down to \"we want to save programmers from doing stupid things... at the expense of others\".\n. Anyway anyway anyway, I've been writing too much on this too, clearly. I think we just disagree with the level of intervention that aws-sdk should be taking. For me, I find it frustrating, and it's doing things that I don't expect or understand. My vote's for not trying to do anything unusual with errors at all. But if that's not on the cards, then at least making sure they're not swallowed would be great!\n. If we want to get away from the broad issue of how to handle errors, and at least just focus on the specific issue I posted about \u2013 it boils down to not messing with callbacks that users have passed in.\nIf a user does this:\njs\nec2.describeInstances(function() {\n  throw new Error('wtf')\n})\nWhat good reason is there for aws-sdk to trap that exception?\nI think the answer to that (if we keep it specifically about this case) will reveal whether there's a good case to consider this behaviour a bug or not.\n. Perhaps it's just a white-list vs black-list case.\nMaybe assume all errors are terminal unless they match a certain condition? (eg, it has a retryable property, or whatever \u2013 in fact, doesn't the SDK already do this...?)\n. I'd also argue that once you're in the complete state (or any state where you're calling the user's callback) that you want to pass back control to the user, so shouldn't be trying to wrap anything at that point...\n. Great, thanks @lsegal !\n. Cool! So far so good! Everything seems to be working as expected for me... Just hope it's still working for everyone else :-)\n. I mean, the only thing that might make people confused is that the top of the stack is not actually where the user threw the error (because it's been caught and rethrown):\n/Users/michael/github/aws-sdk-js/lib/sequential_executor.js:234\n        throw err;\n              ^\nError\n    at Response.<anonymous> (/Users/michael/github/aws-sdk-js/test.js:5:9)\n    at Request.<anonymous> (/Users/michael/github/aws-sdk-js/lib/request.js:354:18)\n    ...\nBut apart from that minor point, all good :+1: \n. I think you can do it with Error.prepareStackTrace in v8 \u2013 whether you want to or not... can always experiment I guess\n. OK, no probs - it shouldn't be a big deal in any case I don't think - more cosmetic than anything\n. This is for aws-sdk: 2.0.3 and aws-sdk-apis: 3.0.5\n. Using agentkeepalive as described on its README seems to work fine (ie, the example exits ok) - but I guess there may be issues with more advanced usage...?\n. Hmmm, I didn't try https though\n. Just tried, their https example works fine too... Not sure what's going on!\n. Are you sure the instance is running by the time you query it with describeInstances?\n. This should work (sometime after runInstances)\n``` js\nvar instanceId = 'abcd'\nec2.waitFor('instanceRunning', {InstanceIds: [instanceId]}, function(err, data) {\n  if (err) return console.error(err)\nconsole.log('publicDnsName: ' + data.Reservations[0].Instances[0].PublicDnsName)\n})\n```\n. The SDKs typically are more low-level than the console tools \u2013 they tend to just reflect calls that exactly map to what's in the API itself: http://docs.aws.amazon.com/AWSEC2/latest/APIReference/\nSo they will return as soon as the API returns\n. In the underlying HTTP API, RunInstances will return to indicate that the request has been made successfully (and the instance will be in a pending state). DescribeInstances will just describe your instances at any point in time \u2013 so immediately after a call to RunInstances, your instance will typically still be pending.\nhttp://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-RunInstances.html\n\"When you launch an instance, it enters the pending state. After the instance is ready for you, it enters the running state.\"\nec2.waitFor is a utility added to aws-sdk-js that handles the polling of states for you, so you don't need to do it manually.\nBut if you did want to do it manually, then repeated calls to DescribeInstances should eventually show that your instance is running and has a public IP, etc.\n. Heh, Azure :smiling_imp: \nThis is the AWS SDK.\n. Seems to be an issue affecting many: https://github.com/joyent/node/issues/8894\nProbably best to wait for 0.10.35\n. From what I can tell @pcvisit it's not that it's broken, but rather there's a bug in the way you're calculating ContentLength \u2013 it should be in bytes, not characters.\nSo {ContentLength: content.length} is incorrect and will be buggy.\n{ContentLength: Buffer.byteLength(content)} is the correct way to do it if you ever need to.\n. Also, v0.11.16 was just released, so worth trying that (especially if there is an error and it's SSL or URL path related): http://blog.nodejs.org/2015/01/30/node-v0-11-16-unstable/\n. (just checked, works fine in v0.11.16 for me - but breaks in v0.11.15 - so I think it's a regression, perhaps in the path handling stuff)\n. No probs, I was as intrigued as you. Pretty bad regression - or just an unlucky edge case that the SDK was hitting.\n. Just as a sanity check \u2013 DynamoDB Local's not going to be the bottleneck here is it?\n. Because those numbers look pretty low... putItem at 75/sec...?\n. (FWIW, on my MB Air using https://github.com/mhart/dynalite I get 230/240 ops/sec using that benchmark)\n. Fair enough.\nYou can use the --ssl flag and connect using https (although you may need to ignore cert issues using https.globalAgent.rejectUnauthorized = false or whatever) \u2013 in case it's an https issue...\n. Have just tried with SSL (had to also use process.env.NODE_TLS_REJECT_UNAUTHORIZED = \"0\") \u2013 very little difference between 2.1.26 and 1.15.0 AFAICT.\n~153 ops/sec on 2.x, ~157 ops/sec on 1.x\n. Expected behaviour \u2013 your for loop is synchronous, so it will fire off all 100 getItem calls (which return almost immediately) before the next event loop ticks around (when the getItem callbacks will get called).\nIf you want to do this in series, then you need to wait until the callback is called in each getItem before calling another getItem. You can roll your own pattern for this, or just use something like https://github.com/caolan/async#eachSeries\n. For example, this will do what you want (if I understand what you want to achieve correctly):\n``` js\nfunction getAndPut(i) {\n  if (i >= 100) return // all done\n  dynamo.getItem(params, function(err, result) {\n    console.log('get item completed: ' + i)\n    if (!result.Item) {\n      dynamo.putItem(params, function(err, result) {\n        console.log('putItem completed: ' + i)\n        getAndPut(i + 1) // call the next getAndPut\n      })\n    }\n  })\n}\ngetAndPut(0)\n```\n. @doapp-jeremy well, that's still non-deterministic \u2013 that might just be due to the throttling on the http agent from node. There's no guarantees about order in the code you've posted.\nIf you do:\njs\nrequire('http').globalAgent.maxSockets = require('https').globalAgent.maxSockets = Infinity\nBefore you instantiate your DynamoDB client, that might change things.\n. (sorry, I'm assuming you're using node not the browser)\n. This also only happens in node 0.10.x and below btw. Node 0.12 and io.js shouldn't have this issue \u2013 so I'm assuming you're on 0.10.x...\n. I wish I had pull! Have been wanting the same thing. It's certainly possible to run io.js on Lambda, I've done this \u2013 you can include the binary in your function zip (or download it on the fly) and then spawn out to it \u2013 but it does take a good second or two to fire up.\nWould be great if it was an option.\n. A couple of things spring to mind:\n1. It can often take a while for the GC to kick in (ie, many minutes) \u2013 are you sure that nothing is being reclaimed?\n2. Depending on how many requests you've got in flight at any time, it could just be open sockets. Try doing this before your code and see if it helps:\njs\nrequire('http').globalAgent.maxSockets = 20\n(reason being that the default in 0.12 is infinite \u2013 so it could just be more and more sockets being created and not being reused)\n. Oh sorry, I can't tell if you're using https or not \u2013 so you might need to do this just to be sure:\njs\nrequire('http').globalAgent.maxSockets = require('https').globalAgent.maxSockets = 20\n. @TehNrd I think you might want to wait a bit longer checking your GC results when using a fixed number of agents (like in the code I posted)\nI see the heap total stabilise around 77MB when using a maxSockets of 10.\nRSS goes up to around 180MB but then drops again later to 94MB.\n. Check that it's not a ulimit restriction that's causing this \u2013 Mac's have low defaults for this. (Google \"ENOTFOUND ulimit\" for more info).\nMay be the same as #145 \nWhat version of node.js are you using?\n. ",
    "domenic": "FYI it's easy to have dual promises and callbacks with Q:\njs\nfunction callsBackOrReturnsPromiseForInput(input, cb) {\n    return Q.resolve(input).nodeify(cb);\n}\nIf you pass it a cb, it will call back with the usual err, result style, but otherwise it will return a promise. A real promise, that is, as @mjijackson points out.\n. Yes, the event-based API is pretty non-Nodey. (Well, actually, it's Node 0.1.x-ey.) Either use streams or (err, result) callbacks as the base. Then maybe use promises on top of that.\n. Yes, the event-based API is pretty non-Nodey. (Well, actually, it's Node 0.1.x-ey.) Either use streams or (err, result) callbacks as the base. Then maybe use promises on top of that.\n. As @ForbesLindesay says in his latest comment, the problem is your nonstandard promises, i.e. your event emitters with a pair of success/failure events. Since you've clarified you didn't mean those to be promises, the original goal of this issue is no longer relevant. Now the discussion has turned toward removing the pseudo-promise interface in favor of real streams or simply callbacks.\n. As @ForbesLindesay says in his latest comment, the problem is your nonstandard promises, i.e. your event emitters with a pair of success/failure events. Since you've clarified you didn't mean those to be promises, the original goal of this issue is no longer relevant. Now the discussion has turned toward removing the pseudo-promise interface in favor of real streams or simply callbacks.\n. @trevorrowe streams are allowed to and encouraged to have other properties. See for example every stream in node core.\n. @trevorrowe streams are allowed to and encouraged to have other properties. See for example every stream in node core.\n. @lsegal I am flabbergasted by the misunderstanding of streams that you are displaying here. Have you looked at the http module? The one that comes with node.js? Statements like\n\nThis is why we should not make the Request object a stream, because this would be insane. @trevorrowe pointed this out: Request objects are not, and should not, be Streams.\n\nor\n\nI understand that Streams are not inherently low level, but the Request object is inherently low level.\n\nor\n\nI also think it's important to take a step back and realize that the lifecycle of a request to AWS is much more than just an HTTP socket stream that you read and write from\n\nare laughable in the face of http.ServerRequest, for just one example. You might as well say \"a HTTP request is not just a TCP socket stream you read from\"---in both cases, you're missing the point entirely.\nIf you'd find it helpful, I can try to carve out time for a point-by-point rebuttal, but I think first it would be valuable if you familiarized yourself with the Node.js core library and the idioms embodied there.\nNote how the HTTP request, in addition to being a readable stream, has properties like method, url, headers, etc.\n\nYou think this is a stylistic difference, but this is actually a very deep one. You are creating an entirely new and bizarre abstraction in this \"request\" object, which behaves like no other request object in the Node.js ecosystem. It represents a network I/O operation, but is not a stream---this is a clear-cut WAT.\nIt is very accurate to say that you've provided a shim for streams on top of the library, since the core abstraction is some weird object that represents, in the end, your fundamental misunderstanding as to how I/O and network abstractions in Node work, and streams are just something you can get by adapting that weird object via the shim method.\n@ForbesLindesay is right that, if you guys are displaying this level of deep misunderstanding of streams in specific and the Node.js ecosystem in general, moving on to other libraries whose authors actually understand is going to be the right move.\n. @lsegal I am flabbergasted by the misunderstanding of streams that you are displaying here. Have you looked at the http module? The one that comes with node.js? Statements like\n\nThis is why we should not make the Request object a stream, because this would be insane. @trevorrowe pointed this out: Request objects are not, and should not, be Streams.\n\nor\n\nI understand that Streams are not inherently low level, but the Request object is inherently low level.\n\nor\n\nI also think it's important to take a step back and realize that the lifecycle of a request to AWS is much more than just an HTTP socket stream that you read and write from\n\nare laughable in the face of http.ServerRequest, for just one example. You might as well say \"a HTTP request is not just a TCP socket stream you read from\"---in both cases, you're missing the point entirely.\nIf you'd find it helpful, I can try to carve out time for a point-by-point rebuttal, but I think first it would be valuable if you familiarized yourself with the Node.js core library and the idioms embodied there.\nNote how the HTTP request, in addition to being a readable stream, has properties like method, url, headers, etc.\n\nYou think this is a stylistic difference, but this is actually a very deep one. You are creating an entirely new and bizarre abstraction in this \"request\" object, which behaves like no other request object in the Node.js ecosystem. It represents a network I/O operation, but is not a stream---this is a clear-cut WAT.\nIt is very accurate to say that you've provided a shim for streams on top of the library, since the core abstraction is some weird object that represents, in the end, your fundamental misunderstanding as to how I/O and network abstractions in Node work, and streams are just something you can get by adapting that weird object via the shim method.\n@ForbesLindesay is right that, if you guys are displaying this level of deep misunderstanding of streams in specific and the Node.js ecosystem in general, moving on to other libraries whose authors actually understand is going to be the right move.\n. It sounds like part of the problem is you're trying to use the same abstraction for all AWS APIs, instead of using different ones for S3 and for others.\n. It sounds like part of the problem is you're trying to use the same abstraction for all AWS APIs, instead of using different ones for S3 and for others.\n. @trevorrowe that does look quite reasonable. And your concerns about 1-1 mapping of API operations and parameters are sensible, although I am not sure that necessitates having the same return type for every operation.\n. @trevorrowe that does look quite reasonable. And your concerns about 1-1 mapping of API operations and parameters are sensible, although I am not sure that necessitates having the same return type for every operation.\n. @Termina1 thankfully knox has got you covered.\n. @lsegal is there any way you could get the S3 team to support chunked transfer-encoding, so we don't have to do silly extra steps to discover the content length? That would be... amazing.\nNote that multipart upload doesn't work for this, since it only works for files 5 MB and larger.\n. ",
    "mikemaccana": "The existing AWS-lib module in npm already uses err first, and I can confirm it works very nicely with async.waterfall() for very common AWS tasks. \nEg, in my existing aws-lib code (which I'll probably move to the official library) I make a launch config, then an autoscale groups, then tag an autoscale group, then make a scaling policy, then make a metric alarm, then wait for the DNS record to appear. err-first is essential for tracking things in this kind of workflow.\n. ",
    "TomFrost": "Agreed with the above -- a promise implementation would generally be considered clutter in modern node apps.  I'm definitely looking forward to refactoring my projects to depend on this, once the structure follows node convention and works with the various flow control libraries out there!\n. ",
    "trevorrowe": "I wanted to chime in and say you guys are awesome.  Thank you for all of the feedback.  I'm making changes to a branch that I hope to share soon.  I will update here when I do.\n. We've pushed a few updates to the branch \"node-callbacks\" (https://github.com/aws/aws-sdk-js/tree/node-callbacks).  In this branch:\n- all client methods accept a callback function as the last argument, as per node convention\n- the first argument (params) is now optional and defaults to {}\n- updated documentation examples\n``` js\n// without params\ns3.client.listBuckets(function (err, data) {\n  console.log('err', err);\n  console.log('data', data);\n});\n// with params\ns3.client.headObject({Bucket:'bucket', Key:'key'}, function (err, data) {\n  console.log('err', err);\n  console.log('data', data);\n});\n```\nNOTE: you can still register callbacks on the returned AWSRequest object.  This is especially important if you need access to the data event (e.g. when streaming files from S3).\njs\nreq = s3.client.getObject({Bucket:'bucket', Key:'key'})\nreq.data(function (chunk) {\n  // do something with the chunk of data\n}).send();\nIf you do not use the node-style callback function when calling the operation, you must call send() on the returned request.\nWe still want to change the format of how you register event callbacks on the request object to match node's EventEmitter API style.\nPlease give it a spin and send in feedback!\nFYI, to install the node-callbacks branch:\nnpm install [--save] 'git://github.com/aws/aws-sdk-js.git#node-callbacks'\n. This is a known omission at this point.  We will definitely be adding proper support for working with binary data.\n. Fixed in 146b643 and released with 0.9.1-pre.2.\n. Fixed in 146b643 and released with 0.9.1-pre.2.\n. There is a strong pattern across AWS services where the services may respond to a request with an error \"code\" and \"message\".  The code is always a short string and the message is a longer description, often with details.  When I call the EC2 DescribeInstances operation I might get an error back (in XML) that has the following attributes:\n- code: InvalidInstanceID.NotFound\n- message: \"The instance ID 'i-12345678' does not exist\"\nThe original intent was to limit the number of attributes a user needed to check on errors returned by the SDK.  If errors generated by the SDK (or its dependency) mimicked those returned by the services then error handling could be generalized to a point.  \nThat said, I see the issue where these object do not generate stack traces and can actually make error handling more difficult.  At this point, I think standardizing on An error class (like AWS.Error) that extends Error makes good sense.\n. There is a strong pattern across AWS services where the services may respond to a request with an error \"code\" and \"message\".  The code is always a short string and the message is a longer description, often with details.  When I call the EC2 DescribeInstances operation I might get an error back (in XML) that has the following attributes:\n- code: InvalidInstanceID.NotFound\n- message: \"The instance ID 'i-12345678' does not exist\"\nThe original intent was to limit the number of attributes a user needed to check on errors returned by the SDK.  If errors generated by the SDK (or its dependency) mimicked those returned by the services then error handling could be generalized to a point.  \nThat said, I see the issue where these object do not generate stack traces and can actually make error handling more difficult.  At this point, I think standardizing on An error class (like AWS.Error) that extends Error makes good sense.\n. Thank you for pointing this out.  This seems to be an issue with the generated service description where some of the input names are not in the correct case (they are lower-camel-cased instead of upper-camel-cased).  Let me see if I can't create a fix for the other operations as well.\n. Thank you for pointing this out.  This seems to be an issue with the generated service description where some of the input names are not in the correct case (they are lower-camel-cased instead of upper-camel-cased).  Let me see if I can't create a fix for the other operations as well.\n. I just pushed a commit (0a51cd6029) that should address this issue.  I updated the Amazon EC2 API version, and corrected the input parameter case rules.  Let me know if this resolves your issue.\n. I just pushed a commit (0a51cd6029) that should address this issue.  I updated the Amazon EC2 API version, and corrected the input parameter case rules.  Let me know if this resolves your issue.\n. Fixed in 0a51cd6 and released.\n. Fixed in 0a51cd6 and released.\n. I've pushed the fix to master.  I will be also pushing new docs that correct the description of the returned data.  This affected 6 other operations.\n. I've pushed the fix to master.  I will be also pushing new docs that correct the description of the returned data.  This affected 6 other operations.\n. @domenic AWS has only a pair of operations (S3 GetObject and Glacier GetJobOutput) that return raw data as the http response payload.  The current event-based API exists to provide access to everything we get back from AWS.  It would be very simple to wrap the returned request and create a read stream from it.\njs\nstream = s3.getObject(params).createReadStream();\nOr are you looking for something like this?\njs\ns3.getObject(params, function(err, stream) {\n  // ...\n});\nI want to make sure I understand correctly what it is you are looking for.  We purposely have not jumped in and created any higher level abstractions around streams yet. \n. @domenic AWS has only a pair of operations (S3 GetObject and Glacier GetJobOutput) that return raw data as the http response payload.  The current event-based API exists to provide access to everything we get back from AWS.  It would be very simple to wrap the returned request and create a read stream from it.\njs\nstream = s3.getObject(params).createReadStream();\nOr are you looking for something like this?\njs\ns3.getObject(params, function(err, stream) {\n  // ...\n});\nI want to make sure I understand correctly what it is you are looking for.  We purposely have not jumped in and created any higher level abstractions around streams yet. \n. @ForbesLindesay I see the value of s3.client.getObject returning a stream.  That said, what if I also want access to other relevant information available from the response (e.g. content type, etag, metadata, etc).  What is wrong with having the req object define a method that returns a stream?\njs\nreq = s3.client.getObject(params);\nreq.on('success', function(resp) {\n  // access interesting attributes about the object returned\n  resp.data.Etag;\n  resp.data.ContentType;\n  resp.data.Metadata['custom-attribute']; //\n});\nstream = req.createStream();\nIf you don't care about the other object attributes you could simply:\njs\ns3.client.getObject(params).createStream().pipe(...)\n. @ForbesLindesay I see the value of s3.client.getObject returning a stream.  That said, what if I also want access to other relevant information available from the response (e.g. content type, etag, metadata, etc).  What is wrong with having the req object define a method that returns a stream?\njs\nreq = s3.client.getObject(params);\nreq.on('success', function(resp) {\n  // access interesting attributes about the object returned\n  resp.data.Etag;\n  resp.data.ContentType;\n  resp.data.Metadata['custom-attribute']; //\n});\nstream = req.createStream();\nIf you don't care about the other object attributes you could simply:\njs\ns3.client.getObject(params).createStream().pipe(...)\n. @ForbesLindesay  The request is not a stream object, it is a request object.  It carries context of the request parameters, the http request headers, uri, payload, endpoint, etc.  I think it would be better to create a readable stream from the request object.  Here is a simple example:\n``` js\nAWS.Request.prototype.createReadableStream = function() {\n  s = new require('stream');\n  stream = new s.Stream();\n  stream.readable = true;\nvar req = this;\n  stream.on = function(e, callback) {\n    req.on(eventName, callback);\n  });\nthis.on('httpData', function(data) {\n    stream.emit('data', data);\n  });\nthis.on('httpDone', function(data) {\n    stream.emit('end');\n    stream.emit('close');\n    stream.readable = false;\n  });\nthis.on('error', function(err) {\n    stream.emit('error', err);\n  });\nthis.send();\nreturn stream;\n};\n```\nUsing the above addition you could do something very similar to you example:\njs\nstream = s3.client.getObject(params).createReadableStream();\nstream.pipe(require('fs').createWriteStream('./output'));\nIf you want access to the \"other response data\" you could do this instead:\n``` js\nreq = s3.client.getObject(params);\nreq.on('success', function(resp) {\n  // access interesting attributes about the object returned\n  resp.data.Etag;\n  resp.data.ContentType;\n  resp.data.Metadata;\n});\n// now get a stream from the request\nreq.createReadableStream().pipe(('fs').createWriteStream('./output'));\n```\nThis could be expanded to support creating writable streams for operations like S3 PutObject.\nI think the primary difference between this example and the proposed API you gave is that the pair of AWS operations (S3 GetObject and Glacier GetJobOutput) would instead return the streams directly instead of the request object.  Is this correct?  Thoughts?\n. @ForbesLindesay  The request is not a stream object, it is a request object.  It carries context of the request parameters, the http request headers, uri, payload, endpoint, etc.  I think it would be better to create a readable stream from the request object.  Here is a simple example:\n``` js\nAWS.Request.prototype.createReadableStream = function() {\n  s = new require('stream');\n  stream = new s.Stream();\n  stream.readable = true;\nvar req = this;\n  stream.on = function(e, callback) {\n    req.on(eventName, callback);\n  });\nthis.on('httpData', function(data) {\n    stream.emit('data', data);\n  });\nthis.on('httpDone', function(data) {\n    stream.emit('end');\n    stream.emit('close');\n    stream.readable = false;\n  });\nthis.on('error', function(err) {\n    stream.emit('error', err);\n  });\nthis.send();\nreturn stream;\n};\n```\nUsing the above addition you could do something very similar to you example:\njs\nstream = s3.client.getObject(params).createReadableStream();\nstream.pipe(require('fs').createWriteStream('./output'));\nIf you want access to the \"other response data\" you could do this instead:\n``` js\nreq = s3.client.getObject(params);\nreq.on('success', function(resp) {\n  // access interesting attributes about the object returned\n  resp.data.Etag;\n  resp.data.ContentType;\n  resp.data.Metadata;\n});\n// now get a stream from the request\nreq.createReadableStream().pipe(('fs').createWriteStream('./output'));\n```\nThis could be expanded to support creating writable streams for operations like S3 PutObject.\nI think the primary difference between this example and the proposed API you gave is that the pair of AWS operations (S3 GetObject and Glacier GetJobOutput) would instead return the streams directly instead of the request object.  Is this correct?  Thoughts?\n. As @domenic pointed out, the request is not intended to be a promise.  It is a low-level representation of a request object.  It can be used to construct a promise, create a stream, etc.  If you do not need access to the low-level events emitted by request (validate, build, sign, send, httpHeaders, httpData, HttpDone, retry, error, success, complete) then it is possible to pass a traditional node callback function to the operations.  \nAny similarities between the events emitted from request to those emitted by a promise object should be ignored.\n. As @domenic pointed out, the request is not intended to be a promise.  It is a low-level representation of a request object.  It can be used to construct a promise, create a stream, etc.  If you do not need access to the low-level events emitted by request (validate, build, sign, send, httpHeaders, httpData, HttpDone, retry, error, success, complete) then it is possible to pass a traditional node callback function to the operations.  \nAny similarities between the events emitted from request to those emitted by a promise object should be ignored.\n. @domenic I agree.  Currently we are trying to use the same abstractions for all AWS operations at the client level.  Please notice that we have grouped API operations for each service into its own Client class.  The client class is nested beneath the service class (e.g. AWS.S3.Client, AWS.EC2.Client).  This was done purposefully so higher level abstractions for each service could be latter added to the service class.  \nThis intentional grouping and nesting is why you need to do the following:\n``` js\ns3 = new AWS.S3();\ns3.client.someOperation(params)\n// instead of simply\ns3.someOperation(params)\n```\nWould everyone's concerns be eliminated if this existed?\njs\ns3 = new AWS.S3();\ns3.upload(params).pipe(...) // internally uses s3.client.putObject\ns3.download(params).pipe(...) // internally uses s3.client.getObject\nPlease note, in the example above the methods names could be anything that makes sense.  The suggested upload and download are arbitrary.\nI want to stress.  There are currently only 3 streaming upload operations (S3 PutObject, UploadPart and Glacier UploadArchive) and only 2 streaming download operations (S3 GetObject and Glacier GetJobOutput).  The hundreds and hundreds of other API requests would not be well served as a Stream object.  We feel there is value in consistency at the low-level (i.e. Client class) where all of the methods have the same return type.\nThe one-to-one mapping of API operations and their parameters is an explicit design choice.  This allows us to rapidly update the API configuration for each service client keeping them lock step with the service updates.   During this developer preview we plan to expand the coverage of low-level clients across more services.  Are we interested in higher level abstractions? Absolutely yes.  We simply have not spent many cycles on them.\n. @domenic I agree.  Currently we are trying to use the same abstractions for all AWS operations at the client level.  Please notice that we have grouped API operations for each service into its own Client class.  The client class is nested beneath the service class (e.g. AWS.S3.Client, AWS.EC2.Client).  This was done purposefully so higher level abstractions for each service could be latter added to the service class.  \nThis intentional grouping and nesting is why you need to do the following:\n``` js\ns3 = new AWS.S3();\ns3.client.someOperation(params)\n// instead of simply\ns3.someOperation(params)\n```\nWould everyone's concerns be eliminated if this existed?\njs\ns3 = new AWS.S3();\ns3.upload(params).pipe(...) // internally uses s3.client.putObject\ns3.download(params).pipe(...) // internally uses s3.client.getObject\nPlease note, in the example above the methods names could be anything that makes sense.  The suggested upload and download are arbitrary.\nI want to stress.  There are currently only 3 streaming upload operations (S3 PutObject, UploadPart and Glacier UploadArchive) and only 2 streaming download operations (S3 GetObject and Glacier GetJobOutput).  The hundreds and hundreds of other API requests would not be well served as a Stream object.  We feel there is value in consistency at the low-level (i.e. Client class) where all of the methods have the same return type.\nThe one-to-one mapping of API operations and their parameters is an explicit design choice.  This allows us to rapidly update the API configuration for each service client keeping them lock step with the service updates.   During this developer preview we plan to expand the coverage of low-level clients across more services.  Are we interested in higher level abstractions? Absolutely yes.  We simply have not spent many cycles on them.\n. I just pushed f345ce8 which addresses these issue.  You should be able to remove your local modifications now.  The member ('m') rule is now optional for lists and the output rules ('o') is now optional for operations.  \nOmitting the member rule from lists now treats the input as a simple list of strings. Omitting the output rules for XML response now parses them with default conversion logic.\n. I just pushed f345ce8 which addresses these issue.  You should be able to remove your local modifications now.  The member ('m') rule is now optional for lists and the output rules ('o') is now optional for operations.  \nOmitting the member rule from lists now treats the input as a simple list of strings. Omitting the output rules for XML response now parses them with default conversion logic.\n. The missing headers should have been addressed in commit 362ed44955ac759.   I was able to use the latest (unreleased) code to perform a copy object.  This should go out with our next release.\n. The missing headers should have been addressed in commit 362ed44955ac759.   I was able to use the latest (unreleased) code to perform a copy object.  This should go out with our next release.\n. Thank you for reporting this issue.  It was an issue with additional operations (operations that have flattened list members in the response/output).\n. Thank you for reporting this issue.  It was an issue with additional operations (operations that have flattened list members in the response/output).\n. Sorry for the slow response.  I can verify this is an issue.  It looks like the service description does not model the Content-MD5 header for this operation (that should be calculated on the generated XML).  It should be a pretty straight forward fix.\n. Sorry for the slow response.  I can verify this is an issue.  It looks like the service description does not model the Content-MD5 header for this operation (that should be calculated on the generated XML).  It should be a pretty straight forward fix.\n. I was able to reproduce the issue and create a fix.  Thanks for the bug report!\n. I aggree, it would be redundant.  Consider the following:\n``` js\ncurrently\nAWS.config.accessKeyId\nproposed\nAWS.config.AWSAccessKeyId\n```\nI think we will probably just leave this as this.  Thank you for your input.\n. From the DynamoDB API docs (http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/API_UpdateItem.html):\njs\n{\"TableName\":\"Table1\",\n    \"Key\":\n        {\"HashKeyElement\":{\"S\":\"AttributeValue1\"},\n        \"RangeKeyElement\":{\"N\":\"AttributeValue2\"}},\n    \"AttributeUpdates\":{\"AttributeName3\":{\"Value\":{\"S\":\"AttributeValue3_New\"},\"Action\":\"PUT\"}},\n    \"Expected\":{\"AttributeName3\":{\"Value\":{\"S\":\"AttributeValue3_Current\"}}},\n    \"ReturnValues\":\"ReturnValuesConstant\"\n}\nYou should be passing the HashKeyElement type and value but not name.  Try this:\njs\n{\n  \"TableName\": \"my_table_name\",\n  \"Key\": {\n    \"HashKeyElement\": { \"S\": \"5:15\" }  // the hash key element type and value w/out name\n  },\n  \"AttributeUpdates\": {\n    // this would add the attribute \"4\" with the string \"a string of text\" to the item with the hash key \"5:15\"\n    \"4\": {\n      \"Value\": { \"S\": \"a string of text\" },\n      \"Action\": \"ADD\"\n    }\n  }\n}\nCurrently we do not validate the request parameters before serializing and sending the request.  We plan to add param validation prior to serializing and sending the request so we can catch these formatting issues client side and provide helpful errors about where in the request params the mistake is.\n. The SDK does not currently validate request parameters client-side before making the request (this should be added before GA release).  \nYou can see the docs for the Amazon DynamoDB GetItem request here:: http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/API_GetItem.html\nThe issue appears to be that you omitted the type information from your hash key element.  Try this:\njs\ndynamo.getItem({\n  TableName: \"table-name\",\n  Key: {\n    HashKeyElement: { S:\"test-item\" },\n  }\n}, function(err, data) {\n  // ...\n});\nPlease note, the type should reflect the type of the value (e.g. 'S' for string, 'N' for number, 'B' for base-64 encoded binary data).\n. We pushed a release late monday night that I believe addresses this missing functionality.  Could you update to the latest version of the SDK and verify this issue is resolved (or not)?\nThe S3 API configuration for putBucketWebsite (https://github.com/aws/aws-sdk-js/blob/master/lib/services/s3.api.js#L2135) contains the routing rules (https://github.com/aws/aws-sdk-js/blob/master/lib/services/s3.api.js#L2175).  You can also see them in the documentation (http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3/Client.html#putBucketWebsite-property).  \nIf it does not work, then there would be some bug/issue that needs to be resolved.\n. The API configuration for the S3 client correctly indicates the response data should have a Policy property that should be sourced from the HTTP response body.  It appears the RestXml service interface is not looking at this portion of the API configuration and therefore falls back onto the default XML parsing.  \nThis should not be a difficult fix.\n. This is definitely on our todo list.  I can't comment on when this will get done exactly, but it is a priority.\n. I just pushed a fix that resolved an issue related to ReceiveMessage.  You should be able to get the attributes now like this:\njs\nsqs.client.receiveMessage({QueueUrl:'https://...', AttributeNames:['All']}, function(err, data) {\n  console.log(data.Messages[0].Attributes);\n});\nPlease note, you must pass the AttributeNames param if you want to get the attributes with the response.\n. If you rename \"Value\" to \"Values\" in your filter, then it will work.\nI have on the backlog a task to add parameter validation that would catch errors like this and give more meaningful error messages back.\n. Looks good.  We should add a backlog task to eliminate the need for the payload attribut on input/output rules, but this looks good.\n. Thanks for bringing this issue to our attention and for the pull request.  I just pushed a commit that resolves this issue for all three serializers and makes them accept a wider variety of date formats as inputs.  Please let us know if this doesn't resolve the issue for you.\n. Looks great, good work!\n. Thanks for the fix!\n. Look good!\n. The queue url is not a parameter as much as it is an endpoint.  That said, it seems silly to require a user to construct a new SQS service interface for each queue they want to operate against.  This would be the case if we required the queue url up front as the service endpoint.  It makes more sense from user's perspective to provide the url as a runtime parameter.\nIn the DeleteQueue API docs, they mention the \"queue URL\" and give the example with it bound in the HTTP request as the scheme/host/path.\nhttp://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/Query_QueryDeleteQueue.html\n. I hope this helps, let me know if this doesn't resolve the issue.\n. @southpolesteve I've attempted to make the API call to get a resource while passing the querystring embed=true and I haven't seen anything different come across the wire in the response. Can you give an example of what you get back when using the API directly?\n. @southpolesteve Thanks for the tip. It looks like this can be addressed by the SDK by adding the ability to specify a list of resources to embed via the querystring. Using embed=methods I was able to produce what you are expecting. Some minor SDK updates are required for this to work.\n. Glad to hear you resolved your issue.. ",
    "reconbot": "+1\n. This would be very helpful for elastic search service queries too.. This would be very helpful for elastic search service queries too.. https://github.com/mhart/aws4 is current a gold standard, couldn't run production without it. https://github.com/mhart/aws4 is current a gold standard, couldn't run production without it. ",
    "vanb": "+1\n. ",
    "syahmeds": "Suppose the download takes longer than 5000 ms, does the connection get aborted? It doesn't seem to work that way. Is there a way to do that?. ",
    "datfinesoul": "I agree with @syahmeds, I'm doing a test, setting the timeout to 1ms, but it takes anywhere from 10-20 seconds to get a \"Connection timed out after 1ms\" message back.  It would be nice if the connection gets aborted when the timeout is reached, or if httpOptions.timeout is not the proper way to get the connection terminated, is there another option I'm not seeing that does abort the connection?. I agree with @syahmeds, I'm doing a test, setting the timeout to 1ms, but it takes anywhere from 10-20 seconds to get a \"Connection timed out after 1ms\" message back.  It would be nice if the connection gets aborted when the timeout is reached, or if httpOptions.timeout is not the proper way to get the connection terminated, is there another option I'm not seeing that does abort the connection?. ",
    "bhudgens": "We identified these delays in the retry logic and not in the http session.  Setting \"maxRetries: 1\" as a test should result in the behavior expected.  Docs here. ",
    "kyleroche": "Thanks for the quick reply... same error: \nTypeError: Not a string or buffer\ndb = new AWS.DynamoDB()\n    # db = new AWS.DynamoDB()\n    db.client.listTables (err, data) ->\n      console.log data.TableNames\n. Thanks for the quick reply... same error: \nTypeError: Not a string or buffer\ndb = new AWS.DynamoDB()\n    # db = new AWS.DynamoDB()\n    db.client.listTables (err, data) ->\n      console.log data.TableNames\n. actually, just this will cause it: \ndb = new AWS.DynamoDB()\ndb.client.listTables (err, data) ->\n    # nothing\n. actually, just this will cause it: \ndb = new AWS.DynamoDB()\ndb.client.listTables (err, data) ->\n    # nothing\n. i tried logging err and data alone as well. even w/ an empty function i am getting the error. \n. i tried logging err and data alone as well. even w/ an empty function i am getting the error. \n. AWS: 0.9.1-pre.2\n. AWS: 0.9.1-pre.2\n. Region config fixed it. Thanks for the quick reply\n. Region config fixed it. Thanks for the quick reply\n. ",
    "sebs": "I still dont get the Error. It's especially bugging me that I use a copy pasted example of the website \nhttp://aws.amazon.com/de/sdkfornodejs/\n. ",
    "ForbesLindesay": "An event emitter that emits success and error events is still just a really degenerate promise.  You just have an even more different API.  I'd suggest depending on one of the existing promises-a+ compliant libraries:\n- promise\n- Q\n- rsvp\n- avow\n- when\n- promeso\n- legendary\n- vow\nMost of those libraries are tiny and provide really simple APIs to let you build non-degenerate promises out of the requests you're already making.  I can't see anybody wanting to use the event API as it stands.  If you don't want to support promises you might as well just support callbacks, because the only use I see for the event based API is using it to build a real promise.  You can see an example of this being done in dynostore (the database layer API for jepso-ci)\n. I'm not asking you to support choosing something specific and opinionated, I'm asking you to adopt a standard.  I realize you support callbacks already, I'm simply making the point that 'success' events don't add anything.\nIf you use any of the aforementioned libraries, the resulting promises can be assimilated by any of the other libraries.\ne.g. if there was an aws function getFromAWS() which returned a promise (using any of the above libraries) then:\njavascript\nvar res = promise()\n  .then(function () {\n    return getFromAWS();\n  });\nres is then a promise of the same type as that returned by promise(), but for the result of the AWS service function. So you can do:\njavascript\nvar promise = require('my-promise-library-that-conforms-to-promises-a-plus');\nvar res = promise()\n  .then(function () {\n    return getQpromise('foo');\n  })\n  .then(function (foo) {\n    return getWhenPromise(foo);\n  })\n  .then(function (bar) {\n    return getVowPromise(bar);\n  })\n  .then(function (url) {\n    return getFromAWS(url);\n  });\nres would then be a promise of the type used by 'my-promise-library-that-conforms-to-promises-a-plus' for the result of that chain of asyncronous operations.  They also all play nicely in terms of helper functions like all:\njavascript\nQ.all([getFromAWS(url), getFromVowsAPI(url2), getFromWhenAPI(url3)])\n  .spread(function (res1, res2, res3) {\n    //use the result of 3 different types of promise here\n  });\nA lot of effort went into producing a promises/A+ spec that everyone can agree on, and it needs to be adopted at this level in addition to being used in promise libraries.\n. ",
    "Raynos": "To reinforce.\nIf you have an api that returns an object which conceptually represents a stream, i.e. it asynchronously gives you data in multiple chunks, where each chunk is a Buffer or string and eventually terminates with some kind of end symbol then it should be an instance of require(\"stream\").\nOr if your api returns an object which conceptually you can apply back pressure to then it should also be a stream\nIf you have a conceptual stream of non-Buffer / string data then your allowed to use what you deem to be a more suitable abstraction if you wish (i.e. gozala/reducers)\nIt should be noted that an event emitter with arbitary events is not a more suitable abstraction.\nMy personal objection to your API not being a stream is that:\n- your custom events are not documented\n- it is trivial to infer how to read your data if I inspect an AWSRequest instance and see a pipe method. \n- common composable abstractions exist for a reason. I have a tool chain of high order functions that work cleanly with (err, data) callbacks and streams.\n- Writing a wrapper that makes your module a valid stream is silly and fragments the aws developer community. I will do it if necessary\nIt should also be noted that a createStream() method is a reasonable enough compromise although unnecessary.\n. @lsegal \nIn the case of S3 I'm most likely reading a remote file and only care about the file. So I would expect an API very similar to fs.\nFor the rest of your APIs I can't comment other then to say that if it does back pressure it should probably be a node stream.\n. ",
    "Termina1": "Actually, it is very frustrating that I can't use streaming for uploading files to S3.\n. @domenic Thanks!\n. ",
    "mitar": "So are streams supported for getObject or are they not? I am not sure what is current state of this?\n. No. Requester Pays buckets are buckets where a person who does getObject pays for it.\n. I made a simple patch which makes it work for me.\n. Aha, good, a workaround. :-) Thanks.\n. This was not yet fixed?\n. ",
    "vlamic": "Thank you\n. CloudSearch knows all the types of the fields - whether it's a text or text-array. I don't understand why they cannot return the data with a proper format. Now, every SDK user has to write data parsers to preserve the correct data types. This is ridiculous. . CloudSearch knows all the types of the fields - whether it's a text or text-array. I don't understand why they cannot return the data with a proper format. Now, every SDK user has to write data parsers to preserve the correct data types. This is ridiculous. . ",
    "tristanls": "node: 0.8.16\nAWS.VERSION: 0.9.1-pre.2\nnpm list:\n\u251c\u2500\u252c aws-sdk@0.9.1-pre.2\n\u2502 \u251c\u2500\u252c xml2js@0.2.2\n\u2502 \u2502 \u2514\u2500\u2500 sax@0.4.2\n\u2502 \u2514\u2500\u2500 xmlbuilder@0.4.2\nAnother condition necessary for failure is multiple processes ( say 4 ) running on the same machine all making the putObject call within a short period of time:\n2012-12-23T20:15:05.010Z ERROR transport error {\"code\":\"NotImplemented\",\"message\":\"A header you provided implies functionality that is not implemented\",\"statusCode\":501,\"retryable\":true}\n2012-12-23T20:15:05.013Z ERROR transport error {\"code\":\"NotImplemented\",\"message\":\"A header you provided implies functionality that is not implemented\",\"statusCode\":501,\"retryable\":true}\n2012-12-23T20:15:06.284Z ERROR transport error {\"code\":\"NotImplemented\",\"message\":\"A header you provided implies functionality that is not implemented\",\"statusCode\":501,\"retryable\":true}\n2012-12-23T20:15:06.431Z ERROR transport error {\"code\":\"NotImplemented\",\"message\":\"A header you provided implies functionality that is not implemented\",\"statusCode\":501,\"retryable\":true}\nI can confirm that a manually triggered individual call succeeds.\nIt's a somewhat arcane failure case. Those putObject() calls are created using setInterval() (30 seconds in this particular failure case).\nI will continue digging and try to provide more information.\n. response.httpRequest.headers does not exist / is empty (I'm displaying it via DEBUG below after transport response)\n2012-12-24T21:19:34.312Z ERROR transport error {\"code\":\"NotImplemented\",\"message\":\"A header you provided implies functionality that is not implemented\",\"statusCode\":501,\"retryable\":true}\n2012-12-24T21:19:34.314Z DEBUG transport response \n2012-12-24T21:19:34.314Z ERROR transport error {\"code\":\"NotImplemented\",\"message\":\"A header you provided implies functionality that is not implemented\",\"statusCode\":501,\"retryable\":true}\n2012-12-24T21:19:34.588Z ERROR transport error {\"code\":\"NotImplemented\",\"message\":\"A header you provided implies functionality that is not implemented\",\"statusCode\":501,\"retryable\":true}\n2012-12-24T21:19:34.588Z DEBUG transport response \n2012-12-24T21:19:34.588Z ERROR transport error {\"code\":\"NotImplemented\",\"message\":\"A header you provided implies functionality that is not implemented\",\"statusCode\":501,\"retryable\":true}\n2012-12-24T21:19:34.981Z ERROR transport error {\"code\":\"NotImplemented\",\"message\":\"A header you provided implies functionality that is not implemented\",\"statusCode\":501,\"retryable\":true}\n2012-12-24T21:19:34.981Z DEBUG transport response\nI'll keep looking.\n. Ok, I have a contrast between successful and failed operations. Well, there's really no contrast, looks the same to me.\nThe _endpoint property is for my internal use.\nsuccessful operation\n2012-12-24T21:28:48.811Z DEBUG AWS.config {\"credentials\":{\"accessKeyId\":\"ACCESS_KEY_ID\",\"secretAccessKey\":\"SECRET_ACCESS_KEY\"},\"maxRetries\":3,\"sslEnabled\":true,\"s3ForcePathStyle\":false}\n2012-12-24T21:28:48.811Z DEBUG s3 {\"client\":{\"serviceName\":\"s3\",\"config\":{\"credentials\":{\"accessKeyId\":\"ACCESS_KEY_ID\",\"secretAccessKey\":\"SECRET_ACCESS_KEY\"},\"maxRetries\":3,\"sslEnabled\":true,\"s3ForcePathStyle\":false,\"keys\":{\"sslEnabled\":true,\"s3ForcePathStyle\":false},\"endpoint\":\"https://s3.amazonaws.com\"},\"endpoint\":{\"protocol\":\"https:\",\"slashes\":true,\"host\":\"s3.amazonaws.com\",\"hostname\":\"s3.amazonaws.com\",\"href\":\"https://s3.amazonaws.com/\",\"pathname\":\"/\",\"path\":\"/\",\"port\":443}},\"_endpoint\":\"https://s3.amazonaws.com\"}\n2012-12-24T21:28:48.812Z DEBUG s3.client {\"serviceName\":\"s3\",\"config\":{\"credentials\":{\"accessKeyId\":\"ACCESS_KEY_ID\",\"secretAccessKey\":\"SECRET_ACCESS_KEY\"},\"maxRetries\":3,\"sslEnabled\":true,\"s3ForcePathStyle\":false,\"keys\":{\"sslEnabled\":true,\"s3ForcePathStyle\":false},\"endpoint\":\"https://s3.amazonaws.com\"},\"endpoint\":{\"protocol\":\"https:\",\"slashes\":true,\"host\":\"s3.amazonaws.com\",\"hostname\":\"s3.amazonaws.com\",\"href\":\"https://s3.amazonaws.com/\",\"pathname\":\"/\",\"path\":\"/\",\"port\":443}}\n2012-12-24T21:28:48.954Z DEBUG whole response {\"ServerSideEncryption\":\"AES256\",\"ETag\":\"\\\"ETAG_VALUE_HERE\\\"\",\"RequestId\":\"56E04C735CB16D95\"}\n2012-12-24T21:28:48.955Z ERROR transport error \n2012-12-24T21:28:48.955Z DEBUG transport response.httpRequest.headers\nfailed operation\n2012-12-24T21:29:19.725Z DEBUG AWS.config {\"credentials\":{\"accessKeyId\":\"ACCESS_KEY_ID\",\"secretAccessKey\":\"SECRET_ACCESS_KEY\"},\"maxRetries\":3,\"sslEnabled\":true,\"s3ForcePathStyle\":false}\n2012-12-24T21:29:19.725Z DEBUG s3 {\"client\":{\"serviceName\":\"s3\",\"config\":{\"credentials\":{\"accessKeyId\":\"ACCESS_KEY_ID\",\"secretAccessKey\":\"SECRET_ACCESS_KEY\"},\"maxRetries\":3,\"sslEnabled\":true,\"s3ForcePathStyle\":false,\"keys\":{\"sslEnabled\":true,\"s3ForcePathStyle\":false},\"endpoint\":\"https://s3.amazonaws.com\"},\"endpoint\":{\"protocol\":\"https:\",\"slashes\":true,\"host\":\"s3.amazonaws.com\",\"hostname\":\"s3.amazonaws.com\",\"href\":\"https://s3.amazonaws.com/\",\"pathname\":\"/\",\"path\":\"/\",\"port\":443}},\"_endpoint\":\"https://s3.amazonaws.com\"}\n2012-12-24T21:29:19.725Z DEBUG s3.client {\"serviceName\":\"s3\",\"config\":{\"credentials\":{\"accessKeyId\":\"ACCESS_KEY_ID\",\"secretAccessKey\":\"SECRET_ACCESS_KEY\"},\"maxRetries\":3,\"sslEnabled\":true,\"s3ForcePathStyle\":false,\"keys\":{\"sslEnabled\":true,\"s3ForcePathStyle\":false},\"endpoint\":\"https://s3.amazonaws.com\"},\"endpoint\":{\"protocol\":\"https:\",\"slashes\":true,\"host\":\"s3.amazonaws.com\",\"hostname\":\"s3.amazonaws.com\",\"href\":\"https://s3.amazonaws.com/\",\"pathname\":\"/\",\"path\":\"/\",\"port\":443}}\n2012-12-24T21:29:20.891Z DEBUG whole response \n2012-12-24T21:29:20.891Z ERROR transport error {\"code\":\"NotImplemented\",\"message\":\"A header you provided implies functionality that is not implemented\",\"statusCode\":501,\"retryable\":true}\n2012-12-24T21:29:20.891Z DEBUG transport response.httpRequest.headers\n. I have a hint. If Body is an empty string, that would probably result in Content-Length header not being created, which is probably the culprit for NotImplemented error.\nWill verify shortly.\n. Confirmed, Body = '' // empty string will result in the errors I've reported.\n\nSome files were being uploaded empty.\nAllowing uploading empty files is useful from the perspective of having a \"log heartbeat\" if you will. Say, I want to dump activity/log data into s3 at a set time interval. If I have no data, the presence of an empty file still lets me know that the reporting agent is still alive (i.e. heartbeat).\n. Ah, sorry for stepping on you :D . You're more familiar with the code base, so I'm sure I missed a bunch. Thanks!\n. Interesting, I'll have to take a look at what's going on there later on. Going offline. Thanks again, cheers!\n. 15 Sep 2018, still this :(. Continuing to observe this issue on Node.js v5.4.0, setting keepAlive does not prevent this from occurring:\njavascript\nnew AWS.DynamoDB(\n{\n    httpOptions: {\n        agent: new https.Agent(\n        {\n            rejectUnauthorized: true,\n            keepAlive: true\n        })\n    }\n});\njson\n{\n    \"target\": {\n        \"module\": \"aws-sdk\",\n        \"version\": \"2.2.28\",\n        \"export\": \"DynamoDB\",\n        \"method\": \"putItem\",\n        \"args\": [\n            {\n                \"Item\": \"*REDACTED*\"\n            }\n        ]\n    },\n    \"type\": \"log\",\n    \"level\": \"error\",\n    \"message\": \"failed to put item in DynamoDB\",\n    \"error\": {\n        \"message\": \"write EPROTO\",\n        \"code\": \"NetworkingError\",\n        \"errno\": \"EPROTO\",\n        \"syscall\": \"write\",\n        \"region\": \"us-east-1\",\n        \"hostname\": \"dynamodb.us-east-1.amazonaws.com\",\n        \"retryable\": true,\n        \"time\": \"2016-01-23T02:56:44.705Z\"\n    },\n    \"stack\": \"Error: write EPROTO\\n    at Object.exports._errnoException (util.js:856:11)\\n    at exports._exceptionWithHostPort (util.js:879:20)\\n    at WriteWrap.afterWrite (net.js:763:14)\",\n    \"timestamp\": \"2016-01-23T02:56:44.706Z\"\n}\n. I did look into ciphers by refreshing this over and over again (trying to catch different IPs) https://www.ssllabs.com/ssltest/analyze.html?d=dynamodb.us-east-1.amazonaws.com&latest\nFrom my most recent run (54.239.20.144, 54.239.16.203):\n```\nTLS 1.0 only\nTLS_RSA_WITH_AES_128_CBC_SHA (0x2f) 128\nTLS_RSA_WITH_AES_256_CBC_SHA (0x35) 256\nTLS_RSA_WITH_3DES_EDE_CBC_SHA (0xa) 112\n``\n. I forgot an additional detail that I haven't seen reported yet. I happen to be measuring the latency of the HTTPS calls to DynamoDB usingaws-sdk, and when I get anEPROTOerror back, the latency ~~(except in one case)~~ is 25 seconds, more precisely, most of it is between 25600 and 25700, some between 25700 and 25800 milliseconds. ~~(the one exception was 12915 milliseconds\u00af_(\u30c4)_/\u00af`)~~ \n(edit: there was no exception to 25 seconds for EPROTO error, my initial search was too inclusive)\nedit 2: Looks like 25 seconds comes from DynamoDB aws-sdk retry policy https://github.com/aws/aws-sdk-js/blob/master/lib/services/dynamodb.js#L48 which would add 25550 milliseconds of delays across total of 11 attempts it makes\n. @phsstory looking at where EPROTO can come out of based on comments in https://github.com/nodejs/node/issues/3692 the paths look (to my untrained eye) quite different.\nlatest: https://github.com/nodejs/node/blob/master/src/tls_wrap.cc#L593\n0.12: https://github.com/nodejs/node/blob/v0.12.7-release/src/tls_wrap.cc#L607\n. @phsstory understood. I am working on the assumption that this is what's causing the EPROTO issue in v5.5.0 and latest: https://github.com/nodejs/node/blob/master/src/tls_wrap.cc#L592-L593. This code path for throwing EPROTO doesn't seem to exist in 0.12, which could explain the difference.\n. Well, I haven't reproduced the problem (because in the stack below v0.12.7 also fails with EPROTO) error. But I'm not gonna be able to get back to this for a bit so wanted to post findings so far.\nThe c-server below is compiled using gcc server.c -o c-server.\nThe node-patched is node with this patch https://github.com/nodejs/node/issues/3692#issuecomment-158583358 included (I think... it compiled, and it's the main executable that got built).\nnode is vanilla v5.5.0.\nI'm hoping to keep iterating on server.c until v0.12.7 passes and v5.5.0 fails in order to maybe reproduce eventually. I'm guessing next step will be to include SSL along the lines of http://stackoverflow.com/questions/7698488/turn-a-simple-socket-into-an-ssl-socket\nIn the meantime, I dumped the existing setup to docker hub https://hub.docker.com/r/tristanls/eproto-plus-patched-node/\n```\n[root@8fec5290e6f7 patched]# ll \ntotal 18740\n-rwxr-xr-x 1 root root     9244 Jan 25 03:16 c-server\n-rw-r--r-- 1 root root      259 Jan 25 03:16 client.js\n-rwxr-xr-x 1 root root 19161989 Jan 24 00:34 node-patched\n-rw-r--r-- 1 root root     1576 Jan 25 02:55 server.c\ndrwxr-xr-x 6 root root     4096 Jan 25 03:16 v0.12.7\n[root@8fec5290e6f7 patched]# v0.12.7/bin/node -v\nv0.12.7\n[root@8fec5290e6f7 patched]# node -v\nv5.5.0\n[root@8fec5290e6f7 patched]# ./node-patched -v\nv5.5.0\n[root@8fec5290e6f7 patched]# cat client.js \n\"use strict\";\nvar https = require(\"https\");\nvar options = {\n    port: 443\n};\nvar req = https.request(options, function(res)\n{\n    console.log(res.statusCode);\n    res.on(\"data\", function(data)\n    {\n        process.stdout.write(data);\n    });\n});\nreq.end();\n[root@8fec5290e6f7 patched]# cat server.c \n/ A simple server in the internet domain using TCP\n   The port number is passed as an argument /\ninclude \ninclude \ninclude \ninclude \ninclude \ninclude \ninclude \nvoid error(const char *msg)\n{\n    perror(msg);\n    exit(1);\n}\nint main(int argc, char argv[])\n{\n     int sockfd, newsockfd, portno;\n     socklen_t clilen;\n     char buffer[256];\n     struct sockaddr_in serv_addr, cli_addr;\n     int n;\n     if (argc < 2) {\n         fprintf(stderr,\"ERROR, no port provided\\n\");\n         exit(1);\n     }\n     sockfd = socket(AF_INET, SOCK_STREAM, 0);\n     if (sockfd < 0)\n        error(\"ERROR opening socket\");\n     bzero((char ) &serv_addr, sizeof(serv_addr));\n     portno = atoi(argv[1]);\n     serv_addr.sin_family = AF_INET;\n     serv_addr.sin_addr.s_addr = INADDR_ANY;\n     serv_addr.sin_port = htons(portno);\n     if (bind(sockfd, (struct sockaddr ) &serv_addr,\n              sizeof(serv_addr)) < 0)\n              error(\"ERROR on binding\");\n     listen(sockfd,5);\n     clilen = sizeof(cli_addr);\n     newsockfd = accept(sockfd,\n                 (struct sockaddr ) &cli_addr,\n                 &clilen);\n     if (newsockfd < 0)\n          error(\"ERROR on accept\");\n     bzero(buffer,256);\n     n = read(newsockfd,buffer,255);\n     if (n < 0) error(\"ERROR reading from socket\");\n     printf(\"Here is the message: %s\\n\",buffer);\n     n = write(newsockfd,\"I got your message\",18);\n     if (n < 0) error(\"ERROR writing to socket\");\n     close(newsockfd);\n     close(sockfd);\n     return 0;\n}\n[root@8fec5290e6f7 patched]# ./c-server 443 &\n[1] 20\n[root@8fec5290e6f7 patched]# v0.12.7/bin/node client.js \nHere is the message: %\nevents.js:85\n      throw er; // Unhandled 'error' event\n            ^\nError: write EPROTO 139816494778176:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:../deps/openssl/openssl/ssl/s23_clnt.c:782:\nat exports._errnoException (util.js:746:11)\nat WriteWrap.afterWrite (net.js:775:14)\n\n[1]+  Done                    ./c-server 443\n[root@8fec5290e6f7 patched]# ./c-server 443 &\n[1] 26\n[root@8fec5290e6f7 patched]# node client.js \nHere is the message: \nevents.js:154\n      throw er; // Unhandled 'error' event\n      ^\nError: write EPROTO\n    at Object.exports._errnoException (util.js:856:11)\n    at exports._exceptionWithHostPort (util.js:879:20)\n    at WriteWrap.afterWrite (net.js:763:14)\n[1]+  Done                    ./c-server 443\n[root@8fec5290e6f7 patched]# ./c-server 443 &\n[1] 36\n[root@8fec5290e6f7 patched]# ./node-patched client.js \nHere is the message: \nevents.js:154\n      throw er; // Unhandled 'error' event\n      ^\nError: write EPROTO\n    at Object.exports._errnoException (util.js:856:11)\n    at exports._exceptionWithHostPort (util.js:879:20)\n    at WriteWrap.afterWrite (net.js:763:14)\n[1]+  Done                    ./c-server 443\n``\n. @chrisradek, currently my instancei-d69a5d5fis reliably generating EPROTO error when trying to talk to DynamoDB. My instancesi-bd81f834andi-03e9b1b0are talking to DynamoDB ok. I'll see what I can investigate from my side, but perhaps there's someone who can look at the network/connectivity/differences and see what's going on? (us-east-1` region)\n. BOOYAH! :) (hehe.. took a while to track this down)...\nNode.js version is latest, (including fix to get meaningful errors from https://github.com/nodejs/node/commit/ff4006c7b05d677f6b63f01ad9c5faf97e0230bd)\njson\n{    \n    \"target\": {\n        \"module\": \"aws-sdk\",\n        \"version\": \"2.2.28\",\n        \"export\": \"DynamoDB\",\n        \"method\": \"query\",\n        \"args\": [\"*redacted*\"]\n    },\n    \"type\": \"log\",\n    \"level\": \"error\",\n    \"message\": \"failed to query DynamoDB\",\n    \"error\": {\n        \"message\": \"write EPROTO 140550271170368:error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number:../deps/openssl/openssl/ssl/s3_pkt.c:362:\\n\",\n        \"code\": \"NetworkingError\",\n        \"errno\": \"EPROTO\",\n        \"syscall\": \"write\",\n        \"region\": \"us-east-1\",\n        \"hostname\": \"dynamodb.us-east-1.amazonaws.com\",\n        \"retryable\": true,\n        \"time\": \"2016-01-29T03:35:04.548Z\"\n    },\n    \"stack\": \"Error: write EPROTO 140550271170368:error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number:../deps/openssl/openssl/ssl/s3_pkt.c:362:\\n\\n    at exports._errnoException (util.js:859:11)\\n    at WriteWrap.afterWrite (net.js:763:14)\",\n    \"timestamp\": \"2016-01-29T03:35:04.549Z\"\n}\n. With some additional logging...\njson\n    \"type\": \"log\",\n    \"level\": \"debug\",\n    \"target\": \"ssl3_get_record\",\n    \"recv\": \"301\",\n    \"s.v\": \"303\",\n    \"s.ref\": \"1\"\n...it looks like Node.js is expecting (s.v, socket version) TLS 1.2 x303 but getting (recv, received) TLS 1.0 x301. \n. @chrisradek \nI meant sporadic bur reproducible given enough time.\nThe ciphers I posted were from the website here: https://www.ssllabs.com/ssltest/analyze.html?d=dynamodb.us-east-1.amazonaws.com. I'm uncertain what ciphers were attempted to be negotiated at the actual time of the error.\n. @chrisradek, while continuing to troubleshoot this issue, I just had a weird experience that I can't explain.\nI set secureProtocol option to a method that is not supposed to be supported by DynamoDB TLSv1_1_method:\n```\n[root@920c620688d9 /]# vi test.sh \n!/bin/bash\nNODE_DEBUG=net,tls node -e '\n    var c = require(\"tls\").connect(\n    {\n        host:\"dynamodb.us-east-1.amazonaws.com\",\n        port:443,\n        rejectUnauthorized: true,\n        secureProtocol: \"TLSv1_1_method\"\n    });\n    c.pipe(process.stdout);\n    c.write(\"GET / HTTP/1.1\\r\\nHost: dynamodb.us-east-1.amazonaws.com\\r\\n\\r\\n\");\n'\n```\nWhat was surprising is that it \"worked\"?? twice?\n```\n[root@920c620688d9 /]# ./test.sh \nNET 584: pipe false undefined\nNET 584: connect: find host dynamodb.us-east-1.amazonaws.com\nNET 584: connect: dns options { family: undefined, hints: 40 }\nNET 584: _read\nNET 584: _read wait for connection\nNET 584: afterConnect\nTLS 584: start\nNET 584: _read\nNET 584: Socket._read readStart\nTLS 584: secure established\nNET 584: afterWrite 0\nNET 584: afterWrite call cb\nNET 584: onread 255\nNET 584: got data\nHTTP/1.1 200 OK\nServer: Server\nDate: Sat, 30 Jan 2016 18:29:48 GMT\nContent-Length: 42\nConnection: keep-alive\nx-amzn-RequestId: 4IIH8II2DIOA2V3KS7163570ARVV4KQNSO5AEMVJF66Q9ASUAAJG\nx-amz-crc32: 3128867991\nhealthy: dynamodb.us-east-1.amazonaws.com NET 584: _read\n^C\n[root@920c620688d9 /]# ./test.sh \nNET 594: pipe false undefined\nNET 594: connect: find host dynamodb.us-east-1.amazonaws.com\nNET 594: connect: dns options { family: undefined, hints: 40 }\nNET 594: _read\nNET 594: _read wait for connection\nNET 594: afterConnect\nTLS 594: start\nNET 594: _read\nNET 594: Socket._read readStart\nTLS 594: secure established\nNET 594: afterWrite 0\nNET 594: afterWrite call cb\nNET 594: onread 255\nNET 594: got data\nHTTP/1.1 200 OK\nServer: Server\nDate: Sat, 30 Jan 2016 18:29:50 GMT\nContent-Length: 42\nConnection: keep-alive\nx-amzn-RequestId: C7G7PAHI3UR674G0B5LC3LHH6BVV4KQNSO5AEMVJF66Q9ASUAAJG\nx-amz-crc32: 3128867991\nhealthy: dynamodb.us-east-1.amazonaws.com NET 594: _read\n^C\n[root@920c620688d9 /]# ./test.sh \nNET 604: pipe false undefined\nNET 604: connect: find host dynamodb.us-east-1.amazonaws.com\nNET 604: connect: dns options { family: undefined, hints: 40 }\nNET 604: _read\nNET 604: _read wait for connection\nNET 604: afterConnect\nTLS 604: start\nNET 604: _read\nNET 604: Socket._read readStart\n{\"type\":\"log\",\"level\":\"debug\",\"target\":\"ssl3_get_record\",\"recv\":\"301\",\"s.v\":\"302\",\"s.ref\":\"1\"}\n{\"type\":\"log\",\"level\":\"debug\",\"target\":\"ssl3_get_record\",\"contents\":\"16, 03, 01, 0a, 94, \"}\nNET 604: afterWrite -71\nNET 604: write failure { Error: write EPROTO 139766103729984:error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number:../deps/openssl/openssl/ssl/s3_pkt.c:362:\nat exports._errnoException (util.js:859:11)\nat WriteWrap.afterWrite (net.js:763:14) code: 'EPROTO', errno: 'EPROTO', syscall: 'write' }\n\nNET 604: destroy\nNET 604: close\nNET 604: close handle\nevents.js:155\n      throw er; // Unhandled 'error' event\n      ^\nError: write EPROTO 139766103729984:error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number:../deps/openssl/openssl/ssl/s3_pkt.c:362:\nat exports._errnoException (util.js:859:11)\nat WriteWrap.afterWrite (net.js:763:14)\n\n```\nMy hypothesis: ~~Some DynamoDB servers do not enforce the published restriction on only using TLSv1 and not using TLSv1_1 and TLSv1_2.~~ edit: Some DynamoDB servers communicate using TLSv1_1 and TLSv1_2 while others restrict themselves only to TLSv1\nQuestion: ~~Is there a subset of dynamodb.us-east-1.amazonaws.com servers that accept TLSv1_1 and/or TLSv1_2 connections in addition to accepting TLSv1 connections?~~ edit: Is it true that some dynamodb.us-east-1.amazonaws.com servers only accept TLSv1 connections, while others accept TLSv1, TLSv1_1, and TLSv1_2 or some combination thereof?\n. @phsstory setting connection keepAlive to true is not a successful mitigation\nSetting secureProtocol https option to TLSv1_method will ensure that no TLSv1_1 or TLSv1_2 connections are attempted:\njavascript\nvar options = {\n  secureProtocol: \"TLSv1_method\"\n}\n. @phsstory I believe it is. The cause looks to be as follows:\n\nThere is a server pool, however, only some of these servers support TLS1.2 and they also generate sessions for clients. What happens next is that node.js connects to TSL1 server and attempts to reuse the session provided by TLS1.2 server. Thus, for some reason OpenSSL expects server to be at least TLS1.2, and is surprised to find out that it is not.\n-- from https://github.com/nodejs/node/issues/3692#issuecomment-177300756\n\nOnly using TLS1 will never cache a session that is TLS1.2.\n@brandonros also suggests ciphers: \"ALL\" in https://github.com/nodejs/node/issues/3692#issuecomment-177294788\n. This commit just landed in Node.js master https://github.com/nodejs/node/commit/165b33fce2ed26e969beed3d3f7796708f0743e1. It will mitigate the issue at a cost of a bit extra work in case an EPROTO (or any other error occurs) because of retries, but should allow us to use default settings and expect SDK to succeed by default since it uses retries.\n. @westy92 :(\n@indutny provided a reduced test case for the issue here, which will throw EPROTO when run: https://gist.github.com/indutny/a021cca1711aa92e96a9\nThis test case, which adds secureProtocol: \"TLSv1_method\" no longer results in EPROTO error: https://gist.github.com/tristanls/8721d6a25af0b37bac07\nI'm continuing to observe my deployment.\n. @tielur just to confirm, you had secureProtocol: \"TLSv1_method\" set on the client?\n. @jmt0806 did you have secureProtocol: \"TLSv1_method\" set in your DynamoDB options?\n. @westy92 not embarrassing at all :) that's great news, thank you for the update. This way we don't have an unaccounted for failure case.\n. @Unterdrucker it is a mitigation that will cost a retry because if EPROTO occurs, session will be dropped. Previously, retry would fail because session was kept and was reused, this time, retry is expected to succeed.\n. @phsstory the workaround is now pretty well tested? \ud83d\ude09 \n// untested workaround\n. ",
    "deltaepsilon": "My file can be found here: \nhttps://github.com/deltaepsilon/bloggeroid/blob/master/server/AssetsServer.coffee\n. My file can be found here: \nhttps://github.com/deltaepsilon/bloggeroid/blob/master/server/AssetsServer.coffee\n. Node v0.6.12\nOS X Mountain Lion\nMeteor version 0.5.2 (6635ae1007)\nI'm trying to think of what else you might need...\n. Node v0.6.12\nOS X Mountain Lion\nMeteor version 0.5.2 (6635ae1007)\nI'm trying to think of what else you might need...\n. Good plan.  I'll try the newer Node.\nI'm getting the following error when I run your test;\ns3.client.deleteObject({Bucket: 'assets.christopheresplin.com', Key: 'meteor/blog/assets/1356943540784_File0037.jpg'}, function (error, data){ console.log(error, data);})\nTypeError: Not a string or buffer\n    at Object.createHmac (crypto.js:129:21)\n    at Object.hmac (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/util.js:118:24)\n    at SigVS3.sign (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/sigvs3.js:174:28)\n    at SigVS3.addAuthorization (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/sigvs3.js:67:26)\n    at HttpRequest.sign (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/client.js:85:14)\n    at RequestHandler.makeRequest (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/http.js:166:17)\n    at AWSRequest.send (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/promise.js:110:34)\n    at S3Client.makeRequest (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/client.js:47:15)\n    at S3Client.deleteObject (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/client.js:145:21)\n    at repl:1:12\nHowever, If I leave out the callback, I get this...\ns3.client.deleteObject({Bucket: 'assets.christopheresplin.com', Key: 'meteor/blog/assets/1356943540784_File0037.jpg'})\n{ client: \n   { serviceName: 's3',\n     config: \n      { credentials: [Object],\n        region: 'us-east-1',\n        maxRetries: undefined,\n        sslEnabled: true,\n        s3ForcePathStyle: false,\n        constructor: [Object],\n        update: [Function: update],\n        loadFromPath: [Function: loadFromPath],\n        clear: [Function: clear],\n        set: [Function: set],\n        keys: [Object],\n        extractCredentials: [Function: extractCredentials] },\n     endpoint: \n      { protocol: 'https:',\n        slashes: true,\n        host: 's3.amazonaws.com',\n        hostname: 's3.amazonaws.com',\n        href: 'https://s3.amazonaws.com/',\n        pathname: '/',\n        path: '/',\n        port: 443 } },\n  operation: 'deleteObject',\n  params: \n   { Bucket: 'assets.christopheresplin.com',\n     Key: 'meteor/blog/assets/1356943540784_File0037.jpg' },\n  awsResponse: \n   { request: [Circular],\n     data: null,\n     error: null,\n     retryCount: 0,\n     httpRequest: null,\n     httpResponse: null },\n  state: null,\n  callbacks: { data: [], done: [], fail: [], always: [] } }\nNotice that the awsResponse.request is [Circular].  Meteor could be choking on that.  \nI agree that it's probably a Meteor problem.  I've tried calling this function a number of different ways from within Meteor, and I always get the same result.  I'm going to upgrade Node and keep playing with it later this week.  Hopefully I'll find something helpful to report.\n. Good plan.  I'll try the newer Node.\nI'm getting the following error when I run your test;\ns3.client.deleteObject({Bucket: 'assets.christopheresplin.com', Key: 'meteor/blog/assets/1356943540784_File0037.jpg'}, function (error, data){ console.log(error, data);})\nTypeError: Not a string or buffer\n    at Object.createHmac (crypto.js:129:21)\n    at Object.hmac (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/util.js:118:24)\n    at SigVS3.sign (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/sigvs3.js:174:28)\n    at SigVS3.addAuthorization (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/sigvs3.js:67:26)\n    at HttpRequest.sign (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/client.js:85:14)\n    at RequestHandler.makeRequest (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/http.js:166:17)\n    at AWSRequest.send (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/promise.js:110:34)\n    at S3Client.makeRequest (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/client.js:47:15)\n    at S3Client.deleteObject (/Users/christopheresplin/Development/meteor/blog/public/node_modules/aws-sdk/lib/client.js:145:21)\n    at repl:1:12\nHowever, If I leave out the callback, I get this...\ns3.client.deleteObject({Bucket: 'assets.christopheresplin.com', Key: 'meteor/blog/assets/1356943540784_File0037.jpg'})\n{ client: \n   { serviceName: 's3',\n     config: \n      { credentials: [Object],\n        region: 'us-east-1',\n        maxRetries: undefined,\n        sslEnabled: true,\n        s3ForcePathStyle: false,\n        constructor: [Object],\n        update: [Function: update],\n        loadFromPath: [Function: loadFromPath],\n        clear: [Function: clear],\n        set: [Function: set],\n        keys: [Object],\n        extractCredentials: [Function: extractCredentials] },\n     endpoint: \n      { protocol: 'https:',\n        slashes: true,\n        host: 's3.amazonaws.com',\n        hostname: 's3.amazonaws.com',\n        href: 'https://s3.amazonaws.com/',\n        pathname: '/',\n        path: '/',\n        port: 443 } },\n  operation: 'deleteObject',\n  params: \n   { Bucket: 'assets.christopheresplin.com',\n     Key: 'meteor/blog/assets/1356943540784_File0037.jpg' },\n  awsResponse: \n   { request: [Circular],\n     data: null,\n     error: null,\n     retryCount: 0,\n     httpRequest: null,\n     httpResponse: null },\n  state: null,\n  callbacks: { data: [], done: [], fail: [], always: [] } }\nNotice that the awsResponse.request is [Circular].  Meteor could be choking on that.  \nI agree that it's probably a Meteor problem.  I've tried calling this function a number of different ways from within Meteor, and I always get the same result.  I'm going to upgrade Node and keep playing with it later this week.  Hopefully I'll find something helpful to report.\n. Yep.  I set the environment vars and it worked fine.  Thanks for your help.  It's time to do battle with Meteor.\n. Yep.  I set the environment vars and it worked fine.  Thanks for your help.  It's time to do battle with Meteor.\n. ",
    "radekg": "Answering my own question:\njs\nvar writePos = 0;\nvar stream = fs.createWriteStream(\"/tmp/test.jpg\", { flags: 'w', encoding: null, mode: 0666 });\ns3.client.getObject({ Bucket: bucket, Key: key }).data(function(data) {\n    stream.write(data.data);\n}).done(function() {\n    stream.end();\n}).send();\n. Answering my own question:\njs\nvar writePos = 0;\nvar stream = fs.createWriteStream(\"/tmp/test.jpg\", { flags: 'w', encoding: null, mode: 0666 });\ns3.client.getObject({ Bucket: bucket, Key: key }).data(function(data) {\n    stream.write(data.data);\n}).done(function() {\n    stream.end();\n}).send();\n. I've installed this package from master and this example has stopped working.\nTypeError: Object #<Request> has no method 'data'\n. I've installed this package from master and this example has stopped working.\nTypeError: Object #<Request> has no method 'data'\n. Yes, yes, I've found it. The changes were implemented here: https://github.com/aws/aws-sdk-js/pull/22, thank you.\n. Yes, yes, I've found it. The changes were implemented here: https://github.com/aws/aws-sdk-js/pull/22, thank you.\n. Thank you, looking back at this it seems obvious, it takes an array! :) Closing this one.\n. Thank you, looking back at this it seems obvious, it takes an array! :) Closing this one.\n. ",
    "mchotin": "I was able to hack around this by editing ec2.api.js in the source.  Part 1 is adding the 'm' section in Resources.  Part 2 is adding the o: {} near the bottom.  This shouldn't be necessary AFAIK though.\nmy createTags section looks like this:\njavascript\ncreateTags: {\n      n: 'CreateTags',\n      i: {\n        m: {\n          Resources: {\n            n: 'ResourceId',\n            t: 'a',\n            m: {\n              n: 'ResourceId'\n            },\n            r: 1\n          },\n          Tags: {\n            n: 'Tag',\n            t: 'a',\n            m: {\n              n: 'Item',\n              t: 'o',\n              m: {\n                Key: {\n                },\n                Value: {\n                }\n              }\n            },\n            r: 1\n          }\n        }\n      },\n      o: {}\n    },\n. ",
    "originalgremlin": "The parseTimestamp method of aws-sdk/lib/xml/parser_xml2js.js assumed 1-based months.  Changing date.setMonth(parts[1]) to date.setMonth(parts[1] - 1) fixes this bug (which does seem to affect the entire sdk.  I already put in a pull request with the fix.\n. ",
    "gramakri": "Works great! My production code doesn't seem to have any problems either.\nThanks.\n. Are you guys able to reproduce this?\n. Works now, thanks.\nThere is an extra console.log - https://github.com/aws/aws-sdk-js/pull/47\n. I can confirm this is fixed. Thanks.\n. This is a dup of #60 (I didn't realize that creating a pull request creates an 'issue' automatically)\n. I agree doing an array concat for emit is not efficient.\nUnfortunately, it will take me sometime to create a proper pull request with tests. I don't really know the AWS code all that well and neither do I know coffeescript (for the tests). If you can fix this tomorrow, that will be great! (just close my pull request)\n. Test case:\n``` js\nvar AWS = require('aws-sdk');\nAWS.config.loadFromPath('./credentials.json');\nvar s3 = new AWS.S3();\nvar request = s3.client.createBucket({Bucket: 'myBucket'});\nrequest.\non('success', function(response) {\nconsole.log(\"Success!\");\n}).\non('error', function(response) {\nconsole.log(\"Error!\");\nthrow new Error('exception');\n}).\non('complete', function(response) {\nconsole.log(\"Always!\");\n}).\nsend();\n```\nIn the code above, the error handler gets called. The complete handler is not executed. If you remove the throw, the complete handler is called.\nIs this intentional?\n. To avoid array concat, we can do what EventEmitter.prototype.emit does (events.js). They have a fast path for upto 3 arguments.\nThe other thing is that eventParameters must return an array in all cases. It is unnecessarily returning different types.\n. ",
    "mattlockyer": "I'm getting this error using Chrome 32, AWS JS SDK for the browser 2.0.0.rc9\nError {message: \"CRC32 integrity check failed\", code: \"CRC32CheckFailed\", retryable: true, time: Sun Feb 09 2014 15:11:36 GMT-0800 (PST), statusCode: 200\u2026}\ncode: \"CRC32CheckFailed\"\nmessage: \"CRC32 integrity check failed\"\nname: \"CRC32CheckFailed\"\nretryable: true\nstack: (...)\nget stack: function () { [native code] }\nset stack: function () { [native code] }\nstatusCode: 200\ntime: Sun Feb 09 2014 15:11:36 GMT-0800 (PST)\nproto: d\n. I'm getting this error using Chrome 32, AWS JS SDK for the browser 2.0.0.rc9\nError {message: \"CRC32 integrity check failed\", code: \"CRC32CheckFailed\", retryable: true, time: Sun Feb 09 2014 15:11:36 GMT-0800 (PST), statusCode: 200\u2026}\ncode: \"CRC32CheckFailed\"\nmessage: \"CRC32 integrity check failed\"\nname: \"CRC32CheckFailed\"\nretryable: true\nstack: (...)\nget stack: function () { [native code] }\nset stack: function () { [native code] }\nstatusCode: 200\ntime: Sun Feb 09 2014 15:11:36 GMT-0800 (PST)\nproto: d\n. I just starting getting it but it persisted...\nUsing: new AWS.DynamoDB({dynamoDbCrc32: false, ...}); solved the issue.\nThank you for responding so quickly.\nMatt Lockyer - http://www.mattlockyer.com\nOn Sun, Feb 9, 2014 at 4:15 PM, Loren Segal notifications@github.comwrote:\n\n@mattlockyer https://github.com/mattlockyer are you getting this all\nthe time, or occasionally? If so, can you provide code that reproduces this\nbehavior?\n\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-js/issues/29#issuecomment-34592493\n.\n. I just starting getting it but it persisted...\n\nUsing: new AWS.DynamoDB({dynamoDbCrc32: false, ...}); solved the issue.\nThank you for responding so quickly.\nMatt Lockyer - http://www.mattlockyer.com\nOn Sun, Feb 9, 2014 at 4:15 PM, Loren Segal notifications@github.comwrote:\n\n@mattlockyer https://github.com/mattlockyer are you getting this all\nthe time, or occasionally? If so, can you provide code that reproduces this\nbehavior?\n\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-js/issues/29#issuecomment-34592493\n.\n. Do the unauthenticated identities created by a call to Cognito's getId count towards MAUs that one can get charged for? This seems like an issue if someone had malicious intent, almost like a DDOS of Cognito.. \n",
    "NeoPhi": "In looking around a little more it looks like in s3.api.js the copyObject definition doesn't specify including all of the headers that a standard putObject request does which should address the first problem I was seeing of metadata not being included.\n. I was able to work around the Transfer-Encoding issue with this little hack in http.js handleRequest right after the options object is created:\nif ((options.method === 'PUT') && (!request.body)) {\n      if (!options.headers) {\n        options.headers = {};\n      }\n      if (!options.headers['Content-Length']) {\n        options.headers['Content-Length'] = 0;\n      }\n    }\nThis would need to be cleaned up to be a general solution to the problem.\n. ",
    "gramakri-intel": "I can confirm the issue is fixed now :)\n. This actually works fine with latest code from git. Previously I used to get data.Message. Now, I get data.Messages.\n. ",
    "PatrickHeneise": "No, I wrote them from scratch based on the S3 tests.\n. Oh, ok. I didn't see the cucumber tests.\nI'm working my way through SQS, S3 and EC2 at the moment. So far everything works. Cool stuff!\n. Well, accessKeyId is indeed better, but when copy&pasting from the Amazon documentation it's kind of confusing.\n. Hm, I just checked the documentation again and can't find the original using AWSAccessKeyId any more. Sorry for the trouble.\n. ",
    "jeffrgnome": "FYI, this may represent duplicate/similar work to Issue 33. Issue 33 already mentions the need to internally calculate and set the Content-MD5 header on the deleteObjects request. This issue requests it on putObject and multipartUpload related requests.\nhttps://github.com/aws/aws-sdk-js/issues/33\n. ",
    "corymsmith": "Fixed!\n. My bad, I had run the tests initially and they failed (but hadn't realized I fat fingered an extra character in there) so it seemed like those tests always failed. I can revert and add the obj.hasOwnProperty in there and resubmit the pull request if you'd like. After ensuring all tests pass this time :)\n. ",
    "jordanryanmoore": "Verified. Thanks!\n. Verified. Thanks!\n. The issue here is that the HTTP response body is pure JSON, but it's being parsed as XML instead.\n. Awesome. I'm assuming a new version will be published to npm shortly?\n. The problem isn't exactly as originally described. If you look at the method declaration for transactWrite() vs. batchWrite(), you'll see that the transactional requires a Dynamo-based input, while the batch requires a DocumentClient-based input:\n```typescript\n    /*\n     * Puts or deletes multiple items in one or more tables by delegating to AWS.DynamoDB.batchWriteItem().\n     /\n    batchWrite(params: DocumentClient.BatchWriteItemInput, callback?: (err: AWSError, data: DocumentClient.BatchWriteItemOutput) => void): Request;\n...\n/**\n * Synchronous write operation that groups up to 10 action requests\n */\ntransactWrite(params: DynamoDB.TransactWriteItemsInput, callback?: (err: AWSError, data: DynamoDB.TransactWriteItemsOutput) => void): Request<DynamoDB.TransactWriteItemsOutput, AWSError>;\n\n```\nDynamoDB.TransactWriteItemsInput should be DocumentClient.TransactWriteItemsInput. This applies to transactGet() as well.. ",
    "ndemoor": "+1\nIf only for the security best practices of not having keys flowing around.\n. +1\nIf only for the security best practices of not having keys flowing around.\n. ",
    "cjhanks": "Is there reason to not support the AWS_CONFIG_FILE export as well?\n. Is there reason to not support the AWS_CONFIG_FILE export as well?\n. It appears my terminology is wrong.  I understood the features as related since there must be some hierarchy of checking for credentials, correct?  Ie: Checking for IAM role, then bash export, then config file... etc.  Currently I have found the aws-sdk-js to obey /only/ exports of ACCESS_KEY_ID and SECRET_ACCESS_KEY.  If IAM roles were to be implemented, would it be in a different section of the code?  ( config.js: 384 )\nNote:\nThis is of consequence only because there is no afaik no standard export for region defined, however it is implementable via the CONFIG_FILE.  It does not appear ruby SDK supports this feature either, so the request may not be valid.\n. It appears my terminology is wrong.  I understood the features as related since there must be some hierarchy of checking for credentials, correct?  Ie: Checking for IAM role, then bash export, then config file... etc.  Currently I have found the aws-sdk-js to obey /only/ exports of ACCESS_KEY_ID and SECRET_ACCESS_KEY.  If IAM roles were to be implemented, would it be in a different section of the code?  ( config.js: 384 )\nNote:\nThis is of consequence only because there is no afaik no standard export for region defined, however it is implementable via the CONFIG_FILE.  It does not appear ruby SDK supports this feature either, so the request may not be valid.\n. Can you point to the section of code which is swallowing exceptions so I can disable it?  The inability to see where errors propagate from is making development very difficult (think typos or reference errors).\nNote for others having this issue logging the err object in event_emitter.js:126 makes debugging at least possible.\n. Can you point to the section of code which is swallowing exceptions so I can disable it?  The inability to see where errors propagate from is making development very difficult (think typos or reference errors).\nNote for others having this issue logging the err object in event_emitter.js:126 makes debugging at least possible.\n. @lsegal  I've written a 'deeperCopy' function into the AWS.util which makes the this.params item of the AWS.Request more-or-less unique.  However, I am not sure which is less surprising; that the data acted on is not a direct reference to the submitted data, or that your submitted data is not treated immutably.  \nIf it is not accepted, I believe either a new schema type from 'timestamp' should be created (say 'unixtimestamp') or the Simpleworkflow values should be written as 'number' and users can create the unixTimestamp on their own.  Because currently it makes SWF a non starter in JS.\nNote:  While I have attached an additional test function to util.spec.coffee, I do not know how to run these tests and I have never used coffeescript, it is being  submitted as a different commit.\n. @lsegal  I've written a 'deeperCopy' function into the AWS.util which makes the this.params item of the AWS.Request more-or-less unique.  However, I am not sure which is less surprising; that the data acted on is not a direct reference to the submitted data, or that your submitted data is not treated immutably.  \nIf it is not accepted, I believe either a new schema type from 'timestamp' should be created (say 'unixtimestamp') or the Simpleworkflow values should be written as 'number' and users can create the unixTimestamp on their own.  Because currently it makes SWF a non starter in JS.\nNote:  While I have attached an additional test function to util.spec.coffee, I do not know how to run these tests and I have never used coffeescript, it is being  submitted as a different commit.\n. ",
    "erikjjohnson": "Well, I'm glad it was a bug, because I was trying to access the attribute incorrectly regardless.  Thanks for making me look slightly less foolish.\n. FWIW, I agree the fundamental problem, and solution, lies within node itself. But that is of little consolation to those of us behind CONNECT proxies.\nI was able to get the aws sdk to work through my company's CONNECT proxy by piecing together the code and suggestions above.  With all due credit to the others in this thread, here is what I did in hopes it will get others unstuck:\n$ npm install aws-sdk\n$ npm install https-proxy-agent\nThen in your code:\n```\nvar aws = require('aws-sdk');\nvar HttpProxyAgent = require('https-proxy-agent');\nvar proxyAgent = new HttpProxyAgent(process.env.https_proxy || process.env.HTTPS_PROXY);\naws.config.httpOptions = { agent: proxyAgent };\n// Now you can use aws.s3, or aws.ec2, or etc\n```\nThis is valid if your company's proxy uses the CONNECT verb and is specified with an http:// protocol.  That is, your proxy string looks like this \"http://....\" even for https urls, then, fingers-crossed, the above code will hopefully help you. \n. ",
    "BharatMeda": "Thank you for the clarification. I have checked using your suggestion. 'x-amz-meta-metainfo' was not present in the headers. Got the following headers when printed through this.httpResponse \nx-amz-id-2,x-amz-request-id,date,x-amz-meta-filesize,last-modified,etag,accept-ranges,content-type,content-length,server\nFor the purpose that I have, I am able to store/retrieve file info using db (wanted to avoid by attaching the info to the file itself as meta). Once this issue is resolved, will revert to this straightforward approach\nIndeed, will take the latest SDK code and shift to Buffer implementation instead of \"base64\".\n. ",
    "matthew-dean": "I updated to SDK v2.0.18 and am not seeing metadata in the response data (only the ETag) and don't find it when I run this.httpResponse.headers through JSON.stringify(). I just see x-amz-id-2, x-amx-request-d, date, etag, content-length, and server in the headers.\n. Ohhh.... Okay this helps. Will try this.\n. Dang, still no dice. Updated CORS but no meta tags.\n. ",
    "lifeofzero": "I've actually run into the same issue....the first time I start my Node server the call fails, but subsequent calls work.  Here's the output from the debug line you wanted.  Notice the addition of us-standard in the first non-working call.\nNOT WORKING:\nENDPOINT { protocol: 'https:',\n  slashes: true,\n  host: 's3-us-standard.amazonaws.com',\n  hostname: '*.s3-us-standard.amazonaws.com',\n  href: 'https://s3-us-standard.amazonaws.com/',\n  pathname: '/',\n  path: '/',\n  port: 443,\n  constructor: { [Function: Endpoint] super**: [Function: Object] } }\nWORKING:\nENDPOINT { protocol: 'https:',\n  slashes: true,\n  host: 's3.amazonaws.com',\n  hostname: '*.s3.amazonaws.com',\n  href: 'https://s3.amazonaws.com/',\n  pathname: '/',\n  path: '/',\n  port: 443,\n  constructor: { [Function: Endpoint] super**: [Function: Object] } }\n. If I drop the region completely when I initialize the AWS sdk the problem goes away, if I specify the region I get the following:\nUnable to upload results { [PermanentRedirect: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.]\n  message: 'The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.',\n  code: 'PermanentRedirect',\n  name: 'PermanentRedirect',\n  statusCode: 301,\n  retryable: false }\n. I had it specified two ways...one being \"us-standard\" and then \"us-west-1\" both cause the first call to s3.client.putObject to fail with the above error messages and debug lines.  As soon as I removed region completely from the JSON representation of my AWS credentials I no longer get the error of { [NetworkingError: getaddrinfo ENOTFOUND] and the endpoint URI is constructed correctly.  It's strange that the first call failed yet all subsequent calls to the same function worked.\n. ",
    "clouddueling": "@lifeofzero fixed it.\nWhat caused it:\nWhen you set region to anything.\nWhat solved it:\nI remove the region param when initializing AWS.\naws.json:\n{ \"accessKeyId\": \"akid\", \"secretAccessKey\": \"secret\" }\nserver.js\nAWS.config.loadFromPath('./config/aws.json');\nvar s3 = new AWS.S3();\nThe issue is miscommunication:\nhttp://aws.amazon.com/sdkfornodejs/\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/node-examples.html\n\n\n. ",
    "aykutyaman": "+1 for CloudFront\n. +1 for CloudFront\n. ",
    "randallknutson": "Thanks for the update. Looking forward to the features.\n. ",
    "ChristopherBiscardi": "Yea, I'll fix it up.\n. Yea, I'll fix it up.\n. Previously, \nvar ep = new AWS.Endpoint('awsproxy.example.com', {sslEnabled: null});\nwould have resulted in using http, even if sslEnabled was true in AWS.config.\nSince it's a public api, Endpoint() should be able to handle unexpected values without failing.\nalso\nvar s3 = new AWS.S3({endpoint: ep, sslEnabled:null});\npasses the null to Client, which merges it into the default config\nAWS.util.merge(defaultConfig, config);\nand then uses it to create an endpoint\nthis.endpoint = new AWS.Endpoint(endpoint, this.config);\nwhich eventually makes it's way to the constructor function, creating the same effect as above.\nAs far as I know, null is not passed to Endpoint internally.\n. Previously, \nvar ep = new AWS.Endpoint('awsproxy.example.com', {sslEnabled: null});\nwould have resulted in using http, even if sslEnabled was true in AWS.config.\nSince it's a public api, Endpoint() should be able to handle unexpected values without failing.\nalso\nvar s3 = new AWS.S3({endpoint: ep, sslEnabled:null});\npasses the null to Client, which merges it into the default config\nAWS.util.merge(defaultConfig, config);\nand then uses it to create an endpoint\nthis.endpoint = new AWS.Endpoint(endpoint, this.config);\nwhich eventually makes it's way to the constructor function, creating the same effect as above.\nAs far as I know, null is not passed to Endpoint internally.\n. ",
    "shargors": "Loren, when you say ' I've now added it', do you mean that you added functionality to calculate ContentMD5 for putBucketLifecycle request if it is not provided?\n. When?  I got the SDK 2 days ago and it was not there....\n. Got you.  Thank you so much.\n. ",
    "blmarket": "Nice reason and resolution, thanks.\n. Nice reason and resolution, thanks.\n. Oh... I should have searched more. thanks!\n. Oh... I should have searched more. thanks!\n. ",
    "MikeBild": "Thanks.\n. Thanks.\n. ",
    "jechenique": "Hi lsegal,\nyou're totally right, now it works.\nThank you very much! :)\n. ",
    "vthunder": "Thanks a ton for fixing this - being able to catch exceptions at least at the run loop will make it much easier to debug issues.\n. Thanks a ton for fixing this - being able to catch exceptions at least at the run loop will make it much easier to debug issues.\n. Thanks! Apologies - I didn't pay attention to my Github notifications for a couple of months.\n. Thanks! Apologies - I didn't pay attention to my Github notifications for a couple of months.\n. ",
    "bigeasy": "Where does this stand. Exceptions are still being swallowed as far as I can tell. It would be nice, for the little utilities that I'm writing, to just allow a thrown exception propagate up and kill the program.\n``` javascript\nvar AWS = require('aws-sdk')\nAWS.config.loadFromPath(process.env.HOME + '/.aws')\nAWS.config.update({region: 'us-east-1'})\nnew AWS.EC2().describeInstances(function () { throw new Error })\n```\nThe above runs without writing any output.\n. Heres some work that made me think of this issue.\nhttps://github.com/bigeasy/strata/commit/87fccc13632e81db0cee20192e4aa41c5ee6a94b\nhttps://github.com/bigeasy/strata/commit/2912e8775454b8933b85bae820489e58a340a75a\n. Why not do this inside aws-sdk?\njavascript\ntry {\n    callback(error)\n} catch (e) {\n    AWS._thrownByUser = e\n    throw e\n}\nLater:\njavascript\ntry {\n    AWS._somethingThatWillCallTheBlockAbove()\n} catch (e) {\n    if (e === AWS._thrownByUser) {\n        throw e\n    }\n    AWS._handleError(e)\n}\n. @lsegal None of that that you said would happen would happen. That's not how JavaScript works.\n. There would be no async race condition because try/catch is synchronous. There is no way to asynchronously continue the unwinding of the stack. The reference is set after the callback, not before.\nThe leak is a non issue. You would only ever leak one exception. That would get all cleaned up when the exception unwound the stack to all the way to the event loop and the process crashed.\nIf the program continued, that would exceptional and incorrect. If it did continue, you could simply set the reference to null the next time you called the user back.\nfunction callUserBack (callback, error, result) {\n    AWS._thrownByUser = null\n    if (callback) {\n        try {\n            callback(error, result)\n        } catch (error) {\n            AWS._thrownByUser = error\n            throw error\n        }\n    }\n}\nBut, again, recovering from an exception thrown through async calls is wrong.\n. @mhart Because I'd rather not alter an exception thrown by the user in any way. That's just me. It's not like a package scoped property (not global) it is any less safe or correct.\n. Thank you for answering this issue and updating the documentation. I'm enjoying the APIs. Great work.\n. ",
    "tomcollins": "I am also seeing the behaviour described in the previous comment. Was this really fixed?\n. ",
    "joonas-fi": "This bit me as well. I had if (err) { throw err; } in my callback, and I'm using process.on('uncaughtException') to log exception and end process.\nI think this is really surprising behaviour and this should be fixed. It is outrageous for a library to just swallow exceptions and hide them.\n. @lsegal:\n\u251c\u2500\u252c aws-sdk@2.0.17\n\u2502 \u251c\u2500\u2500 aws-sdk-apis@3.1.8\n\u2502 \u251c\u2500\u252c xml2js@0.2.6\n\u2502 \u2502 \u2514\u2500\u2500 sax@0.4.2\n\u2502 \u2514\u2500\u2500 xmlbuilder@0.4.2\nFresh install from npm a few days ago.\nI'm using s3.putObject()\n(edited:)\n$ node --version\nv0.10.32\n. This is somewhat what I'm doing:\n```\nvar aws = require('aws-sdk');\nvar fs = require('fs');\n// set AWS credentials here...\nprocess.on('uncaughtException', function (err){\n    console.log('uncaughtException:', err);\n// re-throw: exits process\nthrow err;\n\n});\nvar s3 = new aws.S3();\ns3.putObject({\n    Bucket: 'my-bucket',\n    ACL: 'INVALID-ACL', // this should trigger error since its value is invalid\n    Key: 'foo.txt',\n    Body: fs.createReadStream('/tmp/foo.txt')\n}, function (err, data) {\n    if (err) {\n        console.log('throwing; not reaching process.on(\"uncaughtException\")', err);\n        throw err;\n    }\nconsole.log('operation was ok - should not happen');\n\n});\n```\nAnd this is the output:\nthrowing; not reaching process.on(\"uncaughtException\") { [InvalidArgument: null]\n  message: null,\n  code: 'InvalidArgument',\n  time: Sun Oct 19 2014 10:14:10 GMT+0000 (UTC),\n  statusCode: 400,\n  retryable: false }\n. Oh okay, I kind of understand. Still that does not make much sense, since surprising behaviour is always surprising.\nAs a user I wouldn't expect things to get handled differently if I just throw the original error object out, instead of throwing \"custom\" error object out.\nGood to know all tests still pass when the check is ripped out, sounds promising.\nAnyways, thanks for helping with this. I will use your trick of throw new Error(err) for now. I hope this gets fixed because I do think many people get bit by this.\n. ",
    "cgrebeld": "Ah, actually this was my mistake - my apologies!\n. ",
    "danmilon": "Knox has a similar issue. Perhaps it's the same root cause. Please share your findings! learnboost/knox#116.\n. ",
    "milyord": "After playing with it for a few minutes and also referencing http://docs.aws.amazon.com/ses/2010-12-01/APIReference/API_Content.html I realized that the info is there it's just that the indentation of the bullets is messed up. \n. After playing with it for a few minutes and also referencing http://docs.aws.amazon.com/ses/2010-12-01/APIReference/API_Content.html I realized that the info is there it's just that the indentation of the bullets is messed up. \n. Same problem with params.Message.Body its properties are not indented as well. So the bottom line is that the documentation is there but with the wrong indentation it can be a bit misleading. \nI don't know it might be just me being stupid but it threw me off for a minute.\n. Same problem with params.Message.Body its properties are not indented as well. So the bottom line is that the documentation is there but with the wrong indentation it can be a bit misleading. \nI don't know it might be just me being stupid but it threw me off for a minute.\n. Cool, thanks.\nI like that you guys are on top of that stuff, keep up the good work.\n. Cool, thanks.\nI like that you guys are on top of that stuff, keep up the good work.\n. ",
    "ghost": "Hi,\nI just try this example\n``` javascript\nvar AWS = require('aws-sdk');\nAWS.config.update({accessKeyId:'something-here', secretAccessKey:'another-here'});\nAWS.config.update({region:'eu-west-1'});\nvar s3 = new AWS.S3();\ns3.client.createBucket({Bucket: 'myBucket'}, function() {\n    var data = {Bucket: 'myBucket', Key: 'myKey', Body: 'Hello!'};\n    s3.client.putObject(data, function(err, data) {\n        if (err) {\n            console.log(\"Error uploading data: \", err);\n        } else {\n            console.log(\"Successfully uploaded data to myBucket/myKey\");\n        }\n    });\n});\n```\nI'll try with the master branch.\n. Hi,\nI just try this example\n``` javascript\nvar AWS = require('aws-sdk');\nAWS.config.update({accessKeyId:'something-here', secretAccessKey:'another-here'});\nAWS.config.update({region:'eu-west-1'});\nvar s3 = new AWS.S3();\ns3.client.createBucket({Bucket: 'myBucket'}, function() {\n    var data = {Bucket: 'myBucket', Key: 'myKey', Body: 'Hello!'};\n    s3.client.putObject(data, function(err, data) {\n        if (err) {\n            console.log(\"Error uploading data: \", err);\n        } else {\n            console.log(\"Successfully uploaded data to myBucket/myKey\");\n        }\n    });\n});\n```\nI'll try with the master branch.\n. when I try this code without AWS.config.update({region:'eu-west-1'}); or with a different region AWS.config.update({region:'us-east-1'}); I have got this error :\nError uploading data:  { [AccessDenied: Access Denied]\n  message: 'Access Denied',\n  code: 'AccessDenied',\n  name: 'AccessDenied',\n  statusCode: 403,\n  retryable: false }\nWhich means ? Need to change permissions ?\n. when I try this code without AWS.config.update({region:'eu-west-1'}); or with a different region AWS.config.update({region:'us-east-1'}); I have got this error :\nError uploading data:  { [AccessDenied: Access Denied]\n  message: 'Access Denied',\n  code: 'AccessDenied',\n  name: 'AccessDenied',\n  statusCode: 403,\n  retryable: false }\nWhich means ? Need to change permissions ?\n. All works fine for me now but I think there is something weird that prevents me upload a file in a bucket that I just created.\nHere you can find my code to upload a file in a bucket recently (or not) created : http://pastebin.com/X2x2TYzQ\nForget to show you the output :\n2013-07-09T11:28:16.428Z - trace: UPLOAD : UPLOAD_FILES\n2013-07-09T11:28:16.430Z - data: @FILES : [object Object]\n2013-07-09T11:28:16.430Z - data: @BUCKET_NAME : mllXkdjSi8736gdjUUEyhhsbkfliofuzbb00D9f\n2013-07-09T11:28:16.431Z - data: @RESPONSE_FORMAT : json\n2013-07-09T11:28:16.432Z - trace: UPLOADER : SEND_FILES_TO_AMAZON_S3\n2013-07-09T11:28:16.432Z - trace: UPLOADER : IS_BUCKET_CREATED\n2013-07-09T11:28:16.855Z - debug:  Buckets=[Name=mllxkdjsi8736gdjuueyhhsbkfliofuzbb00d9f, CreationDate=Tue Jul 09 2013 11:16:01 GMT+0000 (UTC)], ID=08585ce13e82846e44f03248bc73f2bc80e847ed3a529f3d53d3723228ba6fd8, DisplayName=amazon, RequestId=61C098432063348A\n2013-07-09T11:28:16.857Z - info: bucket found\n2013-07-09T11:28:16.858Z - trace: UPLOADER : SEND_FILES\n2013-07-09T11:28:16.858Z - debug:  size=4746, path=/tmp/db16391116623ebebc829db08ff8422e, name=Icon@2x.jpg, type=image/jpeg\n2013-07-09T11:28:16.859Z - trace: UPLOADER : SEND_FILE\nIf the callback send me an error I display an error message time - error : MESSAGE_ERROR. If there is no error I display the response data time - debug : DATA. But nothing appears after 10 minutes.\nI'm using v1.3.2 of the library.\nI try also to change the sendFile function like this :\n``` js\nfunction sendFile(options, done) {\n    logger.trace('UPLOADER : SEND_FILE');\ns3.putObject(options)\n    .done(function (data) {\n        logger.debug(data);\n\n        done();\n    })\n    .fail(function (err) {\n        done(err);\n    })\n    .send();\n\n}\n```\nBut same thing no message displayed.\n. All works fine for me now but I think there is something weird that prevents me upload a file in a bucket that I just created.\nHere you can find my code to upload a file in a bucket recently (or not) created : http://pastebin.com/X2x2TYzQ\nForget to show you the output :\n2013-07-09T11:28:16.428Z - trace: UPLOAD : UPLOAD_FILES\n2013-07-09T11:28:16.430Z - data: @FILES : [object Object]\n2013-07-09T11:28:16.430Z - data: @BUCKET_NAME : mllXkdjSi8736gdjUUEyhhsbkfliofuzbb00D9f\n2013-07-09T11:28:16.431Z - data: @RESPONSE_FORMAT : json\n2013-07-09T11:28:16.432Z - trace: UPLOADER : SEND_FILES_TO_AMAZON_S3\n2013-07-09T11:28:16.432Z - trace: UPLOADER : IS_BUCKET_CREATED\n2013-07-09T11:28:16.855Z - debug:  Buckets=[Name=mllxkdjsi8736gdjuueyhhsbkfliofuzbb00d9f, CreationDate=Tue Jul 09 2013 11:16:01 GMT+0000 (UTC)], ID=08585ce13e82846e44f03248bc73f2bc80e847ed3a529f3d53d3723228ba6fd8, DisplayName=amazon, RequestId=61C098432063348A\n2013-07-09T11:28:16.857Z - info: bucket found\n2013-07-09T11:28:16.858Z - trace: UPLOADER : SEND_FILES\n2013-07-09T11:28:16.858Z - debug:  size=4746, path=/tmp/db16391116623ebebc829db08ff8422e, name=Icon@2x.jpg, type=image/jpeg\n2013-07-09T11:28:16.859Z - trace: UPLOADER : SEND_FILE\nIf the callback send me an error I display an error message time - error : MESSAGE_ERROR. If there is no error I display the response data time - debug : DATA. But nothing appears after 10 minutes.\nI'm using v1.3.2 of the library.\nI try also to change the sendFile function like this :\n``` js\nfunction sendFile(options, done) {\n    logger.trace('UPLOADER : SEND_FILE');\ns3.putObject(options)\n    .done(function (data) {\n        logger.debug(data);\n\n        done();\n    })\n    .fail(function (err) {\n        done(err);\n    })\n    .send();\n\n}\n```\nBut same thing no message displayed.\n. Yes I'm using the latest version of the library. I change also the content of the function like you said.\n``` js\nfunction sendFile(options, done) {\n    logger.trace('UPLOADER : SEND_FILE');\ns3.putObject(\n    options,\n    function (err, data) {\n        if (err) {\n            return done(err);\n        }\n\n        logger.debug(data);\n\n        done();\n    }\n);\n\n}\n```\nI solved my issue too. I make a mistake trying to use a callback that is not define (line 101 to 104 in my pastebin link).\nI leave this issue open for the other questions.\nThank you and sorry for the inconvenience.\n. Yes I'm using the latest version of the library. I change also the content of the function like you said.\n``` js\nfunction sendFile(options, done) {\n    logger.trace('UPLOADER : SEND_FILE');\ns3.putObject(\n    options,\n    function (err, data) {\n        if (err) {\n            return done(err);\n        }\n\n        logger.debug(data);\n\n        done();\n    }\n);\n\n}\n```\nI solved my issue too. I make a mistake trying to use a callback that is not define (line 101 to 104 in my pastebin link).\nI leave this issue open for the other questions.\nThank you and sorry for the inconvenience.\n. I experienced this issue today, and fortunately, the AWS NodeJS SDK now has a config option to fix the clock offset by itself:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#correctClockSkew-property\n. I experienced this issue today, and fortunately, the AWS NodeJS SDK now has a config option to fix the clock offset by itself:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#correctClockSkew-property\n. Definitely more of non-traditional unit test, as I am using live AWS services (respective to the environment app is running in, e.g. 'integ_test' would correlate to an S3 bucket as opposed to 'production') w/ local mocks for what I can control.\nThis is extremely helpful! Thank you!\n. Definitely more of non-traditional unit test, as I am using live AWS services (respective to the environment app is running in, e.g. 'integ_test' would correlate to an S3 bucket as opposed to 'production') w/ local mocks for what I can control.\nThis is extremely helpful! Thank you!\n. Now that I think of it, this is still not ideal (unless I'm missing something). Ideally, the Lambda function code remains the same (not the case above) regardless of location (AWS / local), and the aws-sdk provides some means of passing in temporary credentials / credentials which normally would get set by invoker. For example:\nlambda.invokeAsync(temporaryCredentials,\n...would invoke a remote (AWS) Lambda, which looks like:\nvar aws = require('aws-sdk');\nvar s3 = new aws.S3();\nexports.handler = function(event, context) {\n  s3.getObject(...\n};\nFrom your example, I'd have to have something like what's below to be able to require my lambda Node.js Module locally and invoke it:\nvar aws = require('aws-sdk'),\n  s3;\nexports.handler = function(event, context) {\n  if (event.flag) {\n    // Credentials from AWS are hard-coded or passed in, do temp creds even help process here?\n    s3 = new aws.S3(temporaryCredentials);\n  }\n  s3.getObject(...\n};\n...say:\nvar lambda = require('./lambda');\nvar event = {\n  flag: 'useLocalCredentials'\n};\nlambda.handler(event, {\n  done: function() {}\n});\n. Now that I think of it, this is still not ideal (unless I'm missing something). Ideally, the Lambda function code remains the same (not the case above) regardless of location (AWS / local), and the aws-sdk provides some means of passing in temporary credentials / credentials which normally would get set by invoker. For example:\nlambda.invokeAsync(temporaryCredentials,\n...would invoke a remote (AWS) Lambda, which looks like:\nvar aws = require('aws-sdk');\nvar s3 = new aws.S3();\nexports.handler = function(event, context) {\n  s3.getObject(...\n};\nFrom your example, I'd have to have something like what's below to be able to require my lambda Node.js Module locally and invoke it:\nvar aws = require('aws-sdk'),\n  s3;\nexports.handler = function(event, context) {\n  if (event.flag) {\n    // Credentials from AWS are hard-coded or passed in, do temp creds even help process here?\n    s3 = new aws.S3(temporaryCredentials);\n  }\n  s3.getObject(...\n};\n...say:\nvar lambda = require('./lambda');\nvar event = {\n  flag: 'useLocalCredentials'\n};\nlambda.handler(event, {\n  done: function() {}\n});\n. > Keep in mind that Lambda interacts with your process by exporting environment variables to your process.\nThat is what I was missing! Thanks again!\n. > Keep in mind that Lambda interacts with your process by exporting environment variables to your process.\nThat is what I was missing! Thanks again!\n. It helps! There are a few things wrong with my logic:\n- Handler name should have been upload-test.handler\n- Zipped file didn't have a .js extension\nThe AWS Lambda console was useful to debug, but conflicted with the API response; I guess I was expecting the latter to have erred.\nThanks!\n. It helps! There are a few things wrong with my logic:\n- Handler name should have been upload-test.handler\n- Zipped file didn't have a .js extension\nThe AWS Lambda console was useful to debug, but conflicted with the API response; I guess I was expecting the latter to have erred.\nThanks!\n. I've seen the version from the package.json in /node_modules/aws-sdk and it is the 2.0.31.\nNow I'm using the version 2.3.8 and all seems to work just fine.\nI can close the issue,\nRegards.\n. I've seen the version from the package.json in /node_modules/aws-sdk and it is the 2.0.31.\nNow I'm using the version 2.3.8 and all seems to work just fine.\nI can close the issue,\nRegards.\n. @chrisradek \nYes, we want to use the TemporaryCredentials with the master credentials provided by the default credentials provider. The aim was not to have the credentials provider as a fallback but as the means to generate the temporary credentials. (e.g. using ec2metadata credentials to assume a different role). By global credentials I meant AWS.config.credentials which appear to be the only credentials that the temporary credentials pulls from for its master creds. \n. @sandangel \nin typescript use * as\nimport * as AWS from \"aws-sdk/global\";. @sandangel \nin typescript use * as\nimport * as AWS from \"aws-sdk/global\";. I have found the problem. I have forgotten the Prefix field.. I have found the problem. I have forgotten the Prefix field.. But there is a problem with the sdk which asks field non-required. For example, this is my JSON:\nlet params = {\n                DistributionConfig: { /* required */\n                    CallerReference: new Date().toString(), /* required */\n                    Comment: 'disable cloudfront', /* required */\n                    DefaultCacheBehavior: { /* required */\n                        ForwardedValues: { /* required */\n                            Cookies: { /* required */\n                                Forward: 'none', /* required */\n                            },\n                            QueryString: false, /* required */\n                        },\n                        MinTTL: 0, /* required */\n                        TargetOriginId: \"S3-\" + zipBucketName, /* required */\n                        TrustedSigners: { /* required */\n                            Enabled: false, /* required */\n                            Quantity: 0, /* required */\n                        },\n                        ViewerProtocolPolicy: 'allow-all', /* required */\n                        Compress: false,\n                    },\n                    Enabled: false, /* required */\n                    Origins: { /* required */\n                        Quantity: 1, /* required */\n                        Items: [\n                            {\n                                DomainName: zipBucketName + \".s3.amazonaws.com\", /* required */\n                                Id: \"S3-\" + zipBucketName, /* required */\n                                S3OriginConfig: {\n                                    OriginAccessIdentity: '' /* required */\n                                }\n                            },\n                            /* more items */\n                        ]\n                    },\n                    DefaultRootObject: '',\n                    PriceClass: 'PriceClass_100',\n                },\n                Id: id, /* required */\n                IfMatch: data.ETag\n            };\nI have the following error:\n{ IllegalUpdate: Aliases are missing for the resource.                                                       \n    at Request.extractError (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\protocol\\rest_xml.js:44:29)    \n    at Request.callListeners (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\sequential_executor.js:105:20)\n    at Request.emit (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\sequential_executor.js:77:10)          \n    at Request.emit (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\request.js:668:14)                     \n    at Request.transition (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\request.js:22:10)                \n    at AcceptorStateMachine.runTo (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\state_machine.js:14:12)  \n    at E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\state_machine.js:26:10                               \n    at Request.<anonymous> (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\request.js:38:9)                \n    at Request.<anonymous> (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\request.js:670:12)              \n    at Request.callListeners (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\sequential_executor.js:115:18)\n  message: 'Aliases are missing for the resource.',                                                          \n  code: 'IllegalUpdate',                                                                                     \n  time: 2016-12-15T17:45:26.882Z,                                                                            \n  requestId: '46f6c0a0-c2ee-11e6-bff2-a73c203c5e7b',                                                         \n  statusCode: 400,                                                                                           \n  retryable: false,                                                                                          \n  retryDelay: 76.41972873266478 }\nBut, if you look at the doc, you can see that the field is not mandatory.. But there is a problem with the sdk which asks field non-required. For example, this is my JSON:\nlet params = {\n                DistributionConfig: { /* required */\n                    CallerReference: new Date().toString(), /* required */\n                    Comment: 'disable cloudfront', /* required */\n                    DefaultCacheBehavior: { /* required */\n                        ForwardedValues: { /* required */\n                            Cookies: { /* required */\n                                Forward: 'none', /* required */\n                            },\n                            QueryString: false, /* required */\n                        },\n                        MinTTL: 0, /* required */\n                        TargetOriginId: \"S3-\" + zipBucketName, /* required */\n                        TrustedSigners: { /* required */\n                            Enabled: false, /* required */\n                            Quantity: 0, /* required */\n                        },\n                        ViewerProtocolPolicy: 'allow-all', /* required */\n                        Compress: false,\n                    },\n                    Enabled: false, /* required */\n                    Origins: { /* required */\n                        Quantity: 1, /* required */\n                        Items: [\n                            {\n                                DomainName: zipBucketName + \".s3.amazonaws.com\", /* required */\n                                Id: \"S3-\" + zipBucketName, /* required */\n                                S3OriginConfig: {\n                                    OriginAccessIdentity: '' /* required */\n                                }\n                            },\n                            /* more items */\n                        ]\n                    },\n                    DefaultRootObject: '',\n                    PriceClass: 'PriceClass_100',\n                },\n                Id: id, /* required */\n                IfMatch: data.ETag\n            };\nI have the following error:\n{ IllegalUpdate: Aliases are missing for the resource.                                                       \n    at Request.extractError (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\protocol\\rest_xml.js:44:29)    \n    at Request.callListeners (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\sequential_executor.js:105:20)\n    at Request.emit (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\sequential_executor.js:77:10)          \n    at Request.emit (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\request.js:668:14)                     \n    at Request.transition (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\request.js:22:10)                \n    at AcceptorStateMachine.runTo (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\state_machine.js:14:12)  \n    at E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\state_machine.js:26:10                               \n    at Request.<anonymous> (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\request.js:38:9)                \n    at Request.<anonymous> (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\request.js:670:12)              \n    at Request.callListeners (E:\\GitHub\\partyhard\\cli\\node_modules\\aws-sdk\\lib\\sequential_executor.js:115:18)\n  message: 'Aliases are missing for the resource.',                                                          \n  code: 'IllegalUpdate',                                                                                     \n  time: 2016-12-15T17:45:26.882Z,                                                                            \n  requestId: '46f6c0a0-c2ee-11e6-bff2-a73c203c5e7b',                                                         \n  statusCode: 400,                                                                                           \n  retryable: false,                                                                                          \n  retryDelay: 76.41972873266478 }\nBut, if you look at the doc, you can see that the field is not mandatory.. After changing tsconfig.app.json, it worked for me. Changing tsconfig.json hasn't solved this issue. [2]. After changing tsconfig.app.json, it worked for me. Changing tsconfig.json hasn't solved this issue. [2]. @danieladams456 solution worked for me. Seems like a bug in the SDK.... @danieladams456 solution worked for me. Seems like a bug in the SDK.... Link for those coming here after the fact: https://github.com/aws/aws-sdk-js#with-angular. Thanks Jason, I can verify that that works now. This should be clarified in the documentation - how do we get an update request into the doc team?. Thanks Jason, I can verify that that works now. This should be clarified in the documentation - how do we get an update request into the doc team?. @chrisradek  Ok Thanks.. @chrisradek  Ok Thanks.. I'm also using AWS.S3 and AWS.S3.ManagedUpload but correctClockSkew just make the api to retry the upload instead of failing right away.. I'm also using AWS.S3 and AWS.S3.ManagedUpload but correctClockSkew just make the api to retry the upload instead of failing right away.. Doc says Credentials.get() helps to find that credentials have been expired. Will callback for this method will return an error for session expired?. Replicated.. Replicated.. @AllanFly120 explain. @AllanFly120 explain. ",
    "adieuadieu": "I've experienced this issue when using a bucket not in the us-standard region. I depending on how I was trying to put an object to S3, I'd either get the \"PermanentRedirect\" error, or a \"SigningError: Missing credentials in config\" error.\nI was able to get things working by instantiating a new S3 object and specifying the non-us-standard endpoint:\njavascript\n    s3 = new AWS.S3({\n        endpoint: 'https://s3-ap-northeast-1.amazonaws.com'\n    });\n. ",
    "worp1900": "I am running into the same problem, using either an IAM Administrator's Access Key ID (AKID) & Secret Access Key (SAK) (standard permission pattern \"Administrator\" as can be found in the console) or even using the root account's AKID & SAK, not using IAM at all.\nMy code is the tutorial provided here on GitHub:\njavascript\n1 // Load the AWS SDK for Node.js\n2 var AWS = require('aws-sdk');\n3 \n4 /**\n5  * Don't hard-code your credentials!\n6  * Load them from disk or your environment instead.\n7  */\n8 // AWS.config.update({accessKeyId: 'AKID', secretAccessKey: 'SECRET'});\n9 \n10 // Instead, do this:\n11  AWS.config.loadFromPath('./modules/database/credentials.json');\n12 \n13 // Set your region for future requests.\n14 AWS.config.update({region: 'us-east-1'});\n15 \n16 // Create a bucket using bound parameters and put something in it.\n17 var s3bucket = new AWS.S3({params: {Bucket: 'myBucket'}});\n18 s3bucket.createBucket(function() {\n19  var data = {Key: 'myKey', Body: 'Hello!'};\n20   s3bucket.putObject(data, function(err, data) {\n21     if (err) {\n22       console.log(\"Error uploading data: \", err);\n23     } else {\n24       console.log(\"Successfully uploaded data to myBucket/myKey\");\n25     }\n26   });\n27 });\ncredentials.json looks like this:\njavascript\n{ \n    \"accessKeyId\": \"MYACCESSKEYID\",\n    \"secretAccessKey\": \"SECRETACCESSKEY\"\n}\nThis gives me\njavascript\nError uploading data:  { [AccessDenied: Access Denied]\n  message: 'Access Denied',\n  code: 'AccessDenied',\n  name: 'AccessDenied',\n  statusCode: 403,\n  retryable: false }\nwhen called with \"node aws.js\" in MS PowerShell. (SDK for PowerShell installed, although that shouldn't matter, I assume.)\nI tried:\n1. Leaving out\njavascript\n14 AWS.config.update({region: 'us-east-1'});\nto default to the standard region. => Same error.\n2. Including the AKID & SAK directly in the file\njavascript\n8  AWS.config.update({accessKeyId: 'MYACCESSKEYID', secretAccessKey: 'SECRETACCESSKEY'});\n=> Same error.\n3. Using an IAM Administrator's keys as well as the account root keys\n=> Same error both ways.\n4. Using @adieuadieu's solution\njavascript\n17 var s3bucket = new AWS.S3({params: {Bucket: 'myBucket', endpoint: 'https://s3-ap-northeast-1.amazonaws.com'}});\n=> Same error.\n5. Concretely specifying a different region (@adieuadieu 's line was NOT in the code) as found on Configuring the SDK\njavascript\n13 // Set your region for future requests.\n14 AWS.config.update({region: 'us-west-1'});\nAs well as\njavascript\n13 // Set your region for future requests.\n14 AWS.config.update({region: 'us-west-2'});\n=> Error:\njavascript\nError uploading data:  { [PermanentRedirect: The bucket you are attempting to access must be addressed using the specifi\ned endpoint. Please send all future requests to this endpoint.]\n  message: 'The bucket you are attempting to access must be addressed using the specified endpoint. Please send all futu\nre requests to this endpoint.',\n  code: 'PermanentRedirect',\n  name: 'PermanentRedirect',\n  statusCode: 301,\n  retryable: false }\n6. Using endpoints provided at AWS Regions and Endpoints under \"Amazon Simple Storage Service (S3)\" instead of regions\nUsing @adieuadieu 's solution:\njavascript\n17 var s3bucket = new AWS.S3({params: {Bucket: 'myBucket', endpoint: 's3-us-west-2.amazonaws.com'}});\n=> PermanentRedirect\nAs well as updating the config:\njavascript\n14 AWS.config.update({endpoint: 's3-us-west-2.amazonaws.com'});\n=> AccessDenied\nSince I not have any clue on further combination and tries, could you point me in the direction where I am going wrong? I have probably done some weird stuff here, but the first one was simply the tutorial which didn't work.\nIf I can assists in posting any additional data, for example applying your \njavascript\nconsole.log(this.httpResponse.body.toString()); // print response body\nto any of the versions above, I would be more than happy to.\nGreatly appreciated!\nSven\n. ",
    "mave99a": "I have exactly the same issue, and exactly same as @worp1900 described. \n. ",
    "pilani": "Request:\n{ method: 'POST',\n  path: '/',\n  headers: \n   { 'User-Agent': 'aws-sdk-nodejs/v0.9.7-pre.8 linux/v0.8.17',\n     'Content-Type': 'application/x-www-form-urlencoded; charset=utf-8',\n     'Content-Length': 228 },\n  body: 'AWSAccessKeyId=*something*&Action=DescribeSecurityGroups&Signature=WVNJG7aKN3fBd%2FFIivanvr3jRkZSrXiD6GWKfrMCAwI%3D&SignatureMethod=HmacSHA256&SignatureVersion=2&Timestamp=2013-04-11T07%3A11%3A00.620Z&Version=2013-02-01',\n  endpoint: \n   { protocol: 'https:',\n     slashes: true,\n     host: 'ec2.us-east-1.amazonaws.com',\n     hostname: 'ec2.us-east-1.amazonaws.com',\n     href: 'https://ec2.us-east-1.amazonaws.com/',\n     pathname: '/',\n     path: '/',\n     port: 443,\n     constructor: { [Function: Endpoint] __super__: [Function: Object] } },\n  region: 'us-east-1',\n  params: \n   { params: \n      [ [Object],\n        [Object],\n        [Object],\n        [Object],\n        [Object],\n        [Object],\n        [Object] ] } }\nResponse:\n{ statusCode: 403,\n  headers: \n   { 'transfer-encoding': 'chunked',\n     date: 'Thu, 11 Apr 2013 07:11:02 GMT',\n     server: 'AmazonEC2' },\n  body: <Buffer 3c 3f 78 6d 6c 20 76 65 72 73 69 6f 6e 3d 22 31 2e 30 22 20 65 6e 63 6f 64 69 6e 67 3d 22 55 54 46 2d 38 22 3f 3e 0a 3c 52 65 73 70 6f 6e 73 65 3e 3c 45 ...> }\n. ",
    "pagameba": "I'm getting this error (SignatureDoesNotMatch) with s3 getBucketTagging using v0.9.8-pre.9 installed via npm.  \n```\nvar aws = require('aws-sdk');\naws.config.loadFromPath(path.join('.', 'aws-credentials.json'));\naws.config.update({region: 'us-east-1'});\ns3 = new aws.S3();\ns3.client.getBucketTagging({\n  Bucket: ''\n}, function(err, data) {\n  if (err) {\n    return console.log('Error getting bucket tagging: ' + JSON.stringify(err));\n  }\n  console.log(JSON.stringify(data));\n});\n```\noutput:\nError getting bucket tagging: {\"message\":\"The request signature we calculated does not match the signature you provided. Check your key and signing method.\",\"code\":\"SignatureDoesNotMatch\",\"name\":\"SignatureDoesNotMatch\",\"statusCode\":403,\"retryable\":false}\nUsing the same s3 client, other operations (at least listBuckets) do work.\nAny help would be appreciated.  I'm trying to set up cost allocation billing and I want to add a standard set of tags to all our assets and doing so by hand would be ... tedious :)\n. References my comment in #86\n. ",
    "laser": "@pilani - It's been a long while since you posted this question, but, I ran into a similar issue. \nEssentially, the library I was using to generate the Signature query string parameter wasn't escaping spaces. So, every so often a value would be generated that contained a space - and I'd see a 403. Ensuring proper encoding of URI query string parameter-values did the trick.\nErin\nCC @lsegal \n. @pilani - It's been a long while since you posted this question, but, I ran into a similar issue. \nEssentially, the library I was using to generate the Signature query string parameter wasn't escaping spaces. So, every so often a value would be generated that contained a space - and I'd see a 403. Ensuring proper encoding of URI query string parameter-values did the trick.\nErin\nCC @lsegal \n. ",
    "krilnon": "In case this helps anyone: I was getting this same signature failure, but the mistake I was making was including an extra HTTPS header (Content-type) which is apparently used to calculate the signature. \nFrustrating to track down, for sure, but ultimately my fault. \n. ",
    "bedney": "I fought with this error for 2 days, until I generated a new set of keys for my account, after which everything worked magically. Note that my old keys were very old (from 2006). A nice error message of Expired Keys or Obsolete keys or something would have been very helpful here. I realize that this is probably not something that can be detected by the JS library, but maybe a request upstream to add this to AWS in general would be in order here...\n. ",
    "erdogankaya": "i try to create bucket \nvar http   = require('http')\nvar crypto = require(\"crypto\")\nvar isoDate =new Date();\nvar sc     = crypto.createHmac('sha1', \"my-key\").update(new Buffer(\"PUT\\n\\n\\n\"+isoDate+\"\\n/erka\", 'utf-8')).digest('base64');\nvar options = {\nport:80,\nhostname: \"s3.kaya.pvt\", \nheaders:{\n    Host: \"erka.s3.kaya.pvt\", \n    Authorization: 'AWS our-id:'+sc,\n    Date: new Date(),\n    \"Content-Length\": 0,\n    },\n}\ncallback = function(response) {\nvar str = '';\nresponse.on('data', function (chunk) {\n  console.log(\"-------------\");\n  str += chunk;\n  console.log(chunk.toString());\n});\nresponse.on('end', function () {\n  console.log(\"****\");\n  console.log(str);\n});\n}\nhttp.request(options, callback).end();\nthis return \nxml version=\"1.0\" encoding=\"UTF-8\"?SignatureDoesNotMatchThe request signature we calculated does not match the signature you provided. Check your AWS secret access key and signing method.erka/\nwhere is problem? anyone can help?\n. @chrisradek \nwe use our s3 . Dont use amazon s3. so i cant use aws-sdk. i try to http protocol for create bucket. but its said signature fault. Can u help?\n. ",
    "chrisradek": "@erdogankaya \nIt doesn't appear as though you're using the AWS SDK. It exposes an operation to create an s3 bucket:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#createBucket-property\nPlease take a look at our getting started guide for information on how to configure and use the SDK:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/node-intro.html\n. @simoncpu \nWhat version of node are you encountering this issue with?\n. @verveguy \nCan you share your example where an uncatchable error is being thrown?\nLooking at @jzelenkov's example, adding an error event listener to the read stream would catch errors thrown by the service, such as NoSuchKey if trying to download an object that doesn't exist.\n. How are you attempting to use s3.getObject with both promises and streams? The two aren't currently supported together.\nUsing just promises, the catch handler on your promise chain should capture the error.\nPart of the underlying problem with combining streams and promises in the same interface is that promises only really care about 2 states: resolve and reject. Streams have more than 2 states though, such as data/readable, as well as error and end.\nWhat we would need to do is wrap the current stream that we return through the SDK with a new type that exposes promise methods. We'd still have to deal with the question of whether calling then on the stream will pass the full content as a buffer to the resolving function or not. Since streams are often used to pipe data from one source to another in a memory-efficient way, keeping an internal store of all the data so that it can be returned when using promises would be a huge negative for many users.\nIt seems like converting streams to promises is something that should be handled outside of the SDK as it would be an important choice made by the user. There are 3rd party libraries that will do just that.\nAs for the argument for not having to add event handlers to catch errors on the readstream, that's how many event emitter-based APIs work in node.js.  Even calling fs.createReadStream with an invalid path will throw an uncatchable error if you do a simple try/catch, and requires the user to attach an error listener to the stream in order to handle the error. Maybe I'm missing something about this argument, please elaborate if so.\n@lsegal beat me to the question. Can you share an example of what you're doing? It could be that there really is something the SDK is doing wrong.\n. @lsegal \nThank you for your explanation.\n@ronyrun \nWere your errors in 2.4.1 sporadic or consistent? Let us know if you have better results using s3.upload()!\n. Webpack support is something we're looking into, but don't currently have an estimate on when work will be completed to support it. I'm reopening this as a feature request for now.\n. https://github.com/aws/aws-sdk-js/issues/603#issuecomment-166480762 had a very good suggestion, though it looks like it doesn't include any of the service clients.\nBy supplying the transform that the SDK uses with browserify (https://github.com/aws/aws-sdk-js/blob/master/dist-tools/transform.js) instead, it looks like the SDK will work properly with webpack. I had to hack around to actually have our transform used instead of brfs, but it shouldn't be too difficult if we can release our transform as a separate npm module.\nThis would also mean that you could define process.env.AWS_SERVICES to contain the list of services you want included with the SDK, which may result in a smaller SDK footprint.\nSo, to include an SDK that has just S3, my webpack.config.js file for my application would look something like this:\njavascript\nprocess.env.AWS_SERVICES = 's3'; // optional\nmodule.exports = {\n    entry: './src/index.js',\n    output: {\n        path: 'builds',\n        filename: 'bundle.js'\n    },\n    // Relevant config starts here\n    node: {\n        fs: 'empty'\n    },\n    module: {\n        loaders: [\n            {\n                test: /aws-sdk/,\n                loaders: [\n                    'transform?aws'\n                ] \n            },\n            {\n                test: /\\.json$/, loaders: ['json']\n            }\n        ]\n    },\n};\nBecause we use browserify for our browser builder, and the way the SDK is currently architected, it will take a lot of work to update the SDK so that it feels more 'natural' to use with webpack. That said, how would everyone feel about including configuration like the above, at the very least as an interim solution.\n. Just to follow up, the above configuration can actually be used today with a modification to what gets passed to the transform loader.\nFirst, you need to make sure you npm install the json-loader and transform loader:\nnpm install json-loader --save-dev\nnpm install transform-loader --save-dev\nThen assuming you've also npm installed the aws-sdk, the config would look like this:\njavascript\nprocess.env.AWS_SERVICES = 's3'; // optional\nmodule.exports = {\n    entry: './src/index.js',\n    output: {\n        path: 'builds',\n        filename: 'bundle.js'\n    },\n    // Relevant config starts here\n    node: {\n        fs: 'empty'\n    },\n    module: {\n        loaders: [\n            {\n                test: /aws-sdk/,\n                loaders: [\n                    'transform?aws-sdk/dist-tools/transform'\n                ] \n            },\n            {\n                test: /\\.json$/, loaders: ['json']\n            }\n        ]\n    },\n};\nThere will be a few warnings generated but they should be safely ignored. Both warnings are irrelevant when using the SDK in the browser with the above transform.\nAgain, I know it isn't the ideal way for the SDK to work with webpack, but I hope this provides an interim solution until we can make it better.\n. Hello everyone,\nGiven all the feedback, we're actively looking at how to make the experience of using the AWS SDK with webpack better.\nFirst, PR #1116 adds a sample webpack configuration to the SDK's README to help users get started. Thank you @basarat for providing the configuration you use!\nIn the short-term, we're looking into how we can make the SDK work 'out of the box' with webpack without making any breaking changes. The goal here is to make the browser SDK work, but not necessarily support custom builds (additional webpack configuration would be necessary for that.)\nTo fully be able to support webpack and other bundlers, we will have to make some breaking changes. We're actively looking into what is possible with a major version bump, but don't have a timeline for that yet.\n. I've posted pr #1117 as a WIP for webpack support out of the box. I've done some testing, including with the create-react-app project to verify the SDK is being bundled and works correctly.\nIt still needs further testing but given the amount of demand, I wanted to post something as soon as possible to give those who want a chance to start playing with it.\n. @VladShcherbin \nThe latest changes in the PR allow for pulling in individual services.\nIn your code, instead of writing something like:\njavascript\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\nYou could write something like:\njavascript\nvar S3 = require('aws-sdk/browser/s3');\nvar s3 = new S3();\nThen tools like webpack and browserify will only pull in S3 and the SDK core.\nIf you're not using a bundler, you could continue creating custom builds of the SDK using the browser builder.\nThis isn't live yet, but the code is available in the PR.\n. @dukedougal \nYou can npm install specific git branches.\nSo for this PR, you could run:\nnpm install git://github.com/chrisradek/aws-sdk-js.git#webpack\nThis is still a work in progress, so the code isn't completely finalized yet.\nYou'll still need the generic json-loader in your webpack config, but that should be it. I checked, and the create-react-app that people reported using includes this by default.\nFor example, this is the webpack config I was testing with:\n``` javascript\nvar path = require('path');\nmodule.exports = {\n    entry: [\n        path.join(process.cwd(), 'app', 'index.js')\n    ],\noutput: {\n    path: path.join('dist'),\n    filename: 'aws-bundle.js'\n},\nmodule: {\n    loaders: [\n        {\n            test: /\\.json$/, \n            loaders: ['json-loader']\n        },\n    ]\n}\n\n}\n```\n. @daumann \nThe solutions provided in this thread only allow the browser version of the SDK to be used with webpack. In the browser, the SDK won't be able to access the credentials on your hard drive, or via environment variables.\nThere is still more work to be done to allow the SDK to support webpack for node bundles.\n. @jagi \nWhen creating your webpack bundle, are you requiring the SDK completely, or are you pulling in specific services? Are you configuring webpack to build a node.js bundle?\n. @jagi \nIn webpack, you can specify a target By default the target is web, which causes webpack to look at the browser field in the package.json file. In the SDK's case, this essentially means you'll be getting the browser version of the SDK.\nFor Lambda, you won't want the browser version of the SDK. The PR only covers supporting the browser version of the SDK with webpack right now, so if you change the target to node, you'll likely face some issues still. I'll work on updating the PR to also support webpack/browserify when setting the target to node.\n. @jagi \nThe SDK doesn't already work in webpack/browserify for node.js projects largely due to the way API models are consumed. The models are loaded dynamically, when many of these tools require static imports (or custom configuration) to work properly. In the PR, for the browser use-case the APIs are now imported statically. We'll need to do something similar to get this working for node.js as well.\n. @jagi \nSince we've gotten a couple issues now regarding how to use browserify/webpack with the SDK for node.js projects, I'll try to include that with the existing PR. I can't say with certainty when it will be ready, but I plan to work on that this week. Hopefully we can borrow a lot of what we did to support the browser version to also support the node.js version.\n. @deviantony \nI'll be putting up a new PR shortly that should address some problems. In your webpack config, are you specifying a json loader like in the above comment:\nhttps://github.com/aws/aws-sdk-js/issues/603#issuecomment-244189509\n. @deviantony Do you mind sharing your config? With my latest changes I've been testing with a pretty bare bones config, for web-based and node-based projects. If you can share what yours looks like I can expand my test cases.\n. @simonbuchan Could that be because @deviantony's loader is json? In my config above, mine is json-loader (I'm using json-loader@0.5.4)\n. @simonbuchan \nWeird, so for whatever reason, using json-loader works in my test, but just json breaks. If I add the .json extension to all my imports then both options work. Either way, it doesn't hurt to add the extension, that was just unexpected behavior.\n. I've created #1123 that contains all the changes needed to support webpack. It also allows webpack and browserify to be used to generate node bundles as well. Like the other PR, it also allows for importing individual services.\nI incorporated some of the feedback from the previous PR and resolved some merge conflicts. There shouldn't be anymore major changes, and I think we've settled on the method for requiring services individually.\n. @deviantony \nApologies, I created a different branch when resolving some merge conflicts and adding support for node bundles.\nnpm install git://github.com/chrisradek/aws-sdk-js.git#full-webpack should work.\n. The PR has been merged and released as part of 2.6.0. We'll work on updating our documentation on how to use webpack/browserify.\n. @rameshsubramanian \nWhich version of the SDK are you using?\nIf you're having trouble getting the SDK to work with webpack, take a look at these blog posts that walk through a simple example and see if they help:\nUsing webpack and the AWS SDK for JavaScript to Create and Bundle an Application - Part 1\nUsing webpack and the AWS SDK for JavaScript to Create and Bundle an Application - Part 2\n. @kejsiStruga \nThe browser doesn't have access to the file system, so AWS.config.loadFromPath doesn't work when your project is run in a browser. There are a couple of options for getting credentials loaded in the browser SDK, with Cognito being the preferred way in most cases. Here's some documentation that goes into setting up credentials for the browser SDK:\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-credentials-browser.html\nAlso, since version 2.6.0 of the SDK, it is no longer necessary to import the SDK from aws-sdk/dist/aws-sdk. This was a temporary solution that had cons such as webpack being unable to do any 'tree-shaking' to optimize the bundles it generates, and having to use complicated webpack config. \nHere are the docs that explain how to use the SDK with webpack (you can simply import AWS from 'aws-sdk' now):\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/webpack.html. @shyamchandranmec and @gerardmrk \nCan you share more info, like what your webpack config looks like, and what version of the SDK you're using?\nThe error you're reporting isn't one I've seen since before. The error indicates that webpack can't find the lodash dependency on xmlbuilder. Can you check if you are pulling in multiple versions of lodash? My first guess would be webpack isn't correctly resolving the lodash dependency xmlbuilder uses, but any more info you can share would help.\nSide note: This feature request to support webpack has been closed. Since this is a new error you're seeing, please feel free to open up a new issue.. @wearhere \nYes, this will be available in the next release. Thanks for requesting this feature and for your feedback!\n. @stigerland and @bruun, I'll take a look at this PR this week. Especially since this is such a big jump in the dependency version, we want to be very sure that nothing else breaks if we update.\n. @mohamed-kamal \nI like adding the part number and the key, but do you have some example code for how you're getting the name to be set? What you have as the name, I'd expect to be the key. I'm curious how you're setting the name because based on the commit, it looks like it's based on a name property on the body you pass in to be uploaded.\n. Closed by #788\n. @watson \nI ran the code you provided on Node.js v0.10.x and v0.12.x. On Node.js v0.10.x, both the native https client as well as request seem to be working correctly.  However, on Node.js v0.12.x only the native https client works.\nMy initial suspicion is that there may be something else going on with request on Node.js v0.12.x. I'm not aware of the internal details of request, but it doesn't look like the SDK relies on any unsupported Node.js internals.\nLet me know if this information helps and thanks for your patience!\n. Closing. Feel free to reopen if you want to discuss this further.\n. @devinus \nI took a deeper look at what request is returning and what the SDK is doing in an upload vs putObject operation, and have some early thoughts on what may be happening.  I'll re-open and mark as a feature request for now, since this seems to be an issue with request and not https.\n. We'll take another look to see what it would take to support request.\n. It looks like the issue is due to request binding a data event on the response. The upload method binds readable internally, and when it attempts to read the stream isn't able to read any data.\nRequest code:\nhttps://github.com/request/request/blob/d9906af527e0e452bb3e4f680348f017cf27b451/request.js#L958\nBinding data basically has the affect of changing our stream from 'paused mode' to 'flowing mode'. In flowing mode, the readable event doesn't get emitted except for the last one, end-of-stream.\nhttps://nodejs.org/api/stream.html#stream_class_stream_readable\nI still have to see what's possible on our end to mitigate this. We may be able to pause the stream, but I'll have to see if we end up losing some data going down this route. This does however explain why upload doesn't work when using request. \n. @DenisGorbachev \nWe got a response from the SWF team. In order to let the new worker pick up the modified task in the modified code, do the following steps:\n1. Start the worker process # 1.\n2. Modify the worker code.\n3. Change the worker code's version in the interface definition(IMPORTANT STEP)\n4. Change the worker code's taskList to use a different taskList in the worker code (IMPORTANT STEP)\n5. Stop the worker process # 1.\n6. Start the worker process # 2 (which runs the modified code).\n7. Schedule a new activity task (to test the modified code) \nThe version and taskList need to be changed, otherwise SWF will still schedule the old task in the old taskList.\nAdditional information can be found on the SWF docs under the section \"Problems Due to Versioning\" here:\nhttp://docs.aws.amazon.com/amazonswf/latest/awsflowguide/troubleshooting.html#troubleshooting.Versioning\nLet me know if this helps or you need more information, and thanks for your patience!\n. Thank you again @jippeholwerda for the pull request, it is very much appreciated!\n. @jbuck \nI was able to reproduce the new error you were seeing with the code you provided. It looks like the issue is actually with the callback function being supplied to async.whilst.  Once the final listing returns, the test function will return false and the callback function will be called with null provided as the argument.  Adding a check in the callback function for the existence of error before throwing it should (hopefully) resolve your issue.\nLet me know if that helps! \n. I'll need to update the comment.  I originally added a check ArrayBuffer.isView(data), but that isn't supported in nodejs/phantomjs.  Comparing data's prototype to a typed array's prototype leads to false positives in some browsers as well.  I can add a check that the data's buffer field is an instance of ArrayBuffer for some added safety.\n. @lonormaly \nThe upload object has a private failed property that should return true if the upload failed.  Note that this is a private property and may change without notice.\nCan you share more about what you are trying to achieve?  The callback function you pass into your send method should receive an error object if any of the uploadPart() requests fail.\nLet me know if this information helps!\n. @lonormaly \nThe SDK currently doesn't have a way of determining if an upload was canceled under the use case you provided. \nHowever, with pull request #736, multipart uploads can now be resumed if leavePartsOnError is set to true on a ManagedUpload instance. If an error is returned in the callback to the send() method, calling send() again should resume where the previous attempt left off.\nLet me know if this information helps and thank you for your patience!\n. @lonormaly \nI'm marking this as closed because there's not much we can do from the SDK side to support recovering from a process shutdown.  Feel free to reopen this issue or open another issue if you have other questions.\n. @tallboy \nThe waitFor() method is intended to provide information on whether a resource is available for use. The waitFor() method does not report details behind why that resource is or isn't available. \nA possible workaround is to invoke readJob() when waitFor() method returns an error. This should give you details of the status of your job.\nLet me know if this helps.\n. Looks good, merging in.\n. Thanks @DrMegavolt for the pull request. Do you know of any rest-json operations that have a list as an output? When I did a quick scan through our APIs, I only saw outputs defined as structures, which is what we'd expect.\n. @DrMegavolt \nI just merged in #766 that should hopefully address what you're trying to accomplish.\nWe strongly prefer structures as a root element, and in fact don't have any rest-json outputs that are a list. A big reason for this is future-compatibility. If we had an operation that returned a list, if we ever wanted to return something else as well, we would likely have to create a new operation, or break the current one for existing users. By using structures, we just need to add another field to the output.\nThat said, with the change made in #766, you'll be able to accept a response where the body is a list, but still return an output that's a structure. This is done by including the payload field in an operation's output.  \nFor example, imagine a response where the body was simply a list of strings: ['a', 'b', 'c'].\nWe may have an operation configured something like this:\njavascript\n\"Search\": {\n    \"http\": {\n        \"method\": \"POST\",\n        \"requestUri\": \"/account/search\"\n    },\n    \"input\":{\"shape\":\"SearchAccountRequest\"},\n    \"output\":{\n        \"type\": \"structure\",\n        \"payload\": \"AccountList\", //treat the response's body as this shape\n        \"members\": {\n             \"shape\":\"AccountList\"\n        }\n    }\n}\n//...\n\"AccountList\": {\n    \"type\": \"list\",\n    \"member\": {\n        \"shape\": \"AccountName\"\n    }\n},\n\"AccountName\": {\n    \"type\": \"string\"\n}\nThe result of calling our imaginary Search operation with the list above would be:\n{\n    \"AccountList\": [\"a\", \"b\", \"c\"]\n}\nI hope this change helps you accomplish what you were trying to do above. Let me know if this solve your use case or you have any questions about the change.\n. @DrMegavolt \nI'm going to close this pull request due to pr #766.  Feel free to comment if you'd like to discuss your request, or the details of #766 further.\n. @kentor \nResuming should work after an abort if the upload was configured so leavePartsOnError was set to true.\n. I added a minor simplification so it is hopefully clearer which LoginId is getting deleted when the unit test completes.\n. Looks good!  Merging it in!\n. @Lohit9 \nDid you have any luck with the browser version of the SDK?\n. Wanted to give an update here. The react-native branch of this repo adds support for react native using the JavaScript SDK as well. It's still a work in progress, mostly needing tests, but I have tried it with a few XML and JSON-based services with iOS and Android. Feel free to check it out and comment on #1393 or here if you have any feedback.. Closing since the react-native branch was merged in v2.48.0 of the SDK. Please open up new issues regarding any feedback/problems found with react native support!. @lonormaly \nCan you provide some sample code to help me reproduce this issue?\n. @lonormaly \nI'm still trying to reproduce this issue, but can you tell me if there is a common time interval between when you start performing uploads and when you encounter a failure?  Also, are you seeing this issue when retrying a failed or aborted upload, or is it any upload?\nThanks for your patience!\n. Closing old issues.\nPlease open a new issue if you're having any additional problems.. :shipit:\n. @Rich17 \nWhat version of the SDK are you using?\nI tried the following code using SDK v2.2.8 and Nodejs 0.12.7, and was able to successfully write a ~250MB file from S3 to a file on my local computer.\n``` javascript\nvar AWS = require('aws-sdk');\nvar fs = require('fs');\nvar params = {\n  Bucket: 'VALID_BUCKET',\n  Key: 'BIG_FILE.mp4'\n};\nvar s3 = new AWS.S3();\nvar file = fs.createWriteStream('test.mp4');\nfile.on('close', function(){\n    console.log('done');  //prints, file created\n});\ns3.getObject(params).createReadStream().on('error', function(err){\n    console.log(err);\n}).pipe(file);\n```\nLet me know if the above example doesn't work for you.\n. @ReinsBrain \nAre you getting this error when sending the exact params you've listed above?\nWhen sendMessage is called, the service returns in the response the MD5 of the MessageBody that was sent in. The SDK calculates an MD5 using the MessageBody supplied in the sendMessage parameters, then compares this MD5 with the value returned from the service. The error you're seeing occurs when the MD5 from the service doesn't match what the SDK calculates.\nWhat's interesting is that your error message contains undefined, which should be the MD5 value returned by the service in the response. This seems to indicate that something went wrong on the service side. Are you still seeing this issue? What version of the SDK are you using?  I haven't been able to reproduce your results yet, using node 0.12.7 and SDK v2.2.8.\nApologies for the late response.\n. Closing this issue, but feel free to comment or open another issue if you have any further questions.\n. @Ashesh007 \nCan you provide some more information on what operations are causing this error to manifest? Is it all SES operations or just specific ones?\n. @Ashesh007 \nCan you try calling calling the operations using the aws-sdk directly?\nHere's an example:\n``` javascript\nvar AWS = require('aws-sdk');\nvar ses = new AWS.SES({\n    accessKeyId: YOUR_ACCESS_KEY, //Don't use in PRODUCTION\n    secretAccessKey: YOUR_SECRET_ACCESS_KEY, //Don't use in PRODUCTION\n    region: YOUR_REGION\n});\nses.listVerifiedEmailAddresses(function(err, data) {\n    if (err) {\n        console.log(err);\n    }\n    if (data) {\n        console.log(data);\n    }\n});\n```\nI've tried both the example you provided, and the one I listed above, and haven't been able to reproduce your issue yet.  One thing I did notice was that your error shows the region being used is us-west-2, and the example you provided shows you were trying to set it to us-east-1.\nLet me know if the above example helps.\n. @Ashesh007 \nHow are you currently getting your credentials to pass into the SES operations?  Using the above example, did you hard-code them, or did you use some other method?\n. @Ashesh007 \nI've still not had any luck reproducing your exact error message, but have you tried setting the correctClockSkew config parameter to true when creating an instance of SES?\nSome details about this parameter can be found here:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#correctClockSkew-property\nI don't think this is what's causing your issue, because normally the service would return back that your signature is out of date, but it'd be good to make sure about that.\nCan you also tell me what versions of node and the SDK you are using?\n. @Ashesh007 \nAre you still having issues with SES after trying the above?\n. @bantic \nThanks again for your pull request! I ended up moving the logic to lowercase the query parameters into the presigner. Since this was ultimately an issue with the presigner, I wanted to keep the change localized there. Extra thanks for also providing a unit test, that helps tremendously.\n756 has been merged and includes this fix.\n. @andreasherzog \nI switched to using node 0.12.7 and aws-sdk 2.2.8, and haven't been able to reproduce this issue yet. Only the error bound to the stream is being fired when I test. When you're testing, is it only with the code above? I'm curious if there's something else going on in the surrounding code that is causing this.\nHere's the code I'm testing with (almost identical to what you've provided.)\njavascript\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\nvar params = {\n  Bucket: 'VALID_BUCKET',\n  Key: 'not_a_real_key'\n}\nvar s3Stream = s3.getObject(params).createReadStream()\n      .on('error', function(err) {\n          console.log('READSTREAM error');\n          console.log(err);\n      })\n      .on('end', function(){});\n. @andreasherzog \nI ran some tests using archiver. It looks like the error being captured on the stream's error event is also getting captured by archiver, and then archiver emits an error event, causing the error you see to be thrown.\nInstead of attaching the error event to the stream object returned by createReadStream, you can attach an error event directly onto your archive instance.\nSomething like this:\njavascript\nvar AWS = require('aws-sdk');\nvar archive = archiver('zip')\nvar s3 = new AWS.S3();\nvar params = {\n  Bucket: 'VALID_BUCKET',\n  Key: 'not_a_real_key'\n}\nvar s3Stream = s3.getObject(params).createReadStream()\n      .on('end', function(){});\narchive.on('error', function(err) {\n    console.log(err);\n});\narchive.append(s3Stream, {\n    name: item.title || item.fileKey\n});\nLet me know if this helps!\n. The only time they wouldn't is if the request id is returned in the response's metadata, but in that case, there's no way to get the request IDs if we get an error.\n. @samsamm777 \nCurrently alarms can only be retrieved by name or prefix. I would recommend creating a Feature Request thread on the Amazon CloudWatch forums (https://forums.aws.amazon.com/forum.jspa?forumID=138)  to let the service team know about this request.\nI'm closing this issue, but feel free to re-open if you want to discuss this further.\n. @chazmo03 \nThe error event bound to the stream is intended to be passed an error object that indicates the request was aborted. Are you also seeing another thrown error, similar to #750? If so, can you provide a reproducible, minimal code snippet so I can investigate more thoroughly?\n. @chazmo03 \nI'm closing this issue, but feel free to comment or re-open if you're still having issues getting this to work.\n. @onassar \nThe reason you're seeing different behavior is due to how the AWS.S3.ManagedUpload handles uploading files of different sizes.\nThe ManagedUpload has a minimum part size theshold of 5MB (http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3/ManagedUpload.html#minPartSize-property). If the file being uploaded is below this threshold, then only a single upload operation is performed. \nIf the file is above the threshold, then the ManagedUpload will instead do a multi-part upload. When abort is called, any parts that have not yet been sent to the S3 bucket will not be sent, and the parts that have already been uploaded will be deleted from the bucket.\nSince the smaller files are being uploaded in a single operation, they can't be aborted. Is the goal to remove the file from your bucket when abort is called?\nLet me know if this information helps.\n. @onassar \nThis is a restriction set by the S3 service.\n. @onassar \nSorry for the delayed response.\nSince this restriction is set by the S3 service, I'd recommend creating a feature request thread to allow overriding the minimum part-size that can be uploaded on the S3 forums here:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=24\n. @optimisme \nCan you share any code that reproduces this error?\n. @optimisme \nNo problem, glad you got it working!\n. @tgrant59 \nInterestingly, when I tested the abort method in the latest versions of Safari, Firefox, and Chrome, it stopped the upload and aborted as expected. I will need to investigate more thoroughly what issues there were with aborting in a browser. \nHave you encountered a situation where the abort method hasn't worked yet?\n. @Villa41 \nCan you share what version of Chrome and for what OS you're testing in?\n. @Villa41 \nThat makes sense. When the file is smaller than 5MB, then behind the scenes the managed uploader does a single putObject request. There isn't a way to abort that from our end once it's been sent. Anything larger than 5 MB will be uploaded as a multi-part upload. Any uploadPart requests that have already been sent won't abort, but any future requests should never get sent, in theory.\n. @Villa41 \nThat makes sense. When the file is smaller than 5MB, then behind the scenes the managed uploader does a single putObject request. There isn't a way to abort that from our end once it's been sent. Anything larger than 5 MB will be uploaded as a multi-part upload. Any uploadPart requests that have already been sent won't abort, but any future requests should never get sent, in theory.\n. The docs have been updated to clarify how the abort() method works within the browser.\nCurrently, it will simply prevent additional uploadPart requests from being sent, but we may look into aborting the actual requests in the future as well.\n. The docs have been updated to clarify how the abort() method works within the browser.\nCurrently, it will simply prevent additional uploadPart requests from being sent, but we may look into aborting the actual requests in the future as well.\n. @samsamm777 \nWhat version of the SDK are you using? Also, are you running this in the browser or in node?\n. @samsamm777 \nWhat version of the SDK are you using? Also, are you running this in the browser or in node?\n. @samsamm777 \nAt this time AutoScaling isn't supported in a browser environment.  You can use it within node, or an environment that doesn't enforce the CORS standard (i.e. Google Chrome extensions and Windows Store Applications.) \nIf you'd like to see this functionality supported in a browser, I'd suggest creating a feature request thread for CORS support of Auto Scaling on the forums here:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=30\nLet me know if you have any further questions!\n. @samsamm777 \nAt this time AutoScaling isn't supported in a browser environment.  You can use it within node, or an environment that doesn't enforce the CORS standard (i.e. Google Chrome extensions and Windows Store Applications.) \nIf you'd like to see this functionality supported in a browser, I'd suggest creating a feature request thread for CORS support of Auto Scaling on the forums here:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=30\nLet me know if you have any further questions!\n. Closing since the SDK doesn't support AutoScaling in the browser.  Please feel free to comment or reopen if you have any related questions or want to discuss this further.\n. Closing since the SDK doesn't support AutoScaling in the browser.  Please feel free to comment or reopen if you have any related questions or want to discuss this further.\n. @droplab \nHave you verified if the same code runs in a browser such as Firefox or Chrome? And am I understanding correctly that the callback function you supply to the upload method isn't being triggered as well?\n. @droplab \nHave you verified if the same code runs in a browser such as Firefox or Chrome? And am I understanding correctly that the callback function you supply to the upload method isn't being triggered as well?\n. @droplab \nAre you still having issues with IE?\n. @droplab \nAre you still having issues with IE?\n. Closing the issue, but feel free to comment or re-open if you are still encountering issues.\n. Closing the issue, but feel free to comment or re-open if you are still encountering issues.\n. @pchuri \nCan you also try passing in the RoleArn of the unauthenticated role associated with your IdentityPoolId into the CognitoIdentityCredentials constructor?\nPlease let me know if that helps.\n. @pchuri \nCan you also try passing in the RoleArn of the unauthenticated role associated with your IdentityPoolId into the CognitoIdentityCredentials constructor?\nPlease let me know if that helps.\n. @pchuri \nDid providing the RoleArn solve the issue?\n. @pchuri \nDid providing the RoleArn solve the issue?\n. @southpolesteve \nSorry for the delay. The changes required are to the models that all the SDKs share and go a little beyond what's in the pr. Not all the SDKs support the functionality that is required for this change, but we are pushing for them to make the changes necessary to support this. \nI know how frustrating it is to hear \"it's coming soon\" but I talked with the SDKs that need to update to support this and they have it as something to take care of in the near future.\n. @southpolesteve \nSorry for the delay. The changes required are to the models that all the SDKs share and go a little beyond what's in the pr. Not all the SDKs support the functionality that is required for this change, but we are pushing for them to make the changes necessary to support this. \nI know how frustrating it is to hear \"it's coming soon\" but I talked with the SDKs that need to update to support this and they have it as something to take care of in the near future.\n. @southpolesteve \nIn the meantime, I posted a workaround in the related issue #941.\nYou can add the embed=methods to the query string of your request's path manually:\n``` javascript\nvar req = apigateway.getResources({\n    restApiId: 'apiId'\n});\nreq.on('build', function(req) {\n    req.httpRequest.path += '/?embed=methods';\n});\nreq.send(function(err, data) {\n    if (err) {\n        console.log(err);\n    } else {\n        console.log(data);\n    }\n});\n```\nThis at least can help get around having to use a patched version of the SDK, but would have to be taken out once the SDK is updated with the embed fix.\n. @southpolesteve \nIn the meantime, I posted a workaround in the related issue #941.\nYou can add the embed=methods to the query string of your request's path manually:\n``` javascript\nvar req = apigateway.getResources({\n    restApiId: 'apiId'\n});\nreq.on('build', function(req) {\n    req.httpRequest.path += '/?embed=methods';\n});\nreq.send(function(err, data) {\n    if (err) {\n        console.log(err);\n    } else {\n        console.log(data);\n    }\n});\n```\nThis at least can help get around having to use a patched version of the SDK, but would have to be taken out once the SDK is updated with the embed fix.\n. Closing since embed is available in v2.43.0 of the SDK. Please let me know if you still encounter issues.. Closing since embed is available in v2.43.0 of the SDK. Please let me know if you still encounter issues.. @cagataygurturk \nI would suggest posting this question to the AWS Lambda service team on the forums:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=186&start=0\nThey manage updates to the version of the SDK used by Lambda.\nMarking as closed since this isn't something controlled by the SDK, but feel free to reopen if you have further questions.\n. @cagataygurturk \nI would suggest posting this question to the AWS Lambda service team on the forums:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=186&start=0\nThey manage updates to the version of the SDK used by Lambda.\nMarking as closed since this isn't something controlled by the SDK, but feel free to reopen if you have further questions.\n. I would like to add that it is possible to bundle the latest SDK with your code if you wish to use a version other than what's provided by Lambda.\n. I would like to add that it is possible to bundle the latest SDK with your code if you wish to use a version other than what's provided by Lambda.\n. @cagataygurturk \nTake a look at the documentation for creating a deployment package with Node.js:\nhttp://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html\nThe short version is, you will want to create a zip file that includes the code for your lambda function, as well as the JavaScript SDK. If you create a new directory, you can run npm install aws-sdk, create your JavaScript file that contains the code you want to run.  When you require('aws-sdk') from within your JavaScript file, it should reference the version of the SDK bundled in your zip.\nWhen you create your Lambda function, you'll upload the zip you've created, or upload it to S3 and reference it from there.\nLet me know if that helps!\n. @cagataygurturk \nTake a look at the documentation for creating a deployment package with Node.js:\nhttp://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html\nThe short version is, you will want to create a zip file that includes the code for your lambda function, as well as the JavaScript SDK. If you create a new directory, you can run npm install aws-sdk, create your JavaScript file that contains the code you want to run.  When you require('aws-sdk') from within your JavaScript file, it should reference the version of the SDK bundled in your zip.\nWhen you create your Lambda function, you'll upload the zip you've created, or upload it to S3 and reference it from there.\nLet me know if that helps!\n. @jammy-srikanth \nYou'll probably get a more comprehensive answer by asking on the S3 AWS forums.  As far as potential issues, you may run into an issue when using sigv4 signing.  For example, if your default region was us-east-1, and you were trying to create a new object in eu-central-1 - a region that only supports sigv4 - then there would be an authorization failure because the region is used when creating a v4 signature.  A recent update to the SDK was made available with the latest version (2.2.13) so that when this type of failure is encountered, the request will automatically be retried with the correct region.\nLet me know if this helps!\n. @jammy-srikanth \nYou'll probably get a more comprehensive answer by asking on the S3 AWS forums.  As far as potential issues, you may run into an issue when using sigv4 signing.  For example, if your default region was us-east-1, and you were trying to create a new object in eu-central-1 - a region that only supports sigv4 - then there would be an authorization failure because the region is used when creating a v4 signature.  A recent update to the SDK was made available with the latest version (2.2.13) so that when this type of failure is encountered, the request will automatically be retried with the correct region.\nLet me know if this helps!\n. I'm going to close this issue as this question is better directed at the S3 service team on the forums. Feel free to comment or re-open if you'd like to discuss further.\n. I'm going to close this issue as this question is better directed at the S3 service team on the forums. Feel free to comment or re-open if you'd like to discuss further.\n. @raghunat \nI know the documentation isn't very clear on this, but you're missing a few more parameters for the putIntegration method.\nWhen you're specifying type: 'AWS', you also need to provide the integrationHttpMethod, the uri, and the credentials (execution role).\nBefore the fix referenced above was made, it wasn't possible to set the integrationHttpMethod.\nThe uri is probably the trickiest parameter to specify the first time around.  There's clearer documentation on how to build the uri on the API Gateway docs here:\nhttp://docs.aws.amazon.com/apigateway/api-reference/resource/integration/#uri\nSo, if you wanted to call a lambda function, the uri would look something like this:\narn:aws.apigateway:{region}:lambda:path/2015-03-31/functions/{lambda function arn}\nLet me know if this helps, or if you need further clarification.\n. @raghunat \nI know the documentation isn't very clear on this, but you're missing a few more parameters for the putIntegration method.\nWhen you're specifying type: 'AWS', you also need to provide the integrationHttpMethod, the uri, and the credentials (execution role).\nBefore the fix referenced above was made, it wasn't possible to set the integrationHttpMethod.\nThe uri is probably the trickiest parameter to specify the first time around.  There's clearer documentation on how to build the uri on the API Gateway docs here:\nhttp://docs.aws.amazon.com/apigateway/api-reference/resource/integration/#uri\nSo, if you wanted to call a lambda function, the uri would look something like this:\narn:aws.apigateway:{region}:lambda:path/2015-03-31/functions/{lambda function arn}\nLet me know if this helps, or if you need further clarification.\n. @raghunat \nYes, those parameters look correct.  Just remember that in the uri, replace 'myFunctionName' with the full arn for your lambda function.\nLet me know if that works, or if you're still having problems getting it to work.\n. @raghunat \nYes, those parameters look correct.  Just remember that in the uri, replace 'myFunctionName' with the full arn for your lambda function.\nLet me know if that works, or if you're still having problems getting it to work.\n. I'm closing this issue, but feel free to comment here or re-open if you're still having issues.\n. I'm closing this issue, but feel free to comment here or re-open if you're still having issues.\n. Thanks for the pull request, we're inclined not to merge this at this time.. Thanks for the pull request, we're inclined not to merge this at this time.. @kentor \nThere isn't currently a way to use streams with ManagedUpload in the browser.  What are you using to include support for streams in the browser?\nIt is possible to enable server side encryption so that objects are encrypted when stored in S3. More information on server-side encryption that S3 supports can be found here: http://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\nHowever, I do realize server side encryption may not suit your needs.\n. @kentor \nThere isn't currently a way to use streams with ManagedUpload in the browser.  What are you using to include support for streams in the browser?\nIt is possible to enable server side encryption so that objects are encrypted when stored in S3. More information on server-side encryption that S3 supports can be found here: http://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\nHowever, I do realize server side encryption may not suit your needs.\n. @kentor \nI know this is tangential to your request, but is there a reason you'd prefer client-side encryption in the browser over server-side encryption? \n. @kentor \nI know this is tangential to your request, but is there a reason you'd prefer client-side encryption in the browser over server-side encryption? \n. @optimisme \nThere is a way to abort an upload, by using the ManagedUpload abort method:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3/ManagedUpload.html#abort-property\nWhat's the typical size (in MB) of the Body of your uploads?  Are there differences in the size of your Body when the upload method completes quickly versus when it takes a longer time?\n. @optimisme \nThere is a way to abort an upload, by using the ManagedUpload abort method:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3/ManagedUpload.html#abort-property\nWhat's the typical size (in MB) of the Body of your uploads?  Are there differences in the size of your Body when the upload method completes quickly versus when it takes a longer time?\n. @kyleseely \nYou are correct.  When using ManagedUpload, files that are larger than 5 MB (by default) will automatically be broken up into parts with each part uploaded to S3 individually. The MD5 hash for each part will be different than the one for the entire file, which is why passing in the MD5 to ManagedUpload is causing a BadDigest error on large files.\nAs you've suggested, setting the computeChecksums option in the S3 constructor to true will cause ManagedUpload to automatically calculate the MD5 hash for each part and send it to S3 for validation, so ContentMD5 does not need to be provided in this scenario.\n. @kyleseely \nYou are correct.  When using ManagedUpload, files that are larger than 5 MB (by default) will automatically be broken up into parts with each part uploaded to S3 individually. The MD5 hash for each part will be different than the one for the entire file, which is why passing in the MD5 to ManagedUpload is causing a BadDigest error on large files.\nAs you've suggested, setting the computeChecksums option in the S3 constructor to true will cause ManagedUpload to automatically calculate the MD5 hash for each part and send it to S3 for validation, so ContentMD5 does not need to be provided in this scenario.\n. I've created pull request #786 to address this.  This will update the docs to clarify that ContentMD5 should not be provided when using ManagedUpload.  It also updates the message in the error to clarify that the ContentMD5 is invalid for multi-part uploads. I didn't want to change the error code itself because it is possible some users have already coded around the code specifically.\nLet me know if you still have any concerns with regards to the changes being made!\n. I've created pull request #786 to address this.  This will update the docs to clarify that ContentMD5 should not be provided when using ManagedUpload.  It also updates the message in the error to clarify that the ContentMD5 is invalid for multi-part uploads. I didn't want to change the error code itself because it is possible some users have already coded around the code specifically.\nLet me know if you still have any concerns with regards to the changes being made!\n. #786 has been merged to address this.  These changes will be available via npm with the next SDK release.  They can also be used now by running npm install on this github repo.\n. #786 has been merged to address this.  These changes will be available via npm with the next SDK release.  They can also be used now by running npm install on this github repo.\n. Looks good. Merging into master.\n. Looks good. Merging into master.\n. @gregorskii \nI'm not familiar with the dynasty library. What sort of operations are failing?\nAs of version 2.2.0 of the AWS SDK, much better DynamoDB support has been baked in with the addition of the DocumentClient.  Documentation on this client can be found here:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html\nI would strongly recommend making use of the client built into the AWS SDK if it can suit your needs.\nLet me know if this information is helpful!\n. @gregorskii \nI'm not familiar with the dynasty library. What sort of operations are failing?\nAs of version 2.2.0 of the AWS SDK, much better DynamoDB support has been baked in with the addition of the DocumentClient.  Documentation on this client can be found here:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html\nI would strongly recommend making use of the client built into the AWS SDK if it can suit your needs.\nLet me know if this information is helpful!\n. Closing this issue, but feel free to comment or open a new one if you continue facing issues using the AWS SDK.\n. @kentor \nThis is currently a limitation with resuming a managed upload. This limitation exists because once data is read from a stream, it can't be read again, so trying to automatically resume could result in some lost data.\nThat said, I would like to take a deeper look and see what options there are for us to support resuming on aborted streams.  For now I'm going to mark this as a feature request, and work on updating documentation to specify that resuming when using streams isn't currently supported.\n. @lonormaly \nCan you give some details on what is causing the initial multipart upload to abort? \nOne thing that might be useful is to turn on the logger for S3 while you're troubleshooting (you can set the logger parameter to console when initializing S3 in the SDK).\nUsing the logger, you can verify that all the uploadPart operations for a given upload have the same UploadId and that you're seeing the expected number of uploadPart operations.  Also verify that the completeMultipartUpload command shows the number of parts you'd expect.\nLet me know if this helps and what you find!\n. @kentor @lonormaly \nThanks for the code you've provided to reproduce the issue. I plan on taking a look at this issue and the pull requests as well shortly.\n. @dhatawesomedude \nWhat you posted seems to indicate that there was an issue with loading your cognito credentials. Can you verify if the call to retrieve your credentials is succeeding?\nAlso, if you could provide a code snippet that reproduces your problem (stripping out sensitive information like your cognito identity pool id) that would be very helpful.\n. @dhatawesomedude \nJust to verify, am I correct in thinking that your S3 request goes through when you don't have xhrWithCredentials set to true? When that is set to true, the Access-Control-Allow-Origin can not be a wild-card character. If you set up the CORS rule for that bucket so that AllowedOrigin is http://localhost:8100, that should resolve your issue (assuming you're testing using localhost on port 8100 of course.)\nLet me know if that helps!\n. @dhatawesomedude \nWere you able to resolve your issue with the above advice?\n. I'm closing this issue, but feel free to comment or re-open if you're still having issues.\n. @Prinzhorn \nThanks for the example code. My initial suspicion is that the Cache-Control header needs to be provided when making the request because it is included as a signed header in the presigned URL (X-Amz-SignedHeaders). I'm looking into if this is truly necessary.\n. @Prinzhorn \nIt is indeed necessary to provide the Cache-Control header in your request if it is included as a parameter to getSignedUrl. It is possible to only set the Cache-Control header when making your request, and leave it out of your getSignedUrl call.  If it is included in getSignedUrl however, it is essentially enforcing that Cache-Control must be sent with the same value in the request.\nLet me know if this explanation helps, or if you'd like to discuss further.\n. @Prinzhorn \nJust to confirm, you've been testing in a V4 region only, correct? (i.e. eu-central-1). I may have found an issue in the code causing the request to fail even when the Cache-Control header is set to the same as defined when generating a signed url.\n. @rclark \nAs of version 2.2.10 of the SDK, the requestId of the response object is populated on errors, whereas previously it was only populated when an operation succeeded. The response object can be accessed by referencing this in a callback function to an operation.\nFor example:\njavascript\ns3.getObject({Bucket: 'bucket', Key: 'invalidKey'}, function (err, data) {\n  if (err) {\n    console.log('RequestId: ' + this.requestId);\n  }\n});\nIs this the method you were already using to get the request ids?\n. @rclark \nThe response doesn't currently have as direct of a way of accessing the x-amz-id-2 header value like it does for the request id.  It is possible to look at the headers on the response to grab this header, though I can understand the benefit to having easier access to this information.\nI'm marking this as an enhancement to expose the 2nd header value on the response. May also look into adding this information on the error itself.\n. @markstos \nUsing the example you provided, this should refer to a response object, and requestId should be populated. I tested using your example and was able to print the request id. Can you share what this is when you run your above code?\n. @markstos \nBased on the error message, it looks like the SDK can't find your credentials before making your call. If that's the case, then the getObject method will never actually send its request to the service and instead return the error you're seeing. Since requestId is returned by whichever service is being called, and no calls are actually being made to a service, requestId is undefined.\nLet me know if that clarifies things. If you are providing credentials and still seeing this issue there may be something else going on we can look at.\n. @markstos \nThe existence of requestId is documented here:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Response.html#requestId-property\nand called out in the upgrading docs here:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/upgrading.html\nThat said, this won't help you when using some of the high-level abstractions such as s3.upload. I think in this scenario it would be helpful to have the requestId on the error object as well.\n. I've merged #797 that adds requestId to the error object when the error is caused by the service. For S3, it also adds the extendedRequestId field, which is synonymous to x-amz-id-2.\nThis change will be available via NPM in the next release, or can be pulled in now by installing the SDK from github.\n. @SirCameronMcAlpine \nWith the JavaScript SDK, there usually aren't client classes for each service.  You simply need to instantiate an instance of a service.\nTo use SNS subscribe, you can do something like this:\njavascript\nvar sns = new AWS.SNS();\nsns.subscribe({\n    'TopicArn': 'ARN',\n    'Protocol': 'PROTOCOL',\n    'Endpoint': 'ENDPOINT'\n}, function (err, result) {\n    if (err) {\n        console.log(err);\n        return;\n    }\n    console.log(result);\n});\nLet me know if this helps.\n. @aleksandersumowski \nWe don't specifically test the SDK with Google Closure. That said, I would not feel comfortable saying the SDK will work with advanced compilation. I think there are risks, particularly with regards to compilation renaming properties.\n. @sobytes \nWe don't typically guarantee support with 3rd party frameworks. However, if you can provide the steps you performed to convert the AWS SDK to a common js module, you may get better feedback.\n. Closing due to lack of activity.\n. @andrey-bahrachev \nWe do have a pull request, #679 that would expose the object's key as part of the progress. I had a comment on it, but can look into expediting merging it in.\n. I've merged in #788. This will be available via npm with the next release of the SDK, otherwise you can install the SDK directly from github.\n. @lonormaly \nCan you share how you're refreshing your credentials before making a request?\nWhen you say internet connection is lost, do you mean the connection drops and comes back after a short time, or the system goes to sleep and then when waking up the user returns to the application?\n. Closing old issues.\nThe likely culprit is that your credentials had expired.\nPlease open a new issue if you're having any additional problems.. @markstos \nThanks for the feedback, I will update the docs to include the callback signature. \n. @enagorny \nUnfortunately, removing format=sdk can cause some adverse effects in the results you get back. In my testing, some results that came back with an array of one string when format is set to sdk, would instead return an array where each character in the string had its own element if I removed format=sdk or changed it to format=json.\nThe CloudSearch service team ultimately controls how the format=sdk affects the data that comes back in the response, but my assumption would be that everything comes back as an array so the user doesn't have to worry about checking the type of each returned field under certain conditions.  For example, if they had a field named authors, and in some cases authors contained 1 string and others it contained a list of 2 strings, the user would have to do an additional check to determine if a string or array was used. \nLet me know if that clarifies things. At this point I would suggest that the operation is working as intended in the SDK, which is different than in the console.\n. I'm going to close this issue since the SDK is accepting the data as the service provides it, and changing format to anything other than sdk currently exacerbates the behavior you're seeing.\nIf you want to discuss this further feel free to comment here. \n. Closing this issue, but feel free to comment or re-open if you have further questions.\n. @darkpssngr \nWhat version of the SDK are you using?  Can you also inspect this in the callback and verify data and error are both empty? Your code looks correct, aside from logging error instead of err.\n. @darkpssngr \nThanks for the link to the discussion over at node-inspector. I haven't had a chance to test the code change suggested yet but will take a look shortly.\nCan you confirm which version of node-inspector you're able to replicate the issue with?\n. @darkpssngr \nSorry for the delays. I finally had a chance to read through the issue you linked. Without diving further into the node-inspector code, it's hard to tell exactly why data is being returned as an empty object. While the change that was suggested does appear to fix the issue for the example provided in this thread, it also causes many of our unit tests to fail. \nIt also sounds like earlier versions of node-inspector worked with the SDK, so it'd be interesting to see exactly what changed between those versions and recent ones.\nAt this point, I'm not sure what we can do from the SDK side to make this work better with node-inspector without taking a deeper look into that package.\n. @markstos \nThank you for the feedback.  We'll work on updating the documentation to include helpful links.\n. @Thaina \nThe AWSToolKit for Visual Studio is meant for developing .NET applications. Because service objects in the JavaScript SDK are constructed dynamically once initialized, Visual Studio won't be able to generate IntelliSense documentation.\nIt would not be an easy or quick task to add IntelliSense support, unfortunately. However, we do have extensive documentation of our APIs available: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/\n. I'm closing this issue since it isn't something we're currently capable of supporting in the SDK.\n. @OllieJennings @forestzrd \nIf you want to create a bucket outside of us-east-1, you need to also specify the LocationConstraint.\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#createBucket-property\nThat said, it looks like there may be a bug when AWS.config.region is set to something other than us-east-1 and no LocationConstraint is set. Based on the documentation, it would suggest that the bucket should be created in us-east-1 in this situation.  I'll investigate that issue further.\nIn the meantime, let me know if specifying the LocationConstraint allows you to create buckets in the desired region.\n. @tracend \nThanks for the feedback! This has been tagged as a feature request. We tend to prioritize which feature requests get worked on based on community feedback and by looking at what other SDKs are doing. We also accept pull requests from the community.\n. @rclark \nThank you for not only reporting the issue, but also providing a pull request that addresses it. We greatly value community contributions!\n. @rclark \nThese final changes look good. Thank you for providing this pull request. Again, your contribution is greatly appreciated!\n. We typically release a new version of the SDK at least once a week. This change will make it into the next version, likely sometime this week.\n. @susanlinsfu \nHave you tried setting the ReturnValues parameter on your put operation to something like ALL_NEW? The operation allows you to control what data you get back, such as just updated fields, all fields, etc.\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html#put-property\nLet me know if this resolves your issue!\n. @susanlinsfu \nIt looks like I need to update the documentation for ReturnValues for the DynamoDBClient. The put operation calls the dynamodb putItem operation.  If you take a look at those docs here:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB.html#putItem-property\nThe return value from putItem, and also put is the item attributes as they appeared before they were updated by either operation.  ALL_OLD will return the content of the old item, and NONE (default) will prevent anything from being returned.\n. @weiyin \nThe putObject operation does not support setting content-length-range. See this related issue for more information: #643 \nSince the putObject operation doesn't support this header, the header won't have an affect when using the presigned url either.\nI'm closing this issue since this isn't something the SDK supports, but feel free to comment or re-open if you'd like to discuss further.\n. @e-gineer \nThanks for finding this issue! I was able to reproduce it using the steps you provided. It seems likely this may affect other SDKs as well so the specific fix may depend on that. \nAs a temporary work-around, you can include the qualifier as part of the function name.  For example, instead of setting a Qualifier of 1 to indicate the 1st version of the function, you could append :1 to your FunctionName.\n. @e-gineer \nBased on the Lambda API docs, it looks like including the qualifier as part of the function name is allowed. A max length for function name is also mentioned, so perhaps the Qualifier parameter is a way to handle long aliases.\n. This has been fixed with version 2.2.35 of the SDK. Now the Qualifier parameter is honored, and the workaround of including the qualifier as part of the function name still works as well.\n. @swarajgiri \nThe AWS.DynamoDB.DocumentClient accepts JSON objects, but not instantiated objects. The root of this behavior is in how the client converts inputs into a format that DynamoDB understands.\nSpecifically, you can see what types of inputs are supported by the client by looking at the convertInput function: https://github.com/aws/aws-sdk-js/blob/master/lib/dynamodb/converter.js#L5\nYou'll notice that typeOf is called on inputs, and for objects instantiated from a function, typeOf returns the function name. Since the function name won't fulfill any of the conditionals, those inputs are ignored. If functions were also converted, you may see some unexpected fields, such as those on the prototype chain, being saved to DynamoDB as well.\nI hope that clarifies things!  I'm going to close this issue, but feel free to comment or re-open if you want to discuss this further.\n. @swarajgiri \nDidn't you indicate stringify worked?  If so, I'm not sure what the issue is there. You could also create a function on the prototype to return a plain JSON object as well, in case you were using those classes elsewhere.\n. Thanks for getting the conversation started. We have been investigating a separate encryption client where this functionality would better live.. @chrisguttandin \nThanks for the feedback.  You're correct, only true is an accepted value for a property type of NULL. I'll work on correcting the docs to specify only true is an accepted value.\n. @kaurranjeet12 \nCan you tell me what version of the SDK you are using, and if it is within nodejs or the browser?\nUsing version 2.2.17 of the SDK in node with a large (~240 MB) file, I am able to set ServerSideEncryption using code similar to the following:\njavascript\nvar s3 = new AWS.S3();\nvar params = {\n  Bucket: 'bucket',\n  Key: 'test',\n  Body: stream,\n  ServerSideEncryption: 'AES256'\n};\nvar uploader = new AWS.S3.ManagedUpload({params: params});\nuploader.send(function(err, data) {\n  console.log(data);\n}\n. @kaurranjeet12 \nGlad to hear it!\n. @ar1hur \nAccording to the S3 documentation for user-defined metadata:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html\n\"Each name, value pair must conform to US-ASCII when using REST...\"\nIf you're in the browser, using a POST based upload would allow for UTF-8 encoding for the metadata.\neu-central-1 uses Signature Version 4 signing when performing operations. As part of sigv4 signing, the headers are used when creating a signature. Likely what is happening is S3 isn't reading the metadata value correctly when non ascii characters are supplied, and so generates a different signature on their end than what we supply from the SDK. Unfortunately this is a limitation in the service for now.\nLet me know if this information helps clarify the behavior you're seeing!\n. @Instigater \nThe error indicates that string is undefined when you attempt to log it, and I don't see it defined in your example.  If you remove string or replace it with an actual string, do you still see this issue?\n. @Chatatata \nIn your code, you have the following line:\nconsole.log(string + \", write completed.\")\nstring is being logged, but is not defined.\nThat string is being referenced in the error.  request.js is in the call stack because the error is in the callback that is called once the dynamodb response is returned, and request.js is catching it.\nIf you replace string with an actual string, or remove it entirely, do you still see an error?\n. @marcopiraccini \nWhat version of the SDK are you using?\nDoes the callback function supplied to listObjects ever get called?\nI ran code based off what you've supplied in nodejs version 0.10.40 and haven't seen an issue yet.  That error might be indicating some communication issue between your server and S3. I'm not sure how much it will help, but can you turn on logging to see if your requests are going through?  Just include logger: console in the config you pass into the AWS.S3 constructor.\n. @marcopiraccini \nAre you only having issues with S3, or can you replicate issues with other services as well? Is there any other code you're running alongside the sample you've provided?\n. This is sounding more like a connection issue that manifests when using node 0.10.x instead of something wrong with the SDK. Is there anymore information you can give about the environment node is running on?\n. @Alaneor \nI don't see any issues with the code you've provided. The QueueUrl you provide gets parsed using the nodejs url module, and the error is indicating that host wasn't able to be extracted from the url. However, your url clearly contains a host. I haven't been able to reproduce the issue yet, can you share what version of node you are using?\n. @mgoria \nWhat version of the SDK are you testing with? Using the latest version and your example, I'm also seeing LogResult as part of the data object.\nInside your callback, can you also see if the x-amz-log-result header is being returned? You can access the headers within your callback by looking at this.httpResponse.headers.\n. @mgoria \nThanks for the information so far. \nIs your first example in your last post from Node? Can you also verify what request headers are being sent? You could check it the same way as response using this.request.httpRequest.headers. Specifically looking for X-Amz-Log-Type and X-Amz-Invocation-Type.\n. @mgoria \nJust to be sure, are you testing with the example you provided above?\nThis is the code I'm running in node (with an actual FunctionName) and I see the LogResult in the data object:\n``` javascript\nvar AWS = require('aws-sdk');\nvar lambda = new AWS.Lambda();\nvar params = {\n    FunctionName: '',\n    InvocationType: 'RequestResponse',\n    LogType: 'Tail',\n    Payload: JSON.stringify({\n        foo: 'bar'\n    })\n};\nlambda.invoke(params, function(err, data) {\n    if (err) {\n        console.log(err);\n    } else {\n        console.log(data);\n    }\n});\n```\nOutput:\njavascript\n{ StatusCode: 200,\n  LogResult: '...',\n  Payload: '...' }\n. @mgoria \nI believe this issue may be with how the deep-framework is parsing the responses from Lambda.\nI took a look at the following source code:\nhttps://github.com/MitocGroup/deep-framework/blob/9b6b13176a6cc2f8d5ad2c7d6e62074c7ecbc8b5/src/deep-resource/lib/Resource/LambdaResponse.js\nIt looks like StatusCode and Payload are being extracted from the response's rawData. I'm assuming that rawData contains the data that the AWS SDK would return, though I haven't tested it myself. LogResult gets sent back to the AWS SDK as a header, and isn't part of the Payload object. The SDK knows to look for the header and add LogResult to the data object that gets returned. \nHave you tried the above code sample outside of another framework?\n. @mgoria \nApologies, I was testing with node before you posted your last update. I actually am seeing an issue in the browser as well. I'll keep investigating.\n. @chriskinsman \nCan you share what version of node you're testing with as well?\nI will definitely take a look at your reproduction case. Have you tried using the latest version of the SDK and still see the same issue?\n. @chriskinsman \nThanks for testing further. Is upgrading your version of node a useable workaround for the time being? I will still look into the issue with node 0.10.\n. @chriskinsman \nJust wanted to give you an update on my investigation so far.\nI haven't seen this issue present itself in version 0.12.x of nodejs or higher. Worth mentioning is that Streams3 is available in 0.12.x and higher, but not 0.10.x. I believe the main difference between the two versions of streams is the latter supports 'push' and 'pull' of data at the same time, whereas streams2 only supports one at a time. Right now I'm still figuring out what we can reasonably do from the SDK to resolve this issue.\n. @chriskinsman \nCan you update to at least version 2.4.12 of the SDK?\n2.4.12 added a content-length check when using streams. If the amount of data downloaded is less than the content-length specified by S3, then the stream will throw an error.\n. Closing old issues. The SDK correctly checks the content-length of the object against what S3 sends to raise an error when a file is truncated.\nTo get around this issue, consider using getObject with range or parts to request smaller parts with retries.. @chenliu0831 \nWe don't currently have an ETA for when waiters will be added to ECS. I'll mark this as a feature request.\n. @ataraxus \nI can't comment specifically on what the issue might be with the 3rd party frameworks. However, my best guess based on the error message you're receiving is that they don't handle SigV4 signing.\nThere are two different signature versions that are supported by S3 in various regions. Some regions only support the newest (v4) signing.  There's a list of regions and what signature versions they support here: http://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region\nIf possible, I'd recommend trying the aws-sdk for interacting with S3.  We have API documentation on how to use the SDK to interact with S3 here: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html\nWe also have a Getting Started guide: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/node-intro.html\nLet me know if you have specific questions or issues using the AWS SDK!\n. @nalindak \nThanks for the doc fix!\n. I believe this issue was answered by PlayfulWalrus on your stack overflow post, but I'll explain here.\nIn your policy, you are setting a condition that states that the partition key for the item you are working with must match the user's facebook id.\njavascript\n\"Condition\": {\n  \"ForAllValues:StringEquals\": {\n    \"dynamodb:LeadingKeys\": [\n      \"${graph.facebook.com:id}\"\n    ]\n  }\n}\nHere's some documentation on using IAM policy keys with dynamoDB:\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/UsingIAMWithDDB.html#IAMPolicyKeys\nHere's the important part:\ndynamodb:LeadingKeys \u2013 Represents the first key attribute of a table. For a simple primary key (partition key) or a composite primary key (partition key and sort key), LeadingKeys is just the partition key.\nYou also have StringEquals in your condition, which means the partition key must match the facebook userid.\nIn your application's code, let's take a look at the params you are passing into the put operation:\njavascript\nvar params = {\n  TableName: 'websiteTest',\n  Item: {\n    itemID:'lkjljljlkjlkjlkjlkjlkj',\n    f2:'kjhkjhkjhkjhkjhkjhkjhkjhkjh'\n  } \n};\nAssuming that itemID is the primary partition key for your table, then based on your current iam policy, it would have to match the user's facebook id.  You'd be able to access this within the same function using response.authResponse.userID\nLet me know if that helps!\n. I'm closing this issue, but feel free to comment or re-open if you have further questions.\n. @thoean \nWhat you're seeing is currently documented behavior:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB.html#putItem-property\nString and Binary type attributes must have lengths greater than zero. Set type attributes cannot be empty. Requests with empty values will be rejected with a ValidationException exception.\nThis could be something we support in the future by having something like an ignoreEmptyStrings configuration option for the DocumentClient that was opt in.\n. Locking this thread. This is a service behavior that can not be addressed by the SDK. Locking to prevent continued spam.. @mhart \nThanks for all the work and evidence you've provided. I will look into this and bring it up internally to figure out why there are such discrepancies.  I apologize for the delayed response.\n. @petemounce \nWe don't currently have a location to document extensions/helpers to the SDK. Part of the reason is we don't want to be responsible for maintaining that list as more libraries are created or decay.\nHowever, NPM does maintain a list of projects that depend on the SDK: https://www.npmjs.com/package/aws-sdk\n. @dschenkelman \nSorry for the late response to this. Just to be clear, am I correct in understanding you're only seeing these issues when using keepalive?\n. @adamgoucher \nI'm unclear on what you mean by \"Lambda function doesn't parse\". Can you provide a code snippet as an example for me to look at?\n. Closing this issue due to lack of activity. Feel free to comment or reopen if you have further details!\n. @islam-taha \nThanks for the pull request. The file you modified is a distributable version of the SDK that is built for each release. Instead, please make the change to the source file:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/s3/managed_upload.js#L572\n. @islam-taha \nAccepted! Thanks for your contribution!\n. @sabrinaluo \nThanks for the contribution! Merging it in.\n. @gerardo8a \nWe don't currently support ECS waiters, but will consider this as a +1 for #828.\nAre you starting by launching an EC2 instance that includes the Amazon ECS Container Agent?\nhttp://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_agent.html\nAmazon ECS-optimized AMIs should automatically start the Amazon ECS container agent for you, which registers your instance to an ECS cluster:\nhttp://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-agent-config.html\nYou could set an interval to poll for which services are active.  describeServices allows you to see the current status of services, along with a lot of other information. For waiters on other services, the SDK is essentially calling an operation on an interval x number of times to see if the associated actions have completed. This would be a work-around similar to what the SDK could eventually do.\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/ECS.html#describeServices-property\nLet me know if this helps!\n. Closing this issue in favor of #828.\n. @rclark \nIt looks like there has been a lot of conversations around errors being 'swallowed' by the SDK in the past (#307, #392, #74 to name a few). I believe the throw statement was added to prevent the SDK from swallowing errors that were thrown from an operation's callback.\nI've recreated a few NetworkingErrors but have always seen them get passed through to the callback. I'll dig deeper to try and find why your case isn't. Just to be sure, are you certain your callback isn't being called, and some error is being thrown from there?\n. @Marak \nCan you provide some code or details on your test case? We definitely want to look into this if you have a reproducible case.\n. @Marak \nI'd like to understand what's going on in your example better. Even with that ValidationException error, the callback you provide to your putItem (or any other) operation should still be called with the error passed to it. Where things start to fall apart is when an error is thrown in some code that's called within that callback function. As you've pointed out, these uncaught errors get thrown within request.js.\nI just tested this myself by calling putItem with a 500KB buffer, and logged the error in my callback. I'm not trying to defend the state machine, but want to understand why your example is exhibiting different behavior.\n. @petemounce \nYou could also set your credentials by doing something like this:\njavascript\nAWS.config.credentials = new AWS.SharedIniFileCredentals({profile: 'foo'});\nThen any service client you instantiate will use those credentials.\n. Closing the issue, feel free to comment or reopen if you have further questions!\n. @brianleroux \nYou would want to include the Lambda function as part of the uri. \nThe uri is probably the trickiest parameter to specify the first time around. There's clearer documentation on how to build the uri on the API Gateway docs here:\nhttp://docs.aws.amazon.com/apigateway/api-reference/resource/integration/#uri\nSo, if you wanted to call a lambda function, the uri would look something like this:\narn:aws.apigateway:{region}:lambda:path/2015-03-31/functions/{lambda function arn}\nSee #769 for more discussion on the topic.\n. @brianleroux\nI had a typo in my example, there should be a colon after the first aws in the arn, not a period. Please give that a try!\n. @brianleroux \nI suspect your credentials also need to be the full arn. I'll verify on my end if that's the case.\n. @brianleroux \nI ran a test on my end, and an ARN should be provided for credentials as well.\nLet me know if you still see errors after changing that!\n. @brianleroux \nAre you seeing that message as a result of running putIntegration, or from actually testing the method?\n. @brianleroux \nHave you taken a look at this forum post yet? There are some suggestions related to the errors you're seeing.\nhttps://forums.aws.amazon.com/thread.jspa?threadID=220021\n. @brianleroux \nI'm going to close this issue since the original question is solved, and your errors appear to be related to policies and apigateway.\nFeel free to comment or reopen if you're still having issues with putIntegration.\n. @southpolesteve \nThank you for your pull request. We typically don't like to add customizations to the api json files directly, because they would be overwritten each time that service was updated. That said, we've identified what the correct representation of embed should be and can ensure it is included with future service updates. I'll work on testing/including that model shortly.\n. This has finally been added with today's release v2.43.0.. @murilomothsin \nIs this the same code that's causing the issue? Are you checking any objects for equality within the callback, or any function that gets called from the callback? It sounds like this issue usually happens in angular when two objects are compared for value and a circular reference is encountered.\n. @neeravmehta \nCan you tell me what environment ArrayBuffer isn't defined in?\n. I can add that check for ArrayBuffer to the SDK. I'm not sure that will be enough to fully resolve the issue you're seeing though. Have you tried adding that check and verified it works for your use case?\nJust for reference, here's a list of the currently supported browsers:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-intro.html\n. @neeravmehta \nI included the check with the just released v2.2.29 of the SDK. I'm going to close this issue, but feel free to open a new one if you find your networking issues are related to the SDK.\n. What is ArrayBuffer when you hit that line? Just checking for ArrayBuffer should return a falsey value if it is undefined, which is also what your check is doing.\n. @nhinds \nThanks for the contribution! Merging these changes now, they'll be available in the next release of the SDK.\n. :shipit: \n. @smandava \nRunning the same code using the latest version of the SDK in node, I see failed getting logged to the console. What version of the SDK and what environment are you testing in?\n. @smandava \nI tried with the same version of node, SDK, and used OS X, and can't reproduce your error. Are you testing with the exact code you provided, or is this a snippet from surrounding code?\n. @smandava \nHave you tried running your code on another machine? Even using your exact code as supplied, I am unable to reproduce. Does console.log work at all within your environment?\n. Thanks @smandava \nI'm closing this issue since I can't reproduce, and it appears to be related to your environment.\nIf you find any more information that points to an issue with the SDK, feel free to reopen, comment, or open a new issue!\n. Thanks for the PR, merging!\n. @Qard \nYou also need to specify credentials. Since you're working with your local dynamodb, I don't believe your credentials have to be valid, but the SDK needs something otherwise it will continue to display the Missing credentials in config error.\nLet me know if that helps or if you need an example!\n. @Alex0007 \nCan you provide a code snippet? Are you updating the endpoint to look at your local database?\n. @mseiwald \nThere isn't a publicly available way of modifying the interval or max_attempts on waiters in the SDK. However, this is something we could support. I'll mark this as a feature request for now.\nThat said, the waitFor method is essentially just calling describeTable every 20 seconds, up to 25 times. It would be fairly trivial to write a polling function that does the same in your own code, using parameters you specify.\nThanks for your feedback, let us know if you have any further questions or comments!\n. @kuba142 \nIs getObject working for you in some cases, like with images, but not with other binary files? Are those other binary files very large?\nIf the above cases are true, it might be worth using streams to copy your objects, otherwise the entire buffer needs to be stored in memory first.\nHere's an example of how to do that in node. You might need to make a couple modifications (such as using context.succeed or context.fail where appropriate.)\n``` javascript\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\nvar s3Stream = s3.getObject({\n      Bucket: '',\n      Key: ''\n    }).createReadStream();\ns3.upload({\n    Bucket: '',\n    Key: '',\n    Body: s3Stream,\n  }, function(err, data) {\n    if (err) {\n      console.error(err);\n    } else {\n      console.log(data);\n    }\n});\n```\nLet me know if this helps!\n. @ruandre \nThe SDK will perform it's service operations asynchronously, so you're actually kicking off as many uploads as you have files at once. \nIf you want each upload to occur one after another, you can start the next upload in the callback you pass into the upload function. The async module can likely help you accomplish this as well.\nLet me know if this helps!\n. I'm closing this issue as resolved, but feel free to comment or reopen if you have more questions related to this!\n. @cesarpachon \nSorry about the delay, I'll check this out today. One thing I'm curious about is if xml-based services will work within web workers. My assumption is they won't unless a DOMParser is shimmed in, but this may be worth merging in anyway as a starting point.\n. @cesarpachon \nI'm going to merge this pull request in. There are still some issues where XML-based services don't work out of the box within web workers due to the absence of the DOMParser in web workers. However, there's still value in running the SDK in a web worker even as is, so I don't want to prevent users from doing so if they'd like.\nThanks for the pr!\n. @lsilvs \nAt this time IoT doesn't support CORS, so Iot and IotData aren't included by default in the browser build of the SDK. We are pushing services to support CORS, but there's currently no ETA when IoT will.\nThere is a forum post asking about CORS support on the IoT forums you could +1 here:\nhttps://forums.aws.amazon.com/thread.jspa?threadID=217647&tstart=0\nThere are ways you could work around this. For example, you could set up an API in API Gateway that runs a lambda function that gets the data you need and returns it back to your browser, though I understand this isn't as nice as being able to run the operations directly in a browser and would incur the cost of running Lambda.\n. Closing this issue, but feel free to comment or re-open if you have further questions related to this!\n. @rajinderodz \nThe only parameter deleteBucket accepts is the bucket name:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#deleteBucket-property\nThat said, a bucket can only be deleted after all objects within the bucket have been deleted. Are you having issues deleting an empty bucket?\n. Closing this issue, feel free to reopen if you're still having issues.\n. :shipit:\n. @aichholzer \nIs your intent to specify a presigned url that contains a body? Typically, presigned URLs are used when the body isn't known. In that case, you would not need to supply the Body parameter.\n. @loretoparisi \nCan you provide a code sample that reproduces the issue?\n. @gellin \nCan you try adding a custom header and see if that fixes the issue? There's an example of how to do that here:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Request.html#build-event\nIt's possible we need to add a similar customization as what php did.\n. @gellin \nIf you call the ses operation without passing in a callback, you'll get a request object returned to you. Then you can add a custom header.\njavascript\nmail.build(function (err, message){\n    var req = ses.sendRawEmail({RawMessage: {Data: message}});\n    req.on('build', function() {\n      req.httpRequest.headers['Custom-Header'] = 'value';\n    });\n    req.send(function (err, data) {\n        if (err) {\n            console.log(err, err.stack);\n            context.fail('Internal Error: The email could not be sent.');\n        } else {\n            console.log(message);\n            context.succeed('The email was successfully sent');\n        }\n    }); \n});\nI modified a part of your code snippet to illustrate how you could do this.\n. @gellin \nYou're running from within Lambda? I'll try to reproduce, thanks for giving the custom headers a try.\n. @gellin \nIt might be that mailcomposer is not compatible with ses.sendRawEmail.\nI found an AWS forum post that offers some insight:\nhttps://forums.aws.amazon.com/message.jspa?messageID=336807#336807\nHere's a simplified example of what I did to send an email to BCC:\n``` javascript\nvar AWS = require('aws-sdk');\nvar ses = new AWS.SES();\nvar params = {\n  RawMessage: {\n    Data: 'BCC: TEST_EMAIL_ADDRESS@TEST.COM\\nBody of my message'\n  },\n  Destinations: [],\n  Source: 'MY_SENDER_EMAIL@TEST.COM'\n};\nses.sendRawEmail(params, function(err, data) {\n  if (err) {\n    console.log(err);\n  } else {\n    console.log('Message sent!');\n  }\n});\n```\nThis information is documented by the SES service here but could be clearer:\nhttp://docs.aws.amazon.com/ses/latest/APIReference/API_RawMessage.html\nThe SES developer guide also provides an example of how to build the RawMessage:Data string in Java that should help as an example of how to build your raw data as well:\nhttp://docs.aws.amazon.com/ses/latest/DeveloperGuide/send-email-raw.html\nLet me know if this helps or if you have additional questions!\nEdit: Also, if you want to use the To, CC, BCC fields, Destinations needs to be empty according to the linked forum post.\n. @gellin \nGlad you were able to get it to work!\n. This will also close #655.\nThis is ok to ship, but I wouldn't close #886 until this case is resolved in the browsers we support as well.\n. Just confirming, this works in all the major browsers we support, right?\nIf so, :shipit:\n. @revington \nThanks, looks good to me!\n. @XpressiveCode \nWhen you're using sse-c, you need to send the headers with your request.\nThe S3 docs point this out here:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html#ssec-and-presignedurl\nWhen using the presigned URL to upload a new object, retrieve an existing object, or retrieve only object metadata, you must provide all the encryption headers in your client application.\nUnfortunately, this means that using presigned urls with sse-c is not a great solution when downloading large files from a browser. If you know your files are relatively small, you may be able to add a click event that performs an AJAX request using the presigned url and the required header, store the response in a blob and create an objectURL from that. Then this url could be used to let the customer actually save the file. However this would require that the whole file is loaded in memory first. sse is likely the better method for your use case.\n. @XpressiveCode \nI'll have to do some more investigation to see if that's expected behavior or not. Did the .net SDK work if you did pass the key into the pre-sign url?\n. The docs do specify that When creating a presigned URL, you must specify the algorithm using the x-amz-server-side\u200b-encryption\u200b-customer-algorithm in the signature calculation. so this should probably read that this is the only sse-c header you need to set when creating a presigned URL. The SDK sets this header for you when you specify the SSECustomerAlgorithm parameter. This makes sense, as you probably don't want the key/md5 you used to encrypt the object on S3's side to be easily readable from within a url.\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html#ssec-and-presignedurl\n. @XpressiveCode \nActually, after more investigation, it looks like this is a bug with our SDK. The SSECustomerKeyMD5 is getting hoisted to the url twice, once as x-amz-server-side-encryption-customer-key-md5 and once as x-amz-server-side-encryption-customer-key-MD5. When I removed the x-amz-server-side-encryption-customer-key-md5 entry from the query string and retried the request, it succeeded.\n. Once we fix this issue, you should be able to specify the SSECustomerKey in your parameters. The benefit to doing so would be that the x-amz-server-side-encryption-customer-key header you provide with the URL would have to match what was provided in the parameter, or else the request would fail. It's another layer of validation. The only real differences are that you wouldn't have to delete the SSECustomerKey parameter, and the sse-c-key and the sse-c-key-md5 will be part of the url again.\n. @XpressiveCode \nI created #898 to resolve this issue.\nThanks for your patience and for bringing this issue to our attention!\n. #898 has been merged. It will be available via npm with the next release of the SDK, or you can use it now by installing the sdk from github.\n. @XpressiveCode \nChanging the md5 in the query string to MD5 was indeed intentional. The SDK was erroneously lowercasing MD5 and this caused it to appear twice in the query string.\nHere's a short script to demonstrate how you could generate a presigned url passing in the SSECustomerKey. This was written for node.js but you can adapt it for the browser. Note that I made use of some SDK util functions for brevity. Since these are marked as private methods, they could change in the future so best not to rely on them.\n``` javascript\nvar AWS = require('aws-sdk');\nvar request = require('request');\nvar s3 = new AWS.S3({\n  signatureVersion: 'v4'\n});\nvar ssecKey = new Buffer(32).fill('sample key');\nvar ssecMD5 = AWS.util.crypto.md5(ssecKey.toString(), 'base64');\n//Generate the presigned url\nvar url = s3.getSignedUrl('putObject', {\n  Bucket: '',\n  Key: 'ssec-test.txt',\n  SSECustomerAlgorithm: 'AES256',\n  SSECustomerKey: ssecKey,\n});\n//Test the url by making a request\nrequest({\n  url: url,\n  method: 'PUT',\n  body: 'Testing SSE-C with presigned url.',\n  headers: {\n    'x-amz-server-side-encryption-customer-key': AWS.util.base64.encode(ssecKey),\n    'x-amz-server-side-encryption-customer-algorithm': 'AES256',\n    'x-amz-server-side-encryption-customer-key-MD5': ssecMD5\n  }\n}, function(err, resp, body) {\n //Verify that the object was written by retrieving the object\n s3.getObject({\n    Bucket: '',\n    Key: 'ssec-test.txt',\n    SSECustomerAlgorithm: 'AES256',\n    SSECustomerKey: ssecKey,\n  }, function(err, data){\n    if (err){\n      console.error(err);\n    } else {\n      console.log(data.Body.toString());\n    }\n  });\n});\n``\n. @calendee \nUsing the code you've provided, I'm able to get an object using a presigned URL and SSE-C. What region is the bucket you're testing in? Can you also share the content-type of the file you're testing with as well?\n. @calendee \nThat error message is suggesting that the headers you're sending to the s3 service usingrequestcontain invalid characters. Can you share what version of node and request you're using as well? \n. @calendeex-amz-copy-source-server-side-encryption-customer-algorithmis only used withcopyObjectanduploadPartCopy`.\nWere you trying to create a presigned url with either of these operations?\n@XpressiveCode \nI'll try to reproduce your issue in the browser. Within node, the code I provided works and should work within a browser, but I'll confirm.\n. @calendee \nI tested the following code sample using node 0.12.9, the latest version of the SDK, Request version 2.69.0, on OS X. Please try the attached code, replacing the 3 instances of <BUCKET> with your actual bucket. Note that this example assumes your bucket is in us-east-1.\nThis example will create a text file using sse-c and a presigned putObject url. Once the PUT operation is complete, it uses the SDK's getObject operation to verify the file was uploaded. It then creates a presigned getObject url, and displays the contents of the same object that was uploaded earlier.\nIf this still doesn't work, can you share some more details about your host environment?\nssec_example.txt\n. @tcf909 \nLooks like there is a PR that relates to this:\nhttps://github.com/nodejs/node/pull/4819\n. Closing, this was fixed with version 4.3.1 of node.\n. @davidporter-id-au \nBy default, keepAlive is not enabled in node. Enabling this should speed things up as existing sockets will be reused. Do you mind testing this option out? You can do this by instantiating your DynamoDB client with the following options:\njavascript\nvar dynamo = new AWS.DynamoDB({\n  region: \"ap-southeast-2\",\n  httpOptions: {\n    agent: new https.Agent({\n      rejectUnauthorized: true,\n      keepAlive: true\n    })\n  }\n});\n. @davidporter-id-au \nPlease correct me if I'm misreading the graph, but I don't think it is necessarily showing that a lot of time is spent on retrieving credentials. \nFor each operation, the SDK will validate credentials to determine if they should be refreshed, so we would expect a large sample of credential related calls. The SDK also gets credentials when signing the request (this may be able to be optimized), but again, checks if credentials need to be refreshed before actually retrieving them. The graph doesn't seem to show how much CPU time is actually spent on each method call, but rather how many times each method was called.\nHow long were these tests run for? Since you're grabbing credentials using the EC2 metadata service, we should see the lib/credentials/ec2_metadata_credentials.js file referenced in the graph if credentials were actually being retrieved, which I don't see at all in this graph. The SDK does cache credentials, \nBased on this graph alone, I can't say that the SDK is not spending a lot of CPU time on getting credentials, I just don't think it tells us much except how many times those methods are called.\nDid you create flame graphs for other versions of nodeJS as well? Specifically those that take ~60ms vs ~15ms?\n. Closing old issue. Looks like this has been resolved.. @wjrjerome \nIt will depend on how many concurrent connections are being made against a host. There is a maxSockets parameter that can also be passed to the agent which controls the maximum number of sockets that can be opened to a host at once. By default, node sets this value to Infinity so this can cause each request to use a different socket. You can set a lower value to ensure a socket gets reused. You can find more info on maxSockets and maxFreeSockets in the node documentation.\nI don't know if you can change how long keep-alive is set for, but node might be closing the connection after requests complete if maxSockets Infinity.. @raffi-minassian \nThere are two changes you'll need to make so that you can use streams and request to upload a file using a presigned url.\nFirst, request is using chunked uploads when it is given a stream as the body to upload. Signature Version 4 needs to be used when generating the presigned url to support this. When instantiating your s3 client, pass in the signatureVersion: 'v4' parameter:\njavascript\nvar s3 = new AWS.S3({\n  signatureVersion: 'v4'\n});\nrequest also automatically sets the Transfer-Encoding header when using streams, but S3 needs to know what the final content-length is. In the options you pass into request, you'll need to supply the Content-Length header.\njavascript\nvar stats = fs.statSync('/path/to/file');\nfs.createReadStream('/path/to/file').pipe(request({\n  method: 'PUT',\n  url: url,\n  headers: {\n    'Content-Length': stats['size']\n  }\n}, function (err, res, body) {\n  console.log(body);\n}));\nThere's some additional information here:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-streaming.html\nLet me know if this helps!\n. @rainabba \nThe deletePreset operation doesn't return any data other than an empty JSON object if it succeeds. If the operation fails then data will be null.\nCan you explain a little more what you're trying to accomplish by listening to events emitted by the AWS.Request object?\n. @rainabba \nRegarding the throttling issues, please feel free to post to the AWS forums to voice your pain points with the Elastic Transcoder service team:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=147\nSo if I'm understanding correctly, when your deletePreset operation is run, you want to be able to know which preset was deleted?\nIt looks like you've already set up one way of keeping track of which preset was deleted. Inside the example you provided above, if data is defined (empty object), you can assume the preset was deleted and make use of the id in param to keep track of which preset was deleted. If data is null, then you can assume the same id in param was not deleted (and can look at the error for more details, i.e. due to throttling.)\nAlternatively, you can use the success and error events that get emitted by the request to handle some of this. Here's an example of what that might look like:\njavascript\nvar req = elastictranscoder.deletePreset(param); //Omitting the callback returns a request object\nreq.on('success', function (response) {\n  console.log(param.Id + ' was deleted');\n});\nreq.on('error', function (err) {\n  console.log(param.Id + ' was not deleted');\n  console.error(err);\n});\nreq.send();\nLet me know if that helps or you have other questions/concerns.\n. I understand your frustration. In this case, the SDK doesn't have control over what information is coming back in the data object, it just returns what the Elastic Transcoder service returns. One thing I can do is forward this issue to their team so they can take a look and determine if they'll add this information in the response.\nThe throttling issue is interesting. Our SDKs have logic to exponentially backoff on throttled requests, but this is done per operation. It does sound like having an request batching mechanism within the SDK could be a useful feature. That would probably be better to log as a separate issue for tracking. (It could reference this one)\n. @rainabba \nI reached out to the service team and references this issue, so when I hear back I'll post back here.\nWe would need to update our API model in order to also return the Id. However, these are generated automatically so we don't modify them directly.\n. @rainabba \nI reached out to the service team and references this issue, so when I hear back I'll post back here.\nWe would need to update our API model in order to also return the Id. However, these are generated automatically so we don't modify them directly.\n. Closing old issues. We can't provide support for individual service behavior. If you're having issues with the SDK and its behavior, please open an issue.\nPlease post to the fourms or if you have a support agreement reach out to AWS support.. Closing old issues. We can't provide support for individual service behavior. If you're having issues with the SDK and its behavior, please open an issue.\nPlease post to the fourms or if you have a support agreement reach out to AWS support.. @matsev,\nDoes the IdentityPoolId you ended up using match the pattern REGION:GUID?\nIf you look up the Identity Pool Id in the console, you'll see it in the pattern REGIONGUID, with no colon between the two. The ARN would actually look very different: arn:aws:cognito-identity:region:accountId:identitypool/REGIONGUID.\nIf you call listIdentityPools({MaxResults:60});, you are returned the IdentityPoolIds in the REGION:GUID format that the API calls require.\nWe can add a note to specify that calling this operation will set the roles to exactly what is provided, and doesn't simply update them.\n. @matsev,\nDoes the IdentityPoolId you ended up using match the pattern REGION:GUID?\nIf you look up the Identity Pool Id in the console, you'll see it in the pattern REGIONGUID, with no colon between the two. The ARN would actually look very different: arn:aws:cognito-identity:region:accountId:identitypool/REGIONGUID.\nIf you call listIdentityPools({MaxResults:60});, you are returned the IdentityPoolIds in the REGION:GUID format that the API calls require.\nWe can add a note to specify that calling this operation will set the roles to exactly what is provided, and doesn't simply update them.\n. @BernhardBezdek \nWhen I try uploading an archive to a non-existent vault I get the ResourceNotFoundException as expected. \nAre there any more details you can provide about the error you're seeing? When you said credentials are working in a previous upload, do you mean that this test used to work and doesn't, or a separate test is continuing to work?\nI also only tested using my AWS account credentials, are you using an IAM role?\n. @BernhardBezdek \nWhen I try uploading an archive to a non-existent vault I get the ResourceNotFoundException as expected. \nAre there any more details you can provide about the error you're seeing? When you said credentials are working in a previous upload, do you mean that this test used to work and doesn't, or a separate test is continuing to work?\nI also only tested using my AWS account credentials, are you using an IAM role?\n. What version of the SDK were you using before? If you revert the SDK, does the error code match what you expect?\n. What version of the SDK were you using before? If you revert the SDK, does the error code match what you expect?\n. @BernhardBezdek \nI don't believe this is an issue with the SDK.\nThe SDK is returning the error that the Glacier service returns in the response. Information on what causes the different error codes to be returned by the service can be found here:\nhttp://docs.aws.amazon.com/amazonglacier/latest/dev/api-error-responses.html\nThe model for the uploadArchive hasn't changed recently. While it's possible the service could have changed which error gets returned, I haven't been able to reproduce what you're seeing yet. \nCan you try the following example? When I run it, it returns the error ResourceNotFoundException as well as the current version of the SDK (2.2.37).\n``` javascript\nvar AWS = require('aws-sdk');\nvar treehash = require('treehash');\nAWS.config.update({\n  region: 'eu-central-1'\n});\nvar glacier = new AWS.Glacier();\nvar buffer = new Buffer(500*1024).fill('text');\nvar params = {\n  accountId: '-', //Glacier assumes account id based on provided credentials\n  vaultName: 'fake-name',\n  body: buffer,\n  checksum: treehash.getTreeHashFromBuffer(buffer),\n  archiveDescription: 'desc'\n};\nglacier.uploadArchive(params, function(err, data) {\n  if (err) {\n    console.error(err);\n  } else {\n    console.log(data);\n  }\n});\nconsole.log(AWS.VERSION)\n```\n. @BernhardBezdek \nI don't believe this is an issue with the SDK.\nThe SDK is returning the error that the Glacier service returns in the response. Information on what causes the different error codes to be returned by the service can be found here:\nhttp://docs.aws.amazon.com/amazonglacier/latest/dev/api-error-responses.html\nThe model for the uploadArchive hasn't changed recently. While it's possible the service could have changed which error gets returned, I haven't been able to reproduce what you're seeing yet. \nCan you try the following example? When I run it, it returns the error ResourceNotFoundException as well as the current version of the SDK (2.2.37).\n``` javascript\nvar AWS = require('aws-sdk');\nvar treehash = require('treehash');\nAWS.config.update({\n  region: 'eu-central-1'\n});\nvar glacier = new AWS.Glacier();\nvar buffer = new Buffer(500*1024).fill('text');\nvar params = {\n  accountId: '-', //Glacier assumes account id based on provided credentials\n  vaultName: 'fake-name',\n  body: buffer,\n  checksum: treehash.getTreeHashFromBuffer(buffer),\n  archiveDescription: 'desc'\n};\nglacier.uploadArchive(params, function(err, data) {\n  if (err) {\n    console.error(err);\n  } else {\n    console.log(data);\n  }\n});\nconsole.log(AWS.VERSION)\n``\n. @BernhardBezdek \nThe other possibility, is if the accountId you're passing doesn't match your credentials, and now the archive you were specifying actually does exist for the specified account. If your account doesn't have access to the archive, you'd see anAccessDeniedException.\n. @BernhardBezdek \nThe other possibility, is if the accountId you're passing doesn't match your credentials, and now the archive you were specifying actually does exist for the specified account. If your account doesn't have access to the archive, you'd see anAccessDeniedException.\n. @BernhardBezdek \nDid you get the same error when settingaccountIdto a dash? It's commented out in your example so I can't tell if you used a dash or set it to something else.\n. @BernhardBezdek \nDid you get the same error when settingaccountId` to a dash? It's commented out in your example so I can't tell if you used a dash or set it to something else.\n. @BernhardBezdek \nVery sorry for the delay in responding! I am unable to reproduce your error unless I enter an accountId that is not my own. This error is also coming from the service, not generated by the SDK, so I don't believe it is an SDK issue.\nIf you're still having issues, I believe your best bet is to post to the AWS Glacier forums. If you provide the requestId that's part of the error object, they should be able to look into what happened on their end to cause the error.\n. @BernhardBezdek \nVery sorry for the delay in responding! I am unable to reproduce your error unless I enter an accountId that is not my own. This error is also coming from the service, not generated by the SDK, so I don't believe it is an SDK issue.\nIf you're still having issues, I believe your best bet is to post to the AWS Glacier forums. If you provide the requestId that's part of the error object, they should be able to look into what happened on their end to cause the error.\n. @matsev \nThanks for bringing up this issue. From the SDK side, we're just passing along the error that the service provides us with. The docs do specify that the values in the Roles map should be role ARNs, but I can ask the service team if they could do some validation on the roles passed in to make sure they conform to what an ARN looks like.\n. @matsev \nThanks for bringing up this issue. From the SDK side, we're just passing along the error that the service provides us with. The docs do specify that the values in the Roles map should be role ARNs, but I can ask the service team if they could do some validation on the roles passed in to make sure they conform to what an ARN looks like.\n. Closing old issues. We can't provide support for individual service behavior. If you're having issues with the SDK and its behavior, please open an issue.\nPlease post to the fourms or if you have a support agreement reach out to AWS support.. Closing old issues. We can't provide support for individual service behavior. If you're having issues with the SDK and its behavior, please open an issue.\nPlease post to the fourms or if you have a support agreement reach out to AWS support.. @jgilbert01 \nCan you share what the error is you're getting when calling putBucketReplication?\n. @jgilbert01 \nCan you share what the error is you're getting when calling putBucketReplication?\n. @jgilbert01 \nThanks for the error message. Depending on which signature version I configure the S3 client to use, I am getting different error messages. I haven't pinpointed where the issue is when using the v2 signer yet.\nIt looks like ContentMD5 is actually required for putBucketReplication. With the v4 signer, ContentMD5 isn't calculated unless it is marked as required (which it isn't currently in the SDK.) This is something that we'll have to address in the SDK, but there is a temporary workaround you can do until a fix is in place.\n``` javascript\n//Set the S3 client to use signature version 4\nvar s3 = new AWS.S3({\n  signatureVersion: 'v4'\n});\n//Grab a reference to the AWS.Request object\nvar req = s3.putBucketReplication(params);\n//Workaround - set the Content-MD5 header\nreq.on('build', function(req) {\n  var md5 = AWS.util.crypto.md5(req.httpRequest.body, 'base64');\n  req.httpRequest.headers['Content-MD5'] = md5;\n});\n//Trigger the service call\nreq.send(function(err, data) {\n  if (err) {\n    console.log(err);\n  } else {\n    console.log(data);\n  }\n});\n```\nOne more 'gotcha'. This isn't pointed out in our SDK API docs, but the Destinaton bucket needs to be the bucket ARN, not the name. This wasn't obvious until I looked at the S3 docs:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTreplication.html\nSo, instead of specifying the bucket name, you'd specify the ARN using the following pattern:\narn:aws:s3:::bucket_name\n. @jgilbert01 \nThanks for the error message. Depending on which signature version I configure the S3 client to use, I am getting different error messages. I haven't pinpointed where the issue is when using the v2 signer yet.\nIt looks like ContentMD5 is actually required for putBucketReplication. With the v4 signer, ContentMD5 isn't calculated unless it is marked as required (which it isn't currently in the SDK.) This is something that we'll have to address in the SDK, but there is a temporary workaround you can do until a fix is in place.\n``` javascript\n//Set the S3 client to use signature version 4\nvar s3 = new AWS.S3({\n  signatureVersion: 'v4'\n});\n//Grab a reference to the AWS.Request object\nvar req = s3.putBucketReplication(params);\n//Workaround - set the Content-MD5 header\nreq.on('build', function(req) {\n  var md5 = AWS.util.crypto.md5(req.httpRequest.body, 'base64');\n  req.httpRequest.headers['Content-MD5'] = md5;\n});\n//Trigger the service call\nreq.send(function(err, data) {\n  if (err) {\n    console.log(err);\n  } else {\n    console.log(data);\n  }\n});\n```\nOne more 'gotcha'. This isn't pointed out in our SDK API docs, but the Destinaton bucket needs to be the bucket ARN, not the name. This wasn't obvious until I looked at the S3 docs:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTreplication.html\nSo, instead of specifying the bucket name, you'd specify the ARN using the following pattern:\narn:aws:s3:::bucket_name\n. @jgilbert01 \nI merged in the fix for this with pr #924 \nIt'll be available in the next version of the SDK, or can be installed from this git repo.\nThanks for reporting it!\n. @jgilbert01 \nI merged in the fix for this with pr #924 \nIt'll be available in the next version of the SDK, or can be installed from this git repo.\nThanks for reporting it!\n. @akshath4u \nAre you using the browser version of the SDK? This service isn't included in the browser version of the SDK because it doesn't support CORS:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\nYou can build a browser version of the SDK that includes CloudSearchDomain if you know that the environment you'll be running the SDK in does not enforce CORS:\nhttps://sdk.amazonaws.com/builder/js/\nLet me know if that helps!\n. @akshath4u \nAre you using the browser version of the SDK? This service isn't included in the browser version of the SDK because it doesn't support CORS:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\nYou can build a browser version of the SDK that includes CloudSearchDomain if you know that the environment you'll be running the SDK in does not enforce CORS:\nhttps://sdk.amazonaws.com/builder/js/\nLet me know if that helps!\n. @guymguym \nThanks for the contribution! Merging this PR, it'll be available via npm in the next release.\n. @guymguym \nThanks for the contribution! Merging this PR, it'll be available via npm in the next release.\n. @andrewgaul \nIf no region is specified, the SDK uses the s3.amazonaws.com endpoint when making a request. According to the S3 docs, when this endpoint is used, the request goes to the us-east-1 region:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html\nNote\nIf you send your create bucket request to the s3.amazonaws.com endpoint, the request go to the us-east-1 region. Accordingly, the signature calculations in Signature Version 4 must use us-east-1 as region, even if the location constraint in the request specifies another region where the bucket is to be created.\nThe documentation for LocationConstraint on the same page also shows that the default region, if none is provided, is us-east-1.\nI'm not sure we could change this behavior if we wanted to, but if we did, this would be a breaking change for customers that are creating buckets in us-east-1 without specifying a region.\n. @andrewgaul \nIf no region is specified, the SDK uses the s3.amazonaws.com endpoint when making a request. According to the S3 docs, when this endpoint is used, the request goes to the us-east-1 region:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUT.html\nNote\nIf you send your create bucket request to the s3.amazonaws.com endpoint, the request go to the us-east-1 region. Accordingly, the signature calculations in Signature Version 4 must use us-east-1 as region, even if the location constraint in the request specifies another region where the bucket is to be created.\nThe documentation for LocationConstraint on the same page also shows that the default region, if none is provided, is us-east-1.\nI'm not sure we could change this behavior if we wanted to, but if we did, this would be a breaking change for customers that are creating buckets in us-east-1 without specifying a region.\n. @andrewgaul \nSorry if I'm not understanding what you're asking for exactly.\nIf the user doesn't specify a location constraint, and no region is defined, the SDK doesn't send a location constraint.\nQuick test code:\njavascript\n//If AWS_REGION wasn't set, then AWS.config.region should be undefined\nvar s3 = new AWS.S3({signatureVersion: 'v4'});\ns3.createBucket({Bucket: 'myNewBucket'}, function(err, data) {});\nResult when inspecting request.httpRequest.stream._headers\nPUT / HTTP/1.1\\r\\n\nUser-Agent: aws-sdk-nodejs/2.2.35 darwin/v0.12.9\\r\\n\nContent-Type: application/octet-stream\\r\\n\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\n\nContent-Length: 0\\r\\n\nHost: myNewBucket.s3.amazonaws.com\\r\\n\nX-Amz-Date: 20160302T160915Z\\r\\n\nAuthorization: AWS4-HMAC-SHA256 Credential=xxx/us-east-1/s3/aws4_request,SignedHeaders=host;x-amz-content-sha256;x-amz-date,Signature=55b5ed5aa5f55b5fc55c7810ff32f9cb80ea6a6001a75367faef6d4d4e7fd401\\r\\n\nConnection: close\\r\\n\n\\r\\n'\nYou'll notice the Content-Length is 0 in this case; we aren't providing a location constraint.\nIf the user has a region defined in a region other than us-east-1, then the SDK will automatically set the location constraint to match the region that was defined if no location constraint was provided.\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/services/s3.js#L417\nSo, it looks like we are behaving the same way as the Java SDK when no region is defined.\nWhat behavior are you seeing exactly? Maybe an example would help us understand better.\n. @andrewgaul \nSorry if I'm not understanding what you're asking for exactly.\nIf the user doesn't specify a location constraint, and no region is defined, the SDK doesn't send a location constraint.\nQuick test code:\njavascript\n//If AWS_REGION wasn't set, then AWS.config.region should be undefined\nvar s3 = new AWS.S3({signatureVersion: 'v4'});\ns3.createBucket({Bucket: 'myNewBucket'}, function(err, data) {});\nResult when inspecting request.httpRequest.stream._headers\nPUT / HTTP/1.1\\r\\n\nUser-Agent: aws-sdk-nodejs/2.2.35 darwin/v0.12.9\\r\\n\nContent-Type: application/octet-stream\\r\\n\nX-Amz-Content-Sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\r\\n\nContent-Length: 0\\r\\n\nHost: myNewBucket.s3.amazonaws.com\\r\\n\nX-Amz-Date: 20160302T160915Z\\r\\n\nAuthorization: AWS4-HMAC-SHA256 Credential=xxx/us-east-1/s3/aws4_request,SignedHeaders=host;x-amz-content-sha256;x-amz-date,Signature=55b5ed5aa5f55b5fc55c7810ff32f9cb80ea6a6001a75367faef6d4d4e7fd401\\r\\n\nConnection: close\\r\\n\n\\r\\n'\nYou'll notice the Content-Length is 0 in this case; we aren't providing a location constraint.\nIf the user has a region defined in a region other than us-east-1, then the SDK will automatically set the location constraint to match the region that was defined if no location constraint was provided.\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/services/s3.js#L417\nSo, it looks like we are behaving the same way as the Java SDK when no region is defined.\nWhat behavior are you seeing exactly? Maybe an example would help us understand better.\n. Closing this issue as it looks like our SDK is behaving the same as the Java SDK when no region is defined in the SDK. Please feel free to comment if more clarification is needed, or you have more details to share.\n. Closing this issue as it looks like our SDK is behaving the same as the Java SDK when no region is defined in the SDK. Please feel free to comment if more clarification is needed, or you have more details to share.\n. @djanowski \nThere is a way to make additional errors retryable. Instead of passing in a callback to your operation, you can grab a reference to an AWS.Request object and bind a callback to the extractError event. In this callback, you can set the error to retryable.\nQuick example:\njavascript\nvar req = lambda.invoke(params);\nreq.on('extractError', function(response) {\n  var error = response.error;\n  if (error.code === 'InvalidAccessKeyId') {\n    error.retryable = true;\n  }\n});\n//Add custom retry delay as well\nreq.on('retry', function(response) {\n  var error = response.error;\n  if (error.code === 'InvalidAccessKeyId') {\n    // override retry delay to be 1 second\n    error.retryDelay = 1000;\n  }\n});\nreq.send(function(err, data) {});\nHowever, what's the cause for the error you're seeing? The retry strategy is aggressive by default, though you can supply custom retry logic:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#retryDelayOptions-property\nYou can also define a set retryDelay using the example above.\n. @djanowski \nThere is a way to make additional errors retryable. Instead of passing in a callback to your operation, you can grab a reference to an AWS.Request object and bind a callback to the extractError event. In this callback, you can set the error to retryable.\nQuick example:\njavascript\nvar req = lambda.invoke(params);\nreq.on('extractError', function(response) {\n  var error = response.error;\n  if (error.code === 'InvalidAccessKeyId') {\n    error.retryable = true;\n  }\n});\n//Add custom retry delay as well\nreq.on('retry', function(response) {\n  var error = response.error;\n  if (error.code === 'InvalidAccessKeyId') {\n    // override retry delay to be 1 second\n    error.retryDelay = 1000;\n  }\n});\nreq.send(function(err, data) {});\nHowever, what's the cause for the error you're seeing? The retry strategy is aggressive by default, though you can supply custom retry logic:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#retryDelayOptions-property\nYou can also define a set retryDelay using the example above.\n. Closing this issue, but feel free to comment or open a new issue if you still have any questions on how to implement the above!\n. Closing this issue, but feel free to comment or open a new issue if you still have any questions on how to implement the above!\n. @kalidossmm \nCan you provide details on what the actual error being returned is?\nThere are certain errors that the SDK will retry by default. For dynamodb with those errors, the SDK will retry up to 10 times while applying exponential backoff.\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/services/dynamodb.js#L43-L52\n. @kalidossmm \nCan you provide details on what the actual error being returned is?\nThere are certain errors that the SDK will retry by default. For dynamodb with those errors, the SDK will retry up to 10 times while applying exponential backoff.\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/services/dynamodb.js#L43-L52\n. @kalidossmm \nWhat version of node are you running? Can you share what the specific error message you are getting is, or do you never see one because one of the retries is succeeding?\nOne thing you might want to try is setting the maxSockets your dynamodb client can use.\nExample:\njavascript\nvar dynamodb = new AWS.DynamoDB({\n  httpOptions: {\n    agent:  new https.Agent({\n      rejectUnauthorized: true,\n      maxSockets: 100 // value was chosen arbitrarily\n    })\n  }\n});\nBy default, node v0.12+ sets maxSockets to Infinity, and depending how many requests you're sending at once, this can cause issues. I don't know what limits DynamoDB might have in place, but I have seen issues due to socket timeouts caused when too many requests were being created at once, which were mitigated by setting maxSockets.\n. @kalidossmm \nWhat version of node are you running? Can you share what the specific error message you are getting is, or do you never see one because one of the retries is succeeding?\nOne thing you might want to try is setting the maxSockets your dynamodb client can use.\nExample:\njavascript\nvar dynamodb = new AWS.DynamoDB({\n  httpOptions: {\n    agent:  new https.Agent({\n      rejectUnauthorized: true,\n      maxSockets: 100 // value was chosen arbitrarily\n    })\n  }\n});\nBy default, node v0.12+ sets maxSockets to Infinity, and depending how many requests you're sending at once, this can cause issues. I don't know what limits DynamoDB might have in place, but I have seen issues due to socket timeouts caused when too many requests were being created at once, which were mitigated by setting maxSockets.\n. @kalidossmm \nAre you still having any issues?\n. @kalidossmm \nAre you still having any issues?\n. @drfence \nThe way the SDK is currently written, there isn't an easy way for us to provide code completion capabilities. Most of the methods that get accessed on a service client (i.e. AWS.S3) are generated at run-time, so we don't have methods to add comments to.\nIt does look like Webstorm can use TypeScript definition files to enable code completion on JavaScript files, and there is a community-maintained repo:\nhttps://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/aws-sdk\nThat said, I don't know how up-to-date this is kept.\nWe could look into providing TypeScript definition files in the future as a way to address this.\nI'll mark this as a feature request for now. We'd love to hear feedback from the community on how helpful providing TypeScript definition files would be for your development!\n. @drfence \nThe way the SDK is currently written, there isn't an easy way for us to provide code completion capabilities. Most of the methods that get accessed on a service client (i.e. AWS.S3) are generated at run-time, so we don't have methods to add comments to.\nIt does look like Webstorm can use TypeScript definition files to enable code completion on JavaScript files, and there is a community-maintained repo:\nhttps://github.com/DefinitelyTyped/DefinitelyTyped/tree/master/aws-sdk\nThat said, I don't know how up-to-date this is kept.\nWe could look into providing TypeScript definition files in the future as a way to address this.\nI'll mark this as a feature request for now. We'd love to hear feedback from the community on how helpful providing TypeScript definition files would be for your development!\n. Closing in favor of #994 \n. Closing in favor of #994 \n. Since we're pulling in jmespath now, we should be able to get rid of jamespath from the AWS.util namespace.\n. Since we're pulling in jmespath now, we should be able to get rid of jamespath from the AWS.util namespace.\n. Ship it! This is great in gaining parity with the other SDKs.\n. Ship it! This is great in gaining parity with the other SDKs.\n. @temujin9 \nOverall, the PR looks like an interesting and useful feature.\nOne issue is that this implementation currently looks for the role to use in the ~/.aws/credentials file. Boto3 actually documents that this information is stored in the ~/.aws/config file. We don't currently support reading from the ~/.aws/config file in the JavaScript SDK, but I'll bring this up with the team to see what the other SDKs think about where to store this configuration.\n. @dump247 \nI discussed this with the team, and we agree this makes sense to include with the ~/.aws/credentials file.\nI'll finish reviewing this asap!\n. @roeyazroel \nAre you getting an error when running the setInstanceHealth command? If so, can you share it?\n. @roeyazroel \nCan you share what environment (node version or browser) and what version of the SDK you are using?\nCan you also share how you've set up ASG_AWS?\nWhen you're instantiating your AutoScaling client, can you pass in the logger: console configuration with the constructor? That'll log out the status of any requests made using the client.\n. @roeyazroel \nIt doesn't look like there is anything wrong based on your code snippet. When you were debugging, you were able to verify that setInstanceHealth was called?\nWe can add some logic to enable more logging, but first, can you try just running setInstanceHealth on it's own? Something as simple as:\njavascript\nvar autoscaling = new AWS.AutoScaling({\n  region: 'us-east-1'\n});\nautoscaling.setInstanceHealth({\n  InstanceId: '11111',\n  HealthStatus: 'fake',\n  ShouldRespectGracePeriod: false\n}, function (err, data) {\n  if (err) {\n    console.log(err);\n  else {\n    console.log(data);\n  }\n});\nThe above code should return an error from the service.\n. @roeyazroel \nAlright, I can't reproduce your error using just the script I supplied.\nHere's another example that adds a log to each event that gets triggered during the request's lifecycle:\n``` javascript\nvar AWS = require('aws-sdk');\nvar autoscaling = new AWS.AutoScaling({\n    region: 'us-east-1'\n});\nvar req = autoscaling.setInstanceHealth({\n    InstanceId: '11111',\n    HealthStatus: 'fake',\n    ShouldRespectGracePeriod: false\n});\nreq.on('validate', function(request) {\n    console.log('validate completed');\n});\nreq.on('build', function(request) {\n    console.log('build completed');\n});\nreq.on('sign', function(request) {\n    console.log('sign completed');\n});\nreq.on('send', function(response) {\n    console.log('send completed');\n});\nreq.on('retry', function(response) {\n    console.log('response completed');\n});\nreq.on('extractError', function(response) {\n    console.log('extractError completed');\n});\nreq.on('extractData', function(response) {\n    console.log('extractData completed');\n});\nreq.on('success', function(response) {\n    console.log('success completed');\n});\nreq.on('error', function(error, response) {\n    console.log(error);\n    console.log('error completed');\n});\nreq.on('complete', function(response) {\n    console.log('complete completed');\n});\nreq.on('httpHeaders', function() {\n    console.log('httpHeaders completed');\n});\nreq.on('httpData', function() {\n    console.log('httpData triggered');\n});\nreq.on('httpUploadProgress', function() {\n    console.log('httpUploadProgress triggered');\n});\nreq.on('httpDownloadProgress', function() {\n    console.log('httpDownloadProgress triggered');\n});\nreq.on('httpError', function() {\n    console.log('httpError completed');\n});\nreq.on('httpDone', function() {\n    console.log('httpDone completed');\n});\nreq.send(function(err, data) {\n    console.log('callback triggered');\n});\n```\nWith the above code I see the following:\nvalidate completed\nbuild completed\nsign completed\nhttpUploadProgress triggered\nhttpHeaders completed\nhttpDownloadProgress triggered\nhttpData triggered\nhttpDone completed\nsend completed\nextractError completed\nresponse completed\n{ [ValidationError: Valid instance health states are: [Healthy, Unhealthy].]\n  message: 'Valid instance health states are: [Healthy, Unhealthy].',\n  code: 'ValidationError',\n  time: Mon Mar 14 2016 14:31:04 GMT-0700 (PDT),\n  requestId: '0e111d9b-ea2c-11e5-aa48-c361319c5111',\n  statusCode: 400,\n  retryable: false,\n  retryDelay: 35.94129232224077 }\nerror completed\ncomplete completed\ncallback triggered\nCan you try running just the above code and share what gets logged?\nJust to double check, you ran just the example by itself, no surrounding code? I've seen issues occur in rare cases due to another library being imported, so want to make sure to eliminate as many variables as possible.\n. Do you get the same results when you run it stand-alone?\n. Thanks @roeyazroel \nSo it looks like there's something in your surrounding code that is causing issues with the SDK. Are you making use of any other libraries other than the SDK?\nIf you post the results of logging out the httpRequest in the sign callback, that might provide some clues:\nconsole.log(request.httpRequest);\nYou might want to replace your credential that's logged if you share the log here.\nOtherwise, without seeing the full surrounding code, there's not much more I can do to troubleshoot from my end unfortunately. (which I realize you may not want to share).\n. @roeyazroel \nIt looks like process.exit(0); gets called before setInstanceHealth finishes. \nAll of the SDK operations are asynchronous. You loop through the autoscaling groups and call updateInstanceHealth on each instance, which in turn calls setInstanceHealth. The first few states of the request lifecycle (validate, build, sign) occur synchronously, but then actually making the request is async. This means that before those responses are returned, your forEach loop will complete and then process.exit(0); will be called.\nYou can use a library such as async to more easily manage running asynchronous tasks. This would allow you to do something like only exit the process once all callbacks have returned.\n. @dukedougal \nAborting requests is not supported in a browser currently.\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Request.html#abort-property\nNote: This feature is not supported in the browser environment of the SDK.\n. @dukedougal \nCan you share what browser (including version) you are using, and which SDK version?\n. @dukedougal \nI tried reproducing your issue within some browsers but haven't been able to yet.\nHere's a test function that will attempt to upload three 23 MB files at once. Because I'm uploading 3 files at once, the loaded field on progress will sometimes appear to go down, but looking at the key, it's obvious that the value increases for each file.\n``` javascript\nfunction testFileUpload() {\n    AWS.config.update({\n        credentials: credentials\n    });\nvar s3 = new AWS.S3();\n\nvar opts = {\n    queueSize: 4,\n    partSize: 1024*1024*10\n};\n\nfor (var i = 0; i < 3; i++) {\n    var params = {\n        Bucket: 'BUCKET',\n        Key: 'browser-upload-test-' + i + '.dat',\n        Body: new Blob([new ArrayBuffer(1024*1024*23)])\n    };\n    var uploader = s3.upload(params, opts);\n\n    uploader.on('httpUploadProgress', function(progress) {\n        console.log(progress);\n    });\n\n    uploader.send(function(err, data) {\n        if (err) {\n            console.log(err);\n        } else {\n        console.log(data); \n        }\n    });\n}\n\n}\n```\nCould you try just the above example and see if the results you're seeing are along the same lines as with this example?\nWhen your uploads fail, what error message do you get?\n. Closing this issue since it appears to be related to how much bandwidth the client has. Setting queueSize to 1, or the timeout to 0 should resolve this.. @jkervine \nWhat version of the SDK are you using, and is this in node or the browser?\nI tested the above in node and am getting the expected results when passing in the FunctionVersion.\nCan you run the following code and verify that the path that is logged contains ?FunctionVersion=14 in the querystring?\n``` javascript\nvar versionAliasesParams = {\n  FunctionName : 'simple',\n  FunctionVersion: '14'\n};\nvar lambda = new aws.Lambda({\n  apiVersion: '2015-03-31'\n});\nvar req = lambda.listAliases(versionAliasesParams);\nreq.on('build', function(request) {\n    console.log(request.httpRequest.path);\n    //path should end with ?FunctionVersion=14\n});\nreq.send(function(err, data){\n   if(err) console.log(err, err.stack);\n   else {\n     console.log(\"version aliases:\",data);\n   }\n});\n``\n. @jkervine \nDid you try running the code example in a stand-alone script, without any surrounding code (aside from setting up credentials)? Using the same version of node.js and the SDK, I'm unable to reproduce the issue you're seeing.\n. Thanks for the update @jkervine. This sounds like it was a service-side issue, not one related to the SDK (thank you for providing some of your logged output).\n. :shipit:\n. @jgainfort \nYou'll want to add your seconderrorhandler to the stream returned bycreateReadStream(). This is because the error is coming from the service, and triggers an error on the read stream.pipe()` returns the writeable stream, but in this case an uncaught error is thrown before the error event on the writeable stream can be handled anyway.\nYour first error handler is also not needed. When calling createReadStream on a request object, a new error handler is automatically attached to the request that passes the error along to the stream's error handler. This is also why you're seeing an uncaught error; the error event is still getting emitted on the stream.\nSo, something like the following should work:\njavascript\ns3.getObject(params)\n    .createReadStream()\n    .on('error', function (err) { //Handles errors on the read stream\n        error = true;\n        console.log('Error reading file');\n        file.end();\n    })\n    .pipe(file)\n    .on('error', function (err) { //Handles errors on the write stream\n        error = true;\n        console.log('Error writing file');\n        file.end();\n    })\n    .on('finish', function () {\n        file.end();\n        if (!error) {\n            console.log('Successfully downloaded file from S3');\n        }\n    });\n. @jgainfort \nGlad to hear it!\n. @mathieuruellan \nYou shouldn't be using the new keyword when passing a callback function to any of the operations. Can you remove that and try again?\n. @rdewaele \nIt doesn't look like the embed functionality is working with getDeployments() in the SDK, so I'l reach out to the API Gateway team about that.\nHowever, it is supported with getDeployment(). You're right in that this is related to #764. We haven't yet implemented the changes to support using embed as it requires updating the models that the SDKs share, and we're working on making sure all the SDKs support it before making the change.\nThere is a work-around you can use though. It's possible to add embed=apisummary to the path's query string.\n``` javascript\nvar req = apigateway.getDeployment({\n    restApiId: 'apiId', \n    deploymentId: 'deploymentId'\n});\nreq.on('build', function(req) {\n    req.httpRequest.path += '/?embed=apisummary';\n});\nreq.send(function(err, data) {\n    if (err) {\n        console.log(err);\n    } else {\n        console.log(data);\n    }\n});\n``\n. @yuvalselffer \nWhat's the delta between how long a large upload blocks for compared to a small one? Commands immediately after theupload` operation should run before your callback function is called unless a request wasn't sent.\nYou specified that this happens with larger blobs. The uploader will do 2 different things depending on the size of the file. If it is less than 5 MB by default, then it will make a simple putObject request. If it is 5 MB+, then the uploader will do a multipart upload instead. When it does a multipart upload, it will perform some additional logic so that it can upload multiple parts in parallel (up to 4 by default.) This could take longer, especially if sigv4 signing is used, because each part needs to have a content-md5 or sha256 signature calculated.\n. Can you share which browser you are testing in? My inclination is that the SHA256 calculation is taking the bulk of the time. If you can let me know what browser you're using, and if you're using the File API or ArrayBuffers (how are the array buffers getting populated) I can do some more analysis.\n. @yuvalselffer \nI modified your example a bit and ran the CPU profiler in Chrome. In my example, I 'tick' every 100 ms, and am uploading a ~44 MB zip file.\nIt is the SHA256 calculation that takes the bulk of processing time. As soon as the upload starts (which happens immediately if you supply a callback function to the upload operation), the request is built and signed. These steps happen synchronously, so we would not expect to see any 'ticks' while this is happening. Here's a screenshot of the CPU profiling when using sigv4 signing. Note that the core_sha256 function, which is used to create the signature, takes up the bulk of the time.\n\nWhen I switch to using sigv2 signing, which does not use SHA256 to create the signature, you don't see the same lag. Sigv2 is used by default with S3 for regions that support signature version 2. It can also be set by doing the following:\njavascript\nvar s3 = new AWS.S3({signatureVersion: 's3'});\n\nFuture\nWe are working on ways of reducing the CPU cost of sigv4 signing with S3. \n. @yuvalselffer \nI'm closing the issue since the SDK is functioning as expected.\nIf your bucket is in a region that supports signature version 2, then you can use the default signer which is much less computationally expensive.\nThere are some other things that can be done, but would require not using the upload function. For example, the SHA256 calculation could be done in a webworker, which may help UI performance, but this would be non-trivial to implement.\nYou could also generate a presigned URL, which doesn't generate a sha256 hash of the file, and use that to upload. This method would result in slower uploading of large files, but a faster 'start-up' time.\nWe are also working on reducing the computational cost of using sigv4 with S3, but I can't give an estimate on when that will be available beyond that it's something we are prioritizing.\n. :shipit:\n. @guymguym \nThanks for the PR! I'll take a look at it today and run our integration tests on it. Will especially be testing against older environments of nodejs that we're still supporting (0.8.x and 0.10.x)\n. @guymguym \nJust wanted to give you an update. Some of our regression tests failed when I pulled down your changes, but I haven't had a chance to dig into why yet. I'll work on that some point this week.\nSorry for the delay!\n. @guymguym \nOur regression tests are cucumber tests, specifically the progress tests are failing:\nhttps://github.com/aws/aws-sdk-js/blob/master/features/s3/objects.feature#L164\nhttps://github.com/aws/aws-sdk-js/blob/master/features/s3/step_definitions/objects.js#L160\nSorry I haven't dug deeper into whether this is an actual issue or not yet, but it is on the list!\n. @guymguym \nYes, I will rerun it, thank you!\n. @guymguym \nYou can modify the configuration.sample file (https://github.com/aws/aws-sdk-js/blob/master/configuration.sample) to include just the region and s3 properties.\nNote: Please update the S3 bucket used.\nYou also need to npm install cucumber at version 0.5.3. (I think some newer versions don't work with our suite).\nThen, to run the tests, you type:\ncucumber.js -t @s3 \nto run just the s3 tests. You will not want to run all the tests, because these will make actual requests and possibly create resources that you could be charged for.\nThanks for pinging me here. I should have more time this week to review it. \n/cc @LiuJoyceC  as well.\n. @guymguym \nLooks good! Thanks for all the work you put into this. Merging this in now, and it will be available via npm with the next release!\n. :shipit: \n. @flogball00 \nWhen using your URL, you will need to also pass in the Content-Type header.\njavascript\nvar request = require('request');\nrequest({\n    url: url,\n    method: 'PUT',\n    body: 'Some text to test with.',\n    headers:{\n        \"Content-Type\": \"text/csv\"\n    }\n});\nIf setting the header doesn't fix this for you, can you share the code you're using with the presigned URL?\n. @adolfosrs \nCan you take a look at your network connections through your browser when uploading a file? I'd like to know if any of the requests to S3 are timing out.\nBy default, the SDK has a timeout period of 2 minutes. If the connection is slow enough that a single part can't finish uploading within 2 minutes, then the SDK will attempt to retry uploading it. This could also happen if the connection is unstable/dropping enough packets that the SDK has to retry uploading a part.\nIt's possible to turn off the timeout, or change it.\nUsing the example in your stackoverflow post, you could turn it off by doing the following:\njavascript\nvar bucket = new AWS.S3({\n  apiVersion: '2006-03-01', \n  httpOptions: {timeout: 0}, \n  params: {Bucket: $scope.creds.bucket}\n});\nLet me know what you find out!\n. Closing this issue due to inactivity. The browser sdk was updated to use the version of buffer that was reported to work above.\n. @elidupuis & @cdl \nThanks for reporting that the issue is still happening. @kevin1024, was it fixed for you?\n. @elidupuis \nCan you share a snippet of the code you're running? I'm using testing using Edge 25.10586.0.0 and the version 2.4.0 of the SDK. I'm running the s3.upload command with a small file in us-west-2 using sigv4 signing and am not able to reproduce. Are you uploading a large file? Is this error happening on s3 operations that aren't uploads?\n. @kiraLinden \nThanks for the additional code, I'll take a look!\n. @kiraLinden \nI still haven't been able to reproduce the issue on multiple windows 10 devices, and we've updated our browserify to attempt to address this with mixed messages.\nCan you provide which version of the SDK you're using, and whether it is a custom build, from our CDN (using a script tag), from bower/npm, or from the browser builder? Can you also share if you're seeing this with files larger than 5 MB or for all files, and if you're specifying a signatureVersion and the region for your S3 client?\nI'm wondering if a different version of the buffer module browserify uses is in place based on the way the browser SDK is created. That's something I can look into, but any additional info you can provide to narrow this down is greatly appreciated.\n. So this is a very different issue than what this thread is concerning. The original issue was with using the s3.upload method from within Edge. You're creating a signed url on the server, and then using it on the front-end. \nWhat region is your bucket in? If your bucket is in a different region than what was specified when you created your S3 client, you may also see that signature mismatch error. If you haven't specified a region (I don't see any mention of that in the tutorial), then by default the SDK uses us-east-1 with S3. The SDK knows how to handle redirects if the wrong region was specified when you call S3 operations directly, but can't handle them when you're generating a URL.\nCan you try instantiating your S3 client with the same region that the bucket is located in, and then generating the signed url?\n. @kiraLinden \nIn your case, it looks like Edge isn't sending the Content-Type header when it uploads an image. Other browsers, like Firefox, appear to send it based on the type of the file passed into xhr.send().\nI was able to upload a file once I changed your uploadFile function to add\njavascript\nxhr.setRequestHeader('content-type', file.type);\nafter the xhr.open call.\nYou could also remove ContentType from your s3 params when you're generating your signed url so that it isn't enforced.\n. Closing the issue since this appears to be resolved.. @revic1993 \nI don't believe you need to JSON stringify the gcmMessage.\nThis may be a better question for the SNS team, but can you try formatting your GCM message a little differently? I believe your data object needs to contain a message field with the message as the value.\njavascript\nvar gcmMessage = {\n  \"data\": {\n    \"message\": message\n  }\n};\n. @alexosunas \nWhat version of the SDK are you using, and what environent? (node.js version or browser).\nUsing the latest version of the SDK in node 0.12.x, this code is working for me:\n``` javascript\nvar kms = new AWS.KMS({apiVersion: '2014-11-01'});\nvar generateDataKey = kms.generateDataKey({\n    KeyId: 'alias/MyAlias',\n    KeySpec: 'AES_256'\n}).promise();\ngenerateDataKey.then(function(data) {\n    return kms.decrypt({\n        CiphertextBlob: data.CiphertextBlob\n    }).promise();\n}).then(function(data) {\n    console.log(data);\n}).catch(function(err) {\n    console.log(err);\n});\n``\n. @alexosunas \nThat isn't the AWS SDK. Can you try the above code using just the AWS SDK? If the problem is withjwt-simple, then the issue should be logged against that project.\n. @alexosunas \nGlad to hear it!\n. @inversion \nAs you've probably already discovered, when a callback is supplied to an operation, orsendis called on aAWS.Requestobject, the request is sent. At this point, a stream should not be created, as the request has already been sent. If a stream is created, it will send the request again, and the response that is piped into theWriteStream` will contain roughly twice the data.\nWe'll add a feature request to throw an error if createReadStream is called once the request has already been sent.\n. @kentor \nCan you share what region you are testing in? I'm specifically curious if signature version 4 is being used or not as that can affect performance. Signature version 2 is used by default unless the region only supports version 4.\n. @kentor \nI tried running your code in an m4.xlarge instance in us-west-1, using firefox and chrome. I tested using a ~300 MB video file. In both browsers, the upload speed was at least 200 Mbps, with chrome sometimes approaching 300 Mbps. Just in case, the version of Chrome I was testing with was: 49.0.2623.112 m\nHow large are the files you're transferring? Is Chrome consistently slower?\n. @kentor \nI see the same 'spikey' behavior when the inspector is open. However, I think this is specifically related to Chrome, not the SDK, since the uploads are not spikey when the inspector is closed. \n. @JaKXz \nYes, calling promise() will immediately trigger the request.\nAre your objects showing up in your S3 bucket? If you inspect your object and look at the Metadata, is there a Website Redirect Location key?\n. @JaKXz \nI haven't used ramda before, but based on your example, I think you are missing a functor.\njavascript\n .then(R.map(createAndSendRedirectObject))\nRambda docs:\nhttp://ramdajs.com/0.21.0/docs/#map\nR.map isn't passing in the redirect object that createAndSendRedirectObject needs.\nIf you replaced the above line with a new function that accepted an array of objects you could do something like this:\njavascript\n.then(createAndSendRedirectObjects)\njavascript\nfunction createAndSendRedirectObjects(redirects) {\n  var allObjects = redirects.map(function(redirect) {\n    var objectParams = {\n      Key: redirect.from,\n      ACL: 'public-read',\n      WebsiteRedirectLocation: redirect.to\n    };\n    return s3Bucket.putObject(objectParams).promise();\n  });\n  return Promise.all(allObjects); //Might be able to substitute with Q.all(allObjects);\n}\nUsing this code, you wouldn't need the Q.all that you have in your example either.\nNote: I haven't tested the above code, but that's the general idea. Let me know if that helps!\n. I don't think there's anything we could have done from the SDK side. R.map likely was returning an empty array, as nothing was passed in for the function to iterate over. I'd suspect that s3.putObject wasn't being called.\nIf s3.putObject was called, then I'd be interested to see what R.map was actually returning.\n. @jppellerin \nWhat version of the SDK are you using? I tried running your code using the latest version of the SDK and the update operation is properly replacing my array with the specified values.\nIf updating the SDK doesn't work, can you provide a simple, working version of your script that reproduces the issue. This will help us determine if there's something else causing issues with the operation.\nHere's the code I ran to test. When I check through the console, I can see that my deals array is replaced with the elements I've specified.\n``` javascript\nvar AWS = require('aws-sdk');\nvar docClient = new AWS.DynamoDB.DocumentClient();\ndocClient.update({\n    TableName: 'testExpressions',\n    Key: {\n        'id': '1111-2222-3333-4444'\n    },\n    ExpressionAttributeNames: {\n        '#deals': 'deals'\n    },\n    ExpressionAttributeValues: {\n        ':deals': [\n            {\n                'id': '000-000',\n                'origin': 0\n            },\n            {\n                'id': '000-001',\n                'origin': 0\n            }\n        ]\n    },\n    UpdateExpression: 'SET #deals = :deals'\n}, function(err, data) {\n    if (err) {\n        console.log(err);\n    } else {\n        console.log(data);\n    }\n});\n``\n. Closing due to inactivity. Feel free to comment or open a new issue if you're still encountering problems.\n. :shipit: \n. @olivierlesnicki \nCan you share some more details on how you're doing a multi-part upload with a stream? Internally for theuploadoperation, the SDK will read the stream in 5 MB chunks, and make theuploadPartcalls using those buffered chunks.\n. @vcernomschi \nAre you using a file stream created from thefs` module, or from something else?\n. @PsyTae \nWith version 2.4.0, the S3 client was updated to use Sigv4 by default. It's still possible to use Sigv2 by specifying you want to use that when instantiating your client:\njavascript\nvar s3 = new AWS.S3({signatureVersion: 'v2'})\nThis issue should only occur when using signatureVersion: v4 due to how the body is signed. Can you share how your input stream is created?\n. @PsyTae \nGlad to know the work-around is working for you! I submitted PR #1034 that you're free to try that should resolve your issue even when using signatureVersion: v4. If you have a chance to give it a try, please let me know if it resolves your issue!\nYou can also test it by commenting out this line in your local copy of the SDK:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/services/s3.js#L738\n. @olivierlesnicki \nWith version 2.4.2 of the SDK, you body signing is disabled by default when using signatureVersion: v4 and https. \nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#s3DisableBodySigning-property\nNon-file streams should now work as long as s3DisableBodySigning is set to true and the above conditions are met.\n. @gm758 \nYou mentioned you're using react native. Are you pulling in a 3rd party module to access the filesystem?\nI'm not encountering issues uploading a file in node.js when using the node.js fs core module and the request library to make the PUT request.\n. @rwaldron \nSorry for the delay, merging this in!\n. @njeirath \nThe way you're specifying your credentials in your config is a little off. You'll need to put credential related properties, such as accessKeyId and secretAccessKey in a credentials object.\nExample:\njavascript\nvar goodConfig = {\n  credentials: {\n    accessKeyId: 'GOOD_KEY',\n    secretAccessKey: 'GOOD_SECRET'\n  },\n  region: 'us-east-1'\n};\nThe fields you were adding would have been added to your client's config property. Since credentials wasn't provided in your config, the clients would have used whatever was defined in the global config.\n. @njeirath \nCan you provide a full script that reproduces this issue, minus the actual credentials? Also, what version of the SDK are you testing with?\nI tried your original code in firefox and chrome, and they both work as expected, where one call passes and one fails. I'm wondering if something is happening elsewhere in the code to cause this unexpected behavior.\n. @njeirath \nGlad to hear the example is working at least.\n@anthonygreen \nYou can only specify the accessKeyId, secretAccessKey and sessionToken directly in the config options when you are passing them into the AWS.Config constructor:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#constructor-property\nWhen calling AWS.config.update, you need to pass those fields as properties on the credentials object:\njavascript\nAWS.config.update({ \"credentials\": {\"secretAccessKey\" \u2026\nSee the General Configuration Options for a list of params you can set directly.\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html\n. @anthonygreen \nCan you log the AWS.config object and see if the accessKeyId and secretAccessKey are shown?\nWhen I tested, the operation didn't throw an error, but it also didn't save the fields. There's a flag you can set to allow non-standard fields to be saved, but that still wouldn't help your use case.\nI think we should have thrown an error when an unsupported field was submitted, but we risk breaking users if we started doing that now. We may be able to add a warning though when this happens.\n. :shipit:\n. @felixnext \nIn the Lambda console, where you can select a blueprint, have you tried the s3-get-object blueprint? It walks you through setting up a Lambda function to listen for when objects are created in an S3 bucket, and by default calls getObject. I just created a new Lambda function using this blueprint, changing getObject to headObject and it worked immediately.\n. @felixnext \nThat's a question the Lambda team would be better equipped to answer. You can leave a message on their forums: https://forums.aws.amazon.com/forum.jspa?forumID=186\nAccording to their faq, there is some additional configuration in your VPC that needs to be done to be able to access AWS Service endpoints:\nQ: Can Lambda functions in a VPC also be able to access the internet and AWS Service endpoints?\nLambda functions configured to access resources in a particular VPC will not have access to the internet as a default configuration. If you need access to external endpoints, you will need to create a NAT in your VPC to forward this traffic and configure your security group to allow this outbound traffic.\n. @dainbrump \nDoes your 'profile' in your credentials file contain a roleArn?\n. @dainbrump \nNevermind, I misread your post.\nThe credentials object wasn't intended to be populated until either get or refresh was called. There was some work done recently to support 'assume role' credentials in this file which removed the get from the constructor.\nWere you grabbing the credentials from the creds variable directly? The first time a request is made, the credentials should be populated.\n. Either creds.get() or creds.refresh() will populate the object. If no roleArn is in your profile, then that should be a synchronous task, so you won't need to supply a callback to either operation.\n. Was the secretAccessKey viewable previously by just logging creds? The SDK hides that property from util.inspect so that it's not accidently logged. Logging in node.js shouldn't show it, but if you log it directly you can see it.\ni.e. console.log(creds.secretAccessKey)\n. @dainbrump \nGlad to hear. We'll get the docs updated. We also may need to consider adding get back into the constructor. While it wasn't meant to be there, if users are already writing code based on how it was working (docs haven't been the clearest on how it should work), it might be worth keeping that in there so user's don't break when they upgrade.\n. :shipit:\n. @srkimir \nA crc32 check(a) is done on the body of a response to verify that the the response contains the correct data that DynamoDB meant to send. One way a discrepancy could occur is if there were network issues that caused only part of the response to be sent, which would lead to a JSON parse failure.\nWhen a crc32 check fails, the request should be retried, up to 10 times by default. How are you checking that a request is repeated? The callback you pass to listTables will only be called once, regardless of how many times the request is retried. You can verify requests are being retried by logging them out.\njavascript\nconst dynamodb = new AWS.DynamoDB({\n  accessKeyId: '...',\n  secretAccessKey: '...',\n  region: '...',\n  logger: console\n});\nI agree with you that data should be null when an error exists and we need to investigate why for this service, that's not the case.\nYou posted the API version of the DynamoDB client you're using, can you also post what version of the SDK you're using? One way to find out is by logging AWS.VERSION from within an application.\na. https://github.com/aws/aws-sdk-js/blob/master/lib/services/dynamodb.js#L19\n. @srkimir \nThe crc32 check is a special case where the SDK will also retry the error, even if the status code is 200. The status code is 200 because from the service's perspective, it was able to properly send a response. On the client side, the data integrity check (crc32 in this case) indicates that something went wrong, such as a networking issue when the response was already in flight, so it will retry the request.\nI hope that clarifies the behavior you're seeing!\n. @kevbook \nThanks for logging this issue. Currently, only operations that return an AWS.Request object expose a promise method. We want to extend this to other operations as well, such as s3.upload.\n. Closing in favor of #1076 \n. :shipit:\n. Ship it!\n. Updated the PR based on comments.\n. We talked about the signatureVersion changes in person, so to quickly reiterate, one thing that is difficult to do today is keep track of which options a user specified, vs which options were defaulted based on config or some heuristic. Due to the way the service client constructors are implemented (they are really closer to factory methods that return a different object based on the apiVersion specified), we can't simply override a service client's constructor. Unfortunately, by the time the object is instantiated for us, the configuration will already be altered.\nI added the originalConfig as a way to keep track of user-defined config. This information could also be valuable for other service clients, and it doesn't inject service-specific logic in the generic service constructor either. I took care to 'hide' it so that it is less likely to be altered by a user.\n. Updated the PR based on feedback around the getSignerClass method.\n. Replaced by #1023 \n. @winsome \nThanks for reporting this issue and providing a PR. I'll take a look asap.\n. The PR has been merged. Thank you!\n. @winsome \nThanks for the PR! Sorry this wasn't caught before. I think drain was used instead of finish initially to support node.js 0.8.x, but we never supported the httpUploadProgress event in version 0.8.x of node.js anyway. Merging in the code, it'll be available via NPM with the next release!\n. Ship it! \n. @jjmartin \nSorry about the lack of documentation. We're working on updating our documentation around credentials so we'll get this worked in too.\nWe actually do plan on having the JS SDK read the ~/.aws/config file as well. As an aside, while the CLI/boto don't document it yet, they also support reading the assume role profiles config from the credentials file as well.\n. @andreineculau \nHow were your credentials set up? Were the assume role configurations set in the config file, with a matching profile in the credentials file? \n. @EmmanuelTsouris \nWe've actually been doing some work on generating typescript definition files for our SDK. It's not 100% complete yet, but I can add a typings directory with what's done so far. Would love to get some feedback on it.\nI'll add that in this week.\n. @EmmanuelTsouris \nSorry for the delay, had to make a few changes to get the definitions in a workable state.\nThey are still very much a work in progress, but I created a branch you can checkout here:\nhttps://github.com/aws/aws-sdk-js/tree/support/typescript\nI'm working on automating the task of generating the typescript definition files (most of the definitions are generated based on the api models). Once that's done we'll be able to keep these up to date with each release.\nPlease share any feedback you have!\n. @Nysosis \nThanks for bringing that up. The original intent for adding typings was to enable better code completion support in tools that support that (like vscode, intellij and atom). They are definitely still a work in progress, but this sort of feedback is helpful for improving on it. I think for your particular error, the function type wasn't defined properly, so we should correct that.\n. @fished \nThanks for the feedback! Can you share which version of tsc you're using?\nRegarding the last error you received (TS2345), were you specifying the accessKeyId and secretAccessKey in the constructor for AWS.Config or in an AWS.config.update() statement? Depending on which case you're using, that error is intentional, but I'm not sure we clearly model that distinction.\n. @tomdavidson \nWe're planning on distributing the typings with the SDK. That's being actively worked on here:\nhttps://github.com/aws/aws-sdk-js/pull/1189\nThis should eliminate the need to pull in the typings from DefinitelyTyped then, and the types will match the version of the SDK that you're using.\nWe can create tests for the typings, but I'm not sure what coverage we'll shoot for before including them yet.\nI have to push some changes to fix some issues the typescript compiler is complaining about, but you're welcome to try out the branch in the PR. We're targeting TypeScript 2.x, I've added a sample tsconfig.json file as a comment in that PR.\n. @afaqurk \nAre you referring to the SDK being installed globally, or your module that consumes it?\nThat error you're receiving can occur whenever the SDK calculates a different signature than what the service does for a request. For example, some S3 regions only support sigv4. Part of what goes into the signature calculation with sigv4 is the region. If your SDK was configured to use the region us-east-1, but your bucket was in eu-central-1, that would cause the error to occur because the SDK and the service would be calculating the signature using different regions.\nCan you share how you're calling the code from the CLI?\n. Closing old issue due to inactivity.. @jrbenito \nFirst question:\nS3 sends back a 403 response when a user does not have access to the resource they are attempting to access. Based on what was said in #291, it sounds like this occurs on HEAD operations. The null part of the error is supposed to be the message, but in cases where the response doesn't have a body, it isn't filled out. \nSecond question:\nThe 403 error indicates you don't have access to the object you're trying to access. Have you tried using the AWS SDK directly to make a headObject call with your credentials? If it works with the SDK, then there's likely an issue with how s3-sync-aws is configured.  Can you explain what you mean by Even do I empty bucket and try a new deploy the error is thrown?  Are you deleting an empty bucket and immediately creating a new one with the same name? \n. @jrbenito \nAre you using your account credentials when testing or using something else like EC2MetadataCredentials/TemporaryCredentials/etc?\nWhen s3-sync-aws starts failing on headObject, is this before or after you've deployed a new object? Is it always headObject that is failing?\n. @jimcroft \nCan you share the error message and headers (minus sensitive info) that are returned by the response when you see the 301 error? Do you see this issue using the node.js or browser sdk, and which version?\nAlso, can you share how you're configuring your S3 client through the JavaScript SDK? Are you configuring it to use the eu-west-1 region?\n. @jimcroft \nIf you replace\njavascript\nvar s3 = new AWS.S3();\nwith\njavascript\nvar s3 = new AWS.S3({region: 'eu-west-1'});\ndoes your code run properly?\n. @Tamal\nFor requests to the Go version of the SDK, please create an issue here:\nhttps://github.com/aws/aws-sdk-go\n. Thank you for all the requests for this feature. We appreciate the feedback and are watching this thread. I don't have any updates yet, but will report here as soon as I do. . Hey everyone, first off, thank you for your feedback.\nWe've discussed this internally and have decided not to include this feature directly as part of the AWS JavaScript SDK. We have forwarded all your feedback to the DynamoDB team, and ask that you also request support for this on their forums. If you create a feature request there I will happily forward that to them as well.. @BarryCarlyon \nAre you seeing these errors after a long period of time has passed? Can you share what version of the SDK/node.js you are using?\nIn the SO example, it looks like sqs.receiveMessage gets called on an interval of 500 ms. My initial thought is that enough of these requests get created that overtime a tipping point is reached where there are just too many sockets in use.\nIf the above is the case, there are a couple ways to solve this problem. One would be to configure the SDK to use a maximum number of sockets. \njavascript\nvar sqs = new AWS.SQS({\n  httpOptions: {\n    agent: new https.Agent({\n      maxSockets: 50 // number chosen arbitrarily\n    })\n  }\n});\nThe maxSockets approach won't solve any issues you might be having with high memory usage though, as requests could queue up while waiting for a free socket.\nThe other thing you could do is wait for sqs.receiveMessage to return before calling it again. You would essentially just make sqs.receiveMessage call itself inside of the callback. You can call it within a setTimeout so that it's called on the next tick, and so that the stack size doesn't grow too large. This method would have the biggest positive impact on your memory usage, and you can still have multiple sqs.receiveMessage requests going in parallel.\n. Closing old issues. If you're still encountering this issue, please open a new issue and reference this one.. @michaelwittig \nThanks for reporting the issue! I created a PR to check if we're in the browser instead. As long as you aren't using the browser version of the SDK, readFileSync should work with this change.\n. I merged in a change that should resolve this. It'll be available via npm in the next release, or you can install it from github.\nThanks for reporting this!\n. @lauterry \nCan you share how your json file is set up (minus credentials if those are set)? Just to verify, are you calling AWS.config.loadFromPath prior to instatiating elasticbeanstalk?\nI just tried a very simple example and was able to set the SDK region.\nconfig.json\njson\n{\n  \"region\": \"us-west-1\"\n}\nexample.js\n``` javascript\nvar AWS = require('aws-sdk');\nAWS.config.loadFromPath('./config.json');\nvar ebs = new AWS.ElasticBeanstalk();\nconsole.log(ebs.config.region); // Output: us-west-1\nconsole.log(ebs.config.endpoint); // Output: elasticbeanstalk.us-west-1.amazonaws.com\n```\n. @justinpeake \nWhat version of the SDK are you using, and is it the same as what is being used with Heroku?\n. I can't really explain why Cyberduck would be failing, but can you try using the config variables that Heroku uses with your local test? If that doesn't work, can you try downgrading your SDK locally to match what's in Heroku?\nThis seems more like an issue with config. Cyberduck doesn't use the node.js SDK to connect to S3 (it's written in Java), so differences between versions of the SDK shouldn't be relevant.\n. I should have asked before, what does the error returned to your callback say? When data is null, error should be populated.\n. Thanks. So by default when using node.js, the SDK will try to resolve credentials using the following order:\n1) If credentials are configured directly in code, use those otherwise...\n2) If they are provided as ENV variables, use those otherwise...\n3) If they exist in the shared credentials file, use those otherwise...\n4) If you're running in an EC2 instance, call the metadata service to get credentials.\nSo the heroku app is getting credentials somewhere in this chain. Can I ask where you are currently defining your creds? We can also take a look at what the configured creds look like at the time you made a request to see if there's a discrepancy there as well.\n. @justinpeake \nIf you name your ENV variables as:\nAWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY then the SDK will automatically use those, you won't need to store them and update your config yourself.\nThe clock skew error is interesting. It happens when the date/time of your environment is too far off from the date/time of the service you're calling. You can sometimes see that in browser or mobile environments where an internet connection is dropped and then requests resume once the connection is back, I wouldn't expect that from a node app. You can configure the SDK to attempt to correct the clock skew problems by setting the correctClockSkew config option to true:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#correctClockSkew-property\n. It makes sense to add it to the global scope since the clock skew issue should affect all services if it affects one.\nYou can update it by doing something like this:\njavascript\naws.config.update({correctClockSkew: true});\n. Glad to hear! I'm going to close the issue but feel free to comment or open a new one if you find any new issues.\n. @shesko \nIt looks like you are using version 1.3 of the SDK. It's currently at version 2.4.1. Can you try updating the SDK and seeing if the problem still exists?\n. @skorlir \nJust so I can understand the use-case better, is your primary reason for wanting to use JSPM/SystemJS for better ES6 support through babel?\nI'm really not familiar with using JSPM, so thanks for providing a sample. It's possible to use our SDK with webpack if you're targeting the browser, but it won't be what you want if you're targeting node.js.\n. @lytc \nLooks good to me, thanks for the PR!\n. @saakhan \nWhat is SG? Are you asking if it's possible to determine all successful/failed logins to a specific EC2 box?\n. Closing due to inactivity.\n. @ctindel \nMany of the services only update the api version when they make a backwards incompatible change. That's why many of the services in the SDKs will appear to be an old API version, but they may have been updated anytime since the original version.\nI believe you are looking for the describeAlarmsForMetric operation:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CloudWatch.html#describeAlarmsForMetric-property\nYou can find the namespace to use here:\nhttp://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/aws-namespaces.html\nAssuming AWS/EC2, the metrics and dimensions for metrics (including instance id) are here:\nhttp://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/ec2-metricscollected.html#ec2-metric-dimensions\nIn the future, if you have a question about a specific operation to use for a service, you might have better luck on the forums. We try to focus on SDK specific issues in this repo, and don't always know the services as well as the service teams do!\n. @Iuriy-Budnikov \nIt looks like you're using the s3.upload method. Can you share what your S3 client configuration looks like, as well as the params you're passing into the upload operation?\n. @Iuriy-Budnikov \nCan you also share any specific error messages you're receiving?\n. @Iuriy-Budnikov \nWhat browser/os combination are you testing with? Have you tried any other browsers? Using Chrome on mac os x, I'm unable to reproduce your issue using the same configuration you are.\nAre you exposing any headers in your bucket CORS configuration? Based on your original screenshots I don't see a Access-Control-Expose-Headers response header in any of your responses, and you should at least be exposing ETag.\nBucket CORS configuration - expose headers:\nxml\n<ExposeHeader>ETag</ExposeHeader>\nAlso, the callback you supply to your upload operation receives an error and data object, depending on of it succeeded or failed. Can you log the error if there is one?\nAlso, are your mp3's >= 5MB in size? Do you have a smaller mp3 or mov file you can test to see if you get the same result?\n. @Iuriy-Budnikov \nIt is possible to upload files >= 5mb. When using the s3.upload method, internally the SDK will do a multipart upload if the file size is larger than 5 MB. If the file is less than 5 MB, it will do a single s3.putObject. \nMulti-part uploads need to send the ETag of all the parts to work properly. Since putObject is working, but multi-part uploads aren't, my hunch is that ETag headers aren't exposed in your bucket's CORS configuration.\n. :shipit:\n. @donnut \nWhen uploading files larger than 5 MB, the upload method will do a multipart upload. This allows it to upload multiple parts of your file in parallel, and each part can be retried individually. partSize refers to the size of each individual part. It essentially determines how to slice up your file when uploading.\nHere's a link to S3 documentation that hopefully helps answer part of your question:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html\nBy default, the SDK's upload method will use a partSize of 5 MB, with the maximum number of parts set to 10,000.\nA 100 GB file, split up into 5 MB parts, would need to be sent in 20,480 parts, which is above the 10,000 limit imposed by S3 and the SDK.\nYou can simply take the file size in bytes, divide by the maximum number of parts (10,000), and that will give you the smallest partSize you can use. Your partSize can't be lower than 5 MB, and you might round up your partSize to the nearest MB.\nLet me know if that helps!\n. Do you mind adding a README for these new scripts that explains what they do, or how they should be run? I know we don't expect these particular scripts to be run by users but it would still be helpful for future contributors.\n. :shipit:\n. @ash2k \n1. I like the idea of being able to specify an explicit credentials parameter for TemporaryCredentials to make use of. \n2. It currently takes its master credentials from AWS.config.credentials, so point 2 isn't an issue. The point @mifi makes is relevant here, because if the credentials in AWS.config.credentials haven't been resolved yet and resolve asynchronously, you could wind up getting an error like he saw. That's something we should be able to account for.\n3. This is probably safe to do, but we'll want to tackle this one very carefully since it would be new behavior and history has shown us that users will use credential providers in unanticipated ways. The safest thing would still be to stick this behavior behind a flag but we might not have to do that if we can be confident this won't impact existing applications.\n@mifi \nI think we'll need to address the issue in your comment first, and then look into implementing points 1 and 3 from above.\n. @JamesNova \nCan you share the exact command you're running when you encountered your errors?\nBased on your debug log, it looks like you're calling npm with prune, but npm isn't finding a package.json file to parse. \n. Closing due to inactivity.. @Jonnymcc \nThanks for the PR! I'll just verify this works but don't see any obvious issues.\n. @Jonnymcc \nWould you mind just adding a few more lines to our existing test?\nhttps://github.com/aws/aws-sdk-js/blob/master/test/util.spec.coffee#L203\nSomething like:\n``` coffeescript\nkey3 = value4 # yet another comment\n[emptysection]\nkey1=value1\n```\nand\ncoffeescript\nexpect(map.section1.key3).to.equal('value4')\nAlso, please run the add-change script to create a change-log entry:\nhttps://github.com/aws/aws-sdk-js/blob/master/CONTRIBUTING.md\nnode ./scrips/changelog/add-change.js\nThis will create a JSON file that looks much like this that will help us provide more detail in our changelogs.\nIf you could make those changes, I can merge this in right away. Otherwise I can make those changes in a new PR if you don't have time to do this.\nAgain, thanks for submitting the PR!\n. @Jonnymcc \nThanks for making the updates! Assuming the tests pass, we'll merge this in.\n. Changes have been merged. Thank you for the contribution!\n. @kentor \nNice catch! We'll get this updated.\n. @eetee \nHow are you updating your credentials once you've called assumeRole?\njavascript\nAWS.config.update({\n  accessKeyId: /* */,\n  secretAccessKey: /* */,\n  sessionToken: /* */\n});\nThe credentials should actually be one level lower:\njavascript\nAWS.config.update({\n  credentials: {\n    accessKeyId: /* */,\n    secretAccessKey: /* */,\n    sessionToken: /* */\n  }\n});\n. @sandyleo26 \nYou're setting the config options a little too late. When you instantiate a service client (new aws.S3()), it is instantiated with whatever the config options were at that point in time. Instead, you'll either want to instantiate your s3 client after updating the global config, or move the config into your S3 client declaration:\njavascript\nvar s3 = new aws.S3({\n  sslEnabled: false,\n  httpOptions: {\n    agent: new proxy('...')\n  }\n});\nLet me know if that helps!\n. @ktersius \nThe documentation for this operation could be improved. You're actually supplying too many parameters to this operation. For example, in the docs, you'll see that ExpiredObjectDeleteMarker says it cannot be specified with Days or Date. What's less clear is that you can't specify both Days and Date at the same time. \nFor this operation, the docs on the S3 site or clearer:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketPUTlifecycle.html\nThere is a table that specifies what each parameter does, and when they are required.\n. If it helps, here's an example of valid paramters. The values I chose were arbitrary to satisfy any constraints S3 sent back when making initial requests:\njavascript\n{\n    Bucket: '<MyBucket>',\n    LifecycleConfiguration: {\n        Rules: [\n            {\n                Prefix: 'upload',\n                Status: 'Enabled',\n                AbortIncompleteMultipartUpload: {\n                    DaysAfterInitiation: 1\n                },\n                Expiration: {\n                    Days: 40,\n                },\n                ID: 'my.test',\n                NoncurrentVersionExpiration: {\n                    NoncurrentDays: 60\n                },\n                NoncurrentVersionTransitions: [\n                    {\n                        NoncurrentDays: 30,\n                        StorageClass: 'STANDARD_IA'\n                    }\n                ],\n                Transitions: [\n                    {\n                        Days: 31,\n                        StorageClass: 'STANDARD_IA'\n                    }\n                ]\n            }\n        ]\n    }\n}\n. :shipit:\n. Hey @mojojr \nLambda has deployed the change to some but not all regions yet. We'll update this issue once the change has been deployed to more regions.\n. @wryun \nThanks for submitting a PR!\nSometimes the coveralls step fails even though the tests pass. Rerunning that step now.\n. @wryun \nI'll take a look at this today or tomorrow. A quick cursory look, I don't see any issues with it. Since the docs call out eachItem as being an experimental feature, it's safe to make this change.\n. @wryun \nThanks for the contribution! Changes look good, merging now so it'll be included with the next release!\n. @merlinpatt \nCan you share the exact version of node.js you're using?\nI'm not using Meteor, but it looks like it just uses request internally. I wrote a quick test that mimics your code but using request directly instead:\n``` javascript\nvar request = require('request');\nvar AWS = require('aws-sdk');\nvar url = '';\nrequest({\n    url: url,\n    encoding: 'binary',\n    method: 'get'\n}, function(err, resp, body) {\n    var buffer = Buffer.from(body, 'binary');\n    var contentType = resp.headers['content-type'];\nvar s3 = new AWS.S3();\ns3.putObject({\n    Key: 'test.jpg',\n    Bucket: 'bucket',\n    Body: buffer,\n    ContentType: contentType,\n    ContentLength: buffer.length\n}, function(err, data) {\n    if (err) {\n        console.log(err);\n    } else {\n        console.log(data);\n    }\n});\n\n});\nconsole.log(AWS.VERSION); //2.3.8\n```\nI was able to verify (using node 6.3.0 and SDK 2.3.8) that the image was uploaded to S3 and was not corrupted.\nCan you verify what the type of buffer is in your example, and that the response.content passed into Buffer.from is a string? \nYou mentioned that you traced the issue to version 2.3.8 of the SDK. Is that the same version that your stack trace was generated with?\n. Can you try using version 6.x of node.js? \nBuffer.from wasn't a public API in node.js 4.4.7, and it didn't accept a string until version 5.10.0, so I'm not sure how it's being used in your example.\n. I can't speak to why it wasn't originally used, but flipping it on now for the entire SDK would likely cause some issues. We can transition to using strict mode as we add new code, but we probably won't devote time to add it to existing functions unless there was a strong case to do so.\nThis would be something we could use from the start with a major version bump of the SDK.\nClosing this issue, but feel free to comment if you have a use-case for enabling it.\n. @JakubMatejka \nYour custom UserAttributes should have the custom: prefix when setting their name field.\nOther than that, can you confirm if your user pool client has write permissions for the fields you're trying to update? In the console, you can list the apps that can connect to your user pool. Check the write permissions for one that you're seeing that error with.\n. Closing old issues. This appears to have been resolved by #1549. @clakech \nYou're correct, we don't currently support client-side encryption in the JS SDK.\nI'm not sure I understand your example. The sample you linked creates a signed url on the server, then the front-end uploads the file using that url. s3.upload is never called.\nIf your use case is to send the file from the client to your server without encryption, and then to S3, do you really need client-side encryption? Would server-side encryption work?\n. @vfxBoat \nFor multi-part uploads, which as Joyce pointed out is the default when uploading files > 5MB using s3.upload, 3 things happen.\n1) createMultipartUpload is called\n2) uploadPart is called for each 'chunk' of data. httpUploadProgress is triggered during these requests.\n3) completeMultipartUpload is called\nSo even after httpUploadProgress indicates that all the data has been uploaded, the SDK needs to make one more async request to complete the upload. Depending on how busy your server is, this call may not be made immediately. The callback provided to send will be triggered once all 3 of the above steps are completed.\nYou could use the information I provided to add additional info to your progress bar to indicate which of the 3 'stages' you're in, but the operation can't be considered 'complete' until your final callback is called.\n. @Dayjo \nThe way you're making use of promises in the SDK is a little off.\n\njavascript\nvar lifecycle = S3.putBucketLifecycleConfiguration(params, function(err, data) {\n            if (err) {\n                console.log(\"LIFECYCLE ERROR\");\n                console.log(this.httpResponse.body.toString());\n                console.log(err, err.stack); // an error occurred\n            }\n}).promise();\n\nWhen you're using .promise(), you don't want to specify a callback function. Doing so may actually cause your operation to be called twice under some conditions!\nInstead, you should revise your code to handle errors differently:\njavascript\nvar lifecycle = S3.putBucketLifecycleConfiguration(params).promise();\nlifecycle.then(function(data) {\n  /* Do the notification config */\n}).catch(function(err) {\n  console.log('LIFECYCLE ERROR');\n  console.log(err);\n});\nThat catch will capture any errors in your promise chain. You could also provide a second callback function to then() that would handle an error, though the former approach is generally better since it handles all errors.\nCan you try making these changes and let us know if you're still seeing this issue?\n. Today, the bulk of the SDK is in the part that's shared among all the services. Each service increases the size by a range of about ~8KB - 40KB, which can certainly add up.\nSplitting up the SDK is a cool idea, and we're open to pull requests (realizing this would require the creation of multiple new packages.) I think to really take advantage of some tools' ability to perform \"tree-shaking\" to further reduce the footprint, the core of the SDK would need to be refactored as well, which would likely introduce breaking changes.\nAssuming you're using node.js, can you share your use-case for wanting to reduce the SDK's footprint? There are definitely some very valid reasons (less space on IoT devices, faster Lambda cold boots when using a custom SDK), I just want a better feel for where the demand is at in the node space.\n. @jakubzitny \n1123 added SDK support for bundling the SDK with webpack and browserify. You can now also require individual services, and if you're using one of these tools to create a bundle, they will only include those services you've imported.\nWe didn't create new npm packages for all of the services, but we did enable importing individual services when bundling your code with a 3rd party tool.\nAre you using the AWS SDK from a renderer process or the main process?\nWe're working on improving our docs around this now, but are happy to answer questions here in the meantime.\n. :shipit:\n. Thanks for the reports! I'll have to get in contact with someone from S3, as they may not honor the Range in the querystring.\n. It looks like we're generating the URL correctly, but S3 seems to be ignoring it. Please post to the S3 AWS forums if you'd like to see this functionality supported!\n. @n2liquid \nWhen you see this issue, it happens every time you make the call, but doesn't happen at all with version 2.4.13 of the SDK?\nI tried your code with the same versions of Node and the SDK, but using Mac OSX and haven't been able to reproduce the issue. Have you seen this issue on any other machines as well? There weren't any changes made to S3 between 2.4.13 and 2.5.0 besides a new optional flag for dualstack, can you try version 2.4.14 as well?\nDoes an S3 object get created for you despite the callback never getting called?\n. @n2liquid \nSorry, I meant, if you look in your S3 bucket, was the object created?\nAre you still having issues on your home network? If so, can you successfully connect to any other services, or call any other operations on S3?\n. Closing old issues. If you're still encountering this issue, please open a new issue and reference this one.. @crawfobw \nWhat version of the SDK are you seeing this error with, and do you know which version you were using previously? Also, which SNS operation(s) fail?\nIn the callback function you give when calling an SNS operation, can you log this.response.httpResponse.body.toString()?\nThis will give you the raw XML that was returned. Can you just verify if there are any invalid characters (particularly where the parser is indicating)?\n. @sandyleo26 \nThe default signing for S3 was changed back to using v2 by default in 2.4.12, but the change to v4 by default did not pertain to signed urls. However, some regions only support v4, so if you're creating URLs in one of those regions, then you won't be able to exceed the 7 day expiration.\nCan you share what region your bucket is in?\n. Actually, ap-southeast-2 does support v2 as well:\nhttp://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region\nAre you sure it lasts for a full day? Can you check if it only works for 15 minutes?\nJust to verify, what version of the SDK are you using?\n. v2 and s3 can be considered the same for our purposes, I just meant that your region didn't have to use v4. Are you using node.js 4.3.x in Lambda?\nCan you share what the value for Expires is in your URL? If its set for a date in the future, this might be something to bring up with S3. Is this a new issue?\nI also created a url with a 90 day expiration, and I can test again tomorrow to see if it still works.\n. Thanks for the reports. This is looking like it's an issue on S3's side. Do you mind posting on the AWS forums to S3?\nIf my url stops working, I can also send that info to S3, but it looks like the SDK is creating the url properly, and S3 isn't honoring the expiration past a certain amount of time.\n. Sorry, it looks like that issue was caused due to the way our docs are generated. On the main Config page, there is actually a note that this doesn't work with DynamoDB. \nThat said, there's no reason we can't have this work for DynamoDB as well.\n. @alexandrepage \nThe version of the SDK that is included with Lambda lags behind our releases a bit. You can see the version in Lambda here:\nhttp://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html\nThey are currently on 2.4.9, and API Gateway usage plans were added in 2.5.1\nPlease feel free to post to the AWS forums for Lambda to update to at least 2.5.1. If you don't want to wait, you can also bundle your own version of the SDK up to Lambda:\nhttps://github.com/aws/aws-sdk-js/issues/765#issuecomment-152033324\n. @NotBobTheBuilder \nWe looked at changing this behavior, but S3 doesn't actually accept Expires in the query string:\nhttps://github.com/aws/aws-sdk-js/issues/1015#issuecomment-229488378\nDepending on how you're sending an object to S3 with the URL, you could specify the Expires header when you actually make a request, and it would be honored.\nPlease post to the AWS S3 forums if you'd like S3 to support Expires in the query string in the future.\n. Sure, we can add something in our docs.\n. @xiao \nYou are correct, the docs should say to over-write error.retryDelay instead.\nYou can over-write error.retryCount as well, but that will affect how many more times to retry this request. Once retryCount equals the maxRetries (default of 3 for anything besides DynamoDB), the request won't automatically retry anymore.\nWe'll have to update our docs.\n. @bknill \nIs your connection slow, or taking over 2 minutes to upload a part?\nCan you set the timeout for S3 to 0 like in this thread:\nhttps://github.com/aws/aws-sdk-js/issues/949\nIt sounds like your connection is timing out before it has a chance to finish the upload, and the test in the linked issue should verify that.\n. @rstahl \nI've reached out to the Lambda team to look into this issue. At first glance, it looks it could be a CORS issue, since the headers are returned, but not accessible through JavaScript.\nI'll keep this thread updated when I hear back from Lambda.\n. @rstahl \nThe issue has been reported to Lambda but has not yet been fixed. There aren't any code changes to be made to the SDK, since the issue is on the service side. I know this issue is on their radar but I don't have an ETA on when it will be fixed.. @rstahl \nThe issue is caused by the way the headers are exposed via the Access-Control-Expose-Headers header. The developer tools from a browser will show all the headers received/sent, but the browser will only expose information about the response (i.e. headers) to the JavaScript run-time based on the Access-Control-* headers if the request was a cross-origin request.\nWe've seen that IE is more easily tripped up than other browsers when it comes to reading these headers.\nI know the Lambda team is actively looking into this, I've requested an update on when they'll be able to make the fix.. @rstahl \nI've been told by the Lambda team that they have resolved this issue. If you continue to see this error, please let me know what region you're testing in so I can forward that to the Lambda team for troubleshooting.. Closing, let me know if you continue to see this issue.. @simonbuchan \nThanks for posting!\nYes, I should be able to merge service-loader into service-collector.\nI think we can do something similar to your example. I'll take a look into that today.\n. I added some changes to the PR.\nWith these changes, users can require individual services and webpack/browserify will only pull those in.\nIn the user's code, they would do something like this to require S3:\njavascript\nvar S3 = require('aws-sdk/browser/s3');\nAccess to the AWS namespace (useful for setting config across multiple service clients) is available like so:\njavascript\nvar AWS = require('aws-sdk/browser/global');\naws-sdk/browser/all can be required to get all services.\nOf course, calling var AWS = require('aws-sdk') will continue to work as it does today, returning the AWS namespace with the default service clients on it.\nToday I'll be working on some documentation and some more testing. The PR shows a lot of file changes but this is largely due to generating the individual services so that they can be required individually.\n. @AdityaManohar \nThe way I have things currently set up, the stubs in aws-sdk/browser/* will also build the browser version of AWS, which means it'll be using JS crypto and buffer libraries, as well as xhr instead of https. I did this so that requiring a single service gives you everything you need (including the core of the SDK).\nI chose the browser prefix in case we wanted to do the same for node.js. I don't believe dynamically choosing which AWS to load will work either, since tools will just look at what's required and pull that in, regardless of if it's actually used or not.\nI'll have to look into that issue some more to see if what's here would work for that use case as well.\n. @AdityaManohar \nThat's correct. Essentially, the browser entry point would be loaded when requiring the service.\nI might be able to create a node.js copy of the service stubs, and then in the package.json browser field map the node.js stubs to the browser ones. Then, if the user is using a tool that looks at the browser field (like webpack and browserify), the browser stubs would be used by default.\nI'm open to any ideas on how else to accomplish this (besides having separate packages), and I can at least go down the above route to see how feasible that is.\n. Closing in favor of #1123 \n. @Sunil6591 \nAre you using 3rd party tools, like Webpack or Browserify in your project, or are you pulling in the SDK using script tags?\nIf you're doing the latter, you can use the browser SDK builder to generate an SDK with just the services you want:\nhttps://sdk.amazonaws.com/builder/js/\nWe are currently working on better webpack support as part of #1117, and are also hoping to address importing individual services, but this is still a work in progress.\n. To build on top of what @simonbuchan  said, if you're using webpack or browserify, you can now require individual services:\njavascript\nvar S3 = require('aws-sdk/clients/s3');\nYou can also still access the AWS namespace that includes only services you've imported:\njavascript\nvar AWS = require('aws-sdk/global');\n. @emptyemail \nThanks for reporting this issue! I've submitted PR #1130 to fix this.\n. Closing old issue due to inactivity.. @bhishp \nCan you log what your s3Params looks like, minus the bucket name, when you are trying to upload an svg?\nI recreated your sample (though just using http instead of express), and was able to create a valid url and upload with it.\njavascript\n// s3Params\n{ Bucket: 'BUCKET',\n  Key: 'test.svg',\n  Expires: 60,\n  ContentType: 'image/svg+xml',\n  ACL: 'public-read' }\nURL:\nhttps://BUCKET.s3-us-west-2.amazonaws.com/test.svg?AWSAccessKeyId=ID&Content-Type=image%2Fsvg%2Bxml&Expires=1473874113&Signature=l57f1QQzt7Ilq%2BnYR%2F%2BsurQNRZg%3D&x-amz-acl=public-read\nIn my url, the + is encoded as %2B, not a space.\nCan you also share which browser you are testing in?\n. @bhishp \nAs for your side-note, I can confirm you don't need to specify the ContentType when using signatureVersion: 'v4' with S3. I'll have to investigate if it can also work with signatureVersion: 'v2' (default when you don't specify a signatureVersion for your S3 client, and the region supports both versions), or update the docs if it can't.\n. @timaschew \nI did some testing, and it looks like the SDK is sending the correct request and simply parsing the response from SQS.\nI think the behavior you're seeing is due to setting VisibilityTimeout to 0. See this for more information on the Visibility Timeout.\nIt seems that setting the value to 0 means that any component can read the message. SQS stores copies of your messages on multiple servers. When WaitTimeSeconds is set to 0, then a subset of those servers will be queried for your messages. When you define a positive integer value for WaitTimeSeconds, then all servers will be hit.\nIncreasing the VisibilityTimeout value got rid of the duplicate messages in my testing. I believe this is because the message is blocked for the duration of the timeout from being read again.\nI can't say whether the behavior you're seeing is expected or not. Please reach out on the AWS forums to the SQS team for more information.\nhttps://forums.aws.amazon.com/forum.jspa?forumID=12\n. @cdhowie \nAre you making calls across multiple services? Do you know if it's a specific operation that always throws the exception? How long is 'some time later'?\n. Closing old issues. Please open a new issue if you are still encountering this behavior.. @awerlang \nDo you mean it first broke in 2.5.6? And it's fixed by setting allowUnknownKeys to true in the latest version?\n. @awerlang \nNevermind, I'm able to reproduce the issue you're seeing. Marking as a bug.\n. Gotcha, thanks for the info!\n:shipit:\n. :shipit:\n. @gurpreetatwal \nThanks for the PR! Really appreciate that you provided links to meaningful changes between dependency versions as well. We'll take a look.\n. Sorry, it turns out merging these dependencies has not been straightforward because of some compatibility issues with older versions of node that we still support. I haven't had time lately to dive deeper into resolving them yet, but I should have some more time this coming week.. @jamorales-bsft \nYou'll need to attach the error handler to the stream returned by createReadStream(). The stream itself will emit an error event.\n. @somprabhsharma \nIs it possible that sometimes your keys array has a length of 0? That's what the validation error is suggesting. If you add a check to ensure that you only call batchGetItem when keys.length >== 0, do you still see these errors?\n. @skysteve \nCan you share what version of Node.js you're using, and a sample script that shows this error?\nUsing multiple versions of Node.js and the latest version of the SDK, I tried this simple script:\njavascript\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\ns3.listBuckets(console.log);\nI don't get any errors, and my credentials are also sourced from ~/.aws/credentials\n. @skysteve \nYour example with Node.js 6.6.0 runs perfectly on my system. What OS are you running on?\nAlso, do your credentials assume a role? Does my above example run on your machine?\n. Closing since we haven't been able to reproduce, and the example is working on your machine. Please let us know if you are still having issues.\n. @olalonde \nYou would need to attach an event listener onto the request that gets returned when calling a service operation by the SDK. The SDK handles converting the headers received into fields on the data object, but when using streams that step is skipped. In your scenario, you would either need to make a separate 'headObject' call, or you can parse the headers manually using the below as a starting point:\njavascript\nvar request = client.getObject(buildParams(key));\nrequest.on('httpHeaders', function(statusCode, headers, response) {\n  // metadata gets returned as headers that start with 'x-amz-meta-'\n  // write code to grab the metadata from those headers and do something with them.\n  });\n});\n// create the read stream for the body\nrequest.createReadStream().pipe(process.stdout);\n. @adueck \nThe next major version of the AWS SDK will set the Body field to a stream if the operation supports a streaming response, as is the case with s3.getObject. The next major version of the SDK is currently in developer preview and the S3 package needs to be generated, but feel free to try it out and leave feedback!. @paulxtiseo \nSorry for the delay.\nI was able to get your example to work by doing the following:\n``` javascript\n// Set the signature version to 'v4'\nvar s3 = new AWS.S3({\n    signatureVersion: 'v4'\n});\n/ ... code to generate url, create file stream, and calculate file size ... /\n// Make request using request-promise\nrequest({\n    uri: s3url,\n    method: 'PUT',\n    body: fileStream,\n    headers: {\n        'Content-Length': fileSize\n    }\n}).then(function(result) {\n    console.log('PASS');\n}).catch(function(err) {\n    console.error('FAIL');\n});\n```\nI had to specify the signatureVersion as v4. If this is still an issue, can you give the above a try?\n. @olalonde \ns3DisableBodySigning: true is only meant to work when using an https endpoint, which is why your non-file stream isn't working with minio.\nHowever, I would also expect putObject to give you the same warning. What type of streams are you passing into putObject and uploadPart?\nWhat the SDK does in the ManagedUploader to handle non-file streams is read a stream into a buffer until the specified size for a part has been reached, then it can calculate the SHA256 of the buffer and send that. Does it make sense to do something similar in your case (or use the upload method?)\n. Amazon S3 requires knowledge of the object length in order to upload. As a work-around you can use the s3.upload method which performs a multipart upload in chunks.. @fedot \nThanks for reporting this. This inconsistency is happening because the multi-part upload returns the location URI encoded, whereas when the uploader does a putObject (single part) upload, we generate the Location ourselves.\nWe'll have to see what format the other SDKs return Location as, but we should be able to at least make it consistent across all upload calls.\n. @zymr-keshav \nS3 CORS support only extends to already existing buckets that you configure. The service doesn't support CORS on operations such as createBucket. You can find more information here:\nhttps://github.com/aws/aws-sdk-js/issues/1112\nIf your environment (most browsers) supports CORS, then createBucket won't work at this time.\nPlease feel free to post on the AWS S3 forums to request this feature.\n. @apires \nThere are a couple things you can try to improve the experience for users with poor internet connections.\nSetting the queueSize of the ManagedUpload to 1 would reduce the number of simultaneous uploads occurring at once. The 2 minute timeout applies to each chunk, and only allowing one chunk at a time may allow them the bandwidth they need to complete.\nThe other thing you can do is remove the timeout by setting it to 0, or increase it, as mentioned in this post:\nhttps://github.com/aws/aws-sdk-js/issues/949#issuecomment-204178782\n. Thanks for the PR @jbergknoff \nThis change looks good, I'll run it through our integration tests to make sure nothing fails there.\nWould you mind running the add-change script to generate a changelog entry and add the JSON file to the PR?\nhttps://github.com/aws/aws-sdk-js/tree/master/scripts/changelog#add-change-cli\nThanks!\n. This change passes all tests and seems like the right thing to do when a supplied stream encounters an error. Merging!\nThanks for the PR!\n. @nytins \nCan you try lowering the PageSize and see if subsequent calls return results?\n. The docs are unclear, but the main Config has a note that retryDelayOptions doesn't currently apply to DynamoDB. DynamoDB uses its own retry exponential back-off strategy, with a default retry count of 10.\nTo answer your question here's what the retry delays should be based on the current retry count:\n[ 0, 50, 100, 200, 400, 800, 1600, 3200, 6400, 12800 ]\nWe're currently working on improving our docs, so we'll make a note to call out the retry strategies used by the SDK.\n. When a request is sent but fails with a retry-able error, it calculates aretryDelay, then increments the retryCount. So the first retry will actually be 0ms, using the code you linked above. That's just for DynamoDB. For other services, the first retry will use a base value of 100ms (and add an exponential factor and random jitter).\n. @jppellerin \nSorry for the delayed response. #1418 should address your concerns by adding jitter and the ability to change the base retry delay, or to specify a custom backoff function.. @essapalaxo \nCan you explain what you're trying to accomplish with the SDK? You've mentioned Riak, but I'm not familiar with that library or what it is trying to do.\n. @danbucholtz \nSorry, I'm not too familiar with rollup, can you share how you've got it set up with the SDK?\nThat line should only modify process if it hasn't been defined yet, so I'm not sure how it's overwriting the global variable.\nAll that said, it wouldn't be difficult to move away from using process to determine if we're in the browser or not.\n. @xeniaRen \nCan you share how you're calling SignUp? It would be helpful to see what the returned data object looks like, compared to the results in your devtools as well.\n. @xeniaRen \nIt looks like you're using the amazon-cognito-identity-js sdk, not the AWS JS SDK. That SDK is built on top of this one, but adds the functionality you're using.\nPlease open up an issue on that SDK's repo.\n. @kaihendry \nYou need to remove noParse: /aws-sdk/, from your webpack.config.js file. That line is telling webpack to pull in the file referenced in your code (aws-sdk/clients/s3 and aws-sdk/global), but to ignore anything that those files depend on.\nWe're actively working on improving our docs, and will include a section on using webpack as well. To require an individual service, you can require it by referencing the service using the service's class name, all lowercase:\nrequire('aws-sdk/clients/servicename')\nOne sidenote, in your app.js, you are importing S3, so you don't have to access it via AWS.S3. AWS.S3 will still work since the sdk adds the service constructor to the AWS namespace when it is required.\n. @justinffs \nThanks for reporting. As you've said, it looks like npm version 3.7.3 is affected, but not 3.8.6. I actually tested 3.7.3 by installing my local copy of the SDK, and that worked, so will have to look into why it's not working from the npm registry.\n. @justinffs \nIt looks like this may be an issue with npm itself. Other users have reported seeing this issue for other packages as well when using node 5.9.x with npm 3.x:\nhttps://github.com/npm/npm/issues/11976\nI also tried installing the sdk using multiple versions of node 5.x and npm (by using nvm). Installing with node v5.8.0 with npm v3.7.3 works, as does node v5.10.0 with npm v3.7.3. Node versions 5.9.0 and 5.9.1 are the only ones that I'm encountering issues with, but it seems to be with every 3.x version of npm. Downgrading to npm 2.x with node 5.9.x was able to install the sdk.\nWe didn't make any major changes between 2.6.8 and 2.6.9 of the SDK, and the package.json was unchanged aside from the version number. It's not clear why node 5.9.x with npm 3.x is failing. The comments in the above thread indicate that sometimes publishing a new version of a package will 'fix' the issue, but not always.\n. I just tested using v2.6.10 of the SDK, and that one is working with node v5.9.1 and npm 3.7.3.\n. @thihara \nIf you are using http for some services, and https for others, you can configure the same httpOptions as part of the configuration passed into a service constructor as you would on the global AWS.config object. \nTo make things simpler, you could create a single http Agent, and a single https Agent, then pass one or the other into your service constructors as needed.\n. Closing, let me know if the information above didn't help.\n. @b2mdevelopment \nIt looks like your citizen field is an empty string. If you add a check to delete the citizen field if it's an empty string, does the error go away?\n. Closing this issue due to inactivity. Feel free to reopen if the above suggestion didn't help!. @designreact \nThanks for reporting the issue.\nWhen using the DocumentClient, are you still getting multiple pages of data, or just one?\n. Can you share what your query looks like? As much as you can with fake data if needed. I can't tell which params you're passing into scan with that.\nEdit: Nevermind, based on your example it looks like it's an empty object.\n. @designreact You don't have to give me access to the product data, I'm working on reproducing your issue with a large table. I'll let you know if I can see a discrepancy between the two methods.\n. Last night I did a scan using the DocumentClient on a table with over 17000 items, and it returned all of them.\nIf you do a scan using the document client now, on a table that isn't being updated, do you see the same number of items as if you use the low-level scan method?\nThe only thing the DocumentClient scan method does differently is translate the AttributeValues you provide as native JavaScript objects into the format DynamoDB expects. If your scans are different on a table that is not being updated, then it might be helpful to see a couple examples of the items both scans return, and a couple examples of the items only the low-level scan returns.\n. Closing due to inactivity. Please open a new issue if you're still encountering this behavior.. @binoculars \nThanks for the PR! Merging.\n. Updated the global.d.ts and index.d.ts files.\nUsing Object.assign was causing the typings to only work when targeting ES6, now these will also work when targeting ES5.\nThe index.d.ts file also exports the typings as a global namespace, AWS, so that browser SDK users can get typing support as well.\n. Posting my sample tsconfig.json file here:\njavascript\n{\n    \"compilerOptions\": {\n        \"target\": \"es5\",\n        \"module\": \"commonjs\",\n        \"outDir\": \"dist\",\n        \"declarationDir\": \"dist\",\n        \"declaration\": true,\n        \"lib\": [\n            \"es5\",\n            \"es2015.promise\",\n            \"dom\"\n        ]\n    },\n    \"include\": [\n        \"src/**/*\"\n    ]\n}\nAssumes that .ts files are located under a src directory.\n. Closed by #1218 \n. @decoursin \nThanks for reporting the issue. There was a bug that was fixed in v2.6.5 of the SDK, where for some services, specifying an old apiVersion would break when a new one was introduced, but was backwards compatible with the existing versions.\nTo work-around this, you can specify an EC2 apiVersion of 2016-04-01, which was the latest version in version 2.6.3 of the SDK, the version currently deployed to Lambda:\nhttps://github.com/aws/aws-sdk-js/tree/v2.6.3/apis\nLambda is aware of the issue, but I don't know their current plans on updating the SDK.\n. Closing since this issue is already fixed in the SDK. \n. @davidecantini \nAre you also passing the cache-control header when you're using the presigned url? Since it is a signed header, it would need to be included when making your put request with the URL.\n. Based on your settings, it looks like you are not setting the cache-control header when making your put request. You would need to add it to the headers field within settings.\nYou're seeing different results in us-east-1 vs eu-central-1 because those regions support different signature versions:\nhttp://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region\nus-east-1 will use sigv2 by default, and eu-central-1 will use sigv4 by default, since that's the only signature version it supports. Sigv4 will allow you to specify CacheControl, but then forces you to also pass the cache-control header when making a request. Really, this means that the presigned url is enforcing what cache-control should be set as, not necessarily providing it to S3. \n. @tj \nAre you certain that the CLI is using the same credentials and region as the JS SDK? The CLI can reference the shared config file, which the JS SDK can't, but they should both look at the shared credentials file first.\n. @Noxs \nHave you tried building with version 2.6.10 of the SDK?\nI just ran the same command with version 2.6.10, then used the resulting SDK in an app that calls S3, and didn't encounter any errors.\nCommand:\nnode dist-tools/browser-builder.js s3 > aws-sdk.js\nTried with node 6.7 and 6.9.\nTested in Firefox, on an OS X machine.\n. @Noxs \nHave you tried building with version 2.6.10 of the SDK?\nI just ran the same command with version 2.6.10, then used the resulting SDK in an app that calls S3, and didn't encounter any errors.\nCommand:\nnode dist-tools/browser-builder.js s3 > aws-sdk.js\nTried with node 6.7 and 6.9.\nTested in Firefox, on an OS X machine.\n. @Noxs \nIt looks like your file is missing a single line:\nif (!Object.prototype.hasOwnProperty.call(AWS, 'S3')) { at line 20906. Interestingly, the contents of the if block, including the closing bracket, are present.\nWhen I added that line back in, your SDK worked in my app.\nCould you run npm cache clean, install the SDK in a new directory, and build the browser SDK again to see if that helps?\n. @Noxs \nIt looks like your file is missing a single line:\nif (!Object.prototype.hasOwnProperty.call(AWS, 'S3')) { at line 20906. Interestingly, the contents of the if block, including the closing bracket, are present.\nWhen I added that line back in, your SDK worked in my app.\nCould you run npm cache clean, install the SDK in a new directory, and build the browser SDK again to see if that helps?\n. @Noxs \nI'll get my hands on a windows 7 machine and see if I can reproduce the issue from there. Leave the issue open for now.\n. @Noxs \nI'll get my hands on a windows 7 machine and see if I can reproduce the issue from there. Leave the issue open for now.\n. @Noxs \nHave you been able to test this on Windows 10? Still working on getting a windows 7 machine, but I don't see the issue on Windows 10.\n. Cleaning up old issues. Please open a new issue if you're still encountering this behavior.. @ddura \nYou would expect to pass in an AttributeValue to get back a JavaScript object, right? Would you need to be able to go the other way as well?\n. @ffxsam \n@monken is correct, those fields on the data object are not guaranteed to be there. DeleteMarker and VersionID should be there when working with versioned objects, and RequestCharged should be there is the object is configured such that requesters get charged for downloads.\n. @johndanek \nus-standard is actually us-east-1. In the AWS Regions and Endpoints docs, there's the following note:\n\nAmazon S3 renamed the US Standard Region to the US East (N. Virginia) Region to be consistent with AWS regional naming conventions. There is no change to the endpoint and you do not need to make any changes to your application.\n\nIf you update your region to us-east-1 in the code sample you provided, you should be able to access your objects.\n. @tinnytian \nIt looks like you're trying to use this service in a browser, please correct me if I'm wrong. This service doesn't currently support CORS, so it won't work in an environment that enforces it (many browsers.) We are pushing services to support CORS, so we'll bring this up with them.\n. Cleaning up old issues. Unfortunately there isn't anything the SDK can do to enable CORS for the Budgets service as they have to make a change on the service side. Please open a request on the AWS forums requesting CORS support for the Budgets service:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=8. @samwilcoxon \nThanks for reporting this issue. The link should be working again, apologies for the inconvenience.\n. @nirmalgoswami \nIt looks like using when using IN in the FilterExpression, you need to provide ExpressionAttributeValues individually.\nFor example, I could update your params like this:\njavascript\nvar params = {\n        TableName: \"User\",\n        IndexName:\"a-b-index\",\n        KeyConditionExpression: \"Country = :country and #s = :status\",\n        FilterExpression: \"Id IN (:a, :b, :c)\",\n        ExpressionAttributeValues: {\n          \":country \": \"USA\",\n          \":status\": 1,\n          \":a\": \"1\",\n          \":b\": \"2\",\n          \":c\": \"3\"\n        },\n        ExpressionAttributeNames: {\"#s\": \"Status\"}\n      };\nI'm assuming that when you were attempting to pass the array of values before, it was trying to compare Id to the string [1, 2, 3] instead of treating :e as three separate values, but that's a better question for the DynamoDB team.\n. @zymr-keshav \nSince this is a feature request for service-level functionality, please make a post to the EC2 service forums. The SDK would need EC2 to provide or enhance an operation to return this data.\n. Merging!\n. @dorongutman \nWe do try to follow SemVer today, except service updates result in patch updates instead of minor ones as they typically don't require any code changes.\nOur goal is to not make any breaking changes unless we do a major version bump.\n. @dorongutman \nWe do try to follow SemVer today, except service updates result in patch updates instead of minor ones as they typically don't require any code changes.\nOur goal is to not make any breaking changes unless we do a major version bump.\n. When features are added to the SDK that require code changes. For example, 2.2.0 was created when the DocumentClient was added to the SDK. 2.3.0 added support for promises, 2.6.0 added webpack support. Sometimes we do have to make code changes with a service update, and in general we've also treated these as patch updates. However you're right, according to SemVer, these should be treated as minor updates as well.\n. When features are added to the SDK that require code changes. For example, 2.2.0 was created when the DocumentClient was added to the SDK. 2.3.0 added support for promises, 2.6.0 added webpack support. Sometimes we do have to make code changes with a service update, and in general we've also treated these as patch updates. However you're right, according to SemVer, these should be treated as minor updates as well.\n. I don't think I'd ever recommend updating a dependency without at least running some tests, since sometimes mistakes can happen even with the best intentions. Patch updates should not affect existing code paths though, unless it's to fix a bug.\n. I don't think I'd ever recommend updating a dependency without at least running some tests, since sometimes mistakes can happen even with the best intentions. Patch updates should not affect existing code paths though, unless it's to fix a bug.\n. @hellopeera \nhttps://{service}.{region}.amazonaws.com is a general rule. However, some services use a global endpoint. The region_config.json controls the default endpoints to use based on a service and region, but it is possible for users to specify a different endpoint.\n. @hellopeera \nhttps://{service}.{region}.amazonaws.com is a general rule. However, some services use a global endpoint. The region_config.json controls the default endpoints to use based on a service and region, but it is possible for users to specify a different endpoint.\n. @wjordan \nThanks for the PR! One thing we need to be careful of is putting waiter-specific parameters into the same object that defines the parameters that get passed to the underlying operations that get called. Right now there aren't any operations that accept delay or maxAttempts as top-level params, but we've run into a similar issue with s3.getSignedUrl where we allowed setting Expires, then an operation added Expires as an operation parameter later on.\nThe safest thing to do would be either:\n- Add a new argument to the function call. Would have to be careful that it's done in a way that doesn't break existing users.\n- Create a simpler way to modify the delay/maxAttempts value on a request object directly. This would be the least intrusive, but would require an extra call:\n  var req = route53.waitFor('resourceRecordSetsChanged', {Id: 'ID'}, callback).setWaitersConfig({});\nWhat are your thoughts on either of these methods? Personally, the 2nd option seems 'safer' to me, but I could see the 1st option being more intuitive.\n. @wjordan \nThanks for the PR! One thing we need to be careful of is putting waiter-specific parameters into the same object that defines the parameters that get passed to the underlying operations that get called. Right now there aren't any operations that accept delay or maxAttempts as top-level params, but we've run into a similar issue with s3.getSignedUrl where we allowed setting Expires, then an operation added Expires as an operation parameter later on.\nThe safest thing to do would be either:\n- Add a new argument to the function call. Would have to be careful that it's done in a way that doesn't break existing users.\n- Create a simpler way to modify the delay/maxAttempts value on a request object directly. This would be the least intrusive, but would require an extra call:\n  var req = route53.waitFor('resourceRecordSetsChanged', {Id: 'ID'}, callback).setWaitersConfig({});\nWhat are your thoughts on either of these methods? Personally, the 2nd option seems 'safer' to me, but I could see the 1st option being more intuitive.\n. @zeevl \nFound a similar issue on the AWS SDK for Ruby:\nhttps://github.com/aws/aws-sdk-ruby/issues/808\nUnfortunately, I don't think we can make the SDK start throwing errors in this case, as it could break existing applications that are generating URLs with a longer Expires than the expireTime allows, but also using the url before credentials expires.\nIt is possible to call refresh on the credentials (and check if expireTime is near to further reduce the number of calls made), then generate the url once credentials have been refreshed.\nThe ruby issue mentions it may be possible to update the policy to allow for longer expiration times, but I haven't looked into it myself to see if that's the case.\n. @zeevl \nFound a similar issue on the AWS SDK for Ruby:\nhttps://github.com/aws/aws-sdk-ruby/issues/808\nUnfortunately, I don't think we can make the SDK start throwing errors in this case, as it could break existing applications that are generating URLs with a longer Expires than the expireTime allows, but also using the url before credentials expires.\nIt is possible to call refresh on the credentials (and check if expireTime is near to further reduce the number of calls made), then generate the url once credentials have been refreshed.\nThe ruby issue mentions it may be possible to update the policy to allow for longer expiration times, but I haven't looked into it myself to see if that's the case.\n. @ducduongtmb \nAre you using a tool like webpack or browserify to bundle your code, or are you including the SDK by adding a script tag to your page?\n. @ducduongtmb \nAre you using a tool like webpack or browserify to bundle your code, or are you including the SDK by adding a script tag to your page?\n. You should be able to use the following when using a tool like webpack or browserify (or in node.js):\njavascript\nimport AWS = require('aws-sdk');\nOtherwise if you're including the SDK with a script tag, AWS should be available as a global variable. For type information, if you're using typescript 2.x, you can include this comment at the top of your JS file:\njavascript\n/// <reference types=\"aws-sdk\" />. You should be able to use the following when using a tool like webpack or browserify (or in node.js):\njavascript\nimport AWS = require('aws-sdk');\nOtherwise if you're including the SDK with a script tag, AWS should be available as a global variable. For type information, if you're using typescript 2.x, you can include this comment at the top of your JS file:\njavascript\n/// <reference types=\"aws-sdk\" />. @thinkloop \nThere isn't a good way of requiring just the AWS.CognitoIdentityCredentials. You can import aws-sdk/global to give you the AWS namespace with all the core classes (including credentials) loaded. Services aren't added to AWS when importing from aws-sdk/global unless you explicitly import them elsewhere.\nThis should result in a smaller build, since you won't be importing every single service.\n. @thinkloop \nThere isn't a good way of requiring just the AWS.CognitoIdentityCredentials. You can import aws-sdk/global to give you the AWS namespace with all the core classes (including credentials) loaded. Services aren't added to AWS when importing from aws-sdk/global unless you explicitly import them elsewhere.\nThis should result in a smaller build, since you won't be importing every single service.\n. @thinkloop \nAWS.ConfigService is different from AWS.Config. The former is a service you'd need to import explicitly, the latter controls the SDK configuration settings. In your example, it looks like you're trying to update the SDK's credentials, so you can do something like this:\n``` javascript\nimport AWS from 'aws-sdk/global';\nAWS.config.credentials = new AWS.CognitoIdentityCredentials({\n  / params /\n});\n``\n. @thinkloopAWS.ConfigServiceis different fromAWS.Config`. The former is a service you'd need to import explicitly, the latter controls the SDK configuration settings. In your example, it looks like you're trying to update the SDK's credentials, so you can do something like this:\n``` javascript\nimport AWS from 'aws-sdk/global';\nAWS.config.credentials = new AWS.CognitoIdentityCredentials({\n  / params /\n});\n``\n. @gsuresh92 \nSince this looks like an issue with how the policy is configured, can you post your question to the [DynaomDB forums](https://forums.aws.amazon.com/forum.jspa?forumID=131)? They will be better equipped to answer this sort of question.\n. @gsuresh92 \nSince this looks like an issue with how the policy is configured, can you post your question to the [DynaomDB forums](https://forums.aws.amazon.com/forum.jspa?forumID=131)? They will be better equipped to answer this sort of question.\n. @VictorioBerra \nDo you mind sharing a code sample that illustrates what you're trying to do? I'm not sure I follow how you're linking an id generated from the back-end to a client.\n. @VictorioBerra \nDo you mind sharing a code sample that illustrates what you're trying to do? I'm not sure I follow how you're linking an id generated from the back-end to a client.\n. @ChuntheQhai \nCan you confirm from the browser that theContent-Typeheader you are passing matches the one used when generating the url? It might be worth excluding the Content-Type from the url that is generated to see if the url then works. You can also try setting your S3 client to usesigv4. This may help narrow things down as thesigv4signer works differently than thesigv2` (default for most regions) signer.\njavascript\nvar s3 = new AWS.S3({signatureVersion: 'v4'});\n. @ChuntheQhai \nCan you confirm from the browser that the Content-Type header you are passing matches the one used when generating the url? It might be worth excluding the Content-Type from the url that is generated to see if the url then works. You can also try setting your S3 client to use sigv4. This may help narrow things down as the sigv4 signer works differently than the sigv2 (default for most regions) signer.\njavascript\nvar s3 = new AWS.S3({signatureVersion: 'v4'});\n. @juancabrera \nAccording to the CloudWatchEvents.putTargets docs:\n\nNote: When you add targets to a rule, when the associated rule triggers, new or updated targets might not be immediately invoked. Please allow a short period of time for changes to take effect. \n\nIt seems it might take some time for the change to take effect.\nSince this question is more about how the CloudWatchEvents service functions rather than the SDK, you can also post to the AWS CloudWatch forums if you need more information, like how long it can take for changes to take effect.. @juancabrera \nAccording to the CloudWatchEvents.putTargets docs:\n\nNote: When you add targets to a rule, when the associated rule triggers, new or updated targets might not be immediately invoked. Please allow a short period of time for changes to take effect. \n\nIt seems it might take some time for the change to take effect.\nSince this question is more about how the CloudWatchEvents service functions rather than the SDK, you can also post to the AWS CloudWatch forums if you need more information, like how long it can take for changes to take effect.. It looks like you're using the local version of dynamodb. If the shell was working previously, are you certain that the database is still running? Your error indicates that the shell isn't able to communicate with the local database. If you hard-refresh the webpage, does it still load?\n. It looks like you're using the local version of dynamodb. If the shell was working previously, are you certain that the database is still running? Your error indicates that the shell isn't able to communicate with the local database. If you hard-refresh the webpage, does it still load?\n. Can you try accessing the database using the AWS JS SDK directly? The local dynamodb and shell tools are maintained by another team, so I don't know what it is doing behind the scenes. Based on the error message, the only indication of what's wrong is being unable to connect to the local dynamodb instance. Have you tried a different browser? I tested using the default settings listed here and was able to connect.\nYou might also be able to get more help from the DynamoDB forums.\n. Can you try accessing the database using the AWS JS SDK directly? The local dynamodb and shell tools are maintained by another team, so I don't know what it is doing behind the scenes. Based on the error message, the only indication of what's wrong is being unable to connect to the local dynamodb instance. Have you tried a different browser? I tested using the default settings listed here and was able to connect.\nYou might also be able to get more help from the DynamoDB forums.\n. When using local dynamodb, your credentials don't have to be valid, they can be anything. They do need to be specified as a non-null value though otherwise the SDK will complain that it couldn't find credentials.\nYou'll also need to configure your endpoint to point to your local db host.\nConfigure DynamoDB client with custom endpoint:\nhttps://github.com/aws/aws-sdk-js/issues/880#issue-127564286\nCreate document client (This is what the shell appears to be using behind the scenes):\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html#constructor-property\n. Closing due to inactivity and since I haven't been able to reproduce. If you have this problem using the SDK directly, feel free to comment or open a new issue.. @rclark \nWould you be able to reproduce this test, logging the response details? Specifically, I want to know what the body of the response, and content-length were sent back as. This would help in determining how we should tackle this issue with S3.\n. @evseevnn \nIf @fshirzadi's suggestion didn't fix your issue, please feel free to make a post on the AWS SNS forums. The service team will be better equipped to answer questions related to how their API works.. @lukiano \nThanks for logging an issue. Just so I understand better, can I ask why you need it exposed? Are you getting any errors, trying to specify the type, etc? Very interested in any feedback around this feature.\nDepending on your use case, you may be able to use this as a temporary work-around:\njavascript\nimport {AWSError} from 'aws-sdk/lib/error';\n. Thank you @marcote, and apologies to anyone who was affected. The change has been pushed to master, if anyone encounters more problems please open another issue.\nWas anyone that was affected already using 3rd party typings for the SDK?\n. @marcote \nThanks for the PR! Merging.\n. @blakeembrey \nWhat were you using previously to define types for the SDK?\n1221 has just been merged in, I'll go through and see if there are any other methods returning implicit any.\nRegarding XMLHttpRequest, I'll look into that. I wonder if that can be resolved by including a jsconfig.json with this package that includes dom under compilerOptions.lib.\nFor your third point, on typescriptlang.org it mentions that references to paths should not be used in definition files, and using <reference types=\"...\" /> is preferred. If I removed that reference, wouldn't I also need to remove any imports from that reference as well, such as where I'm importing http/https/stream/etc? Is the best practice to not import those in a library, or am I misunderstanding something?\n. @blakeembry\nAh, thanks for the clarification, I didn't realize the TypeScript compiler would still be able to recognize those imports.\nIt seems like the best thing to do is remove the types reference then. I'll fix that and the issue related to XMLHttpRequest and include them with the next release.\nReally appreciate the quick feedback and the info you've provided!\n. I've created #1228 to fix the issues encountered here. If anyone has time to test it out, I'd love to hear if there are outstanding issues before these can start being used.\n. Since #1228 has been merged, I'm closing this issue. Feel free to comment or open up a new issue if you encounter problems with the typescript definitions!. @blakeembrey \nYou're right, I must have missed some files, updated with #1248.\nI'll add another PR shortly for adding on to ManagedUpload.\n@MarcusNoble \nCan you share a snippet of what your code that's failing looks like?\n. @MarcusNoble \nWere you previously pulling in 3rd party typings for the AWS SDK? Since we started including typings in v2.7.0, we have always hung the service client operation interfaces on the Types namespace.. The main benefit to using the typings shipped with the SDK are that the service definitions are updated with each release, and will reflect the version of the SDK you're using.\nI'll take a look to see how feasible it is to reference those interfaces directly on the service client, without removing them from Types since that might ease transitioning from 3rd party typings.. @maghis \nThanks for the PR! I'll merge to master now. I'm going to try and tackle the other TypeScript issues today, but if I run out of time I'll include this with the next npm publish and follow-up with the rest.\n. @maghis \nThanks for the PR!\nSorry for the delay, in reviewing. It looks good, merging!. @zymr-keshav \nCan you share your CORS configuration for your buckets?\nHere's some background on what's happening with S3:\n\nIf no region is defined in AWS.config or passed into the S3 constructor, then S3 will use us-east-1 by default. This is special for S3.\nIt is not necessary to build an endpoint yourself. If you instantiate the S3 service with the same region that your bucket is in, the SDK will figure out the endpoint on its own.\n\nIt also looks like you are providing a bucket endpoint, not the endpoint to the service. If that's intentional, you should also be setting the s3BucketEndpoint config option you pass into the S3 constructor to true.\nI did run a test in Firefox where my S3 client was configured to use us-east-1, and I called s3.getBucketLocation on a bucket in us-west-2 and was able to get a correct response. I did not specify an endpoint, and the SDK does attempt to automatically redirect when it can.. @blakeembrey \nThanks for the feedback!\nThe stream support is fine, I just wasn't ready to include them with this PR yet, but I can update the PR with them now.\nIf you don't have any concerns with mocking the interfaces, I'll go ahead and push this (and the streams) change. Really appreciate the support you've given!\n. @phubbard \nAre you currently pulling in typings for the node environment? Can you install @types/node as a dependency?\n1228 Will remove the need to install the node typings, but you'll still get errors in some cases if the typescript compiler can't resolve some global imports (like http, streams, other native node modules).\n. You can specify a dependency in your package.json file on the @types/node package. This will provide type definitions for the native node modules that the SDK can then reference. The reason this isn't included as a dependency of the SDK is to avoid type definition conflicts for projects that are already pulling in node typings, and to allow you to specify the version that matches your node environment.\n. What version of TypeScript are you using?\n. Hmm, do you have any other type definitions defined in your project? It seems like something defined in @types/node is defined in another project. If you can share any types you're using, I can create a test case to make sure #1228 fixes your issue as well.\n. @phubbard \nSorry for the delay. Can you actually see if the latest version of the SDK helps? I've merged #1228 which removes the explicit dependency on @types/node, though you still need to have the node native modules typings defined somewhere.\nIf the latest version doesn't work, can you share your tsconfig.json file, and any other files that describe which typings you may be pulling into your project?. Can you share what error you're getting now, or is it the same as before?. Ah I see, I missed a reference to node. I'll double-check all the definition files and update again.\nThanks for your help so far @phubbard!. @phubbard \nI went through and removed all references to /// <reference types=\"node\" />. I also updated the typings to better match patterns some 3rd party typings used.\nCan you attempt using the latest version of the SDK again? If it still doesn't work, can you share your typings.json file?\n. Excellent! Thanks for your patience with this, closing!. @jacktuck \nSorry for the delay, can you also share what version of node.js and the SDK you are using so I can more accurately reproduce your issue?. @AndrewBarba \nIs the script you ran in the test something you can share, or can you provide a simplified form of it so I can reproduce on my end?\nDid you try setting keepAlive while also keeping sslEnabled: true?\nOften times when we see reports of memory leaks, it's due to a huge number of requests being sent in a loop, faster than the connection pool can assign a socket to a request. The queue of requests can then fill up, taking up space in memory. The SDK by default will set maxSockets to 50 for https hosts if the global configuration is Infinity or undefined, which is the case by default in node v0.12 and higher. If you supply your own https.Agent, similar to what you're doing above, you can override maxSockets. Can you change the maxSockets value to something larger and see if the observed leak diminishes?\n. Hi @izogain \nSince this is a question about the behavior of a service, please ask your question on the AWS CloudWatch forums. They will be able to better answer your question, and take feedback on improvements.. @midknight41 \nYou're right. If you don't explicitly define client as DynamoDB.DocumentClient, the typescript compiler is able to infer the correct type, but it isn't exposed in a way for you to explicitly define it.\nYou're welcome to submit a PR, but I think this falls under the generated typings so it is something we'd want to tackle in a generic way (we likely have the same problem for S3.ManagedUpload as well. \n. @sgtoj \nAre you referring to the parameters for DynamoDB.DocumentClient? You should be able to access the interfaces on AWS.DynamoDB.Types.\n. @midknight41  and others.\nRegarding point #2, I am able to create a new AWS.DynamoDB.DocumentClient without any TypeScript compiler errors, I just can't explicitly tell a variable that it should be that type (you first point). Is that not the case for others in here?\n. @midknight41 \nI took a look at exposing DocumentClient (as well as other classes like ManagedUpload on S3) this weekend. I've updated the typings generator to export these classes and their interfaces (see #1240)\nI still need to export the typings for params/output for each operation these classes use. The only way to do this that I can see is to export them in the generated files as well, like the other interfaces that these classes expose: https://github.com/aws/aws-sdk-js/pull/1240/files#diff-78251b595701d9bf6bbe21acda93b196R146\nI don't like having to declare a namespace alongside a class (i.e. class DynamoDB and namespace DynamoDB) just to expose these interfaces since it leads to a lot of duplication, but I haven't found a better way. If you know a better way I'm all ears!. @midknight41 \nSure, I'd like to take a look at your local version of you don't mind sharing. DynamoDB is a unique case compared to the other classes that live on service client namespaces, since it shares so many interfaces. I think you may be right that this is the better route to go.. @midknight41 \nThanks for the file! I should have some time the rest of this week to take a look at this and try tackling the remaining objectives.. I've updated the PR. Thanks @midknight41 for your help, I got some inspiration from your diff :)\nThe latest changes in the PR exposes classes that live on service clients, and their interfaces.\nI made a change so that you can reference interfaces without the Types namespace, though Types is still there for backwards compatibility.\nFor example, you can do the following:\n```javascript\nimport DynamoDB = require('../clients/dynamodb');\n// define the 'DocumentClient' type\nconst client: DynamoDB.DocumentClient = new DynamoDB.DocumentClient();\n// define the DocumentClient.GetItemInput type directly off the DocumentClient\nconst params: DynamoDB.DocumentClient.GetItemInput = {\n    TableName: 'MyTable',\n    Key: {\n        'my-key': 'value'\n    }\n};\n// reference service interfaces directly off the service namespace\nconst getParams: DynamoDB.GetItemInput = {\n    TableName: 'MyTable',\n    Key: {\n        'my-key': {\n            S: 'value'\n        }\n    }\n};\n```\nI've started writing tests, but I need to write some more for S3.ManagedUpload and CloudFront.Signer still. Will also need to update this to support Polly.Presigner, but we're almost there!. @midknight41 \nSure, would appreciate the PR!\n. @Himanshu-Tamrakar \nIn the object you instantiate the S3 client with, can you remove endpoint and set region to the same region as the one your bucket is in? It looks like you are connecting, since you get a requestId back in the error. Can you also try using s3.upload instead of s3.putObject? You shouldn't need to specify the ContentLength either. The only time the SDK can't determine that is if you're uploading a non-file stream.. @stevenelson74708 \nI'm not too familiar with Postman, can you share how the presigned URL is being used?\nDoes the URL look the same between mac and ubuntu? Is it possible to share them?. @dibakardas67 \nWhat does your bucket CORS configuration look like? Do you have the following in it:\nxml\n        <ExposeHeader>ETag</ExposeHeader>. @dibakardas67 \nIf you omit the ContentType from your operation parameters, does the upload work?. In the callback you provide to s3.upload, can you log this.httpRespose.body.toString()? There should be more clues as to why it's failing.\nCan you also try using the latest (2.7.7) version of the SDK just to verify this isn't something that's been fixed?. @dibakardas67 \nCan you log this.httpResponse.body.toString() when you upload from an Android device?. @patrik-piskay \nAre you looking to manually refresh CognitoIdentityCredentials? You could call the refresh method directly instead of get. Behind the scenes, get checks if the credentials have expired (based on expiry date) prior to calling refresh, but calling refresh directly bypasses that check.. @patrik-piskay \nAh ok, for CognitoIdentityCredentials, the reason refresh isn't working for your case is due to the way this provider caches the IdentityId.\nYou can manually clear the cache by calling AWS.config.credentials.clearCachedId(). The provider is doing this internally when it gets a 'NotAuthorizedException' error, which is why the next get works.\n. @patrik-piskay \nI think I misunderstood before. Do you actually want this error to occur more frequently than it currently does?. So, the error you're seeing is coming from a service, it isn't one that the SDK itself generates. The token the service (either CognitoIdentity or STS, depending on the params you used) generates has its own expiration. If you manually overwrite the expireTime for the credentials, that will just cause the provider to pre-emptively refresh the credentials, so that error isn't seen.\nIf you take a look at the params passed into the CognitoIdentityCredentials constructor, you'll  notice that STS.assumeRoleWithWebIdentity is one of the operations the provider calls when you provide a roleArn. If you are doing this, you could provide DurationSeconds, which determines how long the credentials are valid for. I believe the lowest you can specify is 900 seconds, or 15 minutes.\nOtherwise, you'd need to find a way to mock the behavior you want. There isn't anything else we can do from the SDK side to cause the service to return this error.. @DaMouse404 \nCan you share what version of node.js and npm you're running where you see these failed tests? \nI just cloned the project and ran the tests (after setting up the necessary env variables) using node 6.9.1 and npm 3.10.8, with the latest (2.7.7) version of the AWS SDK, and the tests passed. I'm also running the tests on a mac.\nI just want to make sure that the issue is reproducible under some configuration so we can add a test for it as well.. Ah, actually, I was just running 05_upload.test.js separately from the other tests. When run by itself, it passes, but it fails when I run the other tests as well. At least it gives me a lead.. So, I was able to find a simple scenario that reproduces the issue, but I'm not sure what's causing it to surface in the tests you linked.\nIf I launch node and require aws-sdk/lib/core, then I get the same error. However, if I require aws-sdk first, I don't. I'm not sure what conditions are needed for this to happen yet, but it looks like your change will help.\nThe only thing I still want to do is add a test that makes sure I can require core, which is a little tricky since the SDK is already loaded in our tests, but I'll look into that.. I'm going to merge this. Manual testing has shown that importing the lib/core module can be problematic if done directly. I'm still not certain why this issue is happening when running your tests, but making this change is a net positive. I'd still like to get to the bottom of it, but don't want to delay merging this in.\nThanks for the PR!. @erikerikson \nThanks for the PR! I think it's a good idea to have the role_arn on the credentials object now that we're able to assume roles with this credential provider, so this seems like an easy merge.. Thanks @mdurrant \nMerging this change, thanks for contributing!. @north-river \nThe browser version of the SDK by default only includes services that are configured to support CORS.\nIf you're working in an environment where you know CORS won't be enforced, you can build a custom version of the browser SDK. Instructions on how to do this can be found here:\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/building-sdk-for-browsers.html\nWe are pushing service teams to enable CORS support but I don't have an ETA on when that work will be completed.. @dinvlad \nI wouldn't rely on the AWS.Signers.V4 object in production right now. We have talked about refactoring the signers for public use, but I don't have an idea of when that work would be done. If you are trying to sign requests for APIs the SDK doesn't currently support, then a 3rd-party library would be a better choice for now.\nCan I ask what your use case is? I'm just curious about where the need is outside of making requests to service APIs that the SDK supports, and more data could help us prioritize this work.. Added this change into #1240 instead.. Since AWS.Config is already exposed, I think we just need to export the interfaces it uses so you can do something like AWS.Config.HttpOptions.\nI'm all for that. If you want to submit a PR for it I'd be happy to take a look, otherwise I'll try to take a look next week.. @Beyondlost \nThanks for reporting this issue, it should be fixed now!. @Titozzz \nYour link takes me to the API Gateway welcome page, can you update it?\n. @1msoft \nWhat did you set as the queueSize and partSize?\nAre you seeing this consistently for files larger than 5 MB?\nCan you instantiate your S3 client to add some additional logging?\njavascript\nvar s3 = new AWS.S3({\n  logger: console\n});\nThis might give us some clues additional clues.. @1msoft \nI'm unable to reproduce your problem using the same version of the SDK and node.js on mac osx.\nIt looks like the issue is coming from a dependency of aws-sdk, xmlbuilder. Can you try clearing your node_modules directory and re-running npm install? Can you also verify the version of xmlbuilder that's being consumed by the SDK?. @OussamaRomdhane \nIs this within Lambda, or a different environment? Where are your credentials being sourced, and what operation are you calling when you see this error?. @OussamaRomdhane \nGreat! Yes, when using signatureVersion: 'v4', the region is used when signing requests, so the correct region that matches your bucket's location needs to be provided. The SDK has some tricks to try and figure out the correct region, but they aren't fool-proof.\nI'm closing this issue since you've solved the problem.. @yinso \nAre you using types with bluebird? It looks like this may be a common problem with a few workarounds when using bluebird as your global Promise library. \nSee this issue for more info on some work-arounds:\nhttps://github.com/DefinitelyTyped/DefinitelyTyped/issues/10801\nIf you don't care about having type checks for bluebird specific promise apis, you might be able to get around the issue by including es2015.promise in your tsconfig.json. @dzharii \nThanks for the request!\nIt looks like CloudFormation has a repo with some sample templates: https://github.com/awslabs/aws-cloudformation-templates\nThis might be a good place to ask to open source the official type definitions. You can also post to the AWS CloudFormation forums.\nFor our part, I can forward this request on to their team.. @georgesben \nSince this is a question about how to use a service's api, you'll be able to get better advice on the Amazon CloudFront forums.\nLooking at the docs for updateDistribution, I'm not sure you can disable a distribution using this method. The Enabled field says:\n\nSpecifies whether you want CloudFront to save access logs to an Amazon S3 bucket.\n\nThe service forums should be able to give you better advice on how to disable a distribution.. @zymr-keshav \nIf you're trying to create nested directories, you can create them at once.\nFor example:\njavascript\ns3.putObject({\n  Key: 'folderA/subfolder/'\n});\nshould create directories folderA->subfolder even if folderA doesn't exist.\nIf you're trying to create multiple folders that are siblings, then you'll have to use the method you're already using.\nThe SDK doesn't currently expose an interface for copying/uploading/deleting multiple files at once.. Added an additional check based on https://github.com/nodejs/node/issues/8053#issuecomment-238976013\n. :shipit:. @Mickael-van-der-Beek \nThanks for reporting this issue and sorry for the problems it caused you. I don't believe disallowing null was intentional. I'll mark this as a bug and we'll allow null again.. Good call-out, updated!. @sedouard \nYou can pass in configuration into the service clients when you instantiate them.\nTaking a look at the EC2 constructor as an example, you'll notice that the options parameter is nearly identical to AWS.config. Really the only difference is that AWS.config allows you to also add service-specific configuration, whereas that's unnecessary when passing configuration directly to a service client.\nBehind the scenes, the configuration passed to a service client is merged on top of a copy of AWS.config when a service client is instantiated. If you want to have different credentials for each AWS.EC2 client, you just need to provide them in the options you pass into the constructor.. @linshu \nIs this meant to be run in the browser? That's what it looks like to me but correct me if I'm wrong. If the SDK is running in the browser, it won't have access to AWS.SharedIniFileCredentials, since that is meant to source credentials from the file system, which the SDK won't have access to in the browser. You would need to use a different credential provider (such as CognitoIdentityCredentials) instead.\n. @linshu \nApologies, I added a fix for this with version 2.7.28 of the SDK but didn't report it here.\nThe reason you were seeing Missing credentials in config when using CognitoIdentityCredentials was because there wasn't a region configured. The only way to set the region would have been to set it on the AWS.config object before instantiating your credentials.\nNow, you can specify a region to use in the CognitoIdentityCredentials constructor:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityCredentials.html#constructor-property. @patrickhousley \nCan you share how you're running tslint? I created a new project using your exact samples/config, and am not seeing the same tslint error you reported. tslint did warn about no-unused-variable being deprecated, but that was the only message I received.\nI ran tslint on the single file, using 4.2.0 of tslint and 2.1.4 of the typescript compiler.. @patrik-piskay \nCan you share how you're consuming the amazon-cognito-identity-js package in your project?\nThe error you're seeing is because the node.js version of the AWS SDK is being used instead of the browser version.\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/util.js#L696\n AWS.util.isNode() will return true if the node.js version of the SDK is used, or false if the browser version is being used.\nI'm not familiar with how the amazon-cognito-identity-js package is meant to work when using the npm package, so this may be a better question to ask on that project's github.. @itrestian \nIt's useful because it helps services be certain that the body they received is what the client meant to send, though without authentication I suppose the header and body of a request could be subject to manipulation anyways.\nIt should be possible to remove the COMPUTE_SHA256 call from being called by doing something like:\njavascript\nvar cognito = new AWS.CognitoIdentityServiceProvider({/* config */});\nvar req = cognito.makeUnauthenticatedRequest('signUp', {/* params */});\nreq.removeListener('afterBuild', AWS.EventListeners.Core.COMPUTE_SHA256);\nreq.send(callback);. @Ballhorn \nSince this issue is pertaining to how a specific service API should work, you'll get a better response asking on the Amazon Cognito forums.\nIf you post there, feel free to reply with the link here and I can forward the post to that team as well.\nThere is another SDK built on top of this one that makes working with Cogntio User Pools easier:\nhttps://github.com/aws/amazon-cognito-identity-js\nTaking a look at their code:\nhttps://github.com/aws/amazon-cognito-identity-js/blob/4a1bea0501fd8e90d1e57563672b7d9993da6de5/src/CognitoUser.js#L244-L248\nIt looks like that SDK is removing the userAttributesPrefix, which is userAttributes., from each of the elements in requiredAttributes. It might be worth giving that a shot in your code as well, but I'm not too familiar with how this API works.. Awesome, thanks for sharing how you fixed it as well!. @zymr-keshav \nIt looks like you're using the AWS SDK for Java, whose github can be found here:\nhttps://github.com/aws/aws-sdk-java\nPlease re-open if the issue is actually with the AWS SDK for JavaScript. If so, please provide additional information, such as SDK version, node version, and some sample code that throws the error.. @hulbert \nNone of the SDKs support the DownloadCompleteDBLogFile operation at this time, but calling downloadDBLogFilePortion with pagination should work.\nCan you share your code snippet of how you're downloading the log file? The docs do mention that up to 1 MB of data will be downloaded, so if your logs hit 1 MB before reaching 10,000 lines, you'll get less than that.\n. @jcready \nThanks for reporting this issue. This appears to be a service-side issue, so I'm forwarding this issue and the forums link to the Lambda team.\nI'm closing this issue since we can't do anything to fix this from the SDK side, but leaving feedback on that forums thread would be helpful.. @MandeepKalkhanda \nCan you share a code sample that shows how you're uploading your files to S3?\nMy first thought is that you're hitting the 2 minute timeout for an individual upload:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#httpOptions-property\nAre you making putObject, or upload calls? upload will perform a multi-part upload for files larger than 5 MB, and each 'part' then has its own 2 minute timeout. This might be helpful unless you're already using upload.\nAlso, are you uploading files in batches, or all at once? Another potential issue could be that you've created thousands of upload requests, but only have X amount of connections open to S3 at a time. In that case, some of your requests could be waiting over 2 minutes before receiving a socket.. @MandeepKalkhanda \nThe gulp-s3-upload package is likely a better place to post this issue then. What I stated in the last 2 paragraphs could help with/explain the issue, but since you're using a tool that consumes this SDK, you would need to find a work-around specific to that tool.. @howardya \nSorry for the late response.\nThe error object will include an originalError field that could give more details. Did your error happen to contain an originalError that eventually ended with Missing region in config?\nFirst, to workaround the error you're seeing, you can do something like this before instantiating your CognitoIdentityCredentials:\njavascript\nvar AWS = require('aws-sdk/global');\nAWS.config.update({\n  region: <REGION>\n});\nBehind the scenes, CognitoIdentityCredentials calls the CognitoIdentity service. That service client needs to know what region to hit. By setting the AWS.config.region to the region your IdentityUserPoolId is in, CognitoIdentityCredentials should work.\nThis is not the best workflow, so what we should do is also allow a way to pass a user-provided region to the CognitoIdentityCredentials provider as well.. @perhallstroem \nThanks for the very detailed code example, it made reproducing the issue easy.\nIt looks like this line is causing the problem:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/request.js#L623\nIf I remove the pipe to the passthrough stream, the error you're seeing goes away. I still have to dig deeper to find a resolution, I'll update when I have a PR ready.. @perhallstroem \nCan you try removing the callback function you're passing into getObject? Our examples actually exclude this as well, you should be able to listen for the end and error events on the stream instead.\nI originally thought removing the callback didn't have an affect on the errors, but when I just tried testing some code changes for this issue, I noticed that removing the callback made the errors go away, even without my changes.\nIf that works for you, I think we can update the docs to point out that a callback shouldn't be provided when using streams.. @perhallstroem \nThanks for providing a new script, I'll take a look.. @perhallstroem \nHow long do you have to run your script before encountering these hung streams? I've taken your script and modified it slightly to try and reproduce your issue but haven't been able to yet.\nThe only thing I might be doing differently is after I download everything from listObjectsV2, I don't attempt to download again unless in the printStatus method, every object is marked as complete.. @dunkstewart \n@rusteyy \nAny idea how many items are downloaded before seeing this issue? Do you have any pauses where you wait for existing downloads to finish before downloading more, or is it continuous? Also, do you know how large your files are that you're downloading, on average?\nI've been having difficulty reproducing this locally, so any further information you can provide is helpful.. I wanted to give an update. I was able to reproduce this issue when downloading 1k objects concurrently, each about 5 MB in size.\nThe interesting piece is that this issue only happens for me when on WiFi, not when using ethernet. I noticed when running my tests that my WiFi connection would sometimes temporarily disconnect. It seems like when this happens, some of the sockets aren't able to report an error and simple stay assigned.\nFor anyone that's seeing this issue, are you on a stable, ethernet connection? I want to dig in further, but at this point in means digging into Node.js internals.. @perhallstroem \nI did some more testing and it doesn't appear that the issue is caused by anything the SDK is doing.\nOne work-around would be to pass an agent into your S3 client, then after some time has passed either destroy all the sockets or just the ones that have hung.\nWe can look into adding this logic into the SDK as well, but we'll need to come up with a plan on how to do this for the general use case.\nExample:\n```javascript\nvar https = require('https');\nvar agent = new https.Agent({\n    rejectUnauthorized: true,\n    maxSockets: 50 // default\n});\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3({\n    httpOptions: {\n        timeout: 2000,\n        agent: agent\n    }\n});\n// sometime when application should be finished\nagent.destroy(); // destroys all sockets in use\n``. Also, can you add a changelog entry using the./scrips/changelog/add-change.jsscript?. @dsouzamanish \nCan you share a code snippet of how you're calling S3 and DynamoDB? Are you calling S3 or DynamoDB in the callback of the other service?. @harshadyeola \nWhen does this issue occur? Does it happen just when requiring the SDK? If not, can you share a code snippet that reproduces the issue?. WhichPromise` library are you using? Can you also share how you're setting credentials?\nYou might also want to try calling the promise() method on the operation directly, instead of Promisifying it with another library:\njavascript\nfunction fetchScheduledPayload(key) {\n    logger.debug('inside fetchScheduledPayload, key:', key);\n    var s3 = new AWS.S3();\n    return s3.getObject({\n        Bucket: process.env.StoreBucket,\n        Key: key+'.json'\n    }).promise();\n}. @harshadyeola \nCan you share how you're setting up your credentials (leaving out any secrets of course).. Are you having these issues in Lambda, and did this just start happening recently?\nLambda is currently on version 2.7.10 of the SDK, and has been since December.\nCan you also provide any code snippets that reproduce this issue?. @kevbook \nGotcha. We've identified the issue and are working now to correct it. Thanks for reporting!. @Suiname \nI know they're looking into it, but I don't know what their final resolution will be. Anyone else that also needs this functionality should respond to the forum thread to let S3 know they want this feature. I'll update when I hear something new as well.. @thenovelnomad \nCan you share what version of node.js you are using, and any configuration that you use when setting up your SNS client?. @addisonj \nnew AWS.TemporaryCredentials() is actually throwing an error that's getting swallowed.\nIt currently requires for the global AWS.config.credentials to be set. \nThe issue happens when trying to access this.masterCredentials.masterCredentials before AWS.config.credentials is set (and thus null):\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/credentials/temporary_credentials.js#L100\nThe reason this only happens with EC2MetadataCredentials is because the synchronous credential providers are resolved when the SDK is imported, and the global config is updated if credentials are found. Using resolve, it's a manual process.\nThe only work-around right now is to modify the global config. Given the unexpected behavior you're seeing, I'm going to take a look at your related issue again and see how much time it would take to implement.\n. @addisonj \nI created PR #1322 to allow passing master credentials to the TemporaryCredentials provider.. Closing since the PR was released with version 2.7.28 of the SDK. Let us know if that works for you!. :shipit: assuming travis passes (looks like the failures were transient). Hey @PrismaticPolygon \nThere is an Angular 2 quickstart repo that makes use of the SDK that might be a good starting point:\nhttps://github.com/awslabs/aws-cognito-angular2-quickstart\nI haven't looked into it in depth. It looks like it doesn't currently make use of the types shipped with the SDK, but that might not be difficult to add in once you get this working.\nLet me know if this example works for you! If it's lacking in any way, we want to know that too!. Ah, understood. Thanks for that feedback!\nI would have to play around with Angular 2 and SystemJS before answering that question. This SDK does support webpack though, and it looks like Angular 2 does too!. The SDK just needs the json-loader in webpack config to work.\nIf you can use webpack, that might be something to try. Otherwise you might try asking on StackOverflow as well, there may be Angular 2 users that have already solved this problem.. @PrismaticPolygon \nI took a look at the Angular 2 quickstart guide and found a way to get the SDK to play nice with SystemJS. I'm not too familiar with SystemJS, it's not clear to me if it supports the browser field in package.json to resolve sub-modules like webpack and browserify do.\nTaking the quickstart example in the Angular 2 docs, here's what my updated system.config.js looks like:\n```javascript\n/*\n * System configuration for Angular samples\n * Adjust as necessary for your application needs.\n /\n(function (global) {\n  System.config({\n    paths: {\n      // paths serve as alias\n      'npm:': 'node_modules/'\n    },\n    // map tells the System loader where to look for things\n    map: {\n      // our app is within the app folder\n      app: 'app',\n  // angular bundles\n  '@angular/core': 'npm:@angular/core/bundles/core.umd.js',\n  '@angular/common': 'npm:@angular/common/bundles/common.umd.js',\n  '@angular/compiler': 'npm:@angular/compiler/bundles/compiler.umd.js',\n  '@angular/platform-browser': 'npm:@angular/platform-browser/bundles/platform-browser.umd.js',\n  '@angular/platform-browser-dynamic': 'npm:@angular/platform-browser-dynamic/bundles/platform-browser-dynamic.umd.js',\n  '@angular/http': 'npm:@angular/http/bundles/http.umd.js',\n  '@angular/router': 'npm:@angular/router/bundles/router.umd.js',\n  '@angular/forms': 'npm:@angular/forms/bundles/forms.umd.js',\n\n  // other libraries\n  'aws-sdk': 'npm:aws-sdk',\n  'rxjs':                      'npm:rxjs',\n  'angular-in-memory-web-api': 'npm:angular-in-memory-web-api/bundles/in-memory-web-api.umd.js'\n},\n// packages tells the System loader how to load when no filename and/or no extension\npackages: {\n  app: {\n    main: './main.js',\n    defaultExtension: 'js'\n  },\n  rxjs: {\n    defaultExtension: 'js'\n  },\n  'aws-sdk': {\n    main: 'dist/aws-sdk.min.js',\n    defaultExtension: 'js',\n    format: 'global'\n  }\n}\n\n});\n})(this);\n```\nThen in my code, I just import the SDK:\njavascript\nimport * as AWS from 'aws-sdk';\nNote that this pulls in the distributed browser version of the SDK that's normally treated as a global. The downside here is you can't pull in individual services unless you build a version of the SDK with the services you need yourself. That's something webpack/browserify can already handle.\nYou do still get to take advantage of the typescript definitions that come shipped with the SDK though using this method.. @RLovelett \nYour patch looks correct. This file is not auto-generated, so it's safe to edit directly.\nIf you want to provide a PR we'd be happy to accept it! I'd just ask that you also add a new test case in https://github.com/aws/aws-sdk-js/blob/master/ts/config.ts and run npm run tstest to make sure it compiles after your change.\n. @RLovelett\nThanks for the PR! Merging!. @Swizec \nAre you adding the json-loader to your webpack config?\njavascript\n{\n  test: /\\.json$/, \n  loaders: ['json-loader']\n}. @TheLarkInn \nThanks for the info! It won't hurt anything on our end if we rename one of the region_config files, glad to know that should take care of the issue from our end.. @TheLarkInn \nHmmm, I actually took my build that works with webpack 1 and rebuilt it using webpack 2, removing the json-loader config (and rerunning npm install without json-loader as well). The build runs and the webpage loads, and I'm able to access AWS services.\nI tested using webpack version 2.2.1\n@Swizec \nCan you share what your webpack config looks like? Also, does adding json-loader fix the issue in your case?\nI want to make sure that if we make a change, it actually fixes your issue.\n. @Swizec \nThanks, I found out the issue you're seeing happens due to this line in your config:\njavascript\nextensions: ['.json', '.js', '.less', '.css', '.handlebars'],\nI'm not sure how this array is read in ( @TheLarkInn might be able to explain), but it seems order matters.\nBy default '.js' appears before '.json' in this list:\nhttps://webpack.js.org/configuration/resolve/#resolve-extensions\nIf I take your example and switch '.json' and '.js', I don't get an error anymore. \nMy assumption is that when webpack sees region_config is required here, it resolves it using .json instead of .js, based on how you've configured your extensions array. I imagine it only resovles it using .json in this case because there is an actual region_config.json file imported as well.\nAre you able to update your config to see if that fixes things? In any case, I think we can still make the update in your PR since now we know what's causing the issue and can add tests :). @Swizec \nAwesome, glad to hear!\nI'd like to write some new tests specifically for webpack, kind of like what we have for the browser builder today. That may be more work than you'd want to put in. I'm happy to do that, or you're welcome to take a crack at it as well. We would need to cover building the SDK with webpack, and then making an unauthenticated request (could use CognitoIdentity for that) to verify the SDK was pulled in correctly.. @N1N \nCan you explain what sort of environment you're working in? Are you using a bundler like browserify or webpack to package your code for the browser?\nIf you're just trying to set up a simple HTML page with the SDK on it, you can include the SDK using a script tag:<script src=\"https://sdk.amazonaws.com/js/aws-sdk-2.9.0.min.js\"></script>\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/loading-the-jssdk.html. @N1N \nThe SDK doesn't have access to the file system from a browser environment, so you won't be able to use credentials stored in a file here.\nPlease take a look here for instructions on how to load credentials in the browser. The suggested way is to use Cognito Identity for managing credentials.\nWith regards to your use of require('aws-sdk'), you would only import the SDK this way if you were building your front-end project with a tool like browserify or webpack that can bundle up your dependencies into consumable bundles. Browsers don't understand what require means on their own.. @RLovelett \nThanks again for your PR!\nThe typings in the clients folder are all auto-generated with each release, so can't be modified directly.\nIn this case, all services should expose the endpoint parameter. Since each service client extends Service, you could add endpoint: Endpoint there:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/service.d.ts\nFrom a testing perspective, I think you can do you could do something like below in ts/s3.ts:\njavascript\nimport {Endpoint} from '../lib/endpoint';\n/* existing code */\nvar endpoint: Endpoint = s3.endpoint;\n. @RLovelett \nThanks! PR looks good to me, merging.. @vyas07 \nCan you share how you've configured your S3 client (specifically looking for region and if you've set an endpoint) and what parameters you passed into s3.createBucket?\nAlso, can you share your SDK version, and what environment it is running in?. @vyas07 \nIs your meteor app running in a web browser? S3 doesn't currently support CORS for operations like createBucket, so that could be part of the problem.\nCan you share how you've configured the S3 client? I.E., what options did you pass when instantiating: new AWS.S3(/*options*/);\nOn a related note, us-east-1 isn't a valid LocationConstraint: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#createBucket-property\nIf a LocationConstraint isn't specified, the default is 'US Standard' which is the same as us-east-1.. @vyas07 \nDo you have versioning turned on for your bucket?\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/manage-versioning-examples.html\nThe VersionId will only be returned if versioning is enabled.. This sounds like it may be an S3 issue then. I can confirm that the VersionId is returned on data in the callback when versioning is enabled in my tests.\nPlease feel free to reach out to S3 on the forums:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=24\nClosing since there isn't anything actionable from the SDK here.. @vedavyas772 \nYes, I am getting the VersionId returned when versioning is enabled for my bucket. I am not seeing the same issue as you are.. No, in my test all I passed was the Bucket, Key, and Body.. @nathanmalishev \nUnless you are working with versioned objects, this is expected behavior.\nPlease see an earlier discussion at #1197 . @klinquist \nIn the callback you provide to synthesizeSpeech, can you log this.httpResponse.body.toString()?. @klinquist \nIt might also be helpful to log this.httpResponse.headers. The body should be a Buffer if the operation succeeds.\nIs this happening intermittently? I just ran the operation with the exact same parameters in us-east-1 and node 6.9.1. Can you share your environment details and region?. @grahamjenson \nThe SDK can generate the PreSignedUrl for you.\nIf you want to see some examples, we have some tests written around this functionality:\nhttps://github.com/aws/aws-sdk-js/blob/master/test/services/rds.spec.coffee#L39\nAs long as the SourceRegion differs from the DestinationRegion and you don't pass in a PreSignedUrl, one should be generated for you.. @RLovelett \nI agree, the behavior you are seeing is not what I'd expect. You have found a valid work-around by resolving the credential provider chain. Another way would be to set credentials to null instead of undefined.\nThe long explanation is that when you pass configuration into a service client's constructor, behind the scenes the global config is copied, then service-bound params on the global config are merged on top of that, followed by the configuration passed to the constructor. This is done so that users don't have to specify every configuration option for every service client.\nHowever, during this merge, if a field in the configuration is set to undefined, a default value is chosen instead.\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/config.js#L429\nSetting the field to null gets around this.\nWe should document that null and undefined have different behavior when setting configuration. We'll also make sure not to have this behavior in future major versions.. @RLovelett \nYou're right, a union with null is the best option here.. Thanks @RLovelett \nMerging this in!. Aside from the TS comment, looks good! Thanks for writing so many tests!. @vyas07 \nHow are you downloading the object? Can you make a getObject call directly, and check that the body matches your text?\nIf you're downloading the object from the browser and opening it on your computer, the jpg extension with a text body is probably causing the issue.. @testtshoretel \nTo clarify, in node.js, AWS.config.httpOptions.timeout specifies the amount of time an assigned socket can be idle before timing out.\nIt is possible to abort a request. This should effectively allow you to set a response timeout.\njavascript\nvar req = s3.putObject({/*params*/});\nreq.on('send', function() {\n  // abort request after 5 seconds\n  setTimeout(function() {\n    req.abort();\n  }, 1000 * 5);\n});\nreq.send(function(err, data) {\n  console.log(err, data);\n});\nYou can also limit the number of retries for operations by specifying AWS.config.maxRetries\njavascript\nvar s3 = new AWS.S3({\n  maxRetries: 1\n});\nIf you want to limit the amount of delay between retries, there's an option for that as well!\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#retryDelayOptions-property\n. @vanerleo \nIs this is node.js or the browser?\nI ran the following in node.js v6.9 and am not receiving the same error:\n```javascript\nvar AWS = require('aws-sdk');\nAWS.config.update({\n    region: 'us-west-2'\n});\nvar cogdp = new AWS.CognitoIdentityServiceProvider();\ncogdp.listUsers({\n    UserPoolId: 'POOL_ID',\n    Limit: 1,\n    Filter: 'email=\"my@realaddress.com\"'\n}).promise().then(function(data) {\n    console.log(JSON.stringify(data, null, 2));\n}).catch(function(err) {\n    console.error(err);\n});\n``. TheFilterfield was added in version2.4.11of the SDK. That error you're getting indicates thatFilter` field isn't modeled, but it definitely exists:\nhttps://github.com/aws/aws-sdk-js/blob/master/apis/cognito-idp-2016-04-18.normal.json#L3596\nJust to be sure, can you also log AWS.VERSION right after you instantiate your service client? 2.21.0 should definitely have the field, and with my test it is there.. :shipit:. Awesome!\nCan you also update the request.d.ts definition to include the statusMessage:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/request.d.ts#L125\nand update the request documentation:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/request.js#L249-L254\n. :shipit:. @testtshoretel \nThe URL generated should take the region or endpoint into account. If a region was defined using the AWS_REGION environment variable, or your endpoint changed (via the S3_ENDPOINT environment variable in this case), you would expect the url to change as well.\nBy default, if no region is provided to S3, it will use us-east-1. us-east-1 is a special case in that it currently uses the s3.amazonaws.com endpoint.\nIt's always a good idea to set the region to be the same region as where your bucket is located, especially when using signatureVersion v4 as well.\nEdit: \nThere is a s3.getBucketLocation operation you can call to get a bucket's region if you have access to that method.. @testtshoretel \n1) This depends entirely on how you've configured your S3 client. getSignedUrl does not make any network calls to S3, so depending on how you've configured your region or endpoint, the region may change.\n2) If urls with s3.amazonaws.com were failing with connection alerts, that was likely due to the issues with S3 yesterday. . @testtshoretel \nOne more note, the SDK will try to detect what region a bucket is in if an operation fails, then cache the bucket location. So, if before you called getSignedUrl, you had previously called an operation using the same bucket, that could explain why you were seeing the region in your URL change despite no ENV or code changes. In this case, the wrong region would have been used due to the S3 event yesterday.\nClosing this issue since it appears to be related to yesterdays event, but feel free to reopen if you see the same problem today.. @optimisme \nCan you try the same code without using async/await? There were performance/memory-related issues when using async/await in versions of v8 less than 55. Node.js 7.6+ should have the v8 55 and be ok, but replacing async/await with promise chaining should give a clue.\nJust a note, the SDK can also return a promise for operations.\nInstead of the following using callbacks:\njavascript\ns3.getObject(params, (err, data) => {\n/* */\n});\nYou can return a promise instead:\njavascript\ns3.getObject(params).promise();\nCan you also share how you were measuring for the memory leak, and any details, such as the rate at which you saw the leak increase?\n. @osdavison \nAre you running the SDK in a browser or in node.js? The request looks like it's from the browser, in which case AWS.config.loadFromPath() shouldn't work, since it requires access to the filesystem.\nOne thing, it doesn't look like your credentials are being used once you've created them. You can pass them into your AppStream service client like this:\njavascript\nvar appstream = new AWS.AppStream({\n  credentials: new AWS.Credentials('ID', 'SECRET'),\n  region: 'us-east-1'\n});\nNote that I passed in a region instead of an endpoint. The SDK knows how to generate the endpoint based on the region and service name, so if you're using a standard endpoint, you shouldn't have to specify it.. One thing I'm not clear on, does the following scenario work:\n-  profile in credentials file contains a roleArn and source profile\n- source profile is read from config file. @eric-tucker \nThe SDK currently sources credentials using ~/.aws/credentials by default. It actually can use assumed roles if they are defined in the credentials file. This change also allows ~/.aws/config to be used when sourcing credentials, since most of the other SDKs support this now as well.. @dotchev \nWe can't do that until we make a major version bump, because this could lead to the SDK loading different credentials without any code changes in consuming code. We do plan on loading ~/.aws/config by default like the CLI does when we do a major bump, but I can't give a timeline on when that will be.. Agreed on the canary. Really, we need to do this regardless of supporting react native since we've made it a point to support webpack.. #1486 has just been merged into master. The react-native distributable will be available in the next version of the SDK.. @sridharrajagopal \nWhat version of phantomJS are you using? Travis currently runs anytime a PR is submitted to this SDK, and one of the steps is running the browser sdk tests using phantomjs 2.1.14.. @testtshoretel \nIf the data you want to upload to S3 is a stream, then you can just set the stream as the Body.\njavascript\ns3.upload({\n  Bucket: 'BUCKET',\n  Key: 'KEY',\n  Body: your_readable_stream\n}, function(err, data) {\n  /* do stuff */\n});\nBy default this will attempt to upload up to 4 parts, each 5MB, at a time until the upload completes.\nYou can also configure the ManagedUpload to change the part size and concurrency:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3/ManagedUpload.html#constructor-property\nOnce the source stream is ended, the managed uploader should be notified automatically.. Are you using the managed uploader in the SDK, or doing a multi-part upload directly?\nThe managed upload in the SDK only allows one body to be set, so you'd have to either pass in all of your buffers at once, or a stream. Is there a reason you can't pass the stream directly to s3.upload?. @testtshoretel \nIn your case, it sounds like you have 2 options.\n1) Parse the payload using a transform stream, then pass the transform stream to s3.upload.\n2) Do a multipart upload directly.\nSince s3.upload can only handle a single Body, it would have to be a stream for your use case.. The request your server receives should already be a stream. When you call on('data') on the req object, you're listening to the stream's data events. You can pipe this into a transform stream. \nI would suggest looking into some of the node.js API documentation to get more details on working with streams.\nincoming requests are readable streams\nimplementing transform streams. :shipit:. @damonmaria \nIt looks like S3 supports tagging a multi-part upload only after it has finished uploading. I think it should be safe for us to call putObjectTagging after a multi-part upload, though that will mean an extra request.. @damonmaria \nAgreed. I've marked this as a bug. Our plan is to make the extra request if upload does a multi-part upload. I'll comment here once we have a PR up.. @JordanSinko \nDid your version of the SDK change? If so, what were the versions involved?. @JordanSinko \nDid this just start happening? Right now this is sounding like an issue on API Gateway's side, but knowing when the last time you saw the old behavior would help. Also, what region are you running in?. @dcollinsf5 \nGlad this came with good timing! We tend to release multiple times per week, so, not long :). @treyrich \nDid you recently update the version of the SDK you're using? If so, do you know from what to what?\nFor the error message you're getting, could you share what the X-Amz-Target header on that request looks like? I'd expect something like AWSCognitoIdentityService.GetCredentialsForIdentity\nAlso, we have seen some issues where the region the CognitoIdentityCredentials uses was incorrect. By default, it will use whatever AWS.config.region is set to at the time it is instantiated. You can now specify the region when instantiating your credentials directly:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityCredentials.html#constructor-property. Do you happen to get a requestId back in the response, or is that missing as well?\nI meant to ask before, does this error always occur, or is it sporadic? . @treyrich \nAs a sanity check, is it possible to try version 2.7.20 of the SDK? 2.7.21 was the last time we made a change to the CognitoIdentityCredentials that wasn't purely additive.. @treyrich \nI tried reproducing your issue with the same type of credentials, but I haven't been able to get an error message yet.\nAt this point, it might be worth reaching out to the Cognito team directly on their forums:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=173\nThey may be able to help determine if the issue is related to your configuration in some way.\nIf you open an issue, feel free to link back here. It looks like you're calling the parameters correctly, but the error coming back is from the service, so they may be able to provide more info on what's going on.. The change to throw an error on bad tags rather than silently ignore them looks reasonable. Still approved!. @kaihendry \nThanks for the suggestion. Tagging this issue with documentation for tracking.\nFor the example you wrote, you might want to listen for the error events on either stream so you can decide how to handle them as well. \njavascript\nfunction downloadImage (key) {\n  return new Promise((resolve, reject) => {\n    const destPath = `/tmp/${path.basename(key)}`\n    const params = { Bucket: 'EXAMPLE', Key: key }\n    const s3Stream = s3.getObject(params).createReadStream();\n    const fileStream = fs.createWriteStream(destPath);\n    s3Stream.on('error', reject);\n    fileStream.on('error', reject);\n    fileStream.on('close', () => { resolve(destPath);});\n    s3Stream.pipe(fileStream);\n  });\n}. @elasticsteve \nYou would need to wrap the stream in a promise, like in this example.\nNote: like @cvrajeesh commented, you would want to take care to properly close streams in the error events before calling reject, but the example is a good starting point.. @elasticsteve\nYou can totally just pass the resolve function to the close event. My example was for the original poster, and they were returning the destPath so my example did as well.\nEdit: Realized I didn\u2019t explain what it is doing though, just why. If you were to await the returned promise (or chain then on it), you would get the destination path returned to you.. @pfirmin \nThe SDK doesn't currently work with react-native. We have a PR up to add support, but need to work on our testing/deployment strategy before merging it in:\nhttps://github.com/aws/aws-sdk-js/pull/1393\nTake a look at that PR, the README gives instructions on how to try out the SDK. Also would love any feedback you can give about the SDK in react-native.. Closing in favor of #740 . @SumanSingh4 \nWe don't currently officially support running the SDK in NativeScript. Taking a quick look at the NativeScript docs, it looks like NativeScript supports using either fetch or node.js' http for making network calls. The browser SDK uses XMLHttpRequest, while the node SDK uses http. The node sdk depends on a number of native node modules (fs, crypto, url, etc) that don't appear to be available in NativeScript.\nI can mark this as a feature request to support NativeScript, but for this sort of question I'd recommend using StackOverflow to see if anyone else has had luck using S3 from NativeScript.. @kevincollins7 \nCan you share what version of the SDK you're using, and how you're populating your env credentials? What operation are you calling when you see this error?\nAre these by chance temporary credentials?. You shouldn't be using SMTP credentials when calling SES through an SDK, you should be using the credentials associated with an IAM account: https://docs.aws.amazon.com/ses/latest/DeveloperGuide/using-credentials.html?icmpid=docs_ses_console\nJust to be sure, can you confirm you're using the correct region as well?\nIt might also be helpful if you can share what params you passed to sendEmail. Don't need specific values, just want to make sure I can make a request when filling out the same params.\n. @kevincollins7 \nExcellent, glad you were able to figure out the issue!. @lajpatshah \nPlease see https://forums.aws.amazon.com/thread.jspa?messageID=522709\nand\nhttps://forums.aws.amazon.com/thread.jspa?threadID=236854 for more information from SES.\nIt appears that only ASCII characters are allowed in the name part (the part before the @ sign) of an email address.\nFrom the rfc2047 spec:\n\n\nAn 'encoded-word' MUST NOT appear in any portion of an 'addr-spec'.\n\n\nI would recommend either commenting on an existing post, or creating a new one, on the SES forums to support non-ascii characters in an email address.. @charly3pins \nStepFunctions doesn't currently support CORS, a requirement for running in most browsers, so is not included in the browser version of the SDK by default. Please see the following page for a list of services that do or do not support CORS:\nhttps://github.com/aws/aws-sdk-js/blob/master/SERVICES.md\nIf you post a feature request on the AWS Step Functions forum for CORS support and post a link here, I'd be happy to relay the message internally.. @testtshoretel \nDid you update the version of your SDK recently? If so, from what to what?\nUnder the hood, we're making requests using node.js' https module. We haven't made any significant changes to how we call that code in quite a while.\nAre you still seeing slow uploads now? If you did update your SDK, can you revert to the old version and see if uploads are still slow?. @testtshoretel \nThis doesn't seem like an issue we can investigate on the SDK side then if the code hasn't changed, but now uploads are taking longer to complete. You might check the S3 forums to see if anyone else has experienced similar latency issues.\nI'm closing the issue since there isn't anything we can investigate right now. Since this just started happening with no code changes, there isn't enough for us to go off of. If you do discover there was a change in the SDK that caused this, feel free to re-open or open a new issue.\n. @fuchsiagroan \nCan you share what version of the SDK and if this is the browser SDK you're using?\nCan you share what the full error looks like? There should be additional fields on the error object, including possibly originalError that can give more clues as to what is happening.. There's no way to view the error in the debugger?\nIt's possible that the region you're using with your credentials isn't the same as the one your identity pool id is located in. If that's the case, you can pass the correct region in when constructing your CognitoIdentityCredentials.\nIf you can get the full error, we might be able to help more with troubleshooting your issue.. @dario-simonetti \nCan you confirm that the region and logGroupName are the same in the CLI and the JS SDK? Are you able to run the JS code locally to verify the results?\nWe don't have any customizations around CloudWatchLogs in the SDK, so the most likely cause of the missing data is a difference in how the region/credentials are configured between your environments. Testing locally with the same creds as the CLI should help narrow down if there might be an issue with the role Lambda is using.. @tyrsius \nThanks for the detailed analysis. Are your results indicating that it actually takes longer to load the SDK that's in Lambda rather than the one you package up? Have you tried moving the SDK import outside of the handler function (that may only affect runtime, not cold start).\nYou can build a smaller, node.js compatible bundle of the SDK using webpack or browserify.\nHere's some instructions for configuring webpack to generate a node.js bundle:\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/webpack.html#webpack-nodejs-bundles\nYou can also require individual services in your code so that only the services you use get pulled into your bundle:\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/webpack.html#webpack-importing-services\nExample: var DynamoDB = require('aws-sdk/clients/dynamodb');\nIt's true the bunded size of the SDK is still ~200KB when minified. Part of that is because the core of the SDK includes logic for handling requests/responses for any service. That means if a service uses JSON, we are still including the XML builder/parser. We are exploring ways to pull in just what's needed for an individual service, but I don't have a timeline for that yet.\nPlease let me know if the above suggestions have an appreciable affect on your timings.\n. Totally agree on getting the size of core down. lodash is actually a dependency of XML Builder, so if we can remove (or replace) the XML dependency for services that don't use XML, that should help considerably as well.. @iyz1891 \nThe response that comes back after calling createUserImportJob includes a PreSignedUrl you can use to upload your csv file.\nDocs:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html#createUserImportJob-property\n\nPreSignedUrl \u2014 (String)\nThe pre-signed URL to be used to upload the .csv file.\n\nYou could upload the CSV using something like node.js' http module, request, etc.\n. @pawansharma15 \nAre you creating your ec2 client after calling AWS.config.update with the region/credentials? Can you verify that your region/credentials match what you are using with the CLI when making the same call?\nWhat do you see if you don't pass in an array of ids to describeInstances? Do you see your instances in the results?. @babakness \nWhat version of the SDK are you using?\nI just tested your exact code with the latest version and it is not throwing an error:\njavascript\nvar AWS = require('aws-sdk');\nvar config = new AWS.Config({\n  accessKeyId: 'AKID', secretAccessKey: 'SECRET', region: 'us-west-2'\n});\nDo you have any dependencies or code that might be modifying the SDK?. @ramonmulia \nCan you share a code sample that I can reproduce the error with? Running the above sample, I'm unable to reproduce.. @joncursi \nCan you share a code sample showing how you're configuring the SDK?. @seoker\n2.44.0 introduced a new field on the data returned by an operation when using promises that caused this issue.\nA change was pushed to master last night to address the circular reference issue. Are you able to test with the master branch? We will publish a new version of the SDK to npm today as well.\nThanks for reporting the issue and apologies for the trouble this caused.. This has been fixed and released in version 2.45.0 of the SDK. Thanks for reporting the issue! If you encounter any issues after upgrading, please let us know.. @GetKraken \nWe have a fix in master for the circular references issue that we planned to release today.\nI have confirmed the other issue you're seeing is also a bug. Operations whose response has a payload member are affected. Accessing fields of the response works if following what's in our documentation, but we also 'unpack' the results for backwards-compatibility reasons. We will have to address this as well.. I've put up a PR for a fix, #1477. Apologies for the trouble caused by this bug.. @andrewcharnley \nWhich callbacks are you defining on the stream object? I'm not quite understanding your use case.. @jfromaniello \nYou are correct, thanks for the correction!. @meyerbro \nI don't see anything wrong with your example. In this case the problem is likely with the service, since the request is going through and the SDK is parsing the response.\nCan you post a message on the EC2 AWS forums about this issue? If you post the link back here, I can also forward the post to the EC2 team.. @sgnn7 \nWhen calling getSignedUrl, you can only use the sync method if your credentials are resolved synchronously.\nEC2 credentials are resolved asynchronously, so you should always use the callback method (or wrap it in a promise) when getting urls.\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#getSignedUrl-property\n\nNote: You must ensure that you have static or previously resolved credentials if you call this method synchronously (with no callback), otherwise it may not properly sign the request. If you cannot guarantee this (you are using an asynchronous credential provider, i.e., EC2 IAM roles), you should always call this method with an asynchronous callback.. @matsaleh13 \nv2.44.0 of the SDK did introduce the ability to source credentials from the ~/.aws/config file, where previously it was only able to be sourced from ~/.aws/credentials. However, that should only happen if AWS_SDK_LOAD_CONFIG is set to a truthy value.\n\nCan you verify in your node.js script if AWS_SDK_LOAD_CONFIG is set? You can do that by checking process.env.AWS_SDK_LOAD_CONFIG from within your script.\nThe error object you are receiving should also contain some special fields (I believe originalError is one). Can you JSON.stringify the error and share what that looks like as well? That may tell us what profile/file was attempted to be read.. @matsaleh13 \nCan you also verify what credentials the SDK is using?\nIf you log AWS.config.credentials.constructor, you should get the name of the credential provider used (e.g. SharedIniFileCredentials).\nIn the case that you are using SharedIniFileCredentials, can you also check the value of profile, filename, and disableAssumeRole?. Can you share what your credentials look like in the version of the SDK that does work?\naws configure should add credentials to the ~/.aws/credentials file, and the node.js SDK should attempt to pull credentials from there by default. \nAWS.config.credentialProvider does have 4 providers specified based on your util.inspect() output. JSON.stringify won't print functions, which is why they show up as null in your log.\nWhat I'm trying to get at, is what credentials the SDK is loading when using v2.43.0. It sounds like it should be reading a profile from the credentials file. I'm curious if you have a matching profile in ~/.aws/config. Without sharing your actual keys, could you share how your profile (based on v2.43.0 of the SDK) is set up, and if the same profile also exists in the ~/.aws/config file?. @matsaleh13 \nAh ok, are you also using the CLI through git bash? Have you also tried using bash on windows by chance?\nI have a Windows 10 machine I can test with, that at least gives me something to look into! I should have asked before, which version of node.js are you using as well?\nThanks for your patience and detailed notes so far!. @alvelig \nI put up PR #1517 that should address the issue you're seeing. I made sure to test with lambda.invoke with a payload as well.\n. Shipt it!. :shipit:. @brianomchugh \nI just merged in a change to master that should fix the issue you saw. I tested it by downloading an image using s3.getObject in iOS and Android. Interestingly, while Android did have a Body that was populated, it's size did not match the object's content-length. After this change, it should match.. @ericdcobb \nWhat version of the SDK are you using? We haven't been using crypto-browserify since version 2.178.0, and started bundling our crypto dependency in browsers since version 2.183.0.\nRef: #1880 . @ddwp99 \nCan you instantiate your polly client to include logging?\nEx:\njavascript\nnew AWS.Polly({\n  logger: console\n});\nIt will give us the total time the request took, which might not be super helpful, but also let us know if the request is being retried.\nI'm not very familiar with debugging node.js on a Raspberry Pi. Is it possible to profile the code to tell if it's the response that's taking a long time to come back, or if some code is taking a long time to execute?. @ddwp99 \nGlad you found a solution!\nYou should be able to use the same solution you have with a native https agent as well. Very interesting that the SSL handshake takes so much time on a Raspberry Pi!. @shiva151 \nCan you share what version of the SDK you're using, and what operations you're calling? Does this error occur frequently or is it sporadic?. @shiva151 \nSorry for the delayed response. Can you share the operation with parameters that you're calling? The parameters can be fake.\nCan you also log the error with something like JSON.stringify so we can also see the extra fields on the error object?. @loretoparisi \nAre you using the SDK in node.js or the browser?\nIf you are using node.js, then you can download the object from S3 using a stream, instead of buffering it into memory. See http://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/requests-using-stream-objects.html for details.\nIf you are using the browser to download large files, then you would want to generate a presigned url and embed that in your page for the user to download.. @dtyrrell \nAre you using version 2.55.0 of the SDK or higher? This sounds a lot like the bug that was fixed in that version: https://github.com/aws/aws-sdk-js/blob/master/CHANGELOG.md#2550\nRelated issue: https://github.com/aws/aws-sdk-js/issues/1490#issuecomment-301667511. @adenhertog \nI think you're looking for the LexModelBuildingService, not LexModelBuildingRuntime.\nThere's also LexRuntime, but that's different than the builder.\nLexModelBuildingService is in clients/all.js:\nhttps://github.com/aws/aws-sdk-js/blob/master/clients/all.js#L102. @adenhertog \nAh, the browser version of the SDK only contains the services that we have verified support CORS. You can build a version of the SDK that includes the LexModelBuildingService.\nThere's a few ways you can do this.\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/building-sdk-for-browsers.html\nIf you're using script tags to import the SDK, then the easiest way to build a new version of the SDK is to use our browser builder:\nhttps://sdk.amazonaws.com/builder/js/\nIf you're using a tool like webpack, you can follow instructions here:\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/webpack.html\nLet me know if this helps or if you need more info!\n. I gave it a shipit, but don't forget the changelog entry too!. :shipit:!!!!. @ChristophRob \nIn React Native, the SDK doesn't have access to the file system. You'll need to set up credentials similarly to how you would in a web browser. The recommended way is to use Cognito Identity. You should be able to follow the instructions on setting up credentials in a web browser for your project as well:\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-credentials-browser.html. @caub \nYou can also pass in any configuration options to a service client constructor.\nFor example:\njavascript\nvar s3 = new AWS.S3({\n  region: 'us-west-2',\n  signatureVersion: 'v4',\n  credentials: new AWS.Credentials({/**/})\n});\nThen you can call s3.copyObject. Your credentials and region should match your destination bucket.\njavascript\ns3.copyObject({\n  Bucket: 'DestinationBucket',\n  Key: 'filename',\n  CopySource: 'bucket/key' // this is the source bucket/key\n});. @caub \nDoes the same account have access to both buckets, or do you require 2 accounts?\nIf you have to use 2 accounts, I believe you need to configure the source bucket to allow your primary account access to the objects you want to copy. I'm not 100% sure how that works, so I'd suggest asking on the S3 forums if that's the case.\nAlternatively, you could stream the object from one bucket to another, though this makes multiple S3 calls then, while also buffering some data locally.\n```javascript\nvar s3Foo = new AWS.S3({\n    apiVersion: '2006-03-01',\n    params: {Bucket: 'foo'},\n    accessKeyId: '.....', \n    secretAccessKey: '.....'\n});\nvar s3Bar = new AWS.S3({\n    apiVersion: '2006-03-01',\n    params: {Bucket: 'bar'},\n    accessKeyId: '.....', \n    secretAccessKey: '........'\n});\nvar source = s3Bar.getObject({Key: 'test/hello5'}).createReadStream();\ns3Foo.upload({Key: 'test/hello33', Body: source}, function(err, data) {\n       console.log(err, data);\n});\n. @hminaeeBrunswicknews \nYou have a syntax error.javascript\nkms.decrypt({CiphertextBlob: new Buffer(encryptedEnvVar, 'base64')}, (err, data) = > {\n``\nNotice that there is a space between=and>when there shouldn't be, so the runtime thinks you're trying to to do an invalid assignment.. @davidmaxwaterman \nWe can add some documentation on how to use this. I revisited the issue you linked to, and there does appear to be a bug where resuming an aborted upload results in a smaller file being uploaded. In at least some cases, this occurs when all but 1 'part' has been uploaded. The next time the upload is triggered, since there is only 1 part left, it performs aputObjectrequest instead ofuploadPart`. I'm working on a fix for that now.. @kbariotis \nWe don't officially have a way of mocking/stubbing functions. You might be able to get some community responses from StackOverflow.\nHave you taken a look at some of the libraries on NPM that support mocking SDK functions? Some of them allow mocking individual operations, and allow you to specify what the response should look like.. @arian-kh \nIt looks like thingArn isn't modeled as a response field in our Iot model. I've reached out to the service team about this. Since the models are shared across all SDKs, we can't directly modify these models here.\nThe SDKs will ignore data that isn't modeled when reading a service's response, which is why you aren't seeing it now. It is possible to get at the raw response by doing something like the following:\njavascript\niot.listThings(function(err, data) {\n  var body = this.httpResponse.body;\n  // JSON parse body\n  var parsedBody = JSON.parse(body.toString());\n});\nHowever, the SDK does some additional work, such as converting date values to actual Date objects.\nI'll update here once I get some more info from the service team.. @davidmaxwaterman \nIs your bucket in eu-west-1?\nThe request you're seeing with /?max-keys=0 occurs when the region S3 is configured with differs from the region the bucket is in. The SDK has logic in place to determine the correct region the bucket is in. Assuming you're running the SDK in a browser, it will call listObjects with MaxKeys = 0 if it encountered one of these errors: https://github.com/aws/aws-sdk-js/blob/master/lib/services/s3.js#L19-L24\nI think you've already discovered the SDK has a default timeout of 2 minutes. Are you testing with a slow internet connection? How many concurrent requests do you typically have going at once?\n Can you also share which browser(s) you've tested with? It is also possible to completely remove the timeout by setting it to 0 (AWS.config.httpOptions.timeout = 0.. @navaneetharaopy \nDo you see the same issue happening locally? What OS are you running on EC2?\nCan you also share how much RAM usage increases by? Are you downloading multiple files at once?. @benishak \nCan you share what operation you're calling?. @davidmaxwaterman \nThe SHA256 calculation should not be part of the timeout. The 2 minute timeout specified by the SDK gets applied to the underlying XMLHttpRequest after that calculation.\nhttps://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest/timeout\nSDK Timeout Logic\nCan you share which browser you are testing with? I'd like to run a profile and check if there are any quirks with the XMLHttpRequest implementation there.. Thanks @davidmaxwaterman. Do you have an idea how long the requests take when you aren't performing a SHA256 calculation?\nWith regards to your comment about doing the calculation in a webworker, that is something we've talked about supporting and would like to in the future, but isn't supported today. . @exortech \nThere are 2 Cognito services.\n1) CognitoIdentity, at 2014-06-30\n  https://github.com/aws/aws-sdk-js/blob/master/clients/cognitoidentity.js\n  https://github.com/aws/aws-sdk-js/blob/master/apis/cognito-identity-2014-06-30.normal.json\n2) CognitoIdentityServiceProvider, at 2016-04-18\n  https://github.com/aws/aws-sdk-js/blob/master/clients/cognitoidentityserviceprovider.js\n  https://github.com/aws/aws-sdk-js/blob/master/apis/cognito-idp-2016-04-18.normal.json\nAre you sure you've got the right service/apiVersion combo? If you're getting an error, can you share a code snippet that produces it?. @lengk \nPlease see the API docs for the DynamoDB client:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB.html#deleteTable-property. @Christilut \nCan you share the code that generates the presigned url, and the request you make to it? Also, are you using the node.js or browser SDK?\nYou should not have to specify a Content-Type to use a presigned url. The filename should exactly match the key in the presigned url as well.. @wytlytningNZ \nIt looks like there are 2 issues here. First, Buffer is getting pulled in twice because the SDK requires v5, and browserify (used to generate the browser distributable) uses v4. Updating browserify to the latest fixes the Buffer duplication problem.\nHowever, the other issue is that Buffer v5 appears to drop support for IE 10. We can't roll Buffer back to v4, because then we also drop support for android 4.4 devices.\nIt looks like we'll need to provide a pollyfill for Uint8Array.prototype.fill. That may be something you can include for IE 10 clients today as well.. @SanderElias \nAre you using webpack v1 or v2?. @SanderElias \nI tested a sample app where I switched all my commonjs imports of the SDK to use ES2015 import syntax. Webpack indeed added \"use strict\"; to the modules I imported the SDK from. However, it did not add \"use strict\"; to any modules the SDK depends on. \nCan you share what your webpack config looks like? I do see this issue occur if I manually add \"use strict\"; to the top level function in the webpack-generated bundle, but haven't been able to see the same issue otherwise.\n. @matrunchyk \nThe JS SDK no longer uses crypto-browserify in browser/react-native environments, which is where this problem was happening. What issue are you seeing, and are you using the latest version of the SDK?. @sam-qburst \nWhen this error occurs, does it happen sporadically, or consistently? The SDK does perform an MD5 check of SQS messages, so this error indicates that the message received from the service doesn't match what they thought they sent you. Did both * look like MD5 hashes, and were different?. @raytiley \nS3 bucket names aren't allowed to contain forward slashes. Anything after the first forward slash in your bucket name would have actually been considered part of an object's key. \nFor example, if you have a bucket, foo, and want to upload an object test.txt to the directory bar/baz, the object's full key would be bar/baz/test.txt.\nOne of the reasons a bucket name can't contain forward slashes is it leads to ambiguity. Are you trying to access the bucket foo/bar, or the object 'bar' within 'foo'?\nUsing signatureVersion v2, rather than throw an error, S3 appears to treat anything before the first slash as the bucket name. Using signatureVersion v4, the separation between bucket and keys caused an error to be thrown. Going forward, directories should be included as part of an object's key. While you can continue to use signature version 2 to get around this behavior, new regions only support signature version 4, where slashes in bucket names won't work at all.. #1587 will allow using forward slashes in the Bucket field when using signatureVersion 4. This change was made for backwards compatibility, but will be removed when we do a major version bump. New code should instead include 'directories' as part of either the Prefix or Key, depending what the operation calls for.\nSee https://github.com/aws/aws-sdk-js/issues/1572#issuecomment-309869153 for more details.. @KafaltiyaMahesh \nS3 Buckets are not allowed to contain forward slashes in their name. In your example where the bucket is named a/b/c, the bucket would actually be a, and the key's prefix would be b/c/.\nThere has been some discussion about this here:\nhttps://github.com/aws/aws-sdk-js/issues/1046#issuecomment-234098432\nThere are 2 ways you can immediately get around this issue. The recommended way would be to update the bucket names in your code so they only include the bucket name, and not part of an object's key. The other would be to configure your S3 client to use signatureVersion v2.\nExample:\njavascript\nvar s3 = new AWS.S3({\n  signatureVersion: 'v2'\n});. @KafaltiyaMahesh \nIt turned out there was a bug with s3.upload where just the upload method would always default to using signature version 2, instead of 4 as intended. #1587 fixes this so that upload will always use the same signature version that the client it was called on would use.\n1587 also adds back in the ability to specify forward slashes in a bucket name as long as a Key is also provided (even if the Key is an empty string).\nHowever! This was only done to maintain backwards compatibility with behavior prior to updating S3 to use signature version 4 by default. Including the key prefix in a bucket name is not a documented feature, and any new code should avoid doing so. This behavior will be removed the next time we do a major version bump since it was never intended to allow keys to be specified as part of the bucket.. @KafaltiyaMahesh \nIt turned out there was a bug with s3.upload where just the upload method would always default to using signature version 2, instead of 4 as intended. #1587 fixes this so that upload will always use the same signature version that the client it was called on would use.\n1587 also adds back in the ability to specify forward slashes in a bucket name as long as a Key is also provided (even if the Key is an empty string).\nHowever! This was only done to maintain backwards compatibility with behavior prior to updating S3 to use signature version 4 by default. Including the key prefix in a bucket name is not a documented feature, and any new code should avoid doing so. This behavior will be removed the next time we do a major version bump since it was never intended to allow keys to be specified as part of the bucket.. @samuel-gaunt \nAre you running the exact code from your sample from within a lambda function, or is it contained inside of the lambda handler?\nThis simplified sample within a lambda function is working:\njavascript\nconst CognitoIdentityServiceProvider = require('aws-sdk/clients/cognitoidentityserviceprovider');\nexports.handler = (event, context, callback) => {\n    const cognito = new CognitoIdentityServiceProvider({\n        region: 'us-west-2'\n    });\n    cognito.signUp({\n        ClientId: '*****************',\n        Username: 'name',\n        Password: 'password',\n        UserAttributes: [\n            {\n                Name: 'email',\n                Value: '***@***.com'\n            }\n        ]\n    }).promise().then((data) => callback(null, data)).catch(callback);\n};. @samuel-gaunt \nAre you running the exact code from your sample from within a lambda function, or is it contained inside of the lambda handler?\nThis simplified sample within a lambda function is working:\njavascript\nconst CognitoIdentityServiceProvider = require('aws-sdk/clients/cognitoidentityserviceprovider');\nexports.handler = (event, context, callback) => {\n    const cognito = new CognitoIdentityServiceProvider({\n        region: 'us-west-2'\n    });\n    cognito.signUp({\n        ClientId: '*****************',\n        Username: 'name',\n        Password: 'password',\n        UserAttributes: [\n            {\n                Name: 'email',\n                Value: '***@***.com'\n            }\n        ]\n    }).promise().then((data) => callback(null, data)).catch(callback);\n};. @samuel-gaunt \nWhat's the timeout on your Lambda set to? You might need to increase it if the operation is taking too long to complete.\nDo any operations work in your Lambda function? Can you trigger the function directly from Lambda to test it, rather than from API Gateway?. @samuel-gaunt \nWhat's the timeout on your Lambda set to? You might need to increase it if the operation is taking too long to complete.\nDo any operations work in your Lambda function? Can you trigger the function directly from Lambda to test it, rather than from API Gateway?. @rgmembreno \nCan you construct your S3 client with logger: console in your configuration as well? That will log out what parameters each S3 method was called with. It'll let you validate which Bucket is being used when making the upload call in both of your above examples.. @rgmembreno \nWhere your S3 client is declared really should have no affect on your tests. Do any of your tests/beforeEach/afterEach functions modify the S3 client, or AWS.config?\nCan you log out your s3Client.config in both cases and see how they differ?\n. @genifycom \nThe API docs for API Gateway in the .NET SDK can be found here:\nhttp://docs.aws.amazon.com/sdkfornet/v3/apidocs/Index.html\nThis repo is for the JavaScript SDK. Can you post your question to the .NET SDK instead?\nhttps://github.com/aws/aws-sdk-net. @replicat0r \nWhat environment are you running the SDK in? . @replicat0r \nWhat environment are you running the SDK in? . Can you share how you're making the download call? Are you by chance mixing streams and callbacks?. Can you share how you're making the download call? Are you by chance mixing streams and callbacks?. To import a service client using es2015 syntax, you would need to do:\njavascript\nimport * as S3 from 'aws-sdk/clients/s3';\nThis is because we aren't using default exports in the SDK.. To import a service client using es2015 syntax, you would need to do:\njavascript\nimport * as S3 from 'aws-sdk/clients/s3';\nThis is because we aren't using default exports in the SDK.. @jeskew \nSo will reverting this change break android 4.4.2 users?. Ship it!. @PsyTae \nThe error you're getting is coming form OpenSSL, and indicates that your node.js application isn't able to create a connection over https to S3.\nIs your environment configured to use a proxy? Can you also try setting s3ForcePathStyle to true and see if that helps at all?. @Agrumas \nThe error is occurring because we're attempting to call toString on null when collecting the canonical headers to sign:\nhttps://github.com/aws/aws-sdk-js/blob/v2.77.0/lib/signers/v4.js#L142\nWe should be throwing a more helpful error in this case.\nThat said, what are you expecting to happen by specifying null metadata in the presigned url? If you leave it out of your options, then your object should not include the metadata. If you want userId to appear, but me empty, you could also set it to an empty string.. @mcprostar205 \nSorry for the delayed response. This was a difficult issue to try and reproduce!\nFirst, with regards to what you've tried:\n1. try-catch won't catch errors emitted on streams or async operations. Instead you should listen to the error event directly on the streams. You've done this with the S3 stream, but not the response stream.\n2. I'm not sure that you'd need to call unpipe yourself here. Since the error is being thrown before this line, I don't think it is necessary.\n3. I don't believe httpOutgoingMessage streams have a final event. I think you may have meant to use finish instead.\nAs called out in 1, one thing you should do in general is listen for error events emitted on the response stream.\nI was finally able to reproduce your issue when using node v6.10.3. Previously I was using the latest version (8.1.3) and wasn't able to trigger it.\nI think the issue happens because we have a PassThrough stream (used to validate the number of bytes streamed matches the Content-Length) that is configured with {end: false}. There is a node.js issue where streams configured this way don't emit the unpipe event:\nhttps://github.com/nodejs/node/issues/11837\nThis issue is already fixed in some versions of node 7 and 8, but hasn't been backported to 6 yet.\nIt also may explain why #1549 seems to fix this issue. We'll need to review the PR and see if that fixes the error for your use case. Alternatively, we can rewrite the PassThrough stream so that we let node.js handle stream cleanup.\nAre you able to test your code on the latest version of node to see if you can reproduce it? If the error goes away, then we've probably found part of the problem.\n. @mcprostar205 \nSorry for the delayed response. This was a difficult issue to try and reproduce!\nFirst, with regards to what you've tried:\n1. try-catch won't catch errors emitted on streams or async operations. Instead you should listen to the error event directly on the streams. You've done this with the S3 stream, but not the response stream.\n2. I'm not sure that you'd need to call unpipe yourself here. Since the error is being thrown before this line, I don't think it is necessary.\n3. I don't believe httpOutgoingMessage streams have a final event. I think you may have meant to use finish instead.\nAs called out in 1, one thing you should do in general is listen for error events emitted on the response stream.\nI was finally able to reproduce your issue when using node v6.10.3. Previously I was using the latest version (8.1.3) and wasn't able to trigger it.\nI think the issue happens because we have a PassThrough stream (used to validate the number of bytes streamed matches the Content-Length) that is configured with {end: false}. There is a node.js issue where streams configured this way don't emit the unpipe event:\nhttps://github.com/nodejs/node/issues/11837\nThis issue is already fixed in some versions of node 7 and 8, but hasn't been backported to 6 yet.\nIt also may explain why #1549 seems to fix this issue. We'll need to review the PR and see if that fixes the error for your use case. Alternatively, we can rewrite the PassThrough stream so that we let node.js handle stream cleanup.\nAre you able to test your code on the latest version of node to see if you can reproduce it? If the error goes away, then we've probably found part of the problem.\n. @mcprostar205 \nHopefully, #1549 (just released today) fixes the issue you're seeing. Doing 1) would let you catch the error, but you're right that it wouldn't prevent it from being thrown in the first place.\nYes, if #1549 does not resolve the issue, could you try the latest version of node.js in development? I wasn't able to reproduce this error when using node.js 8, so if you're still able to encounter it I might need some more info so I can see it as well.. @mcprostar205 \nHopefully, #1549 (just released today) fixes the issue you're seeing. Doing 1) would let you catch the error, but you're right that it wouldn't prevent it from being thrown in the first place.\nYes, if #1549 does not resolve the issue, could you try the latest version of node.js in development? I wasn't able to reproduce this error when using node.js 8, so if you're still able to encounter it I might need some more info so I can see it as well.. @KurtPattyn \nThe node.js SDK uses the native crypto module included as part of node.js.\nThe browser SDK does use crypto-browserify, but only for calculating SHA256 and MD5 hashes, which don't use random number generators. The only place where Math.random would be called is when calculating a v4 uuid for idempotency tokens when window.crypto or window.msCrypto does not exist.\nUnfortunately upgrading the version of crypto-browserify adds significant bloat to the SDK. We are looking to address this in the future.. @KurtPattyn \nThe node.js SDK uses the native crypto module included as part of node.js.\nThe browser SDK does use crypto-browserify, but only for calculating SHA256 and MD5 hashes, which don't use random number generators. The only place where Math.random would be called is when calculating a v4 uuid for idempotency tokens when window.crypto or window.msCrypto does not exist.\nUnfortunately upgrading the version of crypto-browserify adds significant bloat to the SDK. We are looking to address this in the future.. @oyeanuj \nAre you trying to perform a multipart upload from a browser? The Managed Uploader you linked to accomplishes that, and works directly with File objects. You can also use it indirectly by calling s3.upload. s3.upload will perform a multipart upload behind the scenes if your file is larger than 5 MB.\nThe libraries you linked to don't appear to be using presigned urls to handle multipart uploads. EvaporateJS does ask for a signingUrl, but this is actually a url to a service you host that returns a v4 signature, which isn't the same as a presigned url.\nCan you provide some more feedback on what you're trying to accomplish with the SDK?\n. @oyeanuj \nAre you trying to perform a multipart upload from a browser? The Managed Uploader you linked to accomplishes that, and works directly with File objects. You can also use it indirectly by calling s3.upload. s3.upload will perform a multipart upload behind the scenes if your file is larger than 5 MB.\nThe libraries you linked to don't appear to be using presigned urls to handle multipart uploads. EvaporateJS does ask for a signingUrl, but this is actually a url to a service you host that returns a v4 signature, which isn't the same as a presigned url.\nCan you provide some more feedback on what you're trying to accomplish with the SDK?\n. @justinmchase \nIs your callback to domain.uploadDocuments getting the error you posted?\nThe error's callstack looks like it's being thrown from lib/protocol/query, which makes sense for CloudSearch, but CloudSearchDomain is a rest-json based service so I'd expect the stack trace to look different.. @justinmchase \nIs your callback to domain.uploadDocuments getting the error you posted?\nThe error's callstack looks like it's being thrown from lib/protocol/query, which makes sense for CloudSearch, but CloudSearchDomain is a rest-json based service so I'd expect the stack trace to look different.. @justinmchase \nThanks for the PR!\nThe SDK doesn't use the exception data in the API models. Instead it looks at the x-amz-errortype header or for the __type/code fields in the response body.\nhttps://github.com/aws/aws-sdk-js/blob/v2.80.0/lib/protocol/json.js#L19-L49\nI will reach out to the service team about updating the error type they are sending back for throttling errors and keep your original issue open for tracking.. @justinmchase \nThanks for the PR!\nThe SDK doesn't use the exception data in the API models. Instead it looks at the x-amz-errortype header or for the __type/code fields in the response body.\nhttps://github.com/aws/aws-sdk-js/blob/v2.80.0/lib/protocol/json.js#L19-L49\nI will reach out to the service team about updating the error type they are sending back for throttling errors and keep your original issue open for tracking.. @frauss \nYou're correct that the SDK is overriding the accepts header. I've marked this as a bug.\nAs a temporary fix, you can set the Accept header manually after the SDK sets it like so:\njavascript\nvar params = {\n  restApiId: apiId,\n  stageName: stageName,\n  exportType: \"swagger\",\n  parameters: {\n    extensions: \"integrations,postman\"\n  }\n};\nvar apiGatewayClient = getAWSClients.getAPIGatewayClient({ logger: console });\nvar req = apiGatewayClient.getExport(params);\nreq.on('build', function(request) {\n  request.httpRequest.headers['Accept'] = 'application/yaml';\n});\nreq.send(function(err, data) {\n  if (!err) {\n    responseData.export = data.body;\n  }\n  callback(err, responseData, context);\n});. @frauss \nYou're correct that the SDK is overriding the accepts header. I've marked this as a bug.\nAs a temporary fix, you can set the Accept header manually after the SDK sets it like so:\njavascript\nvar params = {\n  restApiId: apiId,\n  stageName: stageName,\n  exportType: \"swagger\",\n  parameters: {\n    extensions: \"integrations,postman\"\n  }\n};\nvar apiGatewayClient = getAWSClients.getAPIGatewayClient({ logger: console });\nvar req = apiGatewayClient.getExport(params);\nreq.on('build', function(request) {\n  request.httpRequest.headers['Accept'] = 'application/yaml';\n});\nreq.send(function(err, data) {\n  if (!err) {\n    responseData.export = data.body;\n  }\n  callback(err, responseData, context);\n});. @hassankhan \nCan you share which version of React Native you're using, and if you're seeing the error in Android or iOS?\nI am seeing an issue, but not quite the same as you are. When I try to do a scan, I get a CRC32CheckFailed error. This occurs when the CRC32 checksum the SDK calculates on the response body is different than the value sent back by the service. When this issue is encountered, the SDK will automatically retry the request, up to 10 times (with exponential back-off between tries). With 10 retries, it is possible that the operation would take ~24 seconds to respond, are you sure that's not the case here? You can reduce the number of retries as well.\nI think the way the SDK is calculating the checksum for DynamoDB responses may be buggy in React Native, so I'll look into that. As a workaround, you can disable calculating the checksum on the SDK side.\njavascript\nconst awsConfig = new AWSSDK.Config({\n  accessKeyId : '',\n  secretAccessKey : '',\n  region : 'us-east-1',\n  maxRetries: 2, // reduce number of retries from 10 to 2\n  dynamoDbCrc32: false // disable SDK-side crc32 check\n});. @hassankhan \nCan you share which version of React Native you're using, and if you're seeing the error in Android or iOS?\nI am seeing an issue, but not quite the same as you are. When I try to do a scan, I get a CRC32CheckFailed error. This occurs when the CRC32 checksum the SDK calculates on the response body is different than the value sent back by the service. When this issue is encountered, the SDK will automatically retry the request, up to 10 times (with exponential back-off between tries). With 10 retries, it is possible that the operation would take ~24 seconds to respond, are you sure that's not the case here? You can reduce the number of retries as well.\nI think the way the SDK is calculating the checksum for DynamoDB responses may be buggy in React Native, so I'll look into that. As a workaround, you can disable calculating the checksum on the SDK side.\njavascript\nconst awsConfig = new AWSSDK.Config({\n  accessKeyId : '',\n  secretAccessKey : '',\n  region : 'us-east-1',\n  maxRetries: 2, // reduce number of retries from 10 to 2\n  dynamoDbCrc32: false // disable SDK-side crc32 check\n});. @hassankhan \nThe main reason you see a difference between originalXMLHttpRequest and XMLHttpRequest is that the latter does not enforce CORS in a React Native context. There is an x-amz-crc32 header returned by DynamoDB that when present, the SDK will use to compare its own crc32 checksum of the response against.\nIn browser environments that support CORS (most/all browsers), this header isn't accessible in the JavaScript runtime because the service doesn't explicitly expose it. For this reason, the CRC32 check isn't performed in browsers.\nIn React Native, CORS is not enforced, so the JavaScript environment does have access to all headers, including x-amz-crc32. That explains why the check is occurring in a React Native environment and why you wouldn't see this error in a browser.\nThere is one more gotcha. DynamoDB will gzip the response payload if its size exceeds some threshold (I'm not 100% sure what size that is). According to this comment, DynamoDB calculates the crc32 checksum after the payload is gzipped. By the time it reaches the JavaScript runtime, it is no longer gzipped so the CRC32 checksum the SDK calculates won't match. It's this behavior that causes some checksums to pass and some to fail, correlating to the size of the response body.\nWe could try disabling gzip (assuming React Native's XMLHttpRequest implementation doesn't overwrite the Accept-Encoding header), but this will cause more data to be transferred over the wire. That might be useful to add as a configuration option for when payload sizes don't matter. In the meantime, disabling the CRC32 check should unblock you for now.. @hassankhan \nThe main reason you see a difference between originalXMLHttpRequest and XMLHttpRequest is that the latter does not enforce CORS in a React Native context. There is an x-amz-crc32 header returned by DynamoDB that when present, the SDK will use to compare its own crc32 checksum of the response against.\nIn browser environments that support CORS (most/all browsers), this header isn't accessible in the JavaScript runtime because the service doesn't explicitly expose it. For this reason, the CRC32 check isn't performed in browsers.\nIn React Native, CORS is not enforced, so the JavaScript environment does have access to all headers, including x-amz-crc32. That explains why the check is occurring in a React Native environment and why you wouldn't see this error in a browser.\nThere is one more gotcha. DynamoDB will gzip the response payload if its size exceeds some threshold (I'm not 100% sure what size that is). According to this comment, DynamoDB calculates the crc32 checksum after the payload is gzipped. By the time it reaches the JavaScript runtime, it is no longer gzipped so the CRC32 checksum the SDK calculates won't match. It's this behavior that causes some checksums to pass and some to fail, correlating to the size of the response body.\nWe could try disabling gzip (assuming React Native's XMLHttpRequest implementation doesn't overwrite the Accept-Encoding header), but this will cause more data to be transferred over the wire. That might be useful to add as a configuration option for when payload sizes don't matter. In the meantime, disabling the CRC32 check should unblock you for now.. @0190 \nIn your case, you're seeing got result printed twice because you're mixing calling operations by passing a callback and using promises. You should only use one method of getting the result.. @adamatan \nWe appreciate you wanting to contribute to the documentation! The API documentation is generated from the source code/api models, but it looks like you'd want to contribute to the developer guide. \nThe developer guide was moved out of this repo last year, and I don't think it has been put on GitHub yet. I'll reach out internally to find out how contributions can be made.\nI noticed that in your example, you were updating the global configuration after instantiating your service client:\n```javascript\nvar AWS = require('aws-sdk');\nvar cloudwatchlogs = new AWS.CloudWatchLogs();\nconsole.log(AWS.config.region)              // Undefined\nAWS.config.region = 'eu-central-1'          // Define the region with dot notation\nconsole.log(AWS.config.region) .            // eu-central-1\n```\nThe reason the above doesn't work is because the configuration passed to service clients is considered immutable. If updating the global configuration affected already created service clients, then it would be easy to accidentally mutate them by changing the region, credentials, etc.\nIf you moved \njavascript\nAWS.config.region = 'eu-central-1'\n above \njavascript\nvar cloudwatchlogs = new AWS.CloudWatchLogs();\nthen you'd see your expected behavior. This is because when a service client is instantiated, the configuration passed in is merged on top of a copy of the global configuration.\nThere is documentation on the global configuration and service clients here: http://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/global-config-object.html\nAdmittedly this can be hard to find unless you know exactly what you're looking for. We can add a link to this page so it appears in the API reference docs with each service constructor, and do a better job of calling out how the different configuration sources get merged.. @broofa \nWhen a request is called with a callback and createReadStream, it triggers a race condition where 2 requests are opened at once, but the data from both are piped to the same destination. There are some details about this here.\nInstead of passing a callback function to s3.getObject to catch the error from the service, you should attach an error event listener to the s3 stream.\nExample:\n```javascript\nvar fs = require('fs');\nvar path = require('path');\nvar S3 = require('aws-sdk/clients/s3');\nvar s3 = new S3({\n    region: 'us-west-2'\n});\nvar name = 'fake.file';\nvar fileStream = fs.createWriteStream(path.join(__dirname, name));\nvar s3Stream = s3.getObject({Bucket: 'BUCKET', Key: name}).createReadStream();\n// Listen for errors returned by the service\ns3Stream.on('error', function(err) {\n    // NoSuchKey: The specified key does not exist\n    console.error(err);\n});\ns3Stream.pipe(fileStream).on('error', function(err) {\n    // capture any errors that occur when writing data to the file\n    console.error('File Stream:', err);\n}).on('close', function() {\n    console.log('Done.');\n});\n```\nOur plan is to throw an error when mixing callbacks with streams are attempted, since doing so will usually cause unexpected behavior.. @broofa \nWhen a request is called with a callback and createReadStream, it triggers a race condition where 2 requests are opened at once, but the data from both are piped to the same destination. There are some details about this here.\nInstead of passing a callback function to s3.getObject to catch the error from the service, you should attach an error event listener to the s3 stream.\nExample:\n```javascript\nvar fs = require('fs');\nvar path = require('path');\nvar S3 = require('aws-sdk/clients/s3');\nvar s3 = new S3({\n    region: 'us-west-2'\n});\nvar name = 'fake.file';\nvar fileStream = fs.createWriteStream(path.join(__dirname, name));\nvar s3Stream = s3.getObject({Bucket: 'BUCKET', Key: name}).createReadStream();\n// Listen for errors returned by the service\ns3Stream.on('error', function(err) {\n    // NoSuchKey: The specified key does not exist\n    console.error(err);\n});\ns3Stream.pipe(fileStream).on('error', function(err) {\n    // capture any errors that occur when writing data to the file\n    console.error('File Stream:', err);\n}).on('close', function() {\n    console.log('Done.');\n});\n```\nOur plan is to throw an error when mixing callbacks with streams are attempted, since doing so will usually cause unexpected behavior.. @Grummfy \nI think you meant to post this to the php sdk team over here. This is the JavaScript SDK. \ud83d\ude04 . @Grummfy \nI think you meant to post this to the php sdk team over here. This is the JavaScript SDK. \ud83d\ude04 . @solankipriti \nWhen calling operations on a client, you'll need to instantiate the client first.\nS3 in your example is a constructor. If you set const s3 = new S3();, you should be able to call operations on the s3 object.. @solankipriti \nWhen calling operations on a client, you'll need to instantiate the client first.\nS3 in your example is a constructor. If you set const s3 = new S3();, you should be able to call operations on the s3 object.. @arunrreddy \nI believe you need to use sendRawEmail to add attachments. \nThere's an example of adding an attachment from SES here:\nhttp://docs.aws.amazon.com/ses/latest/DeveloperGuide/send-email-raw.html#send-email-raw-mime\nIf you want to request attachment support in sendEmail, you can also reach out to SES on the forums.. @arunrreddy \nI believe you need to use sendRawEmail to add attachments. \nThere's an example of adding an attachment from SES here:\nhttp://docs.aws.amazon.com/ses/latest/DeveloperGuide/send-email-raw.html#send-email-raw-mime\nIf you want to request attachment support in sendEmail, you can also reach out to SES on the forums.. @ffxsam \nOnce you've enabled transfer acceleration on your bucket, you will need to configure the S3 client from the SDK to use the accelerate endpoint. By default, it is turned off.\nWe use the same bucket name you specify, but do change the endpoint.\njavascript\nvar s3 = new AWS.S3({useAccelerateEndpoint: true});. No problem!. @gerebdavid \nCan you share where your credentials are sourced from (shared credentials, env variables, etc) and if you're running on EC2 or ECS? Also, is every request failing, or is it intermittent?\nIt might also be helpful to see what the expected canonical string is (sans credentials) if you can remove identifying info.. @Lanceshi2 \nYes, the SDK does support using the multi-part upload in a browser. You can do so by either calling s3.upload or using the S3.ManagedUpload class directly.\nThe managed uploader supports passing in a File object as the Body. There is an example of that in the developer guide here:\nhttp://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/s3-example-photo-album.html#s3-example-photo-album-adding-photos\nNote: You may need to update your bucket CORS configuration to support multipart uploads. @AllanFly120 posted an example configuration that will work. Note that ETag needs to be exposed to complete a multi-part upload. The DELETE method also needs to be allowed to clean up a multi-part upload in case a failure occurs.. @OpenxcellOmTeam \nSince this question is related to how a service API works, can you post your question to the Amazon Cognito forums? The service team will be better equipped to answer questions related to the API.\nIf it is determined there is an issue with the SDK, please feel free to re-open this issue or create a new one.. @aaroncai-myob \nThis error indicates that Records is not available on the event object. Can you verify that it does exist and is an array by logging the event object?. @seaBass3 \nWhen the documentation says Metadata needs to be a map, it just means you need to pass in an object where keys and values are strings. It requires a map so you can specify multiple Metadata fields/values.\nFor example, your Metadata could look like:\njavascript\nMetadata: {\n  'test': 'just a test'\n}. @dkesler \nAre you able to get any request ids back in the error message? Can you also share what version of node.js you're using?\nAs a work-around, you can force the SDK to use path style URLs which should mitigate the issue you're seeing:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#constructor-property\njavascript\nvar s3 = new AWS.S3({\n  s3ForcePathStyle: true\n});. @dkesler \nThank you for the info.\nCan you also share if your bucket name contains any periods or non-alphanumeric characters?. @seaBass3 \nYou can't use wildcard characters within the ExposeHeader tags.\nFrom what I can tell your configuration looks correct. Below is the CORS configuration I tested with and I was able to see the metadata in the console.\nCan you check in the networking tab that you see x-amz-meta-office in the Access-Control-Expose-Headers header, and that you also see the x-amz-meta-office header itself returned in the S3 response?\nExample Config:\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n<CORSRule>\n    <AllowedOrigin>*</AllowedOrigin>\n    <AllowedMethod>GET</AllowedMethod>\n    <AllowedMethod>PUT</AllowedMethod>\n    <AllowedMethod>POST</AllowedMethod>\n    <AllowedMethod>HEAD</AllowedMethod>\n    <AllowedMethod>DELETE</AllowedMethod>\n    <MaxAgeSeconds>3000</MaxAgeSeconds>\n    <ExposeHeader>ETag</ExposeHeader>\n    <ExposeHeader>x-amz-meta-one</ExposeHeader>\n    <AllowedHeader>*</AllowedHeader>\n</CORSRule>\n</CORSConfiguration>\n. I'm sorry, where are you trying to confirm that your metadata is set on an object? I assumed you were testing the SDK in a browser since you posted your CORS configuration. What error are you encountering?. @seaBass3 \nIn the new S3 console, you should be able to click on the object, then under Properties click on Metadata and see the metadata you applied. Note that these are different from tags.\nGlad to hear you are seeing your metadata using the SDK! I'm closing this issue since the SDK is functioning correctly.. @hashans \nWhere is the endpoint you're specifying coming from? I don't believe you need to specify an endpoint when using Iot, only IotData.. @dumbird \nCan you share which version of the SDK you're using, and which version of node.js?\nDo you know if this is happening when you have multiple requests running concurrently, or if it happens even with just one request at a time?. @seaBass3 \nThe listAlbums code you looked at is a good starting point.\nRight now the SDK doesn't have a built-in way to distinguish folders from files, but here's what you could do:\n\nCall s3.listObjects with a Delimiter set to /. \nThis will cause S3 to return any objects that do not contain / and that start with the current Prefix (empty unless you specify it). This effectively gives you all the 'files' in a given folder. This also gives you any object that contains a '/' in the CommonPrefixes array. Generally, you can treat these prefixes as subfolders.\nCall s3.listObjects with a Delimiter set to / and a Prefix set to a CommonPrefix found in an earlier step.\nSince the CommonPrefixes you found in step 1 should represent sub-folders in your S3 bucket, when you set Prefix to one of these with the Delimiter still set to /, S3 will effectively return a list of objects in the 'folder' defined by Prefix.\n\nYou can add extra checks to ensure that your 'folders' aren't actually objects that contain data, though the above strategy should work otherwise. You can also add metadata to folders to make those checks easier.\nI'm going to mark this as a feature request since this sounds like a useful feature to bake into the SDK as well. Let me know if my response doesn't make sense.. @niteshvirani \nThis sounds suspiciously like a change in CORS support in the Edge browser. \nQuick solution: Turn off crc32 checks in your Document Client by configuring the DynamoDB service you pass in with dynamoDbCrc32: false:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB/DocumentClient.html#constructor-property\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB.html#constructor-property\nThe SDK checks the response body from DynamoDB by calculating a CRC32 hash and comparing it to the one DynamoDB sent in the x-amz-crc32 header. DynamoDB will gzip content that's sent once it exceeds a certain size (I'm not sure what that size is) and calculate the crc32 hash based on the gzipped payload. In browsers, we only have access to the response after it has been unzipped, so our CRC32 calculations won't match what DynamoDB calculates in these cases.\nHowever, x-amz-crc32 is not an exposed header, meaning that browsers that support CORS should not recognize the header, and thus the SDK will skip this check. This is the behavior you're seeing in Chrome/Firefox. In Edge developer tools, can you share a screenshot of what your response headers look like from a DynamoDB response that causes this error?\nSetting the configuration dynamoDbCrc32 to false tells the SDK to skip doing the CRC32 check, which should be helpful when you're running in a browser since this check won't work for larger payloads anyway. We should probably set this value to false when running in a browser context to catch cases like these.. @jonface \nI had to go way back, but it looks like both Content-Type and User-Agent were made 'unsignable' when the browser version of the SDK was developed. Presumably this is because browsers could modify these headers. For example, there was a bug reported years ago that Firefox would always add the charset to the Content-Type, even if it wasn't set in the SDK.\nIt may not be possible to change this with the current version of the SDK as that could break current consumers of the SDK, taht'll require further investigation. This is definitely something we will want to revisit though when we do a major version bump.. @samirbr \nsearchHIT was part of the old Mechanical Turk Requester API (2014-08-15) that was not supported broadly by the SDKs:\nhttp://docs.aws.amazon.com/AWSMechTurk/latest/AWSMturkAPI-legacy/ApiReference_SearchHITsOperation.html\nThe new api version, 2017-01-17, that is supported by the SDKs does not have the searchHIT function. That said, it appears that listHITs should serve the same function as searchHIT did in the previous API.\nAlso feel free to leave API feedback to the Mechanical Turk team on their forums here, I'm sure they'd appreciate it!. @itchingpixels \nWith version 2.83.0 of the SDK, you should be able to import the SDK using import * as AWS from 'aws-sdk' instead of going into the dist directory.\nCan you try importing the SDK as above and test if the typings work then? You will also need @types/node, since the SDK types rely on some of the nodejs typings to be present.. Regarding your question about the TypeScript tests, can you add the definition for getSignedUrlPromise to https://github.com/aws/aws-sdk-js/blob/master/lib/services/s3.d.ts, and then in /ts/s3.ts just attempt to use the new method (this makes sure that the TypeScript compiler recognizes the new definition correctly)?\nLet us know if you need help adding the TypeScript typings. It can look foreign if you haven't had to deal with them before.. @hgonzalez94 \nHow is the data corrupted? Is the base64 string you download different from the base64 string you upload to S3? I don't believe setting ContentEncoding to base64 actually does anything from S3's side (i.e., they won't decode the string on upload), so your data should still be stored as a base64 encoded string representation of your image.\nAre you trying to display your image in React Native? I did a quick test using the latest version of the SDK where I uploaded a base64 image to S3, then downloaded it and displayed it in an Image tag. If you're displaying the image in an Image tag, you do need to prepend the base64 image with something like data:image/jpg;base64, before you use it as your image source.\nHere's my rough example (using TypeScript but should be easy to convert to JS):\n```App.js\n/* App.js /\nimport React from 'react';\nimport { StyleSheet, Text, View, Button, Image } from 'react-native';\nimport {putObject} from './src/putObject';\nimport {getImage} from './src/getImage';\nexport default class App extends React.Component {\n  render() {\n    const state = this.state;\n    return (\n      \nOpen up App.js to start working on your app!\nChanges you make will automatically reload.\nShake your phone to open the developer menu.\n {\n            await putObject();\n            this.setState({\n              hasImage: true,\n              image: await getImage()\n            });\n      }}\n    />\n    {state && state.hasImage && <Image\n      style={{width: 100, height: 200, borderColor: 'red', borderWidth: 1}}\n      source={{uri: state.image}}\n     />}\n  </View>\n);\n\n}\n}\nconst styles = StyleSheet.create({\n  container: {\n    flex: 1,\n    backgroundColor: '#fff',\n    alignItems: 'center',\n    justifyContent: 'center',\n  },\n});\n```\n```javascript\n/* putObject.ts /\nimport * as S3 from 'aws-sdk/clients/s3';\nimport {image} from './image'; //base64 encoded image for testing\nconst s3 = new S3({\n    region: 'us-west-2',\n    credentials: {\n        / From somewhere /\n    }\n});\nexport async function putObject() {\n    const data = await s3.putObject({\n        Bucket: 'BUCKET',\n        Key: 'test-image.jpg',\n        ContentType: 'image/jpeg',\n        Body: image\n    }).promise();\n}\n```\n```javascript\n/* getImage.ts /\nimport * as S3 from 'aws-sdk/clients/s3';\nconst s3 = new S3({\n    region: 'us-west-2',\n    credentials: {\n        / From somewhere /\n    }\n});\nexport async function getImage() {\n    const data = await s3.getObject({\n        Bucket: 'BUCKET',\n        Key: 'test-image.jpg'\n    }).promise();\n    return 'data:image/jpg;base64,' + (data.Body as Buffer);\n}\n```\n. @hgonzalez94 \nWhich platform are you testing on, Android or iOS?\nI ran the same code as above, except I decoded the base64 string into a binary format before calling putObject. When I do that, the image is saved to S3 and does not need any transformations performed to view it. I have only run this test on iOS so far, and have react-native 0.47.0 installed. I've noticed differences in XMLHttpRequest behavior depending on which platform you're targeting so there may be some issue that has either been fixed, or I'm not seeing yet.\nTo decode the base64 image, I'm using AWS.util.base64.decode(image). It's not documented but unlikely to change.. @hgonzalez94 \nAre you able to update your version of react native to see if that fixes your issue? On both iOS and android, using version 0.47.0, I'm not encountering these issues. I can set up a project using v0.36.0, but I suspect that something with the XMLHttpRequest implementation react-native shims changed to better support binary payloads since your version.. @syberkitten \nCan you show how you're calling upload or putObject? The version of the SDK/node.js might be helpful too. Any request IDs you can share with us could be helpful as well.\nFor what it's worth, I'm able to upload objects using node streams with version 2.112.0 of the SDK and 8.4.0 of node.js. I'm also using a bucket in eu-central-1.\nHere's the code I'm using. I included code to turn on request id logging for any S3 operation as well (it can be difficult to find request ids for operations that didn't error when using s3.upload otherwise)\n```javascript\nconst fs = require('fs');\nconst path = require('path');\nconst S3 = require('aws-sdk/clients/s3');\nconst s3 = new S3({\n    region: 'eu-central-1'\n});\n// Add custom request handlers to log out request ids for any S3 client\nS3.prototype.customizeRequests(function(request) {\n    function logRequestIds(response) {\n        const operation = response.request.operation;\n        const requestId = response.requestId;\n        const requestId2 = response.extendedRequestId;\n        console.log(${operation}, requestId: ${requestId}, requestId2: ${requestId2});\n    }\n    request.on('extractData', logRequestIds);\n    request.on('extractError', logRequestIds);\n});\n// 3 MB text file\nconst filePath = path.join(process.cwd(), 'test.txt');\n// upload file\ns3.upload({\n    Bucket: 'BUCKET',\n    Key: 'test.txt',\n    Body: fs.createReadStream(filePath)\n}).promise().then(function(uploadData) {\n    return s3.headObject({\n        Bucket: 'BUCKET',\n        Key: 'test.txt'\n    }).promise();\n}).then(function(headData) {\n    console.log(headData);\n    /\n        { AcceptRanges: 'bytes',\n  LastModified: 2017-09-11T19:25:00.000Z,\n  ContentLength: 3145728,\n  ETag: '\"8f9abcae94ab69f39fc6935b89dabefc\"',\n  ContentType: 'application/octet-stream',\n  Metadata: {} }\n    /\n}).catch(function(err) {\n    console.error(err);\n});\n``. @jeffbyrnes \nIt looks like Bluebird does a few checks on theerror` object that is rejected and then logs this warning itself. They have an explanation about the warning here.\nSome of the errors received from services don't contain message data, so in those cases, the error.message field is null. Bluebird specifically checks for a message field and logs that warning if it isn't available. Native promises allow for this behavior, but Bluebird is being stricter in this case.\nI don't know the internals of Bluebird that well, but it looks like they go down a different code path when you use Bluebird.promisify, so that warning isn't logged even though the error.message field is still null.\nWe could update our logic so that we set the message to an empty string if we can't otherwise determine the error message from what the service sends us. This would satisfy Bluebird.\n. Agreed. An empty string message doesn't provide any additional info. Closing!. @khiem-nguyen \nDoes the issue happen even when just loading the page, or when actually making an AJAX call?\nWhich host is Chrome identifying as having a bad certificate?\nI just tried the above tutorial using Chrome 60 on android, and on mac os x, and am not seeing this issue. You might also find some luck searching for this error on stack overflow, since it is not an SDK specific error.. @napcoder \nI tried a simple lambda function using the same permissions you set up above. I'm not sure what the cause of your issue is. Can you try creating a lambda function that solely tests that you can call getOpenIdTokenForDeveloperIdentity? I wrote a quick example below (Lambda and Cognito are both is us-west-2 in my case).\nIf you still encounter problems, you may need to reach out to Cognito on the forums here. They should be able to provide more insight on what is causing your call to fail.\n```javascript\n/ Lambda Function /\n'use strict';\nconst AWS = require('aws-sdk');\nexports.handler = (event, context, callback) => {\n    AWS.config.update({\n        region: 'us-west-2'\n    });\n    const provider_name = 'CUSTOM';\n    const params = {\n        IdentityPoolId: 'ID',\n        Logins: {\n            [provider_name]: 'test'\n        }\n    };\nconst cognito = new AWS.CognitoIdentity({apiVersion: '2014-06-30'});\nreturn cognito.getOpenIdTokenForDeveloperIdentity(params).promise()\n    .then((data) => {\n        console.log(JSON.stringify(data, null, 4));\n        callback(null, data);\n    }).catch((err) => {\n        console.log(err, err.stack);\n        callback(err);\n    });\n\n};\n``. @napcoder \nGlad to hear that's working! In that case, I'm going to close this issue. Feel free to re-open or make a new issue if you discover something wrong with the SDK.. @afroozeh \nThe TypeScript definitions for services are generated based on the service models (found inapis/`) that are shared across all the SDKs.\nGenerally, we only know which fields are required as input to an operation, not which fields will always be present on the output. I can reach out to S3 to see if they can document which fields are always present.. @leantide \nThere is a list of DynamoDB reserved words that can't be used as AttributeNames. state is one of those.\nIf you want to continue using state, you can define an ExpressionAttributeName and use the # symbol to prefix state.\nExample:\njavascript\nvar params = {\n  /** more fields */\n  UpdateExpression: 'SET #state = :state',\n  ExpressionAttributeValues: {\n    ':state': 'update my state value please'\n  },\n  ExpressionAttributeNames: {\n    '#state': 'state'\n  }\n}. @PrimeObjects \nHave you tried using s3.upload? For files larger than 5 MB, it will automatically handle mutli-part uploads. I was able to successfully upload a 10 MB image using iOS and android on react native version 0.47. What version of react native are you on? Can you try updating to at least version 0.47 if you aren't already at that version?. Make sure to squash, but :shipit: assuming travis passes.. I think applying this change to all binary shapes instead of just base64 shapes actually makes some things more confusing. For example, s3.putObject now sounds like it will base64 encode the body if I pass it text.\n\n. @ffoysal \nAre you by chance using a version of the SDK older than 2.94.0? That's the version where targetInService appears.\nRunning the above in the latest version of the SDK, I don't get a StateNotFoundError.. @schatekar \nAre you referencing the QueryInput interface from AWS.DynamoDB.DocumentClient.QueryInput or AWS.DynamoDB.QueryInput? Since the DocumentClient automatically handles transforming values into AttributeValues, its interfaces are different. In your case, you should be using the QueryInput from the DocumentClient.. The uuid dependency has been updated and is merged into master. The next version of the SDK will include this change.\n3.1.0 did not include any code changes, just documentation. Did you need it upgraded to avoid duplicate copies, or some other reason?. @sastrygunnu \nI'm not sure what you're asking for. Are you trying to make presigned urls for multiple objects in a bucket?. @CoreyAR \nIf you try to use the SDK without importing the aws-sdk-react-native file directly, do you still see this issue?\nCan you also share what version of React Native you're if the above doesn't work? We made some changes so that you shouldn't need to import the dist file anymore, but I haven't run into this issue when testing locally so not 100% sure the above will fix the issue for you.. @shri3k \nMost services will only support https. There is a full list of which services support which protocols here:\nhttp://docs.aws.amazon.com/general/latest/gr/rande.html\nNote that Polly only supports HTTPS according to the table.. @usamamashkoor \nHow fast is your connection? Are you able to make other S3 requests successfully?\nNote:\nYou don't have to do this, but when using s3.upload, you can set the Body parameter to a stream. The benefit here is you don't have to load the entire file into memory first (which is what fs.readFile will do).\nExample:\njavascript\nvar videoStream = fs.createReadStream('filepath');\nvar params = {\n  Key: 'video',\n  Body: videoStream\n};. @shuaishenk \nI'm not clear about what the issue you're reporting is. Are you saying that within your internal cloud, when you try to call s3.createBucket, S3 is returning an error?  Can you share the full error here?\nWe intentionally do not sign the Content-Type header: https://github.com/aws/aws-sdk-js/blob/v2.138.0/lib/signers/v4.js#L192\n. @sibelius \nThe SDK doesn't currently know how to read a file using the uri method. Instead it would require either a Blob, string, or a Buffer/Uint8Array be passed in as the body.\nI'll mark this as a feature request. In the meantime, you could try generating a presigned url and make either a fetch or XMLHttpRequest call directly, setting the Body as an object that contains the uri. I haven't tested this myself, but I think that should work.. Thanks for reporting and fixing this, @nsaboo \nFixed by #1775 . @nsaboo \nThanks for the contribution!. @sam0x17 \nWhat version of the SDK are you using? If you don't specify the signatureVersion of your S3 client, then the S3 client will create a presigned URL using 's3' by default. (Specifying 'v2' also uses 's3'). The only exception to this rule is if the region you've set does not support signatureVersion v2.\nI just tested generating 'v2' signed URLs using the latest version of the SDK, and so far haven't been able to reproduce your issue.\njavascript\n// force signature version v2\nvar url = new AWS.S3({signatureVersion: 'v2'}).getSignedUrl('putObject', {Bucket: 'BUCKET', Key: 'KEY'}). @sam0x17 \nGlad to hear you're able to generate the signed URLs.\nCan you test your URLs against S3 itself? Unfortunately, since your tests are against a 3rd party service there's not much I can do to determine why you're receiving that error. The issue may be with the service itself.\nIf you have the same issue when making requests against Amazon S3, then I'd be happy to help!. @sam0x17 \nI notice now that you're trying to upload FormData. That will cause corrupted data to be uploaded.\nSee this and this.\nYou would either need to upload the binary (you should be able to specify a Blob as the body), or use a presigned Post instead.. @sam0x17 \nAh yes, for that you'd want to use a POST.\nGlad you were able to get the PUT working though!. @sam0x17 \nPlease see the documentation for s3.createPresignedPost\nThe callback will return a data object where fields is the hash of fields your form has to include.\nThere's an example of what an HTML page would look like using the presigned post.\nHowever, it just occurred to me that our implementation of presigned POST only works with signatureVersion v4, so if Digital Oceans Spaces doesn't support v4, then you'd have to resort to using a presigned PUT URL.\n. @TrevorSayre \nCan you share what version of webpack you're using, and your webpack config (if you're using one?). @AWSSteveHa \nIt looks like our travis settings changed so we no longer run travis (or codecov) everytime a new release is pushed. I reran travis on master manually, then reran your PR to update codecov. We'll likely disable codecov in the future and just rely on istanbul reporting, but your PR is no longer blocked.. @samayo \nBased on the S3 documentation on object tagging, it doesn't look like filtering objects via listObjects with tags is currently supported.\nSince this feature would need to be implemented on S3's end, please create a feature request post on the S3 forums so they can collect your feedback.. @dhawalmewada \nThat issue looks like what I'd expect if you didn't have node typings installed.\nCan you verify that node typings exists for your project? If they don't, you can install them as a dev dependency:\nnpm install --save-dev @types/node. @ktalebian \nOnce you have a presigned-url, you should be able to use it directly with your http client. The SDK doesn't use them itself, it just generates them.\nFor example, you could do something like the below using fetch:\njavascript\nfetch(presignedUrl, {\n  method: 'PUT',\n  headers: new Headers() // any extra headers you want to send,\n  body: file\n});. @peterpeterparker \nCan you share the general pattern for your bucket name? Specifically if it contains any dots, like xxx.xx.xxx?. @peterpeterparker \nThanks! I am able to reproduce the issue. \nInterestingly, I don't get a signature mismatch error when I exclude ContentEncoding from the parameters. More for my own curiosity, why do you set the encoding to base64? We've seen this in other examples, and base64 isn't actually a registered content coding, and S3 doesn't perform any transformations based on the value. . @peterpeterparker \nNo worries, I was just curious if it was from following a tutorial, or if it was needed to make something else work. No judgement here!\nIf you're able, it'd be great to get confirmation from you that removing ContentEncoding works in 2.151.0. Even if that does fix it for you, we'll still want to fix it in the SDK.. @peterpeterparker \nSweet! I also found what's causing the new behavior you're seeing. The endpoint resolution for S3 was recently changed (a changelog entry was missed), and it looks like that had the side-effect of changing the default signature version for all presigned urls to V4. Prior to this change, presigned urls in regions that supported signatureVersion v2/s3 would default to that instead.\nWe'll definitely need to fix this, as sigv2 URLs act a little differently than sigv4 URLs (notably, sigv2 have a much longer max expiration time). You could get around this with the latest version of the SDK by instantiating your S3 client with signatureVersion: 'v2', or hold off until we release a fix (should be relatively soon).. @peterpeterparker \nA fix has just been merged into master, and will make it into the next release of the SDK.\nThank you for reporting this!. @iamtekeste \nAre you using the most recent version of React Native?\nThe SDK uses XMLHttpRequest to make requests, and attaches a progress listener to the xhr.upload object:\nhttps://github.com/aws/aws-sdk-js/blob/46a5ab29c1dc0a40e824cbabf623343c5e9e622a/lib/http/xhr.js#L42-L44\nReading through this issue on React Native's repo, it sounds like progress events may not be implemented. It's possible they've added support on a newer version of React Native, but the fact that your callback isn't being triggered suggests they don't support this event yet. I'd suggest posting another issue on their repo asking if they can support this progress events.\n. @bomb-on \nSince this question is about how best to use the Iot APIs, you'll get much better advice from the Iot forums.\nI'm really not familiar enough about this aspect of Iot to offer advice, but I did notice that createCertificateFromCsr includes a setAsActive field. Since you're attempting to activate your certificates anyway, have you tried setting that field to true?. @legokichi \nYou are correct, the CopySource parameter needs to be URL-encoded. This is also mentioned in the S3 API documentation.. @patrick-motard \nThe SDK should already be retrying on Throttling errors, and it looks like it will based on the retryable: true entry in your error.\nYou can tweak the retry behavior by modifying either the global config or the config passed into a client on instantiation:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#retryDelayOptions-property\nBy default, the SDK will attempt to retry 3 times, with exponential back-off/jitter that uses a base delay of 100 ms. \nIt looks like putMetricAlarm has a limit of 3 transactions per second so you may want to add logic to stay within those constraints if you're going vastly over this and retrying isn't enough, or request a limit increase.. @kaushalparik27 \nAre you using the AWS SDK that's bundled with Lambda, or are you bundling your own? Can you inspect your lambda object to see what methods are available to it?\nI just created a new node.js 6.10 lambda function and am not getting that error. Is your code above exactly what your Lambda function look like?. You would need to inspect the lambda object and log the properties. Specifically, we want to make sure you see invoke as a method. If you log properties, you'll want to view the log in CloudWatch logs instead of the viewer embedded in Lambda, otherwise some logs will be cut off.\nHere is my example that does not receive the TypeError:\n```javascript\nexports.handler = (event, context, callback) => {\n    var AWS = require('aws-sdk');\n    AWS.config.apiVersions = {\n        lambda: '2015-03-31'\n    };\nvar lambda = new AWS.Lambda();\n\nvar params = {\n    FunctionName: 'FAKE',\n    Payload: null\n};\n\nvar logs = [];\nvar invokeExists = false;\nfor (var method in lambda) {\n    if (typeof lambda[method] === 'function') {\n        logs.push(`lambda.${method} exists.`);\n        if (method === 'invoke') {\n            invokeExists = true;\n        }\n    }\n}\n\nconsole.log(logs.join('\\n'));\nconsole.log(`invoke exists: ${invokeExists}`);\n\nlambda.invoke(params, function(err, data) {\n    if (err) console.log(err, err.stack);\n    else console.log(data);\n    callback(err, data);\n});\n\n};\n``. @karthiksaligrama \nSo this is puzzling! It looks like the lambda API version you're getting is for [2014-11-11](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Lambda_20141111.html), not [2015-03-31`](https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Lambda.html). By default, the SDK should be using the latest version of lambda if you don't provide an apiVersion.\nCan you remove your apiVersion and check again? If that doesn't work, can you make sure the model for lambda exists by logging:\njavascript\nconsole.log(`Model exists: ${!!require('aws-sdk/apis/lambda-2015-03-31.min.json')}`);\nIt would be very bizarre if that model wasn't available, but we'll see!. @kaushalparik27 \nWere you you using one of the tools from https://github.com/aws/aws-vsts-tools?\nI don't think that should have the affect you're seeing, but if you do try it again and see the same issue, please provide the steps you took so the team can take a look at what's happening.. Thanks for the follow-up! Glad you're no longer facing this issue!. @IvanAlegre \nIs the file you're uploading larger than 5 MB? To me, it sounds like the multipart upload is failing to store the upload id from the initialization of the multipart upload.\nCan you update your code to the following and test it out?\n```javascript\n    const aws = require('aws-sdk');\n    aws.config.update({ region: 'eu-west-1' });\n    const s3Config = { apiVersion: '2006-03-01' };\n    const s3 = new aws.S3(s3Config);\nconst uploadResponse = await s3.upload({\n  Bucket: BUCKET_NAME,\n  Key: filename,\n  Body: fs.createReadStream(filepath)\n}).promise();\n\n```\nIf you still get this error, can you add logger: console to your s3Config and share what operations/params are being used when calling upload? (Just mask potentially sensitive data like bucket name). @steverob \nPlease reach out to the Lambda team on their forums. They handle updating the version of the SDK in Lambda.\nFeel free to take a look here if you want info on packaging up your own version of the SDK to include in a lambda function.. @bootrino \nYou can provide a logger to the global config, which will log data on all requests:\njavascript\nAWS.config.udpate({\n  logger: console\n});\nWe don't provide a public method for adding event listeners to all requests today. There is a private method that accomplishes this, but it could change in any update since it is not something we currently expose through docs or typings.\njavascript\nAWS.EventListeners.Core.on('error', function(err) {\n    console.log(err);\n});. @zacharynevin \nThanks for reporting this! It appears this service does not currently support CORS. I've forwarded this request (and https://forums.aws.amazon.com/thread.jspa?threadID=268939&tstart=0) to the Elemental team to prioritize, but don't have any ETA on when this will be completed.\nI have tested and confirmed it works in node.js, so you can forward the blob to a Lambda function/server as a work-around until this support is added.. Unfortunately, I don't believe there is a way to work-around this aside from running in an environment that doesn't enforce CORS. Keep an eye on the forum post you opened, and I'll follow-up if I hear anything from them as well. For now you would need to send the blob somewhere that doesn't enforce CORS, and send the data to MediaStore from there.. @kevindavee \nWhat does your role policy look like? Based on your error message, it sounds like your policy doesn't have the required permissions. Assuming you've configured your credentials prior to instantiating your serviceProvider, I don't see anything weird about your code.. @kevindavee \nCan you reach out to the Cognito team on the forums? If you post a link here to your forum post I can forward the message to the team internally as well. Unfortunately there isn't anything we can do on the SDK side for this issue.. @ashrafmangalore \nAs far as I know, there isn't a way to create a presigned url for S3 that lasts forever. There's a good explanation on StackOverflow on how long urls can last for.\nThe gist is, a presigned URL can only last as long as the credentials used to generate them, or 7 days (when using signature version v4), whichever is shorter.\nIf you don't care about who can see the URL, you can set it to be public readable , otherwise you'd need to regenerate the URL.. @RoyLiou \nWhat error are you getting when calling getObject?. @polovi \nA possible workaround would be to wrap the service client after you've created the DocumentClient.\n```javascript\nconst AWSXray = require('aws-xray-sdk');\nconst AWS = require('aws-sdk');\nconst client = new AWS.DynamoDB.DocumentClient({\n  service: new AWS.DynamoDB({...})\n});\nAWSXray.captureAWSClient(client.service);\n```\n. @climberwoodi \nHow are you sourcing your credentials when you run the SDK locally?\n@cuipengfei \nYou're right that the default provider chain should have used the ECS Credential provider. It's great that you've already found a solution for your issue, but if you want to keep going down the rabbit hole, could you also log what the credentials in your S3 client's config looks like?\njavascript\nvar s3 = new AWS.S3();\ns3.putObject(/*params*/, (err, data) => {\n  console.log(s3.config.credentials);\n});\nThe AWS.config.credentials is resolved when the SDK is imported/require'd for the first time, unless the credentials are retrieved asynchronously (such as EC2/ECS Credential Providers). When you create a new service client (e.g. new AWS.S3()), the configuration that is passed to the service client is merged into a clone of the global (AWS.config) configuration. When you call an operation, the credentials will be resolved if they haven't already, but that resolution will only affect your local client. Since the AWS.config.credentials was undefined when the config was cloned, it still appears to be undefined after your service client resolves its configuration.\nIn your new example, you manually set the AWS.config.credentials object, so a reference to that object is included with the cloned global config that S3 service client uses.\nSorry for the long response, let me know if that's unclear. Regardless, you shouldn't have to resort to your work-around to use ECS credentials.. @hoodsy \nAre you providing the MD5 yourself? That will only work for putObject calls, and upload calls where only a single putObject call is made behind the scenes (body < 5 MB). The SDK will actually calculate MD5s for you though, so it shouldn't be necessary for you to also calculate them.. Are you uploading streaming data? Do all large files (are those files > 5MB?) fail, or just some of them?\nAlso, if you could provide some info such as SDK version, and a simple code snippet, that may help as well!. @aaronallan \nYou should no longer have to import from aws-sdk/dist/aws-sdk-react-native.\nTo get smaller bundles, you can instead import individual services/global namespace similar to the documentation here.\nExample:\njavascript\nimport {config, CognitoIdentityCredentials} from 'aws-sdk/global';\nimport * as SNS from 'aws-sdk/clients/sns';\nYour last example should also work, but there was a bug introduced in v2.178.0 of the SDK that was causing problems when bundling the SDK in react native. #1880 fixes that issue and was just merged yesterday. It will be in the next version of the SDK that should ship today.\nFor now, can you pin your version of the SDK to 2.177.0 to verify what you're trying works? If it doesn't, there may be a different issue than what #1880 addresses.. Closing, since the code change from the PR is available in the latest version of the SDK. Please let us know if you still encounter this issue after upgrading.. @gohackfelipe \nWhat version of the SDK are you using? startCelebrityRecognition became available in 2.159.0.. Excellent! I'm closing this issue then, let us know if you still encounter problems after upgrading the SDK.. Does your package.json have a version specified for the SDK?\nCan you try installing it in a fresh project and verifying what version of the SDK you get? I'm currently seeing v2.185.0 when I install from npm.. @amitdhawan \nPlease reach out to the EC2 team on their forums here. Since your question is about how to access EC2 UserData from a script, you should be able to get better info there.. @danielrambo \nCan you share a code sample that triggers this error, and which version of TypeScript you are using?. Can you also share your tsconfig.json, and any @types packages you might have installed? Your above example doesn't cause that error for me on a new project using the latest version of the SDK.\nHave you also tried using the ES2015 import syntax?\ntypescript\nimport * as AWS from 'aws-sdk';\n. @danielrambo \nAre you seeing this error with just a file that has the above line on it? Are you depending on any @types packages?\nI actually would have expected a different error around Promise, since your tsconfig.json does not include a \"lib\" section where either \"es2015\" or \"es2015.promise\" is specified. Also, do you have @types/node installed?. Also, needs a changelog entry.. @vinay20045 \nWe know that in some cases, the SDK will receive a successful response but receive malformed XML, which is why this check is in place. We might be able to be a little smarter about how we handle this though.\nHowever, in the short term, you can remove the data extraction part of a request lifecycle so that the XML response body isn't parsed. That might be more applicable in your case since you don't have an XML parser available.\nExample:\n```javascript\nconst request = ses.sendEmail(/ params /);\nrequest.removeAllListeners('extractData');\n// send with callback\nrequest.send((err, data) => {});\n// or use promise\nrequest.promise().then(() => {}).catch(() => {})\n```. @akaNightmare \nDid you already attempt the instructions from the SNS docs here?\nIf that doesn't help, please create an issue on the SNS forums. The SNS team will be able to provide better support for this since it's specifically around using their service/API.. @hlarsen \ncredential_source is not currently supported in the JS SDK. I'll mark this as a feature request; thanks for the feedback!. @miles-po \nI agree with you that either the docs or API have to change, I just wanted to make sure we could at least provide an immediate workaround.\nIt looks like what's happening is when a number value is passed as the date, the SDK assumes it is a unix timestamp and creates a new date object from that. Since the value from Date.now() has millisecond precision instead of second precision, the SDK converts it to a much larger value.\nIt will be difficult to change the SDK to accept timestamps with millisecond precision since existing code may be written based on how the SDK works today. That is however something we can fix in the next major version update. I'd much rather prefer being able to use the value from Date.now() without any modification\nI'll update our documentation to call out that if a number value is provided, it needs to have second precision.\n. @Babloo18v \nWhenever you make a request to a service, that request happens asynchronously. This means that the JavaScript event loop does not pause while it waits for a response to come back from the server (S3 in this case). \nInstead, your for loop is going to create and send N requests immediately, not wait for any responses.\nThere are a lot of tutorials out there on how to handle asynchronous flow in JavaScript, but the easiest (in my opinion) is to use async/await.\nHere's an example:\njavascript\nasync function getFile(key) {\n  for (let i = 0; i < buckets.length; i++) {\n    try {\n      const params = {\n        Bucket: buckets[i],\n        Key: key\n      };\n      const result = await s3.headObject(params).promise();\n      // found file, do stuff\n    } catch (err) {\n      // error received, do stuff\n    }\n  }\n}\nI'm going to close this issue since it's a more general JS question, where stackoverflow would better serve you. If you do encounter an issue with the SDK, please feel free to reply or open a new issue.. @duxing \nI think the reason you aren't able to assume a role using the default credentials is because your roleArn is supplied in the ~/.aws/config file instead of the ~/.aws/credentials file.\nIf you set the environment variable AWS_SDK_LOAD_CONFIG to a truthy value (like 1), then the SDK will load data from the config file as well. This is currently opt-in because support for the config file was added well after the credentials file, and there was a backwards-incompatibility risk otherwise.\nLet me know if this doesn't solve the issue for you! I admittedly couldn't find this in our docs so I'm adding a note to update them as well. . Ah, sorry I missed that. Your configuration looks correct, can you try instantiating the SharedIniFileCredentials directly and see if you can assume a role with that?\nYou should be able to just instantiate it without options if your environment variables are set.\njavascript\nvar aws = require('aws-sdk');\naws.config.update({\n  region: 'you_region'\n  credentials: new aws.SharedIniFileCredentials()\n});\nI have to double-check that we will actually assume role by default. If I'm remembering correctly, we didn't want that behavior without a major version bump, but I'll confirm.\n. Thanks @duxing \nThe other thing you could try is copying the settings from your config file to your credentials file. That shouldn't be necessary but it might unblock you for now and help me narrow down the problem.. The SharedIniFileCredentials class can be found here.\nI'm also working on reproducing the error, but please share your findings if you dig into it. I'll do the same.. @duxing \nIt looks like the issue comes down to this line:\nhttps://github.com/aws/aws-sdk-js/blob/0fb21c06f67fdc960882aeffe3a01e9ab174e2c9/lib/shared_ini.js#L54\nWhen reading the config file, the default profile is stored internally as profile default, but that line always retrieves the default profile regardless of if the config or credentials file is being read. \nIf I used any profile other than the [profile default] in the .aws/config file to specify the role_arn and source_profile, and specified the source_profile in the .aws/credentials file, that works.\nI'm marking this as a bug since I don't see why we would want this to be the expected behaviour.. Ah, checking with the CLI, the ~/.aws/config file treats the default profile as special.\nIf you take a look at their docs here, you'll see that the default profile is not prepended with profile.\nSince we're matching the CLI behaviour, this is actually expected. Can you update your shared config file to change the default profile name?. @fcarrilLB \nI think you're looking for the getBucketAccelerateConfiguration method.\nHowever, this does mean after you make the request, you may need to create a new S3 client with useAccelerateEndpoint set to the value you want.. Awesome!. @devTalha \nCan you share the code that's resulting in the error you're seeing?\nEdit:\nOh I see, do you mean you're having an issue when trying to deploy a package with the aws-sdk included in a zip?. As an immediate workaround, adding 'ieee754': '1.1.8' to your dependencies will pull in a version that does not have files with a very old timestamp. It looks like the most recent version of this dependency still has very old timestamps for some of its files. We'll open an issue there to fix the timestamps, and can pin the dependency to 1.1.8 as well.. @TonyFNZ and others\nZip files don't support files with timestamps prior to 1980. Some tools will get around this by actually changing dates to Jan 1 1980, but this is unexpected (and potentially dangerous) behavior.\nThe CodeBuild job that you're running is using the CLI (written in python) to generate a zip file. Python's zipfile module explicitly throws an error when it encounters files dated before 1980, rather than attempt to change the dates.\nWe patched the SDK so we could be sure none of our dependencies contained files dated prior to 1980. Any other library you're using that has the same problem would need to republish as well. \nAs an aside, it looks like there was an issue with NPM that was causing incorrect file timestamps, so it's likely this is what's leading to packages having incorrect dates:\nhttps://github.com/npm/npm/issues/19968\n. @RaphaelRosa \nAre you using a tool to generate a JS bundle for your electron app?\nIf you are, then that tool is likely loading the browser version of the SDK into your application, instead of the node version. Since the browser version doesn't support streams, you're seeing Streams is undefined.. @cjlarose \nThanks for the contribution! Merging.. @kennu \nThe JS SDK does support the ~/.aws/config file, but for backwards compatibility reasons you also need to set the environment variable AWS_SDK_LOAD_CONFIG to a truthy value for the SDK to read it by default.\nIf you set the above variable to 1, then you should see the same behavior you see with the CLI.. @kaihendry \nCan you share the code you're using to retrieve the object?\nCan you also share how you've configured your S3 client? . @rafaelgfirmino \nCan you try adding the SDK in a new package? I'm not sure where aws.js-sign2 is coming from, that is not related to our SDK.\nI just created a new project running the following and didn't encounter any errors:\nbash\nyarn init\nyarn add aws-sdk\nTested using yarn 1.5.1 and node 8.4.0. @HappyRainy \nYou can construct your own https.Agent and pass it into a client constructor.\nYou can also configure your agent to accept additional certs:\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/node-registering-certs.html\nYou can modify the example in the docs for your use case as well to set rejectUnauthorized to false, though understand that this will prevent checking the certs for any network calls you make using that agent.. Thanks for posting back, closing!. @murphman300 \nSo it looks like based on your error, when you start your app it can't find this file. Can you verify the contents of the AWS SDK when you run npm start and which version it is? If that file is missing, I'd like to check if it's missing when you initially download it from npm, or it is somehow getting deleted afterthe fact.\n. S3 select is not implemented yet but is being worked on. #2054 is the first PR to add support, and we'll be merging into the wip-s3-select branch until it's ready to be released.\nThere will be a few tasks to complete before we merge into master (like documentation/typescript definitions) but I can respond back here once the branch is useable so you can try it out before its final release.. Unfortunately I can't give a better idea except that we are actively working on it now.. @kymc and others\nS3 Select support has officially landed in version 2.256.1 of the SDK (yesterday's release).\nTake a look at the docs for info on how to use it:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#selectObjectContent-property\nPlease let us know if something isn't clear in the docs. This is a new pattern where the JSON output contains a stream that emits the events from S3.\nClosing now that this is in the SDK!. Just wanted to chime in in case this helps someone; there is a more complete example of using S3 Select written in this blog post. It's still a relatively simple example but gives a more in-depth look at the feature.\nI think in general the examples that are detailed in blog posts should find their way back into the developer guide.. @davidgatti \nDo you ever see the network traffic stop? When you pass a callback to an operation, the SDK will buffer the entire response before triggering the callback. Since this is a streaming operation, that could take a long time to complete the stream.\nInstead of passing a callback to the operation, you can use the createReadStream method on the object returned by the operation to stream the response.\nFor example, you could do something like this to save the results to a file:\njavascript\nconst videoStream = kinesisvideomedia.getMedia(options).createReadStream();\nconst fileStream = fs.createWriteStream('/path/to/new/file');\nvideoStream.on('error', (err) => {console.error(err);});\nfileStream.on('error', (err) => {console.error(err);});\nvideoStream.pipe(fileStream);\nCan you try using streams and see if that resolves your issue?. @aviggiano \nI think you're confusing the low-level DynamoDB API client with the DynamoDB Document Client.\nYou can instantiate the low-level DynamoDB API client by writing:\njavascript\nconst DynamoDB = require('aws-sdk/clients/dynamodb');\nconst client = new DynamoDB({/*params */});\n// call updateItem\nclient.updateItem({/*params*/}, function(err, data){});\nDocumentation for updateItem is here, and the typings you shared points to this as well.\nThen there is also the Document Client, which allows you to pass JSON objects directly instead of having to work with DynamoDB AttributeValues.\nYou can create a document client by writing:\njavascript\nconst DynamoDB = require('aws-sdk/clients/dynamodb');\nconst client = new DynamoDB.DocumentClient({/*params*/});\n// call update, which works with JSON objects\nclient.update({/*params*/}, function(err, data){});\nDocumentation for update is here.\nAll of the AWS SDKs have support for the low-level DynamoDB APIs, and that's what DynamoDB will also document. The DocumentClient is a higher-level abstraction that this SDK also provides.\nLet me know if that helps!\n. @bytenaija \nThe Amazon Transcribe service doesn't currently support CORS, so it isn't possible to use it in an environment that enforces it. A temporary workaround would be to write a Lambda function that calls transcribe, and either invoke it directly from the front-end or access it through API Gateway.\nI have notified the service team about your request to support CORS, but feel free to also post to their forums here.. Thanks for the PR, merging!. @mim-Armand \nThe response structure is defined by the service team. The SDK generally abstracts what the request/response over the wire looks like so you can work with JSON all the way, but if you want to see what should be sent over the wire, the service teams' documentation contains samples and documentation on the fields:\nhttps://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\nSample Response\nxml\n<AssumeRoleResponse xmlns=\"https://sts.amazonaws.com/doc/2011-06-15/\">\n  <AssumeRoleResult>\n    <Credentials>\n      <SessionToken>\n       AQoDYXdzEPT//////////wEXAMPLEtc764bNrC9SAPBSM22wDOk4x4HIZ8j4FZTwdQW\n       LWsKWHGBuFqwAeMicRXmxfpSPfIeoIYRqTflfKD8YUuwthAx7mSEI/qkPpKPi/kMcGd\n       QrmGdeehM4IC1NtBmUpp2wUE8phUZampKsburEDy0KPkyQDYwT7WZ0wq5VSXDvp75YU\n       9HFvlRd8Tx6q6fE8YQcHNVXAkiY9q6d+xo0rKwT38xVqr7ZD0u0iPPkUL64lIZbqBAz\n       +scqKmlzm8FDrypNC9Yjc8fPOLn9FX9KSYvKTr4rvx3iSIlTJabIQwj2ICCR/oLxBA==\n      </SessionToken>\n      <SecretAccessKey>\n       wJalrXUtnFEMI/K7MDENG/bPxRfiCYzEXAMPLEKEY\n      </SecretAccessKey>\n      <Expiration>2011-07-15T23:28:33.359Z</Expiration>\n      <AccessKeyId>ASIAIOSFODNN7EXAMPLE</AccessKeyId>\n    </Credentials>\n    <AssumedRoleUser>\n      <Arn>arn:aws:sts::123456789012:assumed-role/demo/Bob</Arn>\n      <AssumedRoleId>ARO123EXAMPLE123:Bob</AssumedRoleId>\n    </AssumedRoleUser>\n    <PackedPolicySize>6</PackedPolicySize>\n  </AssumeRoleResult>\n  <ResponseMetadata>\n    <RequestId>c6104cbe-af31-11e0-8154-cbc7ccf896c7</RequestId>\n  </ResponseMetadata>\n</AssumeRoleResponse>. @ArjunMani \nThere are 2 ways to performa a multi-part download form S3, both using the getObject method.\nIf an object was uploaded using a multi-part upload, then you can use the PartNumber field in the getObject parameters to request a specific part. You can tell if this is the case be calling headObject first and checking the PartsCount field in the response.\nAlternatively, you can specify a range of bytes to download and make the getObject call multiple times. There's an example in the documentation\n\nvar params = {\n  Bucket: \"examplebucket\", \n  Key: \"SampleFile.txt\", \n  Range: \"bytes=0-9\"\n };\n s3.getObject(params, function(err, data) {\n   if (err) console.log(err, err.stack); // an error occurred\n   else     console.log(data);           // successful response\n   /\n   data = {\n    AcceptRanges: \"bytes\", \n    ContentLength: 10, \n    ContentRange: \"bytes 0-9/43\", \n    ContentType: \"text/plain\", \n    ETag: \"\\\"0d94420ffd0bc68cd3d152506b97a9cc\\\"\", \n    LastModified: , \n    Metadata: {\n    }, \n    VersionId: \"null\"\n   }\n   /\n });\n. Closing since we have replaced XMLBuilder for serializing XML.. @warmjaijai \nWhich version of the SDK are you using? Can you also share the stack trace from the error?\n\nOne thing, it looks like you're using the DocumentClient the way you'd expect to use the low-level DynamoDB client. The DocumentClient will convert your standard JSON into the DynamoDB AttributeValues that the low-level client expects for you, so instead of\njavascript\nvar dynamoDB_params = {\n      TableName: table_name,\n      Item:{\n         \"year\": {\n            S: year,\n         },\n         \"title\": {\n            S: title,\n         },\n         \"info\": {\n            M: {\n               \"plot\": {\n                  S: \"Nothing happens at all.\"\n               },\n               \"rating\": {\n                  N: \"0\"\n               }\n            }\n         }\n      }\n   }\nyou could just pass:\njavascript\nvar dynamoDB_params = {\n      TableName: table_name,\n      Item: {\n        \"year\": year,\n        \"title\": title,\n        \"info\": {\n          \"plot\": \"Nothing happens at all.\",\n          \"rating\": 0\n      }\n}\nThe put operation is also async. The return value of put will actually be an AWS.Request object. It's possible that response.json can't handle the Request object. There are a few ways to get the result from the put operation, using callbacks or promises:\n```javascript\n// callback example\ndocumentClient.put(/parameters/, function(err, data) {\n  if (err) {\n    // handle error\n  } else {\n    response.json(data);\n  }\n});\n// promise example\ndocumentClient.put(/parameters/).promise().then(function(data) {\n  response.json(data);\n}).catch(function(err) {\n  // handle errros\n});\n``\n. @meetzaveri \nWhat version of the SDK are you using? Also, what environment are you running this in?\nI just ranadminCreateUser` using the latest version of the SDK in node.js and am not encountering an error.\n. @meetzaveri \nIt looks like the adminCreateUser operation was added in version 2.6.7 of the SDK.\nhttps://github.com/aws/aws-sdk-js/blob/master/CHANGELOG.md#267. tstest job is failing, but will be fixed by #2045 . @ashemedai \nSorry for the delay in reviewing. Thanks for submitting a PR! We are merging this in now!. @anandwahed and @nakulp007 \nSorry for the delayed response. Can you share which versions of aws-amplify and aws-sdk-js you're using? \n@nakulp007 Does your backup copy of node_modules have amazon-cognito-identity-js? And if so, which version?. @AlvaroBernalG \nThe endpoint the service client uses is resolved when you instantiate the client. Since you're updating the region after instantiation, the endpoint isn't resolved correctly.\nIf you add the region to the object you pass to the constructor, you should get the correct endpoint:\njavascript\nconst ec2 = new AWS.EC2({ apiVersion: '2016-11-15', region: 'eu-west-2'}). @AlvaroBernalG \nI'm sure this will help someone in the future too. Glad it's working now!. @eldyvoon \nThe region you are specifying isn't valid. I think you mean to use ap-southeast-1, not s3-ap-southeast-1.  I suspect when you don't have a region specified that the SDK is determining the correct region by defaulting to the global endpoint, but when you specify the s3-ap-southeast-1 region the endpoint that is constructed is invalid so it can't redirect.\nThere's a list of all the S3 regions and endpoints here. . @Rads999 \nThe service returns a response before the message is received by all recipients. You can view logs/metrics for SMS deliveries to check whether they have been delivered or not, and this includes the error message if they are not delivered:\nhttps://docs.aws.amazon.com/sns/latest/dg/sms_stats_cloudwatch.html\nBased on what you see in your logs, you will want to reach out either to AWS Support or to the SNS team via their forums to determine why messages are not being received.. @Rads999 \nThe service returns a response before the message is received by all recipients. You can view logs/metrics for SMS deliveries to check whether they have been delivered or not, and this includes the error message if they are not delivered:\nhttps://docs.aws.amazon.com/sns/latest/dg/sms_stats_cloudwatch.html\nBased on what you see in your logs, you will want to reach out either to AWS Support or to the SNS team via their forums to determine why messages are not being received.. @Rads999 \nEmailMessage isn't one of the listed MessageConfiguration options:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Pinpoint.html#sendMessages-property\nIt looks like DefaultMessage is the right one to use based on it's description for Body:\n\nThe message body of the notification, the email body or the text message.\n\nIf removing EmailMessage doesn't work, you may need to reach out to the service team on their forums. They should be able to provide guidance on how to use the API for your use case.. @Rads999 \nEmailMessage isn't one of the listed MessageConfiguration options:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Pinpoint.html#sendMessages-property\nIt looks like DefaultMessage is the right one to use based on it's description for Body:\n\nThe message body of the notification, the email body or the text message.\n\nIf removing EmailMessage doesn't work, you may need to reach out to the service team on their forums. They should be able to provide guidance on how to use the API for your use case.. @keithdarragh \nCan you share an example of how you're calling the operation?. @keithdarragh \nCan you share an example of how you're calling the operation?. What does your getBinary function look like? Is it simply something like:\njavascript\nreturn Buffer.from(image, 'base64');\nCan you confirm that the Bytes fields are byte arrays?\nI wasn't able to reproduce your issue using the following where both images are less than 5 MB:\n```javascript\nasync function run() {\n    const result = await rekognition.compareFaces({\n        SimilarityThreshold: 50,\n        SourceImage: {\n            Bytes: fs.readFileSync(path.join(__dirname, 'face1.jpg'))\n        },\n        TargetImage: {\n            Bytes: fs.readFileSync(path.join(__dirname, 'face2.jpg'))\n        }\n    }).promise();\nconsole.log(result);\n\n}\n. What does your `getBinary` function look like? Is it simply something like:javascript\nreturn Buffer.from(image, 'base64');\n```\nCan you confirm that the Bytes fields are byte arrays?\nI wasn't able to reproduce your issue using the following where both images are less than 5 MB:\n```javascript\nasync function run() {\n    const result = await rekognition.compareFaces({\n        SimilarityThreshold: 50,\n        SourceImage: {\n            Bytes: fs.readFileSync(path.join(__dirname, 'face1.jpg'))\n        },\n        TargetImage: {\n            Bytes: fs.readFileSync(path.join(__dirname, 'face2.jpg'))\n        }\n    }).promise();\nconsole.log(result);\n\n}\n```. @harishankards \nHave you taken a look at the createPresignedPost API? This looks like it may suit your use-case.. @harishankards \nHave you taken a look at the createPresignedPost API? This looks like it may suit your use-case.. @Misiur \nNope, you're right, it doesn't currently support promise, but it could. Since it doesn't currently return anything, we could probably just make it return a promise directly if no callback is provided, and continue to return undefined otherwise. Marking as a feature request, but if you feel inclined would be happy to review a PR for this!. @Misiur \nNope, you're right, it doesn't currently support promise, but it could. Since it doesn't currently return anything, we could probably just make it return a promise directly if no callback is provided, and continue to return undefined otherwise. Marking as a feature request, but if you feel inclined would be happy to review a PR for this!. @Prashant-Yadav \nThe Etag returned my S3 is not guaranteed to be the MD5 checksum of the entire object. Notably, for multipart uploads1, the Etag might be different when downloading.\nYou could calculate an MD5 or sha256 checksum for the file and save it as part of the object metadata, then calculate/compare that value when you download it.\nThe SDK also doesn't compare checksums on download (currently), so this seems like a feature that may be built into the library you're using. Please open an issue there. If you still believe there is a bug in this package, please provide the error you're seeing and if possible a minimal code sample to reproduce.. @Prashant-Yadav \nThe Etag returned my S3 is not guaranteed to be the MD5 checksum of the entire object. Notably, for multipart uploads1, the Etag might be different when downloading.\nYou could calculate an MD5 or sha256 checksum for the file and save it as part of the object metadata, then calculate/compare that value when you download it.\nThe SDK also doesn't compare checksums on download (currently), so this seems like a feature that may be built into the library you're using. Please open an issue there. If you still believe there is a bug in this package, please provide the error you're seeing and if possible a minimal code sample to reproduce.. @myst729 \nCan you share what the other script is doing where this ends up being incorrect? I'm just curious what issue is occurring with the this context.\nAgree that I think this change is safe since we're already referencing xhr anyway within the function you're modifying.\nI reran the travis job and everything passed, so your failures were likely some transient issue.. @myst729 \nCan you share what the other script is doing where this ends up being incorrect? I'm just curious what issue is occurring with the this context.\nAgree that I think this change is safe since we're already referencing xhr anyway within the function you're modifying.\nI reran the travis job and everything passed, so your failures were likely some transient issue.. @Vadorequest \nCan you share an example of the code you're calling? What are you doing with the data object returned by the service? For example, are you passing it to console.log, the lambda handler's callback function, or using JSON.stringify?. @Vadorequest \nCan you share an example of the code you're calling? What are you doing with the data object returned by the service? For example, are you passing it to console.log, the lambda handler's callback function, or using JSON.stringify?. @decompil3d \nI think you want to use the AWS.TemporaryCredentials class to get your assume role credentials.\nThis will automatically call STS.assumeRole for you.. @decompil3d \nI think you want to use the AWS.TemporaryCredentials class to get your assume role credentials.\nThis will automatically call STS.assumeRole for you.. Just wanted to add, besides taking care of normalizing the credential fields from STS.assumeRole, the TemporaryCredentials provider also handles refreshing credentials for you automatically.. Just wanted to add, besides taking care of normalizing the credential fields from STS.assumeRole, the TemporaryCredentials provider also handles refreshing credentials for you automatically.. I actually do want to include the model changes with this PR. Removed them from #2074 . I actually do want to include the model changes with this PR. Removed them from #2074 . @jjosef \nWhen calling sftp.get, can you try explicitly calling null for the encoding parameter?\nhttps://www.npmjs.com/package/ssh2-sftp-client#get\nThe docs suggest that it will use UTF-8 encoding by default, so it's possible that when we are reading the stream we are getting strings back instead of buffers.\nIf you call read on the stream from it's readable event, can you tell if you're getting back a string or a buffer?\n. @jjosef \nWhen calling sftp.get, can you try explicitly calling null for the encoding parameter?\nhttps://www.npmjs.com/package/ssh2-sftp-client#get\nThe docs suggest that it will use UTF-8 encoding by default, so it's possible that when we are reading the stream we are getting strings back instead of buffers.\nIf you call read on the stream from it's readable event, can you tell if you're getting back a string or a buffer?\n. @nitrocode \nIt sounds like you're looking for the s3.upload method. Behind the scenes, this method will perform a multi-part upload of your object once it is beyond the threshold specified by partSize (defaults to 5 MB). You can change the queueSize to control how many chunks you want to upload in parallel, and the size of each chunk.. @nitrocode \nIt sounds like you're looking for the s3.upload method. Behind the scenes, this method will perform a multi-part upload of your object once it is beyond the threshold specified by partSize (defaults to 5 MB). You can change the queueSize to control how many chunks you want to upload in parallel, and the size of each chunk.. @ada1013 \nI haven't been able to reproduce your issue. Can you share the stack trace in the error being thrown?\nOne thing you'll want to change when using the DocumentClient is that your ExpressionAttributeValues don't need to be specified using the AttributeValue structure.\nSo instead of ExpressionAttributeValues: {\":id\": {S:\"abc\"}}, you can shorten it to ExpressionAttributeValues: {\":id\": \"abc\"}.\nIf you don't do that, then instead of comparing :id to a string, it will compare it to a JSON object with the field S: 'abc'.. @ada1013 \nI haven't been able to reproduce your issue. Can you share the stack trace in the error being thrown?\nOne thing you'll want to change when using the DocumentClient is that your ExpressionAttributeValues don't need to be specified using the AttributeValue structure.\nSo instead of ExpressionAttributeValues: {\":id\": {S:\"abc\"}}, you can shorten it to ExpressionAttributeValues: {\":id\": \"abc\"}.\nIf you don't do that, then instead of comparing :id to a string, it will compare it to a JSON object with the field S: 'abc'.. Closing this issue since it isn't specific to the SDK.\nFor questions specific to how a service works, please reach out on the aws forums for that service:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=210\n. Closing this issue since it isn't specific to the SDK.\nFor questions specific to how a service works, please reach out on the aws forums for that service:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=210\n. @steveLuo1 \nThe S3 managed upload (s3.upload) requires a readable stream for the Body field as input (when the input is a stream). Internally, the SDK will call read on the the stream that's passed in so it can control how quickly to read data. It needs to do this so that it can control how many uploadPart requests to make at a time (this enables controlling the number of parallel requests to have at a time as well as retrying individual parts without storing the entire object in memory).\nCalling request().get() doesn't provide a true readable stream. Instead, the object returned inherits from the Nodejs Stream class, and then implements methods like write. While it supports the common flowing mode operation with pipe and listening for data events, it does not support paused more with read() since read is not implemented by request.\nYou actually figured out a work-around by piping the stream from request into a PassThrough stream. Since PassThrough streams extend Readable streams, they have the read method defined and can operate fully in paused mode.\nHope my response wasn't too long-winded!\n. @steveLuo1 \nThe S3 managed upload (s3.upload) requires a readable stream for the Body field as input (when the input is a stream). Internally, the SDK will call read on the the stream that's passed in so it can control how quickly to read data. It needs to do this so that it can control how many uploadPart requests to make at a time (this enables controlling the number of parallel requests to have at a time as well as retrying individual parts without storing the entire object in memory).\nCalling request().get() doesn't provide a true readable stream. Instead, the object returned inherits from the Nodejs Stream class, and then implements methods like write. While it supports the common flowing mode operation with pipe and listening for data events, it does not support paused more with read() since read is not implemented by request.\nYou actually figured out a work-around by piping the stream from request into a PassThrough stream. Since PassThrough streams extend Readable streams, they have the read method defined and can operate fully in paused mode.\nHope my response wasn't too long-winded!\n. @Blackbaud-ChristiSchneider \nThe Attributes and ReturnSubscriptionArn properties were added to sns.subscribe in version 2.250.1 of the SDK.\nThe version of the SDK in Lambda is not updated as frequently as SDKs are released. It looks like as of right now, version 2.249.1 of the SDK is what's included in Lambda:\nhttps://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html\nIf you want to use a newer version of the SDK, you can upload a project to Lambda that includes a copy of the aws-sdk in it.. @Blackbaud-ChristiSchneider \nThe Attributes and ReturnSubscriptionArn properties were added to sns.subscribe in version 2.250.1 of the SDK.\nThe version of the SDK in Lambda is not updated as frequently as SDKs are released. It looks like as of right now, version 2.249.1 of the SDK is what's included in Lambda:\nhttps://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html\nIf you want to use a newer version of the SDK, you can upload a project to Lambda that includes a copy of the aws-sdk in it.. @Nop0x \nThanks for reporting this issue.\nThis is currently a limitation in the JavaScript SDK due to how we remove exceptions from our service models, so the SDK doesn't know how to parse the response body to provide additional details. Now that additional details are returned in error messages by some services, we'll have to revisit that decision.\nAs a work-around, you can parse the response message:\njavascript\nconst request = client.deleteIntent(/* params */);\n// send request using .send or .promise\nrequest.send((err, data) => {\n  if (err) {\n    switch (err.name) {\n      case 'ResourceInUseException':\n        // JSON parse string to get additional details, or log it\n        console.error(request.response.httpResponse.body.toString());\n        return;\n      default:\n        console.error(err);\n        return;\n    }\n  }\n});\nWe may be able to do something similar if we can tell message is null.. @Nop0x \nThanks for reporting this issue.\nThis is currently a limitation in the JavaScript SDK due to how we remove exceptions from our service models, so the SDK doesn't know how to parse the response body to provide additional details. Now that additional details are returned in error messages by some services, we'll have to revisit that decision.\nAs a work-around, you can parse the response message:\njavascript\nconst request = client.deleteIntent(/* params */);\n// send request using .send or .promise\nrequest.send((err, data) => {\n  if (err) {\n    switch (err.name) {\n      case 'ResourceInUseException':\n        // JSON parse string to get additional details, or log it\n        console.error(request.response.httpResponse.body.toString());\n        return;\n      default:\n        console.error(err);\n        return;\n    }\n  }\n});\nWe may be able to do something similar if we can tell message is null.. @Nop0x \nYou can control retry delays for a specific service client by passing in retryDelayOptions to the configuration you pass into the constructor. This would affect retry delays for any operation you call using that client.\nIf you need to control the retry delay for a specific operation request, you can do that as well by adding a retry listener to the request object.\n. @Nop0x \nYou can control retry delays for a specific service client by passing in retryDelayOptions to the configuration you pass into the constructor. This would affect retry delays for any operation you call using that client.\nIf you need to control the retry delay for a specific operation request, you can do that as well by adding a retry listener to the request object.\n. @lenin-jaganathan \nAre you checking if the object is deleted immediately after calling deleteObject?\nAmazon S3 uses eventual consistency for DELETES, so it's possible that trying to retrieve or list an object that's been deleted will return the deleted data until the change is fully propagated across their servers.\nDoes the object eventually become deleted?. @lenin-jaganathan \nAre you checking if the object is deleted immediately after calling deleteObject?\nAmazon S3 uses eventual consistency for DELETES, so it's possible that trying to retrieve or list an object that's been deleted will return the deleted data until the change is fully propagated across their servers.\nDoes the object eventually become deleted?. @lenin-jaganathan \nThis should probably be a separate issue. Can you open another one and share an example of how you're uploading your object to S3?. @lenin-jaganathan \nThis should probably be a separate issue. Can you open another one and share an example of how you're uploading your object to S3?. @GlauberF \nThat error typically occurs when your system's clock is skewed from what the service clock is (assuming UTC time on both ends).\nTo see if that's the case, can you try setting the correctClockSkew property to true when instantiating your SES client? . @GlauberF \nThat error typically occurs when your system's clock is skewed from what the service clock is (assuming UTC time on both ends).\nTo see if that's the case, can you try setting the correctClockSkew property to true when instantiating your SES client? . @GlauberF \nGlad to hear! I'll close the issue then, but feel free to open a new issue if you encounter errors.. @GlauberF \nGlad to hear! I'll close the issue then, but feel free to open a new issue if you encounter errors.. @justinmchase \nBased on the error message, it looks like the SDK is unable to resolve your credentials using the default credential provider chain. The last credential provider the default provider chain attempts to use is the EC2/ECS metadata credential provider. What you're seeing is the error the metadata credential provider encountered (it's unable to connect to the metadata service).\nCan you try providing credentials to your client as well? I'm not familiar with SimpleDB, so I don't know if AWS credentials are required if you're accessing the local client. If they aren't, you could try passing fake credentials, which is what I think the local DynamoDB client requires.. @justinmchase \nBased on the error message, it looks like the SDK is unable to resolve your credentials using the default credential provider chain. The last credential provider the default provider chain attempts to use is the EC2/ECS metadata credential provider. What you're seeing is the error the metadata credential provider encountered (it's unable to connect to the metadata service).\nCan you try providing credentials to your client as well? I'm not familiar with SimpleDB, so I don't know if AWS credentials are required if you're accessing the local client. If they aren't, you could try passing fake credentials, which is what I think the local DynamoDB client requires.. The IP address is actually for the Metadata service that's available when running your application in an EC2 instance or ECS container:\nhttps://github.com/aws/aws-sdk-js/blob/cc29728c1c4178969ebabe3bbe6b6f3159436394/lib/metadata_service.js#L25\nAs part of sending any request, the SDK will validate if credentials are provided. If they are not, it will attempt to resolve credentials using the default credential provider chain. This step will occur no matter where your endpoint is configured to point to.\nIf you're absolutely sure AWS credentials are not needed, it is possible to remove the credentials check:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EventListeners/Core.html#VALIDATE_CREDENTIALS-property\nHowever, note that this will remove the check from every request the SDK makes. If you wanted to remove them from just SimpleDB requests, you would need to remove the listener on the request returned by each operation call.\nYou can also try setting a fake credentials object.. The IP address is actually for the Metadata service that's available when running your application in an EC2 instance or ECS container:\nhttps://github.com/aws/aws-sdk-js/blob/cc29728c1c4178969ebabe3bbe6b6f3159436394/lib/metadata_service.js#L25\nAs part of sending any request, the SDK will validate if credentials are provided. If they are not, it will attempt to resolve credentials using the default credential provider chain. This step will occur no matter where your endpoint is configured to point to.\nIf you're absolutely sure AWS credentials are not needed, it is possible to remove the credentials check:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EventListeners/Core.html#VALIDATE_CREDENTIALS-property\nHowever, note that this will remove the check from every request the SDK makes. If you wanted to remove them from just SimpleDB requests, you would need to remove the listener on the request returned by each operation call.\nYou can also try setting a fake credentials object.. @ttulka \nSince temporary credentials are resolved asynchronously, you should be passing a callback to the s3.getSignedUrl method:\n\nNote: You must ensure that you have static or previously resolved credentials if you call this method synchronously (with no callback), otherwise it may not properly sign the request. If you cannot guarantee this (you are using an asynchronous credential provider, i.e., EC2 IAM roles), you should always call this method with an asynchronous callback.\n\nI am surprised it's returning a string at all when credentials have not been resolved yet, so that's something we'll need to look into, but can you try switching to using a callback and see if that provides the correct URL?. @ttulka \nSince temporary credentials are resolved asynchronously, you should be passing a callback to the s3.getSignedUrl method:\n\nNote: You must ensure that you have static or previously resolved credentials if you call this method synchronously (with no callback), otherwise it may not properly sign the request. If you cannot guarantee this (you are using an asynchronous credential provider, i.e., EC2 IAM roles), you should always call this method with an asynchronous callback.\n\nI am surprised it's returning a string at all when credentials have not been resolved yet, so that's something we'll need to look into, but can you try switching to using a callback and see if that provides the correct URL?. @korya \nIs that 1.5 MB with just the S3 client in your SDK? If you haven't already, you can build a version of the SDK with just S3, using either the browser builder UI/CLI:\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/building-sdk-for-browsers.html\nOr by using a bundler like Webpack or Browserify and importing just the S3 client:\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/webpack.html\nThat said, we do realize that the core of the SDK is large, and are working on addressing that. I can't provide an ETA on when that work will be done, but it is something we plan to improve on.. @korya \nIs that 1.5 MB with just the S3 client in your SDK? If you haven't already, you can build a version of the SDK with just S3, using either the browser builder UI/CLI:\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/building-sdk-for-browsers.html\nOr by using a bundler like Webpack or Browserify and importing just the S3 client:\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/webpack.html\nThat said, we do realize that the core of the SDK is large, and are working on addressing that. I can't provide an ETA on when that work will be done, but it is something we plan to improve on.. @warnerpinz \nWhat are the parameters you're passing in your receiveMessage call?\nWas this a one-time issue, or do you see this every time with the same parameters? When you encounter the error, it may be helpful if you also provide the body sent to the service. Here's an example of how to get that:\njavascript\nsqs.receiveMessage({/** params **/}, function(err, data) {\n  console.log(this.request.httpRequest.body.toString());\n});\nIf you're using AttributeNames or MessageAttributeNames, we want to make sure that the parameters in the body start at index 1.\nIf you can also share which version of the SDK you're using, as well as the runtime environment, that may help.\n. @warnerpinz \nWhat are the parameters you're passing in your receiveMessage call?\nWas this a one-time issue, or do you see this every time with the same parameters? When you encounter the error, it may be helpful if you also provide the body sent to the service. Here's an example of how to get that:\njavascript\nsqs.receiveMessage({/** params **/}, function(err, data) {\n  console.log(this.request.httpRequest.body.toString());\n});\nIf you're using AttributeNames or MessageAttributeNames, we want to make sure that the parameters in the body start at index 1.\nIf you can also share which version of the SDK you're using, as well as the runtime environment, that may help.\n. @FabricioFFC \nThe SDK determines if credentials can be ignored for an operation if it is modeled with the authtype: 'none' trait:\nhttps://github.com/aws/aws-sdk-js/blob/a56b2c08ba4bb045725c6e5b40e2aa667507e84b/apis/cognito-idp-2016-04-18.normal.json#L571\nIt's up to the service team to decide which operations can be excluded from requiring credentials, so you might want to ask on the Amazon Cognito forums on if this should be possible.\nWe can update your documentation to explicitly call out when credentials are being used or not.\nIf you're absolutely sure AWS credentials are not needed, it is possible to remove the credentials check:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EventListeners/Core.html#VALIDATE_CREDENTIALS-property\nHowever, note that this will remove the check from every request the SDK makes. If you wanted to remove them from just CognitoIdentityServiceProvider requests, you would need to remove the listener on the request returned by each operation call.. @FabricioFFC \nThe SDK determines if credentials can be ignored for an operation if it is modeled with the authtype: 'none' trait:\nhttps://github.com/aws/aws-sdk-js/blob/a56b2c08ba4bb045725c6e5b40e2aa667507e84b/apis/cognito-idp-2016-04-18.normal.json#L571\nIt's up to the service team to decide which operations can be excluded from requiring credentials, so you might want to ask on the Amazon Cognito forums on if this should be possible.\nWe can update your documentation to explicitly call out when credentials are being used or not.\nIf you're absolutely sure AWS credentials are not needed, it is possible to remove the credentials check:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EventListeners/Core.html#VALIDATE_CREDENTIALS-property\nHowever, note that this will remove the check from every request the SDK makes. If you wanted to remove them from just CognitoIdentityServiceProvider requests, you would need to remove the listener on the request returned by each operation call.. @ninthz \nUnfortunately this is one of our known limitations regarding the TypeScript definitions we generate.\nI'm not sure if there's a great way to solve this in the SDK, since we do want the parameters for each operation to indicate which fields are required.\nA possible work-around on your end would be to do something like:\ntypescript\nlet params: Partial<S3.Types.ListObjectsV2Request> = {StartAfter: this.uploadDir};\nreturn this.s3.listObjectsV2(params as any).promise();\nThis would give you some type-safety when you're assigning to your params object.. @ninthz \nUnfortunately this is one of our known limitations regarding the TypeScript definitions we generate.\nI'm not sure if there's a great way to solve this in the SDK, since we do want the parameters for each operation to indicate which fields are required.\nA possible work-around on your end would be to do something like:\ntypescript\nlet params: Partial<S3.Types.ListObjectsV2Request> = {StartAfter: this.uploadDir};\nreturn this.s3.listObjectsV2(params as any).promise();\nThis would give you some type-safety when you're assigning to your params object.. Surprisingly Safari is the one that was supposed to have issues with children, though I wasn't able to get solid stats on mobile devices either.. Surprisingly Safari is the one that was supposed to have issues with children, though I wasn't able to get solid stats on mobile devices either.. parentNode is available in mobile browsers, it's children that was unclear, which is partially why I didn't use that.. parentNode is available in mobile browsers, it's children that was unclear, which is partially why I didn't use that.. @jpb \nThis looks like an awesome change, thank you for the submission!\nI'll take a thorough look at your PR and run against our integration tests, since this is changing how a credential provider works we'll want to be extra careful.\nYour approach looks really solid. That's odd about the failing test, I'll look into what that was supposed to be testing and why it isn't failing now.. @jpb \nThis looks like an awesome change, thank you for the submission!\nI'll take a thorough look at your PR and run against our integration tests, since this is changing how a credential provider works we'll want to be extra careful.\nYour approach looks really solid. That's odd about the failing test, I'll look into what that was supposed to be testing and why it isn't failing now.. Merging! Thank you for the contribution, and good job with your first PR to the SDK!. Merging! Thank you for the contribution, and good job with your first PR to the SDK!. @ddemoll \nPlease post your feedback about Amazon Pinpoint on their AWS forums page:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=225\nTheir service team monitors the forums and I'm sure would be happy to hear your feedback.. @ddemoll \nPlease post your feedback about Amazon Pinpoint on their AWS forums page:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=225\nTheir service team monitors the forums and I'm sure would be happy to hear your feedback.. @donglaizhang \nIn nodejs, the SDK has a default setting where it will only allow 50 connections to a host a time. However, that is configurable by changing the maxSockets setting of your Lambda client:\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/node-configuring-maxsockets.html\nFor your other question, it is possible to invoke lambda functions directly using their REST API, but you would need to be able to sign your request using sigv4 signing (the SDK handles that for you):\nhttps://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html\nhttps://docs.aws.amazon.com/general/latest/gr/signature-version-4.html. @donglaizhang \nIn nodejs, the SDK has a default setting where it will only allow 50 connections to a host a time. However, that is configurable by changing the maxSockets setting of your Lambda client:\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/node-configuring-maxsockets.html\nFor your other question, it is possible to invoke lambda functions directly using their REST API, but you would need to be able to sign your request using sigv4 signing (the SDK handles that for you):\nhttps://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html\nhttps://docs.aws.amazon.com/general/latest/gr/signature-version-4.html. @enGMzizo \nSince you're using the default credential provider to retrieve EC2 Metadata Credentials, and each instance of a service client has its own copy of configuration, your first sample would lead to more calls to the EC2 metadata service.\nAssuming that each call your wrapper class makes to the document client should have the same configuration (region/credentials/etc), it makes sense to instantiate your DocumentClient once per instance so you aren't duplicating work.\nIf you wanted to use separate DocumentClients for each call (this would be needed if each one had to be configured differently, e.g. to support different regions with the same instance of your wrapper class), you could also instantiate an EC2MetadataCredentials provider explicitly and pass that into your DocumentClient when you instantiate it:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EC2MetadataCredentials.html. @enGMzizo \nSince you're using the default credential provider to retrieve EC2 Metadata Credentials, and each instance of a service client has its own copy of configuration, your first sample would lead to more calls to the EC2 metadata service.\nAssuming that each call your wrapper class makes to the document client should have the same configuration (region/credentials/etc), it makes sense to instantiate your DocumentClient once per instance so you aren't duplicating work.\nIf you wanted to use separate DocumentClients for each call (this would be needed if each one had to be configured differently, e.g. to support different regions with the same instance of your wrapper class), you could also instantiate an EC2MetadataCredentials provider explicitly and pass that into your DocumentClient when you instantiate it:\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EC2MetadataCredentials.html. @tzookb \nWith the DocumentClient, you don't need to define attribute values anywhere, including ExpressionAttributeValues. Instead of your current example:\njavascript\nExpressionAttributeValues: {\n  ':value': {S: email}\n}\nyou would want to write:\njavascript\nExpressionAttributeValues: {\n  ':value': email\n}. @tzookb \nWith the DocumentClient, you don't need to define attribute values anywhere, including ExpressionAttributeValues. Instead of your current example:\njavascript\nExpressionAttributeValues: {\n  ':value': {S: email}\n}\nyou would want to write:\njavascript\nExpressionAttributeValues: {\n  ':value': email\n}. @ihd2911 \nSorry for the late response.\nOne thing I want to point out is that the credentials you supply in your first aws.config.update statement get immediately overridden when you set aws.config.credentials to a CognitoIdentityCredentials provider.\nDoes the role that your identity pool has access to also have the ability to assume the role you've defined in your RoleArn parameter? Based on the error message it seems it may not.. @ihd2911 \nSorry for the late response.\nOne thing I want to point out is that the credentials you supply in your first aws.config.update statement get immediately overridden when you set aws.config.credentials to a CognitoIdentityCredentials provider.\nDoes the role that your identity pool has access to also have the ability to assume the role you've defined in your RoleArn parameter? Based on the error message it seems it may not.. @cronvel \nAt least with S3, the length of the object is required to be known. The SDK can calculate that if it's given a filestream, or a non-streaming body, but won't be able to determine what it is for a putObject request otherwise.\nIf you need to work with streams where the content-length is unknown, you can also use the s3.upload method. That method will perform a multi-part upload (so DO would need to support that functionality) and buffer 5MB (configurable, that's the default value) chunks of your stream to perform the upload.\nIf you want to get the raw response from the server, you can access that in your first example by doing something like:\njavascript\ns3.putObject({/** params **/, function(err, data) {\n  // 'this' is the AWS Response object\n  // (https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Response.html)\n  if (err) {\n    console.log(this.httpResponse.body.toString());\n    console.log(this.httpResponse.headers);\n  }\n});\nI'm going to close this for now since this seems like an issue when interacting with DO, but if you do encounter this issue when using S3 please open another issue!. @cronvel \nAt least with S3, the length of the object is required to be known. The SDK can calculate that if it's given a filestream, or a non-streaming body, but won't be able to determine what it is for a putObject request otherwise.\nIf you need to work with streams where the content-length is unknown, you can also use the s3.upload method. That method will perform a multi-part upload (so DO would need to support that functionality) and buffer 5MB (configurable, that's the default value) chunks of your stream to perform the upload.\nIf you want to get the raw response from the server, you can access that in your first example by doing something like:\njavascript\ns3.putObject({/** params **/, function(err, data) {\n  // 'this' is the AWS Response object\n  // (https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Response.html)\n  if (err) {\n    console.log(this.httpResponse.body.toString());\n    console.log(this.httpResponse.headers);\n  }\n});\nI'm going to close this for now since this seems like an issue when interacting with DO, but if you do encounter this issue when using S3 please open another issue!. @ANTGOMEZ \nI suspect the issue is because you are not awaiting the S3 putObject command, and you have context.callbackWaitsForEmptyEventLoop = false;\nThe putObjectToS3 method will run asynchronously when you call it within your async handler and then your handler will immediately resolve. Since you've turned off waiting for an empty event loop before freezing your lambda function, your Lambda function is likely freezing before the full call to S3 is made.\nYou can call S3 putObject such that it returns a promise, then await that within your handler:\n```javascript\nfunction upload() {\n return s3.putObject(/params/).promise();\n}\nasync test() {\n  await upload();\n}\n``. @ANTGOMEZ \nI suspect the issue is because you are notawaiting the S3 putObject command, and you havecontext.callbackWaitsForEmptyEventLoop = false;`\nThe putObjectToS3 method will run asynchronously when you call it within your async handler and then your handler will immediately resolve. Since you've turned off waiting for an empty event loop before freezing your lambda function, your Lambda function is likely freezing before the full call to S3 is made.\nYou can call S3 putObject such that it returns a promise, then await that within your handler:\n```javascript\nfunction upload() {\n return s3.putObject(/params/).promise();\n}\nasync test() {\n  await upload();\n}\n``. Hi @ffxsam \nThis is currently a known limitation with our typings. Please see #2123 for more information.. Hi @ffxsam \nThis is currently a known limitation with our typings. Please see #2123 for more information.. @stripathix \nCan you share how you've configured your credential provider, and which one you're using? Most providers should automatically refresh when the credentials have expired.. @stripathix \nCan you share how you've configured your credential provider, and which one you're using? Most providers should automatically refresh when the credentials have expired.. @stripathix \nWhen do you see this error? Does it happen the first time you callAWS.config.credentials.get`, or after some time?\nSince the error says that your token has expired, I suspect that your user pool token eventually expires. The CognitoIdentityCredentials provider doesn't manage refreshing tokens:\n\nIn addition to AWS credentials expiring after a given amount of time, the login token from the identity provider will also expire. Once this token expires, it will not be usable to refresh AWS credentials, and another token will be needed.\n\n. @stripathix \nWhen do you see this error? Does it happen the first time you call AWS.config.credentials.get, or after some time?\nSince the error says that your token has expired, I suspect that your user pool token eventually expires. The CognitoIdentityCredentials provider doesn't manage refreshing tokens:\n\nIn addition to AWS credentials expiring after a given amount of time, the login token from the identity provider will also expire. Once this token expires, it will not be usable to refresh AWS credentials, and another token will be needed.\n\n. @lpender \nThe secretAccessKey is set so that it doesn't show up when logging the credentials object (to prevent it from accidentally showing up in logs). However, you can still access the field and log it if you log that field directly. Can you confirm it exists when you log AWS.config.credentials.secretAccessKey?\nThe message makes it seem like a session token is needed as well. Are you using temporary credentials by chance?. @lpender \nThe secretAccessKey is set so that it doesn't show up when logging the credentials object (to prevent it from accidentally showing up in logs). However, you can still access the field and log it if you log that field directly. Can you confirm it exists when you log AWS.config.credentials.secretAccessKey?\nThe message makes it seem like a session token is needed as well. Are you using temporary credentials by chance?. @kishoredonepudi \nIt looks like you are reaching the services since you're getting a response. Can you share how you're making your SNS calls, and how you've configured your SNS client?. @kishoredonepudi \nIt looks like you are reaching the services since you're getting a response. Can you share how you're making your SNS calls, and how you've configured your SNS client?. @kishoredonepudi \nDo you see your subscriptionArn when you call sns.listSubscriptions, or do you also get an error via that request?\nThe 404 errors indicate that the resources you're trying to act on don't exist. How are you validating that the resources exist?. @kishoredonepudi \nDo you see your subscriptionArn when you call sns.listSubscriptions, or do you also get an error via that request?\nThe 404 errors indicate that the resources you're trying to act on don't exist. How are you validating that the resources exist?. @kishoredonepudi \nAre you able to unsubscribe to the same arn that fails with the JS SDK from by using the CLI? If so, then we can take a look at the raw http request/response from both tools to see where the issue is. Otherwise you may need to reach out on the AWS SNS forums. @kishoredonepudi \nAre you able to unsubscribe to the same arn that fails with the JS SDK from by using the CLI? If so, then we can take a look at the raw http request/response from both tools to see where the issue is. Otherwise you may need to reach out on the AWS SNS forums. @mclaborn \nJust for clarity, were you trying to set metadata or tags? There is an example for how to set tags when using the ManagedUpload class directly. You can also define the tags in the options object passed to upload:\njavascript\ns3.upload(\n  {Bucket: 'bucket', Key: 'key', Body: stream},\n  {tags: [{Key: 'tag1', Value: 'value1'}]}\n);\nIt does look like upload is missing documentation for the options object.. @mclaborn \nJust for clarity, were you trying to set metadata or tags? There is an example for how to set tags when using the ManagedUpload class directly. You can also define the tags in the options object passed to upload:\njavascript\ns3.upload(\n  {Bucket: 'bucket', Key: 'key', Body: stream},\n  {tags: [{Key: 'tag1', Value: 'value1'}]}\n);\nIt does look like upload is missing documentation for the options object.. @zhiwei-nu \nWhen you're using asynchronous credentials (which is the case when assuming a role), you must provide a callback to getSignedUrl to get the proper URL.\nSee this comment for more information.. @zhiwei-nu \nWhen you're using asynchronous credentials (which is the case when assuming a role), you must provide a callback to getSignedUrl to get the proper URL.\nSee this comment for more information.. @sul4bh \nSorry for that, some unused shapes made it into our generated typings, we're currently working on a short-term fix and will follow-up with something more long-term: #2174 . @sul4bh \nSorry for that, some unused shapes made it into our generated typings, we're currently working on a short-term fix and will follow-up with something more long-term: #2174 . Version 2.286.2 has been released. Please upgrade, you should no longer see this error.. Version 2.286.2 has been released. Please upgrade, you should no longer see this error.. @jstewmon \nThank you for the pull request! Sorry I haven't had time yet to give this the attention it deserves, but I will take a look in the next couple of days. Since it's touching credentials we'll want to be extra thorough in our testing to make sure this doesn't introduce a regression.. @jstewmon \nThank you for the pull request! Sorry I haven't had time yet to give this the attention it deserves, but I will take a look in the next couple of days. Since it's touching credentials we'll want to be extra thorough in our testing to make sure this doesn't introduce a regression.. @jstewmon \nJust updating to let you know I've started reviewing your changes, and have been asking around about when we would want to collapse our masterCredentials (in that while loop you removed). I'll make sure feedback is posted tomorrow. I really like some of the other changes you've incorporated, such as refreshing credentials fix as well.. @jstewmon \nJust updating to let you know I've started reviewing your changes, and have been asking around about when we would want to collapse our masterCredentials (in that while loop you removed). I'll make sure feedback is posted tomorrow. I really like some of the other changes you've incorporated, such as refreshing credentials fix as well.. @jstewmon \nI reached out to some members of the team to find out why we were squashing masterCredentials.masterCredentials, and it sounds like that was originally put in before assumeRole existed. This was done so that if you got temporary credentials, you could pass those to another TemporaryCredentials provider and when the credentials expired they would be refreshed from the original set of credentials.\nUnfortunately, this behavior wasn't changed when assumeRole was added, and it's possible that changing the behavior now could break users. Given that it's been implemented this way for years, it's hard to fix this without a major version bump (even though I agree the current behavior could be considered a bug). My proposal then is that we take your changes and make a new TemporaryCredentials provider, and deprecate the existing one. That seems like the safest way to make this change.\nWould you be open to implementing this as a separate class? It should pretty much be a copy paste into a new file with a new class name. @AllanFly120 thoughts on what it should be named?. @jstewmon \nI reached out to some members of the team to find out why we were squashing masterCredentials.masterCredentials, and it sounds like that was originally put in before assumeRole existed. This was done so that if you got temporary credentials, you could pass those to another TemporaryCredentials provider and when the credentials expired they would be refreshed from the original set of credentials.\nUnfortunately, this behavior wasn't changed when assumeRole was added, and it's possible that changing the behavior now could break users. Given that it's been implemented this way for years, it's hard to fix this without a major version bump (even though I agree the current behavior could be considered a bug). My proposal then is that we take your changes and make a new TemporaryCredentials provider, and deprecate the existing one. That seems like the safest way to make this change.\nWould you be open to implementing this as a separate class? It should pretty much be a copy paste into a new file with a new class name. @AllanFly120 thoughts on what it should be named?. @mohammad1990 \nThe error is showing that it can't resolve your credentials. Can you try putting your accessKeyId and secretAccessKey inside a credentials object, and passing that to your config?\nSide note: You can pass configuration to your client at instantiation:\njavascript\nconst s3 = new AWS.S3({\n  credentials: {\n    accessKeyId: 'foo',\n    secretAccessKey: 'bar'\n  },\n  region: 'us-west-2'\n});. @mohammad1990 \nThe error is showing that it can't resolve your credentials. Can you try putting your accessKeyId and secretAccessKey inside a credentials object, and passing that to your config?\nSide note: You can pass configuration to your client at instantiation:\njavascript\nconst s3 = new AWS.S3({\n  credentials: {\n    accessKeyId: 'foo',\n    secretAccessKey: 'bar'\n  },\n  region: 'us-west-2'\n});. @rix0rrr \nWhile this is a little 'hacky', if this is for the CDK CLI, you could also set the environment variables for your node process, process.env so that the AWS_PROFILE (and any others you need) is present with what the user supplies to your tool. Do that before importing the SDK and it should use the settings you want. That would prevent you from having to duplicate any of the code in the AWS SDK if I understand your use case correctly.. @cyrfer \nWhat error are you actually getting back when you run your sample?\nWhen I run the following example, my S3 call is made using the specified profile:\n```javascript\nconst AWS = require('aws-sdk');\nfunction test() {\n    const credentials = new AWS.SharedIniFileCredentials({\n        profile: 'non-default-profile'\n    });\nconst s3 = new AWS.S3(Object.assign({}, {credentials: credentials}));\ns3.listBuckets((err, data) => {\n    if (err) {\n        console.error(err);\n    } else {\n        console.log(data);\n    }\n});\n\n}\n```\nI've also tested the above using a profile that defined a role_arn, and setting the global AWS.config.credentials to the SharedIniFileCredentials before instantiating the client. I wasn't able to contradict the documented behavior in any of those cases. Can you access your kms client inside your callback and inspect its config.credentials object to confirm what the credentials are set as?\nPart of what @rix0rrr is requesting is an easy way to read configuration from the shared credentials/configuration file besides credentials, like the region. Right now the class we use to do that is private, but we can make that public.. @cyrfer \nThat's curious. Can you inspect your credentials object right after you create it and see if it's what you expect? If it is, I wonder if there's something unexpected going on with the way the configuration passed to the client is created. I just can't seem to reproduce what you're seeing, so need your help narrowing it down.. @adamatnetrist \nWhich version of the SDK are you using?\nHow large is the file you're trying to upload? Can you try setting the Content-Length directly and see if that helps?\njavascript\nupload.on('afterBuild', function(request) {\n  request.httpRequest.headers['Content-Length'] = video.length;\n});. @mclark-newvistas \nCan you try setting AWS_SDK_LOAD_CONFIG to 1 as well? I believe when we added support for AWS_SHARED_CREDENTIALS_FILE, we only made it accessible behind the AWS_SDK_LOAD_CONFIG flag so that existing applications didn't start unexpectedly reading this value. This is something we plan to change when we do a major version bump so our credential/config loading matches the CLI.. @dpmallinger \nThe behavior you're seeing is expected. In order to maintain backwards-compatibility for users that use more than one SDK/tool, when we added support for the AWS_SHARED_CREDENTIALS_FILE, we put it behind the AWS_SDK_LOAD_CONFIG environment variable. See here for more info on that change.\nThis is absolutely something we are changing when we do a major version bump, but couldn't be done at the time we added it without potentially breaking users.. @dpmallinger \nThe behavior you're seeing is expected. In order to maintain backwards-compatibility for users that use more than one SDK/tool, when we added support for the AWS_SHARED_CREDENTIALS_FILE, we put it behind the AWS_SDK_LOAD_CONFIG environment variable. See here for more info on that change.\nThis is absolutely something we are changing when we do a major version bump, but couldn't be done at the time we added it without potentially breaking users.. On your note, I initially didn't do that because:\n1. Key isn't required for all operations, so I was worried there could be cases where other parts of the URI were stripped. However, looking through all the operations, it looks like that is essentially a non-issue since you can't create buckets that contain question marks.\n\nThe object key is escaped before being inserted into the URI. That means we'd have to check if the Key parameter exists on the operation, escape it, and then exit early if the Key matches. This would be fine to do as long as new operations don't change some of our current assumptions about how Key is used (such as not being modeled with a + to indicate we should split the Key before escaping it).\n\nSo after digging more, making the change where you indicated would pass the tests, and should be fine in the future. It just seemed potentially more fragile if the model was updated in a way we didn't anticipate.. On your note, I initially didn't do that because:\n1. Key isn't required for all operations, so I was worried there could be cases where other parts of the URI were stripped. However, looking through all the operations, it looks like that is essentially a non-issue since you can't create buckets that contain question marks.\n\nThe object key is escaped before being inserted into the URI. That means we'd have to check if the Key parameter exists on the operation, escape it, and then exit early if the Key matches. This would be fine to do as long as new operations don't change some of our current assumptions about how Key is used (such as not being modeled with a + to indicate we should split the Key before escaping it).\n\nSo after digging more, making the change where you indicated would pass the tests, and should be fine in the future. It just seemed potentially more fragile if the model was updated in a way we didn't anticipate.. After thinking over your suggestion, I actually like the change you suggested better. Since this is a restful service, it's very unlikely we would see a change in the requestUri of an operation that would break our assumptions. Updated!. After thinking over your suggestion, I actually like the change you suggested better. Since this is a restful service, it's very unlikely we would see a change in the requestUri of an operation that would break our assumptions. Updated!. @MainAero \nJust to make sure I understand, is the metro minifier changing the name of the Blobs constructor field but leaving the global variable Blob intact? If you disable parameter validation, can you confirm that this fixes the Blob when you release your app?\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#paramValidation-property\nLooks like this update breaks node.js since Blob will be undefined there, so an additional check to ensure Blob exists first should get around that.. @MainAero \nJust to make sure I understand, is the metro minifier changing the name of the Blobs constructor field but leaving the global variable Blob intact? If you disable parameter validation, can you confirm that this fixes the Blob when you release your app?\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#paramValidation-property\nLooks like this update breaks node.js since Blob will be undefined there, so an additional check to ensure Blob exists first should get around that.. @MainAero \nDid you actually attempt setting the parameter to see if the error goes away, or did you check the code?\nWhen the parameter is set, we remove the listener that does parameter validation:\nhttps://github.com/aws/aws-sdk-js/blob/462e8d5d3cf032a94e24e2a75e86be0dce77447b/lib/service.js#L255-L258. @MainAero \nDid you actually attempt setting the parameter to see if the error goes away, or did you check the code?\nWhen the parameter is set, we remove the listener that does parameter validation:\nhttps://github.com/aws/aws-sdk-js/blob/462e8d5d3cf032a94e24e2a75e86be0dce77447b/lib/service.js#L255-L258. @kidcosmic \nThis documentation was written long before browsers supported streams, and we likely won't be able to support them in browsers until a major version bump when we can look at using fetch instead of XmlHttpRequest. We can definitely update our documentation to make it clear that this only works with node.js streams today.\nAre you able to use Blobs? I'm not sure how they differ in react native versus in the browsers, but if they work like the File object in browsers, that may be an alternative way to upload large files.\nI didn't know react native supported ReadableStream with fetch. Is is officially supported, or provided through a 3rd party package?. @kidcosmic \nThis documentation was written long before browsers supported streams, and we likely won't be able to support them in browsers until a major version bump when we can look at using fetch instead of XmlHttpRequest. We can definitely update our documentation to make it clear that this only works with node.js streams today.\nAre you able to use Blobs? I'm not sure how they differ in react native versus in the browsers, but if they work like the File object in browsers, that may be an alternative way to upload large files.\nI didn't know react native supported ReadableStream with fetch. Is is officially supported, or provided through a 3rd party package?. You know, we should actually be able to update our ManagedUpload class (used by s3.upload) so that it pulls data from the stream using a reader. The upload method performs a multi-part upload if the file is larger than 5 MB, so it would essentially read up to 5 MB from the stream at a time for each uploadPart call. We wouldn't be able to make this work for putObject. Likewise you should be able to pull the same thing off calling the multipart upload APIs.\nYou're right, I was only able to use a ReadableStream with fetch if it originally came from a separate fetchresponse in (I think) chrome. That was a while ago, but was hoping that story had improved.. You know, we should actually be able to update our ManagedUpload class (used by s3.upload) so that it pulls data from the stream using a reader. The upload method performs a multi-part upload if the file is larger than 5 MB, so it would essentially read up to 5 MB from the stream at a time for each uploadPart call. We wouldn't be able to make this work for putObject. Likewise you should be able to pull the same thing off calling the multipart upload APIs.\nYou're right, I was only able to use a ReadableStream with fetch if it originally came from a separate fetchresponse in (I think) chrome. That was a while ago, but was hoping that story had improved.. @spouzols \nThanks for reporting this. I'll get in touch with the service team to find out why these fields were marked as required. We sometimes allow parameters to be marked required when they weren't originally for the scenario where the service would return a validation error if the parameter was missing, but you've shown that's not the case here.. @spouzols \nThanks for reporting this. I'll get in touch with the service team to find out why these fields were marked as required. We sometimes allow parameters to be marked required when they weren't originally for the scenario where the service would return a validation error if the parameter was missing, but you've shown that's not the case here.. @spouzols \nThis should now be fixed with version 2.297.0 of the SDK. Thanks for bringing the issue to our attention!. @spouzols \nThis should now be fixed with version 2.297.0 of the SDK. Thanks for bringing the issue to our attention!. @dawsbot \nDo you mean that you can't install the aws-sdk in your project in nodejs 9 or 10, or that if you clone the SDK, you can't run npm install? The SDK doesn't have any runtime dependencies on C binaries, so there shouldn't be a problem installing the SDK for use in a project.\nCan you also share what OS you're running into problems on? The Travis logs show that there are some indirect dev dependencies that fail to install, but don't affect the outcome of our tests.. @dawsbot \nDo you mean that you can't install the aws-sdk in your project in nodejs 9 or 10, or that if you clone the SDK, you can't run npm install? The SDK doesn't have any runtime dependencies on C binaries, so there shouldn't be a problem installing the SDK for use in a project.\nCan you also share what OS you're running into problems on? The Travis logs show that there are some indirect dev dependencies that fail to install, but don't affect the outcome of our tests.. I'm going to close this PR since travis jobs for nodejs 9 and 10 were added in #2218.\nIf you are encountering any issues with using the SDK in node 9 or 10, please open an issue!. I'm going to close this PR since travis jobs for nodejs 9 and 10 were added in #2218.\nIf you are encountering any issues with using the SDK in node 9 or 10, please open an issue!. @gmpuran \nThe AWS SDK for JavaScript does not use the code linked to in Stack Overflow. The code in the Stack Overflow answer is not used by xml2js, and the link to Stack Overflow is merely provided for your reference if you are looking for more information. Our AWS SDK for JavaScript remains licensed under Apache 2.0. \n. @gmpuran \nThe AWS SDK for JavaScript does not use the code linked to in Stack Overflow. The code in the Stack Overflow answer is not used by xml2js, and the link to Stack Overflow is merely provided for your reference if you are looking for more information. Our AWS SDK for JavaScript remains licensed under Apache 2.0. \n. @danielopatich \nWe can't make changes to the models in apis directly as they get overwritten the next time a service update is released. This change also looks like it would break some operations, and I did not see the fields you added documented by S3.\nI'm closing this PR. I'm not sure what you're trying to add, but if you have a feature request for the SDK, feel free to open an issue on this repository. If you are asking for an update to the S3 service itself, please open a thread on their forums.. @danielopatich \nWe can't make changes to the models in apis directly as they get overwritten the next time a service update is released. This change also looks like it would break some operations, and I did not see the fields you added documented by S3.\nI'm closing this PR. I'm not sure what you're trying to add, but if you have a feature request for the SDK, feel free to open an issue on this repository. If you are asking for an update to the S3 service itself, please open a thread on their forums.. Also fixes #2210 . Also fixes #2210 . @mvidalis \nCan you share how you're instantiating your S3 client?. @mvidalis \nCan you share how you're instantiating your S3 client?. @mvidalis \nJust starting a new project with the following code and the latest version of the SDK, I'm not seeing that issue:\n```javascript\nconst AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\ns3.listBuckets(console.log);\n```\nCan you share which version of the SDK you're using now, and which version you were using before?\nJust to confirm, are you running this in node.js?. @mvidalis \nJust starting a new project with the following code and the latest version of the SDK, I'm not seeing that issue:\n```javascript\nconst AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\ns3.listBuckets(console.log);\n```\nCan you share which version of the SDK you're using now, and which version you were using before?\nJust to confirm, are you running this in node.js?. @mvidalis \nDoes your package.json show the previous version? You'd probably see something like \"aws-sdk\": \"^2.#.#\", and you were likely using whatever version that field shows.\nCan you try just running my above example and see if you get the same error?. @mvidalis \nDoes your package.json show the previous version? You'd probably see something like \"aws-sdk\": \"^2.#.#\", and you were likely using whatever version that field shows.\nCan you try just running my above example and see if you get the same error?. @DanielSuperSonic \nIt shouldn't be necessary to promisify the SDK. If promises are available in your environment, any operation can return a promise by calling the promise() method on the operation's return value:\n```javascript\n// with callbacks\nsqs.listQueues(function(err, data) {});\n// using .promise\nsqs.listQueues().promise();\n```\nIf you specifically want to use the bluebird implementation of promises, you can configure the SDK to use that instead:\njavascript\nAWS.config.setPromiseDependency(require('bluebird'));\n@jasonfutch \nYou can still use callbacks with the SDK. To avoid making a breaking change, we only return promises on operations if you explicitly call the promise() method on the operation's return value. \n. @DanielSuperSonic \nIt shouldn't be necessary to promisify the SDK. If promises are available in your environment, any operation can return a promise by calling the promise() method on the operation's return value:\n```javascript\n// with callbacks\nsqs.listQueues(function(err, data) {});\n// using .promise\nsqs.listQueues().promise();\n```\nIf you specifically want to use the bluebird implementation of promises, you can configure the SDK to use that instead:\njavascript\nAWS.config.setPromiseDependency(require('bluebird'));\n@jasonfutch \nYou can still use callbacks with the SDK. To avoid making a breaking change, we only return promises on operations if you explicitly call the promise() method on the operation's return value. \n. @sgtoj \nAre you certain you're using the same region and credentials in both the JS SDK and the CLI?\nWhen I ran both of these examples, I was able to get the expected results from both:\nJavaScript SDK\njavascript\nconst ec2 = new AWS.EC2({region: 'us-west-2'});\nec2.describeInstances({\n  Filters: [{Name: 'tag:some:simple:test', Values: ['Foo']}]\n}, console.log);\naws cli\nbash\nAWS_DEFAULT_REGION=us-west-2 aws ec2 describe-instances --filters Name=tag:some:simple:test,Values=Foo. @sgtoj \nAre you certain you're using the same region and credentials in both the JS SDK and the CLI?\nWhen I ran both of these examples, I was able to get the expected results from both:\nJavaScript SDK\njavascript\nconst ec2 = new AWS.EC2({region: 'us-west-2'});\nec2.describeInstances({\n  Filters: [{Name: 'tag:some:simple:test', Values: ['Foo']}]\n}, console.log);\naws cli\nbash\nAWS_DEFAULT_REGION=us-west-2 aws ec2 describe-instances --filters Name=tag:some:simple:test,Values=Foo. @paulfryer \nAre you using browserify to create a node.js bundle, or a browser bundle?\nThe SDK uses the browser field in package.json to inform tools that respect it (such as browserify/webpack) which modules to use when creating a browser bundle. Sagemaker is included in the node.js version of the SDK, but not the browser version since it's unknown if it supports CORS:\nhttps://github.com/aws/aws-sdk-js/blob/master/clients/browser_default.js\nIf you want to pull in SageMaker for a browser bundle, you can also import it via require('aws-sdk/clients/sagemaker'):\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/webpack.html#webpack-importing-services. @rix0rrr \nAgreed we should expose httpOptions directly.\nAs a workaround, instead of importing from aws-sdk/lib/config, you can set the type like this:\n```typescript\nimport * as AWS from 'aws-sdk';\nconst httpOptions: AWS.Config['httpOptions'] = {\n    timeout: 6000\n}\n```. @elliotthill \nTo build on what @srchase said, the browser bundle only includes services that we know support CORS. This helps keep the total bundle size small as it excludes services that likely don't work in the majority of browsers.\nSometimes a service adds CORS support and we don't know about it, or a browser bundle is needed for an environment that doesn't enforce CORS, and that's when you would need to import the service client directly, or build a custom version of the browser SDK, as srchase pointed out.. @elliotthill \nCan you share how you're importing the SDK and instantiating Rekognition?\nHere's an example that will print collections in your account:\n```javascript\nconst AWS = require('aws-sdk');\nconst rekognition = new AWS.Rekognition({\n    region: 'us-west-2'\n});\nrekognition.listCollections(console.log);\n```. @abidingotter \nSERVICES.md actually mentions that not all services are available in the default hosted build:\n\nAlthough all services are supported in the browser version of the SDK, not all of the services are available in the default hosted build (using the script tag provided above).\n\nAre you using a tool like browserify or webpack for your project? If so, you can import the service you want directly:\ntypescript\nimport * as MediaConvert from 'aws-sdk/clients/mediaconvert';\nThis will also result in a smaller bundle size for your app because you won't need to include every service in your app, just the ones you actually import.\n. @erikerikson \nThanks for reporting this. We did not intend to publish test files to NPM, though I can see why we missed them in this case. We'll update our .npmignore file to make sure they aren't published in the future.. @sriramHaven \nCan you provide a code sample that reproduces this issue as well?\nWe currently include testing domains in node 10 in our travis builds and haven't seen this issue. While domains is deprecated, it has not yet been replaced. We can't remove the use of domains since some users use them today and that would require a breaking change.\nThe stack trace you provided points at zone.js as possibly being a culprit, but an code sample would go a long way in helping us dig into what the root cause is.. @mr-person \nAre you using any build tools such as webpack or browserify for your Electron app?\nThe aws-sdk package really contains a node.js, browser, and react-native version of the SDK. While there is a browser version of the SDK (included in the dist folder of the package), it's also possible to use the SDK for browser apps just like you would a node application if you're using one of the above tools. This only works because those tools look at the browser field in the package.json to determine how to substitute the node.js-specific files with browser-specific ones.\nWe do want to make it easier so that you can decide more explicitly whether to use the browser or node version of the SDK, but right now you would need to either depend on the version in the dist folder, or use one of the above tools to get the browser version in your electron app.\nWhich features are you trying to use that don't work in the node.js version of the SDK?. @ztzven \nDid your types work with a previous version of the SDK? Can you share which version of TypeScript you are using?. I'm wondering if it has to do with the version of TypeScript. Can you try using version 3.0?. @ztzven \nGlad to hear it!. Looks good! :shipit:. Just wanted to add that in version 3 of the JS SDK (currently in preview), you no longer have to call .promise(). Instead, operations will return a promise by default, unless you pass a callback function to an operation.. Should this jsonVersion also be a string?\n. These last 2 steps are actually a clean-up step.  When testing in a browser environment, these credentials would be cached, so the tests would pass the first time it was run in a browser, but not subsequent times. I could explicitly delete the credentials form localstorage instead, so it's more clear why I'm doing this.\n. Very minor, missing semicolon.\n. Would you mind adding some docs around this new config parameter?\nSomething like this at line 98:\n* @!attribute signatureCache\n *   @return [Boolean] whether the signature to sign requests with (overriding\n *     the API configuration) is cached. Only applies to the signature version 'v4'.\n *     Defaults to `true`.\nand something like this at line 185:\n* @option options signatureCache [Boolean] whether the signature to sign\n   *   requests with (overriding the API configuration) is cached. Only applies\n   *   to the signature version 'v4'. Defaults to `true`.\n. There shouldn't be any changes needed in this file since the extractError in lib/services/s3.js will handle setting the region.  This functionality is specific to S3 so would like to keep the logic contained there.\n. Just to be safe, can you also add a check that error exists, similar to the conditional before this one?\n. If you remove the changes from the rest_xml.js file, you won't need these tests.\n. Here and in getSignedCookie, we should probably check that a callback was supplied before calling it, and return the signedUrl/cookieHash immediately if one wasn't provided.  This is how s3.getSignedUrl works, for example.\n. So looking at the commit log, it looks like this is the 2nd time we've seen an issue like this. The first time was with Content-MD5 (#675).\nThis fixes the issue with #780, but there could still be issues in the future. For example, if you take the s3.getSignedUrl example in #780, and add the parameter ContentLanguage, we'll have the same issue.\nInstead, it might be worth taking a look at removing line 74 below. According to the S3 v4 signature docs:\nhttp://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-header-based-auth.html\nFor the purpose of calculating an authorization signature, only the host and any x-amz-* headers are required; however, in order to prevent data tampering, you should consider including all the headers in the signature calculation.\nSo it appears we should be able to support signing headers other than x-amz headers.\nI'm not sure why the x-amz check was added below, so we may need to dig into what other ramifications removing that may have.\n. @cesarpachon \nIt looks like the tests are failing on the linter because there isn't a space before module. Do you mind updating that?\n. It might be useful to add one more check after the if block to set params equal to itself or an empty object to protect us in the unlikely event someone passes in a value like null. Imagine this would be a rare edge case but we do this already for other operations.\n. @cesarpachon \nDid you mean to set self.AWS = AWS; instead of module.exports = AWS;?\n. Can you also apply this to line 389  right under if (self.isDoneChunking && !self.isDoneSending) { as well? \nOtherwise, looks good for us to merge in!\n. I don't think we can safely remove the SharedIniFileCredentials from being invoked when the SDK is loaded.\nWhile it's true that the defaultProviders will be called sequentially when a request is made and no valid/current credentials were loaded, there may be users that are performing logic on the credentials prior to making any requests. Removing this has the potential to break them.\n. It might be a good idea to store creds[this.profile] in a variable since we're calling it so frequently within the same function.\n. This check would be better if it was moved before we grabbed properties from the object. The error message would need to be changed a bit so it doesn't use sourceProfileName\n. Please remove any console.log calls from the final code.\n. I don't think there's a way to implement option 3 in a way that solves the issue without requiring users to change their flow.\nIf we added a configuration option to the provider to disable assume role, that could work. Something like disableAssumeRole that would still throw an error if the roleArn was found for the profile. I'd suggest defaulting it to false and setting it to true when it is called here.\nThe argument could be made for another credential provider to handle this, but I also think that having the SharedIniFileCredentials object handle these credentials from the ini file makes sense.\nThe impact is pretty small, only potentially causing an extra STS request when the SDK is required for the first time.\n. We could probably just pass the roleProfile instead of creds so we don't have to grab the roleProfile later. I don't think we'll have to worry about having the whole creds file in the future for this method.\n. I thought removing this.get from the constructor was a safe bet, but there is a scenario that would break existing users with this change. Specifically, if a user is generating presigned url's synchronously, and are not running other operations (a valid use-case), their code would start to fail because the credentials won't have been loaded.\nUnfortunately I think it's not safe to remove the extra get call currently, but we could in a major version bump in the future.\n. Ah, I missed that creds was being used elsewhere, but thanks for making the change.\n. Short of creating a separate credential provider, I don't think there's a way we can safely change this. That would however mean reading the same file twice, but also eliminates the constructor parameter to disable assume role.\n. Nevermind, after further testing, turns out this is not an issue since we didn't remove the SharedIniFileCredentials from the initial CredentialProviderChain.\n. Thanks for catching that! Duplication error on my part, I'll remove it.\nOtherwise, unless you have any other tests you'd like added or changed, I plan on merging this today!\n. It might be good to expand on this list, since the parameters are being removed from the docs.\n. It might also be good to make it more obvious when to use the parameters from which operation.\n. Why is this listener being removed in the test?\n. We should probably have the opposite test case as well, where we ensure an operation that is not search is still using a GET http method.\n. I think tapping into the data event on a stream could cause some unintended side-effects. In node.js 0.10.x, binding a data listener on a stream will cause the stream to emit 'data' events as fast as it can, ignoring the back-pressure that's automatically in place when using pipe. If the writable stream is slow, then this could cause loss of data.\nI don't think this is an issue in versions of node.js >= 0.12.x, and some simple testing seems to confirm that. However, we need to work with node.js 0.10.x as well.\nThe current method creates a new writable stream that also gets piped into in order to emit the 'sendProgress' events. I know it'll require refactoring your logic but that path seems safer across all our supported versions of node.\n. I don't think it's important to emit sendProgress twice. Our docs simply state that the httpUploadProgress event will be triggered when data is uploaded. Emitting once drain is called is probably more accurate, since that indicates that the data has actually been uploaded.\nI suspect that buffers/strings were converted to a stream so that the user had more fine-grained info on how much data was sent. However, I doubt it's worth the performance trade-off.\n. Using a transform stream to emit the events sounds like a great idea!\n. Yes, I think that's a more accurate test.\n. Just one last change! Since we still have users running on node 0.8, we should only pipe into the progress stream if typeof TransformStream !== undefined.\n. sslEnabled is also defaulted to true. The user may have sslEnabled set globally or at the client level, and depending on the order that properties get merged in (which is not guaranteed when using JavaScript objects), we could incorrectly set the 'default' s3DisableBodySigning. For example, if the user globally set sslEnabled to false, and set it to true on the s3 client, if the s3DisableBodySigning property was merged before sslEnabled, it would default to false incorrectly. I think this complicates the behavior unnecessarily, and could lead to trickier behavior to troubleshoot.\nI can update the documentation to specify that this config is also only applied when using https. Besides sslEnabled, users can also specify their own endpoint that uses something other than https, so it'd be good to document this config for that scenario as well.\nI am hesitant to throw an error if both settings are explicitly set. I think documentation is the better route to go, since we can reliably handle the case where sslEnabled: false and s3DisableBodySigning: true, and the behavior is identical to what we're currently doing.\n. I think you're probably right that I don't need to check for the header. I'll remove it and verify my test cases with presigned urls still work.\n. If buckets isn't passed into this method, can't we just set this.bucketRegionCache to an empty object? Or if you want to be thorough, delete this.bucketRegionCache first? Then you wouldn't need to loop through 2 lists to clear the cache completely.\n. That's a fair point, I'll make that change.\n. Solid!\n. Are you removing the populateURI because you want to avoid duplicating some work?\n. Is it necessary to create a new Endpoint object when you have one from the new S3 object already?\n. Is this region available within the callback because the regionReq request updates the same cache that the original request uses?\n. Do we ever see the NetworkingError when the region is set to us-east-1? I'm curious why we're specifically checking this region.\n. MaxKeys isn't a valid parameter for headBucket, so you'll get an UnexpectedParameter error here.\n. Might want to use standard markdown instead of html:\nhttps://guides.github.com/features/mastering-markdown/\n. It works, but parseInt(version[x], 10) is clearer and doesn't make use of a JS quirk to convert a string to a number. We'll never need to worry about floats with our versioning.\n. Great point! I like the idea of having a queue keep track of what should be removed from cache first. \n. At least when using node.js 0.10.x and higher (streams 2+), you'll want to actually call stream.end(). This will emit the unpipe and end events, and clean up some listeners on the source/destination streams that aren't being cleaned when we just emit end.\nThat won't work in node 0.8, but just emitting end might suffice there.\n. It seems messy to attach the Promise constructor to the Request constructor and then to reference that outside of the request class (in managed upload). \nThis might be a good time to refactor the util.setPromisesDependency method to store the Promise constructor, and then add a util.getPromisesDependency method to retrieve it.\n. The default used to be 30 ms but was changed to 100 ms with the configurable retry delays.\n. I know it's an example, but we should probably show increasing the base retry delay above 100 (current default). That puts it more in line with the comment that says you can increase these values on line 14.\n. Absolutely. Webpack actually does polyfill buffer, but I like taking control of which buffer polyfill to use back to us (makes updating when bugs are fixed easier), and to make supporting other tools easier.\n. I'm working on rewriting this script to better support custom builds, so a lot will probably change here.\nI'm not quite sure what you mean. Are you asking why we can't rely on browserify to pull in the service clients, or all the node.js module deps as well? Part of my motivation was so that webpack users wouldn't need browserify as well to customize their build.\n. Why is the refresh queue only implemented for ECS credentials and not EC2 credentials? EC2 credentials has the same issue.\n. I'm not 100% certain since I haven't tested this myself, but if these fields aren't assigned to self, will the next batch of requests cause the credentials to be refreshed again?\n. Ah, so EC2 already supported this, excellent.\n. Have you considered retrying for all the codes we retry when a service requests fails? TimeoutError is thrown by the SDK when a connection times out. I'm not sure if there are other errors the metadata service may also throw, but it shouldn't hurt to check.\n. Minor thing, but might want to specify that the int is a decimal number when parsing parseInt(str, 10) for a little extra safety.\n. We do retry for all 500+ status codes, but not 400+. Some of these we shouldn't retry, like 403 (not authorized) or 404 (not found). You're right that we should retry 429 (nice catch!), but this seems a little too inclusive, though the retryAfter check probably causes the correct behavior in most cases.\n. Good point, I like that idea.\n. Sure, I think that's reasonable, but we'll need to be extra careful to update all of our tooling and make sure we update all references. \n. I think you added an extra script into package.json.\n. AWS.uti.addPromises should be called with the constructors rather than AWS.Request, right?\n. This line should probably be moved into the credential_provider_chain.js file.\n. It looks like constructor.name may not work in all the browsers we support and could have issues when minifiers are used:\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/name#Browser_compatibility\nCan you do an equality check against the constructor instead?\n. Actually, what do you think about giving each class that should return a promise a static method that accepts a Promise constructor, then the class can control how it should promisify itself?\nThe pros to that approach would be the logic for adding promises would be controlled by each class, rather than defined in a long if/switch statement within a utility function. If the method to promisify a class was named the same for all classes, you can just check if the constructor has that method then call it, instead of maintaining a list of enums to check a class against. You could still make the promisifyMethod a utility method if that reduces code duplication.\n. Minor: Can you use the same function name as the field?\n. This works, and is very close to what I was trying to describe before. Is there a reason you put the deletePromisesFromClass and addPromisesToClass on the prototype instead of on the constructor? This works, but the code within those methods is a little confusing, since the this context won't be an instance of the method as you'd typically expect, it would be the prototype itself.\n. Can you confirm this won't cause https://github.com/aws/aws-sdk-js/issues/570 to happen again? Looks like this change was made in the past, and caused a regression. Just want to make sure we don't cause the same regression again.\n. I wanted to export all the interfaces so that users could cast a result if they needed to, but I'm not sure if that's necessary. I'll have to take a look at some other TypeScript libraries to see if that's common practice.\nYou should only see these interfaces when looking at the service client constructors, but not on the service client instances.\n. That's an interesting idea. I don't think this will work for S3, or services that have custom config options (anything with dualstack, just S3 for now) if we stop exporting the service interfaces though. I'll take a look.\n. Yeah, but I'd have to import ServiceParams from somewhere, presumably the base S3 class.\n. So, I tried a few different things. Ultimately, I went with putting the exported types in a sub-namespace:\ndeclare namespace SERVICE.Types {\nWhen I put them on their own namespace, I had to explicitly import them into my app, otherwise the typescript compiler would complain:\nhttps://github.com/Microsoft/TypeScript/issues/9944\nI also wanted them to be exported so user's can specify a type for cases when the typescript compiler can't quite infer what a type should be. That might not be necessary once https://github.com/Microsoft/TypeScript/issues/6606 is addressed.\n. It looks like I'll have to create a custom shape, since only top-level params are actually allowed:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/service.js#L163\n. Nice catch!\n. Good catch!\n. If the required params were specified in the service-level config though, then they wouldn't be required here, so the 2nd option could still be valid.\n. Agreed, I had meant to get to this but hadn't yet. I think we should also still add the primitive type (string) as well, so that users can specify new enums as they become available without having to update the SDK.\n. Fair point, thanks for finding that.\n. What's an example of a timestamp in the output that is returned as a string, instead of a Date?\nIf the shape is modeled as a timestamp, and the value is a string or number, we call util.date.parseTimestamp, and parseTimestamp always returns a Date object, or throws an error.\nFair point about the above for inputs though.\n. Thanks, good catch!\n. Thanks, didn't catch this due to the mistake above.\n. Yes, that's exactly what it'd look like.\nI don't want the user to have to update the SDK to use a new enum. If Lambda added support for nodejs5, and we didn't specify string as an allowable type, they would have to update their SDK to the latest version or else get compilation errors. Today with JavaScript, they could just continue using their current version of the SDK.\n. Can you keep the AWS.util.isBrowser() && window.localStorage !== null checks at the top of the try block? As it is, this will always throw an error in node.js (and possibly some 'browser' environments like electron). I know the catch block should handle this case, but I'd like to avoid throwing an error if we can predict it.. This was unrelated. The test never passed, needs additional work with Iot.. For all these tests where you're making sure we remove empty inputs, can you also add tests to verify that we don't translate empty strings/sets/buffers if convertEmptyValues isn't set?. Can you add CredentialsOptions to the update method in ConfigBase as well? This should be allowed in service configuration in addition to the global config.. Since the Buffer constructors are deprecated, we might want to add some feature detection to try Buffer.from first, then new Buffer.. I went back and forth on that. Since STS uses a global endpoint, it's fine if the region the user passes in isn't us-east-1.\nI would probably also have to update WebIdentityCredentials to accept client config, since it calls STS as well.. Right. Sorry, I wasn't clear in my response. I was initially worried that a different region would cause an issue with STS, but discovered that's not the case. I'll update the PR to pass config to STS (and likewise to WebIdentityCredentials as well).. Agreed, we do this often enough throughout the SDK that we should provide a better way to handle this.. nitpick, npm run add-change should work as well (and has slightly fewer characters!). @RLovelett \nThis should be fine. We really just want to make sure the code compiles correctly, and that we don't break it in the future. This captures the intent.. It doesn't seem like there's a need for this function. Couldn't you store the return value directly in an object, then pass that to the callback/return it?. We had 2 issues reported because this cache key wasn't unique enough:\nhttps://github.com/aws/aws-sdk-js/pull/1054\nYou may need to pass the serviceClientId as well.\n. Just to be safe, might be useful to also make sure we can access the expected fields (at least url and fields) on presignedPost.. Can you add tests to make sure we can explicitly set a variable to the DynamoDB.Converter type? Also, would be nice to verify we can access DocumentClient.ConverterOptions.\nCan be simple, like:\njavascript\nconst converter: Converter = DynamoDB.Converter;\n// and a test for input with converter options\nconst converterOptions: DynamoDB.DocumentClient.ConverterOptions = {convertEmptyValues: true};\nDynamoDB.Converter.input('string', converterOptions);\nHow hard would it be to also expose ConverterOptions on the Converter namespace? Just feels a little odd having to access it off the DocumentClient namespace instead.. This gets resolved to the pure-js version, same as what browsers use.. I'm actually hoping we won't need webpack at all. This is a temporary work-around until the issue can be fixed on react-native's side. I used webpack 1 since I had tested with that before webpack 2 was released, and the fields I'm using don't exist in webpack 2 anymore.\nIf for whatever reason the react native issue can't be fixed, I'll update this to 2+. none was the only authtype originally supported and implemented here\nAn authtype of none will make that operation an unauthenticated request, so credential validation and signing listeners are removed from the request altogether.\nSince this function should not be called when authtype is none, I don't think it is necessary unless we eventually expose this API (but I think we'd be doing a lot of refactoring in that case). Good catch! Fixed this and added additional tests.. Just in case someone wants to enter 0, maybe use a typeof x === 'number' check instead.. We typically try to treat user input as immutable, that way they can reuse the same object in another call if desired.. I think you're right based on the documentation on DynamoDB's page. Today the SDK will perform the first retry immediately, but happy to change that (and simplify the code!). There is a separate test that implicitly tests this, but happy to add an explicit test.. Minor: Might be worth adding an example that uses tags.. Might be good to call out that this only has an effect in node.js. Since this is the exact same implementation for all 3 services, does it make sense to move it to AWS.util or AWS.Service?. Since this is private, you could treat this as a regular module instead of hanging the class on AWS. Then you won't have to do the side-effects require in node_loader.js.. Could you still get this.filename from creds.filename instead? It might still be useful to have the filename in the message, especially now that we will be looking at credentials and config.. I believe we're actually supposed to check the credentials file for region, then the config file, similar to the CLI/boto.. Should we grab defaultProfile from AWS.util.defaultProfile instead?. Can we add a test to make sure the credentials from ~/.aws/credentials is used preferentially over the credentials in ~/.aws/config if the same profile exists in both files?. This mock can probably be removed.. Extra statement :) . I'm not sure this test is actually ensuring that the creds from profile foo are used instead of default.\nWhat do you think about spying on AWS.STS or AWS.Credentials to get the accessKeyId that was used as the source?. Extra statement.. Nit: Can we either use var i here, or declare var i at the top of the function? Just worried if we someday change i in the for loop above, then this i becomes a global.. Do we need to copy resp.data here? I'm a little worried about copying Buffers for operations that return binary data. Since you can't mix promises/callbacks/streams, we shouldn't really have to worry about mutating the data object.. Ah gotcha, looks good then!. I don't think giving the user a truncated stream is really ok. I think this test was checking the behavior the SDK exhibited, rather than what was intended.\nHere, there's a check to see if the incoming data matched the content-length. It should be throwing an error when the content-length is less than the data received. When I was testing though, I discovered that when the body is larger than the content-length, node throws a ParseError. However, because we were swallowing these errors (due to already receiving response headers), and node.js still gave us access to the body up to the content-length, it appeared as though we could never detect when data streamed in exceeded the expected amount.. Should we test multiple platforms too?\nhttps://www.appveyor.com/docs/lang/nodejs-iojs/#testing-under-multiple-versions-of-nodejs-or-iojs. Nice catch!. Agreed, and I can't imagine this would be desired behavior. I wonder if other SDKs collapse multiple slashes into one.. If a user did not supply a key at all, then they would receive a missing required param error. That said, it is possible to turn off param validation so maybe I should just make sure Key exists as a possible parameter.. Copy paste error, I'll fix that.. Thanks for spotting this!. I was actually able to just import lib/core.js on the first line and mutate util directly on that, instead of pulling in util.. I might actually remove this line. This will cause an error to be thrown if createReadStream is called after a request is sent, but it would need to be wrapped in a try/catch. That might be confusing if the user expects to listen for an error on the stream instead.. Can you amend this to state a URL will be returned?\nSomething like:\n\nReturns a 'thenable' promise that will be resolved with a pre-signed URL.. The AWS.util.promisifyMethod function currently only works for functions that accept a callback function as the first parameter. For example, AWS.Request.send and AWS.Credentials.get both accept just a callback.\n\nThe s3.getSignedUrl method accepts an operation name, params, and a callback, so using this method won't work. You should be able to set getSignedUrlPromise to a function that returns a new Promise. This promise can simply wrap the getSignedUrl function.. This test is actually returning a false positive. The catchFunction is being called, but since there are no expect calls within it that can fail, mocha assumes the test passes.. Since new Date().getTime() is the equivalent to Date.now(), can you switch your usage? Date.now() actually ends up being way faster depending on your browser. Use Date.now() here as well.. Can you rename this test? \n\nshould find clock skew if service time is skewed within 30 seconds\n\nisClockSkewed should be false if the service time is skewed within 30 seconds. Maybe rename it to make it clear that the clock should be considered skewed if service time is not within 30 seconds of the client time.. Can you add a test confirming that two service clients can have different clock skews? Basically create two instances of MockService, send 2 mocked responses using mockHttpResponse where the date headers are different, and make sure each client's systemClockOffset is different and correct.. Nit: Spying on AWS.EC2.prototype is more idiomatic.. We have a standard way of testing list/describe operations in most of our feature tests that look like this:\nhttps://github.com/aws/aws-sdk-js/blob/v2.135.0/features/acm/acm.feature#L7-L10\nIf you follow this patten, you don't have to create your own step definitions, since cucumber will use the ones defined here:\nhttps://github.com/aws/aws-sdk-js/blob/v2.135.0/features/extra/hooks.js#L56. It doesn't look like queryParams is used after it's created.. Do you know if there are any headers that we hoist to the query string today that don't also have to be sent as headers when using sigv4 presigned urls? Maybe that's just a feature of sigv2, but I'm wondering if we'll lose out on that feature with this change.. Ah ok. So we may want to add additional headers to that section you pointed to in the future, but as of now, this change won't cause any existing cases to break. This part looks good then!. Spaces should be realigned.. Can you keep the ECSCredentials in it's own module? \n. remote_credentials is located in the same directory, so it should be ./remote_credentials. In node.js, the response body is likely to be a stream. It might be easier in node to create a passthrough stream that generates the md5 hash and compares against the checksum. You'll know the expected content-length since it's passed in the response headers. Your solution here should still cover the browser use-case though.. I think we also want the fall-back of checking the request header to see which algorithm we used. This will be more important in the browser where x-amz-transfer-encoding may not be exposed. Either that or we document which headers have to be exposed in the browser, which I'm also ok with.. What about just naming this disableTrailingChecksum and making it only available on S3 for now? Then, we can add it to other services if this functionality is expanded in the future.\nIf you go this route, you'd want to only add this documentation to the S3 client's configuration, instead of the global configuration. You can see an example with useDualstack here\nhttps://github.com/aws/aws-sdk-js/blob/fa711eb42c76b89506d5266988b6c76e5c9be696/doc-src/templates/api-versions/plugin.rb#L107\nand here\nhttps://github.com/aws/aws-sdk-js/blob/fa711eb42c76b89506d5266988b6c76e5c9be696/doc-src/templates/api-versions/plugin.rb#L128. Can you rename NodeBuffer to Buffer? You're actually using Buffer in your code below as well.. Using new Buffer() is considered unsafe now.\nhttps://nodejs.org/dist/latest-v9.x/docs/api/buffer.html#buffer_new_buffer_array\nUnfortunately, Buffer.from is not available in all versions of node.js we support. You can do a check to determine if Buffer.from is available and choose what to do. We have example for base64 encoding:\nhttps://github.com/aws/aws-sdk-js/blob/fa711eb42c76b89506d5266988b6c76e5c9be696/lib/util.js#L108\nSince we know the checksum is supposed to be 16 bytes, you can actually allocate the 16 bytes up front, then fill it with the trailing checksum as you get it.. Nit: Maybe a better name is providedCheksum, because at first glance I'm not sure if this is the checksum you're generating from the stream, or if it is the one S3 provides.. I think you may actually want to listen for the flush event.\nThis would give you a chance to emit an error before signaling to the downstream stream that you've finished piping data, instead of after.. Grammar:\noverride -> Overrides\ntransfor -> transfer. Assuming you allocate 16 bytes in the original checksum Buffer, you can use fill instead of concat here. This should end up using up less memory since concat always returns a new buffer.. Interesting, what happens if the object itself is 0 bytes? Do we still get a checksum?. You'll probably want to call httpResp.destroy() to clean up the socket if an error is encountered as well.. If responseStream is an IncomingMessage, you'll want to call destroy on it. As of node 8.0.0, readable streams also have the destroy method, so you can check if destroy is a function, then call it if it is.. Do you have to attach these fields to the responseStream? These fields are already being emitted on the headers event.. We'll still need to perform these checks for Node.js 0.8, especially if we start turning it on by default. Unfortunately 0.8.x doesn't include Transform in its stream package, so your implementation won't work for those cases. You'll likely need to resort to data listeners to perform your calculations if Transform doesn't exist.. Do we still need this?. I know it's still a WIP, just noting you won't need this method anymore since you've moved the check closer to the IO!. Right now UnsupportedHashingAlgorithm will trigger a retry, but we probably don't want that.. Just curious, did you add the stream truthy check because you were encountering an error? I'm not against it, but I'm curious when it wouldn't be defined since it should be available from the outer function's scope.. Nit: expect -> expected.\nGood error message!. Should we return here as well? If we know we won't be able to calculate a hash, we can avoid instantiating a bunch of objects.. Since the IntegrityCheckerStream and ContentLengthCheckerStream are two different classes, I think it makes sense to separate them out into their own files.. I think we should be using Buffer.alloc for the added safety. I'm assuming you picked allocUnsafe because the docs say:\n\nCalling Buffer.alloc() can be significantly slower than the alternative Buffer.allocUnsafe() but ensures that the newly created Buffer instance contents will never contain sensitive data.\n\nBut, you're calling fill(0) which is what makes alloc slower, so there shouldn't be a speed benefit.. Prior to node 8.0.0, calling new Buffer(number) had the same effect as calling Buffer.allocUnsafe, so you'll want to call fill(0) when using this method.. You'll also need to increment this.providedChecksumLen with newChecksum.length in case the chunk contains some of the expected content data, and only part of the checksum.. Have you tested to see what happens if the chunk passed in contains only the trailing checksum, but not the entire thing? I suspect this would be an edge case but will _transform continue to be called if there's still more data upstream to read, but you've already sent null to the downstream stream?. Can you just call the callback with the error object? I think that takes care of emitting the error if you do.. Same comment as above about passing the error to the callback instead of emitting it ourselves.. I mean something like the following:\n1st time this is called, chunk contains just the object's data.\n2nd time this is called, chunk contains 14 bytes of the trailing checksum, only.\n3rd time this is called, chunk contains the remaining 2 bytes of the trailing checksum.\nI wonder if that last transform call would happen if we already sent null in the 2nd call. To be fair, I'm not sure if this is a realistic case. If node buffers a minimum number of bytes (until the end of the stream) before emitting that it's ready, then this is likely overkill to consider. If it can happen, this could end up as a frustrating edge case.. I think it makes sense to just throw the error immediately - since this is a constructor method and the condition we're checking is synchronous - rather than emit an error. This way we'll fail fast and not waste resources. It's also what other node APIs, like fs.createReadStream do if you pass in bad parameters.. Call return either on the same line as callback or right under. Right now callback might be triggered twice: once with an error and then once without an error a couple lines below.. Why should this work? I don't think updating an object that is undefined or null is expected, since this method is currently meant to mutate an existing object. Was this change needed to fix something?. Are these tests running in node.js? This should only be running in browser environments, and is using the 3rd party Buffer package instead of node.js' Buffer package. We can place browser-specific tests in a separate folder and exclude them from being run by mocha in the npm unit script.. Can you wrap this in a parseInt like you do on line 53?. Probably want to append a timestamp to this as well to make it somewhat unique. Why don't you create the bucket used by all the tests in the before step? We shouldn't have to create a new bucket for every single test, just this suite of tests. \nI also wouldn't mix the putObject method with createBucket. putObject is doing too much, and adds 'global' (across tests) state. For example, you don't directly pass it the bucket or key, instead relying on a closure that every test has access to (and can change). That could lead to tricky edge cases coming up later that are hard to debug.. If you createBucket in the before hook, you can deleteBucket in the after hook!. Nit: Missing semi-colons here and a few other places.. Does end ever get triggered? If it shouldn't, maybe we should throw an error here. If it should, then done() should land here assuming end is called after error.. This method makes sense in cucumber where you are supposed to write 'human' parseable tests, but I think it's overkill for mocha. You could have constants for 1KB, 1MB if you want, then call new Buffer in the test. \nThen there's less code that can possibly go wrong :wink:. Actually, if you get rid of the createBuffer method, there's really no need for this file. You could add var expect = require('chai').expect; to the top of your integration test file, and import the SDK directly in your test as well.. Is exit exposed as a global? I thought it was process.exit(), and am not seeing exit in the node repl.\nAlso think this line is skipped since you're throwing an error right before it.. +1000 for not using Cucumber!. But you could also make sure the bucket is there in the before hook. Just call done() after the waitFor method completes. Then you also only need to create it once; I don't think there's a reason we need to create a new bucket for every test since we aren't testing any bucket-specific configurations here. Creating a new bucket with each test also creates more points of failure (rate/resource limits, for example).\n. Does process.exit(1) ever get called? The error that's thrown right above it should cause you to break out of this catch block.. Can you pass the bucket name into this function? Right now I would have to look at the createBucket code to understand what bucket name is being chosen and how, and I don't know what the empty object is for.. We shouldn't need it because we always surround attributes with double quotes, but I can add it an rerun integration tests.. Yeah, options.readableObjectMode = true is the documented way of enabling this in current versions of Node.js, but in earlier versions (at least 0.10.x) this._readableState.objectMode = true is the documented way to enable this.. Sure, I don't have a strong preference either way.. Woops, good find! Removed.. Can you make the err in your callback an Error? I see that you're treating this as sort of a 'reason', but I'd like to have this be an actual error so it's consistent with the rest of our callback APIs (and general node pattern of passing an error object as the first parameter to a callback).. Instead of defining a default function if one isn't provided, can you update the if (mfaSerial) check in loadRoleProfile to also check for the existence of this.tokenCodeFn? That will save us from having to create an extra no-op function.. Cool update!. Comment made above to also check for existence of the this.tokenCodeFn.. Assuming err is an Error object, I think it'd be fair to update this to use err.message instead of err.. Can you move these expect statements to inside of your tokenCodeFn, before done()? Right now done() is getting called before these expect statements do, so this test will never report a failure.. Can you update this to return a new Error?. This looks like it can be removed.. Do you know if the services that were using the timestampFormat in the metadata were using a different format than what their protocol defaulted to? If any did differ from what the protocol would require, do all of the timestamp shapes have a timestampFormat defined?. timsStampFormat should be timestampFormat. Does the test pass right now?. Since you renamed this file, also need to update the package.json and travis files:\nhttps://github.com/aws/aws-sdk-js/blob/f16d34054deb5dea47e64682272d369832f96553/package.json#L142\n(Oops, doesn't look like travis has the current test: https://github.com/aws/aws-sdk-js/blob/f16d34054deb5dea47e64682272d369832f96553/.travis.yml#L31). This file seems to have some very aggressive padding!. Just to be sure, if you run node ./scripts/typings-generator.js is this what the file looks like?. This method could also be made it's own function. It probably doesn't need to be made public, but if you extract this one, you can also move getDefaultFilePath out too, and that function may make sense as a public one.. Looks like you're making this public. Can you update the docs for this class as well?\nIf it is public, you'll want to create TypeScript definitions as well, and expose it on the AWS namespace.. Are the typings in this file accurate? It looks like the public interface we are exposing is the IniLoader class, but these are just functions (with no clear way on how to import them as a consumer of our library). I know iniLoader is cached, but I think it still makes sense to put it outside of the while loop since you're using a single instance.. I think references to sharedIniFile need to be updated here to iniLoader.. If you want this to appear in documentation, you also need to attach IniLoader to the AWS namespace:\n```javascript\nAWS.IniLoader = AWS.util.inherit/ ... /\n// optionally also export it:\nmodule.exports = AWS.IniLoader;\n```\nYou'll also want to add doc strings to the public methods.. If we're exposing the IniLoader class, I think you'd want to require it in this file (assuming you're attaching the IniLoader to the AWS namespace to get docs showing). Right now the typings show that IniLoader is exported, but I don't think it actually is.. Is parseFile exposed to consumers of the SDK? If so, it should probably have some documentation, otherwise we don't need typings for it.. This is still not showing up in the documentation. I believe it's because yard is looking for the pattern:\nAWS.IniLoader = AWS.util.inherit. Is this delete necessary? Couldn't you restore process.env before each test, instead of before all?. suggestion\n     *                                  access. Any query params included with\nLooks like a trailing space is causing the linter to complain.. Dead code.. Minor: you'll want to pass utils and mock_agent.js as separate items for windows compat.. Minor: Dead code. Minor: dead code. Minor: dead code. Repeating what we said offline, this will cause the shared-ini loader to be pulled into the browser/react-native builds as well, which is not what we want. You may need to grab the iniLoader from util, similar to how we do for other dependencies that differ between environments instead of requiring it directly.. Minor: Change marshallCustomeIndentifiersHelper to marshallCustomIdentifiersHelper. Naming: Custome should be Custom. Can you spell out optionalDiscoveryVersionEndpoint (assuming that's what Disver means). Disver isn't a common abbreviation and coming back to this in the future it may be confusing what this means.. suggestion\n * @param [object] Service client object.. suggestion\n      message: 'Custom endpoint is supplied; endpointDiscoveryEnabled must not be true.'. Do you want to return a boolean, or the endpoint? Right now this function will return the actual endpoint that's configured.. Is it possible to have an environment variable that is not a string in node.js?\nI haven't tried in Windows, but on MacOS, that doesn't seem possible.\n```bash\nFOO=bar node -e \"console.log(process.env)\"\nFOO: 'bar'\nbash\nFOO= node -e \"console.log(process.env)\"\nFOO: ''\n```\n```bash\nnode -e \"process.env['FOO'] = void 0; console.log(process.env)\"\nFOO: 'undefined'\n```\nEven trying to set it to undefined within the process still results in the variable being a string. If you want to throw an error when this variable is set to nothing, you might want to instead check if it is an empty string.. Sorry for the incoming tangent, but I have 2 thoughts here.\n1. I think we should update isBrowser to no longer depend on process existing:\nhttps://github.com/aws/aws-sdk-js/blob/29fc8d39b73a9c7c15b6bc8fb28cf5ef313691f4/lib/util.js#L39\nWe've heard reports of this breaking with Angular, and it doesn't really make sense that we're depending on a global that doesn't exist natively in browsers. Instead, I'd propose we update our (browser|react-native|node)_loader.js files to set the function to return true or false accordingly.\n\nThis is less important if we do the item above, but I wonder if it would make sense to separate this function. You could have 3 new functions, each in their own file that perform one of these checks. \nThen you could have a separate browser/node file that exports the same function. This function would import whatever checks they need. This has the benefit of your browser build not having to import the code used to check the shared Ini file, and you could then allow the shared ini check to import the shared-ini loader directly. However, it also means more source files and might be more difficult to track.. We don't want to import this file directly as it will add bloat to the browser SDK.. I don't think you want to set your maxRetries here. This is currently set at 60, with a 1 minute retryDelay, meaning if this operation never succeeds it will take an hour before the callback passed to send is invoked.\n\nThat also means this could leave an application running for up to an hour after their process would have normally ended, which isn't what we want.\nI'm also unclear on if we need to wait for the endpoint request to complete before making a request to whichever operation was called. I get that the endpoint request can fail, and if it's optional we should continue, but I don't know if we should force trying to get an endpoint on our first request.. Grammar: Maybe change to something like\n\nWhether to enable endpoint discovery for operations that allow optionally using an endpoint returned by the service.\n\nI couldn't find an example of what other teams were using for their docs.. Also need an entry for endpointCacheSize. Is the EndpointCache class meant to be exposed? It's marked private in lib/core.js. Is endpointCache meant to be a public member?. This variable is unused.. Can you add a comment that explains what this function is supposed to do? It looks like you're populating an object with identifiers and customer-provided values, but it took me a while to grok that and the function name isn't clear. . What would the model have to look like for this else statement to find an endpointDiscoveryId?. Should this also be @api private?. I think this comment is a bit misleading, as is the one for getCacheKey. Both imply that you're going to get a single key (presumably a string), but you're returning a map. I think something like the following is more clear: \njavascript\n/**\n * Get custom identifiers for cache key.\n * Identifies custom identifiers by checking each shape's `endpointDiscoveryId` trait.\n */\nThis would at least help me, because I kept expecting cacheKey later on to be a string you pass to endpointCache.get, but it turns out you pass in a map of elements.. I think operation is only required if custom identifiers are defined for an operation. Is that not the case? Might help to keep the size of your cache down if we omitted operation if it isn't needed.. Do we know yet how to handle the case where multiple endpoints are returned by the service, or will there always be 1 result?. It looks like the request structure for the endpoint operation can be optionally supplied. If the operation does not model Operation or Identifiers, should they still be included? Do you know if it will cause problems to send those fields if the service doesn't expect them?. 1 minute might be a more reasonable default, since we're supposed to try every 60 seconds until we find an endpoint to use.. Still need an entry for endpointCacheSize as well.. Sweet, thanks for verifying.. ",
    "filipegmiranda": "I am having the same issue, but it looks like in the GET (I am trying to retrieve the file)\nProblem accessing S3. Status 403, code SignatureDoesNotMatch, message 'SignatureDoesNotMatch'\nOriginal xml:\nSome(SignatureDoesNotMatchThe request signature we calculated does not match the signature you provided. Check your key and signing method.AKIAI3DOSLNJC4YGPZSQAWS4-HMAC-SHA256\n20160627T212133Z\n20160627/eu-west-1/s3/aws4_request\nf8795580f6de5742706adab4ca6c89f8bfa9565b1e0c837a54297a2dcd8114df0dd99e84f2424d8d46dcff6119a9e34fa049489106149214caefe0fc1bd2d21341 57 53 34 2d 48 4d 41 43 2d 53 48 41 32 35 36 0a 32 30 31 36 30 36 32 37 54 32 31 32 31 33 33 5a 0a 32 30 31 36 30 36 32 37 2f 65 75 2d 77 65 73 74 2d 31 2f 73 33 2f 61 77 73 34 5f 72 65 71 75 65 73 74 0a 66 38 37 39 35 35 38 30 66 36 64 65 35 37 34 32 37 30 36 61 64 61 62 34 63 61 36 63 38 39 66 38 62 66 61 39 35 36 35 62 31 65 30 63 38 33 37 61 35 34 32 39 37 61 32 64 63 64 38 31 31 34 64 66GET\n/test%C2%ADtest%C2%ADtest%C2%ADfilipe/1234.jpg\nWhat can I do?\n. Hi @LiuJoyceC \nI am using play-s3 - it is a Scala Library that is suppose to work fine with Play Framework.\nHere is my complete stack:\n- Scala \n- Play Framework 2.5\nI am trying to retrieve a file from my Bucket, my credentials are fine. And yes, I am providing the region, which is the correct one:\nHere are my properties in application.conf:\naws.accessKeyId=\"AAAA\" // Not the real one\naws.secretKey=\"AAAA\" //NOt the real one\naws.bucket=\"filipe-bucket\"\ns3.region=\"eu-west-1\"\nI have just posted a question with all of this information also in StackOverFlow:\nhttp://stackoverflow.com/questions/38063684/signaturedoesnotmatch-in-s3-using-play-s3-and-play-framework-2-5-scala\nMy next step is to give up on this library, which appear to be really nice and try to do with the Java SDK from Amazon directly.\nTake a look: https://github.com/Kaliber/play-s3/\n. Hi @LiuJoyceC \nI am using v4, I actually don't know which version of the SDK, since I am using using \nplay-s3, I don't think it even uses SDK behind the scenes.\n:)\nhttps://github.com/Kaliber/play-s3\nI will try to use the official AWS client\n. ",
    "LiuJoyceC": "Hi @filipegmiranda \nCan you provide the relevant code you are running to provide more context? What SignatureVersion are you using, and are you specifying the correct region for your bucket? Are you using the AWS SDK, and if so, what version? Are you using Amazon S3, or a third party S3 clone?\n. Hi @filipegmiranda \nSince you are getting a signature mismatch error, can you provide the signature version you are using (v2 or v4)? What version of the AWS SDK are you using? How are you instantiating the S3 service object? What options are you passing in to the S3 constructor when you instantiate it?\nNote that the endpoints configured on an S3 service object is determined when you instantiate it, so changing the region on it later (s3.region = \"eu-west-1\") won't correct the endpoint to the right region. You'll need to pass in the region as an option to the S3 constructor when instantiating it. The latest version of the SDK supports redirecting the endpoint to the right region for Amazon S3 but is not able to override custom endpoints if you are using a 3rd party S3 clone.\n. Closing due to inactivity. If the workaround provided above did not work, please let us know.\n. @kalevet \nAre you saying that you get a CRC32CheckFailed error from DynamoDB even when you construct your DynamoDB client with the dynamoDbCrc32 config set to false? var dynamodb = new AWS.DynamoDB({dynamoDbCrc32: false}); When you later check the config property on the new instance, do you see what the value is stored at dynamodb.config.dynamoDbCrc32, and is it correctly set to false?\n. We are still actively looking at this feature request. Closing this as we have another open issue (#1039) for this same feature request.\n. Support for statistics was added to the SDK on 3/1/2016. Please note that this is a change to the API of AWS.CloudSearchDomain, not AWS.CloudSearch.\nhttp://aws.amazon.com/releasenotes/6103040037792580\n. Hi @manikandants \nUnfortunately, we can't share a timeline for this, but thanks for everyone who has given a +1 or voiced a concern about this feature request, as it helps us prioritize. I will look into this as soon as I can.\n. Hi @oherman1 \nThanks for providing this information.\nCould you provide more of the exact code that you're running? Since you've referenced 'this' twice in your code snippet, it will helpful to see the object that 'this' is referencing. Also, it would help if you could enable the logger and provide us with your log messages when you get this error again. To enable the logger, instantiate your s3 instance with new AWS.S3({logger: console}). Also, have you tried uploading any other large file, in case the error has to do with that specific file rather than the file size? Using your code (modified to not reference 'this') and your specified versions of Node and aws-sdk, I successfully uploaded a 65MB file, so I'd like to see if there are other factors besides file size that could be causing this error.\n. Hi @oherman1 ,\nThanks for the additional information. You mentioned that the file is being written just before the upload. Are you performing the write operation synchronously or asynchronously? And if the file is being written asynchronously, are you ensuring that the file is finished writing before you begin the upload? It's possible that the small file is written quickly enough to not cause a problem, but the larger file takes longer to write, potentially allowing the upload to begin before the write operation finishes. Can you provide the portion of code in your gulpfile that defines the gulp task for the write operation and the gulp task that calls the upload?\n. Hi @rclark \nThanks for bringing this to our attention. I was able to reproduce the error you described and am now actively working on a fix for this bug.\n. Hi @aichholzer \nThanks for your question.\nYou can set your bucket endpoint when you create your instance of S3: var S3 = new AWS.S3({endpoint: 'media.domain.com', s3BucketEndpoint: true}); Then the url returned from getSignedUrl will be set to the specified endpoint. For a more detailed description of these params, please see the Constructor Details section of the API Docs: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html\nLet me know if you have any further questions.\n. Hi @dmitriz \nThanks for your question.\nIf you know which specific AWS services you plan to use, you can use the SDK Builder for the Javascript Browser SDK to customize the SDK and reduce its size. Please see the following links to get started:\nhttps://blogs.aws.amazon.com/javascript/post/Tx39SJYJ8FN7O85/Introducing-the-SDK-Builder-for-the-AWS-SDK-for-JavaScript-in-the-Browser\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-building.html\n. Yes, this has been tested in all major browsers we support. Thanks!\n. :shipit: \n. :shipit: \n. :ship:\n. Hi @tjunnone \nThanks for your question. Could you provide more information about the request you're making? What operation are you using (putObject, createBucket, etc.)? Are you making the request through an S3 method, or are you making a request to a presigned url? If you are using a presigned url, is the bucket name appearing in the domain or the path? (Normally the bucket name is in the domain, but when it has a ., it should appear in the path instead.) Could you share a snippet of your code?\nI successfully used the createBucket and putObject methods on S3 with a bucket name with a . in it (both with and without s3ForcePathStyle), and could not reproduce your error. I also did not encounter an error with requesting a putObject operation to a presigned url.\nIt would also be helpful to know what version of the aws-sdk you are using. Thanks!\n. Closing issue due to inactivity\n. Hi @ffxsam \nWhat version of Node.js are you using? The __dirname variable is a global variable in Node.js and should not be undefined.\n. Hi @miguelcalderon \nWhen you try to put the video file to the signed url via the request module in Node, do you get the same error? When I upload a video to S3 through a presigned url and specify 'video/quicktime' as the content type in the header, the uploaded file still holds the correct content type and is not corrupted. Try something like this in Node as a test:\nvar request = require('request');\nvar fs = require('fs');\nrequest({\n  method: 'PUT',\n  uri: presigned_url,\n  body: fs.readFileSync('path_and_name_of_video_file.mp4'),\n  headers: {\n    'Content-Type': 'video/quicktime'\n  }\n},\nfunction(error, response, body) {\n  if (error) {\n    console.error(error);\n  } else {\n    console.log('upload successful:', body);\n  }\n});\nAfter a successful upload, if the downloaded video can play and it still holds the content type 'video/quicktime', then the issue does not seem to be related to the SDK or the presigned url produced by it. It seems the issue is occurring when the XMLHttpRequest is made. Please let us know if you still experience this same issue with this test and you believe it is related to the SDK.\n. Hi @miguelcalderon \nIf you are looking to make a put request to S3 from the browser, you could use the browser SDK (https://aws.amazon.com/sdk-for-browser/), which will allow you to directly perform a putObject operation in your browser code and you will no longer need to generate a presigned url in Node to send to the browser. In the browser, you could have code similar to:\ns3.putObject(params, callback); where the params has a Body property containing the video file, in addition to the Bucket, Key, and ContentType.\nI am closing this issue regarding the presigned url upload. If you have questions regarding uploading through the browser SDK, feel free to open a separate issue for it. Thanks!\n. Hi @matsev \nThanks for pointing these out! We have put these changes in our backlog and will look into it.\n. Hi @dgreene-r7 \nIt looks like you what you are trying to do can be accomplished by stubbing out the method on the prototype object of the service class. In the example you provided, getObject is not going to be found on the S3 constructor function, but instead on AWS.S3.prototype. If you stub out the method on the service class prototype, that should take effect for all instances that you create for that service, including ones that have already been instantiated. Please let me know if that isn't what you are attempting to accomplish.\n. Hi @dmitri-bazz \nThanks for providing your sample code. The reason you don't see any methods on the s3 instance object when you log Object.getOwnPropertyNames(s3) is because the methods are on the instance object's prototype, AWS.S3.prototype, and when you call s3.upload, the method is looked up on the prototype.\nHowever, I am not able to reproduce the problem you are describing and will need more information. When you log Object.getOwnPropertyNames(AWS.S3.Prototype), do you see upload in the list? If that's not the problem, is it possible the upload function that you defined is being called elsewhere inside of uploadReportToS3, such as in the else block (since upload is hoisted and is defined in all of uploadReportToS3)?\nIt may help to enable the logger by passing the console into the logger param when you instantiate s3: var s3 = new AWS.S3({logger: console}), and providing the log messages here. Also, as additional information, what version of Node are you using?\n. AWS.S3.prototype should have upload defined as its own property, and I have not been able to reproduce your error. The upload method is being added in the file aws-sdk/lib/services/s3.js. I noticed that a few other methods that are defined in this file are also missing from your list of property names on AWS.S3.prototype. Can you check if the code in this file is being run (perhaps put a debugger statement or console.log at the top of this file to check)?\n. Hi @naknomum \nThanks for pointing this out. We will look into whether there is a way to introduce more consistency in the result object without causing breaking changes to current customers.\n. Hi @naknomum and @jardakotesovec \nThank for pointing out this inconsistency. Now 'Key' with uppercase K should appear on the result object for s3.upload() for either the smaller file upload or for the multipart upload. I've also added 'Bucket' to the result object for the smaller file upload to again be consistent with the multipart upload, and because it's useful information to return anyway. Please do note that the result object still has 'key' with lowercase k when the smaller file upload is triggered (so it now has both 'Key' and 'key' and they contain identical values). This is to prevent a breaking change for customers who currently depend on the lowercase k for small file uploads.\n. Hi @andrewgaul \nCould you provide more information about your use case and why you want no location to be specified?\nIf you are looking to change the default region so that you don't need to specify the region each time you call the createBucket operation, you can instantiate your S3 instance with var s3 = new AWS.S3({region: REGION_NAME}). You can also change the default location using AWS.config.update({region: REGION_NAME}). Then any S3 instance objects (or any other service instance) created after this configuration is set will default to your specified region.\n. :rocket: \n. Hi @sylwit \nCould you provide more information about your error? Where in your code are you using a reserved keyword as an attributeValue? What is the error message you are seeing and in which line of your code is the error emerging?\nIt will also be helpful to provide the version number of the JS SDK, and the version of Node you are using (or if you are using the browser SDK instead). Thanks.\n. Hi @sylwit \nI am unable to replicate the error. Here is a screenshot of what I see when I run your code (modified to move some of the attributes to the top level): \n\nNeither NS nor NU is on the list of DynamoDB's reserved keywords: http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ReservedWords.html\nHowever, if they for some reason are being interpreted as keywords, you can try using placeholders:\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ExpressionPlaceholders.html\n. I'm glad the issue was fixed :)\n. Hi @dukedougal \nCan you describe the problem you're having? What is the error you receive? Can you provide the code snippet you're running? Have you tried running it on other browsers and OS to verify that this problem just occurs in Chrome on Windows?\nIf this is only occurring in Chrome on Windows, please provide the versions of Chrome and Windows you are using.\nThanks.\n. Hi @dukedougal \nCan you describe the problem you're having? What is the error you receive? Can you provide the code snippet you're running? Have you tried running it on other browsers and OS to verify that this problem just occurs in Chrome on Windows?\nIf this is only occurring in Chrome on Windows, please provide the versions of Chrome and Windows you are using.\nThanks.\n. Hi @lauterry \nThanks for the helpful feedback. We'll put this into planning in a future sprint.\n. Hi @lauterry \nThanks for the helpful feedback. We'll put this into planning in a future sprint.\n. Hi @Ninir \nIf CORS checking is enabled in your environment, then IAM is currently not a supported service and the preflight request will not succeed. Please see the developer guide for a list of services that are currently supported in environments that enforce CORS: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\nWe have contacted the IAM service team and are working towards getting CORS support for the service in the future. I will keep you updated once CORS support is added for IAM.\n. Hi @Ninir \nIf CORS checking is enabled in your environment, then IAM is currently not a supported service and the preflight request will not succeed. Please see the developer guide for a list of services that are currently supported in environments that enforce CORS: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\nWe have contacted the IAM service team and are working towards getting CORS support for the service in the future. I will keep you updated once CORS support is added for IAM.\n. Hi @Ninir \nYes, you can use a Lambda function to call IAM's getUser operation and use an API Gateway endpoint to access the result.\nIf this is your first time using API Gateway with Lambda, please see this guide for a step-by-step walkthrough: https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started.html\nLet me know if this accomplishes what you are trying to do!\n. Hi @Ninir \nYes, you can use a Lambda function to call IAM's getUser operation and use an API Gateway endpoint to access the result.\nIf this is your first time using API Gateway with Lambda, please see this guide for a step-by-step walkthrough: https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started.html\nLet me know if this accomplishes what you are trying to do!\n. Hi @Ninir \nI am reaching out to the IAM team for an update. It would be helpful post this request for CORS support on the AWS Forum for IAM to add visibility to this issue. Thanks! I will close this issue as it is not an SDK issue.\nhttps://forums.aws.amazon.com/forum.jspa?forumID=76\n. Hi @Ninir \nI am reaching out to the IAM team for an update. It would be helpful post this request for CORS support on the AWS Forum for IAM to add visibility to this issue. Thanks! I will close this issue as it is not an SDK issue.\nhttps://forums.aws.amazon.com/forum.jspa?forumID=76\n. Hi @abhiofdoon \nThanks for raising this issue. You are correct, with EC2 describeInstances, the results will only omit a NextToken if the number of returned results is strictly less than the specified MaxResults. As a workaround, you will just need to check if the Reservations array has length 0 to see if the NextToken returned from the previous result actually contains more results. This is an EC2 service issue rather than an SDK issue, and I will contact the service team regarding this. Also feel free to raise this issue with the service team through the AWS Forum.\n. Hi @abhiofdoon \nThanks for raising this issue. You are correct, with EC2 describeInstances, the results will only omit a NextToken if the number of returned results is strictly less than the specified MaxResults. As a workaround, you will just need to check if the Reservations array has length 0 to see if the NextToken returned from the previous result actually contains more results. This is an EC2 service issue rather than an SDK issue, and I will contact the service team regarding this. Also feel free to raise this issue with the service team through the AWS Forum.\n. Hi @abhiofdoon \nYes, the MaxResults is applied before the Filters. So in your example, the service will get the first 7 instances regardless of their state, and then apply the filter so only those in the 'stopped' state will be returned. So your case 1 is correct.\n. Hi @abhiofdoon \nYes, the MaxResults is applied before the Filters. So in your example, the service will get the first 7 instances regardless of their state, and then apply the filter so only those in the 'stopped' state will be returned. So your case 1 is correct.\n. Hi @abhiofdoon \nThe order in which results appear when paginators are used are not intended to be predicted. I'm not sure I see a use case in which you would want to treat the results on one page differently than on another page. It is more reliable to specify your filters for exactly the results you want, and then get all pages of results that are returned. However, please let me know if you feel that your use case would necessitate knowing the order in which instances appear.\n. Hi @abhiofdoon \nThe order in which results appear when paginators are used are not intended to be predicted. I'm not sure I see a use case in which you would want to treat the results on one page differently than on another page. It is more reliable to specify your filters for exactly the results you want, and then get all pages of results that are returned. However, please let me know if you feel that your use case would necessitate knowing the order in which instances appear.\n. :sheep: \n. :sheep: \n. :sheep: :it: \n. :sheep: :it: \n. :sheep: :it: \n. :sheep: :it: \n. Hi @danthegoodman \nThanks for submitting this issue. Version 2.3.2 of the SDK, which was released an hour ago, now includes support for the value \"nodejs4.3\" in the Runtime parameter.\n. Hi @danthegoodman \nThanks for submitting this issue. Version 2.3.2 of the SDK, which was released an hour ago, now includes support for the value \"nodejs4.3\" in the Runtime parameter.\n. Hi @hulbert \nThanks for bringing this to our attention.\nIn the browser, AWS.CredentialsProviderChain.defaultProviders is an empty array, but as you pointed out, in Node, it contains 4 functions:\nAWS.CredentialProviderChain.defaultProviders = [\n  function () { return new AWS.EnvironmentCredentials('AWS'); },\n  function () { return new AWS.EnvironmentCredentials('AMAZON'); },\n  function () { return new AWS.SharedIniFileCredentials(); },\n  function () { return new AWS.EC2MetadataCredentials(); }\n];\nSo the default chain will check environmental variables first for credentials, and if not found, will fall through to the shared credentials file, and then fall through to checking EC2 metadata.\nWe will update our documentation to reflect this.\n. Hi @hulbert \nThanks for bringing this to our attention.\nIn the browser, AWS.CredentialsProviderChain.defaultProviders is an empty array, but as you pointed out, in Node, it contains 4 functions:\nAWS.CredentialProviderChain.defaultProviders = [\n  function () { return new AWS.EnvironmentCredentials('AWS'); },\n  function () { return new AWS.EnvironmentCredentials('AMAZON'); },\n  function () { return new AWS.SharedIniFileCredentials(); },\n  function () { return new AWS.EC2MetadataCredentials(); }\n];\nSo the default chain will check environmental variables first for credentials, and if not found, will fall through to the shared credentials file, and then fall through to checking EC2 metadata.\nWe will update our documentation to reflect this.\n. Hi @JaKXz \nDo you get any error saying Missing required key 'Bucket' in params? Since you didn't specify a bucket in your objectParams, the SDK's param validation would give an error and not send the request. If you add the bucket name in objectParams, are you still getting the same problem?\n. Hi @JaKXz \nDo you get any error saying Missing required key 'Bucket' in params? Since you didn't specify a bucket in your objectParams, the SDK's param validation would give an error and not send the request. If you add the bucket name in objectParams, are you still getting the same problem?\n. And just to verify, are you using version 2.3.0 (or higher) of the aws-sdk? Thanks\n. And just to verify, are you using version 2.3.0 (or higher) of the aws-sdk? Thanks\n. Thanks for clarifying and updating the code snippet. Can you also provide the part of the code where you define the function generateRedirects?\n. Thanks for clarifying and updating the code snippet. Can you also provide the part of the code where you define the function generateRedirects?\n. Hi @proerp5 \nCan you provide a code snippet to reproduce your error? Are you using the .upload() operation on S3? Is the error occurring when you upload small files (< 5MB) or large files (> 5MB) or both? If you're not using .upload(), what SDK operations are you calling?\n. Hi @proerp5 \nCan you provide a code snippet to reproduce your error? Are you using the .upload() operation on S3? Is the error occurring when you upload small files (< 5MB) or large files (> 5MB) or both? If you're not using .upload(), what SDK operations are you calling?\n. Can you provide your code for uploadDocumentandMessageToQueue, since that's where you believe the error might be coming from? The code snippet above does not show how the SDK is being used. Thanks.\n. Can you provide your code for uploadDocumentandMessageToQueue, since that's where you believe the error might be coming from? The code snippet above does not show how the SDK is being used. Thanks.\n. I'm closing this issue due to inactivity.\n. I'm closing this issue due to inactivity.\n. Hi @cananhas \nIt looks like your stream Writable class is not properly passing a callback function to the _write method. This is out of the SDK's control, as the SDK does not call writer._write, but I can try to point you in the right direction. From your stack trace, you can see that line 176 of _stream_writable.js (a file that comes installed with Node) is where the _write method is being called within Writable.write, so you can trace from there to see why a callback is not being passed. If a callback is not passed to Writable.write, it should be replacing the undefined with a default callback, so _write should never be called without a callback. I was not able to replicate your error when I ran Node versions 0.8 up through 6.0 on an Amazon Linux instance. Since your code is working on your local PC but not on the EC2 instance, make sure you have Node properly installed on your instance. Let me know if this helps!\n. Hi @cananhas \nIt looks like your stream Writable class is not properly passing a callback function to the _write method. This is out of the SDK's control, as the SDK does not call writer._write, but I can try to point you in the right direction. From your stack trace, you can see that line 176 of _stream_writable.js (a file that comes installed with Node) is where the _write method is being called within Writable.write, so you can trace from there to see why a callback is not being passed. If a callback is not passed to Writable.write, it should be replacing the undefined with a default callback, so _write should never be called without a callback. I was not able to replicate your error when I ran Node versions 0.8 up through 6.0 on an Amazon Linux instance. Since your code is working on your local PC but not on the EC2 instance, make sure you have Node properly installed on your instance. Let me know if this helps!\n. Hi @ruthienachmany \nI was not able to reproduce your error with the code you provided above. It will help if you could temporarily replace the minified CDN with the non-minified version (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js) and then provide the same error stack trace as you did above, so that I can see exactly where in the code the error is coming from. Also, which browser are you getting this error in? Does this occur in multiple browsers?\n. Hi @ruthienachmany \nI was not able to reproduce your error with the code you provided above. It will help if you could temporarily replace the minified CDN with the non-minified version (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js) and then provide the same error stack trace as you did above, so that I can see exactly where in the code the error is coming from. Also, which browser are you getting this error in? Does this occur in multiple browsers?\n. Hi @ruthienachmany \nIf there was an issue with the JWT, then you would be receiving back an invalid login token error, as you mentioned above. The error should still be found in the httpResponse.body, so it doesn't seem like the JWT is what is causing your particular error.\nThe \"unknown error occurred\" that you are getting at Request.VALIDATE_RESPONSE is just the generic error the SDK initially attaches to the response when it does not get a successful response from the request, so that part is expected and it is not the error that is returned by the Cognito server. The Request.extractError step is normally where the SDK then extracts the error sent back by the server in httpResponse.body and replaces the generic \"unknown error occurred\" with the actual error, but it looks like in your case, the extractError step is not able to extract the error from httpResponse.body because it is undefined.\nBy the time you get to the Request.VALIDATE_RESPONSE step where you're getting the \"unknown error occurred\", is the httpResponse.body undefined already, or is it a buffer (even if it's a buffer of length 0) and it's somehow getting deleted by the time it gets to the extractError step? If the httpResponse.body is undefined by the time you get to the VALIDATE_RESPONSE step, can you confirm if this function within the SDK is being run? (You can run a search for it and put a debugger to see if it's getting run.)\nfunction HTTP_HEADERS(statusCode, headers, resp) {\n      resp.httpResponse.statusCode = statusCode;\n      resp.httpResponse.headers = headers;\n      resp.httpResponse.body = new AWS.util.Buffer('');\n      resp.httpResponse.buffers = [];\n      resp.httpResponse.numBytes = 0;\n      var dateHeader = headers.date || headers.Date;\n      if (dateHeader) {\n        var serverTime = Date.parse(dateHeader);\n        if (resp.request.service.config.correctClockSkew\n            && AWS.util.isClockSkewed(serverTime)) {\n          AWS.util.applyClockOffset(serverTime);\n        }\n      }\n    }\nIn this function, httpResponse.body is being set to a buffer of length 0 (and any data from the server will later be concatenated to the empty buffer), so we expect that the httpResponse.body would not be undefined if this function is getting run as it should.\n. Hi @ruthienachmany \nIf there was an issue with the JWT, then you would be receiving back an invalid login token error, as you mentioned above. The error should still be found in the httpResponse.body, so it doesn't seem like the JWT is what is causing your particular error.\nThe \"unknown error occurred\" that you are getting at Request.VALIDATE_RESPONSE is just the generic error the SDK initially attaches to the response when it does not get a successful response from the request, so that part is expected and it is not the error that is returned by the Cognito server. The Request.extractError step is normally where the SDK then extracts the error sent back by the server in httpResponse.body and replaces the generic \"unknown error occurred\" with the actual error, but it looks like in your case, the extractError step is not able to extract the error from httpResponse.body because it is undefined.\nBy the time you get to the Request.VALIDATE_RESPONSE step where you're getting the \"unknown error occurred\", is the httpResponse.body undefined already, or is it a buffer (even if it's a buffer of length 0) and it's somehow getting deleted by the time it gets to the extractError step? If the httpResponse.body is undefined by the time you get to the VALIDATE_RESPONSE step, can you confirm if this function within the SDK is being run? (You can run a search for it and put a debugger to see if it's getting run.)\nfunction HTTP_HEADERS(statusCode, headers, resp) {\n      resp.httpResponse.statusCode = statusCode;\n      resp.httpResponse.headers = headers;\n      resp.httpResponse.body = new AWS.util.Buffer('');\n      resp.httpResponse.buffers = [];\n      resp.httpResponse.numBytes = 0;\n      var dateHeader = headers.date || headers.Date;\n      if (dateHeader) {\n        var serverTime = Date.parse(dateHeader);\n        if (resp.request.service.config.correctClockSkew\n            && AWS.util.isClockSkewed(serverTime)) {\n          AWS.util.applyClockOffset(serverTime);\n        }\n      }\n    }\nIn this function, httpResponse.body is being set to a buffer of length 0 (and any data from the server will later be concatenated to the empty buffer), so we expect that the httpResponse.body would not be undefined if this function is getting run as it should.\n. Hi @ruthienachmany \nSorry about the delay. You said that in your network logs, you can see that you're getting back a JSON in your response body. Are you getting back any response headers in your network logs? Since the HTTP_HEADERS callback is never getting called, it could be because no response headers were received.\n. Hi @ruthienachmany \nSorry about the delay. You said that in your network logs, you can see that you're getting back a JSON in your response body. Are you getting back any response headers in your network logs? Since the HTTP_HEADERS callback is never getting called, it could be because no response headers were received.\n. Also, could you enable the logger option on your Cognito Identity client and post your console logs here? To enable the logger, pass in your console when you instantiate the Cognito Identity client:\nvar cognitoidentity = new AWS.CognitoIdentity({logger: console});\nThanks.\n. Also, could you enable the logger option on your Cognito Identity client and post your console logs here? To enable the logger, pass in your console when you instantiate the Cognito Identity client:\nvar cognitoidentity = new AWS.CognitoIdentity({logger: console});\nThanks.\n. Closing due to inactivity.\n. Closing due to inactivity.\n. Hi @kyriesent \nThanks for your question. For MessageAttributes of type Number, you need to use the StringValue property and provide the number in the form of a string:\nparams = {\n    MessageBody: 'myMessageBody',\n    QueueUrl: 'myQueueUrl',\n    MessageAttributes: {\n        'myAttributeName': {\n            DataType: 'Number',\n            StringValue: '4'\n        }\n    }\n}\nLet me know if this resolves your issue!\n. Hi @kyriesent \nThanks for your question. For MessageAttributes of type Number, you need to use the StringValue property and provide the number in the form of a string:\nparams = {\n    MessageBody: 'myMessageBody',\n    QueueUrl: 'myQueueUrl',\n    MessageAttributes: {\n        'myAttributeName': {\n            DataType: 'Number',\n            StringValue: '4'\n        }\n    }\n}\nLet me know if this resolves your issue!\n. Hi @adolfosrs \nThanks for your question. Currently, SES is not built into the browser SDK by default because the service does not yet support CORS, but we are working towards implementing CORS support in more services. As @ondruska mentioned, you can see a list of the services that are currently supported by the default browser SDK here: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\nIf you are working in an environment that doesn't enforce CORS, you can override this default and build your own custom version of the SDK: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-building.html\nHowever, be aware that if your browser enforces CORS, then you will get errors if you attempt to make requests with AWS.SES in your custom build.\nHope that clears up any confusion!\n. Hi @adolfosrs \nThanks for your question. Currently, SES is not built into the browser SDK by default because the service does not yet support CORS, but we are working towards implementing CORS support in more services. As @ondruska mentioned, you can see a list of the services that are currently supported by the default browser SDK here: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\nIf you are working in an environment that doesn't enforce CORS, you can override this default and build your own custom version of the SDK: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-building.html\nHowever, be aware that if your browser enforces CORS, then you will get errors if you attempt to make requests with AWS.SES in your custom build.\nHope that clears up any confusion!\n. That is correct. The SDK supports SES in the browser in version 2.3.11 (released May 12).\n. That is correct. The SDK supports SES in the browser in version 2.3.11 (released May 12).\n. Hi @srkimir \nCan you provide more specific code to replicate the error? The error is arising because dynamodb.crc32IsValid() is returning false, but without further information about your specific case, I could not reproduce this error. Also, when I forced the error to fail by returning false from crc32IsValid, the request was successfully retried 10 times before passing back the CRC32CheckFailed error into my callback, with an empty object {} passed into the data parameter.\nAlso, just to get more information about your case, what version of the SDK and what version of Node are you using?\n. Hi @srkimir \nCan you provide more specific code to replicate the error? The error is arising because dynamodb.crc32IsValid() is returning false, but without further information about your specific case, I could not reproduce this error. Also, when I forced the error to fail by returning false from crc32IsValid, the request was successfully retried 10 times before passing back the CRC32CheckFailed error into my callback, with an empty object {} passed into the data parameter.\nAlso, just to get more information about your case, what version of the SDK and what version of Node are you using?\n. The data was set to an empty object in the 'validate' step of the ASM, since a successful response came back from the server, so it is not the service that is returning back the non-null data. The checkCrc32 validation doesn't occur until the 'extractData' step after the ASM already determined that the response was successful. This bug can be fixed by either setting data back to null each time checkCrc32 fails or by moving that check to the 'validate' step of the ASM. The latter seems more correct, so I will investigate whether it might cause any breaking changes or unintended effects before implementing it. If it does cause undesirable effects, then I will implement the former fix.\n@srkimir Were you able to verify if your request is being retried? Thanks\n. The data was set to an empty object in the 'validate' step of the ASM, since a successful response came back from the server, so it is not the service that is returning back the non-null data. The checkCrc32 validation doesn't occur until the 'extractData' step after the ASM already determined that the response was successful. This bug can be fixed by either setting data back to null each time checkCrc32 fails or by moving that check to the 'validate' step of the ASM. The latter seems more correct, so I will investigate whether it might cause any breaking changes or unintended effects before implementing it. If it does cause undesirable effects, then I will implement the former fix.\n@srkimir Were you able to verify if your request is being retried? Thanks\n. Upon further investigation, it's correct to keep checkCrc32 in the 'extractData' step of the ASM, since it involves extracting the data sent by the server to check its integrity. Furthermore, moving checkCrc32 to the 'validate' step would cause the 'extractError' step to override the helpful error code of CRC32CheckFailed and would require us to override the 'extractError' listener that contains needed logic for errors other than CRC32 errors. So the PR fixes the issue by overriding the response data with null while keeping checkCrc32 in the 'extractData' step.\n. Hi @Madhesk \nI have reached out to the DynamoDB service team, as this does not seem to be an SDK issue, and I will keep you updated. Please also feel free to post your question on the AWS Forum for DynamoDB: https://forums.aws.amazon.com/forum.jspa?forumID=131\nHere is also the link to the forum for IAM: https://forums.aws.amazon.com/forum.jspa?forumID=76\n. Hi @xGabryMC \nIt looks like you are configuring your s3 client to 'us-east-1'. Is the value of this.s3config.awsRegion equal to 'us-east-1'?\nYou will need to pass in 'eu-central-1' to either aws.config.region or to region when you create the s3 instance.\nSo either do:\naws.config.region = 'eu-central-1';\nvar s3 = new aws.S3({\n  // do not override the region\n  signatureVersion: 'v4' // this is actually not necessary because for 'eu-central-1' the default is 'v4'\n});\nor do:\nvar s3 = new aws.S3({\n  region: 'eu-central-1',\n  signatureVersion: 'v4' // this is actually not necessary because for 'eu-central-1' the default is 'v4'\n});\nLet me know if that helps!\n. @xGabryMC \nWhat version of the AWS SDK are you using? If it's version 2.2.13 or later, and as long as you're not specifying maxRetries of 0, then the SDK should be automatically correcting the region for you when you get an AuthorizationHeaderMalformed error that contains information about your bucket's region.\n. Hi @srkprasadam,\nUser Pools are intended to be used with another SDK on top of the AWS SDK:\nhttps://github.com/aws/amazon-cognito-identity-js\nThis SDK provides mechanisms for authentication.\nPlease note that this other SDK is currently in developer preview and may be subject to breaking changes. Feel free to open issues on the amazon-cognito-identity-js repository if you have any questions.\nThanks!\n. \ud83d\udc11 \ud83c\uddee\ud83c\uddf9 \n. Is it necessary to make so many changes to core functions that aren't specific to S3? What if instead, if a user doesn't explicitly specify 'v4', 's3', or 'v2' for the signatureVersion, then we store the string 'default' (or some other string) in the config.signatureVersion of the S3 client. Then we can add the getSignerClass method to S3 that will return the appropriate signer class. After searching through the SDK, the only part of the code that actually uses the value stored in config.signatureVersion is inside the getSignerClass, so as long as you have a custom getSignerClass for S3 that overrides the generic service getSignerClass, then having an arbitrary string value as the signatureVersion shouldn't be a problem. I'll add more notes on specific lines of the PR.\n. Per our discussion, the updated PR will no longer change the endpoint if you specify a custom endpoint, since customers could be using proxies or 3rd party S3 clones. However, if the user sets the endpoint to a real direct S3 endpoint, such as {endpoint: 'https://s3-us-west-2.amazonaws.com'}, then the endpoint will still get updated, since that can't be a proxy or S3 clone anyway. If we don't want to update the endpoint in that scenario, we would have to know the original configuration specified by the customer, so the other PR (defaulting to v4) would need to be merged in first.\nWe discussed making getBucketLocation requests instead of listObjects for retrieving the bucket location. However, after further investigation, I realized that this would not work unless the customer configures their IAM policy to allow the getBucketLocation operation, and would be a breaking change if current customers haven't already done that. The reason listObjects works even if the IAM policy doesn't allow that operation is that it always returns the region in the header, even if you receive an AccessDenied error. In almost all of the scenarios in which the SDK calls listObjects to get the region, the response will return an error that will not be retried because the region will have been extracted and cached. Only in one scenario in the browser, the listObjects may return a successful response, so in the updated PR, I specified a MaxKeys constraint of 0 so that no object keys will be returned.\n. I updated the updateReqBucketRegion method so that the endpoint on the new S3 client does not get copied (through the AWS.Endpoint constructor), since it's unnecessary. I've tested it to make sure the hidden properties do not affect the request in any way.\n. Hi @mlconnor \nCan you tell us which other methods you are using that are returning the LastModified property as a Javascript date? getObject also returns LastModified as a string. You are right that the API docs say it should be a date, and the docs should be corrected. We may not be able to change the return type in the response to a date object because this would be a breaking change for customers who are currently parsing the string. If you would like to convert the string to a Javascript date object, it's not necessary to parse the string. You can just pass it in to the Date constructor: new Date(data.LastModified)\nFor now I will mark this is a documentation issue, but please let us know which methods you are calling return a date object instead of a string so we can determine how to best correct this inconsistency. Thanks!\n. Hi @umarservishero \nCould you provide more context for how you're using the SDK? What operations are you calling? Can you provide steps to replicate the error? It looks like you might have a syntax error somewhere in a callback that you passed to the SDK, and the syntax error is being caught inside the SDK's ASM and being rethrown from there.\nThese debugging steps might help:\nPut a debugger at line 31 where the error is being thrown, and observe the value of self._asm.currentState immediately before the error is thrown. You can also find out which operation is causing the error by looking at self.operation, as well as the params you passed in at self.params. Rerun your code again, this time putting a debugger immediately above the lines of code you provided (the line that says self._haltHandlersOnError works). Then fast-forward until the value of self._asm.currentState is equal to the value you observed earlier when the error is being thrown. Then you can step into self.emit and start debugging from there. You could also narrow down to the exact listener that is being called when the error is thrown by putting a debugger in lib/sequential_executor.js at the line var listener = listeners.shift(); and observing which listener is the last one to be called before the error is thrown. If the error is being thrown after receiving a response from the service (if self._asm.currentState is validateResponse, extractData, extractError, or sometimes retry, afterRetry), then it might also help to look at self.response and self.response.httpResponse.\nAlso, what version of the SDK are you using? What version of Node? Thanks.\n. Hi @umarservishero \nIf you're not able to use a debugger tool because the error is not occurring on your local machine, could you console.log(self._asm.currentState) in the code right above line 31 of request.js where the error is thrown so you could see which step in the SDK's ASM the error is occurring? This may at least help narrow down where this error is coming from.\nHave you verified that your local environment is the same as the environment you have deployed to EB?\n. From your logs, it looks like the ASM was in the 'complete' state already when the error was thrown, so my guess would be that the error is occurring somewhere in a callback that is passes to the receiveMessage operation, since the SDK calls the callback when the request enters the 'complete' state. Could you share your handleMessage function that you're using for SqsConsumer? Even if the function itself doesn't contain a syntax error, it might be expecting and treating its arguments to be in certain format?\n. Closing due to inactivity. Feel free to reopen if the issue is unresolved and you have new information.\n. Hi @casamm \nWhen an S3 (or any AWS service) client is instantiated, it sets the property credentials on its own config object to reference AWS.config.credentials. Note that its a reference, so if you modify the existing AWS.config.credentials, then all service clients referencing that object will receive the same modifications, but if you set AWS.config.credentials to a new object, only new service clients instantiated from that point forward will receive those changes. So for example:\nAWS.config.credentials = {accessKeyId: 'someId1'};\ns3 = new AWS.S3();\nconsole.log(s3.config.credentials.accessKeyId); // 'someId1'\nAWS.config.credentials.accessKeyId = 'someId2';\nconsole.log(s3.config.credentials.accessKeyId); // 'someId2'\nAWS.config.credentials = {accessKeyId: 'someId3'};\nconsole.log(s3.config.credentials.accessKeyId); // 'someId2'\nLet me know if that answers your question, or feel free to provide more clarification if it doesn't.\n. To follow your example of the scoped-down version of AWS, it's more similar to:\napp.js\n```\nvar AWS = require(\"./aws\");\nrequire(\"./s3\");\nAWS(\"key\", \"secret\");\nvar s3 = new AWS.S3();\ns3.listBuckets();\n```\naws.js\n```\nvar AWS = function(accessKeyId, secretAccessKey){\n    AWS.config.credentials.accessKeyId = accessKeyId;\n    AWS.config.credentials.secretAccessKey = secretAccessKey;\n};\nAWS.config = { \n    credentials: {\n        accessKeyId: null,\n        secretAccessKey: null\n    }\n};\nmodule.exports = AWS;\n```\ns3.js\n```\nvar AWS = require(\"./aws\");\nvar S3 = function(){\n    this.config = {};\n    this.config.credentials = AWS.config.credentials;\n};\nS3.prototype.listBuckets = function(){\n    var credentials = this.config.credentials;\n    // make request using these credentials\n};\n// S3 will now be available on the AWS object in any file that requires it\n// as long as this s3 file is required before AWS.S3 is called\nAWS.S3 = S3;\n```\nThe actual code doesn't look like this, but this is essentially what it's doing in this scoped-down example. To see the actual code that's implementing this, take a look at lib/services.js (at the constructor at and at initialize) and lib/config.js (at the constructor and extractCredentials).\n. For the purpose of explaining how the S3 client has access to AWS.config.credentials, either of these work. The real implementation is more complicated and doesn't actually require the service until the client is instantiated for the first time.\nLet me know if this answers your question. Thanks!\n. \ud83d\udea2 \n. \ud83d\udea2 \n. \u26f5 \n. \u26f5 \n. Hi @phil-kahrl \nAre you using the AWS SDK to make these requests? If so, can you provide the relevant parts of the code you are running? Also, what version of the SDK are you using?\nWhen you say a newly created bucket, do you mean that you are calling createBucket and immediately calling listObjects on that bucket right below the createBucket call? Or are you calling listObjects inside of the callback passed to createBucket?\nWhen I tried the following code, I did not get any errors:\ns3.createBucket({Bucket: 'someBucketName'}, function(err, data) {\n    if (!err) {\n        s3.listObjects({Bucket: 'someBucketName'}, function(err, data) {\n            if (!err) console.log(data);\n        });\n    }\n});\n. Do you have a code snippet to reproduce the error? I created a bucket in us-west-2 and then made a listObjects request to the global endpoint, and the call is still successful:\ns3_1 = new AWS.S3({region: 'us-west-2'});\ns3_2 = new AWS.S3({endpoint: 's3.amazonaws.com'});\ns3_1.createBucket({Bucket: 'bucket'}, function(err, data) {\n    if (!err) {\n        s3_2.clearBucketRegionCache(); // included this to ensure request goes to global endpoint\n        s3_2.listObjects({Bucket: 'bucket'}, function(err, data) {\n            if (!err) console.log(data);\n        });\n    }\n});\n. Closing due to inactivity.\n. Hi @dustinbolton \nloadFromPath() is actually creating a new instance of a CredentialProviderChain which will be determined by AWS.CredentialProviderChain.defaultProviders (see lib/aws.js). It then creates an instance of FileSystemCredentials from the path you provided, and adds this credential provider to the front of the chain. The TimeoutError that is getting thrown is unrelated to the .json file you provided a path to, and is coming from the last credential provider in the chain, the EC2MetadataCredentials. In loadFromPath(), chain.resolve() gets called, which will try to source credentials from the chain that was created in order. If your .json is incorrectly capitalizing the property names, the resolver will fail to get credentials from it and will move on to the next provider in the chain. After failing to get credentials from the rest of the providers, it will finally get to the last provider, the EC2MetadataCredentials provider and try to request credentials form EC2's Metadata Service. Since you're not running your application on EC2, the request will timeout, and because there are no more providers down the chain, the error is thrown.\n. If you don't intend to source credentials from anywhere but the specified .json and would like an error to be raised after failing to retrieve credentials from it, you can create your own chain with just the FileSystemCredentials provider:\nvar myCredentials = new AWS.FileSystemCredentials(path);\nvar chain = new AWS.CredentialProviderChain([myCredentials]);\nchain.resolve(function(err, creds) {\n    if (err) {\n        // handle the error however you like here\n    } else {\n        AWS.config.credentials = creds;\n    }\n});\nAnother workaround, if you want to use loadFromPath() and catch the error, is to set the default provider to an empty array. Then loadFromPath() becomes synchronous (since it's no longer sending a request to EC2 Metadata Service), and you can use a try-catch block to catch the error.\nAWS.CredentialProviderChain.defaultProviders = [];\ntry {\n    AWS.config.loadFromPath(path);\n} catch (err) {\n    // handle the error here\n}\n. By STL, do you mean STS (Security Token Service)? If so, what API are you calling to get credentials? It may not be necessary to load the credentials from STS to a JSON file. If you are calling either the assumeRole or getSessionToken APIs to get credentials, then you can directly use the TemporaryCredentials provider to load the credentials from STS to AWS.config.credentials. (See http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/TemporaryCredentials.html). If you are calling assumeRoleWithWebIdentity or assumeRoleWithSAML, you can similarly use the WebIdentityCredentials and SAMLCredentials providers, respectively.\nLet me know if that helps!\n. I'm glad to hear it!\n. Hi @mediaupstream \nThanks for sharing your concern. Do you have a particular use case where you would need to point your params object to null as its prototype in production code? It is common practice to call hasOwnProperty on objects and assume that Object.prototype is in its prototype chain, and we have never seen any customers run into this theoretical edge case in practice. However, please do let us know if you have a special use case that would require this edge case to be addressed. Thanks!\n. Hi @mediaupstream \nThanks for the suggestion. We will add this as a feature request.\n. Thanks for the PR!\n. Hi @jjshoe \nThe SDK does not currently support sourcing configuration from ~/.aws/config, as this is intended for the AWS CLI.\nThese are the supported ways for configuring the region in the AWS SDK for JavaScript:\nYou update the region in your code using AWS.config.update():\nAWS.config.update({region: 'us-east-1'});\nOr you can set your environment variable AWS_REGION to the desired region, and this will allow you to keep the region private and not hard-code it in your code.\nOr you could also set your configuration, including the region, in a .json file, and then load it by providing the file path to AWS.config.loadFromPath():\nAWS.config.loadFromPath('./myConfiguration.json');\nNote that using this method will delete and replace your existing configuration with the configuration in your .json file, so you will either need to specify all of your global configuration here or load it first before updating any additional configuration.\nLet me know if that resolves the error you're running into!\n. This is a feature request that we are looking into implementing in the future.\n. Resolves #1038 \n. Resolves #1038 \n. Hi @glenswift \nWhat is your S3 client's region when you don't explicitly put a region in the options when instantiating? You can check s3Client.config.region. Also, what version of the AWS SDK are you using? Can you provide any further information about your SDK configuration and other steps to replicate this? I was not able to replicate the error with the above information. What is the exact error code and error message you are receiving? Another helpful diagnostic to see if your request is failing on the first try or on subsequent retries is to put to pass in the console as the logger param (s3Client = new aws.S3({logger: console, //...})). If the timeout error you are receiving is happening on a retry rather than the first try, it might help to see what kind of error you are receiving on the first try. You can see this by setting maxRetries to 0 (new aws.S3({maxRetries: 0, //...})).\n. Hi @glenswift \nWhat is your S3 client's region when you don't explicitly put a region in the options when instantiating? You can check s3Client.config.region. Also, what version of the AWS SDK are you using? Can you provide any further information about your SDK configuration and other steps to replicate this? I was not able to replicate the error with the above information. What is the exact error code and error message you are receiving? Another helpful diagnostic to see if your request is failing on the first try or on subsequent retries is to put to pass in the console as the logger param (s3Client = new aws.S3({logger: console, //...})). If the timeout error you are receiving is happening on a retry rather than the first try, it might help to see what kind of error you are receiving on the first try. You can see this by setting maxRetries to 0 (new aws.S3({maxRetries: 0, //...})).\n. Hi @glenswift \nI was able to replicate this error when the Body param is a stream. The reason this is occurring is that when the S3 client is configured to a region that is different from the region of the bucket you are making a request to, the SDK attempts to get the correct region and retry your request in the correct region. However, in Node.js, streams are not rewindable, so the SDK is unable to retry the request, and you will get a timeout.\nThe recommended solution is to configure your S3 client to the correct region. You can either configure the region globally by setting AWS.config.region before instantiating your S3 client or by specifying the region when you instantiate the S3 client, as you have done above.\nIf you don't know a bucket's region beforehand, here are a couple of workarounds:\nCall S3's getBucketLocation operation to get the region before configuring your S3 client.\nIf you prefer to use the SDK's region redirection feature for S3 (such as the case when you are using one S3 client to make requests to multiple buckets in different regions), you can set maxRetries to 0 when instantiating the S3 client, and manually retry the request if an error is returned. The SDK will cache the bucket's region, and when you retry the request, the request will automatically be corrected to use the correct region for that bucket.\nLet me know if these workarounds work for your use case!\n. Hi @glenswift \nI was able to replicate this error when the Body param is a stream. The reason this is occurring is that when the S3 client is configured to a region that is different from the region of the bucket you are making a request to, the SDK attempts to get the correct region and retry your request in the correct region. However, in Node.js, streams are not rewindable, so the SDK is unable to retry the request, and you will get a timeout.\nThe recommended solution is to configure your S3 client to the correct region. You can either configure the region globally by setting AWS.config.region before instantiating your S3 client or by specifying the region when you instantiate the S3 client, as you have done above.\nIf you don't know a bucket's region beforehand, here are a couple of workarounds:\nCall S3's getBucketLocation operation to get the region before configuring your S3 client.\nIf you prefer to use the SDK's region redirection feature for S3 (such as the case when you are using one S3 client to make requests to multiple buckets in different regions), you can set maxRetries to 0 when instantiating the S3 client, and manually retry the request if an error is returned. The SDK will cache the bucket's region, and when you retry the request, the request will automatically be corrected to use the correct region for that bucket.\nLet me know if these workarounds work for your use case!\n. Hi @glenswift \nA slight correction to my comment above: the SDK cannot retry requests with stream bodies for putObject. However, the SDK's managed uploader (s3client.upload) is able to retry them because it buffers the stream (or each upload part if the file is greater than 5MB) into memory before sending the request or series of requests. The PR above adds a comment to the API docs about this. Feel free to open up a new issue if you have any trouble with the managed uploader.\n. Hi @glenswift \nA slight correction to my comment above: the SDK cannot retry requests with stream bodies for putObject. However, the SDK's managed uploader (s3client.upload) is able to retry them because it buffers the stream (or each upload part if the file is greater than 5MB) into memory before sending the request or series of requests. The PR above adds a comment to the API docs about this. Feel free to open up a new issue if you have any trouble with the managed uploader.\n. Hi @jhludwig \nBy default, the dist build only includes services that have enabled CORS support. If you are working in an environment that doesn't enforce CORS, you can create a custom build here: https://sdk.amazonaws.com/builder/js/\nLet me know if you have any questions!\n. Hi @jhludwig \nBy default, the dist build only includes services that have enabled CORS support. If you are working in an environment that doesn't enforce CORS, you can create a custom build here: https://sdk.amazonaws.com/builder/js/\nLet me know if you have any questions!\n. We are working towards adding CORS support for all services in the future. I will mark this as a service change and let you know when CORS support has been added to SWF.\n. We are working towards adding CORS support for all services in the future. I will mark this as a service change and let you know when CORS support has been added to SWF.\n. Hi @jhludwig \nI will reach out to the SWF team again for an update. It would be helpful if you could post this request for CORS support on the AWS Forum for SWF to add visibility to this issue. Thanks! I will close this issue as it is not an SDK issue.\nhttps://forums.aws.amazon.com/forum.jspa?forumID=133\n. Hi @jhludwig \nI will reach out to the SWF team again for an update. It would be helpful if you could post this request for CORS support on the AWS Forum for SWF to add visibility to this issue. Thanks! I will close this issue as it is not an SDK issue.\nhttps://forums.aws.amazon.com/forum.jspa?forumID=133\n. I couldn't add the .changes directory yet because I can't commit an empty directory, but we can add it when the first changelog json is added.\n. I couldn't add the .changes directory yet because I can't commit an empty directory, but we can add it when the first changelog json is added.\n. For now, service updates for releases will need to have JSON files manually created in the next-release directory along with the JSON files for PRs.\n. For now, service updates for releases will need to have JSON files manually created in the next-release directory along with the JSON files for PRs.\n. New changes have been pushed. The README contains a fictional example of an entry in the changelog that we should swap out for a real example when we create our first real entry.\n. New changes have been pushed. The README contains a fictional example of an entry in the changelog that we should swap out for a real example when we create our first real entry.\n. Hi @tepez \nAs you mentioned, creating new service clients (AWS.XXX objects) for each request does have the downside of not allowing those requests to share certain caches or potential caches the SDK could implement in the future. However, it is not unreasonable to create new service clients for each request if this does not negatively impact your use case.\nIf you don't wish to create new service clients for each request, here is a workaround to still create separate logs for each request. Your server's handler could do something like this:\n``\nfunction(request, reply) {\n    // assume you have some function that returns a custom logger for your hapi request\n    var customLogger = getCustomLoggerForRequest(request);\n    var params = getAWSParamsForHapiRequest(request);\n    // for most service operation calls,this` in the callback is equal to the AWS.Response obj\n    s3.getObject(params, function(err, data) {\n        var awsResponse = this;\n        var awsRequest = awsResponse.request;\n        // create your own log message with the desired data from the response or request\n        var logMessage = generateLogMessage(awsResponse, awsRequest);\n        // this example assumes the customLogger is a writable stream\n        customLogger.write(logMessage);\n        // you can call customLogger.end() if you won't be writing any other log messages to it\n    // do other stuff you need for the server\n});\n\n}\n```\nLet me know if this works for your use case.\n. Hi @rahulpathakgit \nThis is a question for the service team, as this is not an SDK issue. Feel free to post your question on the AWS Forum for IoT: https://forums.aws.amazon.com/forum.jspa?forumID=210\n. \ud83d\udea2 \n. Hi @kengoldfarb \nThanks for reporting the issue. This is a service issue, and S3 is working on it.\n. Hi @kengoldfarb \nThe S3 team posted back on the forum thread on Aug 3 that the fix has been deployed. I am closing this issue. If you have any questions about this fix, please feel free to post back on the forum thread. Thanks!\n. \ud83d\ude80 \n. @chrisradek \nI've updated based on the feedback. Note that calling stream.end() will only emit an 'end' event after the stream has been piped into a writeable stream. But this behavior is consistent with what was happening before this PR anyway, so this is the desired behavior.\n. Hi @kunagpal \nThanks for your question. The API docs give descriptions of each property you might get back from the response, but it is not a list of guaranteed properties. The SDK does not require the service to send back certain properties in their response, so you should always check for the existence of a property before using it. This applies to all services, not just Elastic Beanstalk. I hope that answers your question! Please let me know if you have more questions.\n. Hi @kunagpal \nYou mentioned your recent calls did not return the Resources key. On what date did you notice this change? And before that, were you consistently getting back the Resources key for the same calls? I'd like to investigate if there is a change in behavior on the service side.\nIf you need the information on the environment resources, you can call describeEnvironmentResources and pass in the name or ID for the environment.\n. Hi @kunagpal \nYou're right, it would indeed be more convenient to not need to make an extra API call. However, it's possible that adding an extra potentially large object to the response could significantly increase the size of the response and force some customers to have to use pagination (which also requires extra API calls) in cases where they otherwise wouldn't need to when they don't need the Resources information. Feel free to post a feature request to the Elastic Beanstalk service team in the AWS Forums: https://forums.aws.amazon.com/forum.jspa?forumID=86, and the service team can determine what solution will best serve most customers' use cases.\nHope that helps. Let me know if there are other SDK-specific questions you have related to this issue.\n. Hi @clifflu \nThanks for the PR! I will merge it in.\n. Hi @djabry and @lukevanhorn \nThanks for opening this issue. This is a known bug with IE11 and Edge. IE11 and Edge browsers do not always correctly expose headers specified by the server. We have run into this issue recently with other services, and it doesn't seem to affect any browsers other than IE and Edge. I also found a Stack Overflow question regarding this (although it only mentions IE11): http://stackoverflow.com/questions/34696157/ie-misses-headers-angular-http\nI would suggest opening a ticket with Microsoft about this. As this is not an issue with the SDK or with AWS, I will be closing this issue. Thanks!\n. Hi @djabry and @lukevanhorn \nLambda has deployed a fix, and it should now work in IE and Edge. Thanks!\n. Hi @lukevanhorn \nI spoke with the Lambda team and they have not yet deployed the fix to all regions. What region are you making your lambda calls to? I will follow up with the service team and let you know when it has been deployed to your region. And thanks for opening the issue with IE!\n@djabry, glad to hear it works for you!\n. I've confirmed it, too. Glad it's working!\n. Hi @dhatawesomedude \nThanks for your question. Promises are not supported in the SDK prior to version 2.3.0. If you update to 2.3.0 or later, the setPromisesDependency property will be on AWS.config.\n. Hi @dhatawesomedude \nPromises are currently only supported on operations that return a Request object. Since s3.upload is a custom function that returns an instance of ManagedUpload rather than Request, promises are not currently supported for that operation. However, this is a feature request that we are looking into.\nAs a current workaround, if you know your file's size, you can use the putObject operation for file sizes <= 5MB, or use the multipart upload operations for file sizes greater than 5MB. The SDK does support promises on all of these operations. You could also create the promise:\nvar s3UploadPromise = new Promise(function(resolve, reject) {\n    s3.upload(params, function(err, data) {\n        if (err) {\n            reject(err);\n        } else {\n            resolve(data);\n        }\n    });\n});\nI hope that helps!\n. Hi @niftylettuce \nFor your above comment about the unhandled promise rejection, are you referring to when you use es6-promisify? Are you referring to when you use the SDK promise support in the PR I submitted? I've tried to replicate this but have not gotten any rejections thrown (it's expected for the promise to be rejected if not all required params are present, but it shouldn't be getting thrown). Could you provide the exact code you are using to replicate this error? Thanks.\n. @niftylettuce \nAre you using Node v6.6.0 or higher? As of Node v6.6.0, a warning will be emitted when a promise is rejected and a catch handler isn't attached to the promise. It is not throwing an error, and execution will not stop. This is a new feature of Node, not a bug in the SDK.\nhttps://news.ycombinator.com/item?id=12509832\n. Promises don't work with try-catch blocks because they don't throw errors. They reject the errors, and you'll need to add a catch handler to the promise to handle the error. The SDK's promises are designed to reject any errors that would normally get passed into the error-first callback when a callback is used instead of a promise. In the case of callbacks, if you don't pass in the required params, an error will be passed into your callback, and you as the user can decide how you want to handle the error in your callback. If you don't write explicit logic in your callback to handle the error, then the error is silently swallowed (and the SDK cannot control what happens in the user-supplied callback). Similarly for promises, it's up to the user to add a catch handler to handle rejected promises. If you don't add a catch handler, then it is analogous to receiving the error in a callback but not writing any error-handling logic in the callback. The SDK cannot handle the error for you because then the user will no longer be able to choose how to handle the error.\nThere are no unhandled promises somewhere in the internals of the SDK. The SDK does not generate promises anywhere unless you explicitly call the .promise() method on the AWS.Request class (and in the PR, a few additional classes), in which case the method returns the raw promise to you to handle as you see fit. If then and catch handlers are not added by the user, then the behavior is consistent with any regular promises outside of the SDK, which is that rejections and resolutions get swallowed silently. Does that make sense?\n. Hi @valgaze \nThanks for the PR. I will merge it in.\n. The PR has been refactored to move class-specific promisification to static methods on the classes themselves rather than in a switch-case statement. util.addPromises still exists (since there's still some common logic I'd rather not duplicate in the static methods of each class), but has been refactored to not require config.js anymore, which means I had to move the default promise dependency (the native Promise) determination to this method rather than config.getPromisesDependency. I prefer to keep only one source of truth for the default dependency, in case we ever want to change it (for example, if a future ECMAScript specification had a new backwards-incompatible Promise2 or something like that). Not requiring config.js in the util method means the module loading order is no longer wonky, and the promisification of CredentialProviderChain can be done in the credential_provider_chain.js now.\n. Hi @mark3072 \nThanks for bringing up this issue. Amazon has not dropped support for IE. I will investigate this issue further and let you know.\n. Hi @mark3072 \nLambda has rolled back this change and it should work now on IE and Edge. Thanks!\n. \ud83d\ude80 \n. Hi @Jarvie8176 \nThanks for opening an issue. I will investigate this error.\n. Hi @gswalden \nThanks for the PR. Unfortunately, we cannot accept PRs that modify the apis directory, as those are maintained by service teams and are shared amongst all of our SDKs, not just the JavaScript SDK. If you would like, you can post on the AWS Forum for S3 to request the change: https://forums.aws.amazon.com/forum.jspa?forumID=24\nI will close this PR as we cannot merge it. Thanks.\n. Hi @vfxBoat \nAre you using the S3 managed uploader (s3.upload())? How large is the file you are uploading? Are you using streams in the Body param?\nCould you clarify what you mean by the moment all data is sent? Do you mean the moment you call the S3 operation, or are you tracking the progress of a multipart upload through the managed uploader and referring to the moment when the uploader indicates that all of the data has been uploaded (see the docs for the ManagedUpload class for the 'httpUploadProgress' event)? \nAnd could you clarify what you mean by the moment the upload finishes? Is that the moment that the 'httpUploadProgress' event indicates all data has been uploaded, or are you referring to the moment when the callback is triggered?\nIf your file is larger than 5MB and you are using s3.upload(), the managed uploader will break the file up and perform a multipart upload, so not all of the data is sent at once, so there is a delay between when the operation is called and when the multipart upload completes. If your file is extremely large, then it may very well take minutes to upload. Also keep in mind that depending on your connection speed, there will also be a delay in when the callback is triggered after all of the data is sent because the callback is only called after the client receives a response from the service.\n. Hi @vfxBoat \nAs you've said, it is normal for there to be a delay after all of the data is sent and when the callback is triggered, since the completeMultipartUpload operation still needs to be called. However, you mentioned that in the issue title that the delay is minutes?\nYou can see how much time each request takes by adding your console to the logger option when you construct your S3 service client.\nvar s3 = new AWS.S3({logger: console});\ns3.upload(yourCallback);\nThen each response time (from each of the multipart upload requests) will be logged:\n[AWS s3 200 0.095s 0 retries] createMultipartUpload(...)\n[AWS s3 200 0.358s 0 retries] uploadPart(...)\n[AWS s3 200 1.311s 0 retries] uploadPart(...)\n[AWS s3 200 0.216s 0 retries] completeMultipartUpload(...)\nCan you add this logger option and see whether the completeMultipartUpload request is taking minutes? And how much time do the other requests take (if the other requests are very fast, then it's unlikely simply a slow connection).\n. Closing due to inactivity.\n. Hi @borisirota \nThanks for your question. To clarify, is this a question/request for the amazon-cognito-identity-js SDK, or for the AWS SDK? If the amazon-cognito-identity-js SDK is already caching the session credentials in local storage, could you clarify why the AWS-SDK would also need to repeat those actions? Thanks.\n. Hi @NotBobTheBuilder \nThanks for the suggestion! We will look into what we can do without breaking current customers.\n. While I realize this is not an ideal solution and we are looking into your feature request, this temporary workaround makes it at least possible to use this method in conjunction with promises:\nnew Promise(function(resolve, reject) {\n    s3.getSignedUrl('getObject', {Bucket: 'bucket', Key: 'key'}, function(err, data) {\n        if (err) {\n            reject(err);\n        } else {\n            resolve(data);\n        }\n    });\n}).then( ... )\n. Hi @NotBobTheBuilder \nThere is actually already a feature request open for this, in issue #1008 \nClosing this one to avoid duplicates. But thanks for voicing this request, as this helps us prioritize feature requests! Feel free to +1 the other open issue for this request.\n. Hi @dukedougal \nThanks for voicing your concern, as it helps us prioritize issues. We have been actively looking at Webpack support, and we appreciate your patience.\nI am closing this as it is a duplicate of the other one.\n. Hi @pahud \nThanks for your question. At the time when you construct chain, the AWS.CredentialProviderChain.defaultProviders array is copied and stored at chain.providers. When you modify AWS.CredentialProviderChain.defaultProviders, previously constructed instances of the CredentialProviderChain will keep the same providers array that they had at the time they were constructed. Modifying AWS.CredentialProviderChain.defaultProviders will only affect new instances going forward. To make your code above work, you would simply need to move the line where you construct chain below the line where you change defaultProviders. Let me know if that resolves the problem!\n. Hi @riteshatsencha \nWhat version of the AWS SDK are you using? The latest version should have this function. Are you still seeing this problem after you update the version?\n. The earliest version which has this function is 2.3.11, released on May 12.\n. Hi @riteshatsencha \nThis is a service-side issue, as the SDK has no control over the response sent back from the service. Please post this question on the AWS Forums to engage the service team: https://forums.aws.amazon.com/category.jspa?categoryID=3\nThanks!\n. Hi @riteshatsencha \nActually, can you tell us what version of the AWS SDK you are using? This may actually be an issue with your version of the SDK being outdated. For example, the Owner property in the response was not added to the API model until version 2.3.11, so if your SDK version is older than that, you will not see the Owner property in the response. Try updating to the latest version, and if you are still missing properties in the response, then feel free to post on the AWS Forum. Thanks!\n. Hi @riteshatsencha \nCould you post this question on the AWS Forum, as it is a service-side issue rather than an issue with the AWS SDK. The SDK does not have control over the response sent by the service.\nhttps://forums.aws.amazon.com/category.jspa?categoryID=3\nThanks!\n. Hi @riteshatsencha \nThis may actually be due to an outdated SDK version. Try updating to the latest version of the SDK, and if you are still missing properties from the response after that, then feel free to post in the AWS Forum. Thanks!\n. Hi @Jovons \nThanks for your question. Currently there is not a way to make calls to S3 from the browser that are not on the level of a bucket, since CORS is configured for individual buckets, not for the entire service. This is a service issue, not an SDK issue. I have contacted the service team about this feature request, but please post on the AWS Forum for S3 to let them know how important this feature is to you, as this helps them prioritize feature requests: https://forums.aws.amazon.com/forum.jspa?forumID=24\nOne workaround you could try is create an AWS Lambda function that creates the bucket in S3, and you can invoke the Lambda function from the browser, since CORS is fully supported for Lambda. The AWS SDK is automatically included in the Lambda function, so you can make the same SDK calls as you would in the browser, but without the CORS restrictions. Let me know if this works for your use case.\n. Hi @Jovons \nThe createBucket operation can be used in Node and in any environment that doesn't enforce CORS. The code is shared between the Node SDK and the browser SDK, which is why the operations that don't support CORS are in the SDK. Also, some customers are using the browser SDK in environments that don't enforce CORS, so we cannot simply take those operations out of the browser SDK. We can certainly add a note to the API documentation though.\n. Hi @rstahl \nThanks for you question. What versions of IE are you using (and are you including Edge)? Can you share the relevant code snippet from your Lambda function? Which version of Node are you using in your Lambda function?\n. Hi @lennym \nThanks for bringing this to our attention. I will look into why this is happening, as the JavaScript SDK does not have any customizations for Lambda that would restrict MaxItems to a maximum of 50.\n. Hi @lennym \nThe JavaScript SDK does not add any maximum limits on Lambda's MaxItems, so the Lambda service has defined that limit.\nThe CLI has a customization that overrides the maximum items imposed by service teams. It essentially uses Paginators under the hood to request more items until the user-specified maximum is reached. We can mark this as a feature request if you would like this to be implemented in the JavaScript SDK as well. If you would like to implement this yourself in your code, you can currently use the eachPage function on the Request object to do something like this:\nvar results = [];\nvar maxItems = 100;\nvar req = lambda.listFunctions();\nreq.eachPage(function(err, data) {\n    if (err) {\n        console.log(err);\n        return false;\n    } else {\n        for (var i = 0; i < data.Functions.length; i++) {\n            // returning false will stop `eachPage` from requesting more results\n            if (results.length >= maxItems) return false;\n            results.push(data.Functions[i]);\n        }\n    }\n});\nIf you don't want the SDK to make multiple requests and would like to receive the specified maximum results in one request, then you can request that the service team raise the maximum limit on the AWS Forums: https://forums.aws.amazon.com/forum.jspa?forumID=186\nHope that helps!\n. @lennym \nThanks for raising the issue on the AWS Forums. Since this isn't a change you are looking for in the SDK, I will remove this as a feature request. Have a good day!\n. @oliversalzburg \nCurrently, .promise() can only be called on instances of the AWS.Request class, which is what is returned from most service operation calls. We can mark this as a feature request.\nAre you calling .refresh() directly? It's recommended to call .get() instead on credential providers so that new credentials are only retrieved if the current ones are expired. The .get() doesn't return a Request instance that can be promisified either, I just wanted to see your use case for calling .refresh() directly.\n. Hi @oliversalzburg \nI agree it would provide a more consistent and easier experience if all asynchronous functions that take a callback also had a promise interface without users having to write promise wrappers. I've marked this as a feature request.\n. The waiters are already pulling in the specific version (see line 259 of plugin.rb). The paginators.json files aren't used in our docs (instead, we rely on documentation from service teams about their service-specific paginator-related params (such as NextToken, MaxKeys, etc.))\n. Hi @jamesongithub \nIt looks like the data returned from the request is missing a \"Token\" property, which is required by the ECS credentials provider. The documentation you linked to shows \"Token\" as a field that should be returned in the payload. If you have questions on why the \"Token\" field is missing from the payload, please post on the AWS Forum for the ECS service: https://forums.aws.amazon.com/forum.jspa?forumID=187\n. I like these changes. In addition to fixing the bug, it also now lazy loads service API models even when you directly require an individual service, which is consistent with how it was done before version 2.6.0, and consistent with how models are loaded when all services are required. Even better, now it only loads the model for a specific version.\n\ud83d\ude80 \n. Hi @mjmasn \nThis is a service-side issue and not an issue with the SDK. Please post your question on the AWS Forums for SES: https://forums.aws.amazon.com/forum.jspa?forumID=90\nThanks!\n. Hi @vkhazin \nAccording to the error message, you will need to use v4 signing. When you instantiate the s3 client, pass in an object with the property signatureVersion set to value 'v4'.\nconst s3 = promise.promisifyAll(new aws.S3({signatureVersion: 'v4'});\nPlease let me know if this resolves your issue. Thanks.\n. Hi @vkhazin,\nHere's more information on the issue:\nIn version 2.4.0 of the SDK, we introduced a change to default S3 service clients to signature v4 if no signature version is specified. In version 2.4.12, we reverted back to defaulting S3 service clients to signature v2 as a bugfix, due to issues with non-ascii characters in headers with signature v4. Please see 2.4.12 in the our changelog (https://github.com/aws/aws-sdk-js/blob/master/CHANGELOG.md). The reason your code worked in version 2.4.4 without specifying a signature version is because it defaulted to v4, but with version 2.4.12 and later, you will need to explicitly set the signature version to v4. Thanks.\n. Great! I'm glad to hear your issue was resolved.\n. Hi @MaxVita \nSo according to the other issue, version 2.5.6 works, but you start getting this error in 2.6.0 and above? Or does it only start failing with 2.6.3? The version of browserify that the SDK uses was updated in 2.6.0. I'll investigate to see why it's causing this error. Thanks.\n. \ud83d\udea2 \n. Hi @somprabhsharma \nThanks for reporting the issue. I'm looking into this.\n. Hi @somprabhsharma \nIt looks like this error is coming from the service, not from the SDK, as the SDK does not display the error you provided above for param validation. I could not reproduce the error with the example you provided above. Can you log out this.httpResponse.body and this.httpResponse.statusCode inside the callback function you supplied to batchGetItem? If these are defined, then the error is coming from the service's response, not from the SDK. Can you verify if that is the case?\n. :shipit: \n. Hi @donpark \nI was not able to reproduce this behavior. When I use ExposeHeaders as shown in the documentation, there is no error, but using ExposedHeaders causes errors. This matches with the Access-Control-Expose-Headers header in the CORS spec: https://www.w3.org/TR/cors/\nCan you provide more information for reproducing this behavior? What version of the SDK are you using? Can you show the relevant snippet of code? What does your CORSConfiguration json param look like? Does the its structure match what is shown in the documentation? http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#putBucketCors-property\nThanks.\n. I'm glad to hear it resolved!\n. Hi @ramthakur1991 \nThis error is not coming from the SDK. The error code NO_SUCH_SOCKET is thrown from the sails-hook-sockets module (https://github.com/balderdashy/sails-hook-sockets/blob/c52a2bebb89c5ff325b4c8a5a79fd338fbdaa3de/lib/sails.sockets/get-socket-by-id.js#L18). I hope that helps. I am closing this as this is not an issue with the AWS SDK. Thanks!\n. Hi @ranode \nThere is currently not an equivalent operation in the AWS SDK for JavaScript, but I will mark this as a feature request. Thanks!\n. Hi @zymr-keshav \nCertain S3 operations do not support CORS, since CORS must be configured on the bucket level. Operations performed specifically on an already-existing bucket with CORS configured will work, such as getObject, listObjects, etc. Operations that span buckets or that are performed on currently-non-existent buckets will not work, such as listBuckets and createBucket. Even if all of your buckets in your account have CORS properly configured, listBuckets will not work because it does not access any of the CORS configuration on individual buckets. If you just want to perform a check by calling any method, I recommend you call headBucket or listObjects, which each only require the bucket name as a param. Also note that you can pre-attach params when you construct the S3 service client so that you don't need to specify params when making operation calls (which can save some work if you are going to make multiple calls with the same service client):\nS3Bucket = new AWS.S3({\n                        apiVersion: '2006-03-01',\n                        accessKeyId: id,\n                        secretAccessKey: key,\n                        region: region,\n                        params: {Bucket: bucket}\n });\nIf you would like S3 to support CORS for non-bucket operations, you can voice your feature request on the AWS Forums for S3: https://forums.aws.amazon.com/forum.jspa?forumID=24\nI hope that helps. I will close this issue, as it is not an SDK issue. Thanks!\n. That's a great idea! I will work on updating our documentation to show which methods do not support CORS.\n. Hi @andreineculau \nThe AWS JavaScript SDK does not currently support the AWS CLI config file, which is intended for the AWS CLI. However, we are looking into this as a feature request.\nWe already have this open as a feature request (https://github.com/aws/aws-sdk-js/issues/1039), so I will mark this as a duplicate and close it. Thanks.\n. Thanks for the suggestion! I've marked it as a feature request\n. Hi @Answashe \nThanks for the suggestion! I will mark this as a feature request.\n. Hi @Answashe \nThe feature has been merged in and is available immediately on Github, and will be available from NPM in the next release. Thanks again for bringing this to our attention!\n. Hi @mutaflux \nAre you trying to directly pass expiring credentials into the updateForPresigned() method? The entire V4 signer class (and all other signer classes in the SDK), including its methods, are marked as a private API, which means it's only meant to be called internally by the SDK. Tracing back the stack, the only function that calls updateForPresigned() is the addAuthorization() method in the V4 signer class, which is only invoked by the SIGN event listener callback (in lib/event_listeners.js), which invokes service.config.getCredentials() before invoking addAuthorization(). The getCredentials() method (in lib/config.js) refreshes the credentials if they are expiring. All of the other functions I mentioned that get invoked after it are private, and only get invoked after getCredentials() has refreshed the credentials. Users of the SDK are able to successfully use V4 signing in their requests even with expiring credentials, because getCredentials() makes sure they are refreshed. What is your use case for directly passing credentials into updateForPresigned()?\n. Hi @ath88 \nYou should just be able to specify the bucket name as a param when you make the request, and it should override the bound bucket name without altering the original instance. I've tried this multiple times and it works:\nvar s3 = new AWS.S3({params: {Bucket: 'bucket1'}});\n// this should make a call to 'bucket1'\ns3.headBucket(callback);\n// this should make a call to 'bucket2'\ns3.headBucket({Bucket: 'bucket2'}, callback);\n// and since the instance was not altered, this should again go to 'bucket1'\ns3.headBucket(callback);\nIs this not the case for you? If so, which part is not working for you? Is the problem that the bucket name isn't being overridden (second call not going to 'bucket2')? Or is the problem that this is altering your instance (3rd call is going to 'bucket2' instead of back to 'bucket1')? What version of the SDK are you using?\n. Hi @ath88 \nThe first code example you provided is not programmatically possible in its current form (at least for achieving what you described), since when you call uploadApple(s3), neither uploadApple() nor s3 has any way of knowing which function (uploadAppleToRedFruitsBasket() or uploadAppleToSweetFruitsBasket) invoked it, and so it cannot change its params accordingly. No change can be made in the SDK that can make the first code example work. However, you can tweak the code example to make it do what you want without having to clone a new s3 object. The easiest way I believe is:\n```\nvar s3 = aws.S3();\nvar uploadAppleToRedFruitsBucket = () => uploadApple(s3, \"red-fruits\");\nvar uploadAppleToSweetFruitsBucket = () => uploadApple(s3, \"sweet-fruits\");\nuploadAppleToRedFruitsBucket();\nuploadAppleToSweetFruitsBucket();\nfunction uploadApple(s3, bucketName) {\n    // I didn't add it, but don't forget the Key param and the callback!\n    s3.upload({ Body: \"red-and-sweet-apple\", Bucket: bucketName });\n}\n```\nIf you would rather not have to pass a the bucket name into uploadApple(), you could do:\n```\nvar s3 = aws.S3({ params: { } });\nvar uploadAppleToRedFruitsBucket = () => {\n    s3.config.params.Bucket = \"red-fruits\";\n    uploadApple(s3);\n}\nvar uploadAppleToSweetFruitsBucket = () => {\n    s3.config.params.Bucket = \"sweet-fruits\";\n    uploadApple(s3);\n}\nuploadAppleToRedFruitsBucket();\nuploadAppleToSweetFruitsBucket();\nfunction uploadApple(s3) {\n    // I didn't add it, but don't forget the Key param and the callback!\n    s3.upload({ Body: \"red-and-sweet-apple\" });\n}\n```\nIn my opinion, the first one I suggested is cleaner and uses a more appropriate functional design than the second one, but if not passing a bucket name into uploadApple() is important to you, then the second one will work.\nDoes that help?\n. Hi @ath88 \nYour first code example (without modifications) is just simply not possible in JavaScript, and it's not due to any limitation of the SDK, so no change in the SDK can make it possible. Neither uploadApple() nor s3 is able to know which function invoked it, and if you modify s3 outside of the two functions rather than in the functions, then it has no way of knowing which of the two functions is going to get invoked next and will just stay in its modified state (if you don't modify it inside the function, s3 is not going to modify itself depending on which function gets invoked). It doesn't matter if one function is defined after the modification, as both functions are still referring to the same object in memory (and the reason that your current hack works is because the two functions are referring to different objects in memory). You can see the same thing outside of the context of the SDK with this example:\n```\nvar obj = {hello: 'world'};\nvar func1 = () => finalfunction(obj);\nobj.hello = 'everyone';\nvar func2 = () => finalfunction(obj);\nfunc1(); // 'everyone'\nfunc2(); // 'everyone'\nfunction finalfunction(obj) {\n    console.log(obj.hello);\n}\n```\nIf you want uploadApple(s3) to have different behavior (in this case, upload to different buckets), then uploadApple() and s3 cannot both be agnostic to which bucket needs to be used. And the only way s3 knows which bucket to use before being passed into uploadApple() is if the function that invokes uploadApple() modifies s3. The way your first code example is set up, uploadAppleToRedFruitsBasket and uploadAppleToSweetFruitsBasket are ALSO agnostic to the bucket name (in fact, they are identical copies of the same function, just with different function names, so invoking them will have identical behavior). It doesn't make a difference where the functions are defined.\nYou're right that you should always be careful with depending on mutable arguments when dealing with asynchronous code, because of the race conditions caused by uncertainty of when the argument may get modified by other competing functions. This is why I prefer my first example over my second, because it's a more functional approach (and since we expect uploadApple() to have different behavior, I don't believe it should be agnostic to the parameter that determines the behavior). However, in this particular case, the second example also works because I know that s3.upload() copies the params onto the request (synchronously) before signing and sending the request. (You would have to be careful if any of the params were mutable objects).\nI hope that helps :) Let me know if you have additional questions.\n. I see, thanks for clarifying.\nYou can currently copy the config of a service client object by passing the config into the constructor:\nvar s3 = new aws.S3({ params: { Bucket: \"red-fruits\" } });\nvar s3Clone = new aws.S3(s3.config);\ns3Clone.config.params = {Bucket: \"sweet-fruits\"};\nThe S3 constructor function will shallowly copy the object you pass to the config of the new instance. (Note that since it's only a shallow copy, you'll have to assign s3Clone.config.params to a new object rather than directly modifying s3Clone.config.params.Bucket.)\nIs that the functionality you're looking for? Thanks.\n. Hi @ath88 \nI'm glad to hear this works for your use case :)\nIf you already have access to s3 but not aws and would not like to require the aws-sdk, you could do:\nvar s3Clone = new s3.constructor.__super__(s3.config);\ns3Clone.config.params = {Bucket: \"sweet-fruits\"};\nCheers!\n. Hi @OrZipori \nThis could be an issue with your clock not being synced correctly. See:\nhttps://github.com/aws/aws-sdk-js/issues/401\nhttps://github.com/aws/aws-sdk-js/issues/527\nMake sure your clock is correct. You can also pass an option into the service client to correct any clock skew for you:\nvar sns = new AWS.SNS({correctClockSkew: true});\nIf that still doesn't work, what version of the SDK are you using? Looking through our issue history, it looks like updating the version of the SDK has resolved the problem for some.\nhttps://github.com/aws/aws-sdk-js/issues/265\nhttps://github.com/aws/aws-sdk-js/issues/254\nLet me know if neither correcting the clock skew nor updating the SDK version works.\n. Hi @OrZipori \nThis error occurs when your clock is significantly out of sync with the server, which will cause your signature to be invalid. If you want to reliably reproduce this error on any computer, just change the computer's clock to be a time that you know is incorrect, and send a request to any AWS service.\nWhen you set correctClockSkew to true, the SDK, upon receiving this error, will find the difference between the time on your clock and the time sent back by the server in the response header, and will resign the request with the corrected time and retry the request. Also, the time difference is cached, so any subsequent requests will automatically be corrected on the first try.\nI'm glad to hear that your issue is fixed. Let me know if you have additional questions.\n. Hi @zymr-keshav \nAccording to Elastic Beanstalk's Tomcat Platform History (https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/platform-history-java.html), the Solution Stack '64bit Amazon Linux 2015.03 v2.0.0 running Tomcat 8 Java 8' is only supported for Tomcat environments created between August 11, 2015 and September 18, 2015. To see currently supported platforms: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html\nIf you have questions about supported platforms, please feel free to post on the AWS Forum for Elastic Beanstalk: https://forums.aws.amazon.com/forum.jspa?forumID=86\nAs this is not an SDK issue, I will close this issue. Hope this helps!\n. Hi @misterfresh \nAre you using putObject for files over 5MB as well? putObject is not used to initiate multipart uploads. You will have to initiate that with createMultipartUpload, and then subsequent calls to uploadPart before completing it with completeMultipartUpload. I recommend using the managed uploader in the SDK (call the upload operation instead of putObject). This will handle uploads for both small and large files, and will abstract away the complexity of multipart uploads so you don't have to call the individual operations needed. See http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#upload-property\nLet me know if that resolves your issue. Thanks.\n. I'm glad to hear your issue is resolved!\n. Hi @kaihendry \nThe listObjects operation does not send back x-amz-meta- headers even if you do specify the header to be exposed in your bucket's CORS configuration. First, if metadata were sent back in the header, there would be no way of knowing which object each metadata header is associated with. Second, if multiple objects contain the same name for a metadata header, two headers cannot have the same name.\nEven if the SDK were to add a customization to retrieve each object's metadata, it could still only accomplish that by making a separate headObject request to every object in the bucket, which does not solve the efficiency issue you have.\nIf you are requesting that the S3 service attach object metadata in its listObjects response data (rather than as headers), it is beyond the SDK's control, so you can request this on the AWS Forums for S3: https://forums.aws.amazon.com/forum.jspa?forumID=24\nIs that what you were looking for, or is there a specific feature you are requesting from the SDK itself?\n. \u2618 \n. Hi @b2mdevelopment \nCan you share a code snippet that produces this error? What are the params you use in putItem/updateItem?\n. :shipit: \n. This test is testing whether the query string is alphabetically sorted by its field names. The listener that I'm removing here previously did not exist, as it was added as part of this PR, so I'm not removing something that used to be there. Since the listener I added in this PR changes the GET request to a POST, there is no longer a query string, and the test cannot check if it's alphabetically sorted. Therefore, I removed the listener so that just for the purpose of this test, the request is still a GET and has a query string. The other two options I have if I don't remove this listener is to switch the test to use a different operation (I could use suggest instead of search) or to remove this test altogether.\n. I will add a test for suggest, as this is the only operation that uses a GET.\n. It might be good to default s3DisableBodySigning to true only when sslEnabled is true. If sslEnabled is explicitly set to false, then s3DisableBodySigning should default to false (and the documentation comments above should note this). Even though shouldDisableBodySigning() in the s3.js file checks that the protocol is https, this makes it clearer (and may make debugging easier). And if {sslEnabled: false, s3DisableBodySigning: true} is explicitly set, I would even add a validation listener to return an error when a request is being validated, to make it clearer that this combination is not allowed.\nAt the least, the documentation above should note that s3DisableBodySigning doesn't work for non-https protocols.\n. Is there a reason to override the X-Amz-Content-Sha256 header if it already exists? If the reason to disable body signing is to prevent expensive computations that could slow down the browser, then what is the benefit of overriding it when it has already been computed? The only place in the SDK that adds the X-Amz-Content-Sha256 header is in the AWS.EventListeners.Core.COMPUTE_SHA256 listener, which is already removed if shouldDisableBodySigning() returns true.\nIf a customer decides to compute the Sha256 on their own and adds it to the header, it would seem like surprising behavior that their own header gets overridden, especially if they did not explicitly set s3DisableBodySigning to false and did not realize that the SDK defaults that option to true.\n. Keeping the argument as req instead of a boolean indicating if it's presigned would keep this file more generic to all services, since determining signer class based on if the request is presigned is S3 specific. Although the generic service getSignerClass currently does not use any arguments, it's possible that more custom getSignerClass methods could be written for individual services. The req.isPresigned() could easily be moved into S3's custom getSignerClass.\n. The SDK doesn't actually ever call clearBucketRegionCache without passing in bucket, except for in the unit tests where almost all of the tests store at most 1 bucket, so it shouldn't cause any performance issues in the SDK. The reason I decided to delete properties rather than replace the cache with a whole new object was that if customers wanted to call the method themselves, and they had assigned the bucketRegionCache to a variable (so they could avoid typing long chains such as response.request.service.bucketRegionCache multiple times), then clearBucketRegionCache would still behave the way they expect, and the bucketRegionCache they stored in their variable would be cleared and still valid. If I just replaced s3.bucketRegionCache with a new object, not only would that not clear the cache that they have stored in their variable, any modifications they make to the cache stored in their variable would unexpectedly no longer be accessible to the SDK. So the decision was mainly rooted in a more intuitive customer experience, but I don't see it making too big a difference either way, since the method is marked as private and I don't think there are many scenarios in which users would want to clear the entire cache instead of just a select number of buckets.\n. If the isPresigner variable isn't being used anymore, it should be removed.\n. Since the isPresigner variable is no longer used, it can be removed. (Same as a comment I made earlier, but I realized the other comment was made on an outdated diff)\n. Not only would it duplicate work, it would break the endpoint for virtual-hosted buckets because the bucket name gets populated twice, e.g. mybucketname.mybucketname.s3.amazonaws.com. This only is a problem for requests that have not been built yet (requests still on the validate step), since requests that are being retried go straight to the sign step and do not go through the build step anymore. This is also why populateURI needs to be called inside this function, because requests being retried need to update their endpoint but won't go through the build step.\n. Yes. The cache is on S3.prototype, so all S3 instances share the same cache.\n. Because NetworkingError can be caused by other reasons, it can occur when the region is set to us-east-1, but it won't be because of incorrect region. A BadRequest or AuthorizationHeaderMalformed would be received in that case instead, and would be handled by the extractError listener. If the region is set to us-east-1, this listener wouldn't be doing anything helpful for it anyway, since it is just either changing the region to us-east-1, or trying a region request in us-east-1. This exclusion filter is needed to prevent this listener from recursively sending off an infinite number of requests that cause the next request to be sent. For example, if there is something wrong with the network and we're receiving a NetworkingError because of it, then this listener would keep getting triggered because each request it sends off would receive another NetworkingError. So instead of allowing that, this listener will check that the region is not us-east-1, because otherwise the NetworkingError is not due to incorrect region.\n. The original httpRequest.endpoint was constructed by passing the service object endpoint into the AWS.Endpoint constructor. This essentially copies the non-hidden properties of the service object endpoint to a new object, which won't contain any of the hidden properties of the original. If the only purpose of constructing a new endpoint is to create a new copy so it's independent from the service object endpoint, then it won't be necessary to do it here, but I'll have to test to make sure the hidden properties don't make a difference.\n. if (key.toLowerCase().indexOf('x-amz-meta-') === 0) in case the 'x-amz-meta-' part isn't lowercased?\n. Can you include a corresponding test for each of these scenarios that tests that the cache is cleared for unauthorized users? So one for each of getId, getOpenIdToken, and getCredentialsForIdentity.\n. Good catch. I'll modify that.\n. We should put a maximum size constraint on cachedSecret. Before, there was only one cache per service (and only the last signature key for that service is kept), so the cache was necessarily bounded by the number of services. There is no upper bound on the number of service clients generated. Some applications could be generating a new service client for every request. The service client objects get garbage-collected eventually, but the cache doesn't and could grow to millions of objects fairly quickly and hog a lot of memory. Perhaps we could have a cacheIdentifierQueue of some sort so that when it's length reaches a certain number, we can get the oldest cacheIdentifier and delete it from cachedSecret.\n. EC2MetdataCredentials already had this feature, through the MetadataService. The refreshQueue on the ECSCredentials is analogous to the loadCredentialsCallbacks on the MetadataService. They're structured slightly differently but work the same way. (refreshQueue has to retain references to the instances of ECSCredentials because the data isn't given to the callback the way it is in MetadataService).\n. Assigning these fields to self is the only way the credentials can be accessed, since the callbacks for refresh are error-only callbacks, and when the callback returns with no error, the credentials are expected to have been loaded on to the instance of the credential provider itself. This is how it worked before on the ECSCredentials and how it works with the other credential providers. When a service request is made, the SDK checks directly on the credential provider for these fields.\n. I realized I misunderstood your question. It is indeed necessary to assign these fields to self, I've just simply moved them up into the callbacks function, and I'm passing this creds object containing all of the needed fields into callbacks. On line 118 above (if (!err) AWS.util.update(call.provider, creds);) is where these fields are getting assigned to the credential provider instance. The reason I can no longer use self is because if there are multiple callbacks in the refreshQueue coming from multiple instances of the ECS credential provider, only the first refresh() call will actually call this.request(), so self will only refer to that one instance of the credential provider. The call.provider in the refreshQueue is a reference to the instance of the credential provider where the corresponding callback in the refreshQueue is from. Before, assigning the fields directly to self was not a problem because every single refresh() call would invoke 'this.request() and the right instance of the credential provider would be referenced.\n. I've updated the PR to retry for 5xx status codes, as well as for 4xx status codes only if there is a retry-after header. I was able to reliably replicate the Metadata Service returning 429 (Too Many Requests) errors, which also returned a retry-after header which give a minimum number of seconds to wait before retrying.\n. Instead of maintaining a separate list (I know this is what we currently do anyway), we would have fewer places to worry about making manual changes for service updates if we move this information into metadata.json, e.g.\n{\n    \"acm\": {\n        \"name\": \"ACM\",\n        \"defaultService\": true\n    },\n    ...\n}\nThat way we can also get rid of the identical identical defaultServices lists in service-collector.js as well.\n. I can restrict it to just 429\n. Just a minor comment, but it never made much sense to me to have metadata.json in the apis/ directory where service models were. It was always the oddball in that directory, and always required extra logic (like this bit of code here) to exclude it from being counted as a service. It makes sense to me to move metadata.json to the root of lib/, and would allow us to get rid of this extra filter.\n. Sure, and it's not important for it be part of this PR. I noticed this filter function, and this isn't the first time it has crossed my mind that perhaps metadata.json should be moved, so I just decided to mention it.\n. Good catch, I'll modify it.\n. core.js already requires service.js, and the Service constructor can already be accessed at AWS.Service. Why does it need to be required here? We also wouldn't need to export Service in service.js\n. Similarly here, node_loader.js and browser_loader.js already require api_loader.js, and apiLoader can already be accessed at AWS.apiLoader. apiLoader would also not need to be exported from api_loader.js.\n. Delete\n. Since this function isn't used anymore, it can be deleted.\n. Is it possible that the AWS.apiLoader may be called before the api is actually loaded and stored on AWS.apiLoader.services? If that happens, this would throw an error (eg \"Cannot read property '11-30-2014' of undefined\"). We could either throw a more helpful error (such as \"S3 service not yet loaded. Require './clients/s3 first\"), or better yet, require the file for them to load the service (since that's what the original AWS.apiLoader does for you anyway, and although I don't think this is a public method, adding that would allow us not to break this method).\n. Yes, thanks for catching that.\nAnd for your question above about setting third-party promise dependencies when the environment supports native promises: all of the existing promisified methods will update to the set dependency because setPromisesDependency calls AWS.util.addPromises on the constructors (after I make the correction you mentioned above, of course). I've manually tested this. I can update one of the tests to be explicit about it.\n. Due to the order the modules are loaded, moving that line to the credential_provider_chain.js file would throw an error. However, I discovered this is due to the fact that I'm loading config.js from inside the AWS.util.addPromises() (which is needed if I want to get the promise dependency via AWS.config.getPromisesDependency(). I can refactor that so that I can move this line to the right file.\n. The reason I originally checked constructor.name is because I hadn't originally planned on requiring core.js, so I didn't have access to the actual constructors for an equality check before I added that require statement. I may be refactoring the require out anyway (as discussed above). I'll look into adding the static method for each class.\n. I can move these methods to the constructor.\n. Yup, I meant to change all of the function names to match their fields, but I missed that one.\n. Is there a reason the service model's shapes need to be on this namespace? When I type AWS.ACM., I don't think my code completion tool should try to complete AddTagsToCertificateRequest or CertificateBody, since these aren't actually valid properties on AWS.ACM. Instead, this namespace should contain apiVersions, serviceIdentifier, and services (unless these are private, in which case just don't have an ACM namespace). The shapes can live in a separate namespace that isn't exported, and you just have to change the references above. I've tested this by changing this namespace name to be Shapes (and changing references above), and I still get the correct code completion suggestions when I'm creating my params in a request, and I no longer get shape name suggestions when I type AWS.ACM..\n. This might not be a must-have,  but it would be nice to have code completion work when I'm binding params to a service client on construction.\nOne way you could do this is have a constructor such as:\nconstructor(options?: ServiceConfigurationOptions & ServiceParams)\nAnd then near the bottom of the file you can define:\ninterface ServiceParams {\n    params?: ACM.AddTagsToCertificateRequest &\n                    ACM.DeleteCertificateRequest &\n                    ...\n}\nSince the TSGenerator has to iterate through every operation input shape anyway, it could just keep track of a list of all of the input shape names.\n. Well, for S3, since you currently have\nconstructor(options?: ServiceConfigurationOptions & UseDualstackConfigOptions),\ncouldn't that just be changed to\nconstructor(options?: ServiceConfigurationOptions & UseDualstackConfigOptions & ServiceParams)?\n. I think the ServiceParams should be defined in the same file as where the service model shapes are defined (in the clients/#{service}.d.ts file), since they are specific to each service and they will reuse the model shapes definitions already in the file. And that would be the same file that the constructor definition for that service class is defined, too. For S3, it would be the same (the constructor definition is already in the same file as the model shapes definitions). The only exceptions would be any custom params (for example, partSize in the upload operation), which could be imported from the service customizations file (the S3Customizations class is already being imported from there anyway).\n. Also, as a modification to what I said above, instead of merging together all of the input shapes, it would be better if each potential param is explicitly listed, since they are all optional here, and we don't want to keep the requirements of the input params. Most services reuse most of their input params, so for many services the list of possible params may actually be shorter than the list of input shapes. So instead of what I have above, it could be more like:\ninterface ServiceParams {\n    params?: ACM.AllInputParams\n}\nand inside the ACM namespace have:\ninterface AllInputParams {\n    CertificateArn?: Arn;\n    Tags?: TagList;\n    CertificateStatuses?: CertificateStatuses;\n    ...\n}\n. Right, it would only be on the constructor, but there are valid reasons a customer may want to look at the properties of the constructor such as looking up what apiVersions are available for a service, or for the case of S3 and DynamoDB, they want to access the ManagedUpload or DocumentClient. When they try to access these, the constructors will autocomplete with a large number of properties that aren't real and make it difficult to find the actual property they are trying to access.\n. Another reason to have a constructor definition for each service class is to allow code completion for API version dates (by defining the optional apiVersion as an enum in ServiceParams), especially if we want to encourage users to specify API versions when constructing the service clients.\n. if (list.indexOf(member) !== -1)\n. Otherwise it's always returning true if the member isn't actually required, and it's returning false if the member happens to be the first one in the list\n. If checkRequired is supposed to return true when the member is required, then this should be switched.\nself.checkRequired(shape.required || [], memberKey) ? '' : '?'\n. Perhaps timestamp should be Date|string|number? When the SDK validates input params, these 3 types are valid for timestamps, so we don't want to raise a type error if the user is using a valid type. For timestamps in the output, it often is returned as a string, and an error should be raised if someone tries to invoke a Date method on it.\n. Instead of skipping shapes that happen to be named these, we could just add an underscore in front to differentiate the name of the shape from the javascript primitives.\nIt could still be useful to generate a typing for these shapes. For example, some services (like CodeCommit) have a shape named \"Date\" that is modeled as a string rather than a timestamp. If we rename the shape rather than skip it and have it be typed as a JavaScript Date object, then it could prevent errors before runtime, such as calling a Date method like getMonth() on the string.\n. So for example, the typing could look like:\nexport type _Date = string;\nAnd the references to this shape would have to have an underscore added in front as well.\n. It would be helpful to support enums.\n(after checking for complex types and before starting to check primitive types, so between map and string):\nelse if (Array.isArray(shape.enum)) {\n    code += tabs(tabCount) + 'export type ' + shapeKey + ' = ' + shape.enum.map(JSON.stringify).join('|') + ';\\n';\n}\nSo for example, Lambda's Runtime declaration would look like:\nexport type Runtime = \"nodejs\"|\"nodejs4.3\"|\"java8\"|\"python2.7\";\ninstead of just a generic string\nexport type Runtime = string;\n. Perhaps we could check whether the input has any required params, and if it has at least one, then don't generate this second second declaration where params is optional.\n. Add a space before the word 'operation'.\n. waiterState needs to have its first letter lowercased. For example, using the state \"BucketExists\" will result in an error, but \"bucketExists\" is correct.\n. Same comment as above about required params. If the underlying operation has any required params, that perhaps we shouldn't generate this second typing where params is optional.\n. typo: keyPairId instead of keyPaidId\n. Would doing import {Agent as httpAgent} from 'http'; and import {Agent as httpsAgent} from 'https'; make a difference in how much typings data is loaded into memory by Intellisense or by the typescript compiler? I've seen complaints about Intellisense slowing down VS Code or the typescript compiler being too slow when declaration files are large. Since the only typings we need are for Agent from both modules, it seems unnecessary to load all of the typings from those modules into memory.\n. If an output field modeled as a timestamp is sourced from the response header rather than the body, then it remains as a string in the output data. For example, in S3, the LastModified shape is modeled as a timestamp. In the output of getObject and headObject, the LastModified field is sourced from the header rather than the body, so the value is a string. In s3.d.ts, LastModified is typed as a Date, which would be incorrect for the output of these two operations. In some other operations such as listObjects or copyObjects, the LastModified field (modeled by the same LastModified shape) is sourced from the response body and the value is converted to a Date object.\n. So do you mean like this?\nexport type Runtime = \"nodejs\"|\"nodejs4.3\"|\"java8\"|\"python2.7\"|string;\nIf our main goal is for this to be used for code completion, then this is fine. However, if a user wants to catch compile-time errors, having the enum without the catch-all string type could be really useful in catching typos.\n. Well, we have that problem anyway if the user defines required params in the service-level config but has additional parameters. Perhaps we should just make all input params be optional?\n. The Config class also needs to have each service identifier as an optional instance variable. Otherwise the compiler will complain if a user tries to define service-specific parameters and options:\nAWS.config.s3 = {params: {Bucket: 'myBucket'}, useDualstack: true};\n. Instead of maintaining two separate but identical lists of these config options plus comments (here and above in the Config class), perhaps we could define this as an abstract class rather than an interface, and then the Config class could extend this abstract class.\nAnother solution could be to generate this file (which may be a good idea anyway since we need to include all of the service identifiers in the Config class as mentioned above). However, we would then need to maintain the comments for each of these configuration options in another file, since I noticed these comments aren't exactly identical to the documentation in the config.js file.\n. ",
    "parthrao": "Regenerating keys worked for me.\n. ",
    "alessio-dev2rights": "Yes guys I tested this 3 times now and the solution is to regenerate the secret_key.\nFrom https://forums.aws.amazon.com/thread.jspa?messageID=733434 this error is\nmistyped Access Key or Secret Access Key.\n. ",
    "rishaselfing": "Hi, I have related issue. I'm  working with device farm and uploading my app and tests there using aws-sdk npm (v2.6.4).  \nvar params = {\n                name: 'my.apk',\n                projectArn: 'arn:aws:devicefarm:somestuff',\n                type: 'ANDROID_APP',\n                contentType: 'application/octet-stream'\n            };\ndevicefarm.createUpload(params, callback);\nSo after I call  devicefarm.createUpload(options, callback) I receive a response json with upload info and pre-signed s3 upload url. \n{ upload: \n   { arn: 'arn:aws:devicefarm:somearn',\n     name: 'my.apk',\n     created: Thu Sep 22 2016 15:33:43 GMT+0000 (UTC),\n     type: 'ANDROID_APP',\n     status: 'INITIALIZED',\n     url: 's3 url here'\n   }\n}\nThen I'm trying to perform PUT request to actually upload the file and getting this  SignatureDoesNotMatch response. \nI've tried to make same PUT request from terminal using curl and I'm getting the same error.\nBUT if I call create upload from aws cli with the same parameters \naws devicefarm create-upload --project-arn=arn:aws:devicefarm:somestuff --name=my.apk --type=ANDROID_APP\nand obtain another pre-signed url then my curl PUT request is successful.  So I assume there is some problem with signing url inside sdk.\nAWS CLI and aws-sdk are initialized with the same credentials.  I do not have an opportunity to regenerate secret key as proposed and I'm not sure this is the solution because with cli it works fine.\nAny ideas? \n. ",
    "dinesh-ranium": "The request signature we calculated does not match the signature you provided. Check your key and signing method\ngetting this error on ubuntu os ,\nbut not on mac os.\nWhy?\n. ",
    "sauloaguiar": "I'm facing exactly the same situation as @rishaselfing has just described. Any thoughts on it?. I got it working after I specified in the header of my request the \"content-type\": \"application/octet-stream\". ",
    "IamGabrielWu": "i changed the header of my request the \"content-type\": \"application/x-www-form-urlencoded\" then it works. ",
    "trojandnc": "I had the same error and my simple correction was that I was using heroku which was an https and I was calling the non-secure http. \nAdded the 's' and mine worked.. ",
    "dschinkel": "I had to simply reset my credentials locally, not sure why, strange. Everything had been working fine for a week.  \nhttp://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html\nwhat's weird is when I did the command to configure aws, it showed that I did have a key and secret already (same one I've always been using) but for some reason it it was throwing up today and resetting it worked.. I had to simply reset my credentials locally, not sure why, strange. Everything had been working fine for a week.  \nhttp://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html\nwhat's weird is when I did the command to configure aws, it showed that I did have a key and secret already (same one I've always been using) but for some reason it it was throwing up today and resetting it worked.. ",
    "juliancorrea": "I had the same problem, and the cause was metadata with accents.\nJava example:\nObjectMetadata meta = new ObjectMetadata();\nmeta.setContentLength(bytes.length);\nmeta.setContentType(contentType);\nmeta.addUserMetadata(\"description\", \"Maring\u00e1\");\nString nameSave = folder +\"/\" + name ;\ns3.putObject(S3_BUCKET, nameSave, stream, meta);. ",
    "CarringtonCreative": "Fixed my issue. \nApparently, I was confusing my AWS access keys (In this case access to SES) with my SMTP credentials.\nThe former is used to gain access to Amazon's SES API directly or indirectly through the AWS SDK, the AWS Command Line Interface, or the AWS Tools for Windows PowerShell.\nThe latter is used for the Amazon SES SMTP interface.\nMake sure the credentials that you generated are for either Amazon's SES API or Amazon SES SMTP interface. It's easy to confuse them because they both use accessKeyIds and accessSecretKeys.\n--\nAccording to the AWS doc's\n\nYour SMTP user name and password are not the same as your AWS access key ID and secret access key. Do not attempt to use your AWS credentials to authenticate yourself against the SMTP endpoint.\n\nHope this helps. Fixed my issue. \nApparently, I was confusing my AWS access keys (In this case access to SES) with my SMTP credentials.\nThe former is used to gain access to Amazon's SES API directly or indirectly through the AWS SDK, the AWS Command Line Interface, or the AWS Tools for Windows PowerShell.\nThe latter is used for the Amazon SES SMTP interface.\nMake sure the credentials that you generated are for either Amazon's SES API or Amazon SES SMTP interface. It's easy to confuse them because they both use accessKeyIds and accessSecretKeys.\n--\nAccording to the AWS doc's\n\nYour SMTP user name and password are not the same as your AWS access key ID and secret access key. Do not attempt to use your AWS credentials to authenticate yourself against the SMTP endpoint.\n\nHope this helps. ",
    "westredd": "Generating new keys for my account got me past this issue. . ",
    "epabst": "I got this same issue.  What ended up being the cause is that I was accidentally sending a Content-Length of -1.  Once I sent the correct Content-Length, it worked great.. ",
    "Deepak275": "I got the same issue, the issue was there was white-space at the end of the AWS key, which was ignored by VMs, but when i migrating to container, it was considering the white-space. This was really a stupid mistake from my side. Hope it will help someone. ",
    "dkorel-copperleaf": "Thanks @Deepak275 i think i was getting this error because the key was not being set properly.. ",
    "sinzin91": "In my case, it was because I did not have 'bucket' set, i.e.\nconst s3 = new aws.S3({\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n  secretAccessKey: process.env.AWS_SECRET_KEY,\n  bucket: 'BUCKET_NAME'\n}). In my case, it was because I did not have 'bucket' set, i.e.\nconst s3 = new aws.S3({\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n  secretAccessKey: process.env.AWS_SECRET_KEY,\n  bucket: 'BUCKET_NAME'\n}). ",
    "TacticalSlothmaster": "I have the same problem when I deploy to my Kubernetes cluster. But everything works fine when using a ubuntu vm or local (macOS). No white-space at the end of the AWS key.. ",
    "adv4000": "If you are using correct and working AWS Access Key but wrong AWS Secret Key you will get this error, double check your keys.. ",
    "bryevo": "II ran into the same issue. I was using an IAM user with an old access key that didn't have S3FullAccess in the group permissions. Once I added the correct permissions and created a new access key it worked! Hopefully that fixes it!. ",
    "roddik": "Thanks, when should it be updated in npm?\n. Thanks, when should it be updated in npm?\n. ",
    "devilelephant": "Is there any progress on this?  I'm trying to use SWF and ran into the same Date parameter serialization problems as demonstrated on https://github.com/aws/aws-sdk-js/issues/84\nIt appears the only way to pass Date parameters into SWF safely is to call getTime() on them explicitly. \njavascript\nswf.listOpenWorkflowExecutions({domain: 'myDomain', startTimeFilter: {oldestDate: oldDate.getTime() / 1000}}, \n    function (err, data) {\n        if (err) { throw err; }\n    console.log(data);\n});\nEdit: Learned I need to divide oldDate.getTime() by 1000 because oldestDate needs \"seconds\" instead of millis.  Still, I would expect the API to just take the JavaScript Date() object as is and serialize it on my behalf.\n. ",
    "doapp-ryanp": "Thanks @lsegal appreciate the quick response. \nYou are probably already aware, but your colleagues doing the PHP SDK have a great start on a new guide that really helps on-board devs fast.  Something like this would help NodeJS noobs like me use more AWS via your lib. \n. @mhart not sure how much pull you have on the Lambda team, but we've been pushing them to allow consumers of the service to specify node js version for reasons like this.  Right now Lambda only supports 0.10.33.  We understand how the pace of dev is hard to support in a production environment - however node is still in it's infancy, so being able to use new versions is critical.  For example, as @doapp-jeremy states we really would prefer to use io.js so we can leverage things like native ES6 promises.\n. @lsegal done.  For others who come across this thread here is the post (so you can up-vote ;): https://forums.aws.amazon.com/thread.jspa?threadID=180512\n. ",
    "dstrek": "Ok I have fixed it, the s3 API definition was incorrect so that's why xml was failing to build, removed the monkey patch.\nI also added AllowedHeaders to CORS as defined in the spec.\n. yeah that needs a test, as it was passing tests with the wrong xml just then :(\n. Is there something I can do to help push this along?\n. ",
    "ajkerr": "The problem seems to be with the service definition for both GetQueueAttributes and ReceiveMessage.  The name of the property in the response should be Attribute instead of Value.  I will submit a pull request with the change.\n. +1 I ran into the exact same issue today.  Based on https://github.com/aws/aws-sdk-js/issues/3 I assumed that streams were supported in the Body of a putObject, but it appears that this will only work if the stream is created from fs.createReadStream().\nThe code that is throwing the exception is in the util.js file:\n``` javascript\n    byteLength: function byteLength(string) {\n      if (string === null || string === undefined) return 0;\n      if (typeof string === 'string') string = new Buffer(string);\n  if (string.length !== undefined) {\n    return string.length;\n  } else if (string.path !== undefined) {\n    return require('fs').lstatSync(string.path).size;\n  } else {\n    throw AWS.util.error(new Error(), {\n      message: 'Cannot determine length of ' + string, object: string\n    });\n  }\n}\n\n```\nI think that supporting this should be fairly simple, but will require a change to the API so that ContentLength can be specified by the caller.\n. That little trick worked for me, thanks!\n. @AllanFly120 Thanks for your response. Ignoring the second example, the big problem for us is that the first example doesn't work for DELETE requests. POSTs, PUTs, etc all seem to work when a body is present, but not DELETEs.. @AllanFly120 UPDATE: After a lot of tinkering, I noticed that if I add the following line of code to the first request above, the request is sent correctly:\njavascript\nrequest.headers[\"Content-Length\"] = request.body.length;\nMy guess is that the SDK is not setting this header correctly for DELETEs with a body.\nAfter digging in the SDK code a bit, I verified that modifying the setHeaders() function as follows in lib/signers/v4.js seems to fix this problem:\njavascript\n  addHeaders: function addHeaders(credentials, datetime) {\n    this.request.headers['X-Amz-Date'] = datetime;\n    if (credentials.sessionToken) {\n      this.request.headers['x-amz-security-token'] = credentials.sessionToken;\n    }\n    if (this.request.body) {\n      this.request.headers['Content-Length'] = this.request.body.length;\n    }\n  },\nI notice that the older v2 signer did something similar with this header.. @AllanFly120 Any updates on this?. ",
    "hereandnow": "so how do i do that in my example? if i try to get the filesize with the gm-library and stream inside a callback i get this error from the request library: \nwhat i tried was that:\n\ngm(request('http://www.some-domain.com/image.jpg'), 'image.jpg').filesize({ bufferStream: true }, function(err, filesize) {\n  this.stream(function(err, stdout, stderr) {\n    var data, request;\n    stdout.length = filesize;\n    data = {\n      Bucket: 'my-bucket',\n      Key: 'image.jpg',\n      Body: stdout,\n      ContentType: mime.lookup('image.jpg')\n    };\n    s3.client.putObject(data, function(err, res) {\n     console.log('done');\n    });\n  });\n});\n\n. ",
    "rcmonteiro": "Another workaround using just gm, aws-sdk, http, fs\nhttp.get('http://s3-sa-east-1.amazonaws.com/bucket/path/image.jpg', function(res) {\n    if(res.statusCode != 200) {\n      console.log(\"Err\\n\");\n    } else {\n      gm(res).resize(w, h, '^').gravity('Center').extent(w, h).quality(80).stream(function(err, stdout, stderr) {\n        var buf = new Buffer(0);\n        stdout.on('data', function(d) {\n          buf = Buffer.concat([buf, d]);\n        });\n        stdout.on('end', function() {\n          var data = {\n            Bucket: bucket,\n            Key: 'pathtoimage/thumb.jpg',\n            Body: buf\n          };\n          s3.client.putObject(data, function(err, resp) {\n            console.log(\"Done\\n\");\n          });\n        });\n      });\n    }\n  });\n. ",
    "adeleinr": "This last trick, using fs instead of http.get, worked for me\n. ",
    "petermilan": "thanks perfect :)\n. ",
    "terribleplan": "@aws @lsegal You either need to accept readable streams as per the ReadableStream documentation (which does not specify a length) or document somewhere that your implementation is limited in this way.\nIf I have to know the size then I either need to load (potentially) very large amounts of data into memory or write it to disk, neither of which is a good option in the environment I am operating in.\n. @aws @lsegal You either need to accept readable streams as per the ReadableStream documentation (which does not specify a length) or document somewhere that your implementation is limited in this way.\nIf I have to know the size then I either need to load (potentially) very large amounts of data into memory or write it to disk, neither of which is a good option in the environment I am operating in.\n. @lsegal It look like some of I want can be done through the multipart API, and that s3-upload-stream wraps it nicely, so I doubt any change will be made since there are probably benefits on the s3 side knowing the size of the upload in advance.\n. @lsegal It look like some of I want can be done through the multipart API, and that s3-upload-stream wraps it nicely, so I doubt any change will be made since there are probably benefits on the s3 side knowing the size of the upload in advance.\n. ",
    "nmccready": "@rcmonteiro your example may work, but it defeats the purpose of streaming. (your putting everything into memory)\n. @rcmonteiro your example may work, but it defeats the purpose of streaming. (your putting everything into memory)\n. Ultimately they should copy some code from here https://github.com/nathanpeck/s3-upload-stream , as it is done right .\n. Ultimately they should copy some code from here https://github.com/nathanpeck/s3-upload-stream , as it is done right .\n. Ah nice, misread your post. So this supports streaming. Can there be an example where .pipe is being used?\n. Ah nice, misread your post. So this supports streaming. Can there be an example where .pipe is being used?\n. ",
    "kindlyseth": "Google ranks this issue highly, so perhaps it's worth noting for the next person that http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#upload-property handles streaming.\n. @nmccready, according to its README, that is made obsolete by the functionality I linked.\n. Something like this. Beware that I haven't run this so it probably contains typos.\n``` js\nvar https = require(\"follow-redirects\").https;\nvar s3 = new (require(\"aws-sdk\")).S3();\nvar gitHubToken = \"hex string from github personal token\";\nmodule.exports = function(callback) {\n  var request = https.get(\n    {\n      host: \"api.github.com\",\n      path: \"/repos/aws/aws-sdk-js/tarball/4a404cb8c06bba6b7b00c323671376c6377889ed?access_token=\"+gitHubToken,\n      headers: {\n        \"User-Agent\": \"Unique user agent string\",\n      },\n      function(response) {\n        return handleTarball(null, response, callback);\n      }\n    }\n  );\n  request.on(\"error\", function(err) {\n    return handleTarball(err, null, callback);\n  });\n};\nvar handleTarball = function(err, response, callback) {\n  if (err) {\n    return callback(err);\n  }\n  if (response.statusCode != 200) {\n    return callback(new Error(\"unsuccessful status code: \"+response.statusCode));\n  }\n  s3.upload({\n    Bucket: \"some-bucket\",\n    Key: \"aws-sdk-js-latest.tar.gz\",\n    Body: response, // <--------------------- it's that simple if you use s3.upload\n    ContentType: response.headers[\"content-type\"], // shouldn't hurt\n  }, callback);\n};\n```\n. ",
    "0xdevalias": "\nit's that simple if you use s3.upload\n\nI was definitely hoping that would be the case from how it's described in the docs, but in trying to do this, I still end up with this error:\njson\n{\n  \"errorMessage\": \"Cannot determine length of [object Object]\",\n  \"errorType\": \"Error\",\n  \"stackTrace\": [\n    \"byteLength (/var/runtime/node_modules/aws-sdk/lib/util.js:179:26)\",\n    \"ManagedUpload.fillBuffer (/var/runtime/node_modules/aws-sdk/lib/s3/managed_upload.js:385:19)\",\n    \"ManagedUpload.send (/var/runtime/node_modules/aws-sdk/lib/s3/managed_upload.js:199:33)\",\n    \"/var/runtime/node_modules/aws-sdk/lib/util.js:799:25\",\n    \"new Promise (<anonymous>)\",\n    \"ManagedUpload.promise (/var/runtime/node_modules/aws-sdk/lib/util.js:798:14)\",\n    \"/var/task/index.js:55:106\",\n    \"handler (/var/task/index.js:19:54)\",\n    \"<anonymous>\",\n    \"process._tickDomainCallback (internal/process/next_tick.js:228:7)\"\n  ]\n}\nBasic flow of my code is reading a list of 'filenames' from s3, then fetching each as a stream, which are then written out as a stream using the s3 upload function. I'm using highland to handle some of the higher level concepts, but it just returns a node ReadableStream at the end, so I can't see why this would be an issue.\n```typescript\n  const fileList = await client.listResultsFiles();\n  const mergedFiles = highland(fileList)\n    .map(client.streamFile)\n    .sequence()\n    .toNodeStream();\nreturn client.uploadStream('results-AAAAAA.json')(mergedFiles)\n```\nHelpers for reference\n```typescript\nconst listResultsFiles = async function(): Promise {\n    // List\n    const s3Objects = await s3.listObjectsV2({\n      ...commonParams,\n      Prefix: cfg.resultsKeyPrefix\n    }).promise();\n// ..snip.. some bits that make the filenames sortable\n\n// Sort\nreturn lodash\n  .sortBy(files, ['prefix', 'start', 'end', 'suffix'])\n  .map(({filename}) => filename);\n\n};\nconst streamFile = (key: string) => {\n    const fileStream = s3.getObject({...commonParams, Key: key}).createReadStream();\n    return highland(fileStream)\n  };\nconst uploadStream = (destinationKey: string) => (streamToUpload: ReadableStream) => {\n    return s3.upload({\n      ...commonParams,\n      Key: destinationKey,\n      Body: streamToUpload,\n    }).promise()\n  };\n```\nEdit: I'm not sure of the 'why', but this solved (or at least worked around) the issue for me, and now works as expected.. https://github.com/aws/aws-sdk-js/issues/1713#issuecomment-447979639. ",
    "tanx": "Thanks for the quick response. Yes, installing via tarball from master fixes the bug... \nnpm install https://github.com/aws/aws-sdk-js/tarball/48ed485fa30a14e4d19ff975ce8f4ad17ae4dfca\n. ",
    "rstahl": "Thanks.  Wanted to verify that the \"Date' parameter is not actually a Javascript Date but rather a unix timestamp, e.g.,\nvar listParams = {\n        domain:domain,\n        startTimeFilter: {\n            oldestDate:parseInt(endDate.getTime() / 1000) // unix time, seconds since epoch\n        }\n    }\nOn Apr 17, 2013, at 4:28 PM, Loren Segal notifications@github.com wrote:\n\nI'm unable to reproduce this with err={}, but I was able to see an issue with parameter validation of unix timestamps. A quick workaround is to toggle paramValidation off:\nvar swf = new AWS.SimpleWorkflow({paramValidation: false});\nOr you can use the master branch which contains a fix that I just pushed for this issue above. To install the npm from master, run:\nnpm install git://github.com/aws/aws-js-sdk\n\u2014\nReply to this email directly or view it on GitHub.\n. Excellent!  Thank you.\n. Hi,\n\nThanks for your response.  I am testing on  IE 11.0 running on Windows 8.0.  Under the developer tools section of the browser, there's an indication that the browser is running in 'Edge' mode (??).\nRegarding the lambda function, I am using Nodejs 4.3 and here's a snippet of code that is equivalent to what I'm doing:\nexports.handler = function(event, context, callback) {\n    // detect and return some error\n   var authErr = new Error('No OAuth tokens available');\n   authErr.name = 'authError';\n  callback(authErr);\n}\nI'm curious if this is yet another CORS-related issue with IE, since I believe the 'FunctionError' value is returned via a response header.\nThanks again.\n. Any update regarding this problem?\n. Hello?  We need resolution of this issue please.\n. Need an update on this please.. Has this been fixed in the latest released SDK?. So essentially users of the SDK have to create a workaround for IE.  Weird that this would not be a hight priority.. Please provide an update on this.  I was able to look at the actual HTTP responses, and it does not appear to be a lambda service issue.  The HTTP response contains the 'function error' header, but that header is not getting parsed out and returned via the SDK in IE--it is returned in Chrome.  I am using version 2.9.0 of the SDK.  Here's the HTTP response headers as shown in IE:\nAccess-Control-Allow-Origin:*\nAccess-Control-Expose-Headers:x-amzn-RequestId,x-amzn-ErrorType,x-amzn-ErrorMessage,Date\nAccess-Control-Expose-Headers:x-amzn-RequestId,x-amzn-ErrorType,x-amzn-ErrorMessage,x-amz-log-result,x-amz-function-error\nContent-Type:application/json\nDate:Wed, 01 Feb 2017 16:31:16 GMT\nX-Amz-Function-Error:Handled\nx-amzn-Remapped-Content-Length:0\nx-amzn-RequestId:da45ef2b-e89b-11e6-bf3f-83d1d15e132b\nContent-Length:207\nConnection:keep-alive\nHere's the response as shown in Chrome:\nAccess-Control-Allow-Origin:*\nAccess-Control-Expose-Headers:x-amzn-RequestId,x-amzn-ErrorType,x-amzn-ErrorMessage,Date\nAccess-Control-Expose-Headers:x-amzn-RequestId,x-amzn-ErrorType,x-amzn-ErrorMessage,x-amz-log-result,x-amz-function-error\nConnection:keep-alive\nContent-Length:213\nContent-Type:application/json\nDate:Wed, 01 Feb 2017 16:37:43 GMT\nX-Amz-Function-Error:Handled\nx-amzn-Remapped-Content-Length:0\nx-amzn-RequestId:c0e3de27-e89c-11e6-b582-15221e551bf5\nIn both cases the X-Amz-Function-Error header is returned; however in the I.E case the SDK apparently does not parse it out correctly and return the 'FunctionError' property in the result object, whereas Chrome does.\n. ",
    "AdityaManohar": "@mitar Requester pays buckets are now supported in the SDK. Thanks for reporting!\n. @OferE \nI did research this problem some more. It turns out that Node.js isn't going to be implementing proxy support in the immediate future as per https://github.com/joyent/node/issues/2474#issuecomment-3484290. This does seem a bit dated, but there has been no change in direction regarding proxy support since then.\nI did also spend some time figuring out an approach to implement proxy support with the SDK. However this involves a re-write of the http.Agent class, which is a private API. Doing this to support proxies poses a maintainability problem, owing to the numerous different proxy types and authentication schemes.\nThere are a number of http.Agent implementations and request itself uses tunnel-agent to support HTTP tunneling. \nYou can use tunnel-agent with the SDK as well to support proxy behavior. Here is a code sample showing how that can be done:\n``` javascript\nvar AWS = require('aws-sdk');\n/ Pick an agent from:\n * httpOverHttp\n * httpOverHttps\n * httpsOverHttp\n * httpsOverHttps\n / \nvar Agent = require('tunnel-agent').httpsOverHttps;\nvar agent = new Agent({proxy: 'http://127.0.0.1'});\nAWS.config.update({httpOptions: { agent: agent}});\nvar s3 = new AWS.S3();\ns3.listBuckets();\n```\nI hope this helps, and do let me know if there is anything else I can help with!\n. The \"Configuring a Proxy\" section of the developer guide, as well as the \"Using the AWS SDK for JavaScript from Behind a Proxy\" blog post document using the SDK from behind a network proxy.\nI'm marking this as closed. Feel free to open another issue if you have any questions.\n. I just pushed a patch release of the SDK (2.0.14). It looks like my brew install was pulling in an alpha version of npm which caused the checksum issue. Let me know if this new version resolves the problem for you.\nThanks for reporting this! \n. @nickdk thanks for the pull request!\nWe've deliberated on this issue for some time, and given the many different proxy server implementations and protocols, we think the best way to support proxies is to configure the SDK to use an http.Agent that implements proxy support.\nThere a number of http.Agent implementations available like proxy-agent and tunnel-agent that do implement proxy support. The SDK can easily be configured to use these implementations instead of the default native http.Agent.\nWe've documented how to configure the SDK with an http.Agent implementation that supports proxies in our developer guide. We've also written a blog post that describes using the SDK from behind a proxy.\nI'm going to mark this as closed because there is no plan to build proxy support in the SDK at this time.\nFeel free to open an issue if you have any questions, and thanks again!\n. Thanks for the suggestion. Unfortunately String.prototype.localeCompare() is not supported in < IE 11.\nEdit: Oops, I was reading the MDN documentation and it looks like according to MSDN docs it should work.\n. I just pushed a release to fix the issue. Let me know if this fixes the issue for you and thanks for reporting!\n. I've changed the scope of the variable. Thanks!\n. Can you try updating credentials before constructing the service object? \nAWS.config.update({ accessKeyId: AAKID, secretAccessKey: SAK });\nAWS.config.region = 'us-west-2';\nvar bucket = new AWS.S3({ params: { Bucket: bucketPath } });\nThis is because credentials are sourced from AWS.config when constructing a service object.\nLet me know if the error persists after making this change.\n. Sure! I'm closing this as resolved.\n. @allthetime I'm going to close this issue since you seem to have it working. Please feel free to reopen if you run into any problems again or have any further information. \n. Clarifying what @lsegal said earlier, the way to disable CRC32 checksum checks with DynamoDB is to set dynamoDbCrc32: false when constructing a new DynamoDB service object. \njavascript\nvar dynamodb = new AWS.DynamoDB({dynamoDbCrc32: false});\n. Hey @raffi-minassian!\nThis is now fixed in our v2.1.6 release. Let me know if it works for you. Thank you for your patience!\n. Would you be able to describe why you choose to use streams with DynamoDB? Streaming output is intended for use with payload bodies (example Amazon S3, Amazon Glacier).\nThe SDK performs many operations on response bodies like checksum verification, type casting of responses data, and automatic retry for failed requests. These convenience features do not apply to streaming output.\nThis issue is happening because of a failed CRC32 check on the output stream. If you are using this in a test environment, you can get around this by setting dynamoDbCrc32: false when constructing a DynamoDB service object.\njavascript\nvar dynamodb = new AWS.DynamoDB({dynamoDbCrc32: false});\nLet me know if this helps!\n. I'm glad you got it working!\nDynamoDB is the only service that requires a CRC32 check on output. It fails if you try to stream the output because it is not possible to compute a CRC checksum on a stream.\nThe outputs are standardized across all services in the SDK. You can get access to the raw stream for any service request using  createReadStream().\n. Are you hitting live services, or is this a traditional unit test that does not make any network calls?\nBy default the IAM role created by Lambda will only list lambda.amazonaws.com as a trusted entity. To be able to assume this role you need to add yourself as a trusted entity. In the web console, select the IAM role and edit the trust relationship. Add this to the Statement array. This will allow you to assume the role that Lambda is using:\njavascript\n{\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::your_account_id:root\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n}\nListing your Lambda functions will tell you the IAM role that your function is using:\njavascript\nlambda.listFunctions(function(err, data){\n // Get Role ARN from response\n roleArn = data.Functions[0].Role;\n});\nYou can then use STS to assume that role, which will give you temporary credentials to test with the services you are using:\njavascript\nsts.assumeRole({RoleArn: roleArn, RoleSessionName: 'integ_test'}, function(err,data){\n  // Get temporary credentials here that you can use to run your tests.\n  temporaryCredentials = data.Credentials\n});\nIf you are writing traditional unit tests you will not be testing with live services, ergo will not need this configuration.\nLet me know if this helps!\n. Does the root of the zip package contain the file that has the handler? As per Lambda's documentation:\n\nYou zip the folder content, not the folder itself.\n\nThe File name field doesn't seem to accept relative paths. The file containing the handler should be in the root of the zip.\nLet me know if this helps.\n. @assen-totin which version of the SDK are you using? As of SDK version v2.1.22, we use xml2js v0.2.8 and sax v0.5.3. See package.json. \n. Unfortunately, there is no documentation for the API models (these are built directly from our services).\nWe have been discussing the possibility of hosting a separate repository for the models, but until then you could include the API models as a submodule or by manually pulling them in. Here is another project that uses our API models: https://github.com/brendanhay/amazonka. The Apache 2.0 license extends to the models as well.\nLet me know if you have any other questions.\nEdit: To answer your question about shapes, shapes can be either composite or scalar. This JSON parser should explain how shapes work: https://github.com/aws/aws-sdk-js/blob/master/lib/json/parser.js.\n. Which version of the SDK are you using? I am unable to reproduce this with version 2.1.5.\n. @downie Sorry! I missed out the fact that you are using the browser build of the SDK. \nCloudWatchLogs isn't included in the default browser build of the SDK because the service does not support CORS. \nUnfortunately, there is nothing that we can do about this right now. If you are using this in a browser environment where you have permissions to disable CORS, then you can build a custom version of the SDK as mentioned here: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-building.html\n. Sure, no problem!\n. @vkovalskiy The pre-signed URL that is returned by the getSignedUrl() operation is useful for accessing objects without the SDK. \nIf you are using the SDK to perform an operation, you will have to authenticate it. You can use Amazon Cognito or Web Identity Federation to vend credentials in the browser. \nI hope this helps!\n. @vkovalskiy I'm closing this out since there hasn't been any activity. Feel free to re-open this issue or open another issue if you have any other questions.\n. @cbkihong This is fixed in master. Please feel free to re-open if you still experience issues. Thanks for reporting!\n. Hey @gjtorikian \nThis is now fixed in master. Please feel free to re-open if this does not work correctly for you. Thanks for reporting!\n. @gjtorikian No problem!\nWe do have a release scheduled for later this week, but you can use it now from npm if you npm install git://github.com/aws/aws-sdk-js.\n. Did not add a test case for this because this is an internal api implementation.\n. @devinivy There has been some conversation about sharing configuration across SDKs and the CLI. I'll add this as a feature request. In the meantime, FWIW, you can use the AWS_REGION environment variable to source your region.\n. @lsegal Any chance you could get some eyes on this?\n. I'm marking this as closed because there is not much else that we can do from the SDK side of things. Feel free to reopen this issue or another issue if you have any questions.\n. Would you be able to share more on how you installed the SDK? It seems like the SDK Node module is in */AWS-sdk? Was this a manual install from GitHub? \nAlso, can you print AWS.VERSION in your script?\n. I'm closing this out since there hasn't been any recent correspondence and the issue cannot be reproduced. Feel free to reopen this issue or open another issue if the problem still persists.\n. @tmcw \nThanks for letting us know! I can confirm that browserify does work with crypto-browserify/createHmac v1.1.3.\nI'm going to close this issue out as the problem seems to be resolved. Feel free to reopen this issue or open another issue if you run into any other problems.\nAnd again, thanks for reporting!\n. @mdouglass Thanks for reporting this and submitting a fix!\n. Closing this out since the OP seems to have left the conversation.\n. Hey @eugene-dounar !\nThis is now fixed in master. Thanks for reporting!\n. Which version of IE are you using? The SDK works on IE 10+.\n. Are you using some version of sarissa.js (possibly through Salesforce) in your application?\nEdit: The URL in your browser does look like you are using this is a Salesforce application. Salesforce uses sarissa.js which overrides the native XMLHttpRequest object in the browser. This causes the error that you are experiencing. The SDK cannot work with this modified XMLHttpRequest object.\nEdit: I'm also curious to know which version of the SDK you are using.\n. Salesforce/VisualForce seems to be replacing the native XMLHttpRequest object only on IE browsers. \nCan you inspect the XMLHttpRequest on IE?\nThe workaround is to cache a reference of the modified XMLHttpRequest object, and restore the native XMLHttpRequest object before using the AWS SDK.\nEdit: I believe Salesforce has Sarissa embedded in their library. I don't believe it is a separate asset.\n. In IE you can console.log(XMLHttpRequest) to inspect it.\n. @manzanofab anytime!\nYou will need to restore the native XMLHttpRequest only in IE, something to the effect of:\njavascript\n// Cache Sarissa's XHR\nSarissa.XHR = XMLHttpRequest;\nXMLHttpRequest = Sarissa.originalXMLHttpRequest;\n// Run AWS-SDK code here\n// ...\n// Replace Sarissa's XHR\nXMLHttpRequest = Sarissa.XHR\nThis will work provided this function block is sequentially executing. \n. @manzanofab anytime!\nYou will need to restore the native XMLHttpRequest only in IE, something to the effect of:\njavascript\n// Cache Sarissa's XHR\nSarissa.XHR = XMLHttpRequest;\nXMLHttpRequest = Sarissa.originalXMLHttpRequest;\n// Run AWS-SDK code here\n// ...\n// Replace Sarissa's XHR\nXMLHttpRequest = Sarissa.XHR\nThis will work provided this function block is sequentially executing. \n. @manzanofab Glad you got it to work!\n. @manzanofab Glad you got it to work!\n. @ramonck can you help me understand where you want to authenticate users? Would you also be able to elaborate a little on your scenario?\n. @ramonck can you help me understand where you want to authenticate users? Would you also be able to elaborate a little on your scenario?\n. @ramonck From what I understand, you are trying to authenticate the SDK in the browser. There are a couple of ways to do this:\n1. The recommended way to authenticate the SDK in the browser is to use Amazon Cognito. Our developer guide describes how to authenticate users using Cognito. Amazon Cognito is used in conjunction with IAM roles to control access to your AWS resources. Amazon Cognito works with identity providers like Facebook and Google as well as developer authenticated identities (your own backend authentication process).\n2. The other approach is to use web identity federation. Our developer guide also describes using web identity federation to authenticate users in the browser.\nI hope this helps!\n. @ramonck From what I understand, you are trying to authenticate the SDK in the browser. There are a couple of ways to do this:\n1. The recommended way to authenticate the SDK in the browser is to use Amazon Cognito. Our developer guide describes how to authenticate users using Cognito. Amazon Cognito is used in conjunction with IAM roles to control access to your AWS resources. Amazon Cognito works with identity providers like Facebook and Google as well as developer authenticated identities (your own backend authentication process).\n2. The other approach is to use web identity federation. Our developer guide also describes using web identity federation to authenticate users in the browser.\nI hope this helps!\n. Are you referring to the username and password of the provider or accessKeyId / secretAccessKey combination? \n. Are you referring to the username and password of the provider or accessKeyId / secretAccessKey combination? \n. 1. You will have to register your application with a 3rd party provider. Amazon Cognito currently supports Facebook, Google, and Login with Amazon.\n2. Use the provider SDK to authenticate users with their username and password. You will have to refer to the provider documentation to see how to do this. Here is the documentation for the Facebook, Google, and Login with Amazon.\n3. The provider will return a token which you need to pass into Cognito in exchange for credentials.\nThis is covered in detail in the \"Authenticating Users with Amazon Cognito\" section of the developer guide.\nI hope this helps!\n. 1. You will have to register your application with a 3rd party provider. Amazon Cognito currently supports Facebook, Google, and Login with Amazon.\n2. Use the provider SDK to authenticate users with their username and password. You will have to refer to the provider documentation to see how to do this. Here is the documentation for the Facebook, Google, and Login with Amazon.\n3. The provider will return a token which you need to pass into Cognito in exchange for credentials.\nThis is covered in detail in the \"Authenticating Users with Amazon Cognito\" section of the developer guide.\nI hope this helps!\n. @piuccio I was able to reproduce your issue with just a plain s3.upload(). The above commit fixes the error reporting issue. It does not fix the lack of credentials in your plugin.\nHere is the output when I run grunt frequency_graph\n``` bash\nGet file stats\n======================================== 100% 0.0 seconds left\nGet repository history\n======================================== 100% 0.0 seconds left\nWriting results to tmp/frequency_graph.html\n{ [CredentialsError: Missing credentials in config]\n  message: 'Missing credentials in config',\n  code: 'CredentialsError',\n  time: Thu Feb 05 2015 18:17:57 GMT-0800 (PST),\n  originalError:\n   { message: 'Could not load credentials from SharedIniFileCredentials',\n     code: 'CredentialsError',\n     time: Thu Feb 05 2015 18:17:57 GMT-0800 (PST),\n     originalError: { message: 'Credentials not set in /Users/aditm/.aws/credentials using profile nextgen' } } }\nFatal error: Missing credentials in config\nExecution Time (2015-02-06 02:15:08 UTC)\nloading tasks              2.6s  \u2587\u2587\u2587\u2587 2%\nfrequency_graph:files  2m 45.9s  \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 98%\nTotal 2m 49.3s\n```\nLet me know if this works for you, and feel free to re-open this issue if you continue to experience issues!\n. @piuccio I was able to reproduce your issue with just a plain s3.upload(). The above commit fixes the error reporting issue. It does not fix the lack of credentials in your plugin.\nHere is the output when I run grunt frequency_graph\n``` bash\nGet file stats\n======================================== 100% 0.0 seconds left\nGet repository history\n======================================== 100% 0.0 seconds left\nWriting results to tmp/frequency_graph.html\n{ [CredentialsError: Missing credentials in config]\n  message: 'Missing credentials in config',\n  code: 'CredentialsError',\n  time: Thu Feb 05 2015 18:17:57 GMT-0800 (PST),\n  originalError:\n   { message: 'Could not load credentials from SharedIniFileCredentials',\n     code: 'CredentialsError',\n     time: Thu Feb 05 2015 18:17:57 GMT-0800 (PST),\n     originalError: { message: 'Credentials not set in /Users/aditm/.aws/credentials using profile nextgen' } } }\nFatal error: Missing credentials in config\nExecution Time (2015-02-06 02:15:08 UTC)\nloading tasks              2.6s  \u2587\u2587\u2587\u2587 2%\nfrequency_graph:files  2m 45.9s  \u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587\u2587 98%\nTotal 2m 49.3s\n```\nLet me know if this works for you, and feel free to re-open this issue if you continue to experience issues!\n. @piuccio Your plugin also seems to be working fine and picking up credentials. Since you've specified profile: 'nextgen' in grunt-configs/frequency_graph.js you will need to set up the same profile in your shared INI credentials file (~/.aws/credentials)\nHere is what I was able to replicate using your code (logging credentials in tasks/lib/aws.js).\n``` ini\n~/.aws/credentials\n[nextgen]\naws_access_key_id=foo\naws_secret_access_key=bar\n```\nWhen I run grunt frequency_graph:\nbash\n...\nGet file stats\n======================================== 100% 0.0 seconds left\nGet repository history\n======================================== 100% 0.0 seconds left\nWriting results to tmp/frequency_graph.html\naccessKeyId=foo\nsecretAccessKey=bar\n...\n. @piuccio Your plugin also seems to be working fine and picking up credentials. Since you've specified profile: 'nextgen' in grunt-configs/frequency_graph.js you will need to set up the same profile in your shared INI credentials file (~/.aws/credentials)\nHere is what I was able to replicate using your code (logging credentials in tasks/lib/aws.js).\n``` ini\n~/.aws/credentials\n[nextgen]\naws_access_key_id=foo\naws_secret_access_key=bar\n```\nWhen I run grunt frequency_graph:\nbash\n...\nGet file stats\n======================================== 100% 0.0 seconds left\nGet repository history\n======================================== 100% 0.0 seconds left\nWriting results to tmp/frequency_graph.html\naccessKeyId=foo\nsecretAccessKey=bar\n...\n. Which version of the SDK are you using? I tried the following and it doesn't go through (which is expected). I'm using SDK v2.1.8 and Node v0.10.35\n``` javascript\nvar AWS = require('aws-sdk');\nvar ec2 = new AWS.EC2();\nvar params = {\n  SpotPrice: '0.040.050.04 / 3', \n  LaunchSpecification:{\n    InstanceType:'m3.medium', \n    ImageId:'ami-ddfea7ed'\n  }\n}\nec2.requestSpotInstances(params, function (err,data){\n  if (err) console.log(err);\n  else console.log(data);\n});\n```\nHere is the error:\nInvalidParameterValue: Value (0.040.050.04 / 3) for parameter price is invalid. \"0.040.050.04 / 3\" is an invalid spot instance price\n. Which version of the SDK are you using? I tried the following and it doesn't go through (which is expected). I'm using SDK v2.1.8 and Node v0.10.35\n``` javascript\nvar AWS = require('aws-sdk');\nvar ec2 = new AWS.EC2();\nvar params = {\n  SpotPrice: '0.040.050.04 / 3', \n  LaunchSpecification:{\n    InstanceType:'m3.medium', \n    ImageId:'ami-ddfea7ed'\n  }\n}\nec2.requestSpotInstances(params, function (err,data){\n  if (err) console.log(err);\n  else console.log(data);\n});\n```\nHere is the error:\nInvalidParameterValue: Value (0.040.050.04 / 3) for parameter price is invalid. \"0.040.050.04 / 3\" is an invalid spot instance price\n. @jhford \nI'm closing this out as the problem is not reproducible, and there hasn't been any recent activity. \nFWIW, the SDK does not perform any regular expression or value validation. This error is returned by EC2 itself. \nPlease feel free to reopen this issue or open another issue if you have any other questions.\n. @jhford \nI'm closing this out as the problem is not reproducible, and there hasn't been any recent activity. \nFWIW, the SDK does not perform any regular expression or value validation. This error is returned by EC2 itself. \nPlease feel free to reopen this issue or open another issue if you have any other questions.\n. Which version of the SDK are you using? I'm not able to reproduce an error with SDK v2.1.8 and Node.js v0.10.35\njavascript\nvar AWS = require('aws-sdk');\nvar sqs = new AWS.SQS();\nvar params = {\n  QueueUrl: 'https://sqs.us-west-2.amazonaws.com/026153618265/aditm-us-west-2',\n  MessageBody: 'test'\n}\nsqs.sendMessage(params, function(err,data){\n  if (err) console.log(err);\n  else console.log(data);\n});\nNote that the parameters are derived from the SQS service model and are not hand maintained. \n. >  I guess that is the reason there is a buildEndpoint() function in sqs.js to modify the endpoint of the http request, but the that modification is not really reflected in the real http request sent out to the server.\nThe buildEndpoint() handler correctly set the region from the QueueUrl. \nI confirmed this behavior with the AWS CLI as well. There is no queue name in the path.\n```\nPOST\n/\nhost:us-west-2.queue.amazonaws.com\nuser-agent:aws-cli/1.6.4 Python/2.7.6 Darwin/14.1.0\nx-amz-date:20150205T230444Z\n```\n\nonly when you talk to CMB server\n\nCan you explain what you mean by this please?\n. @oldshuren \nI'm closing this out as this doesn't seem to be a problem with the SDK. Please feel free to re-open it or open another issue if you have any other questions.\n. > Do I have to do the AWS.config.credentials = new AWS.CognitoIdentityCredentials(params);\nYes, any JavaScript state that you had on your previous page will be lost when you make a synchronous HTTP request.\nThe AWS SDK however, caches the Cognito IdentityId in localStorage, so you will be refreshing credentials for the same identity. You will need to pass in the provider auth token when you create the new CognitoIdentityCredentials object.\nMost provider SDKs will manage the auth token for you across synchronous requests, but you will have to check the provider documentation for the correct API to check the login status of the user.\n- For Login With Amazon, calling amazon.Login.authorize() will return a valid cached token, as long as the user has not logged out, or the session has not expired. I suspect it is similar for Google.\n- For Facebook you can call FB.getLoginStatus() to return a token if the user has already logged in.\nI would recommend checking out the API documentation for each of the providers you are using for more information on how to maintain session state.\n. Typically this is called from a browser client.\nI don't see why it can't be called from a javascript server though. That might need some more investigation, but theoretically, it seems possible.\n. @ide \nThere is some flexibility in error messages returned by both the SDK and services, and parsing the error message to glean information from it does not scale well because of the variety in error messages.\nYou can rely on error.code across service operations. I'd be interested to know more about how/why you are using this parsed information.\nCC: @lsegal \n. > To my knowledge, this is the only way to get the endpoint ARN for a single token that you've previously registered.\nFrom the documentation: \n\nThe CreatePlatformEndpoint action is idempotent, so if the requester already owns an endpoint with the same device token and attributes, that endpoint's ARN is returned without creating a new endpoint.\n\nIf you are using the same token to create a new endpoint on the same application, you should see the EndpointArn in your data. I'm not sure why you are getting an error. Is your Attributes map identical across requests? Also, it may be a good idea to cache the EndpointArn locally.\nI was able to create an application and an application endpoint. Multiple CreatePlatformEndpoint calls with the same parameters returned the same response:\njavascript\n{ ResponseMetadata: { RequestId: '2a1bc3bb-6e6e-514e-b332-8ef48ebf78fd' },\n  EndpointArn: 'arn:aws:sns:us-west-2:123456789012:endpoint/ADM/sampleapp/ea45d12d-1e2d-3ac7-8337-06c2a0aa4567' }\n\nSo, to make sure that the problem is because of an unregistered endpoint ARN, I need to grep the error message for the invalid parameter name and the reason.\n\nThere are 2 scenarios here:\n1. If you are publishing directly to an application EndpointArn, then it is best to cache this EndpointArn locally in your application instead of making a no-op request.\n2. If you are publishing to an application EndpointArn that has been subscribed to a topic, then you can make a listSubscriptions call and get the Endpoint from the response. Even in this scenario, it might be favorable to cache this information locally and iterate over your local cache to implement your conditional logic.\nI hope this helps!\n. Would you be able to share some sample code that you are using? I'm unable to reproduce it with the following code [SDK v2.1.8 Node v0.12]:\n``` javascript\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\ns3.listBuckets(function(err, data){\n  if (err) console.log(err);\n  else console.log(data);\n});\n```\nWould you also be able to log the output of AWS.VERSION?\n. @stockholmux \nI am still unable to replicate this on Node v0.12 stable and SDK v2.1.10\n``` javascript\nvar callback = function(err, data){ if (err) console.log(err); else console.log(data)};\nsimpledb.putAttributes({\n  Attributes: [{Name: 'foo', Value: 'bar'}], \n  DomainName: 'foobar', \n  ItemName: 'test'\n}, callback);\nsimpledb.select({SelectExpression: 'select * from aditm'}, callback);\n```\nCan you provide some more information about your environment? Are you running this on an EC2 instance?\n. @Alex0007 @stockholmux \nI'm closing this issue as the problem is not reproducible, and the SDK seems to be working with Node.js v0.12 stable. Feel free to reopen this issue or open another issue if you continue to have problems.\n. > S3 allows an x-amz-meta-* query param to be passed along with a GET request to a signed url\nFrom the S3 Object and Metadata documentation\n\nYou provide this optional information as a name-value pair when you send a PUT or POST request to create the object\n\nThis clarifies what @lsegal said about only being able to pass metadata with S3.putObject. When you are generating a pre-signed URL the metadata will be added as a query string. If you remove it you will get a signature error.\nI hope this helps clarify things!\n. @fantapop \nI'm closing this issue since you seem to have it working. Feel free to reopen this issue or another issue if you have any other questions.\n. @jasonvasquez \nBrowserify introduced a breaking change in v4.0 which included a module-deps overhaul. This changed the way transformation modules were fetched. \nFor now you can use  Browserify v3.x, and it should work as expected. I will keep this open to track it when we do consider updating our dependencies.\nThanks for reporting!\n. @blocka \nThanks for tracking this down to the breaking change in browser-resolve v1.7.0. I can confirm that I can get the builder to work with browser-resolve 1.6.0. I was also able to get it to work by locking in \nnode-resolve to 1.0.0 (with browser-resolve v1.7.0).\nReferencing substack/node-resolve/issues/69.\n. @jasonvasquez @blocka \nThis is resolved in node-resolve v1.1.3. I am able to build the SDK as a dependency  with browserify 8.x.\nClosing this issue since the problem seems to be resolved. Feel free to re-open this issue or open another issue if you continue to have problems building the SDK as a dependency.\n. @oz-boots This should fix the builder on a Windows box. Feel free to reopen this issue if you continue to have problems building, or have any other questions.\nThanks for reporting!\n. Referencing #383 \n. I'm closing this issue out as the question seems to be answered. Feel free to re-open this issue, or open another issue if you have other questions.\n. I'm closing this issue out as the question seems to be answered. Feel free to re-open this issue, or open another issue if you have other questions.\n. From what I read in the SNS Publish documentation, you will need to send a platform specific payload to mobile devices.\nFor Android your messsage would need to look like this:\n``` javascript\nvar message = { \n  GCM:{\n    data:{\n      message:'Foo bar!',\n      url:'www.example.com'\n    }\n  }\n}\n// SNS.publish() params\nvar params = {\n  TargetArn: 'arn:aws:sns:us-west-2:123456789012:endpoint/GCM/foobar/1234567890'\n  Subject: 'foo',\n  Message: JSON.stringify(message)\n}\n```\nLet me know if this helps!\n. From what I read in the SNS Publish documentation, you will need to send a platform specific payload to mobile devices.\nFor Android your messsage would need to look like this:\n``` javascript\nvar message = { \n  GCM:{\n    data:{\n      message:'Foo bar!',\n      url:'www.example.com'\n    }\n  }\n}\n// SNS.publish() params\nvar params = {\n  TargetArn: 'arn:aws:sns:us-west-2:123456789012:endpoint/GCM/foobar/1234567890'\n  Subject: 'foo',\n  Message: JSON.stringify(message)\n}\n```\nLet me know if this helps!\n. @jpfranco \nThe AWS.S3.ManagedUpload is a convenience wrapper over AWS.S3.createMultipartUpload, AWS.S3.uploadPart, and AWS.S3.completeMultipartUpload(). \n\nIs it a requirement to use signed requests when using multipart uploads?\n\nEdit:\n~~Yes, because Amazon S3 does not allow unauthenticated access to these operations.~~ \nOops! S3 does allow unauthenticated access based on bucket policy.\nOne way to do this would be to create a separate IAM user with only write access using these operations (and putObject) to this bucket. This would also be easier to manage because it is easier to revoke permissions on this single user.\nThe other approach is to set the s3.api.signatureVerision = ''. This would not sign your request, but will apply to all operations.\nAlternatively,\nYou can authenticate the SDK in the browser using Amazon Cognito. Amazon Cognito can be used with unauthenticated roles as well, so your users will not need to log in to access your AWS resources. \nNote: It is important to scope your unauthenticated roles carefully, to only provide access to the resources that you want to expose.\nLet me know if this helps, and I'd be happy to answer other questions you may have!\n. @jpfranco \nThe AWS.S3.ManagedUpload is a convenience wrapper over AWS.S3.createMultipartUpload, AWS.S3.uploadPart, and AWS.S3.completeMultipartUpload(). \n\nIs it a requirement to use signed requests when using multipart uploads?\n\nEdit:\n~~Yes, because Amazon S3 does not allow unauthenticated access to these operations.~~ \nOops! S3 does allow unauthenticated access based on bucket policy.\nOne way to do this would be to create a separate IAM user with only write access using these operations (and putObject) to this bucket. This would also be easier to manage because it is easier to revoke permissions on this single user.\nThe other approach is to set the s3.api.signatureVerision = ''. This would not sign your request, but will apply to all operations.\nAlternatively,\nYou can authenticate the SDK in the browser using Amazon Cognito. Amazon Cognito can be used with unauthenticated roles as well, so your users will not need to log in to access your AWS resources. \nNote: It is important to scope your unauthenticated roles carefully, to only provide access to the resources that you want to expose.\nLet me know if this helps, and I'd be happy to answer other questions you may have!\n. @jpfranco \nUnauthenticated access is actually allowed. I misread your initial question and have updated my response above.\nI would recommend that you use Amazon Cognito for unauthenticated access anyway. Cognito allows you to configure access using IAM roles at a very granular level. In addition it is also possible to audit identities that users are assigned using Cognito.\n. @jpfranco \nFor a test setup you can set s3.api.signatureVersion = ''. Note that this would not sign any request made to S3, and would affect all your S3 clients.\nLike I mentioned earlier, the other way to do it would be to use a scoped IAM user with hardcoded credentials.\ns3.makeUnauthenticatedRequest() does not currently work with s3.upload() because s3.upload() internally makes 3 (sometimes fewer) different operation calls.\n. @jpfranco \nI am able to confirm the error. It seems like you cannot initiate multipart uploads anonymously. \nFor now you could use Cognito to provide anonymous access. In the meantime, I'll verify this behavior with the Amazon S3 team, just to make sure it's not a configuration issue.\n. @jpfranco \nI just confirmed that multipart uploads do not work with unauthenticated requests. I'm closing this issue out since you already have a workaround. Feel free to open this issue or another issue if you have any other questions.\n. @polythene1337 \nThanks for reporting this! All of our documentation is auto generated and I will reach out to the Route53 service team to report / clarify this parameter.\n. @polythene1337 \nI'm closing this issue out because the documentation needs to be fixed upstream by the Route53 team. I would recommend opening an issue on the Amazon Route53 forums.\nFeel free to reopen this issue, or open another issue if you have other questions.\n. @jpfranco \nPulling out the network cable doesn't simulate a connection timeout. The SDK will retry the error though because you have configured it to retry once.\nIf you are using an OSX install you can use the Network Link Conditioner to simulate a slow connection. There should be other alternatives depending on the platform you are using.\nI was able to set my timeout to 2000ms and simulate timeout errors\n{ [TimeoutError: Connection timed out after 2000ms]\n  message: 'Connection timed out after 2000ms',\n  code: 'NetworkingError',\n  time: Wed Feb 18 2015 14:46:39 GMT-0800 (PST),\n  region: 'us-west-2',\n  hostname: 'testbucket.s3-us-west-2.amazonaws.com',\n  retryable: true }\nLet me know if this helps.\n. @jpfranco any time!\n. @noah-freitas \nI am able to confirm this behavior. This seems like a problem with Amazon S3 and not an SDK issue since the SDK does conform to the S3 wire protocol spec.\nI would recommend opening an issue on the Amazon S3 Forums. I will also try and follow up with the S3 team on this. \n. @noah-freitas \nI'm closing this out since it is not an SDK issue and needs to be fixed upstream by the Amazon S3 team.\nFeel free to reopen this issue or open another issue if you have other questions.\n. @piuccio \nYou can use a combination of Prefix and Marker to filter your keys. Only keys that are lexicographically after the Marker will be listed. Here are some examples:\n``` javascript\nvar bucket = new AWS.S3({params: {Bucket: 'foo'}});\nvar callback = function(err, data) { if (err) console.log(err); else console.log(data);}\n// Objects since 2015-02-23\nbucket.listObjects({Prefix: '2015-02', Marker: '2015-02-23-00:00:00'}, callback);\n// Objects after 4:00 p.m on 2015-02-23\nbucket.listObjects({Prefix: '2015-02-23', Marker: '2015-02-23-16:00:00'}, callback);\n```\nThe S3 API documentation describes this in further detail: ListObjects.\nIf you want something even more sophisticated, then you can use JMESPath to filter through a fetched list of objects. \nLet me know if this helps!\n. Closing this issue since there are a couple of solutions suggested. Feel free to reopen this issue if you have other questions.\n. Which version of the SDK are you using? Also which version of Node.js is this server running? Can you provide some sample code that you are using?\nMy initial guess would be the time on your server is not correctly synchronized.\n. I'm going to mark this as closed since there hasn't been any recent activity. Feel free to reopen this issue or another issue if you continue to run into problems.\n. @demisx \nBower installation instructions have been added to the README as well as the developer guide. The changes to the developer guide will go out with our next release.\nThanks for reporting!\n. @polythene1337 \nThis fix is out in master. I will cut a patch release shortly.\n. @polythene1337 \nI just pushed a release. You can now use v2.1.16 from npm. Let me know if this fixes the issue for you.\nThanks again for reporting!\n. @trozware \nYou'll have to update to the latest version of the SDK:\nsh\n$> npm update aws-sdk\nLet me know if this helps.\n. @evansolomon \nCircling back to the error stack, it looks like the skew is actually 5 min and not 48 hours. Would you be able to re-run the date command on your Docker image and let me know what you find?\nDo you also encounter the error when you run the same code outside your Docker image? \nAlternatively, would you be able to share your Docker image? This would help debug the issue.\n. @evansolomon I've just pushed a patch that offsets the SDK clock when a clock skew error is detected. Simply set the correctClockSkew option when constructing a service client.\nHere's an example:\njavascript\nvar dynamodb = new AWS.DynamoDB({correctClockSkew: true});\nLet me know if this works for you. Thanks for your patience.\n. @evansolomon This is turned off by default because there may be other customers who are applying a clock skew correction using the AWS.config.systemClockOffset property.\n. This error is thrown when the SDK encounters a lower level ENOTFOUND error. This is usually because of a transient network or DNS issue.\nWould you be able to verify network / DNS connectivity of the environment running your code? I'd be interested to know what you find.\nHope this helps!\n. I did some testing with this. It doesn't seem like there is any noticeable impact for clients talking to incorrectly configured regions. \nThis also means that it is unlikely that the retry backoff is long enough to catch transient network issues that are of a longer duration.\n. Merging this in. Tests coming up in just a minute.\n. @shino Can you provide a reproduction test case with a failing request?\n. @shino \nThanks for the reproduction test case. I'm going to verify this behavior and merge this in if everything looks good.\n. @JacobEvelyn \nWhat is the size of the file you are uploading? Also which version of the SDK are you using? This information will help debug the issue.\nEdit: Would you also be able to provide a network log of your upload call from the dev tools console.\n. I'd be interested in the network logs of the request that is failing. For what threshold of object sizes do you notice failures? FWIW, I'd recommend upgrading to v2.1.16 or above.\nI've tried to reproduce it with a large object (~50M) and I have been unable to do so.\n. Can you provide some sample code that you are using?\nEdit: Some more context will be helpful. I haven't been unable to reproduce the issue with a simple s3.upload() call.\nJust to clarify, if a request hits a parameter validation error, it will not progress to the send state where the wire transfer actually happens. If you aren't seeing failures with your wire transfer, is it possible that you have another code path that is actually sending the request correctly? \nAlso can you turn on SDK logging like this:\njavascript\nAWS.config.logger = console.log\nThis will log what the SDK is trying to send.\n. @JacobEvelyn \nI'm marking this as closed because there hasn't been any recent activity. Feel free to reopen this issue or open another issue if you have any questions.\n. @JacobEvelyn anytime! I'm glad you got it working.\n. Pulling this in. Thanks for the contribution @joscha!\n. @ondruska We are looking into this. Thanks for reporting!\n. @milashenko Tests on Node.js 0.12 are now supported and also build on Travis. Thanks for reporting!\n. @chetandhembre \nThe SDK currently doesn't have any optimistic locking abstractions for DynamoDB. I will mark this as a feature request, because this will involve developing a high level abstraction that supports this behavior.\n. @chetandhembre \nI would recommend opening a feature request with the DynamoDB Document SDK project.\nI'm marking this as closed in favor of the feature request that you can open here: awslabs/dynamodb-document-js-sdk/issues.\nFeel free to open another issue if you have any questions.\n. CC @lsegal \n. Which version of the SDK and Node.js are you using?\n. @daguej \nThe error code Throttling is correct and is used by some services including AWS CloudFormation. The CloudFormation API documentation for common errors elaborates on this. I agree that certain services may return a ThrottlingException error code and consequently this should be an additive change. Out of curiosity, which service is returning this exception?\n. @daguej \nMerging this in. Thanks for contributing!\n. AWS.SimpleDB is not included in the default hosted build of the SDK. Here are a list of supported services included in the hosted build. You can of course choose to build your own version of the SDK if your are using it in an environment where you have explicit control of CORS configuration.\nEdit: To answer your questions:\n- Yes it is safe to lock the API version.\n- As I mentioned earlier, you can build a custom version of the SDK that does include support for Simple DB.\nHope this helps!\n. @gagecarto that would not work.\nBuilding your own SDK version is actually quite simple. All you need to is git clone this repository and npm install the dependencies (assuming you have Node.js installed). This is described in detail in the developer guide here: Building the SDK for Use in the Browser.\nI'm curious about where you are using this actually. Are you running this in an environment which does not enforce CORS?\n. @gagecarto \nSimpleDB currently does not support CORS requests. The hosted browser version of the SDK includes support for all services that do support CORS. I would recommend opening an issue on the SimpleDB Forums to request CORS support. \nAs an alternative, you could also use DynamoDB if your application is talking directly to services from the browser.\nI hope this helps.\n. This usually occurs because you are serializing the entire form when you post to your signed URL.\nInstead of serializing your form you will need to supply the File object from the form control as the data to your asynchronous request.\nForm control:\nhtml\n<input type=\"file\" id=\"upload\"/>\nUploading your file:\n``` javascript\nvar upload = document.getElementById('upload');\nvar file = upload.files[0];\n// Assuming jQuery\n$.ajax({\n  type: 'PUT',\n  url: signedUrl,\n  data: file,\n  success: callback\n});\n```\nLet me know if this helps.\n. Like I mentioned earlier, this does look like your are serializing the entire form. Instead of posting the entire form to the signed URL, you will have to upload only the file object asynchronously to the signed URL.\n. I'm not certain about the details of how this plugin works. I would recommend opening an issue with the maintainer of that plugin to help debug any issues with the plugin.\n. @clues \nI'm marking this as closed since you seem to have it working. Feel free to reopen this issue or another issue if you have any questions.\n. @iwhitfield I haven't been able to replicate the issue with the code posted above. Do you have any code/test conditions that can help reproduce the issue?\n. @lostcolony Can you provide the exact size of the file you are uploading in bytes? Can you also enable logging in the SDK as follows:\njavascript\nAWS.config.update({\n  logger: console\n});\nI'd specifically like to know if the request is being retried. Thanks!\n. @zackehh @lostcolony @silveur I was able to reproduce this by creating a bucket in a region that is not us-east-1 and configuring my AWS.S3 client to use us-east-1 and using the above example. \nI should have a fix out for this soon. Thank you for reporting and reopening!\n. @sboora \nThanks for reporting! Would you be able to share details of your sampling method? Are you factoring out network latency in your measurement?\n. @sboora glad to see that you are getting consistent performance with version 2.x of the SDK.\n. @sboora @chaosgame \nI tried reproducing this on a fresh c3.8xlarge instance with Node.js v0.10.38.\n``` sh\nAWS SDK Version 1.15.0\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 125.87/43.14/124, throughput (/sec): 142.9, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 111.87/36.93/108, throughput (/sec): 146.28, errors: 0\nAWS SDK Version 1.17.0\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 137.49/46.07/128, throughput (/sec): 140.15, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 125.12/46.52/125, throughput (/sec): 140.86, errors: 0\nAWS SDK Version 2.1.24 (convertResponseTypes: false)\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 159.71/40.09/147, throughput (/sec): 134.71, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 122.12/22.32/121, throughput (/sec): 139.06, errors: 0\nAWS SDK Version 2.1.24 (convertResponseTypes: true)\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 163.4/50.46/154, throughput (/sec): 135.24, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 142.45/39.3/134, throughput (/sec): 139.83, errors: 0\n```\nThere is some delta between v1.15.0 likely owing to the added events that are emitted in this patch https://github.com/aws/aws-sdk-js/commit/d75bb771c618aba847dcd20de9e09045bb5f0265\nI am not getting any delta in GetItem calls between v1.17.0 and v2.x with convertResponseTypes: false\nThere is some delta in GetItem calls between v2.x with convertResponseTypes: false and v2.x with convertResponseTypes: true. This is expected because of the response type conversion added in v2.x\nI am yet to run these benchmarks on Node.js v0.12.x, but what I'm seeing is not entirely surprising given the incremental features in newer versions. \nI'd be interested to hear your thoughts on this.\n. @kylehg Thanks for keeping us posted! Glad to know that your test results are not showing any significant change in performance between v1.x and v2.x.\n. @shamoons \nThis looks like it is related to #547. It looks like your AngularJS code is posting the entire form to the signed url. You should instead only upload the file to the signed URL. \n. Would you be able to share a full wire trace of your request?\n. @shamoons \nI missed this in the sample code that you provided. It looks like you are not specifying a ContentType when generating a signed URL. You will need to specify the ContentType when generating a signed URL and then set the Content-type header when you PUT to the signed URL.\nI hope this helps!\n. Would you be able to share the code that you're using to upload the file from your web client? \nFWIW, you should quote the signed URL when using it with cURL. \n. I'm not able to reproduce the issue when using a signed URL with cURL. Have you tried quoting the signed URL instead of escaping characters?\nsh\ncurl -v -H \"Content-Type: image/png\" --upload-file \"https://bucket.s3-us-west-2.amazonaws.com/test?AWSAccessKeyId=foo&Content-Type=image%2Fpng&Expires=1428082106&Signature=bar%3D\"\n. @shamoons \nThe ContentType that you provide to AWS.S3.getSignedUrl must be the same as the Content-Type that you are uploading your file with.  \nCan you share some sample code you're using to upload a file the signed URL? I'd also be interested to see the sample code that you're using to generate a signed URL.\n. It does not look like you are setting the Content-Type in your request. \njavascript\nheaders: {\n  'Content-Type': undefined\n}\nYou will have to set the Content-Type to whatever you used when you were generating your signed URL (in this case image/png).\nAs a side note, using FormData will corrupt the binary data that you are sending to S3. You should not use FormData and instead provide only the file object as a payload. Refer to #547/comment.\n. I'm marking this as closed since this is not an issue with the SDK. @shamoons when using a signed URL you have to make sure that the ContentType that was used to generate the signed URL must match the Content-Type header of the request to the signed URL.\nFeel free to reopen this issue or another issue if you have any other questions.\n. @patoncrispy I can't say for sure. Perhaps @chrisradek or @LiuJoyceC might be able to comment on that.\n. @mpranjic \nI am able to confirm that the AccessKeyId is not immediately \"active\" after it is created. There is no IAM API that the SDK can poll to check the status of the AccessKeyId, and this makes it difficult to add a waiter state to poll an AccessKeyId.\nThe workaround would be to use the AccessKeyId after a delay, similar to what you already seem to be doing. I would also recommend opening an issue on the IAM Forums to get further clarification on this behavior. \n. CC: @lsegal \n. CC: @lsegal \n. CC: @lsegal \n. CC: @lsegal \nAll integ tests pass.\n. @stockholmux \nI can verify that SQS does not accept '*' as a possible list value for the AWSAccountIds parameter. I would recommend opening an issue on the SQS Forums to report this behavior.\nIn the meantime, you can also use the AWS.SQS.setQueueAttributes() for more fine grained control of SQS policies.\n``` javascript\nvar AWS = require('aws-sdk');\nvar sqs = new AWS.SQS();\nvar policy = {\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"Queue_Policy\",\n  \"Statement\": \n    {\n       \"Sid\":\"Queue_AnonymousAccess_ReceiveMessage\",\n       \"Effect\": \"Allow\",\n       \"Principal\": \"\",\n       \"Action\": \"sqs:ReceiveMessage\",\n       \"Resource\": \"arn:aws:sqs::111122223333:queue\"\n    }\n}\nsqs.setQueueAttributes({\n  QueueUrl: 'https://sqs.us-west-2.amazonaws.com/111122223333/queue',\n  Attributes: {\n    Policy: JSON.stringify(policy)\n  }\n}, callback);\n```\nLet me know if this helps!\n. @stockholmux \nI'm marking this as closed. Feel free to reopen this issue or open another issue if you have any questions.\n. @balmasi would you be able to verify if you still see this issue with v2.1.28 of the SDK? \nThere were a couple of related fixes and performance improvements in AWS.S3.ManagedUpload that landed in v2.1.28.\n. @balmasi I was wondering if you had a chance to see if the problem still persists after upgrading the SDK to v2.1.28 or higher?\n. @balmasi I'm marking this as closed as we haven't been able to reproduce the issue. I would recommend upgrading to v2.1.28 of the SDK. Feel free to reopen this issue or open another issue if you still notice this behavior after you upgrade.\n. S3 integration tests pass (for S3 and V4 signature versions)\nBrowser integration tests pass\nAll unit tests pass\n. Thanks for the review!\n. @scampiuk \nI'm marking this as closed. Feel free to reopen this issue or another issue if you have any more questions.\n. I've added a couple more tests to ensure that the checksum is computed only once. These fail if the casing of the X-Amz-Content-Sha256 is not honored. \nThe unit test case specifically for the Glacier client is somewhat difficult to write given that crypto.sha256() is invoked in a loop which depends on the size of the payload.\nLet me know if you have other ideas. And thanks much for your time in reviewing this!\n. @patoi \nSMTP credentials are not the same as the AccessKeyId and SecretAccessKey credential pair and therefore cannot be used by the SDK to sign requests.\nFrom the SES documentation:\n\nYour SMTP user name and password are not the same as your AWS access key ID and secret access key\n\nIt looks like nodemailer-ses-transport is using the SMTP credentials to sign requests (see ses-transport.js#L49). This will not work for the aforementioned reason and is not an issue with the JavaScript SDK. \n. @patoi \nI'm marking this as closed since it is not an issue with the SDK. Feel free to reopen this issue or another issue if you have any questions.\n. I'm marking this as closed since this isn't an SDK issue. Feel free to open another issue if you have other questions.\n. @tgroshon Thanks for reporting this! We are looking into this feature request. I will keep this issue updated as we have more information on when this will land in master.\n. @pablocaselas \nThis is a known issue with Node v0.10.x (see joyent/node/issues/#6065. This has also been discussed a couple of times on this repository as well (see #158).\nWe did investigate changing the highWaterMark setting of stream.Readable, but noticed a non-trivial memory performance regression (see #551).\nLet me know if upgrading your Node runtime to v0.12.x fixes this issue.\nHope this helps!\n. CC: @lsegal \n. @LasCondes We are looking at this feature request. In the meantime, there is a related npm module (aws-cloudfront-sign)[https://github.com/jasonsims/aws-cloudfront-sign] which generates CloudFront signed URLs. I would recommend opening a feature request on this projects page for CloudFront signed cookies as well. \nHope this helps!\n. Can you explain what you are trying to do? I am able to include the SDK as a script tag directly on a html partial and use it in that partial page.\n\n. The AWS SDK distributable relies on the existence of the window object. As I understand, Google Apps Script explicitly disallows any global access. I would recommend using an HTML partial with script tag. This way, you have access to the Google Apps Script libraries as well as the AWS SDK. You will even be able to pass data from your AWS resources into Google Apps.\nLet me know if this helps.\n. @cbkihong I'm looking into this and will have a fix out shortly, that should support blob keys as well.\n. @cbkihong the above fix should allow you to use binary keys. Let me know if you run into any issues!\n. @AWSRandall \nWhat you are referencing is the documentation source. Here is the corresponding published documentation: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/. The link to the API reference documentation is pointing to the correct page.\n. To add to what @lsegal said, you can use the async module with the SDK to download your Amazon S3 objects serially or in parallel. For example:\n``` javascript\nvar AWS = require('aws-sdk');\nvar async = require('async');\nvar fs = require('fs');\nvar s3 = new AWS.S3({params: {Bucket: 'foo'}});\n// Called after all iterators complete\n// Write your clean up here\nvar callback = function(err) {}\ns3.listObjects(params, function(err, data){\n  // GET objects in parallel\n  async.each(data.Contents, function(file, cb){\n    var stream = s3.getObject({Key: file.Key}).createReadStream();\n    stream.on('end', cb);\n    var file = fs.createWriteStream('/path/to/' + file.Key);\n    stream.pipe(file);\n  }, callback);\n});\n```\nHope this helps!\n. Can you post the code you are using? \n. Looks like you are trying to do response.send() twice. Here:\n\nres.send(data);\n\nand here:\n\nres.send(data.Body.toString())\n\nThis effectively means you are sending more than one response per request which is not possible. See Stack Overflow: cant-do-multiple-res-send-in-express-js.\nThere are a couple ways to work around this:\n1. Make a AWS.S3.listObjects() call from your client (which may be a browser or mobile application) and follow this up with multiple AWS.S3.getObject() calls. Amazon S3 supports CORS and you can make requests directly to Amazon S3 with the SDK in the browser. You can then render this data incrementally as each AWS.S3.getObject() request completes.\n2. Alternatively, you can assemble all your JSON data before sending a response back to your client. For example:\n``` javascript\nvar payload = {};\npayload.objects = [];\n// Send your response after all requests have completed\nvar callback = function(err) {\n  res.send(payload);\n}\ns3.listObjects(params, function(err, data){\n  payload.list = data.Contents;\n  // GET objects in parallel\n  async.each(data.Contents, function(file, cb){\n    var stream = s3.getObject({Key: file.Key}, function(err, data) {\n      payload.objects.push(data.Body.toString());\n      cb();\n    })\n  }, callback);\n});\n```\nYou can then parse this response data on your client.\n1. In the third approach, you can use response.write() instead of response.send(). This will however, make it harder to delimit and parse the data on your client.\nIf you use approach (2) or (3), you will only have access to the data after all requests have completed. This means you will not be able to incrementally render your objects as they are fetched.\nAlso, irrespective of your approach, the number of requests you make to Amazon S3 does not change.\nHope this helps!\n. The data property is on the Response object, not the Request object. You can access it as response.data.Body. \n. @nousacademy I'm going to mark this issue as closed because it looks like you have this working. Feel free to reopen this issue or open another issue if you have other questions.\n. I'm marking this as closed as there hasn't been any recent activity. Feel free to reopen this issue or open another issue if you have any questions.\n. @lsegal \nI'd appreciate it if you could take a look at this - specifically the addError() function. This looks a little clunky to me. I was wondering if you had ideas on streamlining it.\n. @alanmimms I'm marking this as closed as you do seem to have a workaround for now. Feel free to reopen this issue or open another issue if you have any questions.\n. Can you share the raw HTTP response body?. The raw HTTP response body can be accessed in the following manner:\njavascript\ns3.putBucketNotificationConfiguration(params,  function(err, data) {\n  // Logs the raw HTTP response\n  console.log(this.httpResponse.body.toString());\n});\nMy initial guess is that your S3 resource is not authorized to invoke your Lambda function. The raw HTTP response should throw some light on what is actually happening.\n. @PunkyPot I'm marking this as closed because it doesn't look like an issue with the SDK. As @lsegal mentioned, the AWS Lambda guide discusses how to grant your Lambda function access to your Amazon S3 resources. \nThe  Working with Policies guide further discusses attaching IAM policies to your roles. \nFeel free to reopen this issue or open another issue if you have other questions.\n. @dconnolly I will look into adding some tests for this and then merge it in if everything looks good.\n. All integ tests pass\nAll browser integ tests pass\nCC @lsegal \n. I'm marking this as closed as there doesn't seem to be an issue with the SDK. As a side note, I would recommend opening an issue on the Amazon SES Forums requesting CORS support. \nFeel free to reopen this issue or open another issue if you have other questions.\n. Which client are you specifically referring to? All the API updates are backward compatible and shouldn't affect any consuming code.\n. Oops! I missed that. Thanks for catching this @ondruska !\n. This is now fixed: http://aws.amazon.com/releasenotes/5873934951898767\nThanks much for reporting @ondruska !\n. @mdouglass Thanks for the pull request! We would like to have each error preserve its own stack instead of appending to the original error stack. \nI've pushed up a fix that should resolve this issue as well as a related issue with error names. Thanks again for reporting, as well as the fix.\n. Fixed in #646\n. @amiorin \nI can replicate this issue, and I'm working on a fix for this. I should have it out in master soon.\nThanks very much for reporting!\n. @ramonck \nThis is the service response from AWS Security Token Service (STS). The actual request does include the Accept-Encoding:gzip, deflate, sdch header. It is left to the service whether they wish to compress content before wire transport. There is really nothing that the SDK can do to mitigate this.\nI also picked the response of the STS call from your page and ran it through gzip compression for comparison. Here are the results:\nsh\n-rw-r--r--   1 adityamanohar  Users      1678 Jun 29 12:00 response\n-rw-r--r--   1 adityamanohar  Users      1076 Jun 29 12:00 response.gz\nYour gaining close to 600 bytes on compressing the response. There may not be any cost benefit when you compare the time taken to transmit 600 additional bytes vs. the time taken to compress this content.\nFWIW, I would recommend opening an issue on the AWS STS Forum (part of the AWS Identity and Access Management forum) to request for this feature.\nHope this helps!\n. You should be able to lazy load your Cognito credentials by hooking into the window.onload or $(document).ready() (looks like you are using jQuery) handlers.\nLet me know if this helps!\n. From frontend.js it looks like you are fetching the SDK and getting Cognito credentials when the cjs.min.js loads:\njavascript\n$.getScript('js/aws-sdk.min.js', function() {\n  //  window.addEventListener('awsLoaded', function() {\n  var params = {\n    RoleArn: \"arn:aws:iam::215507325553:role/Cognito_jtsiteUnauth_Role\",\n    IdentityPoolId: 'us-east-1:511614d4-4078-4d39-be6c-cfd9bf7692fe'\n  };\nThis will be treated by the browser as a synchronous script because it will not load and parse the next script file until this script file has been parsed and executed completely.\nWhat I was suggesting earlier was to register a handler with the window.onload event or jQuery's $(document).read() which will execute after all synchronous script loading happens.\nHope this helps!\n. @ururk \nThese models are automatically generated and are not manually maintained. As such we would be hesitant to merge in this fix. I would recommend opening an issue on the Amazon Elastic Compute Cloud (EC2) Forum (also serves Elastic Load Balancing) to request for this to be fixed. \nIn the meantime, I will also try to get in touch with the Elastic Load Balancing team and request that this be fixed upstream as other AWS SDKs also consume the same JSON document.\nThanks for reporting and the pull request!\n. Closing this pull request as discussed above.\n. CC @lsegal \n. CC @lsegal \n. @imranansari This looks like it is related to #528. This is typically related to underlying network / DNS resolver issues and there is not much that the SDK can do here besides retrying. It looks like as of SDK v2.1.18 we are retrying this class of errors. Which version of the SDK are you using? \n. @imranansari I'm marking this as closed as it is not an SDK issue. If you haven't already, I would recommend upgrading to v2.1.18 of the SDK or higher where these class of errors are being retried. \nFeel free to reopen this issue or open another issue if you have other questions.\n. @yoannmoinet @jacob-israel-turner I've pushed a fix which removes the dist/ directory from .npmignore.\n. @askmon It looks like you have incorrectly formatted CoffeeScript. I am not able to reproduce the problem with the following parameters:\njavascript\nparams = { \n  Message: JSON.stringify({default: 'test'}),\n  MessageStructure: 'json',\n  TargetArn: 'arn:aws:sns:us-west-2:012345678912:foo-bar',\n  MessageAttributes: { \n    'AWS.SNS.MOBILE.GCM.TTL': { \n      DataType: 'String', \n      StringValue: '300' \n    },\n    'AWS.SNS.MOBILE.APNS.TTL': { \n      DataType: 'String', \n      StringValue: '300' \n    } \n  } \n}\nOn an related note, the Message body in your sample code above also looks like incorrectly stringified JSON. \nHope this helps! \n. @mdouglass Thanks for for reporting this! I am able to reproduce the issue and it looks like the error is indeed being returned from the service.\nI will try getting in touch with the EC2 team to report this issue. In the meantime, feel free to open an issue on the EC2 Forums letting them know about this issue.\n. @mdouglass \nI did receive confirmation from the EC2 team that the CreateRoute operation does not accept the ClientToken parameter. I'm working on fixing the documentation to reflect this.\nThanks again for reporting this issue!\n. @solker I'm marking this as closed because this is not an issue with the SDK. Feel free to reopen this issue or open another issue if you have other questions.\nFWIW, here is some sample code to elaborate what @lsegal mentioned above:\n``` javascript\nvar AWS = require('aws-sdk');\nvar fs = require('fs');\nvar s3 = new AWS.S3();\nvar fileStream = fs.createWriteStream('path/to/file');\nvar s3Stream = s3.getObject(params).createReadStream();\n// You can alternatively pipe this into a HTTP response stream\ns3Stream.pipe(fileStream);\n``\n. @StuAtGit \n[AWS.Signers.V2](https://github.com/aws/aws-sdk-js/blob/master/lib/signers/v2.js) is not supported by Amazon S3. S3 supports a modified version of Signature Version 2. In the SDK this is implemented by [AWS.Signers.S3](https://github.com/aws/aws-sdk-js/blob/master/lib/signers/s3.js). You shouldn't have to specify this signer as theAWS.S3` client picks this up by default. \njavascript\nvar s3 = new AWS.S3({signatureVersion: 's3'});\nIf you want to use Signature Version 4, then you must specify this as an option.\njavascript\nvar s3 = new AWS.S3({signatureVersion: 'v4'});\nHope this helps clarify things!\n. @gvelo I'm looking into this and will get back to you soon.\n. @gvelo \nThis looks like it is similar to #397. This is just the default behavior of streams in Node.js.\nNote that when using streams you want to hook into the stream's error event and not the event emitted by the AWS.Request object. \nHere is a modified version of your example: \n``` javascript\nvar AWS = require('./');\nvar fs = require('fs');\nvar params = {\n  Bucket: 'foo',\n  Key: 'bar'\n};\nvar s3 = new AWS.S3({region: 'us-west-2'});\nvar req = s3.getObject(params);\nvar read = req.createReadStream();\nvar write = fs.createWriteStream('/Users/aditm/bug');\nread.on('error', function(err) {\n  console.log(err);\n});\nread.pipe(write);\n```\nThis code does not throw for me. Let me know if this helps!\n. @gvelo I'm marking this as closed as there hasn't been any recent activity. Feel free to reopen this issue or open another issue if you have other questions.\n. @bradvogel @wearhere I've opened up a pull request on jsdelivr: https://github.com/jsdelivr/jsdelivr/pull/6465\n. I'm marking this as closed because this seems to have landed in jsdelivr. Feel free to reopen if you run into problems.\n@bradvogel thanks for bringing this up!\n. @BjoernRuberg  can you share the code that you are using?\n. @BjoernRuberg For larger files I would recommend using AWS.S3.ManagedUpload. \nIf you are using AWS.S3.putObject() with large files, you can also increase the socket timeout by setting the httpOptions.\nFor example:\njavascript\nvar s3 = new AWS.S3({\n  httpOptions: {\n    timeout: 240000\n  }\n});\nI'm going to mark this as closed since you already seem to have it working with AWS.S3.ManagedUpload. Feel free to reopen this issue or open another issue if you have other questions.\n. @wearhere if you are using the default 's3' signature version, you can set the params.Expires to 200 years or any other value.\nIf you are using signature version 4 then the maximum expiry time on presigned URLs is one week.\nHope this helps clarify things!\n. @watson I'm going to have to investigate this a little further. I will get onto this sometime soon and let you know what I find.\nThanks for reporting!\n. @Durss It looks like you might be using a JavaScript library that is adding properties Object.prototype - in this case inEnum and getProp.\nYou can verify this by doing:\njavascript\nfor (var key in params) { console.log(key); }\nYou should see only Bucket, Key, and Body. If you see anything else then this is caused by an external JavaScript library.\nThis is similar to #575. Let me know what you find!\n. @Durss  no problem! \nI'm going to mark this as closed since it isn't a problem with the SDK. Feel free to reopen this issue or another issue if you have any questions.\n. @imranansari the SDK delegates directly to Node.js HTTP request.setTimeout(). If you are sending a large number of messages then I would recommend that you increase the default configured httpOptions.timeout value to something greater than 120000 seconds. \nFor example: \njavascript\nvar sqs = new AWS.SQS({httpOptions: { timeout: 240000}});\nLet me know if this helps!\n. @imranansari which version of Node.js are you running? On Node.js v0.10+ it might be worth to increase the number of agent.maxSockets since the default is set to 5. Here is code snippet to increase the global max sockets:\njavascript\nrequire('http').globalAgent.maxSockets = require('https').globalAgent.maxSockets = 10\nLet me know if this helps!\n. @imranansari I'm marking this as closed because there doesn't seem to be any recent activity. Feel free to reopen this issue or open another issue if you continue to have problems.\n. @DenisGorbachev I'm not sure of the details but this sounds like this is related to the behavior of the service and not the SDK. \nDo you have any sample code that you can share?\n. @DenisGorbachev if you haven't already, I would recommend checking out the Amazon SWF Developer Guide for examples.\nI would also recommend opening an issue on the Amazon SWF Forum describing the problem.\nUnfortunately, there is not much that the SDK team can do to mitigate a service related issue. \nLet me know if this helps!\n. @DenisGorbachev I'm marking this as closed as there doesn't seem to be much that can be on the SDK side of things. Feel free to reopen this issue or open another issue if you have other questions.\n. @pbhadauria unfortunately I'm not sure of the details of a SWF workflow lifecycle. I would recommend opening an issue on the Amazon SWF Forum or chiming in on an forum post. In the meantime, I will also try getting in touch with the SWF team internally to get some clarification on the issue. \n. @DenisGorbachev I've tried to get in touch with the SWF team, but I haven't received a response yet. I will keep this thread updated when I do get a response from the SWF team.\n. @jonhester Do you mean support for Amazon EC2 Container Service? The SDK already has support for Amazon EC2 Container Service under the AWS.ECS namespace. See the API documentation for AWS.ECS.\n. @davidporter-id-au It looks like the EC2 metadata service is throttling requests from your code. The SDK itself does cache credentials fetched from the metadata service, so multiple simultaneous requests don't bombard the metadata service. See https://github.com/aws/aws-sdk-js/pull/448\nIs your code part of a shell script that is invoked in a loop of some sort? Hitting the metadata service multiple times in succession can cause the requests to be throttled.\n. @stemail23 \n\nIf I invoke this code from 10 different node processes simultaneously, then I can pretty much guarantee that the error will be raised\n\nIf you are spawning multiple Node.js processes you are more likely to be throttled by the EC2 metadata service. The SDK itself will cache credentials after the first fetch. require()-ing the SDK multiple times is going to cause credentials to be fetched multiple times - once for each instance of the SDK.\nIt looks like some of the other issues that you are having are related to EC2 instance itself and not the SDK. I would recommend opening up an issue on the Amazon EC2 Forum.\nIn the meantime, we can definitely look at adding retries and exponential back-off to the EC2 metadata service requests.\n. @davidporter-id-au @stemail23 @seriousben \nYou can try increasing the timeout of the AWS.EC2MetadataCredentials provider by setting the httpOptions.timout options. This defaults to 1000 ms.\njavascript\nvar AWS = require('aws-sdk');\nAWS.config.credentials = new AWS.EC2MetadataCredentials({\n  httpOptions: { timeout: 4000 }\n});\nThis should help alleviate some of the issues with a slow responding metadata service.\n. The default retry count for all service clients is 3. You can increase the default retry count by setting the maxRetries option when constructing a service client.\nFor example:\njavascript\nvar s3 = new AWS.S3({maxRetries: 10});\nThe delay between retries is a calculated number and increases exponentially with the number of retries per this formula \ndelayn delay = 2n * 30 milliseconds\nFor example:\n- delay0 delay = 20 * 30 = 30 ms\n- delay1 delay = 21 * 30 = 60 ms\n- delay2 delay = 22 * 30 = 120 ms\nHope this helps clarify things.\n. @isotnikov I am not able to reproduce this with two instances that I've set up on a personal account. \nCould you also post the full code that you are using? I see that the response data you are logging differs from the response structure of the AWS.EC2.describeInstances() API operation.\n. @isotnikov I'm marking this as closed as the issue cannot be reproduced and there hasn't been any recent activity on this thread. Feel free to reopen this issue or open another issue if you have other questions.\n. @GoodMirek I've updated the developer guide to include information about the SDK builder. Thanks for reporting!\n. @jbuck Thanks for reporting this! Do you have a minimal reproduction test case that I can use to replication the problem. This is an easy fix, but I'm more curious about the origin of the error.\n. @jbuck I've pushed up a fix that should resolve this issue. Let me know if this works for you. \nFeel free to reopen this issue if you continue to have problems, and thanks for reporting!\n. @greenboxent I am unable to reproduce this issue on a fresh npm install of the SDK.\nsh\n$> npm install aws-sdk@2.1.50\n``` javascript\n// code.js\nvar AWS = require('aws-sdk');\nconsole.log(AWS.VERSION);\n```\nPerhaps you can try reinstalling using npm. Any other information that you can provide in helping reproduce this issue will be helpful.\n. @greenboxent I'm marking this as closed since the issue cannot be reproduced and there hasn't been any recent activity on this issue. Feel free to reopen this issue or open another issue if you have other questions.\n. @rclark I've pushed up a patch that should should fix this for you. Let me know if this works for you. Thanks for reporting! \n. @riseres This change hasn't been released to npm yet. You can use it by installing directly from GitHub\nsh\n$> npm install git://github.com/aws/aws-sdk-js\n. @rclark Thanks for reporting this! This syntactic sugar may be harder to support because the document client works by adding additional handlers to the lower level request. You can however use the following workaround:\n``` javascript\nvar AWS = require('aws-sdk');\nvar dynamodb = new AWS.DynamoDB({region: 'us-west-2'});\nvar dc = new AWS.DynamoDB.DocumentClient({service: dynamodb, params: {TableName: 'foobar'}});\nvar params = {\n  ExpressionAttributeNames: {\n    '#id': 'id'\n  },\n  ExpressionAttributeValues: {\n    ':id': 'foo'\n  },\n  KeyConditionExpression: '#id = :id',\n  Limit: 100\n};\nvar request = dc.query(params);\n(function getOnePage(req) {\n  req\n    .on('error', function(err) { throw err; })\n    .on('success', function(res) {\n      if (res.hasNextPage()) {\n        params.ExclusiveStartKey = res.data.LastEvaluatedKey;\n        return getOnePage(dc.query(params));\n      }\n    })\n    .send();\n})(request);\n```\nLet me know if this works for you. And thanks again for reporting!\n. @rclark \n\nI suspect I would encounter the same problem with request.eachItem and request.eachPage\n\nUnfortunately yes, because these APIs rely on the response.nextPage() API. I'll keep this open while I take a closer look at trying to support these interfaces with the document client.\n. @rclark I've added a patch that should allow you to use the paginator interface directly. \nI've tested it with the following script:\n``` javascript\nvar AWS = require('aws-sdk');\nvar dc = new AWS.DynamoDB.DocumentClient({region: 'us-west-2', params: {TableName: 'foo-bar'}});\nvar params = {\n  ExpressionAttributeNames: {\n    '#id': 'id'\n  },\n  ExpressionAttributeValues: {\n    ':id': 'foo'\n  },\n  KeyConditionExpression: '#id = :id',\n  Limit: 100\n};\nvar request = dc.query(params);\n(function getNextPage(req) {\n  req\n    .on('error', function(err) { throw err; })\n    .on('success', function(res) {\n      if (res.hasNextPage()) return getNextPage(res.nextPage());\n    }).send();\n})(request);\n```\nSimilarly, I've also been able to do:\njavascript\n(function getEachPage(req) {\n  var page = 1;\n  req.eachPage(function(err, data) {\n    if (err) return;\n    console.log('Page: ', pages++, ' Data: ', data);\n  });\n})(request);\nand\njavascript\n(function getEachItem(req) {\n  var items = 1;\n  req.eachItem(function(err, data) {\n    if (err) return;\n    console.log('Item: ', items++, ' Data: ', data);\n  });\n})(request);\nLet me know if you run into any issues when using response.nextPage(), request.eachItem(), and request.eachPage()\nThanks again for reporting!\n. @Aoreias Thanks for reporting this. I'm working on a fix and this should land in master soon.\n. @Aoreias I've pushed up a patch that should fix this issue. Let me know if this works for you. This change will be available in the next release. In the meantime you can install from npm install from GitHub:\nsh\n$> npm install git://github.com/aws/aws-sdk-js\n. @Aoreias It looks like this fix introduces a breaking change to the way the CopySource parameter is supplied.\nYou will have to manually URL encode any keys that have URL unsafe characters as specified in the AWS.S3.copyObject() API documentation.\n. CC @jeskew \n. @CSharpFan Are you running the code in a browser? The second problem you mentioned can be very easily solved by whitelisting the headers in your Amazon S3 bucket's CORS configuration.\nFor example if you are using Foo or foo as a key in your object metadata, the you would have to expose the x-amz-meta-foo header in your S3 CORS configuration like this.\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n    <CORSRule>\n        <ExposeHeader>x-amz-meta-foo</ExposeHeader>\n    </CORSRule>\n</CORSConfiguration>\nThis should allow you to access object metadata with GET/HEAD requests from the browser. \nAs far as validating the case of metadata keys, the SDK does not validate any user content values because this it is difficult to do reliably across the board and does not scale well across services and operations.\n. @CSharpFan \nObject metadata are transported as headers prefixed with x-amz-meta-. This means that metadata keys are lowercased for transport as a header. This is the way that Amazon S3 API for object metadata works. \nAs per the RFC 7230 header field specification, all header field names are case-insensitive. For this reason, I don't believe it is possible to have case-sensitive object metadata keys. The value may be case-sensitive though.\nHope this helps clarify things.\n. @oscar-espada Thanks for reporting. It looks like this change didn't make it into the SDK. I've notified the Amazon CloudSearch team about it. I will keep this issue updated as we get more information. In the meantime, I would recommend opening an issue on the Amazon CloudSearch Forum to let them know about the issue.\n. @kentor \n\nIs there a better way of handling this without making copies of the ArrayBuffer data?\n\nI would recommend checking out the AWS.S3.ManagedUpload abstraction. It performs parallel multipart uploads using Amazon S3's  UploadPart API operation.\n\nwe have to compute the SHA256 of the 5mb chunks in the UI thread which is where we call s3.uploadParts(). This will cause jank no matter what will it not?\n\nUnfortunately, yes. Calculating the SHA256 checksums on a 5MB chunk is computationally intensive. You can try using a Web Worker to compute the SHA256 checksum. This is a little involved because you will have to detach the existing handlers and instead write your own request handler to compute the SHA256 checksum using private APIs which is not recommended. An alternative would be to use the default s3 signature version.\nHope this helps!\n. @calidion This is the way that Amazon S3 returns the etag header. Can you explain a bit more about why you are trying to manipulate this value instead of passing it around as is?\n. @pree011235 I'm marking this as closed as it is not an SDK issue. As @jeskew mentioned, the token parameter is supplied in a confirmation message supplied to the endpoint that has been subscribed to receive SNS notifications.\nFeel free to reopen this issue or open another issue if you have other questions.\n. @tallboy can you share the code you are using?\nI haven't been able to reproduce the issues you are mentioning with the waitFor() method. Here is the code I am using:\n``` javascript\nvar AWS = require('aws-sdk');\nvar transcoder = new AWS.ElasticTranscoder({region: 'us-west-2'});\nvar callback = function(err, data) { if (err) console.log(err); else console.log(data); };\nvar params = {\n  PipelineId: 'foo-bar',\n  Input: {\n    Key: 'file',\n    AspectRatio: 'auto',\n    FrameRate: 'auto',\n    Resolution: 'auto',\n    Container: 'auto',\n    Interlaced: 'auto'\n  },\n  Output: {\n    Key: 'file.mp4',\n    PresetId: '1351620000001-000010'\n  }\n};\ntranscoder.createJob(params, function(err, data) {\n  if (err)\n    console.log(err);\n  else\n    transcoder.waitFor('jobComplete', {Id: data.Job.Id}, callback);\n});\n``\n. @tallboy the reason for this generic error message is because the underlying operation need not return an error. In this casereadJob()operation is not returning an error. ThereadJob()` operation itself is successful. It happens to contain information about an error that occurred with the job that was created.\nThe waiters depend on the state of the resource to decide whether the resource is available or not across all of our services. Mapping the name of an error state to where details of an error are available in the response does not scale well across all the services that we support because this can vary greatly from service to service.\nThe intention of a waiter is to provide information on whether the resource is available or not so that some other subsequent action can be initiated on a eventually consistent resource. If you specifically want information about why a particular resource is not available, I would recommend invoking the underlying API to get that information. It looks like you are already doing this though.\nI hope this helps clarify things.\n. @tallboy \nI'm marking this as closed. Feel free to reopen this issue or open another issue if you have other questions.\n. @jmswhll Thanks for reporting this! I've pushed up a fix that reverts this change. This should land in our next release.\n. Thanks for catching this @prestomation. Merging this in.\n. @benrady Do you have a stack trace that you can provide? This will help debug the issue. Also which version of the SDK are you using?\nFWIW, it is not recommended to explicitly call AWS.CognitoIdentityCredentials.getId(). This is a private API and you shouldn't need to explicitly call getId(). Can you also describe why your use case requires invoking the getId() method? \n. @benrady \nI haven't been able to reproduce the issue with a test setup I have. Is is possible that you have some sort of race condition going on in your code? Do you have multiple blocks of code that construct the credential provider?\nThe Cognito IdentityId is cached in local storage and is available at the AWS.CognitoIdentityCredentials.identityId property after the credentials have been refreshed.\nFor example:\n``` javascript\nAWS.config.update({\n  region: 'us-east-1',\n  credentials: new AWS.CognitoIdentityCredentials({\n    IdentityPoolId: 'foo-bar',\n    Logins: {\n      'accounts.google.com': id_token\n    }\n  })\n})\nAWS.config.credentials.refresh(function() {\n  // IdentityId available now\n  console.log(AWS.config.credentials.identityId);\n  // Write to DynamoDB now\n});\n```\nHope this helps clarify this. \n. :shipit: \n. @misfitdavidl you can check out the API documentation for the AWS.SQS client for information about the operations available as well as examples of how to use them. The documentation applies to both Node.js as well as the browser.\nLet me know if this helps\n. @misfitdavidl \nI'm marking this as closed since there doesn't seem to be anything that needs to be done from the SDK side of things. Feel free to reopen this issue or open another issue if you have other questions.\n. @Lohit9 \nI'm not sure of the internal details of React Native, but the SDK should work similarly to any other npm modules you may use in your React Native applications.\nLet us know if you run into any trouble using the SDK in your React Native applications.\n. @Lohit9 Thanks for the update!\n\nThe AWS node SDK has many core node dependencies, due to which you cannot use it in your React native app. Unless you find a fork of the SDK that doesn't use any core node modules\n\nThis is unavoidable because the core http and https modules are required to make HTTP requests.\nIs there a way you can bundle external dependencies into a React Native app? \nIf so you can use the browser version of the AWS SDK for JavaScript by downloading it here: https://sdk.amazonaws.com/js/aws-sdk-2.2.4.min.js\nYou can also build a custom version of the SDK for the browser here: https://sdk.amazonaws.com/builder/js\nHope this helps!\n. CC @chrisradek @jeskew \n. @rromanchuk The SDK now does support the AWS.DynamoDB.DocumentClient. This solves the problem that you just mentioned. Here is a blog post that introduces the document client: Announcing the Amazon DynamoDB Document Client in the AWS SDK for JavaScript\nHope this helps!\n. @kyleseely \nOn a related note there are many features that the SDK provides as a convenience (generating MD5 checksums, S3 object metadata headers, SSE key checksums and so on). \nEven if you passed in a correct MD5 for a large file, this would need to be ignored because an MD5 checksum is generated for the each of the smaller uploaded chunks.\nSilently ignoring user provided parameters can be very confusing and lead to other quirky side effects. This is the reason the SDK does not ignore any user provided parameters. \nBy default the SDK will compute checksums for the S3 operations that require a content-md5 header to be set. If you want to pass in your own MD5 checksum, you will need to turn off the computeChecksums option in the configuration. This can become much more involved when using AWS.S3.ManagedUpload because you will have to provide the MD5 checksum for each of the generated parts. \nI hope this helps clarify things!\n. @sobytes \n\nI need to take the aws sdk and convert it to a commonJS module\n\nThe SDK is built entirely from CommonJS modules. This is the convention for most (if not all) Node.js modules. It looks like the the tool you are using (titaniumifier) converts any CommonJS module into a Titanium SDK module. As per the titaniumifier wiki this is done using browserify to convert a CommonJS package into a single file dependency.\nThe SDK is fully compatible with browserify, in fact we build our browser distributable with browserify. You can also include the SDK as a dependency of a package that is being built using browserify (see #383).\nThe titaniumfier output bundle that you attached contains an artifact that is identical to our browser distributable that can be found here: http://sdk.amazonaws.com/js/aws-sdk-2.2.15.js\nThat being said, my initial guess is that the JavaScript engine used by Appcelerator may not support certain standard APIs the SDK is using. A full stack trace my reveal more about what is actually failing.\nHope this helps clarify things!\n. @MaxiSantos \nThis is going to be a little involved. To use API Gateway with CloudSearch Domain you will have to use API Gateway as an HTTP Proxy. The endpoint you will be making requests to is the endpoint of you CloudSearch domain (eg. search-foobar.us-west-2.cloudsearch.amazonaws.com). You cannot use API Gateway as an AWS Service Proxy because you will not be able to change the endpoint to which API Gateway makes requests.\nHere are is a more detailed list of steps:\nSetting up the API\n\nCreate a resource and create a GET method on the resource.\nSelect HTTP Proxy as the Integration type\nPut in the endpoint of your CloudSearch Domain in the endpoint field. Note the trailing path and query string. These are required.\n   \nIn the Method Execution pane, choose Method Request, and then choose the arrow next to URL Query String Parameters. This will allow you to add a query string\n   \nGo back to the Method Execution pane and choose Integration Request and add a query string parameter mapping. Note that the query string name here must be q. This is as per the public CloudSearch Search API. This query string is mapped to whatever you specified in the previous screen.\n   \n\nThats all you should need. Hit test and provide a query string parameter value for your query string. \nNote: Don't forget to enable CORS on your API Gateway API.\nAlso your API Gateway API need not necessarily be a GET request. You can make that a POST with a JSON body and tunnel that as a GET request to your CloudSearch Domain endpoint. There's plenty of fancy stuff you can do here.\nMaking requests\nYou really don't need the SDK to make requests to your API Gateway resource endpoint. You can simply use jQuery.ajax() or something similar to make requests to the your endpoint which may look something similar to https://domain.execute-api.us-west-2.amazonaws.com/search. \nI hope this helps explain things up a bit. I'd be happy to answer any other questions.\n. @forestzrd @OllieJennings to create a bucket in a region other that us-east-1, you will need to specify the correct signing region by setting AWS.config.region and set the LocationConstraint parameter to the same region as the region. This is not so much a bug, more a nuance of how the Amazon S3 API works. \n. This edge case should also be unit tested.\ncoffeescript\n    it 'supports sets with falsy values', ->\n      expect(docClient.createSet([0]).type).to.equal('Number')\n      expect(docClient.createSet(['']).type).to.equal('String')\n. @shinzui you can use the SDK builder to only include support for the services that you require. The SDK builder can be found here: https://sdk.amazonaws.com/builder/js/\n. @shinzui This is mostly because of the infrastructure to sign requests in the browser. You could also think about using something like API Gateway or your own proxy to tunnel requests to Amazon S3. You will then be able to use something as simple as jQuery.ajax() to make requests to your proxy which can then use any of the AWS SDKs (depending on your language of choice of course) to upload the object the Amazon S3.\nHope this helps!\n. @yawhide AWS.Request.createReadStream() returns an instance of stream.PassThrough().\nThe returned stream will only emit stream events, i.e: data, error, end and so on. The httpDownloadProgress event is specific to the AWS.Request object and will not be emitted when you return a readable stream. This is mainly to preserve the stream.Readable interface.\nIt is very easy to simulate your own progress event by listening to the data event emitted by the stream. Here is some sample code to do this:\n``` javascript\nvar totalBytes;\nvar bytesLoaded = 0;\nvar s3 = new AWS.S3();\nvar req = s3.getObject(params);\nreq.on('httpHeaders', function(status, headers, response) {\n  totalBytes = parseInt(headers['content-length']);\n});\nvar stream = req.createReadStream();\nstream.on('data', function(chunk) {\n  bytesLoaded += chunk.length;\n  console.log((bytesLoaded / totalBytes) * 100);\n});\nstream.pipe(writeableStream);\n```\nHope this helps!\n. @neeravmehta it looks like you may be using a version of JavaScriptCore that is bundling an unsupported version of WebKit. \nAs @chrisradek mentioned, the linked patch should address any undefined identifiers on a standards compliant JavaScript engine. Even if this patch were to be changed to explicitly check for undefined, there are other issues that you may run into with native JavaScript API support.\nThere have been other issues reported when using using Appcelerator and Appcelerator like environments that use an outdated version of JavaScriptCore. See #784 \n. @neeravmehta\nAWS Kinesis Firehose does have a public REST API that you can use but you may still need the SDK to sign requests (or some other equivalent Signature Version 4 signer). Here is the API documentation for AWS Kinesis Firehose.\nYour other alternative is to try and use Amazon API Gateway to proxy requests to AWS Kinesis Firehose. This becomes trivially simple if you are using unauthenticated roles to put records to Kinesis Firehose.\nLet me know if the helps. Here is a comment that I posted on using API Gateway with CloudSearch - the same concepts apply.\n. @krishnasrinivas looks like this behavior is related to this change: adf583b\nThis has the (potentially desirable) side effect of returning correct response data for the AWS.S3.getBucketLocation() API operation. \njavascript\ns3.getBucketLocation(params, function(err. data) {\n  // use LocationConstraint here\n  console.log(data.LocationConstraint);\n});\nTo answer your question more specifically, the second call is required because error responses are handled differently by the SDK's request handler chain, and correct response data cannot be trivially populated from error responses (this could also lead to strange side effects). So a request that has resulted in an error is retried to return the correct response data that is then usable by any consumer code.\nI hope that helps answer your question. I'm also curious to learn more about your use case? Are you parsing the error response manually? \n. Would you be able to paste the wire trace of your requests (raw XML response)?\nThe data.Location property was added for non-chunked upload responses to provide compatibility with AWS.S3.completeMultipartUpload() response data. See AWS.S3.putObject().\nWe aren't actually altering the data.Location property for chunked uploads - this is being directly parsed from the response of the AWS.S3.completeMultipartUpload() operation.\n. @jakubzitny looks like this landed in v2.6.1 of the SDK.\n. @jakubzitny Here are the relevant PRs: #1123, #1126. The CHANGELOG does seem to have an entry for v2.6.1 and v2.6.0, though I think this may be missing some documentation.\n@chrisradek \nDo you know if there is any public documentation for this update?\n. @nytins \nDo you get another NextMarker back with the response that is not returning results?\n. @dconnolly Is there a reason that we are slicing out the first index here?\n. CC @dconnolly \n. @chrisradek Do you think it might be worthwhile adding a buffer polyfill as well?\n. Curious why we can't just rely on browserify pulling in deps?\n. ",
    "runvnc": "http://docs.aws.amazon.com/nodejs/latest/dg/services.html That page you linked to shows nothing except a single file name.  I am using the previous Node sdk that has a client class.  I need to be able to find that documentation.  Where is it?  Or where is the documentation for the EC2 stuff with Client and Service merged? Like I said that page says nothing.\nI actually figured out the issue I had before, I just wasn't reading the REST API page right, so I translated from that into the Node thing incorrectly.  So I believe that I can figure it out from the REST API documentation.  But it sure would be nice to not have to figure it out.\n. OK, I use Chrome and that was the problem.  Cleared browsing data and it showed up.  Thanks a lot.\n. I assume that I am not using the very latest release.  It does sound like the same issue that is discussed in that thread.  I expect that upgrading to the latest release will fix this.  Thanks!\n. OK so I looked at the AWS Node code and came up with a hack:\naws.NodeHttpClient.sslAgent.options.rejectUnauthorized = false\naws.NodeHttpClient.sslAgent.maxSockets = 500\nNot sure which one of those does it, probably the maxSockets, but when I use that the performance I see in my application is similar to with sslEnabled: false.\n. I tried turning the rejectunauthorized back on since you suggested it.  I think it is a little bit slower now but not nearly as bad as before without the maxSockets.\nDo you think it would make sense to make it so that the https options can be passed through to the ssl agent the way you can with the non-http?\n. That's fine, thanks.  I didn't realize I could pass my own agent in.\n. ",
    "OferE": "Hi,\nThanks for your quick reply!\nI tried to use sslEnabled: false - the issue remains.\nWhat do you mean by \"use your proxy over HTTPS\"? \n(The proxy I use supports https tunnelling - if that is what you mean).\n. Hi,\nThe proxy I use supports both http and https -\n the proxy address's schema is http for both ssl and regular connection.\nAs you can see in my original code - I stated that I want to use ssl using the flag:\n  sslEnabled: true\nand you can see in the print of: this.request.httpRequest \nthat it tries to access aws using port 443 - which is ssl.\nthis is why I think it is a bug.\n. I found some workaround for this bug:\nThe AWS.config.httpOptions.proxy should not be used - it is causing bad signature issues when working with remote proxy.\nInstead, one can set the environment variable https_proxy and work with sslEnabled=true.\n. I work for an enterprise company (HP SW), the proxy is being configured by others.\nI only know how to configure clients to work with it... \nI tried few proxies - they all give the same results.\n. all of our proxies use CONNECT tunneling AFAIK, so probably this explains all.\nI will give the node-tunnel a try.\nthanks!\n. I wrote my own AWS-EC2 REST implementation because of this...\n(not too complicated)\n. I work for a huge enterprise company which allows internet connections through proxies only.\nI couldn't wait for this to get fixed so I wrote the code myself.\nI used request.js for http requests - and configured the proxies with no problem.\nIt took me few hours to make all the API that I need to work...\nI just don't understand the attitude of \"let's wait for node to fix things\" - it is really embarrassing...\n. ",
    "wabmca": "I think proxy setting is still not working. \nI tried export https_proxy='http://USERID:Password@proxy1.wipro.com\"\nand also export proxy='http://USERID:Password@proxy1.wipro.com\" //Using username password as my proxy authenticates.\nSo there is no solution as of now in AWS node.js to do this?\n. ",
    "bindugh": "Hi.. I too have the same proxy issue.. I have tried node_tunnel and httpOptions poxy set - both giving \n{ [NetworkingError: read ECONNRESET]\n  message: 'read ECONNRESET',\n  code: 'NetworkingError',\n  errno: 'ECONNRESET',\nHow to resolve this ??\nmy code :\nvar AWS = require('aws-sdk');\nvar tunnel = require('tunnel');\nvar tunnelingAgent = tunnel.httpOverHttp({\n  proxy: { // Proxy settings\n    host: 'proxyhost',\n    port: 8080, \n    proxyAuth: \"user:pass\",\n}\n});\nAWS.config.httpOptions = { agent: tunnelingAgent };\nAWS.config.update({accessKeyId: 'key', secretAccessKey: 'secret', region: 'region'});\nvar ec2 = new AWS.EC2();\nec2.describeInstances(function(error, data){\n    if(error){\n        console.log(\"\\n===================ERROR================= \\n\")\n       console.log(error);\n     } else {\n        console.log(\"\\n===================DATA================= \\n\")\n       console.log(data);\n     }\n   });\n. ",
    "monken": "I would also like to mention https://www.npmjs.com/package/aws-sdk-proxy which makes this less painful and honors the HTTPS_PROXY environmental variable.. ",
    "lancecarlson": "I just ran into this issue, and this seemed to fix it. Can we make another bump in the version? Seems pretty critical that this behave appropriately.\n. I just ran into this issue, and this seemed to fix it. Can we make another bump in the version? Seems pretty critical that this behave appropriately.\n. Hmm.. why is this closed? I'm having this issue too and it doesn't seem to be resolved\n. Hmm.. why is this closed? I'm having this issue too and it doesn't seem to be resolved\n. Ah nvm.. I added all permissions and it worked. Did not isolate which permission was the culprit however\n. Ah nvm.. I added all permissions and it worked. Did not isolate which permission was the culprit however\n. ",
    "christianbundy": "Just a refactor suggestion! Nothing urgent, just thought I'd throw it on the to-do list.\n. ",
    "etiennea": "+1\n. ",
    "jeskew": "The Mechanical Turk Requester Service (available as AWS.MTurk) is supported in the AWS SDK for JavaScript as of version 2.22.0.. @andrewgoodchild How are you generating the presigned URLs? From your description of the ETags returned by S3, it sounds like you're using PutObject URLs instead of UploadPart URLs.\n. This thread contains some outdated information about the DynamoDB document client, so I'll leave this closed as a duplicate in favor of #1422. Please refer to #1422 for current discussion on this feature request.. This change went out with version 2.2.29, which may not yet be available by default in Lambda.\n. Lambda currently provides version 2.2.12 of the SDK. You can bundle a newer version if you prefer by following the instructions on issue #765.\n. Hi @theplatapi,\nWe have a standalone node module to accomplish this task, but it does not yet have a tagged release in NPM. \n. @theplatapi The standalone module has been published to NPM.\nPlease feel free to reopen this issue (or open an issue on the validator's repository) if you have any questions or concerns.\n. Hi @SwethaMK,\nInvokeAsync will either throw an exception or return an empty response. If you need to access the output of a lambda function, you will need to use Invoke. Additionally, AWS Lambda has deprecated InvokeAsync and suggests you use Invoke instead.. Any reason the middle check isn't verifying that data is an array (as per the comment)?\n. :shipit: \n. Right; the eu-central-1 region requires all signatures to be v4.\n. Hi @pree011235,\nThe token will be provided in a message sent to the endpoint you're subscribing to an SNS channel. The endpoint will receive a POST message with a JSON payload containing, among other fields, a Token parameter and a SubscribeURL parameter. To confirm a subscription, you can perform a GET request on the value provided for SubscribeURL or use the provided Token to call sns.confirmSubscription.\n. @aadamsx You can call listSubscriptions and check to see if your test phone has been successfully subscribed.\n. Hi @aadamsx,\nThe callback passed to sns.ListSubscriptionsByTopic will be called by the operation's response, which has a hasNextPage method and a nextPage method. The latter takes a callback and handles pagination for you. You won't need to keep track of the nextToken.\n``` js\nvar sns = new AWS.SNS({region: 'us-west-2'})\nvar paginate = function (err, data) {\n        if (err) {\n            console.log(err);\n        } else {\n            // current page of data is available as 'data'\n            if (this.hasNextPage()) {\n                this.nextPage(paginate);\n            }\n        }\n    };\nsns.listSubscriptions(paginate);\n``\n. This looks like a good change, but is there any test coverage for a multi-page response? \n. :ship: :it:\n. :shipit: \n. :shipit: \n. :ship:\n. Hi @Ashesh007 -- I work on the AWS SDK for PHP. Could you provide the PHP code that works so we can help you translate it to JavaScript?\n. As @ondruska suspected, the PHP script is using a username and password to send mail over SMTP. TheAWS.SES` constructor you're using in JavaScript is expecting an AWS access key and secret key -- these are not the same as the SMTP credentials that you're successfully using in PHP.\nYou can also use an SMTP library in JavaScript in you'd prefer. I don't have any recommendations, but there seem to be a number of options on NPM.\nHope this helps clarify things. Please feel free to reopen if you have any questions or concerns.\n. The PHP code is connecting to SES over SMTP using SMTP credentials, whereas the JavaScript code is attempting to connect to SES over the HTTP API, which is authenticated with AWS security credentials. \nThese credentials will always be different.\n. Looks good. :shipit: \n. Hi @andreasherzog,\nWhat version of node and of the SDK are you currently using? \n. LGTM. Do error responses always have request IDs?\n. :shipit: \n. Thanks for suggesting this! We're looking into how to support sending signed requests to the Amazon ES search API.\nAre you currently using an Elasticsearch client? If so, which one?\n. I saw this project on the Elasticsearch.js docs. I haven't tested it, but it looks like that library would allow you to use AWS authentication with the official elasticsearch.js client.\n. I'm going to close this issue, as it looks like the http-aws-es project will take care of signing Amazon ES requests.\nPlease feel free to reopen if you have any questions or concerns.\n. LGTM.\n. Multipart upload parts must be at least 5 MB, unfortunately.\n. Hi @gregcope,\nThe timeout option is applied to a connection attempt, not to an entire operation like dynamodb.createTable. Because operations can be retried a number of times with an exponentially increasing delay between attempts, an operation might take significantly longer than its allotted socket timeout to fail. This is especially true of DynamoDB operations, which are retried 11 times. If the connection were actually timing out instead of being refused, the operation could take over a minute to fail.\nI see that you commented out a line limiting the operation to a single retry. That's your best option if you want an operation to fail fast.\n. The retry delay is calculated using this function. You're right that this is where most of the 25 seconds recorded is being spent.\n. Closing for now. Please feel free to reopen if you have any questions or concerns.\n. :shipit:\n. :shipit: \n. I'm not sure this is related to issue 203, as that concerned invoking a callback multiple times in the case of a network failure. \nAre you calling headObjects or headObject? The former is not a supported operation.\n. Thanks! What does the CORS Configuration on your bucket look like?\n. Can you try adding <AllowedMethod>HEAD</AllowedMethod> to your CORS configuration?\n. You'll need to expose additional headers to get that information. It looks like you're only exposing the ETag header in the CORS configuration above; you can add ExposeHeader nodes for 'Content-Length' and other common response headers if you'd like for those to be visible in a browser.\nHope that helps!\n. Closing, as it sounds like you should be all set now. Please feel free to reopen if you have any further questions.\n. Fixed in #1550. LGTM. :shipit: \n. :shipit: \n. Hi @nowakj,\nHave you tried using our script package for the browser? You can find more on how to use it in our user guide.\n. There was some discussion on how to use the SDK with webpack in #603. A few solutions were proposed, but I'm not sure if they'd work for everyone. Webpack support seems pretty popular so I'm going to mark this ticket as a feature request and keep it open for now.\n. Support for webpack was added in version 2.6.0. :shipit: \n. A note was added in https://github.com/aws/aws-sdk-js/commit/9ddc7fb192f7303ff80cc26db84c8ca4e324f416 spelling out which ReturnValues value is accepted by both PutItem and DeleteItem.\n@alayor When making a PutItem request, you're overwriting the entire item, so if the request succeeds then the new values are the ones provided as input parameters. If you're updating an item in place with an UpdateItem operation, you can use ALL_NEW to get the updated value.. Hi @leore,\nThis PR isn't ready to be merged in at the moment, both because the last commit had some failing tests and because it doesn't seem possible to use it in a browser. I see that files over one megabyte are decrypted into a temp file, and I'm not sure what the performance characteristics of doing that through filesystem-browserify would be.\n. :shipit: \n. Thanks for looking into this, @dschenkelman. Do you know if the leak just affects Kinesis uploads or if requests to any other service caused a similar leak?\n. Thanks, @sabrinaluo! That does indeed look like a vestigial if.\n. Thanks for reporting this. It looks like the entire module is deprecated as of Node 4, so we should look into how to port domain-based code to a current version of node. This would require modifying lib/request.js as well as the tests for request and event_listener.\nAs far as removing the call to dispose goes, the method doesn't appear to do much in Node >= 0.12, but it might have desirable side effects in Node 0.10.x, which we do support. @chrisradek -- What would you think about removing this?\n. ooooh shiny :shipit: \n. The NodeJS commit referenced in https://github.com/aws/aws-sdk-js/issues/862#issuecomment-178333617 has been present in all releases since 6.0.0, and DynamoDB has updated their servers to use TLS 1.2 everywhere, so I don't believe customers are continuing to see this issue.. @neeravmehta Is the application running on Appcelerator?\n. HI @jharajeev,\nThe flow framework for Java is part of the AWS SDK for Java, not the AWS SDK for JavaScript. If you open this issue on https://github.com/aws/aws-sdk-java, someone will be able to help you out.\n. @Qard Your credentials are used by DynamoDB local to maintain separate databases for different users. Your credentials won't be checked, but those configuration parameters do serve a purpose.\n. If you scroll up on the page you've linked to, you'll see documentation on the --sharedDb option. If not set, your database will be saved in a file named {myaccesskeyid}_{region}.db. \nI'll let the doc author know that \"Regions and distinct AWS accounts are not supported at the client level\" is confusingly worded.\n. @Alex0007 I'm unable to reproduce the behavior you're experiencing. Have you verified that process.env.AWS_DYNAMODB_ENDPOINT is properly set?\n. Glad to hear everything worked out. Let me know if you have any further questions.\n. ~~The CopySource parameter should be passed in as '/' + bucket + '/' + key. I'll update the documentation for that parameter.~~\nOn further investigation, S3 will accept either form.\n. On further investigation, S3 will accept either form. This sounds like a bug in s3rver, which should probably be normalizing CopySource paths. In the meantime, you can use the '/' + bucket + '/' + key form, which will be accepted by both S3 and s3rver.\n. Clients are free to include or not include an initial slash; S3 will normalize x-amz-copy-source paths server side. In that sense, s3fs is not wrong to omit the slash at the beginning of the path. This is what the AWS SDK for Ruby does, too.\ns3rver should be normalizing paths on incoming requests if their goal is 100% compatibility with the S3 API. You can also always use paths that begin with a slash if you are writing a library that you wish to be compatible with S3 and any already-released versions of s3rver.\n. @ffxsam You might want to try the browser version if you plan on using the SDK in a Cordova app.\n. :shipit: \n. I was checking in on our outstanding documentation issues and was unable to reproduce the behavior described in the ticket description. The object returned from the SDK has a Date object for the LastModified property and a number for the ContentLength property.. @urbien I believe the lambda-based approach may be cheaper in terms of both cost and latency. According to the blog post linked to by the OP, writing to two tables in one transaction would require 18 sequential writes:\n\nTransactional puts are more expensive (in terms of read and write capacity units). A put that does not contend with any other simultaneous puts can be expected to perform 7N + 4 writes as the original operation, where N is the number of requests in the transaction.. @IsaiahJTurner Adding a separate getSignedUrlPromise operation makes sense, though it should not be present on the S3 service client by default. The SDK supports runtime environments that do not support native promises, so the getSignedUrlPromise method should be added in a static addPromisesToClass method on the AWS.S3 constructor (similar to how getPromise and resolvePromise are added to AWS.Credentials).. @niftylettuce I believe you're rethrowing the error inside of an async function. SDK constructors do not use promises in any way. Let's keep the discussion to #1650 . @clakech If you're doing a multipart upload of a client-side encrypted ciphertext, S3 will treat the parts it receives as opaque byte streams and concatenate them. As far as S3 is concerned, the bytes are not altered or inspected. When you download the object, you would get back the ciphertext as it was uploaded. \n\nIn the scenario described above, you would need to proxy the download through a server that could decrypt the ciphertext and return the plaintext to a browser. Please feel free to reopen if you have any questions or concerns.. #1391 was meant to automatically close #1039, not this issue. . I think if this were to be added to the SDK, credentials would need to be stored in session storage rather than local storage due to the sensitive nature of AWS credentials.\n@lrettig I'm not sure there's a reasonable workaround. You could manually save the accessKeyId, secretAccessKey, sessionToken, and expireTime properties from an instance of AWS.CognitoIdentityCredentials in an object in session storage, but I would need to do more research on how session storage is implemented in the browsers the SDK supports before I could say if it's a recommendable to store credentials. . Hi @andyfen,\nThis isn't the best forum for more general questions about how to use individual AWS services. If you're still blocked on this issue, have you tried posting your question to StackOverflow or the Amazon DynamoDB forum? Dynamo engineers and power users monitor those channels for questions and will hopefully be able to give you an answer.. I set up a test environment with Node v0.8.28 and NPM 1.4.29 and was able to execute the following commands without error both before and after applying the update in this diff:\n\na clean npm install --production\nnode node_modules/.bin/mocha -- test/protocol/ test/query/ test/json/ test/xml/ test/model/\nnode node_modules/.bin/cucumber.js --tags ~@file --tags ~@streaming (after tagging some tests of AWS.S3.ManagedUpload with @streaming)\n\nTL;DR: I think this PR is safe to merge in without affecting our Node 0.8 compatibility.\nHowever, I would like to understand the underlying issue a bit better: does having two versions of lodash installed cause issues because both attempt to bind to the _ variable in global scope, is browserify and/or webpack unable to reconcile two versions of lodash, or is the concern just about bundle size? @prabhatsharma @shyamchandranmec @gurpreetatwal any more information on that would be helpful.. I set up a test environment with Node v0.8.28 and NPM 1.4.29 and was able to execute the following commands without error both before and after applying the update in this diff:\n\na clean npm install --production\nnode node_modules/.bin/mocha -- test/protocol/ test/query/ test/json/ test/xml/ test/model/\nnode node_modules/.bin/cucumber.js --tags ~@file --tags ~@streaming (after tagging some tests of AWS.S3.ManagedUpload with @streaming)\n\nTL;DR: I think this PR is safe to merge in without affecting our Node 0.8 compatibility.\nHowever, I would like to understand the underlying issue a bit better: does having two versions of lodash installed cause issues because both attempt to bind to the _ variable in global scope, is browserify and/or webpack unable to reconcile two versions of lodash, or is the concern just about bundle size? @prabhatsharma @shyamchandranmec @gurpreetatwal any more information on that would be helpful.. Closing due to inactivity. If checking the keys array as described in the previous comment does not resolve the original problem faced, please feel free to reopen this issue.. Closing due to inactivity. If checking the keys array as described in the previous comment does not resolve the original problem faced, please feel free to reopen this issue.. Closing due to inactivity. If @chrisradek's advice in the previous comment does not solve the problem encountered, please feel free to reopen this issue.. Closing due to inactivity. If @chrisradek's advice in the previous comment does not solve the problem encountered, please feel free to reopen this issue.. Closing, as @LiuJoyceC's comment explains why refreshing is handled in service.config.getCredentials() rather than directly in the v4 signer. The signer itself is marked private and not designed to be used on its own.. Closing, as @LiuJoyceC's comment explains why refreshing is handled in service.config.getCredentials() rather than directly in the v4 signer. The signer itself is marked private and not designed to be used on its own.. Fixed in #1418.. Fixed in #1418.. @essapalaxo Since Riak supplies a self-hosted storage product described as S3-compatible, any incompatibilities between Riak siblings and S3 object versions should be addressed in that compatibility layer rather than in the AWS SDK for JavaScript. Please feel free to reopen if you have any questions or concerns.. @essapalaxo Since Riak supplies a self-hosted storage product described as S3-compatible, any incompatibilities between Riak siblings and S3 object versions should be addressed in that compatibility layer rather than in the AWS SDK for JavaScript. Please feel free to reopen if you have any questions or concerns.. Thanks for your help in tracking down the right dependencies to include, @gurpreetatwal!. Thanks for your help in tracking down the right dependencies to include, @gurpreetatwal!. Hi @jans510,\nIt doesn't look like the python SDK supports the AWS_CREDENTIAL_PROFILES_FILE environment variable (it uses AWS_CREDENTIAL_FILE instead, and this feature is maintained for compatibility with some older EC2 CLI tools). Could you tell me a bit more about your use case? I think it would be a breaking change if an environment variable that used to only affect Java applications were to suddenly be picked up by any new JavaScript applications, too, but I'd like to help you find a workaround.. An export of AWSError was added to lib/core.d.ts in #1355, which has been merged to master and will be included in the next release.. An export of AWSError was added to lib/core.d.ts in #1355, which has been merged to master and will be included in the next release.. We don't have any immediate plans to add support for Amazon Mechanical Turk to the AWS SDK for JavaScript, but I'll mark this as a feature request. In the meantime, there are a few community-supported modules for using mechanical turk from a node application available on NPM.. We don't have any immediate plans to add support for Amazon Mechanical Turk to the AWS SDK for JavaScript, but I'll mark this as a feature request. In the meantime, there are a few community-supported modules for using mechanical turk from a node application available on NPM.. Closing as a duplicate of #111.. Closing as a duplicate of #111.. Resolved by #1662.. Resolved by #1662.. Closing as a duplicate of #281.. Closing as a duplicate of #281.. It looks you're using the dynamo-doc package, which was merged into the SDK in 2015. Have you tried using the SDK's bundled document client?. It looks you're using the dynamo-doc package, which was merged into the SDK in 2015. Have you tried using the SDK's bundled document client?. Closing due to age. Using the SDK's bundled document client rather than the dynamo-doc package should take care of the issue as reported.. Closing due to age. Using the SDK's bundled document client rather than the dynamo-doc package should take care of the issue as reported.. Since the same URL works on Postman on Ubuntu but not Postman on OS X, I don't believe the issue is related to how the pre-signed URL is being built. The original error reported would appear when S3 received a Transfer-Encoding: chunked header, which Postman may be setting behind the scenes. Does Postman let you see the response exactly as it was sent over the wire?. Since the same URL works on Postman on Ubuntu but not Postman on OS X, I don't believe the issue is related to how the pre-signed URL is being built. The original error reported would appear when S3 received a Transfer-Encoding: chunked header, which Postman may be setting behind the scenes. Does Postman let you see the response exactly as it was sent over the wire?. @stevenelson74708 It sounds like the problem is not with the presigned URLs themselves, but rather with Postman. I'm going to go ahead and close this out, as it's not an SDK issue, but please feel free to reopen if you discover that this is not the case.. @stevenelson74708 It sounds like the problem is not with the presigned URLs themselves, but rather with Postman. I'm going to go ahead and close this out, as it's not an SDK issue, but please feel free to reopen if you discover that this is not the case.. When running in a single process, the call to AWS.config.credentials.get will not overwrite the identityId, as that is only done as part of the refresh process. In the scenario you describe, the network failure encountered is expected, and credentials.get should not be able to source any credentials. \nAre you running into the issue described in the linked amazon-cognito-js issue? I think the solution might be to add an accessor for the identityId property that reads from cache and to mark the property itself as private.. When running in a single process, the call to AWS.config.credentials.get will not overwrite the identityId, as that is only done as part of the refresh process. In the scenario you describe, the network failure encountered is expected, and credentials.get should not be able to source any credentials. \nAre you running into the issue described in the linked amazon-cognito-js issue? I think the solution might be to add an accessor for the identityId property that reads from cache and to mark the property itself as private.. After removing the listener that throws the ContentLength is not supported in pre-signed URLs. and creating a pre-signed URL with a set ContentLength parameter, I observed that the URL could be used to put objects larger than the specified ContentLength without any error being thrown. I believe the listener should be kept, as content lengths specified in pre-signed URLs do not appear to be enforced.\nThe Go SDK's pre-signer works a bit differently from the JavaScript SDK's, which is how it is able to include content-length in the signature. Rather than converting headers to query string parameters and returning a pre-signed URL as a string, the Go SDK takes a pointer to an http.Request struct, modifies it in place with the appropriate signature, and returns a map of the headers with which the request was signed. Restricting ContentLength via presigning only appears to be enforced when the length is sent to S3 as a header and not as part of the query string.. After removing the listener that throws the ContentLength is not supported in pre-signed URLs. and creating a pre-signed URL with a set ContentLength parameter, I observed that the URL could be used to put objects larger than the specified ContentLength without any error being thrown. I believe the listener should be kept, as content lengths specified in pre-signed URLs do not appear to be enforced.\nThe Go SDK's pre-signer works a bit differently from the JavaScript SDK's, which is how it is able to include content-length in the signature. Rather than converting headers to query string parameters and returning a pre-signed URL as a string, the Go SDK takes a pointer to an http.Request struct, modifies it in place with the appropriate signature, and returns a map of the headers with which the request was signed. Restricting ContentLength via presigning only appears to be enforced when the length is sent to S3 as a header and not as part of the query string.. It would be possible to add a new operation presigner, but it would need to be a separate method with a different return type. Out of curiosity, would you want to use this pre-signer in node or a browser environment?. It would be possible to add a new operation presigner, but it would need to be a separate method with a different return type. Out of curiosity, would you want to use this pre-signer in node or a browser environment?. @rhclayto The best way to accomplish what you want is to use a signed policy to do a form-based POST upload. That will allow you to specify an acceptable range rather than a specific number of bytes.\nMy understanding of the Go SDK's presigner is that it will give you a presigned request (i.e., an object with a host, path, scheme, and headers) instead of a presigned URL (which cannot include headers). The JS SDK can add a feature that creates a presigned request with the appropriate headers, but you still would not be able to use a presigned URL for this purpose.. @rhclayto The best way to accomplish what you want is to use a signed policy to do a form-based POST upload. That will allow you to specify an acceptable range rather than a specific number of bytes.\nMy understanding of the Go SDK's presigner is that it will give you a presigned request (i.e., an object with a host, path, scheme, and headers) instead of a presigned URL (which cannot include headers). The JS SDK can add a feature that creates a presigned request with the appropriate headers, but you still would not be able to use a presigned URL for this purpose.. This was addressed in #1662.. This was addressed in #1662.. You can direct the SDK to use a custom HTTP(S) agent by providing it as the httpsOptions.agent configuration option.\nAn example of how to customize the agent can be found in our developer guide. To disable certificate verification, you would want to set rejectUnauthorized to false. Alternatively, you could use the ca option to provide the self-signed certificate used in your development environment.. You can direct the SDK to use a custom HTTP(S) agent by providing it as the httpsOptions.agent configuration option.\nAn example of how to customize the agent can be found in our developer guide. To disable certificate verification, you would want to set rejectUnauthorized to false. Alternatively, you could use the ca option to provide the self-signed certificate used in your development environment.. Hi @ox42,\nContentMD5 isn't stored as metadata on the object in S3. It's used by Amazon S3 on individual HTTP requests to ensure that the request body was not corrupted in transit: when S3 receives a request with a Content-Md5 header, it will hash the request body and compare the two values. The SDK will compute this value for you by default, so it's not something you would need to specify when upload an object to S3. Unless you explicitly set the computeChecksums config property to false, an integrity check will be performed on each part of the large file that is uploaded.. Hi @ox42,\nContentMD5 isn't stored as metadata on the object in S3. It's used by Amazon S3 on individual HTTP requests to ensure that the request body was not corrupted in transit: when S3 receives a request with a Content-Md5 header, it will hash the request body and compare the two values. The SDK will compute this value for you by default, so it's not something you would need to specify when upload an object to S3. Unless you explicitly set the computeChecksums config property to false, an integrity check will be performed on each part of the large file that is uploaded.. The SDK will only retry requests that fail with a status code of 429 or 500 or greater. If only some writes in a batch failed, the request is considered by the service to have succeeded and is therefore not retried.\nRetrying items in the UnprocessedItems list in the response would need to be handled in the code that is calling batchWrite.. The SDK will only retry requests that fail with a status code of 429 or 500 or greater. If only some writes in a batch failed, the request is considered by the service to have succeeded and is therefore not retried.\nRetrying items in the UnprocessedItems list in the response would need to be handled in the code that is calling batchWrite.. Hi @dinvlad,\nIn CloudFront's API documentation for both updateCloudFrontOriginAccessIdentity and deleteCloudFrontOriginAccessIdentity, the IfMatch parameter is listed as optional. The error message you received does seem to indicate that IfMatch must always be specified, but that may not be true in all cases. You might want to try reporting this in the CloudFront AWS forum if you believe that the IfMatch parameter has been mislabeled.. Hi @dinvlad,\nIn CloudFront's API documentation for both updateCloudFrontOriginAccessIdentity and deleteCloudFrontOriginAccessIdentity, the IfMatch parameter is listed as optional. The error message you received does seem to indicate that IfMatch must always be specified, but that may not be true in all cases. You might want to try reporting this in the CloudFront AWS forum if you believe that the IfMatch parameter has been mislabeled.. Did you see a similar issue using the AWS SDK for JavaScript, or was the error only encountered when performing these operations in the AWS console?. Did you see a similar issue using the AWS SDK for JavaScript, or was the error only encountered when performing these operations in the AWS console?. When you sign into the console, there's a link to submit feedback on the landing page. Since this isn't an issue with the SDK, going through AWS support or console feedback will be a better way to get the issue you encountered addressed.\n\n. When you sign into the console, there's a link to submit feedback on the landing page. Since this isn't an issue with the SDK, going through AWS support or console feedback will be a better way to get the issue you encountered addressed.\n\n. In that case, the best way to contact the service team directly is through their AWS customer forum, which is monitored and tracked by EC2 engineers. Unfortunately, individual AWS services do not monitor GitHub issues on any of the AWS SDKs, so they would not be able to respond here.\nIf you ask a question in the forum, please post a link here and I'll make sure to notify EC2 internally as well.. In that case, the best way to contact the service team directly is through their AWS customer forum, which is monitored and tracked by EC2 engineers. Unfortunately, individual AWS services do not monitor GitHub issues on any of the AWS SDKs, so they would not be able to respond here.\nIf you ask a question in the forum, please post a link here and I'll make sure to notify EC2 internally as well.. Thanks for opening communication with the EC2 team on the forum. I sent them an email as well providing a link to this issue as context.. Thanks for opening communication with the EC2 team on the forum. I sent them an email as well providing a link to this issue as context.. \ud83d\udc11 :it: . Hi @devhyunjae,\nCould you tell me a bit more about what kind of support would you be looking for in JavaScript? Athena is meant to be accessed via a database client, so, like with RDS, the AWS SDK does not provide a means to query the data plane of the service. . You could try one of the JDBC wrappers available on npm. Those would allow you to send SQL queries to Athena.. Support for querying over an API was added in version 2.54.0. @rebeccacaroline There's API documentation for the JavaScript client accessible here, but you might also want to take a look at this blog post on how to write Athena SQL queries.. The branch is public on my fork of this repo. (If you're using webpack, you can install a specific branch from an arbitrary fork per the instructions here).. Hi @cmawhorter,\nI understand your frustration, but deprecating the global configuration object and requiring users to instantiate AWS objects would be a pretty big breaking change. It would also be a breaking change to allow AWS.config.update to modify already instantiated service objects (as documented in our developer guide) What you can do, on the other hand, is to instantiate individual service objects with their configuration passed in as an argument rather than using AWS.config. For example, your second code snippet could be rewritten to explicitly specify the desired region on each instance of AWS.S3:\njavascript\nvar AWS = require('aws-sdk');\nvar s3_east = new AWS.S3({region: 'us-east-1'});\ns3_west = new AWS.S3({region: 'us-west-2'});\nAny option that can be set via AWS.config.update can instead be passed in as a constructor option to a service object, and that pattern might suit your needs better than AWS.config.update.. Getting rid of AWS.config.update isn't something we could do without a major version bump, but I'll mark this issue as a feature request to make sure we keep this feedback in mind when we do get an opportunity to make breaking changes.\n\nAlso -- is it possible a node dependency is using a diff version of aws and the two different versions are somehow sharing state or something?\n\nWhen loaded in node, the SDK exports an AWS object, so each installed version of the SDK would maintain its own state. The default credential loader, however, relies on process and machine state to determine which credentials to use to sign a request: it checks environment variables, which would be shared by all installed versions of the SDK running in the same shell or process; it then checks for file-based credentials in a known location (~/.aws/credentials), which would be shared by all shells and processes run as a given user; and it then checks for instance metadata credentials, which would be shared by all users on a given EC2 instance or ECS container.. @aniltallam Have you tried using the createReadStream method on the request to create a readable stream rather than attaching a listener to the 'response' event?. Thanks for clarifying! There's some discussion in https://github.com/aws/aws-sdk-js/issues/686 about request binding to the 'data' event and thereby changing the underlying stream from paused mode to flowing mode, which means that there could be data lost between when the request data starts arriving and when the SDK begins uploading to S3. I'm closing this issue as a duplicate of the original one, but please feel free to renew the discussion on that issue.\nIn the meantime, have you tried using request's .pipe() method instead? They have an example of how to do so in their README.. Hi @kaihendry,\nThere was a bug in how convertEmptyValues operated on members of maps and lists that was fixed in 2.7.21, which was just released. Could you upgrade and see if that fixes the issue for you?. The link you posted to the API reference for DownloadCompleteDBLogFile now redirects to the RDS user guide, so I guess they really want customers to use DownloadDBLogFilePortion :)\nYou're correct about needing to set Marker to '0': omitting both NumberOfLines and Marker will cause the most recent lines (up to 10K) to be downloaded, whereas setting Marker to '0' instructs RDS to read from the head of the file rather than the tail. (This is discussed in the NumberOfLines section of the REST API reference for the operation).\nPlease feel free to reopen if you have any questions or concerns.. Hi @mattbailey,\nI understand your concern, but the SDK does not currently read any values from the ~/.aws/config file. It will, however, assume roles specified using a role_arn provided in the ~/.aws/credentials file. It would arguably be dangerous to start reading from the config file now, as customers might now be relying on that file only being used by the AWS CLI. If we do add support for autoloading profile data from ~/.aws/config, it would need to be opt in.. I reverted the change that addressed this, as it does not appear that sending the body as a buffer has the intended effect in all clients. According to S3's documentation, metadata keys and values should only use characters from the US-ASCII character set. If S3 receives UTF-8 characters, it re-encode the header in RFC 2047 MIME encoding after verifying the signature. S3 will not decode the metadata when the object is retrieved, meaning that clients will receive the value not as multibyte char \u00e7 but instead as =?UTF-8?Q?multibyte_char_=C3=A7?=, which they may or may not decode automatically.\nThis behavior seems not to occur when the body is sent in the same TCP packet as the headers, which is why you see a difference between sending strings (which node attempts to concatenate with the headers) and buffers (which node sends separately). In either case, it's probably not what you want to happen, so this should be handled by URL-encoding metadata values before passing them to the SDK.. Hi @loopsysdev,\nJust to make sure I understand correctly, the PutRecordBatch did not encounter an error but also did not return the appropriate output data? The SDK does not validate the responses returned from services, so we would not convert an empty response to an error, but this is likely something the service team will need to look into. Do you have any additional information logged about the failed batches that could assist the service team in investigating the issue?. You will need to URL encode the CopySource parameter before passing it as an argument to the s3.copyObject. Having the SDK handle this for you would be ideal but would unfortunately be a breaking change for anyone currently URL encoding parameters. Please see https://github.com/aws/aws-sdk-js/issues/712 for further discussion and a workaround.. That's fair. I'll leave this open as a feature request and tag it as needing a version bump.. Hi @wahengchang,\nI see that you're setting the VisibilityTimeout to 0 in the code snippet above. Is that the timeout you want to change?. Closing due to age. \nI believe the timeout you're asking about is the VisibilityTimeout passed to sqs.receiveMessage. It represents the number of seconds for which the message should be considered \"in flight\" and is set to 0 in the snippet above. You can change that to a different integer as necessary. . Dynamo errors are supposed to be JSON encoded, so I've reached out to that team to ask what might be causing the underlying issue. I will update this ticket when I hear back from them.. @dsouzamanish \nAre you using any other SDK services in the same process as the one querying Dynamo?. @dsouzamanish \nOne more question: are you setting a custom endpoint for S3 using AWS.config.update?. @dsouzamanish Are you also calling DynamoDB in the callback passed to an S3 client operation?. @dsouzamanish I was only able to reproduce by calling AWS.config.update({endpoint: s3Endpoint}), which sets the endpoint for all services to be the one you intended to use for S3. If you're certain that's not happening in your code, could you provide a minimal reproduction case or some more details about how you're setting your configuration?. @alexanimal I believe the error you're seeing is related to #1379, not to this ticket.. Hi @GingerAebi,\nThis was a regression in 2.7.23 that was addressed in 2.7.24. Could you try updating and see if that resolves the problem?. Glad to hear the upgrade fixed the problem. Please feel free to reopen this issue if any you see the error again.. If reverting to an older version of the SDK did not correct the unexpected behavior, then I believe you are right that the behavior originates from the service. I was unable to reproduce what you've described (sts.assumeRole is still returning an object with the AssumedRoleUser, Credentials, and ResponseMetadata properties), but you could try directing your question to the service team via the AWS forums.. I believe this was related to #1303, which seems to prevent node's HTTP client from optimizing how a request is translated into packets. That change will be reverted in the next release.. @thenovelnomad Could you try with version 2.7.27?. Glad to hear it! Thanks for running those tests.. Hi @sumit08,\nHave you been able to determine if the latency you mention is in the SDK or in Lambda?. Closing due to inactivity. I believe that you're experiencing the difference between requests that reuse an existing container and those that provision a container, but please feel free to reopen if you determine that the latency is occurring in the SDK rather than in Lambda execution time.. @tysonhummel The SDK requires the @types/node package be installed to use the TypeScript definitions (cf https://github.com/aws/aws-sdk-js#pre-requisites ).. @tysonhummel  I believe the error is due to the @types/node package being installed as a devDependency rather than as a runtime dependency. The instructions in the SDK's readme recommend that you run npm install --save-dev @types/node, as the types are typically not required at runtime but are required by the TypeScript compiler. \nThe error message indicates that Heroku is running npm install --production on deployed code:\n\nremote: -----> Build failed\nremote:      \nremote:        We're sorry this build is failing! You can troubleshoot common issues here:\nremote:        https://devcenter.heroku.com/articles/troubleshooting-node-deploys\nremote:      \nremote:        Some possible problems:\nremote:      \nremote:        - A module may be missing from 'dependencies' in package.json\nremote:        https://devcenter.heroku.com/articles/troubleshooting-node-deploys#ensure-you-aren-t-relying-on-untracked-dependencies\nremote:      \nremote:        - This module may be specified in 'devDependencies' instead of 'dependencies'\nremote:        https://devcenter.heroku.com/articles/nodejs-support#devdependencies\nremote:      \nremote:        Love,\nremote:        Heroku\nremote:        \n\nIf you move @types/node from the \"devDependencies\" section of your package.json file to its \"dependencies\" section, Angular AOT compilation should work.. Wouldn't this mean that pulling in a dependency could potentially alter the credential loading behavior of your application? I think using any global variable for this purpose would be dangerous.\nIf you would prefer to be explicit about where clients are loading credentials, wouldn't it be easiest to just pass your own credential provider chain to the client constructor?. #1342 has been merged in. Thanks for your help on this, @TheLarkInn !. @Swizec Are you still interested in expanding this PR with tests as discussed in https://github.com/aws/aws-sdk-js/issues/1341#issuecomment-277340517 ? If not, @chrisradek or I could pick this up and add some tests.. Thanks!. I think changing the name of that exported type would be a breaking change (anyone who was importing it would encounter a compilation error).\nWhich IDE are you using? Perhaps there's a way to tell it not to prompt an import for types that are already defined in the global namespace.. Closing, since there's not really anything the SDK can do short of changing the name of the exported type. If you do hear back from Jetbrains with tips on how to disable the prompts for importing Partial, please let us know!. :shipit: . I spoke with a member of the CLI team and was told that if there is a role_arn present in both ~/.aws/credentials and ~/.aws/config, the ARN present in ~/.aws/credentials will be used. To maintain consistency, this SDK would use the same preference order.\nHowever, this SDK does not currently support reading configuration from the ~/.aws/config file, though there is an open feature request (#1296) to add such support. I'm closing this as a duplicate of that request so that discussion of this feature can be kept in one place. Please feel free to comment there or +1 that issue.. @vyas07 stream.setTimeout is part of Node's HTTP module and would only be available when running in a Node environment. Do you see this error in Node or in a browser environment?. @vyas07 stream.setTimeout is part of Node's HTTP module and would only be available when running in a Node environment. Do you see this error in Node or in a browser environment?. Could you try using the browser version of the SDK? You can include it in a webpage with a script tag pointed at our hosted copy (<script src=\"https://sdk.amazonaws.com/js/aws-sdk-2.16.0.min.js\"></script>) or download a copy here.. Could you try using the browser version of the SDK? You can include it in a webpage with a script tag pointed at our hosted copy (<script src=\"https://sdk.amazonaws.com/js/aws-sdk-2.16.0.min.js\"></script>) or download a copy here.. @vyas07 It's possible that the bucket you're accessing does not have the correct CORS configuration. The object's version ID is returned as a header, so it must be explicitly exposed in the bucket CORS policy in order to be visible in a browser. Please take a look at the section on CORS in the S3 developer guide.\nJust FYI: StackOverflow is a better venue for usage questions, as you can get quick answers from a large community of developers. If you have a question you would like to get in front of S3 engineers, the S3 developer forum is the best way to do so. As mentioned in the readme, we like to reserve the GitHub issues on this repository for bug reports and feature requests.. @vyas07 It's possible that the bucket you're accessing does not have the correct CORS configuration. The object's version ID is returned as a header, so it must be explicitly exposed in the bucket CORS policy in order to be visible in a browser. Please take a look at the section on CORS in the S3 developer guide.\nJust FYI: StackOverflow is a better venue for usage questions, as you can get quick answers from a large community of developers. If you have a question you would like to get in front of S3 engineers, the S3 developer forum is the best way to do so. As mentioned in the readme, we like to reserve the GitHub issues on this repository for bug reports and feature requests.. @chrisradek Apologies for the rebase; I wanted to grab the browser testing changes from master. The first two commits are unchanged from when you reviewed them before.. @chrisradek Apologies for the rebase; I wanted to grab the browser testing changes from master. The first two commits are unchanged from when you reviewed them before.. @chrisradek I moved the type definitions around to support access via S3.PresignedPost and S3.Types.PresignedPost per @RLovelett's feedback. This is my first time contributing to the generated TS definitions and I'm not sure if everything is correct.. @chrisradek I moved the type definitions around to support access via S3.PresignedPost and S3.Types.PresignedPost per @RLovelett's feedback. This is my first time contributing to the generated TS definitions and I'm not sure if everything is correct.. Hi @BardiaAfshin,\nUnfortunately, Redshift doesn't expose any data-plane functionality through their API but instead is accessed like a traditional database using tools like ODBC. Lambda supports delivering sensitive data to functions via encrypted environment variables, so that might be a way to get the DB credentials to your function.\nYou can communicate this feature request directly to the Redshift team via their AWS forum, and they might have a few ideas to support your use case beyond Lambda environment variables.. Hi @BardiaAfshin,\nUnfortunately, Redshift doesn't expose any data-plane functionality through their API but instead is accessed like a traditional database using tools like ODBC. Lambda supports delivering sensitive data to functions via encrypted environment variables, so that might be a way to get the DB credentials to your function.\nYou can communicate this feature request directly to the Redshift team via their AWS forum, and they might have a few ideas to support your use case beyond Lambda environment variables.. Hi @robbiet480,\nThanks for your contribution, but we are unable to accept changes to API models. They are automatically generated from an internal representation, so the next release would wipe out these changes. I'll follow up with the service team to make sure this change is included in their next release.. Hi @robbiet480,\nThanks for your contribution, but we are unable to accept changes to API models. They are automatically generated from an internal representation, so the next release would wipe out these changes. I'll follow up with the service team to make sure this change is included in their next release.. I haven't set up a repro case for this yet, but wouldn't you expect to encounter a UserNotFoundException when the MessageAction parameter is set to RESEND? The API reference implies that the \"RESEND\" action should only work on existing users.. I haven't set up a repro case for this yet, but wouldn't you expect to encounter a UserNotFoundException when the MessageAction parameter is set to RESEND? The API reference implies that the \"RESEND\" action should only work on existing users.. Hi @neerajaset71,\nThat's the intended behavior if you set the MessageAction parameter to 'SUPRESS'. To create a new user and send a notification email, omit the MessageAction parameter.. Hi @neerajaset71,\nThat's the intended behavior if you set the MessageAction parameter to 'SUPRESS'. To create a new user and send a notification email, omit the MessageAction parameter.. The stack traces from the DynamoDB client would be significantly longer than stack traces from SES or S3 due to the higher number of retries enabled by default for that service. \nI haven't used longjohn, but their documentation states that stack trace length is configurable. You should be able to set that configuration parameter to -1 to get a full trace.. The stack traces from the DynamoDB client would be significantly longer than stack traces from SES or S3 due to the higher number of retries enabled by default for that service. \nI haven't used longjohn, but their documentation states that stack trace length is configurable. You should be able to set that configuration parameter to -1 to get a full trace.. So is the issue that the message provided by the service could be clearer?. So is the issue that the message provided by the service could be clearer?. @vyas07 Is this code running in node.js or the browser? I see that the node-specific HTTP handler is being used, so the stream variable on line 52 is an instance of http.ClientRequest, which has a setTimeout method.. @vyas07 Is this code running in node.js or the browser? I see that the node-specific HTTP handler is being used, so the stream variable on line 52 is an instance of http.ClientRequest, which has a setTimeout method.. Closing as a duplicate of #1348 . Closing as a duplicate of #1348 . Hi @DanTup,\nIs there anything a javascript package can do to make js-interop with Dart work better or more transparently? In an ideal world, we wouldn't need a separate SDK for compile-to-js languages like Dart or CoffeeScript but could instead support them better with the JavaScript SDK.. Hi @DanTup,\nIs there anything a javascript package can do to make js-interop with Dart work better or more transparently? In an ideal world, we wouldn't need a separate SDK for compile-to-js languages like Dart or CoffeeScript but could instead support them better with the JavaScript SDK.. The SDK is not split vertically across services but instead horizontally across concerns, so porting DynamoDB would mean porting most of the SDK. You would need to port (at least) the request signer, the credentials sourcing code, the JSON marshaller and unmarshaller, and the HTTP abstraction.\nI'm going to go ahead and close this issue, as it isn't something the JS SDK can address, but I have passed this request along to see if this is something that could be taken on. There are also a few community-supported SDKs (such as Rusoto, PAWS, the Haskell SDK, etc), so there might already be a Dart SDK or one that's partially completed. \nPlease feel free to reopen if you have any questions or concerns.. The SDK is not split vertically across services but instead horizontally across concerns, so porting DynamoDB would mean porting most of the SDK. You would need to port (at least) the request signer, the credentials sourcing code, the JSON marshaller and unmarshaller, and the HTTP abstraction.\nI'm going to go ahead and close this issue, as it isn't something the JS SDK can address, but I have passed this request along to see if this is something that could be taken on. There are also a few community-supported SDKs (such as Rusoto, PAWS, the Haskell SDK, etc), so there might already be a Dart SDK or one that's partially completed. \nPlease feel free to reopen if you have any questions or concerns.. Hi @vyas07,\nYou could set the appropriate parameters as bound parameters on the S3 client you're using.\nFYI: Usage questions are a better fit for StackOverflow, which lets you pick the best answer and also ensures people googling similar questions will see your question and the answer provided. Make sure to tag usage questions with aws-sdk-js.. Hi @vyas07,\nYou could set the appropriate parameters as bound parameters on the S3 client you're using.\nFYI: Usage questions are a better fit for StackOverflow, which lets you pick the best answer and also ensures people googling similar questions will see your question and the answer provided. Make sure to tag usage questions with aws-sdk-js.. Hi @etsangsplk,\nCould you post this as a feature request in the ECS developer forum? That's the best way to get a feature request in front of a service team. Please update this issue with a link to the forum question so that anyone who lands on this issue from a Google search can follow the discussion.. Hi @etsangsplk,\nCould you post this as a feature request in the ECS developer forum? That's the best way to get a feature request in front of a service team. Please update this issue with a link to the forum question so that anyone who lands on this issue from a Google search can follow the discussion.. Hi @asplogic,\nThe InvalidParameter error is coming from the service, so the best place to get help on this issue would be the AWS SNS developer forum or on StackOverflow. Since you would encounter the same issue in any SDK or when using the REST API, it's best to keep the discussion to a forum where your question and the answer are visible to any developers using SNS, not just JavaScript developers.. Hi @asplogic,\nThe InvalidParameter error is coming from the service, so the best place to get help on this issue would be the AWS SNS developer forum or on StackOverflow. Since you would encounter the same issue in any SDK or when using the REST API, it's best to keep the discussion to a forum where your question and the answer are visible to any developers using SNS, not just JavaScript developers.. Hi @marckaraujo,\nA PR exposing the DynamoDB DocumentClient's converter has been merged to master and will go out with the next release. To convert the \"NewImage\" hash in the JSON snippet above to the expected output, you would call AWS.DynamoDB.Converter.output(newImageObject) where newImageObject represents the JSON you receive.\nPlease feel free to reopen if you have any questions or concerns.\n. Hi @marckaraujo,\nA PR exposing the DynamoDB DocumentClient's converter has been merged to master and will go out with the next release. To convert the \"NewImage\" hash in the JSON snippet above to the expected output, you would call AWS.DynamoDB.Converter.output(newImageObject) where newImageObject represents the JSON you receive.\nPlease feel free to reopen if you have any questions or concerns.\n. \ud83d\udc11 \ud83c\uddee\ud83c\uddf9 . \ud83d\udc11 \ud83c\uddee\ud83c\uddf9 . @vyas07 The Uint8Array in the response data dump contains the bytes for the string \"hello\" encoded in UTF-8. Try running the following in the Chrome console:\njs\nconst arr = new Uint8Array([104, 101, 108, 108, 111])\nconst decoder = new TextDecoder('utf8')\nconsole.log(decoder.decode(arr)) // prints \"hello\"\nTo upload a JPEG image from a browser to S3, you will need to load the image from the file system into the browser's JavaScript runtime. MDN has a good tutorial on how to use files in web applications.. @vyas07 The Uint8Array in the response data dump contains the bytes for the string \"hello\" encoded in UTF-8. Try running the following in the Chrome console:\njs\nconst arr = new Uint8Array([104, 101, 108, 108, 111])\nconst decoder = new TextDecoder('utf8')\nconsole.log(decoder.decode(arr)) // prints \"hello\"\nTo upload a JPEG image from a browser to S3, you will need to load the image from the file system into the browser's JavaScript runtime. MDN has a good tutorial on how to use files in web applications.. @vanerleo Was this related to the version loading issue discussed in #1374? I'm unable to reproduce the error you've encountered when I run the above code with version 2.21.0.. @vanerleo Was this related to the version loading issue discussed in #1374? I'm unable to reproduce the error you've encountered when I run the above code with version 2.21.0.. This seems like a usage pattern that the DocumentClient should support. I'll mark this as a feature request and add an item to our backlog.. This seems like a usage pattern that the DocumentClient should support. I'll mark this as a feature request and add an item to our backlog.. That's fair. I'll update the tag.. That's fair. I'll update the tag.. Services using the JSON protocol also return their errors in JSON format, which is why the SDK attempts to parse 400- & 500-level responses.\nWould it help if the SDK surfaced an explicit \"unable to parse JSON\" error in such cases?. Services using the JSON protocol also return their errors in JSON format, which is why the SDK attempts to parse 400- & 500-level responses.\nWould it help if the SDK surfaced an explicit \"unable to parse JSON\" error in such cases?. :shipit: . :shipit: . In the example code provided, only the input passed to expireS3object is logged if no error is encountered. The PutBucketLifecycleConfiguration operation does not return any response data, so you would need to call aws.S3Client.getBucketLifecycleConfiguration to see the updated value.\nFYI: StackOverflow is a better forum for questions about how to use an AWS service, as answers provided there are more discoverable than answers provided on GitHub.. In the example code provided, only the input passed to expireS3object is logged if no error is encountered. The PutBucketLifecycleConfiguration operation does not return any response data, so you would need to call aws.S3Client.getBucketLifecycleConfiguration to see the updated value.\nFYI: StackOverflow is a better forum for questions about how to use an AWS service, as answers provided there are more discoverable than answers provided on GitHub.. @osdavison Per the CORS spec, browsers must exclude user credentials from preflight requests. XHR is acting as it should.\nThe issue is that AppStream does not support CORS and is therefore not handling OPTIONS requests any differently from GET or POST requests, which is to say that it attempts to authenticate them. I believe that you would see similar behavior from most services on our supported services list that do not have a :tada: icon in the 'Allows CORS' column.. @osdavison Per the CORS spec, browsers must exclude user credentials from preflight requests. XHR is acting as it should.\nThe issue is that AppStream does not support CORS and is therefore not handling OPTIONS requests any differently from GET or POST requests, which is to say that it attempts to authenticate them. I believe that you would see similar behavior from most services on our supported services list that do not have a :tada: icon in the 'Allows CORS' column.. You cannot use AppStream from a web browser. Their API does not support cross-origin resource sharing, so browsers will block the request from being sent if it contains an Authorization header.\nYou can submit open a thread on the AppStream forum to request that they add support for CORS. \nIn the meantime, you could use Lambda as a middleman to invoke AppStream operations. Lambda does accept CORS requests, and CORS limitations do not apply when invoking an API from a node environment like AWS Lambda.. You cannot use AppStream from a web browser. Their API does not support cross-origin resource sharing, so browsers will block the request from being sent if it contains an Authorization header.\nYou can submit open a thread on the AppStream forum to request that they add support for CORS. \nIn the meantime, you could use Lambda as a middleman to invoke AppStream operations. Lambda does accept CORS requests, and CORS limitations do not apply when invoking an API from a node environment like AWS Lambda.. If you're referring to this Fiddler, I believe browser extensions are not subject to the same restrictions as code running on a web page. \nIn any case, requiring authentication on an OPTIONS request means that AppStream does not support CORS per the specification. The best way to get the service to change this is to request on their AWS developer forum that they add support for CORS.. If you're referring to this Fiddler, I believe browser extensions are not subject to the same restrictions as code running on a web page. \nIn any case, requiring authentication on an OPTIONS request means that AppStream does not support CORS per the specification. The best way to get the service to change this is to request on their AWS developer forum that they add support for CORS.. It looks like you need to put the data you wish to send under the key custom in the object that gets JSON.stringify'd and base64 encoded. For example, with the following client code:\njavascript\nvar AWS = require('aws-sdk');\nvar la = new AWS.Lambda({ region: 'us-west-2' });\nvar ctx = {\n    custom: { foo: 'bar' },\n    client: { snap: ['crackle', 'pop']},\n    env: { fizz: 'buzz' },\n};\nla.invoke({\n    FunctionName: 'contextPrinter',\n    ClientContext: AWS.util.base64.encode(JSON.stringify(ctx)),\n    InvocationType: 'RequestResponse',\n    Payload: JSON.stringify({ baz: 'quux' })\n}, function (err, data) { return console.log(err, data); });\nand the following lambda:\npython\ndef lambda_handler(event, context):\n    print(\"context\", context.client_context)\n    print(\"client\", ', '.join(i for i in dir(context.client_context.client) if not i.startswith('__')))\n    print(\"custom\", context.client_context.custom)\n    print(\"env\", context.client_context.env)\nI get the following log output:\n('context', <__main__.ClientContext object at 0x7f5705898170>)\n('client', 'app_package_name, app_title, app_version_code, app_version_name, installation_id')\n('custom', {u'foo': u'bar'})\n('env', {u'fizz': u'buzz'})\nThe client context contains data about the client, but data under the custom or env keys is made available to the lambda function.. I had to force push due to a merge conflict in AWS.util. The last commit add the ability to assume a role for a profile defined in ~/.aws/credentials even if the source profile is defined ~/.aws/config and vice versa.. The AWS_SDK_LOAD_CONFIG environment variable is only documented on that page in the \"Order of Precedence for Setting the Region\" section. I'll try to get the \"Using a Shared Config File\" section updated to reflect this.. Aren't .js files covered by the '.*' just to the left of this addition?. Closing due to age. I believe this change would be a no-op (.js files should be caught by the .* glob), but please comment or reopen if I'm wrong about that.. Looks like we had a real testing gap on using multipart_upload with streams. Thanks for your help on this.. Hi @harishreddy-m,\nHave you tried asking on the ECS developer forum? That forum is monitored by engineers who work on ECS.. I opened a question on your behalf at https://forums.aws.amazon.com/thread.jspa?threadID=251584. Hi @harishreddy-m,\nI heard back from the service team, and it looks like you need to specify a targetGroupArn property in the load balancer definition in the call to ECS.createService when the load balancer is an ALB. (This is documented on the createService property.)\nIf the targetGroupArn is missing, ECS assumes that the load balancer specified is a classic ELB load balancer, and classic LBs require a static host port. The ECS team is looking into ways to improve the error messaging for this particular issue, but adding targetGroupArn should unblock you.\nPlease feel free to reopen if you have any questions or concerns.. Hi @huoqi,\nBy default, listObjectsV2 does not include the data's owner in response returned. You can modify this behavior by setting the fetchOwner input parameter to true:\njavascript\ns3.listObjectsV2({Bucket: bucketName, fetchOwner: true}, function(err, data) {\n    // if err is null, data.Contents.Owner should be set\n});. Hi @maxcountryman,\nI wasn't able to reproduce with a simplified example. It looks like the getActiveHosts function will convert any error into an empty array; it's possible that an unrelated error is occurring. Can you add a logging statement to the callback statement passed to docClient.scan to capture any error returned?. Just to clarify, you don't see any issues when the CRC32 check is disabled? I believe that a related issue may have been reported to the Java SDK where CRC32 checks fail when receiving throttling exceptions. Scans tend to use a lot of read capacity, so executing one every 30 seconds could cause throttling issues.. I think you're running into throttling issues, as that would explain why the behavior is different in setInterval vs just directly calling the scan operation a single time. I'll need to look into why no error is being surfaced to the callback.. The amount of read capacity used by a scan is proportional to the amount of data in the table. (See http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithTables.html#CapacityUnitCalculations for details.) Without knowing how large the table being read is, I can't say whether a 100 read capacity units is enough to allow a full scan every 30 seconds.\nCould you inspect the HTTP response being returned from the scan operation? It would be the calling context for the callback passed to docClient.scan; if you use an unbound function rather than an arrow function, you should be able to view the raw HTTP response by calling console.log(this.httpResponse.body.toString()). Could you log the headers and status code? (this.httpResponse.headers and this.httpResponse.statusCode, respectively.) \nSorry to keep sending you back to gather more data, but I'm still trying to reproduce this.. I heard from the AWS Support rep you were working with that the root cause had been identified as a third-party module. Please feel free to reopen if that turns out to be incorrect or if you have any further questions.. The SDK will only listen to data events if the underlying node engine does not support v2 of the streams API. It's the only way we can be sure that we don't lose any data from responses.\nIt looks like Raven made a change to keep the library from automatically transitioning response streams from 'paused' mode to 'flowing' mode. Do you see any issues when using the latest version of Raven with the SDK?. It's not configuration that's exposed to the SDK; any readable stream can be transitioned to flowing mode by attaching a listener to its data event. I'll look into how we can better document incompatibilities with libraries that coerce streams from paused to flowing mode.. Hi @tomowens3,\nWhether a request gets retried get determined in AWS.util.handleRequestWithRetries, which always checks if retryCount < maxRetries. Did you observe requests never resolving? . Closing as answered by the earlier comments. Please feel free to reopen if you have further questions.. The CLI's query argument evaluates a JMESPath expression, which the SDK uses internally but does not expose at the moment. You could include the JavaScript version of JMESPath as a top-level dependency of your application and run the jmespath.search method it exposes on any JS object or array.. @gitisz Does using the JMESPath library directly solve your issue, or were you looking for something built into the SDK? Not sure if I should close this out or mark it as a feature request.. Happy to help! Let me know if you have any questions about how to use JMESPath.js.. Which version of the SDK are you using? Support for Rekognition was added in 2.11.0. Hi @Ballhorn,\nAre all resources used by the Lambda function available in the VPC in which the function is running? \nYou might also want to try posting this question in the AWS Lambda forum, the AWS DMS forum, and on StackOverflow. The former two are monitored by Lambda and DMS engineers, respectively.. Glad you were able to track that down. Please feel free to reopen if you have any questions or concerns.. Hi @riteshatsencha,\nIt looks like Ec2::DescribeVolumesModifications would be a good candidate for a waiter. Marking as a feature request.. It is not. The Bug label is used when we can confirm that there is a bug in the SDK or a bug that can be fixed by a code change to the SDK. When it is unclear that any remedy is available to the SDK -- for instance, when we're unsure if the issue is how the SDK interacts with a third-party library not listed as an SDK dependency -- more investigation is required before something can be labeled a bug or not a bug.\nI think the point you're trying to make is that we could be faster in relabeling issues, which is fair. Typically, once an issue has a label, we don't relabel it unless the requester makes the case that the issue has been mislabeled. With the exception of the Bug label, labels do not factor into how we prioritize issues but are instead there to provide context for SDK contributors. \nI'll go ahead and apply an in progress tag to #1385, as it's being looked at but isn't really a question.. \ud83d\udc11 \ud83c\uddee\ud83c\uddf9 . It looks like the AWS SDK for Java implementation is only available in the context of the DynamoDB DataMapper; do you think this feature would make sense as a stand-alone addition to the SDK?. Hi @treyrich,\nI heard back from the Cognito team, and it looks like the request in question was being sent to https://cognito-identity.us-east-1.amazonaws.com/index.html (instead of to the domain root). Since the issue isn't fixed by using a different version of the SDK, is it possible that a browser plugin or a proxy server is altering the request in flight?. While some services may work in some browsers, I wouldn't recommend using the SDK in service workers. The browser version of the SDK has a hard dependency on XMLHttpRequest, which doesn't appear to be available in all ServiceWorker implementations. XML-based services (like S3 and Ec2) also use the Document API to parse responses, which would not be available in service or web workers.. I think you're probably right: there may be something in the particular service worker implementation being used that's resolving requests from the bare domain to /index.html. I can set up a test case with S3 and Dynamo to determine if we support using the SDK on a page that also happens to be using service workers.. I put together a small test that uses service workers in the background and also uses Cognito and was not able to reproduce.\nI didn't get a chance to dig too deeply into the mobile toolkit, but are you using a manifest like the one on this page for any endpoints? There's a routing section at the bottom of the manifest that maps the root to /index.html, which may be what's causing the issue you're encountering.. Hi @testtshoretel,\nI left the following code snippet running overnight and have only encountered https endpoints:\n```javascript\nconst AWS = require('../../index');\nconst url = require('url');\nconst s3 = new AWS.S3({\n  region: 'us-west-2',\n  sslEnabled: true,\n});\nwhile (true) {\n  const signedUrl = s3.getSignedUrl('getObject', {Bucket: 'bucket', Key: 'key'});\n  const parsed = url.parse(signedUrl);\n  if (parsed.protocol !== 'https:') {\n    console.error(signedUrl);\n    process.exit(255);\n  }\n}\n```\nAre you specifying a custom endpoint for any buckets? . If you specify an endpoint, that endpoint is used for presigned URLs instead of one that is constructed from the region, service, and scheme. \nV4 presigned URLs are also only valid in one region, so you wouldn't be able to point the URL at a different region after signing it.. Yes, that should take care of it. If you do need to use a custom endpoint, make sure it starts with either https:// or {scheme}:// (which will resolve to https:// if sslEnabled is true and http:// otherwise).. I believe the call to resume was added to shift a stream into flowing mode so that it would exhaust the source and close itself. You can use the managed uploader to upload any stream, not just those created with node's fs module. The fs.close method only works on file descriptors and is not part of the generic stream.Readable interface.\nYou will need to call fs.close in the callback provided to AWS.S3::upload or AWS.S3.ManagedUpload::send.. Most testing libraries require you to call a function to indicate when asynchronous code should be recognized as done. In the Mocha testing framework, for example, you would need to have the function passed as the second parameter to describe take a single argument in order for the function to flagged by Mocha as asynchronous. See Mocha's documentation and this StackOverflow question for more discussion.\nWhen you deployed the code to AWS Lambda, was it wrapped in handler function as described in the Lambda docs?. Hi @baweaver,\nThanks for putting this together. There's another PR (#1143) that updates sax and uuid as well and keeps xmlbuilder at version 4.2.1. Later versions of xmlbuilder do not support node 0.8, which the AWS SDK does.\nWas the concern with having two versions of lodash the size of the generated bundle, or did you encounter any errors building code that relies on the SDK?. True, but we still have users running Node 0.8 and NPM v1. 4.2.1 is the latest version of xmlbuilder we would be able to use without introducing a breaking change for those users, and it does use lodash 4.. Hi @dinvlad,\nWhich parameters are marked as required comes from the model files shared by all the AWS SDKs, and the ultimate source for that information is the service team themselves. The same is true for the documentation provided for services and API operations. \nI've let the CloudFront team know that they should investigate whether the IfMatch parameter is always required. I would recommend asking this question on the forums as well.. Closing, as there's nothing the SDK can do on its own. I will update this issue when I hear back from the CloudFront team.. Hi @riteshatsencha,\nCalling createAppCookieStickinessPolicy or createLBCookieStickinessPolicy creates a policy but does not enable it on any particular load balancer. To enable one or more policies, you would need to call setLoadBalancerPoliciesOfListener and provide an array of the names of the policies you would like to enable on the load balancer identified with the LoadBalancerName property. To disable one or more policies, you would need to call the setLoadBalancerPoliciesOfListener operation again and omit the names of the policies you would like to disable. ELB has a section in their developer guide on sticky sessions if you want to dive a little deeper on the topic.. It's stable; we're just working on the deployment strategy. Right now, you need to build a version of the SDK for react native (as you would build a version for the browser) instead of just using require statements.\nThere's more discussion and a link to the relevant issue on the react native repository on the pull request at https://github.com/aws/aws-sdk-js/pull/1393. Hi @uopeydel,\nS3 bucket names are case sensitive, but lowercasing a host is permissible under RFC 3986. S3 allows users to address a bucket in two ways: either by prepending the bucket name to the host (\"virtual-hosted addressing,\" as in samplelink 2) or by prepending the bucket name to the path (\"path-style addressing,\" as in samplelink 1). The SDK will use virtual-hosted addressing unless a bucket's name contains uppercase characters or is otherwise DNS-incompatible, so the myBucketFolder bucket would use path-style addressing.\nS3 does not allow uppercase characters in bucket names outside of the us-east-1 region, so you wouldn't be able to create a bucket called myBucketFolder in the ap-southeast-1 region. You will need to use only lowercase letters, numbers, hyphens, and periods.. You can force the S3 client to use path-style addressing with the s3ForcePathStyle configuration option.. That does sound like the SDK is not being stubbed in the way you intended, but it doesn't sound like an issue in the SDK. Which mocking library are you using?. Hi @jmparsons,\nOther than the latency encountered doing BigInteger math in the Cognito SDK, have you encountered any difficulties integrating the two SDKs in a project?. Hi @karna41317,\nI'm not sure it would be a good idea for the SDK to take on this responsibility, as we would need to find a way to offer that functionality in the browsers we support. Both the putObject and the upload methods on an S3 client will accept a stream object, so you could pipe the uncompressed file through a zlib.Gzip stream before handing it to the S3 client.. This SDK supports automatically generating a pre-signed URL for the appropriate RDS operations. Cf https://github.com/aws/aws-sdk-js/blob/master/lib/services/rds.js#L13.\nYou need to specify a SourceRegion parameter per the documentation at http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/RDS.html#copyDBSnapshot-property, but we should look at how to make that documentation clearer.. I added a note to the docs for the PreSignedUrl property for CopyDBClusterSnapshot, CopyDBSnapshot, CreateDBCluster, and CreateDBInstanceReadReplica. For each of those operations, a pre-signed URL will be calculated on your behalf if the SourceRegion parameter is present and the PreSignedUrl parameter is empty.. At the moment, only Amazon S3 offers VPC endpoints. You would need to add a rule to the VPC to allow outbound connections to https://email.us-east-1.amazonaws.com/  (as discussed in the AWS forums).. Since file names are not unique across time, I think this would be tricky to get right. If you replace a file with another of the same name, that change would be difficult to detect in a browser environment. Clues like file size and last modified date are not available in all the browser environments we support.\nIt would be helpful, though, to offer a helper for resuming a multipart upload in a generic way. (E.g., something that calls ListParts and resumes based on which parts have already been uploaded.). It might be as simple as populating completeInfo with the completed part numbers and their associated ETags, but I would need to do some testing to be sure.\nThe SDK should probably support a generic way of picking up a multipart upload from a known state (e.g., how the PHP SDK supports creating a multipart upload from a state object) and provide a helper method that creates a state object from an upload ID (by calling ListParts and extracting the relevant data). If the SDK added that feature, then you could just store the upload ID in local storage.. It looks like the HTTP_DATA event listener assumes the HTTP_HEADERS event will have been dispatched first, which seems like it should be a safe assumption. It also assumes that HTTP_DONE has not yet been dispatched, which concats the collected data buffers and then deletes the buffers array (and again seems like it should be a safe assumption).\nWhich version of node are you using?. @villagebike Are you using the version of the SDK provided by Lambda or bundling your own? If the latter is the case, the issue should be addressed in versions 2.50.0 and later.. Hi @torbenbru,\nThe SDK invokes callbacks from Request.callListeners, which means that errors thrown in a callback can surface with a stack trace that includes the SDK. I wasn't able to find any errors thrown with the message \"Incorrect arguments\" by the SDK, but it looks like something similar is thrown by bcrypt-nodejs. It's possible the error is first being thrown by the call to isValidPassword.. The DynamoDB documentation for Scan operations matches what's in the JS docs. \nAre you using the an instance of AWS.DynamoDb.DocumentClient? The document client will automatically convert ExpressionAttributeValues in the same way it converts items supplied to PutItem operations.. The document client automatically converts ExpressionAttributeValues from JavaScript types to DynamoDB envelope objects. This is the documented behavior of instances of AWS.DynamoDB.DocumentClient and differs from the behavior of instances of AWS.DynamoDB.. The document client automatically converts JavaScript types to DynamoDB's AttributeValue types. \nFor example documentClient.put({TableName: \"table1\", Item: \"myApp\"}) converts \"myApp\" to {S: \"myApp\"} and calls DynamoDB's PutItem operation with {TableName: \"table1\", Item: {S: \"myApp\"}.\nSimilarly, documentClient.scan converts the value passed to ExpressionAttributeValues to a DynamoDB type, so ExpressionAttributeValues: {\":v\": \"myApp\"} is being sent as ExpressionAttributeValues: {\":v\": {S: \"myApp\"}} to DynamoDB. \nIf you provide an already converted item to the document client, it will convert the object to a DynamoDB map, so ExpressionAttributeValues: {\":v\": {S: \"myApp\"}} would be converted to ExpressionAttributeValues: {\":v\": {M: {S: {S: \"myApp\"}}}} before being sent to the service.. Hi @prestomation,\nFrom the sample code in the developer guide you linked to, signed websocket URLs shouldn't include any headers but should instead hoist everything that would normally be a header to the query string. That's pretty close in spirit to the way the SDK's S3 and Polly presigners work, but different enough that I think we would need a custom signer.. When using an instance of AWS.MobileAnalytics, you would need to manually call putEvents with the collected events. The Mobile Analytics SDK handles batching and dispatching for you, but I'm not sure if the library is directly usable in react native. I see that you've opened an issue on their GitHub page; I would recommend asking on the Mobile Analytics AWS developer forum as well.\nTaking a look at the mobile analytics sdk, it looks like clientContext should be a JSON document containing information about the client, environment, and any custom data you want to include. . I believe this property is not exposed because we can't append anything to the user agent in a browser environment (in browsers, the SDK's user agent are sent via the X-Amz-User-Agent header). That said, I don't have any objection to exposing this property so long as that limitation is called out in documentation. Could you update the doc string to mention that the User-Agent header will only be modified in a node environment?. Thanks for your contribution!. Adding some way to access the response from the resolved value of a promise would be one way to solve this. The response property isn't part of the request object's publicly documented interface, so a better way to access the request ID etc should be added.. RDS offers a REST endpoint to download complete log files, but that API is not exposed in the AWS SDKs. I'll reach out to the RDS team to see if they have any suggestions that don't rely on that endpoint.. Looking at the Cognito User Pools documentation and console, I only see a way to customize the email messages for user invitations and user email verifications.\nI would recommend reaching out to the Cognito team on the AWS developer forums to request that they add this feature.. The ARN being sent looks like a task definition ARN instead of a task ARN. Task ARNS would look like arn:aws:ecs:<region>:<aws_account_id>:task/<UUID>; it should be returned in the response returned when StartTask or RunTask is called.\nYou should be able to identify the tasks started from a given task definition family by calling ListTasks and specifying the task definition family (StandupAPIProd in the case of the ARN provided above).. I verified that with this fix, the following commands still run without an issue on Node 0.8:\n a clean npm install --production\n node node_modules/.bin/mocha -- test/protocol/ test/query/ test/json/ test/xml/ test/model/\n* node node_modules/.bin/cucumber.js --tags ~@file --tags ~@streaming\nThanks for putting this PR together!. @tyrsius Unfortunately, we're already on the latest version of XMLBuilder that is guaranteed to work with Node 0.8. Updating that dependency isn't an option, but replacing it with something smaller or with a smaller dependency graph could be.. We still have many users running EOL'd versions of Node. Linux distros maintain and patch older versions of Node; running apt-get install nodejs on a fresh installation of Ubuntu 14.04, which will be supported by Canonical through 2019, will install Node 0.10.\nOutside of a major version bump of the SDK, we wouldn't drop support for a platform unless doing so were unavoidable, and I believe we could probably replace XML Builder while still maintaining compatibility with older versions of Node.. Thanks for the contribution!. @kevinbror When a callback is provided, the body will always be returned as a buffer. The only way to get the body as a stream is to call createReadStream on the S3 request.. Hi @clark-pan,\nThe default file path for ~/.aws/credentials is still used, as is the default config file path of ~/.aws/config.\n@matsaleh13  As of 2.44.0, the SDK is using os.homedir if the function is available, which will not respect the HOME environment variable when running on git bash but will instead use the USERPROFILE environment variable:\n```\nuser@computer MINGW64 ~\n$ HOME='/foo/bar' node\n\nconst os = require('os')\nundefined\nos.homedir()\n'C:\\Users\\user'\n```\n\nos.homedir is still able to locate a home directory when the system's standard environment variables have not been set, but giving it higher precedence than $HOME looks like a regression for Windows users. \n(@chrisradek This would not affect users running node on the Windows subsystem for Linux, as they would be running a copy of node compiled on Ubuntu that uses the *nix implementation of uv_os_homedir.). @matsaleh13 The CLI uses boto (the AWS SDK for Python) under the hood, and it looks like Python's os.path.expanduser will use HOME over USERPROFILE on Windows. Since the change in 2.44.0 was meant to increase compatibility with the CLI, surfacing this runtime difference between Node and Python isn't a great user experience.\n1484 changes the precedence of how home directories are located so that HOME, USERPROFILE, HOMEPATH, and HOMEDRIVE will be checked before os.homedir is called.. Hi @testtshoretel,\nDid you upgrade the SDK in between when the expiration was working and when it stopped working? Or did you experience both behaviors using the same version of the SDK?. Since you've experienced both behaviors with the same version of the SDK, the change would necessarily be due to a server-side change by S3. The best way to reach out to S3 engineers is via their AWS developer forum.. Have you tried running an NTP sync on the production server?. If the same code base is deployed across three environments and you're encountering 403 errors in one of them, the errors are almost certainly related either to credentials or to server time. Clock skew issues come up more frequently with containerized environments, but if running an NTP sync didn't cause the errors to go away, then clock skew probably isn't the root cause.\nHow are you loading credentials? Are they coming from a synchronous source (e.g., environment variables or an ini file) in one environment and an asynchronous source (e.g., STS or container roles) in another?. ...I'm not sure I know enough about Kubernetes to be of much help on this one. If the .env file is shared between pods via NFS or a similar shared mount, you might be running into a race condition where the file is not fully available when the application is booting (and the environment is therefore populated with partial credentials like a full access key ID and a truncated secret key). Are you able to verify that the .env file is present in its entirety on each pod?. Happy to help!. The SDK isn't catching errors thrown by a dependency, which is causing the error you've encountered. I opened a PR to fix catch those errors and an issue on the dependency to improve their error messages.. Our tests are currently failing when run on Node 7.10. I'll address that issue in a separate PR and rebase.. Can you share a bit more about the execution environment, such as:\n Which version of the SDK are you using?\n Are you using the react-native distributable described in the readme?\n* Are you seeing this issue on iOS, Android, or both?. Hi @antonsamper,\nI can definitely see how the delay between when the modifyDBInstance promise is resolved and when the instances begin reporting their status as modifying can be frustrating, but I don't think adding an initial delay to waiters will solve it in all cases. If RDS requires a few retries to contact the node or there's some transient networking issue that keeps the modify command from reaching the affected instance, then the instance might still report as available even after 5 or 30 seconds.\nA more reliable solution would be to add a DBInstanceModifying waiter that uses the same delay and maxAttempts count as DBInstanceAvailable but that uses modifying as its target instance status instead of available. You could then wait for the modifications to take effect by chaining the waiters together:\njavascript\nreturn RDS.modifyDBInstance({\n    DBInstanceIdentifier: instanceId,\n    DBInstanceClass: instanceClass,\n    ApplyImmediately: true\n}).promise().then(() => {\n    return RDS.waitFor('dBInstanceModifying', {\n        DBInstanceIdentifier: instanceId\n    }).promise();\n}).then(() => {\n    return RDS.waitFor('dBInstanceAvailable', {\n        DBInstanceIdentifier: instanceId\n    }).promise();\n}).then(() => {\n    // the modifications have been applied\n});\nWould that be a reasonable remedy?. By default, Cognito credentials will only be able to access Cognito Sync and Mobile Analytics. You will need to grant a more permissive role to the Cognito Identity Pool used by your application in order to access Pinpoint APIs.. I believe this would solve #1453 as well.\n:shipit: . You can accomplish this by setting the s3BucketEndpoint configuration parameter to true when creating the S3 client:\njavascript\nconst s3 = new AWS.S3({\n    region: 'us-west-2',\n    s3BucketEndpoint: true,\n});\nYou can read more about the configuration options availabe here.. It looks like a SignatureDoesNotMatch error without a message is being returned by a service running on a non-routable IP (10.47.2.22).\nWhat's running at that IP, and what kind of signature does it expect?. Closing due to inactivity. Please feel free to reopen if you have any questions or concerns.. Hi @mano-bharathi,\nYou can accomplish this by setting the logger constructor parameter when creating a client. You will need to pass in an object that has a defined log or write method. For example, if you want to log directly to the console, you can pass in the global console object:\njavascript\nconst s3 = new AWS.S3({\n    region: 'us-west-2',\n    logger: console,\n});. Hi @iamanjowong,\nIt looks like the policy is being created with two separate dates, which means all uploads will fail at least one policy condition. The x-amz-date, x-amz-credential, and x-amz-algorithm fields are automatically added to the policy as exact match fields when you call the s3.createPresignedPost method, so there's no need to determine the right value for these fields yourself. \nI'll post a gist with an example using a FormData object, but the basic idea is that the map of fields returned by the pre-signer should be included in your form.. Hi @LucaPaterlini,\nI'm not sure I understand the question. Are you using the AWS.SES::sendEmail operation and not receiving any response from the server?. Closing due to inactivity. Please feel free to reopen if you have any updates about the issue.. Hi @westpole,\nI reached out to the relevant team and asked if TTL is supported in DynamoDB local. I will update this issue when I hear back from them.. @westpole DynamoDB Local does not yet support TTL, but I've passed the feedback on to the service team. \nThe best place to look for announcements of features like that would the DynamoDB blog or the AWS twitter feed.. Just to be on the safe side, I think the SDK should continue to throw an error if provided another falsy value like null or false. I'll open a PR to fix this.. I'd be hesitant to add this feature to the SDK because I'm not sure if the SDK can do any validation here short of calling out to S3 to check if the item exists. That would change getSignedUrl from a purely CPU-bound operation to one that must make a network call, which would negatively impact the performance of applications that generate a large number of presigned URLs.\nWhile bucket names in most regions must be DNS-compatible, S3 lists the following exception to that rule:\n\nThe rules for bucket names in the US East (N. Virginia) region allow bucket names to be as long as 255 characters, and bucket names can contain any combination of uppercase letters, lowercase letters, numbers, periods (.), hyphens (-), and underscores (_).\n\nIn that particular region, '192.168..5.4' would be a valid bucket name. \nSimilarly, the rules on object naming are fairly lax:\n\nThe name for a key is a sequence of Unicode characters whose UTF-8 encoding is at most 1024 bytes long.\n\nAdditionally, I've seen a number of \"S3-compatible\" services that are designed to be used with an official AWS SDK but that do not necessarily enforce the same naming rules as Amazon S3.. That's a legal object name, and I suspect that some otherwise S3-compatible services would accept it as a valid bucket name, too.. This PR also locks the version of chai used by the SDK to a specific major version, as our test suite does not appear to be compatible with today's release of Chai 4.0.0.. Hi @renatogil,\nThanks for your contribution! I think this case is covered by an if statement in the config object's constructor; the extractCredentials function is marked @api private and is not meant to be called from outside of the AWS.Config class. What triggered the error this change is meant to address, and would it be possible to create a unit test that exercises that behavior?. Closing due to inactivity. Please feel free to reopen if you have any questions or concerns.. Thanks!. The browser builder rake task appears to be incompatible with NPM 5.0.0.. Hi @benishak,\n__type has a special meaning in DynamoDB's serialization format and is used to identify the type into which a document should be unmarshalled. (If you take a look at the documentation on error handling, you can see how __type is used to tell clients what species of error they've encountered.) This is a feature of the wire protocol and not something that the DocumentClient would be able to work around.\nAs a workaround for your application, you could use a different key for the discriminator field in the date map. As you mentioned, _type and type do not trigger any errors, nor would ___type or __dateType. The document client could also proactively reject maps with a __type field and include a more detailed and specific error message. . The __type field has a special meaning to the serializer, which is why its usage triggers a SerializationException from DynamoDB. The best example of this particular usage in DynamoDB's documentation that I could find was in the section on error handling, but __type is not specific to error handling, and other error-related keys like ok and errorCode do not have a special meaning to the serializer.. Hi @mrtcode,\nThanks for your contribution! The change looks good to me, but could you add a unit test to catch any future regressions?. Hi @stevedomin,\nconnectTimeout settings only take effect once a socket is assigned to a request and won't cause requests to time out if node is taking 15-30 seconds to provision a socket. Are you providing an HTTP agent to the SDK? If so, are you specifying a value on the agent for maxSockets?. If the agent is only allowed to provision a small number of sockets, then a request could potentially have to sit idle waiting for a socket to free. The SDK's connectTimeout only kicks in once a socket is assigned to a request.\nI'll admit that I'm not very familiar with strace, but it looks like the error that took 30 seconds to trigger is a read timeout rather than a connect timeout. getsockopt is only called three times in the strace log in the linked gist, and twice it's getting the error for a socket that was opened within the same second on the system clock:\n```\n08:52:48 socket(PF_INET, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 66\n08:52:48 connect(66, {sa_family=AF_INET, sin_port=htons(443), sin_addr=inet_addr(\"52.216.226.152\")}, 16) = -1 EINPROGRESS (Operation now in progress)\n08:52:48 recvmsg(18, 0x7fff2d233200, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:52:48 recvmsg(24, 0x7fff2d233210, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:52:48 recvmsg(24, 0x7fff2d233240, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:52:48 recvmsg(18, 0x7fff2d233200, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:52:48 recvmsg(24, 0x7fff2d233210, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:52:48 recvmsg(24, 0x7fff2d233240, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:52:48 getsockopt(66, SOL_SOCKET, SO_ERROR, [0], [4]) = 0\n...\n08:53:32 socket(PF_INET, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 59\n08:53:32 connect(59, {sa_family=AF_INET, sin_port=htons(443), sin_addr=inet_addr(\"54.231.80.240\")}, 16) = -1 EINPROGRESS (Operation now in progress)\n08:53:32 recvmsg(18, 0x7fff2d233200, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:53:32 recvmsg(24, 0x7fff2d233210, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:53:32 recvmsg(24, 0x7fff2d233240, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:53:32 recvmsg(18, 0x7fff2d233200, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:53:32 recvmsg(24, 0x7fff2d233210, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:53:32 recvmsg(24, 0x7fff2d233240, 0) = -1 EAGAIN (Resource temporarily unavailable)\n08:53:32 getsockopt(59, SOL_SOCKET, SO_ERROR, [0], [4]) = 0\n```\nThe second time getsockopt is called, however, occurs ~30 seconds after the socket reported that the operation was in progress:\n08:53:03 socket(PF_INET, SOCK_STREAM|SOCK_CLOEXEC|SOCK_NONBLOCK, IPPROTO_IP) = 66\n08:53:03 connect(66, {sa_family=AF_INET, sin_port=htons(443), sin_addr=inet_addr(\"54.231.98.88\")}, 16) = -1 EINPROGRESS (Operation now in progress)\n...\n08:53:32 getsockopt(66, SOL_SOCKET, SO_ERROR, [0], [4]) = 0\nSince this is a reuse of the socket with the identifier 66, it's possible that the connection was kept alive and that node emitted a socket event with an already connected socket. (This would cause the SDK to skip attaching a connection timeout, as the connection would already be established.) \nThe only timeout that would then apply is the \"socket idle timeout,\" which severs the connection if the socket neither receives nor dispatches messages within a given timeframe. This value is configurable as the timeout HTTP option in the SDK but defaults to 30 seconds. Have you tried altering that value to more aggressively sever connections?. Hi @sbking,\nThe typescript definition files (and the API documentation) are generated from service models shared among the AWS SDKs. I'll pass your feedback on to the service team to see if the documentation could be made more clear.. @anho Could you create a separate issue? That's something we can fix in the SDK, whereas the problem described by the OP would need to be addressed by the underlying AWS service.. @ajmath Could you create a separate issue? We can update the SDK API documentation to call out the validity of only providing a callback when parameters have been bound to a service.. Hi @refaelos,\nAre you calling putRecordsBatch? That operation requires an array of Records, each member of which must be an object containing a buffer or string property named Data. Can you share a bit of code that shows how the operation that resulted in the quoted error is being called?. Hi @Gilis95,\nThanks for the detailed report, but I believe you're looking for the AWS SDK for Java. If you copy and paste the above description into a new issue on that repository, the Java SDK team should be able to help you out.. Hi @benishak,\nYou can use DynamoDB Local for this purpose. The local installation still requires credentials, but you can use any values for your access key and secret key.. Thanks for your contribution!. /cc @chrisradek && @AllanFly120 \nBoth Chrome 60 and the Trusty containers in TravisCI are out of beta, so we are no longer blocked on this update. Dropping PhantomJS significantly speeds up our builds in Travis -- the tstest and lint tests now both run in under a minute.. Hi @benishak,\n#rperm = :rperm_0 checks for equality, whereas you want to know if #rperm contains a value. \nIf _rperm is a string set, you can use the contains filter expression function, as in ( contains(#rperm, :rperm_0) OR attribute_not_exists(#rperm) ). \nIf _rperm is a list, you will need to apply the filter manually on the values you receive from DynamoDB (as described here). Since consumed capacity is evaluated before the filter expression is applied, filtering the query results in your application will not result in increased costs from DynamoDB.. I think right now you would need apply the filter after getting the results back from DynamoDB. Since Dynamo calculates consumed capacity and limits result size before applying any filter expressions, you wouldn't get charged any more if the query filters out fewer results before returning them back to you. You might want to ask this same question on the DynamoDB developer forum in case the service team (or developers who write in another language) has any different ideas.\nJust FYI, the API version is an identifier that doesn't necessarily represent the last time a service was updated. The last time the API itself was updated was when time-to-live was added to DynamoDB and DynamoDB streams earlier this year, and the service has recently launched server-side features like VPC and AutoScaling support.. I'm going to reclassify this as a \"documentation\" issue rather than a \"typings\" issue, as the type definitions are correct. You can create a client in the SDK with \"bound parameters\" that will be applied to every request made with that client. See our developer guide for a discussion of how to use service clients in this way.\nThe parameters passed to an operation will be merged with the client's bound parameters, so if you've already bound a Bucket and Key parameter, calling s3.getObject() or s3.getObject((err, data) => { ... }) is allowed.. You would need to use MIME encoded-word syntax to use non-ASCII characters in the Source text. There's some additional discussion in the SES API reference:\n\nIn all cases, the email address must be 7-bit ASCII. If the text must contain any other characters, then you must use MIME encoded-word syntax (RFC 2047) instead of a literal string. MIME encoded-word syntax uses the following form: =?charset?encoding?encoded-text?=. For more information, see RFC 2047.. If there are any callbacks listening for an HTTP stream's 'data' events, the stream will be consumed before the SDK can collect it into a buffer. Is anything in your application listening to HTTP 'data' events? This could also be happening in a dependency -- for instance, older versions of Sentry's Raven library were known to listen for 'data' events on all HTTP streams. Are you using any dependency that might be attaching listeners to all HTTP streams?. Happy to help. Please feel free to reopen if updating to version 0.21.5 or later of Glimpse does not resolve the issue.. Closing, as it appears that the last comment answered the question. Please feel free to reopen if you have any questions or concerns.. Could you raise this with Cognito via their AWS Developer Forum? I'll follow up with them as well, but the forum is the best way to get something directly to the attention of a service team.. Would you be able to provide a request ID for one of the failed requests? This would be available as a header (typically x-amz-request-id) on a failed request; the value may not be available to JavaScript if you're encountering issues when making the preflight CORS request, but it should still be viewable in the Network pane of Chrome dev tools.. Hi @medaamarnadh,\n\nBecause the changes requested in a ChangeResourceRecordSets operation must be propagated to authoritative Route53 DNS servers in order to take effect, the operation is asynchronous. The service has more extensive documentation on why that is in their developer guide.\nYou can use the resourceRecordSetsChanged waiter on an  SDK Route53 client to register a callback to be invoked when the change has been propagated.. Hi @luketn,\nWe don't have any full TypeScript sample apps in our documentation, but there is a folder of \"typescript tests.\" These are files that include snippets of code invoking SDK functions and making type assertions on the input and return. I wouldn't copy and paste from them directly, as the code is not meant to be executed but just to be compiled.. @teledemic is correct that operations on service clients can be called as operationName(params).promise() instead of as operationName(params, callback) to send a request and obtain a promise.. Hi @steffenweber,\nI think you may be aborting requests that have already completed. Per the XmlHttpRequest spec, aborting an already received message (one whose readystate property is 4) should transition the readystate back to 0 and replace the response with a network error. If you abort a request before it reaches its final readystate, the SDK will halt processing with a RequestAbortedError.\nI agree that the SDK's bucket region lookup shouldn't be invoked if a request is aborted, but I'll need to do some testing to figure out if a networking error triggered by aborting a completed request can be reliably identified. The spec is pretty adamant that network errors should be opaque (status of 0, null body, no headers, etc), so this would probably need to be pieced together from context clues. The bucket region query would still need to rely on a request sent to https://BUCKET_NAME.s3.amazonaws.com/?max-keys=0, as requests sent to https://s3.amazonaws.com/BUCKET_NAME?location require a special IAM permission. It should, however, be sent to the correct region.\nAs a workaround, you could try replacing the call to window.setTimeout with an httpOptions.timeout configuration parameter:\njavascript\nvar s3 = new AWS.S3({\n    accessKeyId: config.accessKeyId,\n    secretAccessKey: config.secretAccessKey,\n    region: config.region,\n    params: {Bucket: config.bucket},\n    httpOptions: {timeout: 1000},\n});\nThe value is used as the XMLHttpRequest timeout, which will halt the underlying request with a TimeoutError. . It looks like the error code is being overwritten in an event listener, which is causing reqRegionForNetworkingError to run for both RequestAbortedErrors and TimeoutErrors. . @ssshah5 Presigned URLs for a multipart upload isn't something that a client library could offer on its own -- you would need to coordinate between the client and server to get the appropriate URLs signed -- but I'll mark this as a feature request. In the meantime, you might want to look at using the createPresignedPost method to construct an HTML form. That will allow the browser to manage access to the filesystem and upload the file as a multipart form.. @ssshah5 - S3 offers a mechanism for chunked transfer encoding, but it requires that each chunk be individually signed and that the length of the complete object be known beforehand.. Hi @benishak,\nI don't think you can enforce uniqueness on non-key attributes in this way. According to the developer guide, the attribute_not_exists function does something different when the field is a key vs not a key. For key fields, the operation will not be executed if an item with the provided key already exists in the table; for non-key fields, the operation will not be executed if the item already in the table with the same key as the PutItemInput.Item value has an attribute with the provided name.\nYou could enforce uniqueness on the client side by creating a global secondary index for each attribute you want to be unique with that attribute as its hash key, then querying that index before inserting a new user record.. Resolved in #1674 and #1676.. Hi @worldgnat,\nI'm having trouble reproducing the error. When I run the following script (in Node 7.10 and 8.1), an epoch milliseconds timestamp is always printed out to the console:\n```javascript\nconst AWS = require('aws-sdk');\nconst s3 = new AWS.S3({region: 'us-west-2'});\nconst Bucket = '';\nconst Key = '';\n(async function() {\n  let result = await s3.getObject({Bucket, Key}).promise();\nconsole.log(result.LastModified.valueOf());\n})();\n```\nCan you provide a bit more information about the runtime environment in which you're seeing the issue? E.g., Is this happening in Node or a browser? If Node, which version? If a browser, which one? Are you using a polyfill for async/await?. I thought a polyfill might be altering the values it received in some way. Which version of the SDK are you using?. I was able to reproduce on version 2.9. Any value that appears in a response header was not being properly deserialized to non-string types; this was fixed in version 2.29.0.. No worries! I've seen this issue reported before but had not previously tracked down which versions were affected.. Hi @alihalabyah,\nDid the issue appear when upgrading to a new version of the SDK, or did the calls stop working without any changes to your application or its dependencies?. Did the issue only occur after upgrading, and, if so, does downgrading to an earlier version fix the issue?\nI don't think anything released recently would have the effect you're seeing, but I'm just trying to determine if the issue is coming from the SDK or the Cognito service.. Hi @tvald,\nWe do use string literals in the TypeScript declaration files, but only when a shape is explicitly modeled as an enumeration in the model files shared by the AWS SDKs. For example, EC2's InstanceType shape is modeled as an enum\n and is therefore defined as a union of string literals in the corresponding .d.ts file. I'll see if the service is open to changing this value in their model from a string to an enum.. Hi @halt-hammerzeit,\nYou can set up Lambda functions to be automatically invoked in response to a number of triggers, including messages on SNS topics, inbound email messages received by SES, objects being uploaded to S3, items being inserted into a DynamoDB table, etc. A full list can be found here.\nI'm not familiar with the UI feature you describe, but it sounds like it's using Cloudwatch Events behind the scenes. Cloudwatch has a tutorial on setting up a scheduled event programmatically with Lambda and the AWS CLI here.. There are a few ways to minimize the size of the SDK used in an application. If you're using a tool like webpack or browserify,  you can require a service client directly (see the README for an example), which will only transitively require the model files for one service and thus allow webpack to exclude any unused model files from the built application bundle.\nYou can also use the SDK's Browser Builder, which will build a custom version of the SDK including only the services you select. By default, the browser builder includes all services that support CORS. I just ran the browser builder to get a default build and a build with all services, and the minified and gzipped files are 198K and 249K, respectively:\nbash\n$ ls -lh\n-rwxrwxrwx 1 root root 1.6M Jul 16 10:42 aws-sdk-2.85.0-all.min.js\n-rwxrwxrwx 1 root root 249K Jul 16 10:45 aws-sdk-2.85.0-all.min.js.gz\n-rwxrwxrwx 1 root root 1.2M Jul 16 09:48 aws-sdk-2.85.0.min.js\n-rwxrwxrwx 1 root root 198K Jul 16 10:45 aws-sdk-2.85.0.min.js.gz\nOut of curiosity, where did you get an 857MB build of the SDK? Even downloading an archive of the full repository (tests, docs, and all) from the releases tab on GitHub is only 5.5MB.. Inside of the artifact bower pulls down is a dist folder, which contains minified and unminified copies of the browser version of the SDK. They are comparable in size to what you would download from the browser builder.. @varenc I don't believe changing the default behavior would be backwards compatible. The current major version of the JS SDK did not launch with any mechanism for automatic clock skew correction, so some users may have written workarounds to handle clock skew. Enabling clock skew correction by default could cause working code that relies on external workarounds to begin failing.. systemClockOffset was introduced at the same time as correctClockSkew, and its value is updated when an operation encounters a clock skew error and compares the service's response against the client clock. There's still a chance defaulting correctClockSkew to true in an environment with unrelated clock skew correction would end up double-correcting the clock.\nThe correctClockSkew property is documented on service constructors and on AWS.config, but I don't think we have anything in the developer guide. That's something we could address without a major version bump. :). Hi @usamamashkoor,\nI don't think I can be of any help here, as it doesn't look like you're using the AWS SDK for JavaScript. You might want to try the S3 AWS Developer Forum, which is a forum dedicated to questions about using S3.. Based on the code sample provided above, it doesn't look like you need to use the AWS SDK for JavaScript, as uploading via HTTP POST with a presigned policy can be handled by browsers natively. The S3 AWS Developer Forum is a good place to ask general questions about using S3.. The node_loader.js file is only used in a Node.JS environment, and all of the modules mentioned are core modules provided by the Node.JS runtime. Can System.JS not automatically handle modules from node's standard library?. When running in a browser, you will need to tell SystemJS to load the browser distributable instead of the package's main file. Please see https://github.com/aws/aws-sdk-js/issues/1337#issuecomment-276866816 for discussion and an example.. If you add the following to your SystemJS declaration, do you still see any errors?\nSystem.config({\n    paths: {\n      // ...\n    },\n    map: {\n      // ...\n    },\n    packages: {\n      // ...\n      'aws-sdk': {\n        main: 'dist/aws-sdk.min.js',\n        defaultExtension: 'js',\n        format: 'global'\n      }\n    }\n  });\nThe main: 'dist/aws-sdk.min.js' section directs SystemJS to use the browser distributable instead of loading the SDK via the node_loader.js file. This is normally handled automatically by browserify or webpack via the browser key in the SDK's package.json.\n. The endpoint property is defined on ServiceConfigurationOptions and can be passed to a client constructor. I believe the intention behind leaving it out of what can be set via AWS.config.update was that endpoint and params only ever apply to one service, whereas setting them globally would apply the same endpoint and bound parameters to all services.. @jstradli If you just intend to use the document client, you could simplify the code above to:\n```typescript\nimport AWS = require('aws-sdk');\nlet docClient = new AWS.DynamoDB.DocumentClient( {\n    region: \"us-west-2\",\n    endpoint: \"http://localhost:8000\",\n    convertEmptyValues: true\n}); \n``. Do you see the error is you remove the line importing the S3 client? Since the compile target is set toes6and thelibdeclaration only allows access to ES5 and promises, TypeScript's generated output may make references toSymbol` that it is then unable to interpret.. The browser build only includes services that support cross origin resource sharing, which CodeBuild does not. We maintain a list of supported services with a notation on which allow CORS here. Hi @caleb0199,\nIn which region is the S3 bucket located, and are you providing that region as configuration to your S3 client?. If you restart the application in an already running container, do you see the same issue?. If the upload is timing out because network or filesystem access is considerably slower on a fresh container, there might be a workaround in docker for that scenario.. Hi @oaleynik,\nI was able to reproduce the issue as described, but I believe this is a result of how strings are represented in JavaScript vs how they are interpreted over HTTP. It's true that the \u00a9 character exists both in the ISO-8859-1 and UTF-8 character sets, but it is encoded differently in them, with the former representing it as 0xa9 and the latter as [0xc2, 0xa9]. When the SDK is signing a request, it uses Node.JS's default string encoding (UTF-8) to determine the byte buffer to sign. Node does not support ISO-8859-1 encoding:\n```\n\nBuffer.from('\u00a9')\n\nBuffer.from('\u00a9', 'iso-8859-1')\nTypeError: \"encoding\" must be a valid string encoding\n    at fromString (buffer.js:264:13)\n    at Function.Buffer.from (buffer.js:150:12)\n    at repl:1:8\n    at ContextifyScript.Script.runInThisContext (vm.js:44:33)\n    at REPLServer.defaultEval (repl.js:239:29)\n    at bound (domain.js:301:14)\n    at REPLServer.runBound [as eval] (domain.js:314:12)\n    at REPLServer.onLine (repl.js:433:10)\n    at emitOne (events.js:120:20)\n    at REPLServer.emit (events.js:210:7)\n```\n\nIf the headers being sent include any characters outside of the US-ASCII character set, I would recommend using the extended encoding scheme discussed in RFC-6266. This allows for the encoding of arbitrary UTF-8 sequences and allows both the SDK and S3 to correctly determine the byte sequence to sign.\nFor the particular filename provided in the example, this would look like \"attachment; filename*=UTF-8''1%20%C2%A9.jpg\". Generically, this could be determined from any filename X with the following function:\nfunction getContentDisposition(filename) {\n    return `attachment; filename*=UTF-8''${encodeURIComponent(filename)}`;\n}. You're right -- decoding the header string as 'latin1' isn't much help, as the signature will still need to match the one calculated by S3. I believe the safest way to guarantee signatures will match is to only use US-ASCII in header values and to use extended filename*=UTF-8'' encoding for any string that could contain characters outside of the US-ASCII set.. Hi @niftylettuce,\nAre you running the snippet above in an async function? async functions implicitly return a promise, so throwing an error (as is done on the third line of the snippet posted above) inside of one will result in a promise rejection. \nRunning the following script will result in a TypeError being thrown, as the SDK does not use promises internally unless a .promise() or similar method is called:\n```javascript\nconst AWS = require('aws-sdk');\nconst cf = new AWS.Cloudfront({region: 'us-east-1'});\n```\nconsole output in Node 8.1.3:\n```\n$ node .sandbox/promiseRejection.js \nconst cf = new AWS.Cloudfront({region: 'us-east-1'});\n           ^\nTypeError: AWS.Cloudfront is not a constructor\n    at Object. (/~/Workspace/aws-sdk-js/.sandbox/promiseRejection.js:3:12)\n    at Module._compile (module.js:569:30)\n    at Object.Module._extensions..js (module.js:580:10)\n    at Module.load (module.js:503:32)\n    at tryModuleLoad (module.js:466:12)\n    at Function.Module._load (module.js:458:3)\n    at Function.Module.runMain (module.js:605:10)\n    at startup (bootstrap_node.js:158:16)\n    at bootstrap_node.js:575:3\n```\n\nThe TypeError can be caught in a standard try/catch block:\n```javascript\nconst AWS = require('aws-sdk');\ntry {\n  const cf = new AWS.Cloudfront({region: 'us-east-1'});\n} catch (e) {\n  console.log('Caught an error: ' + e.message);\n}\n``` \nconsole output:\n$ node .sandbox/promiseRejection.js \nCaught an error: AWS.Cloudfront is not a constructor\n\nThe TypeError is converted to an unhandled promise rejection when wrapped in an async function:\n```javascript\nconst AWS = require('aws-sdk');\n(async () => {\n  const cf = new AWS.Cloudfront({region: 'us-east-1'});\n})();\n```\nconsole output:\n$ node .sandbox/promiseRejection.js \n(node:53125) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 1): TypeError: AWS.Cloudfront is not a constructor\n(node:53125) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\n\nThe TypeError can be caught with try/catch, even in an async context when static values are prefixed with the await keyword:\n```javascript\nconst AWS = require('aws-sdk');\n(async () => {\n  try {\n    const cf = await new AWS.Cloudfront({region: 'us-east-1'});\n  } catch (e) {\n    console.log('Caught an error: ' + e.message);\n  }\n})();\n```\nconsole output:\n$ node .sandbox/promiseRejection.js \nCaught an error: AWS.Cloudfront is not a constructor. While we're investigating if this is a bug in the SDK or an unrelated promise rejection in code calling the SDK, I'm going to remove the **CRITICAL BUG** text from the issue title.. The SDK has built-in clock skew correction for use in browsers and in mobile applications. (On a server, you would want to rely on a system clock synchronized over NTP, but that's not an option on a client device.)\nCould you try setting the correctClockSkew configuration option to true and see if that takes care of the errors?. Is the CognitoUtil.getCognitoUser().getSession() snippet using the Cognito Identity SDK?. Hi @eladidan,\nThe SDK makes this feature available as the AWS.TemporaryCredentials class. . I believe this duplicates AWS.TemporaryCredentials.. Closing as duplicate of #1662 . Closing as duplicate of #1660 . Should have closed #1660 instead of this.. Hi @mbp,\nThe SDK will retry throttling errors up to three times by default, so your callback would only be invoked if the third retry failed. If a retry succeeds, no error would be passed to your callback.. I believe we can update TemporaryCredentials to limit itself to a single in-flight request (as we do with EC2 instance metadata service credentials).. Hi @akaNightmare,\nYou can do this by setting service-specific configuration in the AWS.config object, e.g., by setting the ses property. There's a property on AWS.config for every service, and the names can be found in the Service-Specific Configuration section of the AWS.config documentation page.. Hi @pruhstal,\nYou would need to provide AWS credentials, though you should not share your production credentials. We have a guide on using Cognito or Web Identity in the developer guide.. The only way I know of to do so is to use presigned POST forms (as you describe in the issue description) or presigned PutObject URLs (which are more limited than- and unlikely to avoid the timeout issues encountered with- POST forms).\nI would recommend opening a support ticket with S3 or posting a question on the S3 developer forum, as it really sounds like using boto3's generate_presigned_post  is the right technical solution for you.. Hi @tvald,\nThe AWS SDK for JavaScript API documentation for service clients is largely derived from the service's API documentation, which means that the docs sometimes describe the on-the-wire format of data rather than the data expected by the SDK. (Note the similarity between the linked document and https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_UpdateDistribution.html.)\nAs you point out, the SDK expects to receive (and will return) JavaScript types rather than XML. I'll reach out to the service team to ask them to update the documentation.. According to the shared service model for CloudFront, IfMatch is an optional field. I'll see about getting the shared model updated, but there may be a scenario in which an UpdateDistribution operation can be executed without an IfMatch value.\nIt looks like the same issue was raised in reference to the AWS CLI on https://forums.aws.amazon.com/thread.jspa?threadID=197012. The CreateBucketConfiguration parameter will be populated for your automatically if it is not supplied. \nDo you see any issues if you shorten the above code snippet to something resembling the following code?\njavascript\nconst createResponse = await s3\n    .createBucket({\n        Bucket: awsprovider.deploymentBucket,\n    })\n    .promise();\nAs you point out, if you supply the CreateBucketConfiguration parameter to the SDK, you will need to omit the LocationConstraint for buckets in us-east-1.. I'm still not sure why you need to specify the CreateBucketConfiguration at all. The SDK will automatically populate that parameter if it is not supplied.. The default behavior will create a bucket in the region of the S3 client. You would only need to specify the parameter if you want to create a bucket in a location other than the one to which the CreateBucket request was sent.. Hi @Lanceshi2,\nCan you provide a bit more information? Which service are you using, which operation are you calling?. It looks like signedFormData.fileURL = data.Location; could be triggering the error in the issue's title; if the callback is called with an error, data will be undefined. Can you check and log err in the callback to send?. The only usage of addEventListener in the SDK is the HTTP handler calling addEventListener on an XMLHttpRequest object, which should be supported in IE 11. It's possible that an error is being thrown in a callback supplied to the SDK, as the SDK would catch and rethrow those.. I was referring to the callback function. My hunch is that the $(document).trigger(\"AmazonStatusUpdateEvent\", [\"Uploading\", file.name, percent, signedFormData]); line is causing an error to be thrown.. Thanks for your contribution!. @AllanFly120 when you review, you can ignore the commit with the message \"Un-coffeescript test/services/s3.spec.js.\" That commit is huge and consists entirely of removing artifacts of the coffeescript compilation process from test/services/s3.spec.js.. @colinhemphill Appeasing CodeCov turned out to be more involved than I thought it would be. Thanks for the original work (and identifying the issue, too). Hi @kevinbror,\nThe request ID and extended request ID are both sent to clients as a header. If the bucket's CORS configuration does not expose these headers, then client applications will not be able to access them. The S3 guide to CORS configuration includes an example that lists the correct headers as exposed:\nxml\n<CORSConfiguration>\n <CORSRule>\n   <AllowedOrigin>http://www.example.com</AllowedOrigin>\n   <AllowedMethod>PUT</AllowedMethod>\n   <AllowedMethod>POST</AllowedMethod>\n   <AllowedMethod>DELETE</AllowedMethod>\n   <AllowedHeader>*</AllowedHeader>\n   <MaxAgeSeconds>3000</MaxAgeSeconds>\n   <ExposeHeader>x-amz-server-side-encryption</ExposeHeader>\n   <ExposeHeader>x-amz-request-id</ExposeHeader>\n   <ExposeHeader>x-amz-id-2</ExposeHeader>\n </CORSRule>\n</CORSConfiguration>. DynamoDB's page on allowable attribute names in expressions indicates that this is expected behavior:\n\nYou can use any attribute name in a document path, provided that the first character is a-z or A-Z and the second character (if present) is a-z, A-Z, or 0-9. If an attribute name does not meet this requirement, you will need to define an expression attribute name as a placeholder. For more information, see Expression Attribute Names.\n\nUsing the ExpressionAttributeNames field to avoid including reserved words or invalid characters in expressions is the correct resolution.. @dumbird Have you reached out to S3 support regarding the internal error? That error is not originating in the SDK but is coming directly from S3.. Were you able to identify any pattern to which requests did not have their callbacks invoked? I'm having trouble reproducing, and a simple repro case would really help identify any underlying issues.. This looks related to #1313.\nAs a workaround, headers can be added to the request after it has been signed. If the upload is being sent from a browser, CacheControl and ContentDisposition are supported in POST uploads to S3. The SDK's S3 client includes a helper method for creating and signing POST upload policies.. It looks like either changing single part uploads to match multipart uploads or vice versa would be a change for some users. @zerojuan am I correct in assuming that this is an aesthetic issue rather than a technical one?. @itchingpixels Which typings in particular are missing? The React Native SDK should have the same level of typescript definition coverage as the standard browser and Node distributables.. Hi @mveera21,\nBased on the discussion in https://forums.aws.amazon.com/thread.jspa?threadID=263089&tstart=0, it sounds like Lambda is rolling out an update to address the issue.. @hgonzalez94 If you can post the workaround as a gist, that would be awesome. I know sending HTTP messages with binary payloads in earlier versions of React Native could be tricky, so I'm sure others who are unable to upgrade would love to see a stable way to do so.\nPlease feel free to reopen if you have any questions or concerns.. There seems to be a known bug with Chrome 53's handling of certificate transparency, and accessing URLs affected by the certificate transparency issue via an XmlHttpRequest can cause a net::ERR_INSECURE_RESPONSE error to be shown.\nFrom a discussion on the Google product forums, it sounds like the issue may be resolved by restarting Chrome, or you may need to update to the latest version. In any case, this isn't something the SDK would be able to work around, as JS code relies on the user's browser to handle TLS and certificate chain management.. @leantide The list of reserved words only restricts which strings may be used as unescaped attribute names inside of an expression. The only restriction applied to AttributeNames (outside of the context of their direct use in expressions) is that they must be 255 or fewer characters in length. DynamoDB has a good overview of naming rules and restrictions.. @PrimeObjects Can I ask where you picked up the Content-Encoding: 'base64' approach from? I've seen that used in a number of React Native-related issues, but my understanding is that it's disallowed by the HTTP spec. Is it used in official RN examples or a popular tutorial?. A fix for this issue was included in https://github.com/aws/aws-sdk-js/commit/3ffe62255d99c09680f519bc40ac5b04b0cb220b, which went out in 2.120.0. I believe you will need to specify a filter using the equals operator (=) (as discussed in the EC2 forum).\nJust FYI, StackOverflow is a better forum for this kind of usage question. They have an excellent tagging system and a more targeted search feature, the upshot of which is that questions and answers are more discoverable there than they are in the SDK's issue queue. . Hi @mgeissen,\nAre you sure all the tools are using the same region? The ListRepositories operation will only return repositories located in the region to which the API call was sent, and the code samples above appear to be deducing the AWS region from the environment. Can you check to see what values are used for the AWS_REGION and AWS_DEFAULT_REGION environment variables, as well as if a region is specified in the ~/.aws/config file?\nFor example, this is the output I received when trying to reproduce this issue:\n$ node .sandbox/issues/1727.js \n{ ConfigError: Missing region in config\n    at Request.VALIDATE_REGION (/path/to/aws-sdk-js/lib/event_listeners.js:91:45)\n    at Request.callListeners (/path/to/aws-sdk-js/lib/sequential_executor.js:105:20)\n    at callNextListener (/path/to/aws-sdk-js/lib/sequential_executor.js:95:12)\n    at /path/to/aws-sdk-js/lib/event_listeners.js:85:9\n    at finish (/path/to/aws-sdk-js/lib/config.js:315:7)\n    at /path/to/aws-sdk-js/lib/config.js:333:9\n    at SharedIniFileCredentials.get (/path/to/aws-sdk-js/lib/credentials.js:126:7)\n    at getAsyncCredentials (/path/to/aws-sdk-js/lib/config.js:327:24)\n    at Config.getCredentials (/path/to/aws-sdk-js/lib/config.js:347:9)\n    at Request.VALIDATE_CREDENTIALS (/path/to/aws-sdk-js/lib/event_listeners.js:80:26)\n  message: 'Missing region in config',\n  code: 'ConfigError',\n  time: 2017-09-20T16:42:01.362Z }\n$ AWS_REGION=us-west-2 node .sandbox/issues/1727.js \n{ repositories: \n   [ { repositoryName: 'myRepo',\n       repositoryId: 'a_UUID' } ] }\n$ AWS_REGION=us-west-1 node .sandbox/issues/1727.js \n{ repositories: [] }\nThe .sandbox/issues/1727.js is:\n```javascript\nconst AWS = require('aws-sdk');\nconst cc = new AWS.CodeCommit();\ncc.listRepositories((err, data) => {\n  if (err) {\n    console.error(err);\n  } else {\n    console.log(data);\n  }\n});\n``. You're absolutely right; this note should only appear for non-streaming binary properties. In the screenshot below,Bodyis streaming andSSECustomerKey` is not:\n\n. Hi @sgtoj,\nThe classes and namespace you mention are considered an implementation detail of the SDK. We generally only provide typings for the files that are part of the SDK's public-facing interface.. Is the application where this behavior was observed using the Amazon Mobile Analytics JavaScript SDK? The SDK for JavaScript does not use the AWSMobileAnalyticsStorage storage key, but it appears that the library linked to above does. In that case, I don\u2019t think there\u2019s anything this library could do. Have you opened an issue with the mobile analytics SDK?. https://github.com/aws/aws-sdk-mobile-analytics-js. Can you share the code that triggers the issue?. From what I can tell from the Electron releases page and the Electron releases published to GitHub, there has not yet been a stable release of 1.8.0 yet, and some instabilities and crashes are to be expected. Have you observed any crashes with a 1.6.x or 1.7.x release? Does the crash occur with the latest release in the 1.8.x series (1.8.2-beta.1), and, if so, have you logged an issue with the electron team?. @ajkerr I believe the correct solution is to set the Content-Length header before passing the request to the signer. The SDK's Node HTTP handler is just passing the body to request.end, and something further down the stack is not sending the body for DELETE requests unless Content-Length is set.\nIf the SDK included a data-plane client for Elasticsearch Service, we would be able to handle this specific case transparently for users. However, SDK internals are being called directly in this case, so the calling code will need to ensure that a Content-Length header is set for DELETE requests with bodies.. I did a bit more research to make sure this wasn't a bug and found a justification for the behavior in the HTTP/1.1 spec. Section 4.3 reads in part:\n\nThe presence of a message-body in a request is signaled by the inclusion of a Content-Length or Transfer-Encoding header field in the request's message-headers.\n\nWithout either a Content-Length or Transfer-Encoding header, the delimiter that normally signals the end of the headers (\\r\\n\\r\\n) instead signals the end of the message.. Hi @adjourn,\nThe DAX SDK for Node.JS does not run in browsers or in some end-of-life versions of Node supported by the AWS SDK for JavaScript.\nClosing, as the DAX SDK for Node.JS is now available via NPM.. Based on your findings, @uzimith, I would say that the typings shipped with the SDK are not directly compatible with the structure of events as passed to lambda functions. They are, however, correct with regards to the CloudWatch APIs, which is what our SDK must keep accurate typings for. I would recommend raising an issue on the AWS Developer Forum for CloudWatch, the AWS Developer Forum for Lambda, and/or the AWS Developer Form for SNS, but this doesn't look like something we could accommodate in the SDK's type definitions.\nPlease feel free to reopen or continue the discussion if you have any questions or concerns.. There's no way that coverage number can be right given the scope of this PR. :shipit: . Hi @sabrehagen,\nThis seems like a useful addition to the SDK, but we tend to shy away from adding new environment variables (particularly ones prefixed with AWS_) unless there is a plan to add support across all SDKs. Let me shop this around the other SDK teams and see if they want to offer similar support for an AWS_DEBUG environment variable.. Hi @sabrehagen,\nWould you be open to choosing a new mechanism for enabling debug mode? I would be much more open something that makes it clear that the mechanism is specific to this library or to Node applications in general.\nI know many libraries will enable debug mode if process.env.NODE_ENV !== 'production', but I'm not sure if suddenly supporting that configuration setting and emitting a bunch of log messages would be reasonable behavior for a library frequently used as a dependency of other libraries.. @sabrehagen My concern is that we cannot offer a simple on/off experience in all AWS SDKs, so using the AWS_ prefix implies a level of cross-language support that may not be possible. How about AWSJS_DEBUG?. I'll try to get this prepped for merge today. In the meantime, you can use an environment variable to set the debug level of your code by setting the logger configuration parameter programmatically:\njavascript\nconst AWS = require('aws-sdk');\nAWS.config.update({ logger: process.env.AWSJS_DEBUG ? console : undefined });. This was merged in as part of #1783. Thanks for your contribution!. You would need a different pre-signed URL for each page of results. Each request for a page following the first will include a \"NextMarker\" parameter that is part of the URL that gets signed.. Thanks for your contribution!. When creating the S3 client, you can direct the client not to use the bucket name in the URL by setting s3ForcePathStyle to true. However, the SDK should probably detect when an IP address is used as the endpoint and set s3ForcePathStyle automatically.. Can you take a peek at the line mentioned in the top line of the stack trace inside of the project's node_modules folder (/api-project/node_modules/aws-sdk/lib/sequential_executor.js:115)? That line should read doneCallback.call(self, error);, but the error reported indicates that the file as downloaded to disk is missing a character (doneCallback.cal is invoked instead of doneCallback.call).. Closing, as I can't find a version of the SDK we've shipped in which doneCallback.cal was invoked. Please reopen or comment if you encounter this issue again.. Since the encryption is happening before the value is passed to the SDK, my hunch is that the ciphertext cannot be accurately represented by the latin1 character set (Node's binary encoding target is just an alias for latin1). If you want to send the raw bytes of the ciphertext, could you try using a buffer?. Can you tell me a bit more about options.content? (Is it a stream? A string? A buffer?) Since the encryption is happening before the data is passed to the SDK, I don't believe there's an issue in putObject.. I'm unable to reproduce any error when operating directly on binary data (i.e., Buffer objects). This is the script I'm using to try to reproduce the error:\n```javascript\nconst assert = require('assert');\nconst Buffer = require('buffer').Buffer;\nconst crypto = require('crypto');\nconst AWS = require('aws-sdk');\nconst plaintext = crypto.randomBytes(4 * 1024 * 1024 + 1);\nconst iv = crypto.randomBytes(16);\nconst key = crypto.randomBytes(32);\nconst cipher = crypto.createCipheriv('aes-256-ctr', key, iv);\nlet ciphertext = cipher.update(plaintext);\nciphertext = Buffer.concat([ciphertext, cipher.final()]);\nassert.strictEqual(ciphertext.byteLength, plaintext.byteLength);\nconst Bucket = '3f929dc0-997b-4fde-a4e5-7e83b453aa60';\nconst Key = 'encrypted.ext';\nconst s3 = new AWS.S3({region: 'us-west-2'});\ns3.putObject({Bucket, Key, Body: ciphertext}).promise().then(() => {\n  console.log('upload complete!');\n  s3.getObject({Bucket, Key}).promise().then(result => {\n    console.log('download complete!');\n    assert.ok(result.Body.equals(ciphertext));\n    console.log('ciphertext not corrupted');\nconst decipher = crypto.createDecipheriv('aes-256-ctr', key, iv);\nlet decrypted = decipher.update(result.Body);\ndecrypted = Buffer.concat([decrypted, decipher.final()]);\nassert.ok(decrypted.equals(plaintext));\nconsole.log('plaintext successfully recovered');\n\n});\n});\n```\nIf, however, I attempt to use latin1 text encoding via its alias, 'binary', then I am unable to decrypt the ciphertext even before it is sent to S3. I believe I'm running into the same issue you describe by running the following script:\n```javascript\nconst assert = require('assert');\nconst Buffer = require('buffer').Buffer;\nconst crypto = require('crypto');\nconst plaintext = crypto.randomBytes(4 * 1024 * 1024 + 1);\nconst iv = crypto.randomBytes(16);\nconst key = crypto.randomBytes(32);\nconst cipher = crypto.createCipheriv('aes-256-ctr', key, iv);\nlet ciphertext = cipher.update(plaintext, 'binary');\nciphertext += cipher.final('binary');\nconst decipher = crypto.createDecipheriv('aes-256-ctr', key, iv);\nlet decrypted = decipher.update(ciphertext, 'binary');\ndecrypted = Buffer.concat([decrypted, decipher.final()]);\nassert.ok(decrypted.equals(plaintext));\n```\nBased on these test cases, I do not believe there is any issue in the PutObject operation. Instead, I believe you are seeing the result of encoding errors that can be reproduced without using S3 or the AWS SDK.. That sounds reasonable, but I'm not sure how removing the dist/* files could be done in a backwards-compatible way.. That is a possibility, but we would need to formulate a good long-term plan for keeping the two packages in sync. aws-sdk-node would essentially have the same maintenance overhead as a new major version.. The SDK will retry ThrottlingException errors, but there is a maximum number of times that will be retried before the error is surfaced. By default, if a Glacier client encounters a throttling error on its 3rd retry, then the request will fail. You can adjust the number of retries that will be attempted by setting the maxRetries configuration parameter: \njavascript\nconst glacier = new AWS.Glacier({\n    region: 'us-west-2',\n    maxRetries: 5\n});. Callbacks are only invoked when an operation succeeds, fails with a terminal error, or when all retries have been exhausted. If you do receive a ThrottlingException error in a callback, it likely means that the operation was throttled several times and indicates that your application is exceeding the Glacier API limits.. FIFO queues will reject DeleteMessage requests when the receipt handle is expired, which means that handles with a VisibilityTimeout of 0 cannot be deleted. Please see here for more discussion of this.\nMy understanding from the SQS developer guide is that 12 hours is the longest possible message timeout allowed by SQS. If the visibility timeout has elapsed, then the message may have been delivered to and acted on by another recipient.. The other recipient would be able to delete the message up to 12 hours after they received it. VisibilityTimeout only refers to the lifetime of the receipt handle (i.e., the duration for which the message should be considered \"in flight\" and not sent in response to other ReceiveMessage operations). In addition, each queue has a maximum retention period -- the amount of time messages should be allowed to remain in the queue. By default, all queues have a maximum retention period of four days, though this can be configured to any duration between one minute and fourteen days. The queue's retention period is not related to any given receipt handle's visibility timeout.\nThe SQS Developer Guide contains a brief guide to message lifecycles that probably explains this better.\n. I\u2019ll reach out to the ElasticBeanstalk team to get the inconsistency addressed.. I heard back from someone on the Elastic Beanstalk team, and the properties listed should not appear on the console listed for platforms released in the last 2-3 years. Which platform is the application you saw this console for running on?. The response I received from Elastic Beanstalk is this:\n\nAWS_SECRET_KEY and AWS_ACCESS_KEY_ID are deprecated on AWS Elastic Beanstalk platforms. You won't find those environment variables by default on any platform published in the last 2-3 years. \nIt's not recommended that customers continue to use those variables to establish their AWS credentials on an EC2 host in an Elastic Beanstalk environment.\n\nSince the AWS_SECRET_KEY environment variable is considered a legacy feature by Elastic Beanstalk, I don't think there's a compelling reason to add support for it to the AWS SDK.\nThe Elastic Beanstalk team now recommends using IAM roles to provide credentials to your application.\n. Hi @MatheusParanhos,\nThe documentation you're looking for can be found here. The SDK supports React Native, so you would be able to use AWS.CognitoIdentity to interact with the AWS Cognito Identity service in the same way in React Native as you would in a browser or in a Node.JS application. API documentation for the AWS SDK for JS's Cognito Identity service client can be found here.. It looks like CloudFront is forwarding the request path (including the signature parameters in the query string) on to S3 but overwriting the host. This is expected behavior for CloudFront \u2014 it\u2019s meant to act as a caching proxy whose DNS record is distinct from that of the backend to which it forwards requests \u2014 so I\u2019m not sure it would be possible to access a presigned S3 URL through a CloudFront web distribution.\nCloudFront has its own presigning mechanism that you might want to look into. The developer guide for the feature can be found here, and the API docs for the AWS SDK for JavaScript\u2019s CloudFront presigner can be found here. Because CloudFront presigning uses RSA private keys, it is only available server-side and is not supported in the browser SDK.. Thanks for the deep dive on that, @sqlbot! It didn't occur to me that even if you could get S3 presigned URLs to work, using them would make nearly all requests uncacheable.\n@nalbion, I would strongly recommend using CloudFront's presigning mechanism. In addition to avoiding the complexity described above, CloudFront will be able to cache objects based on their identifiers (bucket name + key name) rather than on the exact signature used.. Hi @christian-bromann,\nThe AWS SDK for JS returns regular JS objects as operation results, so you would just access result.LoadBalancers rather than calling a result.GetLoadBalancers method.\nPlease see http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/ELBv2.html#describeLoadBalancers-property for an example of how to use the elbv2.describeLoadBalancers operation.. The semantics of ES modules are different enough from script-based module formats like CommonJS, UMD, and AMD that I don't think we could automate a conversion of the SDK from its current format to an ESM-based format. Since we release several times a week, if the conversion cannot be automated, then I'm not sure it's a feasible solution.\nThe conversion from ESM to CommonJS, however, is fairly straightforward, so if the SDK were written following ES module semantics, we could release both versions. That, however, would require a major version bump.. The parameters you're using should be working, so the error message might be being provided in error. Trying to reproduce with parameters similar to those described, I consistently get back a successful response that includes the object ETag and the id of the KMS key used to encrypt the object.\nCould you open a support ticket with S3? They would be able to provide more detailed information about a particular request or account.. Understood. When using server-side encryption with KMS keys or S3-managed keys, you only need to supply the Bucket and Key to getObject. The data is encrypted at rest in S3, but they have the information they need to decrypt it when you call getObject.\nThe SSECustomerAlgorithm and SSECustomerKey are used for objects encrypted with customer-managed keys, which S3 does not store.. Hi @mmorearty,\nSorry for the confusion; the file I mentioned is actually called /ts/credentialproviderchain.ts. The SDK's tests for the included type definitions can be found in the repository's ts folder, which contains TypeScript files using the SDK that are compiled (but not executed) as part of npm test.\nTo test this change, you would want to add an invocation of CredentialProviderChain without providing any arguments to /ts/credentialproviderchain.ts.. Thanks for your contribution!. @dijonkitchen You're right, the documentation tool should be updated to better support markdown or use its own file extension. Until that change lands, however, this PR would add a broken link to our API documentation.. Hi @CanGokdere,\nIt looks like we're hoisting headers to the query string after the request has already been signed, which invalidates a V4 signature. This appears to be permitted in V2 presigned URLs, but we'll need to modify the presigner to properly account for SigV4's requirements.. Hi @CanGokdere,\nI was able to successfully use pre-signed URLs to execute a PutObject operation using the following code that relies on the master branch of the SDK (the update has not yet been included in a release):\n```javascript\nconst AWS = require('aws-sdk');\nconst uuidv4 = require('uuid/v4');\nconst https = require('https');\nconst url = require('url');\nconst s3 = new AWS.S3({\n    region: 'us-west-2',\n    signatureVersion: 'v4'\n});\nconst presigned = s3.getSignedUrl('putObject', {\n    Bucket: bucketName,\n    Key: uuidv4(),\n    ContentDisposition: 'Attachment'\n});\nconsole.log(presigned);\nconst options = url.parse(presigned);\noptions.method = 'PUT';\noptions.headers = {'content-disposition': 'Attachment'};\nconst stream = https.request(options);\nstream.on('response', (res) => {\n    console.log(res.statusCode);\n    res.pipe(process.stdout);\n});\nstream.end('body');\n```\nPlease note that you can add a Content-Disposition header after signing.. Hi @tomgallagher,\nThe list of events emitted by AWS.Request (the class that has the createReadStream method) can be found here. The object metadata returned with GetObject are sent as headers, so you would want to attach a handler to the 'httpHeaders' event.. Thanks for your contribution!. Merged in as part of #1813 . Hi @rainnaren,\nCan you provide a bit more context on where you're running into this error and under what circumstances? Please include the runtime used, the version of that runtime, and which tools (if any) you're using to bundle.. I'm not able to reproduce using the SDK we host at https://sdk.amazonaws.com/js/aws-sdk-2.166.0.min.js. The following code logs an Invalid Access Token error (as would be expected if the code loads properly and is able to send a request to Cognito):\nhtml\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <script src=\"https://sdk.amazonaws.com/js/aws-sdk-2.166.0.min.js\"></script>\n</head>\n<body>\n    <script>\n        const cognitoIdp = new AWS.CognitoIdentityServiceProvider({\n            region: 'us-west-2'\n        });\n        cognitoIdp.getUser({AccessToken: \"fake-access-token\"}, console.log);\n    </script>\n</body>\n</html>\nIt sounds like you might be having issues bundling the SDK with the rest of the application hosting it. You can build a single file containing the AWS SDK and only the services you need to use at https://sdk.amazonaws.com/builder/js/. Hi @abjsaha,\nS3 is an XML-based service, so I believe the error you're seeing is originating from the use of JSON.parse in the code sample pasted above. Are you sure the S3 object contains valid JSON?. Can you post a sample of what a body looks like after it has been parsed and split? I saw in the code sample that you posted that data.Body.toString('ascii').split(\"\\n\") is being called, and it's possible the data was not saved using ascii encoding. If you used the SDK to save a JSON string, we assume UTF-8 encoding.. Hi @danieljoppi and @hakanson,\nThe result you encountered is a false positive. The AWS SDK for JavaScript uses Node's crypto module (polyfilled with crypto-browserify in the browser SDK) to perform MD5 and SHA-256 hashing when signing requests and does not use it to directly perform any encryption, decryption, or random generation. The only time Math.random could be called is when UUIDs are generated for use as idempotency tokens in browsers that do no support WebCrypto's RandomSource.getRandomValues method.\nClosing as a duplicate of #1602. Hi @Hemanshu1belani,\nThe browser SDK does not support use in a web worker environment.. Hi @paulfryer,\nThe copy of the SDK provided by Lambda is currently version 2.143.0, whereas support for AWS.WorkMail was only added in version 2.169.0.\nYou can bundle a newer version of the SDK in your Lambda function by following the instructions here.. Hi @TangiX-DIY,\nAs discussed in #1853, the reported result is a false positive. The SDK does not use that part of crypto-browserify in any environment, and crypto-browserify is not used at all in Node.\nClosing as a duplicate of #1853 and #1602. Hi @Manan0510,\nYou'll need to raise this issue with on the AWS SDK for iOS repository; this repo only houses the AWS SDK for JavaScript.. @eyalcohen7 As far as I know, there's nothing in the SDK that would modify the range header, so my hunch is that the modification is being performed by Chrome's HTTP agent. Do you see Chrome sending a GET request with the truncated range and an If-None-Match header when the object is already in Chrome's HTTP cache?. That seems like a bug in Chrome... if you requested the first 100,000 bytes of a file, the fact that the first 57,043 bytes have not changed does not necessarily mean the object is still 57,043 bytes long. Let me see if there's a related bug report in Chrome (which might list some workarounds).\nFor S3 specifically, if a file was uploaded using a multipart upload, you could download those parts individually instead of downloading a range. You would need to provide a PartNumber parameter as described on https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#getObject-property The numbered object parts are the same parts that were uploaded with each UploadPart operation.. I wasn't able to find a relevant bug in Chrome, but do you see similar behavior in other browsers?. Hi @kt3k,\nI understand that new development is occurring on the nyc project, but istanbul has been working fine with the SDK for some time. I'm not sure I see enough value in upgrading this build-time only dependency to justify auditing the differences between nyc's dependency graph and that of istanbul.. Hi @kt3k,\nI understand that new development is occurring on the nyc project, but istanbul has been working fine with the SDK for some time. I'm not sure I see enough value in upgrading this build-time only dependency to justify auditing the differences between nyc's dependency graph and that of istanbul.. The main thing I would need to audit is the license on each package. I appreciate the effort you've put into this, but for backwards compatibility, this library can never use ES6-only language features. I'll leave this open in case another reason to migrate comes up, but we're not likely to make this change in the current major version of the SDK.. The main thing I would need to audit is the license on each package. I appreciate the effort you've put into this, but for backwards compatibility, this library can never use ES6-only language features. I'll leave this open in case another reason to migrate comes up, but we're not likely to make this change in the current major version of the SDK.. Hi @MeBNoah,\nListObjectsV2 is a pageable operation, so you can traverse all the objects in a bucket using the eachPage method on the request returned by s3.listObjectsV2(params).. Hi @MeBNoah,\nListObjectsV2 is a pageable operation, so you can traverse all the objects in a bucket using the eachPage method on the request returned by s3.listObjectsV2(params).. I can forward this question on to the service team, but the fastest way to get an answer would be to open a ticket with AWS support.\nJust to rule out any client side issues, could you post a code snippet so that I can attempt to reproduce the issue and make sure the request being sent is well-formed?. I can forward this question on to the service team, but the fastest way to get an answer would be to open a ticket with AWS support.\nJust to rule out any client side issues, could you post a code snippet so that I can attempt to reproduce the issue and make sure the request being sent is well-formed?. According to the service team, this issue has been resolved. Please feel free to reopen if you have any questions or concerns.. According to the service team, this issue has been resolved. Please feel free to reopen if you have any questions or concerns.. DynamoDB is not a REST service, so you will need more than the URL to send a request. The service uses a specific header (X-Amz-Target) to route requests, and its absence is likely why a 404 error is being returned. Presigning can be a bit tricky, as SigV4 requires that the exact request body be factored into the signature, though some services (e.g., S3 and Lex) allow a special signature algorithm for use with arbitrary request bodies. (All DynamoDB operations use the POST method and require a request body, including GetItem and Query/Scan operations.) I would need to do some testing to determine if DynamoDB supports that algorithm.\nHave you considered using Cognito to generate credentials with limited credentials in the browser? . DynamoDB is not a REST service, so you will need more than the URL to send a request. The service uses a specific header (X-Amz-Target) to route requests, and its absence is likely why a 404 error is being returned. Presigning can be a bit tricky, as SigV4 requires that the exact request body be factored into the signature, though some services (e.g., S3 and Lex) allow a special signature algorithm for use with arbitrary request bodies. (All DynamoDB operations use the POST method and require a request body, including GetItem and Query/Scan operations.) I would need to do some testing to determine if DynamoDB supports that algorithm.\nHave you considered using Cognito to generate credentials with limited credentials in the browser? . The IdentityPoolId would be sufficient to access DynamoDB only if you grant access to unauthenticated identities and if the IAM role assumed for unauthenticated users has read permissions on the DynamoDB table. There's a good guide to setting up an identity pool and configuring how you want that pool to authenticate users available here. The IdentityPoolId would be sufficient to access DynamoDB only if you grant access to unauthenticated identities and if the IAM role assumed for unauthenticated users has read permissions on the DynamoDB table. There's a good guide to setting up an identity pool and configuring how you want that pool to authenticate users available here. Hi @brettneese,\nUnfortunately, the generated example code example referred to only knows about the data types required for each property of an operation's input and output; it is not aware of any semantic restrictions on what data those properties may contain. \nEach property is documented below the example, however, which is where more thorough information about properties is communicated. The Payload property is documented as \"JSON that you want to provide to your Lambda function as input.\"; it might be helpful to link directly to this documentation from the property name in the generated example.. Hi @brettneese,\nUnfortunately, the generated example code example referred to only knows about the data types required for each property of an operation's input and output; it is not aware of any semantic restrictions on what data those properties may contain. \nEach property is documented below the example, however, which is where more thorough information about properties is communicated. The Payload property is documented as \"JSON that you want to provide to your Lambda function as input.\"; it might be helpful to link directly to this documentation from the property name in the generated example.. The integration tests run without issue in IE, Edge, Chrome, and Firefox on my computer, though there is an unrelated failure running the unit tests in IE and Edge. I saw the same error with master, with this PR, and with version 2.177.0 (the last SDK release to include a full version of crypto-browserify), so this is not a new regression but something we should look into. I opened #1881 and assigned it to myself.. The integration tests run without issue in IE, Edge, Chrome, and Firefox on my computer, though there is an unrelated failure running the unit tests in IE and Edge. I saw the same error with master, with this PR, and with version 2.177.0 (the last SDK release to include a full version of crypto-browserify), so this is not a new regression but something we should look into. I opened #1881 and assigned it to myself.. Hi @erwineverts,\nDynamoDB's Scan operation does not have a PageSize input parameter; Limit is used to control the size of pages returned. The CLI aliases this parameter to --page-size, but the API defines it as Limit.\n. Hi @erwineverts,\nDynamoDB's Scan operation does not have a PageSize input parameter; Limit is used to control the size of pages returned. The CLI aliases this parameter to --page-size, but the API defines it as Limit.\n. The only workaround would be to continue paging through scan results until you have gotten all the results you need. \nMy understanding is that what Limit technically does is restrict the number of reads performed so that you consume a bounded number of read capacity units during a single scan or query operation; this has a side effect of limiting the number of results returned, but as you point out, filters are applied after the items have been read.. The only workaround would be to continue paging through scan results until you have gotten all the results you need. \nMy understanding is that what Limit technically does is restrict the number of reads performed so that you consume a bounded number of read capacity units during a single scan or query operation; this has a side effect of limiting the number of results returned, but as you point out, filters are applied after the items have been read.. You could also open this feature request on the IAM developer forum. The standalone IAM policy generator appears to already be a JavaScript application, so it may just need to be repackaged for use in Node.. You could also open this feature request on the IAM developer forum. The standalone IAM policy generator appears to already be a JavaScript application, so it may just need to be repackaged for use in Node.. Hi @akdor1154,\nIs the AWS_SDK_LOAD_CONFIG environment variable set to a truthy value?. Hi @akdor1154,\nIs the AWS_SDK_LOAD_CONFIG environment variable set to a truthy value?. Hi @MeLlamoPablo,\nIt might make sense to use the original error message instead of (or in addition to) the \"Resource is not in the state X\" message used for terminal waiter errors.. Hi @MeLlamoPablo,\nIt might make sense to use the original error message instead of (or in addition to) the \"Resource is not in the state X\" message used for terminal waiter errors.. getSignedUrl does not call an API but instead generates a presigned URL using the credentials with which the client was constructed. Is S3Service an instance of AWS.S3?. getSignedUrl does not call an API but instead generates a presigned URL using the credentials with which the client was constructed. Is S3Service an instance of AWS.S3?. The error will not appear in the generated URL but will be present as the first parameter to the callback. \nWhat I meant by my question is, where are you seeing an XML error? The only errors that should be encountered during signing are errors encountered while refreshing credentials or performing the hashes required for a presigned URL. Calling getSignedUrl should not result in any network traffic.\nCould you post the error you're seeing (redact any sensitive information) and a snippet where S3Service is being instantiated?\nJust to give an example of how the getSignedUrl method is supposed to work:\nGiven a file containing the following code:\n```javascript\nconst AWS = require('aws-sdk');\nif (process.env.FORCE_SIGNING_ERROR) {\n    const crypto = require('crypto');\n    crypto.createHmac = () => { throw new Error('All signing disabled'); }\n}\nconst s3 = new AWS.S3({\n    region: 'us-west-2',\n    credentials: {\n        accessKeyId: 'foo',\n        secretAccessKey: 'bar',\n    },\n});\ns3.getSignedUrl('getObject', {Bucket: 'bucket', Key: 'key'}, (err, url) => {\n    if (err) console.error('ERROR', err);\n    else console.log('SUCCESS', url);\n});\n```\nthe only errors that would occur during URL presigning would originate from the underlying crypto module performing the hashing. Even though the bucket, key, and credentials are all fake, getSignedUrl should work without issue unless a hashing error is encountered:\n$ node 1893.js\nSUCCESS https://bucket.s3.us-west-2.amazonaws.com/key?AWSAccessKeyId=foo&Expires=1517257777&Signature=%2FDPknmB5JC0lOpJlqRFy%2B7jYllk%3D\n$ FORCE_SIGNING_ERROR=1 node 1893.js\nERROR { <stack trace>\n  message: 'Cannot read property \\'split\\' of undefined',\n  code: 'TypeError',\n  time: 2018-01-29T20:14:54.656Z,\n  retryDelay: 61.21753904239551 }. The error will not appear in the generated URL but will be present as the first parameter to the callback. \nWhat I meant by my question is, where are you seeing an XML error? The only errors that should be encountered during signing are errors encountered while refreshing credentials or performing the hashes required for a presigned URL. Calling getSignedUrl should not result in any network traffic.\nCould you post the error you're seeing (redact any sensitive information) and a snippet where S3Service is being instantiated?\nJust to give an example of how the getSignedUrl method is supposed to work:\nGiven a file containing the following code:\n```javascript\nconst AWS = require('aws-sdk');\nif (process.env.FORCE_SIGNING_ERROR) {\n    const crypto = require('crypto');\n    crypto.createHmac = () => { throw new Error('All signing disabled'); }\n}\nconst s3 = new AWS.S3({\n    region: 'us-west-2',\n    credentials: {\n        accessKeyId: 'foo',\n        secretAccessKey: 'bar',\n    },\n});\ns3.getSignedUrl('getObject', {Bucket: 'bucket', Key: 'key'}, (err, url) => {\n    if (err) console.error('ERROR', err);\n    else console.log('SUCCESS', url);\n});\n```\nthe only errors that would occur during URL presigning would originate from the underlying crypto module performing the hashing. Even though the bucket, key, and credentials are all fake, getSignedUrl should work without issue unless a hashing error is encountered:\n$ node 1893.js\nSUCCESS https://bucket.s3.us-west-2.amazonaws.com/key?AWSAccessKeyId=foo&Expires=1517257777&Signature=%2FDPknmB5JC0lOpJlqRFy%2B7jYllk%3D\n$ FORCE_SIGNING_ERROR=1 node 1893.js\nERROR { <stack trace>\n  message: 'Cannot read property \\'split\\' of undefined',\n  code: 'TypeError',\n  time: 2018-01-29T20:14:54.656Z,\n  retryDelay: 61.21753904239551 }. The max-keys=0 request you're seeing is the S3 client attempting to determine the region in which the bucket being interacted with is located. The S3 client will often only see a generic \"NetworkingError\" in the browser, which prevents it from differentiating between legitimate networking errors (such as a laptop disconnecting from the internet) and the errors we would see if the request were sent to or signed for the wrong region. \nIs the internet disconnecting in the middle of a number of simultaneous operations? It's not clear from the screenshot if the SDK is sending one ?max-keys=0 request per failed operation or if the requests are being sent in a loop.. The max-keys=0 request you're seeing is the S3 client attempting to determine the region in which the bucket being interacted with is located. The S3 client will often only see a generic \"NetworkingError\" in the browser, which prevents it from differentiating between legitimate networking errors (such as a laptop disconnecting from the internet) and the errors we would see if the request were sent to or signed for the wrong region. \nIs the internet disconnecting in the middle of a number of simultaneous operations? It's not clear from the screenshot if the SDK is sending one ?max-keys=0 request per failed operation or if the requests are being sent in a loop.. Hi @sabarinathen,\nWe don't official support Ionic, though I know others have gotten it to work, normally by following the second step you describe above. Are you using a version of the AWS SDK for JS with bundled types (>= 2.7.0)?\nI'm not familiar with how Ionic projects are meant to be structured, but you might try asking this question on the Ionic issues forum or on Stack Overflow.. Hi @sabarinathen,\nWe don't official support Ionic, though I know others have gotten it to work, normally by following the second step you describe above. Are you using a version of the AWS SDK for JS with bundled types (>= 2.7.0)?\nI'm not familiar with how Ionic projects are meant to be structured, but you might try asking this question on the Ionic issues forum or on Stack Overflow.. Hi @taylor-a-barnette,\nIt's difficult to say what might be going on without looking at the source of the Lambda function being executed. From the description provided, though, it sounds like this may be more of a general programming question than an SDK issue, so you may want to cross-post this question on Stack Overflow to get a quicker answer.. Hi @taylor-a-barnette,\nIt's difficult to say what might be going on without looking at the source of the Lambda function being executed. From the description provided, though, it sounds like this may be more of a general programming question than an SDK issue, so you may want to cross-post this question on Stack Overflow to get a quicker answer.. Hi @westlakem,\nBy default, DynamoDB::PutItem (and, by extension, the DocumentClient's put method) returns no data. You must specify a ReturnValues parameter on the input to direct DynamoDB to return any attributes, and consumed capacity will only be returned if a ReturnConsumedCapacity parameter is included in the input. For example, if you wanted put to return the updated user record, UserService.ts would need to be updated to:\n```typescript\naddUser(firstName: string, lastName: string) {\n    let params = {\n        TableName: 'MY_USERS_TABLE',\n        Item: {\n            id: uuid(),\n            firstName: firstName,\n            lastName: lastName\n        },\n        ReturnValues: 'ALL_OLD',\n    }\nreturn Observable.fromPromise(database.put(params).promise())\n\n}\n```\nPlease refer to https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB.html#putItem-property for more ReturnValues options.. Hi @westlakem,\nBy default, DynamoDB::PutItem (and, by extension, the DocumentClient's put method) returns no data. You must specify a ReturnValues parameter on the input to direct DynamoDB to return any attributes, and consumed capacity will only be returned if a ReturnConsumedCapacity parameter is included in the input. For example, if you wanted put to return the updated user record, UserService.ts would need to be updated to:\n```typescript\naddUser(firstName: string, lastName: string) {\n    let params = {\n        TableName: 'MY_USERS_TABLE',\n        Item: {\n            id: uuid(),\n            firstName: firstName,\n            lastName: lastName\n        },\n        ReturnValues: 'ALL_OLD',\n    }\nreturn Observable.fromPromise(database.put(params).promise())\n\n}\n```\nPlease refer to https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/DynamoDB.html#putItem-property for more ReturnValues options.. I am indeed the person to ping to review this.\nThere's a subtle incompatibility with the CLI, which is that the presence of both a role_arn and static credentials have different meanings depending on how many profiles have been visited. For the first profile processed, role_arn takes precedence over any static credentials, but for all subsequent profiles, static credentials are used if present, and only in their absence will the profile's source_profile and role_arn keys be used to load another set of credentials.. I am indeed the person to ping to review this.\nThere's a subtle incompatibility with the CLI, which is that the presence of both a role_arn and static credentials have different meanings depending on how many profiles have been visited. For the first profile processed, role_arn takes precedence over any static credentials, but for all subsequent profiles, static credentials are used if present, and only in their absence will the profile's source_profile and role_arn keys be used to load another set of credentials.. I did some manual testing to verify that everything is hunky dory. Thanks for your work on this, @akdor1154!. I did some manual testing to verify that everything is hunky dory. Thanks for your work on this, @akdor1154!. Hi @rizowski,\nYou can do this today by requiring specific service clients as submodules. There's a quick mention of this in the README, but essentially it ends up looking very much like the ideal example given above:\njavascript\nconst S3 = require('aws-sdk/clients/s3');\nconst Lambda = require('aws-sdk/clients/lambda');\nThis can reduce the size of bundles produced by Webpack et al by a considerable margin, but it won't reduce the amount of code deployed to Lambda.. Hi @rizowski,\nYou can do this today by requiring specific service clients as submodules. There's a quick mention of this in the README, but essentially it ends up looking very much like the ideal example given above:\njavascript\nconst S3 = require('aws-sdk/clients/s3');\nconst Lambda = require('aws-sdk/clients/lambda');\nThis can reduce the size of bundles produced by Webpack et al by a considerable margin, but it won't reduce the amount of code deployed to Lambda.. The main difference I can see between the two requests is the presence of an X-DevTools-Request-Id header on the one that received a 403. Is that header being signed? (The Authorization header will have a SignedHeaders section indicating which headers are signed.) If so, it may be a hop-by-hop header that is stripped before the request reaches EC2.. The config file is only loaded if the AWS_SDK_LOAD_CONFIG environment variable is set to a truthy value. Since this feature was not present in version 2.0.0 and could affect the runtime characteristics of applications running in environments already configured for the AWS CLI, config loading was introduced as an opt-in feature.. It is intentional, but the original error should be available as the originalError property of the final error thrown.. It is intentional, but the original error should be available as the originalError property of the final error thrown.. I'm not a React Native expert, so you might want to cross-post this to Stack Overflow to see if you can get more opinions.\nIt looks like React Native just recently added support for blobs to their implementation, but the linked PR does not seem to have been included in any released version yet. Without Blob/File support, I believe you would need to get the contents of the file into JS memory and upload them as buffers or typed arrays. With files of that size, though, you might want to look into a native code solution that uploads a file using a presigned POST form or a presigned PutObject URL. You can also try the aws/amplify library, which has React Native-specific S3 abstractions.. I'm not a React Native expert, so you might want to cross-post this to Stack Overflow to see if you can get more opinions.\nIt looks like React Native just recently added support for blobs to their implementation, but the linked PR does not seem to have been included in any released version yet. Without Blob/File support, I believe you would need to get the contents of the file into JS memory and upload them as buffers or typed arrays. With files of that size, though, you might want to look into a native code solution that uploads a file using a presigned POST form or a presigned PutObject URL. You can also try the aws/amplify library, which has React Native-specific S3 abstractions.. It looks like there is an open PR that might address this issue on the DefinitelyTyped repository. Unfortunately, there's nothing we can do in the SDK about this while the issue is being resolved by the maintainers of the @types/node and @types/react-native packages.\nAs a workaround, you could add bespoke typings for the portions of the React Native API that are called directly by the application being compiled (per the advice given in https://github.com/DefinitelyTyped/DefinitelyTyped/issues/15960). . It looks like there is an open PR that might address this issue on the DefinitelyTyped repository. Unfortunately, there's nothing we can do in the SDK about this while the issue is being resolved by the maintainers of the @types/node and @types/react-native packages.\nAs a workaround, you could add bespoke typings for the portions of the React Native API that are called directly by the application being compiled (per the advice given in https://github.com/DefinitelyTyped/DefinitelyTyped/issues/15960). . AWS.S3#upload will decide based on the input size whether to use a single PutObject operation or to orchestrate a multipart upload. A multipart upload must begin with a CreateMultipartUpload operation and end with a CompleteMultipartUpload operation, both of which use the POST HTTP method. The documentation for the Bucket parameter can be updated to remove the reference to PUT, but you will need to allow POST operations in your CORS configuration if you wish to allow concurrent part uploads of large files.. AWS.S3#upload will decide based on the input size whether to use a single PutObject operation or to orchestrate a multipart upload. A multipart upload must begin with a CreateMultipartUpload operation and end with a CompleteMultipartUpload operation, both of which use the POST HTTP method. The documentation for the Bucket parameter can be updated to remove the reference to PUT, but you will need to allow POST operations in your CORS configuration if you wish to allow concurrent part uploads of large files.. Hi @raducoti,\nThe documentation linked to above refers to CloudFormation templates. Both the API and the SDKs use an SSESpecification object with an boolean Enabled property.. Hi @raducoti,\nThe documentation linked to above refers to CloudFormation templates. Both the API and the SDKs use an SSESpecification object with an boolean Enabled property.. This parameter looks like it was added in version 2.97.0 of the SDK; could you make sure that you're using this version or later?. This parameter looks like it was added in version 2.97.0 of the SDK; could you make sure that you're using this version or later?. Happy to help!. Happy to help!. The Lambda documentation indicates that the MasterRegion parameter filters the list returned to only include functions replicated from the region indicated. If not specified, only non-replicated functions will be returned. The first example should only list functions originally published to us-east-1 and subsequently replicated to the region to which the ListFunctions operation was sent, whereas the second example should only list functions originally published to the region to which the ListFunctions operation was sent.. The Lambda documentation indicates that the MasterRegion parameter filters the list returned to only include functions replicated from the region indicated. If not specified, only non-replicated functions will be returned. The first example should only list functions originally published to us-east-1 and subsequently replicated to the region to which the ListFunctions operation was sent, whereas the second example should only list functions originally published to the region to which the ListFunctions operation was sent.. We generally prefix injected parameters with a dollar sign to avoid overwriting any future parameters added to the service response.\nIs the intention here to be able to use the ETag value as a checksum?. We generally prefix injected parameters with a dollar sign to avoid overwriting any future parameters added to the service response.\nIs the intention here to be able to use the ETag value as a checksum?. Hi @manjbhachu,\nThe API Gateway team does not monitor this forum, so the best place to reach out to them is via their AWS Developer Forum.. Hi @manjbhachu,\nThe API Gateway team does not monitor this forum, so the best place to reach out to them is via their AWS Developer Forum.. Hi @manju16832003,\nPassThrough is an instance of ReadStream. We use a PassThrough stream internally to make sure the body returned has the same length as dictated by the Content-Length header on the response from S3.. Hi @manju16832003,\nPassThrough is an instance of ReadStream. We use a PassThrough stream internally to make sure the body returned has the same length as dictated by the Content-Length header on the response from S3.. I'm not able to reproduce the issue as described, and I do not believe it is related to our using a PassThrough stream (which is a readable stream). The following reliably pipes an S3 object to a local file:\n```javascript\nconst { createWriteStream } = require('fs');\nconst S3 = require('../../clients/s3');\nconst s3 = new S3({region: 'us-west-2'});\ns3.getObject({Bucket: bucketName, Key: objectKey})\n    .createReadStream()\n    .pipe(createWriteStream('./out.ext'));\n``\nThe HTTP response's stream can sometimes be consumed by third-party libraries before it gets to the SDK. Are you using any library that attaches a listener to all HTTP responses'dataevents?. I'm not able to reproduce the issue as described, and I do not believe it is related to our using aPassThrough` stream (which is a readable stream). The following reliably pipes an S3 object to a local file:\n```javascript\nconst { createWriteStream } = require('fs');\nconst S3 = require('../../clients/s3');\nconst s3 = new S3({region: 'us-west-2'});\ns3.getObject({Bucket: bucketName, Key: objectKey})\n    .createReadStream()\n    .pipe(createWriteStream('./out.ext'));\n``\nThe HTTP response's stream can sometimes be consumed by third-party libraries before it gets to the SDK. Are you using any library that attaches a listener to all HTTP responses'data` events?. Closing as a duplicate of #1769 & #1766. There's a fair amount of discussion on those issues, so I'd like to keep the conversation centralized there.. Closing as a duplicate of #1769 & #1766. There's a fair amount of discussion on those issues, so I'd like to keep the conversation centralized there.. Hi @rguiliani,\nIs the bucket located in the us-east-1 region? S3 will intentionally return an empty string for buckets located in that region.. Hi @rguiliani,\nIs the bucket located in the us-east-1 region? S3 will intentionally return an empty string for buckets located in that region.. Is the correctClockSkew configuration parameter set to true?. Is the correctClockSkew configuration parameter set to true?. S3 allows CORS supported to be enabled and configured on a bucket by bucket basis, but as far as I know, there is no way to enable CORS for bucket-level operations (e.g., ListBuckets, CreateBucket, etc.). I'll forward this request onto the service team, but it would be more effective coming directly from a customer. You can reach out to the service team directly via their AWS Developer Forum or through a ticket with AWS support.. S3 allows CORS supported to be enabled and configured on a bucket by bucket basis, but as far as I know, there is no way to enable CORS for bucket-level operations (e.g., ListBuckets, CreateBucket, etc.). I'll forward this request onto the service team, but it would be more effective coming directly from a customer. You can reach out to the service team directly via their AWS Developer Forum or through a ticket with AWS support.. Which bundler in particular are you seeing the reported error with? If you're using RollUp, there's an open bug on the project to support circular CommonJS dependencies.  The browser build of the SDK is produced using browserify, and there are instructions in the README for use with webpack.\nI disagree that this is a bug, as circular dependencies are permitted in CommonJS.. Which bundler in particular are you seeing the reported error with? If you're using RollUp, there's an open bug on the project to support circular CommonJS dependencies.  The browser build of the SDK is produced using browserify, and there are instructions in the README for use with webpack.\nI disagree that this is a bug, as circular dependencies are permitted in CommonJS.. In browsers, the SDK adds an event listener to the underlying XmlHttpRequest\u2019s progress event. In Node, progress is detected by the body stream being read, which can only be done for streaming inputs. For non-streaming inputs, the http module is given the entire body and tasked with sending it in the most efficient way. Thus there is only a single progress event emitted when the body is written in a single chunk to the http request stream.. In browsers, the SDK adds an event listener to the underlying XmlHttpRequest\u2019s progress event. In Node, progress is detected by the body stream being read, which can only be done for streaming inputs. For non-streaming inputs, the http module is given the entire body and tasked with sending it in the most efficient way. Thus there is only a single progress event emitted when the body is written in a single chunk to the http request stream.. Hi @xlukaszgrabowskix,\nThe AWS.NodeHttpClient doesn't convert HTTP messages into errors; that is handled further down the stack. When using the client outside of the SDK, you will need to inspect the response's status code. Errors will be emitted for connection and timeout errors.. Hi @xlukaszgrabowskix,\nThe AWS.NodeHttpClient doesn't convert HTTP messages into errors; that is handled further down the stack. When using the client outside of the SDK, you will need to inspect the response's status code. Errors will be emitted for connection and timeout errors.. I'm having trouble trying to reproduce this issue. When locating the home directory, the SDK doesn't try to deduce the operation system on which it is running; instead, it first checks if process.env.HOME is defined, then falls back to process.env.USERPROFILE, then falls back to process.env.HOMEPATH, then finally tries require('os').homedir().\nTo try to reproduce, I ran the following script:\n```javascript\nconst AWS = require('aws-sdk')\nconst creds = new AWS.SharedIniFileCredentials()\ncreds.get(err => {\n    if (err) console.error(err)\n    else console.log(creds.accessKeyId)\n})\n```\nThis correctly identified my home directory on both Windows 10 and inside the WSL.\nCan you check on what process.env.HOME, process.env.USERPROFILE, and process.env.HOMEPATH are set to inside of the WSL shell? Please note that %HOME% would be recognized as a string literal in Ubuntu (and not expanded as an environment variable). %HOME% is a legal directory name in Linux.. I'm having trouble trying to reproduce this issue. When locating the home directory, the SDK doesn't try to deduce the operation system on which it is running; instead, it first checks if process.env.HOME is defined, then falls back to process.env.USERPROFILE, then falls back to process.env.HOMEPATH, then finally tries require('os').homedir().\nTo try to reproduce, I ran the following script:\n```javascript\nconst AWS = require('aws-sdk')\nconst creds = new AWS.SharedIniFileCredentials()\ncreds.get(err => {\n    if (err) console.error(err)\n    else console.log(creds.accessKeyId)\n})\n```\nThis correctly identified my home directory on both Windows 10 and inside the WSL.\nCan you check on what process.env.HOME, process.env.USERPROFILE, and process.env.HOMEPATH are set to inside of the WSL shell? Please note that %HOME% would be recognized as a string literal in Ubuntu (and not expanded as an environment variable). %HOME% is a legal directory name in Linux.. S3 requires the copy source to be URL-encoded, and unfortunately introducing this behavior to the SDK would be a breaking change. Please see #1302 & #712 for further discussion.. S3 requires the copy source to be URL-encoded, and unfortunately introducing this behavior to the SDK would be a breaking change. Please see #1302 & #712 for further discussion.. Hi @RameshRM,\nThe best introduction to how to make use of UserContextData that I could find was this blog post written by the Cognito team. . Hi @RameshRM,\nThe best introduction to how to make use of UserContextData that I could find was this blog post written by the Cognito team. . Hi @mwasiluk,\nThere's nothing that can be done client-side to enable CORS support in IAM, so this is something the service team would need to handle. I'll update the internal request opened in response to #938, but please reach out to them directly via their AWS Developer Forum.. Hi @mwasiluk,\nThere's nothing that can be done client-side to enable CORS support in IAM, so this is something the service team would need to handle. I'll update the internal request opened in response to #938, but please reach out to them directly via their AWS Developer Forum.. Hi @ronayadid,\nThe Expires parameter is included by S3 as then 'Expires' header on responses to GetObject operations. Setting this value does not cause objects to be automatically deleted at that time by S3, but the header will be used by browsers and similar HTTP clients to control client-side caching as described in RFC 2616.. Hi @ronayadid,\nThe Expires parameter is included by S3 as then 'Expires' header on responses to GetObject operations. Setting this value does not cause objects to be automatically deleted at that time by S3, but the header will be used by browsers and similar HTTP clients to control client-side caching as described in RFC 2616.. Do you have an example of the data being provided to the putObject method? It looks like the logger is trying a follow an infinitely recursive data structure.. Hi @staradayev,\nIt sounds like everything's working fine with presigned URLs when you try to use the S3 REST API, but not when accessing the bucket via its static website URL. My understanding is that this is by design; S3 static website hosting requires that objects be publicly readable; otherwise a 403 is returned. This developer guide from S3 explains permissions and static website hosting in more depth.. Since you're seeing the same behavior with the Java SDK, the issue likely does not reside in the client. Would you be able to open an AWS support ticket with the same question? I can send forward this issue to the DynamoDB team, but contacting them directly will be the fastest way to get a response. You can also reach out to the service team directly via their AWS developer forum.. According to https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Condition.html#DDB-Type-Condition-ComparisonOperator, a CONTAINS filter predicate may only have one item in its AttributeValueList property. You could try using a condition expression instead:\nparams.FilterExpression = 'contains(adversary_name, 'dog') OR contains(adversary_name, 'Dog')'. The ID and access tokens you receive are valid for one hour per the Cognito developer guide. The refresh token is valid for 30 days by default, but this is configurable.\nAs far as I could tell from the developer guide, token validity is not affected by the AuthFlow selected.. The ID and access tokens you receive are valid for one hour per the Cognito developer guide. The refresh token is valid for 30 days by default, but this is configurable.\nAs far as I could tell from the developer guide, token validity is not affected by the AuthFlow selected.. When using the AWS.DynamoDB#query method, is LastEvaluatedKey undefined or an object with no properties?. When using the AWS.DynamoDB#query method, is LastEvaluatedKey undefined or an object with no properties?. @leonetosoft The default request timeout used is two minutes, and the XmlHttpRequest interface will cancel any request that exceeds this maximum duration. Multipart upload requires that each chunk be at least 5 MB (i.e., the chunks being uploaded cannot be made any smaller), so you may need to increase the timeout to something larger.. @leonetosoft The default request timeout used is two minutes, and the XmlHttpRequest interface will cancel any request that exceeds this maximum duration. Multipart upload requires that each chunk be at least 5 MB (i.e., the chunks being uploaded cannot be made any smaller), so you may need to increase the timeout to something larger.. I know it's unlikely that an AWS response will be served from a cache, but I think it would make sense to take account of the Age header here. This should default to 0, and the argument passed to AWS.util.isClockSkewed should be serverTime + date * 1000.\n. Should there be another expect check after this line?\n. OIC; I thought that's what the next line was doing. \n. This could be combined with the next line.\n. Was this failing due to the change in buffer instantiation?. This should be reflected in the jsdoc comment above: \"... If native promises are supported, pass in null to use direct the SDK to use them. If native promises are not supported and null is provided for this argument, the method is a no-op.\". Should the client config be passed to STS, too?. There are other configuration options that customers might want to control (e.g., timeouts, retries, http agents, etc.) . I wish we had a better way to reorder listeners instead of just removing and re-adding them. Changing this is out of scope for this CR, but it's something we should take a look at.. Hopefully, making all of these async should take care of the flickering failures we've been seeing over the past few weeks. The serviceClientId is orthogonal to the uniqueness of this cache key, but I see your point. I'll use an HMAC of the AKID and secret key.. This function should only be invoked once credentials have been resolved, so it needs to be called from the callback.. Sure; will update.. Does this get resolved to the pure-js version used in browsers or something specific to react native?. Why not ^2.0.0? :). Cognito Identity Provider uses \"none\" as an authtype; should support for that be added here?. Wouldn't this case be covered by the change to event_listeners (this.request.headers['X-Amz-Content-Sha256'] should already be 'UNSIGNED-PAYLOAD')?. Makes sense. If someone were constructing a signer with signatureCache = true, options would still be a boolean here, so options.signatureCache would be undefined.. Err, is there any way to turn off the signature cache? This will convert false to true. What about this.signatureCache = options.signatureCache !== false;. I'm not sure this is in line with the recommendation. DynamoDB recommends that the first retry be delayed up to 50 ms, the second up to 100 ms, etc.; whereas this will follow that sequence for retries 2-10 with the first retry being dispatched with no delay.\nApplying the same delay calculation to all retries also means that you can just return AWS.util.calculateRetryDelay(retryCount, retryDelayOptions) instead of having extra branching logic.\nSee https://github.com/aws/aws-sdk-php/issues/1186 for discussion on this in another SDK.. Optional: you might want to add a test to service.spec.coffee verifying that a normal client still gets a retry base of 100 set.. Good point. I added a call to AWS.util.copy to clone the params so that we're not passing any unknown keys to the service operation.. Nit: lines 165-178 have an extra indentation.. Yeah, that seems like an important done call.. I can't believe that's a single operator. :). Sure. Will centralize.. I'll add a test for that.. AWS.util.copy is a shallow object clone, so this would just create a new object and assign the members from the output shape structure to that new object. Doing the copy prevents a reference cycle of data.$response.data.$response.data... etc.. Is this a breaking change? Previously, users would get up to content-length bytes and no more if the returned stream exceeded that length; now, the stream will emit an error event in that case. Was there ever a case where content-length would be wrong but the user was OK with receiving a truncated stream?. +1 to having stream fixtures that aren't just event emitters.. I can't imagine that the data received up to content-length would be usable in most cases... you would end up with an unparsable partial XML or JSON document unless the service was returning a byte stream. I think the change in this PR is correct, but we should be on the lookout for issues where users were receiving partial byte streams and are now encountering stream errors.. Not a blocker, but it would be worthwhile to open an issue on React Native or comment on an existing one. (This looks related to https://github.com/facebook/react-native/issues/10362, but there might be another one that's still open.). I think Travis gives us good coverage of multiple node versions, though there might be some environmental issues that only affect certain versions of node.. CodeCov is being a tad too extreme on this PR, but I think it would be helpful to add a unit test that ensures listeners are removed and that isDoneChunking is left false.. Nit: \"Only validate buckets\". Won't this run into the same BC issue as throwing whenever someone uses a bucket + prefix instead of just a bucket name? Do we currently allow users to provide a bucket name of foo/bar and no key when they want to access the key bar in the bucket foo?. We can't change it, but this behavior does not seem completely intuitive. Bash at least will treat one or more slashes in a sequence as a single directory separator. Just something to keep in mind for any future high-level interfaces, especially those that allow/encourage users to interact with S3 as if it were a filesystem.. This test is using sigv2. That sounds like a fairly extreme edge case. I'd be fine with leaving as is. . Not a blocker, but the tests are much easier to read with empty lines separating them.. ditto. Snip. Happy to see this dependency cycle get removed.. You could collapse lines 3-7 with lines 13-17 as AWS.util.crypto.lib = util.crypto.lib = require('crypto-browserify'); etc. Maybe instead of throwing, this method could take a callback? That would allow the RequestAlreadyTriggeredError to be handled by user callbacks when send is called or emitted onto streams when createReadStream is called.. This isn't quite right. The return value should use Credentials instead of any, and the method itself doesn't take any callbacks.. You should make a type assertion on the resolved value here so that the compiler verifies the parameterized promise.. I think this is from another PR and will need to be rebased out.. IDE artifacts should be in your system-wide .gitignore file. See above re: rebase. You should walk the shape and check the property explicitly. Models can have documentation that might include that string.. I don't believe this shape is streaming (see https://github.com/aws/aws-sdk-js/blob/v2.94.0/apis/lambda-2015-03-31.normal.json#L646 ). This would add Readable to non-streaming blobs in services that support any streaming blobs. That appears to be happening with Lambda (where BlobStream should be a streaming shape but Blob should not).. I don't think this is used anywhere. Style nit: continuation lines (lines that are part of the same statement as the preceding line) should be indented.. Could this check for and use setImmediate instead of process.nextTick? setImmediate is also supported in IE and Edge.. I'm not sure this method name reflects what the function does. How about 'getSkewCorrectedDate' or just 'getDate'?. I'm not sure you need to call AWS.util.date.getDate now that skew is handled by service clients. Why not just 'new Date()'?. Make sure to put an empty line between tests.. Ditto re: empty lines. One should be between each test suite, too.. Shouldn't the return value be Promise<string>?. The dist files are automatically generated when we cut a release and shouldn't be included in feature commits.. Rather than setting the resolved value or error to function-scoped variables, why not just make assertions in the success handler passed to then and throw in the error handler passed to catch?\nThe file to which these tests are being added was originally written in CoffeeScript and subsequently replaced with its compiled JavaScript form, so many of the odd conventions used were injected by the CoffeeScript compiler.. Since most methods take a variable number of arguments, I'm not sure if this is the best approach. I would just add a simple wrapper for getSignedUrl instead.. Could you cast to a boolean? (shape.sensitive === true). I don't see this import being used anywhere.. Since this affects the developer experience, the description should probably be a bit longer. Maybe \"Update the request logger to exclude sensitive input parameters from the message logged\". maybe call this one \"bar\" instead :). Make sure to keep tests and suites separated by empty lines.. I think this function could be more compact if the switch had a default case (to match scalar types) and it simply recurred for each structure, list, and map member. I think you need to return early here if shape is undefined. There are recursive shapes defined in some services, and this function will need to check for some stopping condition.. There are, and they are all handled here. The current V4 presigner will always produce an invalid signature when any other headers are present.. Will remove.. Can you restore this file? This change leaves the ECSCredentials class in place and preserves its interface, so we would still want to provide types for that class.. AWS.ECSCredentials should be in lib/credentials/ecs_credentials.js (the file path previously used), and that file should require AWS.RemoteCredentials. Some customers may be directly importing ECS Credentials as a submodule, so its location on disk is a public interface.. Ah, I see that this was moved. Instead, it should remain in the same file as before in case someone is importing ECSCredentials directly as a submodule.. The indenting throughout this file should be normalized. Please use 4 spaces.. If you make the changes described above, you will still need to require ecs_credentials . Isn't this a duplicate of what's in lib/credentials/ecs_credentials.js?. To match the AWS CLI behavior, this method will need to throw an error if the Version key is not 1. To match current AWS CLI behavior, static credentials should take higher precedence than a credential_process.. Since useCredentialProcess is set to true in the default chain, this is still incompatible with the CLI. Given a profile with like the following:\n[foo]\ncredential_process = /path/to/bin --flag\naws_access_key_id = asdf\naws_secret_access_key = hjkl\nthe SDK for JS would execute /path/to/bin --flag, whereas the AWS CLI would not.\nRather than adding a constructor flag, this could be made compatible by changing the order of the branches in the if statement:\njavascript\nif (profile['aws_access_key_id'] || profile['aws_secret_access_key']) {\n    // use the profile as static creds\n} else if (profile['credential_process']) {\n    // execute the credential process\n}. Ah my mistake; I misread the change to the default chain and missed that it was an addition rather than an update.\nIn any case, just changing the order of the if/else block seems simpler and has fewer moving parts.. ",
    "brianc": "Just wanted to throw in my 2 cents - I stumbled across this issue googling for how to set maxSockets properly in the aws sdk.  Doing what @lsegal suggested sped up my batching processing immensely.  It's really unfortunate node has such a low default setting on that property.  Might be worth while to publish the information on upping max sockets in the documentation somewhere.  Thanks. :+1: \n. ",
    "dsjoerg": "Just got bit by this myself\n. ",
    "pr1ntr": "This is my stacktrace:\nSyntaxError: Unexpected token o\n    at Object.parse (native)\n    at C:\\Users\\pavel.lastname\\Projects\\feedservice\\server.js:185:31\n    at Response.module.exports.get (C:\\Users\\pavel.lastname\\Projects\\feedservice\n\\lib\\storage.js:65:9)\n    at Request.AWS.Service.inherit.makeRequest (C:\\Users\\pavel.lastname\\Projects\n\\feedservice\\node_modules\\aws-sdk\\lib\\service.js:123:18)\n    at Request.callListeners (C:\\Users\\pavel.lastname\\Projects\\feedservice\\node_\nmodules\\aws-sdk\\lib\\sequential_executor.js:132:20)\n    at Request.emit (C:\\Users\\pavel.lastname\\Projects\\feedservice\\node_modules\\a\nws-sdk\\lib\\sequential_executor.js:100:10)\n    at Request.emitEvent (C:\\Users\\pavel.lastname\\Projects\\feedservice\\node_modu\nles\\aws-sdk\\lib\\request.js:392:10)\n    at Request.AWS.Request.inherit.completeRequest (C:\\Users\\pavel.lastname\\Proj\nects\\feedservice\\node_modules\\aws-sdk\\lib\\request.js:343:14)\n    at Request.emitEvents (C:\\Users\\pavel.lastname\\Projects\\feedservice\\node_mod\nules\\aws-sdk\\lib\\request.js:364:20)\n    at Request.AWS.Request.inherit.emitEvents (C:\\Users\\pavel.lastname\\Projects\\\nfeedservice\\node_modules\\aws-sdk\\lib\\request.js:371:16)\nthis is strange. I don't know what is parsing it. If i don't get an error i have it parse it into json, but I don't think its th e same error.\n. I figured it out, I wasn't returning the response body as a string. It was just strange because it was throwing the error seemingly before it tried to parse the response.\n. It looks like I figured out what was going on. The documentation is a bit confusing when it says array() for CORSRules. I wasn't sure if it meant that CORSRules is an array of one object per rule or an object of rule key/value pairs. This ended up working:\n{\n                    CORSRules: [{\n                      AllowedMethods: ['GET'],\n                      AllowedOrigins: origins,\n                      AllowedHeaders: ['*'],\n                      ExposeHeaders: ['x-amz-server-side-encryption', \"content-type\"],\n                      MaxAgeSeconds: 5000\n                    }]\n                }\nI had to look inside of s3/step_definition/buckets.js to see an example.\n. ",
    "mreinstein": "It does, thank you for the explanation, and sorry for polluting the repo with a needless ticket.\n. something that might help narrow down the cause: can you eliminate promise-pool-executor as a dependency in aws-s3-ram-overconsumption, relying solely on aws-sdk and manual promise management to see if the problem goes away? I haven't looked at the implementation of promise-pool-executor internally but it's possible that it's doing something weird with promises and/or generators and gobbling up memory. \nAnyway, just a thought. . ",
    "fastman": "updateDistribution works fine. I thought there might be a method that requires fewer parameters to pass (something like http://awssum.io/amazon/cloudfront/put-streaming-distribution-config.html). Thank you for your response and consider this issue closed :-)\n. Unfortunately not. Problem still remains.\n. ",
    "moos3": "Adding these lines fixed it.\nvar ins = [];\n    ins.push({InstanceId: instanceID})\n    params = {LoadBalancerName: lbName, Instances: ins };\nAmazon needs to document the json objects much better then they do. 90 % of it is a crap shoot.\n\nThanks,\nRichard Genthner\nOn Jul 2, 2013, at 1:49 PM, Loren Segal notifications@github.com wrote:\n\nThe SDK documentation is not clear here. It's a little more clear on the ELB documentation for RegisterInstancesWithLoadBalancer. Basically, the array should be a list of structures with the InstanceId field set.\nTry:\nparams = {\n  LoadBalancerName: lbName,\n  Instances: [{InstanceId: instanceId}]\n};\nDoes this work?\nI will look into seeing if there is a way we can make this more clear in our SDK documentation.\n\u2014\nReply to this email directly or view it on GitHub.\n. Yes I think going out to the community would make much nicer documentation.\n\n\nThanks,\nRichard Genthner\nOn Jul 2, 2013, at 2:01 PM, Loren Segal notifications@github.com wrote:\n\nNoted. We will take this feedback and see what we can do about making these scenarios more clear. I think examples usually help to clarify these scenarios. Do you think that soliciting examples from the community would a good route here?\nI will go ahead and close this ticket since you seem to have it working. Don't be shy to raise more issues if things dont \"just work\" in the SDK!\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "wadeharrell": "in the meantime....\njs\nvar https = require('https'),\n    httpsOptions,\n    regions = [\n        'us-east-1',\n        'us-west-1',\n        'us-west-2',\n        'eu-west-1',\n        'ap-northeast-1',\n        'ap-southeast-1',\n        'ap-southeast-2',\n        'sa-east-1',\n        'us-gov-west-1'\n    ];\n/* https://raw.github.com/aws/aws-sdk-ruby/master/endpoints.json */\nhttpsOptions = {\n    host: 'raw.github.com',\n    path: '/aws/aws-sdk-ruby/master/endpoints.json',\n    port: 443,\n    method: 'GET'\n};\nhttps.request(httpsOptions, function(res) {\n    var onData,\n        onEnd,\n        data = '';\n    onData = function onData( d ){\n        data += d;\n    };\n    onEnd = function onEnd(){\n        regions = JSON.parse(data);\n    };\n    res.on( 'data', onData );\n    res.on( 'end', onEnd );\n}).on('error', function(e) {\n    console.log(e.message);\n}).end();\n. ",
    "psugihara": "I'm seeing the callback triggered twice in the browser as well:\n```\n  var Uploader = function (AWS, keyPrefix) {\n    console.log('constructing Uploader');\n    this.bucket = new AWS.S3({\n      params: {\n        Bucket: AWS.config.bucket\n      }\n    });\n  };\n  Uploader.prototype.getKey = function (file) {\n    var extension = .last(file.name.split('.'));\n    return this.keyPrefix + '' + new Date().getTime() + '.' + extension;\n  };\n  Uploader.prototype.upload = function (file, cb) {\n    console.log('uploading...');\n    var params = {\n      Key: this.getKey(file),\n      ContentType: file.type,\n      Body: file\n    };\nthis.bucket.putObject(params, function (err, data) {\n  console.log('object put');\n  console.log(\"err\", err);\n  console.log(\"data\", data);\n  if (err && cb)\n    cb(err);\n  else if (err)\n    throw err;\n  else\n    cb(null, params.Key);\n});\n\n};\n```\nHere's the output:\nuploading... main.js:22\nobject put main.js:30\nerr null main.js:31\ndata Object {ETag: \"\"e6babf8f0d9af2889d90c1b5f917b251\"\", RequestId: undefined} main.js:32\nobject put main.js:30\nerr ReferenceError {statusCode: 200, retryable: false} main.js:31\ndata Object {ETag: \"\"e6babf8f0d9af2889d90c1b5f917b251\"\", RequestId: undefined}\n. ",
    "noazark": "Got the same problem with Node.js v0.10.24. Fixing it the same way as others by adding a hasCalledBack boolean. Not sure if it was just law of large numbers, but the problem only showed up after uploading ~150-250MB in a multipart upload.\n. ",
    "kixorz": "My issue may be related to this.\n``` javascript\nvar request = new client.EC2().describeInstances({\n  Filters: [filter, {\n     Name: 'instance-state-name',\n     Values: [\n         'running'\n      ]\n   }]\n}, function(error, data) {\n//triggered twice\n});\nrequest.send();\n```\nCallback is being called twice with the same data. I'm running this SDK as a Node.js module.\n. I tried updating to:\naws-sdk@2.0.0-rc13 aws-sdk\n\u251c\u2500\u2500 xmlbuilder@0.4.2\n\u251c\u2500\u2500 xml2js@0.2.4 (sax@0.6.0)\n\u2514\u2500\u2500 aws-sdk-apis@2.0.5\nand changed my code to use\nrequest.on('success', function(response) { ... });\nand it looks like the problem disappeared. I'll update if I experience this again.\n. I just verified that with aws-sdk@2.0.0-rc13 aws-sdk the callback passed directly (https://github.com/aws/aws-sdk-js/issues/130#issuecomment-39393299) into EC2 functions is still being fired twice.\n. That seems to call it only once. This isn't very obvious from the documentation since we're still calling new client.EC2()... which gives us a request object - but this time it's already sent.\nI'd say the same behavior with both approaches - simply calling request.send() when ready is more predictable.\n. Thanks for your explanation and assistance :thumbsup:\n. Thanks for your explanation and assistance :thumbsup:\n. ",
    "Marak": "You should not be using domains like this.\nYour API design for error reporting is flawed, and now causing me some real problems.\nI don't really know what else to say. If you understood JavaScript best-practice this entire thread would have read much differently.\n. aws-sdk 100% will throw instead of calling back.\nHere is an easy to reproduce one for dynamodb. Really inconvenient guys. I've looked at how the sdk is built in JavaScript, and it is not best-practice.\njs\n/Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/request.js:30\n            throw err;\n                  ^\nValidationException: Item size has exceeded the maximum allowed size\n    at Request.extractError (/Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/protocol/json.js:43:27)\n    at Request.callListeners (/Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/request.js:595:14)\n    at Request.transition (/Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/request.js:21:10)\n    at AcceptorStateMachine.runTo (/Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request.<anonymous> (/Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/request.js:37:9)\n    at Request.<anonymous> (/Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/request.js:597:12)\n    at Request.callListeners (/Users/a/dev/frame.io/dynamodb-simple/node_modules/aws-sdk/lib/sequential_executor.js:115:18)\n. This isn't rocket science.\nHere is the line that throws the uncaught error: https://github.com/aws/aws-sdk-js/blob/master/lib/request.js#L30\nI'm able to trigger this error by attempting to set a document in DynamoDB larger then 400k. This results in the following error: ValidationException: Item size has exceeded the maximum allowed size\nWhy a callback continuation is not provided in request.js makes little sense. It looks like there might be support for domain context, but domains are deprecated, and also terrible to work with.\nIt appears that I'll either have to fork aws-sdk, which will probably require a non-trivial patch...or I have to rewrite the dynamodb adapter from scratch or use another client library.\n-10 for whoever decided a state machine with domain context was a good idea for this API client.\n. @chrisradek -\n\nWhere things start to fall apart is when an error is thrown in some code that's called within that callback function.\n\nThis information helped a lot. Thank you.\nI incorrectly assumed aws-sdk was the source of the uncaught error, but in reality the aws-sdk was taking over the error stack so I couldn't see where the original throw was coming from ( from an expected throw inside the application code ).\nSorry for jumping to any conclusions. You could imagine this was a show stopping issue here. I'm still not amused that aws-sdk is squashing the error stack, but at least it's not the actual source of the uncaught errors.\nThanks again for your quick response, it really helped me today.\n. @chrisradek -\n\nWhere things start to fall apart is when an error is thrown in some code that's called within that callback function.\n\nThis information helped a lot. Thank you.\nI incorrectly assumed aws-sdk was the source of the uncaught error, but in reality the aws-sdk was taking over the error stack so I couldn't see where the original throw was coming from ( from an expected throw inside the application code ).\nSorry for jumping to any conclusions. You could imagine this was a show stopping issue here. I'm still not amused that aws-sdk is squashing the error stack, but at least it's not the actual source of the uncaught errors.\nThanks again for your quick response, it really helped me today.\n. ",
    "jaketrent": "+1 !\n. ",
    "koiker": "I'm testing the signing with query parameter for ES service and I got the same issue.\nI need to include X-Amz-Security-Token and x-amz-security-token to work. . ",
    "gedefran": "So here is a sample that produces the failure for me.\nof course I have some credentials stored in ~/.aws-sdk-credentials.json,\nand the path works for my s3 bucket only (change it to something you have\naccess to);\nWhen i run the test I get:\ngio@Morpheus-L2:~/validation-fail$ node sign-fail.js\n{ [UnexpectedParameter: Unexpected key 'Expires' found in params] code:\n'UnexpectedParameter', name: 'UnexpectedParameter' }\nUnexpectedParameter: Unexpected key 'Expires' found in params\n    at fail\n(/home/gio/validation-fail/node_modules/aws-sdk/lib/param_validator.js:124:26)\n    at validateStructure\n(/home/gio/validation-fail/node_modules/aws-sdk/lib/param_validator.js:56:14)\n    at validate\n(/home/gio/validation-fail/node_modules/aws-sdk/lib/param_validator.js:31:17)\n    at Request.VALIDATE_PARAMETERS\n(/home/gio/validation-fail/node_modules/aws-sdk/lib/event_listeners.js:107:32)\n    at Request.callListeners\n(/home/gio/validation-fail/node_modules/aws-sdk/lib/sequential_executor.js:132:20)\n    at Request.\n(/home/gio/validation-fail/node_modules/aws-sdk/lib/sequential_executor.js:123:18)\n    at\n/home/gio/validation-fail/node_modules/aws-sdk/lib/event_listeners.js:94:9\n    at finish\n(/home/gio/validation-fail/node_modules/aws-sdk/lib/config.js:159:7)\n    at /home/gio/validation-fail/node_modules/aws-sdk/lib/config.js:175:9\n    at Credentials.get\n(/home/gio/validation-fail/node_modules/aws-sdk/lib/config.js:422:7)\ngio@Morpheus-L2:~/validation-fail$\nThe pull request ensures the Expires param i removed early enough to\naccount for this issue I think.\nGio\n. It's attached to the email, just in case you didn't get it, I am pasting\nthe contents here:\nvar AWS = require('aws-sdk');\nAWS.config.loadFromPath(process.env.HOME + '/.aws-sdk-credentials.json');\nvar S3 = new AWS.S3({ params: { Bucket: 'morpheus_quality' }});\nS3.getSignedUrl('getObject',\n                {Expires: 5000,\n                 Key:\n'released/current/QSP/QSP_9.1-9_Morpheus_Development_of_Clinical_Evaluation_Report_Rev._B.pdf'},\n                function(err, url) {\n                    if (err) {\n                        console.error(err);\n                        throw err;\n                    }\n                });\nOn Mon, Jul 22, 2013 at 5:09 PM, Loren Segal notifications@github.comwrote:\n\nWhat is the contents of sign-fail.js?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-js/pull/137#issuecomment-21382691\n.\n. \n",
    "kunklejr": "I was getting this same error and it turned out to be an incorrect region configuration. I was setting mine to us-east-1d when it should have been the less specific us-east-1.\nSetting it to us-east-1d was resulting in the api making requests to https://ec2.us-east-1d.amazonaws.com, which is a non-existent domain. My company's DNS provider (OpenDNS) was trying to be \"helpful\" and returned an IP address to their OpenDNS servers, assuming I'd prefer a customized search page with some ads rather than an error. However, the OpenDNS SSL certificate name did not match the expected ec2.us-east-1d.amazonaws.com, hence the error Hostname/IP doesn't match certificate's altnames.\n. ",
    "AndrewRayCode": "Not sure if this will help anyone, but:\nThe domain for my files in S3 is as follows:\ns3-us-west-1.amazonaws.com\nBut when I instantiate the sdk and pass in \"region\": \"s3-us-west-1\", I get the same Hostname/IP error. It turns out the string you're supposed to pass is just \"us-west-1\" without the s3- prefix.\n. ",
    "Meekohi": "Ahh understood. Would be nice to have this tip in the docs if its not a distraction. \n. @lsegal I've created the following small example that throws this error. I'm using node v0.10.26 and aws-sdk 2.0.0-rc.17.\nhttps://gist.github.com/Meekohi/088886081deb828f4b4e\n. ",
    "d-smith": "Thanks for the clarification.\nFrom the perspective of the API consumer (or at least an inexperienced API consumer like myself), receiving a 403 when doing the following leads me to believe my credentials are misconfigured:\ns3bucket.createBucket(function() {\n        var data = {Key: 'myKey', Body: 'Hello!'};\n        s3bucket.putObject(data, function(err, data) {\nFrom the API response I was not able to understand why the call was failing. Only after trying to create the bucket manually from AWS console did I see the 409 error, with an explicit error message that told me exactly what was going on. Having that sort of error context as an API consumer would greatly improve my development efficiency.\nBest regards,\n--doug\n. Ah... makes sense. I probably should have read through the entire manual before creating the issue. Definitely appreciate your clarification.\nThanks and regards,\n--doug\n. ",
    "nodefourtytwo": "Hi,\nI'm on the same team as Saiaman, here is the log after npm install git://github.com/aws/aws-sdk-js :\nError on receiveSQS { [NetworkingError: getaddrinfo ENOTFOUND]\n  code: 'NetworkingError',\n  errno: 'ENOTFOUND',\n  syscall: 'getaddrinfo',\n  region: 'eu-west-1',\n  hostname: 'sqs.eu-west-1.amazonaws.com',\n  retryable: true,\n  name: 'NetworkingError',\n  statusCode: undefined }\nSeems like, from the worker, we can't ping nor tracert the hostname sqs.eu-west-1.amazonaws.com though the security group configuration seems ok.\n. I'll add that it happens not in the beginning but after some time running.\n. We did not touch Agent.maxSockets. But yes, we're quite heavy on read/write/delete on this queue.\nDoes it mean we're getting close to the limits of what an SQS queue can take?\n. ulimit is set to 65536 open files.\nThe problem happens, as far as we can tell, only on medium instances not on micro instances.\n. ",
    "goelvivek": "I tried making 10 calls in sequential order. All are taking the same time.\nOn 13-Aug-2013 1:25 AM, \"Loren Segal\" notifications@github.com wrote:\n\nConnections should be getting re-used. Can you provide information on how\nyou are detecting that the connections are not getting re-used?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-js/issues/146#issuecomment-22520427\n.\n. Hi, I have updated to the version 1.5. \nStill I am facing same issue randomly. \n\nHere is details stack trace. \n\\n'    at Request.<anonymous> (node_modules/aws-sdk/lib/request.js:262:18)',\n      \\n'    at Request.callListeners (node_modules/aws-sdk/lib/sequential_executor.js:132:20)',\n      \\n'    at Request.emit (node_modules/aws-sdk/lib/sequential_executor.js:100:10)',\n      \\n'    at Request.emitEvent (node_modules/aws-sdk/lib/request.js:536:10)',\n      \\n'    at Request.failRequest (node_modules/aws-sdk/lib/request.js:497:10)',\n      \\n'    at Request.<anonymous> (node_modules/aws-sdk/lib/request.js:273:14)',\n      \\n'---------------------------------------------',\n      \\n'    at [object Object].handleRequest (node_modules/aws-sdk/lib/http/node.js:59:12)',\n      \\n'    at MetadataService.request (node_modules/aws-sdk/lib/metadata_service.js:74:10)',\n      \\n'    at MetadataService.loadCredentials (node_modules/aws-sdk/lib/metadata_service.js:93:10)',\n      \\n'    at EC2MetadataCredentials.refresh (node_modules/aws-sdk/lib/credentials/ec2_metadata_credentials.js:52:26)',\n      \\n'    at EC2MetadataCredentials.get (node_modules/aws-sdk/lib/credentials.js:136:12)',\n      \\n'    at resolveNext (node_modules/aws-sdk/lib/credentials/credential_provider_chain.js:112:15)',\n      \\n'    at node_modules/aws-sdk/lib/credentials/credential_provider_chain.js:113:11',\n      \\n'    at node_modules/aws-sdk/lib/credentials.js:138:23'\nError message\nError: Missing credentials in config\n. No. I am v1.15.0. As you suggested that IAM patch was included in that version.  Will update to new version if it is having any IAM related fix. \n. @lsegal  OK thanks. \n. ",
    "johnlouis-swaine-axomic": "That would point to it being an issue with async then. I've refactored the code to avoid it. Thanks for taking a look though. \n. ",
    "doronin": "Looks like this is an issue in node's core modules https://github.com/joyent/node/issues/6065#issuecomment-39396292\n. I use aws-sdk version 2.0.0-rc5\n. great! doing it\n. @lsegal sorry for long response. Yes, updating to latest version of SDK resolved the issue\n. ",
    "aseemk": "That's fantastic! Thank you.\n. (Hat-tip to my colleague @chanadian for finding this.)\nIt'd be great if you guys could mention this lack of feature in the docs, and link to this module and/or issue. Thanks folks!\n. ",
    "jinchizhong": "Hi friends,\nIt looks something goes wrong.\nThis feature not works, now.\nThe signed url include Content-MD5 field correctly.\nBut if I do not put Content-MD5, server did not read Content-MD5 field from query anymore.\nHere's an example:\n$ curl -T fake.png \"https://doutu-debug.s3.us-west-1.amazonaws.com/avatars/4be4b314caafaa3e12bfcb8d16df3aff.png?AWSAccessKeyId=AKIAIQUBQBHQS3GIL7CQ&Content-MD5=S%2BSzFMqvqj4Sv8uNFt86%2Fw%3D%3D&Expires=1517269456&Signature=GvfxNYAwHpPhzw57B6uCitne194%3D&x-amz-acl=public-read\"\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><AWSAccessKeyId>AKIAIQUBQBHQS3GIL7CQ</AWSAccessKeyId><StringToSign>PUT\n\n\n1517269456\nx-amz-acl:public-read\n/doutu-debug/avatars/4be4b314caafaa3e12bfcb8d16df3aff.png</StringToSign><SignatureProvided>GvfxNYAwHpPhzw57B6uCitne194=</SignatureProvided><StringToSignBytes>50 55 54 0a 0a 0a 31 35 31 37 32 36 39 34 35 36 0a 78 2d 61 6d 7a 2d 61 63 6c 3a 70 75 62 6c 69 63 2d 72 65 61 64 0a 2f 64 6f 75 74 75 2d 64 65 62 75 67 2f 61 76 61 74 61 72 73 2f 34 62 65 34 62 33 31 34 63 61 61 66 61 61 33 65 31 32 62 66 63 62 38 64 31 36 64 66 33 61 66 66 2e 70 6e 67</StringToSignBytes><RequestId>1C7DDD3EA5D335CB</RequestId><HostId>ZnSCEhQpnpBnAtR439FhjZKYPgVOmbDSgAn5OybULqtwAvHWUQxmrxqIHESLgiMO3vQNqLtGTEs=</HostId></Error>\n\nAfter checked  in response, I think AWS server does not check Content-MD5 in query arguments anymore?\nbest regards,. ",
    "sampov2": "Thanks for the information! This solved my issue.\n. Thanks for the information! This solved my issue.\n. ",
    "willwhite": "Great, thanks for the fast reply. Does the SDK behave like the Ruby SDK where the metadata API is checked automatically if credentials aren't provide directly or do I have to turn this feature on explicitly?\n. Great thanks. I'll poke around and see about a pull request for the docs.\n. I believe this is still an issue. Let me know if I should open a fresh ticket.\nI made an isolated test case for this: https://gist.github.com/willwhite/6afe2d333a002c33413f. The trick to replicating appears to be in adjusting http.globalAgent.maxSockets to a value around 100 (the default is 5 but that's too low for high concurrency applications).\nIt seems like we need a locking cache around MetadataService.loadCredentials(). To avoid bombarding the metadata service with requests the moment credentials expire.\nHappy to work on a PR if there is agreement.\n. Fresh issue at https://github.com/aws/aws-sdk-js/issues/445.\n. ",
    "onetom": "It would be great to show an example in the documentation what does \"zero configuration\" mean.\nSomething like this:\n\n$ cat aws-credentials.js\n    AWS=require('aws-sdk');\n    ... ??? ...\n    console.log(AWS.config.credentials);\n$ ROLE=$( curl http://169.254.169.254/latest/meta-data/iam/security-credentials/ )\n$ curl http://169.254.169.254/latest/meta-data/iam/security-credentials/$ROLE\n$ node aws-credentials.js\n$ AWS_ACCESS_KEY_ID='xxxx' AWS_SECRET_ACCESS_KEY='xxxx' node aws-credentials.js\n\nQ1: When using the EC2MetadataCredentials will it refresh the credentials when they expire?\nQ2: Why is it necessary to specify the region? Shouldn't it be specified automatically using the metadata too?\n    for example curl http://169.254.169.254/latest/dynamic/instance-identity/document/ | grep region\n. ",
    "joshrowley": "Hi, sorry for the delay in response, this issue was caused by my own async mistakes later on in the code. Really sorry for the erroneous issue, but I learned a lot about async JS.\nReally appreciate @lsegal looking into this, sorry for wasting time. Keep up the great work!\n. ",
    "alexeypetrushin": "Thank you for fast response, maybe I'm wrong, sorry then. My thoughts that it loads stream into memory based on these observations:\n1. I uploaded small (1mb) and larger (30mb) files to s3, in first case process took 30Mb of memory in the second it took 60Mb - as if the whole file has been loaded to memory (but, maybe it has something to do with GC and V8 memory management).\n2. I also found this test case in sources of aws-sdk\n``` JavaScript\nit 'pulls body out as Buffer if body is streaming payload', ->\n      operation.output =\n        type: 'structure'\n        payload: 'Body'\n        members:\n          Body: location: 'body', type: 'binary', streaming: true\n  extractData 'foobar'\n  expect(response.error).toEqual(null)\n  expect(response.data.Body instanceof Buffer).toEqual(true)\n  expect(response.data.Body.toString()).toEqual('foobar')\n\n```\n. Thanks for explanation, yes, issue resolved and can be closed.\n. ",
    "springmeyer": "Noting here that I also saw this with node v0.10.26 on windows.\n. hi @lsegal - trying to create a reproducible testcase for the process.nextTick error, but having trouble. I don't have access to the machine where the warnings happened right now (since they were on a windows appveyor box - logs at https://ci.appveyor.com/project/BergWerkGIS/node-mapnik/build/1.0.17#L939). So I'm trying to replicate locally on  OS X. In the process hitting a different issue, which perhaps is related? I'm seeing that with v2.0.0-rc11 an error inside of the s3.putObject callback or otherwise some invalid state passed into it may cause the s3.putObject callback to never being called or it may lead it being called multiple times. Here is one testcase (an invalid zero length buffer being passed as the body) that will lead to a hang: https://gist.github.com/springmeyer/0a4e06bdec994db751b2.  I'm seeing that with that testcase node test.js hangs for me on OS X with node v0.10.26.\n. Added a readme and second test to https://gist.github.com/springmeyer/0a4e06bdec994db751b2. Neither replicate the process.nextTick issue. But I did see it once while working to create these test cases.\n. @lsegal - excellent, thank you. I can confirm that both issues are also fixed in my local testing with latest master. As far as the process.nextTick mystery I'm planning on moving to multipart uploads (https://github.com/mapbox/node-pre-gyp/issues/51) so I'm unlikely to hit this again. But I'll certainly post back if I do.\nBtw, are there any code samples for how to best implement multipart uploads beyond the docs you mention at https://github.com/aws/aws-sdk-js/issues/173#issuecomment-26271468?\n. In a last try to uncover the process.nextTick issue I noticed something else slightly odd (but maybe intended): If you pass an empty string for the Key then instead of an error being thrown the upload succeeds and becomes named {Key}. Testcase:\njs\nvar s3 =  new AWS.S3();\nvar s3_obj_opts = {  Body: fs.readFileSync('./e'),\n                     Bucket: 'node-pre-gyp-tests',\n                     Key: '',\n                  };\ns3.putObject(s3_obj_opts, function(err, resp){\n    if (err) throw err;\n});\n. Cheers. I'll create new issues for anything else I run into. Thanks for the fast fixes.\n. Locally, on OS X, here is what I just experienced:\n- I cleaned my cache (npm cache clean)\n- Then I tried to install npm install aws-sdk@2.0.13 multiple times. No matter how many times I tried it failed with the same error (quickly).\n- Then I came back to read your post again.\n- Upon trying again it works :)\nSo, perhaps this is just an npm repository issue\n. No problem, wish I had other ideas for a quick and solid solution. I forgot to mention in my original description that I've been unable to replicate any problem when installing v2.0.12 like:\nnpm cache clean && npm install aws-sdk@2.0.12\nBut I consistently (at least right now) hit the error when running:\nnpm cache clean && npm install aws-sdk@2.0.13\n. @AdityaManohar - thanks! I am able to install v2.0.14 without any problem - tried about 15 times in a row and no issues. \n. ",
    "rclark": "I came across this issue when providing a large buffer as the Body to an uploadPart operation. Working with a 2GB file, I could successfully upload it in 5MB parts, but could not in 50MB parts.\nThis is definitely underlain by an upstream node.js issue, and it may be fixed in v0.12 https://github.com/joyent/node/issues/6065#issuecomment-39396292, but I think you may still fall into another trap like https://github.com/joyent/node/issues/7401 and https://github.com/joyent/node/issues/8291. \nI was able to work around the issue by wrapping my buffer in a readable stream, almost identically to aws-sdk's bufferToStream function, but providing a highWaterMark to the readable stream. This drops you out of this read loop in node.js and avoids the recursion problems that are reported here and in other issues. You can accomplish a similar result with a setImmediate function to make bufferToStream feign \"real I/O\".\nHere's a gist showing pass/fail cases of the bufferToStream function: https://gist.github.com/rclark/0a0d40dfd11b52e05030\n. > Are there any downsides \nYeah, maybe, but I'm honestly not sure. The number that you choose does feel somewhat arbitrary. At first, I tried setting it to zero, and that seemed to effectively stop processing. I ended up setting it to 5MB, since I was confident that buffers of that size did not trigger the recursion errors.\n. These are travis builds for node v0.8.26 and npm version 1.2.30.\nI don't understand why npm would be resolving eslint to v0.5.0 when its specified as \"*\" in package.json?\nHere's an aws-sdk-js build I tried to run on 0.8.x, eslint resolved to 0.7.4: https://travis-ci.org/mapbox/aws-sdk-js/jobs/29735694#L48\nThanks!\n. > It might be that we would want to force Travis to update npm prior to running tests in 0.8.x.\nThere's gray area here, but to me it feels dishonest to say \"it works in node 0.8.x as long as your npm is up-to-date\". \nWhy have you specified \"*\" for so many of these devDependencies? It looks to me like they'll all need to be wired to a 0.8.x-valid version if aws-sdk is going to be testable on 0.8.x.\n.  :+1: -- this'll help me out as soon as it gets tagged!\n. I peeked at the official API and I can see how this mistake has propagated from there. So you might consider this an upstream issue I guess.\nWhile I'm at it though, there's a mismatch between the web console UI and the documented uploadFunction API. The console allows specification of a File Name (the name of the file containing your lambda function) and a Handler Name (the name of the function to be executed). \nThe API only appears to expose a Handler parameter, and when I used this to call uploadFunction and then inspected the results in the console, it appears the value had been assigned as the File Name.\n\nfilename should be index.js, handler name should be bundle\n. Sounds good -- thanks for pointing me in the right direction.\n\nNote that the JS SDK does not actually make use of these regex patterns, so it should not actually affect any behavior.\n\nSo I was unable to create a function when my role ARN contradicted the regex. Do you mean that the JS SDK passed my request through and I was blocked by the API itself?\n. Thanks!!\n. > I think the better solution here might be to upgrade to 0.12.x\nThis sounds fine to me. We encounter this problem rarely and can usually find a way around it. I've seen it crop up on all kinds of EC2s. I've never run a t2 instance so I'm afraid I can't speak to that.\n. In my situation today using a buffer is totally reasonable (although #551 can be a problem with larger buffers), but thanks for the workaround. You suspect this logic will move into the v4 signer relatively soon?\n. Thanks for this. This will make a great workaround, though it is a bummer to have this little gotcha lurking....\n. I haven't tried, but I suspect I would encounter the same problem with request.eachItem and request.eachPage?\n. Excellent! Thank you for the patch!\n. I didn't realize that the response was bound to my callback, but this solution still presents the requirement that your application perform that console.log in addition to whatever it is that is done to handle the error itself (which may lose this context).\nThis value corresponds to the x-amz-request-id header, correct? Does the response also provide easy access to the x-amz-id-2 header value?\n. @markstos' issues pretty clearly demonstrate my original point that access to these headers is obscure in the current API. \nIn the s3.upload case, for example, you're making a bunch of requests and you don't have direct access to all of the AWS.Request objects, which is where (I guess only in the event of an error?) the requestId attribute is exposed.\nIf instead, the request ids were attached to error objects, an error triggered by one of the requests in a multi-part upload could \"bubble-up\" to the caller in a predictable way.\n. Glad that his is helpful. Any insight into when it might land in a patch version?\n. Thanks for taking a look here. I also tried reproducing the failure with a NetworkingError, but was unable to. Thanks for pointing out that if I just throw an error in the callback, then I see that it ends up being thrown by request.js.\nI'm not explicitly doing anything in my callback function that could throw a NetworkingError, but:\n- by watching retry events I noticed that every time I encountered this error in production, it was preceded by 3 retries\n- increasing maxRetries on the s3 client has made this issue vanish in my application\n- There is no further network I/O in the callback function that could cause a NetworkingError, but I am using a custom agent (see https://github.com/node-modules/agentkeepalive). Maybe it is possible that a failure in that module could be throwing an error in the context of my putObject callback. \nSince I can't reproduce without some failure in the calling code, I'm happy to close this issue. It is a shame, though, how some aspects of the sdk's architecture lead to misleading or otherwise useless stack traces.\n. We ran this code for a while and encountered this situation a couple of times while running it. \nThe clues here were that the error objects we produced indicated that response.httpResponse.body and response.httpResponse.headers were undefined, while response.httpResponse.statusCode was 200. Beyond that, I'm not sure I know how I can get an answer to your questions.\n. It's been too long since we encountered this for me to say anything about it. We worked around in 2016 and that workaround is still in place. Should we ever encounter this issue again, I wouldn't be aware of it.\nJust to be clear: there are kind of two possible things failing here. One is some kind of S3 API failure which is totally outside the scope of aws-sdk-js, and I don't expect you to fix. The other is that if the S3 API were to fail in that particular way, the aws-sdk-js doesn't handle it very well. The code in the original post outlines one such condition, though honestly I haven't tried it recently. Could be that empty-response-handling in aws-sdk-js has been improved.. Another example of a place where .waitFor would be welcome would be to poll Athena's getQueryExecution method until the Status property indicates the execution has completed.\n\nI'd appreciate some clarification on what the inherited .waitFor function can/should be used for, and if it can help me with either the Athena or ECR use-cases I've outlined.. ",
    "GeorgePhillips": "I have implemented multipart upload for files larger than 5mb and that works. I still get the same error when a do a lot of files in parallel. I sometimes get the error on the upload calls as well\n. I have implemented multipart upload for files larger than 5mb and that works. I still get the same error when a do a lot of files in parallel. I sometimes get the error on the upload calls as well\n. We have throttled it and have seen less of the errors. This would explain it. What are the throttling rates?\n. We have throttled it and have seen less of the errors. This would explain it. What are the throttling rates?\n. Thanks for your help\n. Thanks for your help\n. ",
    "MathieuLoutre": "Thanks @lsegal for the quickest of replies!\nI'm glad to hear that it's not a workaround then. I was using this page as my reference and nothing is said about markers in there.\nOne more question: if the marker has to be passed to be retrieved in the response, does that mean that it will be the same (as in, I pass 'something.txt' and regardless of what would be the real marker it would reply with 'something.txt')?\nThanks for pointing at the pagination docs. It looks very cool and might definitely cut down my recursive method.\nThanks!\n. My doc page fails to mention NextMarket which was what I was really looking for by using Marker. Anyway, checking IsTruncated and making my own marker seems to work like a charm!\nI'll look into the pagination because the code would be a lot more elegant. Thanks for the pointers!\n. ",
    "sjonnet19": "It is a regression to Node that was introduced in v0.10.19\n. ",
    "tomprouse": "I was consistently having this issue with Node v0.10.19 also. Updating to Node v0.10.20 has fixed it for me.\n. ",
    "nathanpeck": "It's pretty clear this was a Node.js regression in 0.10.19 as the 0.10.20 changelog includes \"tls: fix sporadic hang and partial reads (Fedor Indutny)\" which perfectly matches the hanging behavior I was seeing when trying to use s3.getObject. It was probably hanging because the SDK communicates with AWS using TLS.\nAs far as I'm concerned this can be closed since it's not an AWS SDK issue. But just in case anyone else experiences this issue be sure to check your version of Node.js and if it is 0.10.19 either upgrade or downgrade.\n. This is the exact test that I'm attempting. I went back and tried running the script on an EC2 instance and it worked in that environment, so I'm beginning to think that this is some issue with my local internet connection, but possibly badly handled by node.js itself or the SDK causing it to freeze forever instead of timing out.\nWhile experiencing the hang I can see using nettop that node has an established connection to S3, but nothing is happening:\nnode.383                                                                                          24 KiB\n    tcp4 192.168.2.19:49316<->s3-1-w.amazonaws.com:443               en1     Established        1288 B\nHere is a recording of nettop, showing the network requests being made by the process, and how it just stops on one of the requests without ever finishing: https://s3.amazonaws.com/storydesk/share/hang.mov\nWhat I can't tell is if node is actually receiving a response back from S3. I suspect that for some reason my spotty local connection is causing a timeout, and perhaps the S3 SDK is not handling this gracefully, and instead of returning to the callback with an error message it just hangs forever.\n. Ongoing update:\nI added some debug statements to /lib/http/node.js to keep track of if/when a response was sent. I added one console statement at the top of the writeBody where the body of the request to AWS will be sent, and one console statement in the callback to the http request in the handleRequest function.\nThe results were:\nthemes/default/backgrounds/wintour_background.png\nwriting body of request\nreceived response\nthemes/default/thumbnails/catalog_background.png\nwriting body of request\nreceived response\nthemes/default/backgrounds/catalog_background.png\nwriting body of request\nSo I can see that the body of the request is sent, but no response is ever received in the callback by handleRequest. This is obviously an issue outside the realm of the AWS SDK code, but it does seem that the code within handleRequest for handling timeouts is never triggering, and the TimeoutError response code is never triggered as it should be.\nI would expect that despite my apparently crappy internet connection the SDK should trigger a timeout error response after a reasonable wait time instead of just waiting forever.\n. Adding that line solved the problem and with my debug statements still in I can see that the SDK is repeating the request when it gets a timeout until the request goes through. Now I just need to figure out what is wrong with my internet connection that it is having so many timeouts trying to reach S3.\nThanks for the help!\n. Awesome! Thanks a lot!\n. Which NPM package version is this fix included in or going to be included in?\n. ",
    "neonstalwart": "i tried to follow your existing style for tests - let me know if it needs to be improved.\n. ",
    "nicks": "I don't think this affects production AWS services, because, AFAIK, all production AWS services use default ports. This only causes problems when you're emulating AWS services locally for testing with custom ports.\n. There is no stack trace involved. I'm trying to write a fake SQS for testing. SQS has some APIs for getting queue urls, and we want to use the Host header to implement this.\nre: \"that seems like an issue specific to the server itself\": You don't have to trust me. I certainly wouldn't trust some random guy on the internet. :) The format of the Host header is part of the HTTP spec.\nhttp://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html\nSee section 14.23, which says that the Host header must contain the port.\n. For the record, we're trying to implement the GetQueueUrl endpoint. For obvious reasons, this does not take a QueueUrl, because it returns one.\nNow that you bring it up, do you know where the QueueUrl parameter is documented? We noticed that the SDK was using it, but it didn't seem to be documented in any of the official docs (http://awsdocs.s3.amazonaws.com/SQS/latest/sqs-api.pdf)\n. ",
    "spareforjobs": "lsegal, thanks for your quick response. Its working.\nI tried to give secret key as string directly instead of reading from 'file', but 'createPrivateKey' doesn't work. I thing due to format mismatch.\nThanks\n. lsegal, thanks for your quick response. Its working.\nI tried to give secret key as string directly instead of reading from 'file', but 'createPrivateKey' doesn't work. I thing due to format mismatch.\nThanks\n. ",
    "gianluca-sabena": "Hi, I'm working with fprivitera, so I'm talking about the same issue.\nWe have two micro server on EC2 Irland with:\n= Ubuntu precise (12.04.2 LTS)\n= Node js v0.10.13\n= npm 1.3.2\n= aws-sdk 1.7.0 \nI need to better investigate logs, but this error could manifest a first time just after midnight when date change.  We keep server time updated with ntpdate pool.ntp.org so normally we have less then 3 sec of delay. \nError example\n\"err\": {\n    \"message\": \"The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\\n\\nThe Canonical String for this request should have been\\n'POST\\n/\\n\\ncontent-length:122\\ncontent-type:application/x-amz-json-1.0\\ndate:20131005T000005Z\\nhost:dynamodb.eu-west-1.amazonaws.com\\nuser-agent:aws-sdk-nodejs/1.7.0 linux/v0.10.13\\nx-amz-date:20131005T000005Z\\nx-amz-target:DynamoDB_20120810.GetItem\\n\\ncontent-length;content-type;date;host;user-agent;x-amz-date;x-amz-target\\ne615b36***********************************************'\\n\\nThe String-to-Sign should have been\\n'AWS4-HMAC-SHA256\\n20131005T000005Z\\n20131005/eu-west-1/dynamodb/aws4_request\\na800*************************************'\\n\",\n    \"code\": \"InvalidSignatureException\",\n    \"name\": \"InvalidSignatureException\",\n    \"statusCode\": 400,\n    \"retryable\": false\n}\n. Switch back to aws-sdk-js 1.6.0 solved this problem\n. ",
    "zkimmel": "@lsegal we just applied patch 1.8.0, will wait for the next midnight switchover to find out if it works or not.\n. Seems to be working -- we haven't re-encountered the issue. Thanks much!\n. ",
    "manprash": "I am getting this issue again. I am using aws-sdk though and not in js. My aws.rb in initializers looks like\nrequire 'aws-sdk'\nAWS.config({\n  access_key_id: 'key',\n  secret_access_key: 'secret'\n})\nand model looks like\nclass User < AWS::Record::HashModel\n....\nend\nI get the same exact error when I do the following in the controller\n@users = User.all\n@users.each\nI would appreciate any help, thanks.\n. ",
    "satpal82bhandari": "I found this issue today with node.js aws-sdk.\nit was working fine till night. suddenly it shows same error as in the discussion.\nmessage: 'The request signature we calculated does not match the signature you\n provided. Check your AWS Secret Access Key and signing method. Consult the serv\nice documentation for details.\\n\\nThe Canonical String for this request should h\nave been\\n\\'GET\\n/2012-09-25/jobs/%7BId%7D\\n\\ncontent-length:0\\nhost:elastictran\nscoder.us-west-2.amazonaws.com\\nx-amz-date:20140412T003626Z\\n\\ncontent-length;ho\nst;x-amz-date\\ne3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\\n'\\n\\nThe String-to-Sign should have been\\n\\'AWS4-HMAC-SHA256\\n20140412T003626Z\\n\n20140412/us-west-2/elastictranscoder/aws4_request\\n5aeff672d178552b07051ec152b44\nbe5f98fef63a623a79c341bc36c24a13f19\\'\\n',\n  code: 'InvalidSignatureException',\n  time: Sat Apr 12 2014 06:06:27 GMT+0530 (India Standard Time),\n  statusCode: 403,\n  retryable: false,\n  _willRetry: false } 'InvalidSignatureException: The request signature we calcu\nlated does not match the signature you provided. Check your AWS Secret Access Ke\ny and signing method. Consult the service documentation for details.\\n\\nThe Cano\nnical String for this request should have been\\n\\'GET\\n/2012-09-25/jobs/%7BId%7D\n\\n\\ncontent-length:0\\nhost:elastictranscoder.us-west-2.amazonaws.com\\nx-amz-date\n:20140412T003626Z\\n\\ncontent-length;host;x-amz-date\\ne3b0c44298fc1c149afbf4c8996\n. ",
    "sabrinaluo": "same issue, nodejs@5.0.2 aws-sdk@2.2.21\n```\nInvalidSignatureException: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/2015-03-31/functions/usercity_getRegionalTableByLocale/invocations\nQualifier=PROD\nhost:lambda.ap-northeast-1.amazonaws.com\nx-amz-content-sha256:dcf4f50cca02685575a3c860c453eb09c66000b481359c9870e737cce54ad222\nx-amz-date:20151204T084257Z\nhost;x-amz-content-sha256;x-amz-date\ndcf4f50cca02685575a3c860c453eb09c66000b481359c9870e737cce54ad222'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20151204T084257Z\n20151204/ap-northeast-1/lambda/aws4_request\n8fa220f683adb6517b2df8f5558075060e023aabc1bb0308963977e75f161dda'\n```\n. ",
    "TylerSustare": "I'm getting the same issue using version 2.5.2. I need to update my comment, I had a typo in my keys when I got this error. I am not getting this error after using the correct keys. \ud83d\udc48 shame :(. ",
    "mlogan": "Hi, here's what I get right before a failed request:\nESC[90mGET /profile ESC[31m500 ESC[90m159ms - 816bESC[0m\nDYNAMODB CREDENTIALS: { serviceError: null,\n  metadataService: {},\n  metadata: \n   { Code: 'Success',\n     LastUpdated: '2013-10-08T14:48:58Z',\n     Type: 'AWS-HMAC',\n     AccessKeyId: 'XXXXXX',\n     SecretAccessKey: 'XXXXXX/XXXXXX',\n     Token: 'xxxxxxx',\n     Expiration: '2013-10-08T20:59:44Z' },\n  expired: false,\n  accessKeyId: 'XXXXX',\n  secretAccessKey: 'XXXXX/XXXXX',\n  sessionToken: 'xxxxxxx',\n  expireTime: Tue Oct 08 2013 20:59:44 GMT+0000 (UTC) }\n2013-10-08 14:54:14 ERROR db DynamoDbTable.fetchItem: InvalidSignatureException: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/\ncontent-length:99\ncontent-type:application/x-amz-json-1.0\ndate:20131008T145414Z\nhost:dynamodb.us-west-1.amazonaws.com\nuser-agent:aws-sdk-nodejs/1.7.1 linux/v0.8.9\nx-amz-date:20131008T145414Z\nx-amz-security-token:xxxxxxxxxx=\nx-amz-target:DynamoDB_20120810.GetItem\ncontent-length;content-type;date;host;user-agent;x-amz-date;x-amz-security-token;x-amz-target\nd808de928635a8bb70741481daf3bb3e8267e73ebe552b35606b839fbcc11e2f'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20131008T145414Z\n20131008/us-west-1/dynamodb/aws4_request\n4794d61708a964e6dd9044fb2b57a918521bb76d98530b9f47251ffacefd5f80'\n. Hi, here's what I get right before a failed request:\nESC[90mGET /profile ESC[31m500 ESC[90m159ms - 816bESC[0m\nDYNAMODB CREDENTIALS: { serviceError: null,\n  metadataService: {},\n  metadata: \n   { Code: 'Success',\n     LastUpdated: '2013-10-08T14:48:58Z',\n     Type: 'AWS-HMAC',\n     AccessKeyId: 'XXXXXX',\n     SecretAccessKey: 'XXXXXX/XXXXXX',\n     Token: 'xxxxxxx',\n     Expiration: '2013-10-08T20:59:44Z' },\n  expired: false,\n  accessKeyId: 'XXXXX',\n  secretAccessKey: 'XXXXX/XXXXX',\n  sessionToken: 'xxxxxxx',\n  expireTime: Tue Oct 08 2013 20:59:44 GMT+0000 (UTC) }\n2013-10-08 14:54:14 ERROR db DynamoDbTable.fetchItem: InvalidSignatureException: The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\nThe Canonical String for this request should have been\n'POST\n/\ncontent-length:99\ncontent-type:application/x-amz-json-1.0\ndate:20131008T145414Z\nhost:dynamodb.us-west-1.amazonaws.com\nuser-agent:aws-sdk-nodejs/1.7.1 linux/v0.8.9\nx-amz-date:20131008T145414Z\nx-amz-security-token:xxxxxxxxxx=\nx-amz-target:DynamoDB_20120810.GetItem\ncontent-length;content-type;date;host;user-agent;x-amz-date;x-amz-security-token;x-amz-target\nd808de928635a8bb70741481daf3bb3e8267e73ebe552b35606b839fbcc11e2f'\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20131008T145414Z\n20131008/us-west-1/dynamodb/aws4_request\n4794d61708a964e6dd9044fb2b57a918521bb76d98530b9f47251ffacefd5f80'\n. At this point I'm certain it's nothing to do with the midnight issue, as I restarted my application last night well after midnight GMT, and the credentials were expired again this morning, at, as you say, about 14:54 GMT.\nAnd yes, I read the expiry time that same as you do - 6 hours after the time the request was issued at.\n. At this point I'm certain it's nothing to do with the midnight issue, as I restarted my application last night well after midnight GMT, and the credentials were expired again this morning, at, as you say, about 14:54 GMT.\nAnd yes, I read the expiry time that same as you do - 6 hours after the time the request was issued at.\n. I'm going to downgrade to 1.6.0 to see if that resolves the issue.\n. I'm going to downgrade to 1.6.0 to see if that resolves the issue.\n. In this.retry, I'm not sure what object this is intended to refer to. If you fill me in there, I'd be happy to try.\n1. Once it starts failing, it appears to affect all requests until I restart the application. This is happening with something we haven't launched yet, so the application has been left in its broken state for many hours, and the condition has never gone away without a restart.\n2. We don't have any unicode in the request bodies, unless aws-sdk is inserting some for us.\nAlso, downgrading to 1.6.0 appears to fix the problem. I'll keep an eye on it, but if it doesn't fail by tomorrow morning, I'll feel pretty confident that the problem will not crop up.\nIn the meantime, here's my best guess at a reproduction. I haven't verified that it reproduces the problem, because of the long delay involved before the error occurs:\n1. Start an instance with a machine role granting ec2.DescribeRegions\n2. Install aws-sdk@1.7.1\n3. Run this script on that instance: https://gist.github.com/mlogan/6893983\n4. Wait.\nI'll follow these steps myself and let you know if I'm able to reproduce the problem this way.\n. In this.retry, I'm not sure what object this is intended to refer to. If you fill me in there, I'd be happy to try.\n1. Once it starts failing, it appears to affect all requests until I restart the application. This is happening with something we haven't launched yet, so the application has been left in its broken state for many hours, and the condition has never gone away without a restart.\n2. We don't have any unicode in the request bodies, unless aws-sdk is inserting some for us.\nAlso, downgrading to 1.6.0 appears to fix the problem. I'll keep an eye on it, but if it doesn't fail by tomorrow morning, I'll feel pretty confident that the problem will not crop up.\nIn the meantime, here's my best guess at a reproduction. I haven't verified that it reproduces the problem, because of the long delay involved before the error occurs:\n1. Start an instance with a machine role granting ec2.DescribeRegions\n2. Install aws-sdk@1.7.1\n3. Run this script on that instance: https://gist.github.com/mlogan/6893983\n4. Wait.\nI'll follow these steps myself and let you know if I'm able to reproduce the problem this way.\n. Thanks for your help so far, Loren.\nOur application has stayed up for over 24 hours with aws-sdk@1.6.0, so this does appear to be a bug in 1.7.0.\nI'm currently running our reproduction attempt with a lot of logging enabled, and I'll follow up with output when it (hopefully) starts failing.\n. Thanks for your help so far, Loren.\nOur application has stayed up for over 24 hours with aws-sdk@1.6.0, so this does appear to be a bug in 1.7.0.\nI'm currently running our reproduction attempt with a lot of logging enabled, and I'll follow up with output when it (hopefully) starts failing.\n. Same results here, I can't reproduce the problem either. I'll try it again with the DynamoDB.GetItem call that was having problems originally.\n. Same results here, I can't reproduce the problem either. I'll try it again with the DynamoDB.GetItem call that was having problems originally.\n. Ohhh! I avoided dynamodb so that you wouldn't have to bring up a database to run the repro, wrongly assuming that the authentication method would not differ from one service to the other.\n. Ohhh! I avoided dynamodb so that you wouldn't have to bring up a database to run the repro, wrongly assuming that the authentication method would not differ from one service to the other.\n. Thanks @lsegal!\n. Thanks @lsegal!\n. Sorry, this was my error - I was setting the Limit parameter to be a function, and my debug logging didn't print out the value of Limit. (I was using JSON.stringify which elides function-valued parameters.)\n. Sorry, this was my error - I was setting the Limit parameter to be a function, and my debug logging didn't print out the value of Limit. (I was using JSON.stringify which elides function-valued parameters.)\n. @lsegal I just started seeing issues with this error last week. We are still on aws-sdk@1.8.1 (which was the bug fix release you made last time I had a problem with machine role credentials).\nI don't have a lot of information yet, but here's what I do have:\n- Happens with dynamodb requests, only after our server has been up for a long time. (Days, not hours.)\n- Once it starts happening, it doesn't stop happening.\n- tcpdump reveals that the broken process is not contacting either the instance metadata service, or dynamodb. That is to say, the error appears to be happening purely within the client. (I verified that a different process that successfully gets data from ddb can be observed via tcpdump to first go to the instance metadata service, and then to 204.246.162.237.https, a dynamodb endpoint, so I'm fairly confident I'm not just missing the requests among all the other noise in the pcap file.)\nSorry, I haven't been able to get any output via AWS.config.logger yet.\nI will upgrade to a more recent release, and keep you advised if this error occurs again.\n. @lsegal I just started seeing issues with this error last week. We are still on aws-sdk@1.8.1 (which was the bug fix release you made last time I had a problem with machine role credentials).\nI don't have a lot of information yet, but here's what I do have:\n- Happens with dynamodb requests, only after our server has been up for a long time. (Days, not hours.)\n- Once it starts happening, it doesn't stop happening.\n- tcpdump reveals that the broken process is not contacting either the instance metadata service, or dynamodb. That is to say, the error appears to be happening purely within the client. (I verified that a different process that successfully gets data from ddb can be observed via tcpdump to first go to the instance metadata service, and then to 204.246.162.237.https, a dynamodb endpoint, so I'm fairly confident I'm not just missing the requests among all the other noise in the pcap file.)\nSorry, I haven't been able to get any output via AWS.config.logger yet.\nI will upgrade to a more recent release, and keep you advised if this error occurs again.\n. Update from me: I haven't seen this bug since my previous post on this issue. We are currently on 1.18.0. Seems that the upgrade fixed it for us.\n. Update from me: I haven't seen this bug since my previous post on this issue. We are currently on 1.18.0. Seems that the upgrade fixed it for us.\n. I haven't had any issues since my last update, so yes, closing this is fine.\n. I haven't had any issues since my last update, so yes, closing this is fine.\n. It was a one-line fix to my dynamodb code to omit the Value parameter instead of setting it to null, so this is no big deal for me. The new code also works with older API versions.\n. It was a one-line fix to my dynamodb code to omit the Value parameter instead of setting it to null, so this is no big deal for me. The new code also works with older API versions.\n. ",
    "jspiro": "That's great news! Thanks Loren!\nOn Thu, Oct 10, 2013 at 10:21 AM, Loren Segal notifications@github.com\nwrote:\n\nI've identified the issue and we will be putting out a fix shortly. I will keep everyone here posted.\nReply to this email directly or view it on GitHub:\nhttps://github.com/aws/aws-sdk-js/issues/171#issuecomment-26073024\n. That's great news! Thanks Loren!\n\nOn Thu, Oct 10, 2013 at 10:21 AM, Loren Segal notifications@github.com\nwrote:\n\nI've identified the issue and we will be putting out a fix shortly. I will keep everyone here posted.\nReply to this email directly or view it on GitHub:\nhttps://github.com/aws/aws-sdk-js/issues/171#issuecomment-26073024\n. \n",
    "uipoet": "Brilliant, thank you!\n. ",
    "refaelos": "Yeah I'm talking about Node. \nWaiting for 2.0 an hoping to get a usage example of any kind. \nThanka\n. Thanks @jeskew \nI think the problem is with the plugin we're using (https://github.com/mirusresearch/firehoser) and not with this one.. ",
    "opensaurusrex": "I would like to see the httpProgress event listener added before official release I have been using a system that sends to PHP and then PHP SDK up to S3 but it's very inefficient that way and we have experienced a few issues with uploading and would also like to keep the progress bar implementation.\n. Will there be an option to do so in the browser SDK?\n. ",
    "dhiraj72": "@lsegal I am trying to upload a file on S3 by stream using s3.putObject() .I have also configured AWS by  setting  { accessKeyId\": \"*_\",  \"secretAccessKey\": \"*_*\", \"region\": \"eu-west-1\"} and created bucket ,testuser, in eu-west-1 region of S3 . But I got following error. In error It show region:'us-east-1'. What will be solution  for this.Thanks in advance . \n[NetworkingError: Hostname/IP doesn't match certificate's altnames]\n  message: 'Hostname/IP doesn\\'t match certificate\\'s altnames',\n  code: 'NetworkingError',\n  region: 'us-east-1',\n  hostname: 'testuser.testuser.s3-eu-west-1.amazonaws.com',\n  retryable: true,\n  time: Thu Jul 17 2014 16:24:44 GMT+0530 (IST),\n  statusCode: undefined }\n. Hi Isegal ,\nThanks for your response. I am using \"aws-sdk\" version \"2.0.0-rc9\" in node.js.\n. Thanks @lsegal  !!!\n  After updating AWS SDK  It's working for me . :\n. ",
    "john-aws": "Hi, is there any way to get httpUploadProgress events in the browser if you call putObject via makeUnauthenticatedRequest? If I add .on('httpUploadProgress', f) then I see \"TypeError: undefined is not a function\", for example:\n``` javascript\nvar params = { Key: key, Body: body };\ns3.makeUnauthenticatedRequest('putObject', params, function(err, data) {\n  console.log(\"err: \" + err);\n})\n.on('httpUploadProgress', function(progress) { // TypeError here\n  console.log(\"progress: \" + progress);\n});\n```\n. ",
    "sebastientromp": "Just in case someone else arrives here looking for this:\nThe makeUnauthenticatedRequest function returns the request object if you don't pass a callback:\njava\nreturn callback ? request.send(callback) : request;\nSo if you want to add a progress listener, you will do:\n```java\nvar params = { Key: key, Body: body };\nvar req = s3.makeUnauthenticatedRequest('putObject', params);\nreq.on('httpUploadProgress', function(progress) {\n  console.log(\"progress: \" + progress);\n});\nreq,send(function(err, data) {\n  console.log(\"err: \" + err);\n})\n```. ",
    "kaarthikeyansp": "But  I could see only {\"isTrusted\":true} in progress object now, in 2.94 version of aws-sdk. ",
    "skyzyx": "We get around this in other places by either (a) offering a second copy at file.js.gz, or (b) putting a CloudFront distribution in front of an Elastic Beanstalk app and enabling query string parameters to pass through to the app.\n. Nevermind\u2026\n. ",
    "substack": "You might consider having an api where to get at the code currently mounted at AWS.Service you would do var Service = require('aws-sdk/service') from a node/browserify env and then you could have a script to generate a standalone build for the browser with something like browserify --standalone AWS that does AWS = { Service: require('./service'), ... }. This way you can have more idiomatic looking modules in node/browserify-land while preserving the existing style of namespaced API for vanilla browser code.\n. ",
    "unscriptable": "So, I may be missing something.  The desired structure in the browser is AWS.Service.foo.bar.baz?  \nIf so, that's unfortunate and is going to cause all sorts of unnecessary pain trying to maintain this codebase.  \nThat said, what @substack proposes is a much better way to make the deep global hierarchy thing work.\n. ",
    "evannuil": "Had the same problem, now works in node (1.12.0) but the browser (2.0.0-rc1) still has the problem.\n. ",
    "fastwave2004": "Hi node-tunnel can works.thanks\n. Hi node-tunnel can works.thanks\n. ",
    "wescarr": "@lsegal Sounds good, a rename would definitely alleviate our issue.\n. @lsegal Sounds good, a rename would definitely alleviate our issue.\n. Thanks @trevorrowe We'll try it in our staging environment this weekend.\n. Thanks @trevorrowe We'll try it in our staging environment this weekend.\n. ",
    "amagura": "I'm using version 0.9.9-pre.10, I'll update and see if that helps.  \nUpdating to the latest version solves the issue.\n. ",
    "knkher": "I tried this taking the latest for both nock and aws-sdk (v1.12.0) and I still see a problem. Here is a small test case for the issue. It shows that AWS.S3.getObject is giving back a zero length data when used with nock\nIt seems that nock is generating a http.OutgoingMessage for the response, however aws-sdk doesnt deal with it in the response callback.\n. Thanks for the information @lsegal. I have updated the corresponding issue on the nock repo.\nI have also confirmed that the second workaround works perfectly at my end!\n. ",
    "chrishamant": "Its relatively easy to stub in support for new services using their sdks. I have an application I'm trying this service out with and have stubbed rudimentary support for this service.\nJust fetch this library locally, install the deps and npm link it.\nSee: https://gist.github.com/chrishamant/7568030\n- copy kinesis-2013-11-04.js to lib/services/api/kinesis-2013-11-04.js\n- copy kinesis.js to lib/services/kinesis.js\n- you'll also need to add the line 'require('./services/kinesis');' to lib/services.js\nSince this is a json based api this pretty much works how I want it - only nag is I have to manually base64 encode/decode a Buffer for the 'Data' field of 'putRecord'\nThe service definition is really woefully incomplete with only input parameters specified for 'createStream','describeStream','getNextRecords','getShardIterator','listStreams' and 'putRecord' - you'll have to add the parameters for the other operations yourself if needed. I didn't waste too much time doing this cause I'm pretty sure this stuff just gets automagically generated from a service definition somehow. Hopefully they'll add this new definition in soon!\nHope that helps.\n. ",
    "sposmen": "Tks..worked perfect!\n. ",
    "bjj951": "Firefox 25.0.1\nYes it definitely looks browser related, I just tried it on Chrome Version 30.0.1599.101 m and it worked fine.\ncheers.\n. Its Content-Type    image/jpeg; charset=UTF-8\nsee below, cheers\nRequest Headers\nAccept  text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8\nAccept-Encoding gzip, deflate\nAccept-Language en-US,en;q=0.5\nAuthorization   AWS ASIAJA6Oedited46A:TWP+8ZotdaD4+tI2LuchpmDFSJ8=\nContent-Length  0\nContent-Type    image/jpeg; charset=UTF-8\nHost    mybucket.s3-ap-southeast-2.amazonaws.com\nOrigin  https://mymachine\nReferer https://mymachine/viewpage\nUser-Agent  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:25.0) Gecko/20100101 Firefox/25.0\nX-Amz-Date  Tue, 19 Nov 2013 06:09:51 GMT\nX-Amz-User-Agent    aws-sdk-js/2.0.0-rc1\nx-amz-acl   private\nx-amz-security-token    AQoDYXdzEJ///////////wEa8AIYw6MvwCZosSVVeditedZg0KZ2ZAQjRAN2WFSqiCmgayUBQ==\nResponse Headers\nAccess-Control-Allow-Cred...    true\nAccess-Control-Allow-Meth...    GET, PUT, POST, DELETE\nAccess-Control-Allow-Orig...    https://mymachine\nAccess-Control-Expose-Hea...    ETag, x-amz-request-id\nContent-Type    application/xml\nDate    Tue, 19 Nov 2013 06:09:42 GMT\nServer  AmazonS3\nTransfer-Encoding   chunked\nVary    Origin, Access-Control-Request-Headers, Access-Control-Request-Method\nx-amz-id-2  YIbRJkC2nxeeditedDeDPZCWnz1Ua0\nx-amz-request-id    D26D65FAB789B29A\n. Thankyou, that fix is working for me.\n. Thank you that was it.\nI also added an  ExposeHeader to the CORS config for the x-amz-request-id parameter to get the RequestId populated in the response.\n. OK, thanks that would be great to have in there. It makes it difficult to cleanup a cancelled upload without being able to abort the request.\n. Thankyou, that fix is working for me.\n. ",
    "meetzaveri": "@lsegal Thanks for the help. BTW I wanted to know why we should set  <ExposeHeader>ETag</ExposeHeader> ? What's purpose of Etag?. @ffxsam Good to know!. ",
    "ffxsam": "@meetzaveri I'm guessing it's so the multiple parts being uploaded can read the ETag header and know which S3 object they're going to be concatenated to.. @chrisradek Why close this, when people are still having this issue? There's no resolution for this, and making multiple getObject requests in small parts is not workable, as it results in multiple streams which becomes unmanageable when piping to a process or another file.. Never mind, closing. This is probably a Meteor issue, not an issue with this package.\n. It's not a Cordova app. It's just that Meteor is a different ecosystem (even though it's Node-based). And I'm trying to use the aws-sdk package server-side. I've filed a bug in the Meteor project for this.\n. For anyone else who stumbles upon this looking for help with Meteor:\nhttps://github.com/meteor/meteor/issues/6285\n. https://github.com/aws/aws-sdk-js/blob/d3f0701e8aff9697231f023d2e5a5c289a8ec8ec/lib/services/s3.js#L203-L233\nSo it looks like for the bucket name, instead of just supplying 'myBucket', you give it 'myBucket.s3-accelerate.amazonaws.com'? Or it just automatically attempts acceleration since useAccelerateEndpoint defaults to true?. Ahh, it is indeed false by default. Thanks for pointing that out!. Believe me, I've been through tons of different attempts, none of them work. \ud83d\ude2b \nWhat winds up happening is, I upload (for example) a 1MB file. With your code, I would see something like this right away:\nUploaded :: 100%\nAnd then it uploads the whole file, sitting there, then completes.\nHere's my test file I'm running:\n```js\nconst S3 = require('aws-sdk/clients/s3');\nconst fs = require('fs');\nconst data = fs.readFileSync('/Volumes/Internal/Users/samh/Desktop/1MB.pdf');\nconst s3 = new S3();\nconst upload = s3\n  .upload({\n    Bucket: 'rc2-test',\n    Key: 'wow.pdf',\n    Body: data,\n  })\n  .on('httpUploadProgress', event => {\n    console.log(Uploaded ${event.loaded} out of ${event.total});\n  })\n  .send((err, data) => {\n    console.log(err, data);\n  });\n```\nNow, maybe this will behave differently in a web client, using FileReader to get an array buffer from a file that the user has chosen to upload. I suppose I should try that and see what happens.. Ahh, I'm glad I tried that! Yes, in front-end code, it works as you'd expect. Perfect!\nAny idea why, in a Node.js environment, the httpUploadProgress event only gets called once?. @skay973 You should probably be using AWS Amplify for authentication. Using the Cognito JS directly is rather painful.\nhttps://aws-amplify.github.io/docs/js/authentication\nNote: Skip the part involving the amplify CLI tool, it's not needed. Just use the npm package to do what you need.. @vladejs S3 should be for storing data, and if you want to search and find specific objects, you should have some sort of database to keep track.. ",
    "rafiqvns": "thank you @lsegal . ",
    "sakoht": "I've updated my CORS headers as shown in the docs, but I still get an error message with large files.  Small files (non-multipart) upload without a problem.\nMy CORS is straight from the updated docs:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n<CORSRule>\n    <AllowedOrigin>*</AllowedOrigin>\n    <AllowedMethod>HEAD</AllowedMethod>\n    <AllowedMethod>GET</AllowedMethod>\n    <AllowedMethod>PUT</AllowedMethod>\n    <AllowedMethod>POST</AllowedMethod>\n    <AllowedMethod>DELETE</AllowedMethod>\n    <MaxAgeSeconds>3000</MaxAgeSeconds>\n    <ExposeHeader>x-amz-server-side-encryption</ExposeHeader>\n    <ExposeHeader>x-amz-request-id</ExposeHeader>\n    <ExposeHeader>x-amz-id-2</ExposeHeader>\n    <ExposeHeader>ETag</ExposeHeader>\n    <AllowedHeader>*</AllowedHeader>\n</CORSRule>\n</CORSConfiguration>\nMy js error:\nETagMissing: No access to ETag property on response. Check CORS configuration to expose ETag header.\n    at Response.eval (webpack-internal:///./node_modules/aws-sdk/lib/s3/managed_upload.js:570:30)\n    at Request.eval (webpack-internal:///./node_modules/aws-sdk/lib/request.js:367:18)\n    at Request.callListeners (webpack-internal:///./node_modules/aws-sdk/lib/sequential_executor.js:115:20)\n    at Request.emit (webpack-internal:///./node_modules/aws-sdk/lib/sequential_executor.js:82:10)\n    at Request.emit (webpack-internal:///./node_modules/aws-sdk/lib/request.js:697:14)\n    at Request.transition (webpack-internal:///./node_modules/aws-sdk/lib/request.js:30:10)\n    at AcceptorStateMachine.runTo (webpack-internal:///./node_modules/aws-sdk/lib/state_machine.js:16:12)\n    at eval (webpack-internal:///./node_modules/aws-sdk/lib/state_machine.js:27:10)\n    at Request.eval (webpack-internal:///./node_modules/aws-sdk/lib/request.js:46:9)\n    at Request.eval (webpack-internal:///./node_modules/aws-sdk/lib/request.js:699:12)\n    at Request.callListeners (webpack-internal:///./node_modules/aws-sdk/lib/sequential_executor.js:127:18)\n    at Request.emit (webpack-internal:///./node_modules/aws-sdk/lib/sequential_executor.js:82:10)\n    at Request.emit (webpack-internal:///./node_modules/aws-sdk/lib/request.js:697:14)\n    at Request.transition (webpack-internal:///./node_modules/aws-sdk/lib/request.js:30:10)\n    at AcceptorStateMachine.runTo (webpack-internal:///./node_modules/aws-sdk/lib/state_machine.js:16:12)\n    at eval (webpack-internal:///./node_modules/aws-sdk/lib/state_machine.js:27:10)\n    at Request.eval (webpack-internal:///./node_modules/aws-sdk/lib/request.js:46:9)\n    at Request.eval (webpack-internal:///./node_modules/aws-sdk/lib/request.js:699:12)\n    at Request.callListeners (webpack-internal:///./node_modules/aws-sdk/lib/sequential_executor.js:127:18)\n    at Request.emit (webpack-internal:///./node_modules/aws-sdk/lib/sequential_executor.js:82:10)\n    at Request.emit (webpack-internal:///./node_modules/aws-sdk/lib/request.js:697:14)\n    at Request.transition (webpack-internal:///./node_modules/aws-sdk/lib/request.js:30:10)\n    at AcceptorStateMachine.runTo (webpack-internal:///./node_modules/aws-sdk/lib/state_machine.js:16:12)\n    at eval (webpack-internal:///./node_modules/aws-sdk/lib/state_machine.js:27:10)\n    at Request.eval (webpack-internal:///./node_modules/aws-sdk/lib/request.js:46:9)\n    at Request.eval (webpack-internal:///./node_modules/aws-sdk/lib/request.js:699:12)\n    at Request.callListeners (webpack-internal:///./node_modules/aws-sdk/lib/sequential_executor.js:127:18)\n    at Request.emit (webpack-internal:///./node_modules/aws-sdk/lib/sequential_executor.js:82:10)\n    at Request.emit (webpack-internal:///./node_modules/aws-sdk/lib/request.js:697:14)\n    at Request.transition (webpack-internal:///./node_modules/aws-sdk/lib/request.js:30:10)\n    at AcceptorStateMachine.runTo (webpack-internal:///./node_modules/aws-sdk/lib/state_machine.js:16:12)\n    at eval (webpack-internal:///./node_modules/aws-sdk/lib/state_machine.js:27:10)\n    at Request.eval (webpack-internal:///./node_modules/aws-sdk/lib/request.js:46:9)\n    at Request.eval (webpack-internal:///./node_modules/aws-sdk/lib/request.js:699:12)\n    at Request.callListeners (webpack-internal:///./node_modules/aws-sdk/lib/sequential_executor.js:127:18)\n    at callNextListener (webpack-internal:///./node_modules/aws-sdk/lib/sequential_executor.js:102:12)\n    at EventEmitter.onEnd (webpack-internal:///./node_modules/aws-sdk/lib/event_listeners.js:302:13)\n    at EventEmitter.emit (webpack-internal:///./node_modules/node-libs-browser/node_modules/events/events.js:136:5)\n    at features.constructor.finishRequest (webpack-internal:///./node_modules/aws-sdk/lib/http/xhr.js:130:13)\n    at XMLHttpRequest.eval (webpack-internal:///./node_modules/aws-sdk/lib/http/xhr.js:41:14)\nI may be missing something obvious.  If any of you have suggestions they would be much appreciated.\n. ",
    "jasonwilson": "Thanks for the quick response! Yes, I verified the region is set correctly via the endpoint:\nendpoint: {\"protocol\":\"https:\",\"slashes\":true,\"auth\":null,\"host\":\"elasticbeanstalk.us-west-2.amazonaws.com\",\"port\":443,\"hostname\":\"elasticbeanstalk.us-west-2.amazonaws.com\",\"hash\":null,\"search\":null,\"query\":null,\"pathname\":\"/\",\"path\":\"/\",\"href\":\"https://elasticbeanstalk.us-west-2.amazonaws.com/\"}\n. I tried calling 'createApplication' and that revealed a more interesting error-- I didn't have permissions to create a new resource. Once I opened up the IAM policy the read worked as expected. Perhaps the read shouldn't fail silently in this case? Thanks again.\n. ",
    "0rigin85": "Ah, and this is how I'm constructing my endpoint.  new AWS.Endpoint( \"http://localhost:8001\" )\n. ",
    "gCurtisCT": "I just needed to load the config before I made the s3 object. Oops.\n. ",
    "nkratzke": "What a pity :-(\n. Thanks a lot for this information.\nNane\nLoren Segal notifications@github.com schrieb am Fr., 12. Juni 2015 um\n01:47 Uhr:\n\n@nkratzke https://github.com/nkratzke Quick update in case you're still\ninterested! Amazon EC2 has recently introduced CORS support and the AWS.EC2\nservice is now available in the hosted build of the AWS SDK for JavaScript\nin the browser:\nhttp://blogs.aws.amazon.com/javascript/post/Tx30V18CDJFNSB/Announcing-CORS-Support-for-Amazon-EC2\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/200#issuecomment-111308172.\n. \n",
    "evansolomon": "I thought about doing that.\nJust to clarify one thing, the default behavior of eachPage in my branch is still to keep iterating on its own.  It only switches into this new mode if the callback you pass accepts 3 arguments (the 3rd is the new callback supplied by eachPage that controls iteration).  Not to say that's definitely the way it should work, just that this is how I first went about it.\n. Since it sounds like you're into it, I'll work on doing the same for eachItem.\n. Awesome, thank you for picking this up after I orphaned it.\n. Thanks for the quick fix.\n. Thanks.\nI also added a fallback in my library to try to work around the issue in versions where it's present. Obvious/canoe#23\n. > it looks like the skew is actually 5 min and not 48 hours\nI assume you mean this line\n20150307T194740Z is now earlier than 20150309T203545Z (20150309T204045Z - 5 min.)\nI parsed that as March 7, 2015 19:47 is earlier than March 9, 2015 20:40.  I don't really know what the 5 minute thing on the end means, I was thinking maybe that was the threshold.\n\nWould you be able to re-run the date command on your Docker image and let me know what you find?\n\nJust tried again on an image I haven't used since yesterday and got \"Tue Mar 10 18:22:48 UTC 2015\" (which is correct at the time I'm writing this).\n\nDo you also encounter the error when you run the same code outside your Docker image?\n\nI'm not sure.  Our workflow is pretty heavily Docker-centric, so this isn't much of a use case for me.\n\nwould you be able to share your Docker image?\n\nIt is based on https://github.com/phusion/passenger-docker.  The only differences are some nginx configs, the shared ~/.aws directory, and my apps.\n. @kyriesent Sorry, never really figured it out\n. Thanks for the update. Is there a reason this shouldn't be on by default?\n. Are there cases where systemClockOffset is set to 0 and the user would not want the skew correction to be applied? It seems like that would be a sensible default, but maybe I'm missing something.\n. ",
    "dernotte": "Thanks for your answer. I will see what I can do on my side to remove the extended path in the URL of the API.\n. ",
    "c4milo": "@lsegal out of curiosity, where do you guys get the services contracts from? \nhttps://github.com/aws/aws-sdk-js/tree/master/lib/services/api\n. @lsegal What is the source? I'm building an AWS SDK for Go, using the EC2 WSDL which as you know, is going to be deprecated in March.\n. Nice, so, I'm going to assume that the Ruby AWS SDK is the canonical source for the services contracts, and I'll get them from there every time.\n. Meh, but what is the source? Where do you guys pull the json contracts from?\n. I don't know what's the problem with just answering saying \"Hey, those JSON files are generated internally by us and we don't want people to know what the source is nor open source the generator code\"\n. I already knew they were contracts. I was more curious about how those JSON files were being generated and from where. Are there any plans for aws-sdk-go? \n. @lsegal no Go plans?\n. @lsegal thanks for answering back. I'm asking because I'm about to start one if you guys don't have plans in the near future for it. I just don't want to waste my time if Amazon is thinking in supporting Go too. \n. ",
    "epowers": "The call to s3.headBucket should call its callback once with the network error as in the case of safari and chrome.\nHowever, firefox calls the callback once with an empty requestid response object, null error, and then calls the callback a second time with the error.  The first callback is incorrect, there should not be a non-error response if the bucket is inaccessible.\nSo please consider the error a red herring in this example.  The error code is \"NetworkingError\" and message is \"Network Failure\" and statusCode 0.\nThe call to ec2 is provided because there is another behavior I was trying to replicate where the requests and responses seem to be getting intermixed somehow, seemed like I was getting a call to the s3.headBucket callback with a response object from ec2.describeInstances call.\nLemme see if I can push this out to s3 so you can run it and debug.\nhttps://s3.amazonaws.com/ec2loadtest/test.html\nThere, it is using the released aws-sdk.js, just throw in credentials and hit start and in firefox, put a debugger breakpoint in the callback.\nThank you!  I have so little time to put into this, really just late at night after the kids are asleep.\n. Wish Github had a Like button!\n. ",
    "Tombert": "Also, upon looking at the debugger in chrome, the admin credentials is indeed returning the proper credentials, and the data object looks like this \n{accessKeyId: 'akid', secretAccessKey: 'secret'}\n. I believe CORS actually is configured. \nif I go to the bucket list, click properties, then click \"Edit CORS configuration\" under Permission, I have the following. \n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n    <CORSRule>\n        <AllowedOrigin>*</AllowedOrigin>\n        <AllowedMethod>HEAD</AllowedMethod>\n        <AllowedMethod>GET</AllowedMethod>\n        <AllowedMethod>PUT</AllowedMethod>\n        <AllowedMethod>POST</AllowedMethod>\n        <AllowedMethod>DELETE</AllowedMethod>\n        <AllowedHeader>*</AllowedHeader>\n    </CORSRule>\n</CORSConfiguration>\nIs there another place to edit CORs that I missed?\n. Is there any update on this?  Can someone please explain what I'm doing wrong? \n. Sadly the company I was working for on this site went bankrupt about a year ago, so I don't have an opportunity to try this.\n. ",
    "mrcoles": "This is probably completely irrelevant now, but did you try adding <AllowedMethod>OPTIONS</AllowedMethod>?\n. ",
    "marcosnils": "\nThis is probably completely irrelevant now, but did you try adding OPTIONS?\n\nS3 doesn't allow you to confgure OPTIONS as a value for the AllowedMethod tag. Any hints on this?\n. ",
    "evandrix": "\n\nThis is probably completely irrelevant now, but did you try adding OPTIONS?\n\nS3 doesn't allow you to confgure OPTIONS as a value for the AllowedMethod tag. Any hints on this?\n\nError\nFound unsupported HTTP method in CORS config. Unsupported method is OPTIONS. ",
    "jfirebaugh": "I am also seeing this error during dynamodb requests on a fairly heavily loaded c3.8xlarge. It manifests with the following stack trace:\nTimeoutError: Missing credentials in config\n    at ClientRequest.<anonymous> (/usr/local/src/app/node_modules/aws-sdk/lib/http/node.js:51:34)\n    at ClientRequest.g (events.js:180:16)\n    at ClientRequest.EventEmitter.emit (events.js:92:17)\n    at Socket.emitTimeout (http.js:1797:10)\n    at Socket.g (events.js:180:16)\n    at Socket.EventEmitter.emit (events.js:92:17)\n    at Socket._onTimeout (net.js:326:8)\n    at Timer.unrefTimeout [as ontimeout] (timers.js:412:13)\nI haven't been able to check, as @mlogan suggests, that once a given process starts exhibiting the error it will never recover. I do see some processes succeeding (and can manually curl the metatdata service fine) after other processes have failed.\nI'm using aws-sdk version 2.0.0-rc9.\n. Hmm, this looks very similar to #135.\n. Consistent with https://github.com/aws/aws-sdk-js/issues/135#issuecomment-21289991, I can reproduce this issue with node's own https module: https://gist.github.com/jfirebaugh/67e6306443f978922f17\n. tcpdump output from when this occurs (this time with a single process): https://gist.github.com/jfirebaugh/f97263afb4362a07b1b3\n. This report was mainly my stream of consciousness as I traced the error further towards the source. I agree with you about the questionableness of adding extra checks to the raw stream interface.\nFrom what I can glean from the tcpdump, the server terminated the TCP connection normally before the response data was complete, which means that https://github.com/joyent/node/issues/6300 describes the source of (and potential solution to) this issue. \nMy workaround thus far has been to revert to the non-streamed call. I'm manually validating the received length, but I would be curious to know if aws-sdk has content length checking in place for this case. (I haven't yet been able to check if my manual validation triggers.)\n. Thanks @lsegal. I will post on the forum or open a support request.\n. ",
    "ryan-digbil": "In xhr.addEventListener('readystatechange', function()\nthe buffer = new Buffer(ab.byteLength); allocated 128mb of memory.\nThis is line 8132 of the aws-sdk-2.0.0-rc3.js\n. ab.byteLength is 10215165\nI'm using the chrome profile and debugger, and profiling the memory as I execute steps of the program.\nSo chrome uses 10mb of memory before that line is called, and 128mb of memory after its called. I checked the initializer for Buffer and it seems to allocate the memory when it does this:\nif (isArrayIsh(subject)) {\n  for (var i = 0; i < this.length; i++) {\n    if (subject instanceof Buffer) {\n      this[i] = subject.readUInt8(i);\n    }\n    else {\n      this[i] = subject[i];\n    }\n  }\n} else if (type == 'string') {\n  this.length = this.write(subject, 0, encoding);\n} else if (type === 'number') {\n  for (var i = 0; i < this.length; i++) {\n    this[i] = 0;\n  }\n}\npassing the subject as ab.byteLength causes type to be defined as number.\n. can I get an update on if anyone else has this issue?\n. Thanks, I will do some tests and report back after.\n. I'm having some trouble building it, just a node config issue on my end I'm sure, but to save time can you pass me a non-minmified aws-sdk-js with the latest version of browserify in it? Thanks.\n. Thanks, this seems to have fixed my issues.\nPreviously 48 hours of run-time would use ~250mb of data initially and leak to crash the browser.\nNow my app uses ~30mb of memory and has no leaks.\n. ",
    "wescleveland": "@ryan-digbil seeing this issue here as well, large memory increase after calling getObject.  Have not started digging into any possible fixes yet though.\n. ",
    "keverw": "Does this work in any Node.js Version? I know it says not supported in 0.8.x. Current Version is v0.10.24. Anyone know if this works in newer versions of Node.js?\n. ",
    "programming-kid": "hey i have implement following code \n``` javascript\nvar S3ref = s3.putObject(params, function(err, data) {\n    if (err) {\n        console.log(err);\n    } else {\n        // update db etc \n        console.log(data);\n    }\n\n});\n\nS3ref.on('httpUploadProgress', function (progress) {\n\n    console.log(\"Uploaded\", progress.loaded, \"of\", progress.total, \"bytes\");\n});\nS3ref.send();\n\n```\ni get console.log(data) outputs following twice , what does this mean why is cb executed twice \n```\n{ ETag: '\"2525182f2f68f3014310c8e65e581ead\"' }\n{ ETag: '\"2525182f2f68f3014310c8e65e581ead\"' }\n```\n. hey i have implement following code \n``` javascript\nvar S3ref = s3.putObject(params, function(err, data) {\n    if (err) {\n        console.log(err);\n    } else {\n        // update db etc \n        console.log(data);\n    }\n\n});\n\nS3ref.on('httpUploadProgress', function (progress) {\n\n    console.log(\"Uploaded\", progress.loaded, \"of\", progress.total, \"bytes\");\n});\nS3ref.send();\n\n```\ni get console.log(data) outputs following twice , what does this mean why is cb executed twice \n```\n{ ETag: '\"2525182f2f68f3014310c8e65e581ead\"' }\n{ ETag: '\"2525182f2f68f3014310c8e65e581ead\"' }\n```\n. ok my bad i fixed it \n. ok my bad i fixed it \n. but if i pass a file path instead of Stream to Body, image  in S3 bucket  is corrupted .\n. but if i pass a file path instead of Stream to Body, image  in S3 bucket  is corrupted .\n. :+1:  that was helpful , thanks for the explanation , i was really confused why was it not accepting file path \n. :+1:  that was helpful , thanks for the explanation , i was really confused why was it not accepting file path \n. ",
    "bemurphy": "@lsegal thanks for the pointer, it's definitely coming back with InternalFailure from the service so I'll take it up in another thread as suggested.\n. @lsegal thanks for the pointer, it's definitely coming back with InternalFailure from the service so I'll take it up in another thread as suggested.\n. @mhart Thanks for the tip, that's great.  My query works against dynalite, I dig it.  I prefer the npm over java tooling, too.\n. @mhart Thanks for the tip, that's great.  My query works against dynalite, I dig it.  I prefer the npm over java tooling, too.\n. ",
    "gfodor": "My getItem call looks like this (CoffeeScript)\n```\n    AWS.config.update accessKeyId: \"TEST\", secretAccessKey: \"TEST\", region: \"local\"\n    @dynamodb = new AWS.DynamoDB(endpoint: \"http://localhost:8000\", sslEnabled: false)\n@dynamodb.getItem\n  TableName: <table>\n  Key: { id: { S: <id> } }\n  ConsistentRead: true\n  (err, data) ->\n    if data\n      # Do something\n    else\n      callback(err, null)\n\n```\n. My getItem call looks like this (CoffeeScript)\n```\n    AWS.config.update accessKeyId: \"TEST\", secretAccessKey: \"TEST\", region: \"local\"\n    @dynamodb = new AWS.DynamoDB(endpoint: \"http://localhost:8000\", sslEnabled: false)\n@dynamodb.getItem\n  TableName: <table>\n  Key: { id: { S: <id> } }\n  ConsistentRead: true\n  (err, data) ->\n    if data\n      # Do something\n    else\n      callback(err, null)\n\n```\n. Fix confirmed -- thanks for the quick turnaround!\n. Fix confirmed -- thanks for the quick turnaround!\n. ",
    "mattrenaud": "I am also having this issue with 1.17.1 (1.17.0 works fine):\nTypeError: Object #<Object> has no method 'translate'\n    at AWS.JSON.Builder.inherit.translate.list (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/json/builder.js:30:35)\n    at each (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/util.js:391:32)\n    at translate (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/json/builder.js:28:21)\n    at JSONBuilder.AWS.JSON.Builder.inherit.translate.timestampFormat (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/json/builder.js:50:27)\n    at JSONBuilder.each (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/util.js:391:32)\n    at JSONBuilder.translate (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/json/builder.js:48:21)\n    at JSONBuilder.AWS.JSON.Builder.inherit.translate.list (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/json/builder.js:30:35)\n    at JSONBuilder.each (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/util.js:391:32)\n    at JSONBuilder.translate (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/json/builder.js:28:21)\n    at JSONBuilder.build (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/json/builder.js:20:32)\n    at Request.buildRequest (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/service_interface/json.js:18:32)\n    at Request.callListeners (/Users/matt/Code/user/node_modules/dx/node_modules/aws-sdk/lib/sequential_executor.js:117:20)\n...(more stack)\n. ",
    "deckchairhq": "Great - Many thanks for the quick response and patch.\n. ",
    "jeevankk": "@lsegal \nhttps://github.com/aws/aws-sdk-js/blob/5ae1bd936829675e41e50d83cc0765d6166db648/lib/service_interface/rest_json.js#L36\nAccording to this line \njavascript\nresp.data.RequestId = resp.httpResponse.headers['x-amz-request-id'] ||\n                          resp.httpResponse.headers['x-amzn-requestid'];\nyou are setting the request id to resp.data.RequestId. Can I access like this.data.RequestId ? \n. I think request id should be a part of error which is returned by AWS-SDK, since AWS support team will not be able to help us until we provide them the request id. \n. @lsegal \nIs request object exposed for aws-sdk for Node.js?\n. ",
    "ivanakimov": "Thanks for quick reply. +1\n. Thanks for quick reply. +1\n. ",
    "gordingin": "Thanks for the quick response. I tried all browsers. \nI also tried getSignedURL with this code.\nvar params = { Bucket: 'KrausseFamily', Key: 'AKIAIDXA5UQNTXIRTLZQ' };\nvar url = s3.getSignedUrl('getObject', params);\nconsole.log('The URL is', url);\nbut it returns this URL\nhttps://s3.amazonaws.com/KrausseFamily/AKIAIDXA5UQNTXIRTLZQ?AWSAccessKeyId=AKIAIDXA5UQNTXIRTLZQ https://s3.amazonaws.com/KrausseFamily/AKIAIDXA5UQNTXIRTLZQ?AWSAccessKeyId=AKIAIDXA5UQNTXIRTLZQ&Expires=1390213125&Signature=sS%2FHo4XsOxR%2FLE%2B3q92WonxoIsc%253 &Expires=1390213125&Signature=sS%2FHo4XsOxR%2FLE%2B3q92WonxoIsc%3\nIf I use that link, I get NoSuchKey.\nNot sure why my key is at the end of my bucket. If I try to replace it with a real path to my image, I get Access Denied.  \nMy images aren't public, I rather they not be.\nthx\nRalph\nFrom: Loren Segal [mailto:notifications@github.com] \nSent: Monday, January 20, 2014 4:51 AM\nTo: aws/aws-sdk-js\nCc: gordingin\nSubject: Re: [aws-sdk-js] Calling listObjects returns toString Error (#218)\nI'm unable to reproduce the failure with your code. What browser are you testing this in?\nTo answer your question, you can use the S3.getSignedUrl http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-examples.html#Getting_a_pre-signed_URL_for_a_getObject_operation  function to get URLs for images. If the image is public, you can just point directly to the endpoint: https://yourbucket.s3-us-west-2.amazonaws.com/yourimage.jpg.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/aws/aws-sdk-js/issues/218#issuecomment-32746040 .  https://github.com/notifications/beacon/6098691__eyJzY29wZSI6Ik5ld3NpZXM6QmVhY29uIiwiZXhwaXJlcyI6MTcwNTc0NDI1OCwiZGF0YSI6eyJpZCI6MjM4NDc5MzB9fQ==--50ee15b67838e8c8cfb167b301e1b904aa996747.gif \n. See, that is what happens when you code at 2am.\nThanks, that works. But it should be name path or something other than key.\n. I still have the other issues. So I create a simple app.\n```\n\n\n    try\n    {\n        AWS.config.update({ accessKeyId: 'AKIAIDXA5UQNTXIRTLZQ', secretAccessKey: 'xxx' });\n        var s3 = new AWS.S3({ apiVersion: '2006-03-01' });</p>\n<pre><code>    var params = {};\n    var url = s3.listBuckets(params, function (err, data) {\n        alert(err)\n    });\n}\ncatch (err) {\n    alert(err);\n}\n</code></pre>\n<p>\n```\nI get this error: Unable to get property 'toString' of undefined or null reference. Looks like I am getting this other api call also.\nthx\nRalph\n. Thanks, I will take a look. Do you have a page I can refer to to resolve this?\n. ",
    "carlnordenfelt": "Hi,\nYou are correct, this appears not to come through at all.\ndynamo.describeTable(requestData, function (error, dynamoResponse) {\n    console.log('raw', this.httpResponse.body.toString())\n}\nGives the following for GlobalIndexes:\n\"ProvisionedThroughput\":{\"ReadCapacityUnits\":3,\"WriteCapacityUnits\":5}}\nNo info at all about decreases or change times.\nShould I raise an issue directly with Amazon instead?\n. Hi @lsegal,\nI have created a post in the forum on the issue.\nYou can find the thread here: https://forums.aws.amazon.com/thread.jspa?messageID=517914\nFeel free to add more details if you think it might help.\nRegarding the low level APIs, it does say that this infomration should be included:\nUnder the headline GlobalSecondaryIndexes:\n\"ProvisionedThroughput - The provisioned throughput settings for the global secondary index, consisting of read and write capacity units, along with data about increases and decreases. \"\nFurther reading: http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_ProvisionedThroughputDescription.html\nSince the indexes appear to have individual limits it makes sense to provide them. And they are, as I mentioned previously, included in the updateTable response so for consistency it should be there in describeTable as well.\nThanks for you help!\nPS. I'll unavailable during the coming two weeks but I've asked a colleague to keep an eye on this thread and the one on Amazon.\n. Hi Loren,\nYou are absolutely right! :)\nI tried disabling all my workers except for one so that I only have two simultaneous polls and that did the trick. \nThank you :)\n. Hi,\nThanks for the quick and descriptive response.\nAdding that timeout sorted the issue. \nI must admit that I kind of recognize this now that I have the solution so it might have been something I've come across before which leads me to ask if it wouldn't be a good idea if the SDK set a default timeout?\nIt isn't obvious that it'll behave this way and it should be easy enough to set a default timeout that is bigger then the max wait time since, at least for the time being, the max wait time is 20 seconds. In the presence of WaitTimeSeconds the default timeout could be something like WaitTimeSeconds + 5 seconds for example and otherwise falling back to the standard 25, or even 60 seconds. \nIt's good to be able to override the value by setting the httpOptions but having a default to prevent consumers from freezing like this seems reasonable unless I'm overlooking some aspect?\n. Hi,\nI understand your concerns, especially if they change the maximum 20 in the\nfuture. Mentioning this in the documenation would be a great first step\nthough.\nBr,\nCarl\nOn 30 Sep 2014 00:35, \"Loren Segal\" notifications@github.com wrote:\n\n@polythene1337 https://github.com/polythene1337 it may make sense to\nset a default timeout-- we've looked into it before but backed out due to\nthe possibility of inadvertently breaking other users. If we made that\nchange I'd want to do more investigation to ensure we were properly future\nproofed. My biggest concern would be that we set the timeout to 25 seconds\nnow, and then in a few months SQS increases their max timeout, breaking\ncustomers who might be using the SDK normally, not realizing there is a\ndefault timeout.\nI think following the default timeout behavior of your underlying\nenvironment makes more sense and should be more consistent a behavior. Keep\nin mind that this is an issue with any HTTP connection you make in your\nNode.js process, so being aware of the timeout settings is generally\nimportant in Node. It's possible we just need to make this more clear in\nour SDK documentation / guides. This would be the path that is least likely\nto randomly break users in the future.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/367#issuecomment-57241145.\n. Hi,\n\nYes that helps a lot. It works pretty much as I thought it did :)\nFrom the snippet I pasted I get that the sum of the delay of all retires would be 25550ms ~ 25 seconds which is consistent with the delays we are seeing.\nI agree that in general you want the sdk to execute the retries but in our specific case we're not being throttled on the table but rather on a partition but that's another story.\nJust so that I don't misunderstand, when you mention overriding the properties in AWS.events.on('retry', ...) I assume that doing so is still in the global scope and not possible to do for a specific operation, such as a putItem request?\nCheers\n. Sorry, I completely misread that.\nThanks for your answers, this will help a lot.\nBr,\nCarl\nOn 5 Nov 2014 23:20, \"Loren Segal\" notifications@github.com wrote:\n\nJust so that I don't misunderstand, when you mention overriding\nAWS.events.on('retry', ...) I assume that doing so is still in the global\nscope and not possible to do for a specific operation, such as a putItem\nrequest?\nYou can add event hooks for individual requests, I was just trying to\nprovide some simple debugging code. To attach the event to an individual\nrequest:\nvar req = dynamodb.putItem(params);\nreq.on('retry', function() { ... });\nreq.send(function(err, data) {\n  console.log(err, data);\n});\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/402#issuecomment-61892099.\n. Any updates on this issue?\nThere are several inconsistencies between the JS SDK (or perhaps all SDKs) and the CLI. For example, the CLI expects region in AWS_DEFAULT_REGION whilst the SDK wants AWS_REGION. Same goes for profile where the CLI uses AWS_DEFAULT_PROFILE and the SDK uses AWS_PROFILE.\nIt's not a huge problem once you've discovered these quirks and created scripts for it, it's just a bit annoying when you stumble across it have to figure out why something on one end doesn't work on the other :)\n\nI have written a few simple scripts to manage profiles and they deal with these inconsistencies too:\nhttp://www.jayway.com/2015/09/25/aws-cli-profile-management-made-easy/\n/ Carl\n. I'm running on Lambda and doing just what you want to do I think:\nvar v4 = require('aws-signature-v4');\nfunction signEndpointUrlAwsV4(endpointAddress) {\n    var signedUrl = v4.createPresignedURL(\n        'GET',\n        endpointAddress,\n        '/mqtt',\n        'iotdevicegateway',\n        signatureV4Hash,\n        {\n            key: process.env.AWS_ACCESS_KEY_ID,\n            secret: process.env.AWS_SECRET_ACCESS_KEY,\n            region: process.env.AWS_REGION,\n            protocol: 'wss',\n            expires: WSS_URL_EXPIRY_SECONDS\n        }\n    );\n    return signedUrl + '&X-Amz-Security-Token=' + encodeURIComponent(process.env.AWS_SESSION_TOKEN);\n};. Ha! My mistake!\n\nI only updated the sdk version, I never deployed the app :). ",
    "timkock": "Thanks Isegal , appreciate your answer. I just had the feeling there was some handling that could be done to redirect requests properly. Maybe if I have more time I can look into it. For now setting the region upon initialisation helps :) \n. ",
    "my8bird": "I have run some tests, by adding some logging to Request to log when all the events happen per request, and determined that it appears the requests are happening in parallel, as I had expected initially and then got myself in a tangent.  \nThis leaves me back at square one with trying to figure out why after N putObjects in batches of M I get the time skewed error.\nMy actual code is probably to large for this but this is analogous, perhaps it is a logic not code flaw.  If you say this is correct then I will dig through my code some more and see where it does not conform to this.  With this code the only thing that seems to change as the uploads are in progress is the size of the files.  If, perhaps, a large file takes to long to upload would that cause the subsequent uploads to be skewed (given the below procedure).\n``` javascript\n// Please forgive any syntax errors as I free formed this code\nfunction uploadFiles(files, maxUpload) {\n   var done_d = Q.defer(), \n       results = [],\n       next_idx = 0;\n// Add some work\n   function add_work() {\n      next_idx = next_idx + 1;\n      var file = files[next_idx];\n  var client = new aws.S3();\n  return Q.ninvoke(client, 'putObject', {\n        Bucket: 'bucket',\n        Body:   file,\n        Key:      'some random'\n     })\n     // When this upload finishes schedule the next one\n     .then(work_done, work_done);\n\n}\nfunction work_done(res) {\n      // Store results for the end\n      results.push(res);\n     // If there is more work to do then add it\n     if (next_idx < files.length) {\n         return add_work;\n      }\n      // If all work is done then end this session\n      if (results.length === files.length) { done_d.result(results); }\n      return res;\n   }\n// Start up to N file uploads\n   _(files).first(maxUploads).map(add_work);\nreturn done_d.promise;\n}\n// Heterogeneous set of files (essentially files from 1 to 20 megabytes\nvar files = ['1.txt','2.txt','3.txt','4.txt','5.txt','6.mpg','7.mpg','8.mpg','9.png','10.txt','11.txt','12.txt'];\nvar file_buffers = _.map(files, fs.readFileSync);\nuploadFiles(files, 5);\n```\n. I guess i glossed over that in the sample.  Yes, I am currently making a new client for each upload (just a convenient spot to scope the variable).  Does that tell you something?\n. One other thing that seems very relevant to this discussion is that this same code (at least the real code) works correctly for hours if i set the maxUpload to 1.\nI updated the example to show the client correctly.\n. The code should have N requests in flight at a time and then create a new one after each on finishes.  At least that was the intent and adding debug logging provided what I expected.  \nMy bandwidth could be very slow for these uploads.  If a large file that simply takes longer then 15 minutes to upload can cause this issue then this issue could be completely explained by me over saturating my network and causing the uploads to take too long.  That would also explain why when uploading one at a time things work as expected.\n. 95% certain you just nailed it with the agent.maxSockets thing since I kept trying to upload 10 at a time.  With this flag set to 5 I was forcing the later uploads to be \"paused\" until one of 5 executors was available, which on a slow network could take longer then 15min.\nI will have to setup a new repo (as I don't want to mess with the family photos) but I will report back.\nAside, I will try async again as a helper like that is amazingly useful.  I tired it a long time ago and convinced myself I did not need a lib as callbacks were \"not that bad\".  I also ported the twisted deferred system to JS long ago https://github.com/my8bird/nodejs-twisted-deferreds but never actually used it since others had \"established\" solutions, although every so often I hear from people using it.\n. I have done some more testing and my issue was definitely the maxSockets setting so I was right about something cutting into the parallel'ness of code but was wrong about the spot causing it.\nPerhaps, this setting could be documented somewhere?\nAt any rate I think this issue is resolved.\n. ",
    "brentmc79": "Gah! Sorry I lied.\nI thought I saw 1.17.3 in package.json, but it was really \">= 1.17.3\". Running npm list revealed that I'm actually using v2.0.0-rc8.\nI just repeated your test with v2.0.0-rc8 checked out and it appears to be working correctly in the console. Not sure what's going on in my Node app, as I've verified that I'm passing in an empty hash/object to describeDBInstances, like so:\nrds.describeDBInstances {}, (err, data) ->\n  if err\n    throw \"Error finding DB: #{err}\"\nbut it repeatedly displays this:\nError finding DB: MissingRequiredParameter: Missing required key 'DBInstanceIdentifier' in params\n. Crap, sorry for wasting your time. I just found the issue, and it was a problem with my code. \n. I was, as I mentioned, calling describeDBInstances with an empty params object, so that wasn't the problem. Unfortunately a few lines later I was calling createDBInstance, also with an empty params object which was giving me the MissingRequiredParameter exception. The way I was logging the exception led me to believe it was coming from the describe call, not the create call. Essentially it was a copy/paste error.\nThe only thing that might've helped here, other than me not being a doofus, would be if the error description included the api method that was requiring the missing parameter.\nAgain, sorry for wasting your time and thanks for the super-fast responses.\n. ",
    "Soviut": "I was able to deal with this by overriding the isBrowser function in the code that requires it and making it always return false:\nvar aws = require('aws-sdk');\naws.util.isBrowser = function() { return false; };\nThis feels hacky and prone to failure in the future, so some kind of explicit override would be nice.\n. I was able to deal with this by overriding the isBrowser function in the code that requires it and making it always return false:\nvar aws = require('aws-sdk');\naws.util.isBrowser = function() { return false; };\nThis feels hacky and prone to failure in the future, so some kind of explicit override would be nice.\n. node-webkit is a chrome-less webkit browser that lets you create desktop apps using html,css and js but with access to local node libraries and functions, so my angular app can list my file system using fs for example.\nThe problem is that because node-webkit acts as both a local app and a browser, it confuses the aws library, leading it to believe it's running in a browser when I want it to run as a local application. Because node-webkit is considered local, it has no navigator object defined, and the aws library, which thinks I'm in a browser, but with no navigator, throws a reference error.\n. node-webkit is a chrome-less webkit browser that lets you create desktop apps using html,css and js but with access to local node libraries and functions, so my angular app can list my file system using fs for example.\nThe problem is that because node-webkit acts as both a local app and a browser, it confuses the aws library, leading it to believe it's running in a browser when I want it to run as a local application. Because node-webkit is considered local, it has no navigator object defined, and the aws library, which thinks I'm in a browser, but with no navigator, throws a reference error.\n. @lsegal For whatever reason, while reverting back to an older version of my code, I can't reproduce the issue anymore.\n. @lsegal For whatever reason, while reverting back to an older version of my code, I can't reproduce the issue anymore.\n. ",
    "nscott": "Uhg, maybe I'm using an outdated version of the API. I tried passing my own agent through and it never worked... Well, that would indeed fix this problem. Sorry for the trouble, and thanks.\n. Thanks, I really appreciate that @lsegal!\n. ",
    "seeekr": "@lsegal awesome! I hadn't had the chance yet to get back to the issue and it's great to see that you've figured it out without further input from me! What you're saying sounds great!\nre: leaving out ProviderId for Google+ - I had tried that but it didn't work and at that time I thought it might be that the server side API maybe defaulted to using the Amazon (or FB, but Amazon seemed more reasonable) auth mechanism and I thought that's why it had failed. BUT after trying it with the properly validating ProviderId: null I still couldn't get it to work (and couldn't figure out why) so it probably did default to using Google+, but maybe you can double-check that.\nre docs: Yes, I do think that until there is a new RC release of the API the docs should be updated to reflect that ProviderId: null will not work and instead to leave it out for Google+, maybe ideally with a note highlighted in red so it's clear what would be the \"proper\" way of doing it and that just until RC10 (or whatever release is next) the workaround is to do it this way.\n. @lsegal and wow, I just had a look at the commit - it would have taken me ages to get into the code well enough to be able to make (suggest) the appropriate changes myself :) thanks!\n. @lsegal oh god, I totally used access_token. I just double-checked what the (otherwise pretty good and easy to follow, good work there!) AWS JS SDK docs are saying and there's no mention of this, it just says ACCESS_TOKEN. I feel like it would remove a lot of potential for frustration for newbie \"3rd party auth\" (or w/e we can call this) users like me if there was a note in the docs about this particular thing with using Google+ auth. It didn't even occur to me to look for something other than access_token since the name fits so perfectly with what the docs are saying.\n. OK, so I had to go back and try that. Yes, there is definitely a difference between using ProviderId: null and leaving it out. And actually... it now works if I don't specify that field. If I set it to null (and it puts it into the request like that) I get an HTTP 403 error.\nre: getting started experience - cool, it's pretty good already though :) Docs are very easy to follow, even though some of the process is a bit tedious and requires lots of steps in different UIs.\n. ",
    "cadorn": "Thats too bad about the single expectation. Would be much more useful if I could specify a few attributes.\n. ",
    "theefer": "That's great, thanks!\nI've been working on a CloudWatch web component to be released soon, so that change will definitely make it easier to depend on the AWS JS SDK!\n. ",
    "jasonsims": "Thanks @lsegal. Creating a node specific aws util module sounds good to me. \n. Thanks @lsegal. Creating a node specific aws util module sounds good to me. \n. Alright, I moved the basic CloudFront signing functionality to aws-cloudfront-sign.\n. Alright, I moved the basic CloudFront signing functionality to aws-cloudfront-sign.\n. ",
    "Peksa": "Ah! You're absolutely right. This solves my problems:\n<ExposeHeader>x-amz-website-redirect-location</ExposeHeader>\nCheers!\n. Ah! You're absolutely right. This solves my problems:\n<ExposeHeader>x-amz-website-redirect-location</ExposeHeader>\nCheers!\n. ",
    "msafi": "Definitely needs better documentation.\nSo if you want to get back the metadata that you save, you have to first specify the ExposeHeader in CORS config:\n<ExposeHeader>x-amz-meta-foobar</ExposeHeader>\nAnd then in your JavaScript when you putObject, you have to only use the last part of the exposed header. In this case, foobar, like so:\njs\n.putObject({\n  Bucket: bucketName,\n  Key: file.key,\n  Body: file.body,\n  ContentType: file.type,\n  ACL: file.acl,\n  Metadata: { 'foobar': JSON.stringify(file.metadata) }\n})\n. @alexpearce92 it is just a simple JS object\n. @alexpearce92 It looks to me that you're specifying the file meta-data in the AWS.S3 constructor configurations. As far as I understand, meta-data is per file, so you wanna specify it in your putObject request.\n. ",
    "alexpearce92": "@msafi \nWhere does \"file\" come from in:\n\njavascript\n.putObject({\n  Bucket: bucketName,\n  Key: file.key,\n  Body: file.body,\n  ContentType: file.type,\n  ACL: file.acl,\n  Metadata: { 'foobar': JSON.stringify(file.metadata) }\n})\n. @msafi \nThanks for the quick reply. \nI am trying to replicate what you explained above by exposing one of my meta headers, x-amz-meta-type. I successfully exposed it in my CORS and am currently trying to implement the above code. However, I'm at a loss as to what I try to JSON.stringify(). Here is my current setup:\n\n``` javascript\nbucket = new AWS.S3({\n  params: {\n    Bucket: 'myBucket',\n    Key: 'myKey',\n    Metadata: {\n      'type': JSON.stringify(?)\n    }\n  }\n});\nvar url = bucket.getSignedUrl('putObject');\nconsole.log('The URL is', url);\n```\nIf you could give me any input I would greatly appreciate it.\n. @msafi \nI guess I may be going about this the wrong way then. My goal is to read the x-amz-meta-type of each file in an s3 folder in order to organize them by the returned meta-type, which will be accomplished by looping through all of the files and performing a putObject. \nI was attempting to retrieve the value of meta-type in a url so that I could substring the value out of the url and organize my files that way.\nDo you know of or is there already a thread about a better way of extracting meta tags from several files?\n. ",
    "tsgautier": "Here is my code (in coffee) that produces this problem:\n``` coffeescript\nCalls back with a readable stream\ndownload = (key, cb) ->\n        request = awss3.getObject { Bucket: s3config.bucket, Key: key }\n    events = 0\n    request.on 'complete', (response) ->\n        return if events++ > 0 # wtf y u call back more than once?\n        return cb response.error if response.error\n        cb null, metadata: response.data.Metadata, stream: request.createReadStream()\n\n    request.send()\n\n```\n. (oh, sorry, well, you see that to make the download work properly I had to debounce the multitude of error and completion events that I normally receive....that's probably a separate issue that should be filed)\n. If I don't do the send, how do you recommend I pick out the metadata?\n. If I don't do the send, how do you recommend I pick out the metadata?\n. this error is occurring node.js\n. this error is occurring node.js\n. ",
    "sharadbiradar": "I am using dropfile.js from polyfill to get the file object.\n. ",
    "igbopie": "Hi!\nSorry for the delay. Looks like a temporal problem. I cannot reproduce the error again.\nI apologize for the inconvenience.\n. ",
    "ThisIsMissEm": "Okay, so, it turns out that my web framework (Hapi) creates a Domain for the request/response lifecycle, this represents that:\nhttps://gist.github.com/miksago/115c337327f64a3f086f\nEssentially, the error gets printed twice.\n. @lsegal could you let me know when you ship a new release?\n. Ah, yeah. Ideally projects should keep their dependencies up to date, and correctly licensed, but I recognise it's hard to do that sometimes.\n. Looking at #69 you were depending on what could be an asynchronous function to be synchronous, so I'd be inclined to think it a mis-usage of xml2js, not a regression in the library. \n. ",
    "danie11am": "I experience the same problem. I'm using aws-sdk 2.0.0-rc.17. \nI know the error is due to missing credentials, and want to make sure it's caught if it ever happens. I found that a TimeoutError would be returned in callback-style in the first 2-3 times this happens, but in around the 3rd time, a CredentialsError is thrown that will crash the process.  \n// My codes\nvar ses = new aws.SES({\n  apiVersion : '2010-12-01'\n});\nses.sendEmail(email, function(err, receipt) {\n  if (err) {\n    logger.error(\"notifyImmediate(): ses.sendEmail failed. err: \" + err);\n    return callback(err);\n  }\n  ..\n};\nThe error:\n/..../node_modules/aws-sdk/lib/request.js:34\n    throw e;\n          ^\nTimeoutError: Could not load credentials from any providers\n    at ClientRequest.<anonymous> (/..../node_modules/aws-sdk/lib/http/node.js:51:34)\n    at ClientRequest.g (events.js:180:16)\n    at ClientRequest.EventEmitter.emit (events.js:92:17)\n    at Socket.emitTimeout (http.js:1797:10)\n    at Socket.g (events.js:180:16)\n    at Socket.EventEmitter.emit (events.js:92:17)\n    at Socket._onTimeout (net.js:327:8)\n    at Timer.unrefTimeout [as ontimeout] (timers.js:412:13)\n. I just tried rc20, it no longer has this problem. Error is returned via callback even after many tries. Thanks @lsegal!\n. ",
    "digihaven": "I forgot to mention it was chrome, and I added the sent header.\n. ",
    "kmalakoff": "@lsegal: I've spoken with the team and it is a lot of effort to remove SSL to investigate your hypothesis...\nIf you could please answer this question first: \"Is the known https bug solved with the 0.11.x version of Node?\"\n. @lsegal: Understood. If this is the case, given that the problem seems to be fixed in 0.11 (for whatever reason and 0.11 has been around for a while and hopefully is reasonably stable for the nearing release of 0.12...with disclaimers), we would prefer to go ahead and just use 0.11 in production on this particular server after having it run on staging for a while. So we would consider this closed because we have a work around.\nIf you would like us to test without SSL in Node 0.10, I'm totally fine to allocate time to this if you believe this is useful information for you; otherwise, we'll skip it. Just let me know if it is useful for you.\n. Sounds good. I really appreciate how quickly you respond. Thank you!\n. I believe that I'm getting a similar problem with the streaming interface where the error event is emitted and the error event is thrown on a 'ECONNRESET'.\nWhat would the work around be for a streaming interface given asynchronous flow control? \nThis is what I tried, but it was unsuccessful (aws-sdk/lib/request.js:34 throw e; ^ Error: read ECONNRESET):\nreq = @s3.getObject({Bucket: bucket, Key: key})\n  req._hardError = false\n  req.createReadStream()\n    .on('error', (err) ->)\nI'm using aws-sdk@2.0.0-rc.19\nThank you!\n. I am currently only encountering the ECONNRESET error. We are streaming multiple images from S3 in parallel using pipe to the response in Express 3.x. \nThis is an intermittent error, but when it happens, we get both the error emitted and an uncatchable error also thrown. Sorry I do not have more precise information than that and it is hard to create a test case for. \nI tend to rarely throw errors and only in synchronous code (eg. guaranteed to be catchable by the caller) so maybe this is just a control flow problem? \n. @lsegal thank you for the quick response and sorry for my slow response. We were troubleshooting networking issues between our data center and S3 that caused the errors in the first place. We will use your updated release. \nAs for domains, it sounds like a good medium term solution - from podcasts I've listened to, I understand there are some difference in opinions on whether to use them, but that has put me off on investigating further. I'll give them another look.\nAny idea when the aws-sdk 2.0.0 will leave rc status? Are you waiting for Node 0.12?\n. ",
    "scottsd": "Thanks, that's what I was looking for!\n. ",
    "cosmosof": "I preferred using aws cognito identitypoolid; \n`\n     var bucketName = '....';\n\n     AWS.config.region = 'us-east-1';\n\n var IdentityPoolId = '........';\n\n\n     AWS.config.update({\n     credentials: new AWS.CognitoIdentityCredentials({\n     IdentityPoolId: IdentityPoolId\n     })\n});\n\n    var bucket = new AWS.S3({\n           params: {\n              Bucket: bucketName,\n         }\n    });\n\n................\n...............\n    var params = {\n\n                Key: objKey,\n\n                ContentType: blob.type,\n\n                Body: blob,\n\n                ACL: 'public-read'\n\n            };\n\n    bucket.putObject(params, function (err, data) {\n\n        if (err) {\n\n           results.innerHTML = 'ERROR: ' + err;\n\n        } else {\n\n           console.log('succesfully uploaded the image!');\n                                                        console.log('https://s3.amazonaws.com/..../'+\"\"+objKey);\n\n    }\n\n. I preferred using aws cognito identitypoolid;\n     var bucketName = '....';\n\n     AWS.config.region = 'us-east-1';\n\n var IdentityPoolId = '........';\n\n\n     AWS.config.update({\n     credentials: new AWS.CognitoIdentityCredentials({\n     IdentityPoolId: IdentityPoolId\n     })\n});\n\n    var bucket = new AWS.S3({\n           params: {\n              Bucket: bucketName,\n         }\n    });\n\n................\n...............\n    var params = {\n\n                Key: objKey,\n\n                ContentType: blob.type,\n\n                Body: blob,\n\n                ACL: 'public-read'\n\n            };\n\n    bucket.putObject(params, function (err, data) {\n\n        if (err) {\n\n           results.innerHTML = 'ERROR: ' + err;\n\n        } else {\n\n           console.log('succesfully uploaded the image!');\n                                                        console.log('https://s3.amazonaws.com/..../'+\"\"+objKey);\n\n    }\n\n`. ",
    "rcfrias": "For aws cognito you are using the SDK. I guess there must be a distinction on what the original post is about. \nIf I understand correctly, @scottsd was concerned about creating credentials for each user in order to let them upload files to S3.\n\n\nPre-signed URL's is the preferable non-invasive way of letting unknow users upload files to S3. Although, there must be one user (the one providing credentials server side) that is using his account to generate the pre-signed url.  Obviously, that user needs to have permissions to perform such operation. (SDK client-side or any AWS related interaction other than the upload is not required) *Credentials not required... client side!\n\n\nAssume Role. If you are managing a cognito user pool or any other Authentication provider, you can create a \"Federated Identity\" that will let your users \"Assume a Rol\" with S3 privileges or any other privileges you define for a role, for Authenticated and Non-authenticated users, depending how you configure it. (SDK client-side required for simplicity, AWS interactions for authentication and signing required) *Credentials required client side!\n. Not sure, I came to this hypothesis while trying to adapt the Cognito-Auth to Angular(4) Web app.\nSo yeah, it makes sense that this wont be a problem in a Node.js app.  Now I just need a way of telling CognitoAuth that we are in a browser.  More to follow.. Yeah, so, it requires \"/aws-sdk/global.js\" and \"/aws-sdk/clients/cognitoidentityserviceprovider.js\n\", and that is doing something wrong.... After that update in SystemJS\nim dealing with this:\n\n\n(index):21 Error: (SystemJS) require is not a function\n    TypeError: require is not a function\n        at eval (http://localhost:3000/node_modules/aws-sdk/global.js:1:1). I think the amazon-cognito-auth.js script is the problem, because it has at the beginning this:\n(function webpackUniversalModuleDefinition(root, factory) {\n    if(typeof exports === 'object' && typeof module === 'object')\n        module.exports = factory(require(\"aws-sdk/global\"), require(\"aws-sdk/clients/cognitoidentityserviceprovider\"));\n    else if(typeof define === 'function' && define.amd)\n        define([\"aws-sdk/global\", \"aws-sdk/clients/cognitoidentityserviceprovider\"], factory);\n    else if(typeof exports === 'object')\n        exports[\"AmazonCognitoIdentity\"] = factory(require(\"aws-sdk/global\"), require(\"aws-sdk/clients/cognitoidentityserviceprovider\"));\n    else\n        root[\"AmazonCognitoIdentity\"] = factory(root[\"AWSCognito\"], root[\"AWSCognito\"][\"CognitoIdentityServiceProvider\"]);\n})(this, function(__WEBPACK_EXTERNAL_MODULE_7__, __WEBPACK_EXTERNAL_MODULE_11__) {\nreturn /******/ (function(modules) { // webpackBootstrap\nAnd the console is showing this message:\n(SystemJS) Cannot set property '__esModule' of undefined\n    TypeError: Cannot set property '__esModule' of undefined\n        at eval (http://localhost:3000/node_modules/aws-sdk/dist/amazon-cognito-auth.js:102:50)\n        at Array.forEach (<anonymous>)\n        at Object.exports.__esModule (http://localhost:3000/node_modules/aws-sdk/dist/amazon-cognito-auth.js:101:28)\n        at __webpack_require__ (http://localhost:3000/node_modules/aws-sdk/dist/amazon-cognito-auth.js:48:30)\n        at exports.__esModule (http://localhost:3000/node_modules/aws-sdk/dist/amazon-cognito-auth.js:68:18)\n        at eval (http://localhost:3000/node_modules/aws-sdk/dist/amazon-cognito-auth.js:71:10)\n        at webpackUniversalModuleDefinition (http://localhost:3000/node_modules/aws-sdk/dist/amazon-cognito-auth.js:27:35)\n        at eval (http://localhost:3000/node_modules/aws-sdk/dist/amazon-cognito-auth.js:28:3)\n        at eval (<anonymous>)\n    Evaluating http://localhost:3000/node_modules/aws-sdk/dist/amazon-cognito-auth.js\nNow I guess, I will try again with the sources, maybe I can get a way around.\n. ",
    "kennovak-zy": "Further to this question: AWS supports a managed uploader in the SDK ( https://aws.amazon.com/blogs/developer/announcing-the-amazon-s3-managed-uploader-in-the-aws-sdk-for-javascript/ ) which handles multi-part uploads and supplies progress checks, among other features.  Can this be used with presigned S3 URLs?  Or must the user be logged in to AWS first?. ",
    "kepi0809": "\nIf you already have a presigned URL generated for the browser, you can simply send an XHR request with that URL and the payload to upload to S3. The SDK would not be required to do this. A jQuery example below:\njs\n$.ajax({\n  url: presignedUrl, // the presigned URL\n  type: 'PUT',\n  data: 'data to upload into URL',\n  success: function() { console.log('Uploaded data successfully.'); }\n});\nIf you're asking how to generate a signed URL, this would have to be done on the server side, not the client. Are you running node.js on the server? If not, you will want to use an SDK for the server environment (we have SDKs for Ruby, Java, Python, and .NET, as well).\n\nwhy does it have to be on the server side? I have this in my Vue app and it works perfectly: \n```\nconst aws = require('aws-sdk')\naws.config.update({\n  secretAccessKey: process.env.VUE_APP_AWS_SECRET_ACCESS_KEY,\n  accessKeyId: process.env.VUE_APP_AWS_ACCESS_KEY,\n})\nexport const s3 = new aws.S3({\n  signatureVersion: 'v4',\n  region: process.env.VUE_APP_AWS_REGION,\n})\n```\nand then I'll just use the s3 instance in my functions. Is this a bad design? In case it is then why? \nThanks in advance!. ",
    "danielbeardsley": "Thank you for responding and providing a simple test-case. I believe I discovered the problem, I was running this from a machine on the west coast accessing a bucket on the east coast.. the RTT had a big effect. Even then, it wasn't 3s (more like 1.5-1.7), I must have calculated wrong.\nClosing, not a bug.\n. Thank you for responding and providing a simple test-case. I believe I discovered the problem, I was running this from a machine on the west coast accessing a bucket on the east coast.. the RTT had a big effect. Even then, it wasn't 3s (more like 1.5-1.7), I must have calculated wrong.\nClosing, not a bug.\n. ",
    "joeyvandijk": "From what I see is different: the initialize-function is settings options, but does not grab the global endpoint. Perhaps that is a tip...or maybe not. FYI\n. From what I see is different: the initialize-function is settings options, but does not grab the global endpoint. Perhaps that is a tip...or maybe not. FYI\n. You are right, I was using 2.0.0-rc9 apparantly and it has been fixed, so sorry for the inconvenience.\n. You are right, I was using 2.0.0-rc9 apparantly and it has been fixed, so sorry for the inconvenience.\n. ",
    "alexfarrill": "Sorry, I didn't realize there was a more recent version, going to try your 2.0.0-rc12 release and will reopen if not resolved.. Thanks!\n. ",
    "gfemec": "Great, should I submit a pull request to https://github.com/aws/aws-sdk-js-apis ?\n. Awesome, thanks for the quick response.\n. ",
    "nodeGarden": "Also, confirmed version: aws-sdk@2.0.0-rc13\n. @lsegal : Yes, it is an AMI user. Are there special setups for those users? I switched to my Root credentials, and that works. I'd really like to not have such open permissions though.\n. Thanks. I had thought the ACL was the setting for after it was uploaded. I'll check the other forum for something on this.\n. @lsegal I am pushing a completely new object. I tested on an existing just to be sure and get the same access denied.\n. @thetable , Unfortunately no. I have just been using the Master account (very much un-ideal though)\n. ",
    "thetable": "@nodeGarden Did you find out what the problem was with S3? I seem to have the same trouble setting the the x-amz-acl header to authenticated-read (via plain JavaScript XHR). \n. @nodeGarden Did you find out what the problem was with S3? I seem to have the same trouble setting the the x-amz-acl header to authenticated-read (via plain JavaScript XHR). \n. ",
    "ishener": "I have the same problem using aws sdk for node.js. 'public-read' ACL doesn't work. other ACL work.\n. ",
    "chollier": "same here\n. same here\n. Indeed my bad, I was apply the policy to the wrong role..\n\u2014\nSent from Mailbox\nOn Tue, Jun 9, 2015 at 6:42 PM, Loren Segal notifications@github.com\nwrote:\n\n@chollier I would suggest re-verifying your permissions. In all of the above cases there was an environment / configuration error causing these permission errors, not the SDK. We've yet to reproduce any actual issue in the SDK, and the SDK is not actually passing any extra permission information besides \"public-read\" (if that's what you passed through).\nReply to this email directly or view it on GitHub:\nhttps://github.com/aws/aws-sdk-js/issues/256#issuecomment-110554329\n. \n",
    "bsherrill480": "Hey, making a quick post in case anybody was in the same boat as me and came across this this thread.\nGoing to expand a bit on chollier. The policy on the s3 bucket I was trying to putObject into didn't allow the current IAM role/user to set the ACL. The solution is to update the s3 bucket's policy's Principal to include the IAM role/user ARN. See https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-bucket-policies-use-case-1\n. ",
    "okdewit": "I was running into this problem, where node-lambda (a tool for testing/deploying lambda functions locally) was able to set ACLs on objects, while the same code deployed to AWS generated \"Access Denied\" errors.\nIf anyone is having this problem with lambda functions, be sure to go to IAM role management and edit the policy for the Lambda role (I think the default role is lambda_s3_exec_role). The policy includes \"s3:getObject\" and \"s3:PutObject\", but should also include \"s3:PutObjectAcl\" if you need to set access control for files.\nnode-lambda uses an .env file which contains a key/secret, which in my case gave me more permissions locally than lambda_s3_exec_role had.\n. I was running into this problem, where node-lambda (a tool for testing/deploying lambda functions locally) was able to set ACLs on objects, while the same code deployed to AWS generated \"Access Denied\" errors.\nIf anyone is having this problem with lambda functions, be sure to go to IAM role management and edit the policy for the Lambda role (I think the default role is lambda_s3_exec_role). The policy includes \"s3:getObject\" and \"s3:PutObject\", but should also include \"s3:PutObjectAcl\" if you need to set access control for files.\nnode-lambda uses an .env file which contains a key/secret, which in my case gave me more permissions locally than lambda_s3_exec_role had.\n. ",
    "PrasanthJayaraman": "I had the same issue in my Node runtime V4.3, the S3 bucket is in different IAM Role and My Lambda is in my IAM role created by Admin of AWS account. I used AWS-SDK apiVersion '2006-03-01'. I checked all my polices it looks fine i had all the access, then i used my accesskey and secret key in aws.S3({}) object so that it can able to read the bucket and file using my keys.\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html in this find Constructing a S3 object you will get the details.\nI hope this may help someone..\n. ",
    "ModestoFiguereo": "Thanks @okdewit !!  I just worked perfectly!. ",
    "lmj0011": "@okdewit I almost punched myself in the face because of this. Thanks\naws and their clusterf*** of policies/permissions. ",
    "dukeatcoding": "One year later and I still run into the same problems. I think the SDK should handle this issue, since it looks like S3.getObject does not make sense without a check for a complete Download.\n. One year later and I still run into the same problems. I think the SDK should handle this issue, since it looks like S3.getObject does not make sense without a check for a complete Download.\n. ",
    "simoncpu": "It's already March 2016 but I'm still encountering this problem. :-/\n. It's already March 2016 but I'm still encountering this problem. :-/\n. @chrisradek I'm using AWS Lambda, which is running v0.10.36. I'm writing a Lambda function that is triggered as soon as a user uploads a PDF document to S3*. The function would then grab the PDF document for processing.\nIt looks like the read stream triggers a finish event even though the download from S3 is truncated (incomplete download? dunno). I wrote a similar Lambda function that is triggered from an API endpoint, but it works without problems.\nMy current work-around is to use http.get() to grab the PDF from S3.\nvia HTML POST\n. @chrisradek I'm using AWS Lambda, which is running v0.10.36. I'm writing a Lambda function that is triggered as soon as a user uploads a PDF document to S3. The function would then grab the PDF document for processing.\nIt looks like the read stream triggers a finish event even though the download from S3 is truncated (incomplete download? dunno). I wrote a similar Lambda function that is triggered from an API endpoint, but it works without problems.\nMy current work-around is to use http.get() to grab the PDF from S3.\n*via HTML POST\n. Hurrrrh... I wrote a test script to show that the MD5 hashes don't match, but it seems that Lambda can download to S3 without truncating now:\n```\n'use strict';\nvar fs = require('fs');\nvar exec = require('child_process').exec;\nvar aws = require('aws-sdk');\nvar s3 = new aws.S3({ apiVersion: '2006-03-01' });\nexports.handler = function(event, context) {\n    var params = {\n        Bucket: event.Records[0].s3.bucket.name,\n        Key: event.Records[0].s3.object.key\n    };\nvar file_fn = '/tmp/test-' + Math.random().toString(36).slice(2) + '.pdf';\nvar file = fs.createWriteStream(file_fn);\nvar stream = s3.getObject(params).createReadStream().pipe(file);\n\nstream.on('finish', function() {\n    exec('/usr/bin/md5sum ' + file_fn, function(err, stdout, stderr) {\n        console.log('params', params);\n        console.log('stdout', stdout);\n\n        context.succeed('w00t!');\n    });\n});\n\n};\n/ jshint node: true /\n```\nI'm confused why it didn't work last night. /me scratches head\n. Hurrrrh... I wrote a test script to show that the MD5 hashes don't match, but it seems that Lambda can download to S3 without truncating now:\n```\n'use strict';\nvar fs = require('fs');\nvar exec = require('child_process').exec;\nvar aws = require('aws-sdk');\nvar s3 = new aws.S3({ apiVersion: '2006-03-01' });\nexports.handler = function(event, context) {\n    var params = {\n        Bucket: event.Records[0].s3.bucket.name,\n        Key: event.Records[0].s3.object.key\n    };\nvar file_fn = '/tmp/test-' + Math.random().toString(36).slice(2) + '.pdf';\nvar file = fs.createWriteStream(file_fn);\nvar stream = s3.getObject(params).createReadStream().pipe(file);\n\nstream.on('finish', function() {\n    exec('/usr/bin/md5sum ' + file_fn, function(err, stdout, stderr) {\n        console.log('params', params);\n        console.log('stdout', stdout);\n\n        context.succeed('w00t!');\n    });\n});\n\n};\n/ jshint node: true /\n```\nI'm confused why it didn't work last night. /me scratches head\n. Update: The S3 files are truncated again on AWS Lambda, but I can't replicate the problem using my sample code above.\n. Update: The S3 files are truncated again on AWS Lambda, but I can't replicate the problem using my sample code above.\n. ",
    "igord": "Thanks Loren, but it is not working. Here is my code:\nvar rq = S3.putBucketLifecycle({\n    Bucket: 'mybucket',\n    LifecycleConfiguration: {Rules: [{\n    Prefix: '9nht7ceg7n/',\n    Status: \"Enabled\",\n    Expiration: {Date: '2014-04-08T17:32:59.157Z'}\n    }]}\n}, function(err, r) {\n    console.log(err);\n});\nconsole.log(rq.httpRequest.body);\n...and the output is:\n<LifecycleConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"><Rule><Expiration><Date>2014-04-08T17:32:59.157Z</Date></E\nxpiration><Prefix>9nht7ceg7n/</Prefix><Status>Enabled</Status></Rule></LifecycleConfiguration>\n{ [InvalidArgument: Date must be at midnight GMT]\n  message: 'Date must be at midnight GMT',\n  code: 'InvalidArgument',\n  time: Tue Apr 08 2014 20:37:48 GMT+0300 (GTB Daylight Time),\n  statusCode: 400,\n  retryable: false,\n  _willRetry: false }\n. ",
    "kcivey": "Unfortunately the dot doesn't help, since \"rc9\" is greater than \"rc\". It still thinks 2.0.0.-rc9 is the latest. But I guess this problem will go away when 2.0.0 comes out.\n. Actually if I remove the \"aws-sdk\" line from package.json and reinstall it does get 2.0.0-rc.19. Not sure why, since semver.gt('2.0.0-rc9','2.0.0-rc.19') is true, just like semver.gt('2.0.0-rc9','2.0.0-rc13').\n. ",
    "dylanlingelbach": "@lsegal - sure, here is a minimal repo:\n```\nvar aws = require('aws-sdk');\nvar s3 = new aws.S3();\nvar bucket = 'fair-pricing';\nvar copyParams = {\n    Bucket: bucket,\n    Key: 'csv_results_dev/20140415/test.csv',\n    CopySource: 'csv_results_dev/20140415/results.csv'\n};\ns3.client.copyObject(copyParams, function(err, data) {  \n    if (err) {\n        console.log(\"ERROR copyObject\");\n        console.log(err);\n    }\n    else {\n        console.log('SUCCESS copyObject');\n}\n\n});\nvar getParams = {\n    Bucket: bucket,\n    Key: 'csv_results_dev/20140415/results.csv'\n}\ns3.client.getObject(getParams, function(err, data) {\n    if (err) {\n        console.log('ERROR getObject');\n        console.log(err);\n    } else {\n        console.log('SUCCESS getObject')\n    }\nvar putParams = {\n    Bucket: bucket,\n    Key: 'csv_results_dev/20140415/test.csv',\n    Body: data.Body\n}\n\ns3.client.putObject(putParams, function(err, data) {\n    if (err) {\n        console.log('ERROR putObject');\n    } else {\n        console.log('SUCCESS putObject')\n    }\n});\n\n});\n```\nHere is my console output when I put the above text in test.js and run with node 0.10.24 on Mac OS X 10.9.2.\n~/Code/xxxxx/xxxxxx (master)$ node test.js \nERROR copyObject\n{ [NoSuchBucket: The specified bucket does not exist]\n  message: 'The specified bucket does not exist',\n  code: 'NoSuchBucket',\n  time: Wed Apr 16 2014 13:48:00 GMT-0500 (CDT),\n  statusCode: 404,\n  retryable: false,\n  _willRetry: false }\nSUCCESS getObject\nSUCCESS putObject\nI've tried wiping out my node_modules and re-running npm install to ensure I have 2.0.0-rc13.  I have my access key and secret key set via environment variables.\nI haven't tried to see if the bucket name changes things.  I'll try that now and comment when I find out.\n. Yep, I saw that right as you were commenting.  When I tried with a keyname without a / I got an error saying I needed to pass the bucket name as well.  \nI just misread the documentation when I was coding this up.\nThanks and sorry for the trouble!\n. ",
    "rfink": "I'm actually having this same problem, using a \"copyObject\" immediately following a \"putObject\".  Please advise.\n. Also having this issue +1\n. Also having this issue +1\n. Same here, single instance of SDK, still problems.\n. Same here, single instance of SDK, still problems.\n. Oddly enough, this is still happening to me even when specifying the credentials in environment variables.\n. Oddly enough, this is still happening to me even when specifying the credentials in environment variables.\n. So this is still happening in v2.6.9 on an EC2 instance (utilizing elastic beanstalk). \n{\"message\":\"Missing credentials in config\",\"name\":\"CredentialsError\",\"stack\":\"Error: connect ECONNREFUSED 169.254.169.254:80\\n    at Object.exports._errnoException (util.js:874:11)\\n    at exports._exceptionWithHostPort (util.js:897:20)\\n    at TCPConnectWrap.afterConnect as oncomplete\",\"code\":\"CredentialsError\"}\n. So this is still happening in v2.6.9 on an EC2 instance (utilizing elastic beanstalk). \n{\"message\":\"Missing credentials in config\",\"name\":\"CredentialsError\",\"stack\":\"Error: connect ECONNREFUSED 169.254.169.254:80\\n    at Object.exports._errnoException (util.js:874:11)\\n    at exports._exceptionWithHostPort (util.js:897:20)\\n    at TCPConnectWrap.afterConnect as oncomplete\",\"code\":\"CredentialsError\"}\n. Still happening for me in ECS with aws-sdk version 2.270.1 and node.js version 10.11. Still happening for me in ECS with aws-sdk version 2.270.1 and node.js version 10.11. ",
    "kaushikwavhal": "Introducing a new parameters called \"sourceBucket\" and removing the bucket-name prefix in the CopySource path would have been more developer friendly.\nIt took me a long time to reach to this issue as i had assumed that both source and destination paths would have identical conventions which is generally the case.  . ",
    "aichholzer": "Man, I was going crazy on this issue and I swear I tried the JSON.stringify() before and it still was not working. I just did a complete new start, re-wrote the whole thing and tried the stringify method again. Magically; it works now.\nSome times a simple, plain and different point of view (or explanation) makes a complete difference sense.\nThank you. Next time I am around your city you have a free burger + beer (same applies if you are around my city)\n:) \n. Man, I was going crazy on this issue and I swear I tried the JSON.stringify() before and it still was not working. I just did a complete new start, re-wrote the whole thing and tried the stringify method again. Magically; it works now.\nSome times a simple, plain and different point of view (or explanation) makes a complete difference sense.\nThank you. Next time I am around your city you have a free burger + beer (same applies if you are around my city)\n:) \n. And yes, I agree; the AWS documentation has room for a lot of improvement. A JSON encoded string is not a JSON object, not to me, at least.\n. And yes, I agree; the AWS documentation has room for a lot of improvement. A JSON encoded string is not a JSON object, not to me, at least.\n. Another question; what about this:\n'InvalidParameter: Invalid parameter: JSON must contain an entry for \\'default\\' or \\'APNS_SANDBOX\\'\nI was sending:\nparams = {\n  MessageStructure: 'json',\n  Message: {\n      APNS_SANDBOX: {aps:{alert:\"Hello and have a good day.\"}}\n   },\n   TargetArn: '...'\n};\nThe error says \"or\" so in my opinion the \"APNS_SANDBOX\" should do the trick, no?\n. Another question; what about this:\n'InvalidParameter: Invalid parameter: JSON must contain an entry for \\'default\\' or \\'APNS_SANDBOX\\'\nI was sending:\nparams = {\n  MessageStructure: 'json',\n  Message: {\n      APNS_SANDBOX: {aps:{alert:\"Hello and have a good day.\"}}\n   },\n   TargetArn: '...'\n};\nThe error says \"or\" so in my opinion the \"APNS_SANDBOX\" should do the trick, no?\n. Thank you, will do.\nHave a good one.\n. Thank you, will do.\nHave a good one.\n. And found another issue which might be related to this module:\nparams = {\n  MessageStructure: 'json',\n  Message: {\n      default: \"I am default\":\n      APNS_SANDBOX: {aps:{alert:\"Hello and have a good day.\"}}\n   },\n   TargetArn: '...'\n};\nAnd guess what I receive on my iOS device? - Yes, the \"default\". It seems to be overriding the whole object's other properties. If, of course, I try to put the payload into the \"default\" it also complains.\nAny ideas?\nUsing Amazon is really starting to get frustrating.\n. And found another issue which might be related to this module:\nparams = {\n  MessageStructure: 'json',\n  Message: {\n      default: \"I am default\":\n      APNS_SANDBOX: {aps:{alert:\"Hello and have a good day.\"}}\n   },\n   TargetArn: '...'\n};\nAnd guess what I receive on my iOS device? - Yes, the \"default\". It seems to be overriding the whole object's other properties. If, of course, I try to put the payload into the \"default\" it also complains.\nAny ideas?\nUsing Amazon is really starting to get frustrating.\n. Found the issue about the \"default\" overriding the APNS property's value; the \"Massage\" itself must be encoded as a valid string, say JSON.stringify() for example. The trick is applying the same to the \"APNS_SANDBOX\". For example:\nparams = {\n  MessageStructure: 'json',\n  Message: JSON.stringify({\n      default: \"I am default\":\n      APNS_SANDBOX: JSON.stringify({aps:{alert:\"Hello and have a good day.\"}})\n   }),\n   TargetArn: '...'\n};\nThe above will make AWS SNS \"ignore\" the default and actually deliver what is expected.\nI wish I had access to the SNS documentation, I would make quite some changes.\nAnyway, hope this helps somebody.\n. Found the issue about the \"default\" overriding the APNS property's value; the \"Massage\" itself must be encoded as a valid string, say JSON.stringify() for example. The trick is applying the same to the \"APNS_SANDBOX\". For example:\nparams = {\n  MessageStructure: 'json',\n  Message: JSON.stringify({\n      default: \"I am default\":\n      APNS_SANDBOX: JSON.stringify({aps:{alert:\"Hello and have a good day.\"}})\n   }),\n   TargetArn: '...'\n};\nThe above will make AWS SNS \"ignore\" the default and actually deliver what is expected.\nI wish I had access to the SNS documentation, I would make quite some changes.\nAnyway, hope this helps somebody.\n. @lsegal Indeed a SNS issue and not JS SDK related. Sadly no one from the SNS team seems to read the forums as issues mentioned ages ago still persist, but that is a whole new story, I guess.\nThank you.\n. @lsegal Indeed a SNS issue and not JS SDK related. Sadly no one from the SNS team seems to read the forums as issues mentioned ages ago still persist, but that is a whole new story, I guess.\nThank you.\n. @gloopeezza most welcome, I know how frustrating it can be.\n. @gloopeezza most welcome, I know how frustrating it can be.\n. @pixelogik glad it was of any help.\n. @pixelogik glad it was of any help.\n. @typemismatch anytime!\n. @typemismatch anytime!\n. Yes, I am not sure where the AWS team's priorities are.\n. Yes, I am not sure where the AWS team's priorities are.\n. :+1: \n. :+1: \n. @chrisradek I guess you did not read my question or it was not clear.\n\nTrying to get a signature for a file to be uploaded.\n\nYes, I want to get a presigned URL for a PUT request. I assume the presigned Body parameter is to ensure that the same presigned URL cannot be used to upload another image, since there is no other way to check the payload.\nvar params = {\n    Bucket: 'myBucket',\n    Key: 'myKey',\n    Body: 'EXPECTED CONTENTS'\n};\nAgain, my question is what is the Body supposed to hold? Which data?\n. @chrisradek I guess you did not read my question or it was not clear.\n\nTrying to get a signature for a file to be uploaded.\n\nYes, I want to get a presigned URL for a PUT request. I assume the presigned Body parameter is to ensure that the same presigned URL cannot be used to upload another image, since there is no other way to check the payload.\nvar params = {\n    Bucket: 'myBucket',\n    Key: 'myKey',\n    Body: 'EXPECTED CONTENTS'\n};\nAgain, my question is what is the Body supposed to hold? Which data?\n. Hey @LiuJoyceC \nThank you for your reply.\nIt worked, however, I had to do a little more research since the region my bucket is in only supports v4 signatures and signatureVersion must be properly set in the configuration.\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version\n. Hey @LiuJoyceC \nThank you for your reply.\nIt worked, however, I had to do a little more research since the region my bucket is in only supports v4 signatures and signatureVersion must be properly set in the configuration.\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAWSSDK.html#specify-signature-version\n. @srchase -Thank you. It's been over a year... I would not have imagined issues have such a low priority. In the meantime, I have moved away from AWS. So this is not relevant anymore.. @srchase -Thank you. It's been over a year... I would not have imagined issues have such a low priority. In the meantime, I have moved away from AWS. So this is not relevant anymore.. ",
    "ploddi": "I have same issue when using AWS SNS CLI.\n. Thanks for your investigation. CLI and Java SDK working as well.\n. ",
    "pixelogik": "@aichholzer Thanks, you saved our asses. SNS documentation is really lacking very important pieces of information. \n. @aichholzer Thanks, you saved our asses. SNS documentation is really lacking very important pieces of information. \n. ",
    "ei02028": "Guys,\nI'm trying my luck in this forum. I'm having an issue with Amazon SNS when I try to send a push notification to several devices at the same time.\nI've explained my issue here: https://forums.aws.amazon.com/thread.jspa?threadID=171981\nAny help?\n. ",
    "typemismatch": "@aichholzer wow, thanks! - the double stringify is what made it work, it's stupid but it's perfect, thanks!\n. ",
    "robolivable": "This issue is still just as relevant as it was when it was posted. @lsegal There's still nothing in the documentation about JSON encoded strings. Your comment really helped me out.\n. ",
    "contractorwolf": "yes this is definitely weird behavior, it doesn't make sense to set it to \"json\" if you are then forced to pass a string.\n. yes this is definitely weird behavior, it doesn't make sense to set it to \"json\" if you are then forced to pass a string.\n. ",
    "igor-yamshchykov": "Thanks a lot man! Wasted around two hours with that !\n. ",
    "DavidSunny": "@aichholzer Thank you!\n. ",
    "gsarfati": "@aichholzer  Thank you!\n. ",
    "ir-fuel": "Same issue here. Serious issues with the documentation and would never have figured this json -> string stuff out myself.\nThanks, and please, someone fix those docs :/\n. ",
    "jnimis": "Thanks you so much for this answer!. ",
    "mdw19873": "@aichholzer You're still saving people hours and hours of headaches! Thanks!. For anyone else that comes across this, there still seems to be some kind of an issue with SQS receiving and memory leaks. Using a custom http/https agent seems to alleviate the issue. The example provided by @AndrewBarba above seems to work well. In my implementation I used the following for the agent.\nnew https.Agent({\n  keepAlive: true,\n  maxSockets: 100\n}). ",
    "surf2day": "Thanks for the answer, never would have solved it without it!  Cheers.. ",
    "malithjkmt": "For GCM the structure is like below, which is taken from this gist \n```\n    let payload = {\n            GCM: {\n                data: {\n                    message:'Hello world'\n                }\n            }\n    };\n    // first have to stringify the inner GCM object...\n    payload.GCM = JSON.stringify(payload.GCM);\n// then have to stringify the entire message payload\npayload = JSON.stringify(payload);\n\nvar params = {\n    MessageStructure: 'json',\n    Message: payload,\n    Subject: 'TEST SUBJECT',\n    TargetArn: 'TargetArn'\n};\n\n```. ",
    "builtbyproxy": "My GCM implementation is: \n```\n    const params = {\n        Message: JSON.stringify({\n            default: \"This is the message for you...\",\n            GCM: JSON.stringify({ \n                data: {\n                    message: \"Hello World\",\n                }\n            })\n        }),\n        MessageStructure: \"json\",\n        TargetArn: 'arn:aws:...', // This is the endpointARN\n    };\nconst sns = new AWS.SNS();\nsns.publish(params as any, function(err, data) {\n    if(err) {\n        console.log('error occured: ', err, err.stack); \n    } else {\n        console.log('Success: ', data);\n    }\n});\n\n```. ",
    "Roach": "No problem! I Had the same typo in my project and found this when I searched my repo.\n. ",
    "felipesabino": "@lsegal I am having this issue using 2.0.0-rc.14 version :disappointed: \n. ",
    "kirasagi": "Thanks for the answer. Unfortunately this did not fix my problem. Still blocking whenever I listen to that progress event.\n// Add max sockets.\nhttp.globalAgent.maxSockets = 30;\nhttps.globalAgent.maxSockets = 30;\nAlso did a fresh npm install for aws-sdk.\n. I'll do this when I get home. In the meantime, how do I check my version of the sdk?\n. The problem was fixed after some cleaning up on my functions (never found out which one was the culprit). Sorry for the trouble and thank you very much for the help!\n. ",
    "jqdometita": "Hi Loren,\nIt is running on NodeJS version 0.10.26. I am using the latest SDK.\n2.0.0-rc.14.\nThanks,\nJohn\nOn Apr 30, 2014 2:12 PM, \"Loren Segal\" notifications@github.com wrote:\n\nI was not able to reproduce this behavior with the latest version of the\nSDK. Can you provide the environment details and AWS SDK version? Is this\nin Node.js or the browser? If Node, what version? If the browser, which?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-js/issues/267#issuecomment-41763987\n.\n. @lsegal , weird I  tried it again now but it didn't happen. I had this issue yesterday. I just thought I should report it today.\n. Thanks btw.\n. \n",
    "yonatang": "Sorry, I had the ~ operator on the dependency version, causing my to check\nit on an old (rc9) version of the sdk. Using rc14 does resolve the issue.\nThanks.\nOn Wed, Apr 30, 2014 at 11:46 PM, Loren Segal notifications@github.comwrote:\n\nWhat version of the SDK are you using? This was previously reported and\nfixed in #202 https://github.com/aws/aws-sdk-js/issues/202. The latest\nversion (2.0.0-rc.14) contains logic to handle adding the path portion of\nthe endpoint to the URI.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/aws/aws-sdk-js/issues/268#issuecomment-41848153\n.\n. \n",
    "neilstuartcraig": "Hi. \nSorry, I wasn't clear but no, it's the EC2 instances which are being reported incorrectly. \nAm I still best to report as you suggested? Does seem likely to be the underlying layer I guess.  \nCheers. \nSent from my iPhone\n\nOn 2 May 2014, at 17:51, Loren Segal notifications@github.com wrote:\nI assume by \"instance\" you mean the ELB itself, not EC2 instances, correct?\nIf so, I would recommend opening a support ticket or posting on the EC2 forums about this, as this sounds like it would be an issue with the service, not the SDKs. The service team will be much better equipped to help you debug and fix this issue.\n\u2014\nReply to this email directly or view it on GitHub.\n. Hi. \n\nSorry, I wasn't clear but no, it's the EC2 instances which are being reported incorrectly. \nAm I still best to report as you suggested? Does seem likely to be the underlying layer I guess.  \nCheers. \nSent from my iPhone\n\nOn 2 May 2014, at 17:51, Loren Segal notifications@github.com wrote:\nI assume by \"instance\" you mean the ELB itself, not EC2 instances, correct?\nIf so, I would recommend opening a support ticket or posting on the EC2 forums about this, as this sounds like it would be an issue with the service, not the SDKs. The service team will be much better equipped to help you debug and fix this issue.\n\u2014\nReply to this email directly or view it on GitHub.\n. I can to an extent, it's in a few libs/files but the important bit is very simple:\n\n```\nvar path=require(\"path\");\nvar app=require(path.join(__dirname, \"/lib/endavaOSSAWSCLI.js\"));\nvar AWS=require('aws-sdk'); \nAWS.config.loadFromPath(path.join(__dirname, '/config/aws-api-credentials.json'));\nvar ELB=new AWS.ELB();\n...\nvar params=\n{\n    LoadBalancerNames:[masterELBName]\n};\nvar masterNodeDataVolumeID=null;\nELB.describeLoadBalancers(params, function(err, data)\n{\n    ...\n}\n```\nso as you can see it's really basic in this part. \nWhat's returned in the data var is as expected except that there are 2 instances in the data.LoadBalancerDescriptions[0].Instances array. The 0th instance is the one i don't recognise but the 1st is one of mine which is running and is the only instance attached to the ELB instance.\nThis was not happening prior to the 23rd April but has been happening since.\nI will see if i can reproduce via another API type e.g. CLI but only one instance is showing in the AWS web portal. \nI should say that at some point previously, there were several other instances in use by my account.\nDoes that help at all?\nCheers\n. I can to an extent, it's in a few libs/files but the important bit is very simple:\n```\nvar path=require(\"path\");\nvar app=require(path.join(__dirname, \"/lib/endavaOSSAWSCLI.js\"));\nvar AWS=require('aws-sdk'); \nAWS.config.loadFromPath(path.join(__dirname, '/config/aws-api-credentials.json'));\nvar ELB=new AWS.ELB();\n...\nvar params=\n{\n    LoadBalancerNames:[masterELBName]\n};\nvar masterNodeDataVolumeID=null;\nELB.describeLoadBalancers(params, function(err, data)\n{\n    ...\n}\n```\nso as you can see it's really basic in this part. \nWhat's returned in the data var is as expected except that there are 2 instances in the data.LoadBalancerDescriptions[0].Instances array. The 0th instance is the one i don't recognise but the 1st is one of mine which is running and is the only instance attached to the ELB instance.\nThis was not happening prior to the 23rd April but has been happening since.\nI will see if i can reproduce via another API type e.g. CLI but only one instance is showing in the AWS web portal. \nI should say that at some point previously, there were several other instances in use by my account.\nDoes that help at all?\nCheers\n. Thanks for your help, I\u2019ll do that.\nCheers\nOn 2 May 2014, at 19:47, Loren Segal notifications@github.com wrote:\n\nBased on that data, I would recommend opening a support ticket with ELB. It does sound like an issue with the service.\n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks for your help, I\u2019ll do that.\n\nCheers\nOn 2 May 2014, at 19:47, Loren Segal notifications@github.com wrote:\n\nBased on that data, I would recommend opening a support ticket with ELB. It does sound like an issue with the service.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "talha-asad": "Just pass in an invalid upload ID and you'll see what i mean.\nThe callback in the above code is never called instead the process terminates from an error thrown by the sdk.\n. node v0.10.26 / aws-sdk@2.0.0-rc.15\nI actually had this call inside my eventEmitter object, i don't think it should make a difference. Is there any other call that can produce a \"NoSuchUpload\" error ? if there is it could be that call.\nTo be honest the issue is that i don't know which triggers the error because the process terminates and the stack trace has all references to the aws-sdk, none to my file.\n. @lsegal Thanks for the info, but why would 1.18 run fine and the latest not ? Any thing in particular regarding errors ?\n. Hi Isegal, i think the issue happens when you try to upload 10-15 files in parallel. I checked the network usage and it seems to be adequately available. I can't provide you the complete code of the EventEmitter, i don't own it, but i'll try. The thing is it doesn't fail in every case, it fails like in some particular cases and in those cases it almost always fails. I can't see a anything particular in those cases though, only the number of files increase. The uploads are being done using MultiPart operations. I'll try to get you a code that can reproduce the issue.\n. Thanks for the detailed info, i have been using the last stable release and have had no issues, i'll try to reproduce this issue again and let you know if i find anything, and sorry for not being able to get back to you on this.\n. ",
    "gavllew": "Thanks @lsegal.  I think the custom endpoint (a small Eucalyptus test cloud) does support the DNS bucket addressing, but I suspect it does not need the path prefix when this is used - that was likely a mis-configuration on our part.\n. ",
    "slang800": "Sorry, turns out this was an issue with calling via nodefn.call, not with aws-sdk. But thank you for your help!\n. It's always \"us-east-1\"? That's fantastic! You just saved me a ton of time trying to find a workaround for this issue. :D\n. ",
    "faermanj": "@slang800 i am having a similar issue, would you recall your solution to this?\n. ",
    "saadtazi": "@slang800 : I got stuck with the same issue.. The problem I had:\nI was using bluebird promisify (which seams similar to nodefn.call from when.js) but I forgot to bind the method:\n```\n// this does not work and throw an exception \"TypeError: Cannot read property 'hostname' of undefined\"\nvar rawCreateBucket = Promise.promisify(bucket.createBucket);\n// this works\nvar rawCreateBucket = Promise.promisify(bucket.createBucket.bind(bucket));\n```\nI hope it helps...\n. Maybe I misunderstood your request, but after looking that your package.json file, it looks like you are loading aws-sdk from npm and aws-sdk-js from github... But aws-sdk-js is just the repo for aws-sdk npm package ( = aws-sdk-js is aws-sdk): check repo link in npm aws-sdk package page.\nWhy so you want to load twice the same package (once in a dep, once as devdep)? Why don't you use only aws-sdk package (as a dep)? Is it because you want to eventually have different versions for nodejs and web browser? If that's the case, then you might find this npm issue interesting.. FYI:\n\nPerhaps it could be considered, in the future, to GITHUB name as NPM package name to prevent future confusion.\n\nFor companies that offer sdks for multiple languages (like amazon aws), it make sense to postfix their sdk repo names with the language (aws-sdk-js, aws-sdk-java). But having the language name in the package name is not a good practice, at least with npm.\n\nDon't put \"js\" or \"node\" in the name. \n",
    "why-jay": "Great, that seems like a much easier way to handle it. Thanks!\n. Great, that seems like a much easier way to handle it. Thanks!\n. ",
    "billinghamj": "Worth noting another way to do this is:\n``` js\nvar AWS = require('aws-sdk');\nvar db = new AWS.DynamoDB();\nvar Q = require('q');\nQ.ninvoke(db, 'listTables', {}).then(function(data) {\n  console.log(data);\n});\n```\nninvoke ensures that this is correctly bound.\n. Worth noting another way to do this is:\n``` js\nvar AWS = require('aws-sdk');\nvar db = new AWS.DynamoDB();\nvar Q = require('q');\nQ.ninvoke(db, 'listTables', {}).then(function(data) {\n  console.log(data);\n});\n```\nninvoke ensures that this is correctly bound.\n. This introduced an (arguably) breaking change: changing S3 URLs from the hyphenated format s3-eu-west-1.amazonaws.com to s3.eu-west-1.amazonaws.com.\nThis broke a bunch of our systems due to CSP configuration.\nProbably should document the breaking change in the changelog for anyone else who unfortunately upgrades and doesn't realise this has changed.. This introduced an (arguably) breaking change: changing S3 URLs from the hyphenated format s3-eu-west-1.amazonaws.com to s3.eu-west-1.amazonaws.com.\nThis broke a bunch of our systems due to CSP configuration.\nProbably should document the breaking change in the changelog for anyone else who unfortunately upgrades and doesn't realise this has changed.. ",
    "grncdr": "I don't see that it's possible to use copyObject across regions & availability zones? If it is then that would indeed solve my problem...\n. Sorry, I misspoke earlier. The reason I needed to do this with two different clients is because they are using two different sets of credentials. I am backing up objects from one S3 account to another one.\n. \"Please send all future requests to this endpoint.\" certainly implies that the endpoint to use should be specified in the response (maybe the Location header?). It is indeed slower, but the client has enough information to correct it after the first request. I guess it's more of a feature request than a bug, so your call on whether you think it would be worth supporting.\n. To clarify the way I'd expect this to work:\n1. Client makes a vhost style request to the generic s3 endpoint\n2. s3 endpoint responds with the permanent redirect error, presumably with a Location header\n3. Client handles that error by retrying the request at the new location.\nExtra awesome bonus feature:\n1. If (3) succeeds, the client internally stores a mapping of bucket name -> endpoint, and future requests against that bucket are sent to the correct endpoint automatically. Storing these mappings in an LRU-cache would keep the client object from taking too much memory in pathological cases.\n. ",
    "hungryblank": "But would the copy object command also work if we want to copy cross region? say from a bucket in us-east-1 to one in eu-west-1 as an example? Because this kind of copy poses the same original problem.\n. ",
    "seankovacs": "Why was this closed? aws-sdk is raising an exception bypassing any event emitters or err callbacks if a key doesn't exist. Resorting to a  global \"catchall\" event seems wrong. \n. That's odd, fs doesn't crash an app if a file doesn't exist. It actually uses the err in the callback. http doesn't crash when you get on an URL that doesn't exist... interesting.\n\\_(\u30c4)_/\u00af\n. ",
    "jayzelenkov": "I run into the same exact issue.\nTrying to download an object from s3 that doesn't exist is a programmer error. There is nothing that could be done about it. That's why aws-sdk throws an uncaughtException at this point.\nNetwork issue would produce an operational error that will be passed to the stream. Then it is up a developer to do something about it:\njs\ns3.getObject(params)\n      .createReadStream()\n      .pipe(file)\n      .on('error', (err) => {\n        console.log('download failed')\n        // retry download ?\n      })\nOne way to avoid uncaughtException is to check if an object exists in s3 bucket before trying to download it.\n\nA good article covering errors in NodeJS: https://www.joyent.com/developers/node/design/errors\n. I run into the same exact issue.\nTrying to download an object from s3 that doesn't exist is a programmer error. There is nothing that could be done about it. That's why aws-sdk throws an uncaughtException at this point.\nNetwork issue would produce an operational error that will be passed to the stream. Then it is up a developer to do something about it:\njs\ns3.getObject(params)\n      .createReadStream()\n      .pipe(file)\n      .on('error', (err) => {\n        console.log('download failed')\n        // retry download ?\n      })\nOne way to avoid uncaughtException is to check if an object exists in s3 bucket before trying to download it.\n\nA good article covering errors in NodeJS: https://www.joyent.com/developers/node/design/errors\n. good point. APIs are hard\n. good point. APIs are hard\n. ",
    "logidelic": "Is this being looked at? Is there a workaround? To be clear, the problem is that this exception cannot be caught by the client code. This a fatal API bug. No API should throw an exception internally that is not caught internally. This will, at worst, simply crash the node app or, at best, will trigger a global exception handler to allow clean shutdown. Both of these are unacceptable for obvious reasons.\nFWIW, I'm getting this issue with s3.getObject when an invalid key name is specified.\n. ",
    "verveguy": "@logidelic: I'm with you on this one.\n@lsegal this is insane. I'm in lambda using node and cannot catch this exception. S3 is literally killing my function with no way for me to gracefully recover.\nTo the point earlier, fs doesn't crash your app if a file doesn't exist. That's what exceptions are for.\n. @chrisradek sure. \nNote: I now realize that I have not explicitly tried to add event handlers to the read stream. But here's the thing, I shouldn't have to IMHO. In my case, even more so because I'm trying to use the Promise support offered by the AWS SDK.\nSo what am I doing?\nI'm writing a simple (!) lamdba function in node and using the AWS S3 SDK. \nSince node is all about async, I'm therefore using Promises. I'm also trying to use the stream support to easily get the file content from S3 onto the /tmp filestore. \nWhat should happen (IMHO) is that by using a Promise, my catch() portion of the promise chain should in fact be invoked with whatever exception is thrown by the machinery underlying S3.getObject . However, that's not the case. S3.getObject() wants to be \"special\" despite the promise of the promise() support in the AWS SDK. As soon as I call s3.getObject().promise() we're off to the races and the exception is thrown pretty promptly. Note that the promise() is wrapping the AWSRequest, not wrapping the readStream as you propose doing with event handlers. \nIn the end, I've managed to cobble together a solution that wraps things in a Promise of my own and uses the event handlers on the output file at the end of the pipe to invoke the resolve() , reject(). \nHowever, that's too far down the pipe (so to speak) to catch the errors thrown from the event handlers underlying the S3 request. You're right: perhaps I could use the readStream instead. I might even try that sometime.\nBut for now, I put calls to headObject in place first to avoid even trying to make a getObject call at all since it was unsafe to do so.\nHaving read various threads around here in growing frustration, this all appears to boil down to a fundamental philosophical argument about what the behavior of an async operation from S3 should be like vs what other async operations (such as fs calls) are like. The (broken) promise support only makes it even more frustrating. And trying to use promises plus streams is making me insane.\nI'd love a clean code example of a properly promisified S3.getObject with streams writing to a local file that works without exception even when the key doesn't exist.\n. @lsegal: I'll open a new issue regarding the use of s3.getObject, streams and promises. \n. ",
    "bdeitte": "I've run into this a bit myself and was really happy to see this fix get in, thanks.  Eagerly awaiting the next version!\n. I did not realize the cli was doing something special on top of the API.  We are using the deleteObjects call currently, just haven't added the pagination and was hoping to get this for free.  :)  I can understand though the desire to stick with a straight API mapping- feel free to close this out.  Thanks for the info!\n. I did not realize the cli was doing something special on top of the API.  We are using the deleteObjects call currently, just haven't added the pagination and was hoping to get this for free.  :)  I can understand though the desire to stick with a straight API mapping- feel free to close this out.  Thanks for the info!\n. ",
    "PeppeL-G": "I don't have time to debug this, but I got the error as well. I uploaded a file from a website to my server (Node) using Ajax, and then uploaded it from my serverto S3 with code looking like this:\n``` javascript\nvar stream = fs.createReadStream(pathToFile)\ns3.putObject({\n    Bucket: bucketName,\n    Key: key,\n    Body: stream\n}, function(err, data){ / Do stuff... / })\n```\nThe funny thing is that it always fails for the first Ajax request I send, but it works for all that comes after. I don't know if this is a bug in the AWS SDK or the package multer (with express) I use to receive the uploaded file on my server (although I receive the file perfectly well as far as I can tell; it's saved on my server).\nHowever, when I change my code to:\njavascript\nfs.readFile(pathToFile, function(err, data){\n    if(err){\n        // Handle fail...\n    }else{\n        s3.putObject({\n            Bucket: bucketName,\n            Key: key,\n            Body: data\n        }, function(err, data){ /* Do stuff... */ })\n    }\n})\nIt works for all my Ajax request.\nI might have done a mistake somewhere (I'm not an experience Node programmer), but hopefully this will help someone.\n. I don't have time to debug this, but I got the error as well. I uploaded a file from a website to my server (Node) using Ajax, and then uploaded it from my serverto S3 with code looking like this:\n``` javascript\nvar stream = fs.createReadStream(pathToFile)\ns3.putObject({\n    Bucket: bucketName,\n    Key: key,\n    Body: stream\n}, function(err, data){ / Do stuff... / })\n```\nThe funny thing is that it always fails for the first Ajax request I send, but it works for all that comes after. I don't know if this is a bug in the AWS SDK or the package multer (with express) I use to receive the uploaded file on my server (although I receive the file perfectly well as far as I can tell; it's saved on my server).\nHowever, when I change my code to:\njavascript\nfs.readFile(pathToFile, function(err, data){\n    if(err){\n        // Handle fail...\n    }else{\n        s3.putObject({\n            Bucket: bucketName,\n            Key: key,\n            Body: data\n        }, function(err, data){ /* Do stuff... */ })\n    }\n})\nIt works for all my Ajax request.\nI might have done a mistake somewhere (I'm not an experience Node programmer), but hopefully this will help someone.\n. @jeskew, there shouldn't be any need for a retry if it would work as it should, right? Something is wrong and needs to be fixed? Or am I missing something?. @jeskew, there shouldn't be any need for a retry if it would work as it should, right? Something is wrong and needs to be fixed? Or am I missing something?. ",
    "fikriauliya": "I encountered this issue on the latest npm version: 2.82.0\nIt works ok on 2.63.0\nI have re-tested this on both version so many times, very less likely the root issue is somewhere else. Now I have pinned point the issue on 2.68.0\n<=2.67.0 have no problems.\nFurther investigation:\nThe release change on 2.68.0 is:\n\nfeature: S3: Switches S3 to use signatureVersion \"v4\" by default. To continue using signatureVersion \"v2\", set the signatureVersion: \"v2\" option in the S3 service client configuration. Presigned URLs will continue using \"v2\" by default.\n\nSetting signatureVersion to v2 solve this problem:\nconst s3 = new AWS.S3({signatureVersion: 'v2'});\nThe followings don't work:\nconst s3 = new AWS.S3(); => RequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\nconst s3 = new AWS.S3({signatureVersion:'v4'); => RequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\nconst s3 = new AWS.S3({signatureVersion:'v3'); => AccessDenied: Access Denied. ",
    "Ishank-dubey": "It doesn't work still?. I think u r missing the params something like(here) -\nvar params = {\n  nextToken: 'STRING_VALUE',\n  order: ascending | descending,\n  sortBy: repositoryName | lastModifiedDate\n};\ncodecommit.listRepositories(params, function(err, data) {\n  if (err) console.log(err, err.stack); // an error occurred\n  else     console.log(data);           // successful response\n});. I think u r missing the params something like(here) -\nvar params = {\n  nextToken: 'STRING_VALUE',\n  order: ascending | descending,\n  sortBy: repositoryName | lastModifiedDate\n};\ncodecommit.listRepositories(params, function(err, data) {\n  if (err) console.log(err, err.stack); // an error occurred\n  else     console.log(data);           // successful response\n});. ",
    "Bochenski": "Thanks,  I can create a PR if it helps? Is it this file, or is that auto generated from elsewhere?\n. ",
    "tcsafeway": "\n\nAWS.config.update({accessKeyId: key1, secretAccessKey: key2});\n```\n    var myDynamoDB = new AWS.DynamoDB({\n                                        region:'us-east-1',\n                                        apiVersion:'2012-08-10',\n                                        maxRetries:10,\n                                        logger:console\n                                });\n\n```\n. \n\nAWS.config.update({accessKeyId: key1, secretAccessKey: key2});\n```\n    var myDynamoDB = new AWS.DynamoDB({\n                                        region:'us-east-1',\n                                        apiVersion:'2012-08-10',\n                                        maxRetries:10,\n                                        logger:console\n                                });\n\n```\n. http://update2.greenstreetscene.com/testing/widget/dynamoTest.htm\n. http://update2.greenstreetscene.com/testing/widget/dynamoTest.htm\n. sorry I closed the issue by mistake\n. sorry I closed the issue by mistake\n. The script sequencing is \n1)   AWS.config.update({accessKeyId: key1, secretAccessKey: key2});  // keys hardcoded\n2)   var myDynamoDB = new AWS.DynamoDB({region:'us-east-1',\n                                 apiVersion:'2012-08-10',\n                                 maxRetries:10,\n                                logger:console\n                                    });\n3)   myDynamoDB.getItem(params,GSE.dynamo.processGetItem);  // the callback specified\n. The script sequencing is \n1)   AWS.config.update({accessKeyId: key1, secretAccessKey: key2});  // keys hardcoded\n2)   var myDynamoDB = new AWS.DynamoDB({region:'us-east-1',\n                                 apiVersion:'2012-08-10',\n                                 maxRetries:10,\n                                logger:console\n                                    });\n3)   myDynamoDB.getItem(params,GSE.dynamo.processGetItem);  // the callback specified\n. 3) FYI  params = GSE.dynamo.buildParamsSettings();\nwhich returns the constructed params object\n. 3) FYI  params = GSE.dynamo.buildParamsSettings();\nwhich returns the constructed params object\n. Hmmmm this is strange\nas you see from my console login url in dynamodb\n url : console.aws.amazon.com/dynamodb/home?region=us-east-1#table:name=zw_settings\ntable:name=zw_settings   exists ..\nnoticed from my snapshots uploaded above..\"\" double quotes within the request\npayload for     TableName: \"zw_settings\"       could that be causing the ResourceNotFound\n?  \nAlso  I am using  USER IAM keys  should I just try and reset the secret key... but dont see why that would change matters...\n. Hmmmm this is strange\nas you see from my console login url in dynamodb\n url : console.aws.amazon.com/dynamodb/home?region=us-east-1#table:name=zw_settings\ntable:name=zw_settings   exists ..\nnoticed from my snapshots uploaded above..\"\" double quotes within the request\npayload for     TableName: \"zw_settings\"       could that be causing the ResourceNotFound\n?  \nAlso  I am using  USER IAM keys  should I just try and reset the secret key... but dont see why that would change matters...\n. One last question Isegal  \nI appreciate your staying with me on this..\nI changed the keys  and indeed I did not get 400\nbut heheh I did not get a payload back  as expected.\nyou can test the page again and see..\nThe only thing I can think of is in the params object I did not specific AttributesToGet\nas I thought the default was if you dont include it ...then all attributes are returned..\n?\nany ideas as to why no payload \nresponse was : \nHTTP/1.1 200 OK\nx-amzn-RequestId: NGVS6CD23S2DSS5GFNC8630L63VV4KQNSO5AEMVJF66Q9ASUAAJG\nAccess-Control-Allow-Origin: *\nx-amz-crc32: 2745614147\nContent-Type: application/x-amz-json-1.0\nContent-Length: 2\nDate: Fri, 23 May 2014 07:28:26 GMT\n. One last question Isegal  \nI appreciate your staying with me on this..\nI changed the keys  and indeed I did not get 400\nbut heheh I did not get a payload back  as expected.\nyou can test the page again and see..\nThe only thing I can think of is in the params object I did not specific AttributesToGet\nas I thought the default was if you dont include it ...then all attributes are returned..\n?\nany ideas as to why no payload \nresponse was : \nHTTP/1.1 200 OK\nx-amzn-RequestId: NGVS6CD23S2DSS5GFNC8630L63VV4KQNSO5AEMVJF66Q9ASUAAJG\nAccess-Control-Allow-Origin: *\nx-amz-crc32: 2745614147\nContent-Type: application/x-amz-json-1.0\nContent-Length: 2\nDate: Fri, 23 May 2014 07:28:26 GMT\n. ok will work on it...  perhaps \nmy Key object is not correct...   \nmy table is a   Hash + Range   as  primary key\nso when I format  \nKey: {\ndomain: {..},\nbasePath : {}\n}\nis it assumed that the second member of  the Key object is the RangeKey name\n\ndocumentation does not show example of a    Hash  Range  table\nor does params require format\nKey : {\nHashKey : {  domain : { }} ,\nRangeKey : { basePath: { } }\n}\nanyway will try different combos... I know the data is there..\nHave a good night  Thanks alot Isegal\n. ok will work on it...  perhaps \nmy Key object is not correct...   \nmy table is a   Hash + Range   as  primary key\nso when I format  \nKey: {\ndomain: {..},\nbasePath : {}\n}\nis it assumed that the second member of  the Key object is the RangeKey name\n\ndocumentation does not show example of a    Hash  Range  table\nor does params require format\nKey : {\nHashKey : {  domain : { }} ,\nRangeKey : { basePath: { } }\n}\nanyway will try different combos... I know the data is there..\nHave a good night  Thanks alot Isegal\n. ",
    "hparra": "Thank you!\n@lsegal Is it generally safe to use HEAD for this project?\n. ",
    "kyledecot": "I see. Thanks for the explanation!\n. I see. Thanks for the explanation!\n. Sorry, thanks for the explanation! I've just started working with the SDK so I'll probably have LOTS of additional questions :p\n. Sorry, thanks for the explanation! I've just started working with the SDK so I'll probably have LOTS of additional questions :p\n. ",
    "andrewrk": "2.0.0-rc.18\nI put some instrumentation in and will try to trigger the error again.\n. Have not seen this after upgrading to rc19. Will re-open if it does.\n. Spoke too soon.\nHere's the log: http://superjoe.s3.amazonaws.com/temp/s3debug-out2.txt\n```\nWarning: AWS SDK JS called callback twice.\nerr RequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\n    at Request.extractError (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/services/s3.js:257:35)\n    at Request.callListeners (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/sequential_executor.js:114:20)\n    at Request.callListeners (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/sequential_executor.js:115:16)\n    at Request.emit (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/sequential_executor.js:81:10)\n    at Request.emit (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/request.js:578:14)\n    at Request.transition (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/request.js:12:12)\n    at AcceptorStateMachine.runTo (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/state_machine.js:28:10\n    at Request. (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/request.js:28:9)\n    at Request. (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/request.js:580:12)\ndata null\n/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/request.js:34\n        throw e;\n              ^\nRequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\n    at Request.extractError (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/services/s3.js:257:35)\n    at Request.callListeners (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/sequential_executor.js:114:20)\n    at Request.callListeners (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/sequential_executor.js:115:16)\n    at Request.emit (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/sequential_executor.js:81:10)\n    at Request.emit (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/request.js:578:14)\n    at Request.transition (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/request.js:12:12)\n    at AcceptorStateMachine.runTo (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/state_machine.js:28:10\n    at Request. (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/request.js:28:9)\n    at Request. (/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/request.js:580:12)\n```\n. Got callback called twice again, this time with a different error message:\nhttp://superjoe.s3.amazonaws.com/temp/s3debug-out3.txt\n```\nWarning: AWS SDK JS called callback twice.\nerr Error: write ECONNRESET\n    at errnoException (net.js:904:11)\n    at Object.afterWrite (net.js:720:19)\ndata null\n/home/andy/dev/node-s3-cli/node_modules/s3/node_modules/aws-sdk/lib/request.js:34\n        throw e;\n              ^\nError: write ECONNRESET\n    at errnoException (net.js:904:11)\n    at Object.afterWrite (net.js:720:19)\n```\nNote that also an error is thrown from AWS SDK here that I am unable to catch.\n. > Can you explain what you mean by \"there is no way for me to handle the error\"?\nI mean that the error is thrown instead of returned in a callback or event. And that it is thrown in a function that I don't call directly so try/catch won't work.\n\nThe Forbidden error is generated by S3 for resources that you do not have access to, so this is the correct error to return\n\nI am running code that is calling putObject thousands of times, and several thousand times it works, followed by one time that I get this error.\n\nCan you provide code that reproduces your issue along with your expected result?\n\nNot reliably. It seems to be a sporadic error.\nHere is the code I am running: https://github.com/andrewrk/node-s3-client/blob/master/index.js#L407\nDoing it on a folder with ~6,000 files each about 5 MB.\n\nAlso, what version of the SDK are you using?\n\nVersion 2.0.0-rc.18. I've updated now and will try to reproduce with the latest.\n. I turned on the logger and here's the file http://superjoe.s3.amazonaws.com/temp/s3debug-out.txt\nOn this run I got no error but instead the program stalled, as if for some reason a callback wasn't being called. This particular error on this run is probably a bug in my code. I'll do another run.\nWhat I noticed about this log file is that the response codes start out as 200s and then they turn into 403s at the end. Same credentials and everything. I also noticed that the time is monotonically increasing. If that is request time, then perhaps what is happening is that I'm starting a bazillion requests at once, and then taking too long to fulfill them. Would a 403 error be given in that situation?\nMy understanding is that I should not worry about starting as many requests as possible, because the global agent socket pooling will make sure that only N number of them will run at once.\n. I'm in the middle of another run. In this one the number of concurrent putObject calls is limited to 20 and the global maxSockets number is 30. So far the log is up to 363 seconds for each request, still monotonically increasing, and number of putObject calls is at 158. How can request times still be increasing?\n. In this run I am not constructing all multiple-thousand requests at the same time, only 20 at a time. Yet the logger is printing a monotonically increasing time.\nThis run completed and it had the same behavior where it stalled, as if a callback were never called. That's 4 in a row with the same behavior after upgrading to rc19.\nI think after upgrading to rc19 I'm not seeing the issue that I filed originally. Instead what I think is happening is I'm not getting a callback from putObject. The code is set up to crash and print a stack trace if I get an error from putObject, and that is not happening.\nI'm doing another run where I will only call putObject one at a time, with output that will enable us to make sure the callback is actually being called.\n. OK I have verified that there is clearly a bug in my code that manages when to call putObject. It was not throttling it like I thought it was.\n. Ah ok. No bug, I just put the configuration value in the wrong spot for this comment: https://github.com/aws/aws-sdk-js/issues/291#issuecomment-45150160\nNow I'm doing the real test with putObject only allowed to have 20 requests at a time.\n. Maybe this issue should be merged with #290 but the results of this run is that the putObject callback was called twice. Results here: https://github.com/aws/aws-sdk-js/issues/290#issuecomment-45154763\n. I just ran my test case again with 2.0.0-rc.20. The test case is uploading a directory of files that totals to 46 GB to S3. 35% of the way through I started getting AWS SDK JS called callback twice warnings and then progress halted. No error was thrown, and the process looks like it was hanging.\nProgress: 17630093853/49834727653 35% 11.94 MB/s                    Warning: AWS SDK JS called callback twice.\n. I'm able to get the duplicate callback with a modified test case:\n``` js\nrequire('graceful-fs'); // v2.0.3\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3({\n  sslEnabled: false,\n});\nvar fs = require('fs');\nvar Pend = require('pend');\nrequire('https').Agent.maxSockets = 30;\nrequire('http').Agent.maxSockets = 30;\nvar pend = new Pend();\nvar findit = require('findit');\nvar walker = findit(\"/home/andy/music/\");\nwalker.on('file', function(file) {\n  pend.go(function(cb) {\n    var params = {\n      Bucket: process.env.S3_BUCKET,\n      Key: \"1tmp1/\" + file,\n      Body: fs.createReadStream(file),\n    };\n    var callbackCalled = false;\n    s3.putObject(params, function(err, data) {\n      if (callbackCalled) {\n        console.log(\"!!! Duplicate callback!\", file);\n      }\n      callbackCalled = true;\n      if (err && err.retryable) {\n        console.log(\"!!! Retryable error!\", file);\n      }\n      console.log(file, err, data);\n      cb();\n    });\n  });\n});\nwalker.on('end', function() {\n  pend.wait(function() {\n    console.log(\"done\");\n  });\n});\n```\n...\n!!! Duplicate callback! /home/andy/music/50 Best Trance & Progressive Tunes Of 2012/(Disc 2) 25 - Darker Shades Of Black (Original Mix).mp3\n/home/andy/music/50 Best Trance & Progressive Tunes Of 2012/(Disc 2) 25 - Darker Shades Of Black (Original Mix).mp3 { [RequestTimeTooSkewed: The difference between the request time and the current time is too large.]\n  message: 'The difference between the request time and the current time is too large.',\n  code: 'RequestTimeTooSkewed',\n  time: Thu Jun 19 2014 16:44:43 GMT+0000 (EDT),\n  statusCode: 403,\n  retryable: false } null\n...\nThis re-introduces the problem of requests timing out, but even in that situation the callback should never be called twice.\nI'll work on trying to reduce this test case further, notably removing the graceful-fs dependency.\n. OK here's a test case without graceful-fs:\n``` js\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3({\n  sslEnabled: false,\n});\nvar fs = require('fs');\nvar Pend = require('pend');\nrequire('https').Agent.maxSockets = 30;\nrequire('http').Agent.maxSockets = 30;\nvar pend = new Pend();\npend.max = 500;\nvar findit = require('findit');\nvar walker = findit(\"/home/andy/music/\");\nwalker.on('file', function(file) {\n  pend.go(function(cb) {\n    var params = {\n      Bucket: process.env.S3_BUCKET,\n      Key: \"1tmp1/\" + file,\n      Body: fs.createReadStream(file),\n    };\n    var callbackCalled = false;\n    s3.putObject(params, function(err, data) {\n      if (callbackCalled) {\n        console.log(\"!!! Duplicate callback!\", file);\n      }\n      callbackCalled = true;\n      if (err && err.retryable) {\n        console.log(\"!!! Retryable error!\", file);\n      }\n      console.log(file, err, data);\n      cb();\n    });\n  });\n});\nwalker.on('end', function() {\n  pend.wait(function() {\n    console.log(\"done\");\n  });\n});\n```\nI get the first duplicate callback after the 4th log statement is printed for a RequestTimeTooSkewed.\n!!! Duplicate callback! /home/andy/music/50 Best Trance & Progressive Tunes Of 2012/(Disc 2) 08 - Cupid's Casualty (Mike Saint-Jules Remix).mp3\n/home/andy/music/50 Best Trance & Progressive Tunes Of 2012/(Disc 2) 08 - Cupid's Casualty (Mike Saint-Jules Remix).mp3 { [RequestTimeTooSkewed: The difference between the request time and the current time is too large.]\n  message: 'The difference between the request time and the current time is too large.',\n  code: 'RequestTimeTooSkewed',\n  time: Thu Jun 19 2014 13:12:16 GMT-0400 (EDT),\n  statusCode: 403,\n  retryable: false } null\n. Same thing, except this time I used SSL and set maxSockets to 0 like you did.\n``` js\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\nvar fs = require('fs');\nvar Pend = require('pend');\nrequire('https').Agent.maxSockets = 0;\nvar pend = new Pend();\npend.max = 500;\nvar findit = require('findit');\nvar walker = findit(\"/home/andy/music/\");\nwalker.on('file', function(file) {\n  pend.go(function(cb) {\n    var params = {\n      Bucket: process.env.S3_BUCKET,\n      Key: \"1tmp1/\" + file,\n      Body: fs.createReadStream(file),\n    };\n    var callbackCalled = false;\n    s3.putObject(params, function(err, data) {\n      if (callbackCalled) {\n        console.log(\"!!! Duplicate callback!\", file);\n      }\n      callbackCalled = true;\n      if (err && err.retryable) {\n        console.log(\"!!! Retryable error!\", file);\n      }\n      console.log(file, err, data);\n      cb();\n    });\n  });\n});\nwalker.on('end', function() {\n  pend.wait(function() {\n    console.log(\"done\");\n  });\n});\n```\n...\n!!! Duplicate callback! /home/andy/music/Andy Kelley/Airplane.mp3\n!!! Duplicate callback! /home/andy/music/Beatport Progressive House Top 100 February/Thomas Gold, Kaelyn Behr - Remember (Original MIx).mp3\n!!! Duplicate callback! /home/andy/music/Beatport Progressive House Top 100 February/Tritonal, Paris Blohm, Sterling Fox - Colors (Original Mix).mp3\n!!! Duplicate callback! /home/andy/music/CHVRCHES - The Bones of What You Believe [Deluxe Version] (2013)/07 Recover.mp3\n!!! Duplicate callback! /home/andy/music/CHVRCHES - The Bones of What You Believe [Deluxe Version] (2013)/08 Night Sky.mp3\n!!! Duplicate callback! /home/andy/music/D-Vision/It's a Dream (D-Vision vs East Clubbers Remix).mp3\n...\n. Next test. This one sets pend.max to 4 instead of 500, to avoid RequestTimeTooSkewed errors:\n``` js\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\nvar fs = require('fs');\nvar Pend = require('pend');\nrequire('https').Agent.maxSockets = 0;\nvar pend = new Pend();\npend.max = 4;\nvar findit = require('findit');\nvar walker = findit(\"/home/andy/music/\");\nwalker.on('file', function(file) {\n  pend.go(function(cb) {\n    var params = {\n      Bucket: process.env.S3_BUCKET,\n      Key: \"1tmp1/\" + file,\n      Body: fs.createReadStream(file),\n    };\n    var callbackCalled = false;\n    s3.putObject(params, function(err, data) {\n      if (callbackCalled) {\n        console.log(\"!!! Duplicate callback!\", file);\n      }\n      callbackCalled = true;\n      if (err && err.retryable) {\n        console.log(\"!!! Retryable error!\", file);\n      }\n      console.log(file, err, data);\n      cb();\n    });\n  });\n});\nwalker.on('end', function() {\n  pend.wait(function() {\n    console.log(\"done\");\n  });\n});\n```\nThis one took a lot longer before I saw any duplicate callbacks.\nHere's the first time I got a callback for this certain file:\n/home/andy/music/057 Children Of Bodom/001 Follow the Reaper/002 Taste of My Scythe.mp3 { [RequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.]\n  message: 'Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.',\n  code: 'RequestTimeout',\n  time: Thu Jun 19 2014 15:46:06 GMT-0400 (EDT),\n  statusCode: 400,\n  retryable: false } null\nAnd then later:\n!!! Duplicate callback! /home/andy/music/057 Children Of Bodom/001 Follow the Reaper/002 Taste of My Scythe.mp3\n/home/andy/music/057 Children Of Bodom/001 Follow the Reaper/002 Taste of My Scythe.mp3 { [RequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.]\n  message: 'Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.',\n  code: 'RequestTimeout',\n  time: Thu Jun 19 2014 15:46:47 GMT-0400 (EDT),\n  statusCode: 400,\n  retryable: false } null\n. > Are you sure you are using the rc.20 release?\nYep, 2.0.0-rc.20.\nOn Monday I'll see if I can get another test case going which is independent of the particular files on my computer.\n. Running a test now.\n. With that commit I no longer get callback called twice. I do occasionally get a RequestTimeout which is a retryable error, but that is expected. Glad you found the fix!\n. What's the best number of parallel requests to maximize bandwidth? \n. Wait wait wait. In this example code I am not sending any parallel requests. It is sending one request at a time. And I'm seeing this aborted request happening quite frequently.\nParallel requests is not an explanation for the behavior I'm seeing. \n. Also regarding retry logic - this example code doesn't have it but the real code does, and I was seeing a request sometimes fail three times in a row, each with a second delay before retrying. It should not be this spotty. \n. Also regarding retry logic - this example code doesn't have it but the real code does, and I was seeing a request sometimes fail three times in a row, each with a second delay before retrying. It should not be this spotty. \n. > Downloading many files at once can also hit that limit.\nHow is this different than \"parallel requests\"? Again, this test code downloads a single file at once, then repeats.\n\nIf you add the Expect 100-continue header to your requests, do you see any error returned by S3?\n\nI observe no difference:\n```\n$ node test.js\ndownloaded 1\neTag \"abac71913645791ed7a98e4461ee1a71\" digest 76b63bd096db02bcc83a092c1d479307 path: test_out.json\n/home/andy/tmp/npmtest/test.js:21\n    if (err) throw err;\n                   ^\nError: ETag does not match MD5 checksum\n```\nS3 still gives a 200 status code and then aborts the stream early.\nI haven't done anything with S3 in many hours. I download a file once, and then once that is done I try to download it again and the download is aborted midway. There's no way I hit a real limit here.\n. > Downloading many files at once can also hit that limit.\nHow is this different than \"parallel requests\"? Again, this test code downloads a single file at once, then repeats.\n\nIf you add the Expect 100-continue header to your requests, do you see any error returned by S3?\n\nI observe no difference:\n```\n$ node test.js\ndownloaded 1\neTag \"abac71913645791ed7a98e4461ee1a71\" digest 76b63bd096db02bcc83a092c1d479307 path: test_out.json\n/home/andy/tmp/npmtest/test.js:21\n    if (err) throw err;\n                   ^\nError: ETag does not match MD5 checksum\n```\nS3 still gives a 200 status code and then aborts the stream early.\nI haven't done anything with S3 in many hours. I download a file once, and then once that is done I try to download it again and the download is aborted midway. There's no way I hit a real limit here.\n. > What version of Node.js are you using?\nv0.10.29\n. > What version of Node.js are you using?\nv0.10.29\n. I'm confused as to why you're unable to duplicate the issue. It happens regularly and frequently for me. The most \"downloaded nnn\" number I've ever reached was 40.\nFor what it's worth, here's a slightly simplified test case that only checks file size and removes the etag/md5 calculation from the equation:\n``` js\nvar AWS = require('aws-sdk');\nvar fs = require('fs');\nvar client = new AWS.S3({\n    accessKeyId: process.env.S3_KEY,\n    secretAccessKey: process.env.S3_SECRET,\n});\nvar s3Path = \"1tmp1/node_modules/chem-cli/node_modules/watchify2/node_modules/browserify/node_modules/browser-builtins/node_modules/http-browserify/example/json-stream/node_modules/JSONStream/test/fixtures/all_npm.json\";\nvar localFile = \"test_out.json\";\nvar s3Params = {\n  Bucket: process.env.S3_BUCKET,\n  Key: s3Path,\n};\nvar count = 1;\ndownloadOnce();\nfunction downloadOnce() {\n  doTheDownload(function(err) {\n    if (err) throw err;\n    console.log(\"downloaded\", count++);\n    downloadOnce();\n  });\n}\nfunction doTheDownload(cb) {\n  var request = client.getObject(s3Params);\n  var response = request.createReadStream();\n  var outStream = fs.createWriteStream(localFile);\n  var errorOccurred = false;\n  var contentLength;\nresponse.on('error', handleError);\n  outStream.on('error', handleError);\nrequest.on('build', function() {\n    request.httpRequest.headers.Expect = '100-continue';\n  });\nrequest.on('httpHeaders', function(statusCode, headers, resp) {\n    if (statusCode < 300) {\n      contentLength = parseInt(headers['content-length'], 10);\n    } else {\n      handleError(new Error(\"http status code \" + statusCode));\n    }\n  });\noutStream.on('close', function() {\n    if (errorOccurred) return;\n    fs.stat(localFile, function(err, stat) {\n      if (err) return handleError(err);\n      if (stat.size !== contentLength) return handleError(new Error(\"size mismatch\"));\n      cb();\n    });\n  });\nresponse.pipe(outStream);\nfunction handleError(err) {\n    if (errorOccurred) return;\n    errorOccurred = true;\n    cb(err);\n  }\n}\n```\n```\n...\ndownloaded 24\ndownloaded 25\ndownloaded 26\ndownloaded 27\n/home/andy/tmp/npmtest/test.js:20\n    if (err) throw err;\n                   ^\nError: size mismatch\n    at /home/andy/tmp/npmtest/test.js:54:59\n    at Object.oncomplete (fs.js:107:15)\n```\n. I'm confused as to why you're unable to duplicate the issue. It happens regularly and frequently for me. The most \"downloaded nnn\" number I've ever reached was 40.\nFor what it's worth, here's a slightly simplified test case that only checks file size and removes the etag/md5 calculation from the equation:\n``` js\nvar AWS = require('aws-sdk');\nvar fs = require('fs');\nvar client = new AWS.S3({\n    accessKeyId: process.env.S3_KEY,\n    secretAccessKey: process.env.S3_SECRET,\n});\nvar s3Path = \"1tmp1/node_modules/chem-cli/node_modules/watchify2/node_modules/browserify/node_modules/browser-builtins/node_modules/http-browserify/example/json-stream/node_modules/JSONStream/test/fixtures/all_npm.json\";\nvar localFile = \"test_out.json\";\nvar s3Params = {\n  Bucket: process.env.S3_BUCKET,\n  Key: s3Path,\n};\nvar count = 1;\ndownloadOnce();\nfunction downloadOnce() {\n  doTheDownload(function(err) {\n    if (err) throw err;\n    console.log(\"downloaded\", count++);\n    downloadOnce();\n  });\n}\nfunction doTheDownload(cb) {\n  var request = client.getObject(s3Params);\n  var response = request.createReadStream();\n  var outStream = fs.createWriteStream(localFile);\n  var errorOccurred = false;\n  var contentLength;\nresponse.on('error', handleError);\n  outStream.on('error', handleError);\nrequest.on('build', function() {\n    request.httpRequest.headers.Expect = '100-continue';\n  });\nrequest.on('httpHeaders', function(statusCode, headers, resp) {\n    if (statusCode < 300) {\n      contentLength = parseInt(headers['content-length'], 10);\n    } else {\n      handleError(new Error(\"http status code \" + statusCode));\n    }\n  });\noutStream.on('close', function() {\n    if (errorOccurred) return;\n    fs.stat(localFile, function(err, stat) {\n      if (err) return handleError(err);\n      if (stat.size !== contentLength) return handleError(new Error(\"size mismatch\"));\n      cb();\n    });\n  });\nresponse.pipe(outStream);\nfunction handleError(err) {\n    if (errorOccurred) return;\n    errorOccurred = true;\n    cb(err);\n  }\n}\n```\n```\n...\ndownloaded 24\ndownloaded 25\ndownloaded 26\ndownloaded 27\n/home/andy/tmp/npmtest/test.js:20\n    if (err) throw err;\n                   ^\nError: size mismatch\n    at /home/andy/tmp/npmtest/test.js:54:59\n    at Object.oncomplete (fs.js:107:15)\n```\n. I'm also trying to do the same file download with s3cmd and it's working fine. I haven't seen a single one fail in 200+ downloads.\nwhile true;\ndo rm -f test_out.json;\n  s3cmd get s3://mybucket/1tmp1/node_modules/chem-cli/node_modules/watchify2/node_modules/browserify/node_modules/browser-builtins/node_modules/http-browserify/example/json-stream/node_modules/JSONStream/test/fixtures/all_npm.json test_out2.json;\n  printf \"%d\" $?;\n  md5sum test_out2.json;\ndone\nI ran the node code simultaneously to this and I've reproduced several failures in the node code all the while s3cmd is humming away, no problem, no retries.\n. I'm also trying to do the same file download with s3cmd and it's working fine. I haven't seen a single one fail in 200+ downloads.\nwhile true;\ndo rm -f test_out.json;\n  s3cmd get s3://mybucket/1tmp1/node_modules/chem-cli/node_modules/watchify2/node_modules/browserify/node_modules/browser-builtins/node_modules/http-browserify/example/json-stream/node_modules/JSONStream/test/fixtures/all_npm.json test_out2.json;\n  printf \"%d\" $?;\n  md5sum test_out2.json;\ndone\nI ran the node code simultaneously to this and I've reproduced several failures in the node code all the while s3cmd is humming away, no problem, no retries.\n. What is different about our environments? Bucket? Node.js version? Network distance between local computer and S3 region? Operating system (Ubuntu 14.04 LTS 64-bit)?\nThis issue is perfectly reproducible for me. The culprit must be in the delta between our environments.\n. What is different about our environments? Bucket? Node.js version? Network distance between local computer and S3 region? Operating system (Ubuntu 14.04 LTS 64-bit)?\nThis issue is perfectly reproducible for me. The culprit must be in the delta between our environments.\n. Related? https://github.com/joyent/node/commit/2efe4ab761666\n(Note the comments at the bottom and https://github.com/mscdex/busboy/commit/8aea552aa478e351329b2b04fd5bbca22dae3375)\n. Related? https://github.com/joyent/node/commit/2efe4ab761666\n(Note the comments at the bottom and https://github.com/mscdex/busboy/commit/8aea552aa478e351329b2b04fd5bbca22dae3375)\n. Nope, I upgraded to 0.10.29 as soon as it came out and never looked back :P\nI'll try it.\n. Nope, I upgraded to 0.10.29 as soon as it came out and never looked back :P\nI'll try it.\n. I'm at 400+ with v0.10.28.\n. I'm at 400+ with v0.10.28.\n. Cool, glad we're on the right track. I'm going back to v0.10.29 now. It is the latest \"stable\" version after all.\n. Cool, glad we're on the right track. I'm going back to v0.10.29 now. It is the latest \"stable\" version after all.\n. I've git bisected it to this commit: https://github.com/joyent/node/commit/c862c0348530824bfacef8eadfaf448120d91664\n. I've git bisected it to this commit: https://github.com/joyent/node/commit/c862c0348530824bfacef8eadfaf448120d91664\n. I tested v0.10.29 with that commit reverted and it fixes it. I've also tried setting sslEnabled to false in the test case and it fixes it.\n. I tested v0.10.29 with that commit reverted and it fixes it. I've also tried setting sslEnabled to false in the test case and it fixes it.\n. I tested node.js directly and it's fine. aws-sdk-js is using the streams API incorrectly. Here's an example of using the streams API correctly and it working: https://github.com/andrewrk/aws-sdk-js/commit/522fca46b708d820d11af55357526ce0ddf69fd6 (I have run the above test again with this code and it works fine).\n. I tested node.js directly and it's fine. aws-sdk-js is using the streams API incorrectly. Here's an example of using the streams API correctly and it working: https://github.com/andrewrk/aws-sdk-js/commit/522fca46b708d820d11af55357526ce0ddf69fd6 (I have run the above test again with this code and it works fine).\n. Any update on this? Also is this related to #135 ?\n. Any update on this? Also is this related to #135 ?\n. On Jul 8, 2014 5:50 PM, \"Loren Segal\" notifications@github.com wrote:\n\n@andrewrk I've been looking into this. Your patch could work for 0.10.x,\nbut unfortunately PassThrough is not available in 0.8.x, so it's not\npossible to take that in wholesale.\n\nThis is what the readable-stream module is for. Code your app for 0.10.x\nand use the readable-stream for PassThrough and friends in 0.8.x\n\nI'm not convinced that the SDK is using the streams API incorrectly.\n\nThis statement concerns me.\nSpecifically you should not iterate over the events and blithely re-emit\nthem.\nIf the tests are fixed now I can provide a patch that works for both 0.8\nand 0.10 using readable-stream.\n. On Jul 8, 2014 5:50 PM, \"Loren Segal\" notifications@github.com wrote:\n\n@andrewrk I've been looking into this. Your patch could work for 0.10.x,\nbut unfortunately PassThrough is not available in 0.8.x, so it's not\npossible to take that in wholesale.\n\nThis is what the readable-stream module is for. Code your app for 0.10.x\nand use the readable-stream for PassThrough and friends in 0.8.x\n\nI'm not convinced that the SDK is using the streams API incorrectly.\n\nThis statement concerns me.\nSpecifically you should not iterate over the events and blithely re-emit\nthem.\nIf the tests are fixed now I can provide a patch that works for both 0.8\nand 0.10 using readable-stream.\n. I just looked at your gist. This code is no good:\njs\n      Object.keys(stream._events).forEach(function(event) {\n        resp.on(event, function(arg) {\n          console.log(event);\n          stream.emit(event, arg);\n        });\n      });\nNowhere in the API does it say you can read the _events property. That's a private field.\njs\n  stream.on('newListener', function(event) {\n    if (!sent && event === 'readable') {\n      sent = true;\n      process.nextTick(req.send);\n    }\n  });\nThis is scary, error prone, and unnecessary.\njs\n        // Taken from:\n        // http://nodejs.org/api/stream.html#stream_example_simpleprotocol_v1_sub_optimal\n        // if the source doesn't have data, we don't have data yet.\n        if (chunk === null) stream.push('');\nThis hack is unnecessary if you use the API correctly.\nJust pipe everything and let Node.js handle all the tricky details. \n\nIn all usages of pipe(), both readable and end would need forwarding\n\nThis is trivially proven wrong:\n``` js\nvar fs = require('fs');\nvar inStream = fs.createReadStream(\"blah.txt\");\nvar PassThrough = require('stream').PassThrough;\nvar outStream = new PassThrough();\noutStream.on('readable', function() {\n  console.log(\"got readable\");\n});\noutStream.on('end', function() {\n  console.log(\"got end\");\n});\ninStream.pipe(outStream);\nvar writeStream = fs.createWriteStream(\"blah-out.txt\");\noutStream.pipe(writeStream);\n```\nOutput:\ngot readable\ngot end\nThe code example I gave you earlier is simple and effective. Delete all the weird hacks, just use pipe, and everything is fixed.\nThere's no bug in Node.js. The mystery has been solved. The bug is in aws-sdk-js. Please fix your code.\n. I just looked at your gist. This code is no good:\njs\n      Object.keys(stream._events).forEach(function(event) {\n        resp.on(event, function(arg) {\n          console.log(event);\n          stream.emit(event, arg);\n        });\n      });\nNowhere in the API does it say you can read the _events property. That's a private field.\njs\n  stream.on('newListener', function(event) {\n    if (!sent && event === 'readable') {\n      sent = true;\n      process.nextTick(req.send);\n    }\n  });\nThis is scary, error prone, and unnecessary.\njs\n        // Taken from:\n        // http://nodejs.org/api/stream.html#stream_example_simpleprotocol_v1_sub_optimal\n        // if the source doesn't have data, we don't have data yet.\n        if (chunk === null) stream.push('');\nThis hack is unnecessary if you use the API correctly.\nJust pipe everything and let Node.js handle all the tricky details. \n\nIn all usages of pipe(), both readable and end would need forwarding\n\nThis is trivially proven wrong:\n``` js\nvar fs = require('fs');\nvar inStream = fs.createReadStream(\"blah.txt\");\nvar PassThrough = require('stream').PassThrough;\nvar outStream = new PassThrough();\noutStream.on('readable', function() {\n  console.log(\"got readable\");\n});\noutStream.on('end', function() {\n  console.log(\"got end\");\n});\ninStream.pipe(outStream);\nvar writeStream = fs.createWriteStream(\"blah-out.txt\");\noutStream.pipe(writeStream);\n```\nOutput:\ngot readable\ngot end\nThe code example I gave you earlier is simple and effective. Delete all the weird hacks, just use pipe, and everything is fixed.\nThere's no bug in Node.js. The mystery has been solved. The bug is in aws-sdk-js. Please fix your code.\n. Thanks for solving the issue :)\nLooking forward to the next release.\n. Thanks for solving the issue :)\nLooking forward to the next release.\n. :thumbsup: \n. :thumbsup: \n. I recommend locking the major and minor versions of dependencies, including devDependencies.\n. I recommend locking the major and minor versions of dependencies, including devDependencies.\n. That looks good. If aws-sdk-js had a public API that did equivalent this would solve my use case.\n. Cool. I'll have a chance to try this on Monday.\n. 2 questions.\n1. What is this referring to? Can this be explicit instead?\n2. I don't see the parameter to request.send() documented. What does that do?\n. Here's what I've got based on this API. It's broken because nothing gets written to the pipe destination. When I use this instead of resp then createUnbufferedStream is undefined. https://github.com/andrewrk/node-s3-client/compare/no-pass-through?expand=1\n. I tried both\nthis.response.httpResponse.createUnbufferedStream()\nand\nrequest.response.httpResponse.createUnbufferedStream()\nboth have the same incorrect behavior as the existing code that I linked.\nWhat happens to the error if a callback is not supplied to send? Looks like that callback is mandatory if you want to catch and handle all errors?\n. aha, my mistake - I failed to remove the var response = request.createReadStream();\nquestion about send callback error parameter remains.\n. Thanks for the help.\n. Thanks for the response. This could very well be me not understanding my network tool. I'll close the issue until I have a case to present.\n. Followup: My understanding of how to use iftop was incorrect. Instead I used this:\nss -p | grep node | wc -l\nAnd I saw the expected number of open connections. Sorry for the bogus issue.\n. It happens when the upload takes a long time on a slow connection. Why does the S3 server not use the request start time instead of the request finish time to compare with the value in the user-supplied header, since the header corresponds to the request start time?\n. It happens when the upload takes a long time on a slow connection. Why does the S3 server not use the request start time instead of the request finish time to compare with the value in the user-supplied header, since the header corresponds to the request start time?\n. This seems like a fundamental problem with the S3 protocol that should be fixed.\n. This seems like a fundamental problem with the S3 protocol that should be fixed.\n. Thanks for keeping the issue open for 10 days while I was busy.\nWhen I run this example code, I do not get to the \"Created MP upload\" console.log. It hangs forever on s3.createMultipartUpload.\n$ uname -a\nLinux andy-bx 3.16.0-25-generic #33-Ubuntu SMP Tue Nov 4 12:06:54 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\n$ node -v\nv0.10.29\n$ lsb_release -a\nDistributor ID: Ubuntu\nDescription:    Ubuntu 14.10\nRelease:    14.10\nCodename:   utopic\nv0.10.29 is not the newest 0.10.x version, but sadly this is the version that is available in Ubuntu and Debian's package managers, so it's an important version to support.\n. Got the same behavior with node v0.10.35 (latest stable)\n. ",
    "pyrasis": "I will track the change. thanks.\n. I will track the change. thanks.\n. I updated the latest version of master branch and I tested the code which send a sqs message.\nbut, this error has been occured.\n```\n/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/service.js:478\n          throw AWS.util.error(err, {\n                         ^\nError: Could not find API configuration sqs-2012-11-05\n    at Function.defineServiceApi (/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/service.js:476:23)\n    at getLatestServiceClass (/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/service.js:65:19)\n    at loadServiceClass (/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/service.js:55:19)\n    at Service (/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/service.js:25:29)\n    at new features.constructor (/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/util.js:547:24)\n    at Object. (/home/pyrasis/sqs/sqs_1.js:4:11)\n    at Module._compile (module.js:456:26)\n    at Object.Module._extensions..js (module.js:474:10)\n    at Module.load (module.js:356:32)\n    at Function.Module._load (module.js:312:12)\n```\n. I updated the latest version of master branch and I tested the code which send a sqs message.\nbut, this error has been occured.\n```\n/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/service.js:478\n          throw AWS.util.error(err, {\n                         ^\nError: Could not find API configuration sqs-2012-11-05\n    at Function.defineServiceApi (/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/service.js:476:23)\n    at getLatestServiceClass (/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/service.js:65:19)\n    at loadServiceClass (/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/service.js:55:19)\n    at Service (/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/service.js:25:29)\n    at new features.constructor (/home/pyrasis/sqs/node_modules/aws-sdk-js/lib/util.js:547:24)\n    at Object. (/home/pyrasis/sqs/sqs_1.js:4:11)\n    at Module._compile (module.js:456:26)\n    at Object.Module._extensions..js (module.js:474:10)\n    at Module.load (module.js:356:32)\n    at Function.Module._load (module.js:312:12)\n```\n. I sent and received a sqs message with binary Buffer successfully.\nthanks :+1: \n. I sent and received a sqs message with binary Buffer successfully.\nthanks :+1: \n. ",
    "wlingke": "I see. I did specify the region the bucket was created in the parameters but that did not fix it. However, oddly enough, the next morning, things worked normally again (it actually worked without me specifying the region parameter).\nI'll report back and try your suggestions if this occurs again.. I can't seem to replicate the exact conditions.\n. ",
    "fratuz610": "Hello Loren\nI'm unable to reproduce the error now too. It must have been linked to a temporary redirect misconfiguration on the Amazon side that has been fixed now\nRe: full error / stack trace\nSilly me for not posting it yesterday :(\nI think we can close this for now. If I'm able to reproduce again I will reopen the issue.\nThank you for your time anyway.\nStefano\n. ",
    "BrandonCopley": "Frequently, around every 10 calls to s3.putObject, my calls to s3 were being dropped and they would never timeout, but that was when the memory would leak\n. Thanks, I am adding the socket.setTimeout to my node configuration as well, but this was really being problematic for me since AWS was one of the only external services I was using.\n. I have done a bit of research but am unable to determine where the socket.setTimeout would be set in my files. I am using Express 4 as well, perhaps this is a function of express.\n. I think it is curious as well - I am running node 0.10.26\nI have not checked netstat. For now this appears to be solved by setting the timeout on the amazon request\nOn Wed, Jun 18, 2014 at 6:54 PM, Loren Segal notifications@github.com\nwrote:\n\nIt's a little curious that S3 is not ending this request with a\nRequestTimeout error, as it usually does for inactivity on its end. I would\nbe really surprised if S3 was keeping the socket open indefinitely. In my\ntests, S3 would always eventually kill my connections if they were\ninactive. Are you still running into this issue where S3 is not closing the\nconnection? Have you checked netstat info to see what state the socket was\nin when it was not closed?\nAlso, what version of Node.js are you using?\n\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/297#issuecomment-46509140.\n\n\nhttp://novationmobile.com/\nBrandon Copley\nCEO & FOUNDER\nPHONE::: 512.784.6060\nEMAIL::: copley.brandon@gmail.com  copley.brandon@gmail.com\nWEBSITE::: http://novationmobile.com/ http://www.novationmobile.com\n. This code will fail, and respond with multiple good responses in some instances. My rate was about 1 out of 10 runs if would fail:\nDONE:  57 57\nDONE:  57 58\nWas an actual log from me running this with 57 prefixes.\n```\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\ns3.config = new AWS.Config({\n    maxRetries : 3,\n    sslEnabled : true,\n    httpOptions : {\n    timeout : 10000\n  }\n});\nvar folderCounts = {};\nvar bucket = 'MYBUCKET-NAME';\nvar prefixes = ['folder1','folder2']; //Make sure this has over 50 prefixes, I got this to recreate with 57 prefixes, with around 200 files with each prefix.\nvar numberOfFolders = prefixes.length;\nvar numberOfResponses = 0;\nnumberOfItemsInFolders(bucket,prefixes,function(err){\n  if(err){\n    return console.log('err');\n  }\n  if(numberOfResponses>=numberOfFolders){\n    //I don't know if my count is done right, this is all untested code, but this Done message will appear multiple times, and the number of times it appears changes frequently.\n    console.log('DONE: ',numberOfFolders,numberOfResponses);\n  }\n});\nfunction numberOfItemsInFolders(bucket,prefixes,fn){\n  prefixes.forEach(function(prefix,i){\n    var params = {\n      Bucket : bucket,\n      Prefix : prefix,\n      Delimiter : '',\n      MaxKeys : 1000\n    };\n    s3.listObjects(params, function(err, awsData) {\n      if(err || awsData.Contents === undefined)\n        return fn('listObjects err: ' + prefix + err);\n      numberOfResponses++;\n      folderCounts[prefix] = awsData.Contents.length;\n      return fn(err);\n    });\n});\n}\n```\n. It's not as predictable today, I had it happen twice in a row, but twice, out of hundreds of tries. Perhaps it is more to do with S3's api and that has to fail first.\n. I'm trying, but it's being hard to catch today, I was able to get it twice in a row, but I wasn't recording the stack trace at that point.\n. I have not tested yet. I am not super familiar with using my own repos in\nplace of npm modules. So I need to talk to my node expert on that.\nOn Monday, June 23, 2014, Loren Segal notifications@github.com wrote:\n\nAny response based on the above changes?\n\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/300#issuecomment-46911146.\n\n\nhttp://novationmobile.com/\nBrandon Copley\nCEO & FOUNDER\nPHONE::: 512.784.6060\nEMAIL::: copley.brandon@gmail.com  copley.brandon@gmail.com\nWEBSITE::: http://novationmobile.com/ http://www.novationmobile.com\n. I believe this solves this, thanks!\nOn Tue, Jul 1, 2014 at 3:08 PM, Loren Segal notifications@github.com\nwrote:\n\nClosed #300 https://github.com/aws/aws-sdk-js/issues/300.\n\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/300#event-137140958.\n\n\nhttp://novationmobile.com/\nBrandon Copley\nCEO & FOUNDER\nPHONE::: 512.784.6060\nEMAIL::: copley.brandon@gmail.com  copley.brandon@gmail.com\nWEBSITE::: http://novationmobile.com/ http://www.novationmobile.com\n. ",
    "kosmasgiannis": "Great thanks!\n. Great thanks!\n. ",
    "ryanstout": "Yes, thanks.  The docs on the getSignedUrl method don't really mention that you can use those parameters from getObject.  Anyway, thanks a bunch.\n. It does, thanks.\n. ",
    "dhollenbeck": "+1\n. +1\n. ",
    "cstigler": "@lsegal thanks for the change, but it looks like .gitignore is still in the package published to npm (I ran npm install aws-sdk@* to verify and it created a .gitignore file). Not sure what's going wrong there with your publish process?\n. @lsegal thanks for the change, but it looks like .gitignore is still in the package published to npm (I ran npm install aws-sdk@* to verify and it created a .gitignore file). Not sure what's going wrong there with your publish process?\n. ",
    "jstamerj": "Thanks!  v2.0.3 is working for me.\n. @lsegal - I do not have an easy consistent repo -- it is an error that I've seen very infrequently, around once per month.  I have not tried using v2.1.24 yet.  I will update to v2.1.24 now and check if we still see it occurrences of this error in the next month.\nI can make the error reproduce reliably by injecting an error into the sdk code.  With v2.1.24, when I edit lib/services/s3.js and modify computeContentMd5 to throw one retryable error per request, like this:\ncomputeContentMd5: function computeContentMd5(req) {\n  if (!this.threwSimulatedError) {\n    this.threwSimulatedError = true;\n    var err = new Error('simulated error')\n    err.retryable = true;\n    throw err;\n  }\n  if (req.service.willComputeChecksums(req)) {\n    var md5 = AWS.util.crypto.md5(req.httpRequest.body, 'base64');\n    req.httpRequest.headers['Content-MD5'] = md5;\n  }\n},\n...so that the above function throws the first time and succeeds the second time, it is then easy to reproduce the error with this code:\n```\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\ns3.listObjects({Bucket: 'hurley-config'}, function(err, data) {\n    console.log('result', err, data);\n});\n```\nRunning this in node spits out:\nresult { [NetworkingError: Hostname/IP doesn't match certificate's altnames]\n  message: 'Hostname/IP doesn\\'t match certificate\\'s altnames',\n  code: 'NetworkingError',\n  region: 'us-east-1',\n  hostname: 'hurley-config.hurley-config.s3.amazonaws.com',\n  retryable: true,\n  time: Mon Apr 20 2015 19:57:41 GMT+0000 (UTC) } null\nWithout injecting any errors into aws-sdk, I suspect something in the build step of s3 requests is throwing a retryable error very infrequently, but I don't know what/where/why that may be happening or how to make it occur more frequently.\n. @lsegal - I do not have an easy consistent repo -- it is an error that I've seen very infrequently, around once per month.  I have not tried using v2.1.24 yet.  I will update to v2.1.24 now and check if we still see it occurrences of this error in the next month.\nI can make the error reproduce reliably by injecting an error into the sdk code.  With v2.1.24, when I edit lib/services/s3.js and modify computeContentMd5 to throw one retryable error per request, like this:\ncomputeContentMd5: function computeContentMd5(req) {\n  if (!this.threwSimulatedError) {\n    this.threwSimulatedError = true;\n    var err = new Error('simulated error')\n    err.retryable = true;\n    throw err;\n  }\n  if (req.service.willComputeChecksums(req)) {\n    var md5 = AWS.util.crypto.md5(req.httpRequest.body, 'base64');\n    req.httpRequest.headers['Content-MD5'] = md5;\n  }\n},\n...so that the above function throws the first time and succeeds the second time, it is then easy to reproduce the error with this code:\n```\nvar AWS = require('aws-sdk');\nvar s3 = new AWS.S3();\ns3.listObjects({Bucket: 'hurley-config'}, function(err, data) {\n    console.log('result', err, data);\n});\n```\nRunning this in node spits out:\nresult { [NetworkingError: Hostname/IP doesn't match certificate's altnames]\n  message: 'Hostname/IP doesn\\'t match certificate\\'s altnames',\n  code: 'NetworkingError',\n  region: 'us-east-1',\n  hostname: 'hurley-config.hurley-config.s3.amazonaws.com',\n  retryable: true,\n  time: Mon Apr 20 2015 19:57:41 GMT+0000 (UTC) } null\nWithout injecting any errors into aws-sdk, I suspect something in the build step of s3 requests is throwing a retryable error very infrequently, but I don't know what/where/why that may be happening or how to make it occur more frequently.\n. ",
    "stoufa06": "Because this small option, we were forced to install the whole aws cli to make delete bucket. ",
    "jamessharp": "I've just come up against this problem. I was trying to download a 12MB zip file and was getting incomplete files probably 90% of the time.\nDowngrading to 0.10.28 has fixed it for now, but it's not ideal\n. I've just come up against this problem. I was trying to download a 12MB zip file and was getting incomplete files probably 90% of the time.\nDowngrading to 0.10.28 has fixed it for now, but it's not ideal\n. ",
    "ybogdanov": "aws-sdk@2.0.17 is also breaks in between node 0.10.26 and 0.10.31. Can't provide a lot of details on that, didn't have time to reproduce it carefully. Just rolled back to 0.10.26 to keep everything working.\nI'm doing HEAD requests on S3 (about few hundred in a row), narrowing them by 50 maximum in parallel. Everything was smooth, without any errors or timeouts, until we updated nodejs to 0.10.31. Some of requests (about 5%) started failing. Well, actually they were not failing - the callback simply didn't fire. However, as S3 access logs shown, those requests were actually there and response code was 200.\n``` javascript\nvar params = {\n  Bucket: 'my_bucket',\n  Key: 'my_key'\n}\ns3.headObject(params, function(err, resp) {\n  console.log('you will never see me!')\n})\n```\nMaybe the problem is some way related to the current issue. So I'll just leave it here.\n. ",
    "simon-p-r": "Thanks that worked, I think the docs are in need of updating. Does the createKeyPair api have options to save the file locally before uploading into AWS?\n. Thanks that worked, I think the docs are in need of updating. Does the createKeyPair api have options to save the file locally before uploading into AWS?\n. Again docs are copious but not clear, I only passed in DryRun false to function and it worked.\n. Again docs are copious but not clear, I only passed in DryRun false to function and it worked.\n. I will take a look and see if I can make a pull request for this.  Any pointers on where to begin or any requests from other users what they would like?\nHappy to improve the library! \n. I will take a look and see if I can make a pull request for this.  Any pointers on where to begin or any requests from other users what they would like?\nHappy to improve the library! \n. Ok no problem, just thought there maybe an issue with sdk however have managed to build an instance I can now login to!\nSorry for all the questions?!\n. Ok no problem, just thought there maybe an issue with sdk however have managed to build an instance I can now login to!\nSorry for all the questions?!\n. ",
    "laddi": "Yes, updating the documentation would be EXTREMELY helpful since I've now spent half a day trying to figure out why my base64 encoded PublicKeyMaterial wasn't working properly.  But thanks for the clarification! :smiley: \n. ",
    "magicalhobo": "Since the SDK is only reading the file, not writing, it should be consistent with the aws configure command. The AWS CLI is using Python's os.path.expanduser, which uses HOME, then USERPROFILE, then HOMEDRIVE and HOMEPATH.\nHowever, if you decide to combine HOMEDRIVE and HOMEPATH, there doesn't need to be a backslash in between. :)\n. Looking at it again, can it look for ~/.aws/config instead of ~/.aws/credentials by default? The first is the location that aws configure uses.\n. Thanks @lsegal!\n. ",
    "mattroberts297": "Apologies for the duplicate and thanks for getting back to me so quickly. I completely understand your point regarding the trade-offs around Node-only code. Does the aws-sdk-js contain any Node-only code? I'm wondering if a precedent has been set.\n. Apologies for the duplicate and thanks for getting back to me so quickly. I completely understand your point regarding the trade-offs around Node-only code. Does the aws-sdk-js contain any Node-only code? I'm wondering if a precedent has been set.\n. It most definitely does. Thank you!\n. It most definitely does. Thank you!\n. I had the same issue as @pvamshi. @chrisradek's solution tsconfig.app.json worked for me, but because that file extends ../tsconfig.json you can also just delete \"types\": [\"node\"] if you followed his previous instructions re: tsconfig.json. Thanks for all your hard work troubleshooting this.\n\n. ",
    "lavelle": "I'm using Grunt Mocha version 0.4.11, which uses PhantomJS 1.9.0-1.\nThe call is\njs\ns3.listObjects({ Bucket: 'my-bucket' }, function(error, data) {\n    if (error) {\n        throw error;\n    }\n});\nDo I need to dig into the library internals to get a log of the HTTP Request?\n. I got the same error with 1.9.7.\nHere's the HTTP request.\njs\n{\n  method: 'GET',\n  path: '/',\n  headers: {\n    'X-Amz-User-Agent': 'aws-sdk-js/2.0.5',\n    'Content-Type': 'application/octet-stream; charset=UTF-8',\n    'Content-Length': 0,\n    Host: 'chromeostest.s3-us-west-2.amazonaws.com',\n    'X-Amz-Date': 'Mon, 21 Jul 2014 20:31:44 GMT',\n    Authorization: 'AWS AKIAJAAMWRX6TZ72MR2A:QlyErRA0zOPGCPZQEoNUq7eaw3Y='\n  },\n  body: '',\n  endpoint: {\n    protocol: 'https:',\n    host: 'chromeostest.s3-us-west-2.amazonaws.com',\n    port: 443,\n    hostname: 'chromeostest.s3-us-west-2.amazonaws.com',\n    pathname: '/',\n    path: '/',\n    href: 'https://s3-us-west-2.amazonaws.com/',\n    constructor: [Object]\n  },\n  region: 'us-west-2',\n  virtualHostedBucket: 'chromeostest',\n  stream: {\n    readyState: 4,\n    response: '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><StringToSignBytes>47 45 54 0a 0a 0a 0a 78 2d 61 6d 7a 2d 64 61 74 65 3a 4d 6f 6e 2c 20 32 31 20 4a 75 6c 20 32 30 31 34 20 32 30 3a 33 31 3a 34 34 20 47 4d 54 0a 78 2d 61 6d 7a 2d 75 73 65 72 2d 61 67 65 6e 74 3a 61 77 73 2d 73 64 6b 2d 6a 73 2f 32 2e 30 2e 35 0a 2f 63 68 72 6f 6d 65 6f 73 74 65 73 74 2f</StringToSignBytes><RequestId>68C94C74ACB805BD</RequestId><HostId>Q+1NSJaI505LCLdHXYGn9gk2yT/f0MClPEhMfb+cFsC3EKqWeeZntXcxlG7vy5Wf</HostId><SignatureProvided>QlyErRA0zOPGCPZQEoNUq7eaw3Y=</SignatureProvided><StringToSign>GET\\n\\n\\n\\nx-amz-date:Mon, 21 Jul 2014 20:31:44 GMT\\nx-amz-user-agent:aws-sdk-js/2.0.5\\n/chromeostest/</StringToSign><AWSAccessKeyId>AKIAJAAMWRX6TZ72MR2A</AWSAccessKeyId></Error>',\n    responseXML: [Object],\n    onload: null,\n    onerror: null,\n    onloadstart: null,\n    status: 403,\n    onabort: null,\n    upload: [Object],\n    onreadystatechange: null,\n    responseType: '',\n    onprogress: null,\n    withCredentials: false,\n    responseText: '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><StringToSignBytes>47 45 54 0a 0a 0a 0a 78 2d 61 6d 7a 2d 64 61 74 65 3a 4d 6f 6e 2c 20 32 31 20 4a 75 6c 20 32 30 31 34 20 32 30 3a 33 31 3a 34 34 20 47 4d 54 0a 78 2d 61 6d 7a 2d 75 73 65 72 2d 61 67 65 6e 74 3a 61 77 73 2d 73 64 6b 2d 6a 73 2f 32 2e 30 2e 35 0a 2f 63 68 72 6f 6d 65 6f 73 74 65 73 74 2f</StringToSignBytes><RequestId>68C94C74ACB805BD</RequestId><HostId>Q+1NSJaI505LCLdHXYGn9gk2yT/f0MClPEhMfb+cFsC3EKqWeeZntXcxlG7vy5Wf</HostId><SignatureProvided>QlyErRA0zOPGCPZQEoNUq7eaw3Y=</SignatureProvided><StringToSign>GET\\n\\n\\n\\nx-amz-date:Mon, 21 Jul 2014 20:31:44 GMT\\nx-amz-user-agent:aws-sdk-js/2.0.5\\n/chromeostest/</StringToSign><AWSAccessKeyId>AKIAJAAMWRX6TZ72MR2A</AWSAccessKeyId></Error>',\n    statusText: 'Forbidden'\n  }\n}\nI logged it with depth=1 to avoid the circular structures but if you need the contents of upload, constructor or responseXML let me know.\n. Fantastic, that fixed it! Thanks a lot for your help.\nI personally don't mind about this -- I'm only using PhantomJS for testing, so having some extra code to fix it isn't a problem as it doesn't affect production code at all.\nHope you find a way to fix it anyway though, sure it'd still be useful for some people.\n. Oh okay, I will make a request there. Thanks.\n. Oh okay, I will make a request there. Thanks.\n. Yeah that's great, thanks. I will try and implement it using those. We have a generic 'FAILED' code so I'm sure we can use that if it doesn't match any known mapping. Cheers.\n. Yeah that's great, thanks. I will try and implement it using those. We have a generic 'FAILED' code so I'm sure we can use that if it doesn't match any known mapping. Cheers.\n. ",
    "srchase": "Closing for inactivity.. Closing out this issue.  We appreciate the contribution, but due to the age, feel it does not need to be merged at this time.. Closing out this issue.  We appreciate the contribution, but due to the age, feel it does not need to be merged at this time.. @ilanle \nIs this still an open issue for you?  There have been significant updates to the SDK since this was reported, so we're curious so see if you encounter the same issue using the latest version of the SDK.. This has been removed. Closing this issue.. This has been removed. Closing this issue.. Hello All!\nDynamoDB Transactions are now available as of version 2.365.0.\nCheck out documentation on transactGetItems and transactWriteItems.\n. Hello All!\nDynamoDB Transactions are now available as of version 2.365.0.\nCheck out documentation on transactGetItems and transactWriteItems.\n. @austinkelleher \nThis is something we're reviewing for the next major version bump.  Appreciate you chiming in with your feedback.. @austinkelleher \nThis is something we're reviewing for the next major version bump.  Appreciate you chiming in with your feedback.. Fixed in https://github.com/aws/aws-sdk-js/pull/2246.. DMS waiters were added in 2.345.0.\nClosing out this request.. Closing out this issue.. @cantuket \nThanks for pointing that out.\nAlso, V3 of the SDK, now in Developer Preview, has been modularized solving for the originally requested issue here.. @cantuket \nThanks for pointing that out.\nAlso, V3 of the SDK, now in Developer Preview, has been modularized solving for the originally requested issue here.. @aneilbaboo \nThanks for following up on this issue.\nYou're correct, DynamoDB does offer its own encryption transparently now.\nOne difference would be that the Java Client offers encryption during transit.  Some customers may need that in addition to encryption at rest.  \nWe can hold this Feature Request open for now as a place for that discussion.. @aneilbaboo \nThanks for following up on this issue.\nYou're correct, DynamoDB does offer its own encryption transparently now.\nOne difference would be that the Java Client offers encryption during transit.  Some customers may need that in addition to encryption at rest.  \nWe can hold this Feature Request open for now as a place for that discussion.. @rclark \nIs this still an open issue for you?  We haven't seen anything like this lately, so I'd like to close this unless you have some additional information to add in tracking down the issue.. @rclark \nIs this still an open issue for you?  We haven't seen anything like this lately, so I'd like to close this unless you have some additional information to add in tracking down the issue.. Since there have been significant changes to the SDK, I'm going to go ahead and close the issue.  If you do find that this happens with a more recent version of the SDK, do let us know and we'll be happy to take a look.. Since there have been significant changes to the SDK, I'm going to go ahead and close the issue.  If you do find that this happens with a more recent version of the SDK, do let us know and we'll be happy to take a look.. Create bucket operations are eventually consistent, as shown by getting the correct output on older buckets.  If it's possible, you can use regionalized endpoints instead of making calls to retrieve that location. . Create bucket operations are eventually consistent, as shown by getting the correct output on older buckets.  If it's possible, you can use regionalized endpoints instead of making calls to retrieve that location. . @jacktuck \nWere you able to get this working?\nIf it's no longer an issue, I'd like to close this out.. @jacktuck \nWere you able to get this working?\nIf it's no longer an issue, I'd like to close this out.. @dibakardas67,\nWere you able to get this sorted out on Android devices?  Could you provided the logged httpResponse.body?\nIf this is no longer an issue, I'd like to close this out.. @dibakardas67,\nWere you able to get this sorted out on Android devices?  Could you provided the logged httpResponse.body?\nIf this is no longer an issue, I'd like to close this out.. Closing out this issue.  Let us know if you have anything additional to add.. Closing out this issue.  Let us know if you have anything additional to add.. @adjourn \nAWS Samples is an official AWS organizaion on GitHub.  Hopefully it's a useful resource for you.. @adjourn \nAWS Samples is an official AWS organizaion on GitHub.  Hopefully it's a useful resource for you.. Refer to SignatureV4 in v3 of the SDK.  It's still in preview, but is worth checking out now.\n. Refer to SignatureV4 in v3 of the SDK.  It's still in preview, but is worth checking out now.\n. @itrestian \nWas the suggestion @chrisradek helpful in getting this working for you?  If you don't have any followup questions, I'd like to close out this issue.. @itrestian \nWas the suggestion @chrisradek helpful in getting this working for you?  If you don't have any followup questions, I'd like to close out this issue.. Closing out this issue.  Let us know if you have anything additional to add.. Closing out this issue.  Let us know if you have anything additional to add.. @perhallstroem,\nDid you get this resolved?  Or are you still seeing the same problem?  If it's been resolved for you, I'd like to close out this issue.. @perhallstroem,\nDid you get this resolved?  Or are you still seeing the same problem?  If it's been resolved for you, I'd like to close out this issue.. @dsouzamanish \nIs this still an open issue for you?  If not, we'd like to close it out.. @dsouzamanish \nIs this still an open issue for you?  If not, we'd like to close it out.. This looks like an old issue and may have been resolved.  I'm unable to reproduce the issue with the current version of the SDK, so I'm opting to close this issue.  \nLet us know if you encounter the issue with a current version of the SDK and we can re-open.. This looks like an old issue and may have been resolved.  I'm unable to reproduce the issue with the current version of the SDK, so I'm opting to close this issue.  \nLet us know if you encounter the issue with a current version of the SDK and we can re-open.. @Suiname \nThis is currently working.  Are you able to get this working with the latest version of the SDK?. @andrewgoodchild \nAs it looks like you went a different direction, and there is also the option of presigned POSTs as @jeskew suggested, I'm going to close this issue.. @andrewgoodchild \nAs it looks like you went a different direction, and there is also the option of presigned POSTs as @jeskew suggested, I'm going to close this issue.. @testtshoretel \nI apologize we haven't responded sooner.\nI'm not sure I fully understand your last question.  Can you restate your question?  Or did you get this resolved already?. @testtshoretel \nI apologize we haven't responded sooner.\nI'm not sure I fully understand your last question.  Can you restate your question?  Or did you get this resolved already?. @optimisme \nIs this still an open issue for you?  Quite a bit of time (and node versions) have passed since it was originally opened.. @optimisme \nIs this still an open issue for you?  Quite a bit of time (and node versions) have passed since it was originally opened.. @sridharrajagopal \nIs this still an outstanding issue?  Since you raised this issue, development on PhantomJS has stopped.\nThere are some more recent alternatives that would likely be a better fit for you long-term as they are being developed.. @sridharrajagopal \nIs this still an outstanding issue?  Since you raised this issue, development on PhantomJS has stopped.\nThere are some more recent alternatives that would likely be a better fit for you long-term as they are being developed.. As PhantomJS is no longer supported, I'm going to close this issue.  Let us know if you have any issues using the SDK with the current alternatives.. As PhantomJS is no longer supported, I'm going to close this issue.  Let us know if you have any issues using the SDK with the current alternatives.. @jdilkes,\nDid you have any further questions?  It looks like you got it sorted in #1599.. @jdilkes,\nDid you have any further questions?  It looks like you got it sorted in #1599.. Closing out this issue.  Let us know if you have anything additional to add.. Closing out this issue.  Let us know if you have anything additional to add.. @softprops \nDid you have any followup questions?  Between the change by @chrisradek and suggestion from @mdobro, I hope you were able to figure out a solution.. @softprops \nDid you have any followup questions?  Between the change by @chrisradek and suggestion from @mdobro, I hope you were able to figure out a solution.. Closing out this issue.  Let us know if you have anything additional to add.. Closing out this issue.  Let us know if you have anything additional to add.. @tyrsius \nI just wanted to chime in that v3 of the SDK is available now as a preview.  It has a modular design that should improve performance in Lambda and elsewhere.. @tyrsius \nI just wanted to chime in that v3 of the SDK is available now as a preview.  It has a modular design that should improve performance in Lambda and elsewhere.. @pawansharma15 \nWere you able to get this working?. @pawansharma15 \nWere you able to get this working?. Closing out this issue.  Let us know if you have anything additional to add.. Closing out this issue.  Let us know if you have anything additional to add.. It looks like this was resolved.  Let us know if you have followup questions.. It looks like this was resolved.  Let us know if you have followup questions.. @meyerbro \nDid you followup with the Service Team on their Developer Forum?. @meyerbro \nDid you followup with the Service Team on their Developer Forum?. @loretoparisi \nIt looks like you got this working.  Let us know if there are any additional details you can add regarding the original issue you encountered.. @loretoparisi \nIt looks like you got this working.  Let us know if there are any additional details you can add regarding the original issue you encountered.. @arian-kh \nThis has been added to the model by the Service Team.\nClosing out this issue.. @arian-kh \nThis has been added to the model by the Service Team.\nClosing out this issue.. @navaneetharaopy \nCan you share how the ec2 ram usage compares to doing the same thing locally?. @navaneetharaopy \nCan you share how the ec2 ram usage compares to doing the same thing locally?. @benishak \nWere you able to get this resolved?  \nI'm curious if you engaged the DynamoDB team.  The apparent discrepancy in behavior on __type between the local version and the service would be interesting to get them to weigh-in on.. @benishak \nWere you able to get this resolved?  \nI'm curious if you engaged the DynamoDB team.  The apparent discrepancy in behavior on __type between the local version and the service would be interesting to get them to weigh-in on.. @davidmaxwaterman \nFrom the comments exchanged between you and chrisradek, it appears this was resolved.\nI'd like to close out this issue, but can re-open if you have followup questions.. @davidmaxwaterman \nFrom the comments exchanged between you and chrisradek, it appears this was resolved.\nI'd like to close out this issue, but can re-open if you have followup questions.. @sam-qburst,\nIs this working for you?\n@iassal has posted a comment on another issue that may be helpful.. @sam-qburst,\nIs this working for you?\n@iassal has posted a comment on another issue that may be helpful.. Closing out this issue.  Let us know if you have anything additional to add.. Closing out this issue.  Let us know if you have anything additional to add.. @rgmembreno \nWere you able to log the s3Client.config inside and outside a test to see how they differ?\nIs this still an open issue for you?. @awitherow \nDid you get this working?  Are there any further details you can provide?. Closing for inactivity.. Closing for inactivity.. Closing for inactivity.  Let us know if this is still an open issue and we can re-open.. Looks like @chrisradek explanation was suitable to get things working for React-Native.\nClosing out this issue.. @aichholzer \nI apologize we did not respond sooner.\nPer the documentation, httpUploadProgress is \"Triggered when the uploader has uploaded more data\".\nIs there a different kind of behavior that you are expecting?. This was investigated further.  httpUploadProgress is triggered as an event when that part is sent.  It is not based on when the upload of a given part succeeds.\nhttpUploadProgress gives some approximation for working through large uploads, but may not give the full picture because of the timing of how those events are fired.. @piercus \nWere you able to get this resolved?\nI'm curious if the Service Team had any insight.  Did you reach out on their AWS Developer Forum?. @rcfrias \nIs this still an open issue for you?. @caleb0199 \nAre you seeing this issue with the current version of the SDK?  . I'm going to close this issue then.  If anyone has this issue with a reliable repro, feel free to open a new issue.. @wmagda \nIt looks like this isn't an issue with the SDK itself, but more of a usage question.\nYou can get started with our documentation here: AWS SDK for JavaScript Documentation. Both the Developer Guide and API Reference on that page should be helpful in showing you how to utilize the SDK to authenticate and make calls with the s3 client to fetch the objects in a bucket.\nIf you have specific questions on using the SDK, you can also post on the AWS Developer Forums or use the aws-sdk-js tag to post a question on Stack Overflow.\nIf you encounter an issue with the SDK where it doesn't seem to be working as documented, feel free to open a new issue here.. @Lanceshi2,\nWere you able to sort this out?  It looks like you have a usage question, and not an issue with the SDK itself.  I'd like to close this out if you have no further questions.. Closing out this issue.  Let us know if you have anything additional to add.. @sjakthol \nWere you able to sort this out?  Or are you now able to provide some simplified code for reproducing the issue?. @roman-io \nWould you be able to provide an example that would help us with a reliable repro for the issue?. @roman-io \nAre you also using Lambda?\nWould you be able to capture the request-ids for these failures?  Those request-ids could be used by the Service Team to investigate further.. We've reached out to the IoT Service team to understand why this CORS error is being returned.. @msl-kabo \nattachPrincipalPolicy is deprecated.\nDo you encounter the same issue if you use attachPolicy instead?. @dkesler,\nDid you sort out this issue?  Are you seeing consistent behavior between buckets?. Closing out this issue.  Let us know if you have anything additional to add.. @dumbird,\nDid this issue persist?  Or were you able to track down anything that hints at the cause?. As there have been significant updates to the SDK, I'm going to close out this issue.  If you encounter this issue in a reliable way with a recent version of the SDK, let us know and we'll be happy to re-open and take another look.  Thanks!. Looks like the suggestion here was helpful.  As there isn't an problem with the SDK itself, I'm going to close this issue.. Hello,\nI'm opting to close this issue.\nCheck out SignatureV4 in v3 of the SDK. It was purpose built to be public and supported.  v3 is in preview, but would be a good solution once it's ready for production use.. Closing this issue as it was referred to the mobile analytics repository.. @bradennapier \nIs this still an issue for you?  Did you see these crashes on a stable release of Electron?\nIf it's not, I'd like to close out this issue.. Closing out this issue.  Let us know if there is any followup needed.. @sastrygunnu,\nDid this get things working for you? I'm going to close out this issue as it looks like you've got more of a usage question.. @usamamashkoor,\nDid this get things working for you?  Do you have any followup questions?  It looks like this is a usage questions, and not an issue with the SDK, so I'd like to close this if there's nothing further.. Closing out this issue.  Let us know if you have anything additional to add.. Closing without further information to reproduce.. Hello @doapp-jeremy \nI apologize we have not responded sooner.  Is this still an open issue for you, or were you able to get it resolved?. Closing without further information to reproduce.\n. @dhawalmewada \nWere you able to get this resolved?  I'm curious if you have the same issue with a current version of the SDK and Ionic 2?. @patrick-motard \nDo you have any further questions? If not, I'd like to close this issue.. Closing out this issue.  Let us know if you have anything additional to add.. @IvanAlegre \nIs this still an open issue for you?  I apologize we did not respond sooner.. Closing this issue as an answer was provided.. A change to make this available in the browser build has been merged.  It will be available in the next release: v2.382.0. @rainnaren \nIs this still an open issue for you?. Hi @e-gineer,\nAs there is not an issue with the SDK here, I'm electing to close this thread.  If anyone has suggestions for alternatives, they'll still be able to post it here.. @RoyLiou,\nDo you have any   further questions? If not, I'd like to close this issue.\n. Closing out this issue.  Let us know if you have anything additional to add.. We're electing to continue testing against these versions.  Thanks for the contribution.. @roberthelmick08,\nWere you able to get this resolved? . Closing out this issue.  Let us know if you have anything additional to add.. Closing at this was handled in the thread on Cognito Identity.. As there has been no followup, I'm assuming this has been resolved.  Let us know if it needs re-opened.. I'm going to close out this issue, as it looks like it was resolved.  Let us know if you have any further questions.. Glad to see this was resolved.. @hoodsy,\nWere you able to get this sorted? If there are not further questions, I'm going to close out this issue.. Closing out this issue.  Let us know if you have anything additional to add.. @danielrambo \nWere you able to get this sorted?  If there are not further questions, I'm going to close out this issue.. Closing out this issue.  Let us know if you have anything additional to add.. @dcchristopher \nDid you have a followup question?  If not, I'd like to close this issue.. @riteshapatel,\nLooks like you were able to find a sufficient answer over on the CLI repository.  I'm going to close out your issue here.. @michaelyfan,\nDo you have any further questions?  If not, I'd like to close this issue.. Closing out this issue.  Let us know if you have anything additional to add.. Hi @taylor-a-barnette,\nSounds like you were able to look to other resources to get you questions answered.  As there isn't an issue with the SDK, I'm going to close out this issue.. @normanhuang \nCan you provide an example of your code?. @spilliton \naws-sdk is available in all Lambdas.\nLambda is currently using 2.290.0, but this page will be updated when that's moved to a newer version: Lambda Current Supported Versions. @KMiso,\nWere you able to get this figured out?. Closing out this issue.  Let us know if you have anything additional to add.. @RomainGeffraye \nDid you raise this issue with the Service Team?\nThat kind of validation would need to be handled on the Service side.. @dheffx \nWorking to get this functionality pulled in with https://github.com/aws/aws-sdk-js/pull/2559.\nAppreciate your contribution!  Apologize we didn't close the loop sooner on getting this pulled in.. As this is still being sorted out in DefinitelyTyped, I'm going to close this isssue.. @simonbuchan \nSee @jeskew's comment above: https://github.com/aws/aws-sdk-js/issues/1939#issuecomment-366773580\nI'll leave this issue open to track improving the documentation, but the S3 service team would need to implement CORS support for listBuckets to work from the browser.. @TrejGun \nOpened a PR to add a note.\nIf there are any other docs issues you feel need addressing, we'd be happy to take PRs.. Closed for inactivity.. @JimtotheB \nWere you able to get this resolved? Did you confirm it is a Firefox issue?. Closing for inactivity.. This looks like a usage question and not an issue with the SDK.  Closing this out, but let us know if you have any issues getting things configured properly.. @bluepeter \nWere you able to get this resolved, or to reliably reproduce it with another table?. Per the DynamoDB Query Documentation:\n\nIf LastEvaluatedKey is not empty, it does not necessarily mean that there is more data in the result set. The only way to know when you have reached the end of the result set is when LastEvaluatedKey is empty.\n\nAs this is the intended behavior of the DynamoDB Service team, I'm opting to close this issue.. Closing for inactivity.. @mazerab \nDid @nickmacia's suggestion take care of this issue for you?. @no-on3 \nI apologize we did not respond to this issue earlier.\nAre you still having the same problem or were you able to resolve this?. @leonetosoft \nRefer to https://github.com/aws/aws-sdk-js/issues/1435\nThe SDK does not perform the compression itself, but @jeskew offers a suggestion that could potentially work for you: https://github.com/aws/aws-sdk-js/issues/1435#issuecomment-290448968\nLet us know if you have any further questions.. No response. Closing this issue.. Closing for inactivity.. Hello @Aedalus,\nThanks for reaching out with this question.  I apologize that we did not respond sooner.\nCurrently, the SDK's CloudFront signer produces V2 signatures.  There could be work done to add support for V4 signatures, but we first need to confirm that CloudFront supports KMS encrypted uploads to S3.  I've contacted the CloudFront team with that question and will update this issue once I've gotten a response.\nIf they confirm it's possible, we can then track this as a feature request.. @Aedalus \nThe Service Team confirmed that KMS encrypted PUTs via CloudFront are possible.\nThere are however a few requirements:\n1.  This cannot be done with an Origin Access Identity in place.  That OAI cannot be accessed by KMS.\n2. The signature needs to be calculated for the underlying S3 API endpoint: a PUT for 'foo.txt' to 'mybucket.s3.amazonaws.com' would have to be calculated against: 'mybucket.s3.amazonaws.com/foo.txt'.\n3. That signature must be added as a query string or headers on the PUT to CloudFront.. @mdaum \nThanks for your patience.\nI apologize we missed this issue.\nWere you able to resolve this issue?  If so, what change did you make?  \nWhich simple iterative approach did think might be a better approach for the SDK to use?. Closing for inactivity.. @jewelsjacobs Lambda is now using 2.290.0, so you'll be able to use this operation.\n@willvincent \nWe appreciate your candid feedback. Is there a specific spot where you got stuck?  If you were interested in posting an example, we'd be happy to help clear up any issues (and improve the documentation where possible).\n@robermar2\nI do see there is a typo in the Async Iterator EventStream Example (Experimental).  The line const events = data.Payload; should be const events = result.Payload;\nWould that solve your issue?  Or is there something else that isn't working?. Fix for the Async Iterator EventStream example in https://github.com/aws/aws-sdk-js/pull/2549. Hello @maxeber,\nI apologize that we did not respond to your question sooner.\nThis line const url = S3.getSignedUrl('getObject', {, includes 'getObject'.  Did you intend for that to be 'putObject'?\nIf you already resolved this issue, let us know and we can close this out.  If it's still an issue for you, we're happy to help.. I'm opting to close this issue.  As @AllanFly120 mentioned, it is not recommended to extend from client classes.. @davidgatti \nI apologize we didn't get back to you sooner.\ncreateReadStream() is documented on the AWS.Request.\nHow are you saving the file to disk?\n. Closing for inactivity.  Let us know if you have code to share in helping to resolve this issue.. @bkarv,\nDo you still have an open question?  If not, we'd like to close out this issue soon.. Hello @dougwit,\nWere you able to uncover any additional details to help reproduce this issue?. @dougwit,\nThanks for following up.\nThis is the expected behavior.  The code you referenced was introduced to drive consistency between the responses returned by simple and multi-part upload operations.\nAdditional details are on the original issue.\n. @Baraksi \nThanks for elaborating.  I apologize we didn't get a response to you sooner.\nHow are you getting the file name from the S3 bucket?  Via the console?  Or a call with the SDK?. @taylor-zr \nThe 'response' you're attempting to return is the actual request, which has a circular reference to itself, resulting in the error you're seeing.\nCould you try this instead?\nexports.handler = async (event) => {\n    return await s3.getObject(params).promise()\n    .then((res) => {\n        return res.Body.toString('utf-8');\n    })\n    .catch((err) => {\n        return err;\n    });\n};. @angrychewie \nI'm opting to close this issue.\nThis is a service issue and the Cognito Team would be best suited to discuss this in greater depth.\nYou can post on their AWS Developer Forum. . @xtianus79 \nDid you have any followup questions?  Did @AllanFly120's answers provide any help in getting this resolved?. @nicolaerusan \nCan you try using the --debug command with the CLI?\nYou can compare that against the response object using the SDK to see if there are any differences in the requests being made that might help account for the differences in results that you're seeing.\nHere is documentation on using the Using the Response Object .. Glad you got it working.\nI'm going to close this issue.  If you have any further details from logging the calls that would point to an issue with the SDK itself, let us know and we can re-open.. Hi @nitrocode,\nWas @AllanFly120's suggestion helpful?  Going to close this issue out soon if there's nothing further.. I'm going to close this issue.  If you have a separate memory issue you feel needs addressing, please feel free to open that separately.. @nitrocode,\nIs this request to support Combine still needed?  Or was this issue solved via the suggestions in #2083?. Hi @roccomuso,\nAre you able to call DescribeStream to get back the relevant StreamInfo object?\nOr were you able to sort this out another way?\n. @roccomuso \nDo you have any additional questions?  If there's nothing further, I'm going to close out this issue.. Hi @Nop0x,\nIt looks like your questions have been answered. I'm going to close this issue soon unless there's anything further.. @ccorcos \nThanks for the suggestion.  There is a note in the documentation about using an asynchronous callback when credentials need to be resolved, but we appreciate the suggestion that this could be made more clear.. Hello @lenin-jaganathan,\nWere you able to determine if axios is setting the wrong content-length?. This is a known limitation.  Closing out this issue as we've done our best to document this in the README.. @danielbdias,\nIt looks like this has been resolved.  I'm going to close this issue soon unless there's anything further.. Fixed in https://github.com/aws/aws-sdk-js/pull/2246.\nClosing out this issue.. @jayeshsheta \nI apologize we didn't get back to you on this issue.\nAre you still encountering this issue?  Can you provide additional details if it is still happening for you?. Closing without further information to reproduce.. @jchirschy \nWere you able to get this resolved?\nThe jobs endpoint should be something like: XXX.jobs.iot.us-west-2.amazonaws.com. @nsraptor \nCan you provide an example of your code?. @stripathix \nGetting a 'refresh token' will depend on the identity provider's implementation.  You'll need to refer to the documentation from whichever identity provider you are using.. Closing without further information to reproduce.. @kishoredonepudi \nWere you able to get this issue resolved?  Or are you still seeing the same behavior?\nDid you engage the SNS Service Team on their AWS Developer Forum?. @Autushka \nI apologize we did not followup with you on this issue sooner.\nAre you still seeing this behavior?  Using the latest version of the SDK, I am able to successfully use the expires option to create a signed URL or Cookie.. Hello @Yaswanth-C,\nTo update a single field, instead of updating the entire item, your ExpressionAttributeValues should only include the field to be updated.  The fields that are not being updated cannot be included as empty fields or set to null.\nFor example, if you just want to update the description, params will need to look like this:\nparams = {\n  TableName: tablename,\n   Key:{\n     \"ID\": event.ID\n   },\n  UpdateExpression: \"set description = :d\u201d,\n ExpressionAttributeValues:{\n    \":d\":event.description\n  },\n  ReturnValues:\"UPDATED_NEW\"\n}\nIf the fields to be updated can vary in your call to the Lambda function, you\u2019ll need to dynamically generate params to include only the appropriate fields for the update call.. @isaacrdz \nIf you have questions on usage of the SDK, you can refer to the AWS Developer Forums.. Hello @coolrz,\nThanks for reaching out.\nWhen you say that \"the same code/role another account is ok.\" are you referring to running this same code in another Lambda function?  Or are you executing it another way?. Hi @Berycled,\nThere's an open Feature Request for this:\n1164\nClosing this issue.  Please feel free to join the discussion on that thread.\n. Hello @andrew-aladev,\nThe CognitoIdentityServiceProvider supports creating a group and you do not need an IdentityPool or the associated IDs.\nPer the documentation linked below, only the GroupName and UserPoolId are required to call createGroup.\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html#createGroup-property\nClosing this issue. If you have further questions, please refer to the AWS Developer Forums.. Thanks for submitting this issue.  We need to consider the full scope of how modeled exceptions should to be handled.  I'll followup once we're able to dig into this and determine if adding support or accepting a PR is the right approach.. Hello @seanirby,\nWe appreciate your interest in submitting a pull request.   At this time, we\u2019re not able to accept a customization to provide the structured errors for this specific case.\nInstead, this is something we\u2019re considering as part of a larger effort to support structured errors across the SDK and is something that would potentially be included in a future major version release.. As this is a known limitation and is actively being considered for a a future major release, I'm going to close out this issue.  Thanks for submitting this originally.. Hello @crjacinro,\nThanks for bringing this to our attention.\nI've opened an issue on the Amplify repository to sort out this issue: https://github.com/aws-amplify/amplify-js/issues/1442. Since this is being tracked as a bug on the Amplify repository, I'm going to close out this issue.. Hello @andrew-aladev,\nThanks for bringing this to our attention.  This is a server-side issue, and not a problem with the JavaScript SDK itself.  You'll get the same error if you tried to do this with the CLI or another SDK calling the CognitoIdentityServiceProvider.\nWe've reached out to the service team to get this resolved.. The Service Team has not provided an update on getting this resolved.\nI'm going to opt to close this issue for now, as it is not something the SDK Team can resolve independently.  I can update when that team does get this resolved.. Hello @mohammad1990, \nCan you log the request headers?\nThat might help explain what's happening here.\nThanks.. Hi @mohammad1990,\nI'm not familiar with Passport and Passport-JWT, but I suspect the JWT token being set as an authorization header is causing the mismatch.\nLogging the request headers would help to explain what's happening.. Are additional Request Headers being included by the browser?\nDepending on which browser you are using, the Developer Console can show the HTTP Request Headers that are being sent.. By default, Passport-jwt uses the authorization header to store the token, which is interfering with that header being used by s3. passport-jwt provides alternative strategies: https://www.npmjs.com/package/passport-jwt#extracting-the-jwt-from-the-request\nI'm going to close this issue as there isn't a problem with the SDK.  You can check the jwt documentation for more info on alternative strategies or refer to the AWS Developer Forums for questions on using the SDK.. Hello @d1sco,\nThe API Gateway service handles the parsing of the authorizer policy object.  That isn't done by the SDK itself.\nI'm going to refer you over to the AWS Forums.  They'll be best suited to help you understand why these attributes are being lost.. @dpmallinger \nWhat's your use case for saving options?. Hello @olahivepriyanka,\nHas the ContentType been properly set on the object?\nYou can use headObject to get the metadata, including ContentType, of an object without retrieving the object itself.. The signed URL does include the file-extension.\nThe extension is just prior to the \"?\", where the key is used as the final part of the path, in this case as \"1531994728558.mp3\".  The extension is not one of the query params and would not be added onto the \"X-Amz-SignedHeaders\" value of \"host\" at the end of the full SignedURL.\nI'm not sure what media player you're using, but you may want to check their documentation to understand how URLs with query params need to be handled.. Hello @sgil-carecloud,\nWhich version of the SDK are you using?\nCan you provide the code you are using?. @sgil-carecloud \nClosing as inactive.  Feel free to respond if you have additional details.. Hello @Grimml,\nWhich version of the SDK are you using?  Are you using Node.js (which version?) or running this somewhere else?. Executing the code you've provided, I get an error that the Port needs to be a string:\n\nInvalidParameterType: Expected params.DefaultActions[0].RedirectConfig.Port to be a string\n\nFixing that, I'm able to create a listener successfully.\nIs there some other part of your code that could be involved?  Anything with the execution role?\nAre you able to run this outside of Lambda successfully?. Glad to hear it's working for you.\nI'll hold this issue open for a few days before closing it in case you're able to find a consistent way to reproduce the errors.. @bright-future \nIs there something specific that isn't working on the SDK?  Or is your question more about using sinon?\nWhat version of the SDK are you using and in which environment is the code running?. @bright-future,\nHere's an example using Sinon and Promises: https://stackoverflow.com/questions/26243647/sinon-stub-in-node-with-aws-sdk/49260374#49260374. @bright-future \nYou may want to cross-post to Stack Overflow for specific questions on using the Sinon with the SDK.\n. This suggested change is on the resource model, which is managed by the Service Team. Those models cannot be changed from this repository.  Thank you very much for this contribution, but we will need to close out this request.. @tglines,\nHow are you currently using the response from createPresignedPost?\nIf you append the key/value pairs to formData before submitting, and attempt to upload a file with a different filetype than what was set in createPresignedPost, you'll get back a 403 response that includes the following:\nAccessDenied\nInvalid according to Policy: Policy Condition failed: [\\\"eq\\\", \\\"$Content-Type\\\", \\\"image/png\\\"]. @tglines \nWere you able to get this resolved?  I was hoping to get some additional details your use case.. Closing out this issue.  Let us know if you have anything additional to add.. @Ferrari \nDoes the latest version of the SDK give you the same error now?. Hi @mclaborn,\nThat feature has been requested of GitHub, but is not currently possible: https://github.com/isaacs/github/issues/410\nYou can get an atom feed of only releases here: https://github.com/aws/aws-sdk-js/releases.atom\nThis comment lists the other atom feeds for a repository: https://github.com/isaacs/github/issues/410#issuecomment-163761492. hello @dougmoscrop,\nThanks for pointing out this inconsistency.  Unfortunately, this message is being left out by the underlying call to the s3 service.  You'll get the same inconsistency if you were to use a different SDK or the CLI.\nWe've reached out to the service team to see if we can get this resolved.. Hello again,\nI have not received a response from the service team.  Since this cannot be resolved via the SDK, I'd encourage you to open a question on the AWS S3 Forum.\nI'm going to close this issue.  If you do open a request on the forums, let us know and we can pass along the link to the service team with our original request.. @jortegamo \nDue to inactivity, I'm going to close this issue.  If you have further details, please respond and we can reopen.. Hi @alex-at-cascade,\nThanks for reaching out with this feedback on where you'd like to see us improve documentation on parameter limits.. Hi @alex-at-cascade,\nOne quick note, it's up to the service to fully specify those limits.  We've passed along your feedback, but you're welcome to submit that feedback to directly on the AWS Developer Forums.. As this isn't an issue with the SDK itself, I'm going to close out this issue.. @LovikaJain \nYou can get started with our documentation here: AWS SDK for JavaScript Documentation. Both the Developer Guide and API Reference on that page should be helpful in showing you how to utilize the SDK to authenticate and make calls with the s3 client to fetch the objects in a bucket.\nIf you have specific questions on using the SDK, you can also post on the AWS Developer Forums or use the aws-sdk-js tag to post a question on Stack Overflow.\nIf you encounter an issue with the SDK where it doesn't seem to be working as documented, feel free to open an issue here.. @sdrioux \nThe eventSourceArn is not provided by the underlying service (as documented here).\nWe will pass along the request to have it added, and you could make the same request on the AWS Developer Forums.\n. @sdrioux \nUnfortunately, these are not perfectly consistent inside and outside of Lambda.\nWe've passed along the feedback to have it added.  You're welcome to submit that feedback to the DynamoDB service directly.\nAs there isn't an issue with the SDK itself, I'm going to close out this issue.  Thanks for your understanding.. @mvidalis \nThat command prints the version of NPM itself, not the version of the installed package.\ntry this instead: npm show aws-sdk version. @mvidalis \nGo ahead and try uninstalling and reinstalling the package.  Let us know if that makes a difference. . @ztj1993 \nDid you refer to this document for setting up your CORS Configuration?\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/cors.html\nI don't know the specifics of your required setup, but just wanted to check if that was something that needed addressed ahead of integrating a proxy.\n. @ztj1993 \nIs there a specific issue you can't get resolved with the CORS configuration?  That configuration is done at the bucket level, so as long as you have an existing bucket, you should be able to do do whatever you need to without a proxy.  Are you needing to create buckets?\nYou may want to ask on Stack Overflow for help in getting mult-account authentication working with Nginx.. @ztj1993 \nLet us know if there's anything that looks like an issue on the workings of the SDK itself.. @paulfryer \nSageMaker is included in the current SDK.\nThe documentation can be found here\nWhich version of the SDK are you using?\n. @marmuel,\nWhich version of the SDK are you using?  How are you executing this code?. @marmuel \nAre you able to import Comprehend into your app like this?\nimport { Comprehend } from 'aws-sdk';\nlet comprehend = new Comprehend(options);. Closing for inactivity.. Closing as this has been implemented.. @davidadas \nWhich version of the SDK are you using?. @davidadas \nAlso, have you tried to use the AWS CLI to get the same signed URL?. @davidadas \nIt looks like you're passing in milliseconds for the expiration instead of seconds.  This is putting the expiration date past the allowed maximum of 2147483647 (January 19th, 2038 at 03:14:07 UTC) and resulting in a bad policy.\nCan you try switching that to seconds to see if that resolves the malformed policy error?. @davidadas \nI'm going to close out this issue.  If that doesn't do the trick, let us know and we can re-open.. Hi @davidadas,\nAre you still getting a malformed policy error?  Or a 403? Or a different error?\nIn your code, are you using the CloudFront distribution url?  Or the url of the actual s3 bucket?\n```\nconst AWS = require(\"aws-sdk\");\nconst fs = require(\"fs\");\nconst key = fs.readFileSync('./cloudFrontPrivateKey.pem').toString(\"ascii\");\nconst id = 'cloudFrontAccessKeyId';\nconst signer = new AWS.CloudFront.Signer(id, key);\nconst params = {\n  url: 'https://myDistribution.cloudfront.net/foobar.jpeg',\n  expires: 1538999532,\n};\nsigner.getSignedUrl(params, function (err, data) {\n  if(err) { console.log(err) }\n  console.log(data);\n});\n```\nThe url I get from something like this is working for me.. Yes, Restrict Viewer Access is set to \"Yes\" and I've set Trusted Signers to \"self\" with a CloudFront key pair setup on the \"My Security Credentials\" page.. @empty-Seth \nI'm not getting this error when executing this code with node using the latest version of the SDK.\nWhich version of the SDK are you using?\nAre you able to successfully upload those same larger png's and jpg's using the AWS CLI?. @empty-Seth \nHow about with the AWS CLI?\nhttps://aws.amazon.com/cli/. @empty-Seth \nLet us know if you're able uncover any additional details that would help to get a reliable reproduction of the issue.. @elliotthill \nWhen building the SDK, you can use the 'all' parameter to include every service:\nnode dist-tools/browser-builder.js all > aws-sdk-full.js\nBy default, only select services are included to keep the package size down.\nSimilarly, you could use Webpack and only import the individual services you need, which will also help to minimize the package size.  Documentation can be found here: Bundling with Webpack. If you're installing with NPM, you won't need to do any of the build steps.\nCould you try an npm install aws-sdk in a fresh directory?. Glad to hear it's working.\nIf necessary, NPM can install a specific version of the SDK, but by default it will install the latest.\nLet us know if there's anything else on this issue.  If not, we'd like to close it out.. 2.6.7 does exist: https://www.npmjs.com/package/aws-sdk/v/2.6.7\nYou can see the full list of versions on the NPM package page here.. Closing out this issue.  Let us know if you have any further questions.. @univerze \nAre you receiving Rendering Failure Event Notifications?  Those will be generated when an email is accepted, but cannot be rendered and delivered due to the missing template variables.\nDouble check you're setup to receive those notifications.  The documentation can be found here.\n. How are you populating the accessKeyId and secretAccessKey?\nIf those are hardcoded strings (as you acknowledge is not recommended), this should be working.\nAre you doing anything else that would be trying to resolve credentials elsewhere in your code?. Which version of the SDK are you using?  How are you executing the code?\nUsing node and the latest version of the SDK, logging s3.config.credentials should give you something like this (based on your credentials):\nCredentials {\n  expired: false,\n  expireTime: null,\n  accessKeyId: '{your_access_key_id}',\n  sessionToken: undefined }. Thanks for submitting this feature request.\nTo make sure we understand your use case, is this something you'd want to specify when calling runJobFlow?. @whitfin \nOne additional thought:\nThe SDK does support addJobFlowSteps.  It's not documented (and certainly not as clean as supplying a flag like the CLI), but you could follow the pattern for the Java SDK to enable debugging that way: Example Enabling debugging using the Java SDK. @ANTGOMEZ \nWhen you're calling getParameter, you're supplying a callback, so that call that is being made asynchronously.\nIf you want to use the response object, you can do something like this:\n```\nvar request = ssm.getParameter(params);\nrequest.on('success', function(response) {\n  console.log(response.data); // or return in your function\n});\nrequest.on('error', function(err) {\n  throw err;\n});\nrequest.send();\n```\n. @ANTGOMEZ \nI apologize.  The function, getP, will return before request.on is able to return the response.\nIf you console log inside the function, do you get the correct response.data you're expecting?. This has been fixed.  Closing this issue.. @pabloDon \nAre you able to get this working with a local file, rather than one that's getting handled through Express?\nYou could also try testing with Rekognition, as that also requires a base64 image.\n```const aws = require('aws-sdk');\nconst rekognition = new aws.Rekognition({region: 'us-west-2'});\nconst fs = require('fs');\nconst bitmap = fs.readFileSync('localFile.jpg');\nconst buffer = new Buffer.from(bitmap, 'base64')\nconst params = {\n  Image: {\n    Bytes: buffer\n  }\n}\nrekognition.detectText(params, function(err, data) {\n  console.log(err,data)\n})\n```\nIf the image is not encoded correctly, Rekognition will raise the same InvalidImageFormatException.. Hello @skazska \nYou need to set the region before constructing the lambda client.\nHere are two ways you can do that:\n\nLoad the entire SDK\n```\nconst AWS = require('aws-sdk')\n\nAWS.config.loadFromPath('config.json')\nconst lambda = new AWS.Lambda({apiVersion: '2015-03-31'})\n```\n\nPull in global AWS namespace, but only load the lambda client:\n```\nconst AWS = require('aws-sdk/global')\nconst Lambda = require('aws-sdk/clients/lambda')\n\nAWS.config.loadFromPath('config.json')\nconst lambda = new Lambda({apiVersion: '2015-03-31'})\n```\nAdditional details can be found here: Creating and Calling Service Objects. @hanginwithdaddo \nIt looks like this issue was meant to be opened on another repository.\nI'm going to close it, but if this is actually an issue with the SDK, let us know and we can re-open.. @sriramHaven \nCan you provide us with a code snippet to reproduce this issue?  Your package.json file would also be helpful.. @sriramHaven \nI put together a quick test using the same versions of node, mocha and source-map-support and didn't encounter any errors when importing the latest aws-sdk.  I ran mocha against transpiled JS as well as against typescript directly (using ts-node).\nDid you try removing zone.js from the equation to see if that changes anything?. Closing for inactivity.. @PinnacleOne \nAre you using the AWS SDK?  From the code you supplied, it look like you're using a third-party package to create the presigned url.. @fyn-dev \nThere's an open feature request to add that capability to the SDK: https://github.com/aws/aws-sdk-js/issues/1456. Hello @PinnacleOne,\nUnfortunately, the SDK doesn't currently support creating presigned urls for use with IoT endpoints.  If you have usage questions for IoT and getting it to work with Third-Party clients for creating presigned urls, you'll need to refer to the AWS IoT Forum for additional help with your use case.. @armorgreg \nWhich version of the aws-sdk are you using?\nCan you provide your package.json file? . Glad to hear it's working!. @Dom-HMH-MTL \nCan you provide any sample code?\nHow are you applying the credentials returned by assumeRole?\n. @Dom-HMH-MTL \nThis page from the Developer Guide has details on Using the Response Object.\nYou can access the originating AWS.Request object by using this syntax:\ns3.getObject({Bucket: 'bucket', Key: 'key'}).on('success', function(response) {\n   console.log(\"Key was\", response.request.params.Key);\n}).send();. @jadbox \nWhich specific commands are you calling that return this error?  Which version of the SDK are you using?. Thanks for following up.  We've reached out to the IoT Service team to understand why this CORS error is being returned.. @jadbox \nCan you confirm credentials are being set properly?  \nThere was a recent related issue being caused by a missing sessionToken: https://github.com/aws/aws-sdk-js/issues/2329. @jadbox \nI'm going to close this issue for now.  If you have time to re-engage, let us know and we'll be happy to re-open.  We appreciate your feedback regarding the difficulty in debugging and will evaluate the best way we can improve that experience in the SDK.. @SnooHD \nThanks for trying the CLI again.  That makes more sense!\nCan you use this syntax to inspect the getBatchItem request?\ns3.getObject({Bucket: 'bucket', Key: 'key'}).on('success', function(response) {\n   console.log(\"Key was\", response.request.params.Key);\n}).send();. @SnooHD \nSorry I wasn't clear.\nI meant to use that syntax, with DynamoDB, so that you could inspect the outbound request.\nLike this:\ndynamo.batchGetItem(params).on('success', function(response) {\n    console.log(response.request)\n  }).send();\nThe response object will have the original request, that way you can inspect what is happening when you are calling the different tables that would account for behavior you're seeing.. @SnooHD \nGlad to hear it's working!\nUnfortunately, I was never able to get the same empty response, and thus wasn't able to dig any deeper into what was happening for your specific example.\nIf you get this behavior again and have any additional details, let us know and we can re-open this issue.. @Shery11 \nWhich versions of the SDK and Ionic are you using?. Thanks for following up.  We'll close this issue.. @jhohlfeld-otto \nI was able to execute your code using the latest version of the SDK and did not encounter that error.\nWhat version of the SDK are you using?. @againksy \nIn your code, what is res?\nI'm not exactly sure what you are trying do with your code example.  Can you add additional details on what are you trying to achieve? . @againksy \nCan you add additional details on what are you trying to achieve?. Closing for inactivity.. Hello @victorbartel,\nThanks for reaching out.  The SDK Team does not maintain the DAX Client, but I've gone ahead and passed along your issue to the DynamoDB team.\nI'm going to close out this issue, as we try to reserve this repository for SDK issues, but we'll pass along any followup from that team.  You can also engage them directly via the AWS Forums.. @victorbartel \nThe DAX Team responded with the suggestion that each findItem function call is opening a new connection:\npublic async findItem(tableName: string,\n                          id: string,\n                          subId?: string): Promise<IOperationResult<GetItemOutput>> {\n        const documentClient = createDocumentClient();  // This line\n        const params = {...\nInstead, a better pattern would be to re-use an existing connection (where possible).\nThis example re-uses the client between calls and will hopefully help speed things up.. @blakewilson \nThanks for reporting this issue!  That is not the desired effect.  The team is taking a look and will update this issue when a fix is in place.. @dppower \nCustomDomainConfig is supported in the SDK as of version 2.301.0 with this change: https://github.com/aws/aws-sdk-js/pull/2209.\nLambda's current supported version of the SDK is 2.290.0.  Details on all the supported versions are documented here.\nYour options are to wait until Lambda updates the SDK, or you could include a later version of the SDK to use in place of the Lambda provided package.\n. @jpike88 \nThanks for submitting this feedback.\nWhich editor/IDE are you using?. @jpike88 \nThanks for following up.\nPrefixing every User definition with AWS is not a change that we will likely make.  At a minimum, it would be a breaking change and would require a major version bump.  Further, I'm not sure it would completely fix your concern.  Depending on how a specific editor/IDE's auto-import works, it could still suggest AWSUser when adding the User typing.\nIn your instance, it looks like you're importing your own package as a 1st party package.  If it were imported as a local package, you might see VSCode give it precedence in auto-import suggestions.  I did not find any documentation to confirm this however.\nYou may want to research if there is an extension for VSCode that would allow you to configure and modify the standard auto-import behavior to list your package first.. Hello @ChenFeldman,\nThis doesn't look like an issue with the SDK, but with Lambda and reqeust.\nI was able to get it working with request-promises-native instead though:\n```\nconst req = require('request-promise-native');\nasync function upload(body) {\n  let params = {\n    Body: body,\n    Key: 'imageFileName.jpg,\n    Bucket: 'bucketToUpload'\n  };\n  await s3.putObject(params, function(err, data) {\n    if(err) { console.log('error');\n    } else { console.log('succes')}\n  });\n}\nexports.handler = async (event) => {\n  await req.get(myUrl)\n  .then((response) => {\n    upload(response)\n  })\n  .catch((err) => {\n    console.log('error: ', err);\n  })\n};\n```. @ChenFeldman \nCan you try uploading something to s3 directly?  Even populating the body with some text will be a good test to see if you can successfully use putObject on the s3 bucket you're specifying.\nOnce you confirm that you're able to use putObject, you can introduce fetching the file using request-promise-native, and then put the two pieces together.. Hello @AntonSmatanik,\nYes, this SDK works with React Native.  Check out the blog post by @chrisradek on React Native Support.\nAWS Amplify also works with React Native.. @AntonSmatanik \nThis SDK allows you to administer IoT Devices, but does not include any functionality to act as a device via MQTT.\nAWS Amplify does include a PubSub module, but I suspect that might not fit your use case as you're wanting to use certificates for authentication.. Hello @jaisrael1 \nThanks for reporting this issue.\nIs AWS_PROFILE being properly loaded in the nodejs script?\nTo make sure I'm understanding this fully, which command is giving you these paths?\n\\Users\\Jake\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\jaisrael.aws\\config. @jaisrael1 \nHave you set AWS_SDK_LOAD_CONFIG?\nThat needs to be set for the shared credentials file to be loaded.\nSee this PR for details: https://github.com/aws/aws-sdk-js/pull/1391#issue-109380362 with additional context here: https://github.com/aws/aws-sdk-js/issues/2197#issuecomment-412942928. You're welcome!  Closing this issue.. @DoodahProductions \nThanks for following up.\nYou're correct that this index.html file does include a pre 3.0.0 version of jQuery, but that index file is only used for the online playground.  The SDK does not call that file, so it's not an issue.  If you are using that index.html file and are concerned about XSS issues, you cold followup with the maintainer of that package.. Hi @DoodahProductions,\nI'm not sure I follow what you mean by \"fix it on our side\".\nI've reached out to the maintainer of jmespath.  Hopefully they'll have a chance to make the change.. @shawnmclean \nWas this issue meant to be opened on the DefinitelyType repository?\n. @shawnmclean \n@types/aws-lambda is maintained by the community in the DefinitelyTyped repository.  Their maintained definitions are not something that can be fixed on the aws-sdk side.\nYou are correct that callback param is optional though.\n. @hemanth-sp \nYou are able to use that same presigned url to perform the upload via Postman?\nIf that's the case, it would indicate that Rails is generating the presigned url properly, but something on the client side in Angular is not working.. @hemanth-sp \nThe body is null for both the Angular app and Postman?  Or is there a difference between the two?\nCan you use Curl to successfully PUT the file?\ncurl -X PUT -T myFile.txt -L \"MyPressignedURL\"\n. Unfortunately, I don't have an AWS authored tutorial on that specifically.\nAre you able to get Angular to successfully PUT a file to another location (one that doesn't require the presigned URL)?\nI would try to get your Angular PUT working and then re-introduce the presigned URL.\nYou may want to ask on Stack Exchange.. Hello @gatsbyz,\nWhat error are you receiving?  From webpack?  Or Lambda?. @gatsbyz \nThat sounds like a zip packaging error.  Are you zipping the directory instead of the contents of the directory?\nYour zip file should have index.js at the top level, not inside of a directory.. I'm having a bit of trouble following what errors you are getting under which scenarios.\nWhen you include aws-sdk (removing new webpack.IgnorePlugin(/aws-sdk/)), everything is working?. Are the directory structures the same whether or not aws-sdk is ignored? (Minus the ignored aws-sdk files).\nIn the first screenshot you posted, there is a build-lambda directory at the top level.  Does that get added when you ignore aws-sdk?. You want to use webpack's Externals, rather than ignore.\nTry adding this to your webpack config:\nexternals: {\n    'aws-sdk': 'aws-sdk'\n  }\nThat will keep aws-sdk from being bundled into index.js, and will instead use lambda-provided aws-sdk instead.. You're welcome!  Glad this helped.. Hello @erothmayer,\nThanks for submitting this request.   There's an open Feature Request to add that functionality to the SDK: https://github.com/aws/aws-sdk-js/issues/1435\nAs this is a duplicate of that issue, I'd like to close this issue.  If you have additional feedback, please join the discussion in the other issue.. @erothmayer \nI apologize for misunderstanding.  I'll re-open and we will take a look.. @erothmayer \nThe SDK does support setting headers:\ncloudWatch.putMetricData(params)\n  .on('build', function(req) { req.httpRequest.headers['Content-Encoding'] = 'gzip'; })\n  .send((err,data) => {\n    console.log(err,data); \n  });\nBut as you pointed out, the validator won't accept a gzip blob.\nI'm marking this as its own Feature Request so that the team can discuss further.. @erothmayer \nCan you give this a try?  This uses onAsync to set the encoding header, and to replace the body with the gzipped after validation.\ncloudWatch.putMetricData(params)\n  .onAsync('build', function(req,done) {\n    req.httpRequest.headers['Content-Encoding'] = 'GZIP';\n    zlib.gzip(req.httpRequest.body, function(err, result) {\n      req.httpRequest.body = result;\n      done();\n    })\n  })\n  .send()\nonAsync is not currently public, but there is now an open PR to make that documented and add types for its use: https://github.com/aws/aws-sdk-js/pull/2299\nYou can pin to a current version of the SDK to avoid any issues for when that gets pulled in, or else wait until we've released it.. This change was merged in.  Going to close out this issue now.. @rix0rrr \nI've opened a PR for this.. This has been implemented. Closing this issue.. Thanks for submitting this request.\nThis is a feature that we're considering for the next major version of the SDK.. Hello @bamapookie \nThanks for submitting this issue.\nDoes the call work with v2.293.0?  How about v2.292.0?. That's the same error you get if you submit an invalid or unknown stack name:\nCF.waitFor('stackCreateComplete', {StackName: 'some unknown stack name'}).promise())\nIn your code, the waitFor is sometimes getting called before createStack call has been completed.\nCan you try something like this?\n```\nconst AWS = require('aws-sdk'),\n  CF = new AWS.CloudFormation({region: 'us-west-2'}),\n  UUID = require('uuid/v4');\nlet stackName = TempStack-${UUID()};\nconsole.log(Stack name: ${stackName});\nconst params = {\n  StackName: stackName,\n  DisableRollback: false,\n  EnableTerminationProtection: false,\n    TemplateBody: AWSTemplateFormatVersion: '2010-09-09'\nResources:\n  SampleResource:\n    Type: AWS::CloudFormation::WaitConditionHandle\n    Metadata:\n      Comment: Dummy resource\n      StackTime: ${new Date().getMilliseconds()}}\nconst createStackPromise = CF.createStack(params).promise()\ncreateStackPromise.then(function(data) {\n  console.log('stack creation started')\n  const waitForPromise = CF.waitFor('stackCreateComplete', {StackName: stackName}).promise();\n  waitForPromise.then(function(data) {\n    console.log('stack creation completed')\n  }).catch(function(err) {\n    console.log(err)\n  })\n}).catch(function(err) {\n  console.log(err)\n})\n```. Does the change I suggested also work for you?. As it's no longer failing, I'm going to close out this issue.  If you see this crop back up in a reliable way, let us know and we'll be happy to re-open.. Thanks for opening this issue.\nThis is a known issue that the S3 Service Team is reviewing.\n. I'm going to close this issue for now.  The Service Team needs to make the change on their end in order to resolve this issue.  You're welcome to engage that team on the AWS Developer Forums if you have additional questions.. @neoacevedo,\nAre you using the SDK to perform the upload?  Which version?. Hello @sashakorman,\nThanks for reporting this issue.\nAre you interested in submitting your fix as a PR?. @lemmikens \nWhere is this error being raised?\nA similar error was encountered here: https://github.com/aws/aws-sdk-js/issues/2069#issuecomment-427941525\nMy response there may be of some help.. Hello @m-athar,\nThanks for opening this issue.\nWhich version of the SDK are you using?  Can you provide any additional details about the environment you're using when you get these errors?. Here are a couple of suggestions to check:\nDo the content-types match?  Does it work if you remove the content-type?\nHas the URL expired in any of these cases?  I'm not sure what you're using for AWS_SIGN_URL_EXPIRATION_TIMEOUT.. Do you get an errors with a longer timeout? . What environment are you using for these requests?  What version of the SDK are you using?. Is there any pattern to when you see the failures vs when the upload succeeds?  Do you get failures for specific files only?  Or file types?  Or uploads at specific times?. @m-athar,\nWhich error did that return?  Could you provide the error and stacktraces you have received?. Which console is this?  Is this from a browser?. Can you try to use the presigned URL to make PUTs using Node instead of the browser?\nAs the presigned URLs do mostly work, this seems to be an issue between the browser and the S3 service.  If your internet connection is inconsistent, or you have high latency, the browser's connection may close, resulting in intermittent errors.. Do you get the same error if you use a node script (and not the browser) to upload the files?\nUsing a node script to upload with a presigned url will help to determine if there is an issue with the browser or now.. Ok.  Let us know what you find.. Hello @Thaina,\nThanks for submitting this request.\nCan you give an example of how would expect this part to work?\n\"calling getOpenIdTokenForDeveloperIdentity with that identityID along with customID but without facebook access token\"\nI'm wanting to make sure we understand what you are trying to accomplish.  An example of how you would use this will hopefully clarify.. Is this how you are expecting that call to look?\nvar params = {\n  IdentityPoolId: 'myIdentityPoolId',\n  Logins: { \n    'myCustomProvider': 'myCustomId'\n  },\n  IdentityId: 'myIdentityId',\n  TokenDuration: 0\n};\ncognitoidentity.getOpenIdTokenForDeveloperIdentity(params, function(err, data) {\n  if (err) console.log(err, err.stack); // an error occurred\n  else     console.log(data);           // successful response\n});. I'm opting to close this issue.  This is a feature request that would need to be implemented at the service level.  If the service were to implement this request, the SDK would get updated accordingly.. Hello @Rishi74744 \nThanks for opening this issue.\nUsing the same code in a script embedded in html, I do not encounter this error.\nBy chance, I did set an endpoint that conflicted with the region configuration, which resulted in the same error you are seeing.  Removing that conflicting endpoint (or adjusting the region), solved the issue.\nYou shouldn't need to configure the endpoint, that will get set based off of the region you have configured.  Can you try removing the endpoint?\n. @Rishi74744 \nThanks for following up to let us know the issue.  The missing sessionToken would have caused issues with the OPTIONs negotiation and the subsequent CORS header.. Hello @RLThomaz,\nThanks for submitting this issue.\nCould you provide an example of the template you are using and the calls you are making with that template?. @RLThomaz \nThanks for submitting this thorough response.\nThe CLI has a greater level of abstraction built into it than the SDK.  For example, the aws deploy command wraps several underlying CloudFormation API calls and there are additional flags to suppress the changset failures you are encountering.\nThe SDK offers the access to the CloudFormation calls directly.  If you have the possibility of submitting creating empty changesets and want to catch those specific errors, you'll need to add logic look for the StatusReason of 'The submitted information didn\\'t contain changes. Submit different information to create a change set.'\nYou're correct to point out the inconsistency between the console and the CloudFormation API.  You're welcome to raise this question with that team directly on the AWS Developer Forums.. @oshklyaruk \nThanks for opening this issue.\nCan you give additional details so we can recreate this issue?. Hello,\nThanks for submitting this issue.\nDo you get the same error if you access each picture individually?  Can you run detectFaces on each of them separately?\nSince you're getting the same error with both the SDK and the CLI, that would point to an issue with the underlying Rekognition service, and not the SDK itself.. Hello @stevewillard,\nRefer to this documentation on Protecting Data Using Server-Side Encryption with AWS KMS\u2013Managed Keys (SSE-KMS) .\nThat documentation states:\n\nAll GET and PUT requests for an object protected by AWS KMS will fail if they are not made via SSL or by using SigV4. \n\nYou won't be able to upload to S3 (a PUT) via an HTTP proxy.  You'd need an HTTPS proxy instead.. @jstewmon Thanks for the rebase!. Hello,\nThanks for opening this issue.\nI'm trying to reproduce this issue, but am not getting the same error.\nAre you able to load credentials from a JSON file?  \nCan you try loadFromPath?. @leonetosoft \nThanks for submitting this issue.\nHow are you creating the objects in S3?. Hello @leonetosoft,\nThanks for following up.\nYou could calculate and set your own MD5 checksum as metadata on the object, then verify that metadata when downloaded.\nIf you set the following param during upload:\nMetadata: {\n   myMD5: myMD5CheckSum\n }\nYou could then get this header back when retrieving the object:\nx-amz-meta-mymd5\n. You can't update the metadata on an S3 Object outside the Console, but you can replace an object with using copyObject.  Use the same source (CopySource) and destination (Bucket/Key) and it will replace the object, allowing you to set new metadata:\n```\nconst params = {\n  CopySource: \"/myBucket/myKey\", \n  Bucket: \"myBucket\",\n  Key: \"myKey\",\n  Metadata: {\n    myMD5: myMD5CheckSum\n  },\n  MetadataDirective: 'REPLACE'\n}\ns3.copyObject(params, function(err,data) {\n  console.log(err,data);\n})\n```\n. I'll close this issue for now.  Let us know if there are any further questions.. @devourment77 \nThanks for submitting this issue.\nI am able to reproduce the issue in the browser, as well as in Node.  We will update you once we've got the right way to get this resolved sorted out.. @devourment77 \nWe're working with the KMS team to get this resolved.\nUnfortunately, when there is an InvalidSignatureException, the KMS service is not returning a Date header on the 400 error.  That Date header is what the SDK uses to correct clock skew and retry for other services.\nAs a work around, you could catch those exceptions, parse the error text, and handle your own retries after setting systemClockOffset.. You can set the systemClockOffset on your KMS instance directly, and not through the global config:\nmyKMSClient.config.systemClockOffset = 1. Yes, I'll update this issue when I have more information about the KMS Team getting this change made.. Thanks for opening this issue.\nWe're getting this fixed.\nWe kept the scripts and dist-tools.  The browser-builder is shipped in dist-tools and there are some users of the scripts.. Closing out this issue as the changes have been merged and this will get fixed with the next release.. If you want awsConfigurationParameters to be used globally, you'll need to update aws.config.  In your example, only a awsConfig is getting updated.\nTry this instead:\n```\n import * as aws from 'aws-sdk';\n ...\n const awsConfigurationParmeters: ConfigurationOptions = {\n    logger,\n    maxRetries: 10,\n    region\n  };\nif (Object.keys(customHttpOptions).length > 0) {\n    awsConfigurationParmeters.httpOptions = customHttpOptions;\n  }\naws.config.update(awsConfigurationParmeters);\n```\nOnce the global config is updated, any clients will use that config:\nconst kmsClient = new aws.KMS();. If you do not want to use the Aws.config.update, you could use the other options documented here:\nSetting Region.. Great!  I'll close this issue for now.  Let us know if you have further questions.. Thanks for opening this issue.\nWe've opened a request with the Mechanical Turk Service Team to resolve whether or not the Reason field is required on this operation.  We will let you know when we get an update and can get this issue resolved (either by updating the documentation, or changing the API model in the SDK).. @mike-feldmeier \nThe Mechanical Turk Service Team confirmed that the reason parameter is required.  They are in the process of updating the documentation.. @chrisfowler \nGlad to see you got this sorted.  Let us know if there's anything else we can help with.. @rohit1018 \nThanks for opening this issue.\nCan you give us an example of the code you are using to get this error?. I am able to successfully use listObjects with versions 2.352.0 and 2.353.0 with Node and the browser.\n```\nconst aws = require('aws-sdk');\nconst s3 = new aws.S3();\nconst params = {\n  Bucket: 'myBucket',\n  Prefix: 'myPrefix' \n}\nmyPromise = s3.listObjects(params).promise()\nmyPromise.then(function(data) {\n  console.log(data)\n}).catch(function(err) {\n  console.log(err)\n})\n```\nAre there any other parts of your code that could be causing issues?\nWhich browser and version are you using?\n. Which version of Angular are you using?. @rohit1018 and @Temkit \n@AllanFly120's change to fix this issue has been merged in.  This will get fixed with the next release.  Thanks for bringing this to our attention.. :shipit: . Wording update looks good. :ship:. @portacha \nThe current version of the SDK is 2.354.0\nCan you try using this instead?\n\n. @jstewmon is correct.  The TranscribeService does not support CORS (see metadata.json), which is why it is not included in the default browser package.. @portacha \nYes, you will have to use the TranscribeService outside of the browser to get this to work.  You could use Node and do this server-side, or you could also use Lambda to make those calls instead.. @SodaGremlin \nThe Transcribe Service can best answer if/when CORS support is planned.  You can engage the Transcribe Service through the AWS Developer Forums to provide the feedback that you would like to see that feature.. @hongaar \nThanks for opening this issue.\nBuffer() is required for compatibility support, but this is something we're looking to address in the next major version of the SDK.\nSince you're using a newer version Node, this is a good solution to use a different implementation.. Closing this issue.  This has been addressed on V3 of the SDK.. @Goal54 \nCan you share how you're setting tmpKey?. @Goal54 \nIf you replace tmpKey with a string for testing, does it work as expected?\nWhat does typeof(tmpKey) return?\n. How are you handling the credentials to make this call?  The trace you've provided includes Request.VALIDATE_CREDENTIALS.\nAre those credentials being set in params somewhere?. @Goal54 \nYour function to get tmpKey is async, so it has not been resolved and is being evaluated as a Promise instead a string.\nCan you try removing async on that function?. \ud83d\udea2 . @truongluong1314520 \nHave you looked at this guide? AWS Mobile React-Native Starter\nThere's a specific section on authentication that would be helpful.. @pacopicon \nHow are you confirming that the credentials are valid?  Can you successfully do something simple like a headObject call on the object you are attempting to create a signed URL for.\nPer the documentation, you must use static or previously resolved credentials when using getSignedUrl.. @koladilip \nCan you give us an example of the code you are using?. @koladilip \nThat is an error returned directly from the CloudWatchEvents Service.\nHave you followed up with the Service Team?  They would be better suited to answer questions regarding RunCommandParameters validation.. @koladilip \nYes, the SDK is officially supported by AWS.\nGitHub issues on this repository are meant to be for tracking issues with the working of the SDK itself.\nThe error you are receiving is being returned from the CloudWatchEvents service.  As it is not an error raised by the SDK, I do not have reason to believe this is the SDK not working correctly, but is something that needs resolved with the CloudWatchEvents service.\nIf you have an AWS Support account, you can open a case through your AWS Console, or you can engage the CloudWatch team on their AWS Developer Forum.. @andres-lowrie \nThanks for submitting this feature request.. @chrisradek \nThanks for chiming in!  I've added the 'needs-major-version' tag on this Feature Request.  As @chrisradek noted, this has already been addressed in v3.. Opting to close this issue.  This change will not be made on V2 of the SDK.. @truongluong1314520 \nAre you having a specific issue with the SDK?\nFrom the code you provided, I don't see where you are using the SDK.. @truongluong1314520 \nI'm not seeing anything in your usage of the SDK to indicate the SDK is not working as expected.\nHave you double-checked your credentials and bucket configuration?\n. @truongluong1314520 \ncreatePresignedPost returns a pre-signed POST policy to support uploading to S3 directly from an HTML form.\ngetSignedUrl only returns a URL which can be used for getObject or putObject operations.\nYou're correct.  If you need to set an ACL, you'll need to use a pre-signed POST.\nIf you have something specific in the SDK that you think is not working, let us know.  If you have general usage questions, you will find better resources on Stack Exchange or the AWS Developer Forums.  These GitHub issues are reserved for resolving issues with the operation of the SDK itself.. @hemanth-sp \nThis GitHub repository is a place for resolving issues with the JavaScript SDK.\nIf you need general guidance on using AWS, you can refer to our AWS Developer Forums.\nAlso refer to the Cognito Developer Resources.\nIf you have a specific issue where you think the SDK is not working, let us know and we can re-open this issue.. Thanks for opening this issue.\nPR to fix in:\nhttps://github.com/aws/aws-sdk-js/pull/2365. Fix for https://github.com/aws/aws-sdk-js/issues/2364. :ship:. @simonmit \nThanks for opening this issue.\nDocumentClient is intended to give you an unmarshalled response, allowing you to use JavaScript types in place of the DynamoDB AttributeValues.\nI want to be sure I'm understanding the issue you have opened.  Is the issue that the response is not what you are expecting?  Or that it DocumentClient is returning responses that are as expected, but there is confusion that they are still called an AttributeValue?. @simonmit \nThanks for following up.  That's helpful to understand regarding the VS Code behavior.. @marshalld139 \nCan you give an example of the SDK call you're making?  And the CLI command that is also resulting in the error?. @marshalld139 \nThanks for replying.\nWhen switching to the S3 Website Endpoint, you're actually switching to CustomOrigin.\nIf you make the change in the Console, then inspect the response from getDistributionConfig, you'll see that S3OriginConfig has been replaced with CustomOriginConfig.\nEven though it's actually an S3 Bucket, using the Website Endpoint requires configuration via a CustomOriginConfig.. Sounds good.  Let us know if that gets it working, or if you have followup questions.. @oliviertassinari \nHow are you setting up sqs?\nIs that the same client?  Or are you re-instantiating that between calls?. @oliviertassinari \nThanks for following up with this detail.\nDoes enabling keepAlive improve the performance?\nthis.sqs = new AWS.SQS({\n  apiVersion: '2012-11-05',\n  httpOptions: {\n    agent: new http.Agent({\n      keepAlive: true\n    })\n  },\n  params: {\n    MaxNumberOfMessages: 10,\n    QueueUrl,\n    WaitTimeSeconds: 5, // Long polling\n  },\n  region: config.get('aws.region'),\n}). @oliviertassinari \nYes, it should be https.\nHere's a simplified example:\n```\nconst AWS = require('aws-sdk');\nconst https = require('https');\nconst sqs = new AWS.SQS({\n  region: 'us-west-2',\n  httpOptions: {\n    agent: new https.Agent({\n      keepAlive: true\n    })\n  }\n})\nsqs.listQueues({}, (err,data) => {\n  console.log(data)\n})\n```. @oliviertassinari \nI should've asked earlier, but are you using Node?  Or by 'worker' did you mean web worker?\nWhat version of Node?  What version of the SDK?  Any other environmental details that would help for digging into profiling this?. @oliviertassinari,\nThere were some recent changes to the credential provider that may help to improve the performance you're seeing, depending on what kind of credentials you are using.\nWould you have time to re-test using the latest version of the SDK?. @rainnaren \nPlease provide a full example of the code that will reproduce this issue, and an explanation of why you believe there is an issue with the operation of the SDK.. @rainnaren \nThanks for following up.\nPlease refer to this comment: https://github.com/aws/aws-sdk-js/issues/1247#issuecomment-264285589\nSigners.V4 is not part of the public API.\nThere are Third-Party libraries available that can be useful for doing this signing.\n. @tverney \nThanks for opening this issue.  We need to review how the SDK is handling this encoding.\nFor now, doing NFD normalization on your keys will give you the right signed URL that matches S3.\n```\nconst params = {\n  Bucket: 'myBucket',\n  Key: 'Let\u00ed'.normalize('NFD')\n}\ns3.getSignedUrl('getObject', params, (err,data) => {\n  console.log(data) // https://myBucket.s3.us-west-2.amazonaws.com/Leti%CC%81?AWSAccessKeyId...\n})\n```. @pietrop \nRefer to the TranscribeService documentation.\nThe MediaFileUri must be an S3 location and must be in the same region as the API endpoint that you are calling.\nInput filetypes are: mp3 | mp4 | wav | flac\nYou can upload the input file to S3 with a multi-part form and then call the TranscribeService on that uploaded file.. @hagiosofori \nThanks for opening this issue.\nCould you provide an example of the AppSync query you are performing?\nAre you supplying your own version of the SDK in Lambda?  What version?  What Lambda runtime are you using?. @hagiosofori \nRefer to @chrisradek's comment on an earlier issue: https://github.com/aws/aws-sdk-js/issues/1247#issuecomment-264285589\nAlso, check out SignatureV4 in v3 of the SDK.  v3 is in preview, but would be a good solution once it's ready for production use.. I'll close this issue for now.  If you have a followup, let us know and we can reopen. . @asyba \nThanks for opening this issue.\nDo you have a default region configured?. @asyba \nWhich region is that?. Can you try setting the region to 'us-east-1' explicitly?\nvar rdsDataService = new AWS.RDSDataService({apiVersion: '2018-08-01', region: 'us-east-1'});\n. Is your RDS a cluster?  Or a single instance?\nThere is a similar issue here for a customer receiving the same error when using an instance ARN: https://github.com/aws/aws-sdk-ruby/issues/1922. @asyba \nRegarding your feature request, the region can be configured a number of ways, including from AWS_REGION environmental variable or from a shared config file:\nSetting the AWS Region. @svenmilewski \nRefer to Lambda Execution Environment and Available Libraries.\nLambda currently bundles 2.290.0, which does not have the RDSDataService included.\nNPM has the latest version of the SDK.\nYou could manually bundle a current version of the SDK into your Lambda deployment package.\n. @svenmilewski \nCurrently, there is not a set schedule.  \nPer Lambda's Best Practices, the SDKs are updated periodically.\nSupplying a specific version of the SDK with your deployment package gives you full control over your dependencies.. @asyba \nThe Service Team has indicated that only Aurora Clusters and Serverless Aurora are supported at this time.. Please refer to this documentation:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html\nNote that the Data API is in beta for Aurora Serverless and is subject to change.. @rix0rrr \nAssumeRole does not have a region option.\nYou can however set the endpoint on STS.  If that region has been disabled (see Activating and Deactivating AWS STS in an AWS Region ), you'd get the RegionDisabled error.. Setting the endpoint on STS:\n```\nconst sts = new AWS.STS({\n  region: 'eu-west-1',\n  endpoint: 'sts.eu-west-1.amazonaws.com'\n});\nconst params = {\n  ExternalId: '123ABC',\n  RoleArn: 'arn:aws:iam::myAccountID:role/myRole',\n  RoleSessionName: 'Bob'\n}\nsts.assumeRole(params, (err,data) => {\n  console.log(err,data)\n})\n```. Proposed change for https://github.com/aws/aws-sdk-js/issues/2379. See https://github.com/aws/aws-sdk-js/issues/2379 for details.  The Service advises using CloudWatch for monitoring jobs.. Thanks for submitting this PR.  We will update you once we've had a chance to review.. @yousharizvi \nWe've opened a request with the MediaConvert Team to review.  I'll update the PR, https://github.com/aws/aws-sdk-js/pull/2378, as there is additional feedback to share.. @yousharizvi \nThe preferred way to monitor MediaConvert Jobs is via CloudWatch.\nSee PART 2: Setup notifications when MediaConvert jobs finish for an example and details on setting up CloudWatch and SNS to receive notifications.. @edzillion,\nThanks for submitting this feature request.\nThis is a limitation of the Rekognition service: see \"Q: Can I use Amazon Rekognition with images stored in an Amazon S3 bucket in another region?\" on the Rekognition FAQs.\nIf the Rekognition service adds support for this feature, the SDK will be updated to make that available.\nYou're welcome to engage the Rekognition service on the AWS Developer Forums.\n. I'm going to close out this issue, since it isn't actionable by the SDK without that change on the Service.  For anyone else that wants to express interest in this feature, @edzillion's forum post would be a good place to do that.. @Yijx \nAre you wanting to pause the upload?\nAbort() will clean up the multipart upload, which is why it cannot be restarted with send().  . leavePartsOnError: true is the correct way to be able to resume a managed upload.. I'll close this issue for now.  We're happy to re-open if you have additional questions.. @govindrai \nThanks for opening this issue.\nAre you referring to the getPolicy for the Lambda API specifically?\nThe Lambda API Reference for getPolicy calls out that a ResourceNotFoundException will be returned when the resource (for example, a Lambda function or access policy statement) specified in the request does not exist.\nIs there another getPolicy operation that is unclear?. @govindrai \nThanks for following up.  I'll tag this as a feature request.\nFor most services, the SDKs are programmatically generated from API resource models.  The SDK documentation is auto-generated at the same time from those resource models.  I can see how it would be useful if we were able to incorporate the documented errors within the SDK documentation.\nFor now, I would suggest checking the API Reference pages if you encounter any behavior in the that is not making sense.\n. @viamuli \ngetQueueAttributes returns a number of attributes about the queue, including counts of ApproximateNumberOfMessages, ApproximateNumberOfMessagesDelayed and ApproximateNumberOfMessagesNotVisible.\nDo those attributes cover what you're looking for?\n. @jsphweid \nThanks for opening this issue. \nThis is the expected behavior for v2.  Because operations are generated dynamically, it is not possible to destructure a single operation.\nHowever, this is possible in v3.  There's an open PR to update the README with an example of this: https://github.com/aws/aws-sdk-js-v3/pull/121/files\nWith v3, you can do this:\nconst { getObject } = require('@aws/sdk-s3-node/commands/GetObjectCommand')\nLet us know if you have further questions and we can re-open.. @SocMinarch \nThanks for opening this issue.\nThe raw response body will look like the documented XML.  You could log that by doing the following:\n```\nvar req = cw.getMetricWidgetImage(params)\nreq.on('success', (resp) => {\n  console.log(resp.httpResponse.body.toString());\n});\nreq.send();\n```\nThe SDK un-marshalls the XML into the response object.\n. Can you provide the example of how you are using SQS?  Which version of the SDK are you using?\nDoing the following will console.log the original request header set as 'bar':\n```\nconst sqs = new AWS.SQS({region: 'us-west-2', http_wire_trace: true})\nconst params = {\n  QueueName: 'foo',\n}\nvar req = sqs.getQueueUrl(params);\nreq.on('build', () => {\n  req.httpRequest.headers['Custom-Header'] = 'bar';\n});\nreq.on('success', (resp) => {\n  console.log(resp.request.httpRequest.headers)\n})\nreq.send()\n```. @marceloneias \nYou're very welcome.   Closing out this issue.. :ship:. @anwarhamr \nHow are you confirming what params looks like at the time of sending?. @sliem \nThanks for opening this issue.  We appreciate the clear and thorough steps to reproduce the issue.\nWe'll look at getting this resolved so tags behave the same between small and large files using the upload operation.\n. @sliem \nA change for this has been merged.  It will go out with the next release.. @jmm \nThanks for submitting this issue.\nI'm able to execute the invoke operation using the alias successfully.  The error, returned by the Lambda service, indicates an issue with the permissions for the User invoking that function.\nDid you verify that the user has permissions to invoke that Lambda function?\nI'd suggest reviewing the Using Identity-Based Policies (IAM Policies) for AWS Lambda documentation.. @jmm \nGetting this working is a bit tricky:\nIf you use this as the resource in your policy:\narn:aws:lambda:...:...:function:someFunction\nYou can use the qualifier to invoke specific versions or aliases on that function (or leave out the qualifier, and invoke $LATEST).\nIf you need to restrict permissions on and only want to allow your policy permissions to a specific alias or version, the resource needs to be fully qualified:\narn:aws:lambda:...:...:function:someFunction:myAlias\nor\narn:aws:lambda:...:...:function:someFunction:myVersion\nWhen using the fully qualified resource in your policy, you cannot use the qualifier and must invoke that function with the matching fully qualified ARN:\n```\nconst invokeParams = {\n  FunctionName: 'arn:aws:lambda:...:...:function:someFunction:myAlias'\n}\nlambda.invoke(invokeParams, (err,data) => {\n  console.log(err,data)\n})\n```\nThis page has details on Versioning, Aliases, and Resource Policies.. I've passed along this feedback for the Lambda Service Team to review.. @jmm \nYes, the majority of the documentation in the SDK is generated from the API models.\nThe Service Team is reviewing and would update their model, which would update the documentation in each of the SDKs.\nYou can build the documentation locally (from source) via the docs:api rake task:\nbundle exec rake docs:api. We'll update this thread with that progress.  It's tracked internally, but this is the best place for you to stay updated.. @varnitgoyal95 \nThanks for opening this issue.\nYou are correct, DynamoDB API was recently updated with added support for transactions and those operations were made available in the SDK in version 2.365.0.\nThe DynamoDB DocumentClient has not been updated to use transactions with that style of interaction with DynamoDB (namely, abstracting away the notion of attribute values).\nMarking this issue as a feature request.. @varnitgoyal95 \nA change to support this has been merged.  It will be available with the next release of the SDK.. @MartinMasek \nThe version of the SDK provided by Lambda does lag behind the latest version.\nThe best practice is to bundle your own version of the SDK anyways.  That will allow you control over the upgrade cycle for all of your dependencies.. @AndrewBarba \nThanks for opening this issue.\nWhat do you mean by 'startup'?  Can you provide an example of the code you are running and include which environment is executing the code?. @AndrewBarba and @mrapczynski \nWhich registry are NPM and Yarn using?  Are you both using Serverless?. @AndrewBarba \nGlad to hear you got it working!\nGoing to close out this issue, but happy to re-open if anything further is needed.. :shipit: . @uptownaravi \nThanks for opening this issue.\nThe AccessDenied error is being returned by S3.  It would be up to the Service Team to make the error more detailed.\nI would recommend posting on the AWS Developer Forums.  I can also pass along that feedback internally.\nClosing this issue as it is not an issue with the SDK itself.. @muzab \nThe formatting of your code example is a bit difficult to decipher.\nWhere is s3control initialized?\nAre you using trying to use the credentials in ~/.aws/credentials?. @muzab \nThe formatting of your code example is a bit difficult to decipher.\nWhere is s3control initialized?\nAre you using trying to use the credentials in ~/.aws/credentials?. refactor looks good! Great to get https://github.com/aws/aws-sdk-js/issues/2392 supported.\n:ship:. refactor looks good! Great to get https://github.com/aws/aws-sdk-js/issues/2392 supported.\n:ship:. @abbottdev \nComprehendMedical was added to the SDK as of version v2.364.0, so that shouldn't be the issue.\nCan you share details on how you're using webpack?. @abbottdev \nComprehendMedical was added to the SDK as of version v2.364.0, so that shouldn't be the issue.\nCan you share details on how you're using webpack?. @abbottdev \nCould you publish the dummy project in a new repo?. @abbottdev \nCould you publish the dummy project in a new repo?. Did you include your own version of the SDK with that Lambda function?  Or use the Lambda-provided SDK?\nThe Lambda-provided version is currently at 2.290.0.  See this Lambda documentation page for details.\nWe'll take a look at the TS repository.. Did you include your own version of the SDK with that Lambda function?  Or use the Lambda-provided SDK?\nThe Lambda-provided version is currently at 2.290.0.  See this Lambda documentation page for details.\nWe'll take a look at the TS repository.. @abbottdev \nWe've made an internal request for the ComprehendMedical to support CORS which would enable this to be used from the browser (and included by the SDK in the default browser build).\nMarking this as a feature-request.. @abbottdev \nWe've made an internal request for the ComprehendMedical to support CORS which would enable this to be used from the browser (and included by the SDK in the default browser build).\nMarking this as a feature-request.. A change for this has been merged.  It will be available with the next release of the SDK.. A change for this has been merged.  It will be available with the next release of the SDK.. change for https://github.com/aws/aws-sdk-js/issues/2390. change for https://github.com/aws/aws-sdk-js/issues/2390. @AllanFly120 \nYes.  I tested against S3 and you can set tags like this now on either a small file, or one large enough to do a multipart-upload:\ntags = [\n    {Key: \"stringTag\", Value: \"fooString\"}, \n    {Key: \"numberTag\", Value: 100}\n];. @AllanFly120 \nYes.  I tested against S3 and you can set tags like this now on either a small file, or one large enough to do a multipart-upload:\ntags = [\n    {Key: \"stringTag\", Value: \"fooString\"}, \n    {Key: \"numberTag\", Value: 100}\n];. @nehagupta-itbhu \nCan you help me understand why you think the requests are being made sequentially?. @nehagupta-itbhu \nCan you help me understand why you think the requests are being made sequentially?. @nehagupta-itbhu \nRefer to the Lambda documentation on Managing Concurrency.\nRather than an individual function that makes multiple SQS calls, a better model would be to have multiple invocations of a Lambda function that makes a single SQS call.\nIf you have additional questions, post on the AWS Lambda Developer Forum.  They are better suited to help answer Lambda specific questions and provide best practices beyond the documentation.. @nehagupta-itbhu \nRefer to the Lambda documentation on Managing Concurrency.\nRather than an individual function that makes multiple SQS calls, a better model would be to have multiple invocations of a Lambda function that makes a single SQS call.\nIf you have additional questions, post on the AWS Lambda Developer Forum.  They are better suited to help answer Lambda specific questions and provide best practices beyond the documentation.. @knalx \nThanks for opening this issue.\nI'm not able to get any malformed responses from the Step Functions service in trying to reproduce this issue.\nDo you get this error every time?  Or was it a single instance?\nAre there any specifics of when this error occurs that could help narrow the issue?. @knalx \nThanks for opening this issue.\nI'm not able to get any malformed responses from the Step Functions service in trying to reproduce this issue.\nDo you get this error every time?  Or was it a single instance?\nAre there any specifics of when this error occurs that could help narrow the issue?. Which 'symbol number 17268' are you referring to?. Which 'symbol number 17268' are you referring to?. @knalx \nAre there any additional details you can provide that would help with a reliable reproduction of this error?  \nIf there is a malformed response being returned by the service, it would help if we could figure out exactly when that happens.. @knalx \nAre there any additional details you can provide that would help with a reliable reproduction of this error?  \nIf there is a malformed response being returned by the service, it would help if we could figure out exactly when that happens.. @Strydom \nThanks for opening this issue.\nDo you have a then or catch on the promise?\nRefer to this page on Using JavaScript Promises.. @Strydom \nThanks for opening this issue.\nDo you have a then or catch on the promise?\nRefer to this page on Using JavaScript Promises.. @Strydom \nIf you run this with expired credentials:\n```\nconst resp = dynamo.listTables().promise();\nconsole.log(resp);  /// logged as Promise {  }\n```\nYou should eventually receive this error UnhandledPromiseRejectionWarning: ExpiredTokenException: The security token included in the request is expired\nAre you using await within another async function?. @Strydom \nIf you run this with expired credentials:\n```\nconst resp = dynamo.listTables().promise();\nconsole.log(resp);  /// logged as Promise {  }\n```\nYou should eventually receive this error UnhandledPromiseRejectionWarning: ExpiredTokenException: The security token included in the request is expired\nAre you using await within another async function?. @Strydom \nUsing Try/Catch outside of Async/Await will catch this issue earlier, rather than having to wait for the UnhandledPromiseRejectionWarning.. @Strydom \nUsing Try/Catch outside of Async/Await will catch this issue earlier, rather than having to wait for the UnhandledPromiseRejectionWarning.. @dominiceden \nThanks for opening this feature request.   We're reviewing the best way to support this in the SDK.. @dominiceden \nThanks for opening this feature request.   We're reviewing the best way to support this in the SDK.. StartStreamTranscription requires HTTP2, which is not available in the SDK currently.\nHTTP2 is likely going to be made available in the Version 3 of the SDK.  I've tagged this issue as 'needs-major-version' to reflect that direction for this request.\nIf you haven't already, give V3 a look.  We'd love any feedback while it's in Developer Preview.. StartStreamTranscription requires HTTP2, which is not available in the SDK currently.\nHTTP2 is likely going to be made available in the Version 3 of the SDK.  I've tagged this issue as 'needs-major-version' to reflect that direction for this request.\nIf you haven't already, give V3 a look.  We'd love any feedback while it's in Developer Preview.. @wedwin53 \nCan you provide a code samples and details on how you are getting this error?\nWhich version of the SDK?  What environment?. @wedwin53 \nCan you provide a code samples and details on how you are getting this error?\nWhich version of the SDK?  What environment?. @wedwin53 \nHow did you include the SDK in your React app?\nThe default browser build does not include ComprehendMedical because that service does offer CORS Support.. @wedwin53 \nHow did you include the SDK in your React app?\nThe default browser build does not include ComprehendMedical because that service does offer CORS Support.. @wedwin53 \nIs this a react-native app?  Or a normal react app for use in the browser?. @wedwin53 \nIs this a react-native app?  Or a normal react app for use in the browser?. Comprehend does work with CORS, we've enabled that with this change: https://github.com/aws/aws-sdk-js/pull/2427.  You'll be able to use it with the standard build once that is released.\nComprehendMedical does not currently work in the browser.  You should be getting a CORS error.. Comprehend does work with CORS, we've enabled that with this change: https://github.com/aws/aws-sdk-js/pull/2427.  You'll be able to use it with the standard build once that is released.\nComprehendMedical does not currently work in the browser.  You should be getting a CORS error.. Closing this as a duplicate of this request: https://github.com/aws/aws-sdk-js/issues/2408. Closing this as a duplicate of this request: https://github.com/aws/aws-sdk-js/issues/2408. @batrudinych \nThanks for opening this issue.  Appreciate the clear repro.\nWe're working to get this fixed and will update this issue when we get it resolved.. @batrudinych \nThanks for opening this issue.  Appreciate the clear repro.\nWe're working to get this fixed and will update this issue when we get it resolved.. A fix for this was merged.  Closing this issue.. A fix for this was merged.  Closing this issue.. Hello @batrudinych \nThanks for opening this request.\nOn the Object Key and Metadata documentation page, it's specified that:\n\nYou can set object metadata at the time you upload it. After you upload the object, you cannot modify object metadata. The only way to modify object metadata is to make a copy of the object and set the metadata. \n\nYou can use the copyObject operation in the SDK and do something like this:\n```\nconst params = {\n  Bucket: 'myBucket',\n  Key: 'myObjectKey',\n  CopySource: '/myBucket/myObjectKey',\n  MetadataDirective: 'REPLACE',\n  Metadata: {\n    'Cache-Control': 'max-age=86400'\n  }\n}\ns3.copyObject(params, (err,data) => {\n  console.log(err,data)\n})\n```. Hello @batrudinych \nThanks for opening this request.\nOn the Object Key and Metadata documentation page, it's specified that:\n\nYou can set object metadata at the time you upload it. After you upload the object, you cannot modify object metadata. The only way to modify object metadata is to make a copy of the object and set the metadata. \n\nYou can use the copyObject operation in the SDK and do something like this:\n```\nconst params = {\n  Bucket: 'myBucket',\n  Key: 'myObjectKey',\n  CopySource: '/myBucket/myObjectKey',\n  MetadataDirective: 'REPLACE',\n  Metadata: {\n    'Cache-Control': 'max-age=86400'\n  }\n}\ns3.copyObject(params, (err,data) => {\n  console.log(err,data)\n})\n```. @batrudinych \nThe S3 Service does not provide an API which allows metadata to be modified on an existing object.\nThe console sometimes provides capabilities that are not directly possible with an API call.\nYou're welcome to post on the S3 AWS Developer Forum requesting they add this API operation.  If S3 makes that change, it will get added to the SDK.. @batrudinych \nThe S3 Service does not provide an API which allows metadata to be modified on an existing object.\nThe console sometimes provides capabilities that are not directly possible with an API call.\nYou're welcome to post on the S3 AWS Developer Forum requesting they add this API operation.  If S3 makes that change, it will get added to the SDK.. @batrudinych \nThanks for posting! \nAs I mentioned before, this API addition would be up to the S3 Team.  I'll opt to close this issue here.  Anyone else with feedback is welcome to respond to your post.. @batrudinych \nThanks for posting! \nAs I mentioned before, this API addition would be up to the S3 Team.  I'll opt to close this issue here.  Anyone else with feedback is welcome to respond to your post.. @lenawal \nDoes your instance meet the EC2 Hibernation Prerequisites?. @lenawal \nDoes your instance meet the EC2 Hibernation Prerequisites?. @lenawal \nWhich version of the SDK are you running?\nHibernation was made available in the SDK on the EC2 StartInstances beginning in 2.366.0.  See Changelog.\nUpgrading to a version of the SDK 2.366.0 or newer should resolve this issue.. @lenawal \nLambda is still using version 2.290.0.  Unfortunately, Lambda doesn't pull in the latest version automatically.\nSee this page: Lambda Execution Environment and Available Libraries\nYou'll need to bundle the SDK into your Lambda to use HibernationOptions.  Are you already doing that?  Or relying on the SDK that's included with Lambda?. \ud83d\udea2 . Have you confirmed the credentials on this.codepipeline?. @j \nWhich MessageAttributeValue are you referring to?  The one specified for SNS or SQS?. @byF \nThanks for opening this issue.\nThat error is returned by the ECS service and not the SDK itself.  Have you raised the issue with the ECS Service Team?\nHave you referred to this documentation? Tagging Your Amazon EC2 Resources. The Amazon ECS Developer Forum is the best place to post this kind of question.\nHave you tried using the AWS CLI run-task command?. @byF \nAppreciate you following up on this issue!. @j1mb0jay and @Gr1mR3aver,\nThe Amazon ECS Developer Forum is the best place to engage the ECS team.  \nHave either one of you posted a question there?. This issue is being worked on by the ECS and EC2 Service Teams.  Once it is resolved, I will update this issue.. @byF \nThe service team has indicated that this issue has been resolved.. @ANTGOMEZ \nCan you provide an example of your code?. @ANTGOMEZ \nWhich version of the SDK are you using?\nThat should work if you're using a version of the SDK later than v2.357.0 when retention periods for manual snapshots was introduced.. @ANTGOMEZ \nLambda currently uses 2.290.0.\nSee this page for the Current Supported Versions.\nIf you need to use a later version of the SDK, you will need to include that with your deployment package.. @sm2017 \nI assume you're using Node, but wanted to check that you're not trying to do this in a browser.\nIs that correct?  Which version of the SDK are you using?. @sm2017,\nI'm able to do the following and the error is caught successfully:\n```\nconst AWS = require('aws-sdk');\nconst s3 = new AWS.S3({region: myRegion});\nconst { Readable } = require('stream');\nconst readStream = new Readable();\nasync function doUpload(body) {\n  try {\n    const params = {\n      Bucket: 'myBucket',\n      Key: 'myKey',\n      Body: body,\n    }\n  await s3.upload(params).promise()\n  } catch (err) {\n    console.log('err: ', err);\n  }\n}\ndoUpload(readStream)\nreadStream.push('some text');\nreadStream.emit('error', new Error('foo error'))\nreadStream.push(null);\n```\nreturns:\nerr:  Error: foo error\nPerhaps something else is going on outside the SDK?. @sm2017 \nIs the error being emitted properly?  That would be the next place to check.. @prismhr-alexandraz \nFor 1.), does DefaultSenderId get set properly for non-Russian phone numbers?\nFor 2.)  because this is not working in the console either, it would point to an issue with the SNS Service and not the SDK itself.  Please followup with the SNS Service Team on the AWS Developer Forums.. This issue was resolved in https://github.com/aws/aws-sdk-js/pull/2451.\nSee https://github.com/aws/aws-sdk-js/issues/2455, which has also been merged and is pending release.. @huynhtuanh,\nThanks for opening this issue!\nKinesisVideoMedia was not marked as supporting CORS.\nThere's now a pending change to get that enabled.  Once it is enabled, that client will get included in the default browser distribution.\nFix in https://github.com/aws/aws-sdk-js/pull/2443. This has been fixed.  KinesisVideoMedia will be included in the browser distribution by default starting with the next release.. @batrudinych \nThanks for the feedback.  I've marked this issue as a documentation cleanup item.. Fix for https://github.com/aws/aws-sdk-js/issues/2441. Fix for https://github.com/aws/aws-sdk-js/issues/2441. @NamelessHH \nThanks for opening this issue.  There appears to be an underlying issue with the S3 Service that is being investigated.  This error is generated and returned by the S3 service, not the SDK.\nI will update this issue once there is additional information from the S3 Service Team.\n. @NamelessHH \nThanks for opening this issue.  There appears to be an underlying issue with the S3 Service that is being investigated.  This error is generated and returned by the S3 service, not the SDK.\nI will update this issue once there is additional information from the S3 Service Team.\n. @NamelessHH \nI'm awaiting on a public notice.\nThis S3 issue only impacted SigV2.  \nRefer to this S3 notification regarding SigV2 deprecation as it's something that you should be aware of for 2019: Announcement: AWS Signature Version 4 to replace AWS Signature Version 2 for signing S3 API requests .. @NamelessHH \nI'm awaiting on a public notice.\nThis S3 issue only impacted SigV2.  \nRefer to this S3 notification regarding SigV2 deprecation as it's something that you should be aware of for 2019: Announcement: AWS Signature Version 4 to replace AWS Signature Version 2 for signing S3 API requests .. @NamelessHH \nI don't have any further information to add.  You could raise the question on the S3 Developer Forum.. @NamelessHH \nI don't have any further information to add.  You could raise the question on the S3 Developer Forum.. @richardsengers \nDoes including --source-maps cause any differences in the headers being sent on that request?\nI would start with a comparison between those requests with and without source maps.. @richardsengers \nDoes including --source-maps cause any differences in the headers being sent on that request?\nI would start with a comparison between those requests with and without source maps.. @richardsengers \nDid you have a chance to revisit with a new app?. @richardsengers \nDid you have a chance to revisit with a new app?. @Agorn \nThanks for opening this issue.\nI'll take a look and see if there is a workaround for using that Yard tag.\nDoes google-clouser-compiler offer a configuration option for that could make this work as-is?. @Agorn \nThanks for opening this issue.\nI'll take a look and see if there is a workaround for using that Yard tag.\nDoes google-clouser-compiler offer a configuration option for that could make this work as-is?. Fixing in https://github.com/aws/aws-sdk-js/pull/2452. Fixing in https://github.com/aws/aws-sdk-js/pull/2452. Fix has been merged. Closing out this issue.. Fix has been merged. Closing out this issue.. fix for https://github.com/aws/aws-sdk-js/issues/2450. fix for https://github.com/aws/aws-sdk-js/issues/2450. Looks good! \ud83d\udea2 . Looks good! \ud83d\udea2 . @holtc \nThanks for opening this issue.\nAre you referring to the CognitoUserPoolTriggerEvent.request on the AWS Lambda on DefinitelyTyped package?. @holtc \nThanks for opening this issue.\nAre you referring to the CognitoUserPoolTriggerEvent.request on the AWS Lambda on DefinitelyTyped package?. @holtc \nThat is a different repository.  You'll need to open an issue or PR there instead. . @holtc \nThat is a different repository.  You'll need to open an issue or PR there instead. . @andrewryan1906 \nRefer to the getSignedUrl documentation.\nIf calling getSignedUrl synchronously (like your example does), you need to have previously resolved credentials.  The other option is to use a callback so that credentials can be resolved before the url is generated.. @andrewryan1906 \nRefer to the getSignedUrl documentation.\nIf calling getSignedUrl synchronously (like your example does), you need to have previously resolved credentials.  The other option is to use a callback so that credentials can be resolved before the url is generated.. 2.383.0 has been released with this issue resolved.. 2.383.0 has been released with this issue resolved.. :ship:  Good to have a test for this specifically.. :ship:  Good to have a test for this specifically.. @theneverstill \ngetFile was added in 2.324.0, which is why it's missing from that CodeBuild image.\nI do not have any further information regarding when that CodeBuild image will be upgraded to use a more recent version of the SDK.\nThe CodeBuild team may be able to provide additional information via the AWS Developer Forums.\n. @theneverstill \ngetFile was added in 2.324.0, which is why it's missing from that CodeBuild image.\nI do not have any further information regarding when that CodeBuild image will be upgraded to use a more recent version of the SDK.\nThe CodeBuild team may be able to provide additional information via the AWS Developer Forums.\n. @reefqi037 \nDoes your API show up in Items response of a request to getApis?\napigatewayv2.getApis({}, function(err,data) {\n  if (err) console.log(err, err.stack);\n  else     console.log(data);\n});\nWhat kind of API are you using?\nWhich exact version are you using?. @reefqi037 \nDoes your API show up in Items response of a request to getApis?\napigatewayv2.getApis({}, function(err,data) {\n  if (err) console.log(err, err.stack);\n  else     console.log(data);\n});\nWhat kind of API are you using?\nWhich exact version are you using?. V2 is for creating and deploying WebSocket APIs.\nYou should use V1 for interacting with the PetStore API.. V2 is for creating and deploying WebSocket APIs.\nYou should use V1 for interacting with the PetStore API.. Yes, that's correct!  Closing out this issue.. Yes, that's correct!  Closing out this issue.. @huynhtuanh \nHave you raised this question on the AWS Developer Forums?\n. @huynhtuanh \nHave you raised this question on the AWS Developer Forums?\n. @rajeshbala01 \nDid you run npm install first?. @rajeshbala01 \nDid you run npm install first?. @ramaguruprakash \nThis is a known issue with 2.382.0: https://github.com/aws/aws-sdk-js/issues/2455 \nIt has been resolved with a fix, but 2.383.0 has not yet been released.\nUntil 2.383.0 is available, you can rollback to 2.381.0, or see this post: https://github.com/aws/aws-sdk-js/issues/2455#issuecomment-450003493. @ramaguruprakash \nThis is a known issue with 2.382.0: https://github.com/aws/aws-sdk-js/issues/2455 \nIt has been resolved with a fix, but 2.383.0 has not yet been released.\nUntil 2.383.0 is available, you can rollback to 2.381.0, or see this post: https://github.com/aws/aws-sdk-js/issues/2455#issuecomment-450003493. @andrewryan1906  (Andrew),\nCould you provide an example of the describeExecution call you are making?\nIs the actual response from the StepFunction API empty?  Or does it appear that the SDK itself is causing the issue?\n. @andrewryan1906  (Andrew),\nCould you provide an example of the describeExecution call you are making?\nIs the actual response from the StepFunction API empty?  Or does it appear that the SDK itself is causing the issue?\n. You can follow the example on Using a Request Object Event Listener in order to inspect the httpResponse:\nvar req = stepFunctions.describeExecution({executionArn: executionInfo.executionArn})\nreq.on('error', (resp) => { console.log(err) });\nreq.on('success', (resp) => {\n  console.log(resp.httpResponse.body.toString());\n});\nreq.send();\nI suspect that the output would also be empty in the raw response, but it would be helpful to confirm.. You can follow the example on Using a Request Object Event Listener in order to inspect the httpResponse:\nvar req = stepFunctions.describeExecution({executionArn: executionInfo.executionArn})\nreq.on('error', (resp) => { console.log(err) });\nreq.on('success', (resp) => {\n  console.log(resp.httpResponse.body.toString());\n});\nreq.send();\nI suspect that the output would also be empty in the raw response, but it would be helpful to confirm.. MediaStoreData now supports cors: Announcement: Self-service CORS configurations now available for AWS Elemental MediaStore \nFix for https://github.com/aws/aws-sdk-js/issues/1827. MediaStoreData now supports cors: Announcement: Self-service CORS configurations now available for AWS Elemental MediaStore \nFix for https://github.com/aws/aws-sdk-js/issues/1827. Thanks for submitting this feature request.  I've tagged it so we can review for prioritization. . Thanks for submitting this feature request.  I've tagged it so we can review for prioritization. . For anyone that needs to access the CancellationReasons right now, they could be parsed out of the response body directly:\n```\nvar request = ddb.transactWriteItems(params)\nrequest.on('extractError', (resp) => {\n  console.log(resp.httpResponse.body.toString());\n});\nrequest.send()\n```\n. For anyone that needs to access the CancellationReasons right now, they could be parsed out of the response body directly:\n```\nvar request = ddb.transactWriteItems(params)\nrequest.on('extractError', (resp) => {\n  console.log(resp.httpResponse.body.toString());\n});\nrequest.send()\n```\n. \ud83d\udea2 Looks good.. \ud83d\udea2 Looks good.. @andrewryan1906 \nDo you believe this an issue with the SDK?  Can you provide a sample of your code?. @andrewryan1906 \nDo you believe this an issue with the SDK?  Can you provide a sample of your code?. @andrewryan1906 \nThis repository is reserved for issues with the SDK itself.\nIf you're having an issue with API Gateway and Lambda, you should contact the API Gateway service team: AWS Developer Forums or through AWS Support.. @WilliamEddy \nDo you get the same behavior from the AWS CLI?\nlist-transaction-jobs. If the SDK and CLI return the same results, that would point to an issue with the underling Transcribe service.\nHave you engaged the Transcribe Service team via the AWS Developer Forums or through opening a support case?\nDoes getTranscriptionJob return the correct result if you use the job name for '805df'?. @WilliamEddy \nAre you able to retrieve the job using getTranscriptionJob?. @anton-107 \n@jstewmon is correct.  For now, you can rollback to 2.381.0.  The fix in #2456 will be included with the next release of the SDK.. @wheresrhys \nYes, this is related to the credential resolution issue in 2.382.0.\nIt has been fixed and will be resolved with the next release of the SDK.  For now, you'll need to use 2.381.0.. @wheresrhys \n2.383.0 has been released.  Closing out this issue.. @makoit \nWhat state is given for the instance returned on return await this.ec2.waitFor(\"instanceRunning\", waitParams)?\nCan you confirm the instance is in a running state when the subsequent call to get the public DNS name is made?\nWhich version of the SDK are you using?. The waitFor should be used with a .promise().  The request (for use with a callback) is being returned.. @rustyconover \nThanks for opening this issue.  We're getting the documentation updated to make this explicit.. The changes to the documentation will go out with the next release.. @mungojam \nThanks for opening this request.\nThis feature-request would need to be handled by the API Gateway team, not the JavaScript SDK.\nHave you contacted that team?  You're welcome to post on their AWS Developer Forum.\nI'm going to close this issue, as this repository is reserved for SDK issues and requests.. Change for https://github.com/aws/aws-sdk-js/issues/2471. \ud83d\udea2 . @CaptainCharlieGreen \nThanks for opening this issue.\nYou're welcome to submit a PR on the test for us to review.  I understand some packages alter Object.prototype, but so far this has been a bit of an edge case.. @maghis \nThanks for opening this issue.  \nDo you have a code sample that illustrates this issue?. @niftylettuce \nSee https://github.com/aws/aws-sdk-js/issues/918.\nThe 'key' returned on small files, =< 5mb and uploaded with a single part, was left in place for backward compatibility.. @Exitialis1 \nRefer to the Cognito API Reference:\n\nResponse Elements\nIf the action is successful, the service sends back an HTTP 200 response with an empty HTTP body. \n\nThis is the expected behavior.  If data is null you can refer to the error.  Otherwise you can consider the operation a success.\nWhat are you looking for in the data object to confirm that the action was successful?\n. Take a look at Using a Request Object Event Listener.\nThe extractError and extractData listeners can be useful for debugging.  See the Request documentation for all of the available listeners.\nChanges to the underlying API would need to be made by the Cognito Service Team.  You're welcome to followup with them for that feature request on their AWS Developer Forum.. :ship:. Looks good. :shipit:. @SeongwoonHong \nIt looks like you're missing the Content-Encoding header on axios.put.\nSince you're setting that header in the signed URL, it's expected when doing the PUT.. Does removing the body make a difference for you?\nconst presignedUrl = await s3.getSignedUrl('putObject', {\n  Bucket,\n  Key: key,\n  ContentEncoding: 'base64',\n  ContentType: 'image/jpeg'\n});. @bugb,\ndata returned from assumeRole needs to be used to set the credentials for the costexplorer client, otherwise, that client will just use the same credentials as the rest of the Lambda function.\nOnce data is returned, you could do either of the following:\nUpdate the global config (using Account B credentials for any clients created thereafter):\nAWS.config.update({\n  accessKeyId: data.Credentials.AccessKeyId,\n  secretAccessKey: data.Credentials.SecretAccessKey,\n  sessionToken: data.Credentials.SessionToken\n});\nvar costexplorer = new AWS.CostExplorer();\nOr you could use the returned credentials only for costexplorer:\nconst accountBCredentials = new AWS.Credentials(data.Credentials.AccessKeyId, data.Credentials.SecretAccessKey, data.Credentials.SessionToken));\nconst costexplorer = new AWS.CostExplorer(accountBCredentials);\n. @bugb \nRefer to the documentation on the Lamba Execution Environment for details on how to closely match your local and deployed environments.\nThere is also additional documentation on Testing and Debugging which may be useful for you.\nIf you have Lambda specific questions, that are not SDK issues, you can also ask for guidance on Stack Exchange, or the AWS Developer Forum.. The url returned by getSignedUrl can be used to upload from any location on your system.\nThe url could be used like this:\n```\nconst file = fs.readFileSync('a.jpg');\nconst buffer = Buffer.from(file);\nconst opts = {}\naxios.put(url, buffer, opts)\n  .then(function(response) {\n    console.log(response)\n  })\n)\n```\nThe location of a.jpg can be PUT to the url that specifies an entirely different path in the signed URL.. @amitguptagwl \nThanks for the request.  We'll consider if this change makes sense for the SDK at this time.. @ricardolpd \nCan you provide an example of the code you are using, along with the SDK version and any other relevant environmental details?. The unmarshall example uses capitalized types: unmarshall.\nIs there different documentation you are referring to?. @ricardolpd,\nThe type specified for DynamoDB records should be capitalized.\nCan you provide the dataPipeline code that's getting the record?. Thanks for following up.  I've escalated this issue to the DataPipeline team.\nWe would expect that the records exported from DynamoDB to S3 by DataPipeline would have capitalized types.  I am seeing the same behavior that those types are being lower-cased, which is why the unmarshall operation is not working.. Thanks for following up.  I've escalated this issue to the DataPipeline team.\nWe would expect that the records exported from DynamoDB to S3 by DataPipeline would have capitalized types.  I am seeing the same behavior that those types are being lower-cased, which is why the unmarshall operation is not working.. @CarsonMcKinstry \nThanks for opening this issue.\nThis is a use case that we'd like to support in V3 of the SDK.  In V3, Fetch is available in the main browser, so it wouldn't be necessary to switch between XMLHttpRequest and Fetch depending on the context.\nV3 is currently in developer preview.  We'd love for your feedback if you have time to check it out.. @CarsonMcKinstry \nThanks for opening this issue.\nThis is a use case that we'd like to support in V3 of the SDK.  In V3, Fetch is available in the main browser, so it wouldn't be necessary to switch between XMLHttpRequest and Fetch depending on the context.\nV3 is currently in developer preview.  We'd love for your feedback if you have time to check it out.. Currently, not all of the packages are pre-built.\nTo build the s3 package, follow the steps on CONTRIBUTING, including the test steps that build the base packages:\nnpm install\nnpm run bootstrap\nnpm test\nnode ./packages/package-generator/build/cli.js client --model models/s3/2006-03-01/service-2.json --runtime browser --smoke models/s3/2006-03-01/smoke.json. Currently, not all of the packages are pre-built.\nTo build the s3 package, follow the steps on CONTRIBUTING, including the test steps that build the base packages:\nnpm install\nnpm run bootstrap\nnpm test\nnode ./packages/package-generator/build/cli.js client --model models/s3/2006-03-01/service-2.json --runtime browser --smoke models/s3/2006-03-01/smoke.json. @shivarajbakale \nCan you provide the error you receive?  Using this code, I would expect some issues due to using \nassumeRole, assumeRole1 and assumeRoleStep1.. @shivarajbakale \nCan you provide the error you receive?  Using this code, I would expect some issues due to using \nassumeRole, assumeRole1 and assumeRoleStep1.. @rhaegar453 \nDid you make changes to the assumeRole variables?. @rhaegar453 \nDid you make changes to the assumeRole variables?. @rhaegar453 \nCan you consolidate these variables and see if you can get your code working that way?\nassumeRole, assumeRole1 and assumeRoleStep1. @rhaegar453 \nCan you consolidate these variables and see if you can get your code working that way?\nassumeRole, assumeRole1 and assumeRoleStep1. @Clete2 \nThanks for opening this request.\nCan you provide an example to make sure we understand how you would want to use these types?\n. @Clete2 \nThanks for opening this request.\nCan you provide an example to make sure we understand how you would want to use these types?\n. @thenickdude \nThanks for opening this issue.  The change in #2410 was meant to drive consistency between small and large files using the managed uploader.   Like you point out, because multipart-uploading uses PUT object tagging, it could handle non URL-safe characters.\nI've tagged this as needing further discussion.. @thenickdude \nThanks for opening this issue.  The change in #2410 was meant to drive consistency between small and large files using the managed uploader.   Like you point out, because multipart-uploading uses PUT object tagging, it could handle non URL-safe characters.\nI've tagged this as needing further discussion.. @thenickdude \nResolved this issue in #2492.\nThanks for pointing this out.. @thenickdude \nResolved this issue in #2492.\nThanks for pointing this out.. @1mike12 \nThere is a separate operation for Posts: createPresignedPost.\nDoes that work for your client library?. @1mike12 \nThere is a separate operation for Posts: createPresignedPost.\nDoes that work for your client library?. Change for https://github.com/aws/aws-sdk-js/issues/2490. Change for https://github.com/aws/aws-sdk-js/issues/2490. @VTLee \nIf this isn't resolved for you in the latest, let us know and we can reopen.. @VTLee \nIf this isn't resolved for you in the latest, let us know and we can reopen.. @Salinn \nThanks for opening this issue.\nThis looks like an issue with artifactory.  The package available on the npmjs.org registry has the correct checksum.\nWe'll followup an be sure there isn't any issue on the deployment side.. @Salinn \nThanks for opening this issue.\nThis looks like an issue with artifactory.  The package available on the npmjs.org registry has the correct checksum.\nWe'll followup an be sure there isn't any issue on the deployment side.. @Salinn \nDid you followup with your artifactory provider?  Where do they pull their package version from?\nThere isn't anything out of order on the deployment side, so it seems like this needs resolved through your artifactory registry.. @Salinn \nDid you followup with your artifactory provider?  Where do they pull their package version from?\nThere isn't anything out of order on the deployment side, so it seems like this needs resolved through your artifactory registry.. @sajithvim,\nThe Support documentation states that us-east-1 endpoint must be used for accessing that API: Endpoint.\nIs your Lambda in ap-southeast-2?  Or are you setting that region another way?\n. @sajithvim,\nThe Support documentation states that us-east-1 endpoint must be used for accessing that API: Endpoint.\nIs your Lambda in ap-southeast-2?  Or are you setting that region another way?\n. @sajithvim \nThanks for following up.\nI'll opt to close this issue for now.  We're happy to re-open if you have additional concerns.. @sajithvim \nThanks for following up.\nI'll opt to close this issue for now.  We're happy to re-open if you have additional concerns.. @tan31989 \nCan you try this instead?\nconst params = {\n  Filters: [{ \n    Name: 'image-id', \n    Values: ['ami-xxxxxxxxxxx'] \n  }]  \n}. @tan31989 \nCan you try this instead?\nconst params = {\n  Filters: [{ \n    Name: 'image-id', \n    Values: ['ami-xxxxxxxxxxx'] \n  }]  \n}. The Array is defined as having the following:\n```\nName \u2014 (String)\nThe name of the filter. Filter names are case-sensitive.\nValues \u2014 (Array)\nOne or more filter values. Filter values are case-sensitive.\n```\nWe'd be happy to review a PR.   Where are you interested in making a change?. The Array is defined as having the following:\n```\nName \u2014 (String)\nThe name of the filter. Filter names are case-sensitive.\nValues \u2014 (Array)\nOne or more filter values. Filter values are case-sensitive.\n```\nWe'd be happy to review a PR.   Where are you interested in making a change?. @tan31989 \nEach of the waitFor operations are merely wrappers for another operation.  The documentation for imageExists notes that it is actually a call to describeImages and uses the same params.  describeImages has more thorough examples.\nI'll label this issue as a feature request to improve the documentation by pulling in the underlying operation examples into the documentation for each waitFor.. @tan31989 \nEach of the waitFor operations are merely wrappers for another operation.  The documentation for imageExists notes that it is actually a call to describeImages and uses the same params.  describeImages has more thorough examples.\nI'll label this issue as a feature request to improve the documentation by pulling in the underlying operation examples into the documentation for each waitFor.. @mblag \nHow are you setting up the DynamoDB client?. @mblag \nHow are you setting up the DynamoDB client?. @rhaegar453 \nHow are you setting your credentials for the STS Client?\nHave you reviewed this documentation? Setting Credentials in Node.js. @rhaegar453 \nHow are you setting your credentials for the STS Client?\nHave you reviewed this documentation? Setting Credentials in Node.js. @rhaegar453 \nCan you provide an example of your code?. @rhaegar453 \nCan you provide an example of your code?. @nickyang07 \n@diehlaws's comment, https://github.com/aws/aws-sdk-php/issues/1687#issuecomment-455274120, is the latest information available from the DynamoDB team at this time.\nWe'll comment on this issue when an update is available.. @nickyang07 \n@diehlaws's comment, https://github.com/aws/aws-sdk-php/issues/1687#issuecomment-455274120, is the latest information available from the DynamoDB team at this time.\nWe'll comment on this issue when an update is available.. @Bunnoo \nWhich version SDK are you using?  Which version of Node?\nCan you show a code sample of how you are using the SDK?. @Bunnoo \nWhich version SDK are you using?  Which version of Node?\nCan you show a code sample of how you are using the SDK?. @Bunnoo,\nThe code you've provided runs in the browser, is that correct?\nYou'll need to refer to this documentation: Setting Credentials in a Web Browser. @Bunnoo,\nThe code you've provided runs in the browser, is that correct?\nYou'll need to refer to this documentation: Setting Credentials in a Web Browser. Hello @CooperWolfe,\nThanks for opening this issue.\nemail.us-east-1.amazonaws.com is a valid enpoint for that service.\nDo you get the same behavior if you use us-west-2 or eu-west-1 as your region/endpoint instead?\n. Hello @CooperWolfe,\nThanks for opening this issue.\nemail.us-east-1.amazonaws.com is a valid enpoint for that service.\nDo you get the same behavior if you use us-west-2 or eu-west-1 as your region/endpoint instead?\n. @CooperWolfe \nThanks for following up.\nHave you set the region in the client directly?\nconst ses = new AWS.SES({region: 'us-east-1'})\nCan you console.log the SES client (removing any sensitive information)?. @CooperWolfe \nThanks for following up.\nHave you set the region in the client directly?\nconst ses = new AWS.SES({region: 'us-east-1'})\nCan you console.log the SES client (removing any sensitive information)?. @CooperWolfe \nThis looks like a DNS issue.  Are you behind a firewall that would be preventing you from reaching email.us-east-1.amazonaws.com?. @CooperWolfe \nThis looks like a DNS issue.  Are you behind a firewall that would be preventing you from reaching email.us-east-1.amazonaws.com?. @dannyskim,\nThanks for raising this issue.  We'll need to dig into this a bit further.  Thanks for including these details. . @dannyskim,\nThanks for raising this issue.  We'll need to dig into this a bit further.  Thanks for including these details. . @dannyskim \nI wanted a chance to review this a bit further.\nI was able to build RN v0.56.1 using the latest version of the SDK.  v0.57.0 and later produced heap out of memory errors.\nDid running react-native bundle or Metro Bundler return heap out of memory errors for you?\nThose memory errors can be worked around by running the following:\nexport NODE_OPTIONS=\"--max-old-space-size=2048\u201d\nAllocating more memory to Node allowed me to build RN v0.58.3 with the latest version of the SDK imported.\nDepending on your project, you may need to bump up the memory even further.. @dannyskim \nI wanted a chance to review this a bit further.\nI was able to build RN v0.56.1 using the latest version of the SDK.  v0.57.0 and later produced heap out of memory errors.\nDid running react-native bundle or Metro Bundler return heap out of memory errors for you?\nThose memory errors can be worked around by running the following:\nexport NODE_OPTIONS=\"--max-old-space-size=2048\u201d\nAllocating more memory to Node allowed me to build RN v0.58.3 with the latest version of the SDK imported.\nDepending on your project, you may need to bump up the memory even further.. @dannyskim \nDoes using an older version of the SDK make a difference?  How about v2.350.0?\nDid you have success building the app with RN v0.56.1?. @dannyskim \nDoes using an older version of the SDK make a difference?  How about v2.350.0?\nDid you have success building the app with RN v0.56.1?. @shlomisas \nThanks for opening this issue.\n2.233.1 is an older version.  Did you mean you upgraded to 2.286.2?. @shlomisas \nThanks for opening this issue.\n2.233.1 is an older version.  Did you mean you upgraded to 2.286.2?. It will likely be easier to rely on the error code instead of the message.\nThe list of S3 Error Codes can be found here: ErrorCodeList\nIn this case, you would receive a NoSuchKey error code if there is no key in the bucket you've specified.. It will likely be easier to rely on the error code instead of the message.\nThe list of S3 Error Codes can be found here: ErrorCodeList\nIn this case, you would receive a NoSuchKey error code if there is no key in the bucket you've specified.. @SeongwoonHong \nCan you add some logging to verify that the client is setup properly?  Are you catching and logging any errors that are being thrown?. @SeongwoonHong \nCan you add some logging to verify that the client is setup properly?  Are you catching and logging any errors that are being thrown?. @SeongwoonHong \nIs the client setup properly?  Are you able to do other operations successfully?  Perhaps headObject or something that will return a response for you to verify?. @SeongwoonHong \nIs the client setup properly?  Are you able to do other operations successfully?  Perhaps headObject or something that will return a response for you to verify?. @piyumi25 \nRefer to the documentation: putObject.\nYou should set the ContentType in the top level of params:\nlet uploadParams = {\n  Body: byteContent,\n  Bucket: credentials.name,\n  ContentType: 'valid content type',\n  Key: UId\n}. @piyumi25 \nRefer to the documentation: putObject.\nYou should set the ContentType in the top level of params:\nlet uploadParams = {\n  Body: byteContent,\n  Bucket: credentials.name,\n  ContentType: 'valid content type',\n  Key: UId\n}. @VivithaAlamur Does @hughjdavey's suggestion resolve your issue?. @VivithaAlamur Does @hughjdavey's suggestion resolve your issue?. @jono99 \nI'll go ahead and close our this issue for now.  If you're interested in re-engaging, let us know and we can re-open.. @worc \nThanks for raising this issue.  This is actually an issue with the underlying service.  S3's documentation does state that this field is required: RESTBucketPUTlifecycle, however, the model use to generate this operation in the JavaScript SDK (and all of the other SDKs) does not have Filter marked as required.\nI've escalated this to the S3 team to get it resolved.. @worc \nThanks for raising this issue.  This is actually an issue with the underlying service.  S3's documentation does state that this field is required: RESTBucketPUTlifecycle, however, the model use to generate this operation in the JavaScript SDK (and all of the other SDKs) does not have Filter marked as required.\nI've escalated this to the S3 team to get it resolved.. @dacacioa \nDoes your proxy provide logging?  Which proxy are you using?\nDoes setting sslEnabled: false make a difference?. @dacacioa \nDoes your proxy provide logging?  Which proxy are you using?\nDoes setting sslEnabled: false make a difference?. @dacacioa \nThanks for following up.\nWhich version of the SDK are you using?. @dacacioa \nThanks for following up.\nWhich version of the SDK are you using?. @dacacioa \nThanks for following up.  I haven't been able to reproduce this issue.\nAre there any other environmental details that might be relevant that you can share?   How are you handling credentials?  What OS?. @dacacioa \nThanks for following up.  I haven't been able to reproduce this issue.\nAre there any other environmental details that might be relevant that you can share?   How are you handling credentials?  What OS?. @cchakkaravarthi \nCan you please provide a detailed example of what you code and environment, including versions of the SDK?. @cchakkaravarthi \nCan you please provide a detailed example of what you code and environment, including versions of the SDK?. @cchakkaravarthi \nCan you get details on which headers are invalid?\nIt looks like there have been some UWP issues regarding the Authorization header specifically.. @cchakkaravarthi \nCan you get details on which headers are invalid?\nIt looks like there have been some UWP issues regarding the Authorization header specifically.. @cchakkaravarthi,\nDoes UWP provide details on which header is invalid?. @cchakkaravarthi,\nDoes UWP provide details on which header is invalid?. @kevinresol \nThanks for opening this issue.\nI'm seeing the same behavior and have escalated this issue to the IoT team.\n. @kevinresol \nThanks for opening this issue.\nI'm seeing the same behavior and have escalated this issue to the IoT team.\n. @kevinresol \nI'm not able to provide an ETA.  We'll reply on this issue when there is an update to share.. @kevinresol \nI'm not able to provide an ETA.  We'll reply on this issue when there is an update to share.. @kevinresol \nThe service made a fix for this issue.  Are you able to successfully update a job execution now?. @kevinresol \nThe service made a fix for this issue.  Are you able to successfully update a job execution now?. @skay973 \nWhich version of the SDK are you using?  Is error null?. @skay973 \nWhich version of the SDK are you using?  Is error null?. @vladejs \nThere is not a direct way to do this.\nlistObjectsV2 allows you to retrieve objects by key prefix, but would not be able to filter for 'was' in the key 'foowasbar'.\nThe AWS CLI gives you a the following operation:\naws s3api list-objects --bucket bucketName --query \"Contents[?contains(Key, 'was')]\"\nThat makes one or more API calls to retrieve all Objects in the bucket and filters the results locally.. @vladejs \nThere is not a direct way to do this.\nlistObjectsV2 allows you to retrieve objects by key prefix, but would not be able to filter for 'was' in the key 'foowasbar'.\nThe AWS CLI gives you a the following operation:\naws s3api list-objects --bucket bucketName --query \"Contents[?contains(Key, 'was')]\"\nThat makes one or more API calls to retrieve all Objects in the bucket and filters the results locally.. @vladejs \nRefer to this Whitepaper: AWS Storage Services \nOverview\n\nAmazon S3 doesn\u2019t offer query capabilities to retrieve specific objects. When you use Amazon S3 you need to know the exact bucket name and key for the files you want to retrieve from the service. Amazon S3 can\u2019t be used as a data base or search engine by itself. Instead, you can pair Amazon S3 with Amazon DynamoDB, Amazon CloudSearch, or Amazon Relational Data base Service (Amazon RDS) to index and query metadata about Amazon S3 buckets and objects\n\nThe recommended solution is to pair S3 with an additional service.  With this approach, your index can return only the relevant keys.. @vladejs \nRefer to this Whitepaper: AWS Storage Services \nOverview\n\nAmazon S3 doesn\u2019t offer query capabilities to retrieve specific objects. When you use Amazon S3 you need to know the exact bucket name and key for the files you want to retrieve from the service. Amazon S3 can\u2019t be used as a data base or search engine by itself. Instead, you can pair Amazon S3 with Amazon DynamoDB, Amazon CloudSearch, or Amazon Relational Data base Service (Amazon RDS) to index and query metadata about Amazon S3 buckets and objects\n\nThe recommended solution is to pair S3 with an additional service.  With this approach, your index can return only the relevant keys.. \ud83d\udea2 . \ud83d\udea2 . @nakedible-p \nThanks for submitting this feature request.  We'll review for prioritization.. @nakedible-p \nThanks for submitting this feature request.  We'll review for prioritization.. @renatoargh\nWhich older versions did you try?  Was this working in any version for you?. @renatoargh\nWhich older versions did you try?  Was this working in any version for you?. @renatoargh \nThanks for following up and confirming that resolved the issue.\nEverything else working as expected?  Or can we close out this issue?. @renatoargh \nThanks for following up and confirming that resolved the issue.\nEverything else working as expected?  Or can we close out this issue?. @KapilJethava \nThe signature changes based off of when it is calculated.\nHow long are you wanting to cache the images?. @KapilJethava \nThe signature changes based off of when it is calculated.\nHow long are you wanting to cache the images?. @robermar2 \nSince you're using Node (and not a Browser), does the suggested EventStream example work for you?. @robermar2 \nSince you're using Node (and not a Browser), does the suggested EventStream example work for you?. @laynemoseley \nThanks for opening this issue.  I've escalated this to MediaConvert team to make that new parameter available via the SDKs.\nOnce the MediaConvert team makes their update, the SDKs will include that change in the next release.. @laynemoseley \nThanks for opening this issue.  I've escalated this to MediaConvert team to make that new parameter available via the SDKs.\nOnce the MediaConvert team makes their update, the SDKs will include that change in the next release.. Not at this time. \nI'll update this issue when there is an update I can share.. Not at this time. \nI'll update this issue when there is an update I can share.. @laynemoseley \nv2.411.0 is out with 7729619134b77729cfa34db047aa72e39caf8bbe\nIt includes the new Rotate parameter.. @laynemoseley \nv2.411.0 is out with 7729619134b77729cfa34db047aa72e39caf8bbe\nIt includes the new Rotate parameter.. @tijanirf \nAre you supplying your own version of the SDK?  Or using the SDK supplied by Lambda?\nThe version supplied by Lambda is 2.290.0 and does not include the update that added the Rotate option.. Fix for https://github.com/aws/aws-sdk-js/issues/2408. @akefirad \nAre there any notable differences in the headers being sent between the HTML and the Firefox Extension?\nWhich version of Firefox are you testing with?. @mreinstein \nThanks for opening this issue.\nThe SDK Team is able to provide any information regarding the ThrottlingException returned by the service.\nYou can engage the Transcribe Team via their AWS Developer Forum: AWS Transcribe\nIf there's any followup questions regarding the behavior of the SDK, let us know and we can re-open this issue if needed.. Refer to this issue: https://github.com/aws/aws-sdk-js/issues/1334\nThe Step Functions Service does not support CORS.\nThere is a thread on their AWS Developer Forum if you'd like to add your feedback: Request for CORS support. \ud83d\udea2 looks good.. resolved in https://github.com/aws/aws-sdk-js/pull/2586. @tan31989 \nThanks for submitting this issue and this PR.  \nI went ahead and bumped other dev dependencies while resolving the issue in https://github.com/aws/aws-sdk-js/pull/2586\nThanks!!. :ship:. Are you able to generate a valid URL directly (without Express being involved)?. Are you wanting to do a GET or a PUT?\nYour code example has putObject.  Is that correct?. @stayingcool \nCan you try this instead?\nprivate params = {\n    UserPoolId: this.cognitoUserPoolID,\n  };\nAttributesToGet should be removed entirely if you want all attributes returned.  Does that return given_name on the Users?. Changing the default behavior here could introduce issues for existing users of the SDK.  If this were enabled by default, it would require a major version bump.\nIt is possible to enable keep-alive through configuration.  \nThat can be done on the global config:\n```\nconst AWS = require('aws-sdk')\nconst https = require('https');\nconst agent = new https.Agent({\n  keepAlive: true\n})\nAWS.config.update({\n  httpOptions: {\n    agent: agent\n  }\n})\n```\nOr on a per-client basis:\n```\nconst AWS = require('aws-sdk')\nconst https = require('https');\nconst agent = new https.Agent({\n  keepAlive: true\n});\nconst DDB = new AWS.DynamoDB({\n  httpOptions: {\n    agent: agent\n  }\n});\n```. :shipit:. @javisabalete \nThanks for submitting this issue.\nThis is a feature that would need to be implemented by the ECS Service Team, and is not something that could be implemented in the SDK directly.\nIf you haven't already, you can engage the ECS Team on their AWS Developer Forum.. @greg-peters \nCan you provide a code example and which version of the SDK you're using?. @greg-peters \nHave you tried the equivalent query with the AWS CLI?. @sawaikar-gauri \nYour last message did not include any code.  Did you mean to include that?. @sawaikar-gauri \nIs there a specific operation of the SDK that you think is not working correctly?. In addition to putObject, the SDK also has an upload operation: S3 upload.\nEither of these will work:\n```\nconst AWS = require('aws-sdk')\nconst s3 = new AWS.S3();\nconst params = {\n  Body: '\uc548\ub155',\n  Bucket: 'myBucket',\n  Key: 'myKey'\n}\ns3.putObject(params, (err,data) => { \n  console.log(err,data) \n})\ns3.upload(params, (err,data) => { \n  console.log(err,data) \n})\n```\nAre you certain that the issue is the SDK and not the upstream functions preparing the body that you're trying to send to S3?. @sawaikar-gauri \nOk.  Followup if there is something not working or documented properly with the SDK.. :shipit:. @cocobiz \nAre you having issues with every query?  Or is it one specific query that is long-running?. Resolves https://github.com/aws/aws-sdk-js/issues/2563. @SimonSchick \nThanks for the suggestion.  We're opting to stick with only using .npmignore for now.  We'll keep that option in mind though.. @AllanFly120 \nHow about this message instead:\n\nMissing credentials in config, if using AWS_CONFIG_FILE, set AWS_SDK_LOAD_CONFIG=1. Yep, I'll be updating this PR with additional tests and functionality to cover the expireTime.. Followed the botocore implementation and ignored stderr. thanks!. Tightened to only accept 1. Thanks!. \n",
    "cs2dsb": "Hi Loren,\nThat makes sense.\nWould it be possible to update the sdk documentation to mention this scenario? It would seem to fit in http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/node-configuring.html around the bit where it mentions \"Service-Specific Configuration\"? A link to the regions and endpoints URL you mentioned would also probably be really useful around there.\nThanks,\nDaniel\n. That sounds good. I'll close this pull request now.\nThanks\n. ",
    "Infotaku": "Yes, I finally did the same to be able to continue testing, I spawned a new EC2 instance with Ubuntu 14.04 and node 0.10.31 and everything was fine.\nhere is the result of node -v on the old instance:\nv0.11.5-pre\nand npm list on the old server\n/var/www/nodejs/tests\n\u2514\u2500\u252c aws-sdk@2.0.15\n  \u251c\u2500\u2500 aws-sdk-apis@3.1.6\n  \u251c\u2500\u252c xml2js@0.2.6\n  \u2502 \u2514\u2500\u2500 sax@0.4.2\n  \u2514\u2500\u2500 xmlbuilder@0.4.2\n. ",
    "capaj": "@lsegal I did run into the issue just now-I am on ubuntu 17.04 and node 8.1.0 when using S3\n```\nS3 Upload: begin upload file /home/capaj/git_projects/looop/project-alpha/back-end/src/tmp/U1-HIIeET67vMpKDcHX4Vw1d.png\nS3 Upload: error while uploading file /home/capaj/git_projects/looop/project-alpha/back-end/src/tmp/U1-HIIeET67vMpKDcHX4Vw1d.png { XMLParserError: Non-whitespace before first tag.\nLine: 0\nColumn: 1\nChar: {\n    at error (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/sax/lib/sax.js:667:10)\n    at strictFail (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/sax/lib/sax.js:693:7)\n    at beginWhiteSpace (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/sax/lib/sax.js:967:7)\n    at SAXParser.write (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/sax/lib/sax.js:1019:11)\n    at Parser.exports.Parser.Parser.parseString (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/xml2js/lib/xml2js.js:508:31)\n    at Parser.parseString (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/xml2js/lib/xml2js.js:7:59)\n    at NodeXmlParser.parse (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/aws-sdk/lib/xml/node_parser.js:30:10)\n    at Request.extractError (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/aws-sdk/lib/services/s3.js:525:39)\n    at Request.callListeners (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/aws-sdk/lib/request.js:682:14)\n    at Request.transition (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /home/capaj/git_projects/looop/project-alpha/back-end/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request. (/home/capaj/git_projects/looop/project-alpha/back-end/node_modules/aws-sdk/lib/request.js:684:12)\n  message: 'Non-whitespace before first tag.\\nLine: 0\\nColumn: 1\\nChar: {',\n  code: 'XMLParserError',\n  retryable: true,\n  time: 2017-06-15T08:10:09.028Z,\n  statusCode: 500 }\n```. @shouze nope, I still get this error. Good thing we're running on node 6 in production.. ",
    "shouze": "We encounter the same S3 one than @capaj after the upgrade to node 8.7.0 today:\nMessage:\n     XMLParserError: Non-whitespace before first tag.\n     Line: 0\n     Column: 1\n     Char: 4\n         at error (/home/node/node_modules/sax/lib/sax.js:667:10)\n         at strictFail (/home/node/node_modules/sax/lib/sax.js:693:7)\n         at beginWhiteSpace (/home/node/node_modules/sax/lib/sax.js:967:7)\n         at SAXParser.write (/home/node/node_modules/sax/lib/sax.js:1019:11)\n         at Parser.exports.Parser.Parser.parseString (/home/node/node_modules/xml2js/lib/xml2js.js:508:31)\n         at Parser.parseString (/home/node/node_modules/xml2js/lib/xml2js.js:7:59)\n         at NodeXmlParser.parse (/home/node/node_modules/aws-sdk/lib/xml/node_parser.js:31:10)\n         at Request.extractError (/home/node/node_modules/aws-sdk/lib/services/s3.js:563:39)\n         at Request.callListeners (/home/node/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n         at Request.emit (/home/node/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n         at Request.emit (/home/node/node_modules/aws-sdk/lib/request.js:683:14)\n         at Request.transition (/home/node/node_modules/aws-sdk/lib/request.js:22:10)\n         at AcceptorStateMachine.runTo (/home/node/node_modules/aws-sdk/lib/state_machine.js:14:12)\n         at /home/node/node_modules/aws-sdk/lib/state_machine.js:26:10\n         at Request.<anonymous> (/home/node/node_modules/aws-sdk/lib/request.js:38:9)\n         at Request.<anonymous> (/home/node/node_modules/aws-sdk/lib/request.js:685:12)\nWe were running aws sdk 2.117.0 and it still occurs after upgrading to 2.133.0.\nPing @capaj have you fixed this one? If yes, how?. Ok. Are you experiencing this issue with the real S3 platform? I didn't mentioned that but this error occurs on our CI ATM, which uses minio to simulate a S3 infra. So maybe I need to ensure it's not a related minio issue too.. ",
    "bcoe": "@lsegal I tried a 1.x version of aws-sdk, and continued to have problems. I think it might actually be related to an underlying Heroku problem:\nhttps://discussion.heroku.com/t/runtime-memory-leaks/773\n. @lsegal I tried a 1.x version of aws-sdk, and continued to have problems. I think it might actually be related to an underlying Heroku problem:\nhttps://discussion.heroku.com/t/runtime-memory-leaks/773\n. ",
    "L42y": "Deeply sorry for the inconvenience, it was because a missing new when calling AWS.CognitoIdentityCredentials.\n. ",
    "EricYang": "var AWS = require('aws-sdk');\ntry{\nvar csd = new AWS.CloudSearchDomain({endpoint: 'my.host.tld'});\n}catch(error){\n    console.log('error:',error.stack)\n}\nTypeError: undefined is not a function\n    at Object. (/home/ec2-user/ericyang/dev/rest/test.js:3:11)\n    at Module._compile (module.js:449:26)\n    at Object.Module._extensions..js (module.js:467:10)\n    at Module.load (module.js:356:32)\n    at Function.Module._load (module.js:312:12)\n    at Module.runMain (module.js:487:10)\n    at process.startup.processNextTick.process._tickCallback (node.js:244:9)\n. I tried to print property of AWS below and I can't found any function about 'CloudSearchDomain'.\nAWS's property:\nutil,VERSION,ServiceInterface,Signers,XML,Service,Credentials,CredentialProviderChain,TemporaryCredentials,WebIdentityCredentials,SAMLCredentials,Config,config,Endpoint,HttpRequest,HttpResponse,HttpClient,SequentialExecutor,JSON,QueryParamSerializer,EventListeners,Request,Response,ResourceWaiter,ParamValidator,events,NodeHttpClient,SES,IAM,SimpleDB,CloudTrail,ElastiCache,Route53,Support,CloudWatch,ELB,OpsWorks,SWF,EC2,DataPipeline,SQS,RDS,DynamoDB,Glacier,CloudSearch,StorageGateway,AutoScaling,Redshift,STS,CloudFront,SNS,EMR,CloudFormation,ElasticBeanstalk,S3,DirectConnect,ImportExport,ElasticTranscoder,Kinesis,MetadataService,EC2MetadataCredentials,EnvironmentCredentials,FileSystemCredentials\n.  \"name\": \"aws-sdk\",\n  \"description\": \"AWS SDK for JavaScript\",\n  \"version\": \"2.0.15\"\n. ",
    "janearc": "Can that be passed in during the constructor? I mean, is the correct pattern to create the instance and then update its configuration?\n(and thank you)\n. I've successfully configured it and it runs as described. I think this can be closed.\n. I am not, especially. I guess I assumed that the callback was only passed the data after the nodes were \"running.\" Is it more a \"promise that they will be running\" at some point?\n. I am not, especially. I guess I assumed that the callback was only passed the data after the nodes were \"running.\" Is it more a \"promise that they will be running\" at some point?\n. Ah, yes. Thank you again @mhart. That has done the trick. I'll close this and the one over on the forum.\n. Ah, yes. Thank you again @mhart. That has done the trick. I'll close this and the one over on the forum.\n. ",
    "pcoady": "The reason I am posting here is because:\n1. It is a bug with the SDK.\n2. It is a major security issue\n3. The response from AWS support is to provide unrestricted access to the bucket because Cognito does not recognise policy variables. Hmmm.... where does security come into this?\n. Isegal. As I have stated the problem is that the policy variables are not recognised. It does not matter whether they are Cognito, Facebook, Amazon, Google or whatever. ${cognito-identity.amazonaws.com:sub} does not work.\n. I can't answer that. I only use the Javascript SDK.\n. I can confirm that it doesn't happen when not using Cognito.\n. Thank you very much for your help.\n. ",
    "nickdk": "Fixed the lint failure, I'll see if I can write a testcase for this when I've got some time to spare.\nGood point about BASIC not being the only authentication scheme but I'm not really sure if it could potentially introduce more bugs, currently proxy authentication isn't working for any scheme I assume.\nSince we're doing deployment through the grunt-aws-s3 module I'm unsure we can use the global events to inject the header.\nThanks for the feedback.\n. Thanks @AdityaManohar, forgot that this pull request was actually still open. I think we're still using my fork at my company to support our proxy, so thanks for the developer guide to do this properly. We'll start using this solution so we can ditch my fork as soon as possible.\n. ",
    "EnFinlay": "Now this is embarrassing, I completely forgot to check the versions of everything.\nNode is v0.10.28\naws-sdk is v2.0.0-rc11\n. Now this is embarrassing, I completely forgot to check the versions of everything.\nNode is v0.10.28\naws-sdk is v2.0.0-rc11\n. ",
    "syadykin": "The \u00absort\u00bb function could be something like\nfunction(a,b) {\n    return a.split('=', 2)[0].localeCompare(b.split('=', 2)[0]);\n}\n. ",
    "rodrigoreis22": "That worked @AdityaManohar . Thanks!\n. That worked @AdityaManohar . Thanks!\n. ",
    "egeland": "About a year on from the initial ticket, is there support for promises in this SDK? (I skimmed the docs and see only callbacks and Request objects in the examples)\nPromises will be hugely important with both ES2015 and ES2016 (see async/await for ES2016, especially), and if I've just missed the doco, would really appreciate a quick pointer.\n. ",
    "ash2k": "Maybe a workable approach would be to add an additional method that returns a promise to the Request object? Take a look at https://github.com/lightsofapollo/aws-sdk-promise/blob/master/index.js (but it should use native Promise probably).\n. Maybe a workable approach would be to add an additional method that returns a promise to the Request object? Take a look at https://github.com/lightsofapollo/aws-sdk-promise/blob/master/index.js (but it should use native Promise probably).\n. It works for us but ideally we think it should:\n1. Look for explicit credentials parameter passed into constructor;\n2. Look for global credentials in AWS.config.credentials;\n3. Use AWS.config.credentialProviderChain as the last resort.\nAdding 1 and 3 should not break current behaviour.\n. ",
    "saj1919": "Hi Isegal, I followed the code from here http://docs.aws.amazon.com/aws-sdk-php/latest/class-Aws.DynamoDb.DynamoDbClient.html and it is working ... Just for some names having non utf-8 characters it throws error (1% of the lot). But for some non utf-8 it works fine.\nI am using cakephp, aws-sdk 2.6.15 (In latest sdk aws-autoloader.php was throwing an error so opted for previous release), I tested it on chrome browser mostly. Not using node.js\n$client->putItem(array(\n                        'TableName' => _DYNAMODB_TABLE,\n                        'Item'      => array(\n                                      'Id'          => array('S' => $id),\n                                      'name'    => array('S' => $name)\n                      )));\nPlease tell me if you want more info. Bcz 1% is also a huge number for us. Thanks.\n. ",
    "coveralls": "\nCoverage decreased (-0.02%) when pulling 600f18feb29666519ebc23853b53c3ca243c9fe3 on darbs:master into c1a09d850b4e414ac977f9f979f83ba1cd12c08c on aws:master.\n. \nCoverage increased (+0.01%) when pulling 62b08c9339d4c65853e24b60b98f4d24b663ff28 on fix/metadata-service-no-timeout into f2086db2c678b98d63fe6ec8df366eb43ff7f579 on master.\n. \nCoverage remained the same when pulling 4b8f8d1ff3f4da6599676933991477e0da831586 on fix/send-typed-arrays into 69805fc69bda3993174908ee45b32a77b4cdf95d on master.\n. \nCoverage remained the same when pulling e8e358f6fad8f722faab7ea6d5fc00fa7421ca2c on konklone:browserify-docs into 8eba9a081d9458bf5e4da346b61659c0dd62f3d6 on aws:master.\n. \nCoverage increased (+0.01%) when pulling f9ead7b7e8717f41653f1fa585d0f9ec4bf170c8 on ec2-normalized into 02b677952a0089f52bbb11878655aab319f19fd8 on master.\n. \nCoverage increased (+0.0%) when pulling 15f17eef31c34219cab6ae9e6440544639ac9a05 on fix-http-host into 8e2e66c9c53146ba69e358c73bc2369644bf2fe5 on master.\n. \nCoverage decreased (-0.2%) when pulling bc104987066f81552488dd1253bad8cad2c93eda on bkw:betterLogs into 2b463ce1e418f6771327d08c9912628e621bb1f8 on aws:master.\n. \nCoverage remained the same when pulling 4a8224d2014de839d5e355310eac4bd2c9453899 on joaostein:fixTypo into 6fc999b0644eb71518eb00a3445f3e1472b834e0 on aws:master.\n. \nCoverage remained the same when pulling a5cf282c629b920dad559c7d160a88fcf2e73fd3 on fix-travis-builds into 98be17d31b3ad0feb27624f11738c8d2e34d2626 on master.\n. \nCoverage remained the same when pulling fb6f8043bcaa8b2de95c992ac0ea3f63a0bea20e on fix-proxy-config-docs into 6b95504d722bfea806e495936c4324042ffe3bef on master.\n. \nCoverage remained the same when pulling e072af41dfe841fc3539a76fef4c8f6b77ca0296 on feature-expect-100-s3 into 772df6897a214c7741a7f080754d2e462a47941d on master.\n. \nCoverage increased (+0.01%) when pulling 2698bbadc68dc0e68b03c5bc6aea26bbba578d09 on mick:creds_lock into 8a15f085d87c52de38c2bbf645a80b8649627b30 on aws:master.\n. \nCoverage increased (+0.01%) when pulling 0c7071305acd1f29fa9f5b1364ad6017bbcad307 on mick:creds_lock into b768e76bb8a46cadf9970397acdf1821db609c0b on aws:master.\n. \nCoverage increased (+0.16%) when pulling bf8ece7783898add8d78dab588bcaf7019eaee6c on fix-cloudsearchdomain-signing into 8a15f085d87c52de38c2bbf645a80b8649627b30 on master.\n. \nCoverage decreased (-0.11%) when pulling 6cbc4e1c95c3bf046387640ce9784c4f1a1a08f2 on zjjw:master into 8a15f085d87c52de38c2bbf645a80b8649627b30 on aws:master.\n. \nCoverage decreased (-0.11%) when pulling 6cbc4e1c95c3bf046387640ce9784c4f1a1a08f2 on zjjw:master into 8a15f085d87c52de38c2bbf645a80b8649627b30 on aws:master.\n. \nCoverage remained the same when pulling d66bcfce22cf5a416b96359d5c2f42ae4ad0dae8 on awsdocs:eronh_link_fix into 8c0c26841dcc7b4592f96d4f29ec25948a6df41e on aws:master.\n. \nCoverage remained the same when pulling d66bcfce22cf5a416b96359d5c2f42ae4ad0dae8 on awsdocs:eronh_link_fix into 8c0c26841dcc7b4592f96d4f29ec25948a6df41e on aws:master.\n. \nCoverage increased (+0.18%) when pulling bd21afce1bb0275be79f047dfa1c888ddbc0cd25 on jaxgeller:master into b768e76bb8a46cadf9970397acdf1821db609c0b on aws:master.\n. \nCoverage increased (+0.18%) when pulling bd21afce1bb0275be79f047dfa1c888ddbc0cd25 on jaxgeller:master into b768e76bb8a46cadf9970397acdf1821db609c0b on aws:master.\n. \nCoverage remained the same when pulling c0cf280ec71da20c93d634dc43540fac43bd3d29 on jaxgeller:master into b768e76bb8a46cadf9970397acdf1821db609c0b on aws:master.\n. \nCoverage remained the same when pulling c0cf280ec71da20c93d634dc43540fac43bd3d29 on jaxgeller:master into b768e76bb8a46cadf9970397acdf1821db609c0b on aws:master.\n. \nCoverage increased (+0.19%) when pulling da28db40081c44c875d9be8646c2d322bb045cc1 on expose-request-id into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage increased (+0.19%) when pulling da28db40081c44c875d9be8646c2d322bb045cc1 on expose-request-id into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage increased (+0.02%) when pulling 977026a7f1581babcd16bd82d71d1f4768cd3178 on expose-request-id into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage increased (+0.02%) when pulling 977026a7f1581babcd16bd82d71d1f4768cd3178 on expose-request-id into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage increased (+0.19%) when pulling afe9c7dc9891bb3f92add3eb8f68fd0ffa64aa20 on expose-request-id into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage increased (+0.19%) when pulling afe9c7dc9891bb3f92add3eb8f68fd0ffa64aa20 on expose-request-id into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage increased (+0.19%) when pulling 4d6ddee357538c761df5cab1fc26cd5ea1061c85 on expose-request-id into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage increased (+0.19%) when pulling 4d6ddee357538c761df5cab1fc26cd5ea1061c85 on expose-request-id into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage increased (+0.01%) when pulling 3ec1208d94fa94cbca72fce757076a3723c5dfa5 on expose-request-id into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage increased (+0.01%) when pulling 3ec1208d94fa94cbca72fce757076a3723c5dfa5 on expose-request-id into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage remained the same when pulling 5f31d35a01e230f6f52693d714b7361a2a747b77 on fix-s3-ssec-encoding into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage remained the same when pulling 5f31d35a01e230f6f52693d714b7361a2a747b77 on fix-s3-ssec-encoding into fc04887905f4a8dca1a96e1799c2f8bedab1cd01 on master.\n. \nCoverage remained the same when pulling 54a4330e306a35febef14992e698ddfa2e1d8892 on fix-fillbuffer into 7c0baf00fd335daf26cb22fac2c5ceda894ec459 on master.\n. \nCoverage remained the same when pulling 54a4330e306a35febef14992e698ddfa2e1d8892 on fix-fillbuffer into 7c0baf00fd335daf26cb22fac2c5ceda894ec459 on master.\n. \nCoverage decreased (-0.15%) when pulling 33a1b8506a5a4ac56788659b128be9c0e66979c4 on cognitoidentity-credentials into e4eddec453e7ba4301ea36107ef9259e06286e41 on master.\n. \nCoverage decreased (-0.15%) when pulling 33a1b8506a5a4ac56788659b128be9c0e66979c4 on cognitoidentity-credentials into e4eddec453e7ba4301ea36107ef9259e06286e41 on master.\n. \nCoverage decreased (-0.15%) to 94.4% when pulling 2be6288280d340abf429a0341f19e9fb4eec8ad9 on cognitoidentity-credentials into e4eddec453e7ba4301ea36107ef9259e06286e41 on master.\n. \nCoverage decreased (-0.15%) to 94.4% when pulling 2be6288280d340abf429a0341f19e9fb4eec8ad9 on cognitoidentity-credentials into e4eddec453e7ba4301ea36107ef9259e06286e41 on master.\n. \nCoverage remained the same at 94.4% when pulling 9c140605053714d85eb3c94a836d6507f2ab5901 on cognito-guide-docs into e165457c46ced385e7cdd9d1b567929193e6730c on master.\n. \nCoverage remained the same at 94.4% when pulling 9c140605053714d85eb3c94a836d6507f2ab5901 on cognito-guide-docs into e165457c46ced385e7cdd9d1b567929193e6730c on master.\n. \nCoverage remained the same at 94.4% when pulling bfc33bf70050968e4a4e3c203f6eb2a0ec0ac91a on cognitocredentials-apidocs into b08e4297601189f66b8c10f3858fed25c054963b on master.\n. \nCoverage remained the same at 94.4% when pulling bfc33bf70050968e4a4e3c203f6eb2a0ec0ac91a on cognitocredentials-apidocs into b08e4297601189f66b8c10f3858fed25c054963b on master.\n. \nCoverage increased (+0.0%) to 94.4% when pulling 388a71e061f551d093ca9e7019cceae810360130 on mdouglass:master into 12ba49f4cc4bae730385c0a93d82bcb0fb810984 on aws:master.\n. \nCoverage increased (+0.0%) to 94.4% when pulling 388a71e061f551d093ca9e7019cceae810360130 on mdouglass:master into 12ba49f4cc4bae730385c0a93d82bcb0fb810984 on aws:master.\n. \nCoverage decreased (-0.18%) to 94.4% when pulling d773f5917b9e80a175b49f24a9994742a0de0dc6 on trailing-whitespace into 223c37a22f3621bb9c5cf1580f67ae4bf72bd22e on master.\n. \nCoverage decreased (-0.18%) to 94.4% when pulling d773f5917b9e80a175b49f24a9994742a0de0dc6 on trailing-whitespace into 223c37a22f3621bb9c5cf1580f67ae4bf72bd22e on master.\n. \nCoverage increased (+0.0%) to 94.4% when pulling 347296d7c6731609bb4ac0d64eca95ebb8815939 on fix-xml-map-parser into 81fbc964be694e0a57dab3f17f43be5f1530593c on master.\n. \nCoverage increased (+0.0%) to 94.4% when pulling 347296d7c6731609bb4ac0d64eca95ebb8815939 on fix-xml-map-parser into 81fbc964be694e0a57dab3f17f43be5f1530593c on master.\n. \nCoverage decreased (-0.03%) to 94.37% when pulling 209f589f201830188daf0ab80af5a87401eba04f on fix-managed-upload into 81fbc964be694e0a57dab3f17f43be5f1530593c on master.\n. \nCoverage decreased (-0.14%) to 94.41% when pulling c8e6464ffedff587e5d0ab5a8177f03616f032a4 on fix-rest-json-parser into f47bd3097addba7eb32b58af509fc5473cd20ef7 on master.\n. \nCoverage increased (+0.18%) to 94.59% when pulling 61c9fc9adbc5fc15ef603b7e50b82288ad00ab54 on fix-browser-builder into 01ff101547ccf819b33db71eafea34e6d66fccfe on master.\n. \nCoverage remained the same at 94.41% when pulling 68e0a46e334039fd363d52e352af0d673ea38ba6 on fix-global-reference into 01ff101547ccf819b33db71eafea34e6d66fccfe on master.\n. \nCoverage increased (+0.01%) to 94.41% when pulling 675dcce04433aa6b2a13e0e35f0c950b890c4274 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.01%) to 94.41% when pulling 675dcce04433aa6b2a13e0e35f0c950b890c4274 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.01%) to 94.41% when pulling 20da4ea434b8f9c264d5de5212ad018e85a44eed on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.01%) to 94.41% when pulling 20da4ea434b8f9c264d5de5212ad018e85a44eed on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.01%) to 94.41% when pulling feeb00024a278a2d7e602f3169ef17e175f42bb8 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.01%) to 94.41% when pulling feeb00024a278a2d7e602f3169ef17e175f42bb8 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.01%) to 94.41% when pulling 04eb88ccaec74a973fbb5be7a38caa2d6a0200a9 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.01%) to 94.41% when pulling 04eb88ccaec74a973fbb5be7a38caa2d6a0200a9 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.01%) to 94.41% when pulling c1b901e9c7086e88c6c89c10c2746b501189f865 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.01%) to 94.41% when pulling c1b901e9c7086e88c6c89c10c2746b501189f865 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage decreased (-0.02%) to 94.38% when pulling 7ea67657e3364befa40860f4ab00845ae75b84e5 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage decreased (-0.02%) to 94.38% when pulling 7ea67657e3364befa40860f4ab00845ae75b84e5 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.02%) to 94.43% when pulling e49dc377d9070ef8bad34c3dfdd1084c8ea63102 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.02%) to 94.43% when pulling e49dc377d9070ef8bad34c3dfdd1084c8ea63102 on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.02%) to 94.43% when pulling 851547284a6ac9a76f4a5481396c61e0d634e57c on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.02%) to 94.43% when pulling 851547284a6ac9a76f4a5481396c61e0d634e57c on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage increased (+0.2%) to 94.61% when pulling 41aae3a81644255b575731c31c520d10e8330f4a on fix-protocol-tests into 26f5750d98f774c72cc74aabd9bc084b7e9628a4 on master.\n. \nCoverage remained the same at 94.43% when pulling 2dad42aa1f13d00c8bda06f22339738e2387cdef on add-bower-instructions into 45a1bd64146469899ed536f3051f358167577875 on master.\n. \nCoverage remained the same at 94.43% when pulling fe202bd1684d1514edda20eb8259edcb78fb1600 on add-bower-instructions into 45a1bd64146469899ed536f3051f358167577875 on master.\n. \nCoverage remained the same at 94.43% when pulling 65656f49b8ebcd1c4760adf56cde5c4fc1f46722 on mick:dns_error_retryable into eb83937b0dafec81cb0ac2a0b94b0ab426dd9a26 on aws:master.\n. \nCoverage decreased (-0.0%) to 94.43% when pulling c2aeff5b840ff1044427c6ccb7dbe08eb3178be3 on shino:fix-subresource-to-sign into eb83937b0dafec81cb0ac2a0b94b0ab426dd9a26 on aws:master.\n. \nCoverage decreased (-0.0%) to 94.43% when pulling 2930831885a786014fedaa4c2ae13ff77e52cb9a on shino:fix-subresource-to-sign into eb83937b0dafec81cb0ac2a0b94b0ab426dd9a26 on aws:master.\n. \nCoverage decreased (-0.18%) to 94.43% when pulling 55efd5750bb2b9abeb0e5f9c86e111c79906a6cd on enotfound-retryable into 11ba34cffa28da18513da7fa6776070238e53cce on master.\n. \nCoverage decreased (-0.18%) to 94.43% when pulling 66fe5d2b7afcc6050adac4a91346d8993720be72 on joscha:patch-1 into 0f9ce7212334026f983df4227cd79146b4476d6f on aws:master.\n. \nCoverage increased (+0.18%) to 94.61% when pulling 855100c2ce87d2b3dd7a6cb11993d160a6bdf2e5 on node-12-support into 55e2be7ded7015b79639f85acf999443c405cac8 on master.\n. \nCoverage decreased (-0.03%) to 94.59% when pulling 1c2cee19c6bd98544a1314eae6065dc57326ccd7 on metadata-service into 69f05c427f503caea6845dabaf9183a0bb181846 on master.\n. \nCoverage decreased (-0.17%) to 94.45% when pulling b0bb7febb72fbdba62b94745f07cb12ca7815598 on lazy-credential-clients into cff2ba13be0cf89a8fa51925865fe44cb1d4a675 on master.\n. \nCoverage increased (+0.01%) to 94.62% when pulling ff1fd2c5f2cbcd58e18f508d5708ab9f1553a732 on lazy-credential-clients into cff2ba13be0cf89a8fa51925865fe44cb1d4a675 on master.\n. \nCoverage increased (+0.01%) to 94.62% when pulling ff1fd2c5f2cbcd58e18f508d5708ab9f1553a732 on lazy-credential-clients into cff2ba13be0cf89a8fa51925865fe44cb1d4a675 on master.\n. \nCoverage decreased (-0.18%) to 94.43% when pulling cc9ffc176863797f1717ac44e71d2eb39f216ec0 on daguej:throttle-fix into cff2ba13be0cf89a8fa51925865fe44cb1d4a675 on aws:master.\n. \nCoverage increased (+0.01%) to 94.62% when pulling 15697e5c8a4d30f6cbf3fdaa9f16c7a5329740df on daguej:throttle-fix into cff2ba13be0cf89a8fa51925865fe44cb1d4a675 on aws:master.\n. \nCoverage increased (+0.01%) to 94.62% when pulling 15697e5c8a4d30f6cbf3fdaa9f16c7a5329740df on daguej:throttle-fix into cff2ba13be0cf89a8fa51925865fe44cb1d4a675 on aws:master.\n. \nCoverage decreased (-0.18%) to 94.44% when pulling ef62d9fca13c5c0cd8cb273d5ba932dd145c0401 on add-throttling-errors into 42811f911c51c02806a408092b2b84220bf9c785 on master.\n. \nCoverage increased (+0.18%) to 94.63% when pulling 30e6420b8fe23968aa62c979407a1309e722aa86 on se-refactor into 4cf7661ffcae0a24ba07f1bfbab3c0ff8ddfe93f on master.\n. \nCoverage increased (+0.18%) to 94.63% when pulling 30e6420b8fe23968aa62c979407a1309e722aa86 on se-refactor into 4cf7661ffcae0a24ba07f1bfbab3c0ff8ddfe93f on master.\n. \nCoverage increased (+0.6%) to 95.04% when pulling 2c8bddddc4683e4f5e021e9907f17ccac3e37422 on compute-sha256 into a9468fe070d22660aa62a156bc38f48eec2da078 on master.\n. \nCoverage increased (+0.18%) to 94.62% when pulling 1fd12b25a199ae29d0fdcc239f7be50ae989e487 on s3-0b-upload into a9468fe070d22660aa62a156bc38f48eec2da078 on master.\n. \nCoverage increased (+0.6%) to 95.04% when pulling 53bce85dcc0994b13d4fe209c2907296b14a3ee9 on compute-sha256 into a9468fe070d22660aa62a156bc38f48eec2da078 on master.\n. \nCoverage increased (+0.6%) to 95.04% when pulling 53bce85dcc0994b13d4fe209c2907296b14a3ee9 on compute-sha256 into a9468fe070d22660aa62a156bc38f48eec2da078 on master.\n. \nCoverage increased (+0.18%) to 94.62% when pulling 23708e5b4f65a058763ee7fa63fae8a808c445c4 on update-xml2js into a9468fe070d22660aa62a156bc38f48eec2da078 on master.\n. \nCoverage increased (+0.18%) to 94.62% when pulling 23708e5b4f65a058763ee7fa63fae8a808c445c4 on update-xml2js into a9468fe070d22660aa62a156bc38f48eec2da078 on master.\n. \nChanges Unknown when pulling 8eaaa6c006fa4d836df3fb185e35a6bbba65ee05 on only-mocha-tests into * on master*.\n. \nChanges Unknown when pulling b346e461e29a9b5f0f7c1458d374a59d9459645a on only-mocha-tests into * on master*.\n. \nCoverage increased (+0.0%) to 95.06% when pulling c011d8e4f2a31784d9de0d3f8d0741150f2ae8a4 on presign-sigv4 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage increased (+0.0%) to 95.06% when pulling c011d8e4f2a31784d9de0d3f8d0741150f2ae8a4 on presign-sigv4 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage increased (+0.0%) to 95.06% when pulling c011d8e4f2a31784d9de0d3f8d0741150f2ae8a4 on presign-sigv4 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling e3974e90b3e3910f96813525deba743488bbb3de on presign-sigv4 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling e3974e90b3e3910f96813525deba743488bbb3de on presign-sigv4 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage remained the same at 95.05% when pulling e6eeee15cb055c71671c0cd4e7e5d4f35c0fcc3d on presign-sigv4 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage remained the same at 95.05% when pulling e6eeee15cb055c71671c0cd4e7e5d4f35c0fcc3d on presign-sigv4 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage remained the same at 95.05% when pulling e6eeee15cb055c71671c0cd4e7e5d4f35c0fcc3d on presign-sigv4 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage remained the same at 95.05% when pulling e6eeee15cb055c71671c0cd4e7e5d4f35c0fcc3d on presign-sigv4 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage remained the same at 95.05% when pulling 0210b0a71f893bd3d16390e7ebe9231da8d18ad4 on fix-credential-provider-docs into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage remained the same at 95.05% when pulling 0210b0a71f893bd3d16390e7ebe9231da8d18ad4 on fix-credential-provider-docs into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on master.\n. \nCoverage remained the same at 95.05% when pulling 381aaf8db590b6422a897814bd8387e3789017ff on PeterDaveHello:patch-1 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on aws:master.\n. \nCoverage remained the same at 95.05% when pulling 381aaf8db590b6422a897814bd8387e3789017ff on PeterDaveHello:patch-1 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on aws:master.\n. \nCoverage remained the same at 95.05% when pulling 381aaf8db590b6422a897814bd8387e3789017ff on PeterDaveHello:patch-1 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on aws:master.\n. \nCoverage remained the same at 95.05% when pulling 381aaf8db590b6422a897814bd8387e3789017ff on PeterDaveHello:patch-1 into c08cac01ae9b7a29f5f28783d308d440b9c62d9b on aws:master.\n. \nCoverage remained the same at 95.05% when pulling c2cb2f465d194b37521403761e13d145e848f1bc on refactor-integ-test into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage remained the same at 95.05% when pulling c2cb2f465d194b37521403761e13d145e848f1bc on refactor-integ-test into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling ff8f2f1da703bd002578a59b416d1dc1bbfffd95 on refactor-integ-test into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling ff8f2f1da703bd002578a59b416d1dc1bbfffd95 on refactor-integ-test into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling ff8f2f1da703bd002578a59b416d1dc1bbfffd95 on refactor-integ-test into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling ff8f2f1da703bd002578a59b416d1dc1bbfffd95 on refactor-integ-test into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling 9f0705da092191ddaef9f35e1593bed04547ac92 on glacier-sigv4 into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling 9f0705da092191ddaef9f35e1593bed04547ac92 on glacier-sigv4 into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling 1046116a662e2ba5317b5e644756699b1c4f62c0 on glacier-sigv4 into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling 1046116a662e2ba5317b5e644756699b1c4f62c0 on glacier-sigv4 into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling 1046116a662e2ba5317b5e644756699b1c4f62c0 on glacier-sigv4 into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling 1046116a662e2ba5317b5e644756699b1c4f62c0 on glacier-sigv4 into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling 293937e2a1bd6b65afa189c6400798afcf462f2a on refactor-integ-test into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling 293937e2a1bd6b65afa189c6400798afcf462f2a on refactor-integ-test into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling 293937e2a1bd6b65afa189c6400798afcf462f2a on refactor-integ-test into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage increased (+0.0%) to 95.05% when pulling 293937e2a1bd6b65afa189c6400798afcf462f2a on refactor-integ-test into 413e00e916d972cf8870327b58609b7c2b2d37bf on master.\n. \nCoverage remained the same at 95.05% when pulling 4cfb0be950fa9a66f244beef21cce6388b87a75a on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage remained the same at 95.05% when pulling 4cfb0be950fa9a66f244beef21cce6388b87a75a on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage remained the same at 95.05% when pulling 4cfb0be950fa9a66f244beef21cce6388b87a75a on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage remained the same at 95.05% when pulling 8f0c02fc109dfb813674300c23c58f6561654043 on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage decreased (-0.18%) to 94.88% when pulling 0a8c2c89b8d64442d71b66eb1f105d7dff25e778 on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage decreased (-0.18%) to 94.88% when pulling 52f20abfa9c53c902d174f03143dd74653d775f1 on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage increased (+0.0%) to 95.06% when pulling eae80e62f7cad984df8d4ffc20608db3228228d6 on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage decreased (-0.18%) to 94.88% when pulling b3ea4fe20b2c4ac54dbc589ffa32f00c784429e2 on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage increased (+0.0%) to 95.06% when pulling 7e86ff5155e7f1a17616306af5c37ac432019884 on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage decreased (-0.18%) to 94.87% when pulling 61d7e9cd2ee892b3d15e7ef0d5caa54cd7bf9ebf on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage decreased (-0.36%) to 94.7% when pulling c2b1ae1dd934f114d07b292fcb2d515d4dfa7e2c on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage decreased (-0.18%) to 94.87% when pulling ff454b6e57a4e6f99df554c9646640af3f517796 on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage decreased (-0.18%) to 94.87% when pulling 2fde083aeb0bbc40b048987ad3096a9a9c315347 on refactor-integ-test into 71a3c0abbfe06f2382de321dceeecf252ed1d6c7 on master.\n. \nCoverage increased (+0.0%) to 95.06% when pulling 66ddb832452231fc10b156ee8bf0fe4d53aa0399 on chaosgame:nathan-disableProgressEvents into 4e1ac91600a7db65f42c0ccfbb6fef07d7fe0681 on aws:master.\n. \nCoverage decreased (-0.18%) to 94.88% when pulling b530d00e2796c7fcb654b88515bf41326ebe0d3f on adrianblynch:patch-1 into ff0729150b7a0ea2d12b5482f6e853ff42321e96 on aws:master.\n. \nCoverage decreased (-0.18%) to 94.88% when pulling b530d00e2796c7fcb654b88515bf41326ebe0d3f on adrianblynch:patch-1 into ff0729150b7a0ea2d12b5482f6e853ff42321e96 on aws:master.\n. \nCoverage decreased (-0.18%) to 94.88% when pulling b530d00e2796c7fcb654b88515bf41326ebe0d3f on adrianblynch:patch-1 into ff0729150b7a0ea2d12b5482f6e853ff42321e96 on aws:master.\n. \nCoverage decreased (-0.18%) to 94.88% when pulling b530d00e2796c7fcb654b88515bf41326ebe0d3f on adrianblynch:patch-1 into ff0729150b7a0ea2d12b5482f6e853ff42321e96 on aws:master.\n. \nCoverage remained the same at 95.06% when pulling 2044b68652db32ca56cc661636d178edeb691d46 on fix/put-empty-bucket-notifications into ff0729150b7a0ea2d12b5482f6e853ff42321e96 on master.\n. \nCoverage decreased (-0.37%) to 94.69% when pulling c6f3effb6e3d1794280ab4eaba9647def22daeb6 on fix/zero-byte-and-perf-streaming-upload into 1027de1e1318c40142d404cfef283e257776fa4e on master.\n. \nCoverage decreased (-0.3%) to 94.76% when pulling b496595f90455e4afadcf5a5dd58386a925f1db7 on fix/zero-byte-and-perf-streaming-upload into 1027de1e1318c40142d404cfef283e257776fa4e on master.\n. \nCoverage decreased (-0.18%) to 94.87% when pulling b496595f90455e4afadcf5a5dd58386a925f1db7 on fix/zero-byte-and-perf-streaming-upload into 1027de1e1318c40142d404cfef283e257776fa4e on master.\n. \nCoverage remained the same at 94.87% when pulling 376c260e5baa0f5e7f48f768e45bf1ef1a615107 on drone-yml into a53b30f24f03c482e2cbd0fdf0abf65f102e27ed on master.\n. \nCoverage remained the same at 94.87% when pulling faaf0fb93a258c23ff2877751c615c41689a3c4a on fix/managed-upload-unit-tests into a53b30f24f03c482e2cbd0fdf0abf65f102e27ed on master.\n. \nCoverage remained the same at 94.87% when pulling 22dee71433305f3d04b4b70b574aca9e6a4dc7d3 on mixmaxhq:jeff/add_phantomjs into a53b30f24f03c482e2cbd0fdf0abf65f102e27ed on aws:master.\n. \nCoverage remained the same at 94.87% when pulling a13e6fbc8a677bf6b5ebb95755cc697224a44e80 on mixmaxhq:jeff/add_phantomjs into aabfbea4ebf2b01c06f7b1c9c1c1bcb07b92b8c8 on aws:master.\n. \nCoverage decreased (-0.18%) to 94.7% when pulling 0bbd76ddb39722dac6b20de5152d05cfe1b2f155 on mixmaxhq:jeff/add_phantomjs into aabfbea4ebf2b01c06f7b1c9c1c1bcb07b92b8c8 on aws:master.\n. \nCoverage increased (+0.0%) to 94.88% when pulling 8770fbe1cbb7bfb1b22718994451233d2639023a on mixmaxhq:jeff/clear_cached_identity into 4c24ec05ad3b32a856267feae67aa98449dfd0bc on aws:master.\n. \nCoverage remained the same at 94.7% when pulling 38cebbc9dc80b99b8580568395caa2c93566bd2c on fix/iso-8601-precision into f8fe0b0cac4f7131f12c9a9b1f5cc0374b7d7e84 on master.\n. \nCoverage increased (+0.05%) to 94.92% when pulling ebd10c8a37c366361a91f9f8644babd53731ee4f on fix/getobject-memory-leak-node-0.12.x into 351a5441f43a90ec658ecd4075765e1a77131063 on master.\n. \nCoverage decreased (-0.18%) to 94.7% when pulling e3dafd983261e0e7692c12dc3fbe49765f44e26e on pgilad:patch-1 into 351a5441f43a90ec658ecd4075765e1a77131063 on aws:master.\n. \nCoverage remained the same at 94.74% when pulling 24ce52f2a05b00a99258e0b8245e041cc0c201a6 on add/browser-integration-tests into 02164d0a77bd07051e9e1fcd6fabf8e9e7cc9129 on master.\n. \nCoverage increased (+0.18%) to 94.92% when pulling 141e9500c19d76e1bf2bf7d0efa67ecdcad66da6 on add/browser-integration-tests into 02164d0a77bd07051e9e1fcd6fabf8e9e7cc9129 on master.\n. \nCoverage increased (+0.18%) to 94.92% when pulling 5a29f664310834377a9287279a7e96d59ac2c7d3 on add/browser-integration-tests into 02164d0a77bd07051e9e1fcd6fabf8e9e7cc9129 on master.\n. \nCoverage increased (+0.18%) to 94.92% when pulling 06af0b9293bb2c3086bacfa3c1da8179c5d9ab0d on add/browser-integration-tests into 02164d0a77bd07051e9e1fcd6fabf8e9e7cc9129 on master.\n. \nCoverage remained the same at 94.74% when pulling 0df1ebddcbddd5a9a1b4f29852e73d6e24ad6013 on add/browser-integration-tests into 02164d0a77bd07051e9e1fcd6fabf8e9e7cc9129 on master.\n. \nCoverage increased (+0.09%) to 88.394% when pulling a290fb651dc775ca7ee1a68329ff82737ad7c6a0 on LiuJoyceC:moveToWaiters2 into 21c2b66d796cbe3df7c0ccb8a7b9658fd64f94c6 on aws:master.\n. \nCoverage increased (+0.3%) to 88.564% when pulling a3104515f169b11bc2390f30d9c21052a642ab4a on LiuJoyceC:moveToWaiters2 into f04eeedaf38b2cd06450fb909b68f3a4f784ce1e on aws:master.\n. \nCoverage remained the same at 88.295% when pulling 874087f8cff4a5c0fc651bd311d42b92c1518306 on docs/managed-uploader into cb5ff41d04a7237b32ec10bcb521a974e693f6bb on master.\n. \nCoverage increased (+0.006%) to 88.301% when pulling 87b06ec1269d54f1daab16421f6dc964e6870117 on LiuJoyceC:addUppercaseKeytoManagedUpload into a467169acf01e510d63cc3fce10e24a1aaa8452c on aws:master.\n. \nCoverage increased (+0.01%) to 88.311% when pulling 30d756693c7496e68beaaf285ab3f57304821550 on guymguym:s3-copyless-upload into 21c2b66d796cbe3df7c0ccb8a7b9658fd64f94c6 on aws:master.\n. \nCoverage increased (+0.01%) to 88.516% when pulling d8ecff079a086ea81fe669b52cabc2d86b0976f6 on guymguym:s3-copyless-upload into 1babda0c31df6f4dfe3618b686feb6a93fa8a322 on aws:master.\n. \nCoverage decreased (-0.04%) to 88.465% when pulling 8ba988f1c7396ab08978643edb095189c2ad232a on guymguym:s3-copyless-upload into 1babda0c31df6f4dfe3618b686feb6a93fa8a322 on aws:master.\n. \nCoverage decreased (-0.03%) to 88.474% when pulling ea627db61b338d2dd45ad86a28e0311ef94524fb on guymguym:s3-copyless-upload into 1babda0c31df6f4dfe3618b686feb6a93fa8a322 on aws:master.\n. \nCoverage decreased (-0.03%) to 88.474% when pulling 82ae3729b78d7478cfa8289a9944233d952b9256 on guymguym:s3-copyless-upload into 1babda0c31df6f4dfe3618b686feb6a93fa8a322 on aws:master.\n. \nCoverage increased (+0.04%) to 88.55% when pulling d24aa4aa7df8d4dcfdbc796e142ac8a653083118 on guymguym:s3-copyless-upload into 1babda0c31df6f4dfe3618b686feb6a93fa8a322 on aws:master.\n. \n\nCoverage decreased (-0.04%) to 88.465% when pulling 8ba988f1c7396ab08978643edb095189c2ad232a on guymguym:s3-copyless-upload into 1babda0c31df6f4dfe3618b686feb6a93fa8a322 on aws:master.\n. \nCoverage increased (+0.003%) to 88.304% when pulling af4bbbc5a97147af828bf7828753afe8d7e96196 on LiuJoyceC:s3Err200Status into 21c2b66d796cbe3df7c0ccb8a7b9658fd64f94c6 on aws:master.\n. \nCoverage remained the same at 88.301% when pulling 47a5d1fcb914f7465c38346fabbbd4c7c5c8cebc on docs/credentials into 21c2b66d796cbe3df7c0ccb8a7b9658fd64f94c6 on master.\n. \nCoverage remained the same at 88.304% when pulling e36514815950f69a1a1a159104c85eb337431460 on maowug:fix-dynamodb-doc-CDATA into f04eeedaf38b2cd06450fb909b68f3a4f784ce1e on aws:master.\n. \nCoverage increased (+0.02%) to 88.505% when pulling c745978f7be62fbde2b126f1946263bccaed864c on fix/getsdk into 266875c2e6a6ef5f9ff875e2cdaf36e10018d2da on master.\n. \nCoverage remained the same at 88.505% when pulling eb85b9e49ceca75b99e8a4a9d7242d8128533c1d on LiuJoyceC:CredentialProviderChainDefaultDocumentation into 5b49f61185f1b0f56569fd5cab858b5e74cbd9f4 on aws:master.\n. \nCoverage decreased (-0.005%) to 88.5% when pulling a491fd36303aeadcc907ba13118b845a91992902 on add/assume-role-profile into d8724e679857dc6b19867db0145f5a216eec948b on master.\n. \nCoverage decreased (-0.005%) to 88.5% when pulling 4aefe845a68f61fb768be22697f93906a953103c on add/assume-role-profile into d8724e679857dc6b19867db0145f5a216eec948b on master.\n. \nCoverage remained the same at 88.505% when pulling bcec759494ce6fcb69d8071283be20cad14beb8a on LiuJoyceC:cognitoIdentityCredentialsDoc into d8724e679857dc6b19867db0145f5a216eec948b on aws:master.\n. \nCoverage increased (+0.04%) to 88.55% when pulling a0a2e8b46af46a5230e3a25bda957488ca163bb0 on LiuJoyceC:cognitoIdentityCredentialsDoc into d8724e679857dc6b19867db0145f5a216eec948b on aws:master.\n. \nCoverage remained the same at 88.5% when pulling caafeb63111bdf4fdf8ad0dc27c81abc4dd5c09b on rwaldron:967 into c50bf0b51a59bae40d1ab785f9b3aa44546c46a6 on aws:master.\n. \nCoverage remained the same at 88.5% when pulling 8e10e42497a4fa1563c47b523594823241fd9508 on rwaldron:967 into c50bf0b51a59bae40d1ab785f9b3aa44546c46a6 on aws:master.\n. \nCoverage remained the same at 88.55% when pulling e7ab2d2f576d12db5e9532225a75ba8e6d2c01fc on LiuJoyceC:accelerateSignature into de7622da941334252b300de73f2a50febd672b3c on aws:master.\n. \nCoverage increased (+0.03%) to 88.579% when pulling 12c31bb981097c8c81e80d1889f4ea45e2d93021 on LiuJoyceC:cloudSearchDomainGetToPost into a1027ec0e22a447426b66b5274b59346800ffa25 on aws:master.\n. \nCoverage increased (+0.03%) to 88.579% when pulling be1052ceab3beebe7d3ace21328e1d3c390f805a on LiuJoyceC:cloudSearchDomainGetToPost into 0bfea057945229105f2c7024ff3a8bc8444120fa on aws:master.\n. \nCoverage increased (+0.003%) to 88.543% when pulling 4b6034fceaceb34b464461951903702366a5e8e7 on LiuJoyceC:dynamoCrc32ErrorDataNull into 0529f3981b156fdb9c64b5584992090d60d34d4c on aws:master.\n. \nCoverage remained the same at 88.54% when pulling 8ab9b6a70f451490b602db812098434040d7323b on LiuJoyceC:nodeReplSendOnceOnly into 3ecb3b4dccaeee3fc25250a172aa05c6156c5d49 on aws:master.\n. \nCoverage decreased (-0.02%) to 88.522% when pulling 881ac0e6aa953882f37c0d46c17c92535bc5abb5 on s3/unsigned-payload into 898708262c83facebd86c93734eed4933d300a89 on master.\n. \nCoverage increased (+0.004%) to 88.547% when pulling 285c802ca88994845a7f70b9e914e178bb7d7b99 on s3/unsigned-payload into 898708262c83facebd86c93734eed4933d300a89 on master.\n. \nCoverage increased (+0.06%) to 88.598% when pulling ed5182e53b049f0ef3044c6c2e291551e283cf1f on s3/sigv4-default into 898708262c83facebd86c93734eed4933d300a89 on master.\n. \nCoverage increased (+0.08%) to 88.627% when pulling d1c49d24d83fe92b90c203044acbabed88e39362 on s3/sigv4-default into 898708262c83facebd86c93734eed4933d300a89 on master.\n. \nCoverage increased (+0.03%) to 88.569% when pulling e904e3c0a19ac102e2f63ca99d78a5f4cb13ec13 on winsome:master into 898708262c83facebd86c93734eed4933d300a89 on aws:master.\n. \nCoverage increased (+0.3%) to 88.861% when pulling 479a17c227c1d1148c959a47a5c56221fdb475a4 on LiuJoyceC:s3RegionRedirect into ad17e01a77e5959f0f5fdbd4ab2bc13dc3ae3c28 on aws:master.\n. \nCoverage increased (+0.3%) to 88.867% when pulling 4f0aaba6745dabdedf5cdbf7ecccd0e8fc87b2be on LiuJoyceC:s3RegionRedirect into ad17e01a77e5959f0f5fdbd4ab2bc13dc3ae3c28 on aws:master.\n. \nCoverage increased (+0.3%) to 88.864% when pulling 81ff7cd6cf7635a98df016fd97d86d82125d6791 on LiuJoyceC:s3RegionRedirect into 78a5d09c3815e0563269bfdb8b99853e5dfc8cf2 on aws:master.\n. \nCoverage increased (+0.01%) to 88.921% when pulling 8dabf42f15597af58265ee8083213312b6eb3b0d on chrisradek:fix/sigv4-getSignedUrl-metadata into 4f7751382529ed22ce06edcf6cb119ba3ca8bc1e on aws:master.\n. \nCoverage increased (+0.02%) to 88.927% when pulling 898df3d8369a8e7f1b1a944b946d41ba00264e6c on mdravida19:master into 4f7751382529ed22ce06edcf6cb119ba3ca8bc1e on aws:master.\n. \nCoverage increased (+0.02%) to 88.927% when pulling 89a7244d4303389c7016fdd4dde02dd68d736e34 on mdravida19:master into 4f7751382529ed22ce06edcf6cb119ba3ca8bc1e on aws:master.\n. \nCoverage increased (+0.1%) to 89.009% when pulling 959ae615150e65f9186332639b258130d0690e18 on mdravida19:master into 4f7751382529ed22ce06edcf6cb119ba3ca8bc1e on aws:master.\n. \nCoverage increased (+0.1%) to 89.009% when pulling 19f56142cd606202675a9387e3be7c5462f1e672 on mdravida19:master into 4f7751382529ed22ce06edcf6cb119ba3ca8bc1e on aws:master.\n. \nCoverage decreased (-0.05%) to 88.873% when pulling e57184541ce333fed7e8f8432b56fea0d714dbad on lytc:master into 88fba8c03654b620aa6158f2c851522d549d06ed on aws:master.\n. \nCoverage remained the same at 88.921% when pulling e57184541ce333fed7e8f8432b56fea0d714dbad on lytc:master into 88fba8c03654b620aa6158f2c851522d549d06ed on aws:master.\n. \nCoverage increased (+0.06%) to 88.979% when pulling aeb2650c724e417fae8c71916535a5007ee82b0c on chrisradek:allow-nonfile-streams-s3-sigv4 into 88fba8c03654b620aa6158f2c851522d549d06ed on aws:master.\n. \nCoverage increased (+0.06%) to 88.979% when pulling aeb2650c724e417fae8c71916535a5007ee82b0c on chrisradek:allow-nonfile-streams-s3-sigv4 into 88fba8c03654b620aa6158f2c851522d549d06ed on aws:master.\n. \nCoverage increased (+0.06%) to 88.979% when pulling aeb2650c724e417fae8c71916535a5007ee82b0c on chrisradek:allow-nonfile-streams-s3-sigv4 into 88fba8c03654b620aa6158f2c851522d549d06ed on aws:master.\n. \nCoverage increased (+0.06%) to 88.979% when pulling aeb2650c724e417fae8c71916535a5007ee82b0c on chrisradek:allow-nonfile-streams-s3-sigv4 into 88fba8c03654b620aa6158f2c851522d549d06ed on aws:master.\n. \nCoverage remained the same at 89.009% when pulling 1489e7f736e00ef41c17a0a8d066dc7b678c12b0 on mediaupstream:master into 078e478592d0d6a0a367731baf86eb850af00923 on aws:master.\n. \nCoverage increased (+0.003%) to 89.011% when pulling 9c2b2372bba7d07f3e0f4e9720cc5a629ba6507c on LiuJoyceC:s3RegionRedirect into 4da4aaf1f2ca9c94c0a4fabeb1c54c5f87f82b2e on aws:master.\n. \nCoverage increased (+0.008%) to 89.017% when pulling bb4e309599c637a55230f27db482b6bf5a952423 on LiuJoyceC:s3RegionRedirect into 4da4aaf1f2ca9c94c0a4fabeb1c54c5f87f82b2e on aws:master.\n. \nCoverage remained the same at 89.017% when pulling dcabbf55d86beb6a1bbb3046c7d1ec14c1466021 on LiuJoyceC:changelog into 0f20de29728072dc6024f25d6a4e0b978dc6c2f3 on aws:master.\n. \nCoverage remained the same at 89.017% when pulling 57eaf8455963a6832ce4bbfb8cbf084b717703cd on LiuJoyceC:changelog into 0f20de29728072dc6024f25d6a4e0b978dc6c2f3 on aws:master.\n. \nCoverage remained the same at 89.017% when pulling 38bc99ae9d99b466b5574a895f88fc29a271d50d on LiuJoyceC:changelog into 9787eaa2357bbba9941b8ab64f89f0e632817577 on aws:master.\n. \nCoverage remained the same at 89.017% when pulling d14d290884d55c9f109c9846eb6011742ef50316 on LiuJoyceC:changelog into 9787eaa2357bbba9941b8ab64f89f0e632817577 on aws:master.\n. \nCoverage remained the same at 89.017% when pulling 2a74fe71f2d1b60738a1673d7e9934e997469c0a on LiuJoyceC:changelog into 9787eaa2357bbba9941b8ab64f89f0e632817577 on aws:master.\n. \nCoverage remained the same at 88.749% when pulling a9c843240c8aee309cc34b9b1e467898f207a313 on LiuJoyceC:changelog into 8ba78749c4b25eb9d125d10d82a1b1c24a639469 on aws:master.\n. \nCoverage remained the same at 88.808% when pulling da3e1c84f4a6194ac69dc6fd5e043bbe3bdfa049 on Avocarrot:fix/keepAlive into 466490f02c57dce312e8938aa076820a9699ac7d on aws:master.\n. \nCoverage remained the same at 88.808% when pulling cb6a9435816012d1840e3a23eefe49e9894f3582 on Jonnymcc:fix-parse-ini-comments into 466490f02c57dce312e8938aa076820a9699ac7d on aws:master.\n. \nCoverage remained the same at 88.808% when pulling 9baf00240cbce5032fa1831dc0a8a63548dcaedb on Jonnymcc:fix-parse-ini-comments into 466490f02c57dce312e8938aa076820a9699ac7d on aws:master.\n. \nCoverage increased (+0.06%) to 88.873% when pulling d2afa0ea4570d4304a1f43501d913828b181766b on LiuJoyceC:s3StreamEmitError into a97c2baf784a49bd46e938f3228db9398cb999c4 on aws:master.\n. \nCoverage increased (+0.06%) to 88.873% when pulling 370326ba3ee2e3d23cc92ac899fef11ace6b7721 on LiuJoyceC:s3StreamEmitError into a97c2baf784a49bd46e938f3228db9398cb999c4 on aws:master.\n. \nCoverage increased (+0.06%) to 88.87% when pulling 053609445a5cad182fead916a2f572c421bda6a8 on LiuJoyceC:s3StreamEmitError into a97c2baf784a49bd46e938f3228db9398cb999c4 on aws:master.\n. \nCoverage increased (+0.07%) to 88.876% when pulling 55493f83f1da773c195fc882584f4b2f4b61acdc on LiuJoyceC:s3StreamEmitError into a97c2baf784a49bd46e938f3228db9398cb999c4 on aws:master.\n. \nCoverage remained the same at 88.808% when pulling dc40b34f150affe005fddd37da34035755d56c6a on clifflu:fix/typo-refreshhing into 9334f4083e6e86c56df2dde7d53ce3eb15765fd7 on aws:master.\n. \nCoverage remained the same at 88.808% when pulling 53545c78b8b577ceb10830b273acecc59e723900 on valgaze:valgaze-patch-1 into f084bb078fbce7bf40c6ba302c2cce2689a0f28d on aws:master.\n. \nCoverage increased (+0.01%) to 88.822% when pulling 4f5a7f1750ef57ea4729048e9031cfdf611f1b52 on wryun:terminate-eachitem into f084bb078fbce7bf40c6ba302c2cce2689a0f28d on aws:master.\n. \nCoverage increased (+0.003%) to 88.811% when pulling a21feb54c657b6dbbaad88db643aa28d9c0484c6 on LiuJoyceC:s3UploadPromise into 4a404cb8c06bba6b7b00c323671376c6377889ed on aws:master.\n. \nCoverage increased (+0.1%) to 88.098% when pulling 9196554388fbad0f139f86b46fca3cbc4584f214 on LiuJoyceC:s3UploadPromise into 54a18bbd3caaefddc2ccfb747ab8443930ae1e6e on aws:master.\n. \nCoverage increased (+0.1%) to 88.098% when pulling 9196554388fbad0f139f86b46fca3cbc4584f214 on LiuJoyceC:s3UploadPromise into 54a18bbd3caaefddc2ccfb747ab8443930ae1e6e on aws:master.\n. \nCoverage increased (+0.07%) to 88.138% when pulling 4dc9bdc570bc930691584d597377700c9d86ad1d on LiuJoyceC:s3UploadPromise into 983211849bceb30630cb3391866959d8c68961c8 on aws:master.\n. \nCoverage increased (+0.09%) to 88.157% when pulling 82ad028512fbf1249d1b4daac4120905e7e650af on LiuJoyceC:s3UploadPromise into 983211849bceb30630cb3391866959d8c68961c8 on aws:master.\n. \nCoverage increased (+0.07%) to 88.137% when pulling 82ad028512fbf1249d1b4daac4120905e7e650af on LiuJoyceC:s3UploadPromise into 983211849bceb30630cb3391866959d8c68961c8 on aws:master.\n. \nCoverage increased (+0.08%) to 88.141% when pulling 3b760a1b284849599702004ec1113bf306416501 on LiuJoyceC:s3UploadPromise into a9a6032a8605e7af5f1162b961ba189082320b6b on aws:master.\n. \nCoverage increased (+0.08%) to 88.141% when pulling a075cab4a162ec49fda6c081977892f27142da97 on LiuJoyceC:s3UploadPromise into a9a6032a8605e7af5f1162b961ba189082320b6b on aws:master.\n. \nCoverage increased (+0.08%) to 88.141% when pulling a075cab4a162ec49fda6c081977892f27142da97 on LiuJoyceC:s3UploadPromise into a9a6032a8605e7af5f1162b961ba189082320b6b on aws:master.\n. \nCoverage decreased (-0.005%) to 88.87% when pulling b45c60bd267447b74e09a133aeb83d96363f0d2c on chrisradek:revert/s3-sigv4-default into d1823e8b0313ddee896981b584811c6d6a5654ff on aws:master.\n. \nCoverage remained the same at 88.87% when pulling 80e7a98a83d06848c457ddc44094cda5d6b0131e on gswalden:patch-1 into 692e005ace60b3b4725513499566f4d2ddf0ce5e on aws:master.\n. \nCoverage remained the same at 88.892% when pulling 1d1a7d4f8fa6170a0cfdfe3adaf6bbe9229ea972 on LiuJoyceC:yardocDualstack into f007442cc0aae250760ffad3ab34da6ac122a61c on aws:master.\n. \nCoverage increased (+0.3%) to 89.198% when pulling 44fb727a3dce699a2be8b1b1b5ffbc4f662ee462 on LiuJoyceC:metadataServiceRetry into 7e3c279acb0fefecac6b2cf41bbabea2e0ce3a3a on aws:master.\n. \nCoverage increased (+0.3%) to 89.198% when pulling 44fb727a3dce699a2be8b1b1b5ffbc4f662ee462 on LiuJoyceC:metadataServiceRetry into 7e3c279acb0fefecac6b2cf41bbabea2e0ce3a3a on aws:master.\n. \nCoverage increased (+0.3%) to 89.198% when pulling 44fb727a3dce699a2be8b1b1b5ffbc4f662ee462 on LiuJoyceC:metadataServiceRetry into 7e3c279acb0fefecac6b2cf41bbabea2e0ce3a3a on aws:master.\n. \nCoverage increased (+0.3%) to 89.198% when pulling bb6745d91d41e6007ddd01aa5241f2f61b04858d on LiuJoyceC:metadataServiceRetry into 7e3c279acb0fefecac6b2cf41bbabea2e0ce3a3a on aws:master.\n. \nCoverage increased (+0.03%) to 88.931% when pulling dcc6109724fb7c35ccbdf3fb2f3fee26ae88eb94 on chrisradek:webpack into 7e3c279acb0fefecac6b2cf41bbabea2e0ce3a3a on aws:master.\n. \nCoverage increased (+1.6%) to 90.535% when pulling a7f7882f302108918ed30efcce82cfa32b3646f3 on chrisradek:full-webpack into 02c54e1846ca4e9ebfd09982ce963dbc8bcc0969 on aws:master.\n. \nCoverage increased (+1.6%) to 90.535% when pulling a7f7882f302108918ed30efcce82cfa32b3646f3 on chrisradek:full-webpack into 02c54e1846ca4e9ebfd09982ce963dbc8bcc0969 on aws:master.\n. \nCoverage remained the same at 90.198% when pulling f6cb19da217fc7efc0d4bfce8d16be4ed0097042 on mediasilo:upload-to-edgecast into 4c768539749f4c3056d30a58fa0ddba28df5504c on aws:master.\n. \nCoverage remained the same at 90.198% when pulling 56f7c1998d95f52247b1244ae9f76f88b01c3cb7 on LiuJoyceC:yardocDebug into 4c768539749f4c3056d30a58fa0ddba28df5504c on aws:master.\n. \nCoverage remained the same at 90.198% when pulling 56f7c1998d95f52247b1244ae9f76f88b01c3cb7 on LiuJoyceC:yardocDebug into 4c768539749f4c3056d30a58fa0ddba28df5504c on aws:master.\n. \nCoverage remained the same at 90.198% when pulling f0deb259db6427663f505b718add67f337ecea1c on LiuJoyceC:fixIntermittentTemporalTestFailure into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \nCoverage remained the same at 90.198% when pulling f0deb259db6427663f505b718add67f337ecea1c on LiuJoyceC:fixIntermittentTemporalTestFailure into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \nCoverage remained the same at 90.198% when pulling 53b0da61c37a3828bf42fb090e75af8ad33ff32c on gurpreetatwal:master into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \nCoverage decreased (-0.04%) to 90.158% when pulling 085c5203fc25007c789361d023dab7852170b77a on gurpreetatwal:master into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \nCoverage decreased (-0.04%) to 90.158% when pulling 085c5203fc25007c789361d023dab7852170b77a on gurpreetatwal:master into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \nCoverage remained the same at 90.198% when pulling 085c5203fc25007c789361d023dab7852170b77a on gurpreetatwal:master into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \nCoverage remained the same at 90.198% when pulling 989111b7bb2be477b32b1d93ca90a5871c0c090a on gurpreetatwal:master into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \nCoverage remained the same at 90.198% when pulling 989111b7bb2be477b32b1d93ca90a5871c0c090a on gurpreetatwal:master into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \nCoverage remained the same at 90.198% when pulling 989111b7bb2be477b32b1d93ca90a5871c0c090a on gurpreetatwal:master into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \nCoverage remained the same at 90.198% when pulling 989111b7bb2be477b32b1d93ca90a5871c0c090a on gurpreetatwal:master into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \n\nCoverage remained the same at 88.135% when pulling 738b7e3faa57798eb6764635667d5bd1f83f0b1c on gurpreetatwal:master into 39c3a4704ea4c0fb23ee585fd8b703ec65b96a9a on aws:master.\n. \n\nCoverage decreased (-0.09%) to 88.042% when pulling 204f22779fe36504e1936d8f6d5dd42189e7fb4b on gurpreetatwal:master into 39c3a4704ea4c0fb23ee585fd8b703ec65b96a9a on aws:master.\n. \n\nCoverage decreased (-0.09%) to 88.042% when pulling ad0aadcff6e3b6252da5dc1c1fd88234ead4249c on gurpreetatwal:master into 39c3a4704ea4c0fb23ee585fd8b703ec65b96a9a on aws:master.\n. \nCoverage decreased (-2.4%) to 87.828% when pulling 662dd3d4d6780270cbd37620e8a5a2c45dd6eba0 on chrisradek:fix-service-config into 5a26680dbf734f312015b4a6700e894e3781717a on aws:master.\n. \nCoverage increased (+0.004%) to 90.202% when pulling 7c0d1648491f390a17cf0748527a4ebc8dae72c1 on fix-url-dep into 5a26680dbf734f312015b4a6700e894e3781717a on master.\n. \nCoverage increased (+0.004%) to 90.202% when pulling 64ca3ec466bb087f310202bea9f03b01a6027e9d on fix-url-dep into 5a26680dbf734f312015b4a6700e894e3781717a on master.\n. \nCoverage increased (+0.2%) to 87.993% when pulling ac7819d000abe5d759b643695a4636fd36c56378 on fix-fuzzy-versions into 513bdebb26739a298f522e58277e7d340afaa3f5 on master.\n. \nCoverage increased (+0.01%) to 88.002% when pulling 615b2ea1d1bd03c53155d1f22c68ed54b4ae47fb on LiuJoyceC:route53Retry into ee4148330a38d51f7806b7e014b11c9c0be3a5c5 on aws:master.\n. \nCoverage increased (+0.06%) to 88.062% when pulling d4df59f0f8dbe9d9d07cb3d1af3434ade68593c2 on jbergknoff:stream-error-propagation into b0de518dab2ceb08a62ad2abe59a222b62d90003 on aws:master.\n. \nCoverage increased (+0.06%) to 88.062% when pulling d4df59f0f8dbe9d9d07cb3d1af3434ade68593c2 on jbergknoff:stream-error-propagation into b0de518dab2ceb08a62ad2abe59a222b62d90003 on aws:master.\n. \nCoverage increased (+0.06%) to 88.062% when pulling ec0188fb7f392147d31f474f77d295fa4e27367a on jbergknoff:stream-error-propagation into b0de518dab2ceb08a62ad2abe59a222b62d90003 on aws:master.\n. \nCoverage increased (+0.06%) to 88.065% when pulling cbbf0c1192912e4175e9bbebe54ef85f4874d3dd on jbergknoff:stream-error-propagation into b0de518dab2ceb08a62ad2abe59a222b62d90003 on aws:master.\n. \nCoverage increased (+0.06%) to 88.065% when pulling 0794efa45f186e2d08311ae557034e14a3ca288d on jbergknoff:stream-error-propagation into 54a18bbd3caaefddc2ccfb747ab8443930ae1e6e on aws:master.\n. \nCoverage increased (+0.06%) to 88.065% when pulling 0794efa45f186e2d08311ae557034e14a3ca288d on jbergknoff:stream-error-propagation into 54a18bbd3caaefddc2ccfb747ab8443930ae1e6e on aws:master.\n. \nCoverage increased (+0.06%) to 88.065% when pulling 32ca3ff6594ee2848f28097d211b948bf95d5ad5 on jbergknoff:stream-error-propagation into 54a18bbd3caaefddc2ccfb747ab8443930ae1e6e on aws:master.\n. \nCoverage increased (+0.06%) to 88.065% when pulling ec99a248de1814dc9f700fb0ecb479c19f11a0f1 on add-dualstack-accelerate into 471dd1a4d3bcdbda3436c0f6c64c00a5cd544682 on master.\n. \nCoverage remained the same at 88.065% when pulling fe2c60fae4ac435cd13208685ed88b43d40738fc on binoculars:patch-1 into 8593f8dcc1830ea53d7f83ec3ef19ff46024e227 on aws:master.\n. \nCoverage increased (+0.01%) to 88.104% when pulling d11a69acebf312de15100cfe26b01b2e12d403b1 on wjordan:waitFor-custom into 9328c205edb859ff9d4f22ec943c9c56c69a1a6d on aws:master.\n. \nCoverage remained the same at 88.09% when pulling 063b14874a9ccfc4c0eb77663b0dabad17507645 on chrisradek:typescript into d8e171ce6021f7a7fe437af7ccdd6765d1080d77 on aws:master.\n. \nCoverage remained the same at 88.09% when pulling ad903e82abb4df08060dd4fe5618ebfae9d9ed3f on marcote:master into 17cd1cc529f34f74588227322f7d076c6de63b54 on aws:master.\n. \nCoverage remained the same at 88.09% when pulling fe130dd3ec1c43013ee4b04ba7e9796e77d717c5 on maghis:patch-1 into 9d5881ed04c223d8c7e1303bbe1cc81db2d9835c on aws:master.\n. \nCoverage remained the same at 88.09% when pulling 6ef5ae0d02bfe659b06e66711d7dac016c045383 on maghis:ts-managed-upload-promise into 02586f5d282bdf362e2ed7295efe4f6d756262a3 on aws:master.\n. \nCoverage remained the same at 88.09% when pulling ba6bae0b1fa2c4959cfaa40a5821ea8ab2d8869a on maghis:ts-managed-upload-promise into 02586f5d282bdf362e2ed7295efe4f6d756262a3 on aws:master.\n. \nCoverage remained the same at 88.111% when pulling 07c786b2f7f8ac9605550a9de23d87cd8c9ac677 on maghis:ts-managed-upload-promise into e5b340b8fd11c6aaec35fc338e7fb8107927b84a on aws:master.\n. \nCoverage remained the same at 88.111% when pulling f2b6348767d251c6d7e831b27819a5a49ff43673 on chrisradek:fix/typescript-config-issues into c8ecca09d4f354eeff527ff5925ac7501f322a18 on aws:master.\n. \n\nCoverage remained the same at 88.111% when pulling a2387a3026955b4183d587798ada414b914fc219 on chrisradek:fix/ts-child-classes into e02103e5f82d4251062cf09e3a38f8e1c3ac5dcc on aws:master.\n. \n\nCoverage decreased (-0.07%) to 88.042% when pulling 6cd8c0025e6cb20e48fb60356d7d5e908374482a on chrisradek:fix/ts-child-classes into e02103e5f82d4251062cf09e3a38f8e1c3ac5dcc on aws:master.\n. \n\nCoverage decreased (-0.07%) to 88.042% when pulling 6cd8c0025e6cb20e48fb60356d7d5e908374482a on chrisradek:fix/ts-child-classes into e02103e5f82d4251062cf09e3a38f8e1c3ac5dcc on aws:master.\n. \n\nCoverage remained the same at 88.042% when pulling a57e651ecfb51b38bc1b9c1ee55d00053a7eba84 on chrisradek:fix/ts-child-classes into 6c1e3fc74f5d58dda62e02c57c71554f77f5149d on aws:master.\n. \n\nCoverage remained the same at 88.111% when pulling bcd3ad59e6509e89ab5988510005c56f0ea97d51 on DaMouse404:config-fix into e02103e5f82d4251062cf09e3a38f8e1c3ac5dcc on aws:master.\n. \n\nCoverage increased (+0.002%) to 88.137% when pulling 397ce998ceeb6f7ec14d482ae76276ceb0a88b59 on erikerikson:master into 39c3a4704ea4c0fb23ee585fd8b703ec65b96a9a on aws:master.\n. \n\nCoverage decreased (-0.01%) to 88.12% when pulling 5b0defb6f7be83c577c550c13a634e6369447c06 on mdurrant:fix-safari-private into 39c3a4704ea4c0fb23ee585fd8b703ec65b96a9a on aws:master.\n. \n\nCoverage decreased (-0.01%) to 88.12% when pulling 81eb1f49cd2dd65518e7b2225da1ee34eb48bdaf on mdurrant:fix-safari-private into 39c3a4704ea4c0fb23ee585fd8b703ec65b96a9a on aws:master.\n. \n\nCoverage decreased (-0.01%) to 88.12% when pulling 81eb1f49cd2dd65518e7b2225da1ee34eb48bdaf on mdurrant:fix-safari-private into 39c3a4704ea4c0fb23ee585fd8b703ec65b96a9a on aws:master.\n. \n\nCoverage decreased (-0.7%) to 87.483% when pulling a6ae408a5badcc8edf24442aa1569cc9996fa520 on mdurrant:fix-safari-private into 39c3a4704ea4c0fb23ee585fd8b703ec65b96a9a on aws:master.\n. \n\nCoverage decreased (-0.6%) to 87.502% when pulling a6ae408a5badcc8edf24442aa1569cc9996fa520 on mdurrant:fix-safari-private into 39c3a4704ea4c0fb23ee585fd8b703ec65b96a9a on aws:master.\n. \n\nCoverage remained the same at 88.035% when pulling 74660f07ff1e058928bcb0b9f9cdd516c3a172c3 on chrisradek:remove-ts-types-ref into 5dba638fd7c58aeb776e7a5cc42c52c692314093 on aws:master.\n. \n\nCoverage remained the same at 88.042% when pulling 06a2bd9b0db6792524dd36e269bafa9bb6e44663 on chrisradek:ts-managed-upload-on into 6c1e3fc74f5d58dda62e02c57c71554f77f5149d on aws:master.\n. \n\nCoverage remained the same at 88.042% when pulling 6f893017fe79cbc38b01d89001ffbd62cbb256ff on goblindegook:1225-s3-body-readablestream into 63960f1efc422d778d9762612e99cad044758f0f on aws:master.\n. \n\nCoverage remained the same at 88.042% when pulling a33973349e90f5201a3c4aba8014472b87748c90 on goblindegook:1225-s3-body-readablestream into 63960f1efc422d778d9762612e99cad044758f0f on aws:master.\n. \n\nCoverage increased (+0.1%) to 88.147% when pulling 39cdc94201cef01ba40b9070da162dee5042d576 on jeskew:feature/add-execution-environment-to-user-agent into 685bffb77ba6bb3f1124f1785310661e1e061619 on aws:master.\n. \n\nCoverage increased (+0.01%) to 88.158% when pulling e996cd7a597b0e8c7fb62c28d96a57d73ba4e218 on chrisradek:update-base64-buffers into 6ffe8fe2777bea118bc2d950fa563cc180de66a4 on aws:master.\n. \n\nCoverage increased (+0.01%) to 88.158% when pulling e996cd7a597b0e8c7fb62c28d96a57d73ba4e218 on chrisradek:update-base64-buffers into 6ffe8fe2777bea118bc2d950fa563cc180de66a4 on aws:master.\n. \n\nCoverage increased (+0.01%) to 88.158% when pulling e996cd7a597b0e8c7fb62c28d96a57d73ba4e218 on chrisradek:update-base64-buffers into 6ffe8fe2777bea118bc2d950fa563cc180de66a4 on aws:master.\n. \n\nCoverage decreased (-0.004%) to 88.143% when pulling a22d99f8ba1d8c61302d79181d71b000a7fa7761 on chrisradek:update-base64-buffers into 6ffe8fe2777bea118bc2d950fa563cc180de66a4 on aws:master.\n. \n\nCoverage decreased (-0.004%) to 88.143% when pulling a22d99f8ba1d8c61302d79181d71b000a7fa7761 on chrisradek:update-base64-buffers into 6ffe8fe2777bea118bc2d950fa563cc180de66a4 on aws:master.\n. \n\nCoverage decreased (-0.3%) to 87.816% when pulling 888cf4a983cace13c492be08a31a09c82643f089 on jeskew:feature/persist-empty-document-members into 6ffe8fe2777bea118bc2d950fa563cc180de66a4 on aws:master.\n. \n\nCoverage increased (+3.8%) to 91.915% when pulling 339106f09842f37d01bd31b1a48391ec2e89deb3 on jeskew:feature/persist-empty-document-members into 6ffe8fe2777bea118bc2d950fa563cc180de66a4 on aws:master.\n. \n\nCoverage increased (+3.8%) to 91.901% when pulling d5f46866403b71c1db5ea65ed95d67484e409650 on jeskew:feature/persist-empty-document-members into 50b95419b312c1d9d3dee82c87443c853d91e3ce on aws:master.\n. \n\nCoverage increased (+3.7%) to 91.883% when pulling e1b13090804d65963788d01eeed0e529cffea40f on jeskew:feature/persist-empty-document-members into 50b95419b312c1d9d3dee82c87443c853d91e3ce on aws:master.\n. \n\nCoverage remained the same at 88.147% when pulling 062d37a890f07dabe96a1a6ece5c799b3c568c0a on jeskew:fix/leave-trace-id-header-unsigned into 6ffe8fe2777bea118bc2d950fa563cc180de66a4 on aws:master.\n. \n\nCoverage remained the same at 88.147% when pulling 062d37a890f07dabe96a1a6ece5c799b3c568c0a on jeskew:fix/leave-trace-id-header-unsigned into 6ffe8fe2777bea118bc2d950fa563cc180de66a4 on aws:master.\n. \n\nCoverage increased (+0.009%) to 88.152% when pulling 8471cf5f37875945ac59fb9907cd1544a758ff1d on jeskew:fix/ensure-cognito-identityId-always-loaded-from-cache-on-access into d0aa9db29be01cd909eec4780dffb9d182cde5e4 on aws:master.\n. \n\nCoverage increased (+0.01%) to 88.154% when pulling 2c42bfc45e6af5f749915a6628c1aa3598c62414 on jeskew:fix/ensure-cognito-identityId-always-loaded-from-cache-on-access into d0aa9db29be01cd909eec4780dffb9d182cde5e4 on aws:master.\n. \n\nCoverage increased (+0.003%) to 91.886% when pulling 17a1da28d56a8c70122fa738fa650ee064fc563d on chrisradek:fix-promise-null into 38d38d4292d3804c1b5d3934ee912fbb2e9c71a7 on aws:master.\n. \n\nCoverage increased (+0.003%) to 91.886% when pulling c54e69a1ae12d679739e9e12211fbec1ca487849 on chrisradek:fix-promise-null into 38d38d4292d3804c1b5d3934ee912fbb2e9c71a7 on aws:master.\n. \n\nCoverage decreased (-0.03%) to 91.9% when pulling 8008fbf81eb10576913b0128f7614fb21a25f2a9 on jeskew:fix/typeof-null-prototype into 4064bbe4006060100d0aeb91593d2d49eac858a9 on aws:master.\n. \n\nCoverage increased (+0.003%) to 91.938% when pulling abb8a56da05d18ae710ac167c020a0c1e7cd47ee on jeskew:fix/typeof-null-prototype into 4064bbe4006060100d0aeb91593d2d49eac858a9 on aws:master.\n. \n\nCoverage increased (+0.003%) to 91.938% when pulling e6c6727667e8edd372731fe0c8627a7a09cbdc53 on jeskew:fix/typeof-null-prototype into 4064bbe4006060100d0aeb91593d2d49eac858a9 on aws:master.\n. \n\nCoverage increased (+0.003%) to 91.901% when pulling 646a1e4f09fa15fb27d0785ca6cf534487630123 on jeskew:fix/s3-buffer-encoding into be97613cb5c682180493007b2c266b73707b94df on aws:master.\n. \n\nCoverage increased (+0.003%) to 91.901% when pulling 5bfd9fd31acbe411784b9283123d02fe447d5ca6 on jeskew:fix/s3-buffer-encoding into be97613cb5c682180493007b2c266b73707b94df on aws:master.\n. \n\nCoverage increased (+0.003%) to 91.901% when pulling 5bfd9fd31acbe411784b9283123d02fe447d5ca6 on jeskew:fix/s3-buffer-encoding into be97613cb5c682180493007b2c266b73707b94df on aws:master.\n. \n\nCoverage remained the same at 91.901% when pulling e8515a2c0c9a9054209d3408014d55dcd3cec951 on jeskew:fix/broken-node-bufferfrom into 7e2703e43596e7383e247bbee36d21fa63313ba5 on aws:master.\n. \n\nCoverage remained the same at 91.901% when pulling 9c7b27b0b5b48d1ae6c35f5c60c9fd2b08720eed on jeskew:fix/broken-node-bufferfrom into 7e2703e43596e7383e247bbee36d21fa63313ba5 on aws:master.\n. \n\nCoverage remained the same at 91.901% when pulling 4ef16c6031e669b2b4feabc43456a36eb47764e2 on jeskew:fix/broken-node-bufferfrom into 7e2703e43596e7383e247bbee36d21fa63313ba5 on aws:master.\n. \n\nCoverage increased (+0.003%) to 91.904% when pulling 8186891e36db171558fb28f7db5eb5ec84cdff19 on jeskew:fix/restrict-buffer-cast-to-s3 into 3ea51a63042aeffedf47a9e8de1d28d484ecc551 on aws:master.\n. \n\nCoverage decreased (-0.003%) to 91.898% when pulling cd61b3ff76dab4351a31c50a820338851a5b57c5 on jeskew:fix/restrict-buffer-cast-to-s3 into 3ea51a63042aeffedf47a9e8de1d28d484ecc551 on aws:master.\n. \n\nCoverage decreased (-0.003%) to 91.898% when pulling 2780569fd04aea71843aab84bbc8b61843cd5963 on jeskew:fix/restrict-buffer-cast-to-s3 into 3ea51a63042aeffedf47a9e8de1d28d484ecc551 on aws:master.\n. \n\nCoverage decreased (-0.003%) to 91.898% when pulling 2780569fd04aea71843aab84bbc8b61843cd5963 on jeskew:fix/restrict-buffer-cast-to-s3 into 3ea51a63042aeffedf47a9e8de1d28d484ecc551 on aws:master.\n. \n\nCoverage decreased (-0.003%) to 91.898% when pulling 2780569fd04aea71843aab84bbc8b61843cd5963 on jeskew:fix/restrict-buffer-cast-to-s3 into 3ea51a63042aeffedf47a9e8de1d28d484ecc551 on aws:master.\n. \n\nCoverage decreased (-0.003%) to 91.898% when pulling 2780569fd04aea71843aab84bbc8b61843cd5963 on jeskew:fix/restrict-buffer-cast-to-s3 into 3ea51a63042aeffedf47a9e8de1d28d484ecc551 on aws:master.\n. \n\nCoverage decreased (-0.003%) to 91.898% when pulling 2780569fd04aea71843aab84bbc8b61843cd5963 on jeskew:fix/restrict-buffer-cast-to-s3 into 3ea51a63042aeffedf47a9e8de1d28d484ecc551 on aws:master.\n. \n\nCoverage decreased (-0.003%) to 91.898% when pulling 2780569fd04aea71843aab84bbc8b61843cd5963 on jeskew:fix/restrict-buffer-cast-to-s3 into 3ea51a63042aeffedf47a9e8de1d28d484ecc551 on aws:master.\n. \n\nCoverage increased (+0.008%) to 91.906% when pulling 98e46946f7cf044bb5217181e57cbbf825fcd2d6 on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.008%) to 91.906% when pulling 98e46946f7cf044bb5217181e57cbbf825fcd2d6 on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.008%) to 91.906% when pulling 98e46946f7cf044bb5217181e57cbbf825fcd2d6 on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.912% when pulling 16d9d08f3842277ba6f0ed3dce0c690fc3a475bb on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.912% when pulling 16d9d08f3842277ba6f0ed3dce0c690fc3a475bb on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.912% when pulling 16d9d08f3842277ba6f0ed3dce0c690fc3a475bb on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.912% when pulling 16d9d08f3842277ba6f0ed3dce0c690fc3a475bb on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.912% when pulling 16d9d08f3842277ba6f0ed3dce0c690fc3a475bb on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.912% when pulling 16d9d08f3842277ba6f0ed3dce0c690fc3a475bb on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.912% when pulling 16d9d08f3842277ba6f0ed3dce0c690fc3a475bb on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.912% when pulling 16d9d08f3842277ba6f0ed3dce0c690fc3a475bb on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.912% when pulling 16d9d08f3842277ba6f0ed3dce0c690fc3a475bb on chrisradek:region-cognito-credentials into c40f0d6d914852a68d35af181d7981a44413fb41 on aws:master.\n. \n\nCoverage remained the same at 91.912% when pulling 47a5097cd673a002e47952058485874434d2bf86 on jeskew:docs/move-convertEmptyValues-documentation into f9f49cb76c33e4108ec7024d9e34e7bfdd6e857c on aws:master.\n. \n\nCoverage remained the same at 91.912% when pulling 51016be17ada052ed0df134bc9a61fc594a4926d on chrisradek:temporary-creds-specify-master into d72b723173a084669216a74b8f2d15b7c4b1da5c on aws:master.\n. \n\nCoverage increased (+0.008%) to 91.902% when pulling baa2276c589a39484bebad3834f471cb610df92c on chrisradek:fix/rds-presigned-url into 4e20b4ff4a3a7f98769d483c37b456d63bf39c29 on aws:master.\n. \n\nCoverage increased (+0.008%) to 91.902% when pulling baa2276c589a39484bebad3834f471cb610df92c on chrisradek:fix/rds-presigned-url into 4e20b4ff4a3a7f98769d483c37b456d63bf39c29 on aws:master.\n. \n\nCoverage increased (+0.008%) to 91.902% when pulling baa2276c589a39484bebad3834f471cb610df92c on chrisradek:fix/rds-presigned-url into 4e20b4ff4a3a7f98769d483c37b456d63bf39c29 on aws:master.\n. \n\nCoverage increased (+0.008%) to 91.902% when pulling baa2276c589a39484bebad3834f471cb610df92c on chrisradek:fix/rds-presigned-url into 4e20b4ff4a3a7f98769d483c37b456d63bf39c29 on aws:master.\n. \n\nCoverage increased (+0.008%) to 91.902% when pulling baa2276c589a39484bebad3834f471cb610df92c on chrisradek:fix/rds-presigned-url into 4e20b4ff4a3a7f98769d483c37b456d63bf39c29 on aws:master.\n. \n\nCoverage increased (+0.008%) to 91.902% when pulling baa2276c589a39484bebad3834f471cb610df92c on chrisradek:fix/rds-presigned-url into 4e20b4ff4a3a7f98769d483c37b456d63bf39c29 on aws:master.\n. \n\nCoverage increased (+0.008%) to 91.902% when pulling ceb5628a1435f6b1d5a13c4cc55deeda1df5caff on chrisradek:fix/rds-presigned-url into 4e20b4ff4a3a7f98769d483c37b456d63bf39c29 on aws:master.\n. \n\nCoverage remained the same at 91.902% when pulling 910f3f4fd1fa7f0beee245d1dfd6820409847020 on jeskew:docs/tests-in-prs into cbfc8cf6cf4561826d56175dde98e38bee6506d3 on aws:master.\n. \n\nCoverage decreased (-0.04%) to 91.863% when pulling 4e48b6b1223b8580effb224ef75c817d13962165 on jeskew:docs/tests-in-prs into cbfc8cf6cf4561826d56175dde98e38bee6506d3 on aws:master.\n. \n\nCoverage decreased (-0.04%) to 91.863% when pulling 224c6556b4053446cca329e78fc6df9de6d00e61 on jeskew:docs/tests-in-prs into cbfc8cf6cf4561826d56175dde98e38bee6506d3 on aws:master.\n. \n\nCoverage remained the same at 91.902% when pulling dc8be477e77fe40e104accae7e9234e56e4bffa3 on jaoromi:patch-1 into c8d55cb28105180f47e5e16cc48b613364791444 on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 4fe17d4186987f23e974c83f5cb43bace8544a24 on pe8ter:master into c59e4b7cfb20d7f14844fab3b8d63a3494542f7a on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 159a7687449dfea07310fa47ad22f0cf80109517 on RLovelett:bugfix/issue-1338 into 67f8e309fb2ab748880a826911573beaa0e7986c on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 159a7687449dfea07310fa47ad22f0cf80109517 on RLovelett:bugfix/issue-1338 into 67f8e309fb2ab748880a826911573beaa0e7986c on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 159a7687449dfea07310fa47ad22f0cf80109517 on RLovelett:bugfix/issue-1338 into 67f8e309fb2ab748880a826911573beaa0e7986c on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 159a7687449dfea07310fa47ad22f0cf80109517 on RLovelett:bugfix/issue-1338 into 67f8e309fb2ab748880a826911573beaa0e7986c on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 159a7687449dfea07310fa47ad22f0cf80109517 on RLovelett:bugfix/issue-1338 into 67f8e309fb2ab748880a826911573beaa0e7986c on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 159a7687449dfea07310fa47ad22f0cf80109517 on RLovelett:bugfix/issue-1338 into 67f8e309fb2ab748880a826911573beaa0e7986c on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 159a7687449dfea07310fa47ad22f0cf80109517 on RLovelett:bugfix/issue-1338 into 67f8e309fb2ab748880a826911573beaa0e7986c on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 159a7687449dfea07310fa47ad22f0cf80109517 on RLovelett:bugfix/issue-1338 into 67f8e309fb2ab748880a826911573beaa0e7986c on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 6c761de6b8e8b0b13151c923dc80349d1fd0d16d on RLovelett:bugfix/issue-1338 into 67f8e309fb2ab748880a826911573beaa0e7986c on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling 6c761de6b8e8b0b13151c923dc80349d1fd0d16d on RLovelett:bugfix/issue-1338 into 67f8e309fb2ab748880a826911573beaa0e7986c on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling c1d4af9f389acbb92a4f66693732a41d918d8bf1 on Swizec:fix-webpack-2 into 0642da0822e9699862a81e424af69152248bdf6a on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling c1d4af9f389acbb92a4f66693732a41d918d8bf1 on Swizec:fix-webpack-2 into 0642da0822e9699862a81e424af69152248bdf6a on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling bf8885e43f8b099c7684d6f023fa2186c978c131 on RLovelett:bugfix/s3-without-endpoint into 0642da0822e9699862a81e424af69152248bdf6a on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling bf8885e43f8b099c7684d6f023fa2186c978c131 on RLovelett:bugfix/s3-without-endpoint into 0642da0822e9699862a81e424af69152248bdf6a on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling bf8885e43f8b099c7684d6f023fa2186c978c131 on RLovelett:bugfix/s3-without-endpoint into 0642da0822e9699862a81e424af69152248bdf6a on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling bf8885e43f8b099c7684d6f023fa2186c978c131 on RLovelett:bugfix/s3-without-endpoint into 0642da0822e9699862a81e424af69152248bdf6a on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling bf8885e43f8b099c7684d6f023fa2186c978c131 on RLovelett:bugfix/s3-without-endpoint into 0642da0822e9699862a81e424af69152248bdf6a on aws:master.\n. \n\nCoverage remained the same at 91.863% when pulling bf8885e43f8b099c7684d6f023fa2186c978c131 on RLovelett:bugfix/s3-without-endpoint into 0642da0822e9699862a81e424af69152248bdf6a on aws:master.\n. \n\nCoverage remained the same at 91.83% when pulling 5d9be305dc1bae20db1b5468ffa2fb3409f6e030 on RLovelett:bugfix/s3-without-endpoint into d0fb1a0d61aa0e463be3e20879e7632d0a08872c on aws:master.\n. \n\nCoverage remained the same at 91.83% when pulling 5d9be305dc1bae20db1b5468ffa2fb3409f6e030 on RLovelett:bugfix/s3-without-endpoint into d0fb1a0d61aa0e463be3e20879e7632d0a08872c on aws:master.\n. \n\nCoverage remained the same at 91.83% when pulling 5d9be305dc1bae20db1b5468ffa2fb3409f6e030 on RLovelett:bugfix/s3-without-endpoint into d0fb1a0d61aa0e463be3e20879e7632d0a08872c on aws:master.\n. \n\nCoverage remained the same at 91.83% when pulling 5d9be305dc1bae20db1b5468ffa2fb3409f6e030 on RLovelett:bugfix/s3-without-endpoint into d0fb1a0d61aa0e463be3e20879e7632d0a08872c on aws:master.\n. \n\nCoverage increased (+0.006%) to 91.869% when pulling 1e90afe8454ea3e488ffb48174beeca7d2f350b5 on chrisradek:fix-cloud-directory into 0642da0822e9699862a81e424af69152248bdf6a on aws:master.\n. \n\nCoverage decreased (-0.05%) to 91.788% when pulling 925f035206b962f1c34b5c4384fac3bf84975f7a on jeskew:feature/presigned-post into e56cd9ea269edd457dbe9496b62c1c87374c31a3 on aws:master.\n. \n\nCoverage decreased (-0.05%) to 91.791% when pulling bcfe03b436cdc8175ff5ea053eca7612aa162e04 on jeskew:feature/presigned-post into e56cd9ea269edd457dbe9496b62c1c87374c31a3 on aws:master.\n. \n\nCoverage decreased (-0.05%) to 91.792% when pulling 200370a962b9953b0535226798a574f747db04e3 on jeskew:feature/presigned-post into e56cd9ea269edd457dbe9496b62c1c87374c31a3 on aws:master.\n. \n\nCoverage decreased (-0.05%) to 91.792% when pulling b0125c5b404d0d2898858a5cc4840995458877ad on jeskew:feature/presigned-post into e56cd9ea269edd457dbe9496b62c1c87374c31a3 on aws:master.\n. \n\nCoverage decreased (-0.05%) to 91.792% when pulling dcf2781e1a108269aafa75a3f6db8544033d9829 on jeskew:feature/presigned-post into e56cd9ea269edd457dbe9496b62c1c87374c31a3 on aws:master.\n. \n\nCoverage increased (+0.009%) to 91.839% when pulling 9b60ffe1f2a983752553ed7c50ebeb069a6a1315 on chrisradek:add-interface-logging into 2928856fdc4e051e1d19ae6be2622865db1464c3 on aws:master.\n. \n\nCoverage increased (+0.009%) to 91.839% when pulling 9b60ffe1f2a983752553ed7c50ebeb069a6a1315 on chrisradek:add-interface-logging into 2928856fdc4e051e1d19ae6be2622865db1464c3 on aws:master.\n. \n\nCoverage increased (+0.009%) to 91.839% when pulling 9b60ffe1f2a983752553ed7c50ebeb069a6a1315 on chrisradek:add-interface-logging into 2928856fdc4e051e1d19ae6be2622865db1464c3 on aws:master.\n. \n\nCoverage increased (+0.009%) to 91.839% when pulling 9b60ffe1f2a983752553ed7c50ebeb069a6a1315 on chrisradek:add-interface-logging into 2928856fdc4e051e1d19ae6be2622865db1464c3 on aws:master.\n. \n\nCoverage remained the same at 91.83% when pulling 3a4f015d36f9a05f79d46b326ad2c3df31caecb5 on jeskew:fix/die-phantomjs-die into 2928856fdc4e051e1d19ae6be2622865db1464c3 on aws:master.\n. \n\nCoverage remained the same at 91.83% when pulling 6bb81ad705fb1d17ce74610c6cd7d75892f11a91 on jeskew:fix/die-phantomjs-die into 2928856fdc4e051e1d19ae6be2622865db1464c3 on aws:master.\n. \n\nCoverage remained the same at 91.83% when pulling 6fca13ac20572ce17674c6a322acc00a59a4e234 on ehartford:master into 044852c6881bca04e34bf06d86054886568082b6 on aws:master.\n. \n\nCoverage remained the same at 91.83% when pulling 5c7ef694689cb42ea84a9ef22f61472db16a97d2 on robbiet480:patch-1 into 044852c6881bca04e34bf06d86054886568082b6 on aws:master.\n. \n\nCoverage remained the same at 91.839% when pulling 6abbbd55d05475eac8355ff810f4498ceab7f83d on jeskew:chore/sync-protocol-tests into d911f8df388279403fa9c3e6d2e615ca87ab1729 on aws:master.\n. \n\nCoverage remained the same at 91.841% when pulling 48e8eb15a96688491d19be5e2b672c6c20604ec1 on RLovelett:patch/1366-credential-provider-chain-resolution-null into 2ebc99b4c4c0ca59040f13ef343da6f2a90ca789 on aws:master.\n. \n\nCoverage increased (+0.003%) to 91.827% when pulling f79934aa80caa9fa4c5f14fa60a85cbfb47c1038 on jeskew:feature/expose-converter into 2b3b3555940c3ef9221c43703134ae1cf3118547 on aws:master.\n. \n\nCoverage increased (+0.3%) to 92.075% when pulling c2a798e3994510a90061719183678fda13f4c09b on jeskew:feature/expose-converter into 2b3b3555940c3ef9221c43703134ae1cf3118547 on aws:master.\n. \n\nCoverage increased (+0.3%) to 92.075% when pulling c2a798e3994510a90061719183678fda13f4c09b on jeskew:feature/expose-converter into 2b3b3555940c3ef9221c43703134ae1cf3118547 on aws:master.\n. \n\nCoverage increased (+0.3%) to 92.075% when pulling c2a798e3994510a90061719183678fda13f4c09b on jeskew:feature/expose-converter into 2b3b3555940c3ef9221c43703134ae1cf3118547 on aws:master.\n. \n\nCoverage increased (+0.3%) to 92.075% when pulling c2a798e3994510a90061719183678fda13f4c09b on jeskew:feature/expose-converter into 2b3b3555940c3ef9221c43703134ae1cf3118547 on aws:master.\n. \n\nCoverage remained the same at 91.778% when pulling ae198974e0f41cf357a1e31b2be5c99823cd7788 on chrisradek:add-onabort into e685e8124507465c344e181046eb025aa016b800 on aws:master.\n. \n\nCoverage remained the same at 91.635% when pulling d8f1b604cd8a8e46090bedd2e3635f72a3a239da on jeskew:docs/generate-service-list-to-separate-file into 975b315d9d6cd4dd4a3de727585bb3b81227b9f3 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.645% when pulling 061486a1da4d63d4ab4461f9fa66d1b1287fd19a on jeskew:fix/build-service-error-from-http-code-when-body-unparsable into 975b315d9d6cd4dd4a3de727585bb3b81227b9f3 on aws:master.\n. \n\nCoverage increased (+0.01%) to 91.648% when pulling f68b32887d7541c52cdcbc72ec7e767dc9c397d0 on jeskew:fix/build-service-error-from-http-code-when-body-unparsable into 3fc5ce68bacae6e90b8dac2cd91e579cf9172491 on aws:master.\n. \n\nCoverage decreased (-0.2%) to 91.473% when pulling b5351e057ca7024f3843444e9da36bfb28f89e43 on johanneswuerbach:fix-buf-size-eq-part-size into 975b315d9d6cd4dd4a3de727585bb3b81227b9f3 on aws:master.\n. \n\nCoverage decreased (-0.02%) to 91.619% when pulling d585eed670bca290b73eea667410312080b697d0 on johanneswuerbach:fix-buf-size-eq-part-size into 3fc5ce68bacae6e90b8dac2cd91e579cf9172491 on aws:master.\n. \n\nCoverage decreased (-0.02%) to 91.619% when pulling d585eed670bca290b73eea667410312080b697d0 on johanneswuerbach:fix-buf-size-eq-part-size into 3fc5ce68bacae6e90b8dac2cd91e579cf9172491 on aws:master.\n. \n\nCoverage increased (+0.09%) to 91.729% when pulling b982ac8276962e25bd98bc6ce544c535a0d660d1 on johanneswuerbach:fix-buf-size-eq-part-size into 3fc5ce68bacae6e90b8dac2cd91e579cf9172491 on aws:master.\n. \n\nCoverage remained the same at 91.635% when pulling 71546718ee76939e31e35b27418a0518d34bd432 on chrisradek:fix-doc-generator into 975b315d9d6cd4dd4a3de727585bb3b81227b9f3 on aws:master.\n. \n\nChanges Unknown when pulling f3c444e3922e841031d0b16be17dd1c46b343c20 on jeskew:docs/delete-dev-guide into  on aws:master.\n. \n\nCoverage remained the same at 91.635% when pulling f3c444e3922e841031d0b16be17dd1c46b343c20 on jeskew:docs/delete-dev-guide into 75899852176fae84a4101a07c61423ba8a55ea27 on aws:master.\n. \n\nCoverage increased (+0.005%) to 91.639% when pulling 3027dcb2ba2b7d13c21c82c6df8980f396243d1b on jeskew:fix/refresh-master-credentials into 3fc5ce68bacae6e90b8dac2cd91e579cf9172491 on aws:master.\n. \n\nCoverage decreased (-1.2%) to 90.445% when pulling e59c2410ada7c679da7c28301f7abd983d639b01 on jeskew:fix/refresh-master-credentials into 3fc5ce68bacae6e90b8dac2cd91e579cf9172491 on aws:master.\n. \n\nCoverage decreased (-0.02%) to 90.382% when pulling 3e7306095fa0249cda7bb02ac0327f96cdbfc1eb on jeskew:jans510-Add-AWS_CREDENTIAL_PROFILES_FILE-Environment-Variable into cd6f7281d3453b8b70e8a6bd4f2192aa5d3a2524 on aws:master.\n. \n\nCoverage decreased (-0.01%) to 90.392% when pulling ff2e841ef3ecbb9ae67043adfe602fafb587b020 on react-native into cd6f7281d3453b8b70e8a6bd4f2192aa5d3a2524 on master.\n. \n\nCoverage remained the same at 90.405% when pulling ff2e841ef3ecbb9ae67043adfe602fafb587b020 on react-native into cd6f7281d3453b8b70e8a6bd4f2192aa5d3a2524 on master.\n. \n\nCoverage remained the same at 90.405% when pulling 8949d99d1186f35b4b9cd333f520ef0959df9c6b on scrpatlolla:Enhancements into cd6f7281d3453b8b70e8a6bd4f2192aa5d3a2524 on aws:master.\n. \n\nCoverage increased (+0.02%) to 90.429% when pulling 0b08904a4514505aa9f4708903209eeaff256a65 on johanneswuerbach:fix-missing-last-chunk into b98cc2e447c75811a1829fe4cbe69b8ba5df6c35 on aws:master.\n. ",
    "e-gineer": "I agree with your suggestion that setting the timeout only through the EC2MetadataCredentials provider makes sense, or would at least suggest updating the documentation to reflect the fact that there actually is a timeout in this case.\nAnother question / suggestion for the docs is how to set to no timeout? Is it zero as per http://nodejs.org/api/http.html?\n(BTW - I'm incredibly impressed by both the quality of the code and your participation in this issues forum with everyone. Thanks for such fantastic work...)\n. I agree with your suggestion that setting the timeout only through the EC2MetadataCredentials provider makes sense, or would at least suggest updating the documentation to reflect the fact that there actually is a timeout in this case.\nAnother question / suggestion for the docs is how to set to no timeout? Is it zero as per http://nodejs.org/api/http.html?\n(BTW - I'm incredibly impressed by both the quality of the code and your participation in this issues forum with everyone. Thanks for such fantastic work...)\n. Thanks very much for the quick response and link to clear documentation. I'm closing this ticket.\n(For what it's worth, I find it surprising that the iam:* API actions are restricted when accessing via temporary session credentials, especially since it works for role-based credentials and the IAM console also works for federated users...)\n. Thanks @AdityaManohar, my testing shows that s3ForcePathStyle resolves the issue per your example.\nBut, I'm still confused as to why this operation succeeds when calls are made to any region other than us-east-1? If I understood your answer correctly, then I'd expect the sample script to fail in all regions, not just us-east-1...\nFor example, this non-s3ForcePathStyle code works in us-west-2:\n```\nAWS = require('aws-sdk');                                                                                                                             \nvar s3 = new AWS.S3({\n  accessKeyId: 'SECRET',\n  secretAccessKey: 'SECRET',\n  signatureVersion: 'v4',\n  region: 'us-west-2'\n});\ns3.getBucketLocation({Bucket: 'not-us-west-2-bucket'}, function (err, result) {\n  console.log(err);\n  console.log(result)\n});\n```\nbut the same code called from us-east-1 fails:\n```\nAWS = require('aws-sdk');                                                                                  \nvar s3 = new AWS.S3({\n  accessKeyId: 'SECRET',\n  secretAccessKey: 'SECRET',\n  signatureVersion: 'v4',\n  region: 'us-east-1'\n});\ns3.getBucketLocation({Bucket: 'not-us-east-1-bucket'}, function (err, result) {\n  console.log(err);\n  console.log(result)\n});\n```\nThoughts? Is the expected behavior that us-east-1 is a special case?\n. Thanks, this clarifies things completely ... us-east-1 really is a special case for S3.\n. Thanks for the quick confirmation! I've moved forward with the workaround, so please close this issue when you consider it complete from the AWS side.\n. Thanks @chrisradek. The workaround is functioning well, and ironically was my preferred way to call the code in the first place. I was actually splitting out the Qualifier specifically to provide it separately .\nFrom the Lambda API docs and SDK docs it's fairly confusing at this point about the use of the qualifier in the FunctionName. Regex says it's allowed, but docs don't mention it, etc.\nFrom the API response and doc definition, I assume this isn't so much a temporary workaround as a model that can be used into the future even after the Qualifier specification is fixed?\n. Thanks @chrisradek. The workaround is functioning well, and ironically was my preferred way to call the code in the first place. I was actually splitting out the Qualifier specifically to provide it separately .\nFrom the Lambda API docs and SDK docs it's fairly confusing at this point about the use of the qualifier in the FunctionName. Regex says it's allowed, but docs don't mention it, etc.\nFrom the API response and doc definition, I assume this isn't so much a temporary workaround as a model that can be used into the future even after the Qualifier specification is fixed?\n. Perfect - thanks!\n. Thanks - I agree that the ability to set a proxy for each individual connection is helpful.\nUnfortunately, our software uses 50+ AWS services in a range of enterprise scenarios, each of which may have a highly specific proxy configuration. Unfortunately the per-service approach would require us to add custom parsing of the proxy settings to cover all those cases. Not impossible, just hoping there is a better way!\nWe use the aws cli which supports no_proxy. We use the node request module which supports no_proxy settings.\nBefore we embark on a custom approach, I'd really appreciate it if anyone can suggest an alternative to proxy-agent - or even just confirm they are not aware of an alternative. Unfortunately, our searching has come up empty... Thanks!. ",
    "makered": "Wow, I have been ripping my hair out for the past few weeks trying to figure out why our app silently stops receiving messages from SQS on random occasions. This thread hit the nail on the head. \nThanks!!! \n. ",
    "dennismonsewicz": "@lsegal Node version is v0.10.25. Not getting any errors. In fact if I console.log a simple \"Hello world\" inside the sqs.createQueue callback function nothing is printed.\naws-sdk version is 2.0.17\n. @lsegal Node version is v0.10.25. Not getting any errors. In fact if I console.log a simple \"Hello world\" inside the sqs.createQueue callback function nothing is printed.\naws-sdk version is 2.0.17\n. Figured out my issue. I needed to make my grunt task run in async mode\nFinal Code:\n``` javascript\n  grunt.registerTask(\n    'configure-sqs',\n    'Configure SQS Queues',\n    function () {\n  AWS.config.update(_.pick(config.aws,\n    'accessKeyId',\n    'secretAccessKey',\n    'region'), true);\n\n  var sqs = new AWS.SQS(),\n      done = this.async();\n\n  var queues = config.sqs.queues;\n\n  for(var q in queues) {\n    var param = {\n      QueueName: queues[q]\n    };\n    sqs.createQueue(param, function(err, data) {\n      if (err) {\n        grunt.log.error([err]);\n        done();\n      } else {\n        grunt.log.ok('Ensured queue Exists: ' + queues[q] + ' with url: ' + data.QueueUrl);\n        done();\n      }\n    });\n  }\n}\n\n);\n```\n. Figured out my issue. I needed to make my grunt task run in async mode\nFinal Code:\n``` javascript\n  grunt.registerTask(\n    'configure-sqs',\n    'Configure SQS Queues',\n    function () {\n  AWS.config.update(_.pick(config.aws,\n    'accessKeyId',\n    'secretAccessKey',\n    'region'), true);\n\n  var sqs = new AWS.SQS(),\n      done = this.async();\n\n  var queues = config.sqs.queues;\n\n  for(var q in queues) {\n    var param = {\n      QueueName: queues[q]\n    };\n    sqs.createQueue(param, function(err, data) {\n      if (err) {\n        grunt.log.error([err]);\n        done();\n      } else {\n        grunt.log.ok('Ensured queue Exists: ' + queues[q] + ' with url: ' + data.QueueUrl);\n        done();\n      }\n    });\n  }\n}\n\n);\n```\n. Thanks so much for the help! I think I figured out my issue. \nIt appears even if you label an item as an N type, you have to convert it the item to a string. So your object mapping would look like...\njavascript\n{ \"N\": \"1234\" }\n. @lsegal so sorry for the delay, been heads down on this project and forgot to chime back in.\nShouldn't declaring a value of N be saved as Numeric type?\n. ",
    "dland512": "I was having this problem when trying to delete an S3 object in a cordova app using the deleteObject method in the aws javascript sdk. Adding ResponseCacheControl: 'no-cache' when initially pulling the file down did the trick. Surprising that this issue still exists in iOS 3 years later.. ",
    "5inline": "Yes, that resolves my issue. Thanks for the quick response.\n. ",
    "AReallyGoodName": "OK I have a problem with how this all works. I had the same issue. BUT... I was still getting the ETag back in the header from the remote side if i read the header manually without the Amazon SDK. \nIncorrect CORS configuration merely seems to change what the SDK sees. If you do a manual request without the Amazon SDKs uploadPart function you can see the ETag regardless of that particular setting.\nWhich raises the question. Why require this ExposeHeader option in CORS? It doesn't change what gets sent to the user at all. It just changes what the SDK library will output.\n. ",
    "mpelham": "I was on a AWS Support Call at the time of reporting, and right before downgrading AWS-SDK to 2.0.19 (which resolved our issue), Victor of AWS confirmed the same incorrect Shasum, and also mentioned another customer had just called in reporting a similar issue.\nI had recently (at about 3:05 PM PST) deployed an Elastic Beanstalk instance which required a hotfix. The hotfix deploy at 3:20 PM immediately failed. I discovered this issue in the logs, called AWS Support to highlight it, and redeployed with 2.0.19.\nI was not able to reproduce this on my local instance either, which I found interesting.\nosx 10.9.2\nnpm 1.3.24\ngit version 1.8.5.2 (Apple Git-48)\n[Edit - Adding Elastic Beanstalk Versions and Fuller Log]\nThe Elastic Beanstalk instance however:\n64bit Amazon Linux 2014.09 v1.0.9 \nrunning Node.js 0.10.31 with nginx as the proxy server and \nnpm -v 1.4.23.\n[2014-10-16T22:37:26.806Z] ERROR [1634]  : Command execution failed: [CMD-Startup/StartupStage0/AppDeployPreHook/50npm.sh] command failed with error code 1: /opt/elasticbeanstalk/hooks/appdeploy/pre/50npm.sh\n- /opt/elasticbeanstalk/containerfiles/ebnode.py --action npm-install\n  npm ERR! Error: shasum check failed for /tmp/npm-1786-7agjLmVX/registry.npmjs.org/aws-sdk/-/aws-sdk-2.0.20.tgz\n  npm ERR! Expected: 0d73fab6eb815fd80aa6805ec999cc2d908bcaf5\n  npm ERR! Actual:   da39a3ee5e6b4b0d3255bfef95601890afd80709\n  npm ERR! From:     https://registry.npmjs.org/aws-sdk/-/aws-sdk-2.0.20.tgz\n  npm ERR!     at /opt/elasticbeanstalk/node-install/node-v0.10.31-linux-x64/lib/node_modules/npm/node_modules/sha/index.js:38:8\n  npm ERR!     at ReadStream. (/opt/elasticbeanstalk/node-install/node-v0.10.31-linux-x64/lib/node_modules/npm/node_modules/sha/index.js:85:7)\n  npm ERR!     at ReadStream.emit (events.js:117:20)\n  npm ERR!     at stream_readable.js:943:16\n  npm ERR!     at process._tickCallback (node.js:419:13)\n  npm ERR! If you need help, you may report this _entire log,\n  npm ERR! including the npm and node versions, at:\n  npm ERR!     http://github.com/npm/npm/issues\nnpm ERR! System Linux 3.14.20-20.44.amzn1.x86_64\nnpm ERR! command \"/opt/elasticbeanstalk/node-install/node-v0.10.31-linux-x64/bin/node\" \"/opt/elasticbeanstalk/node-install/node-v0.10.31-linux-x64/bin/npm\" \"--production\" \"install\"\nnpm ERR! cwd /tmp/deployment/application\nnpm ERR! node -v v0.10.31\nnpm ERR! npm -v 1.4.23\nnpm ERR! \nnpm ERR! Additional logging details can be found in:\nnpm ERR!     /tmp/deployment/application/npm-debug.log\nnpm ERR! not ok code 0\nRunning npm install:  /opt/elasticbeanstalk/node-install/node-v0.10.31-linux-x64/bin/npm\nRunning npm with --production flag\nFailed to run npm install. Snapshot logs for more details.\nTraceback (most recent call last):\n  File \"/opt/elasticbeanstalk/containerfiles/ebnode.py\", line 605, in \n    main()\n  File \"/opt/elasticbeanstalk/containerfiles/ebnode.py\", line 587, in main\n    node_version_manager.run_npm_install(options.app_path)\n  File \"/opt/elasticbeanstalk/containerfiles/ebnode.py\", line 124, in run_npm_install\n    self.npm_install(bin_path, self.config_manager.get_container_config('app_staging_dir'))\n  File \"/opt/elasticbeanstalk/containerfiles/ebnode.py\", line 154, in npm_install\n    raise e\nsubprocess.CalledProcessError: Command '['/opt/elasticbeanstalk/node-install/node-v0.10.31-linux-x64/bin/npm', '--production', 'install']' returned non-zero exit status 1 (ElasticBeanstalk::ActivityFatalError)\n    at /opt/elasticbeanstalk/lib/ruby/lib/ruby/gems/2.1.0/gems/beanstalk-core-1.0/lib/elasticbeanstalk/activity.rb:189:in `rescue in exec'\n    ...\ncaused by: command failed with error code 1: /opt/elasticbeanstalk/hooks/appdeploy/pre/50npm.sh\n- /opt/elasticbeanstalk/containerfiles/ebnode.py --action npm-install\n. Also, funny enough, googling \"npm bad shasum\" shows a stackoverflow on AWS-SDK from May.\nhttp://stackoverflow.com/questions/23771755/npm-install-does-not-work-for-aws-sdk\n. ",
    "konklone": ":+1:\n. ",
    "Nibbler999": "It does, many thanks.\n. ",
    "jdoavila": "You've been able to solve this problem?, I could not send a picture of $cordovaCamera to Amazon S3. Help me, maybe you can show me the final code.\n. ",
    "BorePlusPlus": "Thanks @lsegal, that works for me.\nIs there some documentation mentioning this that I have missed?\n. ",
    "gdbtek": "I bumped into this issue as well when I use S3 lib: https://www.npmjs.org/package/s3.\n. I bumped into this issue as well when I use S3 lib: https://www.npmjs.org/package/s3.\n. @fkjaekel would you suggest that user-end should handle this case (using a sample above code) or AWS SDK should be able to handle it in the next release or so?\n. @fkjaekel would you suggest that user-end should handle this case (using a sample above code) or AWS SDK should be able to handle it in the next release or so?\n. it breaks everything for me as well. My application run just fine under 0.10.33 but NOT 0.10.34 Here is my sample package.json and source:\npackage.json\n```\nroot@nam-itc<~/tmp>\ncat package.json\n{\n  \"name\": \"tmp\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"aws-sdk\": \"2.1.4\"\n  }\n}\n```\nindex.js\n```\nroot@nam-itc<~/tmp>\ncat index.js\nvar AWS = require('aws-sdk');\nvar options = {\n  \"accessKeyId\": \"MY_ACCESS_KEY_ID\",\n  \"secretAccessKey\": \"MY_SECRET_ACCESS_KEY\",\n  \"region\": \"us-east-1\"\n};\nvar params = {\n  StackStatusFilter: [\n    'CREATE_COMPLETE'\n  ]\n};\nvar cloudformation = new AWS.CloudFormation(options);\ncloudformation.listStacks(params, function (err, data) {\n  if (err) {\n    console.log(err, err.stack);\n  } else {\n    console.log(data);\n  }\n});\n```\nRun using node.js v0.10.34\n```\nroot@nam-itc<~/tmp>\nnode --version\nv0.10.34\nroot@nam-itc<~/tmp>\nnode index.js\n{ [NetworkingError: CERT_UNTRUSTED]\n  message: 'CERT_UNTRUSTED',\n  code: 'NetworkingError',\n  region: 'us-east-1',\n  hostname: 'cloudformation.us-east-1.amazonaws.com',\n  retryable: true,\n  time: Thu Dec 18 2014 03:35:53 GMT-0800 (PST) } 'Error: CERT_UNTRUSTED\\n    at SecurePair. (tls.js:1381:32)\\n    at SecurePair.emit (events.js:92:17)\\n    at SecurePair.maybeInitFinished (tls.js:980:10)\\n    at CleartextStream.read [as _read] (tls.js:472:13)\\n    at CleartextStream.Readable.read (_stream_readable.js:341:10)\\n    at EncryptedStream.write [as _write] (tls.js:369:25)\\n    at doWrite (_stream_writable.js:226:10)\\n    at writeOrBuffer (_stream_writable.js:216:5)\\n    at EncryptedStream.Writable.write (_stream_writable.js:183:11)\\n    at write (_stream_readable.js:602:24)'\n```\nRun using node.js v0.10.33\n```\nroot@nam-itc<~/tmp>\nnode --version\nv0.10.33\nroot@nam-itc<~/tmp>\nnode index.js\n{ ResponseMetadata: { RequestId: '2a7ee8e0-86aa-11e4-9df0-49ebbb7934b9' },\n  StackSummaries: \n   [ { StackId: 'arn:aws:cloudformation:us-east-1:639132917637:stack/DEVELOPMENT-NAM-20141210-095615-US-EAST-1/391c62c0-8053-11e4-a2a7-50e241629418',\n       StackName: 'DEVELOPMENT-NAM-20141210-095615-US-EAST-1',\n       TemplateDescription: 'OPS',\n       CreationTime: Wed Dec 10 2014 01:59:25 GMT-0800 (PST),\n       StackStatus: 'CREATE_COMPLETE',\n       StackStatusReason: '' } ] }\n```\n. it breaks everything for me as well. My application run just fine under 0.10.33 but NOT 0.10.34 Here is my sample package.json and source:\npackage.json\n```\nroot@nam-itc<~/tmp>\ncat package.json\n{\n  \"name\": \"tmp\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"aws-sdk\": \"2.1.4\"\n  }\n}\n```\nindex.js\n```\nroot@nam-itc<~/tmp>\ncat index.js\nvar AWS = require('aws-sdk');\nvar options = {\n  \"accessKeyId\": \"MY_ACCESS_KEY_ID\",\n  \"secretAccessKey\": \"MY_SECRET_ACCESS_KEY\",\n  \"region\": \"us-east-1\"\n};\nvar params = {\n  StackStatusFilter: [\n    'CREATE_COMPLETE'\n  ]\n};\nvar cloudformation = new AWS.CloudFormation(options);\ncloudformation.listStacks(params, function (err, data) {\n  if (err) {\n    console.log(err, err.stack);\n  } else {\n    console.log(data);\n  }\n});\n```\nRun using node.js v0.10.34\n```\nroot@nam-itc<~/tmp>\nnode --version\nv0.10.34\nroot@nam-itc<~/tmp>\nnode index.js\n{ [NetworkingError: CERT_UNTRUSTED]\n  message: 'CERT_UNTRUSTED',\n  code: 'NetworkingError',\n  region: 'us-east-1',\n  hostname: 'cloudformation.us-east-1.amazonaws.com',\n  retryable: true,\n  time: Thu Dec 18 2014 03:35:53 GMT-0800 (PST) } 'Error: CERT_UNTRUSTED\\n    at SecurePair. (tls.js:1381:32)\\n    at SecurePair.emit (events.js:92:17)\\n    at SecurePair.maybeInitFinished (tls.js:980:10)\\n    at CleartextStream.read [as _read] (tls.js:472:13)\\n    at CleartextStream.Readable.read (_stream_readable.js:341:10)\\n    at EncryptedStream.write [as _write] (tls.js:369:25)\\n    at doWrite (_stream_writable.js:226:10)\\n    at writeOrBuffer (_stream_writable.js:216:5)\\n    at EncryptedStream.Writable.write (_stream_writable.js:183:11)\\n    at write (_stream_readable.js:602:24)'\n```\nRun using node.js v0.10.33\n```\nroot@nam-itc<~/tmp>\nnode --version\nv0.10.33\nroot@nam-itc<~/tmp>\nnode index.js\n{ ResponseMetadata: { RequestId: '2a7ee8e0-86aa-11e4-9df0-49ebbb7934b9' },\n  StackSummaries: \n   [ { StackId: 'arn:aws:cloudformation:us-east-1:639132917637:stack/DEVELOPMENT-NAM-20141210-095615-US-EAST-1/391c62c0-8053-11e4-a2a7-50e241629418',\n       StackName: 'DEVELOPMENT-NAM-20141210-095615-US-EAST-1',\n       TemplateDescription: 'OPS',\n       CreationTime: Wed Dec 10 2014 01:59:25 GMT-0800 (PST),\n       StackStatus: 'CREATE_COMPLETE',\n       StackStatusReason: '' } ] }\n```\n. ",
    "fkjaekel": "@lsegal no problem, I should have waited more, but I got a bit anxious because of my clients complaints. I created a limited IAM user so they can upload files directly at the S3 console while I solve this with calm.\nAwswearing your question I think it would be very nice if the SDK could handle time sync natively, through a config parameter. If this could be done out of the box it would be even better.\nI exposed the Date header in my CORS config, backwarded my computer clock one hour and tried your code(changed parseDate() to new Date(Date.parse())). The skew seems to be calculated correctly, but when the request retries I get 403 error:\nPOST https://spdata-apps-dev.s3-sa-east-1.amazonaws.com/arquivos/clientes/especificos/1/old-projects.tar?uploads 403 (Forbidden) aws-sdk-2.0.22.min.js.jsf?ln=js:6\nAny ideas?\nThanks\n. Same thing with 2.0.23:\nPOST https://spdata-apps-dev.s3-sa-east-1.amazonaws.com/arquivos/implantacao/firefox.png?uploads 403 (Forbidden) aws-sdk-2.0.23.min.js.jsf?ln=js:formatted:1868\nI don't think it's a resource permission issue, because when my computer clock is correct it works fine. The user I mentioned is different from the one I use with the SDK.\nI'll attach my user policy and CORS config in case it helps:\nxml\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:*\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n    <CORSRule>\n        <AllowedOrigin>*</AllowedOrigin>\n        <AllowedMethod>PUT</AllowedMethod>\n        <AllowedMethod>POST</AllowedMethod>\n        <AllowedMethod>DELETE</AllowedMethod>\n        <MaxAgeSeconds>3000</MaxAgeSeconds>\n        <ExposeHeader>ETag</ExposeHeader>\n        <ExposeHeader>Date</ExposeHeader>\n        <AllowedHeader>*</AllowedHeader>\n    </CORSRule>\n</CORSConfiguration>\n. Interesting\nI'll attach the response headers and body. Actually the error is still the RequestTimeTooSkewed, but I didn't figured out if is there something missing with the custom AWS.util.getDate function.\nxml\nRemote Address:54.231.253.9:443\nRequest URL:https://spdata-apps-dev.s3-sa-east-1.amazonaws.com/arquivos/implantacao/gabarito.xcf?uploads\nRequest Method:POST\nStatus Code:403 Forbidden\nRequest Headersview source\nAccept:*/*\nAccept-Encoding:gzip,deflate\nAccept-Language:pt-BR,pt;q=0.8,en-US;q=0.6,en;q=0.4\nAuthorization:AWS ******:********\nConnection:keep-alive\nContent-Length:0\nContent-Type:image/x-xcf; charset=UTF-8\nHost:spdata-apps-dev.s3-sa-east-1.amazonaws.com\nOrigin:http://localhost:8080\nReferer:http://localhost:8080/colaborador/template/fileUploadDialog.jsf\nUser-Agent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36\nX-Amz-Date:Thu, 06 Nov 2014 11:25:31 GMT\nx-amz-meta-LastModified:1406299007000\nX-Amz-User-Agent:aws-sdk-js/2.0.23\nQuery String Parametersview sourceview URL encoded\nuploads:\nResponse Headersview source\nAccess-Control-Allow-Methods:PUT, POST, DELETE\nAccess-Control-Allow-Origin:*\nAccess-Control-Expose-Headers:ETag, Date\nAccess-Control-Max-Age:3000\nContent-Type:application/xml\nDate:Thu, 06 Nov 2014 12:25:35 GMT\nServer:AmazonS3\nTransfer-Encoding:chunked\nVary:Origin, Access-Control-Request-Headers, Access-Control-Request-Method\nx-amz-id-2:UB855KNYkFsVhDawND1X2Ow49ASmhBn8+6lP4HS4QvcwfAt5/5cJ6cJNzgyavslx\nx-amz-request-id:FC1066011AF97AA9\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error>\n<Code>RequestTimeTooSkewed</Code>\n<Message>The difference between the request time and the current time is too large.</Message>\n<RequestTime>Thu, 06 Nov 2014 11:25:31 GMT</RequestTime>\n<ServerTime>2014-11-06T12:25:37Z</ServerTime>\n<MaxAllowedSkewMilliseconds>900000</MaxAllowedSkewMilliseconds>\n<RequestId>FC1066011AF97AA9</RequestId>\n<HostId>UB855KNYkFsVhDawND1X2Ow49ASmhBn8+6lP4HS4QvcwfAt5/5cJ6cJNzgyavslx</HostId></Error>\n. I asked my client to run \"new Date()\" in Chrome's console, as suggested here https://forums.aws.amazon.com/message.jspa?messageID=576487#576487\nThere is a GMT difference:\nxml\nMy computer:  Thu Nov 06 2014 13:49:36 GMT-0200 (Hor\u00e1rio brasileiro de ver\u00e3o) // works fine\nClient computer:  Thu Nov 06 2014 13:47:15 GMT-0300 (Hora oficial do Brasil) // RequestTimeTooSkewed error\nIn case it helps, I'm using the South America (S\u00e3o Paulo) region. S\u00e3o Paulo state is in GMT-0200 (BRST) too. \n. @lsegal I updated the SDK to 2.0.25, and on my first attempt with systemClockOffset + clock backward one hour it didn't worked, so I inverted the order that the offset is calculated:\nxml\nAWS.config.systemClockOffset = serverTime.getTime() - new Date().getTime();\nNow it works with the clock backward or forward one hour.\nYour first code sample works when the clock is backward one hour. What I missed is that it should be AWS.util.date.getDate = function() instead of AWS.util.getDate = function(). It didn't worked with clock forward one hour. Some minor changes would solve, but as setting systemClockOffset solved the problem I didn't go further.\nI'll close the issue.\nThanks\n. ",
    "ostaptan": "So this piece of code is tested and works correctly if someone needs\njavascript\nAWS.events.on('retry', function(response) {  \n      if (response.error.name === 'RequestTimeTooSkewed') {\n        console.error('User time is not correct. Handling error!');\n        console.log('AWS systemClockOffset:', AWS.config.systemClockOffset);\n        var serverTime = Date.parse(response.httpResponse.headers['date']);\n        var timeNow = new Date().getTime();\n        console.log('AWS timestamp:', new Date(serverTime));\n        console.log('Browser timestamp:', new Date(timeNow));\n        AWS.config.systemClockOffset = Math.abs(timeNow - serverTime);\n        response.error.retryable = true;\n        console.log('Setting systemClockOffset to:', AWS.config.systemClockOffset);\n        console.log('Retrying uploading to S3 once more...');\n      }\n});\nYou should add  this one to your CORS AWS setup: Access-Control-Expose-Headers: Date so that this one works properly\nvar serverTime = Date.parse(response.httpResponse.headers['date']);\n. So this piece of code is tested and works correctly if someone needs\njavascript\nAWS.events.on('retry', function(response) {  \n      if (response.error.name === 'RequestTimeTooSkewed') {\n        console.error('User time is not correct. Handling error!');\n        console.log('AWS systemClockOffset:', AWS.config.systemClockOffset);\n        var serverTime = Date.parse(response.httpResponse.headers['date']);\n        var timeNow = new Date().getTime();\n        console.log('AWS timestamp:', new Date(serverTime));\n        console.log('Browser timestamp:', new Date(timeNow));\n        AWS.config.systemClockOffset = Math.abs(timeNow - serverTime);\n        response.error.retryable = true;\n        console.log('Setting systemClockOffset to:', AWS.config.systemClockOffset);\n        console.log('Retrying uploading to S3 once more...');\n      }\n});\nYou should add  this one to your CORS AWS setup: Access-Control-Expose-Headers: Date so that this one works properly\nvar serverTime = Date.parse(response.httpResponse.headers['date']);\n. @lonormaly It will run as many times as you allow it to retry. General use case is actually just a one more protection from bugs, if user's computer is not synched with time. \nRegarding systemClockOffset is not being set just provide some code in jsfiddle for me to help you.\n. @lonormaly It will run as many times as you allow it to retry. General use case is actually just a one more protection from bugs, if user's computer is not synched with time. \nRegarding systemClockOffset is not being set just provide some code in jsfiddle for me to help you.\n. ",
    "lonormaly": "Thanks for sharing! \nCan you please show a general use case of this code? Should it be ran only after a first try?\nIt seems that the code indeed runs when this error arises, and it seems that the calculation of the new offset is correct, but it seems that the systemClockOffset is not being set so this error actually continues..\n. +1(!!) Let's support pause / resume\n@dconnolly Can you please provide an example of how to use (leavePartsOnError) this great feature?\n. @dconnolly I've been using your version for some time now, works great!\nQuestion: is there a way to abort an upload and to force clean the uploaded parts even when using the leavePartsOnError?\n. Thanks! Regarding the latter comment, in that case (when you close your\nlaptop for example), how can I detect that the upload was interrupted and\nit's not active anymore, after I open the laptop again?\nBecause it is shows on the network tab clearly that the upload is canceled\nbut can't find a way to check if its current status is active or not using\nJavaScript. Obviously the upload request didn't update its Failed field.\nThanks\nOn Tuesday, 22 September 2015, chrisradek notifications@github.com wrote:\n\nHey @lonormaly https://github.com/lonormaly\nThe upload object has a failed field that should return true if the upload\nfailed. I tested this on my own machine by starting a multipart upload and\ndisconnecting my internet connection. After a period of retries passed, I\ncould inspect the failed field on my upload object to see that it had\nfailed to upload.\nIf the computer goes to sleep during the upload, that may stop the upload\nobject from updating its failed status.\nLet me know if this information helps!\n-Chris\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/720#issuecomment-142146249.\n\n\nThanks\n       Shai\n. @chrisradek Thanks for your rapid response, sorry that mine wasn't as quick :)\nThe thing is that more often than not when resuming a failed upload the parts keep failing over and over again no matter how many times I send() the request again.\nSo we need to know exactly the state of the upload in order to determine wether it's active and correct or something went wrong with it, regardless of the firing of the error event, because it's not that rare that the event wasn't fired.\nHope that I'm clear enough.\nThanks\n. Thanks! Is this version available on 2.2.4?\n. @chrisradek Sure: \nhttps://gist.github.com/lonormaly/9302863584550715a57d\nLet me know if it's enough.\nNotice that it's written using angularjs so $q, $interval, $log are being used.\nCheers\n. Hey, thanks for your time :)\nThe 403 error, caused by the Cognito service, was starting to appear, as far as we could test it, after the computer entered to some kind of a sleep process. Right after that, every retry failed with same error - a retry process includes an .abort() and a .send() calls.\nIn last couple of days I added a fix that before every retry, right after the call to .abort() function, I get the Cognito credentials again.\nThanks for help and time friend :)\nShai\n. headObject\n. <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n    <CORSRule>\n        <AllowedOrigin>*</AllowedOrigin>\n        <AllowedMethod>GET</AllowedMethod>\n        <AllowedMethod>POST</AllowedMethod>\n        <AllowedMethod>PUT</AllowedMethod>\n        <AllowedMethod>DELETE</AllowedMethod>\n        <MaxAgeSeconds>3000</MaxAgeSeconds>\n        <ExposeHeader>ETag</ExposeHeader>\n        <AllowedHeader>*</AllowedHeader>\n    </CORSRule>\n</CORSConfiguration>\nI use an IAM role that is attached to the Cognito pool:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::<bucket>/Local/Artworks/Artwork*/${cognito-identity.amazonaws.com:sub}/*\"\n            ]\n        }\n    ]\n}\nWith getObject it works and I get the object, yet with headObject I get Network Failure and I do have internet connectivity...\n. Ok, now it works, yet I don't get most of the properties, such as ContentLength:\n\n. No, I'm ready to go. Thanks for you help :)\nPS - It would be great to add this information on documentation as well next to the headObject command\n. The http progress announces \"total\" of 5MB and part 1 instead of total of original file size.\nIt happened to me especially when disconnecting from internet and reconnecting again for some reason.\n. Thanks, but we don't use streams and yet we still get this error quite a bit\n. Sure:\nAWS.config.region = 'us-east-1'; // Region\n            AWS.config.credentials = new AWS.CognitoIdentityCredentials({\n                IdentityPoolId: COGNITO_POOL\n            });\n            if ($localStorage.cognitoId) {\n                AWS.config.credentials.refresh(function(err) {\n                    if (err) {\n                        getNewCognitoCredentials()\n                            .then(function(res) {\n                                deferred.resolve(AWS.config.credentials.identityId);\n                            }, function(err) {\n                                deferred.resolve(err);\n                            });\n                    } else {\n                        deferred.resolve(AWS.config.credentials.identityId);\n                    }\n                });\nWhen I say connection is lost I mean the connection was dropped for several minutes.\n. Amazing work! +1\n. Hey, Thanks for answering :)\nIt fails constantly; Any filesize but tested mostly on files under 5MB; The filename doesn't contain any non-english character.. ",
    "vhmth": "https://www.opentest.co/share/5a54713059cd11e69680a3b55516ed77\n. @imouaddine is that new to the API?. @imouaddine \ud83d\ude4f . Have y'all used EvaporateJS? https://github.com/TTLabs/EvaporateJS\nYou supply an endpoint on your server that returns a signed url. It asks for a signed URL for each part and handles keeping track of the ETags, aborting, canceling, etc.\n. ",
    "imouaddine": "I was able to fix this issue by using correctClockSkew config.\nAWS.config.update({\n    correctClockSkew: true,\n  });. I was able to fix this issue by using correctClockSkew config.\nAWS.config.update({\n    correctClockSkew: true,\n  });. @vhmth not sure. \nReference: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html. @vhmth not sure. \nReference: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html. ",
    "chris-prince": "Ah, you're right. I was on 2.0.22.  I have verified that it's working in 2.0.23.  Thanks!\n. Ah, you're right. I was on 2.0.22.  I have verified that it's working in 2.0.23.  Thanks!\n. ",
    "pvsrivathsa": "The err object is\n{ [SignatureDoesNotMatch: Signature expired: 20141105T071117Z is now earlier than     20141105T071121Z (20141105T072621Z - 15 min.)]\n  message: 'Signature expired: 20141105T071117Z is now earlier than 20141105T071121Z (20141105T072621Z - 15 min.)',\n  code: 'SignatureDoesNotMatch',\n  time: Wed Nov 05 2014 12:56:21 GMT+0530 (IST),\n  statusCode: 403,\n  retryable: false }\nretryCount is 0\n. Is this a bug introduced in 2.0.23 ? Will a downgrade help? If so, to what version?\n. ",
    "binoculars": "It's a system time issue http://stackoverflow.com/questions/18295185/invalidsignatureexception-signature-expired-timestamp-is-now-earlier-than-t\n. :cool: I've been very happy with v2.3 and promise support especially since Lambda updated to Node 4.3.2. Thanks for the quick reply!\n. ",
    "ymorired": "I just realized this closed ticket[https://github.com/aws/aws-sdk-js/issues/57] looks quite similar to the current issue..\n. @lsegal thanks for your prompt action! We'll test this on next build. We'll let you know if there is anything else we find.\n. ",
    "allthetime": "As an aside I am also attempting the same thing with https enabled on the proxy and getting this error\n{ \n [NetworkingError: Hostname/IP doesn't match certificate's altnames]\n message: 'Hostname/IP doesn\\'t match certificate\\'s altnames',\n code: 'NetworkingError',\n region: 'us-east-1',\n hostname: 'MY_BUCKET.s3.amazonaws.com',\n retryable: true,\n time: Fri Nov 07 2014 15:23:01 GMT-0800 (PST) \n}\nI'm guessing this is because 'MY_BUCKET.s3.amazonaws.com' is not included in my servers ssl certificate? The proxy is being redirected to from a load balancer that contains the ssl certificate via a subdomain. The certificate is for *.MY_DOMAIN.com.\n. @lsegal where can I access the finalized request object sent by the SDK?\n. @lsegal I have managed to get the proxy forwarding the request without issue now by turning off caching. But the entire reason I'm using the proxy is for caching. I hope this might make something clearer? \n. If you're interested... I found an excellent way to compare the headers sent my the SDK to the proxy, and the ones sent from the proxy to S3.\nDoing tcpdump -s 0 -w http.pcap -ni eth0 port 80 on the server and then reading the pcap file in wireshark exposes all the headers. I'm analyzing the information now. \n. @lsegal  It seems this is entirely to do with the proxy's handling of request forwarding. \nWith the cache turned off, first the HEAD is sent to the proxy and then to S3, and then the GET is sent to the proxy and then to S3, but with the cache turned on the HEAD is sent to the proxy and then somehow transformed into a GET which is passed to S3 and rejected. \nI guess this doesn't have anything to do with the SDK, so you can close this issue unless you are interested in the result. Thank you for your help.\n. @lsegal Is there a way to get the SDK to create signed urls and use those instead of passing the authentication in headers?\n. AWS.config.loadFromPath('./config_east.json')\ns3_east = new AWS.S3()\nAWS.config.loadFromPath('./config_west.json')\ns3_west = new AWS.S3()\nLoading two separate instances of s3 works\n. ",
    "bpevangelista": "The CRC32 values do not match! And what I discover is that on OSX/Chrome the response doesn't have the 'x-amz-crc32', thus, it doesn't do CRC check.\ncrc32IsValid: function crc32IsValid(resp) {\n    var crc = resp.httpResponse.headers['x-amz-crc32'];\n    if (!crc) {\n        console.log('NO CRC');\n        return true; // no (valid) CRC32 header\n    }\n    console.log( 'CRC    ' + parseInt(crc, 10) );\n    console.log( 'crypto ' + AWS.util.crypto.crc32(resp.httpResponse.body) );\n    ...\n }\nOn my browser I get \"NO CRC\" printed many times, on iPhone the HTTP response does have a crc32 and they all don't match the one computed by crypto:\n2014-11-08 15:36:19.647 Test1[482:60b] CRC    926634611\n2014-11-08 15:36:19.648 Test1[482:60b] crypto 2133101938\n2014-11-08 15:36:19.649 Test1[482:60b] CRC    2371721952\n2014-11-08 15:36:19.649 Test1[482:60b] crypto 3427207660\n2014-11-08 15:36:19.650 Test1[482:60b] CRC    2541117549\n2014-11-08 15:36:19.651 Test1[482:60b] crypto 91872494\n2014-11-08 15:36:23.115 Test1[482:60b] CRC    4126797375\n2014-11-08 15:36:23.115 Test1[482:60b] crypto 308262685\nLooks broken to me... Any thoughts?\n. ",
    "kalevet": "I'm having the same issue with limitation for iPhone. the workaround doesn't seem to work...\nAny ideas?\n. ",
    "bkw": "The failed travis test is due to linting violations, which were already fixed in master.\nI do have an integration branch ready to merge that is in sync with master and also fixes the test for the mock region name (see bkw/aws-sdk-js@d4caa6fa4)\n. agreed, I just reenabled it.\n. > What version of Node.js are you using?\n$ node --version\nv0.10.38\n. > What version of Node.js are you using?\n$ node --version\nv0.10.38\n. ",
    "mliszewski": "It fails in both cases.\n. ",
    "kunagpal": "@allthetime I'm experiencing a similar issue. Could you share a complete working example?. @kurt343 Yes, I was able to figure this out as follows:\n```js\nvar sdk = reqiure('aws-sdk'),\n    s3 = new AWS.S3(); // the lack of any options is important here. You could use a fixed api version, though.\ns3.copyObject({\n    Bucket: 'full name of the target bucket',\n    CopySource: 'full name of the source bucket' + '/' + 'full path to the file being copied',\n    Key: 'full path to the file being copied'\n}, function (err, data) {\n    // further handling logic goes here\n});\n```\nThe bottom line here is that S3 is pseudo region agnostic, so specifying complete bucket names and file paths is sufficient. If you've worked with ElasticBeanstalk before, you would have also noticed how S3 bucket names for ElasticBeanstalk version bundles include the region name as well.. @LiuJoyceC\nThanks for the explanation, this clears a lot of issues. Out of curiosity, under what circumstances would the aforementioned method call actually provide the Resources key? \n. @LiuJoyceC: The absence of the Resources field has been consistent ever since I've started using ElasticBeanstalk.describeEnvironments. Since the Resources key was missing from square one, I did switch over to ElasticBeanstalk.describeEnvironmentResources. However, the nature of ElasticBeanstalk.describeEnvironmentResources requires that an environment id or name be specified, for which describeEnvironments needs to be called first. In such a case, having Resources served within describeEnvironments would have save one extra AWS-SDK API call.\n. @LiuJoyceC: Thanks for all the help, this clears things up quite nicely :smile:\n. ",
    "kurt343": "@kunagpal did you happen to hear back from @allthetime or solve this in the end?\nI'm very new to S3 and have not come across many examples for copying/moving across regions.. Thank you very much @kunagpal. That got me across the line and was much easier than I anticipated.. ",
    "sturadnidge": "I guess it looks like something is parsing that value as a number somewhere along the way when it shouldn't be - the key values go in as a strings, get returned as a strings, and are designated as string types in DynamoDB as you can see above. But the behaviour is exactly what happens in JavaScript when you try and do anything with a number that is too big (which is why I'm treating these as strings everywhere in my code).\n. I guess it looks like something is parsing that value as a number somewhere along the way when it shouldn't be - the key values go in as a strings, get returned as a strings, and are designated as string types in DynamoDB as you can see above. But the behaviour is exactly what happens in JavaScript when you try and do anything with a number that is too big (which is why I'm treating these as strings everywhere in my code).\n. Just tested again using strings that look like 10-digit numbers (ie within the bounds of what JavaScript can handle), and everything works as expected. \nSo pretty certain something somewhere is indeed treating a string as a number when it definitely should not be (again, these values are being handled as strings everywhere outside of the aws-sdk module, including within DynamoDB itself).\n. Just tested again using strings that look like 10-digit numbers (ie within the bounds of what JavaScript can handle), and everything works as expected. \nSo pretty certain something somewhere is indeed treating a string as a number when it definitely should not be (again, these values are being handled as strings everywhere outside of the aws-sdk module, including within DynamoDB itself).\n. Cheers - I will try a bit later. That's not really use case though... I have an existing record, I just want to update a single field - I shouldn't have to pass the entire record (with a single updated field) in should I?\nSo just to be clear, if you run your sample code to create the correct record shape, then run my sample code above to update 'd' to a value of '2', you can't replicate the problem?\nThanks for looking into it!\n. Cheers - I will try a bit later. That's not really use case though... I have an existing record, I just want to update a single field - I shouldn't have to pass the entire record (with a single updated field) in should I?\nSo just to be clear, if you run your sample code to create the correct record shape, then run my sample code above to update 'd' to a value of '2', you can't replicate the problem?\nThanks for looking into it!\n. Hmmm... i can't seem to reproduce it with this contrived example either, however I looked at the insertion code of the environment that had the issue and am wondering if it's the string conversion that is to blame... basically it gets passed an object to insert and calls .toString() on everything before running putItem - I'm guessing that is the culprit.\nWill test that more fully later but in the meantime, I'll consider this closed - thanks again!\n. Hmmm... i can't seem to reproduce it with this contrived example either, however I looked at the insertion code of the environment that had the issue and am wondering if it's the string conversion that is to blame... basically it gets passed an object to insert and calls .toString() on everything before running putItem - I'm guessing that is the culprit.\nWill test that more fully later but in the meantime, I'll consider this closed - thanks again!\n. ",
    "shushantarora": "How does dynamo db stores number internally ?Does dynamodb stores number datatype as string only. Whats the size of number say with value 123456. will it be 6 bytes or 4 bytes - stored as string or int in dynamodb table ?. ",
    "patrick-motard": "I'm getting this same error for Cloudwatch putMetricAlarm. Is this expected?\n```\n{ Throttling: Rate exceeded\n    at Request.extractError (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/protocol/query.js:47:29)\n    at Request.callListeners (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/request.js:673:14)\n    at Request.transition (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /home/han/code/aws-utilities/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request. (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/request.js:675:12)\n    at Request.callListeners (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/sequential_executor.js:115:18)\n    at Request.emit (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/request.js:673:14)\n    at Request.transition (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /home/han/code/aws-utilities/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/home/han/code/aws-utilities/node_modules/aws-sdk/lib/request.js:38:9)\n  message: 'Rate exceeded',\n  code: 'Throttling',\n  time: 2017-05-24T00:11:10.473Z,\n  requestId: '7da27912-4015-11e7-a4b7-19825624989e',\n  statusCode: 400,\n  retryable: true }\n```. ",
    "justinmchase": "I am getting this error from the CloudSearch api, would this need to be added for each api?\nError: Rate exceeded\n    at Request.extractError (/var/runtime/node_modules/aws-sdk/lib/protocol/query.js:47:29)\n    at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:678:14)\n    at Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request.<anonymous> (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request.<anonymous> (/var/runtime/node_modules/aws-sdk/lib/request.js:680:12)\n    at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:115:18)\nThe calling code:\njs\nimport { CloudSearch, CloudSearchDomain } from 'aws-sdk'\n// get domain and items then...\nfunction send (domain, items) {\n  let params = {\n    contentType: 'application/json',\n    documents: JSON.stringify(items)\n  }\n  domain.uploadDocuments(params, callback) // unhandled exception is thrown\n}. Nevermind, it's something else :-X my bad.\n. Turns out I had a range key in an index against a boolean field and this was causing the error. Somehow it was working and then wasn't... Not sure how that happened though, rolling back and trying every version of the aws sdk produced the same error. Must have had my DB in a weird state while it was working.\n. Actually that's interesting because I am also calling the CloudSearch api in order find the CloudSearchDomain first. It's possible that that is what is throttled, I hadn't considered that because it is called much less frequently than the other. Thanks for the clue!. I'm pretty sure the other API is what is throwing the throttled error. Thanks for this clue.. Well that's true that's what the error is, but I think the reason why that error is happening because it is going up to the public endpoint, which does require credentials. You see the IP address 169.254.169.254:80 is the AWS service endpoint address.\nIf it were to actually go to localhost it would not need credentials and everything would work fine. So it appears to be ignoring the endpoint: 'https://localhost:12345' option, the server is not recording any calls to it.. I see what you're saying! If I add AWS_PROFILE=example when calling my app then it doesn't resolve the credentials and goes right to the api. It makes sense now, thanks!. ",
    "vitorbaptista": "Are there plans on adding this functionality? I'm trying to generate a policy to limit the uploaded file size and accept any Content-Type with direct uploads, and it seems I can't do that simply with getSignedUrl().\n. ",
    "raffi-minassian": "Thanks for the reply and confirming the issue. Is there anything I can do now to get the body parameter to return as a Buffer? Or will this just change in the next release?\nI agree on the createReadStream. I tried a million variations with the test case and thought it was best to post the issue here in a way where I could check and show the data.body type as it is returned in the callback from getJobOutput.\n. Piping to file with createReadStream() does work. Thanks!!!\nI actually tried that at one point but now realize I was doing some other stupid thing that made it not work which sent me off on this whole tangent.\nAlso, if you try to wrap the data.body string in a Buffer, it still doesn't work in my other code above because of it's previous encoding as string I guess.\nThanks for your help. Much appreciated!\n. That's great. Thanks for letting me know @lsegal , and thanks for helping with the alternative method. All is well. \n. Thank you for the great job @AdityaManohar !\n. Thanks @chrisradek !!\nWith some help from AWS support and further digging I have found that the problem seems to be something in the way request is doing the PUT much as you described. The following code DOES work, so I'm closing this as it does not seem to have anything to do with aws-sdk not working.\n```\nvar req = require('request');\nvar fs = require('fs');\nvar AWS = require('aws-sdk');\nAWS.config.update({\n  accessKeyId: \"MYKEY\",\n  secretAccessKey: \"MYSECRET\"\n});\nvar s3 = new AWS.S3();\nvar params = {Bucket: 'aws4test', Key: 'excalibur/items/stark.jpg', Expires: 300};\nvar url = s3.getSignedUrl('putObject', params);\nfs.readFile('./stark.jpg', function(err, data){\n  if(err){\n    return console.log(err);\n  }\n  req({\n    method: \"PUT\",\n    url: url,\n    body: data\n  }, function(err, res, body){\n    console.log(body);\n  })\n});\n```\n. Also, @chrisradek I just tried it like you said and sure enough that all worked too ... which I like better. So thanks again!!\n. ",
    "mateodelnorte": "Would also love to see streaming implemented in node. . ",
    "samclement": "Thanks - that works. \nI appreciate that streaming is great for large payloads - however it's also a great interface for IO in general. I can't say it much better than substack:\nhttps://github.com/substack/stream-handbook\nI noticed that the SQS service works without the configuration option you mentioned:\nvar sqs = new AWS.SQS();\nvar req = sqs.listQueues({}).createReadStream().pipe(process.stdout)\nBut it returns XML. Are there any plans on standardising the outputs or is this something that isn't going to be supported going forward?\n. Brilliant. Thanks for the detail - looks like the SDK is doing a bunch of work behind the scenes to provide a clean interface to your services. Appreciate the example of stream-ifying things too!\n. ",
    "dconnolly": "I gave that a go within populateURI, no dice. But, if I removed the s3ForcePathStyle and force something like this within populateURI:\nhttpRequest.endpoint.hostname = 'mydistribution.cloudfront.net';\nThen everything works lovely. It's currently an either/or based on req.service.pathStyleBucketName(b), this is a third case where we don't want the bucket in the path, but not in the domain either, since the cloudfront distribution knows which bucket. Do you have a recommendation on how best to do this in a PR?\n. I gave that a go within populateURI, no dice. But, if I removed the s3ForcePathStyle and force something like this within populateURI:\nhttpRequest.endpoint.hostname = 'mydistribution.cloudfront.net';\nThen everything works lovely. It's currently an either/or based on req.service.pathStyleBucketName(b), this is a third case where we don't want the bucket in the path, but not in the domain either, since the cloudfront distribution knows which bucket. Do you have a recommendation on how best to do this in a PR?\n. This is perfect! Thanks very much, I am up and running. Cheers!\n. This is perfect! Thanks very much, I am up and running. Cheers!\n. Ah. That's kinda what I thought, and started similar changes before you made the change above. See https://github.com/dconnolly/aws-sdk-js/compare/preserve-endpoint-hostname. If you think this is on the right track I can open the pull request.\n. Ah. That's kinda what I thought, and started similar changes before you made the change above. See https://github.com/dconnolly/aws-sdk-js/compare/preserve-endpoint-hostname. If you think this is on the right track I can open the pull request.\n. Fantastic, your new change looks spot on. Thanks very much for resolving this so quickly! \n\n. Fantastic, your new change looks spot on. Thanks very much for resolving this so quickly! \n\n. The moz prefix was copy-pasta'd from our own client that strives to be further backwards compatible, you guys don't need it. Looks like Blob.slice() was added back and webkitSlice() deprecated over two years ago: http://trac.webkit.org/changeset/120165. But I'm not sure what minimum version of Safari that change ended up in. \nEDIT: well that change was accepted in June 2012, so the earliest possible version of Safari it could have made it into was 5.1.8. https://en.wikipedia.org/wiki/Safari_version_history#cite_ref-Apple_Shocks_Security_World_with_Safari_5.1.8_for_Snow_Leopard_14-0\n. The moz prefix was copy-pasta'd from our own client that strives to be further backwards compatible, you guys don't need it. Looks like Blob.slice() was added back and webkitSlice() deprecated over two years ago: http://trac.webkit.org/changeset/120165. But I'm not sure what minimum version of Safari that change ended up in. \nEDIT: well that change was accepted in June 2012, so the earliest possible version of Safari it could have made it into was 5.1.8. https://en.wikipedia.org/wiki/Safari_version_history#cite_ref-Apple_Shocks_Security_World_with_Safari_5.1.8_for_Snow_Leopard_14-0\n. Ha that's odd. That change looks great, I'll leave this open until it finds its way into a release. Cheers!\n. Ha that's odd. That change looks great, I'll leave this open until it finds its way into a release. Cheers!\n. Awesome, thanks!\n. Awesome, thanks!\n. Related: if the input body is size 0, the callback is never called, because nextChunk() is never called. I would expect he callback to be called with an err value. I can create a separate issue for this specific case. EDIT: see #594 .\n. Related: if the input body is size 0, the callback is never called, because nextChunk() is never called. I would expect he callback to be called with an err value. I can create a separate issue for this specific case. EDIT: see #594 .\n. Awesome, thank you! :rocket: \n. Awesome, thank you! :rocket: \n. Awesome, thanks. :+1: \n. Awesome, thanks. :+1: \n. @lsegal thoughts?\n. @lsegal thoughts?\n. @AdityaManohar @lsegal bump? We've been using this in production for almost a month with success.\n. @AdityaManohar @lsegal bump? We've been using this in production for almost a month with success.\n. That's great, thank you.\n. That's great, thank you.\n. 2.2.4 was released a week before these changes were merged in.\n. 2.2.4 was released a week before these changes were merged in.\n. == @chrisradek \n. == @chrisradek \n. We are slicing as we are following the pattern of keeping track of parts by the correctly indexed key, ie: part 1 has key value 1, and key 0 is null, so we slice it out. Somehow this worked without slicing (before this change) on the first upload pass, but any 'resumed' uploads get into a weird state when trying to push all the parts again.\n. ",
    "kmejka": "Hi,\nThanks for the response. Regarding the credentials you're right, I can do it, I have just chosen this approach as I couldn't get the ~/.aws/credentials file approach to work. But the doesn't seem to be an issue here, right?\nAbout the endpoint - If I don't provide the endpoint my script fails with this exception:\n{ [UnknownEndpoint: Inaccessible host: `opsworks.eu-west-1.amazonaws.com'. This service may not be available in the `eu-west-1' region.]\n...\nIf I don't provide the region (I think opsworks is not a region-aware service) the script fails as well, this time with this exception:\n{ [ConfigError: Missing region in config]\n...\n. ",
    "johannesnagl": "+1\n. ",
    "marcosscriven": "Hi @lsegal \nWow that was a super fast response!\nYes, fully understand this isn't a client side issue, but wasn't sure how strongly client and server side teams were split, particularly with services in preview status.\nPlease feel free to close if you think the Lambda forum post is sufficient.\n. ",
    "comtaler": "Currently, I workaround this by removing the executable flag of the file from sax.\n. ",
    "mataneine": "I have the exact same problem and I don't understand why this issue is closed while it can be fixed with a small dependency update - \"If we can update aws-sdk's dependency on xml2js to at least 0.2.8, this will solve the problem.\" - I think its much better/easier than letting us do workarounds on our side\nbtw,  I found this which is related:\nhttps://bugs.launchpad.net/juniperopenstack/+bug/1391351\nhttps://github.com/Juniper/contrail-third-party-cache/pull/9 \n. ",
    "denodaeus": "This was a really really super annoying issue for me to track down before reading this, and I echo @mateneine, if you guys can just fix it without having to do work arounds and search for things broken, that would be awesome.  The problem is when starting a new project, I have no clue that because I'm using the latest and greatest, one of the dependencies for aws-sdk is broken when bundling as an rpm with grunt-rpm unless I do @comtaler's workaround.  I pulled my hair out looking for the issue within grunt-rpm for almost two hours before tracking this down.\n. ",
    "assen-totin": "Still an issue for me as of today; Azure still pulls 0.2.7. \nI'm using %filter_from_requires in my spec file to fix the broken require.\n. My package.conf has:\n    \"azure-mgmt\": \"\",\n    \"azure-mgmt-vnet\": \"\",\n    \"azure-mgmt-compute\": \"\",\n    \"azure-mgmt-storage\": \"\",\nSo, it should be the latest that npm has to offer - and I;m still getting 0.2.7 for xml2js.\n. ",
    "mfn": "Fair point. Why did I overlook it? Because, although it was only referring to the global configuration object, the bullet list in the next chapter was more an eye catcher for me to get my facts.\nMy suggestion: turn the essential of the first paragraph in a bullet list, example:\n\nConfiguring the SDK in the Browser\nThe SDK requires two settings to be configured in order to make requests:\n- a region for the service(s) being used\n- and credentials to access the resources.\n\nI understand that typography-wise bold isn't really used but to me it really buts the right emphasis on the important parts.\n. ",
    "onassar": "@lsegal is abort currently supported in browser version of sdk? Calling it seems to go through, but the upload is continuing.\n. @lsegal is abort currently supported in browser version of sdk? Calling it seems to go through, but the upload is continuing.\n. Thanks for the response @chrisradek \nThe intention was to offer the user the option to abort an upload (eg. if they chose the wrong file, they realize their internet connection is too slow and don't want to continue it, etc). I stumbled on the exact scenario you're describing: that when a user triggers the abort method against an AWS.S3.ManagedUpload instance, it works as expected if the file is over 5MB\nThe concern is that it's a bit of a strange UX: the user can only (successfully) abort an upload whereby the file is over 5MB. The way I'm dealing with this is within the UI, I'm only showing the option to abort if the file is greater than the minPartSize threshold.\nIs it possible to have abort work if/when the file is smaller? I was under the impression the minPartSize property needed to be greater than 5MB. If I could have it be arbitrarily low (eg. 100kb), that would be ideal.\n. Thanks for the response @chrisradek \nThe intention was to offer the user the option to abort an upload (eg. if they chose the wrong file, they realize their internet connection is too slow and don't want to continue it, etc). I stumbled on the exact scenario you're describing: that when a user triggers the abort method against an AWS.S3.ManagedUpload instance, it works as expected if the file is over 5MB\nThe concern is that it's a bit of a strange UX: the user can only (successfully) abort an upload whereby the file is over 5MB. The way I'm dealing with this is within the UI, I'm only showing the option to abort if the file is greater than the minPartSize threshold.\nIs it possible to have abort work if/when the file is smaller? I was under the impression the minPartSize property needed to be greater than 5MB. If I could have it be arbitrarily low (eg. 100kb), that would be ideal.\n. Gotcha okay. Curious: is that a restriction from the browser or the AWS lib?\n. Gotcha okay. Curious: is that a restriction from the browser or the AWS lib?\n. Gotta. Any chance this can be overridden on a per-bucket basis?\n. Gotta. Any chance this can be overridden on a per-bucket basis?\n. Thanks @chrisradek\nI'll give that a go. Would be great if I could update that on a per-bucket service\n. Thanks @chrisradek\nI'll give that a go. Would be great if I could update that on a per-bucket service\n. Little late to the party @chrisradek but I'm using aws-sdk-2.2.10.min.js and a AWS.S3.ManagedUpload for uploading. How can I access the request ids? Don't seem to be available to me (unfort) via this helper class.. Little late to the party @chrisradek but I'm using aws-sdk-2.2.10.min.js and a AWS.S3.ManagedUpload for uploading. How can I access the request ids? Don't seem to be available to me (unfort) via this helper class.. Thanks @AllanFly120 I'll give that a go; would those headers be exposed for a successful upload as well (so long as I expose them via the CORS rule)? Seems the easiest way to test it :). Thanks @AllanFly120 I'll give that a go; would those headers be exposed for a successful upload as well (so long as I expose them via the CORS rule)? Seems the easiest way to test it :). I tried this, but still don't see the headers being available in the scope of this. Here's CORS rules:\n. I tried this, but still don't see the headers being available in the scope of this. Here's CORS rules:\n. Getting AWS.S3.prototype.customizeRequests is not defined. Which version of the SDK makes the customizeRequests method available?\nI'm using aws-sdk-2.2.10.min.js\nAlso, I don't see that the extractData events are changing the e or d objects. How would those request ids be available via those properties?. Getting AWS.S3.prototype.customizeRequests is not defined. Which version of the SDK makes the customizeRequests method available?\nI'm using aws-sdk-2.2.10.min.js\nAlso, I don't see that the extractData events are changing the e or d objects. How would those request ids be available via those properties?. Incase anyone is curious, release v2.7.1 is the first release to have this method available:\nhttps://github.com/aws/aws-sdk-js/releases/tag/v2.7.1. Incase anyone is curious, release v2.7.1 is the first release to have this method available:\nhttps://github.com/aws/aws-sdk-js/releases/tag/v2.7.1. Which version of the SDK supports this?\nI'm using AWS.S3 and AWS.S3.ManagedUpload, and passing in true for the config option doesn't seem to fix the issue.. Which version of the SDK supports this?\nI'm using AWS.S3 and AWS.S3.ManagedUpload, and passing in true for the config option doesn't seem to fix the issue.. This ended up working for me:\nAWS.config.correctClockSkew = true;. This ended up working for me:\nAWS.config.correctClockSkew = true;. ",
    "utzc": "Thanks! I installed the latest version from git and it is now working for me.\n. ",
    "goozo": "I am getting this issue when I try to stream data to S3 ManagedUpload when the total size is unknown. The upload.send callback is never called. This is not a stream coming from the fs.createReadStream this is a stream of data from a parser. it works fine for small files as soon as the file size is larger than the chunk size it just hangs, no error no callback.\n. ",
    "ecdeveloper": "Oh, thanks! Dunno how did I miss the original issue :)\n. Great. Thanks!\n. ",
    "waltonseymour": "That worrks. Thanks a lot\n. ",
    "pburtchaell": "If you have node installed with homebrew, run brew switch node 0.10.28 to rollback. It solved the issue for me.\n. ",
    "garrows": "I had the same problem when I was on v0.10.34. \nI upgraded to v0.10.35 and instead i get:\n{ [InvalidClientTokenId: The security token included in the request is invalid.]\n  message: 'The security token included in the request is invalid.',\n  code: 'InvalidClientTokenId',\n  time: Mon Jan 05 2015 16:26:42 GMT+1000 (AEST),\n  statusCode: 403,\n  retryable: false,\n  retryDelay: 30 }\nNot quite fixed but probably not your fault. \n. I had the same problem when I was on v0.10.34. \nI upgraded to v0.10.35 and instead i get:\n{ [InvalidClientTokenId: The security token included in the request is invalid.]\n  message: 'The security token included in the request is invalid.',\n  code: 'InvalidClientTokenId',\n  time: Mon Jan 05 2015 16:26:42 GMT+1000 (AEST),\n  statusCode: 403,\n  retryable: false,\n  retryDelay: 30 }\nNot quite fixed but probably not your fault. \n. Turns out the access key was slightly wrong. Sorry.\n. Turns out the access key was slightly wrong. Sorry.\n. ",
    "oldboyedward": "Region is us-west-2(US West (Oregon) Region). \nThis happens with every files we tested recently (file sizes range from 20Mb to 64MB)\nWe tested it on amazon ec2 instances under light load and on our own on-premise server under normal load. The problem occurs on both platform with sdk version > 2.1.0 for large files \n. Thanks for the prompt reply!\nif the readStream is created with options = { start : startPosition ,end : endPosistion} , the putObject method seems unable to handle the readStream if the readStream is only part of a file.\nWith a file size 49MB and we only upload first chunk (32MB)  to S3. The following code would make putObject hang there until request time out.\nSome code snippet:\n var readOption = {\n      start: 0,\n      end: 33554432  //32MB in bytes\n    };\n try {\n      var readStream = fs.createReadStream(file.path, readOption);\n    } catch (e) {\n      logger.error(e);\n    }\nvar params ={\nBucket : \"bucketName\",\nBody:  readStream\nKey : \"fileName\",\ncontentLength : 33554432, //even without contentLength, it's the same request timeout error\n}\ns3.putObject(params,function(err){logger.error(err);}\nI will try upload api to see if it can handle stream created with position option\n. 2.1.4 works now , I can no longer produce the request timeout with putObject method after adding the print in the callback to check hearder. (and now I removed it, it still works)\n I don't know why though...(maybe observation changes the behavior)\n. ",
    "cjpartridgeb": "@lsegal it would appear 2.1.4 does have issues with upload()  (I couldn't reproduce issue with putObject). Piping a 100GB+ stream through was leaking memory and crashing almost instantly, rolling back to 2.1.0 from 2.1.4 seems to resolve the problem. I'll do some more testing and report back.\nEDIT: node v0.10.33 here\nEDIT2: ap-southeast-2 region\n. @lsegal it would appear 2.1.4 does have issues with upload()  (I couldn't reproduce issue with putObject). Piping a 100GB+ stream through was leaking memory and crashing almost instantly, rolling back to 2.1.0 from 2.1.4 seems to resolve the problem. I'll do some more testing and report back.\nEDIT: node v0.10.33 here\nEDIT2: ap-southeast-2 region\n. ",
    "mick": "@lsegal I think this is all set to go, unless there is anything else you think is blocking. Thanks! :octocat: \n. @lsegal I think this is all set to go, unless there is anything else you think is blocking. Thanks! :octocat: \n. @lsegal great! Just curious, what does the release / publish cycle for aws-sdk look like?  I'd like to be able to use this from npm.\n. @lsegal great! Just curious, what does the release / publish cycle for aws-sdk look like?  I'd like to be able to use this from npm.\n. I have got this error to on a few requests to DynamoDB, from an several EC2s in the same region. \nUnknownEndpoint: Inaccessible host: `dynamodb.us-east-1.amazonaws.com'. This service may not be available in the `us-east-1' region.\n    at Request.ENOTFOUND_ERROR (/usr/local/src/app/node_modules/aws-sdk/lib/event_listeners.js:358:46)\n    at Request.callListeners (/usr/local/src/app/node_modules/aws-sdk/lib/sequential_executor.js:100:18)\n    at Request.emit (/usr/local/src/app/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/usr/local/src/app/node_modules/aws-sdk/lib/request.js:604:14)\n    at ClientRequest.error (/usr/local/src/app/node_modules/aws-sdk/lib/event_listeners.js:204:22)\n    at ClientRequest.<anonymous> (/usr/local/src/app/node_modules/aws-sdk/lib/http/node.js:62:19)\n    at ClientRequest.emit (events.js:117:20)\n    at CleartextStream.socketErrorListener (http.js:1551:9)\n    at CleartextStream.emit (events.js:95:17)\n    at Socket.onerror (tls.js:1455:17)\nSince this seems caused by intermittent DNS request failures, shouldnt this error be retryable:true? https://github.com/aws/aws-sdk-js/blob/master/lib/event_listeners.js#L362\ncc @lsegal \n. I have got this error to on a few requests to DynamoDB, from an several EC2s in the same region. \nUnknownEndpoint: Inaccessible host: `dynamodb.us-east-1.amazonaws.com'. This service may not be available in the `us-east-1' region.\n    at Request.ENOTFOUND_ERROR (/usr/local/src/app/node_modules/aws-sdk/lib/event_listeners.js:358:46)\n    at Request.callListeners (/usr/local/src/app/node_modules/aws-sdk/lib/sequential_executor.js:100:18)\n    at Request.emit (/usr/local/src/app/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/usr/local/src/app/node_modules/aws-sdk/lib/request.js:604:14)\n    at ClientRequest.error (/usr/local/src/app/node_modules/aws-sdk/lib/event_listeners.js:204:22)\n    at ClientRequest.<anonymous> (/usr/local/src/app/node_modules/aws-sdk/lib/http/node.js:62:19)\n    at ClientRequest.emit (events.js:117:20)\n    at CleartextStream.socketErrorListener (http.js:1551:9)\n    at CleartextStream.emit (events.js:95:17)\n    at Socket.onerror (tls.js:1455:17)\nSince this seems caused by intermittent DNS request failures, shouldnt this error be retryable:true? https://github.com/aws/aws-sdk-js/blob/master/lib/event_listeners.js#L362\ncc @lsegal \n. @lsegal great thanks. I'll try that out. In the meantime I sent a PR for the change #529 \n. @lsegal great thanks. I'll try that out. In the meantime I sent a PR for the change #529 \n. ",
    "paritosh16": "I have a number of node modules that are running as cron jobs. The modules use the aws-sdk for javascript (Installed using npm install aws-sdk, current version being 2.1.38). It is giving the following error randomly,\n{ [TimeoutError: Missing credentials in config]\n  message: 'Missing credentials in config',\n  code: 'CredentialsError',\n  time: Mon Jul 13 2015 09:20:02 GMT+0000 (UTC),\n  originalError:\n   { message: 'Could not load credentials from any providers',\n     code: 'CredentialsError',\n     time: Mon Jul 13 2015 09:20:02 GMT+0000 (UTC),\n     originalError:\n      { message: 'Connection timed out after 1000ms',\n        code: 'TimeoutError',\n        time: Mon Jul 13 2015 09:20:02 GMT+0000 (UTC) } } }\nThis occurs randomly for all the modules. After the error, it again works fine for some time with another occurence of the error. Please help.\n. ",
    "jjjjw": "Ok, I'll take this up with webpack. It is a shame that the build chokes on ternaries. \n. ",
    "joearasin": "Is there any reason not to remove multiRequire entirely? Unless I'm missing something, it looks like dead code.\n. ",
    "EronHennessey": "Thanks for the fast review+merge!  :)\n. ",
    "perzy": "If I change the params Body to string or buffer, I can success upload object to s3.\nNow only when I set Body to a readableStream I can't upload anything to s3 success.\nThe upload() api I use.\n. @lsegal  Node.js -v 0.10.33\nAWS SDK -v 2.1.4\n```\n    fs.stat(localFile, function(err, file_info) {\n        var param = {\n            Bucket: config.s3.bucket,\n            Key: hash + fileType,\n            Body: \"fsdfsf\"\n        };\n    var href = path.join(s3.endpoint.href, param.Bucket, param.Key.toString());\n    console.log(href);\n\n    s3.upload(param, function(err, data) {\n        if (err) console.log(err, err.stack); // an error occurred\n        else     console.log(data);           // successful response\n    });\n});\n\n```\n. @lsegal  The code will be ok but when I change Body to fs.createReadStream(localFile) I got an error.\n. @lsegal  Thanks for you help and stand my poor english.\nI retry it right now and something like change to normal and I also success upload the image to s3 through stream.\nI am a developer in china.Maybe the problem is fire by firewall...LOL.\n. ",
    "apatil": "No problem, and thanks for the quick response and link.\n. ",
    "gsphanikumar": "Got the below error couple of times with the 2.1.4 sdk. \nSaw a upload issue where 10 retries did not help. Upload eventually succeeded after retrying after a couple of hours. Am using the npm retry module for retries.\n```\n    var stream = fs.createReadStream(filePath);\n    var params = {\n            Bucket: awsBucketName,\n            Key: awsFilename,\n            Body: stream\n            };\n     stream.on('open',function() {\n         s3.putObject(params, function(err, data) {\n             if (operation.retry(err)) {\n                 logger.error(\"Error during upload:\" + err.stack);\n                 logger.error(\"Retrying uploading file \" + path);\n                 return;\n             }\n             if (err) {\n                 logger.error(\"Error uploading file:\" + err.stack);\n             } \n         });\n     });\n```\n2015-1-27 22:09:45,872 - error: Error during upload:RequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\n    at Request.extractError (/node_modules/aws-sdk/lib/services/s3.js:358:35)\n    at Request.callListeners (/node_modules/aws-sdk/lib/sequential_executor.js:100:18)\n    at Request.emit (/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/node_modules/aws-sdk/lib/request.js:604:14)\n    at Request.transition (/node_modules/aws-sdk/lib/request.js:21:12)\n    at AcceptorStateMachine.runTo (/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request.<anonymous> (/node_modules/aws-sdk/lib/request.js:22:9)\n    at Request.<anonymous> (/node_modules/aws-sdk/lib/request.js:606:12)\n    at Request.callListeners (/node_modules/aws-sdk/lib/sequential_executor.js:104:18)\n. @lsegal Can you please re-open this issue\n. @lsegal  We are not using a proxy but it turned out to be our Ec2 Nat instance capacity issue.\n. ",
    "ronyrun": "Hello, \nI encounter the same error when upgrading from version \nv2.0.0-rc.19 to v2.4.7\nI have no errors with v2.0.0-rc.19\nI'm gonna try to detail more on which version this is not working anymore\n``` javascript\n var option = {\n                    Bucket: bucket,\n                    Key: upload.dest,\n                    Body: fs.createReadStream(path.normalize(output + '/' + upload.orig)),\n                    ContentType: upload.contentType\n                };\n            s3.putObject(option, function (err) {\n                if (err) {\n                    console.error(JSON.parse(JSON.stringify(err)));\n                    return cb(err);\n                } \n                cb();\n            });\n\n```\n. Thank you @lsegal \nI found which version cause the issue \n\"aws-sdk\": \"2.4.0\", => Working\n\"aws-sdk\": \"2.4.1\", => not Working\nI'm gonna test the managed s3.upload() method you provided.\n. Hi, my error is consistent.\n. My bad it's sporadic, I have the issue now with a previous version.\n. ",
    "mjurincic": "@ronyrun Any updates on this I'm having the same, almost exact same code,  aws-sdk v2.4.4, it  just happens sporadically \n. ",
    "FouadWahabi": "Hello,\nPlease any solution or a workaround for this issue, I encountered it with the latest version. ",
    "huesforalice": "Switching to 'upload' instead of 'putObject' solved this issue for me.. ",
    "sseetal": "Can confirm that 'upload' also works for me. ",
    "deliverymanager": "+1. ",
    "junajan": "We encountered the exact same problem when upgrading from v2.5.0 to v2.82.0\nOur code:\n```js\nconst params = {\n  Bucket: bucket,\n  Key: folder + objectName,\n  Body: fs.createReadStream(localFile),\n  ContentType: 'text/csv',\n  ACL: 'public-read'\n}\nreturn this.s3Client.putObject(params)\n  .promise()\n  .then(data => {\n    this.logger.info('FileContent was uploaded with etag %s', data.ETag)\n  })\n```. We encountered the exact same problem when upgrading from v2.5.0 to v2.82.0\nOur code:\n```js\nconst params = {\n  Bucket: bucket,\n  Key: folder + objectName,\n  Body: fs.createReadStream(localFile),\n  ContentType: 'text/csv',\n  ACL: 'public-read'\n}\nreturn this.s3Client.putObject(params)\n  .promise()\n  .then(data => {\n    this.logger.info('FileContent was uploaded with etag %s', data.ETag)\n  })\n```. I tried different versions and this error started appearing with this commit: https://github.com/aws/aws-sdk-js/commit/37f8596926dc0736d4e3e5021872fa981a305067\nAlso, it looks like it works when changing putObject to upload.\n. I tried different versions and this error started appearing with this commit: https://github.com/aws/aws-sdk-js/commit/37f8596926dc0736d4e3e5021872fa981a305067\nAlso, it looks like it works when changing putObject to upload.\n. ",
    "grvhi": "@lsegal - thank you very much. Both 1. and 2. are absolutely correct! \n. @lsegal - thank you very much. Both 1. and 2. are absolutely correct! \n. @mortenlyager : https://gist.github.com/fbda7c0afed03ae1ef8c.git\n. @mortenlyager : https://gist.github.com/fbda7c0afed03ae1ef8c.git\n. ",
    "mortenlyager": "@Erve1879 - Can you post a working copy of your code?\n. ",
    "DigitalFrontiersMedia": "@Erve1879 Thanks for posting the gist of the working copy but it appears to be 404 now.  Would it be possible to post it back again?  I'm interested in how you use s3.headObject() with this.  Thank you!\n. ",
    "jaxgeller": "Changed that up, should be good to go.\n. Changed that up, should be good to go.\n. ",
    "chenliu0831": "We are sending the presigned url to someone else that does the upload and they wish to make sure the upload is completed (e.g. they open the file in a weird way that truncate the file). If the Content-Length can be signed through the SDK it'll at least give the client an error. If not, is there any way for the client to check if the uploaded file is of the desired size based on the response from s3 upload? Thanks. \n. @chrisradek Sure. Works for me. Thanks\n. @chrisradek Sure. Works for me. Thanks\n. :+1: \n. :+1: \n. ",
    "mihaiserban": "@nxmohamad life saver!. Surprised that the metadata is not available in listObjects request. Calling headObject request for each item is way too costly . Surprised that the metadata is not available in listObjects request. Calling headObject request for each item is way too costly . Here's a gist with a recursive function to retrieve all listObjects with metadata or without. \nI would really like to see an option in AWS SDK for this issue, but until then I guess we'll call headObject for each object..\nhttps://gist.github.com/mihaiserban/1f35d488405812f2bbd4b16e38e4afb5. Here's a gist with a recursive function to retrieve all listObjects with metadata or without. \nI would really like to see an option in AWS SDK for this issue, but until then I guess we'll call headObject for each object..\nhttps://gist.github.com/mihaiserban/1f35d488405812f2bbd4b16e38e4afb5. ",
    "downie": "Nope. Here are all the properties accessible from AWS:\n```\n\nfor (var x in AWS) { console.log(x); }\n[Log] util\n[Log] VERSION\n[Log] Signers\n[Log] Protocol\n[Log] XML\n[Log] JSON\n[Log] Model\n[Log] apiLoader\n[Log] Service\n[Log] Credentials\n[Log] CredentialProviderChain\n[Log] TemporaryCredentials\n[Log] WebIdentityCredentials\n[Log] CognitoIdentityCredentials\n[Log] SAMLCredentials\n[Log] Config\n[Log] config\n[Log] Endpoint\n[Log] HttpRequest\n[Log] HttpResponse\n[Log] HttpClient\n[Log] SequentialExecutor\n[Log] EventListeners\n[Log] Request\n[Log] Response\n[Log] ResourceWaiter\n[Log] ParamValidator\n[Log] events\n[Log] XHRClient\n[Log] CloudWatch\n[Log] CognitoIdentity\n[Log] CognitoSync\n[Log] DynamoDB\n[Log] ElasticTranscoder\n[Log] Kinesis\n[Log] Lambda\n[Log] S3\n[Log] SNS\n[Log] SQS\n[Log] STS\n``\n. Version 2.1.5\n. Version 2.1.5\n. I'm not sure how you're not able to reproduce it. Even using the published minified code at https://sdk.amazonaws.com/js/aws-sdk-2.1.5.min.js doesn't appear to have theCloudWatchLogsproperty in it.\n. I'm not sure how you're not able to reproduce it. Even using the published minified code at https://sdk.amazonaws.com/js/aws-sdk-2.1.5.min.js doesn't appear to have theCloudWatchLogs` property in it.\n. Bummer. Well, I'm glad I'm not crazy at least. Thanks anyway.\n. Bummer. Well, I'm glad I'm not crazy at least. Thanks anyway.\n. \n",
    "jontelm": "https://github.com/bookingbricks/file-upload-example\nI have only be able to upload a file without any content or if i set Body in the getSignedUrl.\nSeems like Content-Length is set to 0 as default?\nEdit: The problem was Content-Type, works now.\n. Node v0.10.33 with version 2.1.7. The example works now after setting ContentType.\n. ",
    "shooding": "It will work.\nMy application server generates pre-signed url for each uploadPart.\nClient can upload parts to pre-signed urls and each of them will response ETag.\nThe tricky part is that Content-Type is not required (or say should not exists) in UploadPart requests,\nwhich is different from PutObject (If you have tried low-level REST PutObject to presigned url, you will know what i'm saying). \nOtherwise you will get 403 Forbidden (Signature does not match) when UploadPart.\nFinally,  like @lsegal said, you need to keep track of each ETag and complete the multipart so that you will be able to construct parts information at client-side.\n. @vkovalskiy and @musicullum \nHere is a working example of server-side PHP:\n``` PHP\n//initialize your $s3Client first.\n$signedUrl = '';              \n$command = $s3Client->getCommand('UploadPart', array(\n            'Bucket' => 'yourBucket',\n            'Key' => 'yourObjectkey',\n            'PartNumber' => 1,\n            'UploadId' => 'FromCreateMultipartUpload',  \n            //ContentType is not required\n            'Body' => '',\n        )); \n$signedUrl = $command->createPresignedUrl('+10 minutes');\n```\nWhen you upload part from the client side, do:\ncurl -v -X PUT -T {local_path_to_your_file_part} '{signedUrl}'\nSince -v option is on, you can see HTTP debug information and see the ETag in response.\nYou should make a MD5 of your local file part and compare it to the ETag, or\njust irresponsibly accept everything AWS replies.\nSorry i didn't use js at the client side, but i'm pretty sure you guys can do that easily with jQuery.\nIn this way you can avoid proxying large files.\n. It works, but i was using SignatureV2 and not sure if SignatureV4 change anything.\nPHP\n$result = $s3Client->completeMultipartUpload(array(\n            'Bucket' => 'yourBucket',\n            'Key' => 'yourObjectkey',\n            'Parts' => $part->getETags(),\n            'UploadId' => 'FromCreateMultipartUpload',\n            //ContentType is not required for AWS in this request\n        ));\n. ",
    "vkovalskiy": "Thanks @lsegal and @shooding for the info. I'll give it a try - we really need multipart uploads with unstable channels.\n. Thanks @lsegal and @shooding for the info. I'll give it a try - we really need multipart uploads with unstable channels.\n. thanks a lot @shooding !\n. thanks a lot @shooding !\n. ",
    "musicullum": "I try the same thing. The one thing that's missing for the server to generate the pre-signed Url for uploadPart is the \"operation\" parameter. Would that be \"uploadPart\"? I might try but because it's rather complex, i'd very much apprechiate a solution (if it's only a confirmation that it can actually be done this way).\nI'm a bit confused why there is almost no information to be found for this particular case. In order to avoid proxying, it appears to be the only way to acheive direct access to large objects without exposing the secret key in a client application, or am I missing something?\nThanks for any help!\n. Thanks a lot shooding. I figured out the command methods that I was missing, and so far successully downloaded a file this way from a server generated presigned url. Knowing and understanding how it works, this should now be applicable to the multipart upload sequences.\nMy client is actually a desktop (Qt) application, so having download working this way it should work out. So thanks again, also the missing Content-Type hint is certainly welcome!\n. I figured out that the size for a single file upload is sufficient for our needs so I didn't conclude multipart upload, sorry. What I remember is that I would create a handshake sequence between client and server like client asks server to give it a presigned url, server using AWS API returns url, client loads all parts and then requests the server to do the completion via the AWS API again (passing the list of etags). I mean to recall that it worked that way, but don't take my word for it.\n. ",
    "andrewgoodchild": "Hi Everyone,\nI have a partial working solution that provides a rest end point to  a client to start a multipart upload and return an upload ID.  The client can then use that upload id + partnumber to get a presigned url from another rest end point   \nThe client then does a PUT successfully for each part to S3 (without the content type) and they get a 200 OK + an ETag back.  We validate the etag and the value is the same as the MD5 hash we used for upload.  Finally, we then get the client to record the upload Ids and Etags.  So every thing is good ....\nNot quite.  The final step is when the issues arise.   When we try and complete the multipart upload using the upload Ids and part numbers, s3 rejects the complete request complaining that the parts numbers dont exist.  I did a list parts and there are no parts recorded at all for the uploadID, despite the fact that each part upload returned a 200 OK and a valid etag.  \nany thoughts?  @shooding you mentioned you had this working.\n. thanks musicullim.\nThe process you described is pretty much what we are doing.  However, what we are finding is the returned etags dont have part numbers in them and just include the MD5 hash (which is valid for the part - just no part number). If you ask S3 for a list of parts for a multipart uploadid, you find that none of the parts are registered against the upload id.  I suspect that uploading parts with presigned urls is not possible in AWS.    I know that it is supported for Azure:\nhttp://gauravmantri.com/2014/01/06/create-pre-signed-url-using-c-for-uploading-large-files-in-amazon-s3/.\nIn the meantime,  I have raised a support ticket with AWS to find out if it is possible.\nI would like to hear back from @shooding about his experience.\n@vkovalskiy I dont know if you are interested in reopening this question?  \n-Andrew.\n. thanks for that @shooding\n. I had a peak into the bucket this morning and I saw a bunch of files in S3 with part numbers and upload Ids.  I have found that the individual parts are being loaded into s3 as files with names that include part numbers and upload ids.  The presigned multipart uploads are being treated as ordinary files and  and not as parts of a multipart upload.  This explains why during the upload, we are seeing ETags returned without part numbers and in the final step none of the parts are registered to the upload ID\nSo it appears something has changed in S3.  @shooding could get presigned multipart uploads to work with v2 signatures.  But now that we are using v4 signatures, the bucket is treating presigned multipart uploads as ordinary presigned uploads.\n. The putObject url is identical for both except for query parameters\nThe S3 presigned Put is:\nPUT /ObjectName?AWSAccessKeyId=xxxx&Expires=xxxxx&Signature=xxxxxxx HTTP/1.1\nHost: BucketName.s3.amazonaws.com\nDate: date\nContent-Length: Size\nAnd the part upload is\nPUT /ObjectName?partNumber=PartNumber&uploadId=UploadId HTTP/1.1\nHost: BucketName.s3.amazonaws.com\nDate: date\nContent-Length: Size\nAuthorization: authorization string\nWhat I did was sign the url  /ObjectName?partNumber=PartNumber&uploadId=UploadId to get:\nPUT /ObjectName?partNumber=PartNumber&uploadId=UploadId&AWSAccessKeyId=xxxx&Expires=xxxxx&Signature=xxxxxxx HTTP/1.1\nHost: BucketName.s3.amazonaws.com\nDate: date\nContent-Length: Size\nWhich in turn was treated by S3 as a file called \"/ObjectName?partNumber=PartNumber&uploadId=UploadId\"\nAt this stage I am thinking the presigning using query params might not the the right approach and I am better off generating an authorization string on the server, passing it back to the client and getting the client to do a vanilla REST part upload using the authorization string. \n. @dnewkerk Thanks for the heads up.  I had a look at it and the solution references a gist (https://gist.github.com/dnewkerk/ff1bcebf83fb2f1b58b9) which constructs an authorization header for the client to send as part of an upload.  \nThought works has a blog on the issue (https://www.thoughtworks.com/mingle/infrastructure/2015/06/15/security-and-s3-multipart-upload.html), which highlights they bumped into the same problem with pre-signed multipart urls and now they dont presign and get the server end point to generate an authorisation to add to the header.\nLastly, I have been tick tacking with AWS support and on their console they are seeing presigned multipart uploads being treated as single part uploads.\nSo while presigned multi-part uploads may have been possible in the past, it seems they are no longer supported now.  Either the feature has been silently deprecated or it fell through the cracks when AWS upgraded S3 at some point.\nSo for now I am going to work on a solution that provides a rest end point to generates authorisation headers for the the client to include in their upload. \n. Thanks @dnewkerk.  The docs for current javascript sdk doesn't seem to have the extra parameters for method, upload id and part number you have used  in the ruby sdk.\nsee more here:\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#getSignedUrl-property\nSo I tried jamming the extra parameters in there anyway:\n...\nvar params = {\n    Bucket: result.value.bucket,\n    Key: location,\n    PartNumber: partNumber,\n    UploadId: uploadId\n};\ns3.getSignedUrl('uploadPart', params, function(err, url) {\n...\nAnd the AWS javascript SDK seems to have swallowed it.  And gosh dang it - it works.  I can complete a presigned multipart upload ......\nThank you again  @dnewkerk   \nMerry Xmas everyone !!!\n. I had a look at it, but I had some additional requirements. So I used it for ideas. \n. For those who are interested I did solve my problem.  \nI ended up writing a lambda function that creates a presigned URL and runs in the target bucket account.  I then set up cross account privileges to allow the server account to call the lambda in the target bucket account.  \nWorks a treat, but I would have preferred to have the ability for the ACL to be honoured in the first place.. ",
    "dnewkerk": "@andrewgoodchild I came across this issue while trying to solve the same problem, in my case using Ruby. Like you, I was using :put_object and was trying to pass in the key in the same \"multipart\" format I had used for the signed authorization string (including partNumber and uploadId), resulting in multiple files in my bucket, one for each part and named with the partNumber and uploadId.\nI don't yet know much about the JavaScript SDK, however hopefully the same solution can be applied. I asked about the issue in https://gitter.im/aws/aws-sdk-ruby and @trevorrowe pointed me to a solution that worked for me. For reference, here was his reply:\nI've never attempted to pre-sign a multipart upload. I suppose it might be possible. I would attempt something like this:\n``` ruby\npresigner = Aws::S3::Presigner.new\nurl = presigner.presigned_url(:create_multipart_upload, ...)\ninitiate the upload and pull the upload id from the response\npresigner.presigned_url(:upload_part, bucket:'name', key:'key', upload_id:'id', part_number: 1)\npresigner.presigned_url(:upload_part, bucket:'name', key:'key', upload_id:'id', part_number: 2)\netc, you need to capture the etag and part number from each of these requests\npresigner.presigned_url(:complete_multipart_upload, ...)\n```\nThat said, I'm curious what your use case is for pre-signed multipart uploads from the browser. I'm wondering if there might be an easier way. You can use a presigned-POST from the browser. It will not support objects larger than 5GB, but it would much simpler.\nThe ah-ha moment for me was from the presigner.presigned_url(:upload_part, bucket:'name', key:'key', upload_id:'id', part_number: 1) line. I applied the idea in my own code, and browser-based multipart uploads using presigned URLS worked! This seems to be what @jeskew was alluding to in the above comment as well.\nI hope this helps!\n. @andrewgoodchild apologies if I was unclear, though on my end presigned multipart uploads are working. The gist you mentioned was me sharing with trevor how I had previously coded the authorization header before getting presigned multipart uploads working.\nThis is my new version of the (ruby) code that works for getting presigned urls for both single and multipart uploads. Hopefully seeing this will give you ideas for how to solve it similarly in your code:\nruby\npresigner = Aws::S3::Presigner.new\nif upload.multipart?\n  presigner.presigned_url(:upload_part, bucket: upload.bucket, key: upload.filename,\n    upload_id: upload.multipart_id, part_number: upload_part.part_number)\nelse\n  presigner.presigned_url(:put_object, bucket: upload.bucket, key: upload.filename)\nend\nNotice how the first parameter of the presigned_url method is different between single and multipart uploads, and that multipart uploads take the regular key (filename) just like single part uploads, but also add parameters for setting the upload_id and part_number.\nHope this helps :)\n. ",
    "BenjaminPoilve": "Weir, @andrewgoodchild, doing the same as you returns an error for me: \nerror { [MultipleValidationErrors: There were 2 validation errors:\n* UnexpectedParameter: Unexpected key 'PartNumber' found in params\n* UnexpectedParameter: Unexpected key 'UploadId' found in params]\n  message: 'There were 2 validation errors:\\n* UnexpectedParameter: Unexpected key \\'PartNumber\\' found in params\\n* UnexpectedParameter: Unexpected key \\'UploadId\\' found in params',\n  code: 'MultipleValidationErrors',\n  errors: \n   [ { [UnexpectedParameter: Unexpected key 'PartNumber' found in params]\n       message: 'Unexpected key \\'PartNumber\\' found in params',\n       code: 'UnexpectedParameter',\n       time: Fri May 20 2016 19:54:39 GMT+0200 (CEST) },\n     { [UnexpectedParameter: Unexpected key 'UploadId' found in params]\n       message: 'Unexpected key \\'UploadId\\' found in params',\n       code: 'UnexpectedParameter',\n       time: Fri May 20 2016 19:54:39 GMT+0200 (CEST) } ],\n  time: Fri May 20 2016 19:54:39 GMT+0200 (CEST) }\nDoes this feature still works for you? Or is it down? Because no matter how hard I try, those parameters don't seems to be expected by amazon..\n. Weir, @andrewgoodchild, doing the same as you returns an error for me: \nerror { [MultipleValidationErrors: There were 2 validation errors:\n* UnexpectedParameter: Unexpected key 'PartNumber' found in params\n* UnexpectedParameter: Unexpected key 'UploadId' found in params]\n  message: 'There were 2 validation errors:\\n* UnexpectedParameter: Unexpected key \\'PartNumber\\' found in params\\n* UnexpectedParameter: Unexpected key \\'UploadId\\' found in params',\n  code: 'MultipleValidationErrors',\n  errors: \n   [ { [UnexpectedParameter: Unexpected key 'PartNumber' found in params]\n       message: 'Unexpected key \\'PartNumber\\' found in params',\n       code: 'UnexpectedParameter',\n       time: Fri May 20 2016 19:54:39 GMT+0200 (CEST) },\n     { [UnexpectedParameter: Unexpected key 'UploadId' found in params]\n       message: 'Unexpected key \\'UploadId\\' found in params',\n       code: 'UnexpectedParameter',\n       time: Fri May 20 2016 19:54:39 GMT+0200 (CEST) } ],\n  time: Fri May 20 2016 19:54:39 GMT+0200 (CEST) }\nDoes this feature still works for you? Or is it down? Because no matter how hard I try, those parameters don't seems to be expected by amazon..\n. Well, trying to make it work, I can now get a signed Url for each chunk.. I still get a 403 error. It looks like I am missing something.. More in this repo.\n. Well, trying to make it work, I can now get a signed Url for each chunk.. I still get a 403 error. It looks like I am missing something.. More in this repo.\n. Well I didn't found out what caused my bug, but I recommand this repo! Works perfectly. Great work by @Yuriy-Leonov\n. Well I didn't found out what caused my bug, but I recommand this repo! Works perfectly. Great work by @Yuriy-Leonov\n. In the end, we found this repo that works quite well and I had a friend push it to npm\n. In the end, we found this repo that works quite well and I had a friend push it to npm\n. ",
    "abuisine": "I would suggest to have a look at mule uploader, it allows a kind of pre-hash on your server side, and is quite robust.\n. ",
    "FlorinDavid": "@BenjaminPoilve I had the same issue, I've got the same error when I've tried to sign an upload part\n* UnexpectedParameter: Unexpected key 'PartNumber' found in params\n* UnexpectedParameter: Unexpected key 'UploadId' found in params]\nMy mistake was that I've used the putObject operation instead of uploadPart operation\nThe right call is:\njs\ns3Instance.getSignedUrl('uploadPart', {\n  Bucket: 'test-multipart-upload-free',\n  Key: '<your_file.ext>',\n  UploadId: '9PSnW_U3EgpbV8lmOQR8...',\n  PartNumber: <part_number>\n}, (error, presignedUrl) => {... //use the url to upload the file});)\n. ",
    "oyeanuj": "@lsegal @AdityaManohar Just following up to see if this is possible today since the comments on the issue are about a couple of years old?. @chrisradek thank you for responding!\nMy usecase was looking to upload a file from the client, without sending it through server-side (Ruby). I was using it in the context of React/Redux, so I didn't want to deal with getting forms through createPresignedPost. From my research, it seemed the simplest and often recommended way to do that was generating a presigned url to make a PUT Request on the client.\nSince the question yesterday, I chatted with @jeskew and @dinvlad, and it seems like that if I wanted to do multipart sending the files to my server, or without createPresignedPost, I'd have to use STS token (which seems a little bit more complicated than creating presigned_url).\nSo, at this moment, I am doing simple upload without chunking or multipart support. But I'd love to be able to do that, since the presignedUrl method feels the cleanest to use to upload, and I will need to soon upload files upto 2GB. So FWIW, I'd love to put a vote in for that in your backlog.\n(and yes, you are right that those libraries require a server-side signature, my bad). ",
    "tomasdev": "It is definitely possible as of November 27th, 2018.\nhttps://github.com/aws/aws-sdk-js/issues/1603#issuecomment-441926007. It is definitely possible as of November 27th, 2018.\nhttps://github.com/aws/aws-sdk-js/issues/1603#issuecomment-441926007. I am on the same boat, my use case is also partial uploads for big files on a JS client side. I want people to be able to resume uploads if they lose their connection, without losing all previously uploaded chunks. And I don't want to expose any credentials (thus not using SDK on client)\n~I will update this comment once I solve it.~\nUPDATE: following @sandyghai guide, I was able to do it.\nThere may be syntax errors, as my backend does not use express, but I felt writing it ala express would help other devs understand it easier.\nContext: I have an API (behind auth obviously) to which users can send files, and it uploads them to S3. As I didn't want to set IAM for each user of my app, nor put the SDK in the front-end, I decided to go with a back-end authorized approach.\n```js\napp.post('/upload', (req, res) => {\n  let UploadId = req.body.UploadId;\nconst params = {\n    Bucket: 'my-bucket-name',\n    Key: req.body.filename\n  };\n// Initialize the multipart - no need to do it on the client (although you can)\n  if (req.body.part === 1) {\n    const createRequest = await s3.createMultipartUpload(params).promise();\n    UploadId = createRequest.UploadId;\n  }\n// Save createRequest.UploadId in your front-end, you will need it. \n  // Also sending the uploadPart pre-signed URL for part #1\n  res.send({\n    signedURL: s3.getSignedUrl('uploadPart', {\n      ...params,\n      Expires: 60 * 60 * 24, // this is optional, but I find 24hs very useful\n      PartNumber: req.body.part\n    }),\n    UploadId,\n    ...params\n  });\n});\napp.post('/upload-complete', (req, res) => {\n  let UploadId = req.body.UploadId;\nconst params = {\n    Bucket: 'my-bucket-name',\n    Key: req.body.filename\n  };\nconst data = await s3.completeMultipartUpload({\n    ...params,\n    MultipartUpload: {\n      Parts: req.body.parts\n    },\n    UploadId\n  }).promise();\n// data = {\n  //   Bucket: \"my-bucket-name\", \n  //   ETag: \"some-hash\", \n  //   Key: \"filename.ext\", \n  //   Location: \"https://my-bucket-name.s3.amazonaws.com/filename.ext\"\n  // }\n  res.send({\n    ...data\n  });\n});\n```\nTL;DR: it is possible, so feel free to close the ticket, IMHO.. I am on the same boat, my use case is also partial uploads for big files on a JS client side. I want people to be able to resume uploads if they lose their connection, without losing all previously uploaded chunks. And I don't want to expose any credentials (thus not using SDK on client)\n~I will update this comment once I solve it.~\nUPDATE: following @sandyghai guide, I was able to do it.\nThere may be syntax errors, as my backend does not use express, but I felt writing it ala express would help other devs understand it easier.\nContext: I have an API (behind auth obviously) to which users can send files, and it uploads them to S3. As I didn't want to set IAM for each user of my app, nor put the SDK in the front-end, I decided to go with a back-end authorized approach.\n```js\napp.post('/upload', (req, res) => {\n  let UploadId = req.body.UploadId;\nconst params = {\n    Bucket: 'my-bucket-name',\n    Key: req.body.filename\n  };\n// Initialize the multipart - no need to do it on the client (although you can)\n  if (req.body.part === 1) {\n    const createRequest = await s3.createMultipartUpload(params).promise();\n    UploadId = createRequest.UploadId;\n  }\n// Save createRequest.UploadId in your front-end, you will need it. \n  // Also sending the uploadPart pre-signed URL for part #1\n  res.send({\n    signedURL: s3.getSignedUrl('uploadPart', {\n      ...params,\n      Expires: 60 * 60 * 24, // this is optional, but I find 24hs very useful\n      PartNumber: req.body.part\n    }),\n    UploadId,\n    ...params\n  });\n});\napp.post('/upload-complete', (req, res) => {\n  let UploadId = req.body.UploadId;\nconst params = {\n    Bucket: 'my-bucket-name',\n    Key: req.body.filename\n  };\nconst data = await s3.completeMultipartUpload({\n    ...params,\n    MultipartUpload: {\n      Parts: req.body.parts\n    },\n    UploadId\n  }).promise();\n// data = {\n  //   Bucket: \"my-bucket-name\", \n  //   ETag: \"some-hash\", \n  //   Key: \"filename.ext\", \n  //   Location: \"https://my-bucket-name.s3.amazonaws.com/filename.ext\"\n  // }\n  res.send({\n    ...data\n  });\n});\n```\nTL;DR: it is possible, so feel free to close the ticket, IMHO.. I have already posted the back-end code.\nThe front-end doesn't do anything especial, just a fetch with method PUT and passing the body binary buffer.. I have already posted the back-end code.\nThe front-end doesn't do anything especial, just a fetch with method PUT and passing the body binary buffer.. That's not how multipart uploads work, you'd need authentication on each request.\nMy front-end is within an electron app, so it uses fs to read files in chunk and I can't share it due to legal contracts with my company. But should be doable with a FileReader API Stream like https://github.com/maxogden/filereader-stream. That's not how multipart uploads work, you'd need authentication on each request.\nMy front-end is within an electron app, so it uses fs to read files in chunk and I can't share it due to legal contracts with my company. But should be doable with a FileReader API Stream like https://github.com/maxogden/filereader-stream. ",
    "prestonlimlianjie": "@oyeanuj I've created a functioning demo repo with multipart + presigned URL uploads from the browser:\nhttps://github.com/prestonlimlianjie/aws-s3-multipart-presigned-upload. Hi friends!\nI realized that this was a topic that did not have much documentation, so I made a demo repo in case anyone wanted to reference my implementation of multipart+presigned uploads to S3.\nhttps://github.com/prestonlimlianjie/aws-s3-multipart-presigned-upload. ",
    "fadelafuente1": "@shooding thanks for that advise, that was my problem. Now I can upload files with presigned url and multipart upload.\nI will try to post a medium post with the full process of multipart upload with presigned url.\n@shooding : \"The tricky part is that Content-Type is not required (or say should not exists) in UploadPart requests,\nwhich is different from PutObject (If you have tried low-level REST PutObject to presigned url, you will know what i'm saying).\". ",
    "gjtorikian": "Wow, awesome, thanks!\nAn npm release when you get a chance would be :heart: :heart: :heart: .\n. Wow, awesome, thanks!\nAn npm release when you get a chance would be :heart: :heart: :heart: .\n. ",
    "devinivy": "Cool!  Just like to add (purely for clarity) that the SDK does load the INI credentials, just not the INI config, which is an awkward partial amount of support for sharing credentials across the SDK and CLI.  For now I've had Opsworks write this configuration as JSON to the opsworks.js file.  Thanks!\n. ",
    "lpetre": "I just ran into this too. It would be great if there was more consistency here. +1\n. I just ran into this too. It would be great if there was more consistency here. +1\n. ",
    "TimothyGu": "+1. @AdityaManohar would you accept a pull request adding this feature?\n. @lsegal, makes sense. Is such a pull request at all possible? Is there some centralized issue tracker or should I submit the pull request to this repo only and ping all other SDK teams? Thanks.\n. @lsegal cool thanks!\n. ",
    "Dudemullet": "@lsegal Sorry, I might not be understanding the problem or there may be way more underlying issues I don't understand but, isn't just adding variables to ~/.aws/config and ~/.aws/credential the same as having all the environment variables in a system? \nBy this I mean, if you're using the aws.Lambda part of the sdk, it will probably be relevant to pull in function_name from the global config. Whereas if you use aws.S3 maybe pull in bucket and body.\nJust how right now the AWS sdk doesn't care that I have a NODE_ENV or CATALINA_HOME environment variables set, why would the Lambda sdk care you have a variable set for s3?\nI'm just glad this isn't a closed issue and keep up the hope it will make it in some day. I'd be glad (as others have volunteered as well) to contribute a PR :smile: \n. Thanks for the explanation @lsegal :+1:\n. ",
    "craigphume": "Thanks for the reply Isegal\nI have tried that sorry\nSnippet below\nvar s3bucket = new AWS.S3({params: {Bucket: 'itctestbucket', region: 'ap-southeast-2' }});\n    s3bucket.createBucket(function (err) {\n        if(err) {\n            console.log('Error: ' + err);\n            return callback(err, null);\n        }\n        var params = {Key: key, Body: body};\n        s3bucket.upload(params).on('httpUploadProgress', function (evt) {\n            console.log(evt);\n        }).send(function (err, data) {\n            console.log(err, data);\n            return callback(null, true);\n        });\n    });\nUsing the aws command line tool on linux I can create buckets in ap-southeast-2 region\ncraighume@host:~\n$ aws s3 mb s3://isegal-craig-test\nmake_bucket: s3://isegal-craig-test/\n. Thanks for the reply Isegal\nI have tried that sorry\nSnippet below\nvar s3bucket = new AWS.S3({params: {Bucket: 'itctestbucket', region: 'ap-southeast-2' }});\n    s3bucket.createBucket(function (err) {\n        if(err) {\n            console.log('Error: ' + err);\n            return callback(err, null);\n        }\n        var params = {Key: key, Body: body};\n        s3bucket.upload(params).on('httpUploadProgress', function (evt) {\n            console.log(evt);\n        }).send(function (err, data) {\n            console.log(err, data);\n            return callback(null, true);\n        });\n    });\nUsing the aws command line tool on linux I can create buckets in ap-southeast-2 region\ncraighume@host:~\n$ aws s3 mb s3://isegal-craig-test\nmake_bucket: s3://isegal-craig-test/\n. Oops you are correct, but it didn't fix anything.\nI have tried setting \nAWS.config.region = 'ap-southeast-2' and AWS.config.update({region:  'ap-southeast-2'  });\nBoth to no avail.\nThis is a full script that I just did and it doesn't work with the 'ap-southeast-2' region either\nvar AWS = require('aws-sdk');\nAWS.config.region = 'ap-southeast-2';\nAWS.config.update({\n    accessKeyId: '<ID>',\n    secretAccessKey: '<KEY>'\n});\nvar s3bucket = new AWS.S3({\n    params: {\n        Bucket: 'itctestbucket'\n    },\n    region: 'ap-southeast-2'\n});\ns3bucket.createBucket(function (err) {\n    if(err) {\n        console.log('Error: ' + err);\n    }\n    var params = {Key: 'CRAIG', Body: 'HUME'};\n});\n. Oops you are correct, but it didn't fix anything.\nI have tried setting \nAWS.config.region = 'ap-southeast-2' and AWS.config.update({region:  'ap-southeast-2'  });\nBoth to no avail.\nThis is a full script that I just did and it doesn't work with the 'ap-southeast-2' region either\nvar AWS = require('aws-sdk');\nAWS.config.region = 'ap-southeast-2';\nAWS.config.update({\n    accessKeyId: '<ID>',\n    secretAccessKey: '<KEY>'\n});\nvar s3bucket = new AWS.S3({\n    params: {\n        Bucket: 'itctestbucket'\n    },\n    region: 'ap-southeast-2'\n});\ns3bucket.createBucket(function (err) {\n    if(err) {\n        console.log('Error: ' + err);\n    }\n    var params = {Key: 'CRAIG', Body: 'HUME'};\n});\n. Update...\nI build the same example from above in php/laravel4, got the same result until I added the key 'LocationConstraint' => 'ap-southeast-2' to the create bucket array\n$s3 = App::make('aws')->get('s3');\n    $s3->createBucket(array('Bucket' => 'itctestbucket', 'LocationConstraint' => 'ap-southeast-2',));\nonce I did this it created the bucket as expected.\nDoes the node code have this parameter, I added it to the to \nvar s3bucket = new AWS.S3({\n    params: {\n        Bucket: 'itctestbucket'\n    },\n    LocationConstraint: 'ap-southeast-2'\n});\nBut no change\n. Update...\nI build the same example from above in php/laravel4, got the same result until I added the key 'LocationConstraint' => 'ap-southeast-2' to the create bucket array\n$s3 = App::make('aws')->get('s3');\n    $s3->createBucket(array('Bucket' => 'itctestbucket', 'LocationConstraint' => 'ap-southeast-2',));\nonce I did this it created the bucket as expected.\nDoes the node code have this parameter, I added it to the to \nvar s3bucket = new AWS.S3({\n    params: {\n        Bucket: 'itctestbucket'\n    },\n    LocationConstraint: 'ap-southeast-2'\n});\nBut no change\n. Node is v0.10.35\naws-sdk is aws-sdk@2.1.7\nI copied your script and it worked!?\nMust be my code then.\nThan you for your quick help, Craig\n. Node is v0.10.35\naws-sdk is aws-sdk@2.1.7\nI copied your script and it worked!?\nMust be my code then.\nThan you for your quick help, Craig\n. ",
    "ramonck": "Hi,\nThe function works fine within NodeJS, but it's not working from web interface.\nthe \" .to\" is really the first argument it's expecting from the function call, and it's not getting it from the web interface library, from nodejs it's working fine.\n\nvar AWS = require(\"aws-sdk\");\nexports.handler = function(event, context) {\n  var _callbacked = false;\n  var _callback = function(retval) {\nif (_callbacked) { return; }\n_callbacked = true;\nvar _sqsOptions = {\"accessKeyId\":\"[ACCESSKEY]\",\"secretAccessKey\":\"[SECRETKEY]\",\"region\":\"us-east-1\",\"Role\":\"arn:aws:iam::9999999:role/lambda_exec_role\",\"FunctionName\":\"MYFUNCTIONNAME\",\"Description\":\"send Email\",\"QueueUrl\":\"https://sqs.us-east-1.amazonaws.com/99999999/LambdaResultsQueue\"};\nvar _sqsClient = new AWS.SQS(_sqsOptions);\nvar _sqsParams = {\n  QueueUrl: \"https://sqs.us-east-1.amazonaws.com/9999999999/LambdaResultsQueue\",\n  MessageBody: JSON.stringify({FunctionName: \"MYFUNCTIONAME\", ReturnValue: retval, CallId: event.callId})\n}\n_sqsClient.sendMessage(_sqsParams, function(err, data) {\n  if (err) { throw err; }\n  context.done();\n});\n  };\n  (function (event, callback) {\n  // Running on AWS Lambda\n  var aws         = require('aws-sdk');\n  var ses = new aws.SES({apiVersion: '2010-12-01'});\n  var to = event.to;\n  var cc = event.cc;\nMy desire is to use the official library :8ball: \n. Hi,\nI don't see that as being related to what I'm actually reporting as the problem, I'm having correct permissions to access my function in Lambda from Web and Backend, I understand it as being a better practice your point of view, the problem is exactly in this line var to = event.to; in which I'm defining a variable coming in from my JSON invoke parameter and it's not there from the WEB API aws-sdk-js and in the backend api works fine with the same parameters, that's the problem.\n. Hi,\nI would understand that's AWS Lambda's parsing problem if the problem also ocurred from the backend, which is not case, it's only happening from the aws-sdk-js, that's why I'm directing the problem through here.\nOk, here's the full code.\n```\nvar AWS = require(\"aws-sdk\");\nexports.handler = function(event, context) {\n  var _callbacked = false;\n  var _callback = function(retval) {\n    if (_callbacked) { return; }\n    _callbacked = true;\n    var _sqsOptions = {\"accessKeyId\":\"ACCESSKEY\",\"secretAccessKey\":\"SECRETKEY\",\"region\":\"us-east-1\",\"Role\":\"arn:aws:iam::9999999999:role/lambda_exec_role\",\"FunctionName\":\"FUNCTIONAME\",\"Description\":\"send Email\",\"QueueUrl\":\"https://sqs.us-east-1.amazonaws.com/999999999/LambdaResultsQueue\"};\n    var _sqsClient = new AWS.SQS(_sqsOptions);\n    var _sqsParams = {\n      QueueUrl: \"https://sqs.us-east-1.amazonaws.com/99999999/LambdaResultsQueue\",\n      MessageBody: JSON.stringify({FunctionName: \"FUNCTIONNAME\", ReturnValue: retval, CallId: event.callId})\n    }\n    _sqsClient.sendMessage(_sqsParams, function(err, data) {\n      if (err) { throw err; }\n      context.done();\n    });\n  };\n  (function (event, callback) {\n  // Running on AWS Lambda\n  var aws         = require('aws-sdk');\n  var ses = new aws.SES({apiVersion: '2010-12-01'});\n  var to = event.to;\n  var cc = event.cc;\n  var bcc = event.bcc;\n  var from = event.from;\n  var subject = event.subject;\n  var textMessage = event.text_message;\n  var htmlMessage = event.html_message;\n  var charSet = \"UTF-8\";\n  var params = {\n    Destination: {\n      BccAddresses: bcc ? bcc : [],\n      CcAddresses: cc ? cc : [],\n      ToAddresses: to\n    },\n    Message: {\n      Body: {},\n      Subject: {\n        Data: subject ? subject : \"No subject\",\n        Charset: charSet\n      }\n    },\n    Source: from,\n    ReplyToAddresses: [from]\n  };\nif (textMessage != null && textMessage.length > 0) {\n    params[\"Message\"][\"Body\"][\"Text\"] = {\n      Data: textMessage,\n      Charset: charSet\n    }\n  }\n  if (htmlMessage != null && htmlMessage.length > 0) {\n    params[\"Message\"][\"Body\"][\"Html\"] = {\n      Data: htmlMessage,\n      Charset: charSet\n    }\n  }\nses.sendEmail(params, function(err, data) {\n    if (err) {\n      console.log(err, err.stack); // an error occurred\n    } else {\n      console.log(data);           // successful response\n      context.done();\n    }\n  });\n})(event.args, _callback);\n};\n```\nThe invoking part from aws-sdk-js\nvar params = {\n        FunctionName: functionName,\n        InvokeArgs: JSON.stringify({to: ['NAME@NAME'], from:'NAME@NAME', subject:'MY SUBJECT', text_message:'BODY TEXT MESSAGE'})\n      }\n      console.log(invokeArgs);\n      this.lambdaClient.invokeAsync(params, function(err, data) {\n            if (err) { throw 'Erro: '+err } else { console.log(data);}\n      });\n. You tackled it! Thanks for the response, that solved my day! I'm now back on track on utilizing Lambda :)\n. I get this feedback in CloudWatch\nUnable to parse input as json\n. I did try that in the first place.\nAlso tried now again to retest, and still the same problem.\nFirst string worked\nteste\nSecond didn't\nteste\\n\\nasdfasd\nFeedback on CloudWatch\nUnable to parse input as json\n. Ok Thanks\n. Hi Aditya,\nSo I'm building a scenario to authenticate a user directly on a HTML page utilizing aws-sdk-js.\nI'm worried about the security implications of this scenario. I've drawed it so you can understand ir better.\n\nI want to know what are the best practices and what are the security implications regarding what's in red.\nBest Regards,\n. Hi Aditya,\nThanks for the feedback, yes you nailed it. I never thought about using Cognito, always had in mind as being a game api, it sure fits like a glove in my solution puzzle.\n Found this post on utilizing the AWS-SDK-JS API\nhttp://mobile.awsblog.com/post/TxBVEDL5Z8JKAC/Use-Amazon-Cognito-in-your-website-for-simple-AWS-authentication\n```\n// The parameters required to intialize the Cognito Credentials object.\n// If you are authenticating your users through one of the supported\n// identity providers you should set the Logins object with the provider\n// tokens. For example:\n// Logins: {\n//   graph.facebook.com : facebookResponse.authResponse.accessToken\n// }\nvar params = {\n    AccountId: \"YOUR_AWS_ACCOUNT_ID\",\n    RoleArn: \"arn:aws:iam::6157xxxxxxxx:role/a_valid_aws_role_arn\",\n    IdentityPoolId: \"YOUR_COGNITO_IDENTITY_POOL_ID\"\n};\n// set the Amazon Cognito region\nAWS.config.region = 'us-east-1';\n// initialize the Credentials object with our parameters\nAWS.config.credentials = new AWS.CognitoIdentityCredentials(params);\n// We can set the get method of the Credentials object to retrieve\n// the unique identifier for the end user (identityId) once the provider\n// has refreshed itself\nAWS.config.credentials.get(function(err) {\n    if (!err) {\n        console.log(\"Cognito Identity Id: \" + AWS.config.credentials.identityId);\n    }\n});\n// Other service clients will automatically use the Cognito Credentials provider\n// configured in the JavaScript SDK.\nvar cognitoSyncClient = new AWS.CognitoSync();\ncognitoSyncClient.listDatasets({\n    IdentityId: AWS.config.credentials.identityId,\n    IdentityPoolId: \"YOUR_COGNITO_IDENTITY_POOL_ID\"\n}, function(err, data) {\n    if ( !err ) {\n        console.log(JSON.stringify(data));\n    }\n});\n```\nWanted to know now how do I pass in the username and password fields from the form, I don't want to do federated login for now (seems that it's going to complicate for now). \nBest Regards,\n. I just want a simple login and password with Cognito authentication scenario, how do I do it in JS API?\n. I've seen in some tutorials that I can send them to login at Facebook and when they comeback Cognito will fetch the identity, does it work like that or do I have to send the user to login to facebook through a special Cognito API on the JS SDK?\n. Aditya,\nYou nailed it once again. Thank you very much for all your help, I now continue on being a happy aws-sdk-js user :).\nKeep up the great work.\nBest Wishes,\n. Worked like a charm, thank you.\n. Yes,\nI can provide some feedback :).\nI don`t want to load AWS SDK JS synchronously due to the critical rendering path, I want to keep up with the 100% on the Page Insights (Google) so I want to keep on loading the majority of my script async.\nWill do what you said.\nBest Regards\n. The error doesn't come up anymore as your solution solves it, but the Cognito behavior is a bit weird now, sometimes is works sometimes it doesn't, I don't see the Cognito log in a couple of refreshes sometimes yes, others no. Here`s the url http://m.jtavares.com.br.s3-website-us-east-1.amazonaws.com/\nBest Regards\n. I'm certain you're right about the caching, the caching mechanism is messing up the loading of the aws sdk.\nI'm loading is as <script async> call separatly, will try to call it dynamically inside my script so I can enforce the order (frontend.js)\nThanks for the advice, will give feedback if it worked well.\nBest Regards,\n. Solved it, loaded async within script and putted the AWS code (Cognito) dependent on it.\nThanks again for the help, it's now working fully cached ;).\nBest Regards\n. Thanks for the feedback, I'm not too worried about the size, I'm worried about the score, want to keep up the 100%.\nWhat would be a good practice for lazy loading the unath role in cognito so I don't have this issue on page load, that way I would turn around this issue and keep the 100% without waiting for a 600kb reduction.\nBest Regards,\n. I'm already doing that, I had a problem calling the Cognito long after aws sdk has been loaded, it's async already. \nThe problem is that today I call cognito right after exactly within a \"document.ready\" in JQuery, but the problem is that Google's Page Insights is parsing the page until it finished the initial load, therefore I have to call cognito when I click on a link.\nI'll play around and see calling up cognito unauth within a \"link\", JQuery -> on('click'...\nlet's see how it rolls out, not sure if it will be fast enough.\nBest Regards,\n. Thanks for the help, I did get the 100% back :), yes!\nI only call cognito now after clicking where there is an action with Lambda, so there's a need for cognito. It's good because my customer will save on API calls to Cognito at least on the first page look only if someone visited by mistake, if it was a bot, etc. So the effort was valid :+1: \nBest Regards\n. Thank you guys for the help!\n. ",
    "kilaka": "We had this issue recently (but for receive) and after we restarted the java client, the message receive worked.\nThis is quite strange, because we poll for only 10 messages and when looking at the queue using Amazon's UI, the largest message was 7KB.\nWe have a suspicion that something got corrupted in the client SDK code, causing to poll a larger chunk of data.. We had this issue recently (but for receive) and after we restarted the java client, the message receive worked.\nThis is quite strange, because we poll for only 10 messages and when looking at the queue using Amazon's UI, the largest message was 7KB.\nWe have a suspicion that something got corrupted in the client SDK code, causing to poll a larger chunk of data.. ",
    "nemanjavuk": "Here's the raw request:\n<CompleteMultipartUpload xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\" getKey=\n\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\"><Part getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\"><ETag getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\">&quot;be18919d68022a42053129186a3c7da1&quot; getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\"</ETag><PartNumber getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\">1 getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\"</PartNumber></Part><Part getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\"><ETag getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\">&quot;f4ff1342ee44e1c27405fb7c88a18140&quot; getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\"</ETag><PartNumber getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\">2 getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\"</PartNumber></Part><Part getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\"><ETag getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\">&quot;b37ea7944af3a36e79a9c4e803b0afc0&quot; getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\"</ETag><PartNumber getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\">3 getKey=\"function (value) {\n    for (var key in this) {\n        if (this[key] == value) {\n            return key;\n        }\n    }\n    return null;\n}\"</PartNumber></Part></CompleteMultipartUpload>\nI must admit myself that this doesn't look ok at least according to this: http://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadComplete.html .\nI'll also take a look at s3.upload().\n. @lsegal, you're completely right. The Object.prototype was modified in the project and I wasn't aware of it. Sorry for the false alarm.\n. ",
    "henriquebf": "node v0.10.25 / SDK 2.1.8 (it also fails on node v0.10.36)\njavascript\nError: Unrecognized shape type: map\n    at Function.create (/Users/henriqueferreira/Documents/Projects/www/flux-drive/node_modules/AWS-sdk/lib/model/shape.js:130:13)\n    at /Users/henriqueferreira/Documents/Projects/www/flux-drive/node_modules/AWS-sdk/lib/model/api.js:46:18\n    at /Users/henriqueferreira/Documents/Projects/www/flux-drive/node_modules/AWS-sdk/lib/model/collection.js:5:12\n    at Collection.Sy (/Users/henriqueferreira/Documents/Projects/www/flux-drive/node_modules/AWS-sdk/lib/util.js:713:23)\n    at Function.resolve (/Users/henriqueferreira/Documents/Projects/www/flux-drive/node_modules/AWS-sdk/lib/model/shape.js:80:38)\n    at Function.create (/Users/henriqueferreira/Documents/Projects/www/flux-drive/node_modules/AWS-sdk/lib/model/shape.js:94:24)\n    at /Users/henriqueferreira/Documents/Projects/www/flux-drive/node_modules/AWS-sdk/lib/model/shape.js:159:20\n    at /Users/henriqueferreira/Documents/Projects/www/flux-drive/node_modules/AWS-sdk/lib/model/collection.js:5:12\n    at Collection.Metadata (/Users/henriqueferreira/Documents/Projects/www/flux-drive/node_modules/AWS-sdk/lib/util.js:713:23)\n    at Object.each (/Users/henriqueferreira/Documents/Projects/www/flux-drive/node_modules/AWS-sdk/lib/util.js:535:54)\nSee below my code, but it also fails with a the most simple examples at Amazon's docs using the property \"upload\".\n``` javascript\nvar mainPath = process.cwd();\nvar fluxDriveSettings = require(mainPath + \"/flux-drive.js\");\nvar AWS = require('aws-sdk');\n//\nvar debug = require('debug');\nmodule.exports = {\nuploadS3: function (bucketKey, fileExtension, fileStream) {\n\n    // Upload files to S3 Bucket\n\n    AWS.config.update({\n        accessKeyId: fluxDriveSettings.config.cdn.accessKeyId,\n        secretAccessKey: fluxDriveSettings.config.cdn.secretAccessKey,\n        region: 'us-east-1'\n    });\n\n    var bucketName = fluxDriveSettings.config.cdn.bucket;\n\n    var s3 = new AWS.S3();\n\n    var params = {\n        ACL: \"public-read\",\n        Bucket: bucketName,\n        Key: bucketKey,\n        Body: fileStream\n    };\n\n    // Set Content-Type for video/image visualization on S3 link\n\n    if(fileExtension == \"png\" || fileExtension == \"gif\") {\n        params['ContentType'] = \"image/\" + fileExtension;\n    } else if(fileExtension == \"jpg\" || fileExtension == \"jpge\") {\n        params['ContentType'] = \"image/jpeg\";\n    } else if(fileExtension == \"mp4\") {\n        params['ContentType'] = \"video/\" + fileExtension;\n    } else if(fileExtension == \"mov\") {\n        params['ContentType'] = \"video/quicktime\";\n    }\n\n    // Upload stream\n\n    s3.putObject(params, function(err, data) {\n        if(err) {\n            // Upload error\n            debug(err.stack);\n        } else {\n            // Upload success\n            debug(\"cdn::uploadS3: upload \" + bucketKey);\n        }\n    });\n\n}\n\n}\n```\n. ",
    "tmcw": "I'm using browserify: it looks like the culprit is https://github.com/crypto-browserify/createHmac/pull/4\n. Fixed upstream in createHmac 1.1.3.\n. ",
    "cprcrack": "I'm experiencing the same issue. As per the source file at metadata_service.js, the httpOptions and its timeout is still being ignored. Shouldn't this fix be present already in 2.391.0?. Can we reopen this issue or should I open a new one with the same information?. ",
    "lazdmx": "No, it works as expected in 0.11.14 and less versions. Lib version is 2.1.8\n. Here is a bash session:\n```\ncomp:~ nvm use default\nNow using node v0.11.14\ncomp:~ node -v\nv0.11.14\ncomp:~ node junk.js \nurl is: https://.amazonaws.com/some.key?AWSAccessKeyId=&Expires=1422649851&Signature=MF2qqIw0r5Cp3TYxx0xBB53L9aw%3D\ncomp:~   nvm use next\nNow using node v0.11.15\ncomp:~  node -v\nv0.11.15\ncomp:~  node junk.js \nurl is: https://.s3.amazonaws.com/\ncomp:~  cat junk.js \nvar AWS = require( \"aws-sdk\" )\nvar s3  = new AWS.S3()\ns3.getSignedUrl('putObject', {Key: 'some.key', Bucket: '/'}, function(err, url){\n  console.log('url is: %s', url)\n})\n```\n. Checked, it works on node 0.11.16. \n. ",
    "arbitrarytech": "@lsegal Thanks for the info! I have started mocking out the calls to the SDK that I am using and it is very successful so far. I now have functional mocks of an SQS queue as well as some of the EC2 calls I am making. In each of my mocks I now basically have something like this:\n```\nvar AWS = require('aws-sdk'),\n  sqs = new AWS.SQS();\n...\nvar build = sqs.sendMessage(params).build();\nif (build.response.error) return cb(build.response.error);\n```\nWith this, I get the the validation you guys provide in the SDK and I can mock the backend behavior myself. If it turns out decently, I would be happy to release my \"mock-aws-sdk\" open source. Thanks again for the help.\np.s. I consolidated my GitHub accounts which is why I turned to a \"ghost\".\n. ",
    "manzanofab": "Hi Thank you so much for prompt reply, I am using IE 11. I dont understand why, I have also try IE in different computers with the same outcome.\nThis is a bigger portion of my code\nThis morning I got a new error, not sure if it is because before I was usingan older version of the sdk\naddEventLIstener\n    \n\n      j$ = jQuery.noConflict(); </p>\n<p>```\n  function blockme() {</p>\n<pre><code>var stringvalue='{!RandomFileName}';\nAWS.config.update({accessKeyId: '.......', secretAccessKey: '......'});\nAWS.config.region = 'ap-southeast-2';\n\nvar bucket = new AWS.S3({params: {Bucket: 'jdl-sf-lu5n6evy'}});\nvar fileChooser = document.getElementById('file-chooser');             \nvar file = fileChooser.files[0];\n\nvar type = document.getElementById('{!$Component.formId.blockid.sectionid.itemid1.type}').value;\nvar documetn = document.getElementById('{!$Component.formId.blockid.sectionid.itemid2.documentId}').value;\nvar description = document.getElementById('{!$Component.formId.blockid.sectionid.itemid3.descriptionId}').value;\n\nif(type != \"\" &amp;&amp; documetn !=\"\" &amp;&amp; description !=\"\") {\n\n  if (file) {\n    j$.blockUI({ css: {\n      border: 'none',\n      padding: '15px',\n      backgroundColor: '#000',\n      '-webkit-border-radius': '10px',\n      '-moz-border-radius': '10px',\n      opacity: .5,\n      color: '#fff'\n      }\n    });\n    var finenamefull=file.name;\n    finenamefull = finenamefull.replace(\",\", \"\"); \n    var arrayOfStrings = finenamefull.split('.');\n    var noofStrings=arrayOfStrings.length;\n    var extension=arrayOfStrings[noofStrings-1];\n    var filenamewithext=stringvalue+'.'+extension;\n    var params = {Key: filenamewithext, ContentType: file.type, Body: file};\n    var r = finenamefull+','+file.type;\n    document.getElementById(\"awsId\").value = r;\n\n    bucket.putObject(params, function (err, data) {\n      console.log('data inside function: ' + data);\n      console.log('err inside function: ' + err);\n      alert('Message inside bucket function');\n\n      j$.unblockUI();\n      var z = 'status'\n      document.getElementById(\"awsIdStatus\").value = z;\n      console.log(err);\n      console.log(data);\n      showStatus();\n    });\n  } \n}\n</code></pre>\n<p>}//END FUNCTION</p>\n<p> \n```\n\n. \n. Hi Thank you so much for your prompt reply, I am sorry i just realised that it didnt  copy all my code\nyes I am using t in a Visualforce page \nthe current version that I am using is \"sdk.amazonaws.com/js/aws-sdk-2.1.8.min.js\"\non firebug I wasnt able to find sarissa.js in the find tool\n\n. So this means that I cant use aws-sdk-2.1.8.min.js in visualforce? or is there a work around ?\nActually (not sure if already mentioned but) this visualforce page works fine in chrome and in firefox\n. HI Thank you so much for the prompt response.\nI am sorry I know that this will be out of topic, but I am not familiar with XMLHttpRequest, from google I understand that this are the headers of the page?\n\nthis is what  get in the console tool network tab\nThank you so much\n. Hi \nthank you very much for that, this is the result when calling it from a onload:\nwindow.onload=function(){\nis the following:\nfunction() {\n    if (!_SARISSA_XMLHTTP_PROGID) {\n        _SARISSA_XMLHTTP_PROGID = Sarissa.pickRecentProgID([\"Msxml2.XMLHTTP.6.0\", \"MSXML2.XMLHTTP.3.0\", \"MSXML2.XMLHTTP\", \"Microsoft.XMLHTTP\"]);\n    }\n    return new ActiveXObject(_SARISSA_XMLHTTP_PROGID);\n}\n\nthis is from firefox \n\nSo I will need to manipulate that ?\n. again thank you so much for the help\n. Thank you so much, that works perfectly!!!!!\n. @proerp5 \nYou dont have to do anything special just put your js code between this lines\n```\n\n    Sarissa.XHR = XMLHttpRequest;\n    XMLHttpRequest = Sarissa.originalXMLHttpRequest;</p>\n<pre><code>//YOUR CODE\n\n\nXMLHttpRequest = Sarissa.XHR\n</code></pre>\n<p>\n```\n`\n. @proerp5\nCan you paste your code? or add on a repository?\nmaybe try having first only those 3 lines and where it says //YOUR CODE put console.log('anything'), and one after, just to check if it is your browser doing something \n. Now that you mentioned IE, I just remember (as this was from my old job), I couldn't work it out in IE, from memory I dont think it was the same errror, but I couldnt make it work in IE.\nI am sorry\n. ",
    "proerp5": "Hi Aditya,\nWe are also facing the similar issue in our salesforce Environment, so can you please elaborate the solution as we do not have much idea about how the XMLHTTP works and how to replace the Sarissa's XHR to get this working.\nit would be nice if you could immediately reply to this, asit is causing a major issue for us\nthanks,\nproerp5\n. Thanks for the reply.\nyes, i tried doing that and it is not working even after this.\nthen , i tried assigning XMLHttpRequest = Sarissa.originalXMLHttpRequest; after my code but then i am getting error for a param 'QueueId'.\nthe solution that had been suggested doesn't seems to work in my case and i dont seem to understand why it isnt working as it did  for you. \nThanks,\nproerp\n. below is the code snippet and it seems that the uploadDocumentandMessageToQueue seems to error out in case of IE 10+ browser:\nvar meta2 = JSON.parse(metadata);\nif(filesList[i].type=='text/xml' || filesList[i].type=='application/xml'){\nmeta2.structured = 'Y';\n}\nmeta2.originalFileName= filesList[i].name;\nmeta2.mimeType=filesList[i].type;\nmeta2.loanEvent = 'Origination';\n```\n            Sarissa.XHR = XMLHttpRequest;\n            alert('reqearlier: '+XMLHttpRequest);//same as git\n            XMLHttpRequest = Sarissa.originalXMLHttpRequest;\n            alert('req: '+XMLHttpRequest);//native code block\n        uploadDocumentandMessageToQueue(token, jQuery('#fileToUpload')[0].files[i], JSON.stringify(meta2), 'handleReturn');\n\n\n        if(Sarissa.originalXMLHttpRequest) {\n            XMLHttpRequest = XMLHttpRequest;\n        } else {\n                XMLHttpRequest = Sarissa.XHR;\n            }\n\n```\n. below is the code snippet and it seems that the uploadDocumentandMessageToQueue seems to error out in case of IE 10+ browser:\nvar meta2 = JSON.parse(metadata);\n                    if(filesList[i].type=='text/xml' || filesList[i].type=='application/xml'){\n                        meta2.structured = 'Y';\n                    }\n                    meta2.originalFileName= filesList[i].name;\n                    meta2.mimeType=filesList[i].type;\n                    meta2.loanEvent = 'Origination';\n```\n                Sarissa.XHR = XMLHttpRequest;\n                alert('reqearlier: '+XMLHttpRequest);//same as git\n                XMLHttpRequest = Sarissa.originalXMLHttpRequest;\n                alert('req: '+XMLHttpRequest);//native code block\n            uploadDocumentandMessageToQueue(token, jQuery('#fileToUpload')[0].files[i], JSON.stringify(meta2), 'handleReturn');\n\n\n            if(Sarissa.originalXMLHttpRequest) {\n                XMLHttpRequest = XMLHttpRequest;\n            } else {\n                    XMLHttpRequest = Sarissa.XHR;\n                }\n\n```\n. ",
    "piuccio": "I figured out what's causing the error. Somewhere the following exception is thrown\n{ [CredentialsError: Missing credentials in config]\n  message: 'Missing credentials in config',\n  errno: 34,\n  code: 'CredentialsError',\n  path: '~/.aws/credentials',\n  syscall: 'open',\n  time: Wed Feb 04 2015 16:53:08 GMT+0000 (GMT),\n  originalError: \n   { message: 'Could not load credentials from SharedIniFileCredentials',\n     errno: 34,\n     code: 'CredentialsError',\n     path: '~/.aws/credentials',\n     syscall: 'open',\n     time: Wed Feb 04 2015 16:53:08 GMT+0000 (GMT),\n     originalError: \n      { errno: 34,\n        code: 'ENOENT',\n        path: '~/.aws/credentials',\n        syscall: 'open',\n        message: 'ENOENT, no such file or directory \\'~/.aws/credentials\\'' } } } error\n. :grin: I'm the author of that plugin. Honestly I didn't understand the documentation, how to use the default credentials provider. A snippet of code would have helped. I'll try to change that\nAnyway the question is why on my machine I get a nice error message while on CI the message is hard to understand.\nTeamCity is using node 0.10.24\nLocal machine uses 0.10.35\n. @lsegal off topic, how do i specify a different file for credentials using the default config?\nAnyway, it's not related to the node version. The error Cannot read property 'removeAllListeners' of undefined happens when validate fails, the Body is a Buffer and the stream is chunked.\nYou can reproduce\n- clone this repo https://github.com/guardian/frontend.git\n- checkout branch debug-frequency-graph (it has extra logging)\n- npm install\n- (remember to use node 0.10.24)\n- grunt frequency_graph\nIt takes a while (~3 minutes) and generates a file (~5MB) that is then sent to S3.\n. oh sorry, any version is fine\n. Thanks\n. ",
    "oldshuren": "The current javascript implementation is using POST, not GET. But the problem is the same. No queue in the URL PATH, only QueueUrl in parameters.\n. My solution is add one line in lib/services/sqs.js to set the http request path, but I'm not sure it is the right solution.\n``` javascript\n  buildEndpoint: function buildEndpoint(request) {\n    var url = request.httpRequest.params.QueueUrl;\n    if (url) {\n      request.httpRequest.endpoint = new AWS.Endpoint(url);\n     // http request need queue in the url path\n      request.httpRequest.path = request.httpRequest.endpoint.path || '/';\n  // signature version 4 requires the region name to be set,\n  // sqs queue urls contain the region name\n  var matches = request.httpRequest.endpoint.host.match(/^sqs\\.(.+?)\\./);\n  if (matches) request.httpRequest.region = matches[1];\n}\n\n}\n```\n. Both npm install aws-sdk and github version have the problem. \nThe aws server won't complain, only when you talk to CMB server or use wireshark to capture the packet  you'll see it.\nAnd Java SDK version is correctly set the queue as URL PATH.\nThe parameters are fine. The problem  is the URL's path. I think the URL is initially derived from the endpoint of the server,  there is no queue info in there. I guess that is the reason there is a buildEndpoint() function in sqs.js to modify the endpoint of the http request, but the that modification is not really reflected in the real http request sent out to the server. \n. Comcast released an open source implementation of Amazon's SQS and SNS server. They call it Cloud Message Bus (CMB), here is the github link\nhttps://github.com/Comcast/cmb\nIn Comcast's implementation, if the queue info is missing in the path, the server will return AWS.SimpleQueueService.NonExistentQueue error.\n. The JAVA AWS SDK uses only queue name in path, no QueueUrl in the body. And the Amazon's own document \"Making Query Request\" http://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/MakingRequests_MakingQueryRequestsArticle.html says the queue name should be in the URL path. And the SQS service wsdl http://queue.amazonaws.com/doc/2012-11-05/QueueService.wsdl doesn't mention QueueUrl as parameter\n. @lsegal I'll open an issue in Comcast/cmb, but aws-sdk should also use its own standard. At least among different languages, such as java and javascript. Now java and javascript use different implementation, it really confuses people.\nBy the way, I notice another difference between java and javascript implementations. Java will reuse the TCP connection, but javascript will open a new TCP connection for each request. It impacts performance somehow :-1: \n. Thanks! \nBy the way, I submitted an issue in comcast/cmb.\n. @AdityaManohar I think aws-sdk-js should be modified to put queue info in the URL PATH, the QueueUrl can be still used. The reasons are,\n1. It is what Amazon's documentation specified. If someone implements a server they will follow the specification, not reverse engining existing code.\n2. Different language implementations should use the same protocol on the wire. \nFor me personally I have to patch the aws-sdk, because it is much easier to patch aws-sdk-js (just add one line) than to patch the cmb/sqs.\nThanks!\n. ",
    "AlaRuba": "Cool. Is this the way you recommend putting stuff into an S3 bucket?\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-examples.html\nI tried that but I had problem doing it that way. Is this the appropriate place to ask that kind of question?\n. I decided to use the SDK. I want to try loading all the things in a bucket. My bucket is empty btw.\nvar config = new AWS.Config({\n  accessKeyId: 'ACESSKEY', secretAccessKey: 'SECRETKEY', region: 'us-west-2'\n});\nvar bucket = new AWS.S3({params: {Bucket: 'stratusview'}});\n  bucket.listObjects(function (err, data) {\n    if (err) {\n      console.log(err)\n      document.getElementById('status').innerHTML =\n        'Could not load objects from S3';\n    } else {\n      document.getElementById('status').innerHTML =\n        'Loaded ' + data.Contents.length + ' items from S3';\n      for (var i = 0; i < data.Contents.length; i++) {\n        document.getElementById('objects').innerHTML +=\n          '<li>' + data.Contents[i].Key + '</li>';\n      }\n    }\n  });\nHowever, I get a Error: Missing credentials in config {message: \"Missing credentials in config\", code: \"SigningError\", name: \"SigningError\".\nHowever, I believe I configured the credentials the right way. I know hardcoding the keys is a no-no but in this case I just need something to work really fast then I'll move to a better system.\nDoes this seem wrong?\nvar config = new AWS.Config({\n  accessKeyId: 'ACESSKEY', secretAccessKey: 'SECRETKEY', region: 'us-west-2'\n});\n. I just tried that and I still get the same error.\nI'm passing it like so\nvar bucket = new AWS.S3({credentials: config, params: {Bucket: 'stratusview'}});\nWhen I do a console.log(err). I see the keys in the credentials. However, my endpoint is null. Is that a cause of the problem.\n. Oh nevermind. I changed config to config.credentials. Now I'm getting different errors. I'll dig around and a bit before bugging you again. \nThanks so much the help and sorry to bug.\n. I'm closing this because my issue is solved.\n. ",
    "odeke-em": "If doing this in NodeJS or io.js on the server side, this is how I do it.\n``` javascript\n...\nvar fs = require('fs');\nvar outStream = fs.createWriteStream(LOCALPATH);\nvar awsStream = s3.getObject(params).createReadStream();\nawsStream.pipe(outStream);\n// Then monitor the events\nawsStream.on('end', function() {\n    // Done here\n}).on('error', function(err) {\n    // Handle your error\n});\n```\nBut yeah @lsegal solution is one I also use for the client side.\n. Thank you @AdityaManohar, to build upon your response, don't forget to use some sort of pagination since @piuccio you say you have > 1K objects.\nThe response returns values  of interest\nIsTruncated (type: boolean)\nNextMarker  (type: string)\nIf IsTruncated is set, repeat almost the previous query performed except this time set Marker to the value retrieved from NextMarker.\nI could be wrong so feel free to educate me.\n. Just curious are you able to synchronize your NTP server/clock with AWS' ? Several months ago, I encountered this error a couple of times, when creating signatures on my local machine. Hopefully this doc will be relevant: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html\n. False alarm. I found a method after searching through the documentation\nhere http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#restoreObject-property\njavascript\nvar params = {\n  Bucket: 'STRING_VALUE', /* required */\n  Key: 'STRING_VALUE', /* required */\n  RequestPayer: 'requester',\n  RestoreRequest: {\n    Days: 0 /* required */\n  },\n  VersionId: 'STRING_VALUE'\n};\ns3.restoreObject(params, function(err, data) {\n  if (err) console.log(err, err.stack); // an error occurred\n  else     console.log(data);           // successful response\n});\n. ",
    "miickel": "@AdityaManohar thanks! That did not work either. However, I got it to work. :pray: First, I had to set MessageStructure to 'json'. Then I formatted the message like this, so that quotes got escaped by stringify:\njavascript\n  var message = JSON.stringify({\n    \"GCM\": util.format('{\"data\":{\"message\":\"%s\"}}', 'Foo bar!')\n  });\n. ",
    "andryanmiller": "@miickel This saved me - thank you so much for following up with your solution.\n. @miickel This saved me - thank you so much for following up with your solution.\n. ",
    "biranchi2018": "I am trying from from AWS Console, but still its not working.\n\n. I am trying from from AWS Console, but still its not working.\n\n. ",
    "jpfranco": "Thanks for the info, I will take a look at Amazon Cognito then. I'm a bit curious though, is there  a particular reason why unauthenticated access is not allowed for multipart uploads?\n. Thanks for the info, I will take a look at Amazon Cognito then. I'm a bit curious though, is there  a particular reason why unauthenticated access is not allowed for multipart uploads?\n. @AdityaManohar thanks again. I'm trying out clearing the signature version and that makes the CredentialsError: Missing credentials in config error go away, but I now get this new error: \nAccessDenied: Anonymous users cannot initiate multipart uploads.  Please authenticate.\nI'm very positive that this bucket is unrestricted because I'm able to do a putObject without credentials, so maybe unauthenticated access is forbidden for multipart uploads after all?\n. @AdityaManohar thanks again. I'm trying out clearing the signature version and that makes the CredentialsError: Missing credentials in config error go away, but I now get this new error: \nAccessDenied: Anonymous users cannot initiate multipart uploads.  Please authenticate.\nI'm very positive that this bucket is unrestricted because I'm able to do a putObject without credentials, so maybe unauthenticated access is forbidden for multipart uploads after all?\n. Thanks. You're right, I'm able to get consistent results with Network Link Conditioner, sorry about that.\n. Thanks. You're right, I'm able to get consistent results with Network Link Conditioner, sorry about that.\n. ",
    "AnonymousOpposum": "Ah, wait. I think I found what I'm looking for here\nI'll come back and close once I've done a little testing.\n. Is it possible to decouple the config object of each service class from the global config object? I would like it if it were possible to make sure there were absolutely no way config settings leak from one user session to another.\n. Is it possible to decouple the config object of each service class from the global config object? I would like it if it were possible to make sure there were absolutely no way config settings leak from one user session to another.\n. Regarding arbitrary code, what users can and can not run, I'm creating one user interface element in my GUI for each method in the Node.js AWS API. After the user logs into my app, they can interact with the GUI, triggering calls to a restful interface on my server, bound to a wrapper of aws-sdk-js.\nThe wrapper provides the same interface as the clients in the aws-sdk namespace, but is chainable, promise based, and uses verbs like fetch instead of describe to match internal naming standards at the company.\nAfter finding the documentation above, I'm trying something like this for the wrapper's constructor:\n```\nvar AWS = require(\"aws-sdk\");\nfunction Interface( credential, region ) {\nthis.ec2 = new AWS.ec2({ ... });\n  [ ...other services are instantiated in the same way... ]\nthis.fetchVpc = fetchVpc;\nfunction fetchVpc() {\n this.ec2.describeVpcs( ... );\n\n ...\n\n}\n}\nmodule.exports = Interface;\n```\nThe wrapper is instantiated once per user session in the app server.\nI'll add a line that clobbers AWS.config on instantiation of the wrapper as well, and will write back with the result.\nThank you for the quick response! I appreciate it.\n. Regarding arbitrary code, what users can and can not run, I'm creating one user interface element in my GUI for each method in the Node.js AWS API. After the user logs into my app, they can interact with the GUI, triggering calls to a restful interface on my server, bound to a wrapper of aws-sdk-js.\nThe wrapper provides the same interface as the clients in the aws-sdk namespace, but is chainable, promise based, and uses verbs like fetch instead of describe to match internal naming standards at the company.\nAfter finding the documentation above, I'm trying something like this for the wrapper's constructor:\n```\nvar AWS = require(\"aws-sdk\");\nfunction Interface( credential, region ) {\nthis.ec2 = new AWS.ec2({ ... });\n  [ ...other services are instantiated in the same way... ]\nthis.fetchVpc = fetchVpc;\nfunction fetchVpc() {\n this.ec2.describeVpcs( ... );\n\n ...\n\n}\n}\nmodule.exports = Interface;\n```\nThe wrapper is instantiated once per user session in the app server.\nI'll add a line that clobbers AWS.config on instantiation of the wrapper as well, and will write back with the result.\nThank you for the quick response! I appreciate it.\n. Oh, wow. Object.definePrroperty is a beautiful, beautiful thing! Thank you; I'll try that.\n. Oh, wow. Object.definePrroperty is a beautiful, beautiful thing! Thank you; I'll try that.\n. Using Object.defineProperty does the trick. Thank you!\n. Using Object.defineProperty does the trick. Thank you!\n. After looking a little closer, it seems clobbering AWS.config isn't isolating the config data for different client instances afterall.\nHere are the steps to reproduce:\n1. ) Create two AWS accounts, Account A and Account B\n2. ) Put the credentials for Account A in your ~/.aws/credentials file under [default]\n3. ) Put the credentials for Account B in your project's root directory, repo/accountB.js\n4. ) Put this Gist in repo/index.js\n5. ) Run node index.js\n6. ) Observe that index.js returns data from Account A\n7. ) Delete ~/.aws/credentials\n8. ) Run node index.js\n9. ) Observe Missing credentials in config error\nI think maybe setting AWS.config to an immutable object does not ensure config encapsulation between client instances? Also, it seems it is not possible to use the aws-sdk node module without having a valid, global account registered in ~/.aws/credentials?\n. After looking a little closer, it seems clobbering AWS.config isn't isolating the config data for different client instances afterall.\nHere are the steps to reproduce:\n1. ) Create two AWS accounts, Account A and Account B\n2. ) Put the credentials for Account A in your ~/.aws/credentials file under [default]\n3. ) Put the credentials for Account B in your project's root directory, repo/accountB.js\n4. ) Put this Gist in repo/index.js\n5. ) Run node index.js\n6. ) Observe that index.js returns data from Account A\n7. ) Delete ~/.aws/credentials\n8. ) Run node index.js\n9. ) Observe Missing credentials in config error\nI think maybe setting AWS.config to an immutable object does not ensure config encapsulation between client instances? Also, it seems it is not possible to use the aws-sdk node module without having a valid, global account registered in ~/.aws/credentials?\n. I can see that credentials aren't meant to be passed directly to AWS.EC2. I wonder where credentials can be passed to make sure a call to  new AWS.EC2() knows to use them.\n. I can see that credentials aren't meant to be passed directly to AWS.EC2. I wonder where credentials can be passed to make sure a call to  new AWS.EC2() knows to use them.\n. I tried using this Gist in repo/index.js, instead. The difference being I use AWS.Credentials( accountB ) to generate credentials that are then assigned to new AWS.EC2( region ).config.credentials\nRepeating Steps 7 and 8 above with that gist returns an AWS was not able to validate the provided access credentials error.\nSo, so close now, I think. I wonder what I'm missing.\n. I tried using this Gist in repo/index.js, instead. The difference being I use AWS.Credentials( accountB ) to generate credentials that are then assigned to new AWS.EC2( region ).config.credentials\nRepeating Steps 7 and 8 above with that gist returns an AWS was not able to validate the provided access credentials error.\nSo, so close now, I think. I wonder what I'm missing.\n. We were prepending aws on all the keys in accountB ( So, accountB.accessKeyId was actually found on accountB.awsAccessKeyId )... No longer doing that, everything works fine now.\n...time for a cup of coffee and a good cry. Lol\nSorry about that.\n. We were prepending aws on all the keys in accountB ( So, accountB.accessKeyId was actually found on accountB.awsAccessKeyId )... No longer doing that, everything works fine now.\n...time for a cup of coffee and a good cry. Lol\nSorry about that.\n. ",
    "vladejs": "How about returning a list of filtered keys by a random text?\nGiven the text: was. Return all this files:\n\nbucketName/itwasfine.txt\nbucketName/alreadyinthere.was.txt\n\nAnd exclude everything else.\nMy goal is to create a client side table with a fuzzy search functionality on the bucket's keys, including incremental pagination.\nHow to achieve that without getting all objects on the bucket?. OMG, so what about  having one million items in my bucket?\nGiven your answer, is literally impossible to do a performant search on s3.\nI'm then forced to download 1 million items locally and do the search. Is\nthat the approach I should take?\nOn Mon, Feb 18, 2019, 1:41 PM Chase Coalwell notifications@github.com\nwrote:\n\n@vladejs https://github.com/vladejs\nThere is not a direct way to do this.\nlistObjectsV2\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#listObjectsV2-property\nallows you to retrieve objects by key prefix, but would not be able to\nfilter for 'was' in the key 'foowasbar'.\nThe AWS CLI gives you a the following operation:\naws s3api list-objects --bucket bucketName --query \"Contents[?contains(Key, 'was')]\"\nThat makes one or more API calls to retrieve all Objects in the bucket and\nfilters the results locally.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/2543#issuecomment-464839656,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AMDUAlAd2HtQ-6PyJWhngb1B7BbkINd4ks5vOvPqgaJpZM4bBWDD\n.\n. \n",
    "RavikiranDasari": "Sorry, Correction in my post.\nSignature Methode - HmacSHA256\nAction = Publish\n. Sorry, Correction in my post.\nSignature Methode - HmacSHA256\nAction = Publish\n. ",
    "blzabub": "Ah yes, I did see that list of 11 services which does not include Glacier, but when I saw Glacier in the javascript documentation I thought otherwise. Thanks for the explanation.\n. Thanks @lsegal, am going to consider implementing an S3 client instead and just moving files from S3 to Glacier, or maybe send chunks to my web host and have the host send the chunk to Glacier and then delete the chunk on success. My web host has about 20GB total storage and these archives I'm moving are hundreds of GB in size. Cheers!\n. ",
    "LinusU": "AWS version 2.1.16 running in Node.js v0.12.0.\n. I'm so sorry, I didn't realise that this just happens locally where we are using fake-s3. The issue really is with the fake-s3 project.\nAgain, terribly sorry for wasting your time.\n. Yes, the error presented to me was TimeoutError: Missing credentials in config when it really ought to be CredentialsError: Could not load credentials from any providers.\nI was not specifying where to retrieve credentials at all, rather relying on the default lookup.\n. > It works with other aws-sdk functions, [...]\nYeah, I should add that I also tested a call to S3, and that went thru without issues.... Yes, that was it \ud83d\ude04 \nWould love for the \"Using a Shared Config File\" section to get a little note about that here:\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-region.html\nThanks for the help!. ping @chrisradek, any chance of getting this in? \u263a\ufe0f . Is there anywhere else I can submit this PR to?. I see, thanks for running it by the team \u2764\ufe0f . ",
    "kyriesent": "@evansolomon did you ever find the cause/solution for this? I've got the same issue on a docker machine running on debian:jessie. Pretty much same exact stack trace as yours.\n. @evansolomon did you ever find the cause/solution for this? I've got the same issue on a docker machine running on debian:jessie. Pretty much same exact stack trace as yours.\n. Turns out mine was a clock syncing issue on my VM. Running a sync between the VM and the host cleared it up.\n. Turns out mine was a clock syncing issue on my VM. Running a sync between the VM and the host cleared it up.\n. Thanks for all the hard work on this. I've got the same deal here. It's really a tough situation for developers. If even a minor code change during development requires a change in my taskList and all of my workflow versions, I think this is a lot more overhead than it should be.\nHas there been any progress on this at all?\n. Thanks for all the hard work on this. I've got the same deal here. It's really a tough situation for developers. If even a minor code change during development requires a change in my taskList and all of my workflow versions, I think this is a lot more overhead than it should be.\nHas there been any progress on this at all?\n. ",
    "lraulier": "hi all,\nsame issue 'Signature expired' on aws command line\nresolved by 'calling ntpd'  (The Network Time Protocol (NTP) is used to synchronize the time of a computer).\nhope it helps\nregards\n. ",
    "Clee681": "I encountered this issue locally, and restarting my docker daemon fixed it.\n. I encountered this issue locally, and restarting my docker daemon fixed it.\n. ",
    "tonyjiang": "@Clee681 thank you for sharing your experience and solution! Restarting docker daemon fixed my problem as well - could take much longer to solve it without seeing your comment.. ",
    "mankins": "None of the above mentions workarounds worked for me, even though the environment was similar.\nOne difference may be that I was uploading via the --zip-file method of the aws-cli over a slow network. The upload was actually taking more than 5 minutes and consequently giving the SignatureDoesNotMatch error.\nI was able to work around this slow network by uploading first to s3, and then instead of the zip-file command line option using the --s3-bucket and --s3-key options. Perhaps this is an unrelated issue but it took me long enough to solve that I thought I'd document it here in case.\n. ",
    "harish-swamy": "go to vm settings -> date&time -> enable date and time with internetaccess ... this issue should be fix. . ",
    "saikiran91": "I have an exactly same issue. I run Ubuntu terminal on Windows 10. Any solution for this?. ",
    "Kovaloff": "a restart of the VM helped me. ",
    "zeeshanjamal16": "Run the command to sync clock\nntpdate pool.ntp.org. Run the command to sync clock\nntpdate pool.ntp.org. ",
    "abdrmdn": "restarting docker daemon fixed it\nsudo systemctl start docker\nor\nsudo service docker start. restarting docker daemon fixed it\nsudo systemctl start docker\nor\nsudo service docker start. ",
    "devinus": "@AdityaManohar This is actually super useful for us. Right now I suspect we're having issues with DNS resolve caching in Docker containers, so at high load dns.resolve just fails to work and we get these errors (maybe 1 in 50 times). Obviously the \"correct\" solution is to figure out why this is happening in the first place, but for now this is going to make it much more robust for us. Thanks @mick !\n. I'm also experiencing this issue on Node v4.2.1.\n. @chrisradek Could this be reopened? The fact that putObject works but upload doesn't is indicating there's a discrepancy with the SDK itself. The response is a standard node http.IncomingMessage\n. Can anybody from the AWS team or the @aws or @request team comment on this?\n. @chrisradek Thanks for taking a look at this.\n. ",
    "vladgolubev": "In case anyone is still here. For me it appears when I miss http://in the queue url.\n. @jeskew thanks for the fix! I had the same problem after I added X-Ray to my Lambda which generated a signed link for getObject (downloading). So I didn't add extra headers from client-side, but look slike Lambda adds extra query string parameters like xray-trace-id or smth like that. After updating to 2.156.0 the signature is valid again.\nBut it's a pity Lambda environment has still old 2.129.0 version, so people need to deploy extra dependencies.. ",
    "GeoffreyPlitt": "Getting this error a lot, none of the workarounds above work for me.. ",
    "rsmolkin": "I've also gotten this error a few times, and one time, I left a process running, and it just resumed after a few fails on its own.  . ",
    "kodayashi": "I was doing development work at the office, then went home and restarted my node app and started getting this. I ran sudo killall -HUP mDNSResponder on my Mac (macOS) to flush the DNS cache and boom the errors went away.. I was doing development work at the office, then went home and restarted my node app and started getting this. I ran sudo killall -HUP mDNSResponder on my Mac (macOS) to flush the DNS cache and boom it started working.. ",
    "shino": "Pushed the commit that includes fixing wrong test.\n. Access denied case with s3curl.pl was as follows, if that helps.\npartNumber is 1 but it is intentionally URL-encoded to %31\n(it's arbitrary to URL-encode url-encoding-safe charactoers.)\nThen AWS S3 responded with correct (for them) StringToSign and partNumber value\nwas URL-decoded as 1.\n```\n% s3curl.pl --put tmp --id admin -- -s http://some-buckets.s3.amazonaws.com/'foo?partNumber=%31&uploadId=AA'\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\nSignatureDoesNotMatchThe request signature we calculated does not match the signature you provided. Check your key and signing method.AKIAIT2M2HIBZSYWPZHAPUT\nx-amz-date:Thu, 12 Mar 2015 08:53:53 GMT\n/some-buckets/foo?partNumber=1&uploadId=AAASD2oFQMizISn6rgQ3pzkGM/rRE=50 55 54 0a 0a 0a 0a 78 2d 61 6d 7a 2d 64 61 74 65 3a 54 68 75 2c 20 31 32 20 4d 61 72 20 32 30 31 35 20 30 38 3a 35 33 3a 35 33 20 47 4d 54 0a 2f 73 6f 6d 65 2d 62 75 63 6b 65 74 73 2f 66 6f 6f 3f 70 61 72 74 4e 75 6d 62 65 72 3d 31 26 75 70 6c 6f 61 64 49 64 3d 41 41DEB2EF9AF838DA845PnAxyxIJmaTi3Mql5gLA7rxz+3muivl2pBeprxLtQAF+hatOrZ85Wu/9f6Dcwt5%                                                                                                               %\n```\nAs a comparison, with normal (not URL-encoded) uploadId, S3 responds\nwith NoSuchBucket, I guess it's after authentication.\n% s3curl.pl --put tmp --id admin -- -s http://some-buckets.s3.amazonaws.com/'foo?partNumber=1&uploadId=AA'\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>NoSuchBucket</Code><Message>The specified bucket does not exist</Message><BucketName>some-buckets</BucketName><RequestId>F4D7295E8BBEA974</RequestId><HostId>HkndoAW6MbPhongMTwhas3oYdcJFkkRRSpzHQOgISZHoe7BYjywGa5lkm3lzKht3</HostId></Error>\n. @AdityaManohar I wrote simple sample code [1] using List Parts API [2].\nCommand line usage:\n% nodejs list_parts_node.js <REGION> <BUCKET> <KEY> <UPLOAD-ID>\nNow only signature is related, we can use any bucket, key and uploadId.\nSuccessful authentication case for both original and this PR, error code\nis NoSuchBucket.\n% node list_parts_node.js ap-northeast-1 b k u\n[snip]\n<Error><Code>NoSuchBucket</Code>[snip]\nIf uploadId is url-unsafe, then original code returns error SignatureDoesNotMatch.\nThis manifests that client side signature calculation is wrong.\nFull output is at [3] and it can be confirmed that query paramter value\nis not URL-decoded as uploadId=% in <StringToSign> element of response XML.\n% node list_parts_node.js ap-northeast-1 b k '%'\n[snip]\n<Error><Code>SignatureDoesNotMatch</Code>[snip]\nThis PR fixes this case and get error NoSuchBucket properly.\n[1] https://gist.github.com/shino/602caae62725a82c559c\n[2] http://docs.aws.amazon.com/AmazonS3/latest/API/mpUploadListParts.html\n[3] https://gist.github.com/shino/eea05420dd8c826442bd#file-url-unsafe-txt\n. I forgot to add region argument, so I added it to the above comment.\n. Ping. If there is any subtle points or inconvenience for merge, please ping back me :smile: \n. Ping. How's going?. @orinciog Could you describe your case?\nThe misbehavior I wrote at the above comment (https://github.com/aws/aws-sdk-js/pull/530#issuecomment-78443601) is that expected and actual cases  are both error but error code is wrong.\nIf you find the case where the expected behavior is success (2xx) but error happens, it's valuable as additional information to this issue.. ",
    "orinciog": "I've spent three days to hunt this bug. \nThank you a lot @shino for this commit.\n@AdityaManohar It's been !!two years!! from this pull request and !!4 lines of code!! Do you ever want to merge it or do I have to publish a merged version, that is actually working with v2 signatures?\nThank you.. @shino Yes, my case is identical with https://github.com/basho/riak_cs/issues/1327 \nI can't use multipart upload with v2 signatures in riak because uploadPart fails with AccessDenied. \nWhat describe SBRK in first post is identical to me. \nThe problem is that when computing the signature for uploadPart, the value of uploadId is not url-encoded. \nI have patched with your version (thanks again), that encodes all values and now I can use uploadPart.\n. The issue is still valid for the current version of aws-sdk-js (2.392.0). \nI had to fork your lib in order to make v2 signatures work. . ",
    "JacobEvelyn": "That page handles fairly small files\u201460 KB for one I'm seeing it on. I'm using version 2.1.15.\nIs there particular network information you're looking for? I'm seeing:\nRequest:\nOPTIONS https://s3.amazonaws.com/mybucket/path/in/s3/142625520387278wqrc6udfc.png\nHost: s3.amazonaws.com\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:36.0) Gecko/20100101 Firefox/36.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nDNT: 1\nOrigin: http://admin.lvh.me:3001\nAccess-Control-Request-Method: PUT\nAccess-Control-Request-Headers: authorization,content-type,x-amz-acl,x-amz-date,x-amz-server-side-encryption,x-amz-storage-class,x-amz-user-agent\nConnection: keep-alive\nResponse:\n200 OK\nAccess-Control-Allow-Origin: \"*\"\nContent-Length: \"0\"\nDate: \"Fri, 13 Mar 2015 14:05:25 GMT\"\nServer: \"AmazonS3\"\nVary: \"Origin, Access-Control-Request-Headers, Access-Control-Request-Method\"\naccess-control-allow-headers: \"authorization, content-type, x-amz-acl, x-amz-date, x-amz-server-side-encryption, x-amz-storage-class, x-amz-user-agent\"\naccess-control-allow-methods: \"PUT, POST\"\naccess-control-expose-headers: \"ETag\"\nx-amz-id-2: \"...\"\nx-amz-request-id: \"...\"\nRequest:\nPUT https://s3.amazonaws.com/mybucket/path/in/s3/1426255544594vsnh7ruh09.png\nUser-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:36.0) Gecko/20100101 Firefox/36.0\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-US,en;q=0.5\nAccept-Encoding: gzip, deflate\nDNT: 1\nX-Amz-User-Agent: aws-sdk-js/2.1.15\nx-amz-acl: private\nContent-Type: image/png\nx-amz-server-side-encryption: AES256\nx-amz-storage-class: STANDARD\nX-Amz-Date: Fri, 13 Mar 2015 14:05:44 GMT\nAuthorization: AWS ...\nReferer: http://admin.lvh.me:3001/upload_page\nContent-Length: 59570\nOrigin: http://admin.lvh.me:3001\nConnection: keep-alive\nResponse:\n200 OK\nAccess-Control-Allow-Origin: \"*\"\nContent-Length: \"0\"\nDate: \"Fri, 13 Mar 2015 14:05:25 GMT\"\nEtag: \"\"...\"\"\nServer: \"AmazonS3\"\nVary: \"Origin, Access-Control-Request-Headers, Access-Control-Request-Method\"\naccess-control-allow-methods: \"PUT, POST\"\naccess-control-expose-headers: \"ETag\"\nx-amz-id-2: \"...\"\nx-amz-request-id: \"...\"\nx-amz-server-side-encryption: \"AES256\"\n. Sorry for being unclear in my original description... there is no failing request. It works perfectly, except for the fact that I get this message in the console:\nMissingRequiredParameter: Missing required key 'Bucket' in params\nStack trace:\n[21]</t.ParamValidator<.fail@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:916\n[21]</t.ParamValidator<.validateStructure@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:5:31539\n[21]</t.ParamValidator<.validateMember@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:5:31893\n[21]</t.ParamValidator<.validate@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:5:31030\n[10]</t.EventListeners.Core</<@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:5:10653\n[34]</r.SequentialExecutor<.callListeners@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:27776\nn@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:27658\n[10]</t.EventListeners.Core</</<@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:5:10346\nr@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:5:478\n[2]</t.Config<.getCredentials@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:5:1064\n[10]</t.EventListeners.Core</<@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:5:10198\n[34]</r.SequentialExecutor<.callListeners@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:27751\n[34]</r.SequentialExecutor<.emit@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:27569\n[30]</</n.Request<.emitEvent@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:15978\n[30]</</u.setupStates/e@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:12289\n[48]</r.prototype.runTo@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:7:23402\n[30]</</n.Request<.runTo@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:13790\n[30]</</n.Request<.send@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:13688\n[33]</r.S3.ManagedUpload<.nextChunk@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:24508\n[33]</r.S3.ManagedUpload<.fillBuffer@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:23806\n[33]</r.S3.ManagedUpload<.send@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:6:22497\n[38]</<.upload@https://sdk.amazonaws.com/js/aws-sdk-2.1.15.min.js:7:9755\n$.fn.initializeUploader/</<@https://d3sjyjw8di3y5z.cloudfront.net/assets/admin/application-29c548168e93ec9f626fb6bd3d29f87a.js:1787:8185\n.each@https://d3sjyjw8di3y5z.cloudfront.net/assets/admin/application-29c548168e93ec9f626fb6bd3d29f87a.js:1014:25321\n$.fn.initializeUploader/<@https://d3sjyjw8di3y5z.cloudfront.net/assets/admin/application-29c548168e93ec9f626fb6bd3d29f87a.js:1787:7885\njQuery.event.dispatch@https://d3sjyjw8di3y5z.cloudfront.net/assets/admin/application-29c548168e93ec9f626fb6bd3d29f87a.js:1015:23250\njQuery.event.add/elemData.handle@https://d3sjyjw8di3y5z.cloudfront.net/assets/admin/application-29c548168e93ec9f626fb6bd3d29f87a.js:1015:18012\n. Sorry, I didn't see you had responded to my comment. I couldn't get AWS.config.logger = console.log to show me any extra output, but you were right that I had multiple code paths\u2014the same uploader callback was firing multiple times, and for some reason one of them wasn't forming the request correctly. Thanks for the help!\n. ",
    "ondruska": "http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\n. AWS SES is not amongst the services you can use from browser: http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\n. Compare with release notes for 2.1.35 http://aws.amazon.com/releasenotes/3577030144306744 . Supported API versions section.\n. https://aws.amazon.com/sdk-for-node-js/\nand http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/index.html has all what I need. \n. In my opinion you are using SMTP credentials instead of correct access keys. That explains PHP (mailer?) works. Read the docs http://docs.aws.amazon.com/ses/latest/DeveloperGuide/smtp-credentials.html\n. SES does not work in browser (angular): for more see http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\n. ",
    "buu700": "Ahh, thanks! Building it myself fixed that.\n. ",
    "jasonfill": "I just filed the issue over on the awslabs/dynamodb-document-js-sdk/issues repo but looks like that repo has been depreciated for the AWS.DynamoDB.DocumentClient which is part of this this main js sdk.  Unless I am missing something, should this issue be re-opened here?\n. ",
    "benoittgt": "Could we reopen this issue ?\n. Hello\nJust tried this code with and without the httpOptions\n``` javascript\nvar aws = require('aws-sdk');\nvar https = require('https');\nexports.handler = function(event, context) {\n  var dynamo = new aws.DynamoDB({\n    region: 'eu-west-1',\n    httpOptions: {\n      agent: new https.Agent({\n        rejectUnauthorized: true,\n        keepAlive: false,\n        ciphers: 'ALL',\n        secureProtocol: 'TLSv1_method'\n      })\n    }\n  });\ndynamo.listTables(function(err, data) {\n    console.log('inside listTables');\n    if (err)\n      console.log(JSON.stringify(err, null, 2));\n    else\n      console.log(data.TableNames);\n  });\n};\n```\nAnd I get timeout. No problem locally calling AWS database.\n. Thanks to both of you for the fast answers.\nWith : \n``` javascript\nvar aws = require('aws-sdk');\nvar https = require('https');\nexports.handler = function(event, context) {\n  var dynamo = new aws.DynamoDB({\n    region: 'eu-west-1',\n    maxRetries: 8\n  });\ndynamo.listTables(function(err, data) {\n    console.log('inside listTables');\n    if (err)\n      console.log(JSON.stringify(err, null, 2));\n    else\n      console.log(data.TableNames);\n  });\n};\n```\nI get\nSTART RequestId: 6ebafb49-59b1-11e6-b05b-f79f63e71369 Version: $LATEST\nEND RequestId: 6ebafb49-59b1-11e6-b05b-f79f63e71369\nREPORT RequestId: 6ebafb49-59b1-11e6-b05b-f79f63e71369  Duration: 10000.24 ms   Billed Duration: 10000 ms   Memory Size: 128 MB Max Memory Used: 24 MB  \n2016-08-03T19:35:38.781Z 6ebafb49-59b1-11e6-b05b-f79f63e71369 Task timed out after 10.00 seconds\n. I did the test with context on both Node version available with the same errors posted on https://github.com/aws/aws-sdk-js/issues/862#issuecomment-237348407. \n``` javascript\nvar aws = require('aws-sdk');\nexports.handler = function(event, context) {\n  var dynamo = new aws.DynamoDB({\n    region: 'eu-west-1',\n    maxRetries: 8\n  });\ndynamo.listTables(function(err, data) {\n    if (err) {\n      context.fail(err.stack)\n    } else {\n      context.succeed('Function Finished! Data :' + data.TableNames);\n    }\n  });\n};\n```\n. Thanks @chrisradek \nI've the lambda locally with this three simple lignes\njavascript\nvar aws = require('aws-sdk');\nvar dynamo = new aws.DynamoDB({\n  region: 'eu-west-1',\n  maxRetries: 1\n});\ndynamo.listTables(function(err, data){console.log(data)});\n// > { TableNames: [ 'appaloosa' ] }\nBut on the lambda with the export handler I get the timeout error. I've tried with the context fail and succeed\n``` javascript\nvar aws = require('aws-sdk');\nexports.handler = function(event, context) {\n  var dynamo = new aws.DynamoDB({\n    region: 'eu-west-1',\n    maxRetries: 0\n  });\ndynamo.listTables(function(err, data) {\n    if (err) {\n      context.fail(err.stack)\n    } else {\n      context.succeed('Function Finished! Data :' + data.TableNames);\n    }\n  });\n};\n```\nSTART RequestId: fd43b184-5a0f-11e6-91bb-a5dd7dd469f7 Version: $LATEST\nEND RequestId: fd43b184-5a0f-11e6-91bb-a5dd7dd469f7\nREPORT RequestId: fd43b184-5a0f-11e6-91bb-a5dd7dd469f7  Duration: 12003.64 ms   Billed Duration: 12000 ms   Memory Size: 128 MB Max Memory Used: 24 MB  \n2016-08-04T06:52:32.519Z fd43b184-5a0f-11e6-91bb-a5dd7dd469f7 Task timed out after 12.00 seconds\nIt's a small database that I made for testing. It's the only one dynamodb table.\nAnd then the surprise. I've created a lambda and dynamo table in us-west-2 with this code on a new lambda \n``` javascript\nvar aws = require('aws-sdk');\nexports.handler = function(event, context) {\n  var dynamo = new aws.DynamoDB({\n    region: 'us-west-2',\n    maxRetries: 0\n  });\ndynamo.listTables(function(err, data) {\n    if (err) {\n      context.fail(err.stack);\n    } else {\n      context.succeed('Function Finished! Data :' + data.TableNames);\n    }\n  });\n};\n```\nI just changed the region and then \n\"Function Finished! Data :appaloosa2\"\nThe issue was related not to the region but to VPC. I remove all VPC configuration on eu-west-1 and it's working. Strange I get a timeout an not a VPC error.\nBut it's fixed.\nThanks @chrisradek for your time. Feel free to close the issue.\n. It was not easy to setup properly VPC for DynamoDB and lambda. This gist help me.\nhttps://gist.github.com/reggi/dc5f2620b7b4f515e68e46255ac042a7\n. ",
    "leggiero": "Currently I'm using Java Lambda's instead of Node.js ones, just because of the lack of Optimistic Locking feature, that is a crucial point on some projects that I'm working on.\nCould we reopen this issue?. Currently I'm using Java Lambda's instead of Node.js ones, just because of the lack of Optimistic Locking feature, that is a crucial point on some projects that I'm working on.\nCould we reopen this issue?. ",
    "youcangetme": ":+1: this would be nice to have in Lambdas with JavaScript/Node.js. ",
    "spazard1": "Yes that makes sense. I thought that since the URL that is given back had the ContentDisposition inside it, that the AWS endpoint would read that information accordingly. Thanks for your answer.\n. ",
    "irwin-implementations": "aws-sdk 2.1.19\nnode v0.10.25\nUbuntu 14.04 LTS\n. Sorry....I was being stupid. it works, it was on my end.\n. ",
    "daguej": "Tweaked to check for both Throttling and ThrottlingException.\nI saw ThrottlingException when calling Lambda's deleteFunction.\n. ",
    "gagecarto": "I am a total n00b when it comes to building my own SDK versions. \nCould I just include the source of the 2009-04-15.. Something ugly like below or would that be a total failure?\n<script src=\"https://sdk.amazonaws.com/js/aws-sdk-2.1.17.min.js\"></script> \n<script src=\"https://raw.githubusercontent.com/aws/aws-sdk-js/ebc438876945d55707d84b8986bd2fc3aa3004e9/apis/sdb-2009-04-15.normal.json\"></script>\n. @AdityaManohar \nThe environment I am working in does enforce CORS. Why do you ask? Is there a different approach I should be taking entirely?\n. ",
    "billyshena": "Hi Aditya ! Thanks for your quick answer.\nWell, this is how my Ajax request looks like:\n\n\n\n. Hi, your example is working like a charm and the file is not corrupted anymore after being uploaded to Amazon S3.\nHowever, I've tried to use some angular plugins to handle the upload for me https://github.com/nervgh/angular-file-upload but this is not working (I have to do something with transformRequest?). If I use any kind of plugin => the file is still corrupted. I'm sending the same parameters as the working sample u provided.\n. ",
    "guumaster": "I wrote this simple gist with a working example to upload a file with an $.ajax() call. \n. ",
    "clues": "@lsegal :OK\n. Oh no, my code can upload empty file, but will lead to new problem: file size more than (4~5)mb seem will dead chunk and send large http request to s3.So i expect new solution.\n. @lsegal I have fixed it,it's my fault,i not update AWS config property 'signatureVersion' in my code.When i config follow,it's work (sdk #v2.1.5).\n//If s3 in Beijing,it's special config follow\nif (\"CN_Beijing\".toLowerCase() == global.awsConfig.region){\n            global.awsConfig.region =\"cn-north-1\";\n            global.awsConfig.signatureVersion =\"v4\";\n}\nAWS.config.update(global.awsConfig);\n. ",
    "whitfin": "@lsegal I'm using v0.10.36, and the file was only a few hundred bytes after compression. If you want me to try it out on another version of node (perhaps 0.12.x), I can do so.\n. @lsegal yeah, I tried out that exact snippet. It's odd, I can't reproduce it now either - perhaps it was something on the actual S3 side?\nI tried again with the original code I found the bug in by reverting some stuff locally and it's not happening there anymore either. I think we can close this!\n. @silveur when I experienced it, it was also fine one minute and changed the next - I think maybe it's an S3 issue or something? As I mentioned above, the snippet which reproduced it suddenly stopped working.\nI'll reopen this just so that someone can follow up if they deem necessary. \n. @AdityaManohar that's sort of the point - the above code did reproduce it for a while, but then it suddenly stopped - exact same code (I triple checked). After reading what @silveur posted, it appears this could be an intermittent issue, which could very well be on the S3 server side. I just re-opened this in case anyone wants to follow it up in the code. \n. Yeah, looks like it trims the last character of the actual location, then dups the dir tree once more.\n. @srchase yep, exactly! :). @srchase that's what I'm doing right now - found that after an hour of so looking around. I have a feeling it would be more familiar for people to just be an option. Appreciate the tip though!. ",
    "silveur": "I'm experiencing exactly the same issue where an mp3 file 1.mp3 is returned in the data field with a strange location: \nhttps://mybucket.s3-eu-west-1.amazonaws.com/1.mp/1.mp3\nYesterday seemed to be fine though... I don't really know. My version of the aws-sdk is latest 2.1.50\n. ",
    "christianhaller": "same here!\nIt isconcatenating the data.Location string with itself.\n. ",
    "lostcolony": "I'm seeing this now. Though it's only concatenating -part- of the string. This is what I'm firing - \nvar saveImageToS3 = function(name, buffer, callback) {\n    console.log(name);\n    var params = {Bucket: '(my_bucket)', Key: name, Body: buffer};\n    s3.upload(params, {}, function(err, data) {\n        console.log(data);\n        return callback(err, data);\n    });\n}\nIn my case, name is \"pro-images/1.jpeg\". What I'm getting back is https://(my_bucket).s3-us-west-2.amazonaws.com/pro-images/1.jpe/pro-images/1.jpeg\nThankfully I can work around it since the URL is well known after I get the region off, but this is...kind of silly to see.\n(What's also interesting, is if I change it so the key is just '1', no folder, the URL becomes https://(my_bucket).s3-us-west-2.amazonaws.com//1. I.e., double //)\n. Hmm. Now I'm unable to reproduce, but I'm seeing 0 retries. It may be an issue with AWS itself? \n. ",
    "sboora": "@AdityaManohar both these benchmarks are done on the same ec2 instance and to the same DynamoDB table, so not sure if factoring out network latency helps.  (this is assuming there were no issues in the network since I ran this benchmark couple of times and the results were consistent)\n@lsegal, we are on v0.10.35, will try again with AWS.Config.convertResponseTypes turned off. \nSorry about the delayed response, really appreciate your feedback on this.\n. Here's the script I am using for benchmarking. \nhttps://github.com/Medium/local-dynamo/blob/master/benchmark/sdk_benchmark.js\n. Here are the stats from benchmarking with converResponseTypes set to off.\nConvertResponseTypes: On\nAWS SDK Version 2.1.24\nConvertResponseTypes: On\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 229.85/77.12/220, throughput (/sec): 131.15, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 188.48/61.13/180, throughput (/sec): 136.16, errors: 0\nConvertResponseTypes: Off\nAWS SDK Version 2.1.24\nConvertResponseTypes: Off\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 189.44/65.97/181, throughput (/sec): 134.26, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 146.18/32.71/145, throughput (/sec): 137.82, errors: 0\nAWS SDK Version 1.15.0\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 144.54/49.64/138, throughput (/sec): 141.62, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 132.36/62.53/119.5, throughput (/sec): 145.06, errors: 0\n. @lsegal, You are right. My benchmarking results are off since there is another process on the machine affecting the results.\n@chaosgame on our team ran these on a standalone machine to rule out other factors affecting the benchmarking and results look promising.\nVersion 2.1.24 (ConvertResponseTypes: True)\nAWS SDK Version 2.1.24\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 186.63/58.12/182, throughput (/sec): 134.61, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 166.92/49.6/151, throughput (/sec): 139.59, errors: 0\nVersion 2.1.24 (ConvertResponseTypes: False)\nAWS SDK Version 2.1.24\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 163.12/59.36/153.5, throughput (/sec): 135.09, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 145.23/40.45/140, throughput (/sec): 139.42, errors: 0\nVersion 2.1.9 (ConvertResponseTypes: True)\nAWS SDK Version 2.1.9\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 169.3/57.44/162, throughput (/sec): 135.3, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 192.74/69.31/185, throughput (/sec): 139.44, errors: 0\nVersion 1.15.0\nAWS SDK Version 1.15.0\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 158.88/75.6/142, throughput (/sec): 142.13, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 138.58/53.06/130, throughput (/sec): 145.82, errors: 0\n. ",
    "chaosgame": "Unfortunately it's not as clear as that.  It looks like the numbers were being masked by the overhead of setting up an HTTPS connection for each request.  When I re-ran the experiment on node 0.12.2 (http.globalAgent.keepAlive = true) and sslEnabled set to false (this is a closer reflection to how we run AWS-SDK in production, I get the numbers below.\nVersion 2.1.24 (ConvertResponseTypes: False)\nAWS SDK Version 2.1.24\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 156.65/34.53/152, throughput (/sec): 125.62, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 167.58/31.72/161, throughput (/sec): 130.29, errors: 0\nVersion 1.15.0\nAWS SDK Version 1.15.0\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 109.93/30.29/105, throughput (/sec): 130.41, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 99.07/31.71/94, throughput (/sec): 134.04, errors: 0\n. Unfortunately it's not as clear as that.  It looks like the numbers were being masked by the overhead of setting up an HTTPS connection for each request.  When I re-ran the experiment on node 0.12.2 (http.globalAgent.keepAlive = true) and sslEnabled set to false (this is a closer reflection to how we run AWS-SDK in production, I get the numbers below.\nVersion 2.1.24 (ConvertResponseTypes: False)\nAWS SDK Version 2.1.24\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 156.65/34.53/152, throughput (/sec): 125.62, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 167.58/31.72/161, throughput (/sec): 130.29, errors: 0\nVersion 1.15.0\nAWS SDK Version 1.15.0\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 109.93/30.29/105, throughput (/sec): 130.41, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 99.07/31.71/94, throughput (/sec): 134.04, errors: 0\n. Okay, so I spent some time bisecting between versions.  The initial performance regression we noticed is an embarrassingly long time back \u2014 when we tried upgrading to AWS-SDK 1.18.0.  On our benchmark, the problematic commit (specifically getItem requests) seems to be between 1.16.0 and 1.17.0: https://github.com/aws/aws-sdk-js/commit/d75bb771c618aba847dcd20de9e09045bb5f0265.\nBefore:\nAWS SDK Version 2.0.0-rc4\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 110.04/28.6/106, throughput (/sec): 130.87, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 100.56/24.09/96, throughput (/sec): 134.34, errors: 0\nAfter:\nAWS SDK Version 2.0.0-rc4\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 120.9/31.27/119, throughput (/sec): 129.24, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 148.63/27.8/146, throughput (/sec): 133.26, errors: 0\nMy read on this is that for light data workloads this is benign, but when we start fetching lots of data, the all of the extra emitted events stress the system.  Thoughts?\n. Okay, so I spent some time bisecting between versions.  The initial performance regression we noticed is an embarrassingly long time back \u2014 when we tried upgrading to AWS-SDK 1.18.0.  On our benchmark, the problematic commit (specifically getItem requests) seems to be between 1.16.0 and 1.17.0: https://github.com/aws/aws-sdk-js/commit/d75bb771c618aba847dcd20de9e09045bb5f0265.\nBefore:\nAWS SDK Version 2.0.0-rc4\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 110.04/28.6/106, throughput (/sec): 130.87, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 100.56/24.09/96, throughput (/sec): 134.34, errors: 0\nAfter:\nAWS SDK Version 2.0.0-rc4\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 120.9/31.27/119, throughput (/sec): 129.24, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 148.63/27.8/146, throughput (/sec): 133.26, errors: 0\nMy read on this is that for light data workloads this is benign, but when we start fetching lots of data, the all of the extra emitted events stress the system.  Thoughts?\n. @AdityaManohar For the tests you ran, your numbers line up pretty closely with mine (I'm running on a c3.large).  My getItem numbers are slightly better for 1.15, but look close enough.\nA note on testing with 1.12:\nYou're benchmark needs to open up a new HTTP connection each time we make a request.  This is super duper slow in my experience when we're making lots of requests (as in this benchmark).  The added overhead of all this additional work masks the underlying regression.  Internally, when we're running Node 10, we have some code that hacks in basic HTTP pooling.  I suggested running the benchmark on Node 12 only because the support for pooling there is native.  FWIW, I didn't notice a big difference in performance between Node 10 and Node 12 when pooling was disabled.\n@lsegal when I comment out the emit httpDownloadProgress event and the if (body && WritableStream && ReadableStream) block on line 85 of lib/http/node.js, I'm able to get comparable performance on 2.1.24.  Adding a disableProgressEvents on the httpOptions object sounds like a great idea.\n. @AdityaManohar For the tests you ran, your numbers line up pretty closely with mine (I'm running on a c3.large).  My getItem numbers are slightly better for 1.15, but look close enough.\nA note on testing with 1.12:\nYou're benchmark needs to open up a new HTTP connection each time we make a request.  This is super duper slow in my experience when we're making lots of requests (as in this benchmark).  The added overhead of all this additional work masks the underlying regression.  Internally, when we're running Node 10, we have some code that hacks in basic HTTP pooling.  I suggested running the benchmark on Node 12 only because the support for pooling there is native.  FWIW, I didn't notice a big difference in performance between Node 10 and Node 12 when pooling was disabled.\n@lsegal when I comment out the emit httpDownloadProgress event and the if (body && WritableStream && ReadableStream) block on line 85 of lib/http/node.js, I'm able to get comparable performance on 2.1.24.  Adding a disableProgressEvents on the httpOptions object sounds like a great idea.\n. Alright, I have a PR ready for y'all based on the discussion above:\nAWS.config.httpOptions.disableProgressEvents = false\nAWS SDK Version 2.1.25\nNode Version v0.12.2\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 141.18/36.05/136, throughput (/sec): 125.26, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 167.83/35.81/163, throughput (/sec): 129.35, errors: 0\nAWS.config.httpOptions.disableProgressEvents = true\nAWS SDK Version 2.1.25\nNode Version v0.12.2\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 131.8/27.24/130, throughput (/sec): 126.86, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 114.37/26.73/110, throughput (/sec): 130.83, errors: 0\n. Alright, I have a PR ready for y'all based on the discussion above:\nAWS.config.httpOptions.disableProgressEvents = false\nAWS SDK Version 2.1.25\nNode Version v0.12.2\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 141.18/36.05/136, throughput (/sec): 125.26, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 167.83/35.81/163, throughput (/sec): 129.35, errors: 0\nAWS.config.httpOptions.disableProgressEvents = true\nAWS SDK Version 2.1.25\nNode Version v0.12.2\nSample Size 10000\nRecord Size (byte) 2048\nWaiting for the table to be ready in Dynamo\n[putItem] lantecy (ms): mean/std_dev/median 131.8/27.24/130, throughput (/sec): 126.86, errors: 0\n[getItem] lantecy (ms): mean/std_dev/median 114.37/26.73/110, throughput (/sec): 130.83, errors: 0\n. Hi there,\nWe haven't heard back from you in a little bit and I wanted to see if there was anything I could do from our side to get this fix into master faster.\nThanks for your help thus far!\n. Hi there,\nWe haven't heard back from you in a little bit and I wanted to see if there was anything I could do from our side to get this fix into master faster.\nThanks for your help thus far!\n. Thanks for the quick response @lsegal, does your team have an ETA on when the fix will land in master?\n. Thanks for the quick response @lsegal, does your team have an ETA on when the fix will land in master?\n. Thanks for the response @lsegal.  I'm going to try and take a look at reproducing your benchmark tomorrow when I have more time, but two things jump out at me right now.\n1. Use node 1.12 and set require('http').globalAgent.keepAlive = true (for both http and https so that we're not opening up a new SSL connection on each request).\n2. In the benchmark we're using above, we're using a 2048 byte record size (slightly bigger than what you have above and might be putting more stress on your http listener).\n. Thanks for the response @lsegal.  I'm going to try and take a look at reproducing your benchmark tomorrow when I have more time, but two things jump out at me right now.\n1. Use node 1.12 and set require('http').globalAgent.keepAlive = true (for both http and https so that we're not opening up a new SSL connection on each request).\n2. In the benchmark we're using above, we're using a 2048 byte record size (slightly bigger than what you have above and might be putting more stress on your http listener).\n. Hey sorry for the super long delay in updating.  I'm being pulled in a number of different directions at work and while this is a high priority for me, it's unfortunately not the highest.  I will keep you posted when I get more time to work on this.  Thank you for all the help you've given us.\n. Hey sorry for the super long delay in updating.  I'm being pulled in a number of different directions at work and while this is a high priority for me, it's unfortunately not the highest.  I will keep you posted when I get more time to work on this.  Thank you for all the help you've given us.\n. ",
    "kylehg": "[edit: these results are invalid, see comment below]\nHi all, I work with @chaosgame, sorry for the long delay on our end. I'll be picking this up for him.\nI've rerun tests using the benchmark script you provided, and I'm consistently seeing a performance regression. Here's the result of 10 tests, run on an EC2 m1.small machine:\n```\nVERSION=v1.15.0 node index.js\nputItem x 544 ops/sec \u00b15.19% (78 runs sampled)\nputItem x 551 ops/sec \u00b13.68% (80 runs sampled)\nputItem x 545 ops/sec \u00b13.86% (82 runs sampled)\nputItem x 542 ops/sec \u00b14.01% (80 runs sampled)\nputItem x 538 ops/sec \u00b15.15% (81 runs sampled)\nputItem x 545 ops/sec \u00b13.97% (79 runs sampled)\nputItem x 556 ops/sec \u00b13.13% (77 runs sampled)\nputItem x 538 ops/sec \u00b15.18% (77 runs sampled)\nputItem x 557 ops/sec \u00b12.97% (68 runs sampled)\nputItem x 555 ops/sec \u00b13.46% (83 runs sampled)\ngetItem x 599 ops/sec \u00b11.58% (83 runs sampled)\ngetItem x 594 ops/sec \u00b11.95% (70 runs sampled)\ngetItem x 596 ops/sec \u00b12.32% (71 runs sampled)\ngetItem x 587 ops/sec \u00b11.81% (83 runs sampled)\ngetItem x 589 ops/sec \u00b11.72% (81 runs sampled)\ngetItem x 571 ops/sec \u00b15.23% (77 runs sampled)\ngetItem x 596 ops/sec \u00b11.41% (83 runs sampled)\ngetItem x 591 ops/sec \u00b12.26% (66 runs sampled)\ngetItem x 602 ops/sec \u00b12.19% (71 runs sampled)\ngetItem x 584 ops/sec \u00b11.55% (83 runs sampled)\nVERSION=v2.1.25 node index.js\nputItem x 443 ops/sec \u00b14.99% (70 runs sampled)\nputItem x 464 ops/sec \u00b14.69% (64 runs sampled)\nputItem x 441 ops/sec \u00b15.78% (73 runs sampled)\nputItem x 467 ops/sec \u00b13.87% (68 runs sampled)\nputItem x 437 ops/sec \u00b15.54% (71 runs sampled)\nputItem x 452 ops/sec \u00b14.49% (64 runs sampled)\nputItem x 463 ops/sec \u00b14.79% (69 runs sampled)\nputItem x 327 ops/sec \u00b153.76% (65 runs sampled)\nputItem x 471 ops/sec \u00b14.16% (65 runs sampled)\nputItem x 459 ops/sec \u00b14.86% (65 runs sampled)\ngetItem x 481 ops/sec \u00b17.37% (74 runs sampled)\ngetItem x 515 ops/sec \u00b13.24% (80 runs sampled)\ngetItem x 508 ops/sec \u00b13.89% (80 runs sampled)\ngetItem x 499 ops/sec \u00b14.01% (79 runs sampled)\ngetItem x 508 ops/sec \u00b14.79% (82 runs sampled)\ngetItem x 524 ops/sec \u00b12.37% (84 runs sampled)\ngetItem x 506 ops/sec \u00b13.88% (73 runs sampled)\ngetItem x 514 ops/sec \u00b13.33% (80 runs sampled)\ngetItem x 504 ops/sec \u00b13.69% (79 runs sampled)\ngetItem x 527 ops/sec \u00b12.50% (84 runs sampled)\n```\nExcluding the clear outlier in the v2 run, this represents a regression of 13% for GETs and 16% for PUTs. \nThese tests were run using the latest version of DynamoDB Local (2015-04-27_1.0), Node v0.12.2, and Java v1.7.0_79. For reference, here's the repo I've been using for testing: https://github.com/Medium/sdk-test\nAFAICT, this regression persists with both the changes suggested above, AWS.config.convertResponseTypes = false and Nathan's disableProgressEvents, so those don't seem to be the root of the issue. The regression also seems to persist through version 1.18.\nI'm going to keep testing with progressively lower versions and attempt to isolate the location of the regression, but I wanted to post these results here now so that you all can begin your own investigations.\n. It seems I spoke too soon\u00a0\u2014\u00a0turns out the benchmark script was not even connecting to DynamoDB Local, and was swallowing errors, so the results above are invalid. After fixing the script (in the repo above) and rerunning, I'm seeing results with little significant difference:\n```\nAWS SDK version 1.15.0\nputItem x 107 ops/sec \u00b121.58% (76 runs sampled)\nputItem x 142 ops/sec \u00b11.24% (84 runs sampled)\nputItem x 130 ops/sec \u00b13.59% (78 runs sampled)\nputItem x 149 ops/sec \u00b11.21% (80 runs sampled)\nputItem x 149 ops/sec \u00b12.97% (81 runs sampled)\nputItem x 151 ops/sec \u00b13.36% (80 runs sampled)\nputItem x 153 ops/sec \u00b11.22% (82 runs sampled)\nputItem x 154 ops/sec \u00b11.27% (81 runs sampled)\nputItem x 155 ops/sec \u00b11.21% (82 runs sampled)\nputItem x 155 ops/sec \u00b11.05% (82 runs sampled)\ngetItem x 165 ops/sec \u00b10.80% (79 runs sampled)\ngetItem x 170 ops/sec \u00b11.00% (81 runs sampled)\ngetItem x 174 ops/sec \u00b11.32% (83 runs sampled)\ngetItem x 179 ops/sec \u00b10.99% (84 runs sampled)\ngetItem x 184 ops/sec \u00b10.88% (82 runs sampled)\ngetItem x 186 ops/sec \u00b10.75% (80 runs sampled)\ngetItem x 185 ops/sec \u00b11.14% (79 runs sampled)\ngetItem x 183 ops/sec \u00b12.71% (79 runs sampled)\ngetItem x 183 ops/sec \u00b11.66% (80 runs sampled)\ngetItem x 186 ops/sec \u00b10.96% (81 runs sampled)\nAWS SDK version 2.1.25\nputItem x 105 ops/sec \u00b121.84% (75 runs sampled)\nputItem x 139 ops/sec \u00b11.27% (82 runs sampled)\nputItem x 143 ops/sec \u00b11.24% (84 runs sampled)\nputItem x 114 ops/sec \u00b14.28% (69 runs sampled)\nputItem x 144 ops/sec \u00b12.95% (79 runs sampled)\nputItem x 147 ops/sec \u00b13.22% (79 runs sampled)\nputItem x 151 ops/sec \u00b11.31% (81 runs sampled)\nputItem x 150 ops/sec \u00b11.39% (79 runs sampled)\nputItem x 150 ops/sec \u00b11.38% (80 runs sampled)\nputItem x 150 ops/sec \u00b11.33% (81 runs sampled)\ngetItem x 159 ops/sec \u00b11.44% (83 runs sampled)\ngetItem x 164 ops/sec \u00b11.10% (84 runs sampled)\ngetItem x 169 ops/sec \u00b11.11% (80 runs sampled)\ngetItem x 174 ops/sec \u00b11.17% (82 runs sampled)\ngetItem x 177 ops/sec \u00b11.11% (77 runs sampled)\ngetItem x 179 ops/sec \u00b11.28% (84 runs sampled)\ngetItem x 178 ops/sec \u00b11.12% (84 runs sampled)\ngetItem x 180 ops/sec \u00b11.20% (83 runs sampled)\ngetItem x 176 ops/sec \u00b12.54% (79 runs sampled)\ngetItem x 179 ops/sec \u00b11.27% (78 runs sampled)\n```\nIn light of this, we're going to try upgrading our services to the latest SDK once more. I'll update if we run into regressions again.\n. ",
    "shamoons": "@AdityaManohar I don't think that I'm posting the form. If you look at the data I'm sending:\n\nIt's not a form data\n. How would I get that going?\n. @AdityaManohar tried that. In my generation code, I have\n```\n  AWS.config.credentials = new AWS.Credentials 'myKey', 'mySecret'\ns3 = Promise.promisifyAll new AWS.S3()\n  filename = \"#{req.body.EntityId}_#{req.body.filename}\"\n  s3.getSignedUrlAsync 'putObject',\n    Bucket: 'mybucket'\n    Key: filename\n    Expires: 60 * 20 # 20 minute expiration\n    ContentType: 'image/png'\n  .then (presigned_url) ->\n    res.json\n      data:\n        type: 'presigned-url'\n        url: presigned_url\n  .catch (err) ->\n    next err\n```\nAnd even if a cURL it with the specified Content-Type, it fails\ncurl -v -k -H \"Content-Type:image/png\" -v --upload-file tumblr_nl29hydgtP1uq22wlo1_r2_1280.jpg https://s3.amazonaws.com/mybucket/tumblr_nl29hydgtP1uq22wlo1_r2_1280.jpg\\?AWSAccessKeyId\\=mykey\\&Expires\\=1428010954\\&Signature\\=VFVsPs51X97OcbXSAPXb1KW59Ec\\=\nSame 403 error\n. @AdityaManohar I think if I can resolve the curl case, the rest will flow easily. When I send along a Content-Type, I get the 403 error. I updated the issue title to reflect this.\n. Yeah, it's working it seems. But I created a new issue to resolve a new problem I'm having https://github.com/aws/aws-sdk-js/issues/558\n. To generate the signed URL, I am doing:\ns3 = Promise.promisifyAll new AWS.S3()\n  filename = \"#{req.body.EntityId}_#{req.body.filename}\"\n  s3.getSignedUrlAsync 'putObject',\n    Bucket: 'mybucket'\n    Key: \"#{req.body.EntityId}.test.jpg\"\n    Expires: 60 * 20 * 10 # 20 minute expiration\n    ContentType: 'image/png'\n  .then (presigned_url) ->\n    res.json\n      data:\n        type: 'presigned-url'\n        url: decodeURIComponent presigned_url\n  .catch (err) ->\n    next err\nThe problem is when uploading from the browser, the Content-Type is set to multipart/form-data with a boundary, so Content-Type    multipart/form-data; boundary=----WebKitFormBoundary0gHnwFcAgxP8mmOn\nMy client side code to upload is:\n```\n      var upload_url = response.data.data.url;\n  var fd = new FormData();\n  fd.append('file', $scope.upload_file);\n  console.log(fd);\n\n  $http.put(upload_url, fd, {\n    transformRequest: angular.identity,\n    headers: {\n      'Content-Type': undefined\n    }\n  }).then(function() {\n    return upload_url;\n  });\n\n```\n. ",
    "patoncrispy": "@AdityaManohar, is that informationn regarding the use of FormData mentioned in the documentation anywhere? Really useful info to know! \n. Hmm @dmyers that didn't work for me. That generated a URL with the format https://{bucket}.s3-{region}.amazonaws.com/{key}, whereas what is required is https://s3-{region}.amazonaws.com/{bucket}/{key}. Not sure why there is a difference...\n. ",
    "rdsedmundo": "I'm with the same problem described by him.\n@AdityaManohar your statement is invalid, simply because I don't have the Content-Type before making the request. It's generated by the browser as it does include a boundary alongside the multipart/formdata, i.e Content-Type: multipart/form-data; boundary=A23ieKkw1233edldA.. ",
    "mpranjic": "Thank you @AdityaManohar for confirmation!\n. ",
    "juliangruber": "Yeah, it's from a fs.createReadStream(). The original source of error is that the file doesn't exist in the first place. However the sync lstat call is reached first before the fs module's internal fs.open(...) reaches disk, so it's not the stream emitting an error.\nHm you're right that it can't be this codepath, I'll find the real one causing this.\n. - https://github.com/aws/aws-sdk-js/blob/master/lib/s3/managed_upload.js#L325\n- https://github.com/aws/aws-sdk-js/blob/master/lib/s3/managed_upload.js#L331\n- https://github.com/aws/aws-sdk-js/blob/master/lib/s3/managed_upload.js#L412\nThose look not be be caught by any try/catch. I susped it's the last one, I'll try to come up with a stack trace.\n. Error: ENOENT: no such file or directory, lstat './data/1428314400000-1428316142276-30875.txt'\n    at Error (native)\n    at Object.fs.lstatSync (fs.js:838:18)\n    at Object.byteLength (XXX/node_modules/aws-sdk/lib/util.js:172:39)\n    at Request.SET_CONTENT_LENGTH XXX/node_modules/aws-sdk/lib/event_listeners.js:93:38)\n    at Request.callListeners (XXX/node_modules/aws-sdk/lib/sequential_executor.js:100:18)\n    at Request.emit (XXX/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (XXX/node_modules/aws-sdk/lib/request.js:604:14)\n    at Request.transition (XXX/node_modules/aws-sdk/lib/request.js:21:12)\n    at AcceptorStateMachine.runTo (XXX/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at XXX/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request.<anonymous> (XXX/node_modules/aws-sdk/lib/request.js:22:9)\n    at Request.<anonymous> (XXX/node_modules/aws-sdk/lib/request.js:606:12)\n    at Request.callListeners (XXX/node_modules/aws-sdk/lib/sequential_executor.js:104:18)\n    at callNextListener (XXX/node_modules/aws-sdk/lib/sequential_executor.js:90:14)\n    at Request.computeSha256 (XXX/node_modules/aws-sdk/lib/services/s3.js:230:7)\n    at Request.callListeners (XXX/node_modules/aws-sdk/lib/sequential_executor.js:97:18)\n. This is due to a race condition. So yeah initially it's a problem on my side, but a function taking a callback should always\n- report errors only to that callback\n- do only async work, no *Sync calls\nIf you want I can get into why I think those points are important.\nFor now I'll set stream.length on my own, but I still consider this unexpected behavior, so I'm leaving this open for you to decide.\n. ",
    "DavidTPate": "I was running into the same issue as through the console it seems like it is just sending * for Everyone when setting the permissions. This might be a good caveat to add to the documentation as it's not clear that the policy needs to be attached instead of using the addPermission API. \nAt first it appeared to me that the API implemented in the SDK just wasn't allowing * instead of it needing to be done in a different way.\n. ",
    "toaster33": "Some more examples showing Statement being an Array.\nPossible section to update with an example labeled Granting Anonymous Access to a Queue.\n. ",
    "MohsenElgendy": "node@v0.12.2\nnpm@2.7.4\naws-sdk@2.1.23\n. I had a workaround for the issue for now, but i do not recommend it.\nWhat i did was that i changed my S3 configure array, to this \nnew AWS.S3({params: {Bucket: 'bucket-name', Key: 'test.txt'}, paramValidation: false})\nI added the param called \"paramValidation\" and set it to false, so it stopped the method from validating the issue so the file was uploaded successfully, Now this seems like a very wrong way to work around the issue but since i am in a hurry, i can leave it for now, but i still need an explanation for what is going on.\n. @lsegal Looks like my environment wasn't as clean as i might have thought, I had a method that was added to the Object.prototype called count so it affected the whole environment.\njavascript\nObject.prototype.count = function () {\n    var count = 0;\n    for( var prop in this ) {\n        count = count + 1;\n    }\n    return count;\n};\nThanks for the help. \n. ",
    "johnpryan": "@lsegal thanks for the quick response.  After digging a bit further I think object enumeration works for my use-case.   Full disclosure- I'm using Dart's JS Package for Dart -> JS interoperability. which adds the prototype fields as non-enumerable properties already.\n. I think I figured it out - I was passing an invalid params object to the aws.S3.putObject() function (not properly converted to JS)\n. ",
    "balmasi": "@lsegal Actually I ended up HAVING TO use in-memory buffer to bypass this problem.\n@AdityaManohar I'll let you know as soon as I test it.\n. ",
    "virtuotechnologies": "s3.upload({ Key: \"something/\"+ this.files[0]._file.name, Bucket: bucketName, Body: file, ACL: 'public-read'}, function (err, data) {\nif (err) {\nconsole.log(err, 'there was an error uploading your file');\n}\nvar pname = data.Location;\nvar C_IMAGE = {\"CATEGORY_IMAGE\": pname};\nvar scope = this;\nscope.service.updatecategory(cid, scope.at, C_IMAGE).subscribe(udata => {\nconsole.log(\"Sucess\",udata);\n});\n});\n---in console  this bolded function is said undefined. Please tell how to uplaod the location.. ",
    "alexandarp": "Thanks for the quick reply. I will get back to you with a solution. \n. @lsegal I have an update. The solution for me was to uri encode the image key:\nencode_key = encodeURIComponent(image.Key)\n          params =\n            Bucket: \"backup-#{bucket}-west\"\n            CopySource: \"#{bucket}/#{encode_key}\"\n            Key: encode_key\nI can then use decodeURIComponent() to convert the files to the original characters if I need to. I am not sure if this would be a solution for anyone else experiencing the problem.\n. ",
    "PeterDaveHello": "np\n. ",
    "scampiuk": "Hi,\nThanks, yes I'm going to do that, I've also raised a support ticket with AWS ;) \nThanks again,\nChris -. \n. Hi,\nThanks, yes I'm going to do that, I've also raised a support ticket with AWS ;) \nThanks again,\nChris -. \n. For people who get her via Google, and are looking for answers:\n\nThe Cloudformation team has added your feature request, thank you for the service feedback.\nAt the moment, the only option will be to proxy requests through an intermediate server that can tunnel requests to the Cloudformation API endpoints using any AWS SDK. \nFinally, I would like to add to the previous correspondence. \nWhile for AWS requests you can disable CORS security checking, your clients would be vulnerable to XSS attacks. \n. For people who get her via Google, and are looking for answers:\nThe Cloudformation team has added your feature request, thank you for the service feedback.\nAt the moment, the only option will be to proxy requests through an intermediate server that can tunnel requests to the Cloudformation API endpoints using any AWS SDK. \nFinally, I would like to add to the previous correspondence. \nWhile for AWS requests you can disable CORS security checking, your clients would be vulnerable to XSS attacks. \n. \n",
    "yetithefoot": "Probably it connected with #570. Will check version 2.1.23.\n. Looks like it works in 2.1.23.\nBut what is best strategy for uploading big files?\n. ",
    "h2non": "Thanks for the reply.\nSounds reasonable. I see your point and it could be definitively great introducing support for a pipeable interface. The unique challenge here is about keeping the existent interface consistency.\nAs far I've seen after a brief code review, putObject() (and indeed almost all of the service API methods) returns an instance of Request, so maybe extending the prototype chain with the required _write() method used by node.js streams could be enough. The client should control if the body was already defined (and it's required) for the given request so the request is ready to fly over the network, otherwise it could wait until the input stream chucks of data are ready.\nI wrote a trivial example to demonstrate a possible approach (but breaking the current interface):\nhttps://gist.github.com/h2non/b30e7e6114415cbef172\nIt uses a small module which acts as wrapper.\nI would be happy to give a hand if you're considering this feature seriously\n. Thanks for the reply.\nSounds reasonable. I see your point and it could be definitively great introducing support for a pipeable interface. The unique challenge here is about keeping the existent interface consistency.\nAs far I've seen after a brief code review, putObject() (and indeed almost all of the service API methods) returns an instance of Request, so maybe extending the prototype chain with the required _write() method used by node.js streams could be enough. The client should control if the body was already defined (and it's required) for the given request so the request is ready to fly over the network, otherwise it could wait until the input stream chucks of data are ready.\nI wrote a trivial example to demonstrate a possible approach (but breaking the current interface):\nhttps://gist.github.com/h2non/b30e7e6114415cbef172\nIt uses a small module which acts as wrapper.\nI would be happy to give a hand if you're considering this feature seriously\n. Thanks for the fast reply. Let's separate things.\nForget about the module. I just made made it to satisfy my needs (which are partially related to the topic we're actually discussing here, but not specifically for this purpose). Ideally no third-party library should be used to accomplish this feature.\nNever break interfaces. 100% agree. This is a must software principle. Note that I was just pointing into a representative example, not a real production focused implementation, and should be considered only as an inspirational start point.\nI can see your point about extending the SDK with a sort of plugins or middleware, but I believe that this is simply not required for the feature we're discussing here since it's a low-level feature and probably very coupled to the internal state logic like to be fully externalized.\nThe point I was talking about is, indeed, the idea of implementing a stream writable compatible interface in the SDK Request prototype chain. I think this doesn't break any of the existent interface contract. The unique thing here is how to deal and dispatch the request factory until the chunks has been received at all (in the case that they come from readable stream via pipe)\nI can't understand at all what you are pointing about the retries... I guess that after you have the complete buffer of the body you can cache it and perform the required operations over it when you want, and all of this operations are encapsulated and agnostic to the consumer.\nObviously my knowledge about the internal SDK core is very limited, so what's your analysis about this and possible constraints from a more low-level point of view? Thanks\n. Thanks for the fast reply. Let's separate things.\nForget about the module. I just made made it to satisfy my needs (which are partially related to the topic we're actually discussing here, but not specifically for this purpose). Ideally no third-party library should be used to accomplish this feature.\nNever break interfaces. 100% agree. This is a must software principle. Note that I was just pointing into a representative example, not a real production focused implementation, and should be considered only as an inspirational start point.\nI can see your point about extending the SDK with a sort of plugins or middleware, but I believe that this is simply not required for the feature we're discussing here since it's a low-level feature and probably very coupled to the internal state logic like to be fully externalized.\nThe point I was talking about is, indeed, the idea of implementing a stream writable compatible interface in the SDK Request prototype chain. I think this doesn't break any of the existent interface contract. The unique thing here is how to deal and dispatch the request factory until the chunks has been received at all (in the case that they come from readable stream via pipe)\nI can't understand at all what you are pointing about the retries... I guess that after you have the complete buffer of the body you can cache it and perform the required operations over it when you want, and all of this operations are encapsulated and agnostic to the consumer.\nObviously my knowledge about the internal SDK core is very limited, so what's your analysis about this and possible constraints from a more low-level point of view? Thanks\n. > Hope that explains some things!\nAbsolutely!\n\nThis isn't quite feasible. Many payloads, especially for interfaces that we want to use streaming operations for (namely S3), can be extremely large (500MB, 1GB, or larger)\n\nYou're totally right. It's simply non-viable. Indeed V8 max allocated memory is limited to 1GB.\n\nNode.js streams are uni-directional streams that have no native concept of seeking or length.\n\nIn node/io.js there're duplex streams, so technically they're bi-directional if we consider it as a whole abstract data type entity. For instance, network sockets uses both directions.\n\nEffectively the analysis has always been that Node.js streams are an insufficient abstraction to support our use case. We need to support seekable streams\n\nThat's true, but you can discovery the byte length via the _write in a writable stream on every  buffer chunk (like you're currently doing when using fs.createReadStream), and ignoring the buffer data which should only remain in the stack for a limited amount of time.\nTending to simplify a bit the things. \nEssentially I believe that the problem here is not related to the mechanism to read data from a stream, which is actually done and works fine. It's just to find the way to plug-in different interfaces with the same behavior.\nI think that currently we have: \nReadableStream -> Body -> Request Flow\nAnd the idea is to have something like: \nReadableStream | WritableStream (collector) | ReadableStream -> Body -> Request Flow\nThank you for the reply!\nPS: To be honest I feel like the potential benefit of this feature doesn't justify the invested time discussing/thinking about it, but it's definitively interesting digging into it. \n. > Hope that explains some things!\nAbsolutely!\n\nThis isn't quite feasible. Many payloads, especially for interfaces that we want to use streaming operations for (namely S3), can be extremely large (500MB, 1GB, or larger)\n\nYou're totally right. It's simply non-viable. Indeed V8 max allocated memory is limited to 1GB.\n\nNode.js streams are uni-directional streams that have no native concept of seeking or length.\n\nIn node/io.js there're duplex streams, so technically they're bi-directional if we consider it as a whole abstract data type entity. For instance, network sockets uses both directions.\n\nEffectively the analysis has always been that Node.js streams are an insufficient abstraction to support our use case. We need to support seekable streams\n\nThat's true, but you can discovery the byte length via the _write in a writable stream on every  buffer chunk (like you're currently doing when using fs.createReadStream), and ignoring the buffer data which should only remain in the stack for a limited amount of time.\nTending to simplify a bit the things. \nEssentially I believe that the problem here is not related to the mechanism to read data from a stream, which is actually done and works fine. It's just to find the way to plug-in different interfaces with the same behavior.\nI think that currently we have: \nReadableStream -> Body -> Request Flow\nAnd the idea is to have something like: \nReadableStream | WritableStream (collector) | ReadableStream -> Body -> Request Flow\nThank you for the reply!\nPS: To be honest I feel like the potential benefit of this feature doesn't justify the invested time discussing/thinking about it, but it's definitively interesting digging into it. \n. ",
    "achselschweisz": "Hi,\n+1 for adding piping support\nI, too, was excited at first to see the upload method taking readable streams as parameters, and immediately changed the implementation of one of our applications to use that.\nfyi: the application takes a readable stream and pipes it to several writable streams at the (virtually) same time, writing files, to s3, and other destinations (all implemented in Transform streams).\nAnyway, as you probably guessed, stuff didn't work as expected, since the aws-sdk internally consumes the buffers (you are adding a readable listener in the \"send\" method) in the readable stream which leaves the other pipes with 0 bytes of data.\nHowever, \n\nEffectively the analysis has always been that Node.js streams are an insufficient abstraction to\nsupport our use case. We need to support seekable streams-- something we use in all other SDKs-- \nso that we can support reading a payload multiple times without buffering the entire payload into \nmemory. Reading the payload multiple times is necessary for signing, generating extra checksums \n(a 2nd read pass in input), .... \n\nI am sorry to say, but I have to call bull$hit on this :-) This functionality is what Transform streams can be used for, after all. We've been doing this w/ \"uploadPart\" internally so far (buffer the part you want to retry / sign / etc).\nPiping streams is one of the most powerful and useful features node / iojs have to offer, I reckon all aws-sdk-js users would benefit greatly if this was supported.\nCheerio!\n. Btw: a quick  workaround for piping a readable stream into several writables including handing over the readable stream to the \"upload\" method is to pipe the original readable stream into another readable (Transform) stream, then hand over the reference to the upload method (e.g. the stream.PassThrough class):\n```\nvar stream = require('stream');\nvar fs = require('fs');\nvar readable = fs.createReadStream('xxx');\nvar passThrough = new stream.PassThrough();\nvar s3ref = readable.pipe(passThrough);\n// suppose we have Writable streams \"A\" and \"B\", then we could do:\nreadable.pipe(A);\nreadable.pipe(B);\ns3client.upload({ Body: s3ref });\n```\nThis way the buffer of s3ref will get consumed w/o interfering with the original readable stream.\n. @Mic75 Sorry it's been some time since your question:\nQuoting myself here:\n\nAnyway, as you probably guessed, stuff didn't work as expected, since the aws-sdk internally \nconsumes the buffers (you are adding a readable listener in the \"send\" method) in the readable \nstream which leaves the other pipes with 0 bytes of data.\nThis way the buffer of s3ref will get consumed w/o interfering with the original readable stream.\n\nSo basically by using your own stream (or the pass-through) which you can pipe into the aws-sdk, you will still have control over it despite the fact that the stream on the aws-sdk's end is being consumed.\nThe authors are basically buggering up the way stream piping works, hence the need for \"something in between\" :-)\n. ",
    "Mic75": "@achselschweisz Your workaround work like a charm. I am using it to to upload from one host (with the request package ) directly to a S3 bucket. \nI am not understanding the necessity of to pipe the readable stream into the passThrough though.\nFor instance, any idea why this code isn't working?\njavascript\nrequest.get('somehostname')\n        .on('response', response => upload({Bucket: bucket, Key: 'test-hub/UnityHubSetup.exe', Body: response}). ",
    "emptyemail": "I did not modify the SDK, I just put a transform in between the SDK and my other streams. \nCoffeeScript\nclass BufferingTransform extends Transform\n  constructor: (opts) ->\n    super(opts)\n    @bufferSize = opts.bufferSize\n    @buffers = []\n    @size = 0\n  _transform: (data, encoding, callback) ->\n    @buffers.push(data)\n    @size+=data.length\n    if @size >= @bufferSize\n      return @_flush(callback)\n    callback()\n  _flush: (callback) ->\n    @push(Buffer.concat(@buffers))\n    @buffers = []\n    @size = 0\n    callback()\n. ",
    "lwoggardner": "Thanks\nOn 9 May 2015 4:53 am, \"Loren Segal\" notifications@github.com wrote:\n\nThis is fixed by the above commit and will be in our next release. If you\nwant to run it now you can:\nnpm install git://github.com/aws/aws-sdk-js\nAnd use:\nvar params = {Bucket: 'bucket', NotificationConfiguration: {}};\ns3.putBucketNotification(params, function(err, data) { console.log(err, data) });\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/598#issuecomment-100326409.\n. Thanks\nOn 9 May 2015 4:53 am, \"Loren Segal\" notifications@github.com wrote:\nThis is fixed by the above commit and will be in our next release. If you\nwant to run it now you can:\nnpm install git://github.com/aws/aws-sdk-js\nAnd use:\nvar params = {Bucket: 'bucket', NotificationConfiguration: {}};\ns3.putBucketNotification(params, function(err, data) { console.log(err, data) });\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/598#issuecomment-100326409.\n. \n",
    "doapp-jeremy": "Right, except getItem doens't always return almost immediately.  During testing, I set the read throughput to 1 and get throttling.   Say that I have 99 queries that take 10ms, and 1 query that takes 10 seconds.  I would expect to continue on with my putItem for the 99 that have finished, before waiting for the one that takes 10 seconds, but that's now that I see.  I see all 100 put's waiting until all 100 get's finish.\n. Yep, using node, I'll give that a shot.\n. And now it's working as I expected.  Thanks!\n. Yep, I am.  I wish I could use io.js, but I'm writing this for lambda.  Can you push those guys to support io.js soon ;)\n. @AllanFly120 thanks for the response, yes, it's a consistent issue, however, the exact # of putObject or deleteMessage calls that are delayed varies by 1 or two.. @AllanFly120 sorry for the slow response, been out of the office for quite some time.\nBelow is the code I use to delete the message.  It's a regular queue and the default visibility timeout is 1 minute.\n```\nlet deleted = 0;\nconst deleteMessage = async(receiptHandle) => {\n  let deleteNum = ++deleted;\nlogger.log('info', 'deleting message %d', deleteNum);\n  const start = new Date().getTime();\n  return sqs.deleteMessage({\n      QueueUrl: QUEUE_URL,\n      ReceiptHandle: receiptHandle\n    }).promise()\n    .then(result => {\n      const end = new Date().getTime();\n      logger.log('info', 'deleted %d message in %dms: %s', deleteNum, (end - start), result.ResponseMetadata.RequestId);\n      return result;\n    })\n    .catch(err => {\n      logger.log('error', err.stack);\n    });\n}\n```. ",
    "jblack10101": "I'm having this same issue. Was a stack overflow ever created?\n. I'm having this same issue. Was a stack overflow ever created?\n. ",
    "geminiyellow": "@AdityaManohar @lsegal  \n+1 i want webpack support too.\n```\nWARNING in ./~/aws-sdk/lib/util.js\nCritical dependencies:\n40:30-45 the request of a dependency is an expression\n43:11-53 the request of a dependency is an expression\n @ ./~/aws-sdk/lib/util.js 40:30-45 43:11-53\nWARNING in ./~/aws-sdk/lib ^.\\/.$\nModule not found: Error: Cannot resolve directory '.' in /Users/me/Documents/Sources/my-project/client/node_modules/aws-sdk/lib\n @ ./~/aws-sdk/lib ^.\\/.$\nWARNING in ./~/aws-sdk/lib/api_loader.js\nCritical dependencies:\n13:15-59 the request of a dependency is an expression\n104:12-46 the request of a dependency is an expression\n108:21-58 the request of a dependency is an expression\n114:18-52 the request of a dependency is an expression\n @ ./~/aws-sdk/lib/api_loader.js 13:15-59 104:12-46 108:21-58 114:18-52\nWARNING in ./~/aws-sdk/lib/region_config.json\nModule parse failed: /Users/me/Documents/Sources/my-project/client/node_modules/aws-sdk/lib/region_config.json Line 2: Unexpected token :\nYou may need an appropriate loader to handle this file type.\n| {\n|   \"rules\": {\n|     \"/\": {\n|       \"endpoint\": \"{service}.{region}.amazonaws.com\"\n @ ./~/aws-sdk/lib ^.\\/.*$\nERROR in ./~/aws-sdk/lib/api_loader.js\nModule not found: Error: Cannot resolve module 'fs' in /Users/me/Documents/Sources/my-project/client/node_modules/aws-sdk/lib\n @ ./~/aws-sdk/lib/api_loader.js 1:9-22\nERROR in ./~/aws-sdk/lib/services.js\nModule not found: Error: Cannot resolve module 'fs' in /Users/me/Documents/Sources/my-project/client/node_modules/aws-sdk/lib\n @ ./~/aws-sdk/lib/services.js 1:9-22\n```\nit feel so suck.\n. @prescottprue @rstormsf @dbernstein maybe what you need is no parse it.\nwebpack.conf.js\nmodule: {\n  noParse: [/aws-sdk.js/]\n}\n. ",
    "ryanyogan": "This issue isn't specific to this repo, having said that Webpack does not care what you throw at it, you may use Browserify transforms in Webpack.\nThis particular problem looks to be specific to JSON so try adding this to your Webpack config:\nnpm i --save-dev json-loader\njs\nmodule: {\n  loaders: [\n    { test: /\\.json$/, loader: 'json' }\n  ]\n}\nIf that does not fix everything have a look at google \"webpack transform packegify\"\n. ",
    "rstormsf": "Nope, it doesn't fix it. +1 for webpack support\n. The same here :-(\n. ",
    "prescottprue": "+1 for webpack support. This stack overflow question says it has been solved, but I am still seeing errors about Node related stuff.\nThe main errors include the following: Module not found: Error: Cannot resolve module 'fs' in .... If it is in a package that gets browserified before being imported, then I was able to get it to work (still haven't gotten it to work with a loader).\n. I was able to get something working using a solution similiar to @ geminiyellow:\nmodule:{\n noParse: [/aws-sdk/]\n}\nNotice that the regex is for the whole aws-sdk module folder, not just the main aws-sdk.js file (as the error was coming from parsing of other files).\n. Posted another solution\nBasically:\nUsing the noParse method should work if you are creating a node package but not for umd format. To do umd I had to use loaders to Browserify and handle json files.\nInstall the loaders:\nnpm install json-loader --save-dev\nnpm install transform-loader brfs --save-dev\nWebpack Config:\nmodule: {\n  loaders: [\n    { test: /aws-sdk/, loaders: [\"transform?brfs\"]},\n    { test: /\\.json$/, loaders: ['json']},\n  ]\n}\n. Did the same thing as @basarat separately and it works.\n. @jagi and @dukedougal did you try the solution posted by @basarat? I have used this same method to implement aws-sdk in tons of projects.\nIt makes more sense to setup a more specific webpack config rather than modify how this library works just for \"supporting webpack\".  This is a nodejs library, so it makes sense that it uses node specific packages like fs, you just need to make sure that your build config (webpack) knows about this and makes the appropriate adjustments.\n. ",
    "dbernstein": "+1 for webpack support.  I'm seeing the same problem.\n. +1 for webpack support.  I'm seeing the same problem.\n. Thanks @geminiyellow for your suggestion: I tried that - no luck.  when I run npm install I still get  the warnings below.   When I try running my code none of the AWS api's are available ( javascript console error = \"AWS.SNS is not a function\"). It would appear that the messages below are related.  If you have any other ideas I would appreciate it.  I've been cycling on this for a few hours.\nWARNING in ./~/aws-sdk/lib/util.js\nCritical dependencies:\n40:30-45 the request of a dependency is an expression\n43:11-53 the request of a dependency is an expression\n @ ./~/aws-sdk/lib/util.js 40:30-45 43:11-53\nWARNING in ./~/aws-sdk/lib/api_loader.js\nCritical dependencies:\n13:15-59 the request of a dependency is an expression\n104:12-46 the request of a dependency is an expression\n108:21-58 the request of a dependency is an expression\n114:18-52 the request of a dependency is an expression\n @ ./~/aws-sdk/lib/api_loader.js 13:15-59 104:12-46 108:21-58 114:18-52\n. ",
    "wesleyyee": "+1\n. ",
    "jkudish": "This solution worked for me, but it would be great to see better webpack support in the future.\n. I am trying to implement a stream uploader inside of an Electron application, and this is one of the barrier's that I've hit. This PR would help move things forward for me and anyone else who tries to implement something similar in an Electron application.\n. ",
    "dukedougal": "Please provide webpack support.\n. Is this likely to be reviewed? Webpack is fundamental now to the JavaScript community.\n. It's really hard to understand why one of the most popular build tools is not supported.  Surely this justifies priority action.\n. It's challenging to understand why Amazon doesn't fix Webpack support.\n. @prescottprue it should work out of the box.  \nAWS should work seamlessly with the major JavaScript build systems. Users should not need to initially try and fail to make it work then somehow know to dig down in github and to know that one specific comment by one guy is the way to make it work. Amazon should fix it and they haven't more than a year after the initial report on this thread.\nThis thread is labelled as \"feature request\" but its really a bug. No milestone, no-one assigned.\n. Any guesses as to approx when this might become available in the released version?\nthanks\n. I need the server version for node.js, not browser.\n. Hi @chrisradek \nThanks for checking it out.  The browser I am using is Github Electron for Windows.\nThe version numbers come from typing \"process.versions\" at the console.\n\nprocess.versions\nObject {http_parser: \"2.6.0\", node: \"5.1.1\", v8: \"4.7.80.27\", uv: \"1.7.5\", zlib: \"1.2.8\"\u2026}ares: \"1.10.1-DEV\"atom-shell: \"0.36.7\"chrome: \"47.0.2526.110\"electron: \"0.36.7\"http_parser: \"2.6.0\"modules: \"47\"node: \"5.1.1\"openssl: \"1.0.2e\"uv: \"1.7.5\"v8: \"4.7.80.27\"zlib: \"1.2.8\"proto: Object\n\nI'll see if I can find time to try your code above. \nFor now I have rewritten this part of my application to sequentially upload everything and it all works fine.\nThere wasn't any way I could get concurrent uploads to work however so maybe there's something you are doing differently - I'll check it out.\nthanks\n. I've converted all my code to ensure it only ever uploads one file at a time and it works fine.  I gave up trying to get parallel to work.\n. ",
    "amowu": "+1\n. +1\n. ",
    "jontewks": "+1\n. ",
    "mdramos": "+1 for webpack support.\n. ",
    "bendenoz": "+1\n. +1\n. this provides the best work-around so far : http://andrewhfarmer.com/aws-sdk-with-webpack/\nbasically \njavascript\nrequire('aws-sdk/dist/aws-sdk');\nvar AWS = window.AWS;\ninstead of require('aws-sdk')\n. this provides the best work-around so far : http://andrewhfarmer.com/aws-sdk-with-webpack/\nbasically \njavascript\nrequire('aws-sdk/dist/aws-sdk');\nvar AWS = window.AWS;\ninstead of require('aws-sdk')\n. ",
    "slessans": "+1\n. ",
    "joscha": "You can also use a loader to make that a bit cleaner and have no global exposed:\n``` javascript\nvar prePackagedAwsPath = path.join('aws-sdk', 'dist', 'aws-sdk');\nmodule.exports = {\n  resolve: {\n      alias: {\n          'aws-sdk': prePackagedAwsPath\n      }\n  },\n  module: {\n      loaders: [\n          {\n              test: require.resolve(prePackagedAwsPath),\n              loader: 'imports?window=>null'\n          }\n      ]\n  },\n```\nthen you can just use\njavascript\nconst AWS = require('aws-sdk');\n. You can also use a loader to make that a bit cleaner and have no global exposed:\n``` javascript\nvar prePackagedAwsPath = path.join('aws-sdk', 'dist', 'aws-sdk');\nmodule.exports = {\n  resolve: {\n      alias: {\n          'aws-sdk': prePackagedAwsPath\n      }\n  },\n  module: {\n      loaders: [\n          {\n              test: require.resolve(prePackagedAwsPath),\n              loader: 'imports?window=>null'\n          }\n      ]\n  },\n```\nthen you can just use\njavascript\nconst AWS = require('aws-sdk');\n. ",
    "EloB": "+1\n. ",
    "mwildehahn": "This is so painful...\n. ",
    "Iuriy-Budnikov": "+1\n. +1\n. The same issue with .mov files\n. @chrisradek \nI am using custom build \n$ AWS_SERVICES=s3 browserify index.js > browser-app.js\nfrom http://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-building.html\nConfig\nnew AWS.S3({\n      region: config.awsS3Region,\n      signatureVersion: 'v4',\n      params: {\n        ACL: 'public-read',\n        Bucket: config.awsS3Bucket\n      },\n      accessKeyId: config.awsS3AccessKeyId,\n      secretAccessKey: config.awsS3SecretAccessKey,\n    });\nUpload params\n\nResponse with errors are empty\n\n\n\n\n\n\n\n\n. @chrisradek I'm tested on Mac OS, latest version of Chrome.\n\nAlso, are your mp3's >= 5MB in size?\n\nI tried to upload file less 5 mb. It works.\n. I'll check with <ExposeHeader>ETag</ExposeHeader> today.\n. @chrisradek Is it possible to upload files >= 5mb ?\n. > Also, the callback you supply to your upload operation receives an error and data object, depending on of it succeeded or failed. Can you log the error if there is one?\nThe issue that I didn't see any errors in log and response. \n. @chrisradek thanks. ETag resolved my issue.\n. ",
    "bergben": "+1\n. ",
    "mikedizon": "+1\n. +1\n. +1\n. +1\n. ",
    "alfonsodev": "@chrisradek what would be the solution for running it in node.js ? \n. @prescottprue  The title of the repo is \"AWS SDK for JavaScript in the browser and Node.js http://aws.amazon.com/javascript\" Description of the repo \"The official AWS SDK for JavaScript, available for browsers and mobile devices, or Node.js backends\"  so I don't agree this is just a nodejs library.\nAws JS team should listen the needs of users and give support to Webpack out of the box, or write simple instructions in the main Readme. The same problem is happening in other libraries like aws-iot-device-sdk \nI'm don't want to be negative, I think this could be solved if the AWS Javascript team meets and decide a general approach to take in all the JS packages to give true universal support when possible for all Javascript engines, (nodejs, browser, react-native). And adds the information to the Readme. \n. ",
    "pruett": "Not sure if this is useful, but I'm using webpack's exports-loader and can access the AWS variable locally within the js file where it was imported. Note, the AWS object is still attached to the window object. I tried using the imports loader to mitigate that, but wasn't successful. No further webpack config needed.\nbash\nnpm i -D exports-loader\nindex.js\njs\nimport AWS from 'exports?AWS!aws-sdk/dist/aws-sdk'\nconsole.log(AWS) // => Object {...}\n. ",
    "basarat": "I tried a few ways, here is my webpack setup with reasons: \n- Want to redirect aws-sdk to aws-sdk/dist/aws-sdk\nresolve: {\n    alias: {\n      'aws-sdk': 'aws-sdk/dist/aws-sdk'\n    }\n  },\n- Next aws-sdk doesn't do a nice export just take its local AWS variable and make it the export:\n```\n// npm install exports-loader --save-dev\n{\n  test: /aws-sdk.js/,\n  loader: 'exports?AWS'\n}\n```\n- Finally webpack will complain that don't use prebuilt files so tell webpack not to parse this sdk file: \nnoParse: [\n  /aws-sdk.js/\n],\nDone :rose: Now you can import * as AWS from \"aws-sdk\" both in the Browser and NodeJS\n. I tried a few ways, here is my webpack setup with reasons: \n- Want to redirect aws-sdk to aws-sdk/dist/aws-sdk\nresolve: {\n    alias: {\n      'aws-sdk': 'aws-sdk/dist/aws-sdk'\n    }\n  },\n- Next aws-sdk doesn't do a nice export just take its local AWS variable and make it the export:\n```\n// npm install exports-loader --save-dev\n{\n  test: /aws-sdk.js/,\n  loader: 'exports?AWS'\n}\n```\n- Finally webpack will complain that don't use prebuilt files so tell webpack not to parse this sdk file: \nnoParse: [\n  /aws-sdk.js/\n],\nDone :rose: Now you can import * as AWS from \"aws-sdk\" both in the Browser and NodeJS\n. @deviantony nope \ud83c\udf39 You'd need to make a custom build \u2764\ufe0f\n. @deviantony nope \ud83c\udf39 You'd need to make a custom build \u2764\ufe0f\n. > (also works with Java)\nThis is how to do it in Java. Would be great to get this supported in the JavaScript SDK :rose:\n```java\n            SSEAlgorithm sseAlgorithm = SSEAlgorithm.KMS;\n            GeneratePresignedUrlRequest generatePresignedUrlRequest = new GeneratePresignedUrlRequest(\n                                                                            bucket,\n                                                                            filePath,\n                                                                            HttpMethod.PUT);\n            generatePresignedUrlRequest.setSSEAlgorithm(sseAlgorithm);\n            generatePresignedUrlRequest.setKmsCmkId(kmsCmkId);\n            generatePresignedUrlRequest.setExpiration(expiration);\n            generatePresignedUrlRequest.setContentType(contentType);\n            generatePresignedUrlRequest.putCustomRequestHeader(\"Content-Length\", contentLength.toString());\n        URL url = s3client.generatePresignedUrl(generatePresignedUrlRequest);\n\n```. > (also works with Java)\nThis is how to do it in Java. Would be great to get this supported in the JavaScript SDK :rose:\n```java\n            SSEAlgorithm sseAlgorithm = SSEAlgorithm.KMS;\n            GeneratePresignedUrlRequest generatePresignedUrlRequest = new GeneratePresignedUrlRequest(\n                                                                            bucket,\n                                                                            filePath,\n                                                                            HttpMethod.PUT);\n            generatePresignedUrlRequest.setSSEAlgorithm(sseAlgorithm);\n            generatePresignedUrlRequest.setKmsCmkId(kmsCmkId);\n            generatePresignedUrlRequest.setExpiration(expiration);\n            generatePresignedUrlRequest.setContentType(contentType);\n            generatePresignedUrlRequest.putCustomRequestHeader(\"Content-Length\", contentLength.toString());\n        URL url = s3client.generatePresignedUrl(generatePresignedUrlRequest);\n\n```. So new question : Go SDK supports it. Can the JavaScript SDK support it? Would love to have it :heart:\n@jeskew forgive me if I've read your comment wrong :rose:. So new question : Go SDK supports it. Can the JavaScript SDK support it? Would love to have it :heart:\n@jeskew forgive me if I've read your comment wrong :rose:. > Out of curiosity, would you want to use this pre-signer in node or a browser environment?\nnode. > Out of curiosity, would you want to use this pre-signer in node or a browser environment?\nnode. ",
    "deviantony": "Is it possible to only import S3 service using @basarat solution?\n. Got the following issues when trying to use @chrisradek aws-sdk branch:\n```\nERROR in ./~/aws-sdk/browser/s3.js\nModule not found: Error: Cannot resolve 'file' or 'directory' ../apis/s3-2006-03-01.min in /.../website/web/node_modules/aws-sdk/browser\n @ ./~/aws-sdk/browser/s3.js 8:45-81\nERROR in ./~/aws-sdk/browser/s3.js\nModule not found: Error: Cannot resolve 'file' or 'directory' ../apis/s3-2006-03-01.paginators in /.../website/web/node_modules/aws-sdk/browser\n @ ./~/aws-sdk/browser/s3.js 9:56-99\nERROR in ./~/aws-sdk/browser/s3.js\nModule not found: Error: Cannot resolve 'file' or 'directory' ../apis/s3-2006-03-01.waiters2 in /.../website/web/node_modules/aws-sdk/browser\n @ ./~/aws-sdk/browser/s3.js 10:53-94\n```\nI'm using the following code in my app:\n``` javascript\nimport S3 from 'aws-sdk/browser/s3';\nconst s3 = new S3({...});\n...\n```\n. @chrisradek yeap, I'm using a json loader. Thanks for your work !\n. @chrisradek here's the webpack config I've used:\n``` javascript\nvar path = require('path');\nvar webpack = require('webpack');\nvar autoprefixer = require('autoprefixer');\nmodule.exports = {\n  entry: [\n    'webpack-dev-server/client?http://0.0.0.0:8080',\n    'webpack/hot/only-dev-server',\n    './src/index.jsx',\n  ],\noutput: {\n    path: path.join(__dirname, 'public'),\n    filename: 'bundle.js',\n    publicPath: '/',\n  },\nplugins: [\n    new webpack.NoErrorsPlugin(),\n  ],\nresolve: {\n    extensions: ['', '.js', '.jsx'],\n  },\nmodule: {\n    loaders: [\n      // Javascript & React JSX files\n      {\n        test: /.jsx?$/,\n        loaders: ['react-hot', 'babel?presets[]=react,presets[]=es2015'],\n        include: path.join(__dirname, 'src'),\n        exclude: /node_modules/,\n      },\n      {\n        test: /.scss$/,\n        loader: 'style-loader!css-loader!sass-loader!postcss-loader',\n      },\n      // Auth0-Lock Build\n      {\n        test: /node_modules[\\\\/]auth0-lock[\\\\/]..js$/,\n        loaders: ['transform-loader/cacheable?brfs',\n        'transform-loader/cacheable?packageify',\n      ],\n    }, {\n      test: /node_modules[\\\\/]auth0-lock[\\\\/]..ejs$/,\n      loader: 'transform-loader/cacheable?ejsify',\n    }, {\n      test: /.json$/,\n      loader: 'json',\n    },\n    // Bootstrap CSS\n    {\n      test: /.css$/,\n      loader: 'style-loader!css-loader',\n    }, {\n      test: /.(woff|woff2)(\\?v=\\d+.\\d+.\\d+)?$/,\n      loader: 'url?limit=10000&mimetype=application/font-woff',\n    }, {\n      test: /.ttf(\\?v=\\d+.\\d+.\\d+)?$/,\n      loader: 'url?limit=10000&mimetype=application/octet-stream',\n    }, {\n      test: /.eot(\\?v=\\d+.\\d+.\\d+)?$/,\n      loader: 'file',\n    }, {\n      test: /.svg(\\?v=\\d+.\\d+.\\d+)?$/,\n      loader: 'url?limit=10000&mimetype=image/svg+xml',\n    },\n    {\n      test: /.less$/,\n      loader: 'style!css!less',\n    },\n    {\n      test: /.(jpe?g|png|gif|svg)$/,\n      loader: 'file?hash=sha512&digest=hex&name=[hash].[ext]!image-webpack?bypassOnDebug&optimizationLevel=7&interlaced=false',\n    },\n  ],\n  postLoaders: [\n    { test: /.jsx?$/, loader: 'transform?envify' },\n  ],\n},\npostcss: function postCss() {\n  return [autoprefixer];\n},\ndevServer: {\n  historyApiFallback: true,\n  contentBase: 'public/',\n},\n};\n``\n. @chrisradek I just retried withnpm install git://github.com/chrisradek/aws-sdk-js.git#webpack`, is it updated yet? As I still have the same issue.\n. @chrisradek \nThis works perfectly:\n``` javascript\nimport S3 from 'aws-sdk/clients/s3';\nconst s3 = new S3({...});\n```\nAmazing job :clap: \n. ",
    "preetsethi": "I am attempting to use the @basarat solution but I get the following: \naws-sdk.js?085a:9867 Uncaught TypeError: Cannot read property 'crypto' of undefined\nAnyone seen this or know how to resolve?\n. ",
    "konsumer": "I don't think this is webpack-specific. I am having similar issues trying to pull out just what I need in browserify, and I suspect AMD (requirejs) people will too. The interface exposed externally is a giant monkey-patched AWS object with lots of internal dependencies that aren't being tracked. I think the first step is to figure out the right require order/placement for every file and make a top-level require that pulls in all the parts. That could be fed to browserify in this project to generate UMD, but also give developers like me, who just want a few things, a hint as to what needs to be pulled in, to just use some subset\nI think this is the start of that, but because of the dynamic stuff in api_loader it's throwing off everyone who needs a static dependency tree (direct import in ES6, webpack, browserify without brfs, etc.)\nIf that dynamic stuff is really needed (I'm not sure it is) I'd recommend a separate build-step that generates static require-lists, instead, and keep the generated files in this repo.\n. ",
    "lukejagodzinski": "I don't think that it was resolved as I'm still getting this error.\nI'm trying to use the create-react-app cli with aws-sdk. Just trying to import AWS from \"aws-sdk\" but I'm getting the following error\n```\nError in ./~/aws-sdk/lib/api_loader.js\nModule not found: Error: Cannot resolve module 'fs' in /Users/jagi/workspace/react/client/node_modules/aws-sdk/lib\nresolve module fs in /Users/jagi/workspace/react/client/node_modules/aws-sdk/lib\n  looking for modules in /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules\n    /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs doesn't exist (module as directory)\n    resolve 'file' fs in /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules\n      resolve file\n        /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs doesn't exist\n        /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs.js doesn't exist\n        /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs.json doesn't exist\n  looking for modules in /Users/jagi/workspace/react/client/node_modules\n    /Users/jagi/workspace/react/client/node_modules/fs doesn't exist (module as directory)\n    resolve 'file' fs in /Users/jagi/workspace/react/client/node_modules\n      resolve file\n        /Users/jagi/workspace/react/client/node_modules/fs doesn't exist\n        /Users/jagi/workspace/react/client/node_modules/fs.js doesn't exist\n        /Users/jagi/workspace/react/client/node_modules/fs.json doesn't exist\n[/Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs]\n[/Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs]\n[/Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs.js]\n[/Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs.json]\n[/Users/jagi/workspace/react/client/node_modules/fs]\n[/Users/jagi/workspace/react/client/node_modules/fs]\n[/Users/jagi/workspace/react/client/node_modules/fs.js]\n[/Users/jagi/workspace/react/client/node_modules/fs.json]\n @ ./~/aws-sdk/lib/api_loader.js 1:9-22\nError in ./~/aws-sdk/lib/services.js\nModule not found: Error: Cannot resolve module 'fs' in /Users/jagi/workspace/react/client/node_modules/aws-sdk/lib\nresolve module fs in /Users/jagi/workspace/react/client/node_modules/aws-sdk/lib\n  looking for modules in /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules\n    /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs doesn't exist (module as directory)\n    resolve 'file' fs in /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules\n      resolve file\n        /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs doesn't exist\n        /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs.js doesn't exist\n        /Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs.json doesn't exist\n  looking for modules in /Users/jagi/workspace/react/client/node_modules\n    /Users/jagi/workspace/react/client/node_modules/fs doesn't exist (module as directory)\n    resolve 'file' fs in /Users/jagi/workspace/react/client/node_modules\n      resolve file\n        /Users/jagi/workspace/react/client/node_modules/fs doesn't exist\n        /Users/jagi/workspace/react/client/node_modules/fs.js doesn't exist\n        /Users/jagi/workspace/react/client/node_modules/fs.json doesn't exist\n[/Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs]\n[/Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs]\n[/Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs.js]\n[/Users/jagi/workspace/react/client/node_modules/aws-sdk/node_modules/fs.json]\n[/Users/jagi/workspace/react/client/node_modules/fs]\n[/Users/jagi/workspace/react/client/node_modules/fs]\n[/Users/jagi/workspace/react/client/node_modules/fs.js]\n[/Users/jagi/workspace/react/client/node_modules/fs.json]\n @ ./~/aws-sdk/lib/services.js 1:9-22\n``\n. @prescottprue I can use solution proposed by @basarat because I'm usingcreate-react-apptool which does not allow modifying webpack config. Actually it does but it would require \"ejecting\" project and that will make it harder later to update to newest versions of react.\n. @ajmurmann for now I've just setup endpoints and I'm making http requests. It's temporary solution util aws-sdk support for webpack will be fixed.\n. I'm trying to use solution by @chrisradek but I can't make it work on the server. Everything works properly on the client. On the server I'm usingaws-sdkAPI in Lambda function. I have a project usingwebpackthat creates bundle which I upload to Lambda function. And when usingaws-sdk` I get the following error:\n{\n    \"errorMessage\": \"Cannot find module \\\"./region_config.json\\\"\",\n    \"errorType\": \"Error\",\n    \"stackTrace\": [\n        \"Object.<anonymous> (/var/task/build/handler.js:23392:182)\",\n        \"__webpack_require__ (/var/task/build/handler.js:21:30)\",\n        \"Object.<anonymous> (/var/task/build/handler.js:22843:21)\",\n        \"__webpack_require__ (/var/task/build/handler.js:21:30)\",\n        \"Object.<anonymous> (/var/task/build/handler.js:17663:2)\",\n        \"__webpack_require__ (/var/task/build/handler.js:21:30)\",\n        \"Object.<anonymous> (/var/task/build/handler.js:16601:12)\",\n        \"Object.module.exports.module.exports (/var/task/build/handler.js:16624:31)\",\n        \"__webpack_require__ (/var/task/build/handler.js:21:30)\"\n    ]\n}\nThe handler.js file is a bundle file. Here is a fragment of the handler.js:16624:31 file in which is error (the last line)\n``` js\n    / WEBPACK VAR INJECTION /(function(process) {var util = webpack_require(165);\n// browser specific modules\nutil.crypto.lib = __webpack_require__(286);\nutil.Buffer = __webpack_require__(287).Buffer;\n\n// browser stubs\nutil.nodeRequire = function nodeRequire(mod) {};\n\nvar AWS = __webpack_require__(166);\n\n// Load browser API loader\nAWS.apiLoader = function(svc, version) {\n  return AWS.apiLoader.services[svc][version];\n};\n\n/**\n * @api private\n */\nAWS.apiLoader.services = {};\n\n// Load the DOMParser XML parser\nAWS.XML.Parser = __webpack_require__(296);\n\n// Load the XHR HttpClient\n__webpack_require__(297);\n\nif (typeof process === 'undefined') {\n  process = {\n    browser: true\n  };\n}\n/* WEBPACK VAR INJECTION */}.call(exports, __webpack_require__(164)))\n\n```\n. @chrisradek I'm requiring entire library. What do you mean by \"to build a node.js bundle\". It's building project into one file/bundle and my output webpack is:\njs\n  output: {\n    path: paths.serverAppBuild,\n    filename: 'handler.js',\n    library: 'lib',\n    libraryTarget: 'commonjs2'\n  },\nAnd I've tested it works properly if I'm not using aws-sdk.\n. @chrisradek oh I see... I didn't know about the target option. I've changed it and now getting another error Cannot find module 'domain'. but it's probably a problem you're talking about.\nSorry for probably stupid question, but why it's not working with webpack? At the beginning I though that the problem is using import statement but even when I use require in webpack it's still not working. So is it like webpack take the aws-sdk library and just exports it but library is just not prepared to be used with webpack? And actually why it's not webpack compatible? There are many other libraries that when being created, haven't been designed to work with webpack but they work.\nDo you know how much time will it take you to make this change?\n. @chrisradek oh I see. Thanks for explanation. Do you know when can it be ready? Is it a big feature to implement?\n. ",
    "devhyunjae": "+1\n. +1\n. Ok i got it. I was hoping programatical query...\ud83d\ude22. ",
    "simonbuchan": "I've had success with the default entrypoint and ignoring require() calls with module.exprContext*, which doesn't include any services, so I use aws-sdk/dist-tools/service-collector to generate a services definition module:\n```\n// build-aws-services.js\nconst collector = require('aws-sdk/dist-tools/service-collector');\nmodule.exports = var AWS = require('aws-sdk');\n${collector(process.env.AWS_SERVICES)\n  .replace(/\\brequire\\('\\.\\//g, 'require(\\'aws-sdk/lib/')};\n```\nThen just add the output to the entry dynamically with val-loader:\n``\n// Seenode -p 'require(\"aws-sdk/lib/api_loader\").services'`\nconst services = 'cognitoidentityserviceprovider,cognitoidentity,firehose';\nprocess.env.AWS_SERVICES = services;\nmodule.exports = {\n  entry: {\n    app: [\n      'regenerator-runtime/runtime',\n      'val!./build-aws-services', // Enhance AWS with services from AWS_SERVICES\n      './src',\n    ],\n  },\n  module: {\n    // Ignore require(expr) in aws-sdk\n    exprContextRegExp: /$^/,\n    exprContextCritical: false,\n// ...\n``\n. I've had success with the default entrypoint and ignoringrequire()calls withmodule.exprContext*, which doesn't include any services, so I useaws-sdk/dist-tools/service-collector` to generate a services definition module:\n```\n// build-aws-services.js\nconst collector = require('aws-sdk/dist-tools/service-collector');\nmodule.exports = var AWS = require('aws-sdk');\n${collector(process.env.AWS_SERVICES)\n  .replace(/\\brequire\\('\\.\\//g, 'require(\\'aws-sdk/lib/')};\n```\nThen just add the output to the entry dynamically with val-loader:\n``\n// Seenode -p 'require(\"aws-sdk/lib/api_loader\").services'`\nconst services = 'cognitoidentityserviceprovider,cognitoidentity,firehose';\nprocess.env.AWS_SERVICES = services;\nmodule.exports = {\n  entry: {\n    app: [\n      'regenerator-runtime/runtime',\n      'val!./build-aws-services', // Enhance AWS with services from AWS_SERVICES\n      './src',\n    ],\n  },\n  module: {\n    // Ignore require(expr) in aws-sdk\n    exprContextRegExp: /$^/,\n    exprContextCritical: false,\n// ...\n``\n. See aws/amazon-cognito-identity-js#117 where I have to add a bunch of extra stuff in the docs for webpack usage that looks a bit weird to have in another aws project :)\n. See aws/amazon-cognito-identity-js#117 where I have to add a bunch of extra stuff in the docs for webpack usage that looks a bit weird to have in another aws project :)\n. @deviantony @chrisradek Looks like #1117 needs.jsonin (edit)resolve.extensionsright now, which it probably shouldn't. Workaround is to add it for now, but requires of.jsonfiles should include the extension.\n. @deviantony @chrisradek Looks like #1117 needs.jsonin (edit)resolve.extensionsright now, which it probably shouldn't. Workaround is to add it for now, but requires of.jsonfiles should include the extension.\n. @chrisradek Nah, it's the same module, just [webpack adds-loaderif it's not there](https://webpack.github.io/docs/configuration.html#resolveloader)\n. @chrisradek Nah, it's the same module, just [webpack adds-loaderif it's not there](https://webpack.github.io/docs/configuration.html#resolveloader)\n. @chrisradek Weird? Unless you've changed yourresolveLoaders.moduleTemplates, that should only happen if you only have ajson-loader-loadermodule!\n. @chrisradek Weird? Unless you've changed yourresolveLoaders.moduleTemplates, that should only happen if you only have ajson-loader-loader` module!\n. Nice!\nInitial Qs:\n- Can dist-tools/service-loader reuse the existing dist-tools? Seems we want to avoid duplication, and if I'm using those, I expect others are too, so they can't be removed.\n- Could /lib/browser_services be something like /svc/all and require all /svc/{id}?\n. To be a bit clearer on the last point, to do something like what core.js do to support partial use, so you can use only import S3 from 'aws-sdk/svc/s3'; and get just S3 in your build:\n``` js\n// aws-sdk/svc/s3.js\n// generated by aws-sdk/dist-tools/service-loader.js\nvar AWS = require('../lib/aws'); // will be '../lib/browser.js' in browserify, webpack, etc...?\nAWS.apiLoader.services['s3'] = {};\nAWS.S3 = AWS.Service.defineService('s3', [ '2006-03-01' ]);\nrequire('./services/s3');\nAWS.apiLoader.services['s3']['2006-03-01'] = require('../apis/s3-2006-03-01.min');\nAWS.apiLoader.services['s3']['2006-03-01'].paginators = require('../apis/s3-2006-03-01.paginators').pagination;\nAWS.apiLoader.services['s3']['2006-03-01'].waiters = require('../apis/s3-2006-03-01.waiters2').waiters;\nmodule.exports = AWS.S3;\n```\nIt... looks like it should work?\n. To be a bit clearer on the last point, to do something like what core.js do to support partial use, so you can use only import S3 from 'aws-sdk/svc/s3'; and get just S3 in your build:\n``` js\n// aws-sdk/svc/s3.js\n// generated by aws-sdk/dist-tools/service-loader.js\nvar AWS = require('../lib/aws'); // will be '../lib/browser.js' in browserify, webpack, etc...?\nAWS.apiLoader.services['s3'] = {};\nAWS.S3 = AWS.Service.defineService('s3', [ '2006-03-01' ]);\nrequire('./services/s3');\nAWS.apiLoader.services['s3']['2006-03-01'] = require('../apis/s3-2006-03-01.min');\nAWS.apiLoader.services['s3']['2006-03-01'].paginators = require('../apis/s3-2006-03-01.paginators').pagination;\nAWS.apiLoader.services['s3']['2006-03-01'].waiters = require('../apis/s3-2006-03-01.waiters2').waiters;\nmodule.exports = AWS.S3;\n```\nIt... looks like it should work?\n. This is now available in 2.6.0:\njs\nimport S3 from 'aws-sdk/clients/s3';\nTake care to avoid any use of the default aws-sdk module, for example for default config or credentials, instead for now:\njs\nimport 'aws-sdk/lib/node_loader'; // Hack needed before the first import\nimport { config } from 'aws-sdk/lib/core'; // or any other `aws-sdk` export\nHopefully this is cleaned up soon.\n. This is now available in 2.6.0:\njs\nimport S3 from 'aws-sdk/clients/s3';\nTake care to avoid any use of the default aws-sdk module, for example for default config or credentials, instead for now:\njs\nimport 'aws-sdk/lib/node_loader'; // Hack needed before the first import\nimport { config } from 'aws-sdk/lib/core'; // or any other `aws-sdk` export\nHopefully this is cleaned up soon.\n. Looks good! I hit an issue where using the non-client public APIs like default config or CognitoIdentityCredentials without pulling in all the default services has to import from aws-sdk/lib/core, so it may be a good idea to mention that somewhere:\njs\nimport { config, CognitoIdentityCredentials } from 'aws-sdk/lib/core';\n. Looks good! I hit an issue where using the non-client public APIs like default config or CognitoIdentityCredentials without pulling in all the default services has to import from aws-sdk/lib/core, so it may be a good idea to mention that somewhere:\njs\nimport { config, CognitoIdentityCredentials } from 'aws-sdk/lib/core';\n. Also the changes to dist-tools/service-collector broke my older hacky build that was munging and injecting the output of that into my entry, due to the missing .json, but probably not a big use case :)\n. Also the changes to dist-tools/service-collector broke my older hacky build that was munging and injecting the output of that into my entry, due to the missing .json, but probably not a big use case :)\n. Damn, just noticed that if you load aws-sdk/lib/core before any clients, then you need to first import aws-sdk/lib/node_loader, which is ugly.\n. Damn, just noticed that if you load aws-sdk/lib/core before any clients, then you need to first import aws-sdk/lib/node_loader, which is ugly.\n. That is much better, although it still also adds it to the global which is\na little bad mannered\nOn Sat, 10 Sep 2016, 02:30 Christopher Radek notifications@github.com\nwrote:\n\n@simonbuchan https://github.com/simonbuchan\nYou can get the global AWS object from aws-sdk/global. If you do that,\nyou should be able to set config on the AWS object, and any services\ninstantiated after that will use that configuration.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/pull/1123#issuecomment-245930073, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABwVo5dr20Fmb9kJ3hKgtVqwAwOoJwAnks5qoW1qgaJpZM4J3qN6\n.\n. That is much better, although it still also adds it to the global which is\na little bad mannered\n\nOn Sat, 10 Sep 2016, 02:30 Christopher Radek notifications@github.com\nwrote:\n\n@simonbuchan https://github.com/simonbuchan\nYou can get the global AWS object from aws-sdk/global. If you do that,\nyou should be able to set config on the AWS object, and any services\ninstantiated after that will use that configuration.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/pull/1123#issuecomment-245930073, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABwVo5dr20Fmb9kJ3hKgtVqwAwOoJwAnks5qoW1qgaJpZM4J3qN6\n.\n. Caused by this: https://github.com/aws/aws-sdk-js/blob/4e1b1afabe5754b8f7c956e15772c3c0fb0a3a45/lib/services/s3.js#L629\n\nReally should be checking further into the error to see if it's due to missing bucket, and handling errors here, but from the code it looks like you can disable the check by using forcePathStyle: true in the config?. Caused by this: https://github.com/aws/aws-sdk-js/blob/4e1b1afabe5754b8f7c956e15772c3c0fb0a3a45/lib/services/s3.js#L629\nReally should be checking further into the error to see if it's due to missing bucket, and handling errors here, but from the code it looks like you can disable the check by using forcePathStyle: true in the config?. @AllanFly120 that file has listed s3 as supporting cors since sept. 2016.. @AllanFly120 that file has listed s3 as supporting cors since sept. 2016.. Yes, I was just replying to allans comment that you can see if CORS has support in that file, which clearly you can't in this case, hence this issue \ud83d\ude09. Yes, I was just replying to allans comment that you can see if CORS has support in that file, which clearly you can't in this case, hence this issue \ud83d\ude09. Should be this.process = ...?\n. ",
    "ajmurmann": "@jagi I also am trying to use this with create-react-app and would prefer not to eject. Where you able to solve this?\n. ",
    "vladshcherbin": "It would be great to have a possibility to import only needed parts of the library in the project. \nCurrently, you can import the whole library. So, after adding s3 functionality to your 300kb project file, it becomes 1.3mb.\n. @chrisradek sounds awesome, will give it a try this week. Thanks! \ud83d\udc4d \n. ",
    "daumann": "@chrisradek \nWhen following the solutions in this thread, the script fails (with webpack) with the error message:\nError: Missing credentials in config(\u2026) \"CredentialsError: Missing credentials in config\nBut I have both the shared credentials file in ~/aws/credentials as well as environment variables set up.\nOnly when I hardcode the credentials it works... (why ?)\n. ",
    "danbrianwhite": "Heads up to other devs \nIf you start getting webpack build errors from ./~/aws-sdk/lib/util.js and have something like\n{\n                test: /aws-sdk/,\n                loaders: ['transform?brfs']\n }\nin your webpack config, remove it and the build will work again.\n. ",
    "rameshsubramanian": "@chrisradek  I keep getting TypeError: Cannot read property 'crypto' of undefined  error. Anyone else facing the issue?\nThanks\nRamesh\n. ",
    "nickbreaton": "@rameshsubramanian I was still getting this when I was using babel-loader. If you are also using babel, and easy fix is to ensure you have exclude: /node_modules/ in your loader configuration.\n{ test: /\\.js$/,   loader: 'babel-loader', exclude: /node_modules/ }\n. ",
    "jmanuel1609": "https://aws.amazon.com/blogs/developer/using-webpack-and-the-aws-sdk-for-javascript-to-create-and-bundle-an-application-part-1/ . ",
    "kejsiStruga": "Hello, I am using react and I am trying to connect to DynamoDB via aws-sdk. How could this be accomplished ? I have tried the above methods, still cannot set up credentials for dynamo. This works on Javascript but not on React !\n import AWS from 'aws-sdk/dist/aws-sdk';\nAWS.config.loadFromPath('./config.json');\n. ",
    "shyamchandranmec": "@chrisradek  I tried this.. But its not working.. I am getting the following error.\nERROR in ./~/xmlbuilder/lib/index.js\nModule not found: Error: Can't resolve 'lodash/object/assign' in '/Users/shyam/Documents/indee/new-indee/node_modules/xmlbuilder/lib'\n @ ./~/xmlbuilder/lib/index.js 5:11-42\n @ ./~/aws-sdk/lib/xml/builder.js\n @ ./~/aws-sdk/lib/core.js\n @ ./~/aws-sdk/lib/browser.js\n @ ./plugins/aws.js\n. @chrisradek  I tried this.. But its not working.. I am getting the following error.\nERROR in ./~/xmlbuilder/lib/index.js\nModule not found: Error: Can't resolve 'lodash/object/assign' in '/Users/shyam/Documents/indee/new-indee/node_modules/xmlbuilder/lib'\n @ ./~/xmlbuilder/lib/index.js 5:11-42\n @ ./~/aws-sdk/lib/xml/builder.js\n @ ./~/aws-sdk/lib/core.js\n @ ./~/aws-sdk/lib/browser.js\n @ ./plugins/aws.js\n. @gerardmrk I have created a npm package with the changes. You can use this until aws developers resolve this.\nHere is the url. @gerardmrk I have created a npm package with the changes. You can use this until aws developers resolve this.\nHere is the url. @chrisradek I was getting the error when I was importing aws-sdk. This issue was already reported by someone else. Refer this link. Somebody has already raised a pull request but its not merged yet. I have changed the versions of some of the packages. Thats the only change.\nUpdated:\n\nxmlbuilder 2.6.2 -> 4.2.1\nsax 1.1.5 -> 1.2.1\nxml2js 0.4.15 -> 0.4.17\nuuid 3.0.0 -> 3.0.1. @chrisradek I was getting the error when I was importing aws-sdk. This issue was already reported by someone else. Refer this link. Somebody has already raised a pull request but its not merged yet. I have changed the versions of some of the packages. Thats the only change.\n\nUpdated:\n\nxmlbuilder 2.6.2 -> 4.2.1\nsax 1.1.5 -> 1.2.1\nxml2js 0.4.15 -> 0.4.17\nuuid 3.0.0 -> 3.0.1. @chrisradek Any update on when this will be merged?. @chrisradek Any update on when this will be merged?. @gurpreetatwal Any update on when this issue will be fixed?. @gurpreetatwal Any update on when this issue will be fixed?. @menkari I have created a npm package with the changes. You can use this until aws developers resolve this.\nHere is the url. @menkari I have created a npm package with the changes. You can use this until aws developers resolve this.\nHere is the url. \n",
    "gerardmrk": "I am also getting what @shyamchandranmec is getting, the exact same error. I am at my wits ends trying to figure out how to fix this. I spent two days scouring stack overflow and github issues threads and cannot find any working fixes. Please someone address this! If its any help, I'm using Webpack 2 with TypeScript and React.. ",
    "fpereiro": "I just tried to run the script the script with node v.0.10.35 and the\nmemory usage looks normal. However, the transfer speed is very low\n(~100KB/s) compared to the ~5-10MB/s of previous downloads.\nImmediately after, I reran the script with node v.0.12.1 and memory usage\nalso looks normal - however the download speed is still very slow. My guess\nis that because S3 is transferring the files slowly, memory usage doesn't\ngrow as much and the bug cannot be replicated.\nI unfortunately cannot tell you whether 0.10.x would work fine until S3's\nnormal speed is back. I will retry tomorrow.\nMeanwhile, if you have time to replicate the bug, you can use the script I\nposted and attempt to download a file that's at least 4GBs. You can get the\nnode binary for 0.12.1 here http://nodejs.org/dist/v0.12.1/.\nThank you!\n2015-05-14 21:06 GMT+02:00 Loren Segal notifications@github.com:\n\nHas this been happening all the time, or has it only began recently?\nSpecifically, have you tried this in Node 0.10.x before? This would be\nuseful information to diagnose the problem.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/605#issuecomment-102139705.\n. I confirm that this only happens with node v.0.12.x. I tried again with 0.10.35 and memory usage is stable. With v.0.12.x and adecuate bandwidth, RAM usage grows at an average of ~30MB/s.\n\nLet me know if you need further info.\nThanks!\n. Hi there,\nI tried with both 0.12.1 and 0.12.3 (the latest version) and the leak is\npresent in both versions (I am assuming that the same will happen for\n0.12.2).\nCan you point me to the code that delegates to stream.pipe ()?\n2015-05-16 1:16 GMT+02:00 Loren Segal notifications@github.com:\n\n@fpereiro https://github.com/fpereiro could you bisect 0.12.x to figure\nout which version of 0.12.x this is an issue with? It almost seems like\nthis might be a regression in 0.12.x itself. The SDK doesn't actually\nmanage downloads through createReadStream, we delegate to stream.pipe()\ndirectly, so if there is a memory leak that is specific to 0.12 it would be\ndue to some implementation detail in that version.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/605#issuecomment-102545678.\n. I found a possible bug at line 559 of request.js. In node 0.10.x,\nlegacyStreams is set to false, however in 0.12.x it is set to true.\n\nHowever, making the code ignore this condition on 0.12.x (and jumping\nstraight to the else in line 566) still doesn't resolve the leak for 0.12.x.\nUnfortunately I don't have time to keep on finding the error in the stream\ninterface. For now, we will use a node 0.10.x binary to avoid the issue.\nI implore you to look into the issue when you have time - from all I've\nseen, this will happen to anybody downloading a large file from S3 with\nnode 0.12.x. I doubt this is a regression in the node codebase, since\nstreams are widely used, especially for transferring and processing large\namounts of data.\n2015-05-16 2:48 GMT+02:00 Loren Segal notifications@github.com:\n\n@fpereiro https://github.com/fpereiro sorry I may have misspoke\nslightly-- we don't call pipe() ourselves, but we delegate all events\nthrough. You can see the createReadStream implementation here:\nhttps://github.com/aws/aws-sdk-js/blob/master/lib/request.js#L558-L582\nYou can however access the low-level stream object if you want to test\nwith it. Sample code is available in the description for #345\nhttps://github.com/aws/aws-sdk-js/pull/345. Using this lower-level\nstream interface (which createReadStream is built on) will allow you to\npipe directly.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/605#issuecomment-102554845.\n. I confirm that the memory leak is gone with this fix (I used aws-sdk v.2.1.30).\n\nThank you very much for looking into this!\n. ",
    "wearhere": "No trouble. If you don't want the project to bundle phantomjs it'd be nice to offer some other way to get the tests running, though\u2014might we add something to the README? What's your suggested method of installation & what versions do you test with?\n. No trouble. If you don't want the project to bundle phantomjs it'd be nice to offer some other way to get the tests running, though\u2014might we add something to the README? What's your suggested method of installation & what versions do you test with?\n. Speaking more broadly, I was surprised that the contributing guidelines talk about adding code\u2014but not about testing it. I had to do a little bit of digging through package.json, the test folder structure, etc. to figure out how to get things running and where I might add tests e.g. for #608.\n. Speaking more broadly, I was surprised that the contributing guidelines talk about adding code\u2014but not about testing it. I had to do a little bit of digging through package.json, the test folder structure, etc. to figure out how to get things running and where I might add tests e.g. for #608.\n. Ok @lsegal, I've fixed up the request to modify the contributing guidelines instead.\n. Ok @lsegal, I've fixed up the request to modify the contributing guidelines instead.\n. You're welcome, thanks for the speedy response!\n. You're welcome, thanks for the speedy response!\n. In the meantime, a workaround is to call clearCachedId before using an AWS service. But because clearCachedId is an instance method and it doesn't clear the instance's information you've got to re-initialize the instance after clearing the cache:\n```\nvar credentialParams = { IdentityPoolId: ..., Logins: ... };\nvar credentials = new AWS.CognitoIdentityCredentials(credentialParams);\ncredentials.clearCachedId();\ncredentials = new AWS.CognitoIdentityCredentials(credentialParams);\nAWS.config.credentials = credentials;\n// Now you can use S3 or whatever.\nvar bucket = new AWS.S3;\n``\n. In the meantime, a workaround is to call [clearCachedId](http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityCredentials.html#clearCachedId-property) before using an AWS service. But becauseclearCachedId` is an instance method and it doesn't clear the instance's information you've got to re-initialize the instance after clearing the cache:\n```\nvar credentialParams = { IdentityPoolId: ..., Logins: ... };\nvar credentials = new AWS.CognitoIdentityCredentials(credentialParams);\ncredentials.clearCachedId();\ncredentials = new AWS.CognitoIdentityCredentials(credentialParams);\nAWS.config.credentials = credentials;\n// Now you can use S3 or whatever.\nvar bucket = new AWS.S3;\n``\n. Thanks @chrisradek! It looks like this will make it into v2.2.11?\n. Thanks @chrisradek! It looks like this will make it into v2.2.11?\n. +1\n. +1\n. :raised_hands: \n. :raised_hands: \n. Works great, thanks!\n. Works great, thanks!\n. Ok thanks!\n. Ok thanks!\n. Oh! That would be great. All the examples I had seen usedgetSignedUrl`.\n@AdityaManohar is it possible to get a public link like @dmyers suggests using the JS SDK? I don't see getObjectUrl in the JS reference.\n. Oh! That would be great. All the examples I had seen used getSignedUrl.\n@AdityaManohar is it possible to get a public link like @dmyers suggests using the JS SDK? I don't see getObjectUrl in the JS reference.\n. Awesome thanks @dmyers!\n. Awesome thanks @dmyers!\n. ",
    "aaaguirrep": "I have used the suggestion of @wearhere and it worked.. ",
    "alexjfno1": "@aaaguirrep What suggestion did you use that worked? Was it the LoginHint?. ",
    "gregkowalski": "@alexjfno1 LoginId was the new field added to the AWS.CognitoIdentityCredentials definition. ",
    "TehNrd": "Some thoughts:\n1) GC, it never kicks in, or if it does it is missing things. That log selection is just a small sample. It will keep growing into the GB size.\n2) Even if sockets are infinite shouldn't they be closed after receiving the response. And shouldn't they also be re-used with socket pooling if not. (node core is not my strength so not sure here). I am trying the maxSockets change now. Will report back.\n. EDIT: This is incorrect. See response below.\nAdding max sockets made no difference. Memory usage still grows unchecked. I also tried this below, just to be more explicit, in addition to the code you provided but with no success.\nvar http = require('http');\nhttp.globalAgent.maxSockets = 30;\nvar https = require('https');\nhttps.globalAgent.maxSockets = 30;\n. This actually has nothing to do with receiveMessage(). In the code above if you replace receiveMessate() with getQueueAttributes() you still get the same memory growth issues. Seems to be an issue with performing \"lots\" of calls (10/second) and this causing memory growth.\nsqs.getQueueAttributes({QueueUrl: GEOCODE_AWS_SQS_URL, AttributeNames: ['MessageRetentionPeriod']}, function(err, data) {\n    if (err){\n        console.log(err, err.stack); // an error occurred\n    }else{\n        //Handle attributes\n    } \n});\n. Good news, this does not appear to be an issue with AWS SDK. Even doing basic requests to google in a loop are causing memory issues. Bad news, seems to be a larger issues with node core.\nEven doing this call in the loop causes memory issues.\nrequest('http://www.google.com', function (error, response, body) {\n  if (!error && response.statusCode == 200) {\n    //console.log(body) // Show the HTML for the Google homepage.\n  }else{\n    console.log('body');\n  }\n})\n. Yup, I am seeing the same thing @mhart . Thanks so much! For some reason I thought node would be better about throwing away/re-using sockets as 10/sec is really not that many.\nInterestingly enough simply setting maxSockets prevents excessive memory growth. Even setting max sockets to 500, RSS only goes up to ~180MB. Seems like node core is not reusing sockets if maxSockets is not set. (total guess, not a node core expert)\n. Oh, and keep up the great work! Library is so much better to work with than trying to interact with these APIs directly!\n. ",
    "pgilad": "Fixed & rebased, sorry about that :blush:\n. Fixed & rebased, sorry about that :blush:\n. ",
    "tgroshon": ":+1: \n. :+1: \n. This is similar what i've done ^^^. This is similar what i've done ^^^. ",
    "pablocaselas": "Yes, it seems to be working fine. Thanks!\n. ",
    "bshyong": "@AdityaManohar is there any update on this feature? I would like to set up a lambda function that will read cookies on the client and set one if it does not exist.\nThanks\n. ",
    "LasCondes": "@jeskew Johnathan any chance you can help with this implementation? https://forums.aws.amazon.com/thread.jspa?messageID=698539&#698539\n. ",
    "theplatapi": "Here is a solution I came up with. I hope it helps!\n``` js\nvar Crypto = Npm.require('crypto');\nvar Url = Npm.require('url');\nVerifySnsMessage = {\n  certificateCache: {},\n  authenticSubscriptionMessage: function (parsedRequestBody) {\n    var fields = ['Message', 'MessageId', 'SubscribeURL', 'Timestamp', 'Token', 'TopicArn', 'Type'];\n    var optionalFields = [];\n    return this.authenticMessage(parsedRequestBody, fields, optionalFields);\n  },\n  authenticNotificationMessage: function (parsedRequestBody) {\n    var fields = ['Message', 'MessageId', 'Subject', 'Timestamp', 'TopicArn', 'Type'];\n    var optionalFields = ['Subject'];\n    return this.authenticMessage(parsedRequestBody, fields, optionalFields);\n  },\n  authenticMessage: function (requestBody, fields, optionalFields) {\n    var canonicalString = this.getCanonicalString(requestBody, fields, optionalFields);\n    var certificate = this.getCertificate(requestBody.SigningCertURL);\n    var verifier = Crypto.createVerify('RSA-SHA1');\nverifier.update(canonicalString);\nreturn verifier.verify(certificate, requestBody.Signature, 'base64');\n\n},\n  getCanonicalString: function (requestBody, fields, optionalFields) {\n    var message = [];\n    var length = fields.length;\nfor (var i = 0; i < length; i++) {\n  //check if a required field is missing. It can be missing an optional field though.\n  if (!requestBody[fields[i]] && !_.contains(optionalFields, fields[i])) {\n    return '';\n  }\n  message.push(fields[i]);\n  message.push(requestBody[fields[i]]);\n}\n\nreturn message.join('\\n') + '\\n';\n\n},\n  getCertificate: function (url) {\n    if (!this.certificateCache[url]) {\n      this.certificateCache[url] = this.validUrl(url) ? HTTP.get(url).content : '';\n    }\nreturn this.certificateCache[url];\n\n},\n//Adapted from AWS Ruby SDK:\n  //https://github.com/aws/aws-sdk-ruby/blob/1d59bf1cc170f55439887ff6a48cbcd5582389c7/aws-sdk-resources/lib/aws-sdk-resources/services/sns/message_verifier.rb#L39\n  validUrl: function (url) {\n    var parsed = Url.parse(url);\n    var regex = /^sns.[a-zA-Z0-9-]{3,}.amazonaws.com(.cn)?$/;\n    return parsed.protocol === 'https:' && regex.test(parsed.hostname) && parsed.path.split('.')[1] === 'pem';\n  }\n};\n```\n. Ah, thanks for this!\n. ",
    "clemesha": "Hi, thanks for the response. I'm trying to use the AWS SDK as a library. See attached screenshot of using underscore.js and moment.js vs the AWS SDK.\n\n. I'm looking to just create a stand-alone Google Apps Script (GAS) that uses the built-in GAS \"Time-driven\" Events trigger. \nIn that stand-alone script I'm reading in emails from Gmail, then I hope to use the AWS SDK to trigger AWS Lambda functions, post data to S3, etc.\nLet me know if this is possible, or if there are work-arounds to deal with the lack on the window object.\nThanks again.\n. Ok, thank you for the very helpful updates! I just now tried what you suggested,\nand things did get further, but as you also mentioned, there's other dependancies it relies on,\nsee attached screenshot of current test. Are these unfixable issues? Thanks again.\n\n. Ok, understood. Thanks again @AdityaManohar and @lsegal !\n. ",
    "cbkihong": "Haven't tried the fix yet but will do later today, and thank you so much for the quick fix. One question though, I noticed that in your tests you utilized AWS.util.Buffer though it seems from util.js (at present) it is just made an alias to Node.js core Buffer. Can we be certain that we can directly reference Node.js Buffer instead of AWS.util.Buffer when we specify the params, since AWS.util.Buffer is apparently \"internal\" and not documented?\n. Tested and looks like this change worked for us. Will wait for your release, thank you again!\n. ",
    "nousacademy": "I'm using the NodeJS environment, version 2.1.14.  I want to list all objects within a folder which I've done, but then GET all my json files within it, to display them. So far I see that I can only GET one json file at a time.\n. @lsegal @AdityaManohar thanks, I'm testing it out now!\n. I'm getting this error from aws:\n/Users/nousacademy/Desktop/Projects/testapp/node_modules/aws-sdk/lib/request.js:32\n          throw err;\n                ^\nError: Can't set headers after they are sent.\n. ```\ns3.listObjects(params, function(err, data) {\n        res.send(data);\nasync.each(data.Contents, function(key, cb) {\n\n    var params2 = {\n        Bucket: 'johnsbookmarkapp',\n        Key: key.Key\n    }\n    s3.getObject(params2, function(err, data) {\n\n        res.send(data.Body.toString())\n    });\n\n\n    //console.log(key.Key)\n});\n\n});\n```\nBasically wanted to get the list, then get all the objects, without having to write to file\n. @AdityaManohar wow nice work around thanks!\n. @AdityaManohar It works now, but the order is destroyed. payload.list[0] is not equal to the contents associated with payload.objects[0] ... Because one of them isnt a json file ill try to delete it\n. @lsegal I'm just curious as to how ill delete and update that object? Is the only way to do that using the absolute path??\n. I got it nevermind\ns3.getObject({Bucket: 'bucket', Key: 'key'}).on('success', function(response) {\n  console.log(\"Key was\", response.request.params.Key);\n}).send();\n. I cannot get the response.request.data.Body with response.request.params.Key it keeps giving me an undefined when I try to get the Body. What am I doing wrong????\n. haha oops... \n. Thanks @lsegal and @AdityaManohar I have everything! Appreciate the help!\n. Ok I thought I was done:\n```\n for (var i = 0; i < jsonData.length; i++) {\n            $.get('/fldrContents', {\n                contents: jsonData[i].Key\n            }, function(data) {\n                var b = JSON.parse(data.body);\n\n\n            });\n}\n\n```\nI'm lost at how to put all the responses I get from my get call into one object. Theyre all coming individually. I tried to do this for the response with no luck: http://stackoverflow.com/questions/21819905/jquery-ajax-calls-in-for-loop \n. I got it I had to use a promise\n. @AdityaManohar having another issue. When I try to GET one of my folders that has no subfolders and only files within it. It gets all of my files BUT it also gets the folder path which breaks my app. Example response:\nObject {path: \"bookmarks/Bookmarks Toolbar/\", body: \"\"}\nAs you see its getting the folder without any files. Because obviously a folder by itself is nothing! How do I get around this??\n. @lsegal got it\n. ",
    "SwethaMK": "Hi @lsegal , What if I have to use invokeAsync , since I need to make Asynchronous calls ?\nI have been trying to get back results from my lambda function ( written in Node.js ) back to my browser, but to no avail. While testing it on Lambda console, I can see the correct results & proper console output, but when I try getting it from the browser, it always says 'Unexpected token u in JSON at position 0'. . ",
    "alanmimms": "Wow! Thanks. I was looking at the Java SDK when I wrote the above and of course the \".\" is missing from the Java symbol but not the string it's bound to. Thank you, @lsegal , for helping me find it, but I still cannot.\nIf you look at the link you gave me, you will not find any definition of the instances property that is COMPUTER READABLE - only strings in documentation.\nI did \"npm install aws-sdk\" just now and ran \"fgrep -ri c4.xlarge .\" the node_modules/aws-sdk directory. There were no matches. Am I just an idiot, or is there something to this issue? Thanks in advance.\n. No, that's precisely the opposite of my point, really. I WANT to be able to do this programmatically so I can present the user with a list of choices in a GUI for orchestrating his application, and along the way I also want to give him the set of tradeoffs for his application based on the strengths and weaknesses of those instance types. I need a way - and would VERY much prefer a web service rather than an SDK API - to enumerate the instance types and get their resource strengths and weaknesses - all programmatically without a developer having to rebuild anything when you guys add new types. It's wonderful that you change so frequently to keep up with the hardware coming available and innovations in your cloud service capabilities. But it's a huge pain for people to release new versions with silly little strings changing all the time just to keep up. Let me write a piece of software that presents the user (whom only I know well enough to do this for - you guys cannot possibly know him that well) the tradeoffs to decide what instance type to deploy on. Then we can all develop things that add value and stop fiddling around with little strings that are hardcoded in various places.\nDoes that make any sense?\n. Thanks! I can now fish on my own. But I do think my proposal of a web service to do all of this and also to allow queries for capabilities of each instance type is the way to go as the instance types change frequently and there are MANY more each time you add some. Your current scheme doesn't scale well as more are added. It's getting out of hand, no?\nThanks very much for the help.\n. ",
    "dirkraft": "Added some environment details to the description\n. Thanks for the quick response. This is the CORS of s3://bits.dirkraft.com as of now (just added the ExposeHeader element)\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n    <CORSRule>\n        <AllowedOrigin>*</AllowedOrigin>\n        <AllowedMethod>GET</AllowedMethod>\n        <AllowedMethod>HEAD</AllowedMethod>\n        <MaxAgeSeconds>300</MaxAgeSeconds>\n        <ExposeHeader>Content-Length</ExposeHeader>\n        <AllowedHeader>*</AllowedHeader>\n    </CORSRule>\n</CORSConfiguration>\ncurl seems(?) to see a header named Content-Length\n```\n$ curl -XHEAD -v https://s3.amazonaws.com/bits.dirkraft.com/favicon.ico\n Hostname was NOT found in DNS cache\n   Trying 54.231.244.4...\n Connected to s3.amazonaws.com (54.231.244.4) port 443 (#0)\n TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\n Server certificate: s3.amazonaws.com\n Server certificate: VeriSign Class 3 Secure Server CA - G3\n* Server certificate: VeriSign Class 3 Public Primary Certification Authority - G5\n\nHEAD /bits.dirkraft.com/favicon.ico HTTP/1.1\nUser-Agent: curl/7.37.1\nHost: s3.amazonaws.com\nAccept: /\n< HTTP/1.1 200 OK\n< x-amz-id-2: WZUhqzQOsfOI0HZ2QUvcg7S4kxBthaAZH6CAdvOL3HIzEsz6VXVvq9b8pHlXIc9g\n< x-amz-request-id: 253D39519E8216DC\n< Date: Fri, 19 Jun 2015 00:09:14 GMT\n< Last-Modified: Sun, 23 Nov 2014 20:43:42 GMT\n< ETag: \"811119f4bb4ac6e00593c2b6eb92acc6\"\n< Accept-Ranges: bytes\n< Content-Type: image/x-icon\n< Content-Length: 318\n Server AmazonS3 is not blacklisted\n< Server: AmazonS3\n<\n transfer closed with 318 bytes remaining to read\n* Closing connection 0\ncurl: (18) transfer closed with 318 bytes remaining to read\n```\n\nso does Chrome dev tools\n\nAny more pointers?\n. Ah it does now for me too. Fantastic. Thanks very much and keep up the great stuff\n. ",
    "AScripnic": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error>\n    <Code>InvalidArgument</Code>\n    <Message>Unable to validate the following destination configurations</Message>\n    <ArgumentName1>arn:aws:lambda:us-east-1:AccountID:function:LambdaName, null</ArgumentName1>\n    <ArgumentValue1>Not authorized to invoke function [arn:aws:lambda:us-east-1:AccountID:function:LambdaName]</ArgumentValue1>\n    <RequestId>52D12E87577573C9</RequestId>\n    <HostId>aOVXqainz/IPQufy33AdA/5S5RDAkR/bKcCQPplIrKJSY3LkFkfr6L2QD7VfSx5jvuMDk00FRO0=</HostId>\n</Error>\nYou are right, but how should i give s3 permissions?\n. ",
    "ozbillwang": "https://aws.amazon.com/premiumsupport/knowledge-center/unable-validate-destination-s3/. ",
    "ddimitrioglo": "How about this ambiguous article?\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Service.html\n. I have already implemented with AWS.Lambda + AWS.SES, therefore we can close this issue.\nThank you for your help!\n. ",
    "mdouglass": "Thanks @AdityaManohar, just tried that out and it fixes what I was looking for.\n. Thanks @AdityaManohar for checking on that.\n. ",
    "nicka": "Still don't get why this isn't possible with signed urls.\n. Still don't get why this isn't possible with signed urls.\n. ",
    "aleclarson": "How can I limit contentLength of a pre-signed url for putObject, if content-length-range is not available?. I'll try creating only the Policy and Signature query params on the server-side (instead of creating a pre-signed url). Then I'm using FormData with XMLHttpRequest to perform the upload on the client-side. I'm filling the FormData with the name/value pairs from this example.\nNote: The client is not a browser, in my case.. ",
    "chadbrewbaker": "Here is how Boto does it, https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html#generating-presigned-posts. Here is how Boto does it, https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html#generating-presigned-posts. ",
    "ururk": "I wondered if this came from somewhere else. I'm fine closing the pull request - or you can.\n. ",
    "bipvanwinkle": "It seems I have wasted your time. I apologize for this. I was confused by the stack trace, but you are correct. The error was in my callback. Thank you for responding so quickly and pointing me in the right direction. Also, want to say the documentation for the module is superb. Well done.\n. It seems I have wasted your time. I apologize for this. I was confused by the stack trace, but you are correct. The error was in my callback. Thank you for responding so quickly and pointing me in the right direction. Also, want to say the documentation for the module is superb. Well done.\n. ",
    "jrencz": "Just for the record: I bumped into the same issue (yet via Serverless) and the answer by @mhart was helpful enough to make me reboot the machine. It helped. ",
    "miguelramos": "Stupid thing. Application and description won't go as parameters. Just got a successful environment update. Sorry for this silly issue open.\n. ",
    "yoannmoinet": "I understand.\nIt was just a move in the direction of npm's way of seeing things.\nI'm okay with keeping my fork up to date myself.\n. @lsegal thank you for the tip.\nI don't think I can add this into our workflow, but I'll have another way of getting this at least.\n. thanks, appreciated.\n. ",
    "jacob-israel-turner": "I want to +1 this.  I'm creating a ReactNative app, which out of the box uses NPM as the package manager.  It's not possible for us to switch our project to Bower.\n. :clap: \n. ",
    "askmon": "Oh sorry! The format was gone when I pasted the code. Anyway, I've made a Javascript to send and it works, so it's not an SDK issue. Thanks for the help and sorry for the work!\n. Oh sorry! The format was gone when I pasted the code. Anyway, I've made a Javascript to send and it works, so it's not an SDK issue. Thanks for the help and sorry for the work!\n. ",
    "bruun": "Would it be possible to get this PR looked at? Right now we are depending on a fork of this project in order for our code to work correctly. Thanks!\n. Would it be possible to get this PR looked at? Right now we are depending on a fork of this project in order for our code to work correctly. Thanks!\n. ",
    "ianbytchek": "@lsegal thanks! That makes perfect sense, I'll dig in that direction.\n. ",
    "joshrtay": "+1\n. ",
    "SDiniz": "@AdityaManohar actually, no, I didn't had that configured, the put and post were already configured, but the forward headers and query strings no. I will test this new configurations ( just need to wait a little for the changes to update on the distribution, and I will get back to you.\nThanks\n. EDIT:\n@AdityaManohar, you were right, everything is working now, I guess it was missing the forward query strings, I didn't even noticed that there were parameters begin passed by query string.\nThanks once again, this can be closed!\n. ",
    "freeslugs": "well, nevermind, actually had to do with npm shrinkwrap etc. \n. ",
    "thattommyhall": "I was commenting and uncommenting those lines, sorry I committed the one with it commented out.\nI have updated the project, with the example directly PUTing using the request module immediately after generating the signed url using your code.\nhttps://github.com/thattommyhall/aws-sdk-bug/blob/master/example.js\nBoth Content-MD5 are commented out now, but I have tried all 4 options of using them, the only one that works is not setting it in both the sign and PUT\n. ",
    "StuAtGit": "It does help :)\nI was mainly testing my config against aws to cross-check what I was doing against an internal S3-lookalike cluster that did not support v4. It looks like it does support s3, so I'm good. Closing.\nAs far as clarify... I may have just moved the confusion in my head to a new layer.\n. ",
    "gvelo": "Sorry for the long silence. You are right , this is the same that #397 , adding the event handler on the stream works for me.\nThanks,\n. ",
    "jwulf": "It's documented here: https://github.com/aws/aws-sdk-js/blob/c0ab39a7abd2672d05554f495aef8d39c9e09af4/apis/swf-2012-01-25.normal.json#L1737\n. This works in aws-sdk@2.1.45 \n. ",
    "phillip-haydon": "It seems the 2 minute limitation is hard coded into the SDK, overriding the value int he SDK itself to 10 minutes the chunks successfully upload without fail.\nHowever the timeout cannot be modified from outside the SDK.\njs\nvar managedUpload = new AWS.S3.ManagedUpload({\n    queueSize: options.aWSQueueSize || 4,\n    params: params,\n    httpOptions: {\n        timeout: 60 * 1000 * 10 // 10 minutes\n    }\n});\nThe httpOptions specified in http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#constructor-property\n. It seems the 2 minute limitation is hard coded into the SDK, overriding the value int he SDK itself to 10 minutes the chunks successfully upload without fail.\nHowever the timeout cannot be modified from outside the SDK.\njs\nvar managedUpload = new AWS.S3.ManagedUpload({\n    queueSize: options.aWSQueueSize || 4,\n    params: params,\n    httpOptions: {\n        timeout: 60 * 1000 * 10 // 10 minutes\n    }\n});\nThe httpOptions specified in http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#constructor-property\n. Ok I have to classify this as a bug until said otherwise. The documentation suggests the value can be changed. But the SDK does not honor the value passed in. \nThe 2 minute limit is too low for us and we need to be able to increase it to 3-5 minutes. \n. Ok I have to classify this as a bug until said otherwise. The documentation suggests the value can be changed. But the SDK does not honor the value passed in. \nThe 2 minute limit is too low for us and we need to be able to increase it to 3-5 minutes. \n. OH it looks like you can configure it globally with:\njs\nAWS.config.httpOptions = {\n    xhrWithCredentials: false,\n    xhrAsync: true,\n    timeout: 60 * 1000 * 10\n};\nIs this sort of config acceptable? It seems a bit backward from the documentation. \nI would have assumped it should be done on line 3449 which configures the ManagedUpload request:\n``` js\n  configure: function configure(options) {\n    options = options || {};\n    this.partSize = this.minPartSize;\nif (options.queueSize) this.queueSize = options.queueSize;\nif (options.partSize) this.partSize = options.partSize;\nif (options.leavePartsOnError) this.leavePartsOnError = true;\n\nif (this.partSize < this.minPartSize) {\n  throw new Error('partSize must be greater than ' +\n                  this.minPartSize);\n}\n\nthis.service = options.service;\nthis.bindServiceObject(options.params);\nthis.validateBody();\nthis.adjustTotalBytes();\n\n},\n```\n. OH it looks like you can configure it globally with:\njs\nAWS.config.httpOptions = {\n    xhrWithCredentials: false,\n    xhrAsync: true,\n    timeout: 60 * 1000 * 10\n};\nIs this sort of config acceptable? It seems a bit backward from the documentation. \nI would have assumped it should be done on line 3449 which configures the ManagedUpload request:\n``` js\n  configure: function configure(options) {\n    options = options || {};\n    this.partSize = this.minPartSize;\nif (options.queueSize) this.queueSize = options.queueSize;\nif (options.partSize) this.partSize = options.partSize;\nif (options.leavePartsOnError) this.leavePartsOnError = true;\n\nif (this.partSize < this.minPartSize) {\n  throw new Error('partSize must be greater than ' +\n                  this.minPartSize);\n}\n\nthis.service = options.service;\nthis.bindServiceObject(options.params);\nthis.validateBody();\nthis.adjustTotalBytes();\n\n},\n```\n. Awesomesauce.\nI tried the s3.upload but couldn't get the progress update working so I went with the service way. Working fine.\nThanks for the clarification. I find the configuration documentation a little lacking :( hopefully it can be improved a bit.\n. Awesomesauce.\nI tried the s3.upload but couldn't get the progress update working so I went with the service way. Working fine.\nThanks for the clarification. I find the configuration documentation a little lacking :( hopefully it can be improved a bit.\n. ",
    "BjoernRuberg": "@AdityaManohar  sure.\nThis is the extracted code:\n```\nvar Q       = require(\"q\");\nvar AWS     = require('aws-sdk');\nvar fs      = require('fs');\nvar awsS3Client = new AWS.S3();\nvar upload = function(params) {\n    var defer   = Q.defer();\n    awsS3Client.putObject(params, function(err) {\n            if (err) {\n                    defer.reject(err);\n            }\n            else {\n                    defer.resolve();\n            }\n    });\n    return defer.promise;\n\n};\nvar s3params    = {\n    Body    : fs.createReadStream(\"localfile\"),\n    Bucket  : \"bucket\",\n    Key     : remotefile\n};\nvar attempts = 0;\nvar doUpload = function() {\n    return upload(_.assign(s3params, options || {}))\n        .then(function() {\n            return Q.resolve();\n        })\n        .fail(function(err) {\n            if (attempts < 10) {\n                    console.log(\"Retrying upload of file \" + remotefile + \". Turn \" + attempts);\n                    attempts++;\n                    return doUpload();\n            }\n            else {\n                    return Q.reject(err);\n            }\n        });\n};\nreturn doUpload();\n```\nI now got the reason why reuploads did not work. The stream I used was finished after first upload and would never have transferred any new data for the next attempts.\nBut I still see a problem with putObject that it can fail at the first attempt. I now use ManagedUpload which seems to not have the problem.\n. ",
    "dmyers": "Isn't the better solution to make the object public and link to it like normal? The PHP SDK has a getObjectUrl which essentially just build the URL for you without the signature requirement.\n. Isn't the better solution to make the object public and link to it like normal? The PHP SDK has a getObjectUrl which essentially just build the URL for you without the signature requirement.\n. I managed to hack it by looking at the private APIs in the JS internals like this:\njs\nvar client = new AWS.S3();\nvar req = new AWS.Request(client, 'getObject', {Key: key, Bucket: bucket});\nclient.populateURI(req);\nvar fullUrl = req.httpRequest.endpoint.protocol + \"//\" + req.httpRequest.endpoint.host + '/' + path;\nThe PHP SDK mainly looks at the final URL built from the request package Guzzle, I couldn't figure out how to do it in Node myself and that was the best solution other than hardcoding any hostnames as I wanted to have the same logic as the PHP version I used in the past.\nThe reason I prefer it over the signed url is that it doesn't perform a real HTTP request, it simply gets the URL for say linking in an image tag in HTML or storing in your database.\n. I managed to hack it by looking at the private APIs in the JS internals like this:\njs\nvar client = new AWS.S3();\nvar req = new AWS.Request(client, 'getObject', {Key: key, Bucket: bucket});\nclient.populateURI(req);\nvar fullUrl = req.httpRequest.endpoint.protocol + \"//\" + req.httpRequest.endpoint.host + '/' + path;\nThe PHP SDK mainly looks at the final URL built from the request package Guzzle, I couldn't figure out how to do it in Node myself and that was the best solution other than hardcoding any hostnames as I wanted to have the same logic as the PHP version I used in the past.\nThe reason I prefer it over the signed url is that it doesn't perform a real HTTP request, it simply gets the URL for say linking in an image tag in HTML or storing in your database.\n. It turns out I was missing some sort of source bucket prefix on the CopySource param. I think that could be less confusing in the API docs by making it an separate, optional param altogether.\n. ",
    "sandyleo26": "@AdityaManohar but according to this, the default signature is being V4 now. Does it mean longer-than-7-day expiration time is not possible? I use default signature version and pass 7,776,000 (90 days) to Expire but I get The provided token has expired error just after 4 days.\n. @AdityaManohar but according to this, the default signature is being V4 now. Does it mean longer-than-7-day expiration time is not possible? I use default signature version and pass 7,776,000 (90 days) to Expire but I get The provided token has expired error just after 4 days.\n. man that works like a charm! Thanks so much!\n. man that works like a charm! Thanks so much!\n. @chrisradek thanks for the quick reply. my bucket is in Sydney which is ap-southeast-2, which I think use v4.\n. @chrisradek thanks for the quick reply. my bucket is in Sydney which is ap-southeast-2, which I think use v4.\n. I used the default sdk on lambda which I think is 2.4.9. I'm not sure it lasts a full day but it's surely more than 15min and less than 1.5 days. So are you saying using v2 could workaround this? I only tried s3 and none (which is v4 I think).\n. I used the default sdk on lambda which I think is 2.4.9. I'm not sure it lasts a full day but it's surely more than 15min and less than 1.5 days. So are you saying using v2 could workaround this? I only tried s3 and none (which is v4 I think).\n. It is &Expires=1479166229. Yes, I'm using nodejs 4.3 in lambda. I used it a month ago but didn't find it until recently. Let me know if you need more info. Thanks.\n. It is &Expires=1479166229. Yes, I'm using nodejs 4.3 in lambda. I used it a month ago but didn't find it until recently. Let me know if you need more info. Thanks.\n. ",
    "miensol": "Are there any plans to add this feature to aws-sdk-js? Something similar to what is available already in e.g. aws-sdk-ruby would suffice. Currently we need to resort to workarounds like handcrafting the urls or using 3rd party packages.. ",
    "codercodingthecode": "Yep, following this. I need the same feature.\nThis thread has been rolling since 2015. So far we got hacks to make it work. I doubt it will get any implementation towards it. Which makes the function kinda useless in a sense if you can't really use it past short time expiration time.. ",
    "RichardSilveira": "Why only one week? This value should depends on busines rules of each application.... Why only one week? This value should depends on busines rules of each application.... Why only one week? This value should depends on busines rules of each application.... Why only one week? This value should depends on busines rules of each application.... ",
    "watson": "I just did a little digging in the request module because I was puzzled that this doesn't work out of the box and that my original findings indicated that it should.\n1. At request.js#L946 the response event is emitted with the response object\n2. The emitted response object it self was just passed into the parent function Request.prototype.onRequestResponse at request.js#L818\n3. Which in turn was called because self.req emitted its own response event at request.js#L782\n4. self.req is initialised at request.js#L741 to the return value of the self.httpModule.request function\n5. self.httpModule is a reference to the http module that matches the current protocol (either http or https) and is set at request.js#L463\nSo to sum up:\nThe self.httpModule.request function actually just equals the core Node.js http.request or https.request function dependent on the protocol used.\nIn case of the http protocol (this issue affects both http and https), the emitted response event by self.req is just the response event emitted by http.ClientRequest so the object that is assigned to the Body property in the original example above should just be an instance of http.IncomingMessage, which is what I also verified using instanceof when I originally opened this issue.\nConclusion? I'm still just as puzzled as to why the AWS-SDK shouldn't just treat this as any other http.IncomingMessage instance. One answer to this question could be that the request module changes part of the internal state of the http.IncomingMessage object that the AWS-SDK depends on. If that's the case, I think it would happen on one of thise lines: https://github.com/request/request/blob/d18cb487cf4e02740778d4bb464b5a1c7441d800/request.js#L819-L946\n. ",
    "jongyoonlee": "We are also running into this problem on node v5.0.0 and aws-sdk v2.2.21.\n. We are also running into this problem on node v5.0.0 and aws-sdk v2.2.21.\n. ",
    "ahageali": "+1\n. ",
    "edwardysun": "+1\n. ",
    "bryanjhatton": "+1\n. ",
    "kevindpsilitonga": "+1\n. ",
    "maxbrodie": "+1\n. +1\n. ",
    "robertjustjones": "+1\n. +1\n. ",
    "Durss": "Oh nice !\nIndeed i use \"node-persist\" module that adds properties inEnum and getProp to the Object's prototype.\nNow it's up to me to decide how i deal with it !\nThanks for your helpful answer !\n. ",
    "AmareshB": "In my case, I have used putItem of docClient instance to insert an item in the dynamoDb. All I had to do was to create an instance of DynamoDoc and call putItem method of that instance.\nconst doc = require('dynamodb-doc');\n const dynamo = new doc.DynamoDB();\nAnd call the putItem method on dynamo object.. So, the only way to pull all the lambdas in a region is by passing in the top level region and leave masterRegion empty. Is that right?. ",
    "supportcoinsafer": "Same here.\nWhen I call internal direct in my code there is no problem.\nBut if I call by  http://localhost:3001/myfunction post request, there is the same error.\n. Same issue here\n. ",
    "imranansari": "didn't make a difference, still brings down the micro instance.\nbtw, I'm running this as a cron job on an EBS worker, so my steps are :\n- Get records from Salesforce (50k)\n- Insert into SQS Queue\nI thought scaling issues would be the last of my concerns :)\n. the funny thing is, I get network timeout errors on my mac, but it still succeeds eventually, but not so on EBS\n. ",
    "DenisGorbachev": "@AdityaManohar I'd love to, but the current code base is closed.\nHowever, I think it should be easy to reproduce with any default SWF sample: just start the worker, kill it, start the worker again, start a workflow execution. You should see that SWF schedules the task to the previous worker. Just have the worker report the identity as PID, and you'll see the old PID in SWF web interface.\nMost probably, you're right that it's an issue with service behavior. Are there any default samples? (in any language / SDK?)\n. @AdityaManohar Thanks for suggestion, I've posted https://forums.aws.amazon.com/thread.jspa?threadID=215368\n. @dprentis that's right. In my case, users can't wait for 60 seconds to start a new workflow execution :)\n. @dprentis Please consider pinging SWF developers on https://forums.aws.amazon.com/thread.jspa?threadID=215368\n. @pbhadauria Now that the forum post has received zero attention from SWF team, I think the best way to resolve this issue is to ping them directly. \n@AdityaManohar you are our only hope! :)\n. @AdityaManohar how did the SWF team decide to deal with the issue?\n. Pawan, thanks for suggestion! I'll see if it applies in our use case.\nOn Wed, Sep 23, 2015 at 6:44 PM Pawan Bhadauria notifications@github.com\nwrote:\n\nSo for timing being, we solved this with a combination to sequencing and\nsome sleep time. While starting workers, we first start activity workers &\nsleep for few seconds. Once all are up, we start workflow workers. After\nactivity & workflow workers are started, we start the proxy service which\nusers call to submit there workflows.\nDuring downtime, we first stop proxy service, then workflow workers & then\nactivity workers. This makes sure that if workflow worker is running,\nactivity workers are always present.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/689#issuecomment-142643861.\n\nDenis Gorbachev\nCEO of BackupHamster https://backuphamster.com/\n. Meanwhile, it would be great if we didn't have to resort to such setups :)\nIsn't that the value proposition of Simple Workflow Service?\n\nOn Wed, Sep 23, 2015 at 6:49 PM Denis Gorbachev denis.d.gorbachev@gmail.com\nwrote:\n\nPawan, thanks for suggestion! I'll see if it applies in our use case.\nOn Wed, Sep 23, 2015 at 6:44 PM Pawan Bhadauria notifications@github.com\nwrote:\n\nSo for timing being, we solved this with a combination to sequencing and\nsome sleep time. While starting workers, we first start activity workers &\nsleep for few seconds. Once all are up, we start workflow workers. After\nactivity & workflow workers are started, we start the proxy service which\nusers call to submit there workflows.\nDuring downtime, we first stop proxy service, then workflow workers &\nthen activity workers. This makes sure that if workflow worker is running,\nactivity workers are always present.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/689#issuecomment-142643861.\n\nDenis Gorbachev\nCEO of BackupHamster https://backuphamster.com/\n\n\nDenis Gorbachev\nCEO of BackupHamster https://backuphamster.com/\n. @AdityaManohar Is there any way to get this fixed by SWF team?\n. @chrisradek Thank you for the answer! I'm changing my code approx. each 15 minutes while developing. Does that mean I have to change the version each 15 minutes, with every code change?\n. \n",
    "dprentis": "I am running into the same issue - if I:\n1) stop a running decision poller (ctrl+C the node process)\n2) terminate all running workflows\n3) start a new workflow\n4) start a new decision poller\na 'ghost' poller (probably the connection left over from 1) picks up the WorkflowExecutionStarted event, effectively preventing the new workflow from being started by the new poller in 4)\nI expect it's a service issue - I guess we just need to wait 'long enough' (60 seconds?) before starting a new workflow\n. ",
    "pbhadauria": "@AdityaManohar This is critical issue. There are many people of are facing this including me. Can you suggest a workaround?\n. So for timing being, we solved this with a combination to sequencing and some sleep time. While starting workers, we first start activity workers & sleep for few seconds. Once all are up, we start workflow workers. After activity & workflow workers are started, we start the proxy service which users call to submit there workflows.\nDuring downtime, we first stop proxy service, then workflow workers & then activity workers. This makes sure that if workflow worker is running, activity workers are always present.\n. ",
    "mustafashabib": "We're with @DenisGorbachev -- we just encountered this issue during development -- is upping the version # and task list name on each dev iteration really the only solution?\n. ",
    "jonhester": "Yep. Not sure how I missed it. Thanks!\n. ",
    "MeanwhileMedia": "Thanks for the response @lsegal. I'll try to post something in the forums. In the meantime.... I guess I'll have to write an extra build script (then run only that from docker CMD) instead of compressing everything into a multiline cli command. Too bad.\nI'm also going to test if I can make exec form work by inserting '/bin/sh -c' in between each seperate 'line' of my command array. Not ideal though...\n. Just tested something like this:\n[\"sh\", \"-c\", \"git\", \"clone\", \"whatever\", \"sh\", \"-c\", \"cd whatever\", \"sh\", \"-c\", \"npm install\"]\nThis does not work. Everything after the first executable (git) just gets parsed as arguments to that first executable. Oh well.\n. Just tried your suggestion, as well as several other variations... turns out you can get it to work simply by doing:\n[\"sh\", \"-c\", \"git clone whatever; cd whatever; npm install\"]\nSo if you want to use shell form, you just push your string into array [\"sh\", \"-c\"]. Just make sure the entire multi-line command is a single string.\n. Thanks for your help @lsegal \n. ",
    "weevilgenius": "We've been seeing the same issue when our EC2 instances are under heavy load and the application must make many requests to S3 within a short time.\nWe're also using IAM roles applied to EC2 instances, and there are no other applications, cron jobs, or scripts other than a single node.js instance which is using the latest AWS SDK (2.1.49).\nSample error message:\nTimeoutError: Missing credentials in config\n    at ClientRequest.<anonymous> (/opt/cloudio-server/node_modules/aws-sdk/lib/http/node.js:56:34)\n    at ClientRequest.g (events.js:260:16)\n    at emitNone (events.js:67:13)\n    at ClientRequest.emit (events.js:166:7)\n    at Socket.emitTimeout (_http_client.js:534:10)\n    at Socket.g (events.js:260:16)\n    at emitNone (events.js:67:13)\n    at Socket.emit (events.js:166:7)\n    at Socket._onTimeout (net.js:318:8)\n    at Timer.unrefTimeout (timers.js:510:13)\n. I tracked down a detailed error message for my case:\njson\n{\n  \"message\": \"Missing credentials in config\",\n  \"code\": \"CredentialsError\",\n  \"time\": \"Thu Sep 03 2015 17:17:33 GMT+0000 (UTC)\",\n  \"originalError\": {\n    \"message\": \"Could not load credentials from any providers\",\n    \"code\": \"CredentialsError\",\n    \"time\": \"Thu Sep 03 2015 17:17:33 GMT+0000 (UTC)\",\n    \"originalError\": {\n      \"message\": \"Connection timed out after 1000ms\",\n      \"code\": \"TimeoutError\",\n      \"time\": \"Thu Sep 03 2015 17:17:33 GMT+0000 (UTC)\"\n    }\n  }\n}\nI see we're getting a connection timeout when trying to load credentials instead of a connection refused. That might be a different issue, even though the top level error is the same.\n. ",
    "davidporter-id-au": "@AdityaManohar So your point about the endpoint being throttled was my first thought. Regarding the script starting it regularly, no, it's a (koajs) webserver, so it starts and runs indefinitely. \nI put a console.log where I thought I could see the request being processed in the sdk to see if it was being called multiple times and observed that the metadata was being processed on startup, and not thereafter.\nI verified this also by intentionally creating a worst-case scenario: require()ing within a loop, which showed the credentials being fetched each time. This is not what we're seeing in our production app. So I'm certainly not going to preclude us doing something stupid, but I don't think we're hammering the metadata endpoint. \nWe have since also discovered that a delayed retry appears to resolve the issue. However, this is a kludgy workaround rather than something I'd like to rely on. \n. So, I've learned a couple of things: \nVersions matter:\nThis is a comparison of the versions: pretty picture and data. (Coincidentally, the number of requests between versions is different because node keeps crashing with the keepalive change, particularly on 0.12.9). \nFor all tests here I've added the change you've suggested. \n1. I was rather wrong when I said it affected 0.12.x to 5.x, it appears to be version specific. Node 0.12.9 exhibits the 60~ millisecond behaviour. We're running node 0.12.9 in production which is why I was originally investigating that specific version. \n2. The performance for node 5.6.0, 4.3.0 and 4.2.0 appears better. Anecdotally it averages around ~15 milliseconds - still not great - but significantly better. \n3. Node 5.6.0 has a very high variance in the latency. More than any other version I can see with my tiny sample. \n4. Node 4.x seems the best behaved of all versions. It still lags behind python's performance however, even with the keepalive changes. \nKeepalive doesn't seem to matter, breaks occasionally\nI added the changes as you suggested with keepalive. It appears to make no difference on the performance for any of the versions of node that I've checked. It does now seem to throw the following error intermittently now though: \n```\n$ node dynamo.js > results/node-4.2.0.log\nevents.js:141\n      throw er; // Unhandled 'error' event\n      ^\nError: read ECONNRESET\n    at exports._errnoException (util.js:874:11)\n    at TLSWrap.onread (net.js:544:26)\n```\n. All tests run within an ec2 under the same conditions as the first test other than I took NodeJS outside of Docker and ran it on the bare ec2. This appeared to bump performance by about ~5ms when I compared the same version of node (5.6.0) both within and outside a container. \n. @chrisradek So I ran the while loop again, both at a 'high' rate of once every 10 ms and a 'low' rate at once every second and did a flame-graph of what's going on*.\nGraphs for Node 5.7.0: high freq, low freq\nAll the results across node versions look similar. The thing that is immediately outstanding to my fairly ignorant perspective, is there is a hell of a lot of CPU time being spent on credentials. The callstack is huge and the % of CPU time spent there is far higher than what I'd expect. Just my 2c, but I'd be slightly concerned that something in the way credentials are being validated, rotated and expired is hurting performance badly.\nIt's worth noting I'm running this on a Ubuntu Ec2 (standard AMI) with an instance role with permission to access dyanmo, so obviously it's reaching out to the metadata endpoint at some point in the lifecycle to get creds.\n*I took care to wait 5 seconds into the loop to account for the CPU time spent on require() calls, metadata endpoint credential fetches and any other one-time operations at startup which would skew the CPU utilization. This is the shell script to make the graph, note 5 second sleep. \n. @chrisradek \nI must admit to being fairly new to interpreting the data as well, but to quote the guy who wrote the graphing library: \n\n\"The width of the box shows the total time it was on-CPU or part of an ancestry that was on-CPU (based on sample count). Functions with wide boxes may consume more CPU per execution than those with narrow boxes, or, they may simply be called more often. The call count is not shown (or known via sampling).\"\n\nhttp://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html\nRegarding the node version issue, I did node versions 0.12.9, 4.3.0 and 5.7.0. To me they all look similar in basic structure and in that the credential calls dominate CPU time. The repo has all test cases. \n. Ah, sorry, I forgot to mention. the tests run for 30 seconds. \nIn thinking about your point about the frequency of the calls versus the CPU time consumed per call, you're right, I don't think I can make that determination. Nonetheless, would you expect that the quantity of events (in terms of CPU time spent net on those calls, irrespective of their being of great quantity or infrequently but with an expensive call) to be of that distribution? \n. ",
    "stemail23": "I'm seeing exactly this issue too. It is easily reproduced by simply having a script that gets a bunch of processes to create a heap of aws-sdk instances and then consume an api endpoint on them. (I realise that this is not a realistic situation, but it allows an intermittent issue to be reproduced reliably)\nThe example code that I use to reproduce this on a t2.micro instance is:\n```\nvar async = require('async');\nvar handlers = [];\nvar addHandler = function(value) {\n    handlers.push(function(callback) {\n        var queueName = 'your queue name here';\n        var region = 'ap-southeast-2';\n        var createQueueParams = { QueueName: queueName };\n        var aws = require('aws-sdk');\n        aws.config.region = region;\n        var sqs = new aws.SQS();\n        sqs.createQueue(createQueueParams, function(err, data) {\n            if (err) { return callback(err); }\n            var params = {\n                QueueUrl: data.QueueUrl,\n                MaxNumberOfMessages: 10,\n                WaitTimeSeconds: 2\n            };\n            sqs.receiveMessage(params, callback);\n        });\n    });\n};\nfor (var x = 0; x < 500; x += 1) {\n    addHandler(x);\n}\nasync.parallelLimit(handlers, 100, function(err, results) {\n    if (err) { return console.error(err); }\n    console.log(results.length);\n});\n```\nIf I invoke this code from 10 different node processes simultaneously, then I can pretty much guarantee that the error will be raised (returned in the err on sqs.createQueue)\nThere is a bigger problem associated with this situation however. I have found that after encountering the issue:\na) The EC2 instance becomes unreliable and typically is pretty much a write-off. Usually I cannot SSH into the machine, and the only recourse has been to terminate (even restart often fails).\nb) The biggest issue of all: Even though the EC2 instance is effectively dead and unreachable, The EC2 console still reports it as healthy, AND therefore any autoscaler that instantiated the instance is unaware of the failure, and does not therefore replace the instance. In my use case, I'm using an autoscaler group with desired = 1 to ensure failover on my instances. Due to this issue I CANNOT rely on instance monitoring on autoscalers.\nIt occurs to me that the resolution to this problem ought to be relatively trivial in the aws-sdk (surely just an incrementally backing-off retry on retrieving the credentials), but I'm concerned that the EC2 instance issues I'm seeing associated with this issue are symptomatic of a deeper underlying bug in the credentials endpoint code on the instance itself.\n. Yep, I understand why I see the issue, I built the scenario explicitly to expose it!\nThe simple facts: it is possible, in fact innevitable, using only AWS products (EC2 & the SDK), to bring an EC2 instance to its knees. Above are outlined the exact steps to reproduce the situation. What's frustrating to me, as a customer, is the difficulty I'm having raising this as a bug report. I guess I assumed that there would be internal process to route it to the appropriate place, but instead I keep getting redirected myself.\n. @davidporter-id-au\nThanks for the suggestion. Unfortunately, in my case, I don't have an ELB in the equation on these instances (they're job handler machines lifting messages from SQS). I'm exploring other options where I have a monitor machine attempting to recognise the dead instances and terminating them, but it's frustrating to have to expend this effort!\n. > We've also been hit by this today intermittently on code that was working fine before...\nExactly, which is why I suspect that some change in EC2 is complicit in the situation, rather than being solely an AWS-SDK issue.\n. @davidporter-id-au Yes, I'm in Sydney\n. I've rather given up hope of seeing this issue addressed. I provided a 100% reproducible example a year ago, but there's no indication that it is being investigated.\n. @LiuJoyceC Thanks for the feedback. I was able to reliably reproduce the issue with the code I provided, but I admit, I haven't looked into it since, so it's possible that things have changed since then. I notice though that you don't mention running multiple processes however, so perhaps that indicates why you couldn't reproduce? To reproduce the issue I needed to run the provided script up to ten times concurrently.\nThanks for looking into the issue. Hopefully you'll have some success with backed off retries, and hopefully the suggestions above might help you test a fix if you can reproduce the problem.\nCheers!\n. Thanks @LiuJoyceC \n. ",
    "areichman": "We started seeing the issue this week as well. For us, it happened when we updated our Node install to version 4.2.2 instead of 0.10.17. Our process runs on a cron tab every 15 minutes and sends about 20K messages to SQS. With 0.10.17, we ran with no issues. Within 30 minutes of updating to 4.2.2 we started seeing the intermittent issues. In both cases, we had the same 2.2.18 version of the SDK.\nA similar issue was discussed here in the past: https://github.com/aws/aws-sdk-js/issues/445\n@willwhite and @mick, have you seen any similar issues since your update was added to the SDK?\n. ",
    "zbjornson": "This started happening for us recently. Sporadically when uploading to S3 from the nodejs SDK (v2.2.11 and 2.2.33) we would get the same error posted in https://github.com/aws/aws-sdk-js/issues/692#issuecomment-137522504. Increasing the timeout to 4000 ms didn't fix it; increasing it to 10000 ms did.\nWe're also not hammering the endpoint (in fact our test server was making a single request at a time) -- it seems like it's a laggy metadata provider endpoint given that the timeout alleviates it.\n. ",
    "bbarney": "We are seeing this too. Can't be a throttling issue, it is on a staging instance that is only hit a few times per hour. Additionally, it is happening at application startup, so the server never starts.\n. ",
    "Glavin001": "Also experiencing this issue. I tried increasing the timeout to 10 seconds to no avail. \n@bbarney have you found any workarounds? I am experiencing the same issue on startup, every single time.\n. ",
    "ApsOps": "Same issue while using SQS for us. We're using a single instance of SDK object.\n. ",
    "ckknight": "Ran into this issue locally - was due to some shenanigans with process.env.\nFix was to manually pass in accessKey and secretAccessKey to aws.config.update(...).. Ran into this issue locally - was due to some shenanigans with process.env.\nFix was to manually pass in accessKey and secretAccessKey to aws.config.update(...).. ",
    "JoeMcGuire": "I just hit this issue on AWS ECS (Elastic Container Service) which requires ECSCredentials instead of EC2MetadataCredentials.\nAWS.config.credentials = new AWS.ECSCredentials({\n  httpOptions: { timeout: 5000 },\n  maxRetries: 10,\n  retryDelayOptions: { base: 200 }\n}). ",
    "franklynsd": "I get notifications on Android, but not in IOS.\nI get notifications on IOS if I send it using AWS console, creating an endpoint using the device token.\n. Yes,\n``` javascript\nplatconfig={PlatformCredential: '[platform credential]'};\nplatconfig.PlatformPrincipal='-----BEGIN CERTIFICATE-----\\nMIIFejCCBGKgAwIBAgIIULSnaYstx..............PhtCx\\nKakQGdwpfj1T3oq+o/M=\\n-----END CERTIFICATE-----';\nAWS.config.update({accessKeyId: '[accessKeyId]', secretAccessKey: '[secretAccessKey]'});\nAWS.config.region = 'us-west-2';\nsns = new AWS.SNS({apiVersion: '2010-03-31', sslEnabled: true});\nvar params = {\n    Attributes: platconfig ,\n    Name: '[AppName]', \n    Platform: 'APNS_SANDBOX' \n};\nsns.createPlatformApplication(params, function(err, data) {\n    if (err){\n        alert(err);\n        alert(err.stack);\n    }\n    else\n    {\n        alert('Success:'+data.PlatformApplicationArn);\n    }\n }); \n```\n. I added \\n for every new line in the certificate as it says in the documentation.\n. In the real code all keys and private information are fine.\n. Hi, I had an error in the PlatformCredential parameter. Thanks. It's ok now\n. ",
    "davidjwalter": "Update: I think this issue is a problem of my own making. I set aws.config.convertResponseTypes=false for performance reasons. When I set it to true (which I think is the default), I no longer have a problem with double base64 encoding. \nSo I will close this issue with resolution \"Clueless user\".\n. What is the status of this issue? We are seeing very similar behavior where sometimes the async callback is called twice. We are using aws-sdk@2.139.0, async@2.1.5 and Node.js 6.10. The application is uploading a stream to S3 within an async.parallel. It does this successfully thousands of times per day, but once or twice a day async throws an error like below:\nError: Callback was already called.\n at /app/node_modules/async/dist/async.js:840:32\n at ManagedUpload.<anonymous> (/app/node_modules/async/dist/async.js:3691:13)\n at apply (/app/node_modules/async/dist/async.js:21:25)\n at ManagedUpload.callback (/app/node_modules/async/dist/async.js:56:12)\n at Response.finishSinglePart (/app/node_modules/aws-sdk/lib/s3/managed_upload.js:667:12)\n at Request.<anonymous> (/app/node_modules/aws-sdk/lib/request.js:364:18)\n at Request.callListeners (/app/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n at Request.emit (/app/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n at Request.emit (/app/node_modules/aws-sdk/lib/request.js:683:14)\n at Request.transition (/app/node_modules/aws-sdk/lib/request.js:22:10)\nHere is a simplified view of the code:\n```javascript\nlet activityStream = makeReadableJSON(activity);\nasync.parallel([\n    (apc) => {\n        var params = {\n            Bucket: bucketName,\n            Key: s3_path,\n            Body: activityStream\n        };\n        s3.upload(params, apc);\n    },\n    other-work-here...\n], callback);\n```\n. ",
    "robksawyer": "Hmm... It appears that some properties are required even though the docs do not specify. \nLooks like it worked ok by adding the following:\nvar params = {\n                Bucket: 'my-bucket-name',\n                CORSConfiguration: {\n                    CORSRules: [\n                        {\n                            AllowedHeaders: ['*'],\n                            AllowedOrigins: origins,\n                            AllowedMethods: ['GET'],\n                            ExposeHeaders: ['ETag']\n                            //MaxAgeSeconds: 0\n                        }\n                    ]\n                }\n            };\n. ",
    "johneke": "Seems more sane. Thanks @AdityaManohar!\n...\nmemory usage delta: 85.6640625mb, total: 112.05078125mb\n...\nmemory usage delta: 4.56640625mb, total: 116.65625mb\n...\nmemory usage delta: 1.6015625mb, total: 118.296875mb\n...\nmemory usage delta: -4.40234375mb, total: 113.91015625mb\n...\nmemory usage delta: 0.04296875mb, total: 113.96484375mb\n. ",
    "anuradhag": "Thank you @AdityaManohar for your detailed reply. \n. Thank you @AdityaManohar for your detailed reply. \n. ",
    "GoodMirek": "After another research I have found this is outstanding issue for two years already.\nWould be nice to fix at least the documentation.\nhttps://forums.aws.amazon.com/thread.jspa?messageID=551510\n. @lsegal Thanks for your quick response. These links are useful, but if they explicitly mention that CORS is not supported by other than default services then it would be even clearer, at least to me. Also page with CloudSearch documentation API can mention that CloudSearch does not support CORS.\nThe browser builder for the SDK is a new for me, would be great if can be referred from Building the SDK guide .\nWhat is still not clear to me is why SDK tries to send HTTP OPTIONS request towards CloudSearch API, which seems not to support OPTIONS request at all. The OPTIONS request fails even in environment without CORS, see the CURL output in my first post.\n. Thanks for the explanation of OPTIONS. I am sorry I did not know it and blamed AWS SDK.\nDo you think the documentation improvements I have suggested could be implemented?\n. ",
    "jbuck": "Sure! Here's the code that was throwing:\n```\nvar async = require(\"async\");\nvar AWS = require(\"aws-sdk\");\nvar S3 = new AWS.S3();\nvar Bucket = process.argv[2];\nvar IsTruncated = true;\nvar KeyMarker;\nasync.whilst(function test() {\n  return IsTruncated;\n}, function workfn(callback) {\n  var params = {\n    Bucket,\n    KeyMarker,\n    MaxKeys: 1000\n  };\nS3.listObjectVersions(params, function(list_error, data) {\n    if (list_error) {\n      return callback(list_error);\n    }\nIsTruncated = data.IsTruncated;\nKeyMarker = data.NextKeyMarker;\n\ncallback(null);\n\n});\n}, function errfn(error) {\n  throw error;\n});\n```\nThe bucket I was listing and deleting had around 400k objects, and I found that this error would occur more frequently near the end of the deletion run, after running for about 45 minutes. Only the listing code was throwing this error.\n. Sure! Here's the code that was throwing:\n```\nvar async = require(\"async\");\nvar AWS = require(\"aws-sdk\");\nvar S3 = new AWS.S3();\nvar Bucket = process.argv[2];\nvar IsTruncated = true;\nvar KeyMarker;\nasync.whilst(function test() {\n  return IsTruncated;\n}, function workfn(callback) {\n  var params = {\n    Bucket,\n    KeyMarker,\n    MaxKeys: 1000\n  };\nS3.listObjectVersions(params, function(list_error, data) {\n    if (list_error) {\n      return callback(list_error);\n    }\nIsTruncated = data.IsTruncated;\nKeyMarker = data.NextKeyMarker;\n\ncallback(null);\n\n});\n}, function errfn(error) {\n  throw error;\n});\n```\nThe bucket I was listing and deleting had around 400k objects, and I found that this error would occur more frequently near the end of the deletion run, after running for about 45 minutes. Only the listing code was throwing this error.\n. @AdityaManohar I ran it again today, happens in about 5 minutes now:\n```\n(assume-aws-role everything)$ time node bin/s3-delete-everything.js popcorn.webmadecontent.org\n/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/util.js:612\n      if (options.message)\n                 ^\nTypeError: Cannot read property 'message' of null\n    at Object.error (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/util.js:612:18)\n    at Request.callListeners (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:107:28)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:595:14)\n    at Request.transition (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:21:10)\n    at AcceptorStateMachine.runTo (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:37:9)\n    at Request. (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:597:12)\n    at Request.callListeners (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:115:18)\nreal    5m38.866s\nuser    1m26.354s\nsys 0m2.013s\n```\n. @AdityaManohar I ran it again today, happens in about 5 minutes now:\n```\n(assume-aws-role everything)$ time node bin/s3-delete-everything.js popcorn.webmadecontent.org\n/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/util.js:612\n      if (options.message)\n                 ^\nTypeError: Cannot read property 'message' of null\n    at Object.error (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/util.js:612:18)\n    at Request.callListeners (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:107:28)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:595:14)\n    at Request.transition (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:21:10)\n    at AcceptorStateMachine.runTo (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:37:9)\n    at Request. (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:597:12)\n    at Request.callListeners (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:115:18)\nreal    5m38.866s\nuser    1m26.354s\nsys 0m2.013s\n```\n. Exciting, a new error! :)\n```\n/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:30\n            throw err;\n            ^\nError: null\n    at Request.callListeners (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:107:43)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:595:14)\n    at Request.transition (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:21:10)\n    at AcceptorStateMachine.runTo (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:37:9)\n    at Request. (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:597:12)\n    at Request.callListeners (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:115:18)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:595:14)\n```\nI can reproduce this with the same script as above; is there any other information I can add to assist in debugging this issue?\n. Exciting, a new error! :)\n```\n/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:30\n            throw err;\n            ^\nError: null\n    at Request.callListeners (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:107:43)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:595:14)\n    at Request.transition (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:21:10)\n    at AcceptorStateMachine.runTo (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:37:9)\n    at Request. (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:597:12)\n    at Request.callListeners (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:115:18)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/Users/jbuck/Documents/Github/s3-delete-everything/node_modules/aws-sdk/lib/request.js:595:14)\n```\nI can reproduce this with the same script as above; is there any other information I can add to assist in debugging this issue?\n. @AdityaManohar would you like me to refile this as a new issue? It's definitely better than before, since I can actually catch and retry the command, but there's no information about the error.\n. @AdityaManohar would you like me to refile this as a new issue? It's definitely better than before, since I can actually catch and retry the command, but there's no information about the error.\n. Well, I'm a moron :sob: \nThanks for fixing the original issue though, happy that the second one is my fault!\n. Well, I'm a moron :sob: \nThanks for fixing the original issue though, happy that the second one is my fault!\n. ",
    "riseres": "I still got this same problem.\nI have to use from @rclark suggest to get works.\nvar AWS = require('aws-sdk');\nvar dynamodb = new AWS.DynamoDB({ region: 'us-east-1' });\nvar client = new AWS.DynamoDB.DocumentClient({ service: dynamodb });\n. ",
    "fsaldivars": "In current working with sdk-for-javascript and find solution in this doc:\nsdk-for-javascript\nAnd working on macOS High... \nvim ~/.bash_profile\nAnd Add\n\nAWS_REGION=\"us-east-1\"\nexport AWS_REGION. \n",
    "ldobson": "That makes a lot of sense, but thank you!\n. ",
    "CSharpFan": "Hi Aditya,\nNotice that for the second example the issue is that my key is FOO, however the metadata returned looks like { \"foo\": \"bar\" } instead of { \"FOO\": \"bar\" }.\n. Hi Aditya,\nNotice that for the second example the issue is that my key is FOO, however the metadata returned looks like { \"foo\": \"bar\" } instead of { \"FOO\": \"bar\" }.\n. Okay, thank you for the clarification.\n. Okay, thank you for the clarification.\n. ",
    "kentor": "Another question.. we have to compute the SHA256 of the 5mb chunks in the UI thread which is where we call s3.uploadParts(). This will cause jank no matter what will it not?\n. Thanks for the answers. I may have to resort to using web workers to calculate the checksums. I don't think I can use the s3 signature version for the eu-central-1 region right?\n. I was hoping to run the sdk and use a Managed Upload in a web worker to avoid the checksum computation in the main thread. But Managed Uploads do not work in a Worker when parts are involved because it tries to use DOMParser which is not available in a worker.\nEdit: I polyfilled the worker with a DOMParser and that seems to work\n. Any way to use WebCrypto to calculate the SHA256?\nPerhaps consider adding that to the sdk.\nAlso is it possible to include the checksum in the call to s3.uploadPart?\n. In the meantime, why does the sdk have to call readAsArrayBuffer when passed in an ArrayBuffer? Shouldn't it just use the value as is?\n. Closing this since I've found a workaround which is patching AWS.util.computeSha256 to use webcrypto.\n. Is \"resuming\" supposed to work after an abort()?\n. @chrisradek \nI was thinking of using the filereader-stream library to get a readable stream from a File. Then I'm using crypto-browserify to encrypt the stream.\nBtw I did a quick and dirty modification of the send function in lib/s3/managed_upload.js to take a stream in the browser and it does appear to work.\nI changed this\njs\n    if (self.sliceFn) {\n      self.fillQueue = self.fillBuffer;\n    } else if (AWS.util.isNode()) {\n      var Stream = AWS.util.nodeRequire('stream').Stream;\n      if (self.body instanceof Stream) {\n        ...\n      }\n    }\nto this \njs\n    if (self.sliceFn) {\n      self.fillQueue = self.fillBuffer;\n    } else if (typeof self.body._read === 'function') {\n      ...\n    }\nMy app code is something like this:\n``` js\nconst crypto = require('crypto-browserify');\nconst fileReaderStream = require('filereader-stream');\n...\nconst encrypt = crypto.createCipheriv(\n  'aes-256-cbc',\n  '3zTvzr3p67VC61jmV54rIYu1545x4TlY',\n  '60iP0h6vJoEaJoEa'\n);\nconst stream = fileReaderStream(file).pipe(encrypt);\nconst managedUpload = s3.upload({\n  Body: stream,\n  Key: test/${file.name},\n});\nmanagedUpload.send();\n```\n. @chrisradek At my work (the product I am working on) I think it would be a bunch of work to add support something like SSE-C. Another reason is say if we were to back our storage with another storage provider, it would be easier to add support for that if we handled our own encryption. We don't want to be too tied down to what AWS offers.\nAnyway, I just think stream support gives way to more than just encryption in the browser. Say if we want to perform some other type of transformation before uploading to s3, it would be a lot easier with a transform streams.\n. I see. I thought it would just keep what its read from the stream into a buffer until that part has uploaded successfully, or else upload the buffer again if the part was aborted and resumed.\n. Yes in playing around the resuming feature, I can reproduce this using Buffers\nscript (I'm using node 4.2)\n``` js\nconst AWS = require('aws-sdk');\nconst credentials = new AWS.Credentials({\n  accessKeyId: ...,\n  secretAccessKey: ...,\n});\nconst s3 = new AWS.S3({\n  credentials,\n  params: { Bucket: ... },\n  region: ...,\n});\nconst MB = 1024 * 1024;\nvar body = new Buffer(20 * MB);\nbody.fill('1', 0, 5 * MB);\nbody.fill('2', 5 * MB, 10 * MB);\nbody.fill('3', 10 * MB, 15 * MB);\nbody.fill('4', 15 * MB);\nconst upload = new AWS.S3.ManagedUpload({\n  leavePartsOnError: true,\n  params: { Body: body, Key: 'test' },\n  service: s3,\n});\nupload.on('httpUploadProgress', function(progress) {\n  console.log(progress);\n});\nupload.send = upload.send.bind(upload, (err, data) => {\n  if (err) {\n    console.log('error:', err);\n  } else {\n    console.log('success:', data);\n  }\n});\nupload.send();\nsetTimeout(function() {\n  console.log('abort!');\n  upload.abort();\n  setTimeout(function() {\n    upload.send();\n  }, 2000);\n}, 2000);\n```\nI'm only seeing a 5MB file in my bucket, isntead of a 20MB file.\n. us-east-1. I don't believe this is using signature version 4\n. Oh wow. Yeah that's the version I'm using. I get slow speeds with the inspector opened. But full speed when it's closed. \n. Thanks @chrisradek \n. ",
    "Hemanshu1belani": "@kentor For DOMParser you mentioned \n\"Edit: I polyfilled the worker with a DOMParser and that seems to work\"\nCan you please share the code example? or how you made it happen? Even I'm stuck because of DOMParsed in webworker. ",
    "calidion": "Thanks for the reply.\nBut the \"\\\"\" is really not in good smell.\n. Thanks for the reply.\nBut the \"\\\"\" is really not in good smell.\n. ",
    "pho3nixf1re": "Turns out I was calling stream.resume() too early. DOH!\n. ",
    "aadamsx": "I'm trying to do this same thing, using the sms protocol.  I too was placing the confirmSubscription in the subscribe callback.  @pree011235 in my case, when I call the subscribe, my test phone gets the SMS message and I have to reply YES  in order to confirm the subscribe.  From a coding standpoint, How do I confirm that phone subscribed?\n. Thank you @jeskew \nI'm going to use listSubscriptionsByTopic.  But it only returns 100 subscriptions, requiring a NextToken in order to get more.\nDo you have any sudo-code that would help?\nThis is what I came up with:\n```\n      AWS.config.update({\n         accessKeyId: Meteor.settings.awsAccessKeyId,\n         secretAccessKey: Meteor.settings.awsSecretKey,\n         region: \"us-east-1\"\n      });\n  var sns = new AWS.SNS({params: {TopicArn: 'arn:aws:sns:us-east-1:34523452345:test'}});\n  var nextToken = null;\n  var index = 0;\n  var results[];\n  do {\n    result = sns.ListSubscriptionsByTopic({NextToken: nextToken}, function (err, data) {\n      if (err) console.log(err, err.stack); // an error occurred\n      else {\n        // call ListSubscriptionsByTopic again?  How?\n        results[index] = data.Subscriptions;\n        if (NextToken) {\n          nextToken = NextToken;\n          index = index + 1;\n        } else {\n          nextToken = null;\n        }\n      }\n    });  \n  }\n  while (nextToken !== null);\n\n// do something with the results\n\n```\n. Thank you, this is exactly the feedback I was looking for!\nI got things working, but without the nextToken.  I was confused what to do about it, and you just clear it up for me!\n. ",
    "desaiazaz": "var nextToken = null;\n            var index = 0;\n            var results=[];\n            do {\n                results = sns.ListSubscriptionsByTopic({NextToken: nextToken}, function (err, data) {\n                    if (err) console.log(err, err.stack); // an error occurred\n                    else {\n                        // call ListSubscriptionsByTopic again?  How?\n                        results[index] = data.Subscriptions;\n                        if (NextToken) {\n                            nextToken = NextToken;\n                            index = index + 1;\n                        } else {\n                            nextToken = null;\n                        }\n                    }\n                });\n            }\n            while (nextToken !== null);\n            console.log(\"LIST>>>\",results);\ngot the error ListSubscriptionsByTopic function not found . ",
    "tallboy": "I have the transcoder.waitFor as a second step in a waterfall flow.  Is it necessary to have it in the callback of the transcoder.createJob?  I can try it that way as well.\nAre you able to reproduce the error message in the data object that's returned in readJob?  I get this when a filename with the same name already exists in the S3 bucket and I try to create a new transcode job.\n. Here's the polling code I was using:\n```\nlet transcoder = new AWS.ElasticTranscoder();\nlet params = {\n  ....\n};\ntranscoder.createJob(params, (err, data) => {\n  if (err) {\n    log.error('transcoder.createJob err: ', err);\n    return cb(err);\n  } else {\n    log.info('transcoder.createJob data: ', data);\n    transcoder.waitFor('jobComplete', { Id: data.Job.Id }, function(err, data) {\n      log.error('transcoder.pollTranscodeJob: ', err, data);\n      cb(err, data);\n    });\n  }\n});\n```\nThat's how I get the above error:\ntranscoder.pollTranscodeJob:  { [ResourceNotReady: Resource is not in the state jobComplete]\n  message: 'Resource is not in the state jobComplete',\n  code: 'ResourceNotReady',\n  retryable: false,\n  time: Mon Sep 21 2015 10:44:02 GMT-0700 (PDT),\n  statusCode: 200,\n  retryDelay: 30000 } null\nNow if I use a SetInterval function and ping it every 10 seconds or so I'll eventually get a return data object with an error message in the Output Status. \nsetInterval(() => {\n  transcoder.readJob({ Id: jobId }, (err, data) => {\n    if (err) return cb(err);\n    if (data.Job.Status && data.Job.Status == 'Error') {\n      let errMessage = data.Job.Output.StatusDetail;\n      log.error('transcoder readJob: ', errMessage);\n      cb(new Error(errMessage));\n    } else {\n      log.info('transcoder readJob: ', data);\n      cb(null, data);\n    }\n  });\n}, 10000);\nError example:\n{ Job: { Arn: 'xxx', Id: 'xxx', Input: { AspectRatio: 'auto', Container: 'auto', FrameRate: 'auto', Interlaced: 'auto', Key: 'xxx', Resolution: 'auto' }, Output: { Captions: [Object], Id: '1', Key: 'xxx', PresetId: '1351620000001-000020', Status: 'Error', StatusDetail: '3002 68de6d80-e685-45fe-80ac-fdceb32cc3f8: The specified object could not be saved in the specified bucket because an object by that name already exists: bucket=xxx, key=xxx.', Watermarks: [] }, Outputs: [ [Object] ], PipelineId: 'xxx', Playlists: [], Status: 'Error', Timing: { FinishTimeMillis: 1442613305225, StartTimeMillis: 1442613302501, SubmitTimeMillis: 1442613298943 } } }\nI'll probably end up using a guid or something to make sure I don't have any naming collisions in the S3 buckets I just thought it was strange to get a generic [Resource Not Ready] error from the poller when there's a more detailed error message on admin panel for the Job.  That error seems to also match with the Status: 'Error' detail that I get back when I put the readJob on a timer.\n. This is exactly what I ended up implementing with the interval timer I pasted above. It would be beneficial to get back the error message from readJob() if the waitFor() throws an error.  The documentation states it's calling the underlying readJob() function anyway.  \nwaitFor('jobComplele')\nWhy the difference in response payloads?  IMO, it would be nice to get a more descriptive error response other than ResourceNotReady if a job fails.\n. :+1: Feel free to close it\n. ",
    "jmswhll": "@AdityaManohar I believe this may be related to this commit:\nhttps://github.com/aws/aws-sdk-js/commit/af181f8636030e798fe7144aa376b4a7b4f3f4a1\n. @AdityaManohar I believe this may be related to this commit:\nhttps://github.com/aws/aws-sdk-js/commit/af181f8636030e798fe7144aa376b4a7b4f3f4a1\n. ",
    "DrMegavolt": "@chrisradek default SDK has only structures. \nI have a fork that does list output and faced with the issue.\nIf by design list is not supported and only structures can be used as root element, then this PR is not needed \n. ",
    "benrady": "I don't know how helpful it will be, but here's a stack trace:\n\nIf there's an alternative to calling getId, I'm more than happy to use that. Should I just be calling get() instead? I'm trying to get a Cognito ID to attach to a document to be written to DynamoDB, similar to this. I'm using the Cognito ID with a policy variable to ensure the data is only writable by an authenticated user.\n. I'm not sure there's a race condition. There's only one block of code that does this. \nWill the credentials refresh automatically when I make a DynamoDB call with the SDK (putItem, for example), or do I have to refresh them myself after they expire?\n. As far as version goes, I'm using 2.1.48. I used the custom builder you guys provide to create a library with the following services:\n\u2022 CognitoIdentity\n\u2022 STS\n\u2022 CognitoSync\n\u2022 CognitoDynamoDB \n\u2022 Lambda\nAm I missing any?\n. I haven't seen this happen again since switching to using refresh instead of getId. Closing for now. I'll open a new ticket if I see this problem while using refresh.\n. ",
    "davidli2017": "Thanks for your help .node js and browser js have common in operation sqs.\n. ",
    "Lohit9": "Ok, I will be implementing a sample in the next 2 days and will mention in case I encounter any issues. \n. React Native runs JS on JavaScriptCore (when on the device or simulator) and on Chrome itself (when using Chrome debugging), so modules that depend on built-in Node.js modules won't work. \nThe AWS node SDK has many core node dependencies, due to which you cannot use it in your React native app. Unless you find a fork of the SDK that doesn't use any core node modules\n. @AdityaManohar Thanks for the links, trying out stuff to see if the javascript sdks does the job.\n. @chrisradek No I could not get it working. I don't remember the exact issue I had, but the javascript files in the browser version of the SDK were not being imported into the react native javascript code. I believe it was expecting a different kind of file structure/or maybe some dependency related issue.\nWhat I finally did was to use the AWS Mobile SDK and natively bridge it with the react native javascript code. The downside is that I had to do this twice - for Android and iOS, would love to hear from someone who got their react native app working with the browser version of the SDK. \n. ",
    "morenoh149": "I'm currently seeing\n```\nWARNING in ./~/aws-sdk/lib/util.js\nCritical dependencies:\n50:30-45 the request of a dependency is an expression\n53:11-53 the request of a dependency is an expression\n @ ./~/aws-sdk/lib/util.js 50:30-45 53:11-53\nWARNING in ./~/aws-sdk/lib/api_loader.js\nCritical dependencies:\n17:15-59 the request of a dependency is an expression\n108:12-46 the request of a dependency is an expression\n112:21-58 the request of a dependency is an expression\n118:18-52 the request of a dependency is an expression\n @ ./~/aws-sdk/lib/api_loader.js 17:15-59 108:12-46 112:21-58 118:18-52\nWARNING in ./~/aws-sdk/lib/region_config.json\nModule parse failed: /Users/harrymoreno/programming/souscompany/sousReactNative/node_modules/aws-sdk/lib/region_config.json Line 2: Unexpected token :\nYou may need an appropriate loader to handle this file type.\n| {\n|   \"rules\": {\n|     \"/\": {\n|       \"endpoint\": \"{service}.{region}.amazonaws.com\"\n @ ./~/aws-sdk/lib ^.\\/.*$\nERROR in ./~/aws-sdk/lib/api_loader.js\nModule not found: Error: Cannot resolve module 'fs' in /Users/harrymoreno/programming/souscompany/sousReactNative/node_modules/aws-sdk/lib\n @ ./~/aws-sdk/lib/api_loader.js 5:9-22\nERROR in ./~/aws-sdk/lib/services.js\nModule not found: Error: Cannot resolve module 'fs' in /Users/harrymoreno/programming/souscompany/sousReactNative/node_modules/aws-sdk/lib\n @ ./~/aws-sdk/lib/services.js 3:9-22\nwebpack: bundle is now VALID.\n```\na possible solution would be to fork the sdk and use https://github.com/johanneslumpe/react-native-fs in place of the fs module.\n. https://github.com/blog/2119-add-reactions-to-pull-requests-issues-and-comments\n. ",
    "hdzidic": "I'm having this exact issue morenoh149 described. \nAny updates?\n. ",
    "datapimp": "@morenoh149 @hdzidic I discovered this issue researching this topic and saw your posts.  \nIf you could avoid using webpack to parse the AWS SDK dependency I believe you wouldn't encounter those problems.  Webpack doesn't work normally with most dynamic require statements, which is why you're getting those warnings. (see https://github.com/webpack/docs/wiki/context).  You could load the AWS SDK as an external library in your webpack setup, or ignore it in your loader config.  \nI'd love to know if you're able to get it working after that.\n. ",
    "dcflow": "I get requiring unknown module 'crypto'. Even after installing cryto-js the error is the same.\n. @leimd I ended up using the REST api too. I saw somewhere a project that could make node modules work with react native, that maybe could help but still there is a lot of browser specific code in the sdk.\n. @KristoferEng Use this https://github.com/lifuzu/react-native-uploader-s3\n. https://github.com/mybigday/react-native-s3\n. ",
    "KristoferEng": "I have had the same issue for a bit now. End of the day is there a simple solution to connect to S3 while building a react native app?\n. @Lohit9 Did you have any luck connecting your react native app to aws/s3?\n. @DHidee66 @leimd Thanks for pointing me in the right direction. I didn't realize that the module either has to be pure JS or created for react-native which is why the SDK does not work (or other modules I've tried).\nHow did you generate the signature required for the AWS REST api?\n. ",
    "leimd": "@DHidee66 I think that is because module 'crypto' is a node.js standard library which doesn't even exist on the browser, the aws js sdk got around this by using the crypto-browserify package on the browser. My way of get this this SDK working would be using something like browersify, but I haven't succeeded yet, currently I am just calling the AWS REST api which actually isn't that bad. Not having sdk isn't the end of the world, but I'd definitely like to see if anyone can get it work with react-native.\n. Hey everyone, over the weekend I wrote a library to generate the signature needed for AWS calls that is compatible to be used in React-Native, here is the link to the GitHub repo, everyone is welcome to contribute to this project. https://github.com/leimd/AWSSignature\n. @KristoferEng hey I just open sourced a library to generate the signature, see the previous comment.\n. ",
    "tomprogers": "The problem appears to be that there isn't an ES6-module-style version of the SDK that works outside of a nodejs environment.\nThe SDK you can get via npm is designed to run in a node environment. It assumes that core nodejs libraries (like crypto) are available and can be depended upon. This assumption is false in a react-native application.\nThe SDK you can load from the CDN (or via bower) is a browserland script. It doesn't rely on nodejs modules, but it isn't module-aware at all i.e. it doesn't export anything, so import and require don't seem to accomplish anything. As a browserland script, it assumes it can \"publish\" it's public API by declaring a top-level variable in a global namespace. This assumption is also false in a react-native application.\nAs a result, neither the npm module nor the browserland script can be used in a react-native application. The npm module will cause a build-time failure (because of unresolvable dependencies), while the browserland script will fail at runtime (because there's no way to access the AWS object it tries to expose).\nWhat I think we all need is a module-aware script that doesn't rely on nodejs modules -- a mix of the existing npm and CDN scripts. If react-native used webpack (instead of the react-native-packager), we could use custom loader configuration to accommodate the browserland script, or we could use custom resolution mappings to polyfill the npm module's nodejs dependencies. However, react-native does not use webpack, and it's not clear how someone would integrate webpack into their proprietary build pipeline.\nNot having this is a huge blocker. I'm having to reverse-engineer a subset of the official SDK so I can work with SNS. I expect everyone else here is in roughly the same boat.\n. I found a third-party library that appears to have what we need: an npm package with no nodejs dependencies, built as an ES6-style module: https://github.com/nisaacson/aws-v4-sign-small\nI'm trying to get it to work now.\n. ",
    "epeli": "Yeah. I ran to this also when trying to use SNS from RN. We ended up writing a simple proxy server for the api calls we needed.\n\nThe SDK you can load from the CDN (or via bower) is a browserland script. It doesn't rely on nodejs modules, but it isn't module-aware at all i.e. it doesn't export anything, so import and require don't seem to accomplish anything.\n\nHas anyone tried to create an UMD-bundle using Browserify out of the node.js modules and import that from RN code? There is even browser field in package.json so it should load only the browser stuff and add CommonJS exports.\n. Yeah. I ran to this also when trying to use SNS from RN. We ended up writing a simple proxy server for the api calls we needed.\n\nThe SDK you can load from the CDN (or via bower) is a browserland script. It doesn't rely on nodejs modules, but it isn't module-aware at all i.e. it doesn't export anything, so import and require don't seem to accomplish anything.\n\nHas anyone tried to create an UMD-bundle using Browserify out of the node.js modules and import that from RN code? There is even browser field in package.json so it should load only the browser stuff and add CommonJS exports.\n. There is something weird going on. It seems that when signing the url you must set just some random tag with it but you actually set real tags within the PUT operation.\nHere's my signing code:\njs\n    var signedURL = s3.getSignedUrl(\"putObject\", {\n        Bucket: \"mybucket\",\n        Key: \"foobar.pdf\",\n        ContentType: \"application/pdf\",\n        ACL: \"public-read\",\n        Tagging: \"foo=anything\",\n    });\nAnd here's browser upload code \n```js\n    const headers = new Headers();\n    headers.append(\"x-amz-tagging\", \"myrealtag=tagvalue\");\nconst res = await fetch(signedURL, {\n    method: \"PUT\",\n    body: file,\n    headers,\n});\n\n```. ",
    "vespakoen": "Working on supporting this natively: https://github.com/facebook/node-haste/pull/46\n. Working on supporting this natively: https://github.com/facebook/node-haste/pull/46\n. ",
    "benjreinhart": "Hey all, I wrote a library to upload files to S3 in React Native. I had problems with the existing solutions and was hoping to find a solution without native dependencies. The library I wrote doesn't have any native dependencies, uses only XMLHttpRequest & FormData. It has a simple API and does exactly what I need it to. Have plans to add more to it. Hope it's useful:\nhttps://github.com/benjreinhart/react-native-aws3\n. ",
    "procedurallygenerated": "+1\n. ",
    "sesteva": "+100\n. ",
    "dfejgelis": "+1\n. ",
    "newlix": "+1\n. ",
    "hassankhan": "+1\n. +1\n. Indeed, it would be really beneficial if the SDK was updated rather than us having to work around specific issues by trial and error.\n. Indeed, it would be really beneficial if the SDK was updated rather than us having to work around specific issues by trial and error.\n. Thanks for the reply @chrisradek! I did also get a CRC32CheckFailed error once, actually. This was for iOS on the Simulator.\njson\n\"dependencies\": {\n  \"aws-sdk\": \"^2.78.0\",\n  \"react\": \"^16.0.0-alpha.12\",\n  \"react-native\": \"^0.45.1\"\n},\nAs you mentioned, it may be possible that it takes a long time but if I override the global XmlHttpRequest I can see the responses returned fine. It's only when I disable it that it seems to not work at all. I'll give it another go with checksumming/retries disabled and report back.. Thanks for the reply @chrisradek! I did also get a CRC32CheckFailed error once, actually. This was for iOS on the Simulator.\njson\n\"dependencies\": {\n  \"aws-sdk\": \"^2.78.0\",\n  \"react\": \"^16.0.0-alpha.12\",\n  \"react-native\": \"^0.45.1\"\n},\nAs you mentioned, it may be possible that it takes a long time but if I override the global XmlHttpRequest I can see the responses returned fine. It's only when I disable it that it seems to not work at all. I'll give it another go with checksumming/retries disabled and report back.. Hi @chrisradek, sorry for the late response, disabling the CRC32 checks works for me \ud83d\ude04 . Hi @chrisradek, sorry for the late response, disabling the CRC32 checks works for me \ud83d\ude04 . ",
    "seantempesta": "+1\n. ",
    "kdenz": "+1\n. ",
    "cagodoy": "+1\n. ",
    "anthonyalayo": "+1\n. ",
    "warrenronsiek": "+1\n. ",
    "appwiz": "We've released a developer preview of the AWS SDK for React Native. Try it out and let us know what you think.\n. ",
    "lielran": "@appwiz -I'm missing the basic of Cognito integration in this first drop of the sdk. any planning to support that in the future? . ",
    "ZepAviator": "Am I right in reading that AWS SDK for React Native fails to provide any actual login methods to the AWS Cognito User Pools? \nThis can't be right. First, I believe that iOS policy dictates that you have to provide a user/pass login method in addition to 3rd parties (Facebook, twitter, etc...). Thus, if you are in the AWS services, that's User Pools. Not having this is almost an immediate non-starter. :(. ",
    "ChenLi0830": "Hi @appwiz, thanks for the work. It is said in the repository that the react-native SDK is a developer preview version one. Could you please let us know if AWS is planning to publish a formal release soon? . ",
    "BerndWessels": "@appwiz Are you guys actively working on this? Lots of issues there and not many closed yet.. ",
    "mdjnewman": "Thanks for the initial cut of RN support @chrisradek !\nI've tested out the react-native branch with Cognito, S3 and SNS and haven't run into any issues so far. I'd be keen to see #1393 merged :). ",
    "rromanchuk": "cool, thanks! RTFMing now \n. ",
    "NMEsolutionz": "\u200eThank you so much chrisradek\u00a0                                                                                                                                                                                                                                                                          Natalie Ellis                                                                                                                                                                                                                                         From: chrisradekSent: Tuesday, 20 October 2015 00:38To: aws/aws-sdk-jsReply To: aws/aws-sdk-jsSubject: Re: [aws-sdk-js] s3.getObject(params).createReadStream().pipe(file) throws no error, but transfers no data to file (#744)@Rich17 \nWhat version of the SDK are you using?\nI tried the following code using SDK v2.2.8 and Nodejs 0.12.7, and was able to successfully write a ~250MB file from S3 to a file on my local computer.\nvar AWS = require('aws-sdk');\nvar fs = require('fs');\nvar params = {\n  Bucket: 'VALID_BUCKET',\n  Key: 'BIG_FILE.mp4'\n};\nvar s3 = new AWS.S3();\nvar file = fs.createWriteStream('test.mp4');\nfile.on('close', function(){\n    console.log('done');  //prints, file created\n});\ns3.getObject(params).createReadStream().on('error', function(err){\n    console.log(err);\n}).pipe(file);\nLet me know if the above example doesn't work for you.\n\u2014Reply to this email directly or view it on GitHub.\n. ",
    "Rich17": "Thanks a lot, @chrisradek . \nMy error was my file path in fs.createWriteStream. I was writing the path as it exists in the file structure I'm working with during development (pre-build) as opposed to the path once the full Meteor package is built. \n. ",
    "soumya-github": "Hi @chrisradek ,\nI tried to use your solution. After the \"s3.getObject(params).createReadStream().on('error', function(err){\nconsole.log(err);\n}).pipe(file);\" code snippet what I have to write to get the file in my local storage. I am not able to get it in my local path.. ",
    "dembeleu": "@chrisradek Thank you verry much, works perfectly. ",
    "ReinsBrain": "comments? ideas? anyone?\n. ",
    "kleneway": "FYI - I was having a similar issue and discovered that it was only happening when I was running a reverse proxy on port 4000 on my localhost.\n. ",
    "sumpton": "I am getting this error when enqueuing two items quickly, in parallel basically. One is enqueued without an error and the other had the checksum error, plus it enqueues 4 items (messages). Either message works fine when enqueued separately. I didn't test more than two messages, but it appears that somewhere the requests are not asynchronous. \n. ",
    "abhilashsajeev": "@sumpton @chrisradek  Any updates on this? Still getting this issue while using multiple request to sqs from different asgs.\n. @sumpton @chrisradek  Any updates on this? Still getting this issue while using multiple request to sqs from different asgs.\n. ",
    "asthinasthi": "I have this problem if I send messages in a loop. Even for a loop of 3 items. No error for a single message sent.. I have this problem if I send messages in a loop. Even for a loop of 3 items. No error for a single message sent.. ",
    "iassal": "I was facing the same issue and resolved it by fixing the parameter scope. Make sure the params object is not shared concurrently.\nThis snipped wasn't working:\n```javascript\nvar params = {\n    DelaySeconds: 0,\n    QueueUrl: QUEUE_URL\n};\nfunction sendMessage(body) {\n    params.MessageBody = body\n    sqs.sendMessage(params, function (err, data) {\n        if (err) {\n            console.error(err);\n        } else {\n            console.log(\"Success\", data.MessageId);\n        }\n    });\n}\nfor (var i = 0; i < messages.length; i++) {\n    sendMessage(messages[i]);\n}\nAs you can see, all threads were referencing the same params object. I fixed by creating a new object for each invocation as below:javascript\nfunction sendMessage(body) {\n    var params = {\n        DelaySeconds: 0,\n        MessageBody: body,\n        QueueUrl: QUEUE_URL\n    };\nsqs.sendMessage(params, function (err, data) {\n    if (err) {\n        console.error(err);\n    } else {\n        console.log(\"Success\", data.MessageId);\n    }\n});\n\n}\nfor (var i = 0; i < messages.length; i++) {\n    sendMessage(messages[i]);\n}\n```. I was facing a similar issue and solved it as mentioned here https://github.com/aws/aws-sdk-js/issues/745#issuecomment-372317868 . ",
    "Ashesh007": "I m using following code \nvar fmt = require('fmt');\nvar amazonSes = require('awssum-amazon-ses');\nvar ses = new amazonSes.Ses({\n    'accessKeyId'     : process.env.ACCESS_KEY_ID,\n    'secretAccessKey' : process.env.SECRET_ACCESS_KEY,\n    'region'          : amazonSes.US_EAST_1\n});\nses.ListVerifiedEmailAddresses(function(err, data) {\n    fmt.dump(err, 'err');\n    fmt.dump(data, 'data');\n});\nI got above error using this code....\nCan u help me how to solve this issue...\nI m stuck from last 2-3 days and don't got answer...\nAnd Using same credential in PHP script it's working...\nWant to know what's the issue in above code...\nI m tried many packages for SES but in all packages getting same error\n. I m providing same as format as u given in above example and sending to aws but getting same error ??? what should i have to do ??\nany issue while creating sns credential for my API??\n. Giving Hard-code them in my program\n. yes getting same issue .......\n. yes i have use that process as given docs... And Using same credentials in PHP code is working but in nodejs notworking...\n. ```\n<?php\nfunction Send_Mail($to,$subject,$body)\n{\nrequire 'class.phpmailer.php';\n$from = \"From Id\";\n$mail = new PHPMailer();\n$mail->IsSMTP(true); // SMTP\n$mail->SMTPAuth   = true;  // SMTP authentication\n$mail->Mailer = \"smtp\";\n$mail->Host       = \"tls://email-smtp.us-west-2.amazonaws.com\"; // Amazon SES server, note \"tls://\" protocol\n$mail->Port       = 465;                    // set the SMTP port\n$mail->Username   = \"\";  // SES SMTP  username Access Key\n$mail->Password   = \"\";  // SES SMTP password Secret Key\n$mail->SetFrom($from, 'From Name');\n$mail->AddReplyTo($from,'9lessons Labs');\n$mail->Subject = $subject;\n$mail->MsgHTML($body);\n$address = $to;\n$mail->AddAddress($address, $to);\nif(!$mail->Send())\nreturn false;\nelse\nreturn true;\n}\n?>\n```\nAbove function is use for Sending Mail Using SMTP....\nUsing above code its working in PHP but while using this credential in nodejs its getting error.. \n. using aws-sdk-js then which credentials which i have to use as access key and secret key....... while creating SES, got only access key and secret key... and that credentials are works in PHP then what's the difference in that ?????????\n. thanks for yr support........... Its working.........\n. ",
    "bantic": "@chrisradek Awesome, thank you! Glad to see that make it in. Keeping the change to the presigner query params makes sense to me.\n. ",
    "andreasherzog": "Hi @jeskew,\nI am using node 0.12.7 and aws-sdk 2.2.8.\nJust tested with aws-sdk 2.2.10, was able to reproduce.\n. When I test with your code, the error event (and only this) is correctly fired. However, in my code I directly use the stream after creating it, causing the sdk to think that I did not handle the error event:\n```\nvar archive = archiver('zip')\ns3Stream = self.s3sdk.getObject(params).createReadStream()\n                .on('error', function (err) {\n                   console.log(err);\n                })\n                .on('end', function() {\n                    console.log('finished');\n                });\n        archive.append(s3Stream, {\n            name: item.title || item.fileKey\n        });\n\n```\nBecause it is a stream, I would like to be able to directly pass it on to my next function and let my on error function handle the errors.\n. @chrisradek \nYes, this solves this issue. I now just listen for the error event on archiver, and this solves the issue. Thank you very much for your help!\n. We were just able to resolve  #750: It is necessary to listen also to the error object of the stream you pipe to / here it could be a stream you pipe your ReadStream to \n. ",
    "drobtravels": "I would also like to see this feature.  I'm currently using the official elasticsearch.js client which handle some API features like bulk requests which can be tricky to do via a standard HTTP call.\n. ",
    "headlessme": "I'd also like to see this. Currently using the official elasticsearch client for javascript.\n. I'd also like to see this. Currently using the official elasticsearch client for javascript.\n. ",
    "tielur": "@jeskew how are you managing the signing of requests with that module? I'm trying to use that module with IAM roles for Amazon EC2 but that currently doesn't work until sometimes like https://github.com/TheDeveloper/http-aws-es/pull/1 gets merged.\n. @jeskew how are you managing the signing of requests with that module? I'm trying to use that module with IAM roles for Amazon EC2 but that currently doesn't work until sometimes like https://github.com/TheDeveloper/http-aws-es/pull/1 gets merged.\n. I just experienced this EPROTO error while using this solution stack:\n64bit Amazon Linux 2015.09 v2.0.6 running Node.js\nNode version: 4.2.3\n{ [NetworkingError: write EPROTO]\n  message: 'write EPROTO',\n  code: 'NetworkingError',\n  errno: 'EPROTO',\n  syscall: 'write',\n  address: undefined,\n  region: 'us-west-2',\n  hostname: 'dynamodb.us-west-2.amazonaws.com',\n  retryable: true,\n  time: Thu Feb 04 2016 21:26:06 GMT+0000 (UTC) }\n. I just experienced this EPROTO error while using this solution stack:\n64bit Amazon Linux 2015.09 v2.0.6 running Node.js\nNode version: 4.2.3\n{ [NetworkingError: write EPROTO]\n  message: 'write EPROTO',\n  code: 'NetworkingError',\n  errno: 'EPROTO',\n  syscall: 'write',\n  address: undefined,\n  region: 'us-west-2',\n  hostname: 'dynamodb.us-west-2.amazonaws.com',\n  retryable: true,\n  time: Thu Feb 04 2016 21:26:06 GMT+0000 (UTC) }\n. @tristanls no I did not. I'm currently only using new AWS.DynamoDB();. I'm trying to find out what the actual problem is, it seems to be intermittent? I'll look into adding the three config settings(keepAlive, secureProtocol, ciphers)\n. @tristanls no I did not. I'm currently only using new AWS.DynamoDB();. I'm trying to find out what the actual problem is, it seems to be intermittent? I'll look into adding the three config settings(keepAlive, secureProtocol, ciphers)\n. For those that reported it being fixed, how are you testing? Just letting it run for awhile, killing/swapping out EC2 instances?\n. For those that reported it being fixed, how are you testing? Just letting it run for awhile, killing/swapping out EC2 instances?\n. @klinquist :+1:  Good to hear! I'm mid hot-patch push now.\n. @klinquist :+1:  Good to hear! I'm mid hot-patch push now.\n. ",
    "robertoandrade": "How about using it with the standard ES Java REST Client?. How about using it with the standard ES Java REST Client?. ",
    "skilledDeveloper": "I used elasticsearch, http-aws-es and aws-sdk:\n```javascript\nvar AWS = require('aws-sdk'),\n  es = require('elasticsearch').Client({\n    host: 'es-host',\n    connectionClass: require('http-aws-es'),\n    amazonES: {\n      region: 'us-east-1',\n      credentials: new AWS.EnvironmentCredentials('AWS')\n    }\n  });\n//es.search()\n. I used `elasticsearch`, `http-aws-es` and `aws-sdk`:javascript\nvar AWS = require('aws-sdk'),\n  es = require('elasticsearch').Client({\n    host: 'es-host',\n    connectionClass: require('http-aws-es'),\n    amazonES: {\n      region: 'us-east-1',\n      credentials: new AWS.EnvironmentCredentials('AWS')\n    }\n  });\n//es.search()\n```. ",
    "paterninaisf": "The amazonES not work fine for me, its not set the amazon credential. Work for me of this way.\n```\nvar AWS = require('aws-sdk');\nAWS.config.update({\n    credentials: new AWS.Credentials(ACCESKEY, SECRETKEY),\n    region: 'us-east-1'\n});\nvar client = require('elasticsearch').Client({\n    host: 'https:..',\n    connectionClass: require('http-aws-es')\n});\n```\n. ",
    "sriharshakappala": "@skilledDeveloper I am getting \"No Living connections\" when following your solution. Where am I going wrong? Can someone help me?\n```js\nconst AWS = require('aws-sdk');\nconst config = require('../../configuration');\nconst elasticsearch = require('elasticsearch');\nconst esClient = new elasticsearch.Client({\n  host: config.get('elasticsearch.rental.host'),\n  log: config.get('elasticsearch.log_level'),\n  connectionClass: require('http-aws-es'),\n  amazonES: {\n    region: 'us-west-2',\n    credentials: new AWS.EnvironmentCredentials('AWS'),\n  },\n});\nmodule.exports = esClient;\n```. ",
    "rainnaren": "using this when i tried to use cognito unauthRole it is denying my request .a added the unauth role arn to elasticsearch policy as well\n. hi @jeskew,\nIam using angularjs and the version of angularjs is 1.5.3 and the version of aws-sdk is 2.166.0 ,cognito version is 1.28.0 for bundling iam using the gulp that will minify the bower files only not npm. var httpRequest = new AWS.HttpRequest(ElasticUrl +'/'+ elasticIndex, \"ap-northeast-1\");\n                httpRequest.method = \"POST\";\n      httpRequest.headers.host = \"lop3reyr72.execute-api.ap-northeast-1.amazonaws.com\";\n      httpRequest.headers['Content-Type'] = \"application/json\";\n var v4signer = new AWS.Signers.V4(httpRequest, \"execute-api\", true);\n      v4signer.addAuthorization(AWS.config.credentials, AWS.util.date.getDate());\nusing this i get the authorization header. here iam using Aws.signers which is in aws-sdk. ",
    "chazmo03": "Thank you @andreasherzog . @chrisradek, I'm going to try this suggestion and if it doesn't resolve it, I'll post a snippet.\n. ",
    "gregcope": "Thanks,\nWhere is the logic around the delay between attempts?  This is probably where the time is disappearing.\n. ",
    "optimisme": "Ok, it was a problem with my server. Sorry for the inconveniences and thanks for your help. \n. Ok, it was a problem with my server. Sorry for the inconveniences and thanks for your help. \n. Thanks for your help, my uploads are arround 100kilobytes and never bigger than 200. I modified the source so it looks like this:\ncase 's3':\n        params = {Key: pathFull, Body: objGzip};\n        self.loadText(pathFix, function (err, obj) {\n            if (err) {\n                exists = false;\n            }\n            upObj = self.s3bucket.upload(params, function (err, data) {\n                upObj = null;\n                if (exists || pathFix.indexOf(self.s3List) !== -1) {\n                    callback(err);\n                } else {\n                    self.unlink(pathFix.substr(0, pathFix.lastIndexOf('/')) + '/' + self.s3List, function (err) {\n                        callback(err);\n                    });\n                }\n            });\n            setTimeout(function () {\n                if (upObj !== null) {\n                    upObj.abort();\n                    callback({ code: 0, msg: 'Upload aborted by timeout'});\n                }\n            }, 1500);\n        });\n        break;\n. Thanks for your help, my uploads are arround 100kilobytes and never bigger than 200. I modified the source so it looks like this:\ncase 's3':\n        params = {Key: pathFull, Body: objGzip};\n        self.loadText(pathFix, function (err, obj) {\n            if (err) {\n                exists = false;\n            }\n            upObj = self.s3bucket.upload(params, function (err, data) {\n                upObj = null;\n                if (exists || pathFix.indexOf(self.s3List) !== -1) {\n                    callback(err);\n                } else {\n                    self.unlink(pathFix.substr(0, pathFix.lastIndexOf('/')) + '/' + self.s3List, function (err) {\n                        callback(err);\n                    });\n                }\n            });\n            setTimeout(function () {\n                if (upObj !== null) {\n                    upObj.abort();\n                    callback({ code: 0, msg: 'Upload aborted by timeout'});\n                }\n            }, 1500);\n        });\n        break;\n. This bug can be closed, looks like something is wrong with my local server. I can't reproduce it at openshift.\n. This bug can be closed, looks like something is wrong with my local server. I can't reproduce it at openshift.\n. Maybe this is helpful, I use the function with promises:\npromiseLoadText (path) {\n    return new Promise((resolve, reject) => {\n        return this.loadText(path, (err, rst) => { \n            if (err) { return reject(err) } else { return resolve(rst) } })\n    })\n}\n\nAnd the call: \ntry { txt = await promiseLoadText('/file.txt') } catch (e) { console.log(e) }. Hi @chrisradek,\nAt my main source, there is no difference using 'getObject' with \ncallbacks or with '.promise()' (allways from async functions, I can't change that)\nI readied an example reading a file from the filesystem and also from S3, \nthe memory only keeps growing when reading from S3 (using the same file)\nThis is running on node 7.7\nWhen I run 'top' while executing the script these are the results:\n\nMemory stays at 1.1% for node 7.7 reading from filesystem\nMemory KEEPS GROWING for node 7.7 reading from S3\n\nGrows over 5% waiting some hours with 'callback' or with '.promise()'\nThis is the example:\n(call 'promiseLoadFile', 'promiseLoadS3' or 'promiseLoadS3Callback' from 'call' function)\n// main.js ---------------------------------------------------------------------\nvar \n    aKey        = '...',\n    sKey        = '...',\n    buck        = '...',\n    path        = '...'\n\nvar\n    aws         = require('aws-sdk'),\n    https       = require('https'),\n    agent       = new https.Agent({ maxSockets: Infinity }),\n    s3bucket    = null,\n    fs          = require('fs')\n\nfunction promiseLoadFile () {\n\n    return new Promise(async (resolve, reject) => {\n        let path = './file.json.gz',\n            data = null\n\n        fs.readFile(path, function (err,data) {\n            if (err) { return reject(err) }\n            return resolve(data)\n        })\n    })\n}\n\nfunction promiseLoadS3 () {\n\n    return new Promise(async (resolve, reject) => {\n        let params = {Key: path},\n            data = null\n\n        try {\n            data = await s3bucket.getObject(params).promise()\n        } catch (err) {\n            return reject(err)\n        }\n\n        return resolve(data)\n    })\n}\n\nfunction promiseLoadS3Callback () {\n\n    return new Promise(async (resolve, reject) => {\n        let params = {Key: path},\n            data = null\n\n        s3bucket.getObject(params, (err, data) => {\n            if (err) { return reject(err) }\n            return resolve(data)\n        })\n    })\n}\n\nasync function call () {\n\n    let cnt = 0,\n        rst = null\n\n    aws.config.update({sslEnabled: true, accessKeyId: aKey, secretAccessKey: sKey, region: 'us-east-1', httpOptions: { agent: agent, timeout: 1000000 }})\n    s3bucket = new aws.S3({ params: { Bucket: buck }})\n\n    try {\n        for (cnt = 10000000000; cnt >= 0; cnt = cnt - 1) {\n            rst = await promiseLoadS3()\n            console.log(cnt)\n        }\n    } catch (err) {\n        console.log(err)\n    }\n}\n\ncall(). Hi @chrisradek,\n\nYes I tested it without 'async' and 'await' and it also KEEPS GROWING when calling S3, but more slowly. (Does not happen with the filesystem)\nThis is with node 7.7\nAnd its example:\n// mainSync.js -----------------------------------------------------------------\nvar \n    aKey        = '...',\n    sKey        = '...',\n    buck        = '...',\n    path        = '...'\n\nvar\n    aws         = require('aws-sdk'),\n    https       = require('https'),\n    agent       = new https.Agent({ maxSockets: Infinity }),\n    s3bucket    = null,\n    fs          = require('fs')\n\naws.config.update({sslEnabled: true, accessKeyId: aKey, secretAccessKey: sKey, region: 'us-east-1', httpOptions: { agent: agent, timeout: 1000000 }})\ns3bucket = new aws.S3({ params: { Bucket: buck }})\n\nfunction syncLoadFile (resolve, reject) {\n\n    let path = './file.json.gz',\n        data = null\n\n    fs.readFile(path, function (err,data) {\n        if (err) { return reject(err) }\n        return resolve(data)\n    })\n}\n\nfunction syncLoadS3 (resolve, reject) {\n\n    let params = {Key: path},\n        data = null\n\n    s3bucket.getObject(params, (err, data) => {\n        if (err) { return reject(err) }\n        return resolve(data)\n    })\n}\n\nasync function call (cnt) {\n\n    if (cnt >= 0) {\n\n        syncLoadS3(\n            (rst) => {\n                console.log(cnt)\n                setTimeout(() => { return call(cnt - 1) }, 0)\n            },\n            (err) => {\n                console.log(cnt, 'ERROR')\n                setTimeout(() => { return call(cnt - 1) }, 0)\n            }\n        )\n    }\n}\n\n\ncall(10000000000). Any news about this issue?.\n",
    "ig16022": "Hi guys,\nCan anyone confirm that the 'abort' method of AWS.S3.ManagedUpload class should be working in the browser environment now? The official documentations states:\nMethod Details\nabort() \u21d2 void\nNote: This feature is not supported in the browser environment of the SDK.\nIf this is the case, what are our options for abort / resume? \n. ",
    "franleplant": "The abort function is not working for me two, tested with the latest SDK.\nThe file keeps uploading as inspected in the Network tab.\nTested in Chrome\n. The abort function is not working for me two, tested with the latest SDK.\nThe file keeps uploading as inspected in the Network tab.\nTested in Chrome\n. ",
    "Villa41": "The abort function is not working and also I've tested with the latest SDK... The file keeps uploading and callback error is \"Error: RequestAbortedError Request aborted by user\"\nTested in Chrome\nThere is someone that knows another option to abort the upload?\n. The abort function is not working and also I've tested with the latest SDK... The file keeps uploading and callback error is \"Error: RequestAbortedError Request aborted by user\"\nTested in Chrome\nThere is someone that knows another option to abort the upload?\n. I'm testing in \nChrome Versi\u00f3n 48.0.2564.97 (64-bit)\nOS Ubuntu 15.10, 64 bits\n. I'm testing in \nChrome Versi\u00f3n 48.0.2564.97 (64-bit)\nOS Ubuntu 15.10, 64 bits\n. Hi everyone,\nI was testing the abort method and i found the problem. In my case, it's with the file.\nWhen the size is less than 5 MB the mothod doesn't work  but there is no problem with a bigger file. \nI tried with a files 6,8 MB and 14MB.\n. Hi everyone,\nI was testing the abort method and i found the problem. In my case, it's with the file.\nWhen the size is less than 5 MB the mothod doesn't work  but there is no problem with a bigger file. \nI tried with a files 6,8 MB and 14MB.\n. ",
    "samsamm777": "@chrisradek  I am using the 2.2.9 browser version.\n. So it appears the AutoScaling service is not included in the default sdk. So I built my own full sdk with all the services. However the other services do not support CORS. So now im stuck....\n. ",
    "andrey-bahrachev": "It has to do with IE's security settings, namely 'Access data sources across domains' setting - it's disabled by default. Say hi to MS.\n. Ah, found a way around using this in the upload progress callback.\njavascript\ns3.upload(params, function (err, data) {\n    ...\n}).on('httpUploadProgress', function(progress) {\n    // Here you can use `this.body` to determine which file this particular\n    // event is related to and use that info to calculate overall progress.\n});\nI hope this will help someone else.\nThis wasn't obvious though and it still would be nice to have an easier way of doing this.\n. @chrisradek Thanks for quick response.\nThat would be awesome!\n. Brilliant! Thanks a lot!\n. @rossthedevigner Oh, it's been a while, so chances are that things might have changed since then, but here it is:\n/**\n * Uploads file(s) directly to AWS S3.\n * \n * @param form -  jQuery object representing the form.\n * @param {array} files -  Files to be uploaded (derived from HTML5 File API).\n * @param successCallback - Callback function to be called upon successful file(s) upload.\n */\nuploadFiles: function(form, files, successCallback) {\n    var filesData = [];\n    var sizeTotal = 0;\n    var loaded = [];\n    for (var i = 0; i < files.length; i++) {\n        filesData.push({name: files[i].name, size: files[i].size, type: files[i].type});\n        sizeTotal += files[i].size;\n        loaded[files[i].name] = 0;\n    }\n    app.checkStorage(form, filesData, function(resp) {\n        // Initialize the Amazon Cognito credentials provider.\n        AWS.config.region = resp.cognitoRegion;\n        AWS.config.sslEnabled = true;\n        AWS.config.credentials = new AWS.CognitoIdentityCredentials({IdentityPoolId: resp.cognitoPoolId});\n        // Initialize S3 service handler.\n        var s3 = new AWS.S3({params: {Bucket: resp.bucket}});\n        var uploadBar = form.find('.upload-bar');\n        var uploadBarMeter = uploadBar.find('div');\n        uploadBar.removeClass('hidden');\n        for (var i = 0, filesLen = files.length; i < filesLen; i++) {\n            var params = {Key: resp.files[i].key, ContentType: files[i].type, Body: files[i], ACL: 'private'};\n            s3.upload(params, function (err, data) {\n                if (err) {\n                    yii.app.activateForm(form);\n                    swal('File upload error');\n                    return false;\n                }\n            }).on('httpUploadProgress', function(e) {\n                loaded[this.body.name] = e.loaded;\n                var loadedTotal = 0;\n                for (var j in loaded) {\n                    loadedTotal += loaded[j];\n                }\n                var progress = Math.round(loadedTotal / sizeTotal * 100);\n                uploadBarMeter.css({width: progress + '%'});\n                if (loadedTotal === sizeTotal) {\n                    successCallback(form, {files: resp.files, form: form.serialize()});\n                }\n            });\n        }\n    });\n},. ",
    "pchuri": "@chrisradek I'lll try and give you a feedback. Thank you for answer. \n. @chrisradek I solved it. Thank you your answer. \n. ",
    "v71017": "Hi , I am also facing a similar issue .\nbotocore.errorfactory.InvalidArgumentException: An error occurred (InvalidArgumentException) when calling the CreateDeliveryStream operation: Firehose is unable to assume role arn:aws:iam::AWS_ACCOUNT_ID:role/FirehoseRoleArn. Please check the role provided.\nCan someone Help me \nmy python code is \nfirehose_policy_doc = {\n            \"Version\": \"2012-10-17\",\n            \"Statement\": [\n                 { \n                    \"Effect\": \"Allow\",\n                   \"Action\": [\n                        \"\"\n                    ],\n                    \"Resource\": [\n                        \"\"\n                    ]\n                },\n            {   \n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"kinesis:DescribeStream\",\n                    \"kinesis:GetShardIterator\",\n                    \"kinesis:GetRecords\",\n                    \"kinesis:PutRecord\",\n                    \"kinesis:PutRecords\"\n                ],\n                \"Resource\": event['stream-arn']\n            },\n            {   \n                \"Effect\": \"Allow\",\n               \"Action\": [\n                    \"s3:*\"\n                ],\n                \"Resource\": [\n                    \"*\"\n                ]\n            },\n            {   \n                \"Effect\": \"Allow\",\n               \"Action\": [\n                    \"lambda:*\"\n                ],\n                \"Resource\": [\n                    \"*\"\n                ]\n            },\n            {   \n                \"Effect\": \"Allow\",\n               \"Action\": [\n                    \"logs:*\"\n                ],\n                \"Resource\": [\n                    \"*\"\n                ]\n            }\n        ]\n    }\n\n    firehose_policy_response = iam.create_policy(\n        PolicyName='FirehoseRolePolicy',\n        PolicyDocument=json.dumps(firehose_policy_doc),\n        Description='FirehoseRolePolicy'\n    )\n\n    firehosePolicyArn = firehose_policy_response['Policy']['Arn']\n\n    log.info ('creating policy ends')\n    log.info (firehosePolicyArn)\n\n\n    firehose_role =  {\n\n                \"Statement\": [\n                    {\n                        \"Action\": [\n                            \"sts:AssumeRole\"\n                        ],\n                        \"Condition\": {\n                            \"StringEquals\": {\n                                \"sts:ExternalId\": \"AWS_ACCOUNT_ID\"\n                            }\n                        },\n                        \"Effect\": \"Allow\",\n                        \"Principal\": {\n                            \"Service\": \"firehose.amazonaws.com\"\n\n                        }\n                    }\n                ]\n            }\n\n    firehoseRoleResponse = iam.create_role(\n        RoleName='FirehoseRoleArn',\n        AssumeRolePolicyDocument=json.dumps(firehose_role)\n    )\n\n    iam.attach_role_policy(\n    RoleName='FirehoseRoleArn',\n    PolicyArn=firehosePolicyArn\n    )\n\n\nexcept:\n    traceback.print_exc()\n    firehoseRoleResponse = iam.get_role(\n        RoleName='FirehoseRoleArn'\n    )\n\nroleArn = firehoseRoleResponse['Role']['Arn']\n\ntry:\n    response = firehose.create_delivery_stream(\n        DeliveryStreamName=streamname,\n        DeliveryStreamType='KinesisStreamAsSource',\n        KinesisStreamSourceConfiguration ={\n                            'KinesisStreamARN': event['stream-arn'],\n                            'RoleARN' : roleArn\n                        },\n        SplunkDestinationConfiguration={\n            'HECEndpoint': hec_endpoint,\n            'HECEndpointType': hec_event_type,\n            'HECToken': hec_token,\n            'HECAcknowledgmentTimeoutInSeconds': HEC_ACK_TIMEOUT_SECONDS,\n            'RetryOptions': {\n                'DurationInSeconds': HEC_RETRY_DURATION_SECONDS\n            },\n            'S3BackupMode': 'FailedEventsOnly',\n            'S3Configuration': {\n                'RoleARN': firehose_role_arn,\n                'BucketARN': 'arn:aws:s3:::' + backsplash_name,\n                'Prefix': backsplash_prefix,\n                'BufferingHints': {\n                    'SizeInMBs': 1,\n                    'IntervalInSeconds': 60\n                },\n                'CompressionFormat': 'UNCOMPRESSED',\n                'CloudWatchLoggingOptions': {\n                    'Enabled': True,\n                    'LogGroupName': '/aws/kinesisfirehose/' + streamname,\n                    'LogStreamName': 'S3Delivery'\n                }\n            },\n            'ProcessingConfiguration': {\n                'Enabled': True,\n                'Processors': [\n                    {\n                        'Type': 'Lambda',\n                        'Parameters': [\n                            {\n                                'ParameterName': 'LambdaArn',\n                                'ParameterValue': processing_lambda_func_arn\n                            },\n                            {\n                                'ParameterName': 'NumberOfRetries',\n                                'ParameterValue': '3'\n                            },\n                            {\n                                'ParameterName': 'RoleArn',\n                                'ParameterValue': firehose_role_arn\n                            },\n                            {\n                                'ParameterName': 'BufferSizeInMBs',\n                                'ParameterValue': '1'\n                            },\n                            {\n                                'ParameterName': 'BufferIntervalInSeconds',\n                                'ParameterValue': '60'\n                            }\n                            ]\n                    }\n                ]\n            },\n            'CloudWatchLoggingOptions': {\n                'Enabled': True,\n                'LogGroupName': '/aws/kinesisfirehose/' + streamname,\n                'LogStreamName': 'SplunkDelivery'\n            }\n        }\n    )\n\n. ",
    "southpolesteve": "@trevorrowe The docs for this directly in the rest api are here: http://docs.aws.amazon.com/apigateway/api-reference/link-relation/restapi-resources/\nI believe you have to set embed=methods in the query string. I will add an example later this afternoon\n. @trevorrowe @chrisradek I've just submitted a PR for this: https://github.com/aws/aws-sdk-js/pull/869\n. @chrisradek any updates? I've got a downstream library waiting for this change. Currently running on my now out of date patched version.\n. No worries. Thanks for the update!\n. @phsstory @tristanls I can confirm this work around has resolved problems in multiple production systems for us. When using with Lambda I agree that keepAlive: true should not be used.\nHere is a graph of errors/hour across all our lambda functions. You can see where the fix happened :)\n\n. ",
    "dankantor": "@chrisradek when I try that workaround I get this error:\nInvalidQueryStringException: null\n. Ok just figured out my bug, I already had limit set so just needed to add it this way req.httpRequest.path += '&embed=methods';\n. ",
    "cagataygurturk": "Unfortunately i could not do it. Can you explain how to overwrite the installed sdk please?\n. Yes i did like you said and really require() loaded the newer package from zip file. Thanks.\n. ",
    "raghunat": "@chrisradek \nThanks for the response! Yes the documentation then was very unclear :-)\nWould it then be something like this?\njavascript\ngate.putIntegration({\n     httpMethod:'POST', \n     resourceId: '8oqq0x', \n     restApiId: 'r3wraomlb6', \n     type: 'AWS',\n     integrationHttpMethod: 'POST',\n     uri: 'arn:aws.apigateway:us-east-1:lambda:path/2015-03-31/functions/myFunctionName',\n     credentials: 'myFullyQualifiedLambdaRole'\n}, console.log);\n. ",
    "billyjf": "The following line from the serverless application model examples helped me resolve this same issue, the uri is pretty tricky:\nhttps://github.com/awslabs/serverless-application-model/blob/master/examples/2016-10-31/api_swagger_cors/swagger.yaml#L23. ",
    "kyleseely": "Passing the correct md5 of the file to upload shouldn't cause a BadDigest error. The sdk should understand that the user wants to pass along the md5 of the file and calculate it correctly for each part that's uploaded. This took me days to figure out what was going on and I don't want that to happen to other users. \n. Passing the correct md5 of the file to upload shouldn't cause a BadDigest error. The sdk should understand that the user wants to pass along the md5 of the file and calculate it correctly for each part that's uploaded. This took me days to figure out what was going on and I don't want that to happen to other users. \n. Then perhaps an InvalidParameter error should be thrown when providing ContentMD5 to upload a large file. It drove me nuts that I was getting a BadDigest error when the md5 I provided was indeed correct for the file.\n. Then perhaps an InvalidParameter error should be thrown when providing ContentMD5 to upload a large file. It drove me nuts that I was getting a BadDigest error when the md5 I provided was indeed correct for the file.\n. ",
    "joshuajabbour": ":+1: to what @kyleseely says. This is insanely frustrating. InvalidDigest is such a confusing error here...\n. :+1: to what @kyleseely says. This is insanely frustrating. InvalidDigest is such a confusing error here...\n. ",
    "gregorskii": "@chrisradek I will try without the ORM.\nThanks\n. Yes this appears to work. Will update if I find any issues.\nThanks!\n. ",
    "davidmaxwaterman": "Is this issue fixed? Almost 2 years old...and I'm having trouble getting 'resume' functionality to work (it always seems to start from the beginning again, despite setting leavePartsOnError).. Is this issue fixed? Almost 2 years old...and I'm having trouble getting 'resume' functionality to work (it always seems to start from the beginning again, despite setting leavePartsOnError).. I am also wondering if the 'leavePartsOnError' could be used for a 'pause/resume' feature?. I am also wondering if the 'leavePartsOnError' could be used for a 'pause/resume' feature?. I was wrong about the server...never mind.. I was wrong about the server...never mind.. I'm using chrome on Windows; yes all this is client-side.\nThanks for the explanation of what is going on with the logic. I thought my bucket is in eu-west-1 (Ireland), but perhaps I am wrong (I'm a front-end guy so I don't know too much about the back end).. I'm using chrome on Windows; yes all this is client-side.\nThanks for the explanation of what is going on with the logic. I thought my bucket is in eu-west-1 (Ireland), but perhaps I am wrong (I'm a front-end guy so I don't know too much about the back end).. The internet connection might possibly be slow. I can only typically get it to work with a single queue (ie 1), and with the smallest chunk/part size.\nI'll try removing the timeout and see how that works...it sounds like it might be the best option for me, since I suspect the SHA might actually serve a purpose :). The internet connection might possibly be slow. I can only typically get it to work with a single queue (ie 1), and with the smallest chunk/part size.\nI'll try removing the timeout and see how that works...it sounds like it might be the best option for me, since I suspect the SHA might actually serve a purpose :). I'll close this and continue in #1548 - they're probably at least closely related.. I'll close this and continue in #1548 - they're probably at least closely related.. Thanks for looking into this. I'm using Chrome on Windows 10:\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36. Thanks for looking into this. I'm using Chrome on Windows 10:\nMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36. I'm not too sure what you mean. If I don't do SHA calculations, then they usually succeed...and I just tested a 10MiB file 6MiB chunks, one at a time, and chunk#1 took 56.42s and #2 took 38.56s (using the numbers from chrome's network panel).\nIf I restrict the bandwidth using chrome's network panel, it slows down - 'Good 3G' is 1.1m + 43.95s; GPRS causes failures at 2 mins, and after 4 attempts, overall failure (including a final 4 DELETE requests that result in '403/Forbidden' for some reason - I guess I don't have permission to delete).. I'm not too sure what you mean. If I don't do SHA calculations, then they usually succeed...and I just tested a 10MiB file 6MiB chunks, one at a time, and chunk#1 took 56.42s and #2 took 38.56s (using the numbers from chrome's network panel).\nIf I restrict the bandwidth using chrome's network panel, it slows down - 'Good 3G' is 1.1m + 43.95s; GPRS causes failures at 2 mins, and after 4 attempts, overall failure (including a final 4 DELETE requests that result in '403/Forbidden' for some reason - I guess I don't have permission to delete).. At per #1545, removing the timeout seems to improve things immensely - I've removed the SHA disable option so it is enabled again. However, if I throttle to GPRS, it doesn't fail in the same way: part 1 uploaded ok taking 41 mins, but part 2 had four net::ERR_CONNECTION_ABORTED failures after 22.5 mins, 22.5 mins, 25 mins, and 22.5 mins (followed by the usual four 403/Forbidden attempts). If that third one was also 22.5 instead of 25, I'd be tempted to think I had hit another timeout somewhere.\nAnyway, I think I'll leave it this way since it seems to covers more connection attempts.\nI wonder what your thoughts are. What is the purpose of the 2 min timeout?. At per #1545, removing the timeout seems to improve things immensely - I've removed the SHA disable option so it is enabled again. However, if I throttle to GPRS, it doesn't fail in the same way: part 1 uploaded ok taking 41 mins, but part 2 had four net::ERR_CONNECTION_ABORTED failures after 22.5 mins, 22.5 mins, 25 mins, and 22.5 mins (followed by the usual four 403/Forbidden attempts). If that third one was also 22.5 instead of 25, I'd be tempted to think I had hit another timeout somewhere.\nAnyway, I think I'll leave it this way since it seems to covers more connection attempts.\nI wonder what your thoughts are. What is the purpose of the 2 min timeout?. Thanks for the suggestions.\nI used bower install.. Thanks for the suggestions.\nI used bower install.. Actually, I think I'm wrong about the size :\nbower_components/aws-sdk/dist$ du -sh aws-sdk.js\n857M    aws-sdk.js\nbower_components/aws-sdk/dist$ wc -c aws-sdk.js\n1749732 aws-sdk.js\nWhat do you make of that?!\nls -l confirms the wc -c output.\nIt seems du  is playing with my mind. I am inclined to blame WSL....indeed, there are a few issues, including:\nhttps://github.com/Microsoft/BashOnWindows/issues/1894\nSo, I guess 'nothing to see here' :) Sorry for the noise.. Actually, I think I'm wrong about the size :\nbower_components/aws-sdk/dist$ du -sh aws-sdk.js\n857M    aws-sdk.js\nbower_components/aws-sdk/dist$ wc -c aws-sdk.js\n1749732 aws-sdk.js\nWhat do you make of that?!\nls -l confirms the wc -c output.\nIt seems du  is playing with my mind. I am inclined to blame WSL....indeed, there are a few issues, including:\nhttps://github.com/Microsoft/BashOnWindows/issues/1894\nSo, I guess 'nothing to see here' :) Sorry for the noise.. I do get an error regarding region, but I don't have a 'CognitoIdentityCredentials'. My code just has AWS.Credentials and AWS.Config. I've tried the former because it sounds like CognitoIdentityCredentials, but it turned to be in the latter, AWS.Config, that I had to add region: 'eu-west-1',, which kind of seems to contradict @villelahdenvuo's observation.\nFWIW, the error response I had was:\n<Error>\n  <Code>AuthorizationHeaderMalformed</Code>\n  <Message>The authorization header is malformed; the region 'us-east-1' is wrong; expecting 'eu-west-1'</Message>\n  <Region>eu-west-1</Region>\n  <RequestId>snip</RequestId>\n  <HostId>snip</HostId>\n</Error>\nI didn't know what the authorization header was...so had to resort to trial and error.\nNB, my code previously worked fine in 2.67.0, from which I had recently upgraded due to having to make a new clone (and this a new 'bower install').. ",
    "dhatawesomedude": "thanks @chrisradek \nWhen I set s3.config.httpOptions.xhrWithCredentials = false, I see my credentials successfully, but S3 doesn't return Access-control header, but when I set xhrWithCredentials = true I get the wrong wildcard header. I get a token which I use for my cognito credentials.\nmy code is below. I get a temprorary accessToken from the server which I use on my browser. I use aws-js-sdk on both server and client\n``` javascript\n//server code - controller\n    var AWS = require('aws-sdk');\n    var winston = require('winston');\n    var identityPoolId = 'us-east-1:dskjfdksjdk-sdkjk';\n    var accessKeyId = 'sdjk';\n    var secretAccessKey ='sdjdsk';\n    var user_name = 'sdfd';\n    var region = 'us-east-1';\n    var apiVersion = '2014-06-30';\n    var s3Config = {bucketName : 'sss', key : '...', uploadId : '', region : 'singapore'};\n    var awsConfig = {region : 'us-east-1'};\n    var identityConfig = {region : 'us-east-1'};\n//AWS.config.region = 'us-east-1';\nvar awsService = function(data){\n  if (data)\n    this.data = data;\n  return this;\n};\nawsService.prototype._newCognitoIdentity = function(){\n  return new AWS.CognitoIdentity({\n    apiVersion: apiVersion,\n    accessKeyId : accessKeyId,\n    secretAccessKey : secretAccessKey,\n    region : region\n  });\n};\nawsService.prototype._getIdentityPoolId = function(){\n  return identityPoolId;\n};\nawsService.prototype.getTokenForDeveloperIdentity = function(userId, tokenDuration, done){\n  var poolId = this._getIdentityPoolId();\n  s3Config.uploadId = userId;\nvar params = {\n    IdentityPoolId: poolId,\n    Logins: {\n      'aws-cog.sel.com': userId\n    },\n    TokenDuration: tokenDuration\n  };\nthis._newCognitoIdentity().getOpenIdTokenForDeveloperIdentity(params, function(err, data) {\n    if (err) {\n      winston.log('error','getTokenForDeveloperIdentity error', err, err.stack);\n      done(true, err);\n    }\n    else{\n      data.IdentityPoolId = poolId;\n      data.s3Config = s3Config;\n      data.awsConfig = awsConfig;\n      data.identityConfig = identityConfig;\n      winston.log('info','getTokenForDeveloperIdentity successful for user : ', userId);\n      done(null, data);\n    }\n  });\n};\nmodule.exports.awsService = new awsService();\n// server - route\nvar get_token_for_photo_upload = function(req, res){\n  winston.log('info', 'get_token_for_photo_upload called : ', req.params);\n  utils.get_user_from_req(req, function(user_obj) {\n    controllers('aws').awsService.getTokenForDeveloperIdentity(user_obj._id.toString(),60*60, function(error, data){\n      if(error){\n        throw errors.ERR_GENERIC;\n      }\n      else{\n        res.json({\n          token : user_obj.id.toString(),\n          IdentityId : data.IdentityId,\n          tempToken : data.Token,\n          s3Config : data.s3Config,\n          identityConfig : data.identityConfig,\n          awsConfig : data.awsConfig,\n          IdentityPoolId : data.IdentityPoolId,\n          status : true\n        })\n      }\n    });\n  });\n};\n```\n``` javascript\n//client code\nreturn {\n      aws_init : function(done){\n        var _parentScope = this;\n        AWS.config = new AWS.Config();\n        var s3Config = {};\n       //get_token makes a call to the server to get the token.\n        this.get_token(function(err, result){\n          if(err)\n            done(true);\n          else {\n           var s3 = _parentScope.s3_init(result);\n            done(null, s3, result);\n          }\n        })\n      },\n      s3_init : function(data){\n        var s3Config = data.s3Config;\n        AWS.config.region = data.awsConfig.region;\n        var s3 = new AWS.S3({params: {Bucket: s3Config.bucketName, region : s3Config.region}});\n        s3.config.credentials = new AWS.CognitoIdentityCredentials({\n          IdentityPoolId: data.IdentityPoolId,\n          Logins : {},\n          IdentityId: data.IdentityId\n        });\n        s3.config.region = data.s3Config.region;\n        s3.config.credentials.params.Logins['cognito-identity.amazonaws.com'] = data.tempToken;\n        s3.config.httpOptions.xhrWithCredentials = true;\n        s3.config.paramValidation = true;\n        s3.config.signatureCache = false;\n        console.log(s3);\n        return s3;\n      },\n      s3_upload : function(file, done){\n        console.log(typeof file);\n        this.aws_init(function(err, s3, result){\n          var params = {Bucket : 'ekjern',Key: 'sdfkjkd', ContentType: file.type, Body : file};\n          s3.putObject(params, function(err, data) {\n            console.log(err, data);\n          });\n        });\n      }\n}\n``\n. so basically on the server I callgetTokenForDeveloperIdentity()` pass a token to the browser where I use the token to generate a new cognitoIdentity cred: \njavascript\nvar s3 = new AWS.S3({params: {Bucket: s3Config.bucketName, region : s3Config.region}});\n        s3.config.credentials = new AWS.CognitoIdentityCredentials({\n          IdentityPoolId: data.IdentityPoolId,\n          Logins : {'cognito-identity.amazonaws.com' : 'token_from_server'},\n          IdentityId: data.IdentityId\n        });\nAm I doing it the wrong way? @chrisradek \n. thanks @chrisradek I resolved this issue. It was an issue with headers received after initial preflight request.\n. Thanks @LiuJoyceC \nUpdating my version fixed this issue, but I still cannot call .promise on s3.upload(). The promise property is undefined. Any idea how to fix that?\n. ",
    "Prinzhorn": "This unwritten law kicked in again, where you get closer to the issue once you ask for help. It seems like ContentType is ignored, albeit the signature is valid. So how do I sign a POST/PUT request with all parameters except the file itself without having to duplicate the parameters again when making the request? I mean getSignedUrl doesn't seem like the right choice for putObject, is it? I guess I have to supply all the parameters separate from the URL and only need the signature itself.\n. Thanks for the quick reply.\n\nIf it is included in getSignedUrl however, it is essentially enforcing that Cache-Control must be sent with the same value in the request.\n\nI cannot confirm what you said. Given my demo code above, if I sign the request and include CacheControl:'max-age=1337' and then upload the image using curl -T unsplash.jpg -H 'Cache-Control:max-age=1337' the-url I get an invalid signature error. If I exclude the CacheControl from the getSignedUrl params and only send it along with the request then it is applied correctly. So it seems that enforcing a certain Cache-Control header is currently not possible.\nEdit: same with ContentType, which is completely ignored in the signed url (the upload request can set whatever it wants)\n. > Just to confirm, you've been testing in a V4 region only, correct? (i.e. eu-central-1)\nYes, my buckets are all eu-central-1\n. ",
    "markstos": "I'm using 2.2.16, and the syntax of this.requestId returns undefined. I tried this:\n``` javascript\nvar s3 = new AWS.S3();\ns3.getObject({Bucket: 'bucket', Key: 'invalidKey'}, function (err, data) {\n  if (err) {\n    console.log('RequestId: ' + this.requestId);\n    console.log('Err:  %j', err);\n    console.log('Data:  %j', data);\n  }\n});\n```\nEasy access to the requestId is important, as the official docs state the request ID is key to getting help from Amazon support when requests fail.\n. @chrisradek Thanks for the response. Here's the output of the command above. I've added a line to dump out this with console.log.\n{ request: \n   { domain: null,\n     service: { config: [Object], isGlobalEndpoint: false, endpoint: [Object] },\n     operation: 'getObject',\n     params: { Bucket: 'bucket', Key: 'invalidKey' },\n     httpRequest: \n      { method: 'POST',\n        path: '/',\n        headers: [Object],\n        body: '',\n        endpoint: [Object],\n        region: 'us-east-1' },\n     startTime: Thu Nov 12 2015 17:42:07 GMT-0500 (EST),\n     response: [Circular],\n     _asm: { currentState: 'complete', states: [Object] },\n     _haltHandlersOnError: false,\n     _events: \n      { validate: [Object],\n        afterBuild: [Object],\n        restart: [Object],\n        sign: [Object],\n        validateResponse: [Object],\n        send: [Object],\n        httpHeaders: [Object],\n        httpData: [Object],\n        httpDone: [Object],\n        retry: [Object],\n        afterRetry: [Object],\n        build: [Object],\n        extractData: [Object],\n        extractError: [Object],\n        httpError: [Object],\n        beforePresign: [Object],\n        complete: [Object] },\n     emit: [Function: emit] },\n  data: null,\n  error: \n   { [TimeoutError: Missing credentials in config]\n     message: 'Missing credentials in config',\n     code: 'CredentialsError',\n     time: Thu Nov 12 2015 17:42:08 GMT-0500 (EST),\n     originalError: \n      { message: 'Could not load credentials from any providers',\n        code: 'CredentialsError',\n        time: Thu Nov 12 2015 17:42:08 GMT-0500 (EST),\n        originalError: [Object] } },\n  retryCount: 0,\n  redirectCount: 0,\n  httpResponse: \n   { statusCode: undefined,\n     headers: {},\n     body: undefined,\n     streaming: false,\n     stream: null },\n  maxRetries: 3,\n  maxRedirects: 10 }\nRequestId: undefined\nErr:  {\"message\":\"Missing credentials in config\",\"code\":\"CredentialsError\",\"time\":\"2015-11-12T22:42:08.386Z\",\"originalError\":{\"message\":\"Could not load credentials from any providers\",\"code\":\"CredentialsError\",\"time\":\"2015-11-12T22:42:08.386Z\",\"originalError\":{\"message\":\"Connection timed out after 1000ms\",\"code\":\"TimeoutError\",\"time\":\"2015-11-12T22:42:08.385Z\"}}}\nData:  null\n. @chrisradek,\nI was following the exact  syntax that had been suggested above:\nhttps://github.com/aws/aws-sdk-js/issues/781#issuecomment-154499642\nI updated my code to trigger an \"Access Denied\" case, and I confirmed I got \"this.requestId\" back for both a getObject call and a putObject call. \nWhere is this existence of this requestId documented? I would have expected to find a mention of it any or all of these pages, but a search for \"requestId\" on any of these pages finds no results. \nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Request.html\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3/ManagedUpload.html\n. @chrisradek, while requestId is populated for getObject and putObject, I found it is not available in the callback passed to s3.upload. That's the method I actually need to debug. \n. In case it's of interest: I go to the bottom of my \"Access Denied\" issue by disabling SSL on the requests and sending them through the Charles Debug Proxy so I could see the raw HTTP requests and responses. (I was hoping the \"logger\" output might show that, but it doesn't). \nThe root cause that was that I had a Key started with a \"/\", wich I presumed was correct. However, this resolved in a double-slash in the final URL, which caused prefix-pattern-matching rules to fail. \nIt seems the question of leading-slash-or-not has caused problems for developers using a variety of frontends. Guidance on this point would be welcome in the JavaScript SDK docs would be welcome as well. \nRef: https://encrypted.google.com/search?hl=en&q=should%20AWS%20S3%20keys%20start%20with%20a%20slash%3F#hl=en&q=AWS+S3+key+%22leading+slash%22\n. ",
    "yhahn": "@chrisradek can we send you donuts?\n. ",
    "AllanFly120": "Hi @onassar \nThis thread may help you #1688 . Oh sorry, you are using upload instead of putObject.  Because upload calls a serial of operations under the hood, the requestId is unavailable intentionally. If you want to get access to requestId for each of the operations, the quickest way is to use customizeRequests.\n```javascript\nvar AWS = require('aws-sdk')\nAWS.S3.prototype.customizeRequests(function(request) {\n    request.on('extractData', function(response){\n        console.log('success operation: ', response.request.operation, 'requestId: ', response.requestId)\n    });\n    request.on('extractError', function(response){\n        console.log('fail operation: ', response.request.operation, 'requestId: ', response.requestId)\n    });\n})\nvar s3 = new AWS.S3({region: 'us-west-2'});\ns3.upload(uploadParams, function(e, d){\n    if(!e) {\n        console.log(d);\n    } else {\n        console.log(e);\n    }\n})\n``\nI hope this will help you. hey @paulfryer The S3 Service team still doesn't add CORS support forCreateBucket. One workaround is that you can set up a proxy server to relay the requests. You could post on [AWS S3 forum](https://forums.aws.amazon.com/forum.jspa?forumID=24) to request this feature.. Closing the old issues. Since the issue seems to already been solved by @chrisradek response. If the issue still exists please open a new issue.. Already merged another PR using headless chrome.. Hi @adamdry, Do you mind sharing your current tsconfig?\n. Hi @adamdry, Do you mind sharing your current tsconfig?\n. Can you log the error message of S3.upload() function? Also, can you share your CORS configuration?. It seems that 2 configurations are missing in your CORS configuration. First, you need to expose your ETag header so that the upload function can track which part it's going to upload. See [this](http://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html). Basically, inS3.upload()method, if the file size exceeds 5mb, it will be chopped into several 5mb parts. The SDK needs ETag in response headers to keep track on which part is uploaded successfully. But your CORS configuration doesn't expose ETag now.\nThen, you need to allow DELETE method in your configuration.  Since the upload method didn't go through, the SDK will need DELETE method to clean up broken parts. This is why you getAccessForbidden` error.\nHere is an example of what the CORS configuration may look like:\nxml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n<CORSRule>\n    <AllowedOrigin>*</AllowedOrigin>\n    <AllowedMethod>GET</AllowedMethod>\n    <AllowedMethod>PUT</AllowedMethod>\n    <AllowedMethod>POST</AllowedMethod>\n    <AllowedMethod>DELETE</AllowedMethod>\n    <ExposeHeader>ETag</ExposeHeader>\n    <AllowedHeader>*</AllowedHeader>\n</CORSRule>\n</CORSConfiguration>\nLet me know if you have further error messages.. @usamamashkoor Your upload progress drops due to file parts upload failed (possibly because of the time out). One way to solve this problem is to set a longer timeout in config so that the upload requests have more time to get through. Another way is to adjust your queue size setting, which is the number of parts concurrently uploaded. So that the upload process may be faster but browser usually has a restriction on concurrent requests at the same time.. @usamamashkoor Many reasons can lead to network failure. Do you mind show me your network tab, the header information of your failed request as well as the respond? . Due to no update in this thread, it will be closed.. Due to no update in this thread, it will be closed.. @jeskew . @jeskew . Hi, it seems like you are looking for waitfor().  This method can check several states every 5 seconds. In this case, the state is 'objectExists'. Here is an example for waiting for this state.\njavascript\nvar params = {\n  Bucket: BUCKET_NAME, /* required */\n  Key: KEY_VALUE, /* required */\n};\ns3.waitFor('objectExists', params, function(err, data) {\n  if (err) console.log(err, err.stack); // an error occurred\n  else     console.log(data);           // successful response\n});\n. Hi, it seems like you are looking for waitfor().  This method can check several states every 5 seconds. In this case, the state is 'objectExists'. Here is an example for waiting for this state.\njavascript\nvar params = {\n  Bucket: BUCKET_NAME, /* required */\n  Key: KEY_VALUE, /* required */\n};\ns3.waitFor('objectExists', params, function(err, data) {\n  if (err) console.log(err, err.stack); // an error occurred\n  else     console.log(data);           // successful response\n});\n. Hi @jcollum-cambia \nUnfortunately, this error message is not raised by Javascript SDK. I haven't reproduced your case on my machine but does the error message still exist after you set the NODE_TLS_REJECT_UNAUTHORIZED?. I guess you are asking for x-amz-request-id and x-amz-id-2.  The former one is an HTTP response header entry that created by Amazon S3 to identify requests, and the latter one is also called extended request id which used for the internal debugging purpose. They are shown in the HTTP response headers. You can inspect it using the browser or log the res.headers using Node. They are mainly used in the internal trouble shootings. Could you be more specific on the problem you are encountered with? . Hi @joe1chen \nThis issue seems like a Service/API issue, which means it does not arise from the SDK. I will forward it to Polly service team to see how we will solve this issue. For now, you can try passing in the readable words only.\nUpdate: This is an issue with Polly service. The team is working on fixing it.. Hi @SPradnya \nSDK does have a Credentials.get() methods that check the existing credentials and refresh them when expired. If you are using other identity providers (like facebook/google etc.), you may need to manually load the updated tokens if Credentials.get() callback is called with an error. You may use SDKs from other identity providers (for AWS Cognito you can see here) to do so. \n. Basically, if you are using the cognito identity credential, the get() method will first check whether the present credential is expired by comparing the expire time and current time. If expired, it will automatically use cognitoidentity.getId. And if this fails, it will clear the expired Id the execute the call back with error in the last. \nSo, to sum up, this callback is for handling the error in cognitoidentity.getId and the expired credential is already taken care of. And the error is returned because the user's credential gets expired and SDK can not refresh it. . The SDK has some fix on the client side. I know the empty string still ends up being saved as null in Dynamodb, but this is a limitation for Dynamodb. Please refer to the discussion in #833.. Hi, \nBecause IdToken is represented as a JSON Web Key Token, it's signed with a secret or private/public key pairs, which means even if you revoke the IdToken, there is no way to revoke the distributed public key. And IdToken has a short life span, it will expire in a short time. . Hi, \nBecause IdToken is represented as a JSON Web Key Token, it's signed with a secret or private/public key pairs, which means even if you revoke the IdToken, there is no way to revoke the distributed public key. And IdToken has a short life span, it will expire in a short time. . Frankly, I'm not sure what is the best practice for the circumstance you are referring to:)\nBut you can go to developer forum (https://forums.aws.amazon.com/forum.jspa?forumID=199&start=0).. Frankly, I'm not sure what is the best practice for the circumstance you are referring to:)\nBut you can go to developer forum (https://forums.aws.amazon.com/forum.jspa?forumID=199&start=0).. Hi @kevinbror,\nAccording to #797, the requestId and extendedRequestId are now bound to error object of the s3.upload() method's callback as well. You can access the value using following code. \nbucket.upload(params).send((err, data) => {\n    if (err) {\n        console.log(err.extendedRequestId);\n    }\n}\nOne more thing needs to be noticed is that the extraction of requestId and extendedRequestId is based on the response headers. So the bucket's CORS config needs to be set to expose these headers. See this for more information.. Hi @kevinbror,\nAccording to #797, the requestId and extendedRequestId are now bound to error object of the s3.upload() method's callback as well. You can access the value using following code. \nbucket.upload(params).send((err, data) => {\n    if (err) {\n        console.log(err.extendedRequestId);\n    }\n}\nOne more thing needs to be noticed is that the extraction of requestId and extendedRequestId is based on the response headers. So the bucket's CORS config needs to be set to expose these headers. See this for more information.. Hi @kevinbror,\nMany reasons can result in an invalid signature. Do you mind sharing the CORS config of your bucket? . Hi @kevinbror,\nMany reasons can result in an invalid signature. Do you mind sharing the CORS config of your bucket? . Hi @Bramzor \nI think Dynamodb may do server-side validation on the key names and prohibit the those beginning with ''. But on the other hand, they offer an another way around: you can use ExpressionAttributeNames param to replace the name with strings you want. So your params may be like:\nlet params = {\n    TableName:'Items',\n    Key: { \n    \"_id\": \"6c773d17-270c-4b29-a185-629e0b90990e\"\n    },\n    UpdateExpression: \"set #a = :something\",\n    ExpressionAttributeValues:{\n        \":something\": \"STRING TEXT\"\n    },\n    ExpressionAttributeNames: {\n        \"#a\": \"_something\"\n    },\n    ReturnValues:\"UPDATED_NEW\"\n}\nDynamodb should better document what they allow in UpdateExpression though.. Hi @Bramzor \nI think Dynamodb may do server-side validation on the key names and prohibit the those beginning with ''. But on the other hand, they offer an another way around: you can use ExpressionAttributeNames param to replace the name with strings you want. So your params may be like:\nlet params = {\n    TableName:'Items',\n    Key: { \n    \"_id\": \"6c773d17-270c-4b29-a185-629e0b90990e\"\n    },\n    UpdateExpression: \"set #a = :something\",\n    ExpressionAttributeValues:{\n        \":something\": \"STRING TEXT\"\n    },\n    ExpressionAttributeNames: {\n        \"#a\": \"_something\"\n    },\n    ReturnValues:\"UPDATED_NEW\"\n}\nDynamodb should better document what they allow in UpdateExpression though.. Hi @lonormaly,\nThe code snippet you show here looks correct. I'm using sdk 2.29.0 running on a browser. Does your upload always fail or sometimes success? What is the size of the file you are testing? Does the file name contain any non-English characters? . Hi @lonormaly,\nThe code snippet you show here looks correct. I'm using sdk 2.29.0 running on a browser. Does your upload always fail or sometimes success? What is the size of the file you are testing? Does the file name contain any non-English characters? . Do you have any other changes with your project? I have some difficulties reproducing your issue only with the code. There may be reasons other than upgrading lead to the issue.. Do you have any other changes with your project? I have some difficulties reproducing your issue only with the code. There may be reasons other than upgrading lead to the issue.. Hi @ToddAlvord \nI suspect this is due to the signature verification mechanism of S3 server. \nBasically how the pre-signed URL (see SigV2 for more information) works is that the signer signs each header and hash them with other fields(eg. date) and generate a signature, and then send them all together to S3 server, in a URL query. When the server receives such request, it will also extract the same fields as in the request and generate a signature and compare with that in the request.\nHowever, on the server side, when it extract headers, some headers may get ignored from URL query. In your case, Cache-Control and Content-Type headers are both ignored. \nIf you want these headers to be seen by S3 server, you could specify them in request header when using the URL. If you are using Node, your code may be like below:\n```javascript\ns3.getSignedUrl('putObject', params, (err, data) => {\n    if(err){\n        console.log(err);\n        return;\n    }\n    var options = url.parse(data);\n    options.method = 'PUT';\n    options.headers = {\n        \"content-length\": TEST_STRING.length, //since we use stream here, specifying length is needed\n        \"cache-control\": 'public, max-age=31536000',\n    \"content-disposition\": 'inline',\n    };\n    var req = https.request(options, function (res) {\n       //do something\n    });\nreq.on('error', function (err) {\n    console.log(err);\n})\n\nreq.write(TEST_STRING);\nreq.end();\n\n});\n. Hi @ToddAlvord \nI suspect this is due to the signature verification mechanism of S3 server. \nBasically how the pre-signed URL ([see SigV2 for more information](http://docs.aws.amazon.com/general/latest/gr/signature-version-2.html)) works is that the signer signs each header and hash them with other fields(eg. date) and generate a signature, and then send them all together to S3 server, in a URL query. When the server receives such request, it will also extract the same fields as in the request and generate a signature and compare with that in the request.\nHowever, on the server side, when it extract headers, some headers may get ignored from URL query. In your case, `Cache-Control` and `Content-Type` headers are both ignored. \nIf you want these headers to be seen by S3 server, you could specify them in request header when using the URL. If you are using Node, your code may be like below:javascript\ns3.getSignedUrl('putObject', params, (err, data) => {\n    if(err){\n        console.log(err);\n        return;\n    }\n    var options = url.parse(data);\n    options.method = 'PUT';\n    options.headers = {\n        \"content-length\": TEST_STRING.length, //since we use stream here, specifying length is needed\n        \"cache-control\": 'public, max-age=31536000',\n    \"content-disposition\": 'inline',\n    };\n    var req = https.request(options, function (res) {\n       //do something\n    });\nreq.on('error', function (err) {\n    console.log(err);\n})\n\nreq.write(TEST_STRING);\nreq.end();\n\n});\n``. Hi @jonface \nYes. Thecontent-typeis not a signable header and will be ignored when signing the headers. This is mainly because of the fact that the S3 ignores this header, so adding this to query string before signing will lead to the incorrect signature exception. \nAlthough using sigv4, S3 has many customizations on it. Different SDKs may have different implementations. What Go does is removing all these headers out of query string which gets rid of all the hesitation that which header could be ignored by S3.. Hi @jonface \nYes. Thecontent-type` is not a signable header and will be ignored when signing the headers. This is mainly because of the fact that the S3 ignores this header, so adding this to query string before signing will lead to the incorrect signature exception. \nAlthough using sigv4, S3 has many customizations on it. Different SDKs may have different implementations. What Go does is removing all these headers out of query string which gets rid of all the hesitation that which header could be ignored by S3.. Hi @morbo84 \nMultipart upload is also called Managed upload, see here to get more information. Basically, it partitions a big file into several parts (5MB each by default) and sends them individually. When sending these parts, ManagedUpload always maintains a queue with specified length(set as 4 by default) and concurrently upload the parts in the queue. The S3 server will reorganize them when all the parts are successfully uploaded, the method returns success. However, if no less than 1 parts fail, the SDK will use a 'DELETE' request to wipe out all the parts already uploaded by default.\nThe error you are encountered usually results from multiple reasons other than the poor connection. However, the connection may lead to request timeout in this case. \nReturn to your questions. 1) For default request timeout, you could find it in aws.config.httpOptions.timeout(see this for more information), which is set to 2 minutes by default. \n2) The error message shows that the server is complaining that the connection is established but no data is written into the socket. So after the timeout the server will close the connection. This probably because that the connection is slow and only a few of parts in the queue I mentioned previously are actually uploading and others just maintain an idle connection. And when timeout, you will see the error.\n3) With all varieties of network conditions and environments, there isn't a single configuration handles all the situations. But in your case, you can try setting the queueSize to 1, which means only one part is allowed to upload at any time, which enables one request to take up more bandwidth. \njavascript\nvar options = {queueSize: 1};\nvar params = {\n  Bucket: 'myBucket',\n  Key: path.basename(filePath),\n  Body: fileStream\n}\ns3.upload(params, options, function(err, data) {\n  console.log(err, data);\n});\nYou may also try to set a longer request timeout such as 5 minutes. \njavascript\nAWS.config.httpOptions.timeout = 300000;\nUPDATE: This is incorrect. To make the timeout suitable for all slow connection situation, we should set the timeout to 0 to turn off the timeout.\njavascript\nAWS.config.httpOptions.timeout = 0;\n. Hi @morbo84 \nMultipart upload is also called Managed upload, see here to get more information. Basically, it partitions a big file into several parts (5MB each by default) and sends them individually. When sending these parts, ManagedUpload always maintains a queue with specified length(set as 4 by default) and concurrently upload the parts in the queue. The S3 server will reorganize them when all the parts are successfully uploaded, the method returns success. However, if no less than 1 parts fail, the SDK will use a 'DELETE' request to wipe out all the parts already uploaded by default.\nThe error you are encountered usually results from multiple reasons other than the poor connection. However, the connection may lead to request timeout in this case. \nReturn to your questions. 1) For default request timeout, you could find it in aws.config.httpOptions.timeout(see this for more information), which is set to 2 minutes by default. \n2) The error message shows that the server is complaining that the connection is established but no data is written into the socket. So after the timeout the server will close the connection. This probably because that the connection is slow and only a few of parts in the queue I mentioned previously are actually uploading and others just maintain an idle connection. And when timeout, you will see the error.\n3) With all varieties of network conditions and environments, there isn't a single configuration handles all the situations. But in your case, you can try setting the queueSize to 1, which means only one part is allowed to upload at any time, which enables one request to take up more bandwidth. \njavascript\nvar options = {queueSize: 1};\nvar params = {\n  Bucket: 'myBucket',\n  Key: path.basename(filePath),\n  Body: fileStream\n}\ns3.upload(params, options, function(err, data) {\n  console.log(err, data);\n});\nYou may also try to set a longer request timeout such as 5 minutes. \njavascript\nAWS.config.httpOptions.timeout = 300000;\nUPDATE: This is incorrect. To make the timeout suitable for all slow connection situation, we should set the timeout to 0 to turn off the timeout.\njavascript\nAWS.config.httpOptions.timeout = 0;\n. Hey @morbo84, thanks for all your experiment!\nI see your point. My suggestion to set a longer timeout may not be appropriate for your case. Instead, you may set the timeout to 0 to turn off the timeout. \nThe original error message is the timeout on the server side. Sometimes even the data is sent out, it can not make it to the server, so the server will kill the idle connection at timeout.(I can hardly find the document for this but\u00a0it's about 20s~30s according to experience). And the SDK will retry the request 3 times by default.(see here for more information) \nOn the other hand, the error of Connection timed out after 120000ms may not be caused by SDK, given you are sure that there's no idle connection and that connection times out. Actually, the SDK doesn't have any special time out policy. As you can see from the code here, the SDK calls request.setTimeout() to set time out for each connection, which is provided by Node. I don't know that deep how Node works this out, maybe you can dig into that.\nFor now, setting queueSize to 1 and setting timeout to 0 might be the best we can do at the SDK level. Or you can try using putObject(), since there's only one request here, hopefully it will make the upload get through.. Hey @morbo84, thanks for all your experiment!\nI see your point. My suggestion to set a longer timeout may not be appropriate for your case. Instead, you may set the timeout to 0 to turn off the timeout. \nThe original error message is the timeout on the server side. Sometimes even the data is sent out, it can not make it to the server, so the server will kill the idle connection at timeout.(I can hardly find the document for this but\u00a0it's about 20s~30s according to experience). And the SDK will retry the request 3 times by default.(see here for more information) \nOn the other hand, the error of Connection timed out after 120000ms may not be caused by SDK, given you are sure that there's no idle connection and that connection times out. Actually, the SDK doesn't have any special time out policy. As you can see from the code here, the SDK calls request.setTimeout() to set time out for each connection, which is provided by Node. I don't know that deep how Node works this out, maybe you can dig into that.\nFor now, setting queueSize to 1 and setting timeout to 0 might be the best we can do at the SDK level. Or you can try using putObject(), since there's only one request here, hopefully it will make the upload get through.. Hi @zerojuan \nThis issue arises from 2 different mechanisms that how SDK extract data.Location. For multipart upload, the SDK calls completeMultipartUpload() (see here) after all parts get uploaded and the '/with+space/' is what s3 server responds. However, for single-part upload, server's response doesn't contain a Location field. What the SDK does is concatenating URIEncoded paths into a Location.\nSo in this case, you may URIDecode the Location especially when file size is smaller than partSize (see here), which is when using single-part upload.. Hi @zerojuan \nThis issue arises from 2 different mechanisms that how SDK extract data.Location. For multipart upload, the SDK calls completeMultipartUpload() (see here) after all parts get uploaded and the '/with+space/' is what s3 server responds. However, for single-part upload, server's response doesn't contain a Location field. What the SDK does is concatenating URIEncoded paths into a Location.\nSo in this case, you may URIDecode the Location especially when file size is smaller than partSize (see here), which is when using single-part upload.. Hi, this is an issue concerning other SDK. You may open an issue there. This one will be closed.. Hi, this is an issue concerning other SDK. You may open an issue there. This one will be closed.. CI is so strict on coverage.... CI is so strict on coverage.... have you tried managed upload? Does this error still exist when using the upload()?. have you tried managed upload? Does this error still exist when using the upload()?. Hi @CFCSystem \nThis is a bug introduced from V2.116.0. You can use any older versions. Thank you for revealing this to us! We will be working on fixing this.\n. Hi @CFCSystem \nThis is a bug introduced from V2.116.0. You can use any older versions. Thank you for revealing this to us! We will be working on fixing this.\n. Hi @ajkerr \nI believe the difference is whether body presents. In your second example, the body will be ignored by the signer, and 'X-Amz-Content-Sha256' header will be used as encoded body. And I don't think it's problem of signing the body because native SDK also uses JSON.stringify() to serialize body. \nI will contact the service team to verify this issue.. Hi @ajkerr \nI believe that's the case. I will keep following this to the service team. Maybe this service will ignore request body with Content-Length set to 0. But I don't think it's an issue with SDK because the sigV4 should be designed to be compatible with all supported services. We need some more time to find out the root cause and fix this issue. \nThanks a lot for your investigation.. Hi @uzimith \nThe type definitions are generated from the API model, usually it should be Uppercased. Can you tell me more about this issue? Are you using it on Node? which SDK version are you using? Which function are you calling?\n. Hi @mrfez \nAre you using the callback in createPresignedPost? You know it's preferable to use the asynchronous method when you use the IAM credentials. It seems like your credentials cannot be resolved. Which credentials are you using?\n. Hey @aflext,\nI think you are looking for listObjectsV2. This is a revised list object operation, in which you can specify the prefix.\njavascript\nconst params = {\n    Bucket: 'bucket',\n    Prefix: 'unique_id',\n    MaxKeys: 10 //to limit the results number returned from operation\n};\ns3.listObjectV2(params, function(err, data) {\n        if (err) {\n            console.log(err)\n        } else {\n            console.log(data);\n       }\n})\nYou can then get these objects according to the Keys returned by listObjectsV2. \njavascript\ns3.listObjectsV2(params).promise().then(data => {\n    let q = [];\n    for (content of data.Contents) {\n        const getObjectParams = {\n            Bucket: bucketName,\n            Key: content.Key\n        }\n        q.push(s3.getObject(getObjectParams).promise())\n    }\n    return Promise.all(q);\n}).then(data => {\n    console.log(data)\n}).catch(err => {\n    console.log(err);\n}). Hi @jingpengw \nThank you for pointing this out! Also, the link you are looking for is here: https://github.com/aws/aws-sdk-js/blob/master/ts/tsconfig.json. Hey @againer,\nSorry I cannot reproduce the issue here. It's probably because your stream class doesn't support read(). Would you mind sharing your code snippet here?. Thank you very much for contributing. We will review it later.. My suspicion is that even after you successfully create a bucket, the bucket isn't immediately there for your access(even if sometimes you can access it). Here you may use a waiter waiting for the bucket exists. As in your code snippet, you may use the waiter before you putObject.. Thank you for your advice. It's very nice and intuitive to have this feature. However, this may need a major version update. \ud83d\ude09. Hi @aaayushsingh \nIt seems that your npm finds a wrong checksum. You can try running cleaning cache cmd: npm cache --force clean. If this still exists you can remove the .npm file and re-install the sdk. \nThis is not an issue with sdk, so I will just close this issue. But feel free to comment here if you have further questions.. Hi @liesislukas,\nAccording type definition of SDK, the messages property in abortStatement and clarificationPrompt is required. One thing you can do is to check whether you have set these properties.. Hi @liesislukas \nI agree that the documentation in the screenshot is ambiguous. Actually, you can refer to this API Reference as they are more accurate and more update. \nHowever, the inconsistency between creating and updating the bot is not exact. Here, the abortStatement field is indeed optional, but once you set this field, its sub-node messages is required. This doesn't change in creating or updating a bot.\nThank you for bringing this up to us, I will reach out for updating the documentation you mention. Hi @doapp-jeremy,\nI see the deleteMessage operation with 5s delay in your attachment. But I can't reproduce this issue.\nIs this a consistent issue?(the delay always exists in some specific sequence of operations or always the same operation) . Do you mind showing me your code to reproduce this issue? How do you specify the ReceiptHandle when you delete the message? And your Queue config.(regular or fifo queue; default visibility timeout). Hi @sksin28 \nThe s3.upload method doesn't modify the parameters fed into the operation. This behavior may result from the service. You can raise this issue on the S3 service forum, they should be able to answer this question.\ud83d\ude05. Hi @blablabla156, so you already have the encrypted password right? then do you see any error message when you run the decrypting script?. Hey @blablabla156,\nIt seems like you didn't retrieve the ec2 windows password data successfully. Can you show me your error response? Just change the code console.log(\"Could not get the encrypted password\"); to console.log(err);. Hey @Bang-Kwangmin You can try to only run describeAutoScalingGroups in another script and you will see the configuration is already updated. I think the reason for this issue is that SDK calls the 2 methods asynchronously and the sequence of the calls actually hitting the server cannot be guaranteed. In this case describeAutoScalingGroup() always hit the server earlier. \nIf you truly want to describe the AutoScalingGroup after update you can try using it in promise way:\njavascript\nvar client = new AWS.AutoScaling();\nclient.updateAutoScalingGroup({\n        AutoScalingGroupName: 'test-ASG',\n        LaunchConfigurationName: 'B'\n}).promise()\n.then(data => {console.log(data)})\n.then(() => {\n        return client.describeAutoScalingGroups({AutoScalingGroupNames: ['testGroupA']}).promise();\n})\n.then(data => {console.log(data)})\n.catch(err => {console.log(err)}). @Bang-Kwangmin Sorry, I cannot reproduce this issue. Actually, both Python and JS SDK don't tune the request, so they shouldn't behave differently. Could you confirm you are using the correct account and region? . Hi @kensaggy I don't have much experience on CloudwatchLogs. Since the SDK doesn't alternate any parameter in the request, this might be issue from service team. You can directly ask the service team on their forum.. The blog gives us a good example of configuring the whole SDK level. Actually, this setting can also be done at the individual client level, so that you can selectively use the proxy-agent. For example:\n```javascript\nvar AWS = require('aws-sdk');\nvar proxy = require('proxy-agent');\nvar s3 = new AWS.S3({\n        region: 'us-west-2',\n        httpOptions: {agent: 'http://user:password@internal.proxy.com'}\n});\ns3.getObject({Bucket: 'bucket', Key: 'key'}, function (err, data) {\n  console.log(err, data);\n});\nDoes this solve your concern?\n. Hi @roberthelmick08, Could you also show the error message you got?. Sorry for the late reply. This is due to cognito identity SDK depends on one of private functions of our SDK. You can go to here to follow the issue: https://github.com/aws/amazon-cognito-identity-js/issues/646\n. It seems like you can do what like below to use callback on `error` event but not when you are using `request.createReadStream()`. I will do more investigation here.javascript\nvar request = s3.getObject(params);\nrequest.on('error', (error) => {\n    console.log('Failed to download file from S3:', error.message);\n});\nrequest.send();\nIf you just want to promisify this function, the SDK already provided much easier way to accomplish it:javascript\nfunction downloadFontInfoFileFromS3(fileKey) {\n    var s3 = new AWS.S3();\n    const options = {\n        Bucket: awsConf.bucket,\n        Key: fileKey,\n    };\n    const downloadFilePath = SOME_PATH\n    return s3.getObject(options).promise()\n        .then(data => {\n            var body = data.Body;\n            fs.writeFileSync(SOME_PATH, body, {encoding: 'utf8'})\n        })\n}\ndownloadFontInfoFileFromS3().catch(err => console.log(err));\n```. The code looks fine. I think the reason for this issue is the permission setting. One thing you can check is whether permission to S3 has been added to the IAM role you are using for federated identities. The permission policy should contain something like:\njson\n{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"s3:*\"\n  ],\n  \"Resource\": [\n     \"*\"\n  ]\n},\nThis statement grant permission to ervery operation in S3 from any resources. Hi @non-standard \nI don't think this issue is related to our SDK. I find some other people also meet this problem, some of them solve this problem by setting a larger node memory size, maybe you can refer to these issues. \nI will just close this issue. But feel free to reopen it if you have found a reproducible config or code that has something to do with aws-sdk.. Hi @napikle \nThis issue is on the service end, I'm afraid SDK cannot do much mitigation. You can raise this issue on the DynamoDB forum, the DynamoDB service will notice it.. Sorry for the late reply, I think your CORS setting should work. My suspicion is that you preflight request was blocked somewhere. Are you accessing the bucket that in the same region as your client?. Hi @riteshapatel \nThere's still no API for querying all the instance type information as I know. You can retrieve up-to-date full instance type list from here. \nBut this list only contains names, I guess you also want to query the detail information for each instance. I will follow this up with service team inform here if I get any updates.. Hey, @riteshapatel \nI find Price List API offers some useful APIs to query the instance details, such as get attributes. But the query API is only available in 2 regions. For other regions, you can use Bulk API to download instance details files. This is the API closest to what you need for now.. Hi @niftynei \nThank you for contributing. \nCurrently we are testing whether anything is compatible with web worker and how it affects our SDK bundle size.. Hi @michaelyfan \nI'm not sure if I understand this correctly. Is the file link you mention here the presigned url?\n Normally, s3 would return NoSuchKey exception if you try to download a deleted resource. And after you update the resource the SDK cannot download the old file (AccessDenied using old url).  . Hi @sk16 \nThank you very much for bringing this up. Currently, we don't have story on how to support SDK in service worker, but we are open to any PR if you'd like to contribute to our code base. I will close this issue and you can open a PR for better tracking it.\nIt would be nice if you submit the test code along with the PR. You can run npm install then npm run test to test locally.\n. Hi @jferrer21 ,\nI'm not an expert on ec2. I find this in EC2 API reference, hope it can help you. There's an 'instance-state-code' filter available for DescribeInstances method. You can using this operation to get the public IP address as well.\njavascript\nec2.describeInstances({\n    Filters:[{\n        Name:'instance-state-code',\n        Values:['16']//'16' for status running\n    }]\n}, function(err, data) {\n    if(err) {\n        console.log('Error', err)\n    } else {\n        //may differ according your EC2 setting\n        data.Reservations.map((reservation) => {\n            reservation.Instances.map((instance) => {\n                console.log(instance.PublicIpAddress);\n            })\n        })\n    }\n}). The snippet here looks good. My suspicion is that your backend return the redirection quicker than your response actually done written to your bucket. So by the time you actually hit the bucket using the presigned url. The bucket has already been updated. So the timestamp of url mismatches the bucket.. Hi @KMiso,\nPer this doc says, you may need PassRole permission to enable an IAM role to pass a role to another AWS service, if you haven't done yet. May I ask why do you want to use unauthenticated rule instead of authenticated one to create lambda function?. Hi @GuidoS,\nI know the client config credential and global config object is somewhat confusing. The S3 client loads the config from global config object at its construction. Once the s3 client is constructed, the update on global config object won't affect the S3 client then. Which is said, you need to always set global credentials before constructing any clients.\njavascript\nvar AWS = require('aws-sdk');\nAWS.config.credentials = new AWS.SharedIniFileCredentials({profile: 'myBucket'});\nAWS.config.update({region:'us-east-1'});\nvar s3 = new AWS.S3();. Hey @windhamg,\nThe Lex Developer Forum would be a better place for API usage or any feature request. The service team would be happy to answer your questions.. Hi @intergleam ,\nYou can refer to the Developer Guide and JS SDK Reference to get started with it. Feel free to ask questions here when you have issue using JS SDK.. Hey @mi5guided Have you tried to use the ChainableTemporaryCredentials? I think you can use EC2MetadataCredentials as the master credentials here and specify the roleArn you'd like to assume.. Hey @driverpt Are you still interested in working on this PR?. @driverpt \nThanks. I prefer the lowercased flag: $uploadedViaMultipart instead of $UploadedViaMultipart. It's a little nit-picky but this will make the casing consistent. \nCan you also add change log for this feature? Follow this checklist to see what else is needed. Personally I think updating corresponding typescript interface would be very helpful.\n. Hey @RoboSparrow, I believe this is a very valid request. Maybe we can add a note in docs that the service doesn't support cors. You can always check the CORS support by yourself here. The specific service supports CORS if it specifies cors: true. As it's mentioned in #791, the request query is a bit different like here. The service team specifies in the model that the query must contain format=sdk. I think maybe they believe return everything in the array would reduce the complexity to determine whether the response is an array. Since it's not a bug, we cannot update it.. Hi @sebastienfi \nIs the code snippet referring to any tutorial? Usually credentials.get can be automatically called if the credentials are expired. \nI'm also wondering whether the credentials.get succeeded. Can you add console.log(err, data) in the beginning of the callback in your credentials.get operation?. I'm glad to hear you solved this problem. Feel free to reopen this if you have problems in further implementation.. Hi @TrejGun ,\nLike you said, the promise dependency is used to 'promisify' the operation that makes http request. Since getSignedUrl doesn't make any request, this operation is not 'promisified'. I don't think this is an issue with SDK, it more of a style inconsistency. Is there any documentation mentioning using promise on s3.geSignedUrl?. Hi @achelkia \nCan you show me your dependencies in package.json and tsconfig settings? It seems like the @types packages are not imported somewhere.. I see you did not install @types/node. Our type definition uses Buffer, stream types which are declared in @type/node. Just try npm -i -d @types/node. Hi @achelkia \nMaybe you can try remove the \"types\": [] from the tsconfig.server.json? Our readme does mention you need to either add node to type list or remove the entry. Another issue #1271 provides a workaround for similar issue.. Yes you can do this. You can always listen to the httpUploadProgress event for putObject as well as upload. This is the API reference. For example:\njavascript\nbucket.putObject(params).on('httpUploadProgress', function(evt) {\n    console.log(\"Uploaded :: \" + parseInt((evt.loaded * 100) / evt.total)+'%');\n}).send(function(err, data) {\n    if(!err) {\n        alert('upload complete');\n        console.log(data.requestId);\n    } else {\n        console.log(err);\n    }\n});\nShowing the upload persentage for multipart upload is similar, here contains an example of multipart upload.\n. What service are you calling in lambda? If possible, can you show us the exception response data? As far as I know, it's not an intended behavior for SDK to ignore the errors.. I'm not sure if there are any existing workaround for your use case. I believe service team will be more than happy to help you. You can raise your request for Codecommit API at their forum. I will close this issue since this is not actually an issue with SDK.. Hi,\nI cannot reproduce this issue. Have you tried npm reinstall the aws-sdk? or update to newest version. . Hi @manju16832003 \ns3.getObject and almost all other operations are initialized and bound to AWS object at runtime, so you cannot find the function. They are just not there yet. But it's available after you instantiate the s3 client.\njavascript\nconst s3 = new AWS.S3({region: 'region'});\nconsole.log('promise is a function: ', s3.getObject(params).promise);\nI don't have the information of where this error is raised. But I write a simplified mock function that should work:\njavascript\nvar AWS = require('aws-sdk');\ndescribe ('mock promise', () => {\n    it ('test', () => {\n        var s3 = new AWS.S3({region: 'us-west-2'});\n        var request = s3.putObject({\n            Bucket: 'bucket', \n            Key: 'key'\n        });\n        request.promise = () => Promise.resolve('mockdata');\n        request.promise().then((data) => {expect(data).toEqual('mockResp')})\n    })\n})\nHere the request is sent on wire still.. Hey @tnyanhongo \nCan you show me the link to the tutorial you are working on? and it would be helpful if you show me your code snippet especially on common.js line 440, 441 where the code is broken. . I think this issue happens because 'StringEquals' field is missing in the response of lambda.getPolicy(). It may have something to do with the response parsing or your lambda IAM policy setting, rather than issues with aws-sdk. I believe you will get more accurate response directly from the library author if you raise the issue on the lambda-redshift-loader repo. . Hi @JimtotheB \nJust to confirm: Is the data.signedUrl generated by server code above? I believe it's because of the metadata parameter. In putObject API, metadata values are serialized in the header(x-amz-meta-), and they are counted in when calculating the signature in presigned URL. However when you send from the client, the header for metadata is empty. So s3 server will calculate the signature request without metadata header. And when compared to the signature from data.signedUrl which uses the metadata header, you get a signature mismatch. You can try add the headers to ajax.\njavascript\n$.ajax({\n  type: 'put',\n  url: data.signedUrl,\n  data: file,\n  processData: false,\n  contentType: file.type,\n  headers: {uuid: keyuuid, owneruuid: owneruuid},\n })\n. which sdk version are you using? It seems that the parameter is only added after 2.156.0.. pinpoint does not contain cors config: https://github.com/aws/aws-sdk-js/blob/cd11ea38842706a73f9c4a9c130090f2907b44c6/apis/metadata.json#L314\nIt should be ok to remove from browser test.. @gischer Sorry for the late reply. The code you post here looks good. Can you share a reproducable code snippet here?. Hi, @lajpatshah \nCan you share what response did you get? Http response body is also helpful. You can try code below and share the log you get\njavascript\nsqs.receiveMessage(params, function(data, error) {\n    if (data) {\n        console.log('success!');\n        console.log(data);\n    } else {\n        console.log('fail: ', error);\n        console.log('body: ', this.httpResponse.body.toString'utf8')//print response body\n    }\n}). Hi, @saachinsiva  Have you solved this issue? Besides what @xinghul  mentioned, you should also check whether the request is successful.. I think there suppose to be a link, you can refer to this availability table, we will update the documentation shortly. Regarding your question, the SDK itself doesn't check the region but a wrong endpoint may either fail at DNS or S3's network gateway. Either the case you will receive a networking error.. @mt-ronkorving  our SDK doesn't provide such command. The best I can find is in Python SDK boto3 get_available_regions function. Hey @YousefED, thank you very much for following our new features closely. You are right, Javascript SDK doesn't support S3 select yet. But this feature is among the top of our priority list. So please stay tuned.. Hi @mogwai,\nThank you for bringing this up. I agree it's very useful to add this configuration. But our SDK is not able to implement this feature unless DynamoDB service updates the API. I've already cut a ticket to service team concerning this feature. You can also call this out on the DynamoDB forum where it is visible from service team.. @mogwai  I find you can do this using ApplicationAutoScaling service. The basic idea is to call ApplicationAutoScaling.registerScalableTarget to register autoscaling to the DynamoDB table. And then call ApplicationAutoScaling.putScalingPolicy to edit your policy. Comment if you have more question. I guess the reason they don't put this setting into DDB api is that this setting is an integration of 2 services.. Hi @ncave,\nIt's generally not advisable to extend from client classes, because they are not written in normal javascript class. Extending from them may lead to get an empty constructor, in which case your service operations(like putObject) cannot be bond to the client object.  But I still haven't found which difference results to the error. I will reply here after more investigation.. Hi @garygrubb May I ask why do you want to delete the X-Amz-User-Agent header? This will change the signature. . Hi @cekvenich \nThere are different approaches to keep the SDK size smallest. Firstly you can use bundlers like webpack, browserify (or parcel) and import the SDK S3 client like import * as S3 from 'aws-sdk/clients/s3'; to your project. The bundler will automatically prune the unused files. \nAnother approach is to use SDK browser builder. You can build the SDK with only S3 client. I tried this approach and it's 85kB after gzipping. . This is still significantly smaller than the Cloudfront distribution https://sdk.amazonaws.com/js/aws-sdk-2.224.1.min.js. That is 1.7MB before gzipping. We have noticed this defect and working on a big version bump. . Hey @taju20910 \nCan you open the logger(for example: var sqs = new AWS.SQS({logger: console});) to confirm whether the request retries when the operation is 'stuck there'? If it is an issue from SQS service instead of SDK itself, AWS SQS Forum may be a better place to turn to. You can provide the request id and the service time can access the log to confirm the issue. . I cannot reproduce this issue. your code snippet looks right. Which version are you using?. Hi @jaggu07 \nThis probably because the S3 client is requesting a wrong endpoint region. After the first fail try(us-east-1 as default), the S3 client will update its endpoint with correct region so that the following retries are successful. You can try setting the bucket region in S3 constructor. If the bucket is in eu-west-1, you can construct like this: var s3 = new AWS.S3({region: 'eu-west-1'}). Hi @mim-Armand, I cannot reproduce this issue. Are you using any un-default promise library?. Yes. I used the code snippet you shared as above and it works as expected. I was using a test user with all STS permissions. Can you show me what's the printout you get from the code snippet above?. Hi, @aaronpeterson \nCan you share your code how do you catch rejections? When you see the message, did your operation succeed? Can you log out the exact error you get?  Usually, you will see this error when catch statement is not appended to a promise. Is your code look like bellow?\njavascript\nvar sqs = new AWS.SQS();\nsqs.deleteMessage(params).promise().then((data) => {\n    //success\n}).catch((err) => {\n    console.log('error: ', err);\n}). Hi @flavioribeiro \nFor all of our SDK calls, parameters are javascript objects. you can all the service like this: \njavascript\nmp.createChannel({Id: 'dummy', Description: 'dummy-description'}, (err, data)=>{\n     console.warn(err, data)\n})\nYou can refer to our SDK documentation for more information.. Hi @C2P1 \nI don't think DynamoDB will prohibit you from setting '@' in the primary key. Can you share yourAPI.get code? I have a test using DynamoDB.DucumentClient class and I'm able to put and get the item.. This change makes sense, and make it more consistent with the definition in the managed upload class. Thanks for the PR!. Hi @azkarmoulana \nAfter signatureV4 introduced in as default signing algorithm, 7 days is the longest expiration time we can get according to here. This is a limitation of the pre-signed URI. Close this issue as it's duplicated with #685 . Hi @bkarv \nManagedUpload is actually a serial of API calls: createMultipartUpload(), uploadPart(), completeMultipartUpload(). And they each will call the callback if the request is failed: createMultipartUpload, uploadPart, completeMultipartUpload. So technically,  if any of these api call fails they callback should be triggered. Can you share what's the error you are seeing?. I believe it's because the stream type from the _fs.createReadStream(file.path) is not valid here. Only node buffer and stream can be put in Body. Can you try print the type of the fileData? The idea is: require('buffer').Buffer.isBuffer(fileData) || fileData instanceof require('stream').Stream. You are right. We need to be smarter on detecting the running environment. You can forcibly set the SDK to believe that it's running on the Node by this: AWS.util.isBrowser = () => false. \nI'm also wondering if you are using bundler. The concern is that, if you are using a bundler, it is possible you are using the browser build of SDK(although it should default to be Node SDK). If it's this case we need to think of other ways. . Hey @daryllstrauss, this is an interesting use case. Can you let the workers post message back and then integrate with node version of SDK or make all AWS service calls in worker? I mean you need to choose depend on one environment instead of two.. Keeping main thread clean makes a lot sense. I agree this is hard. I'm also thinking of if you can bundle the worker code in individual package with individual node_modules and use webpack(if you use that) to target at 'web' only for the worker's code. I haven't tested it yet but that's the idea.. We don't have an out-of-box solution for previewing the upload Body. I don't think there is a strong necessary integrating this feature to SDK itself. You can always validate the CSV file before supplying to SDK. Also validating file for each request would be a considerable performance loss.\n. That is to say, instead of calling ManagedUpload upload(), you can call createMultipartUpload, uploadPart and completeMultipartUpload by yourself, which is also what upload() does under the hood. And for each uploadPart call, you can read a chunk from the file and do the validation. You can refer to this to get the idea of how to implement it. Your machine will return an error if you are doing something to file but actually a directory. This is not an issue or sharp edge of SDK. When you upload the file, the Node will, in the end, make syscall to read the file, and the syscall will return error here.. You can put your directory in a tar file first and upload. 'tar-fs' packae might be helpful.Here's an example. \njavascript\nconst tar = require('tar-fs');\nconst s3 = new AWS.S3();\ns3.upload({\n    Bucket: bucket,\n    Key: key,\n    Body: tar.pack('path/to/your/directory'),\n}, function (error, data) {\n    console.log(error, data);\n}). Is this the smallest reproducible code? I cannot reproduce the issue. Have you checked your code base? You might unintentionally touch the response object somewhere.. Hi @pgrzesik \nAre you trying to upload in browser or Node? I wonder whether it's possible that the browser overwrite the content-type and content-encoding headers.. Hey @pgrzesik, can you get the object directly from the bucket using Node and check the response header? I believe it's there. You can try this to print the response header:\njavascript\ns3.getObject(params, function(){\n    console.log(this.httpResponse.headers);\n})\nI think MIME content-type is for browsers to determine what to do with the file. For example, you can host a static website in an S3 bucket. When you retrieve an object with content-type gzip, the browser will help you inflate it automatically. . So, that's the expected behavior. The getObject or download doesn't change the file type on your behalf automatically. You need to handle it by yourself accordingly. All the SDK does is encapsulating the download stream and provide it to users. I think the file type application/x-gzip is set by your OS instead of inferring from HTTP response.. Seems like this is a low-level connecting issue. In what circumstance does the issue happen? Can you provide any code or description to help us reproduce this issue?. Hey @Baraksi,\nI'm not sure I understand it correctly. Do you mean you are using the same key above to put then get object but S3 gives back 'key does not exist'? Which version of SDK and Node are you using?. Hey @dennererthal01 ,\nIn browsers, because of the restriction of CORS, the browsers can only read the headers that exposed to them. It's likely that your CloudFront service doesn't expose ETag header to browsers, this is defined in the service. I'm not very familiar with CloudFront service whether you are able to set the expose headers. You can bring this question to the CloudFront service team in their forum, they will be happy to answer your question.. Hey @fedeverdi, currently IAM service doesn't support CORS, which means it isn't available in browsers by default. However, if your environment doesn't enforce CORS, you can get around it by generating a customize SDK. . I'm glad that you resolved this issue. The SES service does not support the environment enforces CORS like browser. Using Lambda instead is a great workaround. Closing this issue due to this has been resolved. Feel free to comment here if you have further problems.. This looks good to me. I only have one question on the XML builder. . Hey @Luckstai,\nSorry for the delayed reply, have you figured out the usage of device farm? If you still have any questions, you can bring them to developer forum. Since this is not about the usage issue of SDK, I will close this issue. Feel free to comment here if you run into any SDK issues.. Looks like the original issue thie PR would fix was fixed elsewhere. Given the issue this PR is trying to tackle is a little obsure, I will close this PR. Please comment here if you would like to reopen it. @myst729 . Hey @lampidudelj, you can go to the Cognito forum for news of new features or any feature request. The forum is run by the service team so that's where you can get a better answer.. Hey @SamWSoftware \nI cannot reproduce this issue. I notice that you actually get the Body back. Can you try the log the data you get like this:\n```javascript\nexports.handler = (event, context, callback) => {\n    // TODO implement\nvar params = {\n    Bucket: 'bucketName',\n    Key: `Key`\n};\n\nreturn s3.getObject(params, function(err, data) {\n    if (err) { // an error occurred\n        console.log(err)\n        callback('Hello from Lambda!, error')\n    } else { // successful response\n        console.log(data.Body.toString('utf8'));\n        callback(null, 'Hello from Lambda! data')\n    }\n});\n\n};\nI don't think node 6.10 supports async methods. Can you make sure this the snippet you are using?. Hey @xtianus79 \nDo you see the similar exception in earlier version of Cordova? have you tried to log the headers? We don't officially support Cordova and I'm not sure about our compatibility with it.. Hi @g-cassie \nYou don't need to specify using the EC2 metadata credentials in the shared credentials file. In fact, the SDK won't resolve the `credentials_source` key. You can explicitly use the EC2 credentials like this:javascript\nAWS.config.credentials = new AWS.EC2MetadataCredentials()\n``\nSorry for the confusion here, this is a discrepancy between SDKs behaviors.. Hey @ktwbc \nThe*.d.tsfor services are generated from service models([here](https://github.com/aws/aws-sdk-js/blob/eff411003edf9037e57fd298d68351e86dd1febe/apis/sns-2010-03-31.normal.json#L1261)) and they are written by service teams. If the field is not included in response shape, it sometimes means that the field may be omitted in some situation. I can reach out to SNS team to confirm whether the field is always exist.. Hey @angrychewie, sorry for the delayed reply. I'm not very familiar with this API. Can you elaborate what's actually returned from thedescribeIdentity()` and what you are expecting?. Hey @xtianus79 \n\nwhere can I see the proper key it is looking for (this would be the most helpful)?\n\nWhen trying to generate correct signature, it's always helpful to compare it with what our SDK is doing. It implements the same thing as in the doc, feel free to reach out if you have any questions reading it. The documentation is good but implementing a signer solely from it may lead to some issues hard to debug.\n\nformat YYYYMMDD >>> is that all that is needed?\n\nFor datetime used in signing, this is the only format you need.\n\ni can get everything working by simply removing the authorization and only passing over a date in the headers... but is that why the signature is failing.\n\nI don't think so. Although your request gets through without signing when the resource is public. But when you provide a signature, the service(s3 e.g.) will validate your signature even if the resource is public. If you are working with s3, it's generally not advisable to set your resource as public.\n. Actually, the SDK doesn't do any customization on this operation, so we shouldn't be different from CLI on the client side. You said the issue is not consistent can you confirm that the CLI always gives you the latest logs?. Looks good to me. Just have 1 unclear above.. Thank you. I will update with the service team to make the error message more helpful!. The service team has acknowledged this issue and working on a fix. I will close this issue for now. Feel free to comment here if you have further question.. Hey @bebu259 \nI wonder how do you get the body fileContents. If you are using FileReader to do the transform, it's possible the file is loaded to memory all at once. It may crash the browser. If that's the case you may need to do the transfer and upload the file chunk by chunk. \nTo upload chunked files you can use a serial of calls to accomplish: createMultipartUpload(), uploadPart(), completeMultipartUpload(). After the upload complete, s3 will automatically combine the parts for you. This is also how s3.upload() works under the hood.\nBasically when you call createMultipartUpload() you will get an uploadId, and s3 knows you will begin to throw them chunks of data. Then you upload your chunks by calling uploadPart() repeatedly and you need to supply the 'uploadId'. And after each call you will get an Etag. After you upload all the chunks you need to call completeMultipartUpload() with all the Etag you got to notify S3 to combine the parts for you.. Hey @christopher-dG, this documentation is not owned by us. Maybe you should reach out to Mapbox.. Thank you for calling this out. Actually, the timeout setting in both Node and browsers actually sets the timeout before the connection will be closed instead of the stream is in inactivity. We will update the documentation. \nyou can set the timeout to 0 AWS.config.httpOptions.timeout = 0, to make the client always wait until the connection is done. . Hi @roccomuso, currently this operation is only available in Java SDK. I'm not familiar with this API, maybe you can bring this up at the Kinesis Stream Forum to see how non-Java users going to push video streams.. Hi @cwardcode, I think this issue is more of the service feature request. Unfortunately, we don't have extra information on each service roadmap. You can directly communicate with service team on Cognito Forum. They will be happy to answer your questions.. @mcarriere-seedbox Thanks a lot for the contribution! Actually, I think we can remove that whole description now since none of the libs is needed unless you are targeting at es5 or even older version of JS.. You can update it to mention that if targeting at es5 or older ECMA standard, the libs should be added to tsconfig.. which version of SDK are you using? Does this exist on the newest version?. I don't think there's a method provided to determine the Lambda trigger. Right now you can have a flag entry in your event to differentiate the resource, but I agree it's better to be provided by the service. I will provide your feedback to the service team and turn back if I have response from them. You can also bring this out on the Lambda forum. The service team is happy to take feature request there as well.. Could you share the Nodejs SDK document about spot instances? And could you also elaborate what you are trying to do and how did you find the inconsistency between doc and the service functionality? I'm not familiar with this feature, not sure whether this is an SDK issue.. Hey @equinox7 \nSorry for the delayed response. I agree that removing the listeners on body stream is not a good idea. Not only this, the resuming stream is not an expected behavior either:\nhttps://github.com/aws/aws-sdk-js/blob/bd6d814c209ff1cf7f566d344873fe287f84dacc/lib/s3/managed_upload.js#L591\nThank you for bringing this to us! This unexpected behavior needs to be fixed, but currently I will do more investigation on whether the fix will effect the performance(memory usage).\nHowever in this case, according to Node stream doc, the end event won't be emitted unless all the content has been consumed. So here if the request is aborted, the end event should not emit based on node stream convention.\n. I cannot reproduce this issue. Can you check the whether the length of your PNG equals that you uploaded?. Hi @lenin-jaganathan \nWhat is the 'data: file' in your code snippet? Is it buffer or stream? Also can you check what is the value of the content-length header in your put request? It's possible the axios is setting the wrong content-length in putObject. Hey @ddemoll \nAccording to the doc, you can call updateEndpoint instead to use a new endpoint.. Hi @adewergifosse \nI don't think Lambda provides the API to do filtering or searching. Looks like even on the Lambda console they do the searching on the user side. This is a useful feature to have actually, you can bring the feature request to the Lambda forum. You can also have a try on the Tagging Service, by which you can find the resources only associated with your intent.. Hey @leonardoesx \nOur SDK does use the profile with role but it doesn't read the ~/.aws/config by default. To enable reading the config file you can set environment variable: export AWS_SDK_LOAD_CONFIG=1. In addition, if you want to load config file from else where, you can use another env variable. For example: export AWS_CONFIG_FILE=~/.aws/another_config.. Can you log the request you are sending. If you are using some libraries like request or axios, it's possible that some headers are automatically added to the request.\nI tested getting object with signed url a bit and I find especially only when using SSE, the default signing version, sigv2, will always lead to SignatureDoesNotMatch error. Although the signed url still works for operation without SSE-C keys. In this case you can try using sigv4 instead. You can try the code snippet bellow:\n```javascript\nvar encryptKey = /*buffer of encryption key/;\nvar encryptKeyHash = createHash('md5').update(encryptKey).digest('hex');\nvar s3 = new AWS.S3({signatureVersion: 'v4', region: 'us-west-2'});\nvar url = new URL(s3.getSignedUrl('getObject', {\n  Bucket: bucket,\n  Key: key,\n  Expires: signedUrlExpireSeconds,\n  SSECustomerAlgorithm: 'AES256',\n  SSECustomerKey: encryptKey,\n  SSECustomerKeyMD5: encryptKeyHash,\n}))\nvar options = {\n  host: url.hostname,\n  protocol: url.protocol,\n  path: url.pathname + url.search,\n  headers: {\n    'x-amz-server-side-encryption-customer-algorithm': 'AES256',\n    'x-amz-server-side-encryption-customer-key': encryptKey.toString('base64'),\n    'x-amz-server-side-encryption-customer-key-MD5': createHash('md5').update(encryptKey).digest('base64'),\n  },\n  method: 'GET'\n}\nvar req = https.request(options, function(res) {\n  console.log(res.statusCode)\n  res.on('data', (chunk) => {\n    //do something\n  })\n})\nreq.on('error', function(e){\n  console.log(e);\n})\nreq.end();\n```\nI need more investigation on why sigv2 in this case. I will update here when I find something.. I found actually you can still use default signer in this case. According to the S3 Doc, you only need to sign the 'x-amz-server-side-encryption-customer-algorithm' header. And to customized key is only needed to put in the request and doesn't need to be signed. Here is the alternative:\n```javascript\nvar encryptKey = /*buffer of encryption key/;\nvar encryptKeyHash = createHash('md5').update(encryptKey).digest('hex');\nvar s3 = new AWS.S3();\nvar url = new URL(s3.getSignedUrl('getObject', {\n  Bucket: bucket,\n  Key: key,\n  Expires: signedUrlExpireSeconds,\n  SSECustomerAlgorithm: 'AES256',\n//only this key is needed\n}))\nvar options = {\n  host: url.hostname,\n  protocol: url.protocol,\n  path: url.pathname + url.search,\n  headers: {\n    'x-amz-server-side-encryption-customer-algorithm': 'AES256',\n    'x-amz-server-side-encryption-customer-key': encryptKey.toString('base64'),\n    'x-amz-server-side-encryption-customer-key-MD5': createHash('md5').update(encryptKey).digest('base64'),\n  },\n  method: 'GET'\n}\nvar req = https.request(options, function(res) {\n  console.log(res.statusCode)\n  res.on('data', (chunk) => {\n    //do something\n  })\n})\nreq.on('error', function(e){\n  console.log(e);\n})\nreq.end();\n```. Sounds fair. Here's your \ud83d\udea2 . But please also make sure the compatibility in mobile devices.. Hi @andy-viv\nI cannot reproduce this issue. Can you show me the response you get and which region is the bucket in? It will also be helpful if you can paste a code snippet here showing how you post the form.. Hi @danielbdias, usually we don't alter the response. Can you please provide the original http response body you got? You can log them by this:\njavascript\nacm.describeCertificate(params, function() {\n        console.log('origin response: ', this.httpResponse.body.toString());\n})\nAnd you can try inspect the http response from CLI as well by attaching --debug at the end of command:\naws acm describe-certificate --certificate-arn XXXXXX --debug\nThen search for DEBUG - Response body: you will see the body. If the fields doesn't exist at all then it is possible the CLI adds some customization for this.. @danielbdias Thanks for the investigation. I believe this is why we cannot fetch the ResourceRecord at first. Also for your information, our SDK provides a waiter to validate the certificate. You can request a certificate and call waiter, SDK will call describeCertificate automatically for as long as 40 minutes. \njavascript\nacm.waitFor('CertificateValidated', {CertificateArn: 'XXX'}, (err, data) => {\n  //do something\n}). Hi @ronjouch \nIn a word, the error will only be set when the request has failed. It can either because of the response status code 4XX / 5XX or the request isn't sent or parsed properly by SDK. In this case, your lambda function is successfully invoked and response status code is 200 so the error shouldn't be set. Yes checking the data.Payload.errorMessage is what supposed to do here.. Hey @robertsj-vmware Thank you for bringing this up. Sometimes we don't get informed of the changes from service. We will get a fix for this soon.. Hi @nodren which SDK version are you using? Can you try using the newest version of SDK? . I cannot reproduce this issue. How do you import the SDK? Is this the only operation that disappears?. Hi @jchirschy When do you see these exceptions? We don't make any API calls when constructing the client. Have you tested locally with the same credentials?. Hi @maitien2004 \nWhere are you getting the endpoint from? Endpoint from the iot jobs data model should be automatically set to something like data.jobs.iot.{region}.amazonaws.com Then you don't need to set the endpoint.\nWhat makes the issue a little weird is that the service returns a successful response instead of a resourceNotFound exception. Can you first try to remove the endpoint setting in constructing the client to see if that solve the problem?. I'm not very familiar with DO service but usually, when you see this error message it means the service returns 4XX / 5XX but the SDK cannot parse the response message. This make sense because the SDK may look for amz-X headers to parse the error message. Can you show the raw HTTP response you get? . Can you elaborate the question? Usually if you use the putItem() or updateItem the old item will be updated if they have the same key, otherwise new item will be put into table. Without knowing the primary key the first question is to find what items you want to update. You can add secondary key to the attribute you want to index with. Or you can use scan operation. But this is generally not advisable because it may possibly traverse the whole table and it's slow. After found the items you can update them with updateItem or putItem. Hi @sashakorman By default, our SDK calculates the content hash for putObject. For big files, using upload of SDK is more preferable than putObject. I'm not sure why you mentioned the 500MB part, but the upload will automatically upload the object in 5 MB chunk by default and you will have lower memory load and even quicker rate because you can make 4(or more) parts upload parallelly. \nAlternatively, you can turn content hashing off by setting config computeChecksums to false: \njavascript\ns3.config.computeChecksums = false;\n. Hey @sashakorman  sorry, I misunderstood in my last reply. You set the big part size because you hit the part number limit. Currently, the multipart uploader will buffer the part so that it can be retried. It's a nice thing to have if we don't need to buffer the part. This is definitely a feature request. Thank you for bringing this up to us. Just by curiosity, is the stream you are trying to upload always be an fs stream or it can be any stream?. @sashakorman actually, I find even the stream signing will not help with this situation since the SDK will try to retry the parts so it will always put at least one part in memory. Unless you ensure the input stream is an fs stream so that it's possible that SDK can chop the chunks by byte range.. I agree with you. The AWS.Translate example in browser is not exact. The SDK get imported in the example is actually Browser SDK, which only has services that support CORS according to the service model we have. But our service model may not always up-to-date with the services. Seems like AWS.Translate does support CORS now, we will fix our code shortly. Thank you for bringing this to us.\nThis is a rough edge of SDK and we do mention this in the repo's README. Hi @ANTGOMEZ to make the SDK request, you can either supply a callback function or await a promise. Looks like you mixed the 2 ways in your code. Also, the d, that will be set asynchronously, is accessed in a synchronous way. You can do it like this:\njavascript\nasync function getS3Data() {\n  var s3 = new aws.S3();\n  var d;\n  try {\n      d = await s3.listObjectsV2(params).promise();\n  } catch(e) {\n      throw e // an error occurred\n  }\n  d.Contents.shift();\n  var content = [];\n  for (var currentValue of d.Contents) {\n    if (currentValue.Size > 0) {\n      var goParams = { Bucket: params.Bucket, Key: currentValue.Key };\n      var data;\n      try {\n        data = s3.getObject(goParams).promise();\n      } catch(e) {\n        throw e; //error\n      }\n      content.push({Size: data.ContentLength, Body: data.Body.toString()});\n    }\n  }\n  return content;\n}. Hi @riteshapatel, Can you share which documentation you are referring to?. Hey @riteshapatel Since this is not an SDK issue / question, you can bring the question to the forum. The service teams are happy to answer you questions. I will close this issue now but feel free to comment here is you have more questions.. Which SDK version are you using? Maybe you can switch to newest SDK version. These parameters are only added to SDK after v2.251. I will close this issue for now, if you still see the same issue please comment here.. Hi @NuonDara \nCan you provide the stack trace? The error message doesn't look like coming from our SDK for S3. I'm looking at the https://github.com/aws/aws-sdk-java/issues/1288. It looks like the connection is closed by S3, where does the filestream come from. S3 will close inactive connection after the timeout.. Hi @arivera-tiempodev Thank you for reporting this issue. I can reproduce it and it seems that this issue only happens in local DynamoDB and it works fine remotely. I'm not sure if this is a known issue, I will contact the service team to verify. You can also call out at the DynamoDB forum. One thing is sure that this is not an SDK issue. \nI will comment here if hear back from the service team.. I'm more inclined to have a new TemporaryCredential class(with a different name), that can be chained and refresh the master credentials recursively if master credentials are expired. The master credentials can be any credential classes that can be refreshed. I think this is what @jstewmon suggest.\nFor the naming I prefer something like ChainableTemporaryCredentials maybe there are better names out there. The yard doc failed at generating the doc for this page. I will open a PR fixing this shortly\nUPDATE: here you go: #2422 . One more note: Why didn't you differentiate the bucket name from the key in the path here and make sure only bucket is removed? Will this also solve the issue?. re-shipit!. Hi @olahivepriyanka, when you upload the file, if you set the content-type properly(like 'audio/mpeg'), the (browser)download file will have the .mp3 extension, no need to change the presigned url.. Hi @ddimitrioglo, thank you for calling this out, this issue has been fix by an API release recently.  Since in this way the lost member can be updated to all SDKs. I will close this PR for the problem having been solved. You can comment here if you have further question.. Hi @dawsbot, Thank you for opening this PR. We have noticed that our unit tests are failed at Node 9 & 10. This is because that the Domain.dispose is deprecated in newer versions. Luckily the Domain.dispose() is only used in one unit test so it won't affect our SDK compatibility in new Node versions. You are welcome if you'd like to work on updating those tests. If you have any concerns or need any assistance, you can comment here so we can help.. Hey @Ferrari,\nCan you share whats your S3 key and bucket like? Also, are you calling a cross-region S3 bucket? The most recent S3 servcie api change is 2.273.1 but we fixed an S3 retry bug in 2.297.0, otherwise there was no S3-specific code change within the version range. Can you specify how you are using the SDK, more detailed information will help us locate the potential issue.. Hi @acheronfail, this is a very good catch! We might also want to add the request argument here as a placeholder.. Hi @jortegamo, it looks like your cognito getId call succeeded but one of the S3 upload calls failed. I assume your file is smaller than 5 MB where SDK will try to upload your file within one single putObject call. A minor issue in your code is that your upload parameters: 'contentType' should be ContentType and x-amz-date shouldn't be set. SDK will do it for you. \nHowever, I've tried your code in browser but cannot reproduce this issue. I'm not testing it on Ligntening, you should also try to reproduce the issue without Lightening. I also notice the auraXhr=true in response. I'm not sure if this is made intentionally by S3, but I would check if your raw request has something like this, which would interfere the signature computation.. Hi @matwerber1 \nCan you make sure when you don't specify filters the response shows the instance is scheduled?. It seems like an issue on the service side. Or maybe we have some misunderstanding on how the scheduled instances are defined. I have reached out to service team internally. To get the response faster you can also bring this out on the ec2 developer forum, the service team will be happy to help you out.\nUPDATE: Since the describeInstance call doesn't return any lifeCycle configuration for scheduled instance, how did you confirmed these instances are actually in the scheduled state?. Hi @bittlingmayer Can you elaborate what the bug is? How can we reproduce this bug?. As a reference, the issue seems to be fixed in here: https://github.com/kelektiv/node-uuid/pull/219. . Hi @cdraper-CS \nThank you for bringing this up, we will add the feature to align with other AWS SDKs.. Hey @rix0rrr\nI'm not sure I understand your request. Can you import the httpOptions inferface like this?\ntypescript\nimport AWS = require('aws-sdk');\nimport {HTTPOptions, ConfigurationOptions} from 'aws-sdk/lib/config';\nconst httpOptions: HTTPOptions = {\n  timeout: 6000\n}\nconst config: ConfigurationOptions = {\n  region: 'us-west-2',\n  httpOptions\n}\nconst s3 = new AWS.S3(config);. Hey @rix0rrr \nI think this is a long-existing issue in the SDK that we didn't notice, thank you for bringing this up. It's the CredentialProviderChain extends from 'Credentials' but doesn't implement the methods(refresh and get).\nThis also leads to a problem when configuring the client, where config.credentials accepts Credentials class but supplying CredentialProviderChain class doesn't work, users should only supply it to config.credentialProvider. I need more investigation on whether changing the runtime JS code will affect any users but fixing the type definitions seems safer to do, by removing the extends Credentials.. Definitions for hand-written classes are hand-written, for example, all the credentials subclasses and public service customizations. All the definitions for clients are generated from the API models.. Definitions for hand-written classes are hand-written, for example, all the credentials subclasses and public service customizations. All the definitions for clients are generated from the API models.. Hey, @hanginwithdaddo I'm using typescript@3.0.3 and the similar compilerOptions as above but cannot reproduce it. \nCan you try to reinstall all the dependencies? And make sure you installed '@types/node' and add \"node\" to \"types\" config? Or maybe you can add \"es2015.promise\" to the \"lib\". You don't need to add this lib since you target at newer ES standard that doesn't require promise lib explicitly, but I'm just curious. Do you see any another compile issues that type cannot be found?. Hey, @hanginwithdaddo I'm using typescript@3.0.3 and the similar compilerOptions as above but cannot reproduce it. \nCan you try to reinstall all the dependencies? And make sure you installed '@types/node' and add \"node\" to \"types\" config? Or maybe you can add \"es2015.promise\" to the \"lib\". You don't need to add this lib since you target at newer ES standard that doesn't require promise lib explicitly, but I'm just curious. Do you see any another compile issues that type cannot be found?. Looks like there is no test covering this branch. Looks like there is no test covering this branch. I'm not sure I understand the issue. The first assumeRole() succeeds and you can access remote DynamoDB. Are you saying you see the errors if you call assumeRole later? If you see the MalformedQueryString exception from the assumeRole() call can you share what the query looks like?. I'm not sure I understand the issue. The first assumeRole() succeeds and you can access remote DynamoDB. Are you saying you see the errors if you call assumeRole later? If you see the MalformedQueryString exception from the assumeRole() call can you share what the query looks like?. Hi @janpoltan \nCognitoIdentityCredentials will use STS.assumeRoleWithWebIdentity() instead of Cognito identity  if you supply the RoleArn. We mention this in the documentation. I this case you can try remove the RoleArn param. If you want to assume a role to call AppSync you can always call STS.assumeRole() later.. Hey @ossdev07 Thank you for your PR. This is a very helpful change. I will test it on our release test environment first.. Just add a little more information to @srchase reply. The SDKs currently don't have enough information on how the URL is constructed or whether they are generated in the same way across all the regions. It would be more reliable to have the URL in the Lambda event.. Hey @jpb Thank you for the contribute. This is the feature I would like to add into SDK. Note that we recently added support for shared ini file cache #2283. You might need to update your PR according to that.. Hey @jpb Thank you for the contribute. This is the feature I would like to add into SDK. Note that we recently added support for shared ini file cache #2283. You might need to update your PR according to that.. @sashakorman Yes. clearCacheFiles() is a public operation that you can call explicitely.. @sashakorman Yes. clearCacheFiles() is a public operation that you can call explicitely.. I did a search, we do not depend on jQuery anywhere and jmespath doesn't depend on jQuery either. We are not affected by this.. I did a search, we do not depend on jQuery anywhere and jmespath doesn't depend on jQuery either. We are not affected by this.. I'm more curious about the root cause of sending a part twice. We do have logic to prevent part from being uploaded twice here and the part number is only monotonically increasing, so this is weird. Do you read the file stream or using buffer when trying to reporduce the issue? Can you try to log the uploadPart() parameters associated to uploading part with same ETag and make sure they have the same part number?. I'm more curious about the root cause of sending a part twice. We do have logic to prevent part from being uploaded twice here and the part number is only monotonically increasing, so this is weird. Do you read the file stream or using buffer when trying to reporduce the issue? Can you try to log the uploadPart() parameters associated to uploading part with same ETag and make sure they have the same part number?. @sashakorman Hey the fix is merged in by the PR above. You are welcome to reply here if you have progress on finding the root cause for this issue.. @sashakorman Hey the fix is merged in by the PR above. You are welcome to reply here if you have progress on finding the root cause for this issue.. Looks good! Can you also add a unit test for the change? You can refer to the test here. Instead of spy on STS.assumeRole(), you can spy on the node httpClient to confirm if the httpOptions is supplied to the client. like this:\njavascript\nvar spy = helpers.spyOn(httpClient, 'handleRequest').andCallThrough();\nexpect(spy.calls.length).to.eql(1);\nexpect(spy.calls[0].arguments[1]).to.eql(/**httpOptions you set*/)\nThe code change works but usually we need a unit test to make sure we don't accidentally break the feature in the future.. Looks good to me. Can you rebase your commit and force update this PR? And then :shipit: . Hey @woodpav, \nI went through the old amplify issue, it looks like issues with babel working together with webpack. Are you using webpack in your project?. Hi can you share your full dependencies? I'm having trouble reproducing this issue.. Hey @sebasmurphy,\nThe API models in the apis folder are released from service team and only vendored by SDK. If we just update it in this repo, next time SQS releases a new feature, the fix will be overwriten. Maybe you can bring this up to the service team on the forum?\nI will close this PR because the issue cannot be fixed by updating SDK.. Hey @sebasmurphy,\nThe API models in the apis folder are released from service team and only vendored by SDK. If we just update it in this repo, next time SQS releases a new feature, the fix will be overwriten. Maybe you can bring this up to the service team on the forum?\nI will close this PR because the issue cannot be fixed by updating SDK.. Codecov seems to fail because of untracked changelog change. Codecov seems to fail because of untracked changelog change. Hey @jstewmon I'm going to go a head and merge this change. Would you mind to rebase out the conflicts?. Hey @jstewmon I'm going to go a head and merge this change. Would you mind to rebase out the conflicts?. Hi @gsuess \nThe API models in the apis folder are released from service team and only vendored by SDK. So I will take this as a feature request and forward this to the service team. You can also bring this up to service team on the forum.\nI will close this PR for now but will keep you updated. You can comment here if you have further questions.. Hi @gsuess \nThe API models in the apis folder are released from service team and only vendored by SDK. So I will take this as a feature request and forward this to the service team. You can also bring this up to service team on the forum.\nI will close this PR for now but will keep you updated. You can comment here if you have further questions.. Thank you @rohit1018 @Temkit . This is possibly related to the endpoint discovery feature. We are investigating this.. Thank you @rohit1018 @Temkit . This is possibly related to the endpoint discovery feature. We are investigating this.. Hey,\nThis issue is raised from isEndpointDiscoveryApplicable() where the environmental variable AWS_ENABLE_ENDPOINT_DISCOVERY is inspected. Actually we shouldn't pull in env in browser platform. I have a PR #2348 out for review. Would you have a look at it? @rohit1018 @Temkit ?. Hey,\nThis issue is raised from isEndpointDiscoveryApplicable() where the environmental variable AWS_ENABLE_ENDPOINT_DISCOVERY is inspected. Actually we shouldn't pull in env in browser platform. I have a PR #2348 out for review. Would you have a look at it? @rohit1018 @Temkit ?. Hey @tambry Your point is very reasonable. Instead of changing the typing casing maybe we should add the correct one to the type definition. We can keep the wrong typing just not to break peoples building?(although I don't feel comfortable with it). Hey @tambry Your point is very reasonable. Instead of changing the typing casing maybe we should add the correct one to the type definition. We can keep the wrong typing just not to break peoples building?(although I don't feel comfortable with it). Hey @tambry after thinking this over again I prefer to make the actual change that may break people's build. First of all, this change is not an actual breaking change that will break the prod environment. The build process will fail-fast before the app as deployed on the machine. And we will mention it in the changelog to help people debug the issue. \nThe other reason is that customers would fail on this change because they are using the config key that doesn't work. Maybe they think they are using dualstack but they never did.  The building failure will remind them to correct their config.. Hey @tambry after thinking this over again I prefer to make the actual change that may break people's build. First of all, this change is not an actual breaking change that will break the prod environment. The build process will fail-fast before the app as deployed on the machine. And we will mention it in the changelog to help people debug the issue. \nThe other reason is that customers would fail on this change because they are using the config key that doesn't work. Maybe they think they are using dualstack but they never did.  The building failure will remind them to correct their config.. @jasdel feedbacks addressed. It's a very good point.. @jasdel feedbacks addressed. It's a very good point.. Hi @Yijx,\nJust add some details to @srchase. Currently upload() method doesn't come with the ability to resume aborted upload. If you config leavePartsOnError: true and abort the the upload, the uploaded parts will be saved in the S3 invisibly(and you will be charged for those parts. Have a look at this doc to save your expense) and you can resume the upload by only uploading the missing parts. But within the MultipartUpload request object, all the internal state will be reset. So if you call send() again it will try to start from beginning. You will see error because you would re-upload the same part.\nTo resume the upload from some of parts already being uploaded, you need to handle the upload by your self instead of using upload(). How to do it depends on your use case, but you can refer to S3 multipart upload documentation for more information, which is also what SDK does under the hood.\nI haven't used any but there might be some NPM packages that could help with the use case, you can have a look at there.\n. Hi @Yijx,\nJust add some details to @srchase. Currently upload() method doesn't come with the ability to resume aborted upload. If you config leavePartsOnError: true and abort the the upload, the uploaded parts will be saved in the S3 invisibly(and you will be charged for those parts. Have a look at this doc to save your expense) and you can resume the upload by only uploading the missing parts. But within the MultipartUpload request object, all the internal state will be reset. So if you call send() again it will try to start from beginning. You will see error because you would re-upload the same part.\nTo resume the upload from some of parts already being uploaded, you need to handle the upload by your self instead of using upload(). How to do it depends on your use case, but you can refer to S3 multipart upload documentation for more information, which is also what SDK does under the hood.\nI haven't used any but there might be some NPM packages that could help with the use case, you can have a look at there.\n. Hey @AndrewBarba Can you try if you still see the issue when not using Serverless?. Hey @AndrewBarba Can you try if you still see the issue when not using Serverless?. @AndrewBarba Have you try remove your node modules and package-lock and install all the packages again? I tried yarn but it still works for me.. @AndrewBarba Have you try remove your node modules and package-lock and install all the packages again? I tried yarn but it still works for me.. looks good. Have you tested with S3?. looks good. Have you tested with S3?. Approved. Just have one more minor change. Approved. Just have one more minor change. Hey @marcpalm \nThanks a lot for bringing this to us. Currently the dynamodb type definition a generated from the API model vendored by service team. Why do you think the value should be any though?. Hey @jordanryanmoore \nThanks a lot for reporting this issue. I have created a PR fixing this issue. Any reviews are welcome!. Hey @marcpalm \nThe type definition of document client corresponding to the document you are referring to should be already any type. The document client is a subclass of dynamodb client that has a different type definition. The change you propose is for DynamoDB client which is generated from the service model.\nI will close this PR since this is a non-issue. But feel free to re-open it if you have other issues to address.. Hi @sm2017 \nThis issue could be either ManagedUpload issue or the promise wrapper issue. Can you try using callback style and see if error can be supplied to callback?\njavascript\ns3.upload({\n    Bucket: 'buket',\n    Key: 'key',\n    Body: readableStream,\n}, function(data, err) {\n    if (err) console.log('error: ', err)\n}). Hi @prismhr-alexandraz \nCan you specify the trouble you ran into? which operation are you calling? The SDK version?\nThis seems to be an issue on service side, SDK doesn't do any customization for SMS service. But I can possibly bring this to the service team.. Hey @prismhr-alexandraz \nAccording to SNS docuemnt, you need to request for a special sender id from AWS support. And here is how you can request one. I hope this is helpful.. Hi @LamaBimal \nAre you seeing this error when evoking Lambda or lambda trying to fetch the resource? The exception shows your request is throttled, you can either limit how many calls you make every minute or contact the service to increase your limit.. Thanks for taking care of this. Can you specify in the change log that this change will make KinesisVideoMedia available by default in browser SDK?. Hi @jstewmon \nThanks a lot for submitting this PR. It looks great to me. \nThis fix is what I'd like to see but I really hope IMDS can solve this problem.. Can you add change log to this PR? Simply run npm run add-change and add an entry of feature(I think this is more of feature than bugfix). Hey @richardsengers \nGiven the error message is 'Incorrect username or password' instead of incorrect signature, I think the request got through but the permission is not correct or parameters are incorrect. Can you share the code of configuring the credentials and making call with RespondToAuthChallenge() operation? \nWhat could be helpful is to setup a new NG app then try calling the same operation to see whether same issue still happens.. Approval still stands!. Hi @andrewryan1906 \nCan you share the code snippet of how you call the getSignedUrl?. Hi @andrewryan1906 \nLooks like the credentials are resolved asynchronously. You shall supply a callback to the call:\njavascript\ns3Client.getSignedUrl('getObject', timelineRetrievalParams, (err, data) => {\n    const timelineDownloadUri = data;\n})\nThis change may due to some behavior change introduced by #2451, I'm investigating that why that's the case. \n. @andrewryan1906 \nThis is a breaking change introduced in the PR I mentioned above. The default credential provider chain should be resolved synchronously but this change makes the default resolution unsynchronous. I will create a CR to address it.. Hi @andrewryan1906 \n\nAm I correct in assuming the reason why I've never observed this behavior before is that this is the first time in my code that I happen to NOT precede this call with an async s3Client operation?\n\nThis is correct. making API call will asynchronously load the credentials to the client so that the following getSignedUrl has resolved credentials\n\nso I have to use a callback, instead of the async/await\n\nIt depends on what credentials are you using. If you are using the default credentials provider chain, you can use resolvePromise() to load the credentials prior to get signed url:\njavascript\ns3.config.credentials = await s3.config.credentialProvider.resolvePromise();\n. Hey @Clete2,\nThe TypeScript definitions provided in the SDK are mainly the I/O shape of the API. Is the event shape you posted here retrieved by calling any PinPoint API? I cannot find any corresponding operation.. @Clete2 Can you elaborate where the event is from? I assume the data comes from the Lambda function kinesis trigger, which originally generated from pinpoint. Am I right? In this case I don't think the feature request falls within the SDK scope. Because there is not API corresponds to this Typescript definition.. Hi, @VTLee \nLooks like a dup of #2433. We have deployed a fix on that, you can try out the newest version of SDK.. Hey, @SeongwoonHong \nYou need to use promise() to await for a request. So the request should be like:\njavascript\nawait s3.deleteObject(params).promise();. Hi @GeekStocks,\nThank you for the advice. We are working with Lambda team to get the SDK updated and figure out a long term solution to solve this. Publishing public Lambda layer can be a good solution. We also want to make customers avoid extra configurations.\nI will response here if we have any updates on this issue.. Hey @chris-pardy I think this is a valid feature request. The the typescript definition doesn't take normal parameters from client construction into consideration. The TS definitions still need the TableName as required since many other users need type security when calling method with all it's parameters. If you have a better idea to solve the issue I'm more than happy to review it. \nIMHO the TS interface system doesn't offer a good way to validate the typing according to the runtime code.. Hey @chris-pardy I think this is a valid feature request. The the typescript definition doesn't take normal parameters from client construction into consideration. The TS definitions still need the TableName as required since many other users need type security when calling method with all it's parameters. If you have a better idea to solve the issue I'm more than happy to review it. \nIMHO the TS interface system doesn't offer a good way to validate the typing according to the runtime code.. Hey @teroxik,\nBoth Lambda and Fargate JS SDK use metadata service for credential. This looks like a connection issue on the service side not the SDK itself. Maybe you can bring it to the service team on Fargate forum and give them some visibility on this issue.. Hey @teroxik,\nBoth Lambda and Fargate JS SDK use metadata service for credential. This looks like a connection issue on the service side not the SDK itself. Maybe you can bring it to the service team on Fargate forum and give them some visibility on this issue.. Hey @rambabusaravanan,  the string here is to make the types backward-compatible. You can specify the API version in the client constructor and the old APIs may support different enum values. However, the TS definitions are generated based on the newest API version. If the string is remove, it may break other customers in the build time. I will tag this as feature request for now.. Hey @rambabusaravanan,  the string here is to make the types backward-compatible. You can specify the API version in the client constructor and the old APIs may support different enum values. However, the TS definitions are generated based on the newest API version. If the string is remove, it may break other customers in the build time. I will tag this as feature request for now.. Hey @chris-pardy \nThis is very awesome, I like the approach it takes very much. I have tried it a little bit and the typings work as expected. I will have a look at the ts-generator part tomorrow. One thing makes me a little concern is that the intellisense in VSCode is unreadable if I import the interface explicitely, like DocumentClient.DocumentClientOptions.. Hey @chris-pardy \nThis is very awesome, I like the approach it takes very much. I have tried it a little bit and the typings work as expected. I will have a look at the ts-generator part tomorrow. One thing makes me a little concern is that the intellisense in VSCode is unreadable if I import the interface explicitely, like DocumentClient.DocumentClientOptions.. The change looks good to me overall. The reason for unreadable intellisense of DocumentClient class is the circular generic referencing between class and generic. I have only one question that left in previous comment. . The change looks good to me overall. The reason for unreadable intellisense of DocumentClient class is the circular generic referencing between class and generic. I have only one question that left in previous comment. . Hi @chris-pardy \nI think my argument doesn't hold true now. Maybe for the reason of that the latest VSCode uses TS3.3.X for code hint, and they have better type union support now, the code hint is actually pretty clean in most of cases. However all the subclasses(Dynamodb.DocumentClient, S3.ManagedUpload) still have unreadable code hint. Since people uses s3.upload rather than S3.ManagedUpload constructor, the thing really annoys me is the Dynamodb.DocumentClient code hint. \nNow I'm fine with progressing this feature with this known limitation because it enables writing Typescript in the way we documented in JS. And the limitation will only have comparatively small effect on IDE experence and won't effect on the existing code compile. I have a few more comments. You also need to add a changelog for this commit.\ncc @srchase for review.\n. Hi @chris-pardy \nI think my argument doesn't hold true now. Maybe for the reason of that the latest VSCode uses TS3.3.X for code hint, and they have better type union support now, the code hint is actually pretty clean in most of cases. However all the subclasses(Dynamodb.DocumentClient, S3.ManagedUpload) still have unreadable code hint. Since people uses s3.upload rather than S3.ManagedUpload constructor, the thing really annoys me is the Dynamodb.DocumentClient code hint. \nNow I'm fine with progressing this feature with this known limitation because it enables writing Typescript in the way we documented in JS. And the limitation will only have comparatively small effect on IDE experence and won't effect on the existing code compile. I have a few more comments. You also need to add a changelog for this commit.\ncc @srchase for review.\n. I think it's legit to bump the Typescript version. It won't break people in their product environment so I think it isn't strictly a 'breaking' change. And we haven't bump Typescript version since the day we support Typescript. . I think it's legit to bump the Typescript version. It won't break people in their product environment so I think it isn't strictly a 'breaking' change. And we haven't bump Typescript version since the day we support Typescript. . Hey @santriseus \nDo you see any other suspicious errors? If you don't see any error and the callback is never triggered, it's possible that the request and send but never responded. If you wait for more than 2 mins(default timeout, may varies according to your http config), you might see the callback being triggered. The errors in the callback is helpful to diagnose this issue.. Hey @santriseus \nDo you see any other suspicious errors? If you don't see any error and the callback is never triggered, it's possible that the request and send but never responded. If you wait for more than 2 mins(default timeout, may varies according to your http config), you might see the callback being triggered. The errors in the callback is helpful to diagnose this issue.. The SDK in EC2 uses ec2 metadata service whereas the local machine uses credentials in shared credential file. Maybe the process hangs at credentials resolvation. Can you try calling the metadataservice and see if you can get correct credentials.\njavascript\nconst metadataService = new AWS.MetadataService(clientConfig);\nmetadataService.loadCredentials((err, creds) => {\n    if (err)console.log(err);\n    else console.log(creds);\n})\nIf the credentials works, as a work around for this issue, you can explicitely set the AWS credentials with the credentials loaded.. The SDK in EC2 uses ec2 metadata service whereas the local machine uses credentials in shared credential file. Maybe the process hangs at credentials resolvation. Can you try calling the metadataservice and see if you can get correct credentials.\njavascript\nconst metadataService = new AWS.MetadataService(clientConfig);\nmetadataService.loadCredentials((err, creds) => {\n    if (err)console.log(err);\n    else console.log(creds);\n})\nIf the credentials works, as a work around for this issue, you can explicitely set the AWS credentials with the credentials loaded.. Hey @BenjD90 Thanks for the easy-to-reproduce cases! I will look into this issue. Interestingly the memory usage is mainly in rss and external. I suspect it has something to do with the streams as @victorboissiere points out, especially the stream buffering. \nWhen doing multi-part upload, the SDK buffers the stream until the buffer reaches the size of the partSize you specify(default 5MB) before making each uploadPart request. When you spin up 100 s3.upload streams, sdk will do the same thing to all these stream, so that you see SDK uses a lot of rss. But the buffer usage for each stream is fixed. I believe you will see less memory usage if you upload the same amount of data but concat them beforehand and use a single stream. \n. Hey @BenjD90 Thanks for the easy-to-reproduce cases! I will look into this issue. Interestingly the memory usage is mainly in rss and external. I suspect it has something to do with the streams as @victorboissiere points out, especially the stream buffering. \nWhen doing multi-part upload, the SDK buffers the stream until the buffer reaches the size of the partSize you specify(default 5MB) before making each uploadPart request. When you spin up 100 s3.upload streams, sdk will do the same thing to all these stream, so that you see SDK uses a lot of rss. But the buffer usage for each stream is fixed. I believe you will see less memory usage if you upload the same amount of data but concat them beforehand and use a single stream. \n. Hey @akefirad \nHave you added corresponding permissions for cross domain requests in the extension's manifest file?. Hey @akefirad \nHave you added corresponding permissions for cross domain requests in the extension's manifest file?. Hey @coxom,\nI suspect this has something to do with the keep-alive settings. The connection might be shut down by intermediate proxies within CodeBuild or Lambda. Can you try enabling socket keep-alive to see whether that mitigate the issue.\nSee: https://nodejs.org/api/net.html#net_socket_setkeepalive_enable_initialdelay . Hey @coxom,\nI suspect this has something to do with the keep-alive settings. The connection might be shut down by intermediate proxies within CodeBuild or Lambda. Can you try enabling socket keep-alive to see whether that mitigate the issue.\nSee: https://nodejs.org/api/net.html#net_socket_setkeepalive_enable_initialdelay . @lorengordon\nThe child_process.exec() spins up a subprocess but only call the callback functions until the process returns, which means by the time you see the stderr content, the subprocess has returned. To support interactive CLI, maybe child_process.spawn() makes more sense here. Because you can wire up the main process' stdin to subprocess' stdin, and supply the stdin to the running subprocess. One thing to note is that while the credential process is spun up yet not return, the SDK excecution should be blocked to avoid more processes are spun up. \n. @lorengordon\nThe child_process.exec() spins up a subprocess but only call the callback functions until the process returns, which means by the time you see the stderr content, the subprocess has returned. To support interactive CLI, maybe child_process.spawn() makes more sense here. Because you can wire up the main process' stdin to subprocess' stdin, and supply the stdin to the running subprocess. One thing to note is that while the credential process is spun up yet not return, the SDK excecution should be blocked to avoid more processes are spun up. \n. Hey @Usamaliaquat123 \nCan you check if your system clock is correct? You will see this exception when your local clock is ahead of the service clock. You can turn on the correctClockSkew config to solve this by const dynamodb = new AWS.DynamoDB({correctClockSkew: true}). There SDK will calculate the clock skew and apply the offset to the request automatically. \nsee: https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#correctClockSkew-property. Hey @Usamaliaquat123 \nCan you check if your system clock is correct? You will see this exception when your local clock is ahead of the service clock. You can turn on the correctClockSkew config to solve this by const dynamodb = new AWS.DynamoDB({correctClockSkew: true}). There SDK will calculate the clock skew and apply the offset to the request automatically. \nsee: https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#correctClockSkew-property. Hey @tan31989 \nUnfortunately dropping support of old node version requires further discussions. We prefer not to drop any node version we currently support(since 0.10).. Hey @tan31989 \nUnfortunately dropping support of old node version requires further discussions. We prefer not to drop any node version we currently support(since 0.10).. Hey @bodyslam \n\nI've had success generating temporary credentials via the aws-cli and storing the creds into environment vars but I have no idea how to inject these temp creds into the sdk's config object\n\nYou can set EnvironmentCredentials explicitly to make use of the environment vars:\njavascript\nconst s3 = new AWS.S3({\n    region: this.REGION,\n    credentials: new AWS.EnvironmentCredentials('AWS')\n//'AWS' is env prefix, if your env looks like 'AWS_ACCESS_KEY_ID'. \n});\n//access your object\n\nbut on a restart we're required to pass in the MFA code\n\nYou can automate this process by using a MFA token callback like below(if applicable). But you need to setup the role settings in shared credential files to use the MFA with CLI\njavascript\nconst s3 = new AWS.S3({\n    region: this.REGION,\n    credentials: new AWS.SharedIniFileCredentials({\n        profile: this.PROFILE_NAME\n        tokenCodeFn: (mfaSerial, done) => {\n            const token = fetchMFAToken();\n            if (token) done(undefined, token);\n            else done(new Error('cannot fetch token'));\n        }\n    });\n});\n//access your object\n. Hey @bodyslam \n\nI've had success generating temporary credentials via the aws-cli and storing the creds into environment vars but I have no idea how to inject these temp creds into the sdk's config object\n\nYou can set EnvironmentCredentials explicitly to make use of the environment vars:\njavascript\nconst s3 = new AWS.S3({\n    region: this.REGION,\n    credentials: new AWS.EnvironmentCredentials('AWS')\n//'AWS' is env prefix, if your env looks like 'AWS_ACCESS_KEY_ID'. \n});\n//access your object\n\nbut on a restart we're required to pass in the MFA code\n\nYou can automate this process by using a MFA token callback like below(if applicable). But you need to setup the role settings in shared credential files to use the MFA with CLI\njavascript\nconst s3 = new AWS.S3({\n    region: this.REGION,\n    credentials: new AWS.SharedIniFileCredentials({\n        profile: this.PROFILE_NAME\n        tokenCodeFn: (mfaSerial, done) => {\n            const token = fetchMFAToken();\n            if (token) done(undefined, token);\n            else done(new Error('cannot fetch token'));\n        }\n    });\n});\n//access your object\n. Hey @bodyslam \nYour approach looks reasonable to me. Can you elaborate how do you 'Autenticate as myself', 'switch to the readonly role' and 'get the temp creds'?. Hey @bodyslam \nYour approach looks reasonable to me. Can you elaborate how do you 'Autenticate as myself', 'switch to the readonly role' and 'get the temp creds'?. @bodyslam If you use this code in Lambda there's a chance that Lambda doesn't support the newer SDK versions, so you might need to bundle your SDK. \nThe original PR provides an example for using the tokenCodeFn:#2126. @bodyslam If you use this code in Lambda there's a chance that Lambda doesn't support the newer SDK versions, so you might need to bundle your SDK. \nThe original PR provides an example for using the tokenCodeFn:#2126. Hey @iffyio \nIn this case the request itself actually succeeded but because you didn't start to read from the response stream, the response stream somehow ended after reading the first chunk. The end event makes SDK believes the response is ended properly and starts to verify the length of response. Since you only got 1 chunk, you saw this content length mismatch exception. The error was emitted on stream object because this is a verification on the stream, the request itself was successful. . Hey @iffyio \nIn this case the request itself actually succeeded but because you didn't start to read from the response stream, the response stream somehow ended after reading the first chunk. The end event makes SDK believes the response is ended properly and starts to verify the length of response. Since you only got 1 chunk, you saw this content length mismatch exception. The error was emitted on stream object because this is a verification on the stream, the request itself was successful. . Seems like there's no new line in any test files. Probably because they are generated from other scripts previously.. if global systemClockOffset is not set, the AWS.util.date.getDate will have the same effect as new Date(). I think the AWS.util.date.getDate can be used as a 'backup' whenever service can not be specified, just like in here (https://github.com/AllanFly120/aws-sdk-js/blob/master/lib/credentials.js#L95). Actually, this may lead to wrong output. For some shape, it' sensitive if its prototype (like 'S19') is sensitive. I think shape.sensitive === true || shape.prototype && shape.prototype.sensitive === true can be a workaround.. Oh right. Thank you for pointing out.. I will add a note to add the exposed header to documentation along with default value in config. Since the feature is defaulted to disabled on the browser, the users who want to enable the checksum they should be able to see documentation.. yes. empty md5 hashing would be '1B2M2Y8AsgTpgAmY7PhCfg=='. Mark: This part will only be performed on Browser.. As we discussed last time, can I use downloadTrailingChecksum config name instead? On the Node it will default to 'md5' and on browsers this config won't be set.. indentations are still not correct. will update. found this when going through it. will update. That makes sense. . Thank you for pointing this out. I will fix it and add test accordingly.. Do you mean the case when the first arriving chunk is checksum but the data chunks haven't arrived yet? Is this case even possible?. I have a unit test that I mock the http.request. In that case the stream is not available.. it might be inevitable. Here data may come earlier than error event get thrown and stream unpiped. Then we need to access instantiated objects to do the transform, in which case if we return too early, we may access undefined objects. Here calling callback twice won't actually write anything to stream. I will fix it though. Ah right! The change mean to fix the test. But actually we can remove it out of node test. I didn't realize this is browser-only although the name already told me. here is where is issue is and I changed to AWS.util.merge. Actually this fix doesn't make sense, instead I should allow merge function to merge undefined or null. All this helper thing is for scalability. I'm thinking of in the future we may need to migrate more tests from cucumberJS. So we don't need to attach this in every test file. This is for sharing the test bucket. Maybe this time occasionally we fail to delete the bucket. But next time we will delete this bucket.. Yes, you can pass in the bucket and key, but right, it will change the global state. But capsulizing them will also guarantee putObject always succeed(bucket always there).. yea. I will move the putObject to before trait. And use timestamp in bucket name. I was thinking that we can write less code if we are going to add more tests here. But now I agree that this is an overkill and not worthwhile. . Do we need &apos for ' here?. if we are not supporting Stream V1, you probably need to run this test only in stream v2. Like this:\njavascript\nif (AWS.HttpClient.streamsApiVersion === 1) return;. extra indentation here. Can you remove the changes from S3 model?. same here.. Is this duplicate with options.readableObjectMode = true;?. This name is a bit confusing to me. buildMessage is probably a better name. Same for the builderHeaders. There's no issue here but a little hard to follow.  Will using another if statement be more readable? like this:\njavascript\nif (!stream || !stream.didCallback) {\n//don't concat response chunks when using event streams unless response is unsuccessful\n    if ((AWS.HttpClient.streamsApiVersion === 2) && operation.hasEventOutput && service.successfulResponse(resp) ) {\n         return\n    }\n   resp.request.emit('httpDone');\n    done();\n}\nI think it's more readable(?) It's your call.. what's this class used for?. So nice to have this!. This is a good question. Only S3 has deferent timestampFormat currently(rfc822 in metadata timestamp format but iso8601 according to this spec). But most of the timestamp members exist in headers so the timestamp format should still be rfc822. This only one exception is ResponseExpires. Unfortunately this is a long-live bug currently, our SDK doesn't properly serialize this member so that the response can not be parsed correctly. This change fixes the issue.. yes. . This needs to be published before we merge in. I put them this way to get around named exporting. So what I'd like to do is require('shared-ini').loadFrom() in commonJS or import {loadFrom} from 'shared-ini' in esm. But in type definition I cannot do default export(export default) of a singleton instance. So I have to explicitely export the public properties of the instance I'd like to export.. beforeAll isn't in the test suites. And there are only this 2 tests set AWS_SDK_LOAD_CONFIG. I think there's no need to reset and restore the process.env fo this suite.. I think you might be confused this with the required endpoint operation. If endpoint operation is optional we make the endpoint request asynchronously, which means the actual API call has been made with regional endpoint already. The endpoint request will retry until it succeeds and updates the cache. On the other hand, if endpoint operation is required then we must wait until endpoint operation returns us the new endpoint then we can make actual API call. \nThe general design implements the 'retry forever' because endpoint operation is supposed to be 'always available'.. It is a typo. And my auto-completion respects my typo of this name elsewhere. Thank you for pointing this out. Currently, there will always be 1. It will return multiple endpoints only after we have a story to support endpoint weight. It will look for the endpointDiscoveryId in depth 2. According to the design, endpointDiscoveryId is also possible to exist as a required string member inside a top level required structure member.. Operation and Identifiers are always supplied. If the endpoint operation input shape doesn't include any of them, marshaller will exclude them anyway.. Just trying to clear things to me. Are you trying to limit the concurrent credential refresh requests? I'm not sure if this is necessary. If a customer supplies different static master credentials objects for different service clients individually. Are they expected to get all the same temporary credentials? If that's the case, it might be a breaking change. For example:\njavascript\nvar masterCredsA = new AWS.Credentials({accessKeyId: 'keyA', secretAccessKey: 'secretA'});\nvar masterCredsB = new AWS.Credentials({accessKeyId: 'keyB', secretAccessKey: 'secretB'});\nvar clientA = new AWS.S3({credentials: new AWS.ChainableTemporaryCredentials(param, masterCredsA)});\nvar clientB = new AWS.S3({credentials: new AWS.ChainableTemporaryCredentials(param, masterCredsB)});\n//will 2 client load the same temp credentials?\nCan you also add a block comment to document this method?. Can you separate this to a different PR and fix all the semicolon warnings?. minor: remove the block. Can you make the example emphasizes how ChainableTemporaryCredentials different to TemporaryCredentials or show credentials are refreshed in chain?. Do you actually want to reset the callbacks array?. setImmediate is not a standard API. It may break in some versions of headless Chrome. You can use setTimeout. I don't know why it passes the CI but this test case should only run in node environment because some providers(like EC2MetadataCredentials) are only available in Node(see AWS.util.isNode()). Why are we removing these tests?. My example above may not be true. What if setting different clients with different credentials with different role names, will they end up having the same credentials?. Thank you for the detailed reply. This a very nice update actually.. I see. I may spend a little more time to review the unit tests changes. The diff is not as clear as it should be.. Just a thought: supporting MFA will help a lot whenever we use assumeRole. This is also what differs from the TemporaryCredentials. Generally, I will suggest people using this class over of the TemporaryCredentials class. . The examples here needs to be updated.. Are you trying to make sure Bucket is ready moved from path to host? This statement fixes the issue but I'm wondering what will happen if the request is using dualstack endpoints, will the appended statement always true?\nI think you can be more specific in the if condition. For example you can check if Key parameter exists in the request path and Bucket parameter not in the path and [optional] Bucket parameter is appended to host. suggestion\n          //path only contains key or path contains only key and querystring. @cjyclaire statusCode always exists so I remove the redundant check at line 309. We might need to make the statement more generic to all the parameters that not signable in V2 not just for Range. It may looks clearer if phrased like The default signer allows altering the request by adding corresponding headers to set some parameters(e.g. Range) and these added parameters won't be validated by signature. You must use signatureVersion v4 to enforce the inclusion and exact matching of header and signed URL. suggestion\nimport {AWSError} from '../error';. suggestion\n        data.$UploadedViaMultipart = true;. I know the Params is required here but why does it have any thing to do with the typings?. Extra lines. You also need to update the typings for S3.ManageUpload (https://github.com/aws/aws-sdk-js/blob/master/lib/s3/managed_upload.d.ts). You might need a similar test for S3.ManageUpload. Should this be this.accessKeyId && this.secretAccessKey?. If credData is invalid and err is constructed, the credData will still be supplied to callback. We usually expect second parameter to be null if err exists. nit: indentation. Nit: you can put these unit tests to a test suite(like leading with describe('credential process')). So than you can clear the spies easily. . ",
    "SirCameronMcAlpine": "Ok, thanks. Will give it a go.\n. ",
    "aleksandersumowski": "Thanks! I've actually found a Clojurescript API for the service I was interested - DynamoDB:\nhttps://github.com/nervous-systems/hildebrand\n. ",
    "sobytes": "Ok so i have managed to build the common js module using titaniumifier.\nhttps://s3.amazonaws.com/samuel.east/aws-sdk-commonjs-2.2.15.zip\nBut getting some errors when running.\nvar AWS = require('aws-sdk');\n[ERROR] :      message = \"Object prototype may only be an Object or null.\";\nDebugging now\n. Here is the error line 42568.\nctor.prototype = Object.create(superCtor.prototype, {\n                    constructor: {\n                        value: ctor,\n                        enumerable: false,\n                        writable: true,\n                        configurable: true\n                    }\n                });\n. ",
    "anthonychung": "Titanium has mechanisms to map the IOS AWS-SDK to a native titanium module. However, because AWS-SDK apis get added so frequently, if there is an easier solution to wrap the javascript AWS-SDK to be used directly in Titanium's javascript environment, this will be more sustainable for using Titanium with AWS-SDK on a regular basis. \nCan we reopen this? I've been going through the same process to explore if we can integrate javascript AWS-SDK with Titanium, hit the same wall. Can an Amazon engineer familiar with Titanium attempt this integration, and recommend ways forward?\nFor other Titanium devs exploring AWS integration, links to previous Appcelerator maintained aws module here. \nhttp://stackoverflow.com/questions/35172821/how-to-create-a-module-for-aws-sdk\nBut also is an example scenario of Ti user trying to use new AWS API but old module not up to date.\nAWS-SDK Javascript with NodeJS works great, keen to find ways for integration with Titanium mobile. Would open up ways for existing Titanium developers to expand integrations with AWS Cognito, Lambda, Amazon Mobile Analytics, IOT and more.\nMuch appreciated, Anthony\n. Titanium has mechanisms to map the IOS AWS-SDK to a native titanium module. However, because AWS-SDK apis get added so frequently, if there is an easier solution to wrap the javascript AWS-SDK to be used directly in Titanium's javascript environment, this will be more sustainable for using Titanium with AWS-SDK on a regular basis. \nCan we reopen this? I've been going through the same process to explore if we can integrate javascript AWS-SDK with Titanium, hit the same wall. Can an Amazon engineer familiar with Titanium attempt this integration, and recommend ways forward?\nFor other Titanium devs exploring AWS integration, links to previous Appcelerator maintained aws module here. \nhttp://stackoverflow.com/questions/35172821/how-to-create-a-module-for-aws-sdk\nBut also is an example scenario of Ti user trying to use new AWS API but old module not up to date.\nAWS-SDK Javascript with NodeJS works great, keen to find ways for integration with Titanium mobile. Would open up ways for existing Titanium developers to expand integrations with AWS Cognito, Lambda, Amazon Mobile Analytics, IOT and more.\nMuch appreciated, Anthony\n. ",
    "samueleastdev": "@anthonychung did you ever get anywhere with this?\n. ",
    "rossthedevigner": "@andrey-bahrachev Do you have an example how you're monitoring overall progress based on this.body info?. ",
    "smrgrg": "Thanks a lot!. ",
    "vedangkoolkarni": "God bless you for this!. God bless you for this!. ",
    "aureq": "Thanks a lot guys !\n. ",
    "nowakj": "Hi jeskew,\nI am able to use the packaged files that come in the 'dist' directory just fine.\nHowever the model we have for our product is to do the packaging with webpack and use npm modules directly with 'require' or 'import'. I was hoping we could do the same with the aws-sdk npm as this would be more consistent with our approach.\nThanks,\nJacek \n. I was able to solve the problems described in #603, so the module loads fine. but than fails at runtime as the CognitoIdentity service is not loaded.\nIt would be great if you could add webpack support. thanks!\n. ",
    "MaxiSantos": "Do you want values not to be returned as an array? Try removing the the format=sdk.\n. ",
    "enagorny": "Hmm. format=sdk is hardcoded in api jsons https://github.com/aws/aws-sdk-js/blob/master/apis/cloudsearchdomain-2013-01-01.normal.json#L18\nWhy not make it configurable?\nIs there a way to override this url?\n. > some results that came back with an array of one string when format is set to sdk, would instead return an array where each character in the string had its own element \nI expect json format strictly define such things.\n. ",
    "ttangww": "Is it the problem resolved ?\nLooks like it is still there.\n. ",
    "tbo": "@chrisradek I was unable to reproduce your test results. If I request the cloudsearch endpoint directly via a REST-request, then I get a proper JSON output as specified in its index options (No \"single character\"-arrays!): \n$ curl \"https://endpoint/2013-01-01/search?q=foo&cursor=initial&size=10&format=json\"\nAlso a test search in the web dashboard yields the same result. I think it is much more likely, that the issue lies in the SDK. Iterating over a string without checking, that it isn't an array would explain your test result.. ",
    "darkpssngr": "I'm using version 2.2.16 and yes I inspected this, at the time when the xmlparse is called on the response the response body in the http is also empty.\n. Closing the issue as it got fixed, once I update my nodejs to the latest stable version.\n. It seems node-inspector is causing this problem. When I update my nodejs version node-inspector broke and everything worked perfectly, but once I fixed the problem with node-inspector it again started giving me an empty response. I'm raising this issue on node-inspector. it is being worked on in https://github.com/node-inspector/node-inspector/issues/703\n. Can you please have a look at this comment and let me know if it is something that should be fixed on node-inspector or aws-sdk.\nhttps://github.com/node-inspector/node-inspector/issues/703#issuecomment-147019826\n. any updates on this ?\n. I'm using the latest version(v0.12.3).\n. Hey,\nany updates?\n. @chrisradek \nSorry for the delay, there was no proper internet connection here due to floods. \nI think this started to fail when node-inspector added the feature to see network logs. Anyways if nothing can be done, I'll use the workaround provided in the thread during development.\nThanks for looking into it. :)\n. closing this issue as it was fixed in node-inspector/node-inspector#703\n. closing this issue as it was fixed in node-inspector/node-inspector#703\n. ",
    "OllieJennings": "Getting this error as well.\nCan't seem to create any buckets with params expect for the default region.\nUsing aws-sdk: 2.2.17\nMain error am getting is the Illegal Location Constraint Exception\n. ",
    "tracend": "Thanks @chrisradek, \nWas considering contributing a pull request but thought I should ask first in case I missed an existing method for performing said task or there were special conventions in dealing with KMS I needed to be aware of. \nI spent some time on this yesterday and was able to decrypt the messages, although they came encrypted as AES/GCM/NoPadding, not as stated in the Ruby lib AES/CBC/PKCS5Padding. I'm not sure if this is variable for SES encrypted messages...\nI'll form a proper solution and use the structure of the Ruby lib as a guide. \nThanks again.\n. Thanks @winzig - I've been using this on a daily basis for over a year now to decrypt emails from SES and it's fine for my needs. \nOccasionally there might be a timeout error or resource leak that will occur from many parallel requests but that can be easily remedied with a fixed cap or even handled on the application level. \nHoping to see this on the AWS SDK as well. I'm guessing other languages have already had this feature because they have greater adoption/interest in enterprise; and it is frankly a feature beyond the needs of the average enthusiast. \n. The code contributed as a pull request in 2015 is now available as a separate module: \nhttps://github.com/makesites/aws-s3-encryption-client\n. Thanks @andylibrian for pointing out that the decryption may fail sometimes on the finalize method. \nAccording to the documentation: \n\nOnce the cipher.final() method has been called, the Cipher object can no longer be used to encrypt data. Attempts to call cipher.final() more than once will result in an error being thrown.\n\nAs suggested, I've wrapped it with a try catch to silent the error (temporarily). If anyone has a more proper solution, feel free to contribute. \n. Oh, I think I did more than just start the conversation, if you were willing to give me credit ;) . The code contributed as a pull request in 2015 is now available as a separate module:\nhttps://github.com/makesites/aws-s3-encryption-client. ",
    "rsweetland": "We would very much like to see this as well. It seems fitting given that the examples from AWS use Node.js with Lambda to receive inbound email.\nThanks @tracend for contributing the PR. Hoping your work helps bring this needed feature more quickly!\n. ",
    "winzig": "@tracend Thanks for the pull request, have you been using your code in production, and does it work well? \nAny chance this is going to be included into the official AWS SDK any time soon? I've even been experimenting with writing my app in .NET Core C#, as it already supports the S3 Encrypted Client!. @tracend Thanks for the pull request, have you been using your code in production, and does it work well? \nAny chance this is going to be included into the official AWS SDK any time soon? I've even been experimenting with writing my app in .NET Core C#, as it already supports the S3 Encrypted Client!. I did struggle with trying to use this fork as a workaround, and appreciate the time that was put into it, but was unable to get it going within my Lambda project. I'm hoping someone that is more familiar than I am with the current state of this SDK might one day pick up the challenge and take a stab at getting it officially integrated within this SDK, including documentation on use. I'm going to work on adding  $200 towards a bounty on this issue.. I did struggle with trying to use this fork as a workaround, and appreciate the time that was put into it, but was unable to get it going within my Lambda project. I'm hoping someone that is more familiar than I am with the current state of this SDK might one day pick up the challenge and take a stab at getting it officially integrated within this SDK, including documentation on use. I'm going to work on adding  $200 towards a bounty on this issue.. @k00k I spent quite a few hours trying to figure out how to add a bounty on the issue, but was ultimately unsuccessful. But I would be willing to contribute towards such a bounty if someone can figure out how to get it going. ;-). @k00k I spent quite a few hours trying to figure out how to add a bounty on the issue, but was ultimately unsuccessful. But I would be willing to contribute towards such a bounty if someone can figure out how to get it going. ;-). ",
    "k00k": "@winzig  \ud83d\udc4d on getting this integrated. Any updates?. ",
    "jlongland": "Also want to express interest in this feature. Any updates?. Also want to express interest in this feature. Any updates?. ",
    "susanlinsfu": "@chrisradek \nI have changed my params to now look like this:\nvar params = {\n        TableName: table,\n        Item:{\n            \"username\": 'bob'\n        },\n        ReturnValues: 'ALL_NEW'\n    };\nBut get the error message when I try to insert now:\nUnable to add item. Error JSON: {\n  \"message\": \"Return values set to invalid value\",\n  \"code\": \"ValidationException\",\n  \"time\": \"2015-11-18T03:25:33.853Z\",\n  \"statusCode\": 400,\n  \"retryable\": false,\n  \"retryDelay\": 0\n}\n. @chrisradek  Thanks Chris, I have just tried ALL_OLD, and NONE and these work!\n. ",
    "calvn": "@chrisradek Since put and putItem doesn't return the new item attributes through ReturnValues, is there a way to have it return the newly inserted entry without querying the table separately with a get?\n. ",
    "sickDevelopers": "+1 for cleung2010 question\n. ",
    "alayor": "I just had the same issue as @susanlinsfu. Changed ReturnValues to ALL_OLD and it worked.\nHowever, I wonder if there is way to return the actual NEW values.. ",
    "shinzui": "@AdityaManohar Thank you. The size of AWS.S3 by itself is still too big unfortunately, so I'll write my custom code to upload from the browser.\n. @AdityaManohar Thank you. The size of AWS.S3 by itself is still too big unfortunately, so I'll write my custom code to upload from the browser.\n. @AdityaManohar I do not have a problem uploading images to S3. I just wanted to make my uploads more reliable by using S3's managed uploader. \n. @AdityaManohar I do not have a problem uploading images to S3. I just wanted to make my uploads more reliable by using S3's managed uploader. \n. ",
    "yawhide": "ah never even thought of that. let me try that out. thanks\n. so yes, it works now. thank you\n. ",
    "swarajgiri": "stringify would remove the prototype chain. If that's not acceptable for some reason, the sdk should at least throw an error instead of ignoring some part of the item to be inserted. \n. @chrisradek - The issue is, as the title suggests,  DocumentClient at least should throw an error instead of silently failing\n. ",
    "pkarl": ":+1: for throwing an error when there's an error. \n. ",
    "leore": "Hey Guys, Any update on this?\nIs this related to http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingClientSideEncryption.html\nI see that the javascript library does not yet support Client Side Encryption\ncc @robinjoseph08\n. What is the best way to do client side encryption through node?\n. ",
    "mallocator": "+1  Very much need this to decrypt data uploaded from a java client using symmetric client side encryption in my nodejs server.\nOnly option I see right now is trying to reverse engineer whatever was done in Java.\n. would be great to support buffers as key\n. ",
    "albertovasquez": "+1 This is a critical feature needed.  If Amazon wanted to allow easy adoption they should have a technical solution, either blog on how to accomplish decrypting or include in sdk.\nI'm using this PR and it is working perfectly.  Thanks for making this available. \n. +1 This is a critical feature needed.  If Amazon wanted to allow easy adoption they should have a technical solution, either blog on how to accomplish decrypting or include in sdk.\nI'm using this PR and it is working perfectly.  Thanks for making this available. \n. ",
    "guerrerocarlos": "Alternatively, if you have 'being able to do ranged GET' among your requirements, stream-cipher can be used for client side encription (node.js).\n. ",
    "fyockm": "We are using Location to store the full URL for the object, so that it's accessible later by whatever method is required for the given client. That might be directly via an html <img> tag, or through getObject. In case of the latter, we have to parse out the Bucket and Key, which becomes more difficult with inconsistent URLs.\nAs you can see from my code, I can easily fix the encoding problem with the Location for a multipart file. \n1. I thought this could trip up some other developer in the future and wanted it to be documented\n2. I believe the Location url should be consistent, regardless of filesize\nTo my understanding, the whole point of the s3.upload function is to make it easier on developers by hiding the underlying getObject vs createMultipartUpload craziness. Unexpected results like this only add to the frustration.\n. @chrisradek While it won't solve my particular problem, I'm sure it would help others if Bucket and Key were always returned from upload. And, I think that's probably a fair compromise for the time being. \nPerhaps you could auto decode Location in the next major release?\n. Please explain why more reliable? just because of the g global flag?\n. ",
    "ilanle": "Hi,\nActually I already did all of that before posting this issue.\nI already did monitor it and did force GC every loop. Whether or not forcing or not forcing GC, in several minutes you see a memory leak that cannot be mistaken.\nThe reason that I think that Dynamo.scan maybe related is that if I just omit the call to Dynamo.scan, there is no memory leak.\nMeaning if I replace the lines:\n        Dynamo.scan(options, function(err, data) {\n                callback(null,null);\n        });\nwith just\nsetTimeout(function(){\n   callback(null,null);\n   },0);\nThere is no memory leak.\nYou could notice that whatever the scan returns, I return callback(null,null) and don't do anything, this is of course just for testing the memory leak.\nP.S. another fact that might be a hint, I don't see memory rise on every loop (with GC forced), but rather every several loops/scans.\nThank you\n. Hi,\nCan you refer to my last comment?\nthank you\n. Getting the same error now.\nJust npmd the latest release a few minutes ago, \nI get this from dynamo\nUnrecognizedClientException: The security token included in the request is invalid\n. Getting the same error now.\nJust npmd the latest release a few minutes ago, \nI get this from dynamo\nUnrecognizedClientException: The security token included in the request is invalid\n. ",
    "no-response[bot]": "This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. This issue has been automatically closed because there has been no response to our request for more information from the original author. With only the information that is currently in the issue, we don't have enough information to take action. Please reach out if you have or find the answers we need so that we can investigate further.\n. ",
    "kaurranjeet12": "Thanks for your reply. I was using aws sdk 2.2.12 and doing direct upload from browser using javascript.\nAfter upgrading to latest version 2.2.17, it works fine.\n. ",
    "ar1hur": "ahh, seems like german characters are making trouble here...\n. ahh, seems like german characters are making trouble here...\n. @chrisradek \nThank you for your fast feedback! I didnt know that.. Your mentioned article should be linked on the api docs page to specifiy this kind of parameter: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#putObject-property\nBest Regards\nArthur\n. @chrisradek \nThank you for your fast feedback! I didnt know that.. Your mentioned article should be linked on the api docs page to specifiy this kind of parameter: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#putObject-property\nBest Regards\nArthur\n. ",
    "Chatatata": "@chrisradek \nAs you see in the error, the error is thrown from a aws-sdk library file, request.js.\nAlso I couldn't understand what you meant with logging.\n. The error was probably unrelated to AWS-SDK, somehow its solved (string was global var).\nThank you, anyways.\n. ",
    "marcopiraccini": "Tried with 2.1.36 and 2.2.19 (the last one). I agree that it can be some communication issue, but if so it depends on node version, since with >=0.12.x it works perfectly.  And I don't see more info on the console running with:\nconfig.logger = console\nvar s3 = new AWS.S3(config)\n  s3.listObjects(params, function (err, data) {\n(...)\n})\n. Good point. I have a code that purge a SQS queue and I have the very same problem. \nThe code I've provided is executed in a tape test, I simply run it using node mytest.  It's something like: \n```\nvar test = require('tape')\n(...)\nfunction tests (config) {\n  test('Cleanup S3 bucket', function (t) {\n        var s3 = new AWS.S3(config)\n        s3.listObjects(params, function (err, data) {\n    })\n  })\n}\n``\n. Ok, found the issue. In the tape test, there's one condition when I usenock` (not for AWS, but for mocking another service). This happens when:\n- Nock is required before AWS. Just required, no further config needed :)\n- Node 0.10.x\nThat's weird but definitely is not a aws-sdk issue. Closing.\n. ",
    "robertrossmann": "Actually, you are right. :smile:\nI had a custom function instead of the url parameter which was supposed to generate the queue URL based on some input. However, I used path.join() in it to construct the URL, but that messed up the protocol to be only https:/.\nSwitched to url.resolve() and all is well.\nSorry for this, my bad! And thanks for the fast response!\n. ",
    "mgoria": "@chrisradek \nI'm using aws-sdk version 2.2.20 and '2015-03-31' lambda version.\nRegarding x-amz-log-result header, it's not being returned by this.httpResponse.headers:\n\nI've noticed that it is returned in browser response headers (see below screen shot), but this doesn't work for our use case unfortunately. It'll be nice to have it in callback data or in httpResponse.headers.\n\n. @chrisradek \nI'm calling lambda from browser, you can try it here, and check out the browser console (I'm logging lambda 'Response' object).\n. I've also checked this.request.httpRequest.headers and X-Amz-Log-Type and X-Amz-Invocation-Type are being sent:\n\n. @chrisradek \nNope, but let me try this, I'll get back to you once I have an working sample without deep-framework.\nBtw, have you inspected the console.log-ed Response object, I'm asking because it is raw lambda Response, not parsed by deep-framework, and it seems it doesn't have LogResult property.\nThis way I'm console logging lambda response:\n```\nthis._lambda = new AWS.Lambda(options);\nthis._lambda.invoke(invocationParameters, function (error, data) {\nconsole.log(\"this._lambda.invoke - resposne - \", this);\ncallback(new LambdaResponse(_this, data, error));\n});\n```\n. @chrisradek \nOk, Thank you! Looking forward.\n. ",
    "eistrati": "@chrisradek We were wondering if there is any resolution to this issue? Please let us know and thanks in advance for your effort.\n. @chrisradek We were wondering if there is any resolution to this issue? Please let us know and thanks in advance for your effort.\n. ",
    "chriskinsman": "@chrisradek I have tested the above on node 0.10.21 and 0.10.40.\nHappy to try the latest SDK tonight.  I didn't see anything in the changelists that would indicate a change in behavior.\n. @chrisradek  Tried 2.2.21 with no luck\n. @chrisradek Looks like it works on node v4.2.2 and node 0.12.8\n. Upgrade was rocky but we have upgraded just the piece of code that is impacted by this.  Moving the whole stack was a non-starter.\n. That was not my experience.  I was using that version and still had the issue on 0.10.XX\n. I am now seeing this again with node v4.2.2 and idk version 2.2.48\n. Just updated to 2.5.3 and running some tests now...\n. chrisradek \nI now see this error:\nStreamContentLengthMismatch: Stream content length mismatch. Received 24405573 of 1628095990 bytes.\nGreat that you warn me but what is the recovery strategy at this point?  I have read 24MB of 1.6GB.  \nIn my case this is gzipped data so I can't randomly seek the stream.  Thoughts?\nAlso how do I find out why it closed the stream early so I can try to prevent it.  This is an EC2 instance in the same region as the S3 item in this case.\n. @tilfin Hilarious!  I built something similar:\nhttps://www.npmjs.com/package/s3-stream-download\nI was waiting to attach it to this thread until we had been using it for a week or so.  It has fixed the issue for us.  I will check out yours also...\n. ",
    "rloehr": "Just wanted to chime in and say I'm suddenly experiencing this on 0.10.30 as well.\n. @chrisradek \nI've tried this with 2.3.8 and 2.3.14 of aws-sdk lib.\nRemoving the ServerSideEncryption param yielded the same error for me.\nIf you have a code snippet that works for you, please share it and I'll be happy to test against my particular buckets and test files.\n. ",
    "TheRoSS": "I have the same issue\naws-sdk version is 2.6.5\nnode version is 6.2.1\nI'm building list of keys and trying to download their content in series (no concurrency at all)\nI didn't see errors if the length of the list is 1000.\nWith the length of some thousands the program always fails with the error:\n===> (1249/55858) Start processing 'archiveHistory/heroes-fb/type/2016/05/21/type-.account.registration.log.zst' of size 312117 ...\nERROR: { StreamContentLengthMismatch: Stream content length mismatch. Received 273853 of 312117 bytes.\n    at PassThrough.checkContentLengthAndEmit (/home/alex/workspace/stash/utils/node_modules/aws-sdk/lib/request.js:599:15)\n    at emitNone (events.js:91:20)\n    at PassThrough.emit (events.js:185:7)\n    at endReadableNT (_stream_readable.js:926:12)\n    at _combinedTickCallback (internal/process/next_tick.js:74:11)\n    at process._tickDomainCallback (internal/process/next_tick.js:122:9)\n  message: 'Stream content length mismatch. Received 273853 of 312117 bytes.',\n  code: 'StreamContentLengthMismatch',\n  time: 2016-10-03T19:16:34.504Z }\nMy code:\n``` javascript\nasync.eachSeries(plan, (planItem, next) => {\n    ...\n    async.waterfall([\n        s3_loadObject.bind(null, planItem.Key, loadedFilePath),\n        ...\n    ], next);\n}, ...);\nfunction s3_loadObject(key, outFileName, callback) {\n    const readerParams = {\n        Key: key\n    };\nvar reader = s3.getObject(readerParams).createReadStream();\nvar writer = fs.createWriteStream(outFileName);\n\nreader.pipe(writer);\n\nwriter.on(\"error\", callback);\nreader.on(\"error\", callback);\nreader.on(\"end\", () => {\n    console.log(`Stored as: '${outFileName}'`);\n    callback();\n});\n\n}\n```\n. It seems that it was my fault.\nI replaced the code\njavascript\nreader.on(\"end\", () => {\n        console.log(`Stored as: '${outFileName}'`);\n        callback();\n    });\nwith\njavascript\nwriter.on(\"finish\", () => {\n        console.log(`Stored as: '${outFileName}'`);\n        callback();\n    });\nand my script processed 5120/46536 objects without any errors\nRESOLVED\n. ",
    "Zugwalt": "Having this problem with node 6.8 and aws-sdk 2.7.7. Happens occasionally downloading larger files (videos) and writing them to the EBS on an EC2 machine. . ",
    "baptistemanson": "Same error. Sounds like a network failure. \nI tried to investigate how easy it would be to retry to query a failing chunk. It is right now a bit out of my league, this lib being pretty wide and this error in an fairly abstracted general request class. . We ended up splitting our files in a bunch of smaller files on S3 + reduced the number of parallel downloads we do with a queue. \n(aka streaming 10x1GB instead of 10GB).\nThat didn't remove the issue, but:\n- it reduces the cost of retrying. \n- it occurs less often per gigabyte transferred.. ",
    "rayjennings3rd": "I'm seeing this error on semi-regular basis. Node v8.9.0. Is the waitTime parameter the total time to completion or a timeout between bytes or other?. ",
    "ataraxus": "Hey @chrisradek, \nafter reading the original documentation I got my error. If you don't specify a region the aws-sdk will use a default region 'us-east-1' which is not compatible with my target region in Frankfurt: 'eu-central-1' after having a look where to set the appropriate region in the loopback sdk i got everything working. Ticket can be closed now. \nThanks again.\n. Hey @chrisradek, \nafter reading the original documentation I got my error. If you don't specify a region the aws-sdk will use a default region 'us-east-1' which is not compatible with my target region in Frankfurt: 'eu-central-1' after having a look where to set the appropriate region in the loopback sdk i got everything working. Ticket can be closed now. \nThanks again.\n. Just for folks ending up on this page, here is the site to look up your region: https://docs.aws.amazon.com/general/latest/gr/rande.html\n. Just for folks ending up on this page, here is the site to look up your region: https://docs.aws.amazon.com/general/latest/gr/rande.html\n. ",
    "johannesboyne": "Ok, seems to be the same as https://github.com/aws/aws-sdk-js/issues/820\n. ",
    "EmilAlipiev": "I am not able to get that useing Cognito Id by specifing as below. What user Id should I pass? I tried using Identity Id returned by Cognito but it doesnt work\n\"Condition\": {\n                \"ForAllValues:StringEquals\": {\n                    \"dynamodb:LeadingKeys\": [\n                        \"${cognito-identity.amazonaws.com:sub}\"\n                    ]\n                }\n            }. ",
    "mayankverma7": "I am getting the following error:\n\"Message\": \"User: arn:aws:sts::XXXXXX:assumed-role/transactions-api-dev-us-east-2-lambdaRole/transactions-api-dev-post-transactions is not authorized to perform: dynamodb:UpdateItem on resource: arn:aws:dynamodb:us-east-2:XXXXX:table/transactions_table\"\nAnd, my policy file is:\n{\n    \"Version\": \"2017-08-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"FullAccessToUserItems\",\n            \"Effect\": \"Allow\",\n            \"Action\":\n                    \"dynamodb:*\",\n            \"Resource\": [\n                \"arn:aws:dynamodb:us-east-2:xxxxx:table/transactions_table\"\n            ],\n            \"Condition\": {\n        }\n    },\n    {\n        \"Sid\": \"AccessRole\",\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"sts:AssumeRole\"\n        ],\n        \"Resource\": [\n            \"*\"\n        ]\n    }\n]\n\n}\nIs there any issue in that ,  for now i m not able to figure out the issue.. ",
    "thoean": "Your proposal sounds great and would solve my use case.\nWhether opt-in or standard behavior depends how much DocumentClient is a simplification over DynamoDB, not only in simplification of providing the necessary parameters, but also trying to do the right thing for the client. I think this case has arguments in both directions, so opt-in to maintain compatibility to its current version makes sense.\n. Your proposal sounds great and would solve my use case.\nWhether opt-in or standard behavior depends how much DocumentClient is a simplification over DynamoDB, not only in simplification of providing the necessary parameters, but also trying to do the right thing for the client. I think this case has arguments in both directions, so opt-in to maintain compatibility to its current version makes sense.\n. @chrisradek , may I ask you to use the lock conversation feature to avoid having another million of +1s and notifying so many users? The DocumentClient solves this already with the convertEmptyValues: true option. And if the request is to store empty strings in dynamoDB, then the client library is the wrong repository to report this.\nThanks.. @chrisradek , may I ask you to use the lock conversation feature to avoid having another million of +1s and notifying so many users? The DocumentClient solves this already with the convertEmptyValues: true option. And if the request is to store empty strings in dynamoDB, then the client library is the wrong repository to report this.\nThanks.. @richard-gebbia I don't know the correct place. But it's not the client library as stated many times above. Best done via your account manager at AWS, or if your at a smaller contract probably just by filing a support ticket from your AWS account.. @richard-gebbia I don't know the correct place. But it's not the client library as stated many times above. Best done via your account manager at AWS, or if your at a smaller contract probably just by filing a support ticket from your AWS account.. ",
    "clicktravel-jaroslav": "+1\n. ",
    "jeffbakerexpedia": "+1\n. ",
    "hammadmlk": "+1\n. ",
    "jesucarr": "+1\n. ",
    "chandan449": "+1\n. +1\n. ",
    "itaykahana": "+1\n. +1\n. ",
    "bo01ean": "I worked around this by just wiping out the empty keys from the object:\njavascript\n    var Item = { Key:\"foo\", MyData: \"\", Item: { Key:\"foo\", MyData: \"\", Item: { Key:\"foo\", MyData: \"\"}}};\n    function removeEmptyStringElements(obj) {\n      for (var prop in obj) {\n        if (typeof obj[prop] === 'object') {// dive deeper in\n          removeEmptyStringElements(obj[prop]);\n        } else if(obj[prop] === '') {// delete elements that are empty strings\n          delete obj[prop];\n        }\n      }\n      return obj;\n    }\n    removeEmptyStringElements(Item);\n/*\n{\n  \"Key\": \"foo\",\n  \"Item\": {\n    \"Key\": \"foo\",\n    \"Item\": {\n      \"Key\": \"foo\"\n    }\n  }\n}\n*/\n. ",
    "pzimny": "+1\n. Glad to see something is being done, but that's really just trading one problem for a new one.. ",
    "joshwils82": "+1\n. ",
    "ossie1971": "+1\n. ",
    "joegesualdo": "+1\n. ",
    "RhysC": "+1 \n. ",
    "bradledford": "+1\n. ",
    "pavei": "+1\n. ",
    "dustinbolton": "+1\n. Note that this was discovered with credentials returned from STL for a federated user not having their starting letter converted to lower-case prior to storing in a .json file.  That STL API returns credentials for a federated user with keys beginning with a capital letter such as:\nAccessKeyId\nSecretAccessKey\nSessionToken\nExpiration\nHowever, this SDK throws the connection timed out after 1000ms error when using config.loadFromPath() unless they are corrected to begin with lower case letters:\naccessKeyId\nsecretAccessKey\nsessionToken\nexpiration\nI'm not sure if this is expected behavior or not either.\n. Thank you very much for this information.  The workarounds you provided did work for my needs and allow me to catch the error. I am indeed using STS so I am looking into using this method as a more 'direct' way of applying the credentials.  Thank you for letting me know about that as well.\nI very much appreciate all of your explanation here as I now understand how this all works much better.\n. Appologies. This issue was encountered when running on AWS Lambda.  AWS Lambda is running an outdated version of the aws sdk resulting in this error.  Manually including a more recent version of the SDK overwrites the included lambda aws sdk solving this issue.\n. Appologies. This issue was encountered when running on AWS Lambda.  AWS Lambda is running an outdated version of the aws sdk resulting in this error.  Manually including a more recent version of the SDK overwrites the included lambda aws sdk solving this issue.\n. ",
    "jppellerin": "+1\n. +1\n. @westy92 Solution works for us as well on ElasticBeanstalk using node v.5.5.  Will keep monitoring and report back if it reappears.\n. @westy92 Solution works for us as well on ElasticBeanstalk using node v.5.5.  Will keep monitoring and report back if it reappears.\n. +1\n. +1\n. Any word on allowing DynamoDB to define it's customBackoff?  The currently implemented (which is forced on us) is greatly flawed as it doesn't take into account any jitter.  Multiple requests that are concurrent will all be retrying at the exact same time.. Any word on allowing DynamoDB to define it's customBackoff?  The currently implemented (which is forced on us) is greatly flawed as it doesn't take into account any jitter.  Multiple requests that are concurrent will all be retrying at the exact same time.. @chrisradek is there a reason why no jitter is added to DynamoDB while it's there for every other service?  We have a case of multiple concurrent requests.  These all end up on the same retry interval, and thus end up creating a bottleneck.\nWhy is DynamoDB different when it comes to specifying a customBackoff algorithm (which would allow us to fix this issue)?. @chrisradek is there a reason why no jitter is added to DynamoDB while it's there for every other service?  We have a case of multiple concurrent requests.  These all end up on the same retry interval, and thus end up creating a bottleneck.\nWhy is DynamoDB different when it comes to specifying a customBackoff algorithm (which would allow us to fix this issue)?. ",
    "cstavro": "+1\n. ",
    "kienpham2000": "+1\n. ",
    "iSkore": "+1\n. function cleanObj( o ) {\n    if( o && o === o )\n        if( typeof o === 'boolean' ) return o;\n        else if( typeof o === 'number' ) return o;\n        else if( typeof o === 'string' ) return o;\n        else if( Array.isArray( o ) ) {\n            let x = [], i = -1, l = o.length, r = 0;\n            while( ++i < l ) if( o[ i ] ) x[ r++ ] = cleanObj( o[ i ] );\n            return x;\n        } else if( typeof o === 'object' ) {\n            for( const k in o ) o[ k ] ? o[ k ] = cleanObj( o[ k ] ) : delete o[ k ];\n            return o;\n        } else\n            return 'Argument Error - Unknown Item';\n}\n. ",
    "tejasmanohar": "+1\n. ",
    "Rajkumar55": "+1\n. ",
    "danieldelouya": "+1\n. ",
    "guillaumefont": "+1\n. +1\nOn Fri, Oct 28, 2016 at 2:29 PM, rtrompier notifications@github.com wrote:\n\n+1\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/833#issuecomment-256908063, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACTfBJDHiiE36MoAsD9aENTITv7zoTfaks5q4eqMgaJpZM4GykZr\n.\n. \n",
    "seanchann": "+1\n. ",
    "superandrew": "+1\n. +1\n. ive never seen a issue desired like this one. Any roadmap to implement the possibility to store an empty string in dynamodb? should everyone move to another database? \ud83d\ude4f\ud83c\udffc. ive never seen a issue desired like this one. Any roadmap to implement the possibility to store an empty string in dynamodb? should everyone move to another database? \ud83d\ude4f\ud83c\udffc. in my opinion, this is more of a client side patch for something that needs to be done on the dynamodb side. And, more importantly, and as someone else said before me, null is no replacement for an empty string. So if more and more people keeps asking for a more complete fix, I wouldn't call this issue \"closed\". But, of course, still my opinion.. this is the single most requested thing I saw in my life.. ",
    "rtrompier": "+1\n. ",
    "whiteswift": "+1\n. +1\n. ",
    "csabakollar": "+1\n. ",
    "DianaIonita": "+1\n. ",
    "Imran99": "+1\n. ",
    "khoi-nguyen-2359": "+1\n. ",
    "sasanala": "+1\n. ",
    "jbuddha": "+1\n. ",
    "Jorenm": "+1\n. +1\n. ",
    "asantibanez": "+1. ",
    "cbarcenas": "+1. ",
    "MerryOscar": "+1. ",
    "brandonmbanks": "+1. ",
    "svoeller99": "+1. ",
    "njgrisafi": "+1. ",
    "ntrivix": "+1. ",
    "ruleechen": "+1. ",
    "elioncho": "+1. ",
    "yipcma": "this is much needed, please.... ",
    "lamon": "+1. ",
    "jeremysuriel": "+1. ",
    "jstradli": "+1. @jeskew Thank you for your explanation for this, makes sense.  For others that stumble upon this, here is how I resolved with @jeskew's help.\n```\nimport AWS = require('aws-sdk');\nimport {ServiceConfigurationOptions} from 'aws-sdk/lib/service';\nlet serviceConfigOptions : ServiceConfigurationOptions = {\n    region: \"us-west-2\",\n    endpoint: \"http://localhost:8000\"\n};\nlet dynamodb = new AWS.DynamoDB(serviceConfigOptions);\nlet docClient = new AWS.DynamoDB.DocumentClient( {\n    region: \"us-west-2\",\n    endpoint: \"http://localhost:8000\",\n    convertEmptyValues: true\n}); \n```. @jeskew yes good catch.   I have some other code that I excluded which is calling dynamoDb.createTable(),. ",
    "ashikkalavadiya": "+1. @jeskew you can accelerate this type of request to AWS Team where plenty of people is facing issue with kind of problems! Though its almost 2 years, Developer finds a way on their(client) side as well!! LET IT GO!. ",
    "rafiek": "+1000. ",
    "tsmiser": "+1. ",
    "worsnupd": "+1. ",
    "amistein": "+1. ",
    "emaildanwilson": "Slight change to the code suggested by @bo01ean so that it tokenizes and detokenizes the empty string. It works for me but it's annoying that I need to do this to preserve the original JSON when using dynamodb.\n```\nconst token = '~&^AWSSTILLHASNOTFIXEDTHIS!!!~&^';\nconst tokenizeEmptyStringElements = function (obj) {\n  for (var prop in obj) {\n    if (typeof obj[prop] === 'object') {// dive deeper in\n      tokenizeEmptyStringElements(obj[prop]);\n    } else if(obj[prop] === '') {\n      // tokenize elements that are empty strings\n      obj[prop] = token\n    }\n  }\n  return obj;\n}\nconst detokenizeEmptyStringElements = function (obj) {\n  for (var prop in obj) {\n    if (typeof obj[prop] === 'object') {// dive deeper in\n      detokenizeEmptyStringElements(obj[prop]);\n    } else if(obj[prop] === token) {\n      // tokenize elements that are empty strings\n      obj[prop] = ''\n    }\n  }\n  return obj;\n}\n```. ",
    "arvinkx": "+1. ",
    "MattTunny": "+1. ",
    "arctair": "+1. ",
    "eddyfabery": "+1. ",
    "terge": "+1. ",
    "vvemulak": "+1. ",
    "bo-ora": "+1. I have exactly the same situation with assumeRole and empty data returned.\n@petterbergman have you found out the reason ?. ",
    "bluepeter": "+8. @jeskew it returns an object. I would have to recreate it to tell you specifically what, however. This almost certainly is related to DocumentClient as doing the same query \"verbosely\" without DocumentClient does not have this issue.. @srchase we just worked around it. In our case we are using LastEvaluatedKey to determine if we need to paginate. We decided having a next button and paginating to an empty page was \"okay.\" Not ideal, but ok.. @rlisnoff our ultimate solution was to query for X + 1 and then only show X in the client, but use the + 1 to determine if we need to paginate.. ",
    "franciscotfmc": "+1. ",
    "JamesDorrian": "+1. ",
    "atulrawat": "+1. ",
    "adimoraret": "+1 . ",
    "M1chaelTran": "+1. ",
    "jhoel": "+1. +1. ",
    "sowmitranalla": "+1. ",
    "a1exwang": "+1. ",
    "Mnilko": "+1. +1. ",
    "bvanloocke": "+1. ",
    "yzhong52": "+1. ",
    "rtatiparthy": "+1. ",
    "XBeg9": "+1. ",
    "frankjl": "+1. Any updates to this? We are also getting this error about 1/3rd of the time. Most of the time, simply retrying the step allows it to succeed. We are sure it's not the container causing issues because the we can see in the logs that our task is running; it just seems to be a bug in the ecs.waitFor implementation.\nThis is a simplified version of what we are doing:\necs.runTask({\n  taskDefinition: taskDefinitionId,\n  cluster: 'ECSCluster'\n}).promise().then((result) => {\n  const taskArn = result.tasks[0].taskArn;\n  console.info(`Checking to make sure task arn ${taskArn} is RUNNING..`);\n  ecs.waitFor('tasksRunning', {\n    tasks: [ taskArn ],\n    cluster: 'ECSCluster'\n  }).promise();\n})\nAnd sporadically we'll see the following errors:\nFailed { ResourceNotReady: Resource is not in the state tasksRunning\nat constructor.setError (/var/runtime/node_modules/aws-sdk/lib/resource_waiter.js:182:47)\nat Request.CHECK_ACCEPTORS (/var/runtime/node_modules/aws-sdk/lib/resource_waiter.js:44:12)\nat Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\nat Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\nat Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:683:14)\nat Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)\nat AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)\nat /var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10\nat Request.<anonymous> (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)\nat Request.<anonymous> (/var/runtime/node_modules/aws-sdk/lib/request.js:685:12)\nmessage: 'Resource is not in the state tasksRunning',\ncode: 'ResourceNotReady',\nretryable: false,\ntime: 2018-10-19T19:56:50.153Z,\nstatusCode: 200,\nretryDelay: 6000 }. Any updates to this? We are also getting this error about 1/3rd of the time. Most of the time, simply retrying the step allows it to succeed. We are sure it's not the container causing issues because the we can see in the logs that our task is running; it just seems to be a bug in the ecs.waitFor implementation.\nThis is a simplified version of what we are doing:\necs.runTask({\n  taskDefinition: taskDefinitionId,\n  cluster: 'ECSCluster'\n}).promise().then((result) => {\n  const taskArn = result.tasks[0].taskArn;\n  console.info(`Checking to make sure task arn ${taskArn} is RUNNING..`);\n  ecs.waitFor('tasksRunning', {\n    tasks: [ taskArn ],\n    cluster: 'ECSCluster'\n  }).promise();\n})\nAnd sporadically we'll see the following errors:\nFailed { ResourceNotReady: Resource is not in the state tasksRunning\nat constructor.setError (/var/runtime/node_modules/aws-sdk/lib/resource_waiter.js:182:47)\nat Request.CHECK_ACCEPTORS (/var/runtime/node_modules/aws-sdk/lib/resource_waiter.js:44:12)\nat Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\nat Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\nat Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:683:14)\nat Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)\nat AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)\nat /var/runtime/node_modules/aws-sdk/lib/state_machine.js:26:10\nat Request.<anonymous> (/var/runtime/node_modules/aws-sdk/lib/request.js:38:9)\nat Request.<anonymous> (/var/runtime/node_modules/aws-sdk/lib/request.js:685:12)\nmessage: 'Resource is not in the state tasksRunning',\ncode: 'ResourceNotReady',\nretryable: false,\ntime: 2018-10-19T19:56:50.153Z,\nstatusCode: 200,\nretryDelay: 6000 }. ",
    "nfons": "+100000. ",
    "synergiclabs": "+1. @JamesTheHacker That is not a viable solution. The option convertEmptyValues converts a string into a NULL value. There are instances you really need EMPTY STRING and not a null data type.\nIn JavaScript, \"\" !== null;. ",
    "nickcoad": "This has been an issue for 5 years and has virtually no response on the DynamoDb forum, which is why you're getting hassled about it here. Maybe it's possible for you to raise this internally with the DynamoDb team so they can at the very least jump into the forum thread and address it with the users?. ",
    "Aristocles": "+1. ",
    "raaone7": "+1. ",
    "fptavares": "convertEmptyValues: true\nwas enough to fix it for me.\n. convertEmptyValues: true\nwas enough to fix it for me.\n. ",
    "BetaMee": "+1. ",
    "nephe": "+1. ",
    "hcho112": "+1. ",
    "chauhanprateek89": "+1. ",
    "Hanslen": "+1. ",
    "seckcoder": "+1. ",
    "wowmama": "+1, we really need to persist empty string into database.. ",
    "kushbhandari": "+1 ... read in some other issue that empty strings will be supported in DDB in 2018 .. any update when?. ",
    "whitechdevs": "+1. ",
    "richard-gebbia": "+1. Just to be clear, convertEmptyValues does NOT solve the problem of dynamodb's inability to store empty strings. @thoean where is the correct place to report this issue if this isn't it? . ",
    "nejcr": "+1. ",
    "kimthangatm": "+1. ",
    "keesvanlierop": "+1. ",
    "smtheard": "can someone explain why setting an empty string would ever be considered invalid? this behavior makes no sense, the issue is closed, and it seems clearly incorrect despite being documented.. ",
    "andreafalzetti": "+1. ",
    "AidanConnelly": "+1. ",
    "jchirschy": "+1. ",
    "PavelSizov": "+1. ",
    "dmhalejr": "+1. ",
    "cmavromichalis": "+1. +1. ",
    "jeremyputeaux": "+1. ",
    "bogdan-maksak": "+1. ",
    "onrul": "+1. ",
    "rqton": "+1 \nThis issue has been created on AWS forum for more than 6 years : https://forums.aws.amazon.com/thread.jspa?threadID=90137. ",
    "davidfurlong": "My secretKey credential was null & I got this error. I'd expect null credentials to throw an error when I do a new AWS.Credentials(key, secret) but at least when used/invoked and not this split nonsense. ",
    "kiwiholmberg": "The body i get in the callback from listFunctions is \n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n<head>\n  <title>Page Not Found</title>\n</head>\n<body>Page Not Found</body>\n</html>\nNot sure if this should be possible to get as a response, since you would expect a JSON response.\nI tried your minimal example and it works, so i guess that means I'm doing something wrong in how i use the SDK. I'll take another shot at it tomorrow and see if i can work it out.\nThanks for your time Chris, much appreciated!\n. ",
    "tyrsius": "@chrisradek has there been any movement on this?\n. I am on the Nodejs 4.3 (aws lambda version). The following does not solve the timeout issue\n``` javascript\nvar dynamoDb = new AWS.DynamoDB({\n    apiVersion: '2012-08-10',\n    httpOptions: { \n        agent: new https.Agent({\n            rejectUnauthorized: true,\n            keepAlive: true, \n            ciphers: 'ALL', \n            secureProtocol: 'TLSv1_method' \n        }) \n    } \n})\nvar dynamo = new AWS.DynamoDB.DocumentClient({service: dynamoDb})\n```\nIf I try to add AWS.config.update({sslEnabled: false}) then everything times out, every time.\n. @magic53 That worked, apparently those extra parameters I was using were getting in the way.\n. @thedevkit I have tried setting context.callbackWaitsForEmptyEventLoop = false, it had no effect.\nThe timeouts seem to have been caused by this, as th eworkaround above solved the problem\n. @chrisradek Just the opposite, the aws-sdk-external bar indicates that the SDK was loaded from the environment, not packaged with the deployment.\nThat webpack bundler option looks promising, but its not easily findable. I did a lot of searching and never encountered it. You should really consider putting a link that on \"https://sdk.amazonaws.com/builder/js/\", or even better add an option to that builder to build a node version with webpack. \nGetting the size of individual pieces down would be a major improvement, I hope you guys consider this a priority. 200kb for a dynamo client is pretty steep.. I tried packaging the aws-sdk/clients/dynamodb client, and here is the result.\n```\n\u2514\u2500\u2500 aws-sdk@2.43.0 (74 files) 388.4 kB\n\u2514\u2500\u2500 xmlbuilder (17 files) 49.7 kB\n\u2514\u2500\u2500 lodash (140 files) 114.5 kB\n\u2514\u2500\u2500 util (2 files) 16.7 kB\n\u2514\u2500\u2500 process (1 files) 3 kB\n\u2514\u2500\u2500 object-assign-polyfill (1 files) 1 kB\n\u2514\u2500\u2500 jmespath (1 files) 57.1 kB\n\u2514\u2500\u2500 fs (1 files) 283 Bytes\n\u2514\u2500\u2500 uuid (5 files) 5.4 kB\n\u2514\u2500\u2500 crypto (1 files) 291 Bytes\n\u2514\u2500\u2500 buffer (1 files) 54.8 kB\n\u2514\u2500\u2500 base64-js (1 files) 3.6 kB\n\u2514\u2500\u2500 ieee754 (1 files) 2.2 kB\n\u2514\u2500\u2500 stream (1 files) 291 Bytes\n\u2514\u2500\u2500 url (1 files) 285 Bytes\n\u2514\u2500\u2500 querystring (1 files) 4.3 kB\n\u2514\u2500\u2500 xml2js (3 files) 18.7 kB\n\u2514\u2500\u2500 sax (1 files) 43.1 kB\n\u2514\u2500\u2500 events (1 files) 9.8 kB\n\u2514\u2500\u2500 http (1 files) 287 Bytes\n\u2514\u2500\u2500 path (1 files) 6 kB\nBundle\nSize: 780 kB\nTime: 547ms\n\nBundle (Uglified)\nSize: 292 kB\nTime: 2s 490ms\n\n```\nA couple things jump out.\n\nGetting core down is critical. 388kb! There has to be stuff in there can be pushed back into the modules that are using it instead of loading it for everyone.\nYou are pulling in the entire lodash library. I know lodash is awesome, but you really shouldn't use it in a library. Adding 114KB so that you can save a little time while coding is a big weight to push onto your consumers.\nYour XML builder is 49kb. This shouldn't even make it into the Dynamo client, which I know you want to address, but if you have to pull in an xml package use xml. Its only 2.9kb, compared to 49kb for xmlbuilder. @chrisradek Oh man, you can get rid of lodash now then, because the latest xmlbuilder no longer depends on it!. Why are you maintaining comparability with node 0.8? It has been\nend-of-life'd. So has node 0.12.\nOn Wed, Apr 26, 2017 at 7:35 AM Jonathan Eskew notifications@github.com\nwrote:\n\n\n@tyrsius https://github.com/tyrsius Unfortunately, we're already on the\nlatest version of XMLBuilder that is guaranteed to work with Node 0.8.\nUpdating that dependency isn't an option, but replacing it with something\nsmaller or with a smaller dependency graph could be.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1469#issuecomment-297428424,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABIeaEukJrMXzuQbrS50UKJBJbsJrgPVks5rz1ZJgaJpZM4NDQ5f\n.\n. Man, that's rough. I'm not sure asking people to upgrade node on a fresh install of an OS is too big of an ask, but this sounds like a pretty firm commitment.\n\nI've been testing out what it would take to get the dynamo client under 100kb, and I've made really good progress, but not with Node 0.10 support. I might just have to fork this work. I am doing this for lambda's, which only support 4.3 and up, so maybe its worth having an AWS Lambda SDK fork that can optimize for that environment.. @jeskew @chrisradek Im still doing performance work, and I thought you guys might be interested in this flame graph of the aws sdk being loaded in a lambda\n\nhere is an interactive version\nJust lazy loading the xmlbuilder until its used, which seems rare, could take 1/4 out of the cold start.. ",
    "petemounce": "I then discovered TheDeveloper/http-aws-es which does a better job, so I've removed my own module.\n. I then discovered TheDeveloper/http-aws-es which does a better job, so I've removed my own module.\n. Problem In Chair, Not In Computer.\nI'd actually done\njs\n  var chain = new AWS.CredentialProviderChain();\n  chain.providers = []; // override default chain\n  var fromIni = new AWS.SharedIniFileCredentials({profile: 'foo'});\n  var fromEnv = new AWS.EnvironmentCredentials('AWS');\n  chain.providers.push(fromEnv); // for build agents\n  chain.providers.push(fromIni); // for local use\nand not cleared the providers property (ie, I hadn't added line 2 above) before resolving the chain. I misunderstood the docs.\njs\n  var fromIni = new AWS.SharedIniFileCredentials({profile: 'foo'});\n  var fromEnv = new AWS.EnvironmentCredentials('AWS');\n  var chain = new AWS.CredentialProviderChain([fromEnv, fromIni]);\nis how I imagine you meant for that to be used, right?\n. Problem In Chair, Not In Computer.\nI'd actually done\njs\n  var chain = new AWS.CredentialProviderChain();\n  chain.providers = []; // override default chain\n  var fromIni = new AWS.SharedIniFileCredentials({profile: 'foo'});\n  var fromEnv = new AWS.EnvironmentCredentials('AWS');\n  chain.providers.push(fromEnv); // for build agents\n  chain.providers.push(fromIni); // for local use\nand not cleared the providers property (ie, I hadn't added line 2 above) before resolving the chain. I misunderstood the docs.\njs\n  var fromIni = new AWS.SharedIniFileCredentials({profile: 'foo'});\n  var fromEnv = new AWS.EnvironmentCredentials('AWS');\n  var chain = new AWS.CredentialProviderChain([fromEnv, fromIni]);\nis how I imagine you meant for that to be used, right?\n. ",
    "dschenkelman": "Should be for all requests. The problem is that live sockets keep a reference to _events and that keeps the request body alive. The following figure shows the retainers:\n\nSo if you are using any sort of connection keep alive, references to the body are kept as well (at the very least for free sockets).\nSame thing hanging from the response:\n\n. Should be for all requests. The problem is that live sockets keep a reference to _events and that keeps the request body alive. The following figure shows the retainers:\n\nSo if you are using any sort of connection keep alive, references to the body are kept as well (at the very least for free sockets).\nSame thing hanging from the response:\n\n. Yes, open sockets are the originators of these.\n. Yes, open sockets are the originators of these.\n. Another thing that might be related: if you are using forever-agent to keep HTTP connections alive, it always creates a new socket instead of reusing old ones (https://github.com/request/forever-agent/blob/ece900a6e8dfac734186db3080c00875c0300450/index.js#L70) because req.useChunkedEncodingByDefault ===  true.\nUse the regular agent with keep alive instead of forever-agent.\n. Another thing that might be related: if you are using forever-agent to keep HTTP connections alive, it always creates a new socket instead of reusing old ones (https://github.com/request/forever-agent/blob/ece900a6e8dfac734186db3080c00875c0300450/index.js#L70) because req.useChunkedEncodingByDefault ===  true.\nUse the regular agent with keep alive instead of forever-agent.\n. ",
    "shimont": "We encountered the same issue - memory leaking when putting records to kinesis.\nI am trying what @dschenkelman suggested above.\n. ",
    "snoop31": "I was faced with the same problem with DynamoDB.updateItem(). Every 2 seconds, I execute this code, and according to Timeline in Chrome, every request aws-sdk creating 5 listeners. I was trying what @dschenkelman suggested but no luck... The number of listeners and memory is still increasing.\nAny suggestion how to solve this problem?\nThanks!\n\n. ",
    "lawlmart": "+1, having the same issue here. @dschenkelman solution doesn't resolve the leak for me.\n. ",
    "rocketspacer": "2017 and we are experiencing the same issue. ",
    "islam-taha": "I have reverted old commits and added the new one, thanks sir.\n. ",
    "gerardo8a": "I already launched the instance and got it registered into a cluster. I will use the api you mentioned to check for the services. \nthanks @chrisradek\n. I already launched the instance and got it registered into a cluster. I will use the api you mentioned to check for the services. \nthanks @chrisradek\n. ",
    "phsstory": "Error is intermittent. S3, DynamoDB and STS are only services used, only calls to DynamoDB reported issues. Did not show in logs on a 0.12.0 node server; however the load on that server is significantly less so no calls could have happened during the error windows.\n37 Failures 0 Success\nStart: Mon Dec 28 2015 21:01:05 GMT+0000 (UTC)\nEnd: Mon Dec 28 2015 21:01:11 GMT+0000 (UTC)\n...\nhundreds of successes\n...\n48 Failures 12 Success\nStart: Mon Dec 28 2015 21:02:25 GMT+0000 (UTC)\nEnd: Mon Dec 28 2015 21:02:30 GMT+0000 (UTC)\n... pattern continues.\nMon Dec 28 2015 21:02:42 GMT+0000 (UTC)\nMon Dec 28 2015 21:02:47 GMT+0000 (UTC)\nMon Dec 28 2015 21:03:13 GMT+0000 (UTC)\nMon Dec 28 2015 21:03:19 GMT+0000 (UTC)\nMon Dec 28 2015 21:03:19 GMT+0000 (UTC)\nMon Dec 28 2015 21:03:29 GMT+0000 (UTC)\nMon Dec 28 2015 21:03:29 GMT+0000 (UTC)\nMon Dec 28 2015 21:03:36 GMT+0000 (UTC)\n...\nTue Jan 05 2016 17:33:51 GMT+0000 (UTC)\nTue Jan 05 2016 17:33:53 GMT+0000 (UTC)\nTue Jan 05 2016 17:34:21 GMT+0000 (UTC)\nTue Jan 05 2016 17:41:18 GMT+0000 (UTC)\nOur client is running on ElasticBeanstalk but there are reports of this error in other environments (see links to amazon forums)\n```\nfunction awslog(namespace) {\n    var debug = require('debug')(namespace);\n    return { log: function (msg) { debug(msg); } };\n}\nvar _db = new AWS.DynamoDB({\n  credentials: new AWS.TemporaryCredentials({\n    RoleArn: REDACTED\n    logger: awslog('phDev:phUser:STS')\n  }),\n  logger: awslog('phDev:phUser:DynamoDB')\n});\n_db.query(params, function (err, data) {\n  if (err) { // Error is defined, app fails, err object failed to log }\n  /\n    at User. (REDACTED)\n    at Request. (/var/app/current/node_modules/aws-sdk/lib/request.js:350:18)\n    at Request.callListeners (/var/app/current/node_modules/aws-sdk/lib/sequential_executor.js:100:18)\n    at Request.emit (/var/app/current/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/var/app/current/node_modules/aws-sdk/lib/request.js:604:14)\n    at Request.transition (/var/app/current/node_modules/aws-sdk/lib/request.js:21:12)\n    at AcceptorStateMachine.runTo (/var/app/current/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /var/app/current/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/var/app/current/node_modules/aws-sdk/lib/request.js:22:9)\n    at Request. (/var/app/current/node_modules/aws-sdk/lib/request.js:606:12)\n/\n}\n```\nThis is close to the extent of information I am able to provide on a public location.\n. @chrisradek \nas an update, it has been 24hrs since we downgraded node to 0.12.0 with no indication of EPROTO errors, same code and sdk versions. I would guess it has to do with a combination of certain machines behind a load balancer (intermittent with varying time periods among reported users) and some internal deprecated feature of node or one of it's libraries. There is some speculation that it might be related to https://github.com/nodejs/node/issues/2244\nHave you had a chance to contact the DynamoDB infrastructure guys to see what might have changed on Dec 28th or if there are any machines/loadbalancers still in the cluster still serving up RC4-SHA?\n. from the sounds of it this is a compatibility issue between the sdk/node and the DynamoDB service clusters, keepAlive is only mitigating the chances of connecting to a problematic machine/loadbalancer.\nUnfortunately the amazon forums are the worst place to get information about amazon changes/issues since there is never a follow up by the techs.\n@chrisradek can you run this through the inter-department channels to see if there are DynamoDB machines/loadbalncers that attempt to TLS negotiate with RC4-SHA?\n. @chrisradek That is unfortunate as it would have been a quick diagnosis to the issue.\nAs a recap:\nDoes not appear to affect node 0.12\nDoes affect 4.1, 4.2, 5.2\nErrors appear to be grouped in 5s windows.\nkeepAlive mitigates the error but not believed to fix the problem.\n. @chrisradek I still believe this to be an issue with deprecated node functionality and a portion, likely small, of the DynamoDB stack using said encryption.\nSince we have no knowledge of the DynamoDB stack architecture, I can only take make a wild guess. based on the time windows and the mitigation effect of keepalive, we will find it in the mechanism that handles dynamic growth while maintaining network connectivity for clients connecting during those brief periods.\nIn the interim, can you get a list of the available cyphers and options from DynamoDB so we can correlate them with the supported options in node to restrict the negotiation as mentioned by @awerlang ?\n. Thanks @tristanls, \nThis is starting to have the signs of a load balancer shuffle with connections being sent to a machine not quite ready to handle load and dumping the connection prematurely causing the TLS endpoint to bail on the client connection mid buffer. The odd ball is node 0.12 not having any issue. I wonder if 0.12 was less strict on this particular protocol error or silently ignored it.\n. @chrisradek,\nyou might want to see if the DynamoDB dev env shows this same behavior with one of the reported node version under heavy load by multiple simulated accounts. You might be able to capture the connection and rebuild for protocol analysis.\n. @tristanls 0.12 doesn't throw EPROTO for this issue, it continues on without issue or degraded performance.\nWe had to downgrade our production servers until this issue can be resolved.\n. @chrisradek \nSome more info for the bug hunt, 0.12.x is statically linked against OpenSSL v1.0.1 whereas v4.x and v5.x depend on OpenSSL v1.0.2.\nWith the reported times of this behavior appearing in us-west, have the DynamoDB guys identified common release components by comparing release schedules?\n. From all of the investigations, this looks to be an issue between nodejs TLS module and DynamoDB. I have updated the issue title to reflect this assumption.\n. @tristanls is this a full mitigation aka workaround?\n. Since the nodejs retries fix is not yet released and our untested workaround requires creating the SDK object with non-default values, I feel this ticket should not be closed until such time as the SDK can function with default values on latest nodejs release.\n. @tristanls \nI personally have not completed full testing as our app is network isolated and able to use the older node version until this is fixed upstream without a code workaround.\nIf someone wants to take on the mantle of test blessing, I'll update the summary to reflect the endorsement.\n. ",
    "luizstacio": "I'm having this same problem.\nnode: v5.2.0\n. ",
    "natelaws": "Also got the same error using node v4.1.2.  Adding what @chrisradek suggested appears to work for me.\n. Also got the same error using node v4.1.2.  Adding what @chrisradek suggested appears to work for me.\n. ",
    "awerlang": "I also have noticed this issue. And because I set keepAlive to true, it was unable to serve any request.\nMy Beanstalk configuration:\n- 64bit Amazon Linux 2015.09 v2.0.4 running Node.js\n- Node v4.2.1\nI was just wondering if we could interfere in choosing the cipher (if that is indeed the case): https://nodejs.org/api/https.html#https_https_request_options_callback\n. It seems to be an issue in node: \nnodejs/node#3692\n. I noticed the error on jan 12 while running on us-east-1.\n. By default it limits on 10 frames, but it doesn't makes any difference, as a single frame is captured.. I'd like to see on the stack trace a trace to the originating call site, something that points to user code. It is okay to achieve this by requiring use of longjohn, as a regular stack trace usually points to library code anyways.. ",
    "mhkhung": "It's happening a lot. I am using only Dynamo with latest node 5.4.0 and is having the problem. \n. ",
    "designoidgames": "This also started happening in our ELB environments after updating nodejs from 0.12.9 to 4.2.3\n{ [NetworkingError: write EPROTO]\n  message: 'write EPROTO',\n  code: 'NetworkingError',\n  errno: 'EPROTO',\n  syscall: 'write',\n  address: undefined,\n  region: 'us-east-1',\n  hostname: 'dynamodb.us-east-1.amazonaws.com',\n  retryable: true,\ntime: Mon Jan 25 2016 16:29:07 GMT+0000 (UTC) } Error\n    at /var/app/current/shared/data.js:333:16 (error handler throwing it)\n    at Response. (/var/app/current/shared/dynamodb.js:272:4)\n    at Request. (/var/app/current/node_modules/aws-sdk/lib/request.js:354:18)\n    at Request.callListeners (/var/app/current/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/var/app/current/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/var/app/current/node_modules/aws-sdk/lib/request.js:596:14)\n    at Request.transition (/var/app/current/node_modules/aws-sdk/lib/request.js:21:10)\n    at AcceptorStateMachine.runTo (/var/app/current/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /var/app/current/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/var/app/current/node_modules/aws-sdk/lib/request.js:37:9)\n. ",
    "magic53": "We're also seeing this issue, even on single instance setups through Elastic Beanstalk on node versions > 0.12.9 and as a result have downgraded certain setups requiring DynamoDB to node v0.12.9.  A last resort might be to migrate to docker and run a custom build of node v5 with the hotfix mentioned in the nodejs repo.\n. We're also seeing this issue, even on single instance setups through Elastic Beanstalk on node versions > 0.12.9 and as a result have downgraded certain setups requiring DynamoDB to node v0.12.9.  A last resort might be to migrate to docker and run a custom build of node v5 with the hotfix mentioned in the nodejs repo.\n. @klinquist Which version of node are you having success with in regards to the https.Agent options?\n. @klinquist Which version of node are you having success with in regards to the https.Agent options?\n. We are having success with:\nconst dynamodb = new AWS.DynamoDB({\n  httpOptions: {\n    agent: new https.Agent({\n      ciphers: 'ALL',\n      secureProtocol: 'TLSv1_method'\n    })\n  }\n});\nHowever, we are not using DocumentClient\n. We are having success with:\nconst dynamodb = new AWS.DynamoDB({\n  httpOptions: {\n    agent: new https.Agent({\n      ciphers: 'ALL',\n      secureProtocol: 'TLSv1_method'\n    })\n  }\n});\nHowever, we are not using DocumentClient\n. @chrisradek I would also be very interested in a formal announcement regarding any fixes on behalf of the dynamodb team regarding this issue.\n. @chrisradek I would also be very interested in a formal announcement regarding any fixes on behalf of the dynamodb team regarding this issue.\n. ",
    "klinquist": "I started seeing this error yesterday for dynamo in us-west-2 (no problems until yesterday - and we've been on node v4.2 for months).    Downgraded to node 0.12.x. \n. Looks like the same thing that I'm doing (that is working):\nvar dynamodb = new AWS.DynamoDB({\n    httpOptions: {agent: new https.Agent({secureProtocol: \"TLSv1_method\", ciphers: \"ALL\"}) } }))\n. @magic53 4.2.6\n. @tielur  The problem IS intermittent - some Dynamo servers are TLS1.0, others are 1.2.  The problem is when node/openssl opens a session to 1.0 and then gets routed to a server using 1.2 for a future request.\n My code block above is working great and in production\n. @tielur   The problem, while intermittent, showed up all the time across our 30+   node services.  \nWe are using the same internal SDK with dynamo connecting code on all of them, so I updated the SDK and did tail -f *.log | grep EPROTO    for 12+ hours, no lines returned :).\n. us-west-2,  node 6.9.4. OS X.    Not intermittent - happens every time.\n\n. body toString is this.httpResponse.body.toString()\n\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>AuthorizationHeaderMalformed</Code><Message>The authorization header is malformed; the region 'us-west-2' is wrong; expecting 'us-east-1'</Message><Region>us-east-1</Region><RequestId>63F538FBA8496F6C</RequestId><HostId>UXa+AHXPijKFkuitMtbi8Cc9Zy3edUSp6Y0E+gx/hVHZi44aZKuJUtH6Z2emPc8z/dBW7xFQntM=</HostId></Error>\"\nLooks like I need to use us-east-1?\n. Aah - nevermind.   Looks like my AWS.config was setting the endpoint (used the same config as I was using for an s3 api call).. ",
    "petemill": "I'm also seeing this error quite often in dynamodb us-west-2. Whilst they are mitigating this for node LTS https://github.com/nodejs/node/issues/3692, it might be worth AWS investigating any consistency issues with cryptography across dyanamodb servers.\n. Though obviously not best practise, in a safe environment I believe you can mitigate the issue completely by turning off SSL for communication with the AWS API.\njavascript\nrequire('aws-sdk').config.update({sslEnabled: false});\n. @indutny I said it's not best practice, but if you know what you're doing, and you need to keep your service running, and you're hosted inside AWS anyway, I don't think it's so terrible if that's what you have decided to do.\n. ",
    "lloyd": "I too am hitting this error in us-west-2 with v2.2.33 of aws sdk and node v4.1.2.  \nour first observation of the problem in us-west-2 was at thursday 1/28 at 5:37pm pacific.  the problem persisted for 3 hours, and then went away, to reoccur almost exactly 24 hours later on 1/29 at 5:17.\nTiming is perhaps interesting for event correlation.\nDisabling SSL appears to fix the issue - not much of a solution - but confirms the diagnosis in this thread..\nA little more color - the issue was sporadic, at its worst, we'd see 100% failure, but often 1 in six.  we confirmed that SSL dynamo connections from the same box via CLI every 500ms would perform well during the event.\n. ",
    "tienthanh8490": "Confirm that it works after downgrading from Node 5.5.0 to 0.12.9 \n. ",
    "camjackson": "+1 to keeping this issue open until the SDK works on stable nodejs.\nAlso, a massive thank you to the crazy amount of work put in by folks in finding and fixing this problem. It's been causing me very frustrating intermittent production issues since Christmas and I had no idea why.\n. ",
    "westy92": "I updated my DynamoDB config to:\njavascript\nconst dynamodb = new AWS.DynamoDB(\n{\n  httpOptions: {\n    agent: new https.Agent(\n    {\n      ciphers: 'ALL',\n      secureProtocol: 'TLSv1_method'\n    })\n  }\n});\nHowever, I'm still seeing the same error(s):\njavascript\n{\n\u00a0\u00a0\u00a0\"stack\": \"Error: write EPROTO\\n    at Object.exports._errnoException (util.js:855:11)\\n    at exports._exceptionWithHostPort (util.js:878:20)\\n    at WriteWrap.afterWrite (net.js:763:14)\",\n\u00a0\u00a0\u00a0\"message\": \"write EPROTO\",\n\u00a0\u00a0\u00a0\"code\": \"NetworkingError\",\n\u00a0\u00a0\u00a0\"errno\": \"EPROTO\",\n\u00a0\u00a0\u00a0\"syscall\": \"write\",\n\u00a0\u00a0\u00a0\"region\": \"us-west-2\",\n\u00a0\u00a0\u00a0\"hostname\": \"dynamodb.us-west-2.amazonaws.com\",\n\u00a0\u00a0\u00a0\"retryable\": true,\n\u00a0\u00a0\u00a0\"name\": \"NetworkingError\",\n\u00a0\u00a0\u00a0\"time\": \"2016-02-04T17:10:25.509Z\"\n}\nI'm running on Node.JS 5.3.0.  Do I also need to wait until a new, stable version of Node.JS is released and use that?\n. @tristanls I restarted my Node server and I haven't been able to reproduce it again.  I know I was using the right version of my application because it is inside a specific Docker container.  I'll continue to monitor as well and report if it comes up again.\n. After allowing my Node server to run overnight, I did see the error pop up quite a few times across multiple instances.  I verified that I am using the latest aws-sdk as well, 2.2.33.  I had the \"fix\" in place that I mentioned above.\nSome more information on my environment: I'm using Docker with a base image of node:5.3.0.\n. How embarrassing.  I'm using both AWS.DynamoDB and AWS.DynamoDB.DocumentClient.  However, I only implemented the fix for AWS.DynamoDB.  Here is my final, working configuration:\n``` javascript\nconst dynamodb = new AWS.DynamoDB({\n  httpOptions: {\n    agent: new https.Agent({\n      ciphers: 'ALL',\n      secureProtocol: 'TLSv1_method'\n    })\n  }\n});\nconst dynamodbDoc = new AWS.DynamoDB.DocumentClient({\n  service: dynamodb // <- JUST ADDED TO FIX MY CONFIGURATION\n});\n```\nAs before, I will continue to monitor my system logs.  I have a good feeling about this, though. \n. Stable Node.JS has been updated to v5.6.0.  Things may work again without this workaround.\n. ",
    "indutny": "@petemill sorry, but this is pretty terrible advice.\n. maxCachedSessions: 0 could probably help too.\n. And additional patch that I am working on is https://github.com/openssl/openssl/pull/852\n. The fix is already in OpenSSL master, working on backporting it to 1.0.2 and then to the node.js master.\n. I would say node >= v4 will get this update. However, the relevant patch should land in OpenSSL 1.0.2 branch first.\n. Could there be any reason to retry requests on errors?\n. @chrisradek I'm afraid that the review process is going a bit slower than I thought it would be. You may try adding \ud83d\udc4d reaction on this PR: https://github.com/openssl/openssl/pull/918 , or something else to get their attention. Maybe it will help?\nSorry for delay!\n. @tylermakin that was a Pull Request for OpenSSL's master, which is still unstable and not used by node.js yet.\n. @TheAlphaNerd did we, though?\n. @TheAlphaNerd that was just an encouraging comment :) Hopefully it will help, though!\n. ",
    "Smbc1": "I have deployed fix like secureProtocol: \"TLSv1_method\", waiting for results.\n. @westy92 is it should work or will work? @indutny said in PR 4982 that it's just mitigation, but not a complete fix.\n. ",
    "tanyakaz": "Adding \nhttpOptions: {\n             agent: new https.Agent({\n                 secureProtocol: \"TLSv1_method\",\n                 ciphers: \"ALL\"\n             })\n         }\nseems to fix this issue on our end\n. Adding \nhttpOptions: {\n             agent: new https.Agent({\n                 secureProtocol: \"TLSv1_method\",\n                 ciphers: \"ALL\"\n             })\n         }\nseems to fix this issue on our end\n. ",
    "jmt0806": "Also getting the same error \"NetworkingError: write EPROTO\" using NodeJS 4.2.3 with AWS Elastic Beanstalk (Amazon Linux 64bit Amazon Linux 2015.09). Error occurs periodically when making calls to DynamoDB with no obvious pattern. \nEdit: Same NodeJS code was previously used as an AWS Lambda function and the issue never appeared. Code was moved to Elastic Beanstalk, due to limitations with Lambda, and error started appearing after the move.\n. Also getting the same error \"NetworkingError: write EPROTO\" using NodeJS 4.2.3 with AWS Elastic Beanstalk (Amazon Linux 64bit Amazon Linux 2015.09). Error occurs periodically when making calls to DynamoDB with no obvious pattern. \nEdit: Same NodeJS code was previously used as an AWS Lambda function and the issue never appeared. Code was moved to Elastic Beanstalk, due to limitations with Lambda, and error started appearing after the move.\n. @tristanls No, going to test and see if anything changes.\n. @tristanls No, going to test and see if anything changes.\n. @tristanls Error message has not reappeared since implementing the suggested fix. \n. @tristanls Error message has not reappeared since implementing the suggested fix. \n. ",
    "vgoloviznin": "Workaround by @westy92 works for us too on ElasticBeanstalk for node v4.2.3\n. Workaround by @westy92 works for us too on ElasticBeanstalk for node v4.2.3\n. ",
    "dvonlehman": "Just started getting this error today. It started when deploying a new app version to ElasticBeanstalk running node v4.2.3 that depended on aws-sdk@2.2.33. In this configuration the error was basically 100% consistent. I rolled back to a previous app version that used aws-sdk@2.2.26 and the error rate went way down, but still happened intermittently. I also tried the https.Agent config recommended by @klinquist and @westy92 but that did not help. Finally changed NodeVersion to 0.12.9 and the error went away.\n. ",
    "MylesBorins": "LTS has been updated to v4.4.0 including a potential fix. Would be great to hear if people are seeing improvements\n. @tylermakin we are waiting for the backport commit to be landed so we can float the patch or get it in an update to openssl \n--> https://github.com/openssl/openssl/pull/918\n. We finally got a lgtm on the openssl backport. Hopefully we can get this in\n4.x very soon!\nOn Jun 24, 2016 11:12 AM, \"Michael M.\" notifications@github.com wrote:\n\n@chrisradek https://github.com/chrisradek I would also be very\ninterested in a formal announcement regarding any fixes on behalf of the\ndynamodb team regarding this issue.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/862#issuecomment-228419355, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAecV7Br1jK1A8AIdnX0-VnvAtX5ADyyks5qPB32gaJpZM4G_BOD\n.\n. I apologize if I misunderstood.\n\nI thought https://github.com/openssl/openssl/pull/918#issuecomment-228344451\nwas a sign off.\nSorry for the confusion\nOn Jun 24, 2016 12:28 PM, \"Fedor Indutny\" notifications@github.com wrote:\n\n@TheAlphaNerd https://github.com/TheAlphaNerd did we, though?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/862#issuecomment-228439469, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAecVzeVSIdwrXxTymTigtKBLoNcB0EGks5qPC_QgaJpZM4G_BOD\n.\n. \n",
    "fdussert": "Hi, I'm not using Dynamo but I'm facing the same error by trying to connect to a secure ftp server.\nUnfortunately, I've tried all the SSL_METHOD without success.\nPerhaps not the same problem but I have tested with v4.40 also and same result, don't know if it answers your question @TheAlphaNerd \n. Hi, I'm not using Dynamo but I'm facing the same error by trying to connect to a secure ftp server.\nUnfortunately, I've tried all the SSL_METHOD without success.\nPerhaps not the same problem but I have tested with v4.40 also and same result, don't know if it answers your question @TheAlphaNerd \n. ",
    "bradennapier": "Quite a long time for this issue to continue.  Thank you guys for providing the information to resolve this.  I have had various issues with SSL / Amazon specifically so when this came up you can imagine my worry!\n. Has anyone noticed that while the issue is \"fixed\" with dynamo - it is still there for all the other amazon services?  Namely if i want to invoke Lambdas from within Lambda....?\n. Yes I am currently experiencing the problem on one function.  I am only assuming it is due to Lambda because allm y other functions appear to be fine.  I am not sure if this is the exact same issue but it always shows in the same way - functions start with a second or so left and I get charged the entire timeout period for every call until I re-upload the function.\n```\n2016-07-22T01:53:46.098Z    1aedc582-4faf-11e6-8110-8350aa252064    Starting System Info \n2016-07-22T01:53:46.098Z    1aedc582-4faf-11e6-8110-8350aa252064    Remaining Time: 1016 \n2016-07-22T01:53:46.150Z    1aedc582-4faf-11e6-8110-8350aa252064    Correct Keys\nMy functions getting called with 1 second to spare... 10 second function window... you charge me for it all... \n2016-07-22T01:52:51.150Z    f9c946bb-4fae-11e6-a4b6-ff7eb659c9ce    Starting System Info \n2016-07-22T01:52:51.150Z    f9c946bb-4fae-11e6-a4b6-ff7eb659c9ce    Remaining Time: 354 \n2016-07-22T01:52:51.211Z    f9c946bb-4fae-11e6-a4b6-ff7eb659c9ce    Correct Keys \n2016-07-22T01:52:51.293Z    f9c946bb-4fae-11e6-a4b6-ff7eb659c9ce    Token is Valid Current: 1469152371251 Expires 1469757102163\n2016-07-22T01:52:59.988Z    ffdf5964-4fae-11e6-9319-61a75113d4f2    Starting System Info \n2016-07-22T01:53:00.008Z    ffdf5964-4fae-11e6-9319-61a75113d4f2    Remaining Time: 1710 \n2016-07-22T01:53:00.029Z    ffdf5964-4fae-11e6-9319-61a75113d4f2    Correct Keys \n2016-07-22T01:53:00.070Z    ffdf5964-4fae-11e6-9319-61a75113d4f2    Token is Valid Current: 1469152380068 Expires 1469757097928 \n2016-07-22T01:53:00.071Z    ffdf5964-4fae-11e6-9319-61a75113d4f2    The Calls Required Parameters are: \nTART RequestId: 07ef8f10-4faf-11e6-bc0e-7f5932792b4b Version: $LATEST \n2016-07-22T01:53:13.949Z    07ef8f10-4faf-11e6-bc0e-7f5932792b4b    Starting System Info \n2016-07-22T01:53:13.991Z    07ef8f10-4faf-11e6-bc0e-7f5932792b4b    Remaining Time: 1253 \n2016-07-22T01:53:13.995Z    07ef8f10-4faf-11e6-bc0e-7f5932792b4b    Correct Keys \n2016-07-22T01:53:14.031Z    07ef8f10-4faf-11e6-bc0e-7f5932792b4b    Token is Valid Current: 1469152394030 Expires 1469757105179 \n2016-07-22T01:53:14.088Z    07ef8f10-4faf-11e6-bc0e-7f5932792b4b    The Calls Required Parameters are: \n2016-07-22T01:53:14.990Z    07ef8f10-4faf-11e6-bc0e-7f5932792b4b    Received Table Data: Dash_Systems\n```\nFor example, the above is a function which has 15 second timeout - the first call I make is to log remaining time.\n. @phsstory are you saying to do that for dynamo?  My other functions seem fine so I believe this one is due to lambda but i cant be sure - its a simple function so it's fairly annoying\n```\nconst Doc = documentPromised({\n    region: 'us-west-2',\n    httpOptions: { agent: new https.Agent({  ciphers: 'ALL', secureProtocol: 'TLSv1_method' }) }\n})\n```\nSo I should turn that into\nconst Doc = documentPromised({\n    region: 'us-west-2',\n    httpOptions: { agent: new https.Agent({  keepAlive: false, ciphers: 'ALL', secureProtocol: 'TLSv1_method' }) }\n})\nin my lambda function?\n(FYI Yes I tried without the promised with the same result)\n. Sure!\nimport AWS from 'aws-sdk'\nor \nconst AWS = require('aws-sdk')\nthat is all that it takes.. Sure!\nimport AWS from 'aws-sdk'\nor \nconst AWS = require('aws-sdk')\nthat is all that it takes.. ",
    "michipili": "I am also hit by this issue, using node 4.2.6 (MacPorts) and the us-west-2 region. I am  doing scan operations to backup by database, and I saw the error several times already, almost a 3rd of my executions.\n. Fantastic! Thank you for the quick reply!\n. @jedi4ever Since this issue is practically caused by heterogeneous server pools, a simple catch-retry strategy around the failing call has good chances to work around it (for me, it does) in the mean time.\n. ",
    "jedi4ever": "@indutny which version of Node do you think this be in ?\n. @indutny which version of Node do you think this be in ?\n. we fixed it with the  secureProtocol: 'TLSv1_method' setting.\nAlthough we wonder if SQS and SNS are not equally affected.\n. we fixed it with the  secureProtocol: 'TLSv1_method' setting.\nAlthough we wonder if SQS and SNS are not equally affected.\n. ",
    "adrai": "fyi: same problem with new node version on aws lambda :-( \n. fyi: same problem with new node version on aws lambda :-( \n. ",
    "wenchunzhang": "Below option is not working for me, by using it I even can't initialize the db instance. The dynamodb variable is null.\ndynamodb = new AWS.DynamoDB({\n                           httpOptions: {\n                              agent: new https.Agent({\n                                keepAlive: true,\n                                ciphers: 'ALL',\n                                secureProtocol: 'TLSv1_method'\n                            })\n                           }\n                        });\nNow I'm trying to disable SSL for db connection(not test yet, will test later):\nAWS.config.update({sslEnabled: false});\n. ",
    "Thaina": "Does this work?\njs\naws.config.update({\n    httpOptions: {\n        agent: new https.Agent({\n            ciphers: 'ALL',\n            secureProtocol: 'TLSv1_method'\n        })\n    }\n});\n. Half year of junk still can't be fix officially\nWhile they are already planned to drop support on node 0.10 on October and suggest us to use this garbage. We can't migrate our production code and the end is near. All this shit while we need to pay to use this service\nWhat the hell amazon?\n. @chrisradek Testing with it last month on tokyo region still not work. And most important is it has no official announcement that it was fix at all. Did you think we could trust our production code with this?\n. @chrisradek Problem now is, you state that you need to wait for resolve, which it mean it not really fixed officially, communicate or not it cannot be trusted with production state\nHence you still have a plan to drop support of old version which is more trusted right now. And it is october. You fix it for 6 months with a plan to drop support of old version in 10 months. Over half of the time we can't migrate to use new version\nNot to mention it waste our time to use old version of system that undeveloped\nAnother problem is, I don't know what the hell you are doing but the old version is still working. How the hell you update things that cannot work into production state and cannot rollback that component\nNot to mention is half a year of fixing garbage with all workaround from customer and we need to spend money to use this product\nSo please stop this junk and just make lambda use docker instead of fixed language and version\n. @thedevkit A new problem is here\nhttp://docs.aws.amazon.com/lambda/latest/dg/programming-model.html\n\nImportant\nThe v0.10.42 runtime will be unavailable to create new functions beginning on October 2016, given the end-of-life announcement for this version. We recommend that you use v4.3\n\nAnd 4.3 is a minefield that still not fixed. But they will force us to use it\n. @srchase For this case you first need to have account that have linked with 3rd party access token, such as facebook access token, by using getId from client side\nThen use that new identityID from the result to call getOpenIdTokenForDeveloperIdentity on server side with admin credential. But you would just set Logins parameter for getOpenIdTokenForDeveloperIdentity only with your own custom ID and the cognito identityID, and must not put the facebook access token into that object\nYou will get the error Logins don't match while I expect that it should just link the identityID with your custom ID and could get the OpenIDToken to be used in the next system. That's right. ",
    "thedevkit": "@Thaina I am not using aws.config.update, I am simply initializing my AWS.DynamoDB instance with those options, exactly how @magic53 is doing it. This has completely resolved the issue.\nI should note that I am also not using DocumentClient.\n. @tyrsius Timeouts would not necessarily be caused by this, unless your timeouts are very short. \nDo you happen to have any open database connections in your Lambda function (RDS, ElasticCache, etc)? If so, you should make sure to use context.callbackWaitsForEmptyEventLoop = false; in your Lambda function entrypoint. Open db connections stay on the event loop, so your functions will timeout.\n. Thaina this isn't a new problem. We await an official resolution.\nOn Fri, Jun 24, 2016, 8:34 PM Thaina Yu notifications@github.com wrote:\n\n@chrisradek https://github.com/chrisradek Problem now is, you state\nthat you need to wait for resolve, which it mean it not really fixed\nofficially, communicate or not it cannot be trusted with production state\nHence you still have a plan to drop support of old version which is more\ntrusted right now. And it is october. You fix it for 6 months with a plan\nto drop support of old version in 10 months. Over half of the time we can't\nmigrate to use new version\nNot to mention it waste our time to use old version of system that\nundeveloped\nAnother problem is, I don't know what the hell you are doing but the old\nversion is still working. How the hell you update things that cannot work\ninto production state and cannot rollback that component\nNot to mention is half a year of fixing garbage with all workaround from\ncustomer and we need to spend money to use this product\nSo please stop this junk and just make lambda use docker instead of fixed\nlanguage and version\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/862#issuecomment-228507115, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/ABphF86CfSEfTcH3339bzXpzWsJ2Re2uks5qPKGygaJpZM4G_BOD\n.\n. \n",
    "enGMzizo": "javascript\nawsOptions.httpOptions = {\n    agent: new https.Agent({\n      rejectUnauthorized: true,\n      keepAlive: true,                \n      secureProtocol: \"TLSv1_method\"\n    })\n  }\nSolved my problem with nodejs version 4.3.0\n. Thanks @chrisradek  , I used DynamoTwo and I don't see this error anymore . ",
    "davidvanleeuwen": "What @enGMzizo suggested solved my problem on 4.3.2 (Lambda). I also contacted AWS support to verify:\n```\nUntil an upgraded version of nodeJS is offered within Lambda the recommended method to eliminate this error is to configure the following \"httpOptions\" setting: \n| secureProtocol: \"TLSv1_method\" \nHere is an example:\nvar dynamoDB = new AWS.DynamoDB({ \nhttpOptions: { \nagent: new https.Agent({ \nrejectUnauthorized: true, \nkeepAlive: true, \nciphers: 'ALL', \nsecureProtocol: 'TLSv1_method' \n}) \n} \n}); \n\nAs noted on the github thread, this is a known issue with some versions of nodeJS. More specifically this issue is caused by the client authentication handshakes resuming a TLS session using an id they got from a TLS 1.2 endpoint. \nNewer versions of nodeJS have the tls_wrap fix commit that has made it upstream. \nhttps://github.com/nodejs/node/pull/4885 \nNode 6.0.0-pre has been confirmed working without the work-a-round above. \n```\n. ",
    "timdp": "Anyone know if other services than DynamoDB are affected? A few replies suggest using AWS.config.update rather than only configuring the DynamoDB client, which would make sense if every service sits behind the same type of encryption layer. I'm guessing that's not the case though.\n. Anyone know if other services than DynamoDB are affected? A few replies suggest using AWS.config.update rather than only configuring the DynamoDB client, which would make sense if every service sits behind the same type of encryption layer. I'm guessing that's not the case though.\n. @rma4ok Then why did @davidvanleeuwen's support contact at AWS suggest using it? I kind of assumed that they had basically copied some code from the AWS SDK source and added the secureProtocol option.\n. @rma4ok Then why did @davidvanleeuwen's support contact at AWS suggest using it? I kind of assumed that they had basically copied some code from the AWS SDK source and added the secureProtocol option.\n. ",
    "rma4ok": "@phsstory please mention in header that keepAlive: true should not be used with AWS Lambda. Because in Lambda can put node process into sleep if not used and keepAlive get disconnected. After process wakes up it throws an exception that's impossible to handle and gets killed\nError: read ECONNRESET \nat exports._errnoException (util.js:870:11) \nat TLSWrap.onread (net.js:544:26)\n. @timdp keepAlive: true is an awesome option if you run node on a server. My statement is only related to AWS Lambda. In Lambda case container with node gets put into sleep between requests.\nI think @phsstory should keep it as a part the fix solution with a comment\njs\nnew AWS.DynamoDB({\n  httpOptions: {\n    agent: new https.Agent({\n      rejectUnauthorized: true,\n      keepAlive: true, // shouldn't be used in AWS Lambda functions\n      secureProtocol: \"TLSv1_method\", // workaround part ii.\n      ciphers: \"ALL\"                  // workaround part ii.\n    })\n  }\n});\n. @timdp I see what you mean. This exception was not happening on every request but sometimes. My lambda gets about 20k hits a day. So if there is not much traffic it's a time-bomb.\n@davidvanleeuwen check you lambda logs for\nError: read ECONNRESET \nat exports._errnoException (util.js:870:11) \nat TLSWrap.onread (net.js:544:26)\nIf there are any, it's caused by keepAlive: true\n. @koresar how do you connect to local?\n. @koresar DynamoDBLocal listens for HTTP on port 8000\nall this https.Agent fix was needed for actual AWS DynamoDB services, not for local. And seems like problem is already fixed on their side.\nFor local you should use just this\nconst dynamo = new aws.DynamoDB({\n    region: 'foo-west-1',\n    apiVersion: '2012-08-10',\n    accessKeyId: 'bar',\n    secretAccessKey: 'baz',\n    endpoint: 'http://localhost:8000'\n});\n. @benoittgt you need to add maxRetries: 8 to you options. Otherwise DynamoDB client keeps retrying and your lambda exceeds 10 sec timeout and you don't get the real ErrorMessage\nvar dynamo = new aws.DynamoDB({\n    region: 'eu-west-1',\n    maxRetries: 8\n  });\nPS make sure that you have 10sec timeout for lambda, cuz it's 6sec by default\n@chrisradek could you guys put this stuff in Docs everywhere, because people never get real dynamo error messages if error is retry-able. And DynamoDB client has hardcoded retry logic which is not configurable as other AWS-SDK service clients.\nI was to determine amount of retries for most common cases\nmaxRetries: 7 for lambda with timeout 6 sec (default)\nmaxRetries: 8 for lambda with timeout 10 sec (10 seconds if much better then 6)\nmaxReties: 12 for lambda with timeout 300 sec (for lambda not behind APIG, like DynamoDB stream worker)\n\ncheers and happy coding\n. ",
    "tylermakin": "@indutny Looks like the OpenSSL issue has been patched? Does this mean 4.3 et al are now working properly? Can we re-upgrade past 0.10 again now?\n. ",
    "koresar": "I experience the same problem with DynamoDBLocal. No workarounds from above help.\nI wonder if anyone has a solution for the locally running Dynamo?\nCouldn't google it out.\nThanks people.\n. ``` js\nconst aws = require('aws-sdk');\nconst dynamo = new aws.DynamoDB({\n    region: 'foo-west-1',\n    apiVersion: '2012-08-10',\n    accessKeyId: 'bar',\n    secretAccessKey: 'baz'\n    endpoint: new aws.Endpoint('localhost:8000'), // tried all kinds of uris\n    httpOptions: {\n        agent: new https.Agent({ // tried all combinations\n            rejectUnauthorized: true,\n            keepAlive: true,\n            secureProtocol: 'TLSv1_method', // tried all the openssl supported methods\n            ciphers: 'ALL'\n        })\n    }\n});\ndynamo.describeTable({TableName: 'my-table'}, callback); // get error after a timeout\n```\nError:\n[NetworkingError: write EPROTO 140735265157120:error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number:../deps/openssl/openssl/ssl/s3_pkt.c:362:\n   ]\n     message: 'write EPROTO 140735265157120:error:1408F10B:SSL routines:SSL3_GET_RECORD:wrong version number:../deps/openssl/openssl/ssl/s3_pkt.c:362:\\n',\n     code: 'NetworkingError',\n     errno: 'EPROTO',\n     syscall: 'write',\n     region: 'foo-west-1',\n     hostname: 'localhost',\n     retryable: true,\n     time: Fri Jul 15 2016 10:00:08 GMT+1000 (AEST)\nHowever, there is a workaround - use non-encrypted connection.\nI.e. use http.Agent instead of https.Agent.\nProblem solved (until you need TLS).\n. ",
    "synthe": "Found my issue after logging output. I was checking TemplateUrl somewhere instead of TemplateURL.\n. ",
    "brianleroux": "awesome thx. (had figured this out after looking at swagger gen \u2026 docs could use some colour!)\n. awesome thx. (had figured this out after looking at swagger gen \u2026 docs could use some colour!)\n. hmmm. still having troubles.\n{ httpMethod: 'POST',\n  resourceId: '6pfvuf',\n  restApiId: 'afz6ejjune',\n  uri: 'arn:aws.apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:455488262213:function:begin-account-api-invite',\n  type: 'AWS',\n  integrationHttpMethod: 'POST',\n  credentials: 'smallwins-api-lambda' }\nWith the error:\n{ [BadRequestException: Invalid ARN specified in the request]\nBut it looks good to me?\n. hmmm. still having troubles.\n{ httpMethod: 'POST',\n  resourceId: '6pfvuf',\n  restApiId: 'afz6ejjune',\n  uri: 'arn:aws.apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:455488262213:function:begin-account-api-invite',\n  type: 'AWS',\n  integrationHttpMethod: 'POST',\n  credentials: 'smallwins-api-lambda' }\nWith the error:\n{ [BadRequestException: Invalid ARN specified in the request]\nBut it looks good to me?\n. jeez, I didn't even see that period. (must be getting late!) unfortunately still getting an error. these params:\n{ httpMethod: 'POST',\n  resourceId: 'dl5emh',\n  restApiId: 'lko08s3rjb',\n  uri: 'arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:455488262213:function:begin-account-api-register',\n  type: 'AWS',\n  integrationHttpMethod: 'POST',\n  credentials: 'smallwins-api-lambda' }\nget this:\n[BadRequestException: Invalid ARN specified in the request]\n:grimacing: \n. jeez, I didn't even see that period. (must be getting late!) unfortunately still getting an error. these params:\n{ httpMethod: 'POST',\n  resourceId: 'dl5emh',\n  restApiId: 'lko08s3rjb',\n  uri: 'arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:455488262213:function:begin-account-api-register',\n  type: 'AWS',\n  integrationHttpMethod: 'POST',\n  credentials: 'smallwins-api-lambda' }\nget this:\n[BadRequestException: Invalid ARN specified in the request]\n:grimacing: \n. Awesome! that (sort of) worked! \nWed Jan 13 23:26:16 UTC 2016 : Execution failed due to configuration error: API Gateway does not have permission to assume the provided role\nguessing this means the thing in my credentials needs but struggling though that config (alas)\n\n\nKind of crazy. Appreciate your help!\n. Awesome! that (sort of) worked! \nWed Jan 13 23:26:16 UTC 2016 : Execution failed due to configuration error: API Gateway does not have permission to assume the provided role\nguessing this means the thing in my credentials needs but struggling though that config (alas)\n\n\nKind of crazy. Appreciate your help!\n. result of actually testing (after editing the role tho / the putIntegration\nworks now w/ the role arn specified)\nOn Wed, Jan 13, 2016 at 4:10 PM, Christopher Radek <notifications@github.com\n\nwrote:\n@brianleroux https://github.com/brianleroux\nAre you seeing that message as a result of running putIntegration, or\nfrom actually testing the method?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/868#issuecomment-171478333.\n. result of actually testing (after editing the role tho / the putIntegration\nworks now w/ the role arn specified)\n\nOn Wed, Jan 13, 2016 at 4:10 PM, Christopher Radek <notifications@github.com\n\nwrote:\n@brianleroux https://github.com/brianleroux\nAre you seeing that message as a result of running putIntegration, or\nfrom actually testing the method?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/868#issuecomment-171478333.\n. Appears the error is role/policy related <AccessDeniedException> but I have it wiiiide open.\n\n```\nThu Jan 14 01:05:46 UTC 2016 : Starting execution for request: test-invoke-request\nThu Jan 14 01:05:46 UTC 2016 : API Key: test-invoke-api-key\nThu Jan 14 01:05:46 UTC 2016 : Method request path: {}\nThu Jan 14 01:05:46 UTC 2016 : Method request query string: {}\nThu Jan 14 01:05:46 UTC 2016 : Method request headers: {}\nThu Jan 14 01:05:46 UTC 2016 : Method request body before transformations: null\nThu Jan 14 01:05:46 UTC 2016 : Endpoint request URI: https://lambda.us-east-1.amazonaws.com/2015-03-31/functions/arn:aws:lambda:us-east-1:455488262213:function:begin-account-api-forgot\nThu Jan 14 01:05:46 UTC 2016 : Endpoint request headers: {Authorization=************************************d0176d, X-Amz-Date=20160114T010546Z, Accept=application/json, User-Agent=AmazonAPIGateway_ys16lfz71c, X-Amz-Security-Token=AgoGb3JpZ2luECUaCXVzLWVhc3QtMSKAAixKX5B/DA2Ene3e/F20q8UOe9I9zxElqGKIU1AkW0EvfOMnZXtyuPdKuSd3pS7ww+6IiGh2jkgaizgzpXHHQZE9og6n2abNYo0TWM2/dKFigRiQLgMHS0rCCeXOAtFSqgsF3U52z10JM2ryGG4Xu9ACBE4wqmK8sUILJvCppwdtCJCGuaQRsh1jZHgnicj2hyPKZuz1HuWAtX/rxEL8YydIHcadn57LuGDXGCNnKUA302j0GY2Cc0lccDvlFpimIo2sGzphVObNg4mVbob0PdVGdo6YJJD//IzCkQnvX8an/S4kQFkigPJZmfjh6/ff+IjBDPsgDHHS7+GacSd45V0q/QEIOhAAGgw0NTU0ODgyNjIyMTMiDJpMEg5mPGsej0kZtiraAbNH1cjLxPO3qOY0H3ctZ8zRdQG38OJXY0+NDrqvkKtJMyZpSmxDeKwYXODiMN7CpNc8+AeX7uFCaXWz3IZSohevY8atUBUkqwAeUlbfL/fF2Jxt3kV+83aPXSgRwsjpzhUM/wHyJyJjzKyUXO5EZ9AYJMORmNLZq0BhWsz0KjKj/T7qUdI+LNyEwAdu/+BWsOb+ [TRUNCATED]\nThu Jan 14 01:05:46 UTC 2016 : Endpoint request body after transformations: \n{\n  \"body\" : {},\n  \"headers\": {\n      },\n  \"method\": \"POST\",\n  \"params\": {\n      },\n  \"query\": {\n      }\n}\nThu Jan 14 01:05:46 UTC 2016 : Endpoint response body before transformations: \nUnable to determine service/operation name to be authorized\n\n``\n. Appears the error is role/policy related` but I have it wiiiide open.\n```\nThu Jan 14 01:05:46 UTC 2016 : Starting execution for request: test-invoke-request\nThu Jan 14 01:05:46 UTC 2016 : API Key: test-invoke-api-key\nThu Jan 14 01:05:46 UTC 2016 : Method request path: {}\nThu Jan 14 01:05:46 UTC 2016 : Method request query string: {}\nThu Jan 14 01:05:46 UTC 2016 : Method request headers: {}\nThu Jan 14 01:05:46 UTC 2016 : Method request body before transformations: null\nThu Jan 14 01:05:46 UTC 2016 : Endpoint request URI: https://lambda.us-east-1.amazonaws.com/2015-03-31/functions/arn:aws:lambda:us-east-1:455488262213:function:begin-account-api-forgot\nThu Jan 14 01:05:46 UTC 2016 : Endpoint request headers: {Authorization=************************************d0176d, X-Amz-Date=20160114T010546Z, Accept=application/json, User-Agent=AmazonAPIGateway_ys16lfz71c, X-Amz-Security-Token=AgoGb3JpZ2luECUaCXVzLWVhc3QtMSKAAixKX5B/DA2Ene3e/F20q8UOe9I9zxElqGKIU1AkW0EvfOMnZXtyuPdKuSd3pS7ww+6IiGh2jkgaizgzpXHHQZE9og6n2abNYo0TWM2/dKFigRiQLgMHS0rCCeXOAtFSqgsF3U52z10JM2ryGG4Xu9ACBE4wqmK8sUILJvCppwdtCJCGuaQRsh1jZHgnicj2hyPKZuz1HuWAtX/rxEL8YydIHcadn57LuGDXGCNnKUA302j0GY2Cc0lccDvlFpimIo2sGzphVObNg4mVbob0PdVGdo6YJJD//IzCkQnvX8an/S4kQFkigPJZmfjh6/ff+IjBDPsgDHHS7+GacSd45V0q/QEIOhAAGgw0NTU0ODgyNjIyMTMiDJpMEg5mPGsej0kZtiraAbNH1cjLxPO3qOY0H3ctZ8zRdQG38OJXY0+NDrqvkKtJMyZpSmxDeKwYXODiMN7CpNc8+AeX7uFCaXWz3IZSohevY8atUBUkqwAeUlbfL/fF2Jxt3kV+83aPXSgRwsjpzhUM/wHyJyJjzKyUXO5EZ9AYJMORmNLZq0BhWsz0KjKj/T7qUdI+LNyEwAdu/+BWsOb+ [TRUNCATED]\nThu Jan 14 01:05:46 UTC 2016 : Endpoint request body after transformations: \n{\n  \"body\" : {},\n  \"headers\": {\n      },\n  \"method\": \"POST\",\n  \"params\": {\n      },\n  \"query\": {\n      }\n}\nThu Jan 14 01:05:46 UTC 2016 : Endpoint response body before transformations: \nUnable to determine service/operation name to be authorized\n\n``\n. For reference this is what I'm using in theputIntegration` call:\n{ type: 'AWS',\n  httpMethod: 'POST',\n  uri: 'arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:455488262213:function:begin-account-api-invite',\n  credentials: 'arn:aws:iam::455488262213:role/smallwins-api-lambda'}\n(again huge thx for your help in getting this working)\n. For reference this is what I'm using in the putIntegration call:\n{ type: 'AWS',\n  httpMethod: 'POST',\n  uri: 'arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:455488262213:function:begin-account-api-invite',\n  credentials: 'arn:aws:iam::455488262213:role/smallwins-api-lambda'}\n(again huge thx for your help in getting this working)\n. :+1: \nI'll be back\u2026!\n. :+1: \nI'll be back\u2026!\n. ",
    "murilomothsin": "@chrisradek \nHi, today i was testing, first updated the lib, my version was 2.0.9. After that i received a different error, \"DOMException 12\". So, the error occurs in line 1851, when the headers are set. I printed in the console the content. I have a key: \"Content-Disposition\" and value: \"attachment; filename=Avaliac\u0327a\u0303o Encontro anual da visa\u0303o Lojas Paty\u2019s.doc\". So with filenames without special characters the error not occurs, I will treat this. \nThank you.\n. ",
    "neeravmehta": "Our environment is a javascript/HTML5 mobile app, it uses an older version of webkit compiler. \n. Thanks ! That would be great! \nYeah I have it working with the ArrayBuffer check.In fact it is already being used with the check elsewhere in your codebase. How long does it take for this makes it to your sdk builder site? \nAlthough I am running into a networking error related to credentials, digging deeper to find out what is going on. \n. @chrisradek \nHi, I checked it in latest version, the check still causes error. Probably javascript tries to think of it as a variable, that is not found. Here is the exact error I am getting \"Can't find variable: ArrayBuffer\"\nHere is the check that fixed it for me\nif (util.isBrowser() && typeof ArrayBuffer !== \"undefined\" && data && data.buffer instanceof ArrayBuffer) isBuffer = true;\ninstead of \nif (util.isBrowser() && ArrayBuffer && data && data.buffer instanceof ArrayBuffer) isBuffer = true;\n. Unfortunately I cannot debug our platform, so i cannot tell you the exact value for ArrayBuffer. But from what I understand it does not know what ArrayBuffer is, hence best guesses it to a variable , that is not defined and throws an error when it sees it being used. This thread helped me understand it\nhttp://stackoverflow.com/questions/3390396/how-to-check-for-undefined-in-javascript \n. @jeskew While it is'nt Appcelerator in particulr but our platform pretty much does the same thing, and it does use an older version of Webkit. \n@AdityaManohar Thanks for the update, I have already run into some issues because of this, I am trying to see if I can get this to work as that will reduce our effort significantly. Our other options are to incorporate both iOS and Android sdk for respective environments, or use REST APIs(where we end up redoing a lot of things that aws-sdk does for us)\nI am not sure if this is the right forum for this but will just put is out there, does AWS firehose have REST APIs that we can access to put records?\n. ",
    "krishnasrinivas": "We are writing S3 compatible JS client library https://github.com/minio/minio-js I was curious as to how aws-sdk-js does getBucketLocation from the browser i.e how would it signs (V4) the request without knowing the region and saw this behavior. Thanks for your response.\n. ",
    "smandava": "Seeing this on OS X node version 5.1.0 and aws-sdk@2.2.30. \n. Seeing this on OS X node version 5.1.0 and aws-sdk@2.2.30. \n. Thanks for looking into it again. It is possible that something is wrong in my environment, sharing the source code just in case.. \nNode version:\n\u276f node -v\nv5.1.0\nSource code folder:\ntest.zip\n. Thanks for looking into it again. It is possible that something is wrong in my environment, sharing the source code just in case.. \nNode version:\n\u276f node -v\nv5.1.0\nSource code folder:\ntest.zip\n. @chrisradek thanks for checking again. console.log is working fine. \nThe same code is working fine on my windows desktop. Some thing must be wrong on my Mac environment..\n. @chrisradek thanks for checking again. console.log is working fine. \nThe same code is working fine on my windows desktop. Some thing must be wrong on my Mac environment..\n. ",
    "sedninja": "@AdityaManohar Upon closer inspection I found it is possible this is an issue with RiakS2 rather than the aws-sdk. I am in fact receiving s3.amazonaws.com from the server which is configured to use my internal address as the root_host in `/etc/riak-cs/riak-cs.conf but it appears to be ignoring that value. According to the comments in the configuration file the default value here is s3.amazonaws.com so if my root_host was not used for some reason I assume this would be the expected behavior. I will go ahead and close this for now and if I find my assumption is wrong I'll let you know.\n. ",
    "papayaglobal": "Hi @LiuJoyceC,\nThe upload is part of a gulp task and the file is being written just before the upload.\nWhen the file is small everything works as expected but when it's big (over 10MB) it fails.\nhere is the code:\n```\n// aws-task.ts\n\"use strict\";\nlet AWS = require(\"aws-sdk\");\nimport * as fs from \"fs\";\n/*\n * Class for uploading a file to S3\n /\nexport class AwsTask {\nprivate filePath: string;\n\n/**\n * constructor\n * @param {string} filePath\n */\nconstructor(filePath: string) {\n    this.filePath = filePath;\n}\n\npublic upload(): Promise<void> {\n    let p = new Promise<void>( (resolve, reject) => {\n        let body = fs.createReadStream(this.filePath);\n        let params = {Bucket: \"my-bucket\", Key: \"my-key.zip\", Body: body};\n        let s3 = new AWS.S3({logger: console});\n        s3.upload(params)\n        .send( (err, data) => {\n            if (err) {\n                console.log(err);\n                reject(err);\n            }\n            else {\n                resolve();\n            }\n        });\n    });\n    return p;\n}\n\n}\n```\nAnd this is the log:\n[AWS s3 200 1.292s 0 retries] createMultipartUpload({ Bucket: 'my-bucket', Key: 'my-key.zip' })\n[AWS s3 undefined 485.203s 3 retries] uploadPart({ Body: <Buffer 50 4b 03 04 14 00 00 08 00 00 b6 8a 36 48 00 00 00 00 00 00 00 00 00 00 00 00 07 00 00 00 61 62 62 72 65 76 2f 50 4b 03 04 14 00 00 08 00 00 b6 8a 36 ... >,\n  ContentLength: 5242880,\n  PartNumber: 1,\n  Bucket: 'my-bucket',\n  Key: 'my-key.zip',\n  UploadId: '3MSnX7s.v7O_FWMD0ww82RGs.S5lsLxtmJWnz3mnjiRhsUlpwsiPIY3xTq_hTl6MyPFRR1OvYj7hX5payvZ2Fgqt7IPpojaPGEENJM2Tvg9dbD3qxPYqZxJbBlpsR46t' })\n{ [TimeoutError: Connection timed out after 120000ms]\n  message: 'Connection timed out after 120000ms',\n  code: 'NetworkingError',\n  time: Fri Jan 22 2016 17:30:13 GMT+0200 (IST),\n  region: 'us-east-1',\n  hostname: 'my-bucket.s3.amazonaws.com',\n  retryable: true }\nand the log of a smaller file:\n[AWS s3 200 1.175s 0 retries] createMultipartUpload({ Bucket: 'my-bucket', Key: 'my-key.zip' })\n  [AWS s3 200 2.397s 0 retries] uploadPart({ Body: <Buffer 4f 0b 51 71 c8 b5 52 d2 dd f8 08 67 bd bd ac ea e6 f5 5c 60 67 e1 5d c3 61 22 ac 94 82 3e 9b 16 9a 83 8c 11 b1 f1 30 af 7b 14 46 62 22 b3 21 cf 67 72 ... >,\n    ContentLength: 72202,\n    PartNumber: 2,\n    Bucket: 'my-bucket',\n    Key: 'my-key.zip',\n    UploadId: 'wHfIDbZD56M3WghgujwCBC7qwfnnN4.s5yqdFnkr7qI4vQXizQhdu3puWSYLDLoF7j7iYcRtb..zYiVy0zQYWfEEkYj8LPs5OK4JONU0QHOyi_PLazcJUe9tdOhcKfZ0' })\n  [AWS s3 200 58.755s 0 retries] uploadPart({ Body: <Buffer 50 4b 03 04 14 00 00 08 00 00 25 8a 36 48 00 00 00 00 00 00 00 00 00 00 00 00 08 00 00 00 62 61 63 6b 65 6e 64 2f 50 4b 03 04 14 00 00 08 00 00 25 8a ... >,\n    ContentLength: 5242880,\n    PartNumber: 1,\n    Bucket: 'my-bucket',\n    Key: 'my-key.zip',\n    UploadId: 'wHfIDbZD56M3WghgujwCBC7qwfnnN4.s5yqdFnkr7qI4vQXizQhdu3puWSYLDLoF7j7iYcRtb..zYiVy0zQYWfEEkYj8LPs5OK4JONU0QHOyi_PLazcJUe9tdOhcKfZ0' })\n  [AWS s3 200 1.601s 0 retries] completeMultipartUpload({ MultipartUpload: \n     { Parts: \n        [ { ETag: '\"a61a5edba1f0c72441e150ed161de5c6\"', PartNumber: 1 },\n          { ETag: '\"8aa5177502f0d50d852d089516524617\"', PartNumber: 2 },\n          [length]: 2 ] },\n    Bucket: 'my-bucket',\n    Key: 'my-key.zip',\n    UploadId: 'wHfIDbZD56M3WghgujwCBC7qwfnnN4.s5yqdFnkr7qI4vQXizQhdu3puWSYLDLoF7j7iYcRtb..zYiVy0zQYWfEEkYj8LPs5OK4JONU0QHOyi_PLazcJUe9tdOhcKfZ0' })\n. Hi @LiuJoyceC,\nTried it in a different network and it's working, thanks very much for your help.\nOfer\n. ",
    "morbo84": "@papayaglobal A bit late to the party, but I think your problem is related to what is described here and here.\nI've noticed the same behavior with my poor home connection (0.5 Mbit/s in upload) and I eventually found that if you can't upload at least 5MB in the timeout period, you are screwed; furthermore, by default the S3.upload() method will spawn several uploads concurrently, so the probability of hitting the timeout with a slow network connection are even greater.. @chrisradek\n\nThe 2 minute timeout applies to each chunk, and only allowing one chunk at a time may allow them the bandwidth they need to complete.\n\nWhere can I find this info in the doc? This is exactly what I eventually figured out, but only after a hard time spent debugging the issue. If this is the case, I think it should be documented.. @AllanFly120 Thanks for the reply!\nReplying point by point:\n1 - The documentation for the timeout parameter of httpOptions states:\n\ntimeout [Integer] \u2014 Sets the socket to timeout after timeout milliseconds of inactivity on the socket. Defaults to two minutes (120000).\n\nIn my case, I'm pretty sure there is not a 2 minutes inactivity on the socket, on any of them. I can prove it by looking at the Wireshark output, for instance, that is something like this:\n\nI've been able to capture this by some slight modifications to the original code, like setting sslEnabled: false in the AWS.S3 constructor and queueSize: 2. As you can see in the image, there are 2 distinct connection (identifiable by the Src Port column) that keep sending data until the timeout is triggered by the aws-sdk with this error message:\nError { TimeoutError: Connection timed out after 120000ms\n    at ClientRequest.<anonymous> (/.../dev/github/awslabs/aws-nodejs-sample/node_modules/aws-sdk/lib/http/node.js:83:34)\n    at ClientRequest.g (events.js:292:16)\n    at emitNone (events.js:86:13)\n    at ClientRequest.emit (events.js:185:7)\n    at Socket.emitTimeout (_http_client.js:629:10)\n    at Socket.g (events.js:292:16)\n    at emitNone (events.js:86:13)\n    at Socket.emit (events.js:185:7)\n    at Socket._onTimeout (net.js:338:8)\n    at ontimeout (timers.js:386:14)\n  message: 'Connection timed out after 120000ms',\n  code: 'TimeoutError',\n  time: 2017-08-31T08:40:15.286Z,\n  region: 'eu-central-1',\n  hostname: '<myBucket>.amazonaws.com',\n  retryable: true }\n\nI'm pretty sure that what is actually happening is not that a two minutes inactivity on the socket occurs, but that one or more parts are not completely uploaded in the time window specified by the timer. This is definitely different and it should be mentioned in the doc, imho. Do you get my point? Am I doing something wrong?\nAnother test that supports my hypothesis is that setting queueSize: 1 and partSize to 10MB, I get the usual timeout error, while the connection was sending data. This is a counterintuitive behavior, because if I use the putObject() method I can successfully upload my file, even with my slow network.\n2 - You are probably right; I have no evidence that what you suggest isn't the case. In fact, I've received the Your socket connection to the server was not read from or written to within the timeout period message only in a small fraction of tries, allegedly always with the default queueSize: 4.\n3 - While is quite reasonable that slow connection won't take advantage of a parallel multipart upload (so adjusting the queueSize value to a lower value if one expects to have a majority of users with slow network connections could be a viable solution), it sounds pretty strange to me that I have to set the timeout value in order to match slow connections; I see timeout mechanism as a solution to another problem, i.e. the one of pending idle connections, pretty much like a garbage collector. On the contrary, the only solution I get in this case to support any connection independently from its speed is to give up with the timeout, setting it to zero. What do you think about it?. In fact, my application will run on AWS EC2, so the network bandwidth won't be much of a concern, I guess. I run into this issue while doing some testing on my machine.\nFor what regards the server side timeout, I was guessing something like that; however, I can hardly see why a two minutes timeout is triggered by the S3 server when I'm streaming data to it; are those data buffered somewhere else?\nAbout the Connection timed out after 120000ms error, now that I've seen the code, I don't know what to think... I can't explain the behavior I've noticed just looking at the code. As far as I know, node.js timeouts are all idle timeouts; so how can they be triggered if the multipart upload is still ongoing? I'd really appreciate if @chrisradek could clarify this point, since he has written on other issues about the chunk timeout for multipart/managed upload.\nFinally @AllanFly120, I think a few words could be spent in the doc of S3.upload() and ManagedUpload about this topic, don't you agree?\nThanks for your time.. ",
    "jharajeev": "Hi @jeskew,\nI am really sorry about that. Will definitely raise this issue on the right platform.\n. Hi @jeskew,\nI am really sorry about that. Will definitely raise this issue on the right platform.\n. ",
    "jimitkr": "Was there a resolution on this issue? I am facing this right now. ",
    "Qard": "Yeah, I figured that much out. It seems broken to me that I'd have to set a bunch of configs that it doesn't actually use to get it to work properly though.\n. Yeah, I figured that much out. It seems broken to me that I'd have to set a bunch of configs that it doesn't actually use to get it to work properly though.\n. Not according to the docs on local dynamodb. http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Tools.DynamoDBLocal.html#Tools.DynamoDBLocal.Differences\n. Not according to the docs on local dynamodb. http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Tools.DynamoDBLocal.html#Tools.DynamoDBLocal.Differences\n. ",
    "Alex0007": "Getting same error \nUnrecognizedClientException: The security token included in the request is invalid. when trying to create tables in local db\nwhen i provided valid credentials, table created, but on aws service, not local\nThis functionality become broken on npm 2.2.32 version\n@chrisradek \n. @chrisradek \nI'm using vogels (vogels.AWS is AWS) and after updating aws-sdk i'm getting error there\n``` javascript\nimport vogels from 'vogels'\nimport requireDir from 'require-dir'\nvogels.AWS.config.update({\n  endpoint: process.env.AWS_DYNAMODB_ENDPOINT,\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\n  region: process.env.AWS_REGION\n})\nmodule.exports = async function() { // create tables\nreturn new Promise(function (resolve, reject) {\n    requireDir()\nvogels.createTables((err) => {\n  if (err) {\n    console.log(`Error creating tables: ${err}`)\n    return reject(`Error creating tables: ${err}`)\n  }\n  resolve()\n})\n\n})\n}\n```\n. @chrisradek @jeskew \nPlease, reopen this issue, because the problem really exist\n. @jeskew \nit seems that code working again on aws-sdk latest version\n. ",
    "TamerShlash": "+1 for this feature request, I had to write a custom polling function and while that's not that hard, it feels unnecessary as there's an existing way to do that.\nHowever, I want to highlight two things:\n1. The default delay is 20 and maxAttempts is 25, that's too much for a default value, and doesn't serve well for testing. Even if there's a way to customize the values, I would suggest to better tune the defaults.\n2. Instead of waiting for a fixed duration, I suggest to follow Incremental Backoff.\n. ",
    "kuba142": "Hey @chrisradek,\nSo I've got to the bottom of this ( and actually didn't notice it earlier, apologies).\nAfter testing with binaries smaller than 1MB and reviewing the logs, this issue is due to the default timeout set within Lambda. Even though 200MB+ images complete within that time, 1MB binaries don't. \nIncreasing it to around 300 seconds does complete the task in around 120 seconds for a 200MB binary file. \nThanks for your help on this.\n. Hey @chrisradek,\nSo I've got to the bottom of this ( and actually didn't notice it earlier, apologies).\nAfter testing with binaries smaller than 1MB and reviewing the logs, this issue is due to the default timeout set within Lambda. Even though 200MB+ images complete within that time, 1MB binaries don't. \nIncreasing it to around 300 seconds does complete the task in around 120 seconds for a 200MB binary file. \nThanks for your help on this.\n. ",
    "cesarpachon": "hi @chrisradek wonder if this PR can be merged.. thks! \n. hi! some chance to merge this? :) \n. done! \n. @chrisradek you are right, I blame myself. I fixed and squashed my commits. thank you. \n. ",
    "lsilvs": "Gotcha. I'm gonna try few options here.\nCheers,\nLucas\n. Gotcha. I'm gonna try few options here.\nCheers,\nLucas\n. ",
    "loretoparisi": "@chrisradek So I have found the issue. By default superagent does not specify a BASEURL, so you have to pass the whole url (hostname + uri) in order to work properly, otherwise it will try to call /uri as relative path.\nThank you.\n. Is possible that this is due to exceeding buffer size, according to:\nThe Buffer can't bigger than 2147483647 bytes. please use stream to process it.\nSee https://github.com/nodejs/node/issues/6560.. @chrisradek Thank you, I'm using within node.js\nroot@69715783975e:/musixmatch# node -v \nv6.10.3\nand Ubuntu 16.0.4 LTS.\nI'm not doing specific buffering, I'm just using the libray as-it-is:\n```\n/*\n     * Download from dataset backend\n     * @param params object\n     * @param callback function\n     /\n    S3Dataset.prototype.download = function(params,callback) {\n        var self=this;\n    var options = {\n        fileType: 'utf-8'\n    };\n    var documentKey=Util.replace(self._options.s3.Key, options.fileName);  \n    self.s3Store.getObject({ Key:  documentKey })\n    //...\n\n```\nBy the way I'm going to try the suggested examples:\nvar s3 = new AWS.S3({apiVersion: '2006-03-01'});\nvar params = {Bucket: 'myBucket', Key: 'myImageFile.jpg'};\nvar file = require('fs').createWriteStream('/path/to/file.jpg');\ns3.getObject(params).createReadStream().pipe(file);\nAnd back here with results.. @chrisradek I have tried this solution\njavascript\n        var file = fs.createWriteStream(options.filePath);\n        file.on('close', function(){\n            if(self.logger) self.logger.info(\"S3Dataset file download saved to %s\", options.filePath );\n            return callback(null,done);\n        });\n        s3.getObject({ Key:  documentKey }).createReadStream().on('error', function(err) {\n            if(self.logger) self.logger.error(\"S3Dataset download error key:%s error:%@\", options.fileName, error);\n            return callback(error);\n        }).pipe(file);\nbut nothing happens i.e. the error or close callbacks are not being fired.. The last approach I have tried was this one via httpData, since I'm successfully using this one when downloading and piping gunzip stream data as well:\njavascript\n        var s3=self.s3Store.GetInstance();\n        var file = fs.createWriteStream(options.filePath);\n        s3.getObject({ Bucket: this._options.s3.Bucket, Key: documentKey })\n        .on('error', function(error) {\n            if(self.logger) self.logger.error(\"S3Dataset download error key:%s error:%@\", options.fileName, error);\n            return callback(error);\n        })\n        .on('httpData', function(chunk) { file.write(chunk); })\n        .on('httpDone', function() { \n            file.end(); \n            if(self.logger) self.logger.info(\"S3Dataset file download saved to %s\", options.filePath );\n            return callback(null,done);\n        })\n        .send();\nAnd this time it worked, so despite of I do not know why the createReadStream,createWriteStream way does not work, I'm going via httpData at this time:\n-rw-r--r--  1 loretoparisi  staff   2,1G 25 Mag 13:04 /root/my_model.bin. ",
    "nalbion": "@LiuJoyceC #1753 is related, but has extra complexity introduced by a CloudFront path pattern. ...oh, the CloudFront behavior path pattern is not a prefix that is removed - the path is not (can not) be rewritten by CloudFront - that's a shame. thanks @jeskew and @sqlbot . ",
    "dmitriz": "Thanks @LiuJoyceC !\n. ",
    "gellin": "I am confused on how to add a custom header like the link suggests, because I am not using the AWS Request class directly, and I do not see anything exposed to do so.\nI am calling the SES sendRawMail with a custom built raw message.\nI did however test adding headers to the mail through mailcomposer, which has never failed me in the past on any email API's or services (except SES) and I received the same error.\n[InvalidParameterValue: Missing required header 'To'.]\nThis is how I added the header into my raw message, this method of adding headers has never failed me in the past. Note, that everything works PERFECTLY, if you use \"to\" instead of \"bcc\", and the headers come out just fine as well.\nvar mailOptions = {\n        from: from,\n        subject: subject,\n        text: textBody,\n        html: htmlBody,\n        bcc: to,\n        headers: [{key: \"X-Key-Name1\", value: \"val1\"}, {key: \"Force-headers\", value: \"1\"}],\n        attachments: attachments ? attachments : []\n    };\n. I am confused on how to add a custom header like the link suggests, because I am not using the AWS Request class directly, and I do not see anything exposed to do so.\nI am calling the SES sendRawMail with a custom built raw message.\nI did however test adding headers to the mail through mailcomposer, which has never failed me in the past on any email API's or services (except SES) and I received the same error.\n[InvalidParameterValue: Missing required header 'To'.]\nThis is how I added the header into my raw message, this method of adding headers has never failed me in the past. Note, that everything works PERFECTLY, if you use \"to\" instead of \"bcc\", and the headers come out just fine as well.\nvar mailOptions = {\n        from: from,\n        subject: subject,\n        text: textBody,\n        html: htmlBody,\n        bcc: to,\n        headers: [{key: \"X-Key-Name1\", value: \"val1\"}, {key: \"Force-headers\", value: \"1\"}],\n        attachments: attachments ? attachments : []\n    };\n. Thank you very much for your time, and your elegant code snippet. However I regret to inform you that I am encountering the same issue. It may be something on my end, however sending via BCC works fine with sendEmail But I cannot make sendRawEmail send via BCC at all without a \"to\" address, AND sending via sendRawEmail via to works just fine as well. It doesn't seem to matter If I have an attachment or not either.\nFull code below \n```\nconsole.log('Lambda/SES Email Fowarder Full - Version 0.1.0');\nvar aws = require('aws-sdk');\nvar ses = new aws.SES();\nvar mailcomposer = require('mailcomposer');\nvar MailComposer = mailcomposer.MailComposer;\nexports.handler = function (event, context) {\nconsole.log(\"Event: \" + JSON.stringify(event));\nif (!event.email) {\n    context.fail('Missing argument: email');\n    return;\n}\nif (!event.subject) {\n    context.fail('Missing argument: subject');\n}\nif (!event.from) {\n    context.fail('Missing argument: from');\n}\nif (!event.html && !event.text) {\n    context.fail('Missing argument: html|text');\n}\nvar to      = event.email;\nvar from    = event.from;\nvar subject = event.subject;\nvar htmlBody = event.html;\nvar textBody = event.text;\nvar attachments = event.attachments;\nvar mailOptions = {\n    from: from,\n    subject: subject,\n    text: textBody,\n    html: htmlBody,\n    bcc: to,\n    attachments: attachments ? attachments : []\n};\nvar mail = mailcomposer(mailOptions);\nmail.build(function (err, message){\n    var req = ses.sendRawEmail({RawMessage: {Data: message}});\nreq.on('build', function() {\n    req.httpRequest.headers['Force-headers'] = '1';\n});\n\nreq.send(function (err, data) {\n    if (err) {\n        console.log(err, err.stack);\n        context.fail('Internal Error: The email could not be sent.');\n    } else {\n        console.log(message);\n        context.succeed('The email was successfully sent');\n    }\n});\n\n}); \n};\n```\n. Thank you very much for your time, and your elegant code snippet. However I regret to inform you that I am encountering the same issue. It may be something on my end, however sending via BCC works fine with sendEmail But I cannot make sendRawEmail send via BCC at all without a \"to\" address, AND sending via sendRawEmail via to works just fine as well. It doesn't seem to matter If I have an attachment or not either.\nFull code below \n```\nconsole.log('Lambda/SES Email Fowarder Full - Version 0.1.0');\nvar aws = require('aws-sdk');\nvar ses = new aws.SES();\nvar mailcomposer = require('mailcomposer');\nvar MailComposer = mailcomposer.MailComposer;\nexports.handler = function (event, context) {\nconsole.log(\"Event: \" + JSON.stringify(event));\nif (!event.email) {\n    context.fail('Missing argument: email');\n    return;\n}\nif (!event.subject) {\n    context.fail('Missing argument: subject');\n}\nif (!event.from) {\n    context.fail('Missing argument: from');\n}\nif (!event.html && !event.text) {\n    context.fail('Missing argument: html|text');\n}\nvar to      = event.email;\nvar from    = event.from;\nvar subject = event.subject;\nvar htmlBody = event.html;\nvar textBody = event.text;\nvar attachments = event.attachments;\nvar mailOptions = {\n    from: from,\n    subject: subject,\n    text: textBody,\n    html: htmlBody,\n    bcc: to,\n    attachments: attachments ? attachments : []\n};\nvar mail = mailcomposer(mailOptions);\nmail.build(function (err, message){\n    var req = ses.sendRawEmail({RawMessage: {Data: message}});\nreq.on('build', function() {\n    req.httpRequest.headers['Force-headers'] = '1';\n});\n\nreq.send(function (err, data) {\n    if (err) {\n        console.log(err, err.stack);\n        context.fail('Internal Error: The email could not be sent.');\n    } else {\n        console.log(message);\n        context.succeed('The email was successfully sent');\n    }\n});\n\n}); \n};\n```\n. Yes I am running from within lambda.\n And no problem giving it a try . Here is the test event configuration I am using to fire off my lambda event. The from and other emails I am using have been 100% verified, because I am still in sandbox mode, and I can confirm that it works, because my script will send the emails if I change my mailoptions from bcc to to. If anyone tests my code exactly, just npm install mailcomposer\n{\n  \"email\": [\n    \"SOME_EMAIL@AOL.com\",\n    \"SOME_OTHER_EMAIL@AOL.com\"\n  ],\n  \"subject\": \"\u03bb SES Test \u03bb\u03bb\u03bb\",\n  \"html\": \"<br><h1>Hello world!!!!!</h1> - from lambda\",\n  \"text\": \"Hello world!!!!! - from lambda\",\n  \"from\": \"YOUR_VERIFIED_FROM_EMAIL@AOL.COM\",\n  \"attachments\": [\n    {\n      \"filename\": \"test.jpg\",\n      \"path\": \"http://www.cosmic-energy.org/wp-content/uploads/2013/01/background.jpg\"\n    }\n  ]\n}\n. Yes I am running from within lambda.\n And no problem giving it a try . Here is the test event configuration I am using to fire off my lambda event. The from and other emails I am using have been 100% verified, because I am still in sandbox mode, and I can confirm that it works, because my script will send the emails if I change my mailoptions from bcc to to. If anyone tests my code exactly, just npm install mailcomposer\n{\n  \"email\": [\n    \"SOME_EMAIL@AOL.com\",\n    \"SOME_OTHER_EMAIL@AOL.com\"\n  ],\n  \"subject\": \"\u03bb SES Test \u03bb\u03bb\u03bb\",\n  \"html\": \"<br><h1>Hello world!!!!!</h1> - from lambda\",\n  \"text\": \"Hello world!!!!! - from lambda\",\n  \"from\": \"YOUR_VERIFIED_FROM_EMAIL@AOL.COM\",\n  \"attachments\": [\n    {\n      \"filename\": \"test.jpg\",\n      \"path\": \"http://www.cosmic-energy.org/wp-content/uploads/2013/01/background.jpg\"\n    }\n  ]\n}\n. If it helps at all cc is working for me with sendRawEmail\nEdit: Or if I supply one to address bcc will work.\n. If it helps at all cc is working for me with sendRawEmail\nEdit: Or if I supply one to address bcc will work.\n. Thank you very much for your hard work on this issue. You are absolutely correct, there is no issue with your API it is an issue with my usage of it and mailcomposer. Thanks to your reply's I have been able to fix the issue, and confirmed there is no issue with the amazon SDK. Sorry for wasting your time, this issue is not an issue.\n. Thank you very much for your hard work on this issue. You are absolutely correct, there is no issue with your API it is an issue with my usage of it and mailcomposer. Thanks to your reply's I have been able to fix the issue, and confirmed there is no issue with the amazon SDK. Sorry for wasting your time, this issue is not an issue.\n. ",
    "xpressivecode": "Thanks for the response. I missed that =. \nI'm not sure if anyone on the team can help with this or if I will need to post somewhere else but based on your response I setup a nginx proxy. \nIt grabs the different query string params etc and then sends them over as headers. However, I'm still getting the SignatureDoesNotMatch error?\nIn the error response I can see the key, signature, encryption type and acl that were sent as query params and or headers. And they match up. \nThe only thing the error response doesn't include in the details is the customer-key and customer-key-md5.. but I assume that's just for security purposes. If I change the header to customer-keyy or customer-key-md55 it shows up in the response. \nAgain using the same nginx proxy.. if I just use the standard SSE vs SSE-C it works. \nBut I can see that the headers are set, they contain the correct values. I'm not sure how else to troubleshoot this? Is there something that I can enable in my s3 account, or bucket to see failed PUT requests?\n. In thinking that maybe something wasn't right with the proxy. I decided to take the pre-signed url and use curl to make the request. That way I can set the headers manually and know for certain they are being sent over.\nI'm getting the same SignatureDoesNotMatch error though?\nI'm probably doing something wrong, but here is the curl request:\ncurl -X PUT -H 'x-amz-acl: private' -H 'x-amz-server-side-encryption-customer-algorithm: AES256' -H 'x-amz-server-side-encryption-customer-key: decoded_key_goes_here' -H 'x-amz-server-side-encryption-customer-key-md5: decoded_md5_goes_here' \"https://bucket_name.s3.amazonaws.com/key_name?AWSAccessKeyId=aws_key_id&Content-Type=&Expires=1454690661&Signature=Ti9G3j4q%2B3ZOyVSK%2FSiM5iugnP4%3D&x-amz-acl=private&x-amz-server-side-encryption-customer-algorithm=AES256&x-amz-server-side-encryption-customer-key=customer_key_goes_here&x-amz-server-side-encryption-customer-key-md5=md5_goes_here\"\n. I was doing some more testing.. using the .net sdk. I didn't pass the key into the pre-sign url method. I generated it later and sent it in the headers. It works.\nSo I did the same thing, I took the SSECustomerKey property out of my getSignedUrl parameters and used the new signature. \nIt works. \nIs this a bug? I thought the sdk was being pretty helpful by automatically returning my base64 key + checkshum. But it looks like in doing so, it messes up the signature?\n. The GetPreSignedUrlRequest method doesn't allow you to set the customer key in the .net SDK. At least I don't see any public properties that would allow me to do so.\n. I guess my confusion came from the PutObject documentation exposing the properties: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#putObject-property\nAnd then compounded by the fact that it nicely base64 encoded my key + returned the checksum for me. \nThere aren't a lot of great examples out there (that I could find, sorry if there are) using the js sdk and SSE-C. So I was just trying to piece this together. \nIf it's wrong to pass those properties in (whether for the inherent security risk or otherwise), then maybe the method should not accept them or even throw an exception?\nEither way, the end result of that call returns a false positive, the signature it generates isn't valid, despite what it looks like. \nThanks for your time though, I appreciate your prompt feedback in helping me fix this.\n. Is it computing the signature against those 2 values? I guess I'm curious to know if I can continue to use the SSECustomerKey value in future or if I have to do.. what I'm doing now:\n``\nvar params = {\n                        Bucket: app.aws_bucket,\n                        Key:s${model.id}`,\n                        Expires: 60,\n                        ContentType: model.type,\n                        SSECustomerAlgorithm: 'AES256',\n                        SSECustomerKey: key\n                    };\n                var url = s3.getSignedUrl('putObject', params);\n                var segments = url.split('&');\n                var tokens = {};\n                segments.forEach((x) => {\n                   var props = x.split('=');\n                   tokens[props[0]] = props[1]; \n                });\n\n                var base64key = tokens['x-amz-server-side-encryption-customer-key'];\n                var md5 = tokens['x-amz-server-side-encryption-customer-key-md5'];\n\n                delete params.SSECustomerKey;\n\n                s3.getSignedUrl('putObject', params, function(err, data){\n\n...\ndata = ${data}&x-amz-server-side-encryption-customer-key=${base64key}&x-amz-server-side-encryption-customer-key-md5=${md5}; \ndone(null, data);\n});\n```\nOh and the nginx proxy is working too. It takes the query string params and migrates them to the header so now I can have links, or img src etc.. \nThanks for your help!\n. Thank you so much. Appreciate the turn around time.\nOn Fri, Feb 5, 2016, 6:44 PM Christopher Radek notifications@github.com\nwrote:\n\n898 https://github.com/aws/aws-sdk-js/pull/898 has been merged. It\nwill be available via npm with the next release of the SDK, or you can use\nit now by installing the sdk from github.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/896#issuecomment-180625743.\n. Sorry to be the bearer of bad news, but I don't think it's working quite right yet. \n\nIf I use the url as is by passing in the SSECustomerKey to the getSignedUrl method, the signature fails.  \nIf I swap back to the method I showed up above where I make 2 calls. One to get the base64 encoded key + checksum. And then the other to generate the clean url, then tack the customer-key and customer-key-md5 values onto the end of the url. This method works. I'm not sure how the signature is being computed, but including the SSECustomerKey seems to throw it off.\nAlso, the md5 query string param is MD5 now instead of md5. The rest are all lower case. Just wondering if that was intentional?\nThanks\n. Interesting, I'm not specifying an API version. So the url it generates looks quite different from yours. \nWhen I attempt to use v4, I'm getting errors with Invalid date for the expires, yet the docs state it should be able to accept an integer value (seconds from current date)?\nWhen I don't pass the expires query string param (defaults to 1 hour), I get an error saying it's mandatory. \nFor whatever reason, if I roll back to whatever the default version of the API is, the signature isn't calculated correctly. The signature is always a mismatch. Unless I don't include the customer key and only set the customer algorithm, then it works. But I have to generate the key + md5 manually and pass them as headers. \n// generates an invalid signature\nvar params = {\n                        Bucket: app.aws_bucket,\n                        Key: `s${model.id}`,\n                        Expires: 60,\n                        ContentType: model.type,\n                        SSECustomerAlgorithm: 'AES256',\n                        SSECustomerKey: key\n                    };\n// generates a valid signature\nvar params = {\n                        Bucket: app.aws_bucket,\n                        Key: `s${model.id}`,\n                        Expires: 60,\n                        ContentType: model.type,\n                        SSECustomerAlgorithm: 'AES256'\n                    };\nBut only if I don't specify the the API version. \n. Hi @calendee, \nI'm not sure if this will help you, but I am successfully using the javascirpt aws sdk to pre-sign for both the PUT and GET methods.\nHowever, I have to be pretty specific with how I call it or the signature fails on the s3 api end point. That or I get other weird header issues about data not being correct. \n1. I can't seem to use v4, it causes weird header issues for me\n2. I can't include the customer key or the signature doesn't validate on the s3 end point\nThe way I get around this is to make 2 calls to the pre-sign url (which is fine, it's all done locally). The first to obtain my base64 encoded key + checksum, the second to get the url I will actually use. \nIt looks something like this:\n``\nreturn new Promise((resolve, reject) => {\n            aws.config.update({ accessKeyId: _options.aws_key, secretAccessKey: _options.aws_secret });\n            // do not specify v4\n            var s3 = new aws.S3();\n            var params = {\n                Bucket: _options.aws_bucket,\n                Key:s${model.id}`,\n                Expires: 60,\n                SSECustomerAlgorithm: 'AES256',\n                SSECustomerKey: passphrase\n            };\n        // call it with the customer key to let the api handle the encoding + checksum, but do NOT use this url, it will fail to validate the signature at the s3 end point.\n        var url = s3.getSignedUrl('getObject', params);\n        var segments = url.split('&');\n        var tokens = {};\n        segments.forEach((x) => {\n            var props = x.split('=');\n            tokens[props[0]] = props[1]; \n        });\n\n        // grab the key + checksum to use later when we piece our url back together\n        var base64key = tokens['x-amz-server-side-encryption-customer-key'];\n        var md5 = tokens['x-amz-server-side-encryption-customer-key-MD5'];\n\n        // we only need to remove this from our params so that the signature is computed properly\n        delete params.SSECustomerKey;\n\n        // call the same method again, but this time the signature will validate at s3\n        // we then add the query string params back on as if it were a normal signed url request\n        s3.getSignedUrl('getObject', params, function(err, data){\n            if(err) return reject(err);\n            data = `${data}&x-amz-server-side-encryption-customer-key=${base64key}&x-amz-server-side-encryption-customer-key-md5=${md5}`;                \n            resolve(data);\n        });\n    });\n\n```\nKeep in mind that you will still need to pass the algorithm, key and checksum headers. I do this with an nginx proxy (takes the query params and adds the values as headers), so that the url acts as an actual url. \nThis is the only way that I have gotten this to work. Even with the fix that was put in place for this ticket. \nI hope this helps. \n. It might be worth noting that I must do the exact same thing with the putObject variation too. \n. hey @Timer here is the OpenResty configuration we used for prototyping the solution. You need to use openresty, as it has functionality that isn't available with nginx ootb. \n```worker_processes        1;\nerror_log                  logs/error.log;\nevents {\n    worker_connections     1024;\n}\nhttp {\n    client_max_body_size            20M;\n    proxy_headers_hash_bucket_size  1024;\n    map $is_args $algorithm {\n        default $arg_x-amz-server-side-encryption-customer-algorithm;\n    }\nmap $is_args $key {\n    default $arg_x-amz-server-side-encryption-customer-key;\n}\n\nmap $is_args $md5 {\n    default $arg_x-amz-server-side-encryption-customer-key-md5;\n}\n\nmap $is_args $ct {\n    default $arg_content-type;\n}\n\nserver {\n\n    listen 8888;\n\nlocation /hc {\n    return 200 'hello';     \n}\n\n    location ~* ^/storage/(.*) {\n        if ($request_method = OPTIONS ) {\n             add_header Access-Control-Allow-Origin \"*\";\n             add_header Access-Control-Allow-Methods \"GET, OPTIONS, PUT\";\n             add_header Access-Control-Allow-Headers \"Content-Type, Authorization, X-Requested-With, Cache-Control, Accept, Origin, X-Session-ID,Access-Control-Allow-Origin\";\n             return 200;\n        }\n\n        resolver                 8.8.8.8 valid=300s;\n        resolver_timeout         10s;\n\n        set_unescape_uri         $sig $arg_signature;\n        set_unescape_uri         $ak $arg_awsAccessKeyId;\n\n        set $ct                 'Content-Type=$ct';\n        set $s3_bucket          'your-bucket.s3.amazonaws.com';\n\n        set $aws_access_key     'AWSAccessKeyId=$ak';\n        set $url_expires        'Expires=$arg_expires';\n        set $url_signature      'Signature=$arg_signature';\n        set $url_full           '$1?$aws_access_key&$url_expires&$url_signature&ct';\n\n        set_unescape_uri         $a $algorithm;\n        set_unescape_uri         $k $key;\n        set_unescape_uri         $m $md5;\n\n        proxy_http_version       1.1;\n        proxy_set_header         Host $s3_bucket;\n        proxy_set_header         Authorization '';\n        proxy_hide_header        x-amz-id-2;\n        proxy_hide_header        x-amz-request-id;\n        proxy_hide_header        Set-Cookie;\n        proxy_ignore_headers     \"Set-Cookie\";\n        proxy_buffering          off;\n        proxy_intercept_errors   on;\n\n        proxy_set_header         x-amz-server-side-encryption-customer-algorithm    $a;\n        proxy_set_header         x-amz-server-side-encryption-customer-key      $k;\n        proxy_set_header         x-amz-server-side-encryption-customer-key-md5      $m;\n\n        proxy_pass               https://$s3_bucket/$url_full;\n    }\n}\n\n}\n```. ",
    "calendee": "I'm also struggling with this issue - however I'm trying to do a GET of an object already in S3 with SSE-C.  I simply can't get the pre-signed URL signature to work.  I modified @chrisradek script to make it easier to drop in test info.\nUsing \"aws-sdk\": \"2.2.35\" : \n```\nvar AWS = require('aws-sdk');\nvar request = require('request');\n// Have also tried this without a signature version\nvar s3 = new AWS.S3({\n  signatureVersion: 'v4'\n});\nvar key = 'PUT YOUR KEY NAME HERE';\nvar bucket = 'PUT YOUR BUCKET PATH HERE';\nvar mediaFile = 'PUT YOUR FILE NAME HERE';\nvar ssecKey = new Buffer(32).fill(key);\nvar ssecMD5 = AWS.util.crypto.md5(ssecKey.toString(), 'base64');\n//Generate the presigned url\nvar url = s3.getSignedUrl('getObject', {\n  Bucket: bucket,\n  Key: mediaFile,\n  SSECustomerAlgorithm: 'AES256',\n  SSECustomerKey: ssecKey\n});\n// Have also tried this :\n//var url = s3.getSignedUrl('getObject', {\n//  Bucket: bucket,\n//  Key: mediaFile,\n//  SSECustomerAlgorithm: 'AES256',\n//  SSECustomerKey: ssecKey,\n//  SSECustomerKeyMD5: ssecMD5\n//});\nconsole.log('URL = ' + url);\n//Test the url by making a request\nrequest({\n  url: url,\n  method: 'GET',\n  headers: {\n    'x-amz-server-side-encryption-customer-key': AWS.util.base64.encode(ssecKey),\n    'x-amz-server-side-encryption-customer-algorithm': 'AES256',\n    'x-amz-server-side-encryption-customer-key-MD5': ssecMD5\n  }\n}, function(err, resp, body) {\n  console.log('Here is response');\n  console.log(body);\n});\n```\nDepending on how I play with these values I get various outputs explaining the calculated signing doesn't match.  The current sample above errors with : \n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>AccessDenied</Code><Message>There were headers present in the request which were not signed</Message><HeadersNotSigned>x-amz-server-side\uffe2\uff80\uff8b-encryption\uffe2\uff80\uff8b-customer-key, x-amz-server-side\uffe2\uff80\uff8b-encryption\uffe2\uff80\uff8b-customer-key-md5, x-amz-server-side\uffe2\uff80\uff8b-encryption\uffe2\uff80\uff8b-customer-algorithm</HeadersNotSigned><RequestId>D996636C6BD070CC</RequestId><HostId>mDrDHGIcq8VqgQUP7PerZbDywEtPK6T95at/o3Wc739w8P2XWee2u0h4q9BehctYuJhCf4ONin8=</HostId></Error>\nI'd happily hire anyone for a few hours to help me successfully GET a previoulsy stored SSE-C encrypted file using a signed URL.\n. Anyone notice the strange symbols (Japanese) in the response? x-amz-server-side\uffe2\uff80\uff8b-encryption\uffe2\uff80\uff8b-customer-key-md5\n. @chrisradek Thanks so much for responding!\nRegion : us-east-1\nNode : v4.2.3\nRequest: 2.69.0\nContent-type of object in bucket : image/jpeg\nFYI: The strange characters in the response above were from Paw.  Here's the raw response:\n<Error><Code>AccessDenied</Code><Message>There were headers present in the request which were not signed</Message><HeadersNotSigned>x-amz-server-side&#11;-encryption&#11;-customer-algorithm, x-amz-server-side&#11;-encryption&#11;-customer-key, x-amz-server-side&#11;-encryption&#11;-customer-key-md5</HeadersNotSigned><RequestId>1B3A26E9C5461594</RequestId><HostId>V4pnht8YliwpfGEyqenAkeYuamDyRA0oumJx/Tl85Y08oIuiT9064VfRL0CY+IqI</HostId></Error>\nYou can see the actual strange dashes in the response are &#11;\n. FYI: I also just tried this on \nNode : v0.12.0\nRequest : 2.69.0\nAWS SDK : 2.2.35\n```\nvar AWS = require('aws-sdk');\nvar request = require('request');\nvar s3 = new AWS.S3({\n  signatureVersion: 'v4'\n});\nvar key = 'MY KEY HERE';\nvar bucket = 'MY BUCKET HERE';\nvar mediaFile = 'MY FILE NAME HERE';\nvar ssecKey = new Buffer(32).fill(key);\nvar ssecMD5 = AWS.util.crypto.md5(key, 'base64')\nvar url = s3.getSignedUrl('getObject', {\n  Bucket: bucket,\n  Key: mediaFile,\n  SSECustomerAlgorithm: 'AES256',\n  SSECustomerKey: key,\n  SSECustomerKeyMD5: ssecMD5\n});\nconsole.log('URL = ' + url);\nrequest({\n  url: url,\n  method: 'GET',\n  headers: {\n    'x-amz-server-side\u200b-encryption\u200b-customer-key': AWS.util.base64.encode(ssecKey),\n    'x-amz-server-side\u200b-encryption\u200b-customer-algorithm': 'AES256',\n    'x-amz-server-side\u200b-encryption\u200b-customer-key-MD5': ssecMD5\n  }\n}, function(err, resp, body) {\n  console.log('Here is response');\n  console.log(body);\n});\n```\nHere's the response : \n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>AccessDenied</Code><Message>There were headers present in the request which were not signed</Message><HeadersNotSigned>x-amz-server-side&#11;-encryption&#11;-customer-algorithm, x-amz-server-side&#11;-encryption&#11;-customer-key, x-amz-server-side&#11;-encryption&#11;-customer-key-md5</HeadersNotSigned><RequestId>AFD79965FBF0F965</RequestId><HostId>Ytg7qDZhAqouoZ2p5gwaHQLg0996d0P6CbcIffT01f3MXgQFjRtJGeWdvYkNee+Un1I6eOAc5cE=</HostId></Error>\n. FYI: I don't think this is related to Request at all.  I can take the generated signed URL and put it in a tool like Paw, add the same headers, and still get:\n```\nHTTP/1.1 403 Forbidden\nx-amz-request-id: D8F9BED8C7CB856E\nx-amz-id-2: i2AuvbuLmnYRAZsmm+nFKtEHsaNdP8KjowOiZG/JHGIGRMF0G/WB+GNUXSNMCREgjVnnGlVtvbg=\nContent-Type: application/xml\nTransfer-Encoding: chunked\nDate: Mon, 15 Feb 2016 18:09:18 GMT\nServer: AmazonS3\nConnection: close\nxml version=\"1.0\" encoding=\"UTF-8\"?\nAccessDeniedThere were headers present in the request which were not signedx-amz-copy-source-server-side-encryption-customer-algorithmD8F9BED8C7CB856Ei2AuvbuLmnYRAZsmm+nFKtEHsaNdP8KjowOiZG/JHGIGRMF0G/WB+GNUXSNMCREgjVnnGlVtvbg=\n```\n\nThat message above seems to indicate the x-amz-copy-source-server-side-encryption-customer-algorithm was not part of the signing.   If I remove the header, I then get this error:\n<Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><AWSAccessKeyId>AKIAISS2M6TNQPZLWQ7A</AWSAccessKeyId><StringToSign>AWS4-HMAC-SHA256......\nSo, on one hand it seems to NOT want the SSECustomerAlgorithm but then on the other hand it errors when it's not included.\n. @chrisradek I'm not actually including the x-amz-copy-source-server-side-encryption-customer-algorithm header in my request or my signing.  However, that's the error that AWS responds with when .... oops. I see that when I was testing using Paw that time, that I did in fact use it.\nWhen do this:\n```\ncurl -X \"GET\" \"https://s3.amazonaws.com/MY-BUCKET/MY_FILE?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAISS2M6TNQPZLWQ7A%2F20160215%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20160215T194753Z&X-Amz-Expires=900&X-Amz-Signature=fba4b4df8b92045944fafe494fda64aa61eff83046f153f1ebced207ff007b2f&X-Amz-SignedHeaders=host%3Bx-amz-server-side-encryption-customer-algorithm%3Bx-amz-server-side-encryption-customer-key%3Bx-amz-server-side-encryption-customer-key-md5&x-amz-server-side-encryption-customer-algorithm=AES256&x-amz-server-side-encryption-customer-key=MjUyYmZkNTI3MjZiYzcxYzc2OTUyMTFkZGE5MTNiYzg%3D&x-amz-server-side-encryption-customer-key-MD5=vWfyRqhzhpeg1hBQ3hG9Ig%3D%3D\" \\\n    -H \"x-amz-server-side\u200b-encryption\u200b-customer-algorithm: AES256\" \\\n    -H \"x-amz-server-side\u200b-encryption\u200b-customer-key: MY-CUSTOMER-KEY\" \\\n    -H \"x-amz-copy-source-server-side-encryption-customer-key-md5: MY-MD5\"\n```\nI still get : \n```\nHTTP/1.1 403 Forbidden\nx-amz-request-id: 59353177A109302C\nx-amz-id-2: wlG4K9z8Y7ps853b75SwhExNcdMbBqWa7NemZrz0SiakA0/+aFm46Ek3odMJ87hX9hal7STeGog=\nContent-Type: application/xml\nTransfer-Encoding: chunked\nDate: Mon, 15 Feb 2016 19:52:42 GMT\nServer: AmazonS3\nConnection: close\nxml version=\"1.0\" encoding=\"UTF-8\"?\nxml version=\"1.0\" encoding=\"UTF-8\"?\nAccessDeniedThere were headers present in the request which were not signedx-amz-server-side\uffe2\uff80\uff8b-encryption\uffe2\uff80\uff8b-customer-key, x-amz-server-side\uffe2\uff80\uff8b-encryption\uffe2\uff80\uff8b-customer-algorithm064EDB22ADF5941ErVN165hxmJSGilmHhzwUp8eieebeRNpy4Ai3dLt83KusUcGRAVf+BfmLoSnONvQA\n```\n\nAgain, notice the funny characters in the supposedly unsigned headers?  It really seems to me that something is wrong on the server side when calculating the signing.\n. Arghh!  I apologize.  I'm having trouble with Paw.  It keeps adding in that copy source header.  Please disregard.  The problem still exists using just request as described in : https://github.com/aws/aws-sdk-js/issues/896#issuecomment-184276127\n. @chrisradek Wow!  Thanks so much for putting that together. \nHowever, I never do uploading via HTTP. My IAM is configured for getObject only; so, I didn't/can't test that.  Instead, I stripped out some of your getObject code and put in a test file.   I can't quite tell the difference between your sample and mine, but this is working.\nThanks so much for clearing this up for me.\nHere's the working sample :\n```\nvar AWS = require('aws-sdk');\nvar request = require('request');\nvar s3 = new AWS.S3({\n  signatureVersion: 'v4',\n  region: 'us-east-1' //Matches bucket region\n});\nvar ssecKey = new Buffer(32).fill('MY-SECRET');\nvar ssecMD5 = AWS.util.crypto.md5(ssecKey.toString(), 'base64');\nvar url = s3.getSignedUrl('getObject', {\n  Bucket: '',\n  Key: 'MY-FILE-NAME',\n  SSECustomerAlgorithm: 'AES256',\n  SSECustomerKey: ssecKey,\n  Expires: 3600\n});\nconsole.log(\"GetObject URL: \" + url);\nconsole.log('x-amz-server-side-encryption-customer-key = ' + AWS.util.base64.encode(ssecKey));\nconsole.log('x-amz-server-side-encryption-customer-algorithm = ' + 'AES256');\nconsole.log('x-amz-server-side-encryption-customer-key-MD5 = ' + ssecMD5);\n//Use the GetObject presigned URL\nrequest({\n  url: url,\n  method: 'GET',\n  //body: 'Testing SSE-C with presigned url again too.',\n  headers: {\n    'x-amz-server-side-encryption-customer-key': AWS.util.base64.encode(ssecKey),\n    'x-amz-server-side-encryption-customer-algorithm': 'AES256',\n    'x-amz-server-side-encryption-customer-key-MD5': ssecMD5\n  }\n}, function(err, resp, body) {\n  console.log(body);\n});\n```\n. I think I see the problem in my original code.  I was including the MD5 in the signing:\nvar url = s3.getSignedUrl('getObject', {\n  Bucket: bucket,\n  Key: mediaFile,\n  SSECustomerAlgorithm: 'AES256',\n  SSECustomerKey: key,\n  SSECustomerKeyMD5: ssecMD5 // This should not have been here\n});\n. ",
    "Timer": "@xpressivecode would you be willing to share your nginx configuration to proxy a SSE-C URL which adds the required headers (pulling from query string)?. @xpressivecode would you be willing to share your nginx configuration to proxy a SSE-C URL which adds the required headers (pulling from query string)?. ",
    "der-flo": "I hope this is not too off-topic: I have a similar issue with the Lambda client. A test lambda calls a noop lambda with an overhead of ~ 50 ms.\nThe logs of the noop lambda show a duration of <1 ms per call (warmed). My measurings in the test lambda show >45 ms.\n. Whoops, working better now!\nWith the SDK version 2.6.3 and enabled keep-alive the overhead dropped to <10 ms.\n. ",
    "dlsniper": "I too am experiencing this issue.\nTo see if it helps, I've hardcoded some credentials and then run the tests again and I've managed to get a 5-10% increase in throughput.\nClearly I don't want to run any production service with hardcoded credentials so I'll have to find a way around this.\n. I too am experiencing this issue.\nTo see if it helps, I've hardcoded some credentials and then run the tests again and I've managed to get a 5-10% increase in throughput.\nClearly I don't want to run any production service with hardcoded credentials so I'll have to find a way around this.\n. ",
    "IhostVlad": "https://github.com/aws/aws-sdk-js/issues/1918. ",
    "wjrjerome": "\n@davidporter-id-au\nBy default, keepAlive is not enabled in node. Enabling this should speed things up as existing sockets will be reused. Do you mind testing this option out? You can do this by instantiating your DynamoDB client with the following options:\njs\nvar dynamo = new AWS.DynamoDB({\n  region: \"ap-southeast-2\",\n  httpOptions: {\n    agent: new https.Agent({\n      rejectUnauthorized: true,\n      keepAlive: true\n    })\n  }\n});\n\n@chrisradek do you know how long is the keep live time? It look like the connection can only keep for 5s only.. ",
    "Sparragus": "Great news! Thank you.\nCan you please report back when the update of the documentation is done?\n. It's nice that S3 will accept either form, but still, the documentation should reflect that. Right now it's ambiguous and a library developer can't know for sure which one to use.\nAgain, no problem with having both as long as they are explicitly documented.\nAnd s3rver isn't the only case. See this line of code on s3fs.  It starts the CopySource parameter without a '/'.\nHad the documentation been explicit this wouldn't happen. (Or at least it would be their fault) Both projects are great tools for S3 development, but right now they are both in conflict due to this issue.\nSo, who should I go bother, s3rver or s3fs? Who's wrong and who's right?\n. ",
    "jlumia": "@chrisradek Thank you. The signatureVersion: 'v4' in the AWS config is what let the PUT succeed.. @denisw \nThank you. The MetadataDirective worked perfectly!. ",
    "rainabba": "On further research, I discovered more documentation that indicates I should be able to use a promises implementation, but none of the events ever file for deletePreset() even though the docs say, You can alternatively register callbacks on events provided by the AWS.Request object returned by each service operation method. This request object exposes the success, error, complete, and httpData events, each taking a callback that accepts the response object. in the section AWS.Request Events\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/node-making-requests.html\nIs this approach only supported on some services/service-methods? If so, where is this documented?\n. On further research, I discovered more documentation that indicates I should be able to use a promises implementation, but none of the events ever file for deletePreset() even though the docs say, You can alternatively register callbacks on events provided by the AWS.Request object returned by each service operation method. This request object exposes the success, error, complete, and httpData events, each taking a callback that accepts the response object. in the section AWS.Request Events\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/node-making-requests.html\nIs this approach only supported on some services/service-methods? If so, where is this documented?\n. Given the throttling issues I'm seeing, I am going to have to maintain a queue to deal with slowing down my code and/or handling retries (I don't see how this issues can exist given the apparent popularity of AWS). To manage that queue, I need to remove items that succeed. Getting an empty response on success makes that kind of hard. I could try to retrieve all the presets and parse them each time I hit that callback, but that would be grossly inefficient and make the throttling issue significantly worse.\nIf the success returned the preset id, I could remove it from my queue, log the success and anything else needed.\nIf we could establish a more private communication I can provide more meaningful details than I can here, but I can say that I will be handling a significant quality and variety of unique videos (not typical 16:9, 30fps and often UHD60). Due to the unique nature of our environment, I will have to constantly maintain rendering profiles and since I'm under the impression that I can't provide them ad-hoc in a job (can I?), I will have to generate as many as 32 at a time, create the job (which will be a 1-off), and when the job is complete, delete them to deal with the 50-preset limit (which significantly slows me down since that really means I can only run 1 job at a time).\nWith my understanding of the current state of Elastic Transcoder (which is hopefully incomplete and/or incorrect), I'm not sure it will scale for us. It is ideal in many ways (save for lacking h.264 Level 5.2 and h.265 support) too so I wish that weren't so. The only other alternative I have doesn't have an API in place and that's very important for us.\nIf you're in an appropriate position, I'd be happy to connect to discuss more. I dare say that we are in a very unique position and are doing something big right now as Oculus, Samsung, C3 and a few others would confirm :)\n. Given the throttling issues I'm seeing, I am going to have to maintain a queue to deal with slowing down my code and/or handling retries (I don't see how this issues can exist given the apparent popularity of AWS). To manage that queue, I need to remove items that succeed. Getting an empty response on success makes that kind of hard. I could try to retrieve all the presets and parse them each time I hit that callback, but that would be grossly inefficient and make the throttling issue significantly worse.\nIf the success returned the preset id, I could remove it from my queue, log the success and anything else needed.\nIf we could establish a more private communication I can provide more meaningful details than I can here, but I can say that I will be handling a significant quality and variety of unique videos (not typical 16:9, 30fps and often UHD60). Due to the unique nature of our environment, I will have to constantly maintain rendering profiles and since I'm under the impression that I can't provide them ad-hoc in a job (can I?), I will have to generate as many as 32 at a time, create the job (which will be a 1-off), and when the job is complete, delete them to deal with the 50-preset limit (which significantly slows me down since that really means I can only run 1 job at a time).\nWith my understanding of the current state of Elastic Transcoder (which is hopefully incomplete and/or incorrect), I'm not sure it will scale for us. It is ideal in many ways (save for lacking h.264 Level 5.2 and h.265 support) too so I wish that weren't so. The only other alternative I have doesn't have an API in place and that's very important for us.\nIf you're in an appropriate position, I'd be happy to connect to discuss more. I dare say that we are in a very unique position and are doing something big right now as Oculus, Samsung, C3 and a few others would confirm :)\n. I am in contact with the team about having limits raised, but I also spent ~16 hours yesterday solving these issues along with implementations around ET. What I don't understand why it was required. Why wouldn't it make 100% sense to just provide the id of the object affected? If AWS isn't going to provide batched requests and can't cope with small bursts, then the library should just abstract out that behavior. I did it inside of a day and now I'll likely turn it into a module to share, but it seems like something so fundamental to this platform that I don't understand its lack of existence in the SDK. As I see it, either 1) Every person/team working with AWS API's has to solve the same problem over and over 2) My perception of the scale that AWS is used at, is grossly over-sized and so the scale of this issues hasn't been recognized. If this is just an issue with my perspective or expectations, I welcome them to be corrected.\nBecause I still see this as an issue (regardless of my ability to work around it), I'm not inclined to close it. It's like an error object without a stack, or user auth without an id. Yes, there are dozens of solutions to get to the same point, but I fully expect that the source in AWS has that id, so it's merely a matter of bubbling that back up with the event right?\n. I am in contact with the team about having limits raised, but I also spent ~16 hours yesterday solving these issues along with implementations around ET. What I don't understand why it was required. Why wouldn't it make 100% sense to just provide the id of the object affected? If AWS isn't going to provide batched requests and can't cope with small bursts, then the library should just abstract out that behavior. I did it inside of a day and now I'll likely turn it into a module to share, but it seems like something so fundamental to this platform that I don't understand its lack of existence in the SDK. As I see it, either 1) Every person/team working with AWS API's has to solve the same problem over and over 2) My perception of the scale that AWS is used at, is grossly over-sized and so the scale of this issues hasn't been recognized. If this is just an issue with my perspective or expectations, I welcome them to be corrected.\nBecause I still see this as an issue (regardless of my ability to work around it), I'm not inclined to close it. It's like an error object without a stack, or user auth without an id. Yes, there are dozens of solutions to get to the same point, but I fully expect that the source in AWS has that id, so it's merely a matter of bubbling that back up with the event right?\n. Understood. That is an important point of clarification and I have to admit that I recognized that potential, but I still wasn't recognizing the difference in teams/projects. To your knowledge, if the API was returning the data, is the SDK setup to relay it already? If so, then I must close this as it's not an issue with the SDK at all, but the API.\n. Understood. That is an important point of clarification and I have to admit that I recognized that potential, but I still wasn't recognizing the difference in teams/projects. To your knowledge, if the API was returning the data, is the SDK setup to relay it already? If so, then I must close this as it's not an issue with the SDK at all, but the API.\n. ",
    "cmawhorter": "I think I'm seeing some key name weirdness too.  \nI'm using aws-sdk 2.2.37 in browser and server (mocha tests) and there seems to be some inconsistency with behavior between browser and web.\nThe documentation says the identity pool id key is IdentityPoolId.  This does not work in node.\ntl;dr; use both IdentityPoolId and IdentityId when constructing your cognito credentials object.\nIn browser:\njs\nvar creds = new AWS.CognitoIdentityCredentials({\n    IdentityPoolId: \"us-east-1:78fc........75\",\n    RoleArn: 'arn:aws:iam::17xxxxxxx9:role/Cognito_identitypoolnameAuth_Role', \n    Logins: logins\n  })\nThis is working and creates a request to cognito with a body that is IdentityPool and NOT IdentityPoolId.  This same key translation does not seem to occur in node.\n\nHowever, using that same config in node it fails I get a IdentityPool 'us-east-1:78fc.........75' not found. error.\nChanging IdentityPoolId to IdentityId in node causes the request to succeed.\nhttp 400 w/IdentityPoolId:\n\nhttp 200 w/IdentityId:\n\n. i take some of that back... it varies from identity pool to identity pool?  yikes.\n. i'm seeing a similar issue to you.  i'll open a separate issue\n. I'd like to see AWS.config.update deprecated entirely and not become even more global.  I understand it'd be a breaking change, but in my weightless opinion, it's a worthwhile one.  \nThe way the js aws sdk loads credentials is... unique.  I can't think of any other node lib that does anything close to this.\n\nAny option that can be set via AWS.config.update can instead be passed \nin as a constructor option to a service object, and that pattern might suit \nyour needs better than AWS.config.update.\n\nIn practice, I've found this to not always be true.  I've seen some mighty weird things around loading credentials.  \nI've implemented with the constructor as you describe and inspect the object and confirm it's correctly set, only to have it still claim the region is missing or make it to the wrong region.  \nI have no idea how that is even possible, which is why I opted to always instantiate a new service client at the time of using.  That was the only thing I've found that is guaranteed to work.\nIs it possible to log where the credentials are being pulled from or better yet -- disable all credential loading magic?  require('aws-sdk').ICanLoadMyOwnCredentials_andalso_disable_all_globals\nEdit: I found #710 which seems to at least partially describe what I've seen most recently.  Also #880.   Perhaps there is some lingering weirdness here?  My problems weren't limited to ddb in the past, but lately, they've all been ddb related.\nAlso -- is it possible a node dependency is using a diff version of aws and the two different versions are somehow sharing state or something? . I think what I'm seeing is a bug related to a race maybe.\nI've recently started using a tool for deployment that requires AWS_PROFILE be set.  I'm a little murky on the internals of how aws sdk handles loading, but my gut says because that var is set, it begins to async load ~/.aws/credentials and clobbers my sync-set entries.\ne.g.\nsh\nexport AWS_PROFILE=whatever\nnode script.js\njs\n// script.js\nvar AWS = require('aws-sdk'); // detects AWS_PROFILE\nAWS.config.update({ ... });\nA race would explain the intermittent nature of what I'm seeing.  Though it doesn't explain why in the past, I've had an s3 client object config say it was set to one region, but ended up making requests against another, and throwing an error that the bucket didn't exist.\nAt any rate.  I'm just going to be putting this as the first line in the entry point for my aws dependent code:\njs\nif (process.env.NODE_ENV === 'development') {\n  Object.keys(process.env).forEach(key => 0 === key.indexOf('AWS_') ? delete process.env[key] : null);\n}\nvar AWS = require('aws-sdk');\n. I think I finally figured out the problem with the voodoo:\n\nI had a preexisting AWS_PROFILE existing in my env from jumping between projects\nEven though I was specifying the credentials on run, require('aws-sdk') would detect/use AWS_PROFILE (i think.  even from browsing aws-sdk code i'm still not entirely sure. GOTO voodoo)\nThe error message being output (sometimes) talked about invalid credentials.  This is because the absolute fallback of aws-sdk by default is to call an aws http endpoint, which times out and outputs an invalid credentials error.  (other aws-sdk issues exist about this)\n\nDoes AWS_PROFILE get higher priority than AWS.config({ ... })?  \nI don't know.  And that's my problem.\nInstead of magic on require('aws-sdk'), I should specifically need to call AWS.loadCredentialsFromEnv() or something.  \nThat auto-loading behavior of aws-sdk has bit me numerous times during dev.  With code being deployed to wrong environments/accounts and seemingly valid credentials being rejected for some unknown reason.\n\nBehind the scenes, the configuration passed to a service client \nis merged on top of a copy of AWS.config when a service client \nis instantiated\n\nIf someone is using AWS.config somewhere, it's very difficult to figure out where/what/when that global method is being invoked and can lead to a race with credentials and different configs being used depending on which async code ran an AWS.config when.\nIn practice, I never use AWS.config.  But dependencies and more junior developers sometimes do.  (Largely because of the abundance of docs and blog posts out there saying to use AWS.config).  And that fact leads to extremely difficult to debug problems.\nAnd if you're all-in on using AWS.config, there is no way to tell an EC2 constructor for example, to not merge the global AWS.config.  So even if you're doing new AWS.EC2({ ...}), unless you're explicitly listing an exhaustive list of options, there is no way to prevent AWS.config from trickling down.\nEdit: Added inverse example\n. ",
    "matsev": "@chrisradek @cmawhorter This morning, I revisited the issue that I created last week and I am unable to reproduce the error as I reported it. I am sorry for spending your time.\n. ",
    "BernhardBehrendt": "@chrisradek \nI have two different unit test.\nExtracted the executed code and showed up how the function is called:\n``` js\nvar AWS =  require('aws-sdk'),\n    treehash=require('treehash');\nAWS.config.update({\n        accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n        secretAccessKey: process.env.AWS_SECRET_KEY,\n        region: global.Constants.AWS.REGIONS.FRANKFURT\n});\nGlacier = new AWS.Glacier();\nsaveToGlacier = function(vaultName, description, buffer){\nvar params = {\n        accountId: process.env.AWS_AMAZON_ID,\n        vaultName: vaultName,\n        body: buffer,\n        checksum: treehash.getTreeHashFromBuffer(buffer);,\n        archiveDescription: description\n    };\n\nreturn new Promise(function (resolve, reject) {\n        Glacier.uploadArchive(params, function (err, data) {\n            if (err) {\n\n                log.message(err);\n\n                reject(error);\n            } else {\n                resolve(data);\n            }\n        });\n    });\n};\n\n}\nsaveToGlacier(existingVault, 'A::Backup....',fileBuffer)\n => resolves and when compare => data.checksum === treehash.getTreeHashFromBuffer(buffer)\nsaveToGlacier(aVaultWhichDoesNotExist, 'A::Backup....',fileBuffer) \n=> rejects and when compare => error.code !== ResourceNotFoundException \n```\nBut at the last unit test run yesterday it returned a ResourceNotFoundException exception. But from today it's a AccessDeniedException Exception\nBefore I run the test the aws sdk was updated.\n. @chrisradek \nJust made a test on an older branch.\nThe exeception now happens also with the previous version of aws-sdk@2.2.37 too.\nCode which depends on the aws package including the unit test weren't changed for weeks.\nThe new error message points from my perspective to an issue which is not the real error because the credentials are correct but the old one pointed exactly to what's wrong -> the specifield vault doesn*t exist.\n. @chrisradek \nTested the script also with a fantasy vault name and still get the AccessDeniedException.\n\n\n. @chrisradek Are there any updates? using the script you provided end in the same results as my previous tests.\n. @chrisradek Just verified that. Yes I get the same error.\n. ",
    "ericblade": "Get same using CloudFormation to attempt to create.  Improving the error message returned from the service would be quite helpful, I've just spent hours digging to find this post.  Of course, had I read the doc pages more closely, I would know that they were looking for a ARN, but it's always easier when the error message tells you more information about how you failed. :-D  Instead of giving the impression that something unrelated to the actual parameter was broken.\n. ",
    "jgilbert01": "```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\nSignatureDoesNotMatchThe request signature we calculated does not match the signature you provided. Check your key and signing method.abcPUT\nPMNCvA+qhZBX4gjojTebeQ==\napplication/octet-stream\nx-amz-date:Wed, 24 Feb 2016 20:38:17 GMT\n/dante-dev-backup-test/?replicationw2GjyBFNcg31oCsfMu0eDNApOos=50 55 54 0a 50 4d 4e 43 76 41 2b 71 68 5a 42 58 34 67 6a 6f 6a 54 65 62 65 51 3d 3d 0a 61 70 70 6c 69 63 61 74 69 6f 6e 2f 6f 63 74 65 74 2d 73 74 72 65 61 6d 0a 0a 78 2d 61 6d 7a 2d 64 61 74 65 3a 57 65 64 2c 20 32 34 20 46 65 62 20 32 30 31 36 20 32 30 3a 33 38 3a 31 37 20 47 4d 54 0a 2f 64 61 6e 74 65 2d 64 65 76 2d 62 61 63 6b 75 70 2d 74 65 73 74 2f 3f 72 65 70 6c 69 63 61 74 69 6f 6e5B455EB59D581317ocutt36TT9zs3r5iC4P0a5WBLw2hZRERpVHtir5nEot9Jm3rO7+kJ78+ftm8Scdh7Ql+bcZzMFw=\n```\n. Excellent!\nThe workaround does the trick.\nThx!\n. ",
    "miguelcalderon": "Hi again,\nI did as suggested (test data omitted):\n``` javascript\nvar AWS = require('aws-sdk');\nvar request = require('request');\nvar fs = require('fs');\nAWS.config.update(testData.AWSConfig);\nvar params = {\n  Bucket: testData.Bucket,\n  Key: 'uploadtest.mov',\n  ContentType: 'video/quicktime'\n};\nreturn new AWS.S3().getSignedUrl('putObject', params, function (err, presigned_url) {\n  if (err) {\n    console.log('AWS.S3().getSignedUrl error' + err);\n  } else {\n    request({\n      method: 'PUT',\n      uri: presigned_url,\n      body: fs.readFileSync(testData.filePath),\n      headers: {\n        'Content-Type': 'video/quicktime'\n      }\n    },\n    function(error, response, body) {\n      if (error) {\n        console.error(error);\n      } else {\n        console.log('upload successful:', body);\n      }\n    });\n  }\n});\n```\nThe file was uploaded and the mime type preserved. It was also playable once downloaded from S3 Browser.\nSo there seems to be a problem with my XMLHttprequest code as you suggest, but I'm not sure which could it be, since the PUT request carries the correct Content-Type header.\nDo you have any ideas that could help me solve this problem?\nThank you in any case for helping me narrow it.\n. Hi again,\nI did as suggested (test data omitted):\n``` javascript\nvar AWS = require('aws-sdk');\nvar request = require('request');\nvar fs = require('fs');\nAWS.config.update(testData.AWSConfig);\nvar params = {\n  Bucket: testData.Bucket,\n  Key: 'uploadtest.mov',\n  ContentType: 'video/quicktime'\n};\nreturn new AWS.S3().getSignedUrl('putObject', params, function (err, presigned_url) {\n  if (err) {\n    console.log('AWS.S3().getSignedUrl error' + err);\n  } else {\n    request({\n      method: 'PUT',\n      uri: presigned_url,\n      body: fs.readFileSync(testData.filePath),\n      headers: {\n        'Content-Type': 'video/quicktime'\n      }\n    },\n    function(error, response, body) {\n      if (error) {\n        console.error(error);\n      } else {\n        console.log('upload successful:', body);\n      }\n    });\n  }\n});\n```\nThe file was uploaded and the mime type preserved. It was also playable once downloaded from S3 Browser.\nSo there seems to be a problem with my XMLHttprequest code as you suggest, but I'm not sure which could it be, since the PUT request carries the correct Content-Type header.\nDo you have any ideas that could help me solve this problem?\nThank you in any case for helping me narrow it.\n. ",
    "imakshath": "@chrisradek Thanks for your response. I have a meteor app from which i have to access cloud search domain service. Currently I am using meteor plugin which is build from browser version of SDK(https://github.com/peerlibrary/meteor-aws-sdk). So Could you please suggest me the exact way to use the SDK in my client app.\n. @chrisradek  ..Actually i was using browser version SDK when i moved it into server basis its working fine. Thanks your support \n. Fixed. I have used JSON.stringify(doc).\n. ",
    "guymguym": "Hi\nThis gives a significant boost of upload performance (~300 MB/sec increase in my case) when the source stream is providing buffers of the same size of the part size, since it avoids the memory copy that Buffer.concat() does even in case of a single buffer.\nWould be great if you want to merge.\nCheers\n. Hi\nThis gives a significant boost of upload performance (~300 MB/sec increase in my case) when the source stream is providing buffers of the same size of the part size, since it avoids the memory copy that Buffer.concat() does even in case of a single buffer.\nWould be great if you want to merge.\nCheers\n. @chrisradek thanks for the comment. updated.\n. @chrisradek thanks for the comment. updated.\n. Great! :)\n. Great! :)\n. Hey @chrisradek, would love to get your feedback on this fix. I know it's in a very common path so let me know if you see anything i could do to write it safer for existing code and make it easier to merge. Thanks!\n. Hey @chrisradek, would love to get your feedback on this fix. I know it's in a very common path so let me know if you see anything i could do to write it safer for existing code and make it easier to merge. Thanks!\n. Hey @chrisradek Sorry about the hassle.\nAre these tests public that I can also run?\nIs it simply to run the project tests with 0.8 and 0.10 (I have nvm so no problem)?\nI'm pretty sure it will easy to resolve.\nThanks\n. Hey @chrisradek Sorry about the hassle.\nAre these tests public that I can also run?\nIs it simply to run the project tests with 0.8 and 0.10 (I have nvm so no problem)?\nI'm pretty sure it will easy to resolve.\nThanks\n. I ran the project's npm install && npm teston all the nvm versions I have below, and it worked for all besides 0.8 where is failed during install with errors that seem unrelated to the change -\n```\n$ nvm ls\n->       v0.8.28\n       v0.10.44\n       v4.4.2\n       v5.10.0\n$ npm install\nnpm http 200 https://registry.npmjs.org/cucumber\nnpm http 200 https://registry.npmjs.org/mocha\nnpm ERR! registry error parsing json\nnpm ERR! SyntaxError: Unexpected token \nnpm ERR! \ufffd\ufffd\ufffdo\ufffd6\ufffd\ufffdA{\ufffd~\ufffd\ufffdm\ufffd\ufffd\n.....\n``\n. I ran the project'snpm install && npm test`on all the nvm versions I have below, and it worked for all besides 0.8 where is failed during install with errors that seem unrelated to the change -\n```\n$ nvm ls\n->       v0.8.28\n       v0.10.44\n       v4.4.2\n       v5.10.0\n$ npm install\nnpm http 200 https://registry.npmjs.org/cucumber\nnpm http 200 https://registry.npmjs.org/mocha\nnpm ERR! registry error parsing json\nnpm ERR! SyntaxError: Unexpected token \nnpm ERR! \ufffd\ufffd\ufffdo\ufffd6\ufffd\ufffdA{\ufffd~\ufffd\ufffdm\ufffd\ufffd\n.....\n```\n. @chrisradek no worries, at least the progress test that you pointed out is quite easy to figure out.\nThis test asserts that once a putObject of 2 MB buffer is used, there will be 2 or more progress events, where the first should have a loaded count of at least 10.\nBefore my change every buffer was converted to a stream, which had a highWaterMark of 16KB which means that the event was issued many times, certainly more than once.\nIn my change, I simply changed to avoid such a conversion from memory buffer to stream that was hurting performance, and simply write the entire buffer at one call (stream.end(body)) and emitted a single progress event that the entire data is loaded.\nIt is quite simple to imitate the asserted behavior, but the question is if it makes sense to assert that a buffer should provide multiple progress events? \nWould be great to know what you think.\nThanks!\n. @chrisradek no worries, at least the progress test that you pointed out is quite easy to figure out.\nThis test asserts that once a putObject of 2 MB buffer is used, there will be 2 or more progress events, where the first should have a loaded count of at least 10.\nBefore my change every buffer was converted to a stream, which had a highWaterMark of 16KB which means that the event was issued many times, certainly more than once.\nIn my change, I simply changed to avoid such a conversion from memory buffer to stream that was hurting performance, and simply write the entire buffer at one call (stream.end(body)) and emitted a single progress event that the entire data is loaded.\nIt is quite simple to imitate the asserted behavior, but the question is if it makes sense to assert that a buffer should provide multiple progress events? \nWould be great to know what you think.\nThanks!\n. I tried a fix that will emit a progress event once the output 'drain' event is received.\nThis should fix that case at least.\nBut I couldn't get to configure the cucumber tests for my env... any instructions for that?\n. I tried a fix that will emit a progress event once the output 'drain' event is received.\nThis should fix that case at least.\nBut I couldn't get to configure the cucumber tests for my env... any instructions for that?\n. BTW the travis failures seem unrelated to my changes - not sure what's that jam:\n$ cat coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js\n/home/travis/build/aws/aws-sdk-js/node_modules/coveralls/bin/coveralls.js:18\n        throw err;\n        ^\nBad response: 422 {\"message\":\"Couldn't find a repository matching this job.\",\"error\":true}\n. BTW the travis failures seem unrelated to my changes - not sure what's that jam:\n$ cat coverage/lcov.info | ./node_modules/coveralls/bin/coveralls.js\n/home/travis/build/aws/aws-sdk-js/node_modules/coveralls/bin/coveralls.js:18\n        throw err;\n        ^\nBad response: 422 {\"message\":\"Couldn't find a repository matching this job.\",\"error\":true}\n. @chrisradek \nThe build issue was transient and not related.\nI think I fixed the issue with the cucumber test that you pointed out - can you help to rerun?\nThanks!\n. @chrisradek \nThe build issue was transient and not related.\nI think I fixed the issue with the cucumber test that you pointed out - can you help to rerun?\nThanks!\n. @chrisradek \nI want to help push this, but not sure how to run the cucumber tests myself...\nI tried naively using a configuration file same as configuration.sample but get this error:\n@acm\nFeature: \n  I want to use AWS Certificate Manager\n. @chrisradek \nI want to help push this, but not sure how to run the cucumber tests myself...\nI tried naively using a configuration file same as configuration.sample but get this error:\n@acm\nFeature: \n  I want to use AWS Certificate Manager\n. thanks, all passed:\n39 scenarios (39 passed)\n242 steps (242 passed)\n. thanks, all passed:\n39 scenarios (39 passed)\n242 steps (242 passed)\n. Hey @chrisradek \nDo you see any gaps?\nI had a minor dilema about the way buffer/string data emits progress events:\n- For best performance it is sent with stream.end(body) since the http request stream can take the entire memory buffer and process it as quickly as it can.\n- However to give progress events as required by the tests I did something fuzzy - I emitted the progress event twice with loadedBytes = totalBytes - once just before pushing the buffer to the stream, and another time once the stream drains. This does pass the tests, but might seem a bit weird to anyone observing this from the client perspective.\n- If you prefer to sacrifice the simplicity (and possibly some performance) for the sake of finer progress events then I can suggest the code change below that will send in bulks. Notice that it will behave exactly the same as before for body of length smaller than bulk size.\nPersonally I would probably prefer to keep it simple, but I can certainly understand if you think the finer events are better for clients, and it depends on how common it is to send very large request body.\nWould be great to know how you think about it.\nThanks,\nGuy\n```\nvar MAX_BULK_SIZE = 8 * 1024 * 1024;\nvar remaining_body = body.slice();\nsend_body_in_bulks();\nfunction send_body_in_bulks() {\n  if (!remaining_body.length) {\n    stream.end();\n    loadedBytes = totalBytes;\n    stream.emit('sendProgress', {\n      loaded: loadedBytes,\n      total: totalBytes\n    });\n    return;\n  } \n// slice and send a bulk\n  var bulk = remaining_body.slice(0, MAX_BULK_SIZE);\n  remaining_body = remaining_body.slice(bulk.length);\n  var completed = stream.write(bulk);\n// report progress\n  loadedBytes += bulk.length;\n  stream.emit('sendProgress', {\n    loaded: loadedBytes,\n    total: totalBytes\n  });\n// if the write completed, we can immediately write more,\n  // otherwise wait for stream to drain it's buffered data\n  if (completed) {\n      setImmediate(send_body_in_bulks);\n  } else {\n      stream.once('drain', send_body_in_bulks);\n  }\n}\n```\n. Hey @chrisradek \nDo you see any gaps?\nI had a minor dilema about the way buffer/string data emits progress events:\n- For best performance it is sent with stream.end(body) since the http request stream can take the entire memory buffer and process it as quickly as it can.\n- However to give progress events as required by the tests I did something fuzzy - I emitted the progress event twice with loadedBytes = totalBytes - once just before pushing the buffer to the stream, and another time once the stream drains. This does pass the tests, but might seem a bit weird to anyone observing this from the client perspective.\n- If you prefer to sacrifice the simplicity (and possibly some performance) for the sake of finer progress events then I can suggest the code change below that will send in bulks. Notice that it will behave exactly the same as before for body of length smaller than bulk size.\nPersonally I would probably prefer to keep it simple, but I can certainly understand if you think the finer events are better for clients, and it depends on how common it is to send very large request body.\nWould be great to know how you think about it.\nThanks,\nGuy\n```\nvar MAX_BULK_SIZE = 8 * 1024 * 1024;\nvar remaining_body = body.slice();\nsend_body_in_bulks();\nfunction send_body_in_bulks() {\n  if (!remaining_body.length) {\n    stream.end();\n    loadedBytes = totalBytes;\n    stream.emit('sendProgress', {\n      loaded: loadedBytes,\n      total: totalBytes\n    });\n    return;\n  } \n// slice and send a bulk\n  var bulk = remaining_body.slice(0, MAX_BULK_SIZE);\n  remaining_body = remaining_body.slice(bulk.length);\n  var completed = stream.write(bulk);\n// report progress\n  loadedBytes += bulk.length;\n  stream.emit('sendProgress', {\n    loaded: loadedBytes,\n    total: totalBytes\n  });\n// if the write completed, we can immediately write more,\n  // otherwise wait for stream to drain it's buffered data\n  if (completed) {\n      setImmediate(send_body_in_bulks);\n  } else {\n      stream.once('drain', send_body_in_bulks);\n  }\n}\n``\n. @chrisradek \nSee the fixes according to our discussion.\nI tested withnpm testandcucumberwith both node v4.4.3 and v0.10.\n. @chrisradek \nSee the fixes according to our discussion.\nI tested withnpm testandcucumber` with both node v4.4.3 and v0.10.\n. BTW I remembered correctly that using multiple writable streams behaves weird in nodejs v4 - I just submitted this issue nodejs/node#6491. \nIt was actually only fixed on nodejs v5.11 but I guess they will want to pull back to v4 for LTS.\nAnyway that's a good reason to use Transform instead.\n. BTW I remembered correctly that using multiple writable streams behaves weird in nodejs v4 - I just submitted this issue nodejs/node#6491. \nIt was actually only fixed on nodejs v5.11 but I guess they will want to pull back to v4 for LTS.\nAnyway that's a good reason to use Transform instead.\n. Great so I can fix the cucumber test to expect > 0 events instead of > 1?\n. Very good point. Thanks for taking the time to review in depth.\nIn one of my previous tests I saw that the pipe to the writable progressStream caused buffers to be copied. But now I'm not sure about it completely, as it might have mixed with the buffer path in the code in my runs. \nAnyhow, I think to change the path of stream-body progress to use Transform stream to emit the events in the familiar pattern body.pipe(progressStream).pipe(stream). Transform stream will push the same buffer so will surely avoid copies. sounds right?\n. Oh sure, so there is no way to provide stream progress for 0.8?\n. ",
    "dgreene-r7": "I swear I tried this a while ago and it wasn't working but damned if it's not behaving perfectly right now. Thanks!\n. I swear I tried this a while ago and it wasn't working but damned if it's not behaving perfectly right now. Thanks!\n. ",
    "kbariotis": "@LiuJoyceC \n\nIf you stub out the method on the service class prototype, that should take effect for all instances that you create for that service, including ones that have already been instantiated.\n\nThis doesn't seem to work. Is it something that was working and broke at some point after this issue?\nThank you. The same happened to us. Could that be the reason for it https://github.com/aws/aws-sdk-js/commit/fe88308a8699b39aef06492c898b265a3a24251f#diff-cca1065697cbb567e3320496b1f7448fR43? That's hilarious.. @chrisradek we happen to have some legacy code that we don't want to update just yet. I haven't been working with the SDK for some time but I thought that a promise() property was the pattern to return a promise, not an Async suffixed function.\nAnyway, this incident was so random Boone could have saw it coming. :). ",
    "dmitri-bazz": "Hello @LiuJoyceC \nThank you for the swift response. It's unlikely that the upload function is being called elsewhere (the else code handles the case of an offline VM), however for prudence's sake I've renamed the function.\nThe version of node being ran is v4.2.1.\nThis is the output of Object.getOwnPropertyNames(AWS.S3.prototype)\n[ 'initialize',\n  'setupRequestListeners',\n  'populateURI',\n  'addContentType',\n  'computableChecksumOperations',\n  'willComputeChecksums',\n  'computeContentMd5',\n  'computeSha256',\n  'pathStyleBucketName',\n  'dnsCompatibleBucketName',\n  'escapePathParam',\n  'successfulResponse',\n  'retryableError',\n  'extractData',\n  'extractError',\n  'setEndpoint',\n  'getSignedUrl',\n  'constructor',\n  'createBucket' ]\nThis is how the code looks as of now\n```\nexports.uploadReportToS3 = function name(tenantGuid, reportType, path, body) {\n  const isUsingAWS = hostsConfig.mode.usingAWS == 'true' ? true : false;\n  if (isUsingAWS) {\n    var tempLogger = {\n      log: function (params) {\n        global.logger.info(params);\n      }\n    };\n\n    var s3 = new AWS.S3({logger: tempLogger}),\n      params = {\n        Bucket: S3_BUCKET,\n        Key: path,\n        Body: body\n    },\n    retryCounter = 5;\n\n    function _upload() {\n      global.logger.info(Object.getOwnPropertyNames(AWS.S3.prototype));\n        s3.upload(params, function(err, data) {\n            if (err) {\n                // If there was an error, log it, and then retry the upload, up to 5 times\n                global.logger.error('Error in s3.upload: ' + err);\n                retryCounter -= 1;\n                if (retryCounter > 0) {\n                    _upload();\n                }\n            } else {\n                global.logger.info('Report data uploaded to ' + path);\n            }\n        });\n    }\n\n    _upload();\n\n} else {\n\n```\n. So I think I found what the issue was. We were previously running aws-sdk \"2.0.0-rc8\". I then manually updated it to the latest version with npm via \"npm update aws-sdk\", which seems to not work correctly. After purging node_modules and reinstalling all the npm modules through package.json everything worked fine. \nThanks for your help @LiuJoyceC !\n. ",
    "jardakotesovec": "This inconsistency also kicked us recently..\n. ",
    "naknomum": "thanks for the quick fix!\n. thanks for the quick fix!\n. ",
    "sholladay": "\nnote that the result object still has 'key' with lowercase k\n\nCan we get lowercase for everything since uppercase is weird and this is JavaScript and none of these things are constructors?  \ud83d\ude07 . > note that the result object still has 'key' with lowercase k\nCan we get lowercase for everything since uppercase is weird and this is JavaScript and none of these things are constructors?  \ud83d\ude07 . There is one tool I am aware of that takes CommonJS modules as input and outputs ES modules, which is exactly what you need to automate the conversion.\nhttps://github.com/rollup/rollup-plugin-commonjs\nThat Rollup plugin is intended to be used as part of a build process to import CommonJS dependencies, but I suspect you could hack it to suit your needs relatively easily.\nBy the way, I disagree that offering a new type of build would demand a major version bump. It's a new feature and should not impact existing users whatsoever.. There is one tool I am aware of that takes CommonJS modules as input and outputs ES modules, which is exactly what you need to automate the conversion.\nhttps://github.com/rollup/rollup-plugin-commonjs\nThat Rollup plugin is intended to be used as part of a build process to import CommonJS dependencies, but I suspect you could hack it to suit your needs relatively easily.\nBy the way, I disagree that offering a new type of build would demand a major version bump. It's a new feature and should not impact existing users whatsoever.. Are you able to trick Rollup into converting the SDK to ES modules by doing...\njs\nformat: 'es'  // instead of 'iife'\n?\nThat might actually work! It would still be bundled, though, presumably. So I guess we'd have to then figure out a way to de-bundle it, in order to get source code to start work on a PR.. Are you able to trick Rollup into converting the SDK to ES modules by doing...\njs\nformat: 'es'  // instead of 'iife'\n?\nThat might actually work! It would still be bundled, though, presumably. So I guess we'd have to then figure out a way to de-bundle it, in order to get source code to start work on a PR.. ",
    "gaul": "@LiuJoyceC Some S3 implementations, e.g., S3Proxy, do not have a notion of regions.  Requiring users to configure this is confusing and diverges from how other AWS SDKs work, e.g., Java SDK, which send no region when not configured.\n. @chrisradek The quoted passage requires a region specified in the v4 signature but not the location constraint, which defaults to us-east-1 on AWS and whatever is appropriate for third party implementations.  Specifically I want the node.js SDK to match the Java SDK behavior, for example:\nD 03-01 17:28:59.247 pool-1-thread-2 org.apache.http.wire:72 |::]  >> \"Host: 127.0.0.1:59770[\\r][\\n]\"\nD 03-01 17:28:59.247 pool-1-thread-2 org.apache.http.wire:72 |::]  >> \"x-amz-content-sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855[\\r][\\n]\"\nD 03-01 17:28:59.248 pool-1-thread-2 org.apache.http.wire:72 |::]  >> \"Authorization: AWS4-HMAC-SHA256 Credential=local-identity/20160302/us-east-1/s3/aws4_request, SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date, Signature=30c3e1e03db913997d035f1f728c03e551466a7c5e2f7d6d11a965591fa18992[\\r][\\n]\"\nD 03-01 17:28:59.248 pool-1-thread-2 org.apache.http.wire:72 |::]  >> \"X-Amz-Date: 20160302T012859Z[\\r][\\n]\"\nD 03-01 17:28:59.248 pool-1-thread-2 org.apache.http.wire:72 |::]  >> \"User-Agent: aws-sdk-java/1.10.50 Linux/3.13.0-79-generic Java_HotSpot(TM)_64-Bit_Server_VM/25.74-b02/1.8.0_74[\\r][\\n]\"\nD 03-01 17:28:59.249 pool-1-thread-2 org.apache.http.wire:72 |::]  >> \"Content-Type: application/octet-stream[\\r][\\n]\"\nD 03-01 17:28:59.249 pool-1-thread-2 org.apache.http.wire:72 |::]  >> \"Content-Length: 0[\\r][\\n]\"\nD 03-01 17:28:59.249 pool-1-thread-2 org.apache.http.wire:72 |::]  >> \"Connection: Keep-Alive[\\r][\\n]\"\nD 03-01 17:28:59.249 pool-1-thread-2 org.apache.http.wire:72 |::]  >> \"[\\r][\\n]\"\nNote that no location constraint is specified, i.e., no entity body.\n. ",
    "djanowski": "Thank you!\n. ",
    "kalidossmm": "Retry logic and delay is working perfectly, We are trying to hit the DynamoDB for querying and we are seeing this retry/delay behavior during peak load and the response for the http request is taking lot of time and As per newRelic request is spending time on retrying the connection with DynamoDB. \nConfiguration on the table 50 read and 50 write. \nWe are analyzing on why this connection is not establishing during the 1st hit it self and why its going on multiple retries. \nHow to find the maximum number of connection open with DynamoDB?\nIs there any restriction on DynamoDB or Aws-sdk on max open connection?\n. We found the root cause of the issue. Thanks for your help and inputs on configuration parameters.\n. ",
    "aldrinleal": "Just in case, we wrote aws-sdk-typescript. Should work from a typings perspective\n. ",
    "sylwit": "Hi @LiuJoyceC \nHere is a screen shot of the result\n\nnode -v\nv5.5.0\n\"_id\": \"aws-sdk@2.2.41\",\nAs described, problem are on these lines\n**//'NS': 'Nova Scotia', //fail**\n**'NU': 'Nunavut', //fail**\nUsing these key are seem to be considered as type , NULL and NUMBER SET\nI added a simpler version to see the bug\n//NU: \"Nunavut\", //for testing, uncomment this line and remove the regions object\nThank you\n. Hi @LiuJoyceC \nThank you for feedback. I guess the problem comes from dynamodb-local then. My screenshot came from the dynamodb-local shell.\nNU and NS are not keywords but they are data type Null and NumberSet.\nhttp://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_AttributeValue.html\nI change my structure to fix this but maybe it would have worked in production.\n. ",
    "dump247": "Boto3 (via botocore) most definitely loads from ~/.aws/credentials. It may also support loading from ~/.aws/config, but I haven't tested that and don't have the ability to test it at the moment.\n. You could leave it in place, but it results in duplicate role assumptions. The reason being that the \"resolve\" here expects to complete synchronously and the role assumption completes asynchronously. So this method ends up returning null for assume-role profiles and the role has to be assumed again later.\n. Updated\n. Oops! That was my debugging.\n. This was a bug. The condition should have been checking sourceProfile. It has been fixed.\n. @chrisradek Is there anything you want me to change here? The credentials method doesn't work for assume role profiles (it will always return null in that case). A couple of things I can think of, if just removing the provider is unacceptable:\n1. Add a configuration option to the provider to disable assume role. It doesn't work in this case anyway. This seems bad to me, but would result in the same behavior as before my change.\n2. Separate the normal roles and the assume roles into different providers. I don't think this is a good idea since it requires parsing the creds file twice and separates the parsing code. Botocore does this, but only because the code was copy+pasted from the cli.\n3. Somehow make this method complete asynchronously. I don't know if this is feasible or not. I haven't looked into it.\nFor options 2 & 3, there is the risk that you get unnecessary assume role API requests if the user doesn't use the default credentials configuration. Those unnecessary API requests could slow down SDK initialization a bit.\n. Done\n. I made the change, but the problem here is that the standard provider chain will result in multiple role assumptions. The reason being that the constructor initiates a role assumption and then any api calls will start another when they get credentials (since the initial one has not completed). Is there any way to address that issue? Another constructor argument to disable the initialization get?\n. Done. The method still requires the creds since the source_profile has to be retrieved.\n. I don't think I really understand the issue with removing the get in the constructor. Are you concerned about someone directly instantiating SharedIniFileCredentials and not calling get? It should get called when the provider chain is resolved right? In what case is get not called before retrieving the creds?\n. How is this test different than the previous?\n. ",
    "kichooo": "I believe that I encounter the same problem. On production server, we create lots of medium (20-50MB) files on S3. After a while, some nodejs processes will stop putting new files, and even hang the express app. In the same time, other processes in the same deployment work nice. \n. ",
    "roeyazroel": "@chrisradek \ni'm using 2 others:\n\"aws-sdk\": \"^2.2.43\",\n\"config\": \"^1.19.0\",\n\"log4js\": \"^0.6.31\"\nthis is the code that calling the function \njavascript\nfunction updateCluster(asGroup, launchConfig) {\n    var M_TAG = \".updateCluster\";\n    awsHandler.getAutoScaleGroup(asGroup, function(err, data){\n        if (err) {\n            log.error(TAG, M_TAG, err);\n            process.exit(1);\n        } else {\n            data.AutoScalingGroups[0].Instances.forEach(function(instance){\n                if (instance.LaunchConfigurationName!=launchConfig) {\n                    log.info(TAG, M_TAG, instance.InstanceId, \"is not up to date\");\n                    awsHandler.updateInstanceHealth(instance.InstanceId, \"Unhealthy\", function(err, data){\n                        if (err) {\n                            log.error(TAG, M_TAG, err);\n                            process.exit(1);\n                        } else {\n                            log.info(TAG, M_TAG, instance.InstanceId, \"Set to Unhealthy\");\n                        }\n                    })\n                } else {\n                    log.info(TAG, M_TAG, instance.InstanceId, \"is up to date\");\n                }\n            })\n            process.exit(0);\n        }\n    })\n}\n. 10x!\nWorking great.\n. ",
    "jimcroft": "Was slightly older version of the SDK. Upgrading to latest release fixed the issue.\n. Was slightly older version of the SDK. Upgrading to latest release fixed the issue.\n. Hi Chris,\nThis is using node.js aws-sdk@2.3.1 and configured using a AWS.SharedIniFileCredentials profile that includes the correct region. I'm not very familiar with with node.js, I was just helping a colleague debug a code deployment issue, I don't know how to get the full headers, request, response, etc. but have some additional information below.\nI created a trivial test script to put a single object in a bucket. When configured like:\nvar credentials = new AWS.SharedIniFileCredentials({profile: 'deployer'});\nAWS.config.credentials = credentials;\nAWS.config.logger = process.stdout;\nvar s3 = new AWS.S3();\nthe logs show: \n[~/Desktop/s3test]$ node test.js\n[AWS s3 301 0.596s 0 retries] putObject({ Bucket: 'REDACTED',\n  Key: 'node.test.file',\n  Body: 'some content for the test object' })\n{ PermanentRedirect: The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.\n    at Request.extractError (/Users/jim/Desktop/s3test/node_modules/aws-sdk/lib/services/s3.js:350:35)\n    at Request.callListeners (/Users/jim/Desktop/s3test/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/Users/jim/Desktop/s3test/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/Users/jim/Desktop/s3test/node_modules/aws-sdk/lib/request.js:615:14)\n    at Request.transition (/Users/jim/Desktop/s3test/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/Users/jim/Desktop/s3test/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /Users/jim/Desktop/s3test/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request.<anonymous> (/Users/jim/Desktop/s3test/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request.<anonymous> (/Users/jim/Desktop/s3test/node_modules/aws-sdk/lib/request.js:617:12)\n    at Request.callListeners (/Users/jim/Desktop/s3test/node_modules/aws-sdk/lib/sequential_executor.js:115:18)\n  message: 'The bucket you are attempting to access must be addressed using the specified endpoint. Please send all future requests to this endpoint.',\n  code: 'PermanentRedirect',\n  region: null,\n  time: 2016-05-24T17:44:43.240Z,\n  requestId: 'BF8F5C85B73F4C2B',\n  extendedRequestId: 'n2FBw04JX0Xs0U0dqO3iTMelYfYxea1w0TrOTh0+q3Kg6vynE0GBNxhfuaEhug+9AJK6Uejf6WM=',\n  cfId: undefined,\n  statusCode: 301,\n  retryable: false,\n  retryDelay: 70.64452964808434 }\nWhen configured like:\nvar credentials = new AWS.SharedIniFileCredentials({profile: 'deployer'});\nAWS.config.credentials = credentials;\nAWS.config.logger = process.stdout;\nvar ep = new AWS.Endpoint('s3-eu-west-1.amazonaws.com');\nvar s3 = new AWS.S3({endpoint: ep});\nall's well:\n[AWS s3 200 0.328s 0 retries] putObject({ Bucket: 'REDACTED',\n  Key: 'node.test.file',\n  Body: 'some content for the test object' })\n{ ETag: '\"eba398d822af73a2b8c91b0d5a62558f\"' }\nThe only two differences in the buckets (aside from their names) is that one was created through the AWS console, the other via CloudFormation. Doing a little digging this is all I could come up with as a difference between them:\n$ aws s3api get-bucket-location --bucket REDACTED_BUCKET_1\n{\n    \"LocationConstraint\": \"EU\"\n}\n$ aws s3api get-bucket-location --bucket REDACTED_BUCKET_2\n{\n    \"LocationConstraint\": \"eu-west-1\"\n}\n$\nREDACTED_BUCKET_1 being the CloudFormation created bucket which exhibits this 301 redirect behaviour.\n. PS. The CloudFormation behaviour of setting LocationConstraint to EU is valid but I raised a support case with AWS and it's being logged as a bug.\n. Yes, that works.\n. New release fixed this issue. Thanks!\n. ",
    "bnussey": "Just to add this I am having a similar issue only when I am trying to upload from Colombia which I can speculate is due to the poorer standard of internet here. From California, have absolutely no problems. Also reported that some users from Sweden have had a similar issue.. \nHowever as OP said can resolve this issue by setting queueSize: 1 \n. ",
    "lauterry": "Nice @LiuJoyceC !\n. Hi @chrisradek \nMy bad,\nI have just figured out that my region was incorrect. I have put us-west-1 instead of eu-west-1   -_-\nSo everything is fine\nSo sorry for the noise\n. ",
    "jkervine": "@chrisradek, this is plain node, v5.7.0, aws sdk through npm \"aws-sdk\": \"^v2.2.41\"\nThe code above says:\n/2015-03-31/functions/simple/aliases?FunctionVersion=14\nversion aliases: { NextMarker: null, Aliases: [] }\nAlso here is a capture how it is in the was console:\n\n. @chrisradek, this is plain node, v5.7.0, aws sdk through npm \"aws-sdk\": \"^v2.2.41\"\nThe code above says:\n/2015-03-31/functions/simple/aliases?FunctionVersion=14\nversion aliases: { NextMarker: null, Aliases: [] }\nAlso here is a capture how it is in the was console:\n\n. @chrisradek \nFinally managed to test this through. It seems like there is something wrong with that particular version and function. Running a minimal example (here: https://raw.githubusercontent.com/jkervine/node-lambda/3a6f9eb2af78d9d2fd4a10a4e156a3d21fe6a9ac/bin/aliastest.js), whenever I give it version 14, nothing is returned. changing it to, for example 13 yields aliases associated with v.13.\nThere's also something wrong with the function itself. when I try to delete the function (using aws console), it says:\nThere was an error deleting your function: Rate Exceeded. (Service: AWSLambda; Status Code: 429; Error Code: TooManyRequestsException; Request ID: f281f873-f111-11e5-9fb2-3d39a6b57e7b)\nThis particular function is not called even once (verified that from cloudwatch metrics).\n. ... and still adding that this is not a problem anymore to me, I have not seen the issue with other functions / versions I have created anymore - so for my part the issue can be closed. Maybe you guys want to still investigate this though. \n. ",
    "jgainfort": "@chrisradek \nThat did it. Thank you so much!\n. ",
    "mathieuruellan": "Well spotted. It's working fine now.\nThank you.\n. ",
    "Ninir": "Hey @LiuJoyceC \nThank you for taking the time to answer.\nSo for now, I guess it could be done using an API Gateway endpoint with a Lambda developed in NodeJS for instance, thus making the exact same call?\nThank your for taking the lead on this from an AWS side, really appreciated.\n. ",
    "abhiofdoon": "Thanks for the reply @LiuJoyceC \nIn that case how are the for 7 determined ? Is it my the instance creation time ?\nBasically how is it decided which instance are in which page ?\n. Got it.\nI am just trying to understand the behavior. I got confused by the difference between the AWS API and AWS CLI behavior.\nFor the same case mentioned above (I have 12 instances. 2 are running and 10 are stopped. I want all the stopped instances and I want only 7 results at a time) the AWS CLI gives me 7 results and a NextToken (after appliying the filters). Whereas the API gives me 6 results and a NextToken.(after applying the filters)\n$aws ec2 describe-instances --max-items 7 --filters \"Name=instance-state-name, Values=stopped\"\n{\n   \"Reservations\":[\n      {  },\n      {  },\n      {  },\n      {  },\n      {  },\n      {  },\n      {  }\n   ],\n   \"NextToken\":\"eyJOZXh0VG9rZW4iOiBudWxsLCAiYm90b190cnVuY2F0ZV9hbW91bnQiOiA3fQ==\"\n}\nDoes the CLI do some other magic behind the scene ?\nAlso the token strings looks very different in API vs CLI\nAPI NextToken = eyJ2IjoiMSIsImMiOiJLUEFJZUxMeU5HRGVTOXVKT2pZL3RGVmFxczczVmJNdmZSK1ZjU1dmWkxub1gvM2JqWXNWRFBzNUp6TmdoQUlVOXkwRjFZSzBRUDQ0Vlo5QUhHRnN0YTUvU01RclcxMGt0emxvdWVjWXA4RWhoTloxQmlYTG0yZ1JXZ3QvZmxyUjhkT0JYRVJpMUVydGtId05odXlOTmlOY01VTXJXN21CYUpzQk9ad2J0bVE9IiwiaSI6IlpzeHk3VmF5WWxFV2RoS2YvOHFKZnc9PSIsInMiOiIxIn0=' }\nCLI NextToken= eyJOZXh0VG9rZW4iOiBudWxsLCAiYm90b190cnVuY2F0ZV9hbW91bnQiOiA3fQ==\n. ",
    "yuvalselffer": "While the delay changes between smaller and larger files, there is also an synchronous feel to it even if the files are 2-3mb.\nI have an interval set to do a console.log every second, and the output stops after the call to s3.upload, and start again only after a few seconds. The time control returns to the program seems to be the same as the first time the httpUploadProgress event is fired.\n. Is there more information I can give to help analyze this?\n. I am testing on Chrome (Windows).\nSample code:\n```\nvar start = new Date().getTime();\nfunction logElapsedTime(msg) { console.log((new Date().getTime() - start) + ' -> ' + msg); }\nvar clock = setInterval(function () { logElapsedTime('tick'); }, 500);\nvar file = imagesToProcess.files[0];\nvar fr = new FileReader();\nfr.onload = function (e) {\n  logElapsedTime('file reading complete');\n  AWS.config.update({ accessKeyId: '#', secretAccessKey: '#' });\n  AWS.config.region = 'us-east-1';\n  var s3 = new AWS.S3();\n  var params = {\n    Bucket: 'bucket',\n    Key: 'test-key',\n    Body: e.target.result,\n    ACL: \"public-read\"\n  };\n  logElapsedTime('starting uploader');\n  var uploader = s3.upload(params, {}, function (err, data) {\n    if (err) {\n      logElapsedTime('error: ' + err.message);\n    }\n    else {\n      logElapsedTime('upload done');\n    }\n    clearInterval(clock);\n  });\n  uploader.on('httpUploadProgress', function (evt) {\n    logElapsedTime('uploaded ' + evt.loaded + '/' + evt.total);\n  });\n};\nlogElapsedTime('reading file started');\nfr.readAsArrayBuffer(file);\n```\nConsole output:\n```\n0 -> reading file started\n(index):104 60 -> file reading complete\n(index):104 64 -> starting uploader\n(index):104 511 -> tick\n(index):104 1005 -> tick\n(index):104 5549 -> uploaded 98304/2688521        <----- no ticks until first progress event is fired\n(index):104 5553 -> tick\n(index):104 5637 -> uploaded 1982464/2688521\n(index):104 6001 -> tick\n(index):104 6389 -> uploaded 1998848/2688521\n(index):104 6501 -> tick\n[..snipped..]\n```\n. @chrisradek did this sample code help in any way?\n. ",
    "olivierlesnicki": "@guymguym well needed! good job on this one\n. @guymguym well needed! good job on this one\n. signatureVersion: 'v4',\n  region: 'eu-west-1',\n. The stream is created by Busboy https://github.com/mscdex/busboy and I just pass it as is to s3\n``` js\nrouter.post('/upload/:key/:uploadId/:partNumber', function(req, res, next) {\n  var busboy = new Busboy({\n    headers: req.headers\n  });\nbusboy.on('file', (fieldname, stream, filename, encoding, mimetype) => {\n      let s3req = s3.uploadPart({\n        Bucket: process.env.AWS_BUCKET,\n        Key: req.params.key,\n        UploadId: req.params.uploadId,\n        PartNumber: req.params.partNumber,\n        Body: stream,\n      });\n  s3req.send((err, data) => {\n    if (err) {\n      return next(err);\n    }\n\n    res.send(data);\n  });\n\n});\nreq.pipe(busboy);\n});\n```\nNote that if I build the incoming busboy stream in memory before handing it to s3.uploadPart() it works fine:\nThe stream is created by Busboy https://github.com/mscdex/busboy and I just pass it as is to s3 - but that's just to slow and memory intensive.\n``` js\nrouter.post('/upload/:key/:uploadId/:partNumber', function(req, res, next) {\n  var busboy = new Busboy({\n    headers: req.headers\n  });\nbusboy.on('file', (fieldname, stream, filename, encoding, mimetype) => {\n    stream.fileRead = [];\n    stream.on('data', (chunk) => {\n      stream.fileRead.push(chunk);\n    });\nstream.on('end', () => {\n  let s3req = s3.uploadPart({\n    Bucket: process.env.AWS_BUCKET,\n    Key: req.params.key,\n    UploadId: req.params.uploadId,\n    PartNumber: req.params.partNumber,\n    Body: stream.fileRead,\n  });\n\n  s3req.send((err, data) => {\n    if (err) {\n      return next(err);\n    }\n\n    res.send(data);\n  });\n});\n\n});\nreq.pipe(busboy);\n});\n```\n. Changing https://github.com/aws/aws-sdk-js/blob/master/lib/util.js#L697 to\ncomputeSha256: function computeSha256(body, done) {\n    if (util.isNode()) {\n      var Stream = util.nodeRequire('stream').Stream;\n      var fs = util.nodeRequire('fs');\n      if (body instanceof Stream) {\n        if (typeof body.path === 'string') { // assume file object\n          body = fs.createReadStream(body.path);\n        }\n      }\n    }\n(removing the error)\nAllows the stream to go forward. And the sha256 is correctly computed. However an error is then thrown because the byteLength can't be read for the stream. Which can be solved by explicitely defining the Content-Length (this one is known).\nHowever when all of this is done, everything just stale.\n. ",
    "flogball00": "wasnt issue with headers or keys,  The cors configuration was incorrect.  Thanks\n. ",
    "adolfosrs": "@chrisradek This was exactly the problem. Thanks for the reply!\n. @chrisradek This was exactly the problem. Thanks for the reply!\n. ",
    "maowug": ":+1: \n. ",
    "elidupuis": "I'm getting this exact error on Edge 25.10586.0.0, using AWS SDK v2.3.15.\n. Just tried with SDK v2.4.0 and getting same error (still on Edge 25.10586.0.0).\nMy authorization header looks like this,  20 characters : 27 characters =l0kAAAAA\nAuthorization: AWS XXXXXXXXXXXXXXXXXXXX:XXXXXXXXXXXXXXXXXXXXXXXXXXX=l0kAAAAA\n. Just tested SDK 2.4.0 on Microsoft Edge 38.14366.0.0 (the current preview version) and I'm still getting the same error with the extra characters on the Authorization header (https://github.com/aws/aws-sdk-js/issues/952#issuecomment-226871401).\n. ",
    "danthegoodman": "@LiuJoyceC Thank you!\n. @LiuJoyceC Thank you!\n. ",
    "revic1993": "@chrisradek  I got it working. Actually I was using keys in message like \"from\" and \"to\" which apparently google also uses. I just changed it to messageFrom and messageTo and now its working. Thanks for responding.\n. ",
    "alexosunas": "@chrisradek \nThe SDK version:\n\"name\": \"aws-sdk\",\n  \"description\": \"AWS SDK for JavaScript\",\n  \"version\": \"2.3.0\",\nNode version is:\nv4.4.1\n. @chrisradek sorry, it was a mistake, the info is updated to show SDK version\n. @chrisradek \nthat code is throwing this error:\njavascript\n{ [ConfigError: Missing region in config]\n  message: 'Missing region in config',\n  code: 'ConfigError',\n  time: Mon Apr 11 2016 22:26:55 GMT+0000 (UTC) }\n. @chrisradek \nIt works fine, I just added the region on the constructor for your code:\njavascript\nvar kms = new AWS.KMS({apiVersion: '2014-11-01', region: 'us-west-2'});\nThanks a lot for your help.\n. ",
    "inversion": "Also reproducible with aws-sdk 2.2.42\n. ",
    "JaKXz": "@LiuJoyceC thanks for the response. The s3bucket object I'm using is defined with a bucket of my choosing, contained the S3_BUCKET_NAME variable. I've updated the code to be a little clearer.\nAnd yes, I'm using v2.3.3 (npm@latest at time of writing). \n. @LiuJoyceC here's a simplified version [which I implied in my original snippet, sorry]: \njs\nfunction generateRedirects(redirectsFile) {\n  //based on redirectsFile, return a promise that resolves to an array.\n  return Q.when([{\n    from: 'some/old/path/',\n    to: 'a/new/path/'\n  }]);\n}\n. @LiuJoyceC do you or the SDK team have any updates? This is blocking our team and we currently have to push / fix redirects manually. Thanks!\n. @chrisradek that was it. Thanks! \n. However, I should suggest that there is some better error reporting so that I know what I'm sending and why nothing is happening - the way I had it written clearly should have returned some sort of error, but every request resolved to a promise that was fulfilled, and the value was always an anonymous function. This made it pretty hard to debug where the actual problem was.\n. @chrisradek I have an example here which illustrates what I was going for - your snippet is definitely the more concise solution, but I'm not sure I was using R.map wrong. Feel free to clone/mess with that tonic notebook :)\n. ",
    "vcernomschi": "I also have this issue for aws-sdk@2.4.1\n. This issue is not reproducible on aws-sdk@2.3.6\n. Thanks @chrisradek!\nIssue is not reproducible after updating to  v2.4.2 with signatureVersion: v4.\n. ",
    "PsyTae": "I am also seeing this issue in aws-sdk@2.4.1.\nuninstalled aws-sdk@2.4.1 and installed aws-sdk@2.3.7 and uploaded file successfully.\n. I am currently using S3@4.4.0 to upload my files to the s3.\nMy Code pertaining to the upload is below.\n```\nvar aws = require('aws-sdk'),\n    s3 = require('s3'); //S3\naws.config.loadFromPath('./AwsConfig.json');\nvar s3sdk = new aws.S3();\nvar s3Client = s3.createClient({\n    s3Client: s3sdk,\n    maxAsyncS3: 20,\n    s3RetryCount: 5,\n    s3RetryDelay: 5000,\n    multipartUploadThreshold: 20971520,\n    multipartUploadSize: 15728640\n});\nfunction putS3(filePath, bucket, prefix, sn, cb){\n    var filename = path2.basename(filePath);\n    console.log(clc.magentaBright(clc.blueBright('Upload -  Started:  ', filename, '\\n')));\n    var s3Path;\nif (bucket.indexOf('fail') === -1){\n    s3Path = prefix + '/' + sn + '/' + filename;\n} else {\n    s3Path = filename;\n}\n\nvar uploadParams = {\n    localFile: filePath,\n    s3Params: {\n        Bucket: bucket,\n        Key: s3Path,\n        Metadata: { source: META }\n    }\n};\n\nvar upload = s3Client.uploadFile(uploadParams);\nupload.on('error', function(err) {\n    console.log(clc.magentaBright(clc.blueBright('Upload - ERROR OCC: ', filename, '\\n')));\n    cb(new Error('Unable to Upload: ' + filename + ': ' + err.message), 'Unable to Upload: ' + filename + ': ' + err.message);\n});\nupload.on('end', function(data) {\n    console.log(clc.magentaBright(clc.blueBright('Upload - Finished:  ', filename, '\\n')));\n    cb(null, data);\n});\n\n}\n```\nEDIT: ---------\nupdating back to aws-sdk@2.4.1 and changing\nvar s3sdk = new aws.S3();\nto\nvar s3sdk = new aws.S3({signatureVersion: 'v2'});\nworked successfully for me to upload my file to the s3.\n. ",
    "olalonde": "I'm also getting this error with v2.6.3. Non-file stream objects are not supported with SigV4\nI setup my client like this:\nconst client = new aws.S3({\n  accessKeyId,\n  secretAccessKey,\n  endpoint: new aws.Endpoint(endpoint),\n  s3ForcePathStyle: true,\n  signatureVersion: 'v4',\n  s3DisableBodySigning: true,\n})\nputObject works, just uploadPart doesn't.\nHere's the code I'm using: https://github.com/blockai/s3-tus-store/blob/master/src/write-part-by-part.js\n. Hmm I'm using a Minio server running in docker for testing, which is not on https :/\n. @chrisradek I'm fairly sure I have an error event handler attached and still getting occasional crashes but it's pretty hard to reproduce reliably. Next time it happens I will try to see if I can pinpoint the problem.\n. I realised I wasn't properly attaching an error handler to the writable stream.\n. Thanks. I'd still like to request a feature that could make it possible for the Body returned by getObject be a readable stream. That would be more similar to how whatwg-fetch and most node.js http client libraries handle this common situation.\n. Thanks. I'd still like to request a feature that could make it possible for the Body returned by getObject be a readable stream. That would be more similar to how whatwg-fetch and most node.js http client libraries handle this common situation.\n. Yes, I was only getting this bug using the uploadPart API directly which is strange. All my streams are non file streams, typically they come from an HTTP request but I use other types of streams when testing. Well, the workaround is working... But couldn't uploadPart also automatically buffer up before issuing the request, like upload does? Here's where I'm using it btw: https://github.com/blockai/s3-tus-store/blob/master/src/write-part-by-part.js#L41\n. Yes, I was only getting this bug using the uploadPart API directly which is strange. All my streams are non file streams, typically they come from an HTTP request but I use other types of streams when testing. Well, the workaround is working... But couldn't uploadPart also automatically buffer up before issuing the request, like upload does? Here's where I'm using it btw: https://github.com/blockai/s3-tus-store/blob/master/src/write-part-by-part.js#L41\n. ",
    "mano-bharathi": "Hi, My bucket is located in our storage layer which is above aws. So when ever i try to upload by passing endpoint it is uploading the url \"bucketname.endpoint\" example (mybucket.10.86.2.44) which is throwing error as my bucket is not the subdomain for me. How to change this to custom endpoint. I want to upload the file to url like this http://10.86.2.44/mybucket/directory. Is that possible ?? . ",
    "griffinmichl": "@chrisradek Thanks so much for your response. The request library worked for me as well. Strange that fetch wouldn't. I'll do some more research into the differences between request and fetch. Feel free to close the issue. \n. ",
    "ar-naseef": "I am facing the same issue with $.ajax().\nAny solution?. ",
    "rwaldron": "This travis failure seems unrelated? Please let me know if that's not the case\n. @chrisradek oh, it's possible yes. I should update this to be sure. I will do a sweep for other fs operations. \n. I've added a rule for lib/services/*.js\u2014are there any other resources that might be necessary, but specifically require(...)'ed?\n. ",
    "njeirath": "Thanks for the reply.  I made the change you mention and seem to be still seeing the same issue.  As an additional test to make sure I'm doing things correctly, I tried the reverse case as well and got similar results:\nThe original test as in the above post but with fixed configs:\n```\nvar goodConfig = {credentials: {accessKeyId: 'GOOD_KEY', secretAccessKey: 'GOOD_SECRET'}, region: 'us-east-1'}\nvar badConfig = {credentials: {accessKeyId: 'BAD_KEY', secretAccessKey: 'BAD_SECRET'}, region: 'us-east-1'}\nvar ec2 = new AWS.EC2(badConfig)\nec2.describeKeyPairs({}, function(err, data){console.log(err, data)})\n//This call fails as we'd expect it to with a 401 response\nvar ec2 = new AWS.EC2(goodConfig)\nec2.describeKeyPairs({}, function(err, data){console.log(err, data)})\n//This call still fails even though the credentials are valid\n```\nThe reverse test:\n```\nvar goodConfig = {credentials: {accessKeyId: 'GOOD_KEY', secretAccessKey: 'GOOD_SECRET'}, region: 'us-east-1'}\nvar badConfig = {credentials: {accessKeyId: 'BAD_KEY', secretAccessKey: 'BAD_SECRET'}, region: 'us-east-1'}\nvar ec2 = new AWS.EC2(goodConfig)\nec2.describeKeyPairs({}, function(err, data){console.log(err, data)})\n//This call works as we'd expect it to\nvar ec2 = new AWS.EC2(badConfig)\nec2.describeKeyPairs({}, function(err, data){console.log(err, data)})\n//This call still works even though it shouldn't\n```\nI did additionally try the above test cases with the suggestion from #516 which also didn't seem to make a difference:\nObject.defineProperty(AWS, 'config', { get: function() { return {} } });\n. @chrisradek \nWell interestingly enough if I try this in isolation with the test page below, it seems to work as expected.  I must have something in my specific application that is causing the behavior.  If I can reproduce the error that I'm seeing I'll post again.\nTest script that works as expected:\n```\n\n\n\n\n\n\nvar goodConfig = {credentials: {accessKeyId: 'GOOD_KEY', secretAccessKey: 'GOOD_SECRET'}, region: 'us-east-1'}\nvar badConfig = {credentials: {accessKeyId: 'BAD_KEY', secretAccessKey: 'BAD_SECRET'}, region: 'us-east-1'}\n\nvar ec2 = new AWS.EC2(badConfig)\nec2.describeKeyPairs({}, function(err, data){\n  document.write(\"Error: \" + String(err) + \"<br>\")\n  document.write(\"Data: \" + String(data) + \"<br>\")\n})\n\ndocument.write(\"<hr>\")\n\nvar ec2 = new AWS.EC2(goodConfig)\nec2.describeKeyPairs({}, function(err, data){\n  document.write(\"Error: \" + String(err) + \"<br>\")\n  document.write(\"Data: \" + String(data) + \"<br>\")\n})\n\n\n\n```\n. ",
    "anthonygreen": "I'm seeing the same issue. Example\nAWS.config.loadFromPath('./config.json');\nAWS.config.update({ \"secretAccessKey\" \u2026\ndoes not work, the config doesn't get updated with the options object \n. > When calling AWS.config.update, you need to pass those fields as properties on the credentials object\nThat doesn't appear to be the case.\nThis code does work\n```\nvar AWS = require('aws-sdk'); \nvar awsSecretAccessKey = process.env.AWS_SECRET_ACCESS_KEY;\nAWS.config.update({ \"accessKeyId\": \"\u2026\", \"region\": \"eu-west-1\", \"secretAccessKey\": awsSecretAccessKey })\n```\n. Here's what I found\nAWS.config.loadFromPath('./config.json');\nwon't set any configuration unless config had secretAccessKey set to a non-empty string\n{ \n    \"accessKeyId\": \"\u2026\", \n    \"region\": \"eu-west-1\",\n   \"secretAccessKey\": \".\"\n}\nThen this:\nAWS.config.loadFromPath('./config.json');\nAWS.config.credentials.secretAccessKey = awsSecretAccessKey;\nwill work\n. ",
    "felixnext": "I ran a policy simulation, so it is not a policy problem. However, if I revoke the policy I also do not get any error message in the callback.\n. I also just tried to set the time limit to 300 secs. Still the same error with no response at all from the headObject call.\n. @chrisradek \nJust tried the sample and it worked. Seems that the error with my lambda is the VPC. If I don't attach the VPC the lambda code works. However the VPC is needed to access RDS. Do you know if there is a way to get access to Buckets in the VPC? I thought the access was restricted only through the role (and policy) attached to the lambda...\n. Alright. Thanks!\n. For folks with similar issues: https://aws.amazon.com/de/blogs/aws/new-vpc-endpoint-for-amazon-s3/\nThis seems to do the trick.\n. ",
    "dainbrump": "No. Does it need to?\n. I define creds like in my original post. Then later I refer to creds.accessKeyId and creds.secretAccessKey. I've dumped the creds object to console and neither key is defined. Do I need to do creds.refresh() before I can use them?\n. Ok. Now I'm getting the accessKeyId but no secretAccessKey. In fact, secretAccessKey is not even present in the creds object after running creds.get() or creds.refresh().\n. Without doing creds.get():\n{ expired: false,\n  expireTime: null,\n  accessKeyId: undefined,\n  sessionToken: undefined,\n  filename: undefined,\n  profile: 'myprofile',\n  disableAssumeRole: false }\nAfter doing creds.get():\n{ expired: false,\n  expireTime: null,\n  accessKeyId: '{MY_ACCESS_KEY}',\n  sessionToken: undefined,\n  filename: '/home/dainbrump/.aws/credentials',\n  profile: 'myprofile',\n  disableAssumeRole: false }\nI was creating creds like the first post. Then right after:\nvar redshift = new AWS.Redshift({\n  accessKeyId: creds.accessKeyId,\n  secretAccessKey: creds.secretAccessKey\n});\n. Ah. The get() seemed to do the trick. I was noticing something else. I think we can close this. Would be handy to update the docs mention the need for the get() before doing things.\n. @chrisradek I'd say adding the get back to the constructor may be necessary. There are a few libraries I've come across that use the SharedIniFileCredentials. None of which I've seen use the .get() after instantiation. Could be a breaking change maybe?\n. ",
    "ruthienachmany": "Hey Joyce,\nThanks so much for your help.\nAfter replacing the minified CDN with the non-minified version, here is my error stack trace for cognitoidentity.getId();\nHere is the error I got on Chrome:\nError: Cannot read property 'length' of undefined(\u2026) \"TypeError: Cannot read property 'length' of undefined\n    at Request.extractError (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3057:24)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:20)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:10)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:14)\n    at Request.transition (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:10)\n    at AcceptorStateMachine.runTo (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:12)\n    at https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:10\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9)\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:12)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:18)\"\nHere is the error trace I got on Safari:\nTypeError: undefined is not an object (evaluating 'httpResponse.body.length') \u2014 aws-sdk-2.3.5.js:52910\n\"extractError@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3057:24\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:25\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:23\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:18\ntransition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:14\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:16\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:15\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:13\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:16\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:22\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:23\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:18\ntransition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:14\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:16\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:15\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:13\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:16\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:22\ncallNextListener@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4733:25\nonEnd@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:1665:15\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:9339:21\nfinishRequest@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2140:17\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2065:27\nhttps://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:8021\nl@https://MYACCOUNTNAMEokta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10274\nT@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10431\nonreadystatechange@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10679\"\nHere is the one from Firefox:\nTypeError: httpResponse.body is undefined\nStack trace:\nextractError@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3057:1\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:11\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:5\n[36]</</fsm.setupStates/transition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:5\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:3\nrunTo/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:5\n[36]</</fsm.setupStates/transition/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9\nemit/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:7\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:5\n[36]</</fsm.setupStates/transition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:5\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:3\nrunTo/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:5\n[36]</</fsm.setupStates/transition/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9\nemit/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:7\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:5\ncallNextListener@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4733:7\nonEnd@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:1665:11\n[72]</EventEmitter.prototype.emit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:9339:9\nfinishRequest@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2140:5\nhandleRequest/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2065:9\no/n[s]@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:8014\nt[g]/l@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10270\nt[g]/T@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10430\nt[g]/I.onreadystatechange@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10678\n extractError@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3057:1\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:11\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:5\n[36]</</fsm.setupStates/transition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:5\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:3\nrunTo/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:5\n[36]</</fsm.setupStates/transition/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9\nemit/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:7\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:5\n[36]</</fsm.setupStates/transition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:5\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:3\nrunTo/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:5\n[36]</</fsm.setupStates/transition/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9\nemit/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:7\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:5\ncallNextListener@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4733:7\nonEnd@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:1665:11\n[72]</EventEmitter.prototype.emit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:9339:9\nfinishRequest@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2140:5\nhandleRequest/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2065:9\no/n[s]@https://MYACCOUNTNAMEokta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:8014\nt[g]/l@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10270\nt[g]/T@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10430\nt[g]/I.onreadystatechange@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10678\nThis is the error I get on Chrome on cognitoidentity.getOpenIdToken():\nerrTypeError: Cannot read property 'length' of undefinedTypeError: Cannot read property 'length' of undefined\n    at Request.extractError (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3057:24)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:20)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:10)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:14)\n    at Request.transition (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:10)\n    at AcceptorStateMachine.runTo (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:12)\n    at https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:10\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9)\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:12)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:18)\nThis is the error I get on Chrome for sts.assumeRoleWithWebIdentity() in Chrome:\nError: Cannot read property 'toString' of undefined(\u2026) \"TypeError: Cannot read property 'toString' of undefined\n    at Request.extractError (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3117:42)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:20)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:10)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:14)\n    at Request.transition (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:10)\n    at AcceptorStateMachine.runTo (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:12)\n    at https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:10\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9)\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:12)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:18)\"\nAll of these calls return a successful response in the network logs.\nHope that's helpful. Thanks! Please let me know if there's any other information I can provide.\n-Ruthie\n. Hey Joyce,\nThanks so much for your help.\nAfter replacing the minified CDN with the non-minified version, here is my error stack trace for cognitoidentity.getId();\nHere is the error I got on Chrome:\nError: Cannot read property 'length' of undefined(\u2026) \"TypeError: Cannot read property 'length' of undefined\n    at Request.extractError (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3057:24)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:20)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:10)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:14)\n    at Request.transition (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:10)\n    at AcceptorStateMachine.runTo (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:12)\n    at https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:10\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9)\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:12)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:18)\"\nHere is the error trace I got on Safari:\nTypeError: undefined is not an object (evaluating 'httpResponse.body.length') \u2014 aws-sdk-2.3.5.js:52910\n\"extractError@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3057:24\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:25\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:23\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:18\ntransition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:14\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:16\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:15\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:13\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:16\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:22\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:23\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:18\ntransition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:14\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:16\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:15\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:13\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:16\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:22\ncallNextListener@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4733:25\nonEnd@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:1665:15\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:9339:21\nfinishRequest@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2140:17\nhttps://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2065:27\nhttps://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:8021\nl@https://MYACCOUNTNAMEokta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10274\nT@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10431\nonreadystatechange@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10679\"\nHere is the one from Firefox:\nTypeError: httpResponse.body is undefined\nStack trace:\nextractError@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3057:1\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:11\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:5\n[36]</</fsm.setupStates/transition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:5\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:3\nrunTo/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:5\n[36]</</fsm.setupStates/transition/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9\nemit/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:7\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:5\n[36]</</fsm.setupStates/transition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:5\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:3\nrunTo/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:5\n[36]</</fsm.setupStates/transition/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9\nemit/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:7\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:5\ncallNextListener@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4733:7\nonEnd@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:1665:11\n[72]</EventEmitter.prototype.emit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:9339:9\nfinishRequest@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2140:5\nhandleRequest/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2065:9\no/n[s]@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:8014\nt[g]/l@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10270\nt[g]/T@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10430\nt[g]/I.onreadystatechange@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10678\n extractError@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3057:1\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:11\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:5\n[36]</</fsm.setupStates/transition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:5\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:3\nrunTo/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:5\n[36]</</fsm.setupStates/transition/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9\nemit/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:7\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:5\nemit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:5\n[36]</</fsm.setupStates/transition@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:5\nrunTo@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:3\nrunTo/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:5\n[36]</</fsm.setupStates/transition/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9\nemit/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:7\ncallListeners@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:5\ncallNextListener@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4733:7\nonEnd@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:1665:11\n[72]</EventEmitter.prototype.emit@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:9339:9\nfinishRequest@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2140:5\nhandleRequest/<@https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:2065:9\no/n[s]@https://MYACCOUNTNAMEokta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:8014\nt[g]/l@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10270\nt[g]/T@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10430\nt[g]/I.onreadystatechange@https://MYACCOUNTNAME.okta.com/js/sdk/okta-sign-in-1.1.0.min.js:230:10678\nThis is the error I get on Chrome on cognitoidentity.getOpenIdToken():\nerrTypeError: Cannot read property 'length' of undefinedTypeError: Cannot read property 'length' of undefined\n    at Request.extractError (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3057:24)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:20)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:10)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:14)\n    at Request.transition (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:10)\n    at AcceptorStateMachine.runTo (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:12)\n    at https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:10\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9)\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:12)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:18)\nThis is the error I get on Chrome for sts.assumeRoleWithWebIdentity() in Chrome:\nError: Cannot read property 'toString' of undefined(\u2026) \"TypeError: Cannot read property 'toString' of undefined\n    at Request.extractError (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3117:42)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4743:20)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4717:10)\n    at Request.emit (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3923:14)\n    at Request.transition (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3705:10)\n    at AcceptorStateMachine.runTo (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6563:12)\n    at https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:6575:10\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3721:9)\n    at Request. (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:3925:12)\n    at Request.callListeners (https://sdk.amazonaws.com/js/aws-sdk-2.3.5.js:4753:18)\"\nAll of these calls return a successful response in the network logs.\nHope that's helpful. Thanks! Please let me know if there's any other information I can provide.\n-Ruthie\n. Hey,\nI pulled the unminified AWS SDK code and putting a debugger in the extractError function - here is what I found:\n1. The error is occurring because there an httpResponse with no body is not handled. Here is my httpResponse: \n   HttpResponse {statusCode: undefined, headers: Object, body: undefined, streaming: false, stream: EventEmitter}\n2. I also got this response with a more detailed stacktrace:\n   Error: An unknown error occurred. at Request.VALIDATE_RESPONSE (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:1631:37) at Request.callListeners (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:4745:20) at Request.emit (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:4719:10) at Request.emit (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:3925:14) at Request.transition (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:3707:10) at AcceptorStateMachine.runTo (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:6565:12) at https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:6577:10 at Request. (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:3723:9) at Request. (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:3927:12) at Request.callListeners (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:4755:18)\nThe only thing I can possibly think of here that might be going wrong is that the JWT which I am posting to Cognito does not use SSL server keys to sign their cert, they generate the keys this way: JWKS_URI=curl -s https://example.okta.com/.well-known/openid-configuration | egrep -o 'jwks_uri\":\"[^\"]*' | cut -d '\"' -f 3;\ncurl -s $JWKS_URI | egrep -o '\"x5c\":\\[\"[^]]*' | cut -d '\"' -f 4 | tr -d '\\' | base64 -D | openssl x509 -inform DER -pubkey -noout . The developer evangelist on that team (which is Okta - specific details on their JWT setup here: https://github.com/jpf/okta-oidc-beta) suggested that that might be the issue, since the generate thumbprint for an OIDC provider instructions here (http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc_verify-thumbprint.html) indicate that AWS generates the cert through openssl s_client -showcerts -connect keys.example.com:443. However, when I tried using the thumbprint generated via the former method, I got back this error in the browser console - \"Invalid login token. OpenIDConnect provider's HTTPS certificate doesn't match configured thumbprint\". When I tried using the thumbprint generated automatically from AWS and also generated when I ran the openssl command above, I did not receive that error, but I do receive the validation error described above. At the same time, this may not be the issue because as I described in a previous comment, when I preserve network logs, I see a successful response with a Cognito identity id which I can then use to post to CognitoIdentity.getopenidtoken and get back a successful response in the network logs (no browser console response) with an openid token I can post to STS and get back temporary credentials (also in the network logs) which I can then use to successfully access my AWS resources via API calls.\nHope that's helpful! Basically, given the relatively non-descriptive error messages, it's a little hard to tell whether there's some simple mistake in my setup or whether this is a bigger problem with OIDC setup and if there's any way I can get around it.\nThank you so much for your help!\nBest,\nRuthie\n. Hey,\nI pulled the unminified AWS SDK code and putting a debugger in the extractError function - here is what I found:\n1. The error is occurring because there an httpResponse with no body is not handled. Here is my httpResponse: \n   HttpResponse {statusCode: undefined, headers: Object, body: undefined, streaming: false, stream: EventEmitter}\n2. I also got this response with a more detailed stacktrace:\n   Error: An unknown error occurred. at Request.VALIDATE_RESPONSE (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:1631:37) at Request.callListeners (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:4745:20) at Request.emit (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:4719:10) at Request.emit (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:3925:14) at Request.transition (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:3707:10) at AcceptorStateMachine.runTo (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:6565:12) at https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:6577:10 at Request. (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:3723:9) at Request. (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:3927:12) at Request.callListeners (https://s3.amazonaws.com/MYBUCKET/js/aws_sdk.js:4755:18)\nThe only thing I can possibly think of here that might be going wrong is that the JWT which I am posting to Cognito does not use SSL server keys to sign their cert, they generate the keys this way: JWKS_URI=curl -s https://example.okta.com/.well-known/openid-configuration | egrep -o 'jwks_uri\":\"[^\"]*' | cut -d '\"' -f 3;\ncurl -s $JWKS_URI | egrep -o '\"x5c\":\\[\"[^]]*' | cut -d '\"' -f 4 | tr -d '\\' | base64 -D | openssl x509 -inform DER -pubkey -noout . The developer evangelist on that team (which is Okta - specific details on their JWT setup here: https://github.com/jpf/okta-oidc-beta) suggested that that might be the issue, since the generate thumbprint for an OIDC provider instructions here (http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc_verify-thumbprint.html) indicate that AWS generates the cert through openssl s_client -showcerts -connect keys.example.com:443. However, when I tried using the thumbprint generated via the former method, I got back this error in the browser console - \"Invalid login token. OpenIDConnect provider's HTTPS certificate doesn't match configured thumbprint\". When I tried using the thumbprint generated automatically from AWS and also generated when I ran the openssl command above, I did not receive that error, but I do receive the validation error described above. At the same time, this may not be the issue because as I described in a previous comment, when I preserve network logs, I see a successful response with a Cognito identity id which I can then use to post to CognitoIdentity.getopenidtoken and get back a successful response in the network logs (no browser console response) with an openid token I can post to STS and get back temporary credentials (also in the network logs) which I can then use to successfully access my AWS resources via API calls.\nHope that's helpful! Basically, given the relatively non-descriptive error messages, it's a little hard to tell whether there's some simple mistake in my setup or whether this is a bigger problem with OIDC setup and if there's any way I can get around it.\nThank you so much for your help!\nBest,\nRuthie\n. Hey @LiuJoyceC, was your response cut off here \"If the httpResponse.body is undefined by the time you get to the VALIDATE_RESPONSE\"? I'm looking into whether it's a buffer now. \n. Hey @LiuJoyceC,\nThis is the value of resp in validate_response:\nfor cognitoidentity.getid\n\nfor cognitoidentity.getopenidtoken\n\nThe function HTTP_HEADERS is not getting run.\nThanks,\nRuthie\n. @LiuJoyceC The reason I think it may have to do with the JWT is that the thumbprint that I believe Cognito assumes to be the thumbprint to sign the OIDC (and there is no way in the UI to change the thumbprint to not be an SSL server key thumbprint) is different than the thumbprint that Okta uses to sign their OIDC (not SSL server keys). I know that's not being messaged in the error, but it seems like a bug on AWS's end either way that when using the thumbprint AWS generates via the SSL keys (but NOT the one that Okta actually uses), I am able to get back the Cognito id, Cognito Open ID token, and the STS temporary web identity credentials in the network logs, even while getting back these errors in the web inspector console. It may be an unrelated bug, but just because that's the only unusual part of my setup, I believe that may be related.\n. ",
    "flackend": "The latest version of aws-sdk supports SES in the browser.\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\n. The latest version of aws-sdk supports SES in the browser.\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/guide/browser-services.html\n. ",
    "srkimir": "@LiuJoyceC Problem happens occasionally, code example that i have posted sometimes yields CRC32CheckFailed and error and sometimes not. I guess only way to replicate this error is to force same call over and over.\nIm using Node v4.4.3 and AWS-SDK API Version 2012-08-10\n\npassing back the CRC32CheckFailed error into my callback, with an empty object {} passed into the data parameter.\n\nIf err exists, no matter how you force it, you should never have data defined at all in that case\n. @LiuJoyceC Problem happens occasionally, code example that i have posted sometimes yields CRC32CheckFailed and error and sometimes not. I guess only way to replicate this error is to force same call over and over.\nIm using Node v4.4.3 and AWS-SDK API Version 2012-08-10\n\npassing back the CRC32CheckFailed error into my callback, with an empty object {} passed into the data parameter.\n\nIf err exists, no matter how you force it, you should never have data defined at all in that case\n. @chrisradek thank you on you response\nAWS.version returns 2.2.48\nI didn't check that request was repeated, i will try to force error again and to check it.\nIm a little bit confused about statusCode: 200 that i received back.\nFrom http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ErrorHandling.html i would expect that few errors from 4xx and all from 5xx will be repeated. To me it seems that AWS-SDK consider CRC32CheckFailed as successful request so it will not repeat it, right? Having retryable: true and statusCode: 200 it seems the two are contradictory\n. @chrisradek thank you on you response\nAWS.version returns 2.2.48\nI didn't check that request was repeated, i will try to force error again and to check it.\nIm a little bit confused about statusCode: 200 that i received back.\nFrom http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ErrorHandling.html i would expect that few errors from 4xx and all from 5xx will be repeated. To me it seems that AWS-SDK consider CRC32CheckFailed as successful request so it will not repeat it, right? Having retryable: true and statusCode: 200 it seems the two are contradictory\n. @chrisradek thanks on very good explanation\n\nI agree with you that data should be null when an error exists and we need to investigate why for this service, that's not the case.\n\nOnly thing that left unclear and it should be fixed\n. @chrisradek thanks on very good explanation\n\nI agree with you that data should be null when an error exists and we need to investigate why for this service, that's not the case.\n\nOnly thing that left unclear and it should be fixed\n. ",
    "schwar": "There are a few reasons DynamoDB may deny access for a fine-grained access control policy.\nHere are a few things to check:\n1) Are all attributes that are keys (either on the table or any indexes) included in the policy? If a user is denied access to a hash key or range key, DynamoDB cannot evaluate the request.\n2) Are you specifying the ProjectionExpression parameter in the request? Here is a link to the GetItem#ProjectionExpression documentation and here is general documentation for ProjectionExpression. If you don't specify this parameter, DynamoDB attempts to retrieve the entire item, which would be denied because the user only has permission to specific attributes.\n. ",
    "Madhesk": "Hi @LiuJoyceC\nThanks for replying me. I will check those links and update it.\n. ",
    "winsome": "Created #991 with the proposed change\n. Created #991 with the proposed change\n. @chrisradek Glad to help! I'll keep a look out for the next release\n. @chrisradek Glad to help! I'll keep a look out for the next release\n. ",
    "jjmartin": "ok well thats good to know so i don't need it in both at least\n. ~~after resetting my system just to use the credentials file - i don't think its the case that the CLI reads the source_profile from the credentials file (just FYI)~~\nnevermind - i had forgotten to set my profile env var\n. ",
    "andreineculau": "Is there any update on this?\nWe've just burned ourselves because we were authenticated in the shell as \"some assumed role on another AWS account\", and then ran some nodejs scripts, only to find out that the communication was against the primary AWS account.\n. Is there any update on this?\nWe've just burned ourselves because we were authenticated in the shell as \"some assumed role on another AWS account\", and then ran some nodejs scripts, only to find out that the communication was against the primary AWS account.\n. @chrisradek yes\nI have duplicate source_profile and role_arn in ~/.aws/credentials and ~/.aws/config. But I cannot see how that can work (the assume role requires MFA).\n. @chrisradek yes\nI have duplicate source_profile and role_arn in ~/.aws/credentials and ~/.aws/config. But I cannot see how that can work (the assume role requires MFA).\n. FWIW, this is our current compat-code i.e. if you are logged in via aws-cli assume role, then you're also logged in in aws-sdk-js (same credentials):\n``` es6\nimport aws from 'aws-sdk';\nimport ini from 'ini';\n// compatibility with aws-cli\nlet awsProfile = process.env.AWS_PROFILE || process.env.AWS_DEFAULT_PROFILE;\nif (awsProfile) {\n  try {\n    let configIni = ini.parse(fs.readFileSync(\n      ${process.env.HOME}/.aws/config,\n      'utf-8'\n    ));\n    let awsProfileConfig = configIni[profile ${awsProfile}];\n    if (awsProfileConfig && awsProfileConfig.role_arn) {\n      let roleArn = awsProfileConfig.role_arn.replace(/:/g, '').replace(/[^A-Za-z0-9-]/g, '-');\n      let awsCliCacheFilename = ${awsProfile}--${roleArn};\n      let awsCliCache =\n          JSON.parse(fs.readFileSync(\n            ${process.env.HOME}/.aws/cli/cache/${awsCliCacheFilename}.json,\n            'utf-8'\n          ));\n      let sts = new aws.STS();\n      aws.config.credentials = sts.credentialsFrom(awsCliCache);\n    }\n  } catch (_err) {\n  }\n}\n```\nUgly? yes!\nBut similarly ugly is the fact that AWS cannot get two clients to work together.\n. FWIW, this is our current compat-code i.e. if you are logged in via aws-cli assume role, then you're also logged in in aws-sdk-js (same credentials):\n``` es6\nimport aws from 'aws-sdk';\nimport ini from 'ini';\n// compatibility with aws-cli\nlet awsProfile = process.env.AWS_PROFILE || process.env.AWS_DEFAULT_PROFILE;\nif (awsProfile) {\n  try {\n    let configIni = ini.parse(fs.readFileSync(\n      ${process.env.HOME}/.aws/config,\n      'utf-8'\n    ));\n    let awsProfileConfig = configIni[profile ${awsProfile}];\n    if (awsProfileConfig && awsProfileConfig.role_arn) {\n      let roleArn = awsProfileConfig.role_arn.replace(/:/g, '').replace(/[^A-Za-z0-9-]/g, '-');\n      let awsCliCacheFilename = ${awsProfile}--${roleArn};\n      let awsCliCache =\n          JSON.parse(fs.readFileSync(\n            ${process.env.HOME}/.aws/cli/cache/${awsCliCacheFilename}.json,\n            'utf-8'\n          ));\n      let sts = new aws.STS();\n      aws.config.credentials = sts.credentialsFrom(awsCliCache);\n    }\n  } catch (_err) {\n  }\n}\n```\nUgly? yes!\nBut similarly ugly is the fact that AWS cannot get two clients to work together.\n. FWIW my snippet does NOT work with 1.14.10 (known working version is 1.11.190 - that's the version bump in homebrew which I currently use) because aws-cli decided to camouflage the filenames storing the temporary credentials in ~/.aws/cli/cache.\nTail culprit https://github.com/boto/botocore/pull/1322 , followed by a bunch of previous commits.\nAn updated version of my hack is available at https://gist.github.com/andreineculau/a186c2181a3099a422abc293c8e79fef. FWIW my snippet does NOT work with 1.14.10 (known working version is 1.11.190 - that's the version bump in homebrew which I currently use) because aws-cli decided to camouflage the filenames storing the temporary credentials in ~/.aws/cli/cache.\nTail culprit https://github.com/boto/botocore/pull/1322 , followed by a bunch of previous commits.\nAn updated version of my hack is available at https://gist.github.com/andreineculau/a186c2181a3099a422abc293c8e79fef. while i subscribe to the points above, I am also interested in the zipped and unzipped size of aws-sdk-js\n2.138.0 for instance looks like this - untouched, without the dist folder, without the dist and the apis folder\n```\n16M aws-sdk\n7.1M    aws-sdk-no-dist\n2.0M    aws-sdk-no-dist-apis\n2.6M    aws-sdk.zip\n1.1M    aws-sdk-no-dist.zip\n480K    aws-sdk-no-dist-apis.zip\n```\nFrom a node (lambda) perspective, dist folder is not needed, and apis folder - I see only one require pointing to ../apis/metadata.json but I guess it is still very much needed.\nAnyways, my point is that it would be nice to at least document (is it already? sorry but I cannot find anything) what is really needed for the library to run on node, and node alone. This way, at least we can clean up aws-sdk-js before generating the lambda zip. Thanks!\nPS: maybe scripts and dist-tools should be added to .npmignore ?. while i subscribe to the points above, I am also interested in the zipped and unzipped size of aws-sdk-js\n2.138.0 for instance looks like this - untouched, without the dist folder, without the dist and the apis folder\n```\n16M aws-sdk\n7.1M    aws-sdk-no-dist\n2.0M    aws-sdk-no-dist-apis\n2.6M    aws-sdk.zip\n1.1M    aws-sdk-no-dist.zip\n480K    aws-sdk-no-dist-apis.zip\n```\nFrom a node (lambda) perspective, dist folder is not needed, and apis folder - I see only one require pointing to ../apis/metadata.json but I guess it is still very much needed.\nAnyways, my point is that it would be nice to at least document (is it already? sorry but I cannot find anything) what is really needed for the library to run on node, and node alone. This way, at least we can clean up aws-sdk-js before generating the lambda zip. Thanks!\nPS: maybe scripts and dist-tools should be added to .npmignore ?. FWIW my snippet does NOT work with 1.14.10 (known working version is 1.11.190 - that's the version bump in homebrew which I currently use) because aws-cli decided to camouflage the filenames storing the temporary credentials in ~/.aws/cli/cache.\nTail culprit https://github.com/boto/botocore/pull/1322 , followed by a bunch of previous commits.\nAn updated version of my hack is available at https://gist.github.com/andreineculau/a186c2181a3099a422abc293c8e79fef. FWIW my snippet does NOT work with 1.14.10 (known working version is 1.11.190 - that's the version bump in homebrew which I currently use) because aws-cli decided to camouflage the filenames storing the temporary credentials in ~/.aws/cli/cache.\nTail culprit https://github.com/boto/botocore/pull/1322 , followed by a bunch of previous commits.\nAn updated version of my hack is available at https://gist.github.com/andreineculau/a186c2181a3099a422abc293c8e79fef. related to this topic - there's no reason to even install (depend on) crypto-browserify, if the intention is to use ~it~ aws-sdk-js within nodejs. Things like this should be left to guides and ultimately to the user of aws-sdk-js to decide how to provide a crypto-module in a browser environment/distro.. related to this topic - there's no reason to even install (depend on) crypto-browserify, if the intention is to use ~it~ aws-sdk-js within nodejs. Things like this should be left to guides and ultimately to the user of aws-sdk-js to decide how to provide a crypto-module in a browser environment/distro.. ",
    "erikerikson": "Also, the SharedIniFilesCredentials docs.  Particularly the \"Using the shared credentials file\" and the callback details under \"(void) refresh(callback)\" sections.\n[Edit: context for \"Also\" being \"with regard to the documentation label\"]. Also, the SharedIniFilesCredentials docs.  Particularly the \"Using the shared credentials file\" and the callback details under \"(void) refresh(callback)\" sections.\n[Edit: context for \"Also\" being \"with regard to the documentation label\"]. ",
    "EmmanuelTsouris": "@chrisradek that would be awesome thanks!\n. @chrisradek I started playing around with it. I created a new Angular2 app using Angular-CLI. Then I imported your branch (support/typescript) in my package.json\n\"aws-sdk\": \"git:github.com/aws/aws-sdk-js#support/typescript\"\nIn one of my *.component.ts files, I imported the sdk and the typings were picked up.\nimport * as AWS from 'aws-sdk';\nSo far adding typed objects seems to be working ... :)\n. ",
    "Nysosis": "Hi,\nI'm trying to use the typing in the way as described by @EmmanuelTsouris, and it's not quite working. When I try to compile a simple project using it, I'm getting a compilation error on line 36:\nsetPromisesDependency(dep: ()|void): void;\nwith the following error:\nnode_modules/aws-sdk/typings/AWS.d.ts(36,32): error TS1005: '=>' expected.\nI know it could be a issue with typescript on my machine (as opposed to an issue with the file), but I'm running the most recent version of typescript (as of now @ 1.8.10), so I don't think it's that? I've tried the various target options in tsconfig.json, and none of them seem to support that particular union operator syntax. Though they happily support type x = number | \"aa\" | \"bb\"; which makes me thinks it's not an issue with my typescript.\n. ",
    "crorodriguezro": "@EmmanuelTsouris \nHi, I'm was looking for the aws-sdk's typings and I found https://github.com/ingenieux/aws-sdk-typescript. maybe he could help. \n. @JanOschii No, I can compile, but I couln't instantiate AWS. I tried many ways.\nAWS = window.AWS\n. ",
    "JanOschii": "@orioniota Did you get the https://github.com/ingenieux/aws-sdk-typescript repository somehow to run with angular 2?\nI came to the point, where I can import the sdk with\nimport {CognitoIdentity} from \"aws-sdk\";\nwithout an error.\nBut at the point where I like to instantiate the CognitoIdentity with\nlet cgi = new CognitoIdentity()\nthe browser stops while it is loading the page, with a bunch of errors.\n@chrisradek Would be nice to have a official solution for angular 2, cause angular depends on a working aws-sdk to run with aws. Is there a roadmap?\n. ",
    "jpnarkinsky": "@chrisradek So, I attempted to use the branch, and ran into some compile issues (possibly related to the version of TSC?)  Specifically:\npnarkinsky-laptop:pureflix-object-cache pnarkinsky$ tsc\nnode_modules/aws-sdk/typings/AWS.d.ts(36,32): error TS1005: '=>' expected.\npnarkinsky-laptop:pureflix-object-cache pnarkinsky$ tsc\nnode_modules/aws-sdk/typings/AWS.d.ts(2,24): error TS1147: Import declarations in a namespace cannot reference a module.\nnode_modules/aws-sdk/typings/AWS.d.ts(3,25): error TS1147: Import declarations in a namespace cannot reference a module.\nnode_modules/aws-sdk/typings/AWS.d.ts(4,26): error TS1147: Import declarations in a namespace cannot reference a module.\nsrc/mpx-cache.ts(224,13): error TS2345: Argument of type '{ accessKeyId: string; secretAccessKey: string; }' is not assignable to parameter of type 'ConfigurationOptions'.\n  Object literal may only specify known properties, and 'accessKeyId' does not exist in type 'ConfigurationOptions'.\nI was able to resolve them pretty handily (by modifying the definitions) but wanted to make you aware that it looks like some changes are needed for this to work, at least in my configuration.  It would be nice if this could be gotten to the point where it was more \"plug and play\".\n(Disclaimer: I'm new to typescript, so apologies if I'm missing something stupid.)\n. ",
    "Prashant-Kan": "Hello,\nI am facing issues using aws-sdk.d.ts file which I have downloaded from github. \nimport * as AWS from 'aws-sdk';\n```\nexport function typeScriptViewModel(first, last) {\n    var self = this;\n    var DataBucket = 'emaildataports';\n    var userDatBucket = 'user-data';\nvar testBucket = 'emailtests';\nvar userTestBucket = 'user--tests';\n\n\nAWS.Config({\n    accessKeyId: 'myaccessKeyID',\n    secretAccessKey: 'mysecretAccessKey'\n});\n\n....\n...\n...\n}\n// many more usage of AWS. but got stuck on the first implementation itself.\n   //var bucket = new AWS.S3({\n   //         params: {\n   //             Bucket: bucketName, Prefix: path, Delimiter: '/'\n   //         }\n   //     });\n```\nBut when run on the browser, it throws error as\n\"Uncaught TypeError: Cannot read property 'Config' of undefined\".\nCan anyone please guide me how to use s3 bucket using aws.sdk.ts? I am new in Typescript.\nother usage in the files are like \n```\nAnyone who has the solution ?\n```\n. ",
    "tomdavidson": "@chrisradek what about the typings at DefinitelyTyped? Will the typings your working on be distributed with the SDK along with AWS official support or is it a side project? I was working on refactoring to support promises - will you be creating tests for the typings?\n. ",
    "afaqurk": "@chrisradek, I meant that the module that is consuming the SDK is installed globally. When that same module is just used manually (node cli), it works. \nI checked and the bucket is in the right region (us-east-1). To be safe, I tried it with other regions as well, with no luck. I even tried removing the region configuration, when constructing the s3 object. Still no luck.\nExample of when it works:\n1. I go to the folder containing the node app (which consumes the sdk)\n2. I run node index.js, which calls the code I pasted above, with the s3 object configured to the right region (us-east-1). Any other region config would fail.\n3. Result: It works\nWhen it doesn't work:\n1. I install the same module globally via npm i -g .\n2. I run the command (configured to run index.js via packa.json's bin option)\n3. I get the signature error.\nIf it is truly a signature mismatch, I am curious to see why it only happens when the module is installed globally. \nIs there any way to tell what the underlying cause of the signature mismatch is? As in, have the SDK tell me if it is the incorrect bucket or some other property that causes the mismatch?\nAny help is appreciated.\n. Btw, this is the exact error:\njs\n{ [SignatureDoesNotMatch: The request signature we calculated does not match the signature you provided. Check your key and signing method.]\n  message: 'The request signature we calculated does not match the signature you provided. Check your key and signing method.',\n  code: 'SignatureDoesNotMatch',\n  region: null,\n  time: Thu May 19 2016 11:53:03 GMT-0400 (Eastern Daylight Time),\n  requestId: '{redacted}',\n  extendedRequestId: '{redacted}',\n  cfId: undefined,\n  statusCode: 403,\n  retryable: false,\n  retryDelay: 42.30998351704329 }\nI have confirmed with an internal team that our bucket does not have a region restriction (if that helps at all).\n. ",
    "meshko": "eh, nevermind, i think i figured it out, it was too async.\n. ",
    "jrbenito": "Hi @chrisradek ,\nFirst of all thanks for your reply.\nI understand what 403  means and now I understand why null body. However, how someone is supposed to know what error is going on if the error did not show 403 or access denied explicit? I only got a clear 403 after tuning on aws-sdk debug. Anyway, this does not solve the issue yet because I do have access to the resource.\nI did not try to call AWS SDK directly however I used same credentials with python boto3 library over the same resources without any issues; I also used s3-sync knox based (not s3-sync-aws that is AWS SDK based) with same credentials over the same bucket and resources with no issues at all.\nSorry, I was not clear on what I meant. When I wrote Even do I empty bucket and try a new deploy the error is thrown, I want to make it clear that I first tried to overwrite existem objects in a given bucket (that is the normal operation of my \"deploy\" action). Since I eventually got 403, I tried to delete all files in my bucket, not the bucket itself, and deploy. In other words: no overwrite of any file. I also got 403errors in this attempt. However, and this is the most important thing, I already did deploy in the same bucket, overwriting the same files, with very same tools (nodejs, s3-sync-aws, aws-sdk) and with the very same credentials. Can\u00b4t understand why I got 403 randomly.\nOnce again thanks for taking time to answer.\n. Hi @chrisradek ,\nFirst of all thanks for your reply.\nI understand what 403  means and now I understand why null body. However, how someone is supposed to know what error is going on if the error did not show 403 or access denied explicit? I only got a clear 403 after tuning on aws-sdk debug. Anyway, this does not solve the issue yet because I do have access to the resource.\nI did not try to call AWS SDK directly however I used same credentials with python boto3 library over the same resources without any issues; I also used s3-sync knox based (not s3-sync-aws that is AWS SDK based) with same credentials over the same bucket and resources with no issues at all.\nSorry, I was not clear on what I meant. When I wrote Even do I empty bucket and try a new deploy the error is thrown, I want to make it clear that I first tried to overwrite existem objects in a given bucket (that is the normal operation of my \"deploy\" action). Since I eventually got 403, I tried to delete all files in my bucket, not the bucket itself, and deploy. In other words: no overwrite of any file. I also got 403errors in this attempt. However, and this is the most important thing, I already did deploy in the same bucket, overwriting the same files, with very same tools (nodejs, s3-sync-aws, aws-sdk) and with the very same credentials. Can\u00b4t understand why I got 403 randomly.\nOnce again thanks for taking time to answer.\n. @chrisradek \nJust to illustrate, I deployed right now with the very same credentials and tools over the very same bucket overwriting files. No errors at all, this deploy failed yesterday and the logs I put here was taken from this failed attempt. right now I just retried (same computer, same terminal used yesterday) and got a success deploy.\nThose 403 errors happens randomly, but once they happy I got a bunch of them. \nEDIT:\nAs suggested I tried to call aws SDK directly with code below:\ns3.headObject(params, function(err, data) {\n      if (err) console.log(err, err.stack); // an error occurred\n        else     console.log(data);           // successful response\n});\nthis returned:\n[AWS s3 200 6.154s 0 retries] headObject({ Bucket: 'benito.com.br', Key: 'page-sitemap.xml' })\nWhile the deploying code returned:\n[AWS s3 403 1.013s 0 retries] headObject({ Bucket: 'benito.com.br', Key: 'page-sitemap.xml' })\nWorth to say, both ran from the same machine, same node.js@4.4.3 and AWS SDK@2.3.14. However, my simple try code above does not set a stream like s3-sync-aws do; my assumption is that this 403 error might be caused by the way s3-sync-aws package is placing calls to S3 bucket.\ndoes make sense?\n. @chrisradek \nJust to illustrate, I deployed right now with the very same credentials and tools over the very same bucket overwriting files. No errors at all, this deploy failed yesterday and the logs I put here was taken from this failed attempt. right now I just retried (same computer, same terminal used yesterday) and got a success deploy.\nThose 403 errors happens randomly, but once they happy I got a bunch of them. \nEDIT:\nAs suggested I tried to call aws SDK directly with code below:\ns3.headObject(params, function(err, data) {\n      if (err) console.log(err, err.stack); // an error occurred\n        else     console.log(data);           // successful response\n});\nthis returned:\n[AWS s3 200 6.154s 0 retries] headObject({ Bucket: 'benito.com.br', Key: 'page-sitemap.xml' })\nWhile the deploying code returned:\n[AWS s3 403 1.013s 0 retries] headObject({ Bucket: 'benito.com.br', Key: 'page-sitemap.xml' })\nWorth to say, both ran from the same machine, same node.js@4.4.3 and AWS SDK@2.3.14. However, my simple try code above does not set a stream like s3-sync-aws do; my assumption is that this 403 error might be caused by the way s3-sync-aws package is placing calls to S3 bucket.\ndoes make sense?\n. @chrisradek \nCredentials are a IAM User with its Access Key and a policy attach allowing it full operation on this bucket. \nRegarding your next question, based on logs I have until now just headObject fails is happening. Also, s3-sync-aws consults a metadata (headObject operation) to determine if a object need to be uploaded (PUT) or not.\n. @chrisradek \nCredentials are a IAM User with its Access Key and a policy attach allowing it full operation on this bucket. \nRegarding your next question, based on logs I have until now just headObject fails is happening. Also, s3-sync-aws consults a metadata (headObject operation) to determine if a object need to be uploaded (PUT) or not.\n. Hi,\nDuring the weekend I run a lot of deployments without a single issue. Temporary instability? Environment issues? I don\u00b4t know, I changed nothing on my work environment and my travis-ci presented no issue too. \nAnyway, I am closing this since no error was found nor reproducible.\n. Hi,\nDuring the weekend I run a lot of deployments without a single issue. Temporary instability? Environment issues? I don\u00b4t know, I changed nothing on my work environment and my travis-ci presented no issue too. \nAnyway, I am closing this since no error was found nor reproducible.\n. Hi @jeskew ,\n\nOne thing that jumps out at me from the packet dumps posted is that Node does not seem to establish a TLS connection, whereas the trace for Python is able to communicate over HTTP and TLS.\n\nIt should be done after the last PSH,ACK but instead, a period without message and consequent proxy server timeout (FIN, ACK) happens.\nAny AWS service is affected and I am able to use request with proxy:\nNo.     Time           Source                Destination           Protocol Length Info\n      2 0.433423       105.103.15.106        105.103.82.47         TCP      74     56000 \u2192 8080 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 TSval=561373560 TSecr=0 WS=128\n      3 0.434944       105.103.82.47         105.103.15.106        TCP      74     8080 \u2192 56000 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460 SACK_PERM=1 TSval=3655577491 TSecr=561373560 WS=128\n      4 0.434960       105.103.15.106        105.103.82.47         TCP      66     56000 \u2192 8080 [ACK] Seq=1 Ack=1 Win=29312 Len=0 TSval=561373560 TSecr=3655577491\n      5 0.437067       105.103.15.106        105.103.82.47         HTTP     150    CONNECT www.google.com:443 HTTP/1.1 \n      6 0.438963       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56000 [ACK] Seq=1 Ack=85 Win=14592 Len=0 TSval=3655577495 TSecr=561373561\n      7 0.442943       105.103.82.47         105.103.15.106        HTTP     185    HTTP/1.1 200 Connection established \n      8 0.442948       105.103.15.106        105.103.82.47         TCP      66     56000 \u2192 8080 [ACK] Seq=85 Ack=120 Win=29312 Len=0 TSval=561373562 TSecr=3655577500\n      9 0.463281       105.103.15.106        105.103.82.47         TLSv1.2  333    Client Hello\n     10 0.503938       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56000 [ACK] Seq=120 Ack=352 Win=15616 Len=0 TSval=3655577561 TSecr=561373567\n     11 1.378050       105.103.82.47         105.103.15.106        TLSv1.2  1995   Server Hello\n     12 1.378061       105.103.15.106        105.103.82.47         TCP      66     56000 \u2192 8080 [ACK] Seq=352 Ack=2049 Win=33152 Len=0 TSval=561373796 TSecr=3655578435\n     13 1.383965       105.103.82.47         105.103.15.106        TLSv1.2  1715   CertificateServer Key Exchange, Server Hello Done\n     14 1.383970       105.103.15.106        105.103.82.47         TCP      66     56000 \u2192 8080 [ACK] Seq=352 Ack=3698 Win=36480 Len=0 TSval=561373797 TSecr=3655578440\n     15 1.387097       105.103.15.106        105.103.82.47         TLSv1.2  257    Client Key Exchange, Change Cipher Spec, Encrypted Handshake Message, Encrypted Handshake Message\n     16 1.388978       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56000 [ACK] Seq=3698 Ack=543 Win=16640 Len=0 TSval=3655578445 TSecr=561373798\n     17 1.879802       105.103.82.47         105.103.15.106        TLSv1.2  363    New Session Ticket, Change Cipher Spec, Hello Request, Hello Request\n     18 1.883315       105.103.15.106        105.103.82.47         TLSv1.2  154    Application Data\n     19 1.884966       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56000 [ACK] Seq=3995 Ack=631 Win=16640 Len=0 TSval=3655578941 TSecr=561373922\n     20 2.346386       105.103.82.47         105.103.15.106        TCP      168    [TCP segment of a reassembled PDU]\n     21 2.384899       105.103.15.106        105.103.82.47         TCP      66     56000 \u2192 8080 [ACK] Seq=631 Ack=4097 Win=39296 Len=0 TSval=561374048 TSecr=3655579403\n     22 2.392992       105.103.82.47         105.103.15.106        TLSv1.2  965    Application Data\n     23 2.392997       105.103.15.106        105.103.82.47         TCP      66     56000 \u2192 8080 [ACK] Seq=631 Ack=4996 Win=42240 Len=0 TSval=561374050 TSecr=3655579450\n     24 2.398057       105.103.15.106        105.103.82.47         TLSv1.2  97     Encrypted Alert\n     25 2.398878       105.103.15.106        105.103.82.47         TCP      66     56000 \u2192 8080 [FIN, ACK] Seq=662 Ack=4996 Win=42240 Len=0 TSval=561374051 TSecr=3655579450\n     26 2.399996       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56000 [ACK] Seq=4996 Ack=662 Win=16640 Len=0 TSval=3655579456 TSecr=561374051\n     32 2.404979       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56000 [FIN, ACK] Seq=4996 Ack=663 Win=16640 Len=0 TSval=3655579461 TSecr=561374051\n     33 2.404986       105.103.15.106        105.103.82.47         TCP      66     56000 \u2192 8080 [ACK] Seq=663 Ack=4997 Win=42240 Len=0 TSval=561374053 TSecr=3655579461\nBased on this old discussion, node has issues with CONNECT method that python CLI seems to be using. I got an answer at stackoverflow.com that point to feature request. \nNode\u00b4s https with tunnel-agent set, goes like:\nNo.     Time           Source                Destination           Protocol Length Info\n      2 1.808497       105.103.15.106        105.103.82.47         TCP      74     56110 \u2192 8080 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 TSval=562171144 TSecr=0 WS=128\n      3 1.810142       105.103.82.47         105.103.15.106        TCP      74     8080 \u2192 56110 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460 SACK_PERM=1 TSval=3658767683 TSecr=562171144 WS=128\n      4 1.810156       105.103.15.106        105.103.82.47         TCP      66     56110 \u2192 8080 [ACK] Seq=1 Ack=1 Win=29312 Len=0 TSval=562171145 TSecr=3658767683\n      5 1.811591       105.103.15.106        105.103.82.47         TCP      332    56110 \u2192 8080 [PSH, ACK] Seq=1 Ack=1 Win=29312 Len=266 TSval=562171145 TSecr=3658767683\n      6 1.813140       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56110 [ACK] Seq=1 Ack=267 Win=15616 Len=0 TSval=3658767686 TSecr=562171145\n      7 1.827001       105.103.82.47         105.103.15.106        TCP      124    8080 \u2192 56110 [PSH, ACK] Seq=1 Ack=267 Win=15616 Len=58 TSval=3658767700 TSecr=562171145\n      8 1.827004       105.103.15.106        105.103.82.47         TCP      66     56110 \u2192 8080 [ACK] Seq=267 Ack=59 Win=29312 Len=0 TSval=562171149 TSecr=3658767700\n[... conversation continues ...]\n     22 1.850856       105.103.15.106        105.103.82.47         TCP      371    56110 \u2192 8080 [PSH, ACK] Seq=632 Ack=1585 Win=34816 Len=305 TSval=562171155 TSecr=3658767717\n     23 1.892164       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56110 [ACK] Seq=1585 Ack=937 Win=18816 Len=0 TSval=3658767765 TSecr=562171155\n     36 122.059081     105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56110 [FIN, ACK] Seq=1585 Ack=937 Win=18816 Len=0 TSval=3658887925 TSecr=562171155\n     37 122.061821     105.103.15.106        105.103.82.47         TCP      66     56110 \u2192 8080 [FIN, ACK] Seq=937 Ack=1586 Win=34816 Len=0 TSval=562201207 TSecr=3658887925\n     38 122.063802     105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56110 [ACK] Seq=1586 Ack=938 Win=18816 Len=0 TSval=3658887930 TSecr=562201207\nAs you can see, it fails more or less as before by timeout on message 36 (messages 24 to 35 are not from this conversation). SDK set with tunnel-agent do more or less the same throwing error like:\n{ Error: socket hang up\n    at TLSSocket.onHangUp (_tls_wrap.js:1124:19)\n    at TLSSocket.g (events.js:292:16)\n    at emitNone (events.js:91:20)\n    at TLSSocket.emit (events.js:185:7)\n    at endReadableNT (_stream_readable.js:974:12)\n    at _combinedTickCallback (internal/process/next_tick.js:80:11)\n    at process._tickDomainCallback (internal/process/next_tick.js:128:9)\n  message: 'socket hang up',\n  code: 'NetworkingError',\n  region: 'us-west-2',\n  hostname: 'bucket.s3-us-west-2.amazonaws.com',\n  retryable: true,\n  time: 2017-07-10T19:40:55.168Z } null\nHowever, if I set tunnel-agent as httpsOverHttp I can use AWS-sdk from behind proxy:\nNo.     Time           Source                Destination           Protocol Length Info\n      1 0.000000       105.103.15.106        105.103.82.47         TCP      74     56158 \u2192 8080 [SYN] Seq=0 Win=29200 Len=0 MSS=1460 SACK_PERM=1 TSval=562390733 TSecr=0 WS=128\n      2 0.001956       105.103.82.47         105.103.15.106        TCP      74     8080 \u2192 56158 [SYN, ACK] Seq=0 Ack=1 Win=14480 Len=0 MSS=1460 SACK_PERM=1 TSval=3659645993 TSecr=562390733 WS=128\n      3 0.001982       105.103.15.106        105.103.82.47         TCP      66     56158 \u2192 8080 [ACK] Seq=1 Ack=1 Win=29312 Len=0 TSval=562390734 TSecr=3659645993\n      4 0.003726       105.103.15.106        105.103.82.47         HTTP     215    CONNECT s3-us-west-2.amazonaws.com:443 HTTP/1.1 \n      5 0.005580       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56158 [ACK] Seq=1 Ack=150 Win=15616 Len=0 TSval=3659645997 TSecr=562390734\n      6 1.067785       105.103.82.47         105.103.15.106        HTTP     185    HTTP/1.1 200 Connection established \n      7 1.067796       105.103.15.106        105.103.82.47         TCP      66     56158 \u2192 8080 [ACK] Seq=150 Ack=120 Win=29312 Len=0 TSval=562391000 TSecr=3659647059\n      8 1.089246       105.103.15.106        105.103.82.47         TLSv1.2  345    Client Hello\n      9 1.090774       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56158 [ACK] Seq=120 Ack=429 Win=16640 Len=0 TSval=3659647082 TSecr=562391006\n     10 1.652870       105.103.82.47         105.103.15.106        TLSv1.2  1995   Server Hello\n     11 1.652881       105.103.15.106        105.103.82.47         TCP      66     56158 \u2192 8080 [ACK] Seq=429 Ack=2049 Win=33152 Len=0 TSval=562391147 TSecr=3659647644\n     12 1.658780       105.103.82.47         105.103.15.106        TLSv1.2  1284   CertificateServer Key Exchange, Server Hello Done\n     13 1.661066       105.103.15.106        105.103.82.47         TLSv1.2  192    Client Key Exchange, Change Cipher Spec, Encrypted Handshake Message\n     14 1.662810       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56158 [ACK] Seq=3267 Ack=555 Win=16640 Len=0 TSval=3659647654 TSecr=562391149\n     15 2.049608       105.103.82.47         105.103.15.106        TLSv1.2  117    Change Cipher Spec, Hello Request, Hello Request\n     16 2.052918       105.103.15.106        105.103.82.47         TLSv1.2  565    Application Data\n     17 2.054778       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56158 [ACK] Seq=3318 Ack=1054 Win=17792 Len=0 TSval=3659648046 TSecr=562391247\n     18 2.959859       105.103.82.47         105.103.15.106        TLSv1.2  845    Application Data\n     19 2.965808       105.103.82.47         105.103.15.106        TLSv1.2  152    Application DataEncrypted Alert\n     20 2.965829       105.103.15.106        105.103.82.47         TCP      66     56158 \u2192 8080 [ACK] Seq=1054 Ack=4183 Win=38912 Len=0 TSval=562391475 TSecr=3659648951\n     21 2.967948       105.103.15.106        105.103.82.47         TLSv1.2  97     Encrypted Alert\n     22 2.968818       105.103.15.106        105.103.82.47         TCP      66     56158 \u2192 8080 [FIN, ACK] Seq=1085 Ack=4183 Win=38912 Len=0 TSval=562391476 TSecr=3659648951\n     23 2.969797       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56158 [ACK] Seq=4183 Ack=1085 Win=17792 Len=0 TSval=3659648960 TSecr=562391475\n     24 3.009816       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56158 [ACK] Seq=4183 Ack=1086 Win=17792 Len=0 TSval=3659649001 TSecr=562391476\n     30 3.258792       105.103.82.47         105.103.15.106        TCP      66     8080 \u2192 56158 [FIN, ACK] Seq=4183 Ack=1086 Win=17792 Len=0 TSval=3659649250 TSecr=562391476\n     31 3.258803       105.103.15.106        105.103.82.47         TCP      66     56158 \u2192 8080 [ACK] Seq=1086 Ack=4184 Win=38912 Len=0 TSval=562391548 TSecr=3659649250\nSo, this is not AWS-sdk fault but node\u00b4s limitation that can be workaround by using tunnel-agent properly configured.\nThanks for support.. ",
    "bdmayes": "Also seeing this behavior on SDK v2.3.8. The ManagedUpload works but creating the multipart upload and uploading parts individually fails with this same error. The two implementations are creating different x-amz-content-sha256 headers which seems wrong to me.\nI tweaked the code above to listen for the 'sign' event on the upload of the first part:\nreq.on('sign', function(stuff) {\n  console.log('sha256: ' + stuff.httpRequest.headers['X-Amz-Content-Sha256']);\n});\nI also wrote a slightly different version that uses the ManagedUpload object:\n```\nvar uploadReq = s3.upload(params, options);\nuploadReq.send(function(err) {\n  if (err) {\n    console.log('Got an error: ' + JSON.stringify(err, null, '  '));\n  } else {\n    console.log('it worked');\n  }\n});\n```\nand then to capture the sha256 sum for this second implementation, I modified aws-sdk/lib/s3/managed_upload.js and added the same event listener below line 475 so that the file looks like this:\n```\nvar partInfo = {ETag: null, PartNumber: partNumber};\nself.completeInfo[partNumber] = partInfo;\nvar req = self.service.uploadPart(partParams);\nreq.on('sign', function(stuff) {\n  console.log('sha256: ' + stuff.httpRequest.headers['X-Amz-Content-Sha256']);\n});\nself.parts[partNumber] = req;\nreq._lastUploadedBytes = 0;\nreq._managedUpload = self;\n```\nLooking at how that HMAC signature is created (http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-header-based-auth.html), it uses the secret key and the date (yyyymmdd) as part of the calculation. Since the date is the same and my secret is the same, I would expect both implementations to generate the same value for this header but they do not.\nI can run the script that uses the ManagedUpload multiple times, and every time it outputs the exact same sha256. Similarly, I can run the \"low-level\" implementation and it generates the same sha256 each time. So the values are consistent each time I run them, but the values are not consistent across the two different implementations.\nIt seems like there might be a bug when using the \"low-level\" implementation that it generates the wrong sha256 header. I'm trying to determine exactly where it generates that to see why the signature is different.\n. I'm on 10.10.5 and node 0.10.40. I'm also testing in us-east-1. I just tried running a fresh npm install aws-sdk in a different directory to get 2.3.14 and I see the same failing behavior. My parent company is known for locking down our network pretty hardcode, so I just came home and tried it here. For grins I also tried using a bucket and an S3 path that has absolutely no special characters in it. It's still failing.\nAll I have to do is comment out that signatureVersion: 'v4' option and it works fine. Unfortunately that means that I cannot do anything with KMS, since uploading with the SSEKMSKeyId option requires the signature version to be set to v4.\n. Super! I just tested these changes and I can confirm that it works for me, with or without the v4 signature. Thanks for the quick turnaround.\n. ",
    "umarservishero": "Hi @LiuJoyceC Thank you for the quick reply,\nInfo:\naws-sdk: 2.3.16\nnode: 4.4.3\nAfter long hours of debugging, I have narrowed it down in my application to this\nvar params = {\n            TableName: TABLE,\n            Key: {\n              'jobId': snsMsg.jobId + '',\n              'key': 'validQuotes'\n            },\n            UpdateExpression: 'SET #val = #val + :incr',\n            ExpressionAttributeNames:{\n              '#val':'value'\n            },\n            ExpressionAttributeValues: {\n              \":incr\": 1\n            },\n            ReturnValues: 'UPDATED_NEW'\n          };\n            docClient.update(params, function(err, data) {\n              if (err) {\n                console.log(JSON.stringify(err, null, 2));\n                return done(new Error(err));\n              }\n              else {\n...\nThe params logged as\n{ TableName: 'JOB_META',\n  Key: { jobId: '46238', key: 'validQuotes' },\n  UpdateExpression: 'SET #val = #val + :incr',\n  ExpressionAttributeNames: { '#val': 'value' },\n  ExpressionAttributeValues: { ':incr': 1 },\n  ReturnValues: 'UPDATED_NEW' }\nSo as to the context of why is this quite difficult to debug, I couldn't replicate this on my local machine but as i deployed it to eb, these throw errs are popping up\n```\nSyntaxError: Unexpected token \njob-booked now polling\njob-closed now polling\njob-new now polling\nquote-validated now polling\nrating-for-hero-new now polling\nUI started on port 8000\n/var/app/current/node_modules/aws-sdk/lib/request.js:31\n            throw err;\n            ^\nSyntaxError: Unexpected token \njob-booked now polling\njob-closed now polling\njob-new now polling\nquote-validated now polling\nrating-for-hero-new now polling\nUI started on port 8000\n/var/app/current/node_modules/aws-sdk/lib/request.js:31\n            throw err;\n            ^\nSyntaxError: Unexpected token \njob-booked now polling\njob-closed now polling\njob-new now polling\nquote-validated now polling\nrating-for-hero-new now polling\nUI started on port 8000\n/var/app/current/node_modules/aws-sdk/lib/request.js:31\n            throw err;\n            ^\nSyntaxError: Unexpected token \n```\nLet me know if you need any additional info.\n. Hey @LiuJoyceC after further debugging and testing, I have narrowed it down sqs-consumer polling SQS. Will try to figure out a way to add console logging to the lib file since the the bundle is build on eb\nThe funny thing about this is, I have 4 more exactly the same SqsConsumer polling different SQS without any issue.\nIs it possible for the message sent to the SQS to trigger this error? \n```\nvar AWS = require('aws-sdk');\nvar SqsConsumer = require('sqs-consumer');\ntry {\n    var quoteValidated = SqsConsumer.create({\n  queueUrl: config.get('Sqs.quote-validated'),\n  handleMessage: function (message, done) {\n    ...\n    done();\n  },\n  sqs: new AWS.SQS()\n\n});\nquoteValidated.start();\nconsole.log('quote-validated now polling');\n\n} catch (e) {\n    console.log('e quote-validated.js', e);\n  }\n```\nMany thanks\n. Hi @LiuJoyceC finally managed to add in the console logging in there, really stumped by this one. Any help is much appreciated.\n```\njob-booked now polling\njob-closed now polling\njob-new now polling\nquote-validated now polling\nrating-for-hero-new now polling\nUI started on port 8000\nself._asm.currentState complete\nself.operation receiveMessage\nself.params { QueueUrl: 'https://sqs.ap-southeast-1.amazonaws.com/XXXXXXXX/sqs-quote-validated',\n  AttributeNames: [],\n  MessageAttributeNames: [],\n  MaxNumberOfMessages: 1,\n  WaitTimeSeconds: 20,\n  VisibilityTimeout: undefined }\n/var/app/current/node_modules/aws-sdk/lib/request.js:34\n            throw err;\n            ^\nSyntaxError: Unexpected token \njob-booked now polling\njob-closed now polling\njob-new now polling\nquote-validated now polling\nrating-for-hero-new now polling\nUI started on port 8000\nself._asm.currentState complete\nself.operation receiveMessage\nself.params { QueueUrl: 'https://sqs.ap-southeast-1.amazonaws.com/XXXXXXXX/sqs-quote-validated',\n  AttributeNames: [],\n  MessageAttributeNames: [],\n  MaxNumberOfMessages: 1,\n  WaitTimeSeconds: 20,\n  VisibilityTimeout: undefined }\n/var/app/current/node_modules/aws-sdk/lib/request.js:34\n            throw err;\n            ^\nSyntaxError: Unexpected token \njob-booked now polling\njob-closed now polling\njob-new now polling\nquote-validated now polling\nrating-for-hero-new now polling\nUI started on port 8000\nself._asm.currentState complete\nself.operation receiveMessage\nself.params { QueueUrl: 'https://sqs.ap-southeast-1.amazonaws.com/XXXXXXXX/sqs-quote-validated',\n  AttributeNames: [],\n  MessageAttributeNames: [],\n  MaxNumberOfMessages: 1,\n  WaitTimeSeconds: 20,\n  VisibilityTimeout: undefined }\n/var/app/current/node_modules/aws-sdk/lib/request.js:34\n            throw err;\n            ^\nSyntaxError: Unexpected token \n```\n. @LiuJoyceC It seems like the SQS itself was problematic, once I created another sqs everything works as expected. Thank you for your time and help. Much appreciated\n. It's Now happening again....\nAny suggestions on what action I should take?\n. ",
    "Tamal": "Can you also add transaction support in Golang? \n. ",
    "anujanavare": "hello Joyce, can we expect the transaction support in the javascript sdk anytime soon?\n. ",
    "lexakozakov": "it's good the have.\n. it's good the have.\n. ",
    "funwun": "Any update on this?. ",
    "efatatrisatya": "+1. ",
    "issfire": "+1. +1. ",
    "gozup": "+1. Even with the last version (2.48.0). ",
    "sesha-nyllabs": "+1. ",
    "Hmachalani": "+1. ",
    "melaniaandrisan": "+1. ",
    "artyomtkachenko": "+1. ",
    "tanzeelrana": "+1\n:+1: . One solution I found was using meta data for items, lock the items related to the transactional queries and once all the queries are complete release the locks. ",
    "adisembiring": "\ud83d\udc4d . ",
    "urbien": "we need it to write to two DynamoDB tables. Alternative is have one Lambda call another, but this creates unneeded latency and ads costs. . ",
    "bitbrained": "+1. ",
    "superandrew213": "+1. @jeskew what if 2 users are querying that index for uniqueness at the same time? Both users will get the green flag to putItem and you will end up with duplicates for a unique field.. ",
    "candidasa": "+1. +1. ",
    "voidale": "+1. ",
    "aryehischechter": "+1. ",
    "bluszcz": "@trevorhreed +1, well said. @trevorhreed +1, well said. ",
    "jodevak": "+1. ",
    "rubemz": "+1. ",
    "mciccarello": "Why the hell not?. ",
    "tatrungkien": "Why Java SDK support but JavaScript SDK not?. ",
    "fredvollmer": "While I respect the decision of the team to focus on other things, an explanation of why this was passed over internally would be nice to have. Is there a technical reason why this feature is less important in the JS SDK than in the Java SDK? Is there a different approach to atomic transactions that we as developers should be using?\nIt would be great for the DynamoDB team to include this support directly, but in the meantime, I just fail to see why the JS SDK should not strive for feature parity with the Java SDK (in this particular respect.). Also, I haven't looked extensively at how this is implemented in Java, but would you be interested in a PR? . ",
    "BarryCarlyon": "I'm looping every 1/2 a second but it only SQS fetches IF there isn't a SQS fetch running. So there shouldn't be any hanging sockets, or sockets running concurrently (well aside from calls to message delete but the running flag should handle that.\nI'm now on 2.3.16 (previously unsure), and seeing the same issue, my watchdog timer last caught and force reset at \"Wed Jun 01 2016 11:27:07 GMT+0100\"\nYes it's after the job has been running for $some_time say a few days or so.\nI can't say I've seen high CPU/Memory usage. New Relic hasn't caught anything abnormal (as it's monitoring the server).\nSo in summary, I should be waiting for the current sqs.receiveMessage to finish before I call it again.\nCode block follows:\n```\n    var running = false;\nrunMonitorJob = setInterval(function() {\n    if (running) {\n        // do nothing\n    } else {\n        running = true;\n\nclearTimeout(watchdogTimeout);\nwatchdogTimeout = setTimeout(function() {\n    console.log('WatchDog');\n    running = false;\n}, 120000);\n        sqs.receiveMessage({\n            QueueUrl: queueUrl,\n            MaxNumberOfMessages: 10,\n            WaitTimeSeconds: 20\n        }, function(err, data) {\n            if (err) {\n                logger.fatal('Error on Message Recieve');\n                logger.fatal(err);\n            } else {\n                // all good\n                if (undefined === data.Messages) {\n                    logger.info('No Messages Object');\n                    timeCheck();\n                } else if (data.Messages.length > 0) {\n                    logger.info('Messages Count: ' + data.Messages.length);\n\n                    var delete_batch = new Array();\n                    for (var x=0;x<data.Messages.length;x++) {\n                        // process\n                        receiveMessage(data.Messages[x]);\n\n                        // flag to delete\n\n                        var pck = new Array();\n                        pck['Id'] = data.Messages[x].MessageId;\n                        pck['ReceiptHandle'] = data.Messages[x].ReceiptHandle;\n\n                        delete_batch.push(pck);\n                    }\n\n                    if (delete_batch.length > 0) {\n                        logger.info('Calling Delete');\n                        sqs.deleteMessageBatch({\n                            Entries: delete_batch,\n                            QueueUrl: queueUrl\n                        }, function(err, data) {\n                            if (err) {\n                                logger.fatal('Failed to delete messages');\n                                logger.fatal(err);\n                            } else {\n                                logger.debug('Deleted recieved ok');\n                            }\n                        });\n                    }\n\n                } else {\n                    logger.info('No Messages Count');\n                }\n            }\n\n            running = false;\n        });\n    }\n}, 500);\n\n```\nlogger is a call to log4js, and receiveMessage basically dumps off to Redis which I munch on later.\nIf I was to add the maxSockets what would I log to detect if I hit maxSockets? (Would it chuck a error somewhere?)\n. ",
    "neoadventist": "Any update on this? \n. ",
    "michaelwittig": "@chrisradek thanks!\n. ",
    "Dayjo": "+1 for this, would be nice (whether it 'needs' a promise or not) to have a standard way of handling all calls. Saves doing something like;\njs\nvar p = new Promise(function(resolve,reject) {\n     S3.getSignedURL('getObject', params, function(err, url) { resolve(url); });\n});\n. @jsonmaur indeed, 'twas but an example \ud83d\udc4d . Hah :) thanks @chrisradek \nYou got there JUST after I figured that out, hah have updated the top post, but yes, this is because of the incorrect use of promises. I didn't realise/think/take on board that you could only use one :) \n. ",
    "NotBobTheBuilder": "Commenting to note my suggestion from another thread, as a bluebird user I've been using\nconst getSignedUrl = Promise.promisify(S3.getSignedUrl.bind(S3)) and calling getSignedUrl().then to get around this.\nI think the sometimes-synchronous interface should be deprecated in favour of returning a Request object. If nothing else it's extremely error prone for people who depend on that interface.\n. Thanks! I'm currently using bluebird so using\nPromise.promisify(s3.getSignedUrl.bind(s3))(.....\nThanks for your speedy response to the issue!\n. Interesting, thanks! Would it be possible to add this to the documentation? It's currently not noted and if this behaviour isn't possible, documenting it would be worth it - I'm sure I'm not the only one who's come up against this issue.\n. @paulxtiseo This issue specifically concerns S3 - when you put an object into S3 you can give it an expiration, after which S3 will delete it. It's not possible to do that using getSignedUrl because the function will interpret the Expires property to be the expiration for the URL, not for the created S3 Object.\nYou only need to worry about this if you want to put objects to S3 and have S3 expire them. If you just want to create time-limited links, it'll work as you expect.\n. ",
    "jsonmaur": "@Dayjo you want to make sure you reject with errors as well.\njavascript\nnew Promise((resolve, reject) => {\n  s3.getSignedUrl('getObject', opts, (err, url) => {\n    if (err) reject(err)\n    else resolve(url)\n  })\n}). ",
    "IsaiahJTurner": "Would love to see this as well.. I'd be willing to do this and open a PR but I'd appreciate design approval from a maintainer before I go ahead since this will require an API change.\nCurrent: getSignedUrl(operation, params, callback) \u21d2 String? doesn't work because we'd need to add a promise function onto String.\ngetSignedUrl(operation, params, [options], callback) \u21d2 String? where options.promises is a Boolean and determines the return type doesn't seem like a good idea because it would result in changing return types.\ngetSignedUrl(operation, params, callback) \u21d2 AWS.Request in a major version upgrade would be the most consistent with the current API but is also likely to upset the most people.\ngetSignedUrlPromise(operation, params) \u21d2 Promise is consistent with other APIs (see: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Credentials.html), would be easy to document, easy to understand, is purely additive, and thus I believe it is the best solution.\nSo maintainers, what do you think about adding?getSignedUrlPromise(operation, params) \u21d2 Promise. Should I add anything to /.changes/next-release/feature-getSignedUrl-9194166f.json? Couldn't find any documentation for how that works/naming convention.. Happy to do that but that solution could lead replicated code down the line. For example, if/when this functionality is added to AWS.CloudFront.Signer.getSignedUrl, we'd need to wrap it with the same logic.\nWhat do you think about modifying AWS.util.promisifyMethod so that the generated promise function accepts arguments? \nSomething like this:\njs\nfunction promisifyMethod(methodName, PromiseDependency) {\n    return function promise() {\n        var self = this;\n        var args = Array.prototype.slice.call(arguments);\n        return new PromiseDependency(function(resolve, reject) {\n            args.push(function(err, data) {\n                if (err) {\n                    reject(err);\n                } else {\n                    resolve(data);\n                }\n            });\n            self[methodName].apply(null, args);\n        });\n    };\n}\nMy solution does lead to one of two downsides:\n- Supplying too many arguments will give unexpected behavior like a \"callback is not a function\" error.\n- To prevent that with a more informative error like \"Too many arguments supplied\" I'd need to read self[methodName].length to get the expected arguments count but this would be incompatible with any functions that support the arguments style Array.prototype.push([element1[, ...[, elementN]]]) since Array.prototype.push.length would only return 1. I'm not sure if any functions in the SDK work like this.\nRegardless of which downside chosen, I still think improving AWS.util.promisifyMethod rather than wrapping the function directly is the best option since it is the most extensible. Also, how AWS.util.promisifyMethod currently works in contrast with how other promisify functions within other JS libraries like Bluebird, es6-promisify, and promisify-node work. . Went ahead and pushed my concept. Let me know what you think, can easily change it back and use your idea.. ",
    "sjmcdowall": "Guys -- I appreciate the \"major version bump\" but you do know it's been 2+ YEARS waiting here.. ?? . ",
    "miguelduarte42": "Just stumbled on this after 1 hour of debugging. Exceptions to the normal .promise() behavior should be made clear somewhere in the documentation. ",
    "apires": "Not working on this anymore, but adding an option to preserve the mime as is sounds good. \nThanks!. Hello @chrisradek,\nThank you for the reply. We've been testing over the last few days with doubling the timeout. We'll try lowering the queue size also.\nWe might have to remove the timeout altogether as you mentioned.\nI guess we can close this as a feature request for having the the uploader instrumented to keep track of of runtime stats and adjust partSize / timeout based on runtime heuristics.\nThanks again!\n. Hello @chrisradek,\nThank you for the reply. We've been testing over the last few days with doubling the timeout. We'll try lowering the queue size also.\nWe might have to remove the timeout altogether as you mentioned.\nI guess we can close this as a feature request for having the the uploader instrumented to keep track of of runtime stats and adjust partSize / timeout based on runtime heuristics.\nThanks again!\n. ",
    "matamx": "it resolved passing this option signatureVersion:'v2'\n. ",
    "LamourBt": "To get the accessKeyId and secretAccessKey, you must go IAM Management Console > User > Create A New User. Once you create the user you will get these info. \nAfter you would need to give to this new user some permissions, for example give access to  S3 or dynamoDB. To do so click on the user then go to Permissions Tab then Attach Policies. Then you should be fine. @casamm \n. ",
    "casamm": "sorry I think my question wasn't clear, I'm actually going through the SDK code and trying to understand the working behind it.\nHow the AWS is storing the configuration data that even actors like S3 which are not instantiated yet are able to access those values later after the instantiation. \nCan some engineer/developer of the SDK help me out on this?\n. @LiuJoyceC we are getting closer, let me narrow AWS files really down as per my understanding and it may not match the actual code but I need help with broader structure to figure out how the S3 and other actors are accessing configuration data.\napp.js\n```\nvar AWS = require(\"./aws\");\nvar aws = new AWS(\"key\", \"secret\");\nvar s3 = new AWS.S3();\ns3.listBuckets();\n```\naws.js\n```\nvar AWS = function(accessKeyId, secretAccessKey){\n    AWS.config.credentials.accessKeyId = accessKeyId;\n    AWS.config.credentials.secretAccessKey = secretAccessKey;\n};\nAWS.config = { \n    credentials: {\n        accessKeyId: null,\n        secretAccessKey: null\n    }\n};\nAWS.S3 = require(\"./s3\");\nmodule.exports = AWS;\n```\ns3.js\n```\nvar S3 = function(){};\nS3.prototype.listBuckets = function(){};\nmodule.exports = S3;\n//console.log(AWS.config); //how S3 is reaching out to configuration data\n``\n. Slightly modified since didn't want to haverequire(\"./s3\");` in the app.js. Please suggest in case you see something wrong with it. Thanks for your awesome help.\napp.js\n```\nvar AWS = require(\"./aws\");\nAWS(\"key\", \"secret\");\nvar s3 = new AWS.S3();\ns3.listBuckets();\n```\naws.js\n```\nvar AWS = function(accessKeyId, secretAccessKey){\n    AWS.config.credentials.accessKeyId = accessKeyId;\n    AWS.config.credentials.secretAccessKey = secretAccessKey;\n};\nAWS.config = {\n    credentials: {accessKeyId: null, secretAccessKey: null}\n};\nmodule.exports = AWS;\nrequire(\"./s3\"); //moved from app.js\n```\ns3.js\n```\nvar AWS = require(\"./aws\");\nvar S3 = function(){\n    this.config = {};\n    this.config.credentials = AWS.config.credentials;\n};\nS3.prototype.listBuckets = function(){\n    var credentials = this.config.credentials;\n    // make request using these credentials\n};\nAWS.S3 = S3;\n```\n. Thanks LiuJoyceC. This clarifies, I'll ask you again in case. Great. \n. Hi LiuJoyceC,\nWondering how multi-tenancy will work independently of two instances of require(\"./aws\");\nIt seems to me that I've to update credentials each time I need to switch to another environment under a different account as follows.\n```\nAWS(\"key\", \"secret\"); // account A\ns3.putObject();\n// wait until pubObject completes\nAWS(\"key2\", \"secret2\"); // account B\ns3.putObject();\n```\nIs there a way to have two distinct objects in an efficient way, and use both of them concurrently to upload to two different accounts. Even If custom changes are required to SDK under the light of our minimalistic example above then please advise. \n. ",
    "shesko": "Yep, that was it. Next time I follow a tutorial I'll double check that the versions are up to date.\nThanks!\n. ",
    "swaminathane": "when the application is online the AWS.config.credentials.get( ) gets the identityId and after getting the identityId switching the application offline identityId is not fetched and there is a error message saying err Error: Network Failure(\u2026)\n. ",
    "neosekar": "Cognito sync while offline access in javascript sdk, \nAWS.config.credentials.get(function(err) {}); this method returns Network failure error\n. ",
    "fluffywaffles": "Hey @chrisradek, thanks for the response.\nEDIT Ignore this; read the comment below. Got it working, albeit with crazy methods.\nWe're using JSPM/SystemJS for a couple reasons, but the big one is that the SystemJS loader is the standards-track future of JS modules. The other things that's nice about SystemJS (at least, most of the time) it that its builder, like webpack, supports bundling with Rollup as a stopgap while we wait for HTTP/2. That, and it has really good support for ES6 and TypeScript, and can manage both browser and node dependencies either separately or together. This support comes in handy when using Angular 2, and it's relatively easy to convince JSPM to compile code down into Node v4 compatibility for AWS Lambda, too.\nRight now, even to use the browser version, we're having to import the AWS SDK in a <script> tag and refer to it as a global in our code. We cannot coerce the SDK to load into node at all - although I had some (fleeting) luck by first compiling our source into a bundle with JSPM, and then transforming the bundle with browserify to resolve a call to require('aws-sdk'). This was untenably slow, though.\nWe're targeting both the browser and Node.js using JSPM/SystemJS. In particular, we're writing an Angular app and a group of Lambda functions that share AWS SDK related code.\nRight now, we're writing a wrapper around AWS and AWSCognito so that we aren't exposing the globals everywhere. We're planning to test our client code by spawning a browser to run our tests. In some places, we're trying to mock the SDK so we can test without a broswer. We're planning to test the Lambda functions by using \"AWS\" as a global and creating a special testing deployment where we deploy all our functions in order to test, which works since the SDK is preloaded on Lambda. Lambda isn't really a CI service, though.\nAs with webpack, the issue revolves around expressions inside of calls to require. While this can be mitigated to an extent by using JSPM overrides, as I have done in my sample project, it only goes so far - you can't write an override to convince Node that \"AWS.APIGateway.prototype\" is defined when it tries to load lib/services/apigateway.js. I'm sure even if we got past that, there would be more errors.\nAs you said in the issue about Webpack, the problem is really that the SDK as it is written isn't particularly friendly toward typical ES6 workflows involving bundling and static dependency resolution. Without an overhaul of the SDK, I think our best path forward right now is just to load the SDK in <script> tags and try to hide the global behind a helper library.\n. Hey @chrisradek, thanks for the response.\nEDIT Ignore this; read the comment below. Got it working, albeit with crazy methods.\nWe're using JSPM/SystemJS for a couple reasons, but the big one is that the SystemJS loader is the standards-track future of JS modules. The other things that's nice about SystemJS (at least, most of the time) it that its builder, like webpack, supports bundling with Rollup as a stopgap while we wait for HTTP/2. That, and it has really good support for ES6 and TypeScript, and can manage both browser and node dependencies either separately or together. This support comes in handy when using Angular 2, and it's relatively easy to convince JSPM to compile code down into Node v4 compatibility for AWS Lambda, too.\nRight now, even to use the browser version, we're having to import the AWS SDK in a <script> tag and refer to it as a global in our code. We cannot coerce the SDK to load into node at all - although I had some (fleeting) luck by first compiling our source into a bundle with JSPM, and then transforming the bundle with browserify to resolve a call to require('aws-sdk'). This was untenably slow, though.\nWe're targeting both the browser and Node.js using JSPM/SystemJS. In particular, we're writing an Angular app and a group of Lambda functions that share AWS SDK related code.\nRight now, we're writing a wrapper around AWS and AWSCognito so that we aren't exposing the globals everywhere. We're planning to test our client code by spawning a browser to run our tests. In some places, we're trying to mock the SDK so we can test without a broswer. We're planning to test the Lambda functions by using \"AWS\" as a global and creating a special testing deployment where we deploy all our functions in order to test, which works since the SDK is preloaded on Lambda. Lambda isn't really a CI service, though.\nAs with webpack, the issue revolves around expressions inside of calls to require. While this can be mitigated to an extent by using JSPM overrides, as I have done in my sample project, it only goes so far - you can't write an override to convince Node that \"AWS.APIGateway.prototype\" is defined when it tries to load lib/services/apigateway.js. I'm sure even if we got past that, there would be more errors.\nAs you said in the issue about Webpack, the problem is really that the SDK as it is written isn't particularly friendly toward typical ES6 workflows involving bundling and static dependency resolution. Without an overhaul of the SDK, I think our best path forward right now is just to load the SDK in <script> tags and try to hide the global behind a helper library.\n. @chrisradek Nevermind!\nI was able to put together an override that lets you import the AWS SDK into source files in a JSPM project. The pull request has been merged into the JSPM registry, but if the version changes here's the override that currently works for 2.4.0 and 2.4.2:\njson\n{\n  \"main\": \"dist/aws-sdk\",\n  \"format\": \"global\",\n  \"jspmNodeConversion\": \"false\",\n  \"shim\": {\n    \"dist/aws-sdk\": {\n      \"exports\": \"AWS\"\n    }\n  },\n  \"map\": {}\n}\nYou can install the override manually via the following command (which you should only use if it doesn't just work automatically):\njspm i npm:aws-sdk -o \"{main:'dist/aws-sdk',format:'global',jspmNodeConversion:'false',shim:{ 'dist/aws-sdk': { exports: 'AWS'}},map:{}}\"\nNOTE this override only works for the browser. If you want to use AWS SDK in a Node project with JSPM, look over this comment I wrote about it.\n. @chrisradek Nevermind!\nI was able to put together an override that lets you import the AWS SDK into source files in a JSPM project. The pull request has been merged into the JSPM registry, but if the version changes here's the override that currently works for 2.4.0 and 2.4.2:\njson\n{\n  \"main\": \"dist/aws-sdk\",\n  \"format\": \"global\",\n  \"jspmNodeConversion\": \"false\",\n  \"shim\": {\n    \"dist/aws-sdk\": {\n      \"exports\": \"AWS\"\n    }\n  },\n  \"map\": {}\n}\nYou can install the override manually via the following command (which you should only use if it doesn't just work automatically):\njspm i npm:aws-sdk -o \"{main:'dist/aws-sdk',format:'global',jspmNodeConversion:'false',shim:{ 'dist/aws-sdk': { exports: 'AWS'}},map:{}}\"\nNOTE this override only works for the browser. If you want to use AWS SDK in a Node project with JSPM, look over this comment I wrote about it.\n. ",
    "RWOverdijk": "This just working with jspm with be really useful. \ud83d\udc4d . This just working with jspm with be really useful. \ud83d\udc4d . ",
    "swickaphil": "Thank you @chrisradek for your response and direction.  The naming appears to have something to do with the issue since I am still able to create a bucket in the root directory without a problem.  \nThe variable I am using for the bucket I am trying to create (myNumberVariable in the previous example) is actually a path to a bucket within another bucket with a forward slash (/) separating the two names indicating a folder structure.  Whenever I try to create a new bucket within another existing bucket using the forward slash (ex: bucket/subBucket, the forward slash (/) gets replaced with the special character code %2F (ex: bucket%2FsubBucket).  \nThis previously worked successfully in v2.  I have tried escaping the forward slash in the bucket name with the same results.  Is there a different way to create a \"sub-bucket\" within an existing bucket using v4?\n. I thought buckets and folders were synonymous.  I'd be happy to use folders, though, if I can achieve the same results.\n. Is the only way to create folders by creating an object, or is there a way to create folders without the dependency of creating an object too?\nThanks again!\n. So now that I'm using putObject, I am getting an error for Missing credentials in config.\nCode:\ns3bucket.putObject({ Bucket: 'marcom-meb', Key: '2016/887/' }, function(err,data){\n                if (err) {\n                    console.log(\"Error creating object \" + emailId);\n                    console.log(\"Request\")\n                    console.log(this.request.httpRequest)\n                    console.log(\"Response\")\n                    console.log(this.httpResponse)\n                    console.log(\"Error Object\")\n                    console.log(err);\n                    console.log(\"Data Object\")\n                    console.log(data);\n                }else{\n                    console.log(\"Object \" + emailId + \" created.\");\n                    callback();\n                }\n            });\nRequest:\nHttpRequest {\n  method: 'POST',\n  path: '/',\n  headers: { 'User-Agent': 'aws-sdk-nodejs/2.4.2 darwin/v5.10.1' },\n  body: '',\n  endpoint: \n   Endpoint {\n     protocol: 'https:',\n     host: 's3-us-west-1.amazonaws.com',\n     port: 443,\n     hostname: 's3-us-west-1.amazonaws.com',\n     pathname: '/',\n     path: '/',\n     href: 'https://s3-us-west-1.amazonaws.com/',\n     constructor: { [Function: Endpoint] __super__: [Function: Object] } },\n  region: 'us-west-1' }\nResponse:\nHttpResponse {\n  statusCode: undefined,\n  headers: {},\n  body: undefined,\n  streaming: false,\n  stream: null }\nError Object:\n{ [TimeoutError: Missing credentials in config]\n  message: 'Missing credentials in config',\n  code: 'CredentialsError',\n  time: Tue Jun 28 2016 11:02:05 GMT-0700 (PDT),\n  originalError: \n   { message: 'Could not load credentials from any providers',\n     code: 'CredentialsError',\n     time: Tue Jun 28 2016 11:02:05 GMT-0700 (PDT),\n     originalError: \n      { message: 'Missing credentials in config',\n        code: 'CredentialsError',\n        time: Tue Jun 28 2016 11:02:05 GMT-0700 (PDT),\n        originalError: [Object] } } }\nAny ideas?  Thank you again!!\n. By environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.  I am also setting the region to \"us-west-1\" before initializing a new s3 instance.\nI've also tried through using AWS_CREDENTIAL_FILE to the csv file with AWSAccessKeyId and AWSSecretKey values.\n. I just tried using ~/.aws/credentials, and that seems to have worked... however I am dependent on using environment variable for my production setup... Did something change on 2.4.* with this?\n. Both process.env.PUBLIC_KEY and process.env.PRIVATE_KEY trace out as undefined using AWS_CREDENTIAL_FILE as an environment variable to point to a csv with credentials as well as directly using AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as environment variables.  Tracing out in the respective environment variables in the terminal is successful, though.\n. Yes, that's it.  I will look into it further.  I haven't tried pushing to my testing server yet, so it could be an issue with my local machine.\nThank you so much for all your help!  Much appreciated!\n. ",
    "brettbeatty": "I, too, would like to see reading of the config file in resolving the credential chain.  In my case, it's for automatic role assumption (using role_arn, source_profile, mfa_serial, etc.) via the config file.  I have a CLI that I wrote with this module, and I would like for users' credentials to resolve however they would with the AWS CLI.. ",
    "donnut": "@chrisradek Thanks for your explanation. There seems to be an upper limit to the partSize as well. I noticed that increasing the partSize and/or queueSize increases the memory requirements. If the partSize to chosen too big w.r.t. the available memory, things get very slow.\n. @chrisradek Thanks for your explanation. There seems to be an upper limit to the partSize as well. I noticed that increasing the partSize and/or queueSize increases the memory requirements. If the partSize to chosen too big w.r.t. the available memory, things get very slow.\n. ",
    "glenswift": "Yes, It setting region works for me from the very beginning. I just wanted to point you to this stuff, as I don't find anything relevant in doc. Thanks for explanation, feel free to close the issue.\n. ",
    "jhludwig": "ah.  ugh.  any idea if swf is going to add CORS support?  thanks.\n. ah.  ugh.  any idea if swf is going to add CORS support?  thanks.\n. ",
    "bisrael": "looking at our S3 console, no the bucket is just like bucket-desktop.\nMaybe the problem is more that the other methods accept the leading slash?\nVersion 2.4.3 from npm:\nnpm list | grep aws\n\u251c\u2500\u252c aws-sdk@2.4.3\n. Thanks for investigating.\nIf it helps, we are not altering the default config in any way other than to set up our credentials (code below).\n``` js\nfunction setupAws() {\n  aws.config.update(findAWSCredentials());\n}\nfunction findAWSCredentials() {\n  var aws_config_files = ['/.aws/credentials', '/.s3cfg', '/.aws/config'];\n  var awsSecrets = {};\nfor (var i = 0, l = aws_config_files.length; i < l; ++i) {\n    var fn = process.env['HOME'] + aws_config_files[i];\n    try {\n      var inistring = fs.readFileSync(fn, 'utf-8');\n      var awscreds = ini.parse(inistring);\n    } catch (e) {\n      // console.log(fn, 'errored:', e);\n    }\n    if (awscreds && awscreds.default) {\n      awsSecrets = awscreds.default;\n      break;\n    } else if (awscreds && awscreds['profile eb-cli']) {\n      awsSecrets = awscreds['profile eb-cli'];\n      break;\n    } else if (awscreds && ((awscreds.access_key && awscreds.secret_key) || (awscreds.aws_access_key_id || awscreds.aws_secret_access_key))) {\n      awsSecrets = awscreds;\n      break;\n    }\n  }\nreturn {\n    accessKeyId: awsSecrets.aws_access_key_id || awsSecrets.access_key,\n    secretAccessKey: awsSecrets.aws_secret_access_key || awsSecrets.secret_key,\n  }\n}\n```\n. ",
    "BehrensT": "To add to this issue: \nIt seems like if the bucket contains a '/' it is getting URL encoding and causing the SignatureDoesNotMatch code to get thrown. \nExample:\n```\ns3.getObject({\n    Bucket: 'logs.abc.com/vin',\n    Key:  \"WP0CB2A95BS754907.xml\"\n}, function(err, data) {\nif (err) {\n    console.log(\"Code:\", err.code);\n    console.log(\"Request:\");\n    console.log(this.request.httpRequest);\n\n} else if (data) {\n    console.log(\"data\", data);\n\n}\n\n});\n```\nOutputs:\nCode: SignatureDoesNotMatch\nRequest:\nHttpRequest {\n  method: 'GET',\n  path: '/logs.abc.com%2Fvin/WP0CB2A95BS754907.xml',\n}\nIf I remove the \"/vin\" for the bucket property and move the file I am looking for in the root of the bucket.  The SignatureDoesNotMatch error goes away and I get my data. \nThanks.\nEDIT:\nI see a comment by @chrisradek on a similar bug a few days ago that had this snippet: \ns3.putObject({Bucket: 'bucket', Key: 'my_folder/'}, function(err, data) { /* ... */ });\nAdding the folder to the key solved my issue. thanks a million!!\n. ",
    "mszlapa": "Thank you! \nYes the updateFunctionCode seems to be working.\n. ",
    "hellsan631": "Just an update. I've tried several things and it looks like it's an AWS caching issue.\nSetting s3DisableBodySigning: false makes all things signed, but the requests still fail after a certain amount of time. I also tried correctClockSkew: true thinking that the date had something to do with it, but unfortunately that didn't work either.\nThat lead me to believe the payload was being manipulated while waiting for an async call. We are sending the payload via a buffer, so I converted the buffer to a base64 string, created an immutable copy of the variable, but still the error SignatureDoesNotMatch persisted.\nThe goal of the function is to just upload a profile picture from an angular front-end (using ng-file-upload), in which I thought I was using some pretty boilerplate code.\nBasically, a user upload an image which is storied in-memory as a buffer, then the image is sent to tinify, resized, and compressed, then sent back to us and converted to a new buffer. This new buffer would be uploaded to AWS. It doesn't need to be signed, its just something I noticed with all of the failed transactions.\nLucky for us, tinify has AWS S3 built in (not using the SDK, but creating its own http API call), which we are using now to upload to AWS. This works just fine at the moment. If you have any more suggestions or need any more information, I would be glad to help in any way that I can to solve this.\n. ",
    "DerekOverlock": "Any updates on this? Ran into this exact issue today, would be great if TemporaryCredentials was handling the default provider credential chain in a consistent manner w.r.t the other SDK services.. ",
    "jthomerson": "Another \"me too\". I need to specify a profile that gets used to load the credentials that the temporary credentials provider can use to assume a role. For example:\njs\nvar params = {\n   Credentials: new AWS.SharedIniFileCredentials({ profile: argv.profile }),\n   RoleArn: argv['role-arn'],\n   SerialNumber: argv['mfa-serial'],\n   TokenCode: argv['mfa-token'],\n};\nAWS.config.credentials = new AWS.TemporaryCredentials(params);\nThen all my connections to AWS resources should use the temporary credentials provided by the assumed role, except for the calls to STS itself, which would use the credentials provided by the specified profile in my ~/.aws/credentials file (from the SharedIniFileCredentials provider).. Okay, an update: I actually was able to do this fairly easily. There was a problem - which I'll explain below. First, what I'm doing now (note that this is in a script intended to be run from the CLI).\n```js\n// set my master credentials to ones that read use the profile in the ~/.aws/credentials file\nAWS.config.credentials = new AWS.SharedIniFileCredentials({ profile: argv.profile });\nvar params = {\n   RoleArn: argv['role-arn'],\n   SerialNumber: argv['mfa-serial'],\n   TokenCode: argv['mfa-token'],\n};\n// now override the master credentials with temporary credentials that will use\n// STS.assumeRole. This credentials object will use the one set above as its\n// credentials for calling STS.assumeRole. It does this automatically.\nAWS.config.credentials = new AWS.TemporaryCredentials(params);\nstartupPromise = Q.ninvoke(AWS.config.credentials, 'refresh');\n}\nstartupPromise\n   .then(function() {\n      return runYourScript();\n   })\n   .done();\n```\nThe problem seems to be that TemporaryCredentials refreshes its credentials lazily and asynchronously, with no lock. So what was happening was this: when the script started running, several different processes were being made simultaneously. The first one might succeed, but then others would fail because they were all also refreshing, using the one-time code that I had passed in.\nBy calling refresh and making sure it completes before I start the script, I ensure that only one operation is using the one-time code, and the credentials are loaded from STS correctly before the script even starts.. @jeskew reported as #1664. Thanks!. Here's the same code as a zip. Unzip it and run npm install, then run node index.js with the args described above.\nprovetempcreds.zip\n. Here's the same code as a zip. Unzip it and run npm install, then run node index.js with the args described above.\nprovetempcreds.zip\n. Any update on this issue? @jeskew?. Any update on this issue? @jeskew?. One year later. Just checking on this issue. \nAlso @srchase I disagree that this is a feature request - to me this is a bug.. One year later. Just checking on this issue. \nAlso @srchase I disagree that this is a feature request - to me this is a bug.. ",
    "davidrissato": "I had a similar problem and I was getting the error message TypeError: Cannot read property 'masterCredentials' of null while I was trying to use sts.assumeRole from an EC2 that was using instance profiles.\nI could get rid of that strange error message doing this way:\nconst AWS = require('aws-sdk');\nconst Promise = require('bluebird');\n\nconst getCrossAccountTemporaryCredentials = Promise.coroutine(function* (accountNumber, roleName, roleSessionName) {\n    const masterCredentials = yield AWS.config.credentialProvider.resolvePromise();\n    const sts = new AWS.STS();\n    const assumeRoleParams = {\n        DurationSeconds: 3600,\n        RoleArn: `arn:aws:iam::${accountNumber}:role/${roleName}`,\n        RoleSessionName: roleSessionName\n    };\n    const assumeRoleRequestData = yield sts.assumeRole(assumeRoleParams).promise();\n    return sts.credentialsFrom(assumeRoleRequestData, masterCredentials);\n});\n\n. ",
    "jstewmon": "This should be resolved in the next release by #2175 using ChainableTemporaryCredentials.. I have a similar problem - the error-first callback signatures indicate that all parameters are required, but they should all be optional, since the error is optional.. I have a similar problem - the error-first callback signatures indicate that all parameters are required, but they should all be optional, since the error is optional.. I've worked around this problem a few times in the past by subclassing TemporaryCredentials and overriding loadMasterCredentials. I finally decided to try my luck with a PR, since there are not tests documenting any desired effects from the old behavior.. I've worked around this problem a few times in the past by subclassing TemporaryCredentials and overriding loadMasterCredentials. I finally decided to try my luck with a PR, since there are not tests documenting any desired effects from the old behavior.. @chrisradek any feedback?. @chrisradek any feedback?. This now fixes #1064.\nThe handling of masterCredentials appears to have become obsolete at some point. Removing it and removing the call to masterCredentials.get simultaneously addressed my original issue and enables support for the default CredentialProviderChain.. This now fixes #1064.\nThe handling of masterCredentials appears to have become obsolete at some point. Removing it and removing the call to masterCredentials.get simultaneously addressed my original issue and enables support for the default CredentialProviderChain.. @chrisradek thanks for the ACK.\nI diligently researched the old behavior, looking for the rationale behind the implementation, but I didn't find any specific use cases that it served. My supposition is that perhaps the handling of AWS.config changed over time, obviating some of the mechanics in TemporaryCredentials' configuration of the STS service instance.\nI'll look forward to your review. :-). @chrisradek thanks for the ACK.\nI diligently researched the old behavior, looking for the rationale behind the implementation, but I didn't find any specific use cases that it served. My supposition is that perhaps the handling of AWS.config changed over time, obviating some of the mechanics in TemporaryCredentials' configuration of the STS service instance.\nI'll look forward to your review. :-). bump @chrisradek. bump @chrisradek. I don't mind doing that. There might be a bigger opportunity to refactor the other refreshable credentials to use a common base class to DRY the concurrent refresh capability (if you're interested in that).\nBut, I don't see what the problem is - the way I've refactored the implementation should satisfy the use case you described b/c the intermediate credentials will be refreshed using the long-lived master credentials. Am I missing something?. If we do make a new class, the name I've been using in my projects is STSCredentials.. @chrisradek & @AllanFly120 , sorry for the long delay - I was busy with another project at work.\nI've updated this PR to implement a new ChainableTemporaryCredentials class as was suggested.\nI wanted to standardize the strategy for coalescing refresh calls for Credentials, so there is a second commit which does that.\nMost of the providers were simple to update and required no changes to their tests, but RemoteCredentials and SharedIniFileCredentials were a little tricky because they each had their own method of coalescing concurrent refreshes.. I'm not sure why the headless browser tests are failing. The log doesn't seem to be formatted cleanly - the assertion errors don't seem to apply to the test mentioned before them. When I run test/browser/runner.html in my browser, all the tests pass.\nAny tips on troubleshooting the failures?. I was able to figure it out - I hadn't fixed all the web identity tests to work with async execution.. @AllanFly120 Thanks for the prompt review! I think I've responded to all of your feedback except the one regarding the docstring of ChainableTemporaryCredentials. I've made a note to come back to that when I'm less tired. :-). @AllanFly120 thanks again for the first round of feedback. I finished addressing everything on Monday, so I wanted to check back in to affirm that I addressed everything and to see if there's anything else I can do to improve confidence in these changes.. @AllanFly120 @chrisradek I'm sure you've been busy with all the new launches around reInvent, but wanted to check in to see if there's any more feedback on this PR. Thanks!. @AllanFly120 thank you for taking the time to review again!\nI update the constructor docs and pruned a couple of extraneous refreshCallbacks assignments in derived Credentials classes.. @AllanFly120 the docs link for the new class 404s. Did I miss adding something?\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/ChainableTemporaryCredentials.html. @AllanFly120 thanks for following up! I rebased on master, so this should be ready to merge once the tests complete.. Caused by TranscribeService being omitted from browser_default.js?. @Gregjarvez did you mean to mention me?. Ah... thanks for fixing @AllanFly120!. @sargun according to AWS the IMDS should always provide credentials that are valid for at least 5 minutes.\nCan you elaborate on the behavior you're observing and how it differs from what you expect to happen?\nIf the metadata service is actually serving expired credentials, resulting in a failed service call using those credentials, what do you think the SDK should do? Try calling the metadata service again? Pass an error to the refresh callback?. The expired flag is just used to control when refresh is called, so I think its value is incidental to your scenario, since it's being set during a refresh.\nSo, the current behavior is that the credentials are fetched successfully, but that they may already be expired, which results in the service call that triggered the refresh to fail with expired credentials. But, you'd prefer that the expiration of the retrieved credentials be checked against the current time and have refresh fail with an error if the expiration is less than the current time.\nThe outcome will be the same (failed service request), but you'd get an error that indicates that the failure is due to the the refreshed credentials already being expired.\nIs that right?. The expired property is checked for every service request in Credentials.prototype.needsRefresh. I created a PR with the proposed change to fail the refresh if the metadata service provides expired credentials.. Thanks, @AllanFly120 I added the change entry :-). This was caused by #2451 and is resolved by #2456.. @lorengordon I think it would be better to have a light-weight protocol to facilitate an interactive credential_process (defined by whomever owns the credential_process interface). A library dumping to stderr for a handled exception would violate my expectations as a user.. @lorengordon Any stderr from the subprocess will be appended to the resulting Error.message property if the exit code is non-zero.\nFor example, if fail is an executable like so:\n```sh\n!/usr/bin/env bash\necho some stdout output\necho some stderr output > /dev/stderr\necho some other stderr output > /dev/stderr\nexit 1\n```\n...and we have a node script:\njs\nproc.exec('./fail', (err, stdout, stderr) => {\n  console.error('err.message:', err.message);\n});\nThen, we will get:\nerr.message: Command failed: ./fail\nsome stderr output\nsome other stderr output. I removed this because it appears unnecessary since refresh will call createClients.. AWS.config.credentials will still be used by the STS service instance if masterCredentials are not provided, but the value of AWS.config.credentials is no longer captured when the TemporaryCredentials instance is instantiated.\nSo, it's possible that this could be considered a breaking change if someone expected changes to AWS.config.credentials to NOT be reflected in the TemporaryCredentials instance.\njs\nnew AWS.CredentialProviderChain.resolve((err, creds) => {\n  AWS.config.credentials = creds;\n  AWS.config.credentials = new TemporaryCredentials({RoleArn: '...'});\n});\nThis example reflects the workaround advice given here: https://github.com/aws/aws-sdk-js/issues/1064#issuecomment-234008942. Unfortunately, it doesn't work with these changes.\nI think a reasonable compromise is to update this PR to use this.masterCredentials = masterCredentials || AWS.config.credentials. If AWS.config.credentials has not been set, the default behavior works as expected. If it has been set, it is captured as the masterCredentials and the example above still works.. AWS.config.credentials is now captured in the constructor if masterCredentials is falsy.. I noticed that someone else mentioned the refresh stampede issue in https://github.com/aws/aws-sdk-js/issues/1064#issuecomment-320279160, so I included it here and added a test for it.. The purpose is to ensure that the refresh method of any given instance of Credentials can be called concurrently by n async requests, resulting in 1 (not n) refreshes of that instance's properties.\nConsider this example:\njs\nvar creds = new RemoteCredentials();\nvar client = new AWS.S3({credentials: creds});\nfor (var i = 0; i < 10; i++) {\n  client.getObject({}, function () { });\n}\nIn that example, creds.refresh will be called 10 times - once for each of the requests because the credentials are lazily loaded asynchronously. Similarly, if the credentials were expired and multiple calls were concurrently triggered, the same \"stampede\" would occur.\nInstances of any credentials which refresh asynchronously should coalesce all concurrently requested refresh requests to a single loading of the new properties.\nSharedIniFileCredentials and RemoteCredentials previously implemented their own strategies of coalescing refresh requests to a single refresh.\nRemoteCredentials previously behaved as your comment suggests - all instances shared a refresh queue because the prototype property was statically initialized. That exposed a subtle bug - the constructor accepts parameters to be used when making the refresh request, but all instances of the class were coalesced together because of the statically initialized refreshQueue property. It's probably rare that anyone directly instantiates EcsCredentials or RemoteCredentials, so that issue has probably gone unnoticed, and the subtle change in behavior will also probably go unnoticed.\nWe're now assigning refreshCallbacks is assigned in the constructor, so each instance will have its own array, and only calls to an instance's refresh method are coalesced.. Sure! I added it to check my own work, but I may have fixed a few places in files I edited that weren't necessary in this PR - sorry if I muddied the waters.. Yeah, onLoad runs when the credentials have been loaded - it notifies all the pending callbacks from concurrent calls to refresh and resets the queue.. EcsCredentials and RemoteCredentials refer to the same class, but the same set of tests were created for both symbols. I replaced the EcsCredentials tests with a single test that asserts the symbol is the same as RemoteCredentials.\nThe online diff view is jacked up for me, so the actual changes might not be apparent.. The available providers are filtered by the if wrapping the test case, so the tests only run for classes which are properties of AWS. setImmediate is poly-filled by browserify, but I can swap it for AWS.util.defer (which will end up using the polly-fill) - I didn't use setTimeout because it is patched with a synchronous function when the tests run.. eslint updates in #2336. Pruned. I've updated the docs and added support for loading an MFA token if SerialNumber is in the params. I also refactored the constructor interface to take an options map instead of positional arguments to provide better continuity with the other credential classes.. I double-checked the diff in kaleidoscope (my preferred diff viewer), and it looks the way I intended, but please let me know if you find anything that doesn't look right.. Doh! Good catch. I've updated.. Is Expiration handled elsewhere?\nI expected to see something like the following here:\njs\nif (credData.Expiration) {\n  this.expireTime = new Date(credData.Expiration);\n}\nhttps://docs.aws.amazon.com/cli/latest/topic/config-vars.html#sourcing-credentials-from-external-processes. I know that some of the existing loaders are required to remain synchronous to avoid breaking existing applications, but since this is a new loader, is there a reason to make it synchronous? The SharedIniFileCredentials already loads asynchronously if the selected profile is for assume role credentials.. stderr is often used for ancillary information (like a deprecation warning) even when the command succeeds, so I would advise against treating a truthy value as a failure.\nFWIW, the [botocore implementation] ignores stderr.\nIf this is retained, I suggest passing AWS.util.error(new Error(stdErr), {code: 'SharedIniCredentialsProviderFailue'}), so that the callback receives a normalized Error, not a string.\n[botocore implementation]: https://github.com/boto/botocore/blob/8d3ea0e61473fba43774eb3c74e1b22995ee7370/botocore/credentials.py#L804-L833. parseInt will allow for a string and drop anything after the first character which is not a digit... It probably doesn't matter, but using parseInt here does make this implementation more lenient than the docs indicate is allowed.\nparseInt returns 1 for all of these inputs:\n1\n1.0\n1.5\n'1what'. I think the control flow affected in this PR can be:\njs\nif (profile['aws_access_key_id'] && profile['aws_secret_access_key']) {\n  // set props\n} else if (profile['credential_process']) {\n  // loadViaCredentialProcess\n} else {\n  throw AWS.util.error(\n          new Error('Credentials not set for profile ' + this.profile),\n          { code: 'SharedIniFileCredentialsProviderFailure' }\n        );\n}\nIt's a bit confusing the way the conditions are currently being checked in multiple places.. ",
    "JamesRimot": "Here is the npm-debug.log\nnpm-debug-log.txt\n. ",
    "eetee": "Hi @chrisradek,\nFirst of all thanks for replying, much appreciated.\nIt seems both methods are accepted: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Config.html#constructor-property\nNonetheless, I tried it again after your comment and still get the same 403 (Forbidden) Error: Access Denied\nDo you have any other suggestions for me to try?\nThanks\n. The issue was the ACL on the bucket itself an extra ACL rule was required for the STS to go through but not the regular IAM user for some reason. \nIt's not a aws-sdk-js issue so I won't go into detail here. I've posted my answer to the stackoverflow post: http://stackoverflow.com/questions/38473103/aws-sdk-upload-with-sts-credentials-403-error\n. ",
    "seventrust": "MAN!!! ty so much :D . ",
    "gangavh": "thanks a lot.. thanks a lot.. ",
    "ktersius": "Thanks, got it working. I had to downgrade to 2.4.7 though because headBucket was giving me Forbidden for some reason.\n. ",
    "thavaamm": "Hi ,\nI am getting \"OperationAborted: A conflicting conditional operation is currently in progress against this resource\" error. I just want to set expiry, is it enough?\nvar params = {\n    Bucket: 's3-bucket',\n    LifecycleConfiguration: {\n        Rules: [\n            {\n                Prefix: 'upload',\n                Status: 'Enabled',\n                Expiration: {\n                    Days: 1,\n                },\n                ID: 'my.test'\n            }\n        ]\n    }\n}\nversion: \n\"aws-sdk\": \"2.4.7\". ",
    "lukevanhorn": "I'm getting this same error in IE 11 on Win8.  \n**Edit -It's a CORS related error: \n\"Request header x-amz-user-agent was not present in the Access-Control-Allow-Headers list.\"\nThe browser makes an OPTIONS request to https://lambda.[region].amazonaws.com/2015-03-31/functions/[function name]/invocations.  Looking at the response headers, x-amz-user-agent is included in the Access-Control-Allow-Headers list, but it obviously is going wrong somewhere farther down the line...\n. Hi @LiuJoyceC,\nThanks so much for the follow-up.  That's great to hear.  Unfortunately, I'm still experiencing the same issue when I tested my site just now, is there a version of the SDK we should be using or is this a change to the Lambda service pre-flight response that somehow works around the IE bug?  \nFyi, I submitted a bug to the IE team:  https://developer.microsoft.com/en-us/microsoft-edge/platform/issues/8433404\nThanks,\nLuke\n. Hi @LiuJoyceC ,\nI'm using us-east-1.  @djabry was kind enough to help test my application and confirmed that the update hasn't been deployed to my region yet.  I'll keep an eye out for your update.\nThanks!\nLuke\n. Hi @mojojr, \nAs of yesterday, IE 11/Edge is working for me in us-east-1.  Big thanks to the Lambda team!\nLuke\n. ",
    "djabry": "Many thanks for the fix @LiuJoyceC! It now works for me.\nDaniel\n. ",
    "mojojr": "Has the fix been deployed to us-east-1 yet?  Seeing the same issues \nThanks\nBrendan\n. Yep, I can confirm.  \nThanks!\n. ",
    "niftylettuce": "Another way to do this in the meanwhile is to use es6-promisify at https://www.npmjs.com/package/es6-promisify.  Here's an example:\njs\nconst params = { ... }\nawait promisify(s3.upload, s3)(params);\n... or if you're not using async/await ...\njs\nconst params = { ... }\npromisify(s3.upload, s3)(params).then(fn).catch(fn);\n. Also, if you attempt to use promises with this and do not pass configuration object, it will throw an unhandled promise rejection!  Definitely a bug?\n(node:38374) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 2): MissingRequiredParameter: Missing required key 'Bucket' in params\n. Referring to SDK in library.\nOn Oct 17, 2016 3:42 PM, \"Joyce Liu\" notifications@github.com wrote:\n\nHi @niftylettuce https://github.com/niftylettuce\nFor your above comment about the unhandled promise rejection, are you\nreferring to when you use es6-promisify? Are you referring to when you\nuse the SDK promise support in the PR I submitted? I've tried to replicate\nthis but have not gotten any rejections thrown (it's expected for the\npromise to be rejected if not all required params are present, but it\nshouldn't be getting thrown). Could you provide the exact code you are\nusing to replicate this error? Thanks.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1076#issuecomment-254311807,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAf7hbREmEPLc0EU4ZQuLvROndbYBjOAks5q08-5gaJpZM4JWdZl\n.\n. Yes I know that, I'm saying that there's somewhere in the AWS SDK where there is a promise that is not wrapped with a try catch.  In my code I wrapped it with a try catch and STILL got an unhandled promise rejection.  I'm on latest Node, which is 6.6.x.\n. Yes it does but I had await in front of the promise..   I'll try again...\n\nOn Oct 17, 2016 8:23 PM, \"Joyce Liu\" notifications@github.com wrote:\n\nPromises don't work with try-catch blocks because they don't throw errors.\nThey reject the errors, and you'll need to add a catch handler to the\npromise to handle the error. The SDK's promises are designed to reject any\nerrors that would normally get passed into the error-first callback when a\ncallback is used instead of a promise. In the case of callbacks, if you\ndon't pass in the required params, an error will be passed into your\ncallback, and you as the user can decide how you want to handle the error\nin your callback. If you don't write explicit logic in your callback to\nhandle the error, then the error is silently swallowed (and the SDK cannot\ncontrol what happens in the user-supplied callback). Similarly for\npromises, it's up to the user to add a catch handler to handle rejected\npromises. If you don't add a catch handler, then it is analogous to\nreceiving the error in a callback but not writing any error-handling logic\nin the callback. The SDK cannot handle the error for you because then the\nuser will no longer be able to choose how to handle the error.\nThere are no unhandled promises somewhere in the internals of the SDK. The\nSDK does not generate promises anywhere unless you explicitly call the\n.promise() method on the AWS.Request class (and in the PR, a few\nadditional classes), in which case the method returns the raw promise to\nyou to handle as you see fit. If then and catch handlers are not added by\nthe user, then the behavior is consistent with any regular promises outside\nof the SDK, which is that rejections and resolutions get swallowed\nsilently. Does that make sense?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1076#issuecomment-254372552,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAf7hcaHMJB93tTzJ8YkE-1JoQq2vpb_ks5q1BGOgaJpZM4JWdZl\n.\n. This is still an issue though.  The following code will result in an unhandled promise rejection.  There is definitely a core bug in the AWS-SDK.\n\njs\ntry {\n  const s3 = new AWS.S3({}); // of course you put config here\n  const data = await s3.upload().promise(); // of course you'd put an object in upload as arg\n} catch (err) {\n  throw err;\n}\nResults in the following:\nlog\n(node:28391) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 1): Error: params.Body is required\n(node:28391) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.. cc @LiuJoyceC @MrHubble . +1 please accept PR\n. Np I think this is a bug as described with my usage of promises and async.\nYou can close it.\nOn Jul 27, 2017 8:58 PM, \"Jonathan Eskew\" notifications@github.com wrote:\n\nWhile we're investigating if this is a bug in the SDK or an unrelated\npromise rejection in code calling the SDK, I'm going to remove the CRITICAL\nBUG text from the issue title.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1650#issuecomment-318527095,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAf7hZKJqjClf9ZeFcc8BpISzEjxOFNdks5sSTJDgaJpZM4Ol6Pr\n.\n. Sorry about that, one of those days \ud83e\udd26\u200d\u2642\ufe0f . \n",
    "MrHubble": "As an update, I believe promises are now supported on ManagedUploads so upload can be used instead of putObject if so desired: http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3/ManagedUpload.html#promise-property. One last response and then I'll leave it for you to discuss in https://github.com/aws/aws-sdk-js/issues/1650 as suggested by @jeskew \n@niftylettuce I am using code similar to you and am not getting an error:\n````js\nvar AWS = require('aws-sdk/dist/aws-sdk-react-native');\nAWS.config.update({\n  accessKeyId: myKeyId,\n  secretAccessKey: mySecretKey,\n  region: myRegion\n})\nvar s3 = new AWS.S3() \nvar params = {\n  Bucket: \"mybucket\",\n  Key: myKeyPrefix,\n  Body: myBody,\n  ContentType: 'image/png',\n  ContentEncoding: 'base64',\n  ACL: 'public-read'\n}\nlet putObjectPromise = await s3.upload(params).promise()\nlet location = putObjectPromise.Location\n````\nGood luck.. @ChristophRob are you able to share what solution you used? Cognito seems too feature rich to me if you simply want to allow all users to upload photos to the same bucket on S3. . ",
    "arvi": "@MrHubble if there is an error in the s3.upload().promise(), how would you handle it on your implementation?. ",
    "bermann": "@arvi You could use try catch block. ",
    "wryun": "Not clear to me why build failed (only on Node 4.2). Seems unrelated:\nhttps://travis-ci.org/aws/aws-sdk-js/jobs/147840412\n. Hey, is there anything I can do to encourage this to be merged? ;)\n. Great, thanks! Appreciate it.\n. This has messed up our mocking of the AWS SDK (which relies on... some messiness):\nhttps://gist.github.com/wryun/347283d8e715afc4f904ffe5838c536c\nDo you have any suggestions for how to do this in a nicer way?. ",
    "rightaway": "+1 I've tested locally and it works for me.\n. +1 I've tested locally and it works for me.\n. ",
    "merlinpatt": "Yeah my stack trace was generated with 2.3.8. I'm running node 4.4.7.\nI can verify that response.content is in fact a string (using typeof). And that buffer is Buffer (using .constructor since typeof just returns object)\n. Yeah my stack trace was generated with 2.3.8. I'm running node 4.4.7.\nI can verify that response.content is in fact a string (using typeof). And that buffer is Buffer (using .constructor since typeof just returns object)\n. Turns out the issue was that I was using Buffer for the browser and not the builtin one. I don't see why that should cause a problem since it's supposed to be the same but that's what it was.\nThanks for your help.\n. Turns out the issue was that I was using Buffer for the browser and not the builtin one. I don't see why that should cause a problem since it's supposed to be the same but that's what it was.\nThanks for your help.\n. ",
    "roo2": "Thanks I also had the issue of buffer for the browser overwriting the nodejs default buffer. In my case I was using electron and webpack and was able to solve the issue by adding buffer to webpack externals https://webpack.js.org/configuration/externals/\n  . ",
    "JakubMatejka": "It looks that I can't write optional standard attributes neither. When I try to set profile or locale in UserAttributes I get the same error as with the custom attribute.\n. It looks that I can't write optional standard attributes neither. When I try to set profile or locale in UserAttributes I get the same error as with the custom attribute.\n. Oh I totally overlooked this setting of write permissions per client. Thanks a lot Chris.\n. Oh I totally overlooked this setting of write permissions per client. Thanks a lot Chris.\n. ",
    "crivera": "hi ... where do i set the write permissions? in IAM?. ",
    "benhulan": "@crivera Write permissions can be set in your AWS Cognito users console. Select your user pool, then go to Apps. Select your app (if you have more than one), click the \"Show Details\" button and you will see the link to \"Set attribute read and write permissions\". ",
    "lunikernel": "I blew several hours on this - it should be clearly explained in the documentation that these permissions need to be set.  . ",
    "dm-grinko": "AWS Console > User Pool > General settings > App Clients > Show details > Set attribute read and write permissions. \nMark your custom attributes.\nHope it saves time for someone.. ",
    "thatshailesh": "@dm-grinko  Even with permissions set in app clients, I am getting this error \n. ",
    "jplew": "\nfor the visual types out there.... ",
    "annjawn": "Wow.... Then this document is completely misleading... - https://aws.amazon.com/blogs/mobile/aws-amplify-adds-support-for-custom-attributes-in-amazon-cognito-user-pools/  it doesn't even mention the custom: prefix part. I followed this doc and kept getting the error like @JakubMatejka and prefixing it solved the issue. The only thing is that the Object format can be much simple like below instead of the \"Name\" \"Value\" type pairs-\n'attributes': {\n        'email': 'me@domain.com',\n        'custom:favorite_flavor': 'Cookie Dough'  // custom attribute, not standard\n        'custom:age': 25             // custom attribute, not standard\n    }. @YaswanthC Auth.currentUserInfo(); returns a Cognito User object in this format-\n{\n  id: '2b3d-6da3-852b-ac1c5dc45dcd',\n  username: 'testUser@email.com',\n  attributes: {\n    \"email\" : \"testUser@email.com\",\n    \"custom:favorite_flavor\" : \"Peach\",\n  }\n}\nso you will have to do this-\nconst { attributes } = await Auth.currentUserInfo();\nconst flavor = attributes['custom:favorite_flavor'];. ",
    "YaswanthC": "You comment really helpful @annjawn :) . Do you have any idea how to extract the field , in the documentation \ntry {\n  const { favorite_flavor } = await Auth.currentUserInfo();\nalert(\"hi\" + favorite_flavor)\n} catch (err) {\n  console.log('error fetching user info: ', err);\n}\nWhere in the const do i need to replace favorite_flavor with custom:favorite_flavor because i tried both but i didn't get any value.\nThanks\n. ",
    "rickydo": "\nWow.... Then this document is completely misleading... - https://aws.amazon.com/blogs/mobile/aws-amplify-adds-support-for-custom-attributes-in-amazon-cognito-user-pools/ it doesn't even mention the custom: prefix part. I followed this doc and kept getting the error like @JakubMatejka and prefixing it solved the issue. The only thing is that the Object format can be much simple like below instead of the \"Name\" \"Value\" type pairs-\n'attributes': {\n        'email': 'me@domain.com',\n        'custom:favorite_flavor': 'Cookie Dough'  // custom attribute, not standard\n        'custom:age': 25             // custom attribute, not standard\n    }\n\nHoly Canoly!\nI spent so many hours pulling my hairs out wondering why I kept getting the unauthorized message when the user pool had the appropriate permissions; adding in the 'custom' part solved it. @annjawn You are right! I followed the same thing.. > Wow.... Then this document is completely misleading... - https://aws.amazon.com/blogs/mobile/aws-amplify-adds-support-for-custom-attributes-in-amazon-cognito-user-pools/ it doesn't even mention the custom: prefix part. I followed this doc and kept getting the error like @JakubMatejka and prefixing it solved the issue. The only thing is that the Object format can be much simple like below instead of the \"Name\" \"Value\" type pairs-\n\n'attributes': {\n        'email': 'me@domain.com',\n        'custom:favorite_flavor': 'Cookie Dough'  // custom attribute, not standard\n        'custom:age': 25             // custom attribute, not standard\n    }\n\nHoly Canoly!\nI spent so many hours pulling my hairs out wondering why I kept getting the unauthorized message when the user pool had the appropriate permissions; adding in the 'custom' part solved it. @annjawn You are right! I followed the same thing.. ",
    "eibay": "Thanks to this thread, it helps me resolve the same issue with regards to custom attribute. My additional note would be to make sure you are setting the right 'app client' for read / write permissions. \nThe one that I am having difficulty now is getting my required standard attribute such as 'family_name' and 'given_name'  to show up which is weird. In spite of all the changes needed above, this time around, it's the standard attribute that is not showing up. Even from the user's detail page at cognito pool, I couldn't see it even though I know that there's value because I can signup on my form without any issue.\n. ",
    "pranavram-n": "I'm facing a very similar issue, and I suspect it could be related to this one.\nI'm trying to access DynamoDB from a Lambda that's inside a VPC, and it times out. Works fine if I remove the VPC, though. \nWould appreciate it if you could share what caused this issue and how it was solved.\nThanks. ",
    "spbrds": "I'm new on AWS and I had a similar issue also, but it was not related with the VPC, tried to follow the tutorial that benoittgt gave, but it didn't work.\nThe lambda timed out on the instatiation of the DynamoDB Client:\nAmazonDynamoDB client = AmazonDynamoDBClientBuilder.standard().build();\n  DynamoDB dynamoDB = new DynamoDB(client);\nIt turns out that my lambda had only 128 MB of Memory allocated to the lambda. Once I passed it to 512MB it worked.\nThis guy saved my life:\nhttps://medium.com/@CodingJoe/dealing-with-dynamodb-write-capacity-limits-and-lambda-timeouts-f4e08d9f4b4f\nThanks. ",
    "hustshawn": "@spbrds Yes. It works when increase the default memory from 128MB to 256MB, but it is really weird that the log says the Max Memory I have used is just 33MB. Quite annoying and painful with no clue about the lambda Logs.. ",
    "whgibbo": "This also helped me.. I have to increase from 128 to 512MB to work with a Java8 Lambda...  Again no error that I could see..  Very strange. . This also helped me.. I have to increase from 128 to 512MB to work with a Java8 Lambda...  Again no error that I could see..  Very strange. . ",
    "mcprostar205": "I open #1601 on a similar behavior was there a resolution to this issue?. @chrisradek\nReally appreciate the thorough analysis for this issue.\nRE: 1) If I listen to error on the http event stream, what would I do to prevent the exception from being thrown? Or, does #1549 address this issue?\nRE: 3) Yes, I meant \"finish\" :-) ...thanks for the catch....sorry.\nThat's great if #1549 may fix the issue on v6.10.3. I'll perform a test in development with the latest AWS SDK and report back. \nMy production environment is on v6.10.3 LTS. We won't be moving production to 7 or 8 in the near future. Are you looking for me to confirm the issues is resolved on node 8 if #1549 didn't resolve this issue to confirm?. @chrisradek\nLooks like aws-sdk v2.82.0 with #1549 resolves the crash issue for node v6.10.3.  Listening to the S3 stream 'error' emits the timeout but no longer causes the node exception that terminates the process.\nTimeoutError: Connection timed out after 120000ms\n    at ClientRequest.<anonymous> (/../node_modules/aws-sdk/lib/http/node.js:83:34)\n    at ClientRequest.g (events.js:292:16)\n    at emitNone (events.js:86:13)\n    at ClientRequest.emit (events.js:185:7)\n    at TLSSocket.emitTimeout (_http_client.js:629:10)\n    at TLSSocket.g (events.js:292:16)\n    at emitNone (events.js:86:13)\n    at TLSSocket.emit (events.js:185:7)\n    at TLSSocket.Socket._onTimeout (net.js:338:8)\n    at ontimeout (timers.js:386:14)\n  message: 'Connection timed out after 120000ms',\n  code: 'NetworkingError',\n  time: 2017-07-08T15:59:56.137Z,\n  region: '<region>',\n  hostname: 's3-<region>.amazonaws.com',\n  retryable: false,\n  statusCode: 206,\n  retryDelay: 41.75920457979259\nA big thank you to you and the AWS team for your professionalism in working with customers to understand and help resolve issues.. ",
    "clakech": "You are right @chrisradek, the sample URL was not the good one. I edited the link. \nI would like to know if we could use libraries like multer and multer-s3 to upload client side encrypted files with multipart  (https://github.com/badunk/multer-s3) \n. ",
    "bikeath1337": "@clakech https://github.com/TTLabs/EvaporateJS supports creating MD5 digests and a bunch of other features; is efficient; and works with node.js, plain old browsers and can also be used with Electron apps.. ",
    "vfxBoat": "We are using the s3.upload() method. The files we are uploading are ~9.4mb each. We are using body = fs.createReadStream(localFile) for the body param.\nWe are monitoring the upload via 'httpUploadProgress' event. By \"all data is sent\" I mean that the 'httpUploadProgress' event loaded value and httpUploadProgress total value are the equal.\nBy \"upload finishes\" I mean the moment the send event is triggered and we recive the {Location: '....', Bucket: '....', Key: '...', Etag: '\"....\"'} object.\nOur code look something like this: \ns3.upload( {Key: '...', Body: fs.createReadStream(file)} )\n.on('httpUploadProgress', function (evt) {\n   task.loaded = evt.loaded\n   task.total = evt.total\n   task.part = evt.part\n})\n.send(function (err, data) {\n    if (err ) {\n        code handling request\n    }else{\n       code handling complete upload\n    }\n}\nBy our answer, it seems that the delay between the all the body data is sent and the callback triggers is a normal thing.\nThe thing is that in our frontend, the upload seems as completed (because the progress bar is full) but it frezzes at 100% waiting for the callback to trigger.\n. ",
    "denisw": "For reference, this is a copyObject call that changes the content type of an object:\njs\ns3.copyObject({\n  Bucket: bucket,\n  Key: key,\n  CopySource: `${bucket}/${key}`,\n  ContentType: newContentType,\n  MetadataDirective: 'REPLACE'\n}). ",
    "ArthurRM": "Similarly it is equally confusing when you just want to update the metadata and thus pass  MetadataDirective = REPLACE but then you lose the original ContentType if you don't explicitly set that in the copy again as well.. ",
    "jakubzitny": "Our use-case is not that space-sensitive as IoTs or Lambdas, but we're counting each MB as well. We have an Electron app (with Node included) and we're uploading files to S3.\n. @AdityaManohar really? Can you link related MR, commit, docs page or something? I looked into Readme and Changelog and couldn't find anything. Thanks.\n. @chrisradek Sorry for late response, thanks for the info and for implementing this.\nI don't really think it matters, but if it's any help to you, we ran this from renderer and we've alse been thinking about creating a special hidden BrowserWindow with this. Using webworkers or calling it from main process is also an option.\n. ",
    "mechanicals": "@chrisradek Although you said that I could Import each service singularly.\nI am trying to use AWS Javascript SDK, but I am getting some errors.\nI am using the latest version of AWS SDK & using Webpack to build.\nI am not able to use\nimport S3 from 'aws-sdk/clients/s3';\ngives me S3 undefined.\nInterestingly this works perfectly\nimport AWS from 'aws-sdk';\nconst S3 = new AWS.S3();\nBut I only want to import S3 not the whole AWS SDK. Any ideas what am i doing wrong.. @chrisradek Although you said that I could Import each service singularly.\nI am trying to use AWS Javascript SDK, but I am getting some errors.\nI am using the latest version of AWS SDK & using Webpack to build.\nI am not able to use\nimport S3 from 'aws-sdk/clients/s3';\ngives me S3 undefined.\nInterestingly this works perfectly\nimport AWS from 'aws-sdk';\nconst S3 = new AWS.S3();\nBut I only want to import S3 not the whole AWS SDK. Any ideas what am i doing wrong.. ",
    "bootrino": "I really wish all the ways of importing and using this SDK were documented in one single place, including small but complete examples.\n. ",
    "cantuket": "Googles doing it right...\nconst { Storage } = require('@google-cloud/storage');\nIronically the aws-sdk is what put us over the 262144000 bytes limit on Lambda after replacing S3 with GCStorage.\n. Googles doing it right...\nconst { Storage } = require('@google-cloud/storage');\nIronically the aws-sdk is what put us over the 262144000 bytes limit on Lambda after replacing S3 with GCStorage.\n. @patgoley Oh wow, that's game changing for us since we're trying to cram multiple binaries on there. Thanks for pointing this out! Don't how we didn't figure that out by now :/\n@srchase That's great to hear for use in the rest of our environments! Love the typescript decision as well.. @patgoley Oh wow, that's game changing for us since we're trying to cram multiple binaries on there. Thanks for pointing this out! Don't how we didn't figure that out by now :/\n@srchase That's great to hear for use in the rest of our environments! Love the typescript decision as well.. ",
    "patgoley": "@cantuket The Lambda execution environment comes with the aws-sdk pre-installed, so no need to include it in your bundle. You can install it in devDependencies to test locally. \nSee here:\nhttps://docs.aws.amazon.com/lambda/latest/dg/current-supported-versions.html. ",
    "austinkelleher": "+1 seeing this issue too. cc @chrisradek \n. ",
    "n2liquid": "Hi, Chris. On my home computer (Debian 9) the behavior was as I explained (you got it right). On my office computer (Debian 8), I just found out that 2.5.0 is working fine, and the problem was a proxy configuration. I was able to get it working using proxychains.\nI will retest this on my home computer later tonight because that makes no sense. My home network is not behind a proxy.\nI see here what looks like an official way of getting aws-sdk-js to work behind a proxy.\nThat said, it's weird that s3.upload never times out, so maybe that's the real bug here:\n```\n\nconst AWS=require('aws-sdk')\nAWS.config.region = 'us-west-2';\nconst s3 = new AWS.S3({params: {Bucket: 'myBucket'}});\nvoid s3.upload({ Key: 'test', ContentType: 'text/plain', Body: 'hello' }, console.log)\nconsole.time()\nconsole.timeEnd()\nundefined: 247350.091ms\n```\n\nShould I create a separate issue for that?\n\nBtw:\n\nDoes an S3 object get created for you despite the callback never getting called?\n\nYes:\n```\n\ns3.upload({ Key: 'test', ContentType: 'text/plain', Body: 'hello' }, console.log)\nManagedUpload {\n  _events: {},\n  body: ,\n  sliceFn: [Function: slice],\n  callback: [Function: bound ],\n  parts: {},\n  completeInfo: [],\n  fillQueue: [Function: fillBuffer],\n  partSize: 5242880,\n  service: \n   Service {\n     config: \n      Config {\n        credentials: [Object],\n        credentialProvider: [Object],\n        region: 'us-west-2',\n        logger: null,\n        apiVersions: {},\n        apiVersion: null,\n        endpoint: 's3-us-west-2.amazonaws.com',\n        httpOptions: [Object],\n        maxRetries: undefined,\n        maxRedirects: 10,\n        paramValidation: true,\n        sslEnabled: true,\n        s3ForcePathStyle: false,\n        s3BucketEndpoint: false,\n        s3DisableBodySigning: true,\n        computeChecksums: true,\n        convertResponseTypes: true,\n        correctClockSkew: false,\n        customUserAgent: null,\n        dynamoDbCrc32: true,\n        systemClockOffset: 0,\n        signatureVersion: 's3',\n        signatureCache: true,\n        retryDelayOptions: [Object],\n        useAccelerateEndpoint: false,\n        params: [Object],\n        constructor: [Object],\n        getCredentials: [Function: getCredentials],\n        update: [Function: update],\n        loadFromPath: [Function: loadFromPath],\n        clear: [Function: clear],\n        set: [Function: set],\n        keys: [Object],\n        extractCredentials: [Function: extractCredentials],\n        setPromisesDependency: [Function: setPromisesDependency] },\n     endpoint: \n      Endpoint {\n        protocol: 'https:',\n        host: 's3-us-west-2.amazonaws.com',\n        port: 443,\n        hostname: 's3-us-west-2.amazonaws.com',\n        pathname: '/',\n        path: '/',\n        href: 'https://s3-us-west-2.amazonaws.com/' },\n     _clientId: 2 },\n  totalBytes: 5,\n  failed: false,\n  partPos: 5242880,\n  isDoneChunking: true,\n  numParts: 1,\n  totalPartNumbers: 1 }\n```\n. \n",
    "Cactucs": "I'm having same/similar issue. I'm uploading a bunch of files (~350 files) to S3 bucket. I call putObject for all of them at once and than wait until the callback calls back. Sometimes it happens stright away. But sometimes I have to wait some time until something happends. Then after some time some files (cca. 20 to 50) are uploaded to bucket and their callback is called. This happends several times until all whole upload is compeleted. \n. I'm having same/similar issue. I'm uploading a bunch of files (~350 files) to S3 bucket. I call putObject for all of them at once and than wait until the callback calls back. Sometimes it happens stright away. But sometimes I have to wait some time until something happends. Then after some time some files (cca. 20 to 50) are uploaded to bucket and their callback is called. This happends several times until all whole upload is compeleted. \n. ",
    "borisirota": "To clarify, I'm using it with the amazon-cognito-identity-js where the session tokens are being cached.\n. Hi @LiuJoyceC thanks for your quick reply.\nBasically this is a request for the aws-sdk :)\nWe are talking about 2 different tokens and both needed.\nThe amazon-cognito-identity-js is used as a 3rd party identity provider so once the user is authenticated with such a service, he should pass the identity provider token to the CognitoIdentityCredentials so the getCredentialsForIdentity call will get access token to the AWS services.\nThe thing is that the token received from the getCredentialsForIdentity is valid for one hour but stored in memory only, so every page refresh within the validity period of the token requires a new call to the aws servers in order to get new access token (even though the old one could be used if it was cached and a network call could be avoided).\nThis is a drawback in the user experience in use cases where the authentication is client side only (e.g. static site) and every reload requires network calls which can be avoided if a cache was used. There is no issue when using this sdk on server side so my request is for client side only (Local Storage)\nThe login flow I'm using is client side :\n1. login to 3rd party identity provider (facebook, google, cognito user pool, etc.) - this sdks (at least the amazon-cognito-identity-js)  caches the tokens and makes a network call only if the token is invalid.\n2. use the token from step 1 and pass it to CognitoIdentityCredentials in order to get token to access aws services - no cache and network call always happens.\nIf there was cache in both steps there was no need for network calls for logged in users within the validity period of the tokens and this could improve the user experience.\nThanks\n. ",
    "lrettig": "I'm having a similar issue. Is there a reasonable workaround in the meantime? Can we easily cache the AWS credentials in local storage and, upon reload, check if they are still valid, and reuse them if so without the additional network call?. Here are some more helpful threads I turned up while researching this:\n\nhttp://stackoverflow.com/questions/32612530/how-to-persist-cognito-identity-across-pages-in-browser\nhttp://serverlessarchitecture.com/2016/12/06/persist-aws-cognito-credentials-between-browser-page-loads-to-be-used-with-aws-apigateway/\nhttps://stormpath.com/blog/where-to-store-your-jwts-cookies-vs-html5-web-storage\nhttps://forums.aws.amazon.com/thread.jspa?threadID=179420\n\nI suppose this is not an issue for an SPA, unless/until the user hits reload or back or something. Another option may be to store the creds using a query string so they persist (ex.: http://stackoverflow.com/a/38986143/2397068).\nThanks :). ",
    "crawfobw": "The aws-sdk version that I got the error with was version 2.4.10. Calling the SNS publish() function generated this error.\nReinstalling this version also corrected the error however. \nUnfortunately I do not believe I can reproduce the error and log the http response body as I did not keep the aws-sdk node module in version control. I wish I could help here! I'll be sure to log the response should the error occur again.\n. We recently reproduced the error and discovered that it is most like a proxy issue. I will follow up if something changes.\n. When we encountered this error, we determined that it was caused by the proxy returning an html page describing a proxy error.\nWe ended up solving the issue by including proxy credentials in the proxy url e.g proxy('http://{username}:{password}@proxyserver')\nThis may or may not solve your issue, it really is more dependent on how the proxy you are using is configured if your error has the same root cause.\n. ",
    "jarjis": "@crawfobw i'm seeing the same issue with ec2 calls  but not with s3 calls. I'm using \n```\nAWS.config.update({\n    httpOptions: { agent: proxy('http://proxyserver') }\n}); \n```\nCan you please share how did you resolve it. \n. ",
    "shirbr510": "I know that this is an ancient issue but I'm currently experiencing this when trying to work with a 3rd party mock server of s3.\nmy aws-sdk version is 2.288.0.\nit seems like  this.response.httpResponse.body.toString() doesn't produce the xml and I'm trying to understand what is wrong with my request.. ",
    "GabiAcea": "I got the same issue with an xml containing an '&': Some tags wich have \"example&Value\". I removed the '&' end up with \"exampleValue\"and all worked well. . ",
    "seriousben": "We've seen the same issue on us-east.\n. ",
    "wuseal": "The same, Creat url with expiration with 7 days, and expired about one hour later when using v2 sign.. @chrisradek Hi, I have just test, The valid period is about 15 minutes, And what the 15-minute meaning?. ",
    "Rhysjc": "+1\nThe docs are incredibly misleading since they show the retryDelayOptions field in the config object for the DynamoDB service. Seems ridiculous that EVERY service supports this apart from DynamoDB.. ",
    "Shadowblazen": "Tested and confirmed that the retryDelayOptions configuration now works as documented in aws-sdk v2.31.0. This ticket can probably be closed. Thanks!. ",
    "pahud": "Hi @LiuJoyceC \nIt works!  Thank you so much !\n. ",
    "paulxtiseo": "@NotBobTheBuilder, I'm not sure I understand the issue. What is the appropriate way to set expiration when using getSignedUrl()? I just started using this library, and do include an Expires property in the params passed to getSignedUrl(), so are you saying this results in a URL that never expires?\n. Ah, ok, that clears it up. Thanks @NotBobTheBuilder!\n. ",
    "riteshatsencha": "Updating the SDK has resolved the issue. Thank you!\n. Updating the SDK has resolved this issue. \n. Updating the SDK version has resolved the issue. Thx.\n. I can try with the SDK and let you know. Where exactly do I report issues with the console?. Thank you. What is the response time? . Yeah, so I am a basic support client. Therefore, can't really contact technical support and if I simply submit a feedback then I have to pretty much \"write the whole story\" without images. \nAs a client I saw an issue and thought should be fixed: whether it's with an API or UI. Isn't it AWS's responsibility to communicate the issue among their teams?\nStumped!. Thank you. Reported via feedback and put the link to this issue. Hopefully someone will take care of it.. Oh here is the link to the issue on support forum.. @jeskew thank you! For now, I made it mimic waitFor but would def. be on a lookout for the waiter resources through api. . ",
    "berlund": "Wrong release\n. ",
    "xiao": "Thanks, @chrisradek. Very helpful!\nI will leave this issue open in case you want to track the doc change.\n. Thanks, @chrisradek. Very helpful!\nI will leave this issue open in case you want to track the doc change.\n. ",
    "bknill": "Thanks I'm trying that, I do have a slow connection however I can upload these files via the browser or cloudberry relatively quickly! \nIt's currently still dropping back to 11%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 917504, totalSize: 6171107, lengthComputable: true\u2026} 917.504 KB 6.171 MB 14%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 1163264, totalSize: 6171107, lengthComputable: true\u2026} 1.163 MB 6.171 MB 18%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 1376256, totalSize: 6171107, lengthComputable: true\u2026} 1.376 MB 6.171 MB 22%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 1654784, totalSize: 6171107, lengthComputable: true\u2026} 1.655 MB 6.171 MB 26%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 1884160, totalSize: 6171107, lengthComputable: true\u2026} 1.884 MB 6.171 MB 30%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 2113536, totalSize: 6171107, lengthComputable: true\u2026} 2.114 MB 6.171 MB 34%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 2342912, totalSize: 6171107, lengthComputable: true\u2026} 2.343 MB 6.171 MB 37%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 2342912, totalSize: 6171107, lengthComputable: true\u2026} 2.343 MB 6.171 MB 37%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 688128, totalSize: 6171107, lengthComputable: true\u2026} 688.128 KB 6.171 MB 11%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 917504, totalSize: 6171107, lengthComputable: true\u2026} 917.504 KB 6.171 MB 14%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 1146880, totalSize: 6171107, lengthComputable: true\u2026} 1.147 MB 6.171 MB 18%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 1359872, totalSize: 6171107, lengthComputable: true\u2026} 1.36 MB 6.171 MB 22%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 1589248, totalSize: 6171107, lengthComputable: true\u2026} 1.589 MB 6.171 MB 25%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 1589248, totalSize: 6171107, lengthComputable: true\u2026} 1.589 MB 6.171 MB 25%\nawsController.js:50 Progress: XMLHttpRequestProgressEvent {isTrusted: true, part: 1, position: 688128, totalSize: 6171107, lengthComputable: true\u2026} 688.128 KB 6.171 MB 11%\n. Setting the Queue size to 1 fixed it\nqueueSize: 1,\n. ",
    "usamamashkoor": "@bknill how setting it to 1 fixed it i have the same problem but still facing the same issue.\nThanks. @stevedomin can you please share which approach you have used.\nThanks.. @stevedomin  and can you please explain it a little bit how did you manage to do that. \nThanks.  . Hi @jeskew Thanks for the reply.Can you please suggest how i can upload files to s3 bucket using JavaScript SDK and can also show user the progress bar during the file upload and note please i am uploading video files thanks.It will be really helpful thanks in advance.  . Thanks @jeskew .. @chrisradek thanks for replying me my connection is 4 MB and downloading speed is 500kbmy s3 file upload functions works for files under 10 MB mean the function works for smaller files but it does not work for files larger than 10 MB which also means the s3 request are working correctly but they are not working correctly for larger files.  .  \nThanks for tip about Fs.readfile i will try it and will let you know. \nThanks again for the reply.   . I think you will have to use queue mean upload file in chunks of 5 mb . You can open 4 queues.. ",
    "Jovons": "Then why createbucket is in the JSSDK? How is it used?\n. Then why createbucket is in the JSSDK? How is it used?\n. Thanks! that helps.\n. Thanks! that helps.\n. ",
    "Masterdanielsan": "Thank you.  I think it should be marked as a bug, unless you can find a workaround.  Polyfills don't seem to be fixing this on my machine.  Can it be confirmed?\n. Sorry it is a buffer.\nline #9868 var zeroBuffer = new Buffer(intSize); zeroBuffer.fill(0);\nI have IE11 and in compatability mode for IE10 it says:\nObject doesn't support property or method 'fill'\n. ",
    "lennym": "Note: this does not seem to be replicated in the cli:\nbash\n$ aws lambda list-functions --max-items 100 | jq '.Functions | length'\n51\n. @LiuJoyceC Thanks for your response.\nTo be honest, the behaviour of the JS SDK isn't really a problem for me, it's reasonably obvious when you see the NextMarker property what has happened, and while it would be good to be documented, it's not a blocker.\nThe real issue is the manifestation of the same limit in the Web UI for SNS subscriptions, which only show the first 50 available Lambda functions when attempting to add a subscription, with no indication at all that the list is truncated, and no means at all by which to add a subscription for a lambda not in that first set of 50. \nI shall raise the same issue in the AWS Forums.\n. ",
    "shawnemullen": "If this isn't going to be \"fixed\" in the sdk, then I think that at a minimum, the documentation for the sdk should be updated.  I've been trying for 2 hours to figure out why I wasn't getting all the functions, since the documentation says, \"MaxItems - specifies the maximum number of AWS Lambda functions to return in response.\"  It definitely implies that you can set the maximum number of functions to return, and since you can't, the documentation is wrong.\n. ",
    "leftclickben": "Please implement this feature, or at least update the documentation, as per @shawnemullen comment above, I too spent a lot of time trying to work out why this doesn't work as written in the docs.\nIt would be useful to be able to list the latest versions rather than starting from the beginning with ancient versions nobody has cared about for a long time.  \nMy use case is a simple script that finds the latest version and updates a specified alias to that version.. ",
    "ashwgupt": "Why isn't the API working as expected i.e. if I give a value of Max Item to be 300, I should receive a list of 300 functions back. Whats the point of having this param if one is always restricted to 50 max entries in one listFunctions call?. ",
    "jhford": "I don't know why, but this started working. without changing my code, no less :/\n. ",
    "oliversalzburg": "Hi @LiuJoyceC, yeah, after posting this, I read about the get vs. refresh in the documentation.\nMy use case is (or, let's say was) a bit complicated. Let me elaborate. We abstract all AWS interaction in our stack into a common npm module. In that module, I used a check for AWS.config.credentials to see if proper credentials were provided, otherwise requests would not be forwarded to the SDK.\nI realized that this is an issue, because it doesn't take into account that the module could be used on EC2, with an IAM role supplying the credentials (asynchronously). This would cause the module to misbehave.\nSo I decided to manually attach EC2MetaDataCredentials to AWS.config.credentials (if none previously existed) and call refresh() on them to make sure I had that path covered as well. So I think using refresh() in that case was legit (because it happened before any other requests were attempted).\nHowever, all of this was counter-productive as we had to introduce asynchronicity into the construction of our AWS SDK wrapper, which was not desirable, so we dropped the whole aspect and now only issue a warning/analysis after failed requests.\nBack to the initial request though. I have now encountered a few methods in the SDK, which feel like they should return promises, but don't. This is unfortunate, because you end up messing with callbacks again or writing your own promise wrappers all over the place. A generic approach would be very welcome :)\n. Hi @LiuJoyceC, yeah, after posting this, I read about the get vs. refresh in the documentation.\nMy use case is (or, let's say was) a bit complicated. Let me elaborate. We abstract all AWS interaction in our stack into a common npm module. In that module, I used a check for AWS.config.credentials to see if proper credentials were provided, otherwise requests would not be forwarded to the SDK.\nI realized that this is an issue, because it doesn't take into account that the module could be used on EC2, with an IAM role supplying the credentials (asynchronously). This would cause the module to misbehave.\nSo I decided to manually attach EC2MetaDataCredentials to AWS.config.credentials (if none previously existed) and call refresh() on them to make sure I had that path covered as well. So I think using refresh() in that case was legit (because it happened before any other requests were attempted).\nHowever, all of this was counter-productive as we had to introduce asynchronicity into the construction of our AWS SDK wrapper, which was not desirable, so we dropped the whole aspect and now only issue a warning/analysis after failed requests.\nBack to the initial request though. I have now encountered a few methods in the SDK, which feel like they should return promises, but don't. This is unfortunate, because you end up messing with callbacks again or writing your own promise wrappers all over the place. A generic approach would be very welcome :)\n. ",
    "kellyjanderson": "@chrisradek As this change has been merged, can information about using with Webpack be added to the documentation?\nhttps://github.com/aws/aws-sdk-js/blob/master/doc-src/guide/browser-intro.md. ",
    "shawnmclean": "I'd love if this was provided by the sdk. I think the ruby sdk provided this url building method.\n. I'm not sure. It's a repository that stems from the code in this repo. What I understood from their repo is to reach out to the original maintainers.. I'm not sure. It's a repository that stems from the code in this repo. What I understood from their repo is to reach out to the original maintainers.. ",
    "lucasfernandes4472": "Another solution is to use upload instead of putObject. It will return public URL in Location key.\n. ",
    "HarshaVardhanPVK": "@chrisradek\nThank you for your response.\nI got that error while using \"2.3.7\" aws-sdk version, I am running it in browser (react) only.\nNow I updated my aws-sdk module to \"2.6.2\" and I am getting another error as \n\"Missing credentials in config(\u2026)\"\nBut am sending config data as shown above.\nGot both these errors in putObject callback only.\n. @chrisradek\nHi, actually I did a small mistake, cause of that I got that error as \"Missing credentials in config(\u2026)\". There first I created s3Bucket object then I updated configData. Now I placed them in correct order and am getting same error as \"target._set is not a function(\u2026)\".\nPlease check this part of code:\nif (this === target && start < targetStart && targetStart < end) {\n    // descending copy from end\n    for (i = len - 1; i >= 0; i--) {\n        target[i + targetStart] = this[i + start]\n    }\n} else if (len < 1000 || !Buffer.TYPED_ARRAY_SUPPORT) {\n    // ascending copy from start\n    for (i = 0; i < len; i++) {\n        target[i + targetStart] = this[i + start]\n    }\n} else {\n    target._set(this.subarray(start, start + len), targetStart)\n}\nGetting that issue in this part of code...\n. ",
    "bhishp": "ah.. I don't know how I didn't see it before. Yes, the problem is that I wasn't encoding the file-type when GETing /sign-s3.\nBefore:\nxhr.open('GET', /api/sign-s3?file-name=${file.name}&file-type=${file.type});\nCreating a HTTP Request like this (the + subsequently being interpreted as a space):\nhttp://localhost:3000/api/sign-s3?file-name=circle.svg&file-type=image/svg+xml\nAfter:\nxhr.open('GET', /api/sign-s3?file-name=${encodeURIComponent(file.name)}&file-type=${encodeURIComponent(file.type)});\nCreating a HTTP request with + correctly encoded:\nhttp://localhost:3000/api/sign-s3?file-name=circle-with-header.svg&file-type=image%2Fsvg%2Bxml\nSide-note\nAdditionally, I can confirm that omitting ContentType is valid now that I have set signatureVersion: 'v4'\n@chrisradek Thanks for your help and quick response! :+1: :tada:\n. ",
    "cdhowie": "These service tests exclusively run against DynamoDB.  At the point where it fails, there are multiple concurrent requests and they are all DeleteItem requests.  It will consistently fail on that test, and I haven't been able to locate any bug in my code (yet).\nIt probably makes it through 50 or so requests before failing, and I know that some of the other tests involve concurrent deletions.  Everything works fine until I enable keepalive.\n. My spidey sense says this is actually a bug in Node's http.Agent class, but I'm not super-familiar with Node's internals so I'm not well-equipped to go about debugging them to make sure.  So I'm not sure if I should leave this issue open or go report it to Node. :/\nThe first stack frame shown is this Node-internal code:\nfunction afterWrite(stream, state, finished, cb) {\n  if (!finished)\n    onwriteDrain(stream, state);\n  state.pendingcb--;\n  cb();\n  finishMaybe(stream, state);\n}\nThe cb() invocation is what's failing.  This parameter gets set earlier from state.writecb, which, according to the comments in that file, are supplied when invoking write() on the stream.  Apparently the callback is required, at least in Node v4?  That doesn't match the docs, but it wouldn't be the first time the docs are wrong.\n. I am seeing the same behavior on Node 6.6.0 using the stock http.Agent with keepAlive: true.\n. ",
    "jamesongithub": "thanks @LiuJoyceC \n. thanks @LiuJoyceC \n. ",
    "gurpreetatwal": "@chrisradek  any updates?. When merged, #1143 will fix this\n. @shyamchandranmec just depends on the maintainers, not much I can do from my side\nIf anything you can give a \"thumbs up\" on the PR. resolved by #1440 . No problem @jeskew ! You should consider enabling greenkeeper if you can, it'll help by automatically notifying you when your dependencies get updated. However since the package is strict versioned, it might notify you a bit too much. . Any updates on when this might be available?. whoops, my bad\nWhen I merged in the changes from master, the entire package.json file got replaced and I neglected to update the versions. ",
    "edrpls": "@gurpreetatwal This fork is now giving me the error:\nError: Cannot find module 'lodash/object/assign'\n    at Function.Module._resolveFilename (module.js:469:15)\n    at Function.Module._load (module.js:417:25)\n    at Module.require (module.js:497:17)\n    at require (internal/module.js:20:19)\n    at Object.<anonymous> (/edited/node_modules/xmlbuilder/lib/index.js:5:12)\n    at Object.<anonymous> (/edited/node_modules/xmlbuilder/lib/index.js:14:4)\n    at Module._compile (module.js:570:32)\n    at Module._extensions..js (module.js:579:10)\n    at Object.require.extensions.(anonymous function) [as .js] (/edited/node_modules/babel-register/lib/node.js:152:7)\n    at Module.load (module.js:487:32)\n    at tryModuleLoad (module.js:446:12)\n    at Function.Module._load (module.js:438:3)\n    at Module.require (module.js:497:17)\n    at require (internal/module.js:20:19)\n    at Object.<anonymous> (/edited/node_modules/aws-sdk/lib/xml/builder.js:2:15)\n    at Module._compile (module.js:570:32)\nI'm going to try and debug this but wanted to give you a heads up.. @gurpreetatwal This fork is now giving me the error:\nError: Cannot find module 'lodash/object/assign'\n    at Function.Module._resolveFilename (module.js:469:15)\n    at Function.Module._load (module.js:417:25)\n    at Module.require (module.js:497:17)\n    at require (internal/module.js:20:19)\n    at Object.<anonymous> (/edited/node_modules/xmlbuilder/lib/index.js:5:12)\n    at Object.<anonymous> (/edited/node_modules/xmlbuilder/lib/index.js:14:4)\n    at Module._compile (module.js:570:32)\n    at Module._extensions..js (module.js:579:10)\n    at Object.require.extensions.(anonymous function) [as .js] (/edited/node_modules/babel-register/lib/node.js:152:7)\n    at Module.load (module.js:487:32)\n    at tryModuleLoad (module.js:446:12)\n    at Function.Module._load (module.js:438:3)\n    at Module.require (module.js:497:17)\n    at require (internal/module.js:20:19)\n    at Object.<anonymous> (/edited/node_modules/aws-sdk/lib/xml/builder.js:2:15)\n    at Module._compile (module.js:570:32)\nI'm going to try and debug this but wanted to give you a heads up.. Looks like the commit from two days ago rolled back all the changes to package.json. Looks like the commit from two days ago rolled back all the changes to package.json. The versions are the same as master now.. ",
    "goldenbearkin": "what is stopping this merge?. what is stopping this merge?. ",
    "prabhatsharma": "While we wait for Chris and team to fix it, I have temporarily solved this by installing latest lodash and xmlbuilder manually in my package.json. Am using aws-sdk on front end and not on node.. While we wait for Chris and team to fix it, I have temporarily solved this by installing latest lodash and xmlbuilder manually in my package.json. Am using aws-sdk on front end and not on node.. @jeskew There is no concern on _ variable in global scope. Everything seems to be playing well. Indeed with tree shaking functionality on front end code bundle size is also not really a problem. Its just an unnecessary minor annoyance for now. On the server side code, package size is generally not a  concern. So for now all looks good.. @jeskew There is no concern on _ variable in global scope. Everything seems to be playing well. Indeed with tree shaking functionality on front end code bundle size is also not really a problem. Its just an unnecessary minor annoyance for now. On the server side code, package size is generally not a  concern. So for now all looks good.. ",
    "baweaver": "Primarily it comes down to bundle size and having multiple versions of Lodash hanging around. \nThankfully the require technologies being used circumvent that issue quite nicely, otherwise most of Node could be crashed with 3 versions of Lodash and a few of Underscore :). Primarily it comes down to bundle size and having multiple versions of Lodash hanging around. \nThankfully the require technologies being used circumvent that issue quite nicely, otherwise most of Node could be crashed with 3 versions of Lodash and a few of Underscore :). Node v0.8 has been out of support for a while now, over two years at this point, so that may be a bit extreme. See: https://github.com/nodejs/LTS\n. Node v0.8 has been out of support for a while now, over two years at this point, so that may be a bit extreme. See: https://github.com/nodejs/LTS\n. I won't drag that topic on then.\nThe primary concern of this one was to try and get Lodash into a reconcilable version to prevent package bloat where possible, so that would be a valid solution.. I won't drag that topic on then.\nThe primary concern of this one was to try and get Lodash into a reconcilable version to prevent package bloat where possible, so that would be a valid solution.. ",
    "jamorales-bsft": "works thanks. It would be nice if that was in the docs somewhere as a gotcha.\n. ",
    "vkhazin": "Confirmed for \"aws-sdk\": \"2.6.3\"\nconst s3                    = promise.promisifyAll(new aws.S3({signatureVersion: 'v4'}));\nresolves the issue reported earlier.\n. ",
    "MaxVita": "Hey @LiuJoyceC \n2.5.6 works well and all versions >2.5.6  (2.6.0 and above) will fail with \"TypeError: punycode.toASCII is not a function\".\n. ",
    "somprabhsharma": "@AdityaManohar Please provide information on this ?\n. I am also not able to reproduce the issue with all possible scenarios of invalid or valid input in my dev environment. But the error is coming more than 100 times a minute in production. That is why I needed your help to determine the possible cause of this issue. And I believe that the error is from the service not from sdk.\n. ",
    "skysteve": "@chrisradek that happened on Node 6.2.2 and 6.6.0\nCode wise it's something like\n``` javascript\nconst AWS = require('aws-sdk');\nconst path = require('path');\nconst glob = require('glob');\nconst fs = require('fs');\nconst mime = require('mime');\nconst uploadToS3 = (bundleDir, file, config) => {\n  const bucket = config.bucket;\n  const s3 = new AWS.S3();\nconst name = file.replace(${bundleDir}${path.sep}, '');\nreturn new Promise((resolve, reject) => {\n    fs.readFile(file, (fileErr, data) => {\n      if (fileErr) {\n        return reject(fileErr);\n      }\n  const params = {\n    ACL: 'public-read',\n    Bucket: bucket,\n    Key: name,\n    Body: data,\n    ContentType: mime.lookup(file)\n  };\n\n  s3.putObject(params, (err) => {\n    if (err) {\n      return reject(`Unable to upload ${params.Key} to ${params.Bucket}. Err: ${err.message}`);\n    }\n\n    console.log(`uploaded ${file} to ${params.Bucket}::${name}`);\n\n    resolve(`/${name}`);// to invalidate cache, name must start with /\n  });\n});\n\n});\n};\nuploadToS3('/example', 'example.js', {bucket : '1234'});\n```\n. That was on OSX El Capitan on my laptop and ubuntu 14.04 on our CI server.\nI'm not at work anymore (UK timezone) so It'll have to wait for the morning, but as I said, downgrading to 2.6.3 everything worked as expected, upgrading to 2.6.4 it broke, and downgrading again it worked (consistently). Not sure what might be going on :S\n. Hey, so your example does run fine on my mac, I'll do a bit of playing around today and see if I can get something which recreates the problem\n. ",
    "adueck": "@olalonde and @chrisradek  Thanks for asking about this as I have a use case exactly like this. Do you know if there has been any feature development like you suggested? It would by nice to simply get the metadata in JSON rather than having to scrape it out from the http headers as explained above.. ",
    "donpark": "Funny you should ask. I just deleted the bucket I successfully set Cors using this code:\n``` typescript\n    ensureBucketPublicCorsRule(bucket: string) {\n      if (TRACE) console.log('ensureBucketPublicCors', bucket);\n      // Promise complicates retry so uses callback here. \n      return new Promise((resolve, reject) => {\n        this.client.getBucketCors({\n          Bucket: bucket,\n        }, (err, data) => {\n          let rules = !err && data ? data.CORSRules : null\n          if (rules && rules.filter(rule => this.isPublicCorsRule(rule)).length > 0) return resolve(rules);\n      // rules either doesn't exist or is not set to public access, create or override\n      let publicCorsRule = {\n        AllowedHeaders: ['Authorization'],\n        AllowedMethods: ['GET'],\n        AllowedOrigins: ['*'],\n        MaxAgeSeconds: 3000,\n        ExposedHeaders: ['Content-Length', 'Content-Type'],\n      }\n\n      if (rules) {\n        rules.push(publicCorsRule)\n      } else {\n        rules = [publicCorsRule]\n      }\n      this.client.putBucketCors({\n        Bucket: bucket,\n        CORSConfiguration: {\n          CORSRules: rules,\n        },\n      }, (err, data) => {\n        if (err) return reject(err);\n\n        // make sure rule is set\n        this.client.getBucketCors({\n          Bucket: bucket,\n        }, (err, data) => {\n          if (err) return reject(err);\n          rules = !err && data ? data.CORSRules : null\n          if (rules && rules.filter(rule => this.isPublicCorsRule(rule)).length > 0) return resolve(rules);\n          reject(new Error('could not add public CORS rule'))\n        })\n      })\n    })\n  })\n}\n\n```\nand reran it and it failed. But if I change ExposedHeaders to ExposeHeaders, it now runs successfully.\nSo either I'm hallucinating or someone fixed it at the backend sometime after I created the bucket.\nEither way, this issue can be closed now.\n. Funny you should ask. I just deleted the bucket I successfully set Cors using this code:\n``` typescript\n    ensureBucketPublicCorsRule(bucket: string) {\n      if (TRACE) console.log('ensureBucketPublicCors', bucket);\n      // Promise complicates retry so uses callback here. \n      return new Promise((resolve, reject) => {\n        this.client.getBucketCors({\n          Bucket: bucket,\n        }, (err, data) => {\n          let rules = !err && data ? data.CORSRules : null\n          if (rules && rules.filter(rule => this.isPublicCorsRule(rule)).length > 0) return resolve(rules);\n      // rules either doesn't exist or is not set to public access, create or override\n      let publicCorsRule = {\n        AllowedHeaders: ['Authorization'],\n        AllowedMethods: ['GET'],\n        AllowedOrigins: ['*'],\n        MaxAgeSeconds: 3000,\n        ExposedHeaders: ['Content-Length', 'Content-Type'],\n      }\n\n      if (rules) {\n        rules.push(publicCorsRule)\n      } else {\n        rules = [publicCorsRule]\n      }\n      this.client.putBucketCors({\n        Bucket: bucket,\n        CORSConfiguration: {\n          CORSRules: rules,\n        },\n      }, (err, data) => {\n        if (err) return reject(err);\n\n        // make sure rule is set\n        this.client.getBucketCors({\n          Bucket: bucket,\n        }, (err, data) => {\n          if (err) return reject(err);\n          rules = !err && data ? data.CORSRules : null\n          if (rules && rules.filter(rule => this.isPublicCorsRule(rule)).length > 0) return resolve(rules);\n          reject(new Error('could not add public CORS rule'))\n        })\n      })\n    })\n  })\n}\n\n```\nand reran it and it failed. But if I change ExposedHeaders to ExposeHeaders, it now runs successfully.\nSo either I'm hallucinating or someone fixed it at the backend sometime after I created the bucket.\nEither way, this issue can be closed now.\n. ",
    "kothari775": "I m also getting same issue. \nDependency : \n\"dependencies\": {\n    \"aws-sdk\": \"^2.6.6\", \n    \"s3\": \"^4.4.0\",......}\nMy snippet is \nvar s3 = require('s3');\nvar client = s3.createClient({\n  maxAsyncS3: 20,     // this is the default\n  s3RetryCount: 3,    // this is the default\n  s3RetryDelay: 1000, // this is the default\n  multipartUploadThreshold: 20971520, // this is the default (20 MB)\n  multipartUploadSize: 15728640, // this is the default (15 MB)\n  s3Options: {\n    accessKeyId: \"myAccessKey\",\n    secretAccessKey: \"secretKey\",\n    region: \"regionId\",\n    s3ForcePathStyle: true,\n    signatureVersion: 'v4'\n    //s3DisableBodySigning: true\n    // endpoint: 's3.yourdomain.com',\n    // sslEnabled: true\n  }\n});\nvar params = {\n  localDir: \"some/dir\",\n  deleteRemoved: true, // default false, whether to remove s3 objects\n                         // that have no corresponding local file.\n  Body: fs.readdir('some/local/dir'),\n  s3Params: {\n    Bucket: 'bucketname',\n    Prefix: \"sample\"\n  }\n};\nvar uploader = client.uploadDir(params);\nuploader.on('error', function(err) {\n  console.error(\"unable to sync:\", err.stack);\n});\nuploader.on('progress', function() {\n  console.log(\"progress temp : \", uploader.progressAmount, uploader.progressTotal);\n});\nuploader.on('end', function() {\n  console.log(\"done uploading\");\n});\nError : \nprogress temp :  34545103 128436553\nprogress temp :  34561487 128436553\nprogress temp :  34577871 128436553\nunable to sync: Error: Non-file stream objects are not supported with SigV4 in AWS.S3\nI tried with uploadFile that works fine\nGuide me I have 2 options  1) upload whole local directory to S3 or 2) Streaming from MongoDB Gridfs and upload to AWS S3.\n. ",
    "ovaillancourt": "Hi! Just wanted to mention that we've noted the same issue.\nWe're working around it by rebuilding the url from the Bucket and Key parts of the response as those are consistent.\n. Hi! Just wanted to mention that we've noted the same issue.\nWe're working around it by rebuilding the url from the Bucket and Key parts of the response as those are consistent.\n. ",
    "Anonyfox": "We would also LOVE to have this feature!. ",
    "ZY-Ang": "+1 for this feature. Need this to integrate a DevOps workflow for my web applications. ",
    "paulfryer": "@chrisradek I just ran into this issue. Any workaround identified? Looks like this post is a couple years old so thought I'd check.. @AllanFly120 I actually did end up using a proxy pattern, in this case I just build a CloudFormation template with one bucket and send it to the CloudFormation API. It's kind of strange, but works and the advantage over proxy via Lambda is that I don't have to install a Lambda.. Hi @srchase I'm using the latest version of the SDK. \nI can see the AWS.SageMaker documentation shows an example of how to use the API, for example:\nvar sagemaker = new AWS.SageMaker();\nHowever when I do that with the default npm install aws-sdk I get the error in the title of this post. I've been trying to find the source code of the SageMaker node client and somehow build a custom version of the aws-sdk. I see I can do that with browserify like this:\nhttps://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/building-sdk-for-browsers.html#using-command-line-tools\nHowever what I really want to do is browserify ALL my node packages together, not just the aws-sdk.. I figured it out, I had to make an explicit reference to the sagemaker client library from my JS file, like this:\nrequire('../../node_modules/aws-sdk/clients/sagemaker')\nNow the browserify step will pull in this sagemaker code, since it's using code reflection to determine what to include.. \n@chrisradek thanks for that, and Yes SageMaker does support CORS (see attached screenshot), seems to be working fine from my browser based application.. ",
    "zymr-keshav": "Thank you for the suggestion. will there any documents where one can check which methods required CORS and which does not?\n. Thank you @LiuJoyceC .. this is really helpful . Thanks again.\n. @chrisradek Thank you for the response, below is my CORS configuration of my buckets\n<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n    <CORSRule>\n        <AllowedOrigin>*</AllowedOrigin>\n        <AllowedMethod>HEAD</AllowedMethod>\n        <AllowedMethod>GET</AllowedMethod>\n        <AllowedMethod>PUT</AllowedMethod>\n        <AllowedMethod>POST</AllowedMethod>\n        <AllowedMethod>DELETE</AllowedMethod>\n        <AllowedHeader>*</AllowedHeader>\n    </CORSRule>\n</CORSConfiguration>\n\n\nand yes I have set s3BucketEndpoint: true in S3 constructor as well and tried the same but same error .\n\n\nSDK respose the correct output for other region bucket when the bucket is old means not just created. I have tried with old bucket and it works fine but not working with newly created bucket.\n\n\nand how can we use AWS.config into S3 contructor in javascript method?\n\n\n. Thanks @chrisradek for the response. Yes i want to create sibling folders and that is not yet supported as you said. I will go with the .putObject()  in loop.. I am using AWS SDK v 2.7.7 for javascript in my angular app to create classic load balancer, launch configuration and auto scaling policy; by that way it creates 2 instance and when I logged in through ssh -i key.pem ec2-user@load-balancer-dns-name and check for the log file than it gives error.  as there is a java program that run through UserData while creating instance and above is the output of that log. \neven I checked that none of the aws-cli command working inside the ec2 instance . my concern is how to access s3 bucket through ELB ? do we need to configure anything else to access s3bucket?\nHere i also posted the same query regarding the security group.\nhttp://serverfault.com/questions/825036/can-we-use-same-security-group-in-launch-configuration-and-classic-load-balancer/825127#825127\n. ",
    "brunobelotti": "@LiuJoyceC is there any development on this? Thanks! :). ",
    "larryboymi": "Would like this also! @LiuJoyceC . ",
    "MarcAMartin": "Bump.. ",
    "ranjeethpt": "+1 @LiuJoyceC . ",
    "aneilbaboo": "It looks like this issue is 2 years old now. Has there been any update on it?\n@LiuJoyceC . Looking at the latest SDK docs, I noticed this:\n\"After your data is encrypted, DynamoDB handles decryption of your data transparently with minimal impact on performance. You don't need to modify your applications to use encryption.\"\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html\nShould this issue be closed?. ",
    "Answashe": "I think the easy way to do this would be to modify the Route53 service to override retryableError similar to the way the S3 service does\n. ",
    "jbergknoff": "Looks like the test failed on node <4 because it uses a fancy new stream constructor; I'll update the test tomorrow morning.\n. Ping @chrisradek \n. @chrisradek thanks for having a look. I added the changelog entry to the PR.\n. ",
    "nytins": "The issue turned out to be the way I set region. I was reading info from 2 regions like this:\nAWS.config.region = 'us-east-1';\nloadInfo(AWS.config.region);\nAWS.config.region = 'us-west-2';\nloadInfo(AWS.config.region);\nSo during the describe call, the region would have changed and it caused the issue.\nTo fix it, I moved setting the region just before the describe function call.\n. ",
    "kalpitad": "Thanks @chrisradek! What does it mean for the first delay to be 0ms? The retry occurs immediately? Or does it refer to the initial request?\n. Got it, retryCount is initially 0. So you actually get 2 requests back-to-back without a delay, assuming the 1st produces a retryable error, and then assuming that the 2nd request also produces a retryable error, the 3rd request will be delayed by 50ms. Thanks for the clarification!\n. ",
    "essapalaxo": "Ok, Looks like while updating, Riak created new siblings object instead of replacing the existing object. I found in Riak documentation that setting allow_mult to false in bucket properties will prevent this from happening i.e. create new siblings.\nNow my question is how can I set bucket propeties in aws-sdk i.e. set allow_mult to false?\nAny idea?\nThanks!\n. Another solution to my problem would be to get the latest sibling i.e. updated object. Is there any way I can get the latest sibling using aws-sdk-js?\nI can get the latest sibling from url but I want to get it through aws-sdk API.\nThanks!\n. @chrisradek \nThanks for the reply. Riak is a distributed NoSQL database. We are using it as endpoint in AWS API. While using Riak when we call putObject to update an existing Body with Key, the Body is not replaced with the new buffer instead it creates a new sibling of buffer and when we try to retrieve that same object using getObject method in AWS API we get the error message as shown in the very first part of the question.\nRiak documentation mentions that we can disable the creation of siblings if we set the bucket property attribute allow_mult to false & this way AWS API will work as it suppose to. Does AWS API provides any method or way to set bucket properties?\nAnother & much better way is that AWS API may provide me a method which can fetch me the latest or any sibling I want from Riak.\nRight now when AWS API tries to fetch the object using getObject method, it gets the following in response:\nSiblings:\n7S5lizOWHAnaeA13EdyDP5\n5XgitL5OstvVBurVUp1d1V\nSee, it returns the ids of siblings & not the file content which is causing error that I pasted in the very first comment. \nI can access any sibling by the following url:\nhttp://example.com/riak/MyBucket/Key?vtag=sibling_id\nI hope you understand my problem.\nThanks!\n. ",
    "ath88": "Hello @LiuJoyceC,\nThanks for the response! \nIll explain my case further. We have some functions that expect an s3-instance with a bound bucket. They can have the interface; uploadObject(s3, callback). This is 'nicer' than having to pass both an s3-instance and a bucket name; uploadObject(s3, bucketName, callback). \nIll try to make a short example:\n``` javascript\nvar s3 = aws.S3({ params: { Bucket: \"red-fruits\" } });\nvar uploadAppleToRedFruitsBucket = () => uploadApple(s3);\ns3.config.params.Bucket = \"sweet-fruits\";\nvar uploadAppleToSweetFruitsBucket = () => uploadApple(s3);\nuploadAppleToRedFruitsBucket();\nuploadAppleToSweetFruitsBucket();\nfunction uploadApple(s3) {\n    s3.upload({ Body: \"red-and-sweet-apple\" });\n}\n```\nIt is a very contrived example, I know. Summa summarum; the upload would only go to one bucket. Imagine the function is way more elaborate, and the s3-instance is initiated somewhere else. I would like something like:\n``` javascript\nvar s3 = aws.S3({ params: { Bucket: \"red-fruits\" } });\nvar uploadAppleToRedFruitsBucket = () => uploadApple(s3);\nvar s3Clone = s3.clone();\ns3Clone.config.params.Bucket = \"sweet-fruits\";\nvar uploadAppleToSweetFruitsBucket = () => uploadApple(s3Clone);\nuploadAppleToRedFruitsBucket();\nuploadAppleToSweetFruitsBucket();\nfunction uploadApple(s3) {\n    s3.upload({ Body: \"red-and-sweet-apple\" });\n}\n```\nDoes this make any sense at all?\n. @LiuJoyceC That is exactly my point. :) In my first example, the s3 instance params is changed in-place, and thus, an upload only happens to the final bucket. \nAs I originally mentioned, I would love to have my function be completely agnostic about bucket-names. Passing one variable and not having to specify the bucket name every time seems cleaner to me. :) Your first example does exactly what I would like to not have the function do.\nYour second code-example is not certain to work, since an asynchronous functions could change the bucket-name just after another function set it, but before the other got to actually use it (depending on when the execution is yielded, which I am not familiar enough with NodeJS to be certain of).\nMy current hack , which actually works, looks like this:\n``` javascript\nvar s3 = aws.S3({ params: { Bucket: \"red-fruits\" } });\nvar uploadAppleToRedFruitsBucket = () => uploadApple(s3);\nvar cloneConfig = { \n    accessKeyId: s3.config.accessKeyId, \n    secretAccessKey: s3.config.secretAccessKey,\n    params: { Bucket: \"sweet-fruits\" }\n};\nvar s3Clone = new aws.S3(cloneConfig);\nvar uploadAppleToSweetFruitsBucket = () => uploadApple(s3Clone);\nuploadAppleToRedFruitsBucket();\nuploadAppleToSweetFruitsBucket();\nfunction uploadApple(s3) {\n    s3.upload({ Body: \"red-and-sweet-apple\" });\n}\n```\nHere, I manually have to extract the parts of the config that I want to use. I don't really like this copying of internal stuff. Besides, there could be other configuration parameters that I should copy too.\nAnd yes, Key and callback are needed, I just left them out for brevity. :)\n. Hello again @LiuJoyceC,\nI know the first example wont work, as I wrote; \"the upload would only go to one bucket\". I fully acknowledge that it will not work, and that the SDK can't handle such an abuse of javascript objects. \nYou wrote:\n\nand the reason that your current hack works is because the two functions are referring to different objects in memory\n\n.. which is exactly my point. I would love to have some supported way to make another object in memory, a clone, which uses the same config as the original one, but I can bind new paramsonto, which will not affect the original object.\nI am not certain how I can explain it any further. This is essentially a feature request, unless there's another way to make this 'different object in memory'. Which was what my original enquiry was about. :)\nIn Perl, we have this 'clone' method on a PostgreSQL connection object; http://search.cpan.org/~timb/DBI-1.636/DBI.pm#clone\n. That works, yes. Thank you! \nA clone-function would only be a convenience, then. :) It would let me not require the aws-sdk in places where I want to clone.\n. Good idea. :) Thanks!\n. ",
    "OrZipori": "Thanks... \nvar sns = new AWS.SNS({correctClockSkew: true});\nFixed it...But why there's an error in the first place?\n. Yes I have another question concerning push notification. \nI'm using SNS for push notifications for android and iOS. Since sending to both devices require different platform application, my endpoints are saved separately. How do I send push notification to all endpoints via SNS publish api in on request instead of many, please?\nI've read that using Topics would give me that result, but from what I read, I need them (users) to accept (confirm) that they would like to get notifications..Isn't it going to complicate things, since users already accept notification when app first started?\nThanks.\n. ",
    "danbucholtz": "@chrisradek,\nThis was user error. If we removed the process shim, this whole issues goes away. We don't actually need it in this case.  Rollup was importing the process shim, then the browser_loader.js was reassigning process.  Rollup will throw an error when an import is reassigned to something else, so that's why this happened.\nSorry about that!\nThanks,\nDan\n. @chrisradek,\nThis was user error. If we removed the process shim, this whole issues goes away. We don't actually need it in this case.  Rollup was importing the process shim, then the browser_loader.js was reassigning process.  Rollup will throw an error when an import is reassigned to something else, so that's why this happened.\nSorry about that!\nThanks,\nDan\n. ",
    "raphaguasta": "@danbucholtz I'm having the same problem! How did you fix it? Thanks!!. ",
    "xeniaRen": "@chrisradek \nI call signUp as follow.\nuserPool.signUp(username, password, attributeList, null, function(err, result){\nif(err) {\nalert(err);\nreturn;\n}\ncognitoUser = result.user;\nsessionStorage.setItem('username', username)\nif(!result.userConfirmed){\n    changePage('confirm_signup')\n    //turn to confirmed ui\n    // alert(\"to confirmed ui\")\n  }\n});\n. ",
    "chetanme": "This is expected because Cognito console, in addition to saving the pool with lambda trigger, adds permissions to your Lambda functions which allows Cognito service to invoke your function. When you associate a lambda function to user pool from CLI or any other SDK, you also need allow Cognito to invoke the trigger.\nCall Lambda's AddPermission API with appropriate parameters. Here SourceArn should be your user pool ARN and Principal should be cognito-idp.amazonaws.com. You can use GetPolicy to see a sample policy on a function which you use through Cognito console. Hope this solves your problem.\n. Currently none of the APIs return the ARN value but it is a well known format which you can make yourself.\narn:aws:cognito-idp:<your Cognito region>:<your AWS account id>:userpool/<your user pool id>\n. ",
    "bedorlan": "Thank you @chetanme \nHow can i get the userpool arn, from the CLI?\nI can't find how, in the api reference.\n. Thank you @chetanme \nHow can i get the userpool arn, from the CLI?\nI can't find how, in the api reference.\n. @chetanme yes. it's working now! thanks\n. @chetanme yes. it's working now! thanks\n. ",
    "kaihendry": "Thank you Chris.\nimport 'aws-sdk/global'\nimport 'aws-sdk/clients/s3'\nWorks great.\n. Yes, I'm asking for object metadata to be in the response please. I don't have time to campaign on the forums, sorry.\n. Thank you, 2.7.21 seems to work.. I did manage to fix it. The confusing element is that the formData ordering seemed to be important!\nI.e. fd.append(\"file\", f.files[0]); has to come after the signed policy et al...\nhttps://github.com/unee-t/s3-transfer-acceleration-cloudinary/blob/aws-sdk/app.js. ",
    "misterfresh": "Thanks, will try that tomorrow and will let you know how it goes.\n2016-10-12 22:45 GMT+02:00 Joyce Liu notifications@github.com:\n\nHi @misterfresh https://github.com/misterfresh\nAre you using putObject for files over 5MB as well? putObject is not used\nto initiate multipart uploads. You will have to initiate that with\ncreateMultipartUpload, and then subsequent calls to uploadPart before\ncompleting it with completeMultipartUpload. I recommend using the managed\nuploader in the SDK (call the upload operation instead of putObject).\nThis will handle uploads for both small and large files, and will abstract\naway the complexity of multipart uploads so you don't have to call the\nindividual operations needed. See http://docs.aws.amazon.com/\nAWSJavaScriptSDK/latest/AWS/S3.html#upload-property\nLet me know if that resolves your issue. Thanks.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1180#issuecomment-253333739,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACMW-tOacOXpeCw47ul6pPK4_Rqiv6zYks5qzUbPgaJpZM4KU8v4\n.\n. Actually I was already using the \"upload\" method, and the problem was \"No access to ETag property on response. Check CORS configuration\". Solved it with adding \"<ExposeHeader>ETag</ExposeHeader>\" in bucket CORS configuration. Thanks ! Closing this now.\n. \n",
    "magnattic": "Why is this not simply fixed by the merge? Killed my application recently. ",
    "menkari": "Has there been any update against this? More and more modules are making a requirement against lodash@4. ",
    "justinffs": "Thanks for looking into it.\n. Thanks @chrisradek \n. ",
    "thihara": "@chrisradek It did, thank you. Basically update the config if your connection type changes. Also http://stackoverflow.com/questions/39970302/aws-sdk-for-node-js-connection-management/40035293#40035293\n. ",
    "designreact": "For reference I have managed to sidestep this issue using: https://www.npmjs.com/package/dynamodb-marshaler\nI am yet to employ the time required to ascertain the exact difference between my objects but perhaps the above will help someone else in the interim.\n. No problem @chrisradek.\nWhen using DocumentClient I get 8 sets of 100 followed by 89 in the last set. When using DynamoDB I get 10 sets of 100 followed by a set of 39. We looked at the data today and could not see anything that jumped out as different in the set of 150 which are not returned by DocumentClient.\n. Further to this it may be worth noting that this is thought to be present in both the local and server versions of DynamoDB, I was first notified by my client pointing out that products were missing from the staging site and was able to replicate in my local environment.\n. I'm not passing any params into my methods just an empty object getScan({}), this is to return the entire database as an array.\ndoScan adds the following:\n{ TableName: tableName, ExclusiveStartKey: lastEvaluatedKey, Limit: 100 }\nI'll create a private repository and I'll give you access to the product data.\n. OK no problem - if you need anything else I'll see if I can help.\n. Could this be something to do with provisioning? I just had an instance where I was updating my cloud DynamoDB. I ran out of provisioned capacity and the number of items added was 889 which feels a bit too coincidental.\nRead 180\nWrite 10\n. Unfortunately I don't have the sample results to hand. It'll take me a day or two to find time but I'll put together a sample project, verify the bug is still present in the sample code and invite you to the repo.\nOnce I've done this - providing the bug is present will be able to see my implementation in action and verify the bug exists using the same code as I.\nApologies for the upcoming delay - I'm up against some pretty tight deadlines atm.\n. ",
    "shruthibhaskar": "Hi,\nDid you find the solution for this problem?\ni am facing the same problem. Kindly let me know your solution.. ",
    "decoursin": "Thank you very much!\n. ",
    "davidecantini": "The issue described above seems to be related to the region:\nregion: \"eu-central-1\",  it fails (with the error described above).\nregion: \"us-east-1\", it works.\nPlease advise, thank you\n. @chrisradek \nI'm sending an ajax PUT $.ajax(sReq, settings) where:\n- sReq, is what the s3.getSignedUrl() returned (please see the first post for its parameters)\n- settings, is just the ajax settings object { method: 'PUT',  contentType: false,  processData: false,  data: blob}\nHopefully that addresses your question. Please take also into account the second post about the region, if it can be useful.\nAs a temporary workaround, assuming that it's acceptable to have all objects in the bucket with the same max-age, can you please confirm that setting <MaxAgeSeconds>31536000</MaxAgeSeconds>\nin the CORS settings would be equivalent to specifying the CacheControl in the signed upload? \nThanks again for your help.\n. @chrisradek \nThat fixed it. Thanks!\n. ",
    "tj": "The only relevant config there would be the region right? I have that hard-coded above, tried via aws.config.update() as well. I get the expected \"The specified log group does not exist.\" if I change the profile, so it seems to be picking that up properly, did some console.log()ing in the Request source as well\n. oh wow interesting, appears to work fine in other regions, even though I have this function in all regions... weird, seems likely to be unrelated then. Even more strange is that us-west-2 works fine via Go and CLI... haha hmm.\nGoing to close for now, definitely needs some more investigation.\n. damn, this got me too haha, seems odd that this one doesn't support CORS. ",
    "Noxs": "@chrisradek \nI tried from 2.6.0 to 2.6.10.\nCan you put here the generated file, so i can make a diff.\nMine doesn't work anywhere. (osx, win7).\naws-sdk.zip\n. @chrisradek\nSame result when I do it in a new directory. I also run npm cache clean.\nI tried on a OS X machine, it works perfectly.\nI will try to reproduce this bug on another win7 and post here the result.\n. @chrisradek\nI tried on other win7 computer, and this is not working.\nI got the same problem.\nBuilding the SDK on win7 is not working since 2.6.0.\nWhat should I do? Close the issue or leave it open?\n. @chrisradek\nNo, I have not been able to test on windows 10. I don't have any.\n. ",
    "zeevl": "Thanks @chrisradek.   I thought about taking the refresh approach, but unfortunately I haven't had any luck with that on my EC2 instances.   It seems to have something to do with https://github.com/aws/aws-sdk-js/issues/246, where AWS.config.credentials never appear populated and AWS.config.credentialProvider is [null, null, null, null], so getCredentials (and consequently, refresh) fails with \"No credentials to load\".\nIs there a better way to check/refresh credentials on EC2 instances?\n. ",
    "thinkloop": "Thanks! That worked. Do I still need the separate Config or does it also come with global:\n```\nimport Config from 'aws-sdk/clients/configservice';\nimport AWS from 'aws-sdk/global';\nConfig.credentials = new AWS.CognitoIdentityCredentials({\n    IdentityPoolId: 'us-east-1_xxxx',\n    Logins : {\n        'cognito-idp.us-east-1.amazonaws.com/us-east-xxxx': result.getIdToken().getJwtToken()\n    },\n    Paranoia : 7\n});\n```\n. +1, thanks again!\n. ",
    "sandangel": "Sorry I'm using Angular typescript,  I have an error that aws-sdk/global doesn't has a default export. How can I config global credential after retrieved it from CognitoIdentityCredentials ?. I found a more efficient way. \nts\nimport {config as AWSConfig} from 'aws-sdk/global';. ",
    "ispyinternet": "its still 17k lines and 1.4MB. Is there no way to expose this as a module so it can be used effectively in the front end? At the moment is still only realistic for backend.\nEDIT:\nJust read:\nBy default this provider gets credentials using the AWS.CognitoIdentity.getCredentialsForIdentity() \nSo looks like you may be able to implement what you need just using this service.. ",
    "brandonburkett": "I am running into this issue today as well, is there a work around for a small build? I do not see CognitoIdentityCredentials in clients, only cognitoidentity and cognitoidentityserviceprovider.. Awesome!  Let me know if you need any additional information.\nBrandon. @srchase Thank you for the update.  I actually tried your recommendation in my original post (item 3 under attempts to fix).  While I was able to set systemClockOffset, it seemed liked it was not respected for the KMS retries (at least with the kms instance I had, I did not try to creating a new instance via new KMS().  \nI can try again... but am hoping KMS team is able to get it resolved w/o having to go the systemClockOffset route.  \ud83d\ude47\u200d\u2642\ufe0f. Got it, I will try this and let you know.  Is this something that the KMS team is still attempting to fix (so I can remove my temporary systemClockOffset code when the fix is live)?\nThanks\nBrandon. ",
    "VictorioBerra": "This is a working example on how to authenticate a user via Identity Pools as an Identity Provider with adminInitiateAuth(), and then turn around and grab an IdentityID by validating a User Pool IdToken against Amazon Cognito Federated Identities via getId(), and finally requesting AWS credentials from Amazon Cognito Federated Identities via getCredentialsForIdentity(). This is all done entirely server-side without using the amazon-cognito-identity-js package (mostly because SRP is not being used). Note to readers: this requires elevated policies to use adminInitiateAuth(). So it might not be practical if you want your server to only have as much AWS access as the user who is authenticating has. My next project is to figure out how to auth via SRP so my server doesnt need \"admin level\" credentials.\n``` javascript\nvar authParams = {\n    AuthFlow: 'ADMIN_NO_SRP_AUTH',\n    ClientId: 'MY_CLIENT_ID',\n    UserPoolId: 'us-east-1_MY_POOL_ID',\n    AuthParameters: {\n        USERNAME: 'USERNAME',\n        PASSWORD: 'PASSWORD'\n    }\n};\nvar cognitoidentityserviceprovider = new req.AWS.CognitoIdentityServiceProvider({\n    region: 'us-east-1' // Why do I need this? Its in .aws/config\n});\ncognitoidentityserviceprovider.adminInitiateAuth(authParams, function(err, sessionData) {\nif (err) {\n    console.log(err, err.stack);\n} else {\n\n    var cognitoidentity = new req.AWS.CognitoIdentity({\n        region: 'us-east-1' // Why do I need this? Its in .aws/config\n    });\n\n    var getIdParams = {\n        IdentityPoolId: 'us-east-1:IDENTITY_POOL_GUID',\n        Logins: {\n            'cognito-idp.us-east-1.amazonaws.com/us-east-1_MY_POOL_ID': sessionData.AuthenticationResult.IdToken\n        }\n    }\n\n    cognitoidentity.getId(getIdParams, function(err, identity) {\n        if (err)\n        {\n            console.log(err, err.stack);\n        }\n        else\n        {\n\n            var getCredentialsParams = {\n                IdentityId: identity.IdentityId,\n                Logins: {\n                    'cognito-idp.us-east-1.amazonaws.com/us-east-1_MY_POOL_ID': sessionData.AuthenticationResult.IdToken\n                }                        \n            };\n\n            cognitoidentity.getCredentialsForIdentity(getCredentialsParams, function(err, credentials) {                        \n              if (err)\n              {\n                  console.log(err, err.stack);\n              }\n              else \n              {\n                  console.log(credentials); // SUCCESS!\n              }\n            });\n\n\n        }\n    });\n\n\n}\n\n});\n```\n. @pnandak It has been so long i barely remember most of this. It would take me a couple hours to ramp back up and get my own examples working.\nYou can see an older example here that may or may not work https://github.com/VictorioBerra/js-cognito-auth-example\nBest of luck.. ",
    "pprabhu3430": "'I want to be able to go from adminInitiateAuth() to new AWS.CognitoIdentityCredentials()' . This is exactly what I want to do and the documentation is unbelievably frustrating. \nBut looks like your code will also work as long as I get hold of a credentials object. My purpose is to first generate the session idTken server side and then use it client side to build a credentials object so as to make AWS Lex calls via Javascript. \nI used adminInitiateAuth and logged the user in. Now I need to use the session.idToken to build the credentials object on the client side. However what is the IDENTITY_POOL_GUID? How do I get hold of that value. Do I have to link a federated identity provider to my user pool? \nThanks for any help or clarification you can provide. . ",
    "pnandak": "I am struggling to get this done from Server side. (admin_no_srp_auth) with Cognito identity. I used your above code but i get following error: Process exited before completing request and    TypeError: Cannot read property 'IdToken' of undefined. @VictorioBerra  I am struggling to get this done from Server side. (admin_no_srp_auth) with Cognito identity. I used your above code but i get following error: Process exited before completing request and TypeError: Cannot read property 'IdToken' of undefined. Can you help me to provide the lambda code for reference. ",
    "JTGaag": "@ChuntheQhai @chrisradek I had the same issue, a few days ago it worked with the default settings, but recently it gave the same \"SignatureDoesNotMatch\" response. I changed the signature version to v4 as @chrisradek mentioned and this did the trick for me. . ",
    "juancabrera": "Just a quick update with more information.\nI'm using aws-sdk v2.6.12 on node v6.4.0\n. Anything else you guys need to provide some feedback ?\n. ",
    "pex": "@juancabrera, I am currently experiencing the same issue (using version 2.7.21). Did you get an answer on how long it takes for changes to take effect?\nThough this might be a question about CloudWatchEvents in general I am only experiencing this behaviour when creating / updating CloudWatchEvents programmatically via API. Using AWS console interfaces reflects changes immediately.. ",
    "Anike1204": "yes it still loads\n. when i close my command prompt it doesn't load however when i reconnect, it loads normally then i get the error message again anytime i run a script. i used it yesterday and it worked perfectly\n. i am using the dynamoDB javascript shell. I got all my information from the website. what can I do please, i even restarted my system already\n. below is my cmd screenshot\n\n. please do you have anyway link to help with that? do I need to configure or download anything.\n. i also need an access key i remember that was why i didn't use the Python \n. okay... let me see what i can do, hopefully its easy to implement.\nthanks\n. ",
    "fshirzadi": "Instead of PhoneNumber try TopicArn: SNSTopicArn that did the trick for Android.\nYou need to choose either PhoneNumber or TopicArn, can't have both\nAlso, You are missing + in \"PhoneNumber: '66XX0122520'\" \nHope that helps.\n. Instead of PhoneNumber try TopicArn: SNSTopicArn that did the trick for Android.\nYou need to choose either PhoneNumber or TopicArn, can't have both\nAlso, You are missing + in \"PhoneNumber: '66XX0122520'\" \nHope that helps.\n. ",
    "pmwisdom": "Just in case anyone ever runs into this, it may be caused by an incorrect region. For me I chose us-west-1 which is not a supported SMS region. List of supported regions here: http://docs.aws.amazon.com/sns/latest/dg/sms_supported-countries.html. ",
    "thegreatgabsby": "And make sure your actual DefaultSenderID is valid: No spaces, Max 11 alphanumerical chars.\nI had something like 'Test App' and it would complain about InvalidParameter. And make sure your actual DefaultSenderID is valid: No spaces, Max 11 alphanumerical chars.\nI had something like 'Test App' and it would complain about InvalidParameter. ",
    "jacobdr": "Not sure what the use case is specifically here, but in general publicly exposing the errors allows you to use certain conveniences around catching errors. \nFor example, the bluebird promise library exposes an .catch method that could be used like this:\n``` typescript\nimport {AWSError} from 'aws-sdk';\nimport * as Promise from 'bluebird';\nPromise.resolve(uploadToS3)\n    .catch(AWSError, (error) => handler)\n    .catch((error)=> {\n        // Handle all other errors down here\n    })\n```\n. ",
    "lukiano": "In my case it's for \"instance of\"s and accessing the error fields like code\n. Also, in error.d.ts, it should say extendedRequestId instead of extendedRequstId\n. ",
    "ehartford": "Of course there is a use case for exporting AWSError.\nIt's a commonly used error class, and we need it.\nOtherwise we are using 'any'\nThere's no reason we should have to do that when there's a perfectly good class that just needs to be exported.\nI made a PR please review . it's not useful to create them,\nit's only useful in interpreting them when aws sdk produces them.\nin short there's no need to make that class visible in raw js.  if you want\nto know what it contains just console.log one.\nOn Mon, Apr 23, 2018, 1:24 AM radishlau notifications@github.com wrote:\n\nIs there anyway to import AWSError in a project not using typescript?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1219#issuecomment-383494559,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABEOBUdE__qg3kqtrawvOi6lJBiPt6DDks5trY-zgaJpZM4KvXu4\n.\n. https://github.com/aws/aws-sdk-js/issues/1219. Cool thanks!\n\nOn Feb 18, 2017 9:08 AM, \"Jonathan Eskew\" notifications@github.com wrote:\n\nMerged #1355 https://github.com/aws/aws-sdk-js/pull/1355.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/pull/1355#event-968056574, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABEOBZHIItc5ajAffGJOge96p29YQtC3ks5rdyV0gaJpZM4L9DBr\n.\n. \n",
    "paklau": "Is there any way to import AWSError in a project not using typescript?. ",
    "austinried": "\ud83d\udc4d and thank you, this started failing builds for me last night.\n. ",
    "mrapptambour": "Thank you as well, uncovered just this morning. You're on top of it!\n. ",
    "marcote": "Kudos goes to my boss @PabloZaiden who discovered the bug and suggested the fix .\n. @blakeembrey the bug about implicit any it's already fixed in #1221 \n. ",
    "PabloZaiden": "Thanks!. Hope it gets pushed to npm soon :)\n. ",
    "maghis": "@chrisradek yes, I was using https://www.npmjs.com/package/@djabry/aws-sdk-typescript (it's a fork of a project that auto-generates them).\nWhen will this be pushed to npm?\n. @blakeembrey see #1224, any other suggestion?\n. @blakeembrey I didn't encounter any issue because of the rootDir but I have tried just a small subset. This is so much better than the funky types I was using before :smile: \n. @blakeembrey ok, I tested and it looks like it doesn't make any difference.\nFor reference, I tested by adding an invalid line in one of the root .d.ts and the test failed as expected.\n. @chrisradek I have synced this pull with master, any feedback?\n. @chrisradek np, thanks! :). sorry @srchase, I did not notice your message.\nYou can simply instantiate the client with:\nconst client = new DynamoDB.DocumentClient({ region: \"us-east-1\" });\nand then call any of the methods.\nThis is the line where it's trying to set a value on the original options object: https://github.com/aws/aws-sdk-js/blob/master/lib/dynamodb/document_client.js#L71. ",
    "blakeembrey": "@marcote Shouldn't you actually enable noImplicitAny so this won't happen again? See https://github.com/aws/aws-sdk-js/blob/master/ts/tsconfig.json. Here's the config I usually recommend for people writing type definitions - https://github.com/types/guidelines#tsconfigjson.\n. @marcote Shouldn't you actually enable noImplicitAny so this won't happen again? See https://github.com/aws/aws-sdk-js/blob/master/ts/tsconfig.json. Here's the config I usually recommend for people writing type definitions - https://github.com/types/guidelines#tsconfigjson.\n. @chrisradek I had my own local definitions that were incomplete (mostly just polyfilled for what I was using). \n\nFor your third point\n\nIt's not entirely true. The types reference only causes a error to be emitted by TypeScript, but all the definitions still exist within TypeScript would resolve. Basically, it emits Cannot find type definition file for 'node' instead of Cannot find module 'http'. It's up to you whether you remove it, though me and a number of others won't be able to use it unless you do and the alternative errors may be preferred - especially if you're typing a definition that shouldn't require node anyway (like the browser version of this library, I shouldn't really need to install node for that).\nEdit: Sorry, I just realised it might still be unclear. You can absolutely use any of the imports, it's just the type reference breaking people not using @types right now. Instead, you can rely on TypeScript encountering the import and not finding it (it will find it anyway if someone is using @types/node, regardless of the reference).\n. @chrisradek I had my own local definitions that were incomplete (mostly just polyfilled for what I was using). \n\nFor your third point\n\nIt's not entirely true. The types reference only causes a error to be emitted by TypeScript, but all the definitions still exist within TypeScript would resolve. Basically, it emits Cannot find type definition file for 'node' instead of Cannot find module 'http'. It's up to you whether you remove it, though me and a number of others won't be able to use it unless you do and the alternative errors may be preferred - especially if you're typing a definition that shouldn't require node anyway (like the browser version of this library, I shouldn't really need to install node for that).\nEdit: Sorry, I just realised it might still be unclear. You can absolutely use any of the imports, it's just the type reference breaking people not using @types right now. Instead, you can rely on TypeScript encountering the import and not finding it (it will find it anyway if someone is using @types/node, regardless of the reference).\n. @chrisradek No problem, happy to help. I usually help people migrate to supporting TypeScript definitions, so if you ever need a second pair of eyes you can ping me. See https://github.com/typings/typings/issues/322 (little outdated, but I still help with other repos). If those two things can be fixed, it'd be amazing \ud83d\ude04 \n@midknight41 Once the definitions here work and are stable, no one should need to rely on DefinitelyTyped anymore. I wouldn't want the developers here getting stuck trying to maintain two repo locations and having to refactor these external module definitions into global definitions for DefinitelyTyped.\n. @chrisradek No problem, happy to help. I usually help people migrate to supporting TypeScript definitions, so if you ever need a second pair of eyes you can ping me. See https://github.com/typings/typings/issues/322 (little outdated, but I still help with other repos). If those two things can be fixed, it'd be amazing \ud83d\ude04 \n@midknight41 Once the definitions here work and are stable, no one should need to rely on DefinitelyTyped anymore. I wouldn't want the developers here getting stuck trying to maintain two repo locations and having to refactor these external module definitions into global definitions for DefinitelyTyped.\n. It seems like it's still an issue, just updated to 2.7.9 and got:\nnode_modules/aws-sdk/lib/config.d.ts(1,1): error TS2688: Cannot find type definition file for 'node'.\nnode_modules/aws-sdk/lib/request.d.ts(1,1): error TS2688: Cannot find type definition file for 'node'.\nnode_modules/aws-sdk/lib/services/glacier.d.ts(1,1): error TS2688: Cannot find type definition file for 'node'.\nsrc/support/upload.ts(115,14): error TS2339: Property 'on' does not exist on type 'ManagedUpload'.\nThe last one is because I'm listening to the httpUploadProgress on http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3/ManagedUpload.html.. It seems like it's still an issue, just updated to 2.7.9 and got:\nnode_modules/aws-sdk/lib/config.d.ts(1,1): error TS2688: Cannot find type definition file for 'node'.\nnode_modules/aws-sdk/lib/request.d.ts(1,1): error TS2688: Cannot find type definition file for 'node'.\nnode_modules/aws-sdk/lib/services/glacier.d.ts(1,1): error TS2688: Cannot find type definition file for 'node'.\nsrc/support/upload.ts(115,14): error TS2339: Property 'on' does not exist on type 'ManagedUpload'.\nThe last one is because I'm listening to the httpUploadProgress on http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3/ManagedUpload.html.. LGTM \ud83d\udc4d Do you think the rootDir should be changed just in case some .d.ts files are missed? Not sure if that's an issue here, but looks great \ud83d\ude04 \n. LGTM \ud83d\udc4d Do you think the rootDir should be changed just in case some .d.ts files are missed? Not sure if that's an issue here, but looks great \ud83d\ude04 \n. Sounds good. I'm excited to be using it myself too, once it's working for me \ud83d\ude04 I only brought up rootDir because I believe the root is treated at where the tsconfig.json file is located otherwise (but I could be wrong, there's too many ways to resolve and I can't remember how that one works because I don't really use it with -p).\n. Sounds good. I'm excited to be using it myself too, once it's working for me \ud83d\ude04 I only brought up rootDir because I believe the root is treated at where the tsconfig.json file is located otherwise (but I could be wrong, there's too many ways to resolve and I can't remember how that one works because I don't really use it with -p).\n. Unfortunately TypeScript doesn't support browser resolution, I have an older issue in the TypeScript repo regarding this as that'd be the best solution for browser type separation. Was there an issue with the streams for now? I haven't used the SDK a ton with TypeScript, but the places you're using these types should mostly be configured based on input options right? Would it make sense to leave it semi-configurable to the user through generics to avoid the messy parts?\nNow I think about it, default generic types may also be used here for narrowing the return type, but that can be an unrelated enhancement I'll comment when TypeScript supports it.\n. Unfortunately TypeScript doesn't support browser resolution, I have an older issue in the TypeScript repo regarding this as that'd be the best solution for browser type separation. Was there an issue with the streams for now? I haven't used the SDK a ton with TypeScript, but the places you're using these types should mostly be configured based on input options right? Would it make sense to leave it semi-configurable to the user through generics to avoid the messy parts?\nNow I think about it, default generic types may also be used here for narrowing the return type, but that can be an unrelated enhancement I'll comment when TypeScript supports it.\n. ",
    "midknight41": "Firstly, thank you for creating these definitions! I've been involved in maintaining these definitions at DefinitelyTyped for years and I can't keep up with you folks. :frowning: \nhttps://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/aws-sdk/aws-sdk.d.ts\nThat said, this broke my builds too. I've not using @types yet either.\nAre you planning to contribute the definitions to DefinitelyTyped? Your definitions should replace mine ideally!\n. The main thing is making sure the community ends up with the correct definitions (yours) and avoids any kind of collision regardless of how they install it.\nIf I can help, let me know. :-)\n. @MarcusNoble\nOnce the official definitions are complete the ones you've been using (from DefinitelyTyped I believe) will be deprecated. Despite best efforts they are missing many services and haven't kept up with all the changes over the years. . There are two issues here actually. I've defined it badly.\n1. You cannot explicitly state the variable type\n2. The AWS.DynamoDB namespace does not contain a DocumentClient class. new AWS.DynamoDB.DocumentClient() gives a error.\nI'll have a peek at the other issue first I think.\n. There is quite a lot of variation between the DefinitelyTyped ones and the ones AWS are producing. There will be lots of breaking changes but, ultimately, the definitions here will be a lot better and maintained better.\n. @chrisradek In regards to the DynamoDB.DocumentClient class, I was getting compile errors but I had a few things wrong with my setup.\nI'm tidying thing up at present. I'll feedback once I've done that.\n. @chrisradek the new AWS.DynamoDB.DocumentClient error was a red herring. Ignore that one.\nWith regard to the fixing the first issue: What is the expectation in terms of the namespaces?\nFor the DocumentClient is it:\njs\nconst client: AWS.DynamoDB.DocumentClient;\nor\njs\nconst client: AWS.DynamoDB.Types.DocumentClient;\nand for the Types exposed for DocumentClient is it:\njs\nconst params: AWS.DynamoDB.DocumentClient.Types.GetItemInput;\nor\njs\nconst params: AWS.DynamoDB.DocumentClient.GetItemInput;\nor something else? I think they can't be in the same namespace as the mainDynamoDB.Types namespace as I believe there are some naming collisions.\nI have to admit, I find the Types namespace a bit counter-intuitive but there you go.\nIf I'm to help with this I'll need to understand which files are generated and which ones are maintained by hand. Could you enlighten me?\n. @chrisradek I'll have a look over this on the way home and see if I can suggest anything better. . Hey @chrisradek,\nIn terms of minimising the amount of code, combining the type definitions for DynamoDB and DynamoDB.DocumentClient into a single file yields the most benefit I think. The DocumentClient API is only slightly different that the main DynamoDB API and their is tonnes of duplication between the DynamoDB.DocumentClient.Types namespace and the DynamoDB.Types namespace.\nWith both namespaces in the same file you avoid mapping every type from the DCInterfaces.d.ts file to the namespace declared in the main client\\dynamo.d.ts file.\nI've hacked a version of that together locally which I can put up somewhere if you'd like to see it.\n. @chrisradek \nHere's the PoC as discussed:\nhttps://github.com/aws/aws-sdk-js/compare/master...midknight41:master\nHere's the test file:\nhttps://github.com/midknight41/aws-sdk-js/blob/master/ts/dynamodb.ts\nI've not eliminated all the duplicates but it's easy to see where you can chop out lots of duplicate declarations. Might be a tidy solution with a generic to cut them down a bit more...not quite sure without looking at it more.\nHope that helps.. Looks really good! . Will do.\n. ",
    "MarcusNoble": "I'm also seeing similar issues in latest version:\napp/services/AwsQueueClient.ts(32,32): error TS2305: Module 'SQS' has no exported member 'SendMessageResult'.\napp/services/AwsQueueClient.ts(34,30): error TS2305: Module 'SQS' has no exported member 'SendMessageParams'.\napp/services/AwsQueueClient.ts(38,69): error TS2305: Module 'SQS' has no exported member 'SendMessageResult'.\napp/services/AwsQueueClient.ts(63,32): error TS2305: Module 'SQS' has no exported member 'Message'.\napp/services/AwsQueueClient.ts(66,30): error TS2305: Module 'SQS' has no exported member 'ReceiveMessageParams'.\napp/services/AwsQueueClient.ts(72,74): error TS2305: Module 'SQS' has no exported member 'ReceiveMessageResult'.\napp/services/AwsQueueClient.ts(104,30): error TS2305: Module 'SQS' has no exported member 'DeleteMessageParams'.\napp/services/QueueWorker.ts(81,52): error TS2305: Module 'SQS' has no exported member 'Message'.\napp/types/queues.d.ts(6,41): error TS2305: Module 'SQS' has no exported member 'SendMessageResult'.\napp/types/queues.d.ts(7,43): error TS2305: Module 'SQS' has no exported member 'Message'.. I've just taken another look (in another project that i'm working on) and it seems that the Message has moved from aws.SQS.Message to aws.SQS.Types.Message. Was this intentional? . Yes. I previously used typings, then used the @types package on npm. \nIs it best to stop using those and update code to use the .Types. namespace?. ",
    "forbesmyester": "To further this... This code is valid and should work (it did with DT/@types typings):\nlet s3obj = new AWS.S3();\nlet body = createReadStream(srcPath).pipe(createGzip());\nlet params = {\n    Body: body,\n    Bucket: getEnv('UPLOAD_S3_BUCKET'),\n    ContentEncoding: 'gzip',\n    Key: 'some-key-here',\n};\ns3obj.upload(params, next);\n. ",
    "DavyCode": "I am using AWS sdk for uploads, after spending some time searching online i stumbled upon this thread. thanks to @lsimoneau 45581857 its turns out the exact same thing was happening. I simply pointed my request Url to the region on my bucket by attaching the region property and it worked.\nconst s3 = new AWS.S3({\n  accessKeyId: config.awsAccessKeyID,\n  secretAccessKey: config.awsSecretAccessKey,\n  region: 'eu-west-2'  // add region here\n});\n. ",
    "phubbard": "Apologies, but I'm new to TS and have inherited this codebase. How do I answer your question? Do you mean 'is node in the package.json'?\nAdditional info: Pinning the aws-sdk version to 2.6.15 fixes the compile error, so I have a workaround.\n. That leads to a different error:\nnode_modules/@types/node/index.d.ts(70,5): error TS2403: Subsequent variable declarations must have the same type.  Variable 'main' must be of type 'any', but here has type 'NodeModule'.\n[14:42:36] gulp-notify: [Gulp] Typescript compiler error\n. 2.0.6\n. Sure, happy to. Again, I'm a newbie, so which files would help?\n. Still broken - the typings directory has aws-sdk, bluebird, connect, cors, crypto-js, express-serve-static-core, form-data, lodash, mime, modules, moment, mustache, node, nodemailer, nodemailer-direct-transport, nodemailer-ses-transport, nodemailer-smtp-transport, redis, request, serve-static, uuid, validator, winston and ws. node_modules/aws-sdk/lib/config.d.ts(1,1): error TS2688: Cannot find type definition file for 'node'.\n[09:02:46] gulp-notify: [Gulp] Typescript compiler error. \nThat fixed it - thank you! \n\nOn Dec 7, 2016, at 3:17 PM, Christopher Radek notifications@github.com wrote:\n@phubbard https://github.com/phubbard\nI went through and removed all references to /// . I also updated the typings to better match patterns some 3rd party typings used.\nCan you attempt using the latest version of the SDK again? If it still doesn't work, can you share your typings.json file?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/aws/aws-sdk-js/issues/1229#issuecomment-265604725, or mute the thread https://github.com/notifications/unsubscribe-auth/AAJl5mPbQ7dZ6ooVrAfElmuunJtKCLVpks5rFz6OgaJpZM4K0rEB.\n\n\n. ",
    "Roam-Cooper": "The problem is that the typings tool only strips references in the typings that it installs whereas there's no way to strip references in a declaration file in a node module afaik. Putting a declaration file in your node module is not a smart move.\nReferences to node in other declarations installed by typings get stripped and replaced with a reference to the common node declaration index.d.ts file. Having a .d.ts in your project breaks this.. ",
    "sudheerg9": "We had the same issue and we are using aws-sdk 2.128.0. ",
    "shystruk": "Hi all,\nI have the same issue with s3.listObjects.\nIn browser the request is canceled (Provisional headers are shown).\nHas anyone found a solution?\n. Close please\nOn Tue, Aug 28, 2018, 14:15 Chase Coalwell notifications@github.com wrote:\n\n@jacktuck https://github.com/jacktuck\nWere you able to get this working?\nIf it's no longer an issue, I'd like to close this out.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1230#issuecomment-416743027,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFuLt-ANyiQHGloQBqSoq4neA0sQfgDpks5uVbLkgaJpZM4K1Ind\n.\n. \n",
    "jacktuck": "This was some time ago not sure what i did in the end. \nI was using some unexposed API in my example so i'm sure that might have caused some issues there.\nHappy to close.. This was some time ago not sure what i did in the end. \nI was using some unexposed API in my example so i'm sure that might have caused some issues there.\nHappy to close.. ",
    "AndrewBarba": "Thanks for the response @chrisradek ! So to give more context, the screenshots above are AFTER I set the WaitTime to just 1 second. I needed to do that so I could test things faster. Originally it was set to the max of 20 seconds and would still climb but obviously much more slowly, and hit the limit every 6 hours or so. We have 4 queues on that node so that was 4 https requests every 20 seconds, not exactly anything crazy... \nBut to actually answer your question, no I never tried keeping SSL enabled with a custom https agent. When I read the docs regarding ssl I remember seeing that you guys use a custom agent when ssl is enabled so I wasn't sure if I should be supplying my own. With ssl off you use the global agent so I figured it didn't hurt to give my own in that case.\nHappy to test again with ssl enabled and a custom agent. I can also share the script that we use. I'll post the relevant piece below, and if you need full script let me know and I can email it to you:\n```\n  /*\n   * @method process\n   /\n  process(concurrency, handler) {\n    if (arguments.length === 1 && _.isFunction(concurrency)) {\n      handler = concurrency;\n      concurrency = DEFAULT_CONCURRENCY;\n    }\nlet queueOptions = {\n  MaxNumberOfMessages: concurrency,\n  WaitTimeSeconds: DEFAULT_WAIT_TIME\n};\n\nthis._loadQueueOptions(queueOptions)\n  .then(options => {\n    return sqs.receiveMessage(options).promise();\n  })\n  .then(data => {\n    return data.Messages || [];\n  })\n  .mapSeries(message => {\n    let body = JSON.parse(message.Body);\n    return handler(body, message).reflect()\n      .then(() => this.complete(message));\n  })\n  .finally(() => {\n    process.nextTick(() => this.process(concurrency, handler));\n  });\n\n}\n```\n. Thanks for the response @chrisradek ! So to give more context, the screenshots above are AFTER I set the WaitTime to just 1 second. I needed to do that so I could test things faster. Originally it was set to the max of 20 seconds and would still climb but obviously much more slowly, and hit the limit every 6 hours or so. We have 4 queues on that node so that was 4 https requests every 20 seconds, not exactly anything crazy... \nBut to actually answer your question, no I never tried keeping SSL enabled with a custom https agent. When I read the docs regarding ssl I remember seeing that you guys use a custom agent when ssl is enabled so I wasn't sure if I should be supplying my own. With ssl off you use the global agent so I figured it didn't hurt to give my own in that case.\nHappy to test again with ssl enabled and a custom agent. I can also share the script that we use. I'll post the relevant piece below, and if you need full script let me know and I can email it to you:\n```\n  /*\n   * @method process\n   /\n  process(concurrency, handler) {\n    if (arguments.length === 1 && _.isFunction(concurrency)) {\n      handler = concurrency;\n      concurrency = DEFAULT_CONCURRENCY;\n    }\nlet queueOptions = {\n  MaxNumberOfMessages: concurrency,\n  WaitTimeSeconds: DEFAULT_WAIT_TIME\n};\n\nthis._loadQueueOptions(queueOptions)\n  .then(options => {\n    return sqs.receiveMessage(options).promise();\n  })\n  .then(data => {\n    return data.Messages || [];\n  })\n  .mapSeries(message => {\n    let body = JSON.parse(message.Body);\n    return handler(body, message).reflect()\n      .then(() => this.complete(message));\n  })\n  .finally(() => {\n    process.nextTick(() => this.process(concurrency, handler));\n  });\n\n}\n```\n. This file throws the error on v2.368.0:\nnode ./aws-test.js\n```javascript\n// aws-test.js\nconst AWS = require('aws-sdk')\nconst Promise = require('bluebird')\nconst https = require('https')\nAWS.config = new AWS.Config({\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\n  region: 'us-east-1',\n  httpOptions: {\n    agent: new https.Agent({\n      keepAlive: true\n    })\n  }\n})\n// Setup promise\nAWS.config.setPromisesDependency(Promise)\nmodule.exports = AWS\n```\nError:\n```\ninternal/modules/cjs/loader.js:582\n    throw err;\n    ^\nError: Cannot find module './licensemanager'\n    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:580:15)\n    at Function.Module._load (internal/modules/cjs/loader.js:506:25)\n    at Module.require (internal/modules/cjs/loader.js:636:17)\n    at require (internal/modules/cjs/helpers.js:20:18)\n    at Object. (Redacted/node_modules/aws-sdk/clients/all.js:168:19)\n    at Module._compile (internal/modules/cjs/loader.js:688:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:699:10)\n    at Module.load (internal/modules/cjs/loader.js:598:32)\n    at tryModuleLoad (internal/modules/cjs/loader.js:537:12)\n    at Function.Module._load (internal/modules/cjs/loader.js:529:3)\n```. This file throws the error on v2.368.0:\nnode ./aws-test.js\n```javascript\n// aws-test.js\nconst AWS = require('aws-sdk')\nconst Promise = require('bluebird')\nconst https = require('https')\nAWS.config = new AWS.Config({\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\n  region: 'us-east-1',\n  httpOptions: {\n    agent: new https.Agent({\n      keepAlive: true\n    })\n  }\n})\n// Setup promise\nAWS.config.setPromisesDependency(Promise)\nmodule.exports = AWS\n```\nError:\n```\ninternal/modules/cjs/loader.js:582\n    throw err;\n    ^\nError: Cannot find module './licensemanager'\n    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:580:15)\n    at Function.Module._load (internal/modules/cjs/loader.js:506:25)\n    at Module.require (internal/modules/cjs/loader.js:636:17)\n    at require (internal/modules/cjs/helpers.js:20:18)\n    at Object. (Redacted/node_modules/aws-sdk/clients/all.js:168:19)\n    at Module._compile (internal/modules/cjs/loader.js:688:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:699:10)\n    at Module.load (internal/modules/cjs/loader.js:598:32)\n    at tryModuleLoad (internal/modules/cjs/loader.js:537:12)\n    at Function.Module._load (internal/modules/cjs/loader.js:529:3)\n``. To simplify, anyrequire('aws-sdk')is failing for me. Node v8.10. Installing viayarn install. To simplify, anyrequire('aws-sdk')is failing for me. Node v8.10. Installing viayarn install`. @mrapczynski I tried that but it seem like Serverless is now installing this version and my deploys are failing even though my code runs fine with older version.. @mrapczynski I tried that but it seem like Serverless is now installing this version and my deploys are failing even though my code runs fine with older version.. I'm using yarn, and if you look at the example above it's not using serverless at all. I'm just running that one file with node and it crashes.\nI edited the lock file to just install 2.365.0 and it works fine.. I'm using yarn, and if you look at the example above it's not using serverless at all. I'm just running that one file with node and it crashes.\nI edited the lock file to just install 2.365.0 and it works fine.. Hey guys I'm really sorry, looks like the issue for me was my .yarnclean file which had an entry for LICENSE.*. After removing that its working fine. Hey guys I'm really sorry, looks like the issue for me was my .yarnclean file which had an entry for LICENSE.*. After removing that its working fine. ",
    "sgtoj": "Other semi related problems are some interfaces are not exported:\n- AWS.DynamoDB.PutParam\n- AWS.DynamoDB.GetParam\n- AWS.DynamoDB.UpdateParam\n- AWS.DynamoDB.QueryParam\n- AWS.DynamoDB.ScanParam\n. Thanks... You're correct. I was using @types/aws-sdk before types definition were added (v2.7.0?) to the package. Those definitions from DefinitelyType has it defined differently.\n. That is my hope. I wasn't aware of the packaged types so I used the DefinitelyTyped one. That is until today. I will be using the native definition from this point forward.\n. You're correct. Using the wrong profile. Closing.. ",
    "MZhoume": "@midknight41 Same issue here. Any updates?\n. ",
    "dinvlad": "So how do we deal with this issue at the moment? Thanks\nFWIW DynamoDB.Types also doesn't seem to be exported, at least in 2.7.5\n. Ok, that's right -\nconst db = new DynamoDB.DocumentClient();\nseems to work for me as well. At the same time,\n```\nModule 'DynamoDB' has no exported member 'DocumentClient'\nModule 'DynamoDB.Types' has no exported member 'DocumentClient'\n```\nSo the workaround for now if we want to pass this object around, is to pass it as 'any' type. Thanks\n. Thanks - this is mainly for signing requests to 'execute-api' on API Gateway.\nWhile we can indeed generate an SDK for that API using the AWS console or this SDK's getSdk() call, it seems less convenient to do that every time we make a change to the API, especially during development and testing. It would be extremely useful to rely on an existing generic library to make direct calls to IAM-protected methods on the API Gateway instead of generating a custom one. In addition, we could use existing Typescript definitions for making those calls.. FWIW with regards to Swagger, I ended up switching to JWT authentication in part because of this issue. It is much easier to auto-generate a Typescript client (for Angular) from a Swagger spec than to generate SDK from API Gateway and create the bindings manually. As a side benefit, JWT allows for more flexibility both in terms of dynamic permissions and clients (no need for special software, e.g. Curl can now be used); and we can also use third-party authentication solutions, e.g. Auth0 or Stormpath (Cognito is unfortunately severely lacking in graphical clients, IMO).. Since then we've moved to Cognito User Pool Auth with JWT authorizer, since Cognito UP has improved substantially in the past half-year. Still, it has a few annoying issues that make it not completely fluent to use (e.g. linking multiple accounts). I wish everything was open-source and extensible out of the box. IAM/SigV4 is a hard dependency to bring around.. Could we please re-open this issue? IfMatch is still indicated as an optional field in the SDK, even though the API lists it as required. That creates unnecessary confusion and unexpected errors - in my case it's The If-Match version is missing or not valid for the resource for (update|delete)CloudFrontOriginAccessIdentity().\nThanks!. Sorry for stealing the thread - I thought that it was about If-Match field, while it's just the same error message. I'll create a new issue here as I believe there are problems with this SDK's documentation.. Ok, thank you. Will ask it on the forums.\nEDIT: the (preview) CLI docs mirror (2) as well. Please feel free to close this issue as non SDK-specific (or I will update here on my progress with the forums).. ",
    "uxsoft": "I can't wait for this to be ready! :-). ",
    "karlwhite": "Also, if I do try to use AttributeValues instead, I get the following for the DocumentClient:\njson\n{\n  \"errorMessage\": \"shape.toType is not a function\",\n  \"errorType\": \"TypeError\",\n  \"stackTrace\": [\n    \"Translator.translateScalar (/var/runtime/node_modules/aws-sdk/lib/dynamodb/translator.js:76:16)\",\n    \"Translator.translate (/var/runtime/node_modules/aws-sdk/lib/dynamodb/translator.js:30:26)\",\n    \"Object.<anonymous> (/var/runtime/node_modules/aws-sdk/lib/dynamodb/translator.js:42:25)\",\n    \"Object.each (/var/runtime/node_modules/aws-sdk/lib/util.js:468:32)\",\n    \"Translator.translateStructure (/var/runtime/node_modules/aws-sdk/lib/dynamodb/translator.js:39:8)\",\n    \"Translator.translate (/var/runtime/node_modules/aws-sdk/lib/dynamodb/translator.js:27:35)\",\n    \"Translator.translateInput (/var/runtime/node_modules/aws-sdk/lib/dynamodb/translator.js:11:15)\",\n    \"Request.<anonymous> (/var/runtime/node_modules/aws-sdk/lib/dynamodb/document_client.js:471:31)\",\n    \"Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\",\n    \"Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\",\n    \"Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:668:14)\",\n    \"Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)\",\n    \"AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)\",\n    \"Request.runTo (/var/runtime/node_modules/aws-sdk/lib/request.js:394:15)\",\n    \"Request.send (/var/runtime/node_modules/aws-sdk/lib/request.js:358:10)\",\n    \"DocumentClient.put (/var/runtime/node_modules/aws-sdk/lib/dynamodb/document_client.js:305:15)\"\n  ]\n}\nand this for dynamodb-doc:\njson\n{\n  \"errorMessage\": \"Unrecognized Datatype to be formatted.\",\n  \"errorType\": \"Error\",\n  \"stackTrace\": [\n    \"DynamoDBDatatype.formatDataType (/var/runtime/node_modules/dynamodb-doc/lib/datatypes.js:115:19)\",\n    \"DynamoDBDatatype.formatRecursiveType (/var/runtime/node_modules/dynamodb-doc/lib/datatypes.js:61:31)\",\n    \"DynamoDBDatatype.formatDataType (/var/runtime/node_modules/dynamodb-doc/lib/datatypes.js:113:40)\",\n    \"formatAttrValInput (/var/runtime/node_modules/dynamodb-doc/lib/formatter.js:19:41)\",\n    \"Request.DynamoDBFormatter.formatInput (/var/runtime/node_modules/dynamodb-doc/lib/formatter.js:281:29)\",\n    \"Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\",\n    \"Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\",\n    \"Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:668:14)\",\n    \"Request.transition (/var/runtime/node_modules/aws-sdk/lib/request.js:22:10)\",\n    \"AcceptorStateMachine.runTo (/var/runtime/node_modules/aws-sdk/lib/state_machine.js:14:12)\",\n    \"Request.runTo (/var/runtime/node_modules/aws-sdk/lib/request.js:394:15)\",\n    \"Request.send (/var/runtime/node_modules/aws-sdk/lib/request.js:358:10)\",\n    \"makeRequest (/var/runtime/node_modules/aws-sdk/lib/service.js:180:27)\",\n    \"svc.(anonymous function) [as putItem] (/var/runtime/node_modules/aws-sdk/lib/service.js:430:23)\",\n    \"/var/task/index.js:50:32\",\n    \"tryCatcher (/var/task/node_modules/lob/node_modules/bluebird/js/release/util.js:16:23)\"\n  ]\n}. Anyone have any clues?\nI have worked around this by using dynamodb-marshaler, however I would prefer to work directly using json without additional dependencies.. I have confirmed that the same issue occurs with the bundled document client, but weirdly only with certain Lambda functions.\nE.g. the following fails with validation errors (expecting Attribute Values) in some functions, but works fine in others.\nvar AWS = require('aws-sdk');\nvar docClient = new AWS.DynamoDB.DocumentClient({region: 'us-west-2'});\n...\ndocClient.put(params, function(err, res) {\n    ...\nBoth functions are configured the same (same runtime, memory, etc). Is it possible that the bundled document client simply isn't the same version in older functions?. @delprofundo No, unfortunately not (although I haven't revisited since). I used the \"dynamodb-marshaler\" npm as a workaround, and that worked for these particular functions (albeit by adding a huge package to the lambda function), and I've not really touched them since.\nNo clue why it only occurs with certain lambda functions and not others (despite identical setup).. ",
    "pargee": "Facing the same issue myself :( Been struggling with coming up with any real solutions.. ",
    "hagen": "@karlwhite I see some functions exhibiting this behaviour too, while others appear to be fine. I know it's a frustration to hear, but it's usually an issue with our code. For example, I've been struggling massively with this error, because it doesn't explain the actual the problem. In my case, I was populating an Item property with a null. Please check that your params object has all properties populated. DynamoDB will not insert nulls. Can you post a full snippet of code that's not working?. ",
    "delprofundo": "@karlwhite did you ever solve this? I've had this just come up in a single lambda, using the document client. the db is passed in as is the library of functions to call it, so there are no syntax errors, i can see in my logging that the payload is correct and i still get the same error. 25 other functions fine.. @karlwhite did you ever solve this? I've had this just come up in a single lambda, using the document client. the db is passed in as is the library of functions to call it, so there are no syntax errors, i can see in my logging that the payload is correct and i still get the same error. 25 other functions fine.. > I was managed to achieve this in serverless architecture by creating a Canonical Request for each part upload using Signature Version 4. You will find the document here https://sandyghai.github.io/AWS-S3-Multipart-Upload-Using-Presigned-Url/\ndo you have a code example? the instructions aren't really that clear in my case.. > I was managed to achieve this in serverless architecture by creating a Canonical Request for each part upload using Signature Version 4. You will find the document here https://sandyghai.github.io/AWS-S3-Multipart-Upload-Using-Presigned-Url/\ndo you have a code example? the instructions aren't really that clear in my case.. ",
    "var1ableX": "I just figure it out how to reproduce this. Add this code to your lambda somewhere:\nObject.prototype.isEmpty = function() {\n    for(var key in this) {\n        if(this.hasOwnProperty(key))\n            return false;\n    }\n    return true;\n}. pilhokim: AWS\u2019s recommended library for doing this work doesn\u2019t require the explicit references in normal scenarios.\nBut to be comprehensive, my Code was working fine, then suddenly it stopped working. I started adding explicit references and one by one the errors related to the sort of code you have in your  snippet resolved. Until eventually the error moved to a deeper level (I believe a constructor call but it\u2019s been a while). So I reverted to the original code with implicit conversions.\nI then remembered the prototype I\u2019d added (see my last comment re isempty). I removed that and the code started working again.. @supportcoinsafer - might want to double check whether you've added any prototypes to Object. Or added another library that may have done something like that.. ",
    "pilhokim": "Check your code if you have specified a data type. For instance, compare your code with the below one and check if you have specified \"{N:\" for numbers and \"{S:\" for strings:\n```\nvar params = {\n  TableName: 'TABLE',\n  Item: {\n    'CUSTOMER_ID' : {N: '001'},\n    'CUSTOMER_NAME' : {S: 'Richard Roe'},\n  }\n};\n// Call DynamoDB to add the item to the table\nddb.putItem(params, function(err, data) {\n  if (err) {\n    console.log(\"Error\", err);\n  } else {\n    console.log(\"Success\", data);\n  }\n});\n```. ",
    "alankarmisra": "@var1ableX You saved my life! I had prototypes on Object and everything magically stopped working. Prototypes removed and everything works now!. ",
    "djalexd": "After some investigation, discovered that it has nothing to do with aws-sdk client, but JS:\nObject.prototype.doSomething = function() {\n  return 'test'\n}\nvalues = {}\nconsole.log(`doSomething = ${values['doSomething']}`)\nOutputs the function contents. \nComing back to the library, perhaps somewhere around this line https://github.com/aws/aws-sdk-js/blob/master/lib/dynamodb/translator.js#L46 one could check the typeof shape, and when this is a function, simply ignore?\n. ",
    "stevenelson94708": "The pre-signed URL is generated by common node.js code running on a Ubuntu 16.04 server. The identical pre-signed URL works fine with a Postman client on Ubuntu 16.04, but fails with a Postman client on the Mac.\nHere is a an example of a pre-signed URL that works with a Postman client on Ubuntu 16.04 but gets the NotImplemented error with a Postman client on the Mac:\n`https://nuubissnelsontestbucket.s3-us-west-2.amazonaws.com/5838bae502d21e140ab892d3/5838bb7a02d21e140ab892ec/e0092910-7f15-4ca0-a261-090fb759aeb1_t.jpg?AWSAccessKeyId=AWSACESSKEYID&Content-Type=image%2Fjpeg&Expires=1480113918&Signature=kZk6NBqVck%2Bg9290Powu%2F3ZUegg%3D&x-amz-acl=authenticated-read&x-amz-meta-object_id=5838bb7a02d21e140ab892ec&x-amz-meta-os_filename=NuubisTestImage.jpg&x-amz-meta-read_only=false&x-amz-meta-s3key=5838bae502d21e140ab892d3%2F5838bb7a02d21e140ab892ec%2Fe0092910-7f15-4ca0-a261-090fb759aeb1_t.jpg`\n\nBelow are screen shots of the Postman PUT with the above pre-signed URL (one showing the Headers and the other showing the Body): \n\n\n. Viewing the exact HTTP request and response with Postman is not straightforward.\nSo, we've come up with a work around where we've removed the pre-signed URL file uploads to the S3 from Postman and instead use a script with curl commands for the pre-signed URL files uploads to the S3 and the curl uploads don't appear to exhibit the problem.. ",
    "mharis99": "add one more header \nx-amz-acl:public-read. Following are the headers:\n      KEY           :       VALUE\n\n1 -  Content-Type : image/jpeg\n2 - x-amz-acl : public-read or private\n3 - Content-Encoding : UTF-8\n4 - Content-MD5 : Values of MD5 string. ",
    "dhananjayrspl": "Did any worked with below scenario.\n1 : Generate pre-signed url with Content-length and Content-MD5 with PUT method\n2 : Use post man to upload file to s3 bucket using generated URL\nQuestion : what header we need to pass and how to pass. GeneratePresignedUrlRequest req = new GeneratePresignedUrlRequest(\"Bucket Name\",\"PATH\");\nreq.setMethod(\"PUT\");\nreq.putCustomRequestHeader(\"Content-Length\", \"1024\");\nreq.putCustomRequestHeader(\"Content-MD5\", \"f1460c165fb3cd6dfe06ef3d033de603\");\nreq.putCustomRequestHeader(\"x-amz-acl\",\"private\");\nreq.putCustomRequestHeader(\"Content-Encoding\",\"UTF-8\");\nI have file with length of 1024 bytes and MD5 generated using CertUtil -hashfile S3Test.txt MD5\nResponse i got\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error>\n    <Code>InvalidDigest</Code>\n    <Message>The Content-MD5 you specified was invalid.</Message>\n    <Content-MD5>f1460c165fb3cd6dfe06ef3d033de603</Content-MD5>\n    <RequestId>{removed_content}</RequestId>\n    <HostId>{removed_content}</HostId>\n</Error>\n\n\n. ",
    "johnborges": "Thank you for the clarification. And yes, the issue linked from amazon-cognito-js is where I'm encountering this.. @jeskew would you mind committing your branch? I'de like to test out your fix with my project.. Thanks. I can confirm this fix is working for me.. ",
    "dibakardas67": "@chrisradek  - Yes I have ETag in my bucket CORS configuration.\nPlease refer attached image for my complete CORS configuration.\n\n. @chrisradek  - Yes I have ETag in my bucket CORS configuration.\nPlease refer attached image for my complete CORS configuration.\n\n. @chrisradek \nI removed the ContentType from my parameters. The upload is still working in Chrome, like before.\nBut I am unable to upload via Firefox/Edge.\nI believe this issue has got something to do with how Firefox calculates the signature. Getting below error:\nThe request signature we calculated does not match the signature you provided. Check your key and signing method.. @chrisradek \nI removed the ContentType from my parameters. The upload is still working in Chrome, like before.\nBut I am unable to upload via Firefox/Edge.\nI believe this issue has got something to do with how Firefox calculates the signature. Getting below error:\nThe request signature we calculated does not match the signature you provided. Check your key and signing method.. @chrisradek  - using the latest (2.7.7) version of SDK fixed the issue in Edge and Firefox(Chrome and IE was working fine before as well as now also).\nBut we are still facing the same issue when we upload from Android devices.. @chrisradek  - using the latest (2.7.7) version of SDK fixed the issue in Edge and Firefox(Chrome and IE was working fine before as well as now also).\nBut we are still facing the same issue when we upload from Android devices.. ",
    "patrik-piskay": "Hi @chrisradek, no, refreshing is not the problem.\nProblem for us is that we need to call get before we do any API Gateway request. And that get request will sometimes fail because the token has expired. In that case the credentials will get renewed in the next request and only after that we can continue with the API Gateway request.\nBut not being able to set expiry time manually means that if we want to handle this \"get request -> request failed -> credentials renewed -> API Gateway call\" scenario, we have to wait 1hour to be able to do that.. Thanks @chrisradek but this doesn't seem to do what we are expecting. We'd like to control (for dev purposes only) when we get 403 Access to Identity XXX is forbidden response from cognito's credentials.get call. Currently we get this only once an hour.. Sorry if it wasn't clear but yes, that's exatly it! So it is easier for us handle this scenario (which we already did but testing it in the future will require us to wait 1hour to test it again). Solution would be to have this expitation time configurable, or am API to invalidate the token.. Cheers @chrisradek!. ",
    "abdulkhn8": "Hi @patrik-piskay ,\ncan you please share the code, how you specify timeout.. ",
    "mansdahlstrom1": "Has there any updates on this from the sdk side? We are currently also looking for a way to handle scenarios where the AWS.credentails expires and to handle it appropriately. For dev purposes it would be super nice to be able to set exprieTime to something around 30 sec. ",
    "DaMouse404": "I tried it with:\n npm 3.10.9 and node 4.6.0\n npm 3.10.9 and node 6.5.0\n* npm 3.10.8 and node 6.9.1\nAll installed from homebrew as node4-lts, node and then node6-lts.\nSince it worked for you with node 6.9.1 and npm 3.10.8 I ran:\nrm -rf node_modules/ && npm i && npm test\nWhich still failed so I think it must be quite the edge case!\n. ",
    "mdurrant": "Updated per your comment, let me know if you need any more adjustments.. ",
    "north-river": "@chrisradek \nI understood that. Thank you.. ",
    "prestomation": "Completely agreed requesting this be part of the public API!\nI'm adapting AWS.SIgners.V4 to add signing to SwaggerJS for use with my API Gateway AWS_IAM APIs and it would be nice to know that this will be stable.\nhttps://github.com/prestomation/swagger-ui/blob/master/dist/client_authorization-sigv4.js\n. Err, yes, in the query parameters of course.\nThanks @carlnordenfelt, makes sense. I just wish this sort of interface was more cleanly available in the SDK so I don't need to have multiple copies of the signing code.\nI hacked something together using the SDK, it's pretty ugly though! https://gist.github.com/prestomation/b0f15b4492146a64b9deffaf7ef011eb#file-iot_presign-js. ",
    "deejvince": "+1 to have a public signer (that can produce both signed query strings as well as signed headers. This will reduce a lot of headache..\nUse cases: IoT, Neptune, APIs and other stuff without an SDK... . ",
    "realknack": "+1 I haven't found reliable, intuitive third-party signer yet.\nI have no idea if these are official examples but this is so much better than the mess I have right now:\nhttps://github.com/aws-samples/amazon-elasticsearch-lambda-samples/blob/master/src/s3_lambda_es.js. This is a cause I can get behind. \nSad to see my bundle sizes to drastically go up and it adds a lot of confusion to bundler configuration, i.e which APIs should be included into bundle and which shouldn't. At the very least I would expect an update after Re:Invent which usually introduces many new services and features. \nBut I recognize that updating SDK is probably not as easy as it looks, why else would it lag behind... ",
    "rhclayto": "So is there any way to enforce content-length/file size upload restrictions using s3.getSignedUrl? Content-Length header signing is not allowed & doesn't work anyway it seems, & ''content-length-range'' is not supported either the way it is when manually signing & doing a form POST. I would like to close any loophole allowing someone to generate a pre-signed URL then use it to PUT huge files to S3, but I haven't been able to find a way to do it using s3.getSignedUrl. Any clues? Node context.. So is there any way to enforce content-length/file size upload restrictions using s3.getSignedUrl? Content-Length header signing is not allowed & doesn't work anyway it seems, & ''content-length-range'' is not supported either the way it is when manually signing & doing a form POST. I would like to close any loophole allowing someone to generate a pre-signed URL then use it to PUT huge files to S3, but I haven't been able to find a way to do it using s3.getSignedUrl. Any clues? Node context.. Oh well. The suggested presigned request creation feature would be handy in any event.\nThanks for your response.. Oh well. The suggested presigned request creation feature would be handy in any event.\nThanks for your response.. ",
    "Titozzz": "To add a bit of context, I use the SDK to upload my swagger definition to AWS, but since amazon last documentation update, can't do that anymore, since It tells me I have too much documentation parts, so I just wan't to disable it for now :) ! . okay, so I had to put the ignore:documentation inside the parameters object.\nI think I can close the issue. ",
    "1msoft": "let Body = file.contents\n            let queueSize = options.queueSize\n            let partSize = options.partSize\n            self.emit('upload_begin', file)\n            s3.upload({Bucket, Key, Body}, {queueSize, partSize}, done). macOS 10.12\nnodejs 6.9.1. when file.size > 5*1024*1024, raise TypeError.\nfile is gulp file , Body is stream. Thank you.\n{queueSize: 1, partSize: 510241024}\nconsole.log test/aws-lib.test.js:106\n    upload begin: /Users/william/projects/Accuragen/CloudAG/test/aws_uploads/guaranty.sql\nconsole.log node_modules/aws-sdk/lib/event_listeners.js:450\n    [AWS s3 200 0.212s 0 retries] createMultipartUpload({ Bucket: 'ag-cloud', Key: 'Big_file_test/guaranty.sql' })\nconsole.log node_modules/aws-sdk/lib/event_listeners.js:450\n    [AWS s3 200 6.328s 0 retries] uploadPart({ Body: ,\n      ContentLength: 5242880,\n      PartNumber: 1,\n      Bucket: 'ag-cloud',\n      Key: 'Big_file_test/guaranty.sql',\n      UploadId: 'kRmm92OHoiAGdqii6CXzKb4mzWUtYVPbWllQ8UWW4Q7elTcHj_x3cVmhxFROt3IKU8G64CgoTRIwAZ_7CHCb9uvEUdd45kLmXcim2gKnDAqPsFog.yYPjdQ7xKYlOseq' })\nconsole.log node_modules/aws-sdk/lib/event_listeners.js:450\n    [AWS s3 200 6.302s 0 retries] uploadPart({ Body: ,\n      ContentLength: 5242880,\n      PartNumber: 2,\n      Bucket: 'ag-cloud',\n      Key: 'Big_file_test/guaranty.sql',\n      UploadId: 'kRmm92OHoiAGdqii6CXzKb4mzWUtYVPbWllQ8UWW4Q7elTcHj_x3cVmhxFROt3IKU8G64CgoTRIwAZ_7CHCb9uvEUdd45kLmXcim2gKnDAqPsFog.yYPjdQ7xKYlOseq' })\nconsole.log node_modules/aws-sdk/lib/event_listeners.js:450\n    [AWS s3 200 2.464s 0 retries] uploadPart({ Body: ,\n      ContentLength: 1867386,\n      PartNumber: 3,\n      Bucket: 'ag-cloud',\n      Key: 'Big_file_test/guaranty.sql',\n      UploadId: 'kRmm92OHoiAGdqii6CXzKb4mzWUtYVPbWllQ8UWW4Q7elTcHj_x3cVmhxFROt3IKU8G64CgoTRIwAZ_7CHCb9uvEUdd45kLmXcim2gKnDAqPsFog.yYPjdQ7xKYlOseq' })\nconsole.log node_modules/aws-sdk/lib/event_listeners.js:450\n    [AWS s3 undefined 0.002s 0 retries] completeMultipartUpload({ MultipartUpload:\n       { Parts:\n          [ { ETag: '\"64f398cda677c3f77055061988da2d6d\"', PartNumber: 1 },\n            { ETag: '\"4cbf2107e4dcfcb00145aa6b43858e01\"', PartNumber: 2 },\n            { ETag: '\"62d6f40e43970882f74ddeb4b54e2e3f\"', PartNumber: 3 },\n            [length]: 3 ] },\n      Bucket: 'ag-cloud',\n      Key: 'Big_file_test/guaranty.sql',\n      UploadId: 'kRmm92OHoiAGdqii6CXzKb4mzWUtYVPbWllQ8UWW4Q7elTcHj_x3cVmhxFROt3IKU8G64CgoTRIwAZ_7CHCb9uvEUdd45kLmXcim2gKnDAqPsFog.yYPjdQ7xKYlOseq' })\nconsole.log test/aws-lib.test.js:99\n    { TypeError: XMLText is not a constructor\n        at XMLElement.Object..module.exports.XMLNode.text (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/node_modules/xmlbuilder/lib/XMLNode.js:168:15)\n        at XMLElement.Object..module.exports.XMLNode.txt (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/node_modules/xmlbuilder/lib/XMLNode.js:279:19)\n        at serializeScalar (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/lib/xml/builder.js:71:7)\n        at serialize (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/lib/xml/builder.js:18:21)\n        at Object. (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/lib/xml/builder.js:37:9)\n        at Object.arrayEach (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/lib/util.js:477:32)\n        at serializeStructure (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/lib/xml/builder.js:23:8)\n        at serialize (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/lib/xml/builder.js:15:30)\n        at Object. (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/lib/xml/builder.js:59:7)\n        at Object.arrayEach (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/lib/util.js:477:32)\n      message: 'XMLText is not a constructor',\n      code: 'TypeError',\n      time: 2016-12-07T01:06:32.322Z }\nconsole.log test/aws-lib.test.js:113\n    upload 0 files, and skip 3 files and 1dirs. set partSize = 20*1024*1024, the file that size < 20M, upload is ok.. version :2.7.10. I switch aws-sdk 2.5, 2.6, same problem.\nand i wirte a simple test code \nAWS.config.update(awsconfig)\n        let s3 = new AWS.S3({logger:console})\n        let Body = fs.createReadStream('./__test__/aws_uploads/Dash.zip')\n        s3.upload(\n            {Bucket: 'ag-cloud', Key: 'sdk_test/Dash.zip', Body}, \n            {queueSize: 4, partSize: 10*1024*1024}, \n            (err,data) => {\n               if (err) console.log(err)\n               else console.log(data)\n            })\nconsole.log test/aws-lib.test.js:127\n    { TypeError: Cannot read property 'Object.' of null\n        at Runtime._runScript (/Users/william/projects/Accuragen/CloudAG/node_modules/jest-runtime/build/index.js:458:49)\n        at Runtime._execModule (/Users/william/projects/Accuragen/CloudAG/node_modules/jest-runtime/build/index.js:440:26)\n        at Runtime.requireModule (/Users/william/projects/Accuragen/CloudAG/node_modules/jest-runtime/build/index.js:297:14)\n        at Runtime.requireModuleOrMock (/Users/william/projects/Accuragen/CloudAG/node_modules/jest-runtime/build/index.js:366:19)\n        at XMLElement.XMLNode (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/node_modules/xmlbuilder/lib/XMLNode.js:39:18)\n        at new XMLElement (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/node_modules/xmlbuilder/lib/XMLElement.js:27:40)\n        at new XMLBuilder (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/node_modules/xmlbuilder/lib/XMLBuilder.js:24:14)\n        at Object..module.exports.create (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/node_modules/xmlbuilder/lib/index.js:11:12)\n        at XmlBuilder.Object..XmlBuilder.toXML (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/lib/xml/builder.js:7:21)\n        at populateBody (/Users/william/projects/Accuragen/CloudAG/node_modules/aws-sdk/lib/protocol/rest_xml.js:23:36)\n      message: 'Cannot read property \\'Object.\\' of null',\n      code: 'TypeError',\n      time: 2016-12-07T02:59:37.875Z }\nconsole.log node_modules/aws-sdk/lib/event_listeners.js:450\n    [AWS s3 undefined 0.013s 0 retries] createMultipartUpload({ Bucket: 'ag-cloud', Key: 'sdk_test/Dash.zip' })\n. xmlbuilder version is 2.6.2, \nI have removed node_modules, and yarn install.. when i run test in node test.js, that is ok.\nso maybe jest . ",
    "wordyallen": "same issue.. It seems like the Jest Environment makes aws loose access to that Constructor.\nI could only find this reference to it. http://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_external_1_1tinyxml2_1_1_x_m_l_text.html\n@chrisradek  how does node access that class?. same issue.. It seems like the Jest Environment makes aws loose access to that Constructor.\nI could only find this reference to it. http://sdk.amazonaws.com/cpp/api/LATEST/class_aws_1_1_external_1_1tinyxml2_1_1_x_m_l_text.html\n@chrisradek  how does node access that class?. ",
    "OussamaRomdhane": "@chrisradek can you please help?\nI saw a lot of this in the closed issues but none match my case\nThanks. Thanks for the reply I don't know what Heroku uses, but we're running a node 6.x, and we are using this sdk to put objects into an S3 bucket. The aws-sdk version we use is the latest. \nThis is mostly it (the part that causes the problem)\n```\nvar s3obj = new AWS.S3({\n            signatureVersion: 'v4',\n            accessKeyId: bucket.accessKey,\n            secretAccessKey: bucket.secret\n        });\n        s3obj.putObject({\n            Bucket: bucket.name,\n            Key,\n            Body: file\n        }).send(function(err, success) {\n            if (err) { reject(err); } else { resolve(success); }\n        });\n```. @chrisradek Update: this seems to be caused by not having specified the region parameter in the AWS.S3 initialization \nnew AWS.S3({\n            signatureVersion: 'v4',\n            accessKeyId: bucket.accessKey,\n            secretAccessKey: bucket.secret,\n            region: bucket.region\n        });\nSeems to solve the problem.. @chrisradek thanks for the help mate \ud83d\ude04 . ",
    "yinso": "@chrisradek - thanks for the help. I was able to make use of your second approach, which is to add es2015.promise to tsconfig.json. I also need to add es5 in order to supress some other errors, so now it looks like this:\n{\n  \"compilerOptions\": {\n    \"lib\": [\n      \"es5\",\n      \"es2015.promise\"\n    ]\n  },\n  ...\n}\nThis at least suppresses the compiler errors. Typescript is good but its errors can be very esoteric to fix, but that's not your issue, so hopefully I wouldn't have to ask again when I have to make future changes, but definitely thanks for the help!\n. ",
    "testtshoretel": "@chrisradek Can you take a took? Is there any way in client options to disable certificate verification so I do not need to install cert in my development, Thanks. . @jeskew Thanks. \nI added this in my code: \n   AWS.config.update({\n      httpOptions: {\n        agent: new https.Agent({\n          rejectUnauthorized: false\n        })\n      }\n    });\nThen I createbucket with params:\n{\n Bucket: \"testbucket\",\n        ACL: 'private'\n };\nI got this error:\nInvalidArgument: null. \nCan you let me know if it is still my cert related error?\nThanks\n. @chrisradek Thanks for reply.  I found some example about managed upload abort example http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3/ManagedUpload.html and I think it is similar to your example. The problem is that req.send response might return after the req.on('send') timeout since req has already been sent out. I have to have complicated logic to determine if timeout happens first or repsonse happens first in this case.  Do you have any idea that one putObject request never return any response and never timeout?. @chrisradek Thanks for the quick response. I will try s3.getBucketLocation. But I still have questions: \n1. Why sometime getSighUrl return region and sometimes not? \n2. Why s3.amazonaws.com have ssl connection allert?\nThanks!. Hi Chrisadek,\nThanks for the quick response. In my case, all data is many buffers from http payload req.on('data'),  If I do not want to streamifier those buffers, is there any way to tell managed upload it is end?. In my case, client send file(5gb)  with http request, server(my job) parse the payload and do the upload to S3. I do not want to save the file to disk or use very large buffer(memories), so what I want to do is get the data buffer and then upload. I guess I could not use managed upload in this case and I should use multipart upload directly. Am I right?. @chrisradek  Thanks. I tried to use option 1 and  googled transform buffers to stream but could not find an   example, can you give me some links for transform multiple buffers to stream? . @jeskew ,\nThanks for the reply. I specified the endpoint: http://s3.amazonaws.com.\nBut I did not specify region since I don't know about the region. It is admin decided. \nThis http url only happens rarely and most url generated is https. Do you have any idea why?\nHere is an example of url:\nhttp://7a8201486b8af0eb.s3.amazonaws.com/1489605388287-e3bfc000-09b3-11e7-8aea-59d090320880?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJG5MZSHQBKXVHYLA%2F20170315%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20170315T192610Z&X-Amz-Expires=900&X-Amz-Signature=9f19993adf4d440a3a0219d949a0bdd7e32d259b0e95d13c6873afc18f1b10c2&X-Amz-SignedHeaders=host,\nThanks,. @jeskew But most of url generated is https only very rare is http. \nSo my solution is remove this endpoint in options? Is it right?. @chrisradek I did not update the SDK. I noticed this problem since I captured Wireshark. I am running my test on windows to my s3 bucket. My pc is 10.57.4.203.  S3 ip is 52.219.28.46. \nYes. Upload is still very slow at this time. . @jeskew  Same SDK 2.4.8. @jeskew I posted a new issue here: \nDiscussion Forums \nWelcome, eee Logout Your Stuff Private Messages (0) Forums Help\nDiscussion Forums > Category: Storage & Content Delivery > Forum: Amazon Simple Storage Service >Thread: Upload file \"Expires\" did not work\nIs this the right place? Nobody updated this issue. How can I ask somebody to take a look?\nThanks. . ",
    "develoser": "Thank you so much, it's clear now.. ",
    "rebeccacaroline": "@jeskew is there any documentation or code examples for querying athena over an API?. @jeskew thank you, this is very helpful!. ",
    "huangzhenhong": "Updated tsconfig.app.json inside src folder, it works for me, thanks!\n{\n  \"extends\": \"../tsconfig.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"../out-tsc/app\",\n    \"module\": \"es2015\",\n    \"baseUrl\": \"\",\n    \"types\": [\"node\"]\n  },\n  \"exclude\": [\n    \"test.ts\",\n    \"**/*.spec.ts\"\n  ]\n}. ",
    "gambheera88": "Worked for me the solution...! thanks. ",
    "dudunato": "Hey there, I'm adding the types to my tsconfig.json then it gives me another error:\nnode_modules/@types/react-native/index.d.ts(8630,14): error TS2300: Duplicate identifier 'require'.\nnode_modules/@types/node/index.d.ts(140,13): error TS2300: Duplicate identifier 'require'.\nHere's my tsconfig.json:\njson\n{\n  \"compilerOptions\": {\n    \"target\": \"es2015\",\n    \"module\": \"es2015\",\n    \"moduleResolution\": \"node\",\n    \"jsx\": \"react\",\n    \"outDir\": \"compiled\",\n    \"rootDir\": \"./src\",\n    \"types\": [\"node\", \"react\", \"react-native\", \"jest\"]\n  },\n  \"include\": [\"./src/**/*\"],\n  \"exclude\": [\n    \"./compiled\",\n    \"./node_modules\"\n  ]\n}\nIt seems that node types are conflicting with react-native. Any thoughts?\ncc @chrisradek. I just ignored it on my tsconfig (but not a good solution at all). I might migrate to yours! Thanks so much for sharing, @kevinsperrine!. I just ignored it on my tsconfig (but not a good solution at all). I might migrate to yours! Thanks so much for sharing, @kevinsperrine!. ",
    "brunolm": "It doesn't make any sense, but on Ionic you have to add this, even if it doesn't make sense. under compiler options\n\"typeRoots\": [\n      \"node_modules/@types\"\n    ]. ",
    "MissAnichka": "Found this while having the same/similar issues implementing awsmobile-cli and aws-amplify with angular5. Was receiving many webpack errors, followed the solutions in this order which solved my problem:\n1. In tsconfig.json added: \"allowJs\": true in the \u201ccompilerOptions\u201d\n2. npm installed stream\n3. In tsconfig.app.json added \u201cnode\u201d to \u201ctypes\u201d array in \u201ccompilerOptions\u201d\nWebpack compiled successfully!\nThank you very much for the solutions \ud83d\udcaf \ud83e\udd47 \ud83d\udc4d \n. ",
    "jayeshkorat18": "Thanks @chrisradek \nYour solution work for me. Thanks again.. ",
    "dreadnautxbuddha": "\nUpdated tsconfig.app.json inside src folder, it works for me, thanks!\n{\n  \"extends\": \"../tsconfig.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"../out-tsc/app\",\n    \"module\": \"es2015\",\n    \"baseUrl\": \"\",\n    \"types\": [\"node\"]\n  },\n  \"exclude\": [\n    \"test.ts\",\n    \"**/*.spec.ts\"\n  ]\n}\n\nThis fixed it for me. Thanks @huangzhenhong !. ",
    "AckerApple": "Ahhhhhhhhh haaaaaaaaaaa, you are so extra right. We missed that all important detail that the npm aws-sdk actually points to the GITHUB aws-sdk-js. We've gotten so wrapped up in making KmsParams work with S3 bucket transfers, which took another package to get involved, that we thought we needed two AWS packages (one for NodeJs and another for Browser)...... Yeah, your reply greatly reduces the complexity we've created... Well thank you kindly.\nI will close this issue. Perhaps it could be considered, in the future, to GITHUB name as NPM package name to prevent future confusion. Otherwise all is well and thank you kindly for your reply.. ",
    "sedouard": "I don't think removing AWS.config is necessary. You can keep it and instead offer a configuration to the service constructor. For example:\njs\nvar ec2Client = new AWS.EC2(awsConfig)\nWhere awsConfig is bascially the AWS.config global object global class but its settings will override that of the global AWS.config. This way current users are not broken, and people who want isolation in their client instances can have that too.\nOtherwise, the idea of managing multiple subscriptions in-process becomes significantly more complicated. Currently today, if we asynchronously use multiple AWS credentials, we'll mix up EC2 instances between different subscriptions because one configuration bleeds over for another client.\nWhy would you allow for dynamically instantiated clients (new AWS.EC2()), but force them to use a global configuration? You may as well only have 1 singleton client...right?. ",
    "joshgoodwin": "I'd like to add my anger and frustration to this thread. I just lost half of a day I couldn't afford to lose hunting this garbage down. How many 10s's of thousands of man hours has this dumb decision (AWS.config overriding other AWS.config) cost us? I don't care if it's a breaking change, you have namespaced versions of your SDK. Make the change. . ",
    "diego-inflightvr": "I'd also like to add my anger and frustration to this thread (I believe this is the right place to do it, but if not, please do tell me).\nI spent all the day trying to fix an Expired Credentials issue just because I cannot update service objects credentials.\nI've tried everything... From trying to update the credentials in the global AWS config, to keeping a reference to the credentials object passed to the service object so I can refresh them with a new token.\nThis is my situation:\n1. I'm using Amazon Cognito with the approach of Developer Authenticated Identities. I think this an architecture/solution recommended by Amazon to avoid distributing long-term credentials in client apps\n2. My API gets a token and an IdentityId by calling getOpenIdTokenForDeveloperIdentity (valid for no more than 24hs)\n3. These two things are sent to my client app so it can create a AWS.CognitoIdentityCredentials that will be used later by an AWS.S3 object to download some files\nThe problem is that the max duration of the token in step #2 is 24hs. So, when my client application creates an AWS.S3 object with some CognitoIdentityCredentials that use this token, this S3 object is doomed to last no more than 24hs (just because I cannot update/refresh the CognitoIdentityCredentials that the S3 is using).\nAm I right? I still think that maybe I'm missing something but I've done my research (i.e. Amazon Doc  see last section \"Immutable Configuration Data\") and I cannot find where my error is. \nI cannot believe that I'm following an approach recommended by Amazon that doesn't allow me download a file from S3 for more than 24hs...\nThanks in advance to anyone that can help me...\n. ",
    "donspaulding": "Got bit by this today.  Here was our use-case, simplified:\n```\nclass OurClass {\n    constructor() {\n        this.options = optionsFromProcessDotEnvOrWherever;\n        this.doSomethingWithS3();\n    }\ndoSomethingWithS3() {\n    s3 = this.s3Config();\n    // does something with s3\n    // throws: InvalidAccessKeyId: The AWS Access Key Id you provided does not exist in our records.\n}\n\ns3Config() {\n    const AWS = require('aws-sdk');\n    // How NOT to configure aws-sdk endpoints:\n    AWS.config = new AWS.Config();\n    AWS.config.accessKeyId = this.options.s3.id;\n    AWS.config.secretAccessKey = this.options.s3.secret;\n    AWS.config.region = this.options.s3.region;\n    return new AWS.S3(AWS.config);\n\n    // Presumably, the reason the 5 lines above don't work is that we're\n    // setting configuration values on the \"global\" AWS.config.\n    // aws-sdk later comes along and clobbers those values with\n    // values found in the environment (~/.aws/credentials). \n}\n\n}\n```\nIn order to fix this, we had to change the return of s3Config() to something like this:\nreturn new AWS.S3({\n            accessKeyId: this.options.s3.id,\n            secretAccessKey: this.options.s3.secret,\n            region: this.options.s3.region,\n        }); \nBasically, you can't bind anything to the global config, because there's a race to write to it and AWS will blow away whatever it finds there whenever it finishes loading up the credentials.\nCount me as one more who was caught off guard by the code above not working as expected.  I'm rarely an advocate for backwards-incompatible changes, but race conditions for your main configuration seems like much more of a bug than a feature.. ",
    "aniltallam": "Investigation:\nActually request module is registering to 'data' event on response streams. Aws s3 upload is listening to readable event and trying to get data using response.read() which is returning null because data was already drained using 'data' event handler by request module. so it empty data is sent to aws server.\nSo currently we are using this workaround:\nrequest(getOptions).on('response', function (response) {\n    var readableStream = new PassThrough()\n    response.pipe(readableStream)\n    var destinationParams = { Key: 'key1', Body: readableStream}\n    s3Client.upload(destinationParams, function (err, data) {\n            //...\n    })\n})\nIt would be clean and idle if s3client handles this scenario like other modules like fs (writeFile), process.stdout which are able to fetch data from request->response stream.\neg: \nfs.writeFile(request_response)\n(or)\nrequest_response.pipe(process.stdout). Investigation:\nActually request module is registering to 'data' event on response streams. Aws s3 upload is listening to readable event and trying to get data using response.read() which is returning null because data was already drained using 'data' event handler by request module. so it empty data is sent to aws server.\nSo currently we are using this workaround:\nrequest(getOptions).on('response', function (response) {\n    var readableStream = new PassThrough()\n    response.pipe(readableStream)\n    var destinationParams = { Key: 'key1', Body: readableStream}\n    s3Client.upload(destinationParams, function (err, data) {\n            //...\n    })\n})\nIt would be clean and idle if s3client handles this scenario like other modules like fs (writeFile), process.stdout which are able to fetch data from request->response stream.\neg: \nfs.writeFile(request_response)\n(or)\nrequest_response.pipe(process.stdout). @jeskew here request method is from request npm module (var request = require('request')). @jeskew here request method is from request npm module (var request = require('request')). ",
    "ggordan": "~~So it looks like you can work around this by calling deleteMessageBatch which appears to delete the message successfully (the callback does not return an error). However, the message continues to be sent up to the Maximum Receives count, at which point it is moved to the dead letter queue.~~. Hi @jeskew . I unfortunately missed the blog post so I wasn't aware. That fixed my issue, thanks!. ",
    "patrickhousley": "Relevant package.json entry:\n\"lint\": \"tslint --type-check --project tsconfig.json -e './src/**/*.spec.ts' './src/**/*'\",. @chrisradek I will try to setup a sample project later today to help troubleshooting.. @chrisradek Sorry it took so long. I setup a repo to reproduce the error: https://github.com/patrickhousley/aws-sdk-tslint-range-error. This issue has been fixed per https://github.com/Microsoft/TypeScript/pull/12808. ",
    "itrestian": "computeSha256 is called to do Sigv4. I understand the signatureVersion is at the API level so that is why we call computeSha256. However, since we are calling makeUnauthenticatedRequest why is it being called? Can we have a way to not do that for unauthenticated requests?. ",
    "WChoy": "IS THIS RELATED to issue.   also in https://github.com/aws/aws-amplify/issues/153\n. ",
    "Omi0": "Note: if you intend to use it in TS in Angular - you need to set up as following:\nAWS.EventListeners['Core'].removeListener('afterBuild', AWS.EventListeners['Core'].COMPUTE_SHA256);. @mayur2345 by the way - if you really want to fix this problem. Because this approach will not 100% solve your problem. But to fix you simple need to tell AWS, that you use a browser, not Node. And the way to to it is:\n(window as any).process = {\n  browser: true\n};. In my case. I used global types and \"types\":[\"node\"] inside tsconfig.app.json was overwriting \"typeRoots\" value inside tsconfig json. So the solution is as follows:\n1. 1 remove \"types\" from tsconfig.app.json\n2. Ensure that global types are declared inside tsconfig json as follows \n\"typeRoots\": [\"node_modules/@types\"]\n3. IMPORTANT. Add browser: true settings. In my case it was inside polyfils.ts as \n(window as any).process = {\n  browser: true //IMPORTANT for aws-sdk\n};. ",
    "mayur2345": "AWS.EventListeners['Core'].removeListener('afterBuild', AWS.EventListeners['Core'].COMPUTE_SHA256);\nworked for me thanks;\nuse it like this\nimport * as AWS from 'aws-sdk';\nAWS.EventListeners['Core'].removeListener('afterBuild', AWS.EventListeners['Core'].COMPUTE_SHA256);. AWS.EventListeners['Core'].removeListener('afterBuild', AWS.EventListeners['Core'].COMPUTE_SHA256);\nworked for me thanks;\nuse it like this\nimport * as AWS from 'aws-sdk';\nAWS.EventListeners['Core'].removeListener('afterBuild', AWS.EventListeners['Core'].COMPUTE_SHA256);. ",
    "Ballhorn": "Thanks for the link. I got it working after taking more closer look to the code!\nThe prefix isn't removed but rebuilt as a string. This means that each userAttributes.attribute must be changed to 'userAttributes.attribute'.\nMy ChallengeResponsesare now in form:\n{ 'userAttributes.name': '******', 'userAttributes.family_name': '********', 'userAttributes.email': '*****@*****', NEW_PASSWORD: '*********', USERNAME: '*****@*****' }. I found a solution. My function was in a VPC to be able to connect the RDS instance. This prevented me from calling DMS.createEndpoint function and timed out instead.\nI put the RDS part that required VPC to a separate Lambda function and called it from another Lambda function that handled the DMS part and wasn't in a VPC.. ",
    "hulbert": "Here's what I'm trying:\n```js\nfunction downloadLogFile(databaseId, logFileName, outputStream) {\n    return new Promise(function(resolve, reject) {\n        let callIter = 0 \n    function download(marker) {\n        console.log('call #' + callIter)\n        callIter++\n\n        rds.downloadDBLogFilePortion({\n            DBInstanceIdentifier: databaseId,\n            LogFileName: logFileName,\n            Marker: marker\n        }, function(err, data) {\n            if (err) {\n                return reject(err)\n            }\n\n            const logData = data.LogFileData\n            const marker = data.Marker\n            const isAdditionalData = data.AdditionalDataPending\n\n            outputStream.write(logData, function(err) {\n                if (err) {\n                    return reject(err)\n                }\n\n                if (isAdditionalData === false) {\n                    outputStream.end('\\n')\n\n                    return resolve()\n                }\n\n                download(marker)\n            })\n        })\n    }\n\n    download()\n})\n\n}\n```\nAFAICT pagination doesn't work the same as I've used in past where the callback has pagination info on the bound context, but maybe I'm overlooking something. This change seems to end up with files of the correct size\u2014I assume it triggers it to start from the beginning rather than tail end of the log?\ndiff\n-                Marker: marker,\n+                Marker: marker || '0'. And a slightly more elegant solution using the SDK's paging:\n```js\nfunction downloadLogFile(databaseId, logFileName, outputStream) {\n    return new Promise(function(resolve, reject) {\n        outputStream.on('error', function(err) {\n            reject(err)\n        })\n    // bound w/ AWS#Response\n    // recurses itself if more pages\n    // ends the outputStream when done\n    function pageCb(err, data) {\n        if (err) {\n            return reject(err)\n        }\n\n        const logData = data.LogFileData\n        if (!logData) {\n            outputStream.end('\\n')\n            return resolve()\n        }\n\n        outputStream.write(logData)\n\n        if (this.hasNextPage()) {\n            return this.nextPage(pageCb)\n        }\n\n        // otherwise we've completed this file\n        outputStream.end('\\n')\n        resolve()\n    }\n\n    rds.downloadDBLogFilePortion({\n        DBInstanceIdentifier: databaseId,\n        LogFileName: logFileName,\n        Marker: '0'\n    }, pageCb)\n})\n\n}\n```. (The other thing that would be helpful is to know when retries are happening; please let me know if I'm missing something obvious here that the SDK already provides. I struggle in navigating the documentation at times.). ",
    "MandeepKalkhanda": "@chrisradek \nI am using gulp-s3-upload package. Which is using putObject and I have set httpOption timeout to 0. but still getting the issue.. ",
    "howardya": "After referring to this I have modified it to be the following\n``jsx\nlet Logins = {};\nLogins[cognito-idp.${appConfig.region}.amazonaws.com/${appConfig.UserPoolId}`] = 'xxx';\nconst myCredentials = new CognitoIdentityCredentials({\n  IdentityPoolId: appConfig.IdentityPoolId,\n  region: appConfig.region,\n  Logins: Logins\n});\nconst dynamodb = new DynamoDB({\n  IdentityPoolId: appConfig.IdentityPoolId,\n  region: appConfig.region,\n  credentials: myCredentials\n});\n```\nI still have Error: Missing credentials in config\nNote: I only import DynamoDB and not the whole AWS using webpack so as to slim down my js file.\nAny advice and help? Thank you \ud83d\udc4d . ",
    "loopsysdev": "Yes, that's correct. I've reported the issue to AWS, and will be following up on that side. I'll reopen this if that doesn't lead anywhere. Thanks!. ",
    "perhallstroem": "@chrisradek Great, happy to hear you could reproduce! I am not familiar at all with the SDK code so I will leave it up to you to figure things out from here, but please let me know if there is any more information you might need.. @chrisradek \nSorry for the delay \u2013 had a couple of days off + weekend. I have removed the callback. The observed problem is a little bit different. Some of the streams now hang indefinitely, i.e. they just stop receiving data, but the outcome is the same: downloading never completes. The number of \"stuck\" streams vary in my experiments from 2 up to 10. The \"stuck\" streams are present even if there are no socket hangups or similar. When I say \"stuck\", I just mean that there is no error emitted and end has not been emitted either.\nI started this about ten minutes ago and after an initial burst of 10+ megs/sec, network activity went down to just some background noise and these streams were stuck:\nece8smc6u5cg8ok8gw4kgokcw: read 2558560 bytes @ Mon Jan 23 2017 08:47:55 GMT+1100 (AEDT)\n3dkomp6ijcyswwoo4kw8g4c8: read 2854496 bytes @ Mon Jan 23 2017 08:47:58 GMT+1100 (AEDT)\n2vlzsu13aqio4owowwcocwwgc: read 1026656 bytes @ Mon Jan 23 2017 08:48:02 GMT+1100 (AEDT)\nThe exact code I am using now is\n```\n\"use strict\";\nconst AWS               = require('aws-sdk');\nconst createWriteStream = require('fs').createWriteStream;\nconst s3 = new AWS.S3({region: 'ap-southeast-2', sslEnabled: true, maxRetries: 0});\nconst status = {};\nconst download = (key) => {\n  console.log(Starting download of ${key});\n  status[key]       = 'starting';\nlet downloaded = 0;\n  const writeStream = createWriteStream(/tmp/${key});\n  const readStream  = s3.getObject({Bucket: 'b7f', Key: key, RequestPayer: 'requester'}).createReadStream();\nreadStream.on('end', () => {\n    console.log(Successfully downloaded ${key});\n    status[key] = 'completed';\n  });\nreadStream.on('error', (err) => {\n    status[key] = ${err};\n    console.log(<${key}> Error on read stream: ${err});\n  });\nreadStream.pipe(writeStream);\n  readStream.on('data', (chunk) => {\n    downloaded += chunk.byteLength;\n    status[key] = read ${downloaded} bytes @ ${new Date().toString()};\n  });\n  status[key] = 'piping';\n};\ns3.listObjectsV2({Bucket: 'b7f', RequestPayer: 'requester'}, (err, data) => {\n  if (err) throw err;\n  data.Contents.sort(() => .5 - Math.random()).forEach((key) => download(key.Key));\n});\nconst printStatus = () => {\n  console.log('\\n\\n=====================================================');\n  Object.keys(status).forEach((k) => {\n    if(status[k] !== 'completed')  {\n      console.log(${k}: ${status[k]});\n    }\n  });\n};\nsetInterval(printStatus, 10e3);\n```\n. @chrisradek This script hangs on every single run for me, except at home on my legendarily slow DSL connection. I wonder if it could be related to connection speed. I'll see if I can get someone else to reproduce the issue independently for additional clues.. @chrisradek \nI haven't seen my WiFi connection disconnect during the tests. If I manually turn my WiFi off and back on, downloads will recover normally. By \"normally\" I mean that if I download one file concurrently I have never seen the problem described and if I turn WiFi off and on again, it will recover. If I download a hundred files concurrently I would expect a couple to get \"stuck\" and if I turn WiFi off and on again I will see maybe 3 or maybe 1 file stuck, i.e. within normal ranges. \nI have seen this problem on Ethernet-connected machines as well, but it is possible that it is somehow related to the stability of the connection since this tends to happen only at higher levels of concurrency. \n. Thanks, @chrisradek. Such an interesting problem! I'll do some experiments with instantiating an agent and an AWS.S3 instance for each call instead; for our application the additional resource overhead is not much of a concern.. @srchase, I haven't seen this issue for a while. Reran the same test as above today, with no hung downloads. Feel free to close it. Thanks!. ",
    "dunkstewart": "I can reproduce the problem running the script above on node v7.4.0.\nHas hung for 30 mins and counting.... ",
    "rusteyy": "+1 to reproducing the problem on node v7.3.0. Hanging for 25 mins +. +1 to reproducing the problem on node v7.3.0. Hanging for 25 mins +. ",
    "laurenthuron": "@chrisradek 's workaround seems to fix the issue for us.. ",
    "andrewcharnley": "In addition, some errors (i.e bucket doesn't exist) aren't piped through to the stream but are passed to the callback, which means omitting it causes them to be swallowed and the stream to hang.. In addition, some errors (i.e bucket doesn't exist) aren't piped through to the stream but are passed to the callback, which means omitting it causes them to be swallowed and the stream to hang.. aws sdk 2.35.0\nNode 6, 7.\n. aws sdk 2.35.0\nNode 6, 7.\n. Not on the stream object, on getObject().\nAny initial errors with getObject() such as invalid credentials, invalid bucket, are passed to the callback (which can't exist when using .createReadStream() ), whereas they should be emitted onto the stream.\nI believe this is the cause of several open bugs such that doing .getObject({ Bucket:... Key:...}).createReadStream() can result in;\n\nerrors are swallowed\na stalled stream\n\nI haven't looked into the implementation but I assume the initial errors are racing against the stream creation so don't get pushed on. A fix would be to include a flag within the getObject() configuration which queues the error(s) and pushes them onto the stream when available.\nAs a temporary fix I also tried creating a transform stream and piping the callback errors onto it, but using a callback causes a write after end to the stream, crashing Node. This is another \"feature\" that's been open bug reported a while back.\nSo;\n```\nx = s3.getObject({ Bucket:xxx, Key:xxx, usingStream:true })\n=> if usingStream, queue errors\nx.createReadStream()\nif queue errors, emit onto stream \n```\nThis is critical, we're trying to use s3 streams and have no means of debugging. Silent stalls are not good!\nRegards,\nAndrew\n. Not on the stream object, on getObject().\nAny initial errors with getObject() such as invalid credentials, invalid bucket, are passed to the callback (which can't exist when using .createReadStream() ), whereas they should be emitted onto the stream.\nI believe this is the cause of several open bugs such that doing .getObject({ Bucket:... Key:...}).createReadStream() can result in;\n\nerrors are swallowed\na stalled stream\n\nI haven't looked into the implementation but I assume the initial errors are racing against the stream creation so don't get pushed on. A fix would be to include a flag within the getObject() configuration which queues the error(s) and pushes them onto the stream when available.\nAs a temporary fix I also tried creating a transform stream and piping the callback errors onto it, but using a callback causes a write after end to the stream, crashing Node. This is another \"feature\" that's been open bug reported a while back.\nSo;\n```\nx = s3.getObject({ Bucket:xxx, Key:xxx, usingStream:true })\n=> if usingStream, queue errors\nx.createReadStream()\nif queue errors, emit onto stream \n```\nThis is critical, we're trying to use s3 streams and have no means of debugging. Silent stalls are not good!\nRegards,\nAndrew\n. In my case it must return a stream as I made the module a drop in interface for fs.*. I tried wrapping with a transform stream but it's the same problem really, the callback isn't possible without a write after end crash and thus various errors get swallowed and the stream hangs. \n@mlund01, there are other scenarios your code doesn't consider such as access/credentials. That was what had me scratching my head for half a day trying to nail down where the silencing was occuring!\nAmazon should be ashamed by this bug.. In my case it must return a stream as I made the module a drop in interface for fs.*. I tried wrapping with a transform stream but it's the same problem really, the callback isn't possible without a write after end crash and thus various errors get swallowed and the stream hangs. \n@mlund01, there are other scenarios your code doesn't consider such as access/credentials. That was what had me scratching my head for half a day trying to nail down where the silencing was occuring!\nAmazon should be ashamed by this bug.. Good work jeskew and thank you!. Good work jeskew and thank you!. ",
    "Janpot": "@jeskew Why don't you do it on a major release then?. ",
    "misantronic": "@jeskew I am facing the same issue. Nevertheless, the workaround does not work for me. Please see my code:\njs\ns3.copyObject(\n    {\n        Bucket: 'MyBucket',\n        CopySource: encodeURIComponent(`MyBucket/verschw\u00f6rung/verschw\u00f6rung-aaae4e29.pdf}`),\n        Key: 'verschw\u00f6rung/verschw\u00f6rung-aaae4e29.pdf',\n        MetadataDirective: 'REPLACE',\n        ACL: 'public-read',\n        ContentType: 'application/pdf',\n        ContentDisposition: `attachment; filename=\"verschw\u00f6rung-aaae4e29.pdf\"\n    },\n    err => {\n        ...\n    }\n);\nI am not sure what's wrong with my code.. ",
    "dsouzamanish": "After some debugging we found error in file  aws-sdk/lib/protocol/json.js at line no 29.\nvar e = JSON.parse(httpResponse.body.toString());\n Here it is trying to parse response from dynamo. However dynamo returns following error which is in xml.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>PreconditionFailed</Code><Message>At least one of the pre-conditions you specified did not hold</Message><Condition>Bucket POST must be of the enclosure-type multipart/form-data</Condition><RequestId>26B96E8FF1A4AF11</RequestId><HostId>K2Ln02V9/B3vhRUk++ydbC//xR7G23otWRqnsa0fyd+O2qqNuoCnyN1y6rKgKznhoFQLYU6naU4=</HostId></Error>\n. Yes we are also using s3 service to download images from s3. On furrher\ndebugging of issue it was found dat whenever we request for resource not\navailable at s3 such error is returned and then for all subsequent requests\nafter the error; to the dynamo fail returning with same error.\nAlthought different clients of aws-sdk have been instantiated it somehow\nmixes the error messeges with dynamo requests\nOn Thu, 19 Jan 2017 at 5:44 AM, Jonathan Eskew notifications@github.com\nwrote:\n\n@dsouzamanish https://github.com/dsouzamanish\nAre you using any other SDK services in the same process as the one\nquerying Dynamo?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1305#issuecomment-273643409,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATZGhWeXR0ctIN8_hU3s_ZBEpwgN6Dhaks5rTqrtgaJpZM4LliUb\n.\n. @jeskew  Yes we are setting custom end point\nOur config is as follows : \ns3: {\n        primarybucket: \"XXXX-staging\",\n        accessKeyId: \"XXXXXXXXXX\",\n        secretAccessKey: \"XXxxxXXXXXXXXXxxxxxxxXXXXXX\",\n        region: \"ap-south-1\",\n        s3BucketEndpoint: true,\n        signatureVersion: \"v4\",\n        endpoint: \"http://xxxxxxxx-staging.s3.amazonaws.com\",\n    } . @jeskew No not in the callback.We simply stream images received to client. But subsequent requests may have dynamo calls through different client. . \n",
    "wrosb": "Hello guys, i reported similar issue with CloudWatch Service. here you can find my issue https://forums.aws.amazon.com/thread.jspa?threadID=247520&tstart=0\nHere my log for this.httpResponse.body.toString():\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n<head>\n<title>Page Not Found</title>\n</head>\n<body>Page Not Found</body>\n</html>\nAnd my code:\n  ```\nvar AWS = require('aws-sdk');\n    var profile = {\n        \"accessKeyId\": \"...\",\n        \"secretAccessKey\": \"...\",\n        \"region\": \"us-east-1\"\n    }\n   AWS.config.update(profile);\n// Create CloudWatchEvents service object\nvar cwevents = new AWS.CloudWatchEvents({ \"apiVersion\": \"2015-10-07\" });\nvar params = {\n        \"Name\": \"DEMO_EVENT\",\n        \"RoleArn\": \"...\",\n        \"ScheduleExpression\": \"rate(5 minutes)\",\n        \"State\": \"ENABLED\"\n\n};\ncwevents.putRule(params, function(err, data) {\n        if (err) console.log(err, err.stack);\n        else console.log(data);\n})\n```. ",
    "alexanimal": "Also getting this error on a Basic CloudWatch Example, can't seem to tell if its a bad CloudWatch Log record or some issue with the SDK parsing the response from AWS.\nvar cloudwatchlogs = new aws.CloudWatchLogs();\nrouter.get('/logs', function(req, res){\n    var params = {\n        logGroupName: 'logGroup', /* required */\n        logStreamName: 'streamName', /* required */\n        limit: 20\n    };\n    cloudwatchlogs.getLogEvents(params, function(err, data) {\n        if (err) {\n            console.log(err, err.stack)\n            res.send(err);\n        } else {\n            console.log(data)\n            res.json(data)\n        };\n    });\n})\nGetting this console response - \nSyntaxError: Unexpected token < in JSON at position 0\n    at JSON.parse (<anonymous>)\n    at Request.extractError (c:\\Users\\user\\Documents\\Code\\aws-log-analysis\\node_modules\\aws-sdk\\lib\\protocol\\json.js:29:18)\n    at Request.callListeners (c:\\Users\\user\\Documents\\Code\\aws-log-analysis\\node_modules\\aws-sdk\\lib\\sequential_executor.js:105:20)\n    at Request.emit (c:\\Users\\user\\Documents\\Code\\aws-log-analysis\\node_modules\\aws-sdk\\lib\\sequential_executor.js:77:10)\n    at Request.emit (c:\\Users\\user\\Documents\\Code\\aws-log-analysis\\node_modules\\aws-sdk\\lib\\request.js:671:14)\n    at Request.transition (c:\\Users\\user\\Documents\\Code\\aws-log-analysis\\node_modules\\aws-sdk\\lib\\request.js:22:10)\n    at AcceptorStateMachine.runTo (c:\\Users\\user\\Documents\\Code\\aws-log-analysis\\node_modules\\aws-sdk\\lib\\state_machine.js:14:12)\n    at c:\\Users\\user\\Documents\\Code\\aws-log-analysis\\node_modules\\aws-sdk\\lib\\state_machine.js:26:10\n    at Request.<anonymous> (c:\\Users\\user\\Documents\\Code\\aws-log-analysis\\node_modules\\aws-sdk\\lib\\request.js:38:9)\n    at Request.<anonymous> (c:\\Users\\user\\Documents\\Code\\aws-log-analysis\\node_modules\\aws-sdk\\lib\\request.js:673:12). ",
    "RegEM": "I get the same error when trying to set the custom endpoint with lambundaler, which calls config on  aws.lambda.\n{ SyntaxError: Unexpected token < in JSON at position 0\n    at JSON.parse (<anonymous>)\n    at Object.extractError (H:\\project\\node_modules\\lambundaler\\node_modules\\aws-sdk\\lib\\protocol\\json.js:29:18)\n. ",
    "harshadyeola": "@chrisradek sure. here is code snippet where it fails \nfunction fetchScheduledPayload(key) {\n    logger.debug('inside fetchScheduledPayload, key:', key);\n    var s3 = new AWS.S3();\n    var getObjectAsync= Promise.promisify(s3.getObject, s3);\n    return getObjectAsync({\n        Bucket: process.env.StoreBucket,\n        Key: key+'.json'\n    });\n}. @chrisradek sure. here is code snippet where it fails \nfunction fetchScheduledPayload(key) {\n    logger.debug('inside fetchScheduledPayload, key:', key);\n    var s3 = new AWS.S3();\n    var getObjectAsync= Promise.promisify(s3.getObject, s3);\n    return getObjectAsync({\n        Bucket: process.env.StoreBucket,\n        Key: key+'.json'\n    });\n}. @chrisradek above solution did not work.. @chrisradek above solution did not work.. @chrisradek we are using bluebird library for Promise. . @chrisradek we are using bluebird library for Promise. . @chrisradek Actually bucket access is available to instance via Instance Role. so no other authentication methods are setup for the application.. @chrisradek Actually bucket access is available to instance via Instance Role. so no other authentication methods are setup for the application.. @chrisradek seems like this is fixed in 2.7.24. So closing this issue.. @chrisradek seems like this is fixed in 2.7.24. So closing this issue.. ",
    "ysg410": "@chrisradek \nWe, @harshadyeola and I also encountered the following issue along with defaultProviders issue. Attaching the error messages : \nserver.js:221 () TypeError: Cannot read property 'prototype' of undefined\n    at Object.<anonymous> (/apps/workers/node_modules/aws-sdk/lib/node_loader.js:46:27)\n    at Module._compile (module.js:456:26)\n    at Object.Module._extensions..js (module.js:474:10)\n    at Module.load (module.js:356:32)\n    at Function.Module._load (module.js:312:12)\n    at Module.require (module.js:364:17)\n    at require (module.js:380:17)\n    at Object.<anonymous> (/apps/workers/node_modules/aws-sdk/clients/sts.js:1:63)\n    at Module._compile (module.js:456:26)\n    at Object.Module._extensions..js (module.js:474:10)\n    at Module.load (module.js:356:32)\n    at Function.Module._load (module.js:312:12)\n    at Module.require (module.js:364:17)\n    at require (module.js:380:17)\n    at Object.<anonymous> (/apps/workers/node_modules/aws-sdk/lib/credentials/temporary_credentials.js:2:11)\n    at Module._compile (module.js:456:26). @chrisradek \nWe, @harshadyeola and I also encountered the following issue along with defaultProviders issue. Attaching the error messages : \nserver.js:221 () TypeError: Cannot read property 'prototype' of undefined\n    at Object.<anonymous> (/apps/workers/node_modules/aws-sdk/lib/node_loader.js:46:27)\n    at Module._compile (module.js:456:26)\n    at Object.Module._extensions..js (module.js:474:10)\n    at Module.load (module.js:356:32)\n    at Function.Module._load (module.js:312:12)\n    at Module.require (module.js:364:17)\n    at require (module.js:380:17)\n    at Object.<anonymous> (/apps/workers/node_modules/aws-sdk/clients/sts.js:1:63)\n    at Module._compile (module.js:456:26)\n    at Object.Module._extensions..js (module.js:474:10)\n    at Module.load (module.js:356:32)\n    at Function.Module._load (module.js:312:12)\n    at Module.require (module.js:364:17)\n    at require (module.js:380:17)\n    at Object.<anonymous> (/apps/workers/node_modules/aws-sdk/lib/credentials/temporary_credentials.js:2:11)\n    at Module._compile (module.js:456:26). ",
    "kenny-house": "Running into this as well in an ECS cluster. Same NetworkingError: this is not a typed array. \nStack trace points to the writeBody function.\nEDIT: Seems like this is likely a duplicate of https://github.com/aws/aws-sdk-js/issues/1308. Running into this as well in an ECS cluster. Same NetworkingError: this is not a typed array. \nStack trace points to the writeBody function.\nEDIT: Seems like this is likely a duplicate of https://github.com/aws/aws-sdk-js/issues/1308. ",
    "magtutu": "Having this problem with KMS as well...\nfailed to decrypt: Could not decrypt KMS data key: this is not a typed array. \n\"this is not a typed array.\" -- that is the only error coming from the npm module. The rest is wrapper from our code.. This happens for KMS in 2.7.23 and not 2.7.22.. ",
    "kevbook": "I am not using amazon lambda SDK. I use it via package.json \nIts happening locally as well. Only on v2.7.23 not v2.7.22. ",
    "jphaas": "Not sure if it's related but upgrading to 2.7.23 broke RDS describeInstances; I'm calling\nrds.describeDBInstances({}, my_callback);\nAnd getting the following stack trace passed to my_callback(err):\nNetworkingError: socket hang up\n    at createHangUpError (_http_client.js:211:15)\n    at TLSSocket.socketOnEnd (_http_client.js:303:23)\n    at emitNone (events.js:72:20)\n    at TLSSocket.emit (events.js:166:7)\n    at endReadableNT (_stream_readable.js:913:12)\n    at nextTickCallbackWith2Args (node.js:442:9)\n    at process._tickDomainCallback (node.js:397:17). ",
    "lukehutton": "When you think this will make it into v2.7.24 and get released? Have to downgrade in meantime. Awesome! thanks for quick fix! Will now patch my apps to update again. ",
    "neftaly": "It was just patched & updated on NPM, should all be fixed now. ",
    "GingerAebi": "I think It appear again in 2.7.24\nI will check for you . It's fine in 2.7.24 Thanks :)\n. ",
    "petterbergman": "No, I never found a reason why.  Instead I worked around it, by calling the API directly using https.request.  I used aws4 to sign my request and xml2js to parse the result.\nIt's been a while since I wrote this work-around, but here's what it should look like:\nvar getS3Credentials = function(){\n    var deferred = q.defer();\n    var duuid = uuid.v4();\nvar\n    https = require('https'),\n    aws4  = require('aws4'),\n    xml2js = require('xml2js');\n\nopts = {service: 'sts', path: '/?Version=2011-06-15&Action=AssumeRole&RoleSessionName=nodeserver' +\n    '&RoleArn=' + process.env.AWS_ASSUME_ROLE_ARN +\n    '&DurationSeconds=3600&ExternalId=node' +\n    '&AUTHPARAMS'};\n\naws4.sign(opts); // assumes AWS credentials are available in process.env\n\n// we can now use this to query AWS using the standard node.js http API\nhttps.request(opts, function(res) {\n    var str = '';\n    res.on('data', function (chunk) {\n        str += chunk;\n    });\n    res.on('error', function (e) {\n        throw new Error(e);\n    });\n    res.on('end', function () {\n        var parser = new xml2js.Parser();\n        parser.parseString(str, function (err, result) {\n            if (err) {\n                throw new Error(err);\n            }\n            var tacccesskeyid = result.AssumeRoleResponse.AssumeRoleResult[0].Credentials[0].AccessKeyId[0];\n            var tsecretaccesskey = result.AssumeRoleResponse.AssumeRoleResult[0].Credentials[0].SecretAccessKey[0];\n            var tsessiontoken = result.AssumeRoleResponse.AssumeRoleResult[0].Credentials[0].SessionToken[0];\n            var data = {Credentials:{}};\n            data.Credentials.uuid = duuid;\n            data.Credentials.AccessKeyId = tacccesskeyid;\n            data.Credentials.SecretAccessKey = tsecretaccesskey;\n            data.Credentials.SessionToken = tsessiontoken;\n            deferred.resolve(data.Credentials);\n        });\n    });\n}).end();\nreturn deferred.promise;\n\n};\nHope that helps.. ",
    "cuipengfei": "Getting exactly same problem here.\nMy code is running in ECS, call to assumeRole returns empty object.\nHowever, the same code works ok locally.\nThe only difference I can see is ECS.. Problem solved.\nIt was somehow related to node version.\nMy code was running in ECS, with a base docker image: node:8.9.4.\nI switched to node:9.4, which is the latest version, that solved the problem.\nDon't know why, but it's working.\n@bo-ora & @petterbergman , was your situation perhaps similar? What is your node version?. Update:\nI have ran this code in ECS\njs\n    const curlCommand = `curl http://169.254.170.2${process.env.AWS_CONTAINER_CREDENTIALS_RELATIVE_URI}`;\n    exec(curlCommand, (error, stdout, stderr) => {\n        console.log(`${curlCommand} err:${error} out:${stdout} stderr:${stderr}`);\n    });\nThe result I got is:\njson\n  {\n    \"SecretAccessKey\": \"long string\",\n    \"Token\": \"very long string\",\n    \"Expiration\": \"2018-01-14T08:55:04+0000\",\n    \"AccessKeyId\": \"shorted string\"\n  }\nSo if the sdk had called http://169.254.170.2${process.env.AWS_CONTAINER_CREDENTIALS_RELATIVE_URI}, it should have gotten a correct response.. Update:\nI have ran this code in ECS\njs\n    const curlCommand = `curl http://169.254.170.2${process.env.AWS_CONTAINER_CREDENTIALS_RELATIVE_URI}`;\n    exec(curlCommand, (error, stdout, stderr) => {\n        console.log(`${curlCommand} err:${error} out:${stdout} stderr:${stderr}`);\n    });\nThe result I got is:\njson\n  {\n    \"SecretAccessKey\": \"long string\",\n    \"Token\": \"very long string\",\n    \"Expiration\": \"2018-01-14T08:55:04+0000\",\n    \"AccessKeyId\": \"shorted string\"\n  }\nSo if the sdk had called http://169.254.170.2${process.env.AWS_CONTAINER_CREDENTIALS_RELATIVE_URI}, it should have gotten a correct response.. ok, got it working.\njs\n    AWS.config.credentials = new AWS.ECSCredentials({\n        httpOptions: {timeout: 5000}, // 5 second timeout\n        maxRetries: 10, // retry 10 times\n    });\nI set the credentials explicitly, then it started working.\nStill don't know why AWS.config.credentials could be null when this code was not there.. ok, got it working.\njs\n    AWS.config.credentials = new AWS.ECSCredentials({\n        httpOptions: {timeout: 5000}, // 5 second timeout\n        maxRetries: 10, // retry 10 times\n    });\nI set the credentials explicitly, then it started working.\nStill don't know why AWS.config.credentials could be null when this code was not there.. @chrisradek \nI will remove the code that sets the credentials explicitly and log the value of s3.config.credentials in the callback.\nI will post back later.\nThanks. @chrisradek \nI will remove the code that sets the credentials explicitly and log the value of s3.config.credentials in the callback.\nI will post back later.\nThanks. Problem solved.\nIt was somehow related to node version.\nMy code was running in ECS, with a base docker image: node:8.9.4.\nI switched to node:9.4, which is the latest version, that solved the problem.\nDon't know why, but it's working now.. Problem solved.\nIt was somehow related to node version.\nMy code was running in ECS, with a base docker image: node:8.9.4.\nI switched to node:9.4, which is the latest version, that solved the problem.\nDon't know why, but it's working now.. ",
    "yesiyan01": "Any updates on this issue? I'm having the exact same issue. ",
    "eczajk1": "Experiencing the same issue here. Any guidance? Thanks.. I was ultimately able to get around this with AWS.Iot.attachPolicy:\n```\nvar params = {\n  policyName: policyName,\n  target: identityId,\n};\n// NOTE: iot is an instance of AWS.Iot\niot.attachPolicy(params, function(err, data) {\n  if (err) console.log(err, err.stack); // an error occurred\n  else     console.log(data);           // successful response\n});\n```\nThe policyName refers to an IoT policy - not an IAM policy. The target is an \"arn\" for the IoT certificate.\nAfter the script runs successfully, you can see a certificate for \"Cognito Identity\" in \"IoT > Secure > Policies > Certificates\" in the AWS console.\nNote that this script was run server-side. And, I do not currently understand how best to access identityID other than to observe the value in the browser after I log in - i.e. after we are authenticated via Cognito, you can observe the identityID at AWS.config.credentials.identityId. So I was only able to get this to work for a Cognito account for which I have the credentials.\nHope this helps.. ",
    "Bassem-Samy": "Guys @eczajk1  @bentranmyriota @louisdvs  , You saved my day (actually days trying to get this to work)\nThe thing though I can add is how to get your user id (Identity Id) used as the 'target' in attaching the policy request , you mentioned getting it from the console and url but according to this link:\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/getting-credentials.html\nthere is a method in the 'CredentialsProvider' which is 'getIdentityId' it's a web request that gets the Identity Id but after providing the userIdToken in the login maps.\nmap.put(\"cognito-idp.<YOUR REGION>.amazonaws.com/<YOUR USER POOL ID>\",yourCurrSession().getIdToken().getJWTToken());\ncredentialsProvider.setLogins(map);\nString identityId = credentialsProvider.getIdentityId();\nThat's Java code and I used it in an android app and thank God it works instead of hard coding the user id.\nAnd dear Amazon please please please put a working step by step samples for authenticated users not just unauthenticated as all the ones in the android and ios sdk samples repositories <3 thank you . ",
    "thenovelnomad": "Sure, @chrisradek.\nStill running nodejs 0.12.18 for this app, since we are running on OpsWorks Chef 11 stack. \nThe only configuration used when setting up the SNS client, is passing in the region, us-west-2.. I didnt really see anything that stood out diffing the two versions, but looks like body-parser version changed:\nbody-parser@1.15.2 -> body-parser@1.16.0. @jeskew yeah, i can test tonight. Sorry for the delay.. 2.7.27 appears to fix the issue. Thanks, @jeskew . ",
    "addisonj": "This looks good to me! . ",
    "surfdude75": "sorry it was a typo err by error. ",
    "joshlewis": "I'd say if you're going to support the lowercase \"key\" in one place, you should support it everywhere until you remove it everywhere.. ",
    "PrismaticPolygon": "Thanks for the speedy response @chrisradek! I have looked around at this repo, but it doesn't use the SDK in a true Angular style - I'm trying to get this working with the Angular build process, rather than importing the script through index.html. \nMy specific problem is how to tell Angular where to load the SDK from in systemjs.config.js in the official Angular quickstart project: https://github.com/angular/quickstart. \nAny pointers would be greatly appreciated. Thanks for your help!. No-one knows, unfortunately: http://stackoverflow.com/questions/37041049/using-aws-sdk-with-angular2\nThanks for your help - I'll try again.  It would be great if you could look into this at some point, I imagine this is actually a fairly limiting issue when it comes to working with Angular2 and the SDK.. Your specific steps don't work for me, unfortunately (404s), but I've been down a similar path whereby I pointed Angular to aws-sdk/dist/aws-sdk.js in systemjs.config.js. I found that when I then logged the resulting AWS object, it only had 6 properties:\ndecode : decode(input)\nencode : encode(input)\ntoASCII : toASCII(input)\ntoUnicode : toUnicode(input)\nucs2 : Object\nversion : \"1.4.1\"\nNot the many that the window.AWS object has:\nACM : ()\nAPIGateway: ()\n...\nDoes your AWS object have all the expected properties? I also hit upon the idea of doing as follows:\n```\n(function (global) {\n  System.config({\n    paths: {\n      // paths serve as alias\n      'npm:': 'node_modules/'\n    },\n    // map tells the System loader where to look for things\n    map: {\n      // our app is within the app folder\n      app: 'app',\n  // angular bundles\n  '@angular/core': 'npm:@angular/core/bundles/core.umd.js',\n  '@angular/common': 'npm:@angular/common/bundles/common.umd.js',\n  '@angular/compiler': 'npm:@angular/compiler/bundles/compiler.umd.js',\n  '@angular/platform-browser': 'npm:@angular/platform-browser/bundles/platform-browser.umd.js',\n  '@angular/platform-browser-dynamic': 'npm:@angular/platform-browser-dynamic/bundles/platform-browser-dynamic.umd.js',\n  '@angular/http': 'npm:@angular/http/bundles/http.umd.js',\n  '@angular/router': 'npm:@angular/router/bundles/router.umd.js',\n  '@angular/forms': 'npm:@angular/forms/bundles/forms.umd.js',\n\n  // other libraries\n  'rxjs':                      'npm:rxjs',\n  'angular-in-memory-web-api': 'npm:angular-in-memory-web-api/bundles/in-memory-web-api.umd.js',\n  'aws-sdk':                   'npm:aws-sdk'\n},\n// packages tells the System loader how to load when no filename and/or no extension\npackages: {\n  app: {\n    main: './main.js',\n    defaultExtension: 'js'\n  },\n  rxjs: {\n    defaultExtension: 'js'\n  },\n  \"aws-sdk\": {\n    main: './index.js' //Seems to be where everything should be loaded from\n  }\n}\n\n});\n})(this);\n``\nBut this instead results in a lot of 404s when running on a server - I presume because SystemJS doesn't know where the files are supposed to be loaded from. Interestingly, SystemJS also tries to load all of the JSONs in theaws-sdk/apisdirectory with an appended.js` on all of them. Any updates, please let me know - I'd like to get this working. . ",
    "cpotter13": "I was able to get the SDK to import with SystemJS. Still troubleshooting how to get the module to work with AOT and Rollup. It is currently causing Rollup to fail.\nFirst you are going to want to include the mapping to the node_module directory in the SystemJS config as follows:\nsystemjs.config.js\n```\n(function (global) {\n  System.config({\n    paths: {\n      \"npm:\": \"node_modules/\",\n    },\n    map: {\n      // App\n      \"app\": \"\",\n  // Angular\n  \"@angular/core\": \"npm:@angular/core/bundles/core.umd.js\",\n  \"@angular/common\": \"npm:@angular/common/bundles/common.umd.js\",\n  \"@angular/compiler\": \"npm:@angular/compiler/bundles/compiler.umd.js\",\n  \"@angular/platform-browser\": \"npm:@angular/platform-browser/bundles/platform-browser.umd.js\",\n  \"@angular/platform-browser-dynamic\": \"npm:@angular/platform-browser-dynamic/bundles/platform-browser-dynamic.umd.js\",\n  \"@angular/http\": \"npm:@angular/http/bundles/http.umd.js\",\n  \"@angular/router\": \"npm:@angular/router/bundles/router.umd.js\",\n  \"@angular/forms\": \"npm:@angular/forms/bundles/forms.umd.js\",\n\n  // Other libraries\n  \"aws-sdk\": \"npm:aws-sdk\", // Add this line\n  \"rxjs\": \"npm:rxjs\",\n},\npackages: {\n  \"app\": {\n    main: './main.js',\n    defaultExtension: \"js\"\n  },\n  \"rxjs\": { defaultExtension: \"js\" },\n}\n\n});\n})(this);\n```\nFrom there you can import the aws-sdk in the service you are using to carry out the logic.\nimport \"aws-sdk/dist/aws-sdk.min\";\nInside the class you can use the window object by declaring the following:\n// Initialize AWS\nwindow: any = window;\nAWS: any = this.window.AWS;\nThe snippet above is Typescript. I hope this helps!. The above snippet seems to work nicely! Thanks @chrisradek.\nTo go off your last piece, that was something I left out earlier. I am building a custom sdk using the dist-tools provided in the node module. You just have to include the services you are using.\n```\nNavigate to AWS module\ncd node_modules/aws-sdk/\nBuild the S3, SNS, SQS, and Elastic Transcoder services\nnode dist-tools/browser-builder.js s3,sns,sqs,elastictranscoder > dist/aws-sdk.js\nBuild and minify the S3, SNS, SQS, and Elastic Transcoder services\nMINIFY=1 node dist-tools/browser-builder.js s3,sns,sqs,elastictranscoder > dist/aws-sdk.min.js\n```\nYou can also use their online builder. ",
    "chjalmar": "Hi guys,\nThank you very much for your work and examples in this thread @cpotter13 @chrisradek ; Thanks to you I've been able to succesfully include the aws-sdk in Angular 4, using the s3 capabilities to upload files from my app to my s3 bucket.\nHowever, everything was working fine until I tried to take the step from JIT compilation to AOT compilation. I followed every step from Angular guide:\nhttps://angular.io/guide/aot-compiler\nWhich has worked for another simpler, non-AWS apps, but it didn't work this time; tried building a custom SDK but couldn't manage to include it in my Angular 4 app. I find it surprising that something that works in JIT doesn't in AOT.\nDo you have any suggestions, anything I should look at considering your system.config.js above (mine is pretty much the same) and the Angular guide for AOT? Any tips to share?\nThanks in advance!. ",
    "tysonhummel": "Hi all, \nI too have been able to get everything working locally, but I get a string of errors when I try to deploy to heroku:\nremote:        ERROR in /tmp/build_6c86e82e98503e334fd0cecfc6e797da/node_modules/aws-sdk/lib/config.d.ts (1,34): Cannot find module 'http'.\nremote:        \nremote:        ERROR in /tmp/build_6c86e82e98503e334fd0cecfc6e797da/node_modules/aws-sdk/lib/config.d.ts (2,35): Cannot find module 'https'.\nremote:        \nremote:        ERROR in /tmp/build_6c86e82e98503e334fd0cecfc6e797da/node_modules/aws-sdk/clients/acm.d.ts (108,37): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_6c86e82e98503e334fd0cecfc6e797da/node_modules/aws-sdk/clients/acm.d.ts (110,38): Cannot find name 'Buffer'.\nThe above is a small sample, the rest are more of the same.\nAnyone had any luck using aws-sdk, angular 4 and aot?. @jeskew I have followed the instructions provided by the aws-sdk repo to the letter. Those instructions do aid in getting things working in a local dev environment but unless I'm mistaken they don't seem to address issues with angular's AOT build process.\nMy issue occurs when trying to deploy my project to heroku (worked fine until aws-sdk). My postinstall in package.json is \"ng build -prod --aot\", which is where I'm experiencing errors.\nHere is the full error sequence with a little before and after for context (XXXXX in place of actual project name);\nremote:        > ng build -prod --aot\nremote:        \nremote:       35% building modules 209/222 modules 13 active .       Time: 115310ms\nremote:        chunk    {0} 0.a9c7d8410fc52e4882fa.chunk.js (common) 69.9 kB {1} {2} {3} {4} {5} {6} {7} {8} {9} {10} {11} [rendered]\nremote:        chunk    {1} 1.5adb954146497aef6b3f.chunk.js 200 kB {2} {3} {4} {5} {6} {7} {8} {9} [rendered]\nremote:        chunk    {2} 2.5e1a082327ba47c37056.chunk.js 240 kB {1} {3} {4} {5} {6} {7} {8} {9} [rendered]\nremote:        chunk    {3} 3.346b6537688ee30f57f8.chunk.js 53.5 kB {1} {2} {4} {5} {6} {7} {8} {9} [rendered]\nremote:        chunk    {4} 4.e8f12cc11f9caac4b2d8.chunk.js 106 kB {1} {2} {3} {5} {6} {7} {8} {9} [rendered]\nremote:        chunk    {5} 5.0802a589783a1cd6abf3.chunk.js 40.7 kB {1} {2} {3} {4} {6} {7} {8} {9} [rendered]\nremote:        chunk    {6} 6.4c2531703498040dc8c4.chunk.js 98.2 kB {1} {2} {3} {4} {5} {7} {8} {9} [rendered]\nremote:        chunk    {7} 7.3d6a903c7901c717bbad.chunk.js 104 kB {1} {2} {3} {4} {5} {6} {8} {9} [rendered]\nremote:        chunk    {8} 8.5684674e48c4124c0041.chunk.js 5.42 kB {1} {2} {3} {4} {5} {6} {7} {9} [rendered]\nremote:        chunk    {9} main.d7e8259c4fb260d68f12.bundle.js (main) 1.06 MB {12} [initial] [rendered]\nremote:        chunk   {10} polyfills.483ae7b00480e63352ed.bundle.js (polyfills) 169 kB {13} [initial] [rendered]\nremote:        chunk   {11} styles.cdd5c10f17848433c2b4.bundle.css (styles) 122 bytes {13} [initial] [rendered]\nremote:        chunk   {12} vendor.150533cc378e6bb2db71.bundle.js (vendor) 6.57 MB [initial] [rendered]\nremote:        chunk   {13} inline.caf546c65045939018c3.bundle.js (inline) 0 bytes [entry] [rendered]\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/lib/config.d.ts (1,34): Cannot find module 'http'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/lib/config.d.ts (2,35): Cannot find module 'https'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/acm.d.ts (108,37): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/acm.d.ts (110,38): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/acm.d.ts (344,32): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/lib/request.d.ts (1,25): Cannot find module 'stream'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/lib/request.d.ts (132,45): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/lib/http_response.d.ts (1,25): Cannot find module 'stream'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/lib/http_response.d.ts (14,18): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/apigateway.d.ts (1071,23): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/clouddirectory.d.ts (973,38): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/cloudsearchdomain.d.ts (41,23): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/cloudtrail.d.ts (141,28): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/codecommit.d.ts (639,22): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/directconnect.d.ts (757,28): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/dms.d.ts (416,35): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/dynamodb.d.ts (309,38): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/lib/dynamodb/document_client.d.ts (2,25): Cannot find module 'stream'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/lib/dynamodb/document_client.d.ts (86,30): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/lib/dynamodb/document_client.d.ts (190,38): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/dynamodbstreams.d.ts (92,38): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/ec2.d.ts (2691,23): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/ecr.d.ts (612,31): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/firehose.d.ts (144,22): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/glacier.d.ts (1116,24): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/lib/services/glacier.d.ts (10,28): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/iam.d.ts (1095,32): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/iam.d.ts (3047,35): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/iotdata.d.ts (73,30): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/iotdata.d.ts (74,25): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/kinesis.d.ts (221,22): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/kms.d.ts (328,32): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/kms.d.ts (962,31): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/lambda.d.ts (331,23): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/lambda.d.ts (332,28): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/lexruntime.d.ts (33,28): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/polly.d.ts (69,29): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/rekognition.d.ts (561,27): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/s3.d.ts (746,22): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/s3.d.ts (1050,42): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/s3.d.ts (3217,32): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/ses.d.ts (1127,32): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/sns.d.ts (275,24): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/sqs.d.ts (192,24): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/support.d.ts (336,22): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/waf.d.ts (424,39): Cannot find name 'Buffer'.\nremote:        \nremote:        ERROR in /tmp/build_2f224e792df4eb469efd7d3952ad5b64/node_modules/aws-sdk/clients/wafregional.d.ts (468,39): Cannot find name 'Buffer'.\nremote:        npm ERR! code ELIFECYCLE\nremote:        npm ERR! errno 1\nremote:        npm ERR! XXXXXXXXX@0.0.0 postinstall: `ng build -prod --aot`\nremote:        npm ERR! Exit status 1\nremote:        npm ERR!\nremote:        npm ERR! Failed at the XXXXXXXXX@0.0.0 postinstall script.\nremote:        npm ERR! This is probably not a problem with npm. There is likely additional logging output above.\nremote:        \nremote:        npm ERR! A complete log of this run can be found in:\nremote:        npm ERR!     /app/.npm/_logs/2017-07-13T22_43_43_762Z-debug.log\nremote: \nremote: -----> Build failed\nremote:        \nremote:        We're sorry this build is failing! You can troubleshoot common issues here:\nremote:        https://devcenter.heroku.com/articles/troubleshooting-node-deploys\nremote:        \nremote:        Some possible problems:\nremote:        \nremote:        - A module may be missing from 'dependencies' in package.json\nremote:        https://devcenter.heroku.com/articles/troubleshooting-node-deploys#ensure-you-aren-t-relying-on-untracked-dependencies\nremote:        \nremote:        - This module may be specified in 'devDependencies' instead of 'dependencies'\nremote:        https://devcenter.heroku.com/articles/nodejs-support#devdependencies\nremote:        \nremote:        Love,\nremote:        Heroku\nremote:\n. @jeskew I could kiss you. Successful AOT build/deployment after moving @types/node to prod dependencies!\nThank you so much.. ",
    "c4adarsh": "@chjalmar If you do find a solution with AOT compilation, please let me know.\nThanks in Advance\n. @chjalmar If you do find a solution with AOT compilation, please let me know.\nThanks in Advance\n. ",
    "volkanx": "In my case importing AWS as\nimport * as AWS from 'aws-sdk';\nworked. Thanks @chrisradek for that. ",
    "RLovelett": "Also, for now I've been working around the issue by applying this patch before compiling.\n```diff\ndiff -ub config.d.ts node_modules/aws-sdk/lib/config.d.ts                    \ue0b2 11:19AM \ue0b2 \ue0a0 typescript-conversion \u00b1\n--- config.d.ts 2017-02-02 11:18:22.000000000 -0500\n+++ node_modules/aws-sdk/lib/config.d.ts    2017-02-02 11:17:38.000000000 -0500\n@@ -2,6 +2,7 @@\n import {Agent as httpsAgent} from 'https';\n import {AWSError} from './error';\n import {Credentials, CredentialsOptions} from './credentials';\n+import { CredentialProviderChain } from './credentials/credential_provider_chain';\n import {ConfigurationServicePlaceholders, ConfigurationServiceApiVersions} from './config_service_placeholders';\nexport class ConfigBase extends ConfigurationOptions{\n@@ -172,6 +173,10 @@\n      /\n     credentials?: Credentials|CredentialsOptions\n     /\n+     * The provider chain used to resolve credentials if no static credentials property is set.\n+     */\n+    credentialProvider?: CredentialProviderChain\n+    /\n      * AWS access key ID.\n      \n      * @deprecated\n```\nI'd submit this as a PR but:\n\nI know it works but I'm not sure this is the \"right\" way.\nPerhaps these files are auto-generated and if that's the case then it's likely I need to fix that instead of just fixing the symptom.. Also, for now I've been working around the issue by applying this patch before compiling.\n\n```diff\ndiff -ub config.d.ts node_modules/aws-sdk/lib/config.d.ts                    \ue0b2 11:19AM \ue0b2 \ue0a0 typescript-conversion \u00b1\n--- config.d.ts 2017-02-02 11:18:22.000000000 -0500\n+++ node_modules/aws-sdk/lib/config.d.ts    2017-02-02 11:17:38.000000000 -0500\n@@ -2,6 +2,7 @@\n import {Agent as httpsAgent} from 'https';\n import {AWSError} from './error';\n import {Credentials, CredentialsOptions} from './credentials';\n+import { CredentialProviderChain } from './credentials/credential_provider_chain';\n import {ConfigurationServicePlaceholders, ConfigurationServiceApiVersions} from './config_service_placeholders';\nexport class ConfigBase extends ConfigurationOptions{\n@@ -172,6 +173,10 @@\n      /\n     credentials?: Credentials|CredentialsOptions\n     /\n+     * The provider chain used to resolve credentials if no static credentials property is set.\n+     */\n+    credentialProvider?: CredentialProviderChain\n+    /\n      * AWS access key ID.\n      \n      * @deprecated\n```\nI'd submit this as a PR but:\n\nI know it works but I'm not sure this is the \"right\" way.\nPerhaps these files are auto-generated and if that's the case then it's likely I need to fix that instead of just fixing the symptom.. I will do that! \ud83d\ude4f for the review @chrisradek . I will do that! \ud83d\ude4f for the review @chrisradek . @chrisradek thank you for the review! I have made the changes you've requested.. @chrisradek thank you for the review! I have made the changes you've requested.. I'm not sure if this counts as a review or not but I've replaced my custom pre-signed post generation code with this and it works well. This seems like a positive addition to me.\n\nThanks @jeskew for pitching this and @chrisradek for reviewing. \ud83d\udc4d. I'm not sure if this counts as a review or not but I've replaced my custom pre-signed post generation code with this and it works well. This seems like a positive addition to me.\nThanks @jeskew for pitching this and @chrisradek for reviewing. \ud83d\udc4d. Related to #1276?. Related to #1276?. I like the null idea; it feels the closest to what I'd like to achieve. Though unfortunately when I went to test that out in TypeScript credentials cannot be nulled. So it looks like I'll be submitting a PR for that shortly. :rofl: \nUpdate\n@chrisradek I think something like this needs to be made in the TypeScript definition. Thoughts?\ndiff --git a/lib/config.d.ts b/lib/config.d.ts\nindex e22134d..9312931 100644\n--- a/lib/config.d.ts\n+++ b/lib/config.d.ts\n@@ -171,7 +171,7 @@ export abstract class ConfigurationOptions {\n     /**\n      * The AWS credentials to sign requests with.\n      */\n-    credentials?: Credentials|CredentialsOptions\n+    credentials?: Credentials|CredentialsOptions|null\n     /**\n      * The provider chain used to resolve credentials if no static credentials property is set.\n      */\nUpdate 2\nI actually think this is necessary because of --strictNullChecks. I assume that does not change anything. I just thought I'd point it out.. I like the null idea; it feels the closest to what I'd like to achieve. Though unfortunately when I went to test that out in TypeScript credentials cannot be nulled. So it looks like I'll be submitting a PR for that shortly. :rofl: \nUpdate\n@chrisradek I think something like this needs to be made in the TypeScript definition. Thoughts?\ndiff --git a/lib/config.d.ts b/lib/config.d.ts\nindex e22134d..9312931 100644\n--- a/lib/config.d.ts\n+++ b/lib/config.d.ts\n@@ -171,7 +171,7 @@ export abstract class ConfigurationOptions {\n     /**\n      * The AWS credentials to sign requests with.\n      */\n-    credentials?: Credentials|CredentialsOptions\n+    credentials?: Credentials|CredentialsOptions|null\n     /**\n      * The provider chain used to resolve credentials if no static credentials property is set.\n      */\nUpdate 2\nI actually think this is necessary because of --strictNullChecks. I assume that does not change anything. I just thought I'd point it out.. @chrisradek I am not exactly sure what the extent of the expected test should be. I feel like this covers the intent of what the change now provides. Obviously, if this is wrong I'll need a little bit more guidance on what to do next.. Typically after importing S3 in TypeScript I can do something like this:\nimport { S3 } from 'aws-sdk';\nconst params: S3.Types.PresignedPostParams = {\n  // ...\n};\nUnfortunately, it seems that the PresignedPostParams are not exposed this way (likewise PresignedPost).. ",
    "jdavisclark": "I hadn't thought of that. That would potentially be way more confusing.\nHadn't stumbled across provider chains yet, but it looks like that will get\nthe job done.\nThanks.\nOn Thu, Feb 2, 2017, 12:05 PM Jonathan Eskew notifications@github.com\nwrote:\n\nWouldn't this mean that pulling in a dependency could potentially alter\nthe credential loading behavior of your application? I think using any\nglobal variable for this purpose would be dangerous.\nIf you would prefer to be explicit about where clients are loading\ncredentials, wouldn't it be easiest to just pass your own credential\nprovider chain to the client constructor?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1340#issuecomment-277034448,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAT_8s0hK4cgde9gEH7ro-17PKjE28Tnks5rYhrOgaJpZM4L1bGJ\n.\n. I hadn't thought of that. That would potentially be way more confusing.\n\nHadn't stumbled across provider chains yet, but it looks like that will get\nthe job done.\nThanks.\nOn Thu, Feb 2, 2017, 12:05 PM Jonathan Eskew notifications@github.com\nwrote:\n\nWouldn't this mean that pulling in a dependency could potentially alter\nthe credential loading behavior of your application? I think using any\nglobal variable for this purpose would be dangerous.\nIf you would prefer to be explicit about where clients are loading\ncredentials, wouldn't it be easiest to just pass your own credential\nprovider chain to the client constructor?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1340#issuecomment-277034448,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAT_8s0hK4cgde9gEH7ro-17PKjE28Tnks5rYhrOgaJpZM4L1bGJ\n.\n. \n",
    "TheLarkInn": "Hi friendly AWS folks!! If you have any particular questions, we (webpack) are happy to assist. Just drop a line. \u2764\ufe0f\ud83d\udd25\u2764\ufe0f. Hi friendly AWS folks!! If you have any particular questions, we (webpack) are happy to assist. Just drop a line. \u2764\ufe0f\ud83d\udd25\u2764\ufe0f. Actually, with @Swizec helpful info provided I think I've isolated to cause of breakage.  It appears that same location of region_config.js, that there is a region_config.json.  webpack v2 introduced a change in which allows users to no longer have to add json-loader to require json files (giving it closer feature parity to node's resolution process). I think this is probably the cause. . Actually, with @Swizec helpful info provided I think I've isolated to cause of breakage.  It appears that same location of region_config.js, that there is a region_config.json.  webpack v2 introduced a change in which allows users to no longer have to add json-loader to require json files (giving it closer feature parity to node's resolution process). I think this is probably the cause. . @chrisradek spot on. Yeah I'm not 100% sure if nodes resolution pattern would favor .json over .js but if it should not, maybe an issue in our side. . @chrisradek spot on. Yeah I'm not 100% sure if nodes resolution pattern would favor .json over .js but if it should not, maybe an issue in our side. . No probs glad to help! \ud83d\ude45\u200d\u2642\ufe0f\ud83d\ude47. No probs glad to help! \ud83d\ude45\u200d\u2642\ufe0f\ud83d\ude47. Awesome glad to know we didn't (unknowingly) break something. When we get http://github.com/webpack-contrib/webpack-canary finished, we'll add the SDK to our list of covered packages. Ping me if anything else is \ud83d\udc1e worthy. Awesome glad to know we didn't (unknowingly) break something. When we get http://github.com/webpack-contrib/webpack-canary finished, we'll add the SDK to our list of covered packages. Ping me if anything else is \ud83d\udc1e worthy. @chrisradek yup that's exactly it. \nenhanced-resolve uses a waterfall pattern for this process and applies extentions left to right via forEach https://github.com/webpack/enhanced-resolve/blob/master/lib/ResolverFactory.js#L170-L172\ntl;dr order does matter. \nMaybe this is an opportunity for a good suggestion or enhancement to warn a user if they are extending the extentions and change the resolution pattern that they may see adverse side effects.. @chrisradek yup that's exactly it. \nenhanced-resolve uses a waterfall pattern for this process and applies extentions left to right via forEach https://github.com/webpack/enhanced-resolve/blob/master/lib/ResolverFactory.js#L170-L172\ntl;dr order does matter. \nMaybe this is an opportunity for a good suggestion or enhancement to warn a user if they are extending the extentions and change the resolution pattern that they may see adverse side effects.. @chrisradek you are welcome to pull down webpack-contrib/webpack-canary as a tool to smoke test with. It allows you to specify version of webpack and version of your library and then installs and runs that libraries tests. Can work in tandem with multiple versions and packages. :) we are.hoping to leverage it like nodejs's CITGM. @chrisradek you are welcome to pull down webpack-contrib/webpack-canary as a tool to smoke test with. It allows you to specify version of webpack and version of your library and then installs and runs that libraries tests. Can work in tandem with multiple versions and packages. :) we are.hoping to leverage it like nodejs's CITGM. ",
    "Swizec": "@chrisradek updated Webpack to 2.2.1, still having the same issue.\nAdding the json-loader config didn't help either.\nMy node version is 6.9.1, npm is 3.10.8, if that's relevant. And I'm running this on a Mac in zsh. Sometimes that affects file list order.\nHere's my entire Webpack config, if that helps. \n```javascript\nconst path = require('path'),\n      webpack = require('webpack'),\n      ExtractTextPlugin = require(\"extract-text-webpack-plugin\"),\n      _ = require('lodash');\nconst envConfig  = {\ngetStage: function(){\n    return !process.env.NODE_ENV ? 'development' : process.env.NODE_ENV;\n},\ninDevelopment: function () {\n    var stage = this.getStage();\n    return stage == 'development';\n},\n\ninProduction: function () {\n    return ['production', 'preproduction', 'qa-1'].indexOf(this.getStage()) >= 0;\n},\n\ngetCDN: function() {\n    var CDNs = {\n        qa: 'd2jq5p9fb3us4h.cloudfront.net',\n        staging: 'd17pwufp6j13bu.cloudfront.net',\n        'staging-1':'d1ffm3l6o8qg4r.cloudfront.net',\n        'staging-2': 'd2kkumcy9vcw22.cloudfront.net',\n        'qa-2': 'd2avcxxv2hdx66.cloudfront.net',\n        'qa-1': 'dazirtq81ffe5.cloudfront.net',\n        'staging-swizec': 'd9wn1skltg0u7.cloudfront.net',\n        production: 'd235oyylauiwpp.cloudfront.net',\n        preproduction: 'dlb2dz5gp174o.cloudfront.net',\n        scripts: 'd3ndbz27vmttup.cloudfront.net'\n    };\n\n    if (!this.inDevelopment()) {\n        return \"//\"+CDNs[process.env.NODE_ENV.toLowerCase()];\n    }\n    return \"\";\n}\n\n};\nfunction isVendor(module, count) {\n    const userRequest = module.userRequest;\nreturn userRequest && userRequest.indexOf('node_modules') >= 0;\n\n}\nconst Apps = {\n    test: './app/assets/javascripts/Apps/TestApp/',\n    distribution: './app/assets/javascripts/Apps/DistributionApp/index.js',\n    corp_transition: './app/assets/javascripts/Apps/CorpTransitionApp/index.js',\n    yup_corp: './app/assets/javascripts/Apps/YupCorpApp/',\n    mobile_webviews: './app/assets/javascripts/Apps/MobileWebviewsApp/',\n    login: './app/assets/javascripts/Apps/LoginApp/',\n    tutor_dashboard: './app/assets/javascripts/Apps/TutorDashboardApp/',\n    chat_snippet: './app/assets/javascripts/Apps/ChatSnippetApp/',\n    written_exam: './app/assets/javascripts/Apps/WrittenExamApp/',\n    chat_bot_script: './app/assets/javascripts/Apps/ChatBotScriptApp/',\n    whiteboard: './app/assets/javascripts/Apps/WhiteboardApp/',\n    tutor_onboarding: './app/assets/javascripts/Apps/TutorOnboardingApp/',\n    block_mobile: './app/assets/javascripts/Apps/BlockMobileApp/',\n    contact_us_chat: './app/assets/javascripts/Apps/ContactUsChatApp/'\n};\nconst GlobalModules = .map({\n    jquery: ['$', 'jQuery', 'window.jQuery'],\n    lodash: [''],\n    backbone: ['Backbone'],\n    'backbone-validation': ['Backbone.Validation'],\n    'raven-js': ['Raven'],\n    moment: ['moment'],\n    string: ['string', 'S'],\n    async: ['async']\n}, (vars, module) => new webpack.ProvidePlugin(\n    _.fromPairs(vars.map(v => [v, module]))\n));\nlet config = module.exports = {\ncontext: __dirname,\n\nentry: Object.assign(\n    {},\n    Apps,\n    {\n        babel_polyfill: 'babel-polyfill',\n        'whatwg-fetch': 'whatwg-fetch'\n    }\n),\n\noutput: {\n    filename: envConfig.inDevelopment()\n      ? '[name].js'\n      : '[name].[chunkhash].js',\n    path: 'public/assets/generated',\n    publicPath: envConfig.getCDN()+'/assets/generated/'\n},\n\nmodule: {\n\n    rules: [\n\n        {\n            test: /\\.jssssss$/,\n            include: [\n                path.resolve(__dirname, \"app/assets/javascripts\")\n            ],\n            exclude: [\n                path.resolve(__dirname, \"node_modules/\")\n            ],\n            loader: 'eslint-loader',\n        },\n\n        {\n            test: /\\.js$/,\n            include: [\n                path.resolve(__dirname, \"app/assets/javascripts\")\n            ],\n            exclude: [\n                path.resolve(__dirname, \"node_modules/\")\n            ],\n            query: {\n                plugins: ['transform-decorators-legacy',\n                          'transform-runtime',\n                          'transform-object-rest-spread',\n                          'transform-react-constant-elements',\n                          'transform-class-properties'],\n                presets: [['es2015', {modules: false}], 'latest', 'react']\n            },\n            loader: 'babel-loader',\n        },\n\n        {\n            test: /\\.(less|css)$/,\n            use: [\n                ExtractTextPlugin.extract({\n                    fallbackLoader: \"style/useable\",\n                    loader: \"style-loader\"\n                }),\n                {\n                    loader: 'css-loader?sourceMap',\n                    query: {\n                        modules: true,\n                        importLoaders: 2\n                    }\n                },\n                'postcss-loader?sourceMap',\n                'less-loader?sourceMap'\n            ]\n        },\n\n        {\n            test: /\\.handlebars$/,\n            include: [\n                path.resolve(__dirname, \"app/assets/javascripts\")\n            ],\n            loader: \"handlebars-loader?helperDirs[]=\"+__dirname+\"/app/assets/javascripts/helpers/handlebars\"\n        },\n\n        {\n            test: /\\.(jpg|png|gif|svg)$/,\n            loader: \"file-loader\"\n        },\n\n        {\n            test: /\\.(eot|ttf|woff|woff2|otf)$/,\n            loader: \"file-loader\"\n        },\n\n        {\n            test: /\\.json/,\n            loader: \"json-loader\"\n        }\n    ]\n},\nplugins: [\n    new webpack.NoEmitOnErrorsPlugin(),\n    new ExtractTextPlugin( envConfig.inDevelopment() ? \"[name]_style.css\"\n                                                     : \"[name]_style.[chunkhash].css\"\n    ),\n].concat(\n    GlobalModules\n).concat(\n    Object.keys(Apps)\n          .map(app => new webpack.optimize.CommonsChunkPlugin({\n              name: `${app}_vendor`,\n              chunks: [app],\n              minChunks: isVendor\n          }))\n).concat([\n    new webpack.optimize.CommonsChunkPlugin({\n        name: 'manifest',\n        chunks: Object.keys(Apps).map(n => `${n}_vendor`),\n        minChunks: (module, count) => {\n            return count >= Object.keys(Apps).length && isVendor(module)\n        }\n    })\n])\n\n};\nconfig.resolve = {\n    extensions: ['.json', '.js', '.less', '.css', '.handlebars'],\n    modules: [\n        'node_modules',\n        'app/assets/javascripts',\n        'app/assets/stylesheets'\n    ],\n    alias: {\n        handlebars: 'handlebars/dist/handlebars.min.js'\n    }\n};\nif ( envConfig.inProduction()) {\n    config.plugins.push(\n        new webpack.DefinePlugin({\n            'process.env': {\n                NODE_ENV: JSON.stringify('production')\n            }\n        }),\n        new webpack.LoaderOptionsPlugin({\n            minimize: true,\n            debug: false\n        }),\n        new webpack.optimize.UglifyJsPlugin({\n            compress: {\n                warnings: false,\n                screw_ie8: true,\n                conditionals: true,\n                unused: true,\n                comparisons: true,\n                sequences: true,\n                dead_code: true,\n                evaluate: true,\n                join_vars: true,\n                if_return: true\n            },\n            output: {\n                comments: false\n            }\n        })\n    );\n    console.log('Webpack production build for Rails, Env: ' +  envConfig.getStage());\n}\nelse {\n    config.devtool = 'eval-source-map';\n    console.log('****Webpack development build for Rails, Env: ' +  envConfig.getStage());\n}\n```. @chrisradek I submitted a PR that fixes this issue for me :). @chrisradek that totally worked!\nIf it still makes sense to use my PR, how do I add tests for something like this? Does the test suite support using different webpack configs?. @chrisradek I'll try to take a crack at it over the weekend. Sounds like I'll learn useful stuff :). ",
    "greygatch": "The above example did not solve this issue. :(. ",
    "N1N": "Hi @chrisradek ,\nI am using just HTML page.I even added the tag  to my HTML page.\nI want to load the credentials from the file (Right now it is Hard coded,which is not preferred).\nIn that case, i need to use, \nvar AWS = require('aws-sdk');\nAWS.config.loadFromPath('C:\\Users\\N1N\\Desktop\\config.json');\nBut its not working for me.\nI tried, bundling with browserify refering the following link,\nhttp://browserify.org/#install\nBut still facing the same issue.\nMy project files looks like this.\n1) Browserify main.js into header.js\n---- main.js ----\nvar AWS = require('aws-sdk');\n----- main.js ----\n2)Add browserified header.js into HTML page.\n      ----- index.html ----\n1   \n2    \n3   \n4   \n5   (body onload=\"inits3()\") (/body)\n6   \n7   function inits3()\n8   {\n9   //Able to get this alert box\n10  alert(\"1\");\n11                  // Load credentials and set region from JSON file\n12          AWS.config.loadFromPath('C:\\Users\\N1N\\Desktop\\config.json');\n13  //Not able to get this alert box\n14  alert(\"2\");\n15          //var credentials = new AWS.SharedIniFileCredentials({profile: 'default'});\n16          //AWS.config.credentials = credentials;\n17    <br />\n18  // create the AWS.Request object\n19          s3 = new AWS.S3({ params: { Bucket: 'nshost.com' }});\n20  alert(\"Welcome to AWS!\");\n21  }\n22  \n----- index.html ----\n\n. ",
    "JustASquid": "I'm using the Webstorm IDE. I suppose it makes sense to file this as a bug in their court, unless there are some settings I'm missing.. ",
    "vyas07": "var params = {\n  Bucket: 'mybucket',\n  ACL: 'public-read-write',\n  CreateBucketConfiguration: {\n    LocationConstraint: 'us-east-1'\n  },\n};\ns3.createBucket(params, function(error, data) {\n  if (error) {\n     console.log(erorr); \n    }\n  else  { \n    console.log(data);  \n     }    \n});\nIam trying to create a new bucket from my local meteor app \nerror: TypeError: stream.setTimeout is not a function.. hello  @chrisradek  ..\nis there anything wrong with this code ?\nconst params = { Bucket : 'bucketname', Key: 'live.jpg', Body: \"This is a test object\"};\ns3.putObject(params, function(error,data)  {\n  if (error) {\n    console.log('s3 error: ', error);\n  } else {\n  console.log('it worked: ', data);\n  }\n             })\ny iam getting s3 error:  TypeError: stream.setTimeout is not a function.\nplease look into it .\nthanks in advance. browser environment @jeskew \ncan you please explain the basic setup needed for using AWS-SDK API'S. i was stuck here .\nmy setup: \nimport AWS from 'aws-sdk';\n  const s3 = new AWS.S3({accessKeyId:'***************',secretAccessKey:'**************',region:'eu-west-1'});\n\nscope.get = function (){\nvar params = {\n  Bucket: mybucket',\n  Key: 'live.jpg '  (how to give key name ? only file name or with the path to folder in bucket?)\n}\ns3.getObject(params, function(err, data) {\n  if (err) console.log(err, err.stack); \n  else     console.log(data);         \n});\n}\nmy error:\ns3 error: TypeError: stream.setTimeout is not a function.\nthanks a lot in advance \n. now its working via CDN \nthankyou so much @jeskew \ni want to get version id of the file when i upload .\niam using s3.putObject API\nin the document it is mentioned that in data callback we will get versionId but iam getting only Etag in the object? :( . hey @chrisradek \nhere is my code \nvar params = {\n          Bucket: \"test\", \n          Key : 'test/example.docx',\n          Body:\"hello\",\n          ServerSideEncryption: 'AES256'\n        };\n      s3.putObject(params, function(err, data) {\n         if(err){\n               console.log(err)\n          } else{\n                console.log(data,\"the data is\")\n    });\n\nmy browser console:\n Object {ETag: \"\"5d41402abc4b2a76b9719d911017c572\"\"}\nI was getting only the Etag in response .i need version Id of the object\ni have used s3.getBucketVersioning API to check if versioning is enabled for bucket . i got status : enabled object in response.\nis there anything iam missing ?? please help me in getting this. :(\n. solved !!! \nthanks a ton @jeskew  and @chrisradek .\ni need to allow it in cors config . :). hey @chrisradek \nthis is what iam getting in console when i get the uploaded file .what should be given in body to get proper image or file.  please help me in getting this.\none more point ..when i upload the image or file to s3 console .it is showing the size of image or file is zero*\n*browser console when i GET the object\nObject\nBody\n:\nUint8Array[5]\n0\n:\n104\n1\n:\n101\n2\n:\n108\n3\n:\n108\n4\n:\n111\nbuffer\n:\n(...)\nbyteLength\n:\n(...)\nbyteOffset\n:\n(...)\nlength\n:\n(...)\nSymbol(Symbol.toStringTag)\n:\n(...)\n__proto__\n:\nUint8Array\nContentType\n:\n\"application/octet-stream; charset=UTF-8\"\nETag\n:\n\"\"5d41402abc4b2a76b9719d911017c592\"\"\nLastModified\n:\n\"Sat, 25 Feb 2017 10:54:33 GMT\"\nMetadata\n:\nObject\nVersionId\n:\n\"kzNZ2io86hjm3GzrxDx6SVnIynRzDnPW\"\n__proto__\n:\nObject. hey @jeskew @chrisradek   iam getting the same body text when i get the object .i came to know that  it is taking the size in bytes for all the objects. Unable to open the downloaded image/file.when i download or open  the file it is saying cannot open the file .the size of file in local after downloading is zero.\nIam passing bucket,key,body in put object. what  actually  BODY is  in aws-sdk\ni want to upload the files to s3.what should i pass in 'Body'.. to upload correctly .  please help me in getting this .it will be helpfull to others as well.\nafter uploading the image/file to s3 it is taking content type by default as 'application/octet-stream' in properties>metadata>\ncontenttype : application/octet-stream.          \nwhat is missing ??\ndo we need to use FS to read the file in body? \nplease respond asap.\nthanks a ton in advance. . ",
    "vedavyas772": "Iam passing keys and location  I.e.  newAWS.S3 (access key ,secret key region ). Hey @chris I have enabled versioning in s3 console ....the issue is when I use s3.putobject API.... in data callback. .not only versiomversion id but it is not showing any other except Etag in console. . Ok thankyou @Chris \nCan you please test it once... I want to know whether  your also  getting the same issue or not . Is there anything we need to pass in params  apart from bucket and key in putObject API to get  version I'd? . Ok thankyou :) I liked your package \n do we need to do anything in s3 console  apart from enabling versioning  for bucket? ?. Hey @jeskew \nEven if I did not give the body.... it is showing the size of the image/file  is zero .I think it's not the issue with 'body\". ",
    "nathanmalishev": "Thanks @chrisradek . Thanks @chrisradek . ",
    "robbiet480": "Thanks for the explanation @jeskew! :). ",
    "rsshilli": "Yup.  Right you are.  Changing RESEND to SUPPRESS fixed it.  Sorry to bother you!. I have the same problem.  @mfaizan1 Can you give a little more insight into what you did wrong?  How did you set the region correctly?\n. I figured out my problem. For the next poor soul, I was updating a file where the encryption was set and I didn't specify the same encryption when updating it. So previously had:\n```\n  let s3Params = {\n    Bucket: someBucket,\n    Key: someKey,\n    Body: contents\n  };\nconsole.log(\"Uploading file with parameters: \" + JSON.stringify(s3Params));\n  return await s3.putObject(s3Params).promise();\n```\nAnd I had to change it to:\n```\n  let s3Params = {\n    Bucket: someBucket,\n    Key: someKey,\n    ServerSideEncryption: \"AES256\",\n    Body: contents\n  };\nconsole.log(\"Uploading file with parameters: \" + JSON.stringify(s3Params));\n  return await s3.putObject(s3Params).promise();\n```\n. ",
    "neerajaset71": "but are you getting welcome email with temp password after this change? I am not getting..what am I missing?. ",
    "DanTup": "@jeskew You could use the JS interop package to create bindings for Dart (like you do TypeScript), however that would limit its use to compiled-to-js Dart (eg. in the browser; maybe Node). Dart has a full VM that runs server side which I think is much more interesting and what I'm trying to do. This would require the code all be native Dart (the Dart VM runs Dart and not JavaScript).\nAt the moment I'm interested in DynamoDB, so I might have a go at porting the JS code for it; though I haven't started looking yet, I don't know how separated it is from the rest of the SDK (which is a much bigger job).. ",
    "grahamjenson": "@chrisradek Cheers this has solved my issue. In case someone else comes along, this is what worked for me:\n```\nRDS = AWS.RDS({region: 'us-west-2'})\nvar params = {\n  SourceDBSnapshotIdentifier: \"arn:aws:...:snap_id\",\n  TargetDBSnapshotIdentifier: \"new_snap\",\n  KmsKeyId: \"...\",\n  SourceRegion: 'us-east-1'\n};\nRDS.copyDBSnapshot(params)\n```. @chrisradek Cheers this has solved my issue. In case someone else comes along, this is what worked for me:\n```\nRDS = AWS.RDS({region: 'us-west-2'})\nvar params = {\n  SourceDBSnapshotIdentifier: \"arn:aws:...:snap_id\",\n  TargetDBSnapshotIdentifier: \"new_snap\",\n  KmsKeyId: \"...\",\n  SourceRegion: 'us-east-1'\n};\nRDS.copyDBSnapshot(params)\n```. ",
    "ide": "Sorry for the false alarm. I tracked down the source of this issue to a bad interaction with another library -- Sentry's \"raven\" library overrides http.ClientRequest and adds a data listener to all response streams, which forces them into flowing mode. That's probably not great behavior but the AWS SDK team might want to consider programming defensively against libraries like these!. ",
    "asplogic": "I tried the same number using twilio without changing the number and it worked.  I also tried a different number but failed on aws.  I would like to use the aws since i dont have to manage another account.. ",
    "vanerleo": "nodejs v7.2.1. nodejs v7.2.1. Great hint, yep, looks like its loading OLD version from somewhere. Great hint, yep, looks like its loading OLD version from somewhere. most likely , will close it . most likely , will close it . ",
    "tgjorgoski": "OK... I should've experimented a little before creating this...\nWhen creating the S3 service instance, I also specified the endpoint and it works again.\nStill not sure what has changed that caused the issue though...\n// s3 client will use the temporary credentials for the upload\n      const bucketS3Service = new S3({\n        credentials: {\n          accessKeyId: creds.accessKey,\n          secretAccessKey: creds.secretAccessKey,\n          sessionToken: creds.sessionToken\n        },\n        s3BucketEndpoint: true,\n        endpoint: `${bucket}.s3.amazonaws.com`\n      });. OK... I should've experimented a little before creating this...\nWhen creating the S3 service instance, I also specified the endpoint and it works again.\nStill not sure what has changed that caused the issue though...\n// s3 client will use the temporary credentials for the upload\n      const bucketS3Service = new S3({\n        credentials: {\n          accessKeyId: creds.accessKey,\n          secretAccessKey: creds.secretAccessKey,\n          sessionToken: creds.sessionToken\n        },\n        s3BucketEndpoint: true,\n        endpoint: `${bucket}.s3.amazonaws.com`\n      });. ",
    "spencerhakim": "I think this should also be considered a bug. At a minimum, if this isn't a supported pattern, an error should be thrown instead of just silently converting my data to undefined.. I think it'd make more sense to just return an error based on the HTTP code in this situation. \"Service Unavailable\" is far more relevant than \"Unable to parse JSON\"; especially for a service like DynamoDB, a JSON related error would probably lead me to believe that my data is somehow corrupt rather than it simply being an issue with AWS.. ",
    "johanneswuerbach": "Thanks for the hint @jeskew. Tests added and confirmed that they were hanging before the fix and pass after it.. Thanks for the hint @jeskew. Tests added and confirmed that they were hanging before the fix and pass after it.. I also just hit the problem and are working on a fix as we speak.. I also just hit the problem and are working on a fix as we speak.. https://github.com/aws/aws-sdk-js/pull/1395. https://github.com/aws/aws-sdk-js/pull/1395. Calling this without further data results in a noop, but when the stream has more data to read it increments the self.numParts (self.totalPartNumbers ) https://github.com/aws/aws-sdk-js/blob/master/lib/s3/managed_upload.js#L449 causing the if below to be skipped.. ",
    "mramato": "@jeskew, I apologize if I should create a new issue instead of commenting here, but I believe this change is causing my uploads to s3 to become corrupted.  While I'm still debugging the problem, in my case sometimes (but not all of the time) self.finishMultiPart(); is being called too early and the upload completes \"successfully\" but before all parts are actually complete.  The end result is that the file on s3 is truncated and missing data.  If I roll back to a version of the sdk before this change, everything works 100% of the time (like it has for months).\nMy server code that performs the upload is really simple:\n```javascript\nS3DataStore.prototype.uploadForProcessing = function (id, stream) {\n    var size = 0;\n    var uploadStream = new PassThrough();\nstream.on('data', function (data) {\n    size += data.length;\n    var canContinue = uploadStream.write(data);\n    if (!canContinue) {\n        stream.pause();\n        uploadStream.once('drain', function () {\n            stream.resume();\n        });\n    }\n});\n\nstream.on('error', function (error) {\n    uploadStream.emit('error', error);\n});\n\nstream.on('end', function () {\n    uploadStream.end();\n});\n\nreturn s3.upload({\n    Bucket: this._bucket,\n    Key: this._prefix + id,\n    Body: uploadStream\n}).promise().then(function () {\n    return {\n        path: id,\n        size: size\n    };\n});\n\n};\n```\nShould I create a new issue with the above information?  I can try and open a PR if I can figure out the problem in more detail, but this is a major regression for me in the meantime.. @jeskew, I apologize if I should create a new issue instead of commenting here, but I believe this change is causing my uploads to s3 to become corrupted.  While I'm still debugging the problem, in my case sometimes (but not all of the time) self.finishMultiPart(); is being called too early and the upload completes \"successfully\" but before all parts are actually complete.  The end result is that the file on s3 is truncated and missing data.  If I roll back to a version of the sdk before this change, everything works 100% of the time (like it has for months).\nMy server code that performs the upload is really simple:\n```javascript\nS3DataStore.prototype.uploadForProcessing = function (id, stream) {\n    var size = 0;\n    var uploadStream = new PassThrough();\nstream.on('data', function (data) {\n    size += data.length;\n    var canContinue = uploadStream.write(data);\n    if (!canContinue) {\n        stream.pause();\n        uploadStream.once('drain', function () {\n            stream.resume();\n        });\n    }\n});\n\nstream.on('error', function (error) {\n    uploadStream.emit('error', error);\n});\n\nstream.on('end', function () {\n    uploadStream.end();\n});\n\nreturn s3.upload({\n    Bucket: this._bucket,\n    Key: this._prefix + id,\n    Body: uploadStream\n}).promise().then(function () {\n    return {\n        path: id,\n        size: size\n    };\n});\n\n};\n```\nShould I create a new issue with the above information?  I can try and open a PR if I can figure out the problem in more detail, but this is a major regression for me in the meantime.. Awesome, thanks!. Awesome, thanks!. ",
    "marcoeg": "But checking the expiry date for the objects on the S3 console doesn't show any updated value if not only for some of them and in an erratic way. Most likely it is a bug.. I misinterpreted the way putBucketLifecycleConfiguration works. I had mistakenly thought it would add a rule when it is instead replacing all the previous ones. Hence only the last was actually stored. You were right in closing this issue.. ",
    "osdavison": "@chrisradek \nI'm running it in a browser. I was testing the loadFromPath function and forgot to change it back. I have updated my original question.\nThe code you suggested doesn't work for me. I did some digging and found that the httpRequest does have the necessary headers, but that the XMLHTTPRequest doesn't actually send them. \nFunction from the SDK that sends the request:\n\n35: [ function(require, module, exports) {\n        var AWS = require(\"../core\");\n        var EventEmitter = require(\"events\").EventEmitter;\n        require(\"../http\");\n        AWS.XHRClient = AWS.util.inherit({\n            handleRequest: function handleRequest(httpRequest, httpOptions, callback, errCallback) {\n                var self = this;\n                var endpoint = httpRequest.endpoint;\n                var emitter = new EventEmitter();\n                var href = endpoint.protocol + \"//\" + endpoint.hostname;\n                if (endpoint.port !== 80 && endpoint.port !== 443) {\n                    href += \":\" + endpoint.port;\n                }\n                href += httpRequest.path;\n                var xhr = new XMLHttpRequest(), headersEmitted = false;\n                httpRequest.stream = xhr;\n                xhr.addEventListener(\"readystatechange\", function() {\n                    try {\n                        if (xhr.status === 0) return;\n                    } catch (e) {\n                        return;\n                    }\n                    if (this.readyState >= this.HEADERS_RECEIVED && !headersEmitted) {\n                        try {\n                            xhr.responseType = \"arraybuffer\";\n                        } catch (e) {}\n                        emitter.statusCode = xhr.status;\n                        emitter.headers = self.parseHeaders(xhr.getAllResponseHeaders());\n                        emitter.emit(\"headers\", emitter.statusCode, emitter.headers);\n                        headersEmitted = true;\n                    }\n                    if (this.readyState === this.DONE) {\n                        self.finishRequest(xhr, emitter);\n                    }\n                }, false);\n                xhr.upload.addEventListener(\"progress\", function(evt) {\n                    emitter.emit(\"sendProgress\", evt);\n                });\n                xhr.addEventListener(\"progress\", function(evt) {\n                    emitter.emit(\"receiveProgress\", evt);\n                }, false);\n                xhr.addEventListener(\"timeout\", function() {\n                    errCallback(AWS.util.error(new Error(\"Timeout\"), {\n                        code: \"TimeoutError\"\n                    }));\n                }, false);\n                xhr.addEventListener(\"error\", function() {\n                    errCallback(AWS.util.error(new Error(\"Network Failure\"), {\n                        code: \"NetworkingError\"\n                    }));\n                }, false);\n                xhr.addEventListener(\"abort\", function() {\n                    errCallback(AWS.util.error(new Error(\"Request aborted\"), {\n                        code: \"RequestAbortedError\"\n                    }));\n                }, false);\n                callback(emitter);\n                xhr.open(httpRequest.method, href, httpOptions.xhrAsync !== false);\n                AWS.util.each(httpRequest.headers, function(key, value) {\n                    if (key !== \"Content-Length\" && key !== \"User-Agent\" && key !== \"Host\") {\n                        xhr.setRequestHeader(key, value);\n                    }\n                });\n                if (httpOptions.timeout && httpOptions.xhrAsync !== false) {\n                    xhr.timeout = httpOptions.timeout;\n                }\n                if (httpOptions.xhrWithCredentials) {\n                    xhr.withCredentials = true;\n                }\n                try {\n                    xhr.send(httpRequest);\n                } catch (err) {\n                    if (httpRequest.body && typeof httpRequest.body.buffer === \"object\") {\n                        xhr.send(httpRequest.body.buffer);\n                    } else {\n                        throw err;\n                    }\n                }\n                return emitter;\n            },\n            parseHeaders: function parseHeaders(rawHeaders) {\n                var headers = {};\n                AWS.util.arrayEach(rawHeaders.split(/\\r?\\n/), function(line) {\n                    var key = line.split(\":\", 1)[0];\n                    var value = line.substring(key.length + 2);\n                    if (key.length > 0) headers[key.toLowerCase()] = value;\n                });\n                return headers;\n            },\n            finishRequest: function finishRequest(xhr, emitter) {\n                var buffer;\n                if (xhr.responseType === \"arraybuffer\" && xhr.response) {\n                    var ab = xhr.response;\n                    buffer = new AWS.util.Buffer(ab.byteLength);\n                    var view = new Uint8Array(ab);\n                    for (var i = 0; i < buffer.length; ++i) {\n                        buffer[i] = view[i];\n                    }\n                }\n                try {\n                    if (!buffer && typeof xhr.responseText === \"string\") {\n                        buffer = new AWS.util.Buffer(xhr.responseText);\n                    }\n                } catch (e) {}\n                if (buffer) emitter.emit(\"data\", buffer);\n                emitter.emit(\"end\");\n            }\n        });\n        AWS.HttpClient.prototype = AWS.XHRClient.prototype;\n        AWS.HttpClient.streamsApiVersion = 1;\n    }, {\n        \"../core\": 21,\n        \"../http\": 34,\n        events: 6\n    } ],\n\n. @jeskew \nAre you saying that it is an issue with the AppStream API? How can I do this differently so I won't encounter this error?. I used Fiddler to ignore the response from the API and send the GET request anyways and it worked fine then. Why is that if the API doesn't support cross origin sharing?. I was actually talking about this Fiddler. I will look into using Lambda. Thanks for your help.. ",
    "smber1": "The clientContext will also only be passed to the invoked lambda if the InvocationType is RequestResponse.  Passing Event as the InvocationType (for fire & forget/async invocations) seems to suppress the ClientContext for reasons unbeknownst to me!  Perhaps the docs should be updated to relate this?. The clientContext will also only be passed to the invoked lambda if the InvocationType is RequestResponse.  Passing Event as the InvocationType (for fire & forget/async invocations) seems to suppress the ClientContext for reasons unbeknownst to me!  Perhaps the docs should be updated to relate this?. ",
    "Aea": "I can confirm @smber1's observation, not getting passed along when running as Event, fine when running as RequestResponse, my sample payload was: eyJjdXN0b20iOnsidXVpZCI6IjA5MGI4ZWJiLWM0NjAtNGZiZS04ZDM0LTI5NWFkZmVjYmE4OCIsInNoYXJkIjowfX0=\nPersonally I'm finding the documentation around this to be quite sparse, it seems like clientContext is really intended to be used with their mobile / ad SDKs and not well supported otherwise.. ",
    "taneemtee": "Hi Travis - thanks for opening this. The sandbox endpoint is: https://mturk-requester-sandbox.us-east-1.amazonaws.com.\nI apologize for the Sandbox endpoint not being easy to find in our docs - we'll be adding it in a few places to make it easier to find.\n\nHere is an example of Javascript code calling the sandbox -\n```\nvar AWS = require('aws-sdk');\nvar region = 'us-east-1';\nAWS.config = {\n    \"region\": region,\n    \"sslEnabled\": 'true'\n};\nvar endpoint = 'https://mturk-requester-sandbox.us-east-1.amazonaws.com';\n// Uncomment this line to use in production\n// endpoint = 'https://mturk-requester.us-east-1.amazonaws.com';\nvar mturk = new AWS.MTurk({ endpoint: endpoint });\n// This will return $10,000.00 in the MTurk Developer Sandbox\nmturk.getAccountBalance(function(err, data){\n    console.log(data.AvailableBalance);\n});\n```\nWhen calling in production, you can also just call  var mturk = new AWS.MTurk(), without specifying the endpoint, but I find it helps keep it clearer when I do.. Hi Travis - thanks for opening this. The sandbox endpoint is: https://mturk-requester-sandbox.us-east-1.amazonaws.com.\nI apologize for the Sandbox endpoint not being easy to find in our docs - we'll be adding it in a few places to make it easier to find.\n\nHere is an example of Javascript code calling the sandbox -\n```\nvar AWS = require('aws-sdk');\nvar region = 'us-east-1';\nAWS.config = {\n    \"region\": region,\n    \"sslEnabled\": 'true'\n};\nvar endpoint = 'https://mturk-requester-sandbox.us-east-1.amazonaws.com';\n// Uncomment this line to use in production\n// endpoint = 'https://mturk-requester.us-east-1.amazonaws.com';\nvar mturk = new AWS.MTurk({ endpoint: endpoint });\n// This will return $10,000.00 in the MTurk Developer Sandbox\nmturk.getAccountBalance(function(err, data){\n    console.log(data.AvailableBalance);\n});\n```\nWhen calling in production, you can also just call  var mturk = new AWS.MTurk(), without specifying the endpoint, but I find it helps keep it clearer when I do.. ",
    "alsmola": "Anything holding back a merge here? This functionality would be really useful for compatibility with other SDKs.. I've encountered this too. The URL I was getting returned by createPresignedPost was https://s3.amazonaws.com/{bucket_name}, which breaks the CORS preflight OPTIONS request when POSTed to directly (301 status code). I feel like the URL returned should have the bucket name in the host https://[BUCKET_NAME].s3.amazonaws.com to prevent this issue. Note that the region may be included and mitigate this issue if it is set in the AWS config, environment var, etc. but adding the bucket name to the host feels like a more straightforward change.. ",
    "codecov-io": "Codecov Report\n\nMerging #1391 into master will decrease coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1391      +/-\n==========================================\n- Coverage   95.35%   95.34%   -0.02%   \n==========================================\n  Files         176      177       +1   \n  Lines        6222     6270      +48   \n  Branches     1278     1293      +15   \n==========================================\n+ Hits         5933     5978      +45   \n- Misses        289      292       +3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/util.js | 93.33% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/shared_ini.js | 100% <100%> (\u00f8) | |\n| lib/node_loader.js | 85.71% <100%> (-11.73%) | :arrow_down: |\n| lib/credentials/shared_ini_file_credentials.js | 98.59% <100%> (+4.38%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 55a8f68...7528c83. Read the comment docs.\n. # Codecov Report\nMerging #1398 into master will increase coverage by 0.02%.\nThe diff coverage is n/a.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1398      +/-\n==========================================\n+ Coverage   90.83%   90.85%   +0.02%   \n==========================================\n  Files         172      173       +1   \n  Lines        6108     6112       +4   \n  Branches     1249     1249            \n==========================================\n+ Hits         5548     5553       +5   \n+ Misses        560      559       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/model/resource_waiter.js | 100% <0%> (\u00f8) | :white_check_mark: |\n| lib/resource_waiter.js | 100% <0%> (\u00f8) | |\n| lib/http/node.js | 90.36% <0%> (+1.2%) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 373a148...0792e02. Read the comment docs.. # Codecov Report\nMerging #1399 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1399      +/-\n==========================================\n+ Coverage   90.85%   90.86%   +0.01%   \n==========================================\n  Files         173      173            \n  Lines        6113     6121       +8   \n  Branches     1249     1251       +2   \n==========================================\n+ Hits         5554     5562       +8   \n  Misses        559      559\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/dynamodb/converter.js | 99.05% <100%> (+0.07%) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fb50c79...815634a. Read the comment docs.. # Codecov Report\nMerging #1405 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1405      +/-\n==========================================\n+ Coverage   95.18%   95.19%   +<.01%   \n==========================================\n  Files         174      174            \n  Lines        6133     6140       +7   \n  Branches     1252     1255       +3   \n==========================================\n+ Hits         5838     5845       +7   \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/service.js | 96.08% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/resource_waiter.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 40dcb0b...10d7b17. Read the comment docs.. # Codecov Report\nMerging #1406 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1406   +/-\n=======================================\n  Coverage   90.85%   90.85%         \n=======================================\n  Files         173      173         \n  Lines        6113     6113         \n  Branches     1249     1249         \n=======================================\n  Hits         5554     5554         \n  Misses        559      559\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/signers/v4_credentials.js | 100% <\u00f8> (\u00f8) | :white_check_mark: |\n| lib/services/s3.js | 98.01% <\u00f8> (\u00f8) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fb50c79...88873ea. Read the comment docs.. # Codecov Report\nMerging #1407 into master will increase coverage by 4.32%.\nThe diff coverage is 100%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1407      +/-\n==========================================\n+ Coverage   90.85%   95.18%   +4.32%   \n==========================================\n  Files         173      174       +1   \n  Lines        6113     6124      +11   \n  Branches     1249     1250       +1   \n==========================================\n+ Hits         5554     5829     +275   \n+ Misses        559      295     -264\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/param_validator.js | 97.1% <100%> (+0.06%) | :white_check_mark: |\n| lib/protocol/rest.js | 100% <100%> (\u00f8) | :white_check_mark: |\n| lib/model/shape.js | 94.3% <100%> (+0.05%) | :white_check_mark: |\n| index.js | 100% <0%> (\u00f8) | |\n| lib/services/iotdata.js | 30.76% <0%> (+15.38%) | :white_check_mark: |\n| clients/cloudsearch.js | 76.47% <0%> (+23.52%) | :white_check_mark: |\n| clients/lambda.js | 76.47% <0%> (+23.52%) | :white_check_mark: |\n| clients/iotdata.js | 100% <0%> (+25%) | :white_check_mark: |\n| clients/pinpoint.js | 100% <0%> (+27.27%) | :white_check_mark: |\n| clients/servicecatalog.js | 100% <0%> (+27.27%) | :white_check_mark: |\n| ... and 64 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b2016fa...b465137. Read the comment docs.. # Codecov Report\nMerging #1408 into master will increase coverage by 4.33%.\nThe diff coverage is 100%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1408      +/-\n==========================================\n+ Coverage   90.85%   95.18%   +4.33%   \n==========================================\n  Files         173      174       +1   \n  Lines        6113     6128      +15   \n  Branches     1249     1257       +8   \n==========================================\n+ Hits         5554     5833     +279   \n+ Misses        559      295     -264\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event_listeners.js | 98.12% <100%> (+0.05%) | :white_check_mark: |\n| lib/service.js | 96.15% <100%> (+0.06%) | :white_check_mark: |\n| lib/signers/v4.js | 100% <100%> (\u00f8) | :white_check_mark: |\n| lib/services/s3.js | 98.01% <0%> (\u00f8) | :white_check_mark: |\n| index.js | 100% <0%> (\u00f8) | |\n| lib/services/iotdata.js | 30.76% <0%> (+15.38%) | :white_check_mark: |\n| clients/lambda.js | 76.47% <0%> (+23.52%) | :white_check_mark: |\n| clients/cloudsearch.js | 76.47% <0%> (+23.52%) | :white_check_mark: |\n| clients/iotdata.js | 100% <0%> (+25%) | :white_check_mark: |\n| clients/firehose.js | 100% <0%> (+27.27%) | :white_check_mark: |\n| ... and 65 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fb50c79...d0849d5. Read the comment docs.. # Codecov Report\nMerging #1411 into master will increase coverage by 4.31%.\nThe diff coverage is n/a.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1411      +/-\n==========================================\n+ Coverage   90.85%   95.17%   +4.31%   \n==========================================\n  Files         173      174       +1   \n  Lines        6113     6114       +1   \n  Branches     1249     1249            \n==========================================\n+ Hits         5554     5819     +265   \n+ Misses        559      295     -264\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| index.js | 100% <0%> (\u00f8) | |\n| lib/services/iotdata.js | 30.76% <0%> (+15.38%) | :white_check_mark: |\n| clients/cloudsearch.js | 76.47% <0%> (+23.52%) | :white_check_mark: |\n| clients/lambda.js | 76.47% <0%> (+23.52%) | :white_check_mark: |\n| clients/iotdata.js | 100% <0%> (+25%) | :white_check_mark: |\n| clients/lightsail.js | 100% <0%> (+27.27%) | :white_check_mark: |\n| clients/mobileanalytics.js | 100% <0%> (+27.27%) | :white_check_mark: |\n| clients/cloudhsm.js | 100% <0%> (+27.27%) | :white_check_mark: |\n| clients/discovery.js | 100% <0%> (+27.27%) | :white_check_mark: |\n| clients/cognitoidentityserviceprovider.js | 100% <0%> (+27.27%) | :white_check_mark: |\n| ... and 61 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b2016fa...547e4d8. Read the comment docs.. # Codecov Report\nMerging #1418 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1418      +/-\n==========================================\n+ Coverage   95.18%   95.19%   +<.01%   \n==========================================\n  Files         174      174            \n  Lines        6125     6136      +11   \n  Branches     1250     1252       +2   \n==========================================\n+ Hits         5830     5841      +11   \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/util.js | 93.24% <100%> (\u00f8) | :arrow_up: |\n| lib/services/dynamodb.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/dynamodb/converter.js | 99.05% <0%> (+0.07%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 18deee0...c39b709. Read the comment docs.. # Codecov Report\nMerging #1419 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1419   +/-\n=======================================\n  Coverage   95.18%   95.18%         \n=======================================\n  Files         174      174         \n  Lines        6125     6125         \n  Branches     1250     1250         \n=======================================\n  Hits         5830     5830         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 18deee0...06ff211. Read the comment docs.. # Codecov Report\nMerging #1420 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1420      +/-\n==========================================\n+ Coverage   95.22%   95.22%   +<.01%   \n==========================================\n  Files         174      174            \n  Lines        6174     6180       +6   \n  Branches     1267     1269       +2   \n==========================================\n+ Hits         5879     5885       +6   \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 92.46% <100%> (+0.18%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6295d62...c2dd950. Read the comment docs.. # Codecov Report\nMerging #1421 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1421   +/-\n=======================================\n  Coverage   95.19%   95.19%         \n=======================================\n  Files         174      174         \n  Lines        6143     6143         \n  Branches     1255     1255         \n=======================================\n  Hits         5848     5848         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 73d1c78...b17aa8d. Read the comment docs.. # Codecov Report\nMerging #1421 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1421   +/-\n=======================================\n  Coverage   95.19%   95.19%         \n=======================================\n  Files         174      174         \n  Lines        6143     6143         \n  Branches     1255     1255         \n=======================================\n  Hits         5848     5848         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 73d1c78...b17aa8d. Read the comment docs.. # Codecov Report\nMerging #1425 into master will decrease coverage by <.01%.\nThe diff coverage is 94.44%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1425      +/-\n==========================================\n- Coverage   95.19%   95.19%   -0.01%   \n==========================================\n  Files         174      174            \n  Lines        6145     6160      +15   \n  Branches     1255     1259       +4   \n==========================================\n+ Hits         5850     5864      +14   \n- Misses        295      296       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 91.86% <94.44%> (+0.09%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1a46ee6...c67afa8. Read the comment docs.. # Codecov Report\nMerging #1425 into master will decrease coverage by <.01%.\nThe diff coverage is 94.44%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1425      +/-\n==========================================\n- Coverage   95.19%   95.19%   -0.01%   \n==========================================\n  Files         174      174            \n  Lines        6145     6160      +15   \n  Branches     1255     1259       +4   \n==========================================\n+ Hits         5850     5864      +14   \n- Misses        295      296       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 91.86% <94.44%> (+0.09%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1a46ee6...c67afa8. Read the comment docs.. # Codecov Report\nMerging #1428 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1428   +/-\n=======================================\n  Coverage   95.21%   95.21%         \n=======================================\n  Files         174      174         \n  Lines        6160     6160         \n  Branches     1259     1259         \n=======================================\n  Hits         5865     5865         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 78b8b18...73eeb93. Read the comment docs.. # Codecov Report\nMerging #1428 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1428   +/-\n=======================================\n  Coverage   95.21%   95.21%         \n=======================================\n  Files         174      174         \n  Lines        6160     6160         \n  Branches     1259     1259         \n=======================================\n  Hits         5865     5865         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 78b8b18...73eeb93. Read the comment docs.. # Codecov Report\nMerging #1438 into master will decrease coverage by 0.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1438      +/-\n==========================================\n- Coverage   95.22%   95.21%   -0.02%   \n==========================================\n  Files         174      174            \n  Lines        6180     6180            \n  Branches     1269     1269            \n==========================================\n- Hits         5885     5884       -1   \n- Misses        295      296       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/cognito_identity_credentials.js | 99.18% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/http/node.js | 89.15% <0%> (-1.21%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1c6135e...65b1cd1. Read the comment docs.. # Codecov Report\nMerging #1438 into master will decrease coverage by 0.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1438      +/-\n==========================================\n- Coverage   95.22%   95.21%   -0.02%   \n==========================================\n  Files         174      174            \n  Lines        6180     6180            \n  Branches     1269     1269            \n==========================================\n- Hits         5885     5884       -1   \n- Misses        295      296       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/cognito_identity_credentials.js | 99.18% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/http/node.js | 89.15% <0%> (-1.21%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1c6135e...65b1cd1. Read the comment docs.. # Codecov Report\nMerging #1441 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1441   +/-\n=======================================\n  Coverage   95.21%   95.21%         \n=======================================\n  Files         156      156         \n  Lines        6044     6044         \n  Branches     1260     1260         \n=======================================\n  Hits         5755     5755         \n  Misses        289      289\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 95.13% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/credentials.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| clients/s3.js | 98.42% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 42b7eea...5de6a81. Read the comment docs.. # Codecov Report\nMerging #1441 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1441   +/-\n=======================================\n  Coverage   95.21%   95.21%         \n=======================================\n  Files         156      156         \n  Lines        6044     6044         \n  Branches     1260     1260         \n=======================================\n  Hits         5755     5755         \n  Misses        289      289\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 95.13% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/credentials.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| clients/s3.js | 98.42% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 42b7eea...5de6a81. Read the comment docs.. # Codecov Report\nMerging #1446 into master will decrease coverage by 0.05%.\nThe diff coverage is 70%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1446      +/-\n==========================================\n- Coverage   95.24%   95.18%   -0.06%   \n==========================================\n  Files         175      175            \n  Lines        6198     6208      +10   \n  Branches     1269     1272       +3   \n==========================================\n+ Hits         5903     5909       +6   \n- Misses        295      299       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/http/node.js | 88.17% <70%> (-2.19%) | :arrow_down: |\n| lib/util.js | 93% <0%> (-0.24%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1f1fb17...d626c1c. Read the comment docs.. # Codecov Report\nMerging #1446 into master will decrease coverage by 0.05%.\nThe diff coverage is 70%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1446      +/-\n==========================================\n- Coverage   95.24%   95.18%   -0.06%   \n==========================================\n  Files         175      175            \n  Lines        6198     6208      +10   \n  Branches     1269     1272       +3   \n==========================================\n+ Hits         5903     5909       +6   \n- Misses        295      299       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/http/node.js | 88.17% <70%> (-2.19%) | :arrow_down: |\n| lib/util.js | 93% <0%> (-0.24%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1f1fb17...d626c1c. Read the comment docs.. # Codecov Report\nMerging #1457 into master will increase coverage by 0.1%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1457     +/-\n=========================================\n+ Coverage   95.24%   95.35%   +0.1%   \n=========================================\n  Files         175      176      +1   \n  Lines        6209     6222     +13   \n  Branches     1272     1278      +6   \n=========================================\n+ Hits         5914     5933     +19   \n+ Misses        295      289      -6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| clients/lambda.js | 77.77% <100%> (+1.3%) | :arrow_up: |\n| lib/services/lambda.js | 100% <100%> (\u00f8) | |\n| lib/model/shape.js | 94.38% <100%> (+0.08%) | :arrow_up: |\n| lib/protocol/rest_json.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/util.js | 93.33% <100%> (+0.09%) | :arrow_up: |\n| lib/protocol/rest_xml.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/services/iotdata.js | 81.25% <100%> (+50.48%) | :arrow_up: |\n| lib/services/apigateway.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update de5abc0...2a56dd6. Read the comment docs.. # Codecov Report\nMerging #1457 into master will increase coverage by 0.1%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1457     +/-\n=========================================\n+ Coverage   95.24%   95.35%   +0.1%   \n=========================================\n  Files         175      176      +1   \n  Lines        6209     6222     +13   \n  Branches     1272     1278      +6   \n=========================================\n+ Hits         5914     5933     +19   \n+ Misses        295      289      -6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| clients/lambda.js | 77.77% <100%> (+1.3%) | :arrow_up: |\n| lib/services/lambda.js | 100% <100%> (\u00f8) | |\n| lib/model/shape.js | 94.38% <100%> (+0.08%) | :arrow_up: |\n| lib/protocol/rest_json.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/util.js | 93.33% <100%> (+0.09%) | :arrow_up: |\n| lib/protocol/rest_xml.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/services/iotdata.js | 81.25% <100%> (+50.48%) | :arrow_up: |\n| lib/services/apigateway.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update de5abc0...2a56dd6. Read the comment docs.. # Codecov Report\nMerging #1459 into master will increase coverage by 0.1%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1459     +/-\n=========================================\n+ Coverage   95.24%   95.35%   +0.1%   \n=========================================\n  Files         175      176      +1   \n  Lines        6209     6222     +13   \n  Branches     1272     1278      +6   \n=========================================\n+ Hits         5914     5933     +19   \n+ Misses        295      289      -6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/apigateway.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/services/lambda.js | 100% <0%> (\u00f8) | |\n| lib/model/shape.js | 94.38% <0%> (+0.08%) | :arrow_up: |\n| lib/util.js | 93.33% <0%> (+0.09%) | :arrow_up: |\n| clients/lambda.js | 77.77% <0%> (+1.3%) | :arrow_up: |\n| lib/services/iotdata.js | 81.25% <0%> (+50.48%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update de5abc0...1c462f7. Read the comment docs.. # Codecov Report\nMerging #1459 into master will increase coverage by 0.1%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1459     +/-\n=========================================\n+ Coverage   95.24%   95.35%   +0.1%   \n=========================================\n  Files         175      176      +1   \n  Lines        6209     6222     +13   \n  Branches     1272     1278      +6   \n=========================================\n+ Hits         5914     5933     +19   \n+ Misses        295      289      -6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/apigateway.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/services/lambda.js | 100% <0%> (\u00f8) | |\n| lib/model/shape.js | 94.38% <0%> (+0.08%) | :arrow_up: |\n| lib/util.js | 93.33% <0%> (+0.09%) | :arrow_up: |\n| clients/lambda.js | 77.77% <0%> (+1.3%) | :arrow_up: |\n| lib/services/iotdata.js | 81.25% <0%> (+50.48%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update de5abc0...1c462f7. Read the comment docs.. # Codecov Report\nMerging #1460 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1460   +/-\n=======================================\n  Coverage   95.24%   95.24%         \n=======================================\n  Files         175      175         \n  Lines        6209     6209         \n  Branches     1272     1272         \n=======================================\n  Hits         5914     5914         \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/dynamodb/document_client.js | 63.28% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update de5abc0...724ba07. Read the comment docs.. # Codecov Report\nMerging #1460 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1460   +/-\n=======================================\n  Coverage   95.24%   95.24%         \n=======================================\n  Files         175      175         \n  Lines        6209     6209         \n  Branches     1272     1272         \n=======================================\n  Hits         5914     5914         \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/dynamodb/document_client.js | 63.28% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update de5abc0...724ba07. Read the comment docs.. # Codecov Report\nMerging #1462 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1462      +/-\n==========================================\n+ Coverage   95.35%   95.42%   +0.06%   \n==========================================\n  Files         176      176            \n  Lines        6222     6224       +2   \n  Branches     1278     1278            \n==========================================\n+ Hits         5933     5939       +6   \n+ Misses        289      285       -4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.92% <100%> (+1.79%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0c1925f...4ae1b94. Read the comment docs.. # Codecov Report\nMerging #1462 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1462      +/-\n==========================================\n+ Coverage   95.35%   95.42%   +0.06%   \n==========================================\n  Files         176      176            \n  Lines        6222     6224       +2   \n  Branches     1278     1278            \n==========================================\n+ Hits         5933     5939       +6   \n+ Misses        289      285       -4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.92% <100%> (+1.79%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0c1925f...4ae1b94. Read the comment docs.. # Codecov Report\nMerging #1465 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1465   +/-\n=======================================\n  Coverage   95.35%   95.35%         \n=======================================\n  Files         176      176         \n  Lines        6222     6222         \n  Branches     1278     1278         \n=======================================\n  Hits         5933     5933         \n  Misses        289      289\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update da5e7b2...d7db1c1. Read the comment docs.. # Codecov Report\nMerging #1465 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1465   +/-\n=======================================\n  Coverage   95.35%   95.35%         \n=======================================\n  Files         176      176         \n  Lines        6222     6222         \n  Branches     1278     1278         \n=======================================\n  Hits         5933     5933         \n  Misses        289      289\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update da5e7b2...d7db1c1. Read the comment docs.. # Codecov Report\nMerging #1467 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1467   +/-\n=======================================\n  Coverage   95.35%   95.35%         \n=======================================\n  Files         176      176         \n  Lines        6222     6222         \n  Branches     1278     1278         \n=======================================\n  Hits         5933     5933         \n  Misses        289      289\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update da5e7b2...f60ed12. Read the comment docs.\n. # Codecov Report\nMerging #1467 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1467   +/-\n=======================================\n  Coverage   95.35%   95.35%         \n=======================================\n  Files         176      176         \n  Lines        6222     6222         \n  Branches     1278     1278         \n=======================================\n  Hits         5933     5933         \n  Misses        289      289\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update da5e7b2...f60ed12. Read the comment docs.\n. # Codecov Report\nMerging #1473 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1473      +/-\n==========================================\n- Coverage   95.42%   95.42%   -0.01%   \n==========================================\n  Files         179      179            \n  Lines        6297     6295       -2   \n  Branches     1293     1293            \n==========================================\n- Hits         6009     6007       -2   \n  Misses        288      288\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.9% <100%> (-0.03%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 40e6643...dbc7909. Read the comment docs.\n. # Codecov Report\nMerging #1473 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1473      +/-\n==========================================\n- Coverage   95.42%   95.42%   -0.01%   \n==========================================\n  Files         179      179            \n  Lines        6297     6295       -2   \n  Branches     1293     1293            \n==========================================\n- Hits         6009     6007       -2   \n  Misses        288      288\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.9% <100%> (-0.03%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 40e6643...dbc7909. Read the comment docs.\n. # Codecov Report\nMerging #1477 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1477      +/-\n==========================================\n+ Coverage   95.42%   95.42%   +<.01%   \n==========================================\n  Files         179      179            \n  Lines        6295     6296       +1   \n  Branches     1293     1294       +1   \n==========================================\n+ Hits         6007     6008       +1   \n  Misses        288      288\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.91% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 892368a...757d32d. Read the comment docs.\n. # Codecov Report\nMerging #1477 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1477      +/-\n==========================================\n+ Coverage   95.42%   95.42%   +<.01%   \n==========================================\n  Files         179      179            \n  Lines        6295     6296       +1   \n  Branches     1293     1294       +1   \n==========================================\n+ Hits         6007     6008       +1   \n  Misses        288      288\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.91% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 892368a...757d32d. Read the comment docs.\n. # Codecov Report\nMerging #1478 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1478   +/-\n=======================================\n  Coverage   95.42%   95.42%         \n=======================================\n  Files         179      179         \n  Lines        6297     6297         \n  Branches     1294     1294         \n=======================================\n  Hits         6009     6009         \n  Misses        288      288\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8bc9976...b5dfafc. Read the comment docs.\n. # Codecov Report\nMerging #1478 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1478   +/-\n=======================================\n  Coverage   95.42%   95.42%         \n=======================================\n  Files         179      179         \n  Lines        6297     6297         \n  Branches     1294     1294         \n=======================================\n  Hits         6009     6009         \n  Misses        288      288\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8bc9976...b5dfafc. Read the comment docs.\n. # Codecov Report\nMerging #1480 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1480   +/-\n=======================================\n  Coverage   95.42%   95.42%         \n=======================================\n  Files         179      179         \n  Lines        6297     6297         \n  Branches     1294     1294         \n=======================================\n  Hits         6009     6009         \n  Misses        288      288\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8bc9976...6fec9d3. Read the comment docs.\n. # Codecov Report\nMerging #1480 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1480   +/-\n=======================================\n  Coverage   95.42%   95.42%         \n=======================================\n  Files         179      179         \n  Lines        6297     6297         \n  Branches     1294     1294         \n=======================================\n  Hits         6009     6009         \n  Misses        288      288\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8bc9976...6fec9d3. Read the comment docs.\n. # Codecov Report\nMerging #1484 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1484   +/-\n=======================================\n  Coverage   95.42%   95.42%         \n=======================================\n  Files         179      179         \n  Lines        6297     6297         \n  Branches     1294     1294         \n=======================================\n  Hits         6009     6009         \n  Misses        288      288\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/shared_ini.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0a54ed2...555dac3. Read the comment docs.\n. # Codecov Report\nMerging #1485 into master will decrease coverage by 0.09%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1485     +/-\n=========================================\n- Coverage   95.45%   95.36%   -0.1%   \n=========================================\n  Files         180      180           \n  Lines        6364     6368      +4   \n  Branches     1311     1311           \n=========================================\n- Hits         6075     6073      -2   \n- Misses        289      295      +6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/node_loader.js | 86% <100%> (-11.96%) | :arrow_down: |\n| lib/util.js | 93.33% <100%> (\u00f8) | :arrow_up: |\n| lib/rds/signer.js | 98.11% <100%> (+0.11%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dca3aec...f01c66a. Read the comment docs.\n. # Codecov Report\nMerging #1486 into master will decrease coverage by 0.09%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1486     +/-\n=========================================\n- Coverage   95.45%   95.36%   -0.1%   \n=========================================\n  Files         180      180           \n  Lines        6364     6368      +4   \n  Branches     1311     1311           \n=========================================\n- Hits         6075     6073      -2   \n- Misses        289      295      +6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/rds/signer.js | 98.11% <100%> (+0.11%) | :arrow_up: |\n| lib/node_loader.js | 86% <100%> (-11.96%) | :arrow_down: |\n| lib/util.js | 93.33% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dca3aec...0c025f5. Read the comment docs.\n. # Codecov Report\nMerging #1487 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1487   +/-\n=======================================\n  Coverage   95.36%   95.36%         \n=======================================\n  Files         180      180         \n  Lines        6368     6368         \n  Branches     1311     1311         \n=======================================\n  Hits         6073     6073         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 515fcbc...42db737. Read the comment docs.\n. # Codecov Report\nMerging #1494 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1494      +/-\n==========================================\n+ Coverage   95.36%   95.37%   +<.01%   \n==========================================\n  Files         180      180            \n  Lines        6368     6374       +6   \n  Branches     1311     1311            \n==========================================\n+ Hits         6073     6079       +6   \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/resource_waiter.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5889485...808e8af. Read the comment docs.\n. # Codecov Report\nMerging #1494 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1494      +/-\n==========================================\n+ Coverage   95.36%   95.37%   +<.01%   \n==========================================\n  Files         180      180            \n  Lines        6368     6374       +6   \n  Branches     1311     1311            \n==========================================\n+ Hits         6073     6079       +6   \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/resource_waiter.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5889485...808e8af. Read the comment docs.\n. # Codecov Report\nMerging #1496 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1496   +/-\n=======================================\n  Coverage   95.37%   95.37%         \n=======================================\n  Files         180      180         \n  Lines        6374     6374         \n  Branches     1311     1311         \n=======================================\n  Hits         6079     6079         \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.91% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a860695...5e304d4. Read the comment docs.\n. # Codecov Report\nMerging #1496 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1496   +/-\n=======================================\n  Coverage   95.37%   95.37%         \n=======================================\n  Files         180      180         \n  Lines        6374     6374         \n  Branches     1311     1311         \n=======================================\n  Hits         6079     6079         \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.91% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a860695...5e304d4. Read the comment docs.\n. # Codecov Report\nMerging #1497 into master will increase coverage by 0.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1497      +/-\n==========================================\n+ Coverage   95.35%   95.37%   +0.01%   \n==========================================\n  Files         180      180            \n  Lines        6374     6374            \n  Branches     1311     1311            \n==========================================\n+ Hits         6078     6079       +1   \n+ Misses        296      295       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/http/node.js | 91.39% <0%> (+1.07%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6c58abf...795e8bb. Read the comment docs.\n. # Codecov Report\nMerging #1497 into master will increase coverage by 0.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1497      +/-\n==========================================\n+ Coverage   95.35%   95.37%   +0.01%   \n==========================================\n  Files         180      180            \n  Lines        6374     6374            \n  Branches     1311     1311            \n==========================================\n+ Hits         6078     6079       +1   \n+ Misses        296      295       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/http/node.js | 91.39% <0%> (+1.07%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6c58abf...795e8bb. Read the comment docs.\n. # Codecov Report\nMerging #1501 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1501   +/-\n=======================================\n  Coverage   95.37%   95.37%         \n=======================================\n  Files         180      180         \n  Lines        6374     6374         \n  Branches     1311     1311         \n=======================================\n  Hits         6079     6079         \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a860695...efe2693. Read the comment docs.\n. # Codecov Report\nMerging #1501 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1501   +/-\n=======================================\n  Coverage   95.37%   95.37%         \n=======================================\n  Files         180      180         \n  Lines        6374     6374         \n  Branches     1311     1311         \n=======================================\n  Hits         6079     6079         \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a860695...efe2693. Read the comment docs.\n. # Codecov Report\nMerging #1502 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1502   +/-\n=======================================\n  Coverage   95.37%   95.37%         \n=======================================\n  Files         180      180         \n  Lines        6374     6374         \n  Branches     1311     1311         \n=======================================\n  Hits         6079     6079         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9f1237b...42e21db. Read the comment docs.\n. # Codecov Report\nMerging #1502 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1502   +/-\n=======================================\n  Coverage   95.37%   95.37%         \n=======================================\n  Files         180      180         \n  Lines        6374     6374         \n  Branches     1311     1311         \n=======================================\n  Hits         6079     6079         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9f1237b...42e21db. Read the comment docs.\n. # Codecov Report\nMerging #1504 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1504   +/-\n=======================================\n  Coverage   95.38%   95.38%         \n=======================================\n  Files         181      181         \n  Lines        6386     6386         \n  Branches     1311     1311         \n=======================================\n  Hits         6091     6091         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 75343e9...eebc01b. Read the comment docs.\n. # Codecov Report\nMerging #1504 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1504   +/-\n=======================================\n  Coverage   95.38%   95.38%         \n=======================================\n  Files         181      181         \n  Lines        6386     6386         \n  Branches     1311     1311         \n=======================================\n  Hits         6091     6091         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 75343e9...eebc01b. Read the comment docs.\n. # Codecov Report\nMerging #1510 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1510   +/-\n=======================================\n  Coverage   95.38%   95.38%         \n=======================================\n  Files         181      181         \n  Lines        6392     6392         \n  Branches     1312     1312         \n=======================================\n  Hits         6097     6097         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8904e9c...f4e3868. Read the comment docs.\n. # Codecov Report\nMerging #1510 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1510   +/-\n=======================================\n  Coverage   95.38%   95.38%         \n=======================================\n  Files         181      181         \n  Lines        6392     6392         \n  Branches     1312     1312         \n=======================================\n  Hits         6097     6097         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8904e9c...f4e3868. Read the comment docs.\n. # Codecov Report\nMerging #1517 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1517   +/-\n=======================================\n  Coverage   95.39%   95.39%         \n=======================================\n  Files         182      182         \n  Lines        6406     6406         \n  Branches     1312     1312         \n=======================================\n  Hits         6111     6111         \n  Misses        295      295\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4950fdb...850406a. Read the comment docs.\n. # Codecov Report\nMerging #1530 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1530      +/-\n==========================================\n+ Coverage   95.39%   95.39%   +<.01%   \n==========================================\n  Files         182      182            \n  Lines        6408     6409       +1   \n  Branches     1312     1312            \n==========================================\n+ Hits         6113     6114       +1   \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 92.49% <100%> (+0.02%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5d31704...53006e5. Read the comment docs.\n. # Codecov Report\nMerging #1530 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1530      +/-\n==========================================\n+ Coverage   95.39%   95.39%   +<.01%   \n==========================================\n  Files         182      182            \n  Lines        6408     6409       +1   \n  Branches     1312     1312            \n==========================================\n+ Hits         6113     6114       +1   \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 92.49% <100%> (+0.02%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5d31704...53006e5. Read the comment docs.\n. # Codecov Report\nMerging #1532 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1532      +/-\n==========================================\n+ Coverage   95.39%   95.42%   +0.03%   \n==========================================\n  Files         182      182            \n  Lines        6408     6582     +174   \n  Branches     1312     1383      +71   \n==========================================\n+ Hits         6113     6281     +168   \n- Misses        295      301       +6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <100%> (\u00f8) | :arrow_up: |\n| lib/s3/managed_upload.js | 92.78% <0%> (+0.32%) | :arrow_up: |\n| lib/request.js | 97.33% <0%> (+0.36%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5d31704...4f19183. Read the comment docs.\n. # Codecov Report\nMerging #1532 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1532      +/-\n==========================================\n+ Coverage   95.39%   95.42%   +0.03%   \n==========================================\n  Files         182      182            \n  Lines        6408     6582     +174   \n  Branches     1312     1383      +71   \n==========================================\n+ Hits         6113     6281     +168   \n- Misses        295      301       +6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <100%> (\u00f8) | :arrow_up: |\n| lib/s3/managed_upload.js | 92.78% <0%> (+0.32%) | :arrow_up: |\n| lib/request.js | 97.33% <0%> (+0.36%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5d31704...4f19183. Read the comment docs.\n. # Codecov Report\nMerging #1533 into master will increase coverage by 0.03%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1533      +/-\n==========================================\n+ Coverage   95.39%   95.42%   +0.03%   \n==========================================\n  Files         182      182            \n  Lines        6408     6582     +174   \n  Branches     1312     1383      +71   \n==========================================\n+ Hits         6113     6281     +168   \n- Misses        295      301       +6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/s3/managed_upload.js | 92.78% <0%> (+0.32%) | :arrow_up: |\n| lib/request.js | 97.33% <0%> (+0.36%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5d31704...791fdcd. Read the comment docs.\n. # Codecov Report\nMerging #1536 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1536   +/-\n=======================================\n  Coverage   95.39%   95.39%         \n=======================================\n  Files         182      182         \n  Lines        6409     6409         \n  Branches     1312     1312         \n=======================================\n  Hits         6114     6114         \n  Misses        295      295\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.96% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 360f743...ffaaa62. Read the comment docs.\n. # Codecov Report\nMerging #1537 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1537      +/-\n==========================================\n+ Coverage   95.39%   95.41%   +0.01%   \n==========================================\n  Files         182      182            \n  Lines        6409     6407       -2   \n  Branches     1312     1311       -1   \n==========================================\n- Hits         6114     6113       -1   \n+ Misses        295      294       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/model/shape.js | 94.84% <100%> (+0.45%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update da8f568...2959326. Read the comment docs.\n. # Codecov Report\nMerging #1540 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1540   +/-\n=======================================\n  Coverage   95.41%   95.41%         \n=======================================\n  Files         182      182         \n  Lines        6407     6407         \n  Branches     1311     1311         \n=======================================\n  Hits         6113     6113         \n  Misses        294      294\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fc2439d...1f89d20. Read the comment docs.\n. # Codecov Report\nMerging #1540 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1540   +/-\n=======================================\n  Coverage   95.41%   95.41%         \n=======================================\n  Files         182      182         \n  Lines        6408     6408         \n  Branches     1311     1311         \n=======================================\n  Hits         6114     6114         \n  Misses        294      294\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5f88452...1280776. Read the comment docs.\n. # Codecov Report\nMerging #1542 into master will increase coverage by 0.01%.\nThe diff coverage is 96.66%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1542      +/-\n==========================================\n+ Coverage   95.41%   95.42%   +0.01%   \n==========================================\n  Files         182      182            \n  Lines        6407     6427      +20   \n  Branches     1311     1319       +8   \n==========================================\n+ Hits         6113     6133      +20   \n  Misses        294      294\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/node_loader.js | 86% <0%> (\u00f8) | :arrow_up: |\n| lib/credentials/ecs_credentials.js | 94.02% <100%> (+2.54%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fc2439d...9e366ed. Read the comment docs.\n. # Codecov Report\nMerging #1549 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1549      +/-\n==========================================\n+ Coverage   95.41%   95.47%   +0.06%   \n==========================================\n  Files         182      182            \n  Lines        6409     6438      +29   \n  Branches     1311     1321      +10   \n==========================================\n+ Hits         6115     6147      +32   \n+ Misses        294      291       -3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.98% <100%> (+0.01%) | :arrow_up: |\n| clients/iot.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/s3/managed_upload.js | 93.84% <0%> (+1.35%) | :arrow_up: |\n| lib/credentials/ecs_credentials.js | 94.02% <0%> (+2.54%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5fcf374...9eaa1b0. Read the comment docs.\n. # Codecov Report\nMerging #1550 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1550      +/-\n==========================================\n+ Coverage   95.41%   95.47%   +0.06%   \n==========================================\n  Files         182      182            \n  Lines        6409     6437      +28   \n  Branches     1311     1321      +10   \n==========================================\n+ Hits         6115     6146      +31   \n+ Misses        294      291       -3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/request.js | 96.96% <100%> (\u00f8) | :arrow_up: |\n| lib/s3/managed_upload.js | 93.84% <100%> (+1.35%) | :arrow_up: |\n| clients/iot.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/credentials/ecs_credentials.js | 94.02% <0%> (+2.54%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5fcf374...90bece7. Read the comment docs.\n. # Codecov Report\nMerging #1555 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1555   +/-\n=======================================\n  Coverage   95.48%   95.48%         \n=======================================\n  Files         183      183         \n  Lines        6448     6448         \n  Branches     1321     1321         \n=======================================\n  Hits         6157     6157         \n  Misses        291      291\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bf07fdc...138897a. Read the comment docs.\n. # Codecov Report\nMerging #1558 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1558      +/-\n==========================================\n+ Coverage   95.48%   95.49%   +0.01%   \n==========================================\n  Files         183      183            \n  Lines        6448     6463      +15   \n  Branches     1321     1325       +4   \n==========================================\n+ Hits         6157     6172      +15   \n  Misses        291      291\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.08% <100%> (+0.07%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 20ecff7...cbcb51a. Read the comment docs.\n. # Codecov Report\nMerging #1562 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1562   +/-\n=======================================\n  Coverage   95.49%   95.49%         \n=======================================\n  Files         183      183         \n  Lines        6463     6463         \n  Branches     1325     1325         \n=======================================\n  Hits         6172     6172         \n  Misses        291      291\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7b28105...85a2b3a. Read the comment docs.\n. # Codecov Report\nMerging #1563 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1563   +/-\n=======================================\n  Coverage   95.49%   95.49%         \n=======================================\n  Files         183      183         \n  Lines        6463     6463         \n  Branches     1325     1325         \n=======================================\n  Hits         6172     6172         \n  Misses        291      291\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7b28105...a8af11b. Read the comment docs.\n. # Codecov Report\nMerging #1568 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1568   +/-\n=======================================\n  Coverage   95.64%   95.64%         \n=======================================\n  Files         189      189         \n  Lines        6583     6583         \n  Branches     1341     1341         \n=======================================\n  Hits         6296     6296         \n  Misses        287      287\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 02b9c97...6d22602. Read the comment docs.\n. # Codecov Report\nMerging #1573 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1573      +/-\n==========================================\n+ Coverage   95.49%   95.56%   +0.06%   \n==========================================\n  Files         183      183            \n  Lines        6463     6467       +4   \n  Branches     1325     1325            \n==========================================\n+ Hits         6172     6180       +8   \n+ Misses        291      287       -4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/dynamodb/types.js | 95.65% <100%> (+17.39%) | :arrow_up: |\n| lib/dynamodb/converter.js | 99.09% <100%> (+0.03%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9b6fcac...953504a. Read the comment docs.\n. # Codecov Report\nMerging #1587 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1587      +/-\n==========================================\n+ Coverage   95.56%   95.56%   +<.01%   \n==========================================\n  Files         183      183            \n  Lines        6469     6478       +9   \n  Branches     1325     1329       +4   \n==========================================\n+ Hits         6182     6191       +9   \n  Misses        287      287\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.11% <100%> (+0.03%) | :arrow_up: |\n| lib/s3/managed_upload.js | 93.89% <100%> (+0.04%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 17612f9...9a0c6eb. Read the comment docs.\n. # Codecov Report\nMerging #1604 into master will decrease coverage by 0.01%.\nThe diff coverage is 87.5%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1604      +/-\n==========================================\n- Coverage   95.57%   95.56%   -0.02%   \n==========================================\n  Files         184      184            \n  Lines        6491     6498       +7   \n  Branches     1329     1332       +3   \n==========================================\n+ Hits         6204     6210       +6   \n- Misses        287      288       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event_listeners.js | 97.16% <87.5%> (-0.3%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ec6f44...89c3646. Read the comment docs.\n. # Codecov Report\nMerging #1606 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1606   +/-\n=======================================\n  Coverage   95.56%   95.56%         \n=======================================\n  Files         184      184         \n  Lines        6498     6498         \n  Branches     1332     1332         \n=======================================\n  Hits         6210     6210         \n  Misses        288      288\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1dd824a...be4c8ca. Read the comment docs.\n. # Codecov Report\nMerging #1608 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1608      +/-\n==========================================\n+ Coverage   95.56%   95.59%   +0.02%   \n==========================================\n  Files         184      184            \n  Lines        6498     6508      +10   \n  Branches     1332     1337       +5   \n==========================================\n+ Hits         6210     6221      +11   \n+ Misses        288      287       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event_listeners.js | 97.19% <100%> (+0.02%) | :arrow_up: |\n| lib/service.js | 96.26% <100%> (+0.04%) | :arrow_up: |\n| lib/signers/v4.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/api_loader.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/services/apigateway.js | 100% <0%> (\u00f8) | :arrow_up: |\n| clients/directoryservice.js | 100% <0%> (\u00f8) | :arrow_up: |\n| clients/all.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/request.js | 96.98% <0%> (+0.01%) | :arrow_up: |\n| lib/xml/node_parser.js | 93.4% <0%> (+0.07%) | :arrow_up: |\n| lib/node_loader.js | 87.5% <0%> (+1.5%) | :arrow_up: |\n| ... and 1 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c2cd058...35f773e. Read the comment docs.\n. # Codecov Report\nMerging #1609 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1609      +/-\n==========================================\n+ Coverage   95.56%   95.56%   +<.01%   \n==========================================\n  Files         184      184            \n  Lines        6498     6501       +3   \n  Branches     1332     1333       +1   \n==========================================\n+ Hits         6210     6213       +3   \n  Misses        288      288\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/signers/v4.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c2cd058...124dc77. Read the comment docs.\n. # Codecov Report\nMerging #1610 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1610      +/-\n==========================================\n+ Coverage   95.56%   95.57%   +<.01%   \n==========================================\n  Files         184      184            \n  Lines        6501     6502       +1   \n  Branches     1333     1334       +1   \n==========================================\n+ Hits         6213     6214       +1   \n  Misses        288      288\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/apigateway.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5d86846...94472a3. Read the comment docs.\n. # Codecov Report\nMerging #1649 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1649   +/-\n=======================================\n  Coverage   95.59%   95.59%         \n=======================================\n  Files         184      184         \n  Lines        6508     6508         \n  Branches     1337     1337         \n=======================================\n  Hits         6221     6221         \n  Misses        287      287\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 28c8cc6...2d33f1f. Read the comment docs.\n. # Codecov Report\nMerging #1651 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1651      +/-\n==========================================\n+ Coverage   95.59%   95.59%   +<.01%   \n==========================================\n  Files         184      185       +1   \n  Lines        6508     6519      +11   \n  Branches     1337     1337            \n==========================================\n+ Hits         6221     6232      +11   \n  Misses        287      287\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/dynamodb/set.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/dynamodb/numberValue.js | 100% <100%> (\u00f8) | |\n| lib/dynamodb/translator.js | 79.62% <100%> (+0.38%) | :arrow_up: |\n| lib/dynamodb/converter.js | 99.12% <100%> (+0.03%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 53e196e...91608d5. Read the comment docs.\n. # Codecov Report\nMerging #1653 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1653   +/-\n=======================================\n  Coverage   95.59%   95.59%         \n=======================================\n  Files         185      185         \n  Lines        6519     6519         \n  Branches     1337     1337         \n=======================================\n  Hits         6232     6232         \n  Misses        287      287\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cd3cc54...8175af6. Read the comment docs.\n. # Codecov Report\nMerging #1655 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1655   +/-\n=======================================\n  Coverage   95.59%   95.59%         \n=======================================\n  Files         185      185         \n  Lines        6519     6519         \n  Branches     1337     1337         \n=======================================\n  Hits         6232     6232         \n  Misses        287      287\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cd3cc54...125f253. Read the comment docs.\n. # Codecov Report\nMerging #1658 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1658      +/-\n==========================================\n+ Coverage   95.59%   95.61%   +0.01%   \n==========================================\n  Files         185      186       +1   \n  Lines        6519     6540      +21   \n  Branches     1337     1341       +4   \n==========================================\n+ Hits         6232     6253      +21   \n  Misses        287      287\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/node_loader.js | 87.71% <100%> (+0.21%) | :arrow_up: |\n| lib/credentials/assume_role_credentials.js | 100% <100%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 92891be...b26f291. Read the comment docs.\n. # Codecov Report\nMerging #1660 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1660   +/-\n=======================================\n  Coverage   95.59%   95.59%         \n=======================================\n  Files         185      185         \n  Lines        6519     6519         \n  Branches     1337     1337         \n=======================================\n  Hits         6232     6232         \n  Misses        287      287\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 92891be...6215f57. Read the comment docs.\n. # Codecov Report\nMerging #1662 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1662   +/-\n=======================================\n  Coverage   95.59%   95.59%         \n=======================================\n  Files         185      185         \n  Lines        6519     6519         \n  Branches     1337     1337         \n=======================================\n  Hits         6232     6232         \n  Misses        287      287\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 92891be...cb3933e. Read the comment docs.\n. # Codecov Report\nMerging #1672 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1672   +/-\n=======================================\n  Coverage   95.59%   95.59%         \n=======================================\n  Files         185      185         \n  Lines        6519     6519         \n  Branches     1337     1337         \n=======================================\n  Hits         6232     6232         \n  Misses        287      287\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ac2cee0...8d74c23. Read the comment docs.\n. # Codecov Report\nMerging #1673 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1673   +/-\n=======================================\n  Coverage   95.59%   95.59%         \n=======================================\n  Files         185      185         \n  Lines        6519     6519         \n  Branches     1337     1337         \n=======================================\n  Hits         6232     6232         \n  Misses        287      287\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ac2cee0...b16b7f6. Read the comment docs.\n. # Codecov Report\nMerging #1674 into master will decrease coverage by 0.02%.\nThe diff coverage is 75%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1674      +/-\n==========================================\n- Coverage   95.59%   95.57%   -0.03%   \n==========================================\n  Files         185      185            \n  Lines        6519     6526       +7   \n  Branches     1337     1340       +3   \n==========================================\n+ Hits         6232     6237       +5   \n- Misses        287      289       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 97.68% <75%> (-0.44%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ac2cee0...6fa793c. Read the comment docs.\n. # Codecov Report\nMerging #1675 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1675   +/-\n=======================================\n  Coverage   95.59%   95.59%         \n=======================================\n  Files         185      185         \n  Lines        6519     6519         \n  Branches     1337     1337         \n=======================================\n  Hits         6232     6232         \n  Misses        287      287\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ac2cee0...c5a02d8. Read the comment docs.\n. # Codecov Report\nMerging #1676 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #1676      +/-\n=========================================\n+ Coverage   95.59%   95.6%   +<.01%   \n=========================================\n  Files         185     185            \n  Lines        6519    6528       +9   \n  Branches     1337    1340       +3   \n=========================================\n+ Hits         6232    6241       +9   \n  Misses        287     287\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.13% <100%> (+0.01%) | :arrow_up: |\n| lib/util.js | 93.42% <100%> (+0.09%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b3ddb05...f19992e. Read the comment docs.\n. # Codecov Report\nMerging #1677 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #1677      +/-\n=========================================\n+ Coverage   95.59%   95.6%   +<.01%   \n=========================================\n  Files         185     185            \n  Lines        6519    6524       +5   \n  Branches     1337    1338       +1   \n=========================================\n+ Hits         6232    6237       +5   \n  Misses        287     287\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/protocol/rest_json.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b3ddb05...0dcf4b6. Read the comment docs.\n. # Codecov Report\nMerging #1705 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1705   +/-\n=======================================\n  Coverage   95.63%   95.63%         \n=======================================\n  Files         188      188         \n  Lines        6571     6571         \n  Branches     1341     1341         \n=======================================\n  Hits         6284     6284         \n  Misses        287      287\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a79945...a41feec. Read the comment docs.\n. # Codecov Report\nMerging #1711 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1711      +/-\n==========================================\n+ Coverage   95.64%   95.64%   +<.01%   \n==========================================\n  Files         189      189            \n  Lines        6583     6590       +7   \n  Branches     1341     1341            \n==========================================\n+ Hits         6296     6303       +7   \n  Misses        287      287\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.15% <100%> (+0.02%) | :arrow_up: |\n| lib/util.js | 93.45% <100%> (+0.02%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c8643e9...43d7599. Read the comment docs.\n. # Codecov Report\nMerging #1715 into master will decrease coverage by 0.05%.\nThe diff coverage is 95%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1715      +/-\n==========================================\n- Coverage   95.64%   95.58%   -0.06%   \n==========================================\n  Files         189      189            \n  Lines        6583     6595      +12   \n  Branches     1341     1346       +5   \n==========================================\n+ Hits         6296     6304       +8   \n- Misses        287      291       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/signers/presign.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/services/s3.js | 98.13% <100%> (\u00f8) | :arrow_up: |\n| lib/request.js | 96.98% <100%> (\u00f8) | :arrow_up: |\n| lib/service.js | 96.38% <100%> (+0.11%) | :arrow_up: |\n| lib/event_listeners.js | 96.87% <88.88%> (-0.32%) | :arrow_down: |\n| lib/util.js | 92.74% <0%> (-0.69%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b8323c4...9edaaa8. Read the comment docs.\n. # Codecov Report\nMerging #1716 into master will increase coverage by 0.06%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #1716      +/-\n=========================================\n+ Coverage   95.64%   95.7%   +0.06%   \n=========================================\n  Files         189     189            \n  Lines        6583    6593      +10   \n  Branches     1341    1344       +3   \n=========================================\n+ Hits         6296    6310      +14   \n+ Misses        287     283       -4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.13% <100%> (\u00f8) | :arrow_up: |\n| lib/event_listeners.js | 97.2% <100%> (\u00f8) | :arrow_up: |\n| lib/signers/presign.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/request.js | 96.98% <100%> (\u00f8) | :arrow_up: |\n| lib/service.js | 96.38% <100%> (+0.11%) | :arrow_up: |\n| lib/util.js | 94.33% <0%> (+0.9%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b8323c4...501a85f. Read the comment docs.\n. # Codecov Report\nMerging #1725 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #1725      +/-\n=========================================\n- Coverage    95.7%   95.7%   -0.01%   \n=========================================\n  Files         189     189            \n  Lines        6593    6592       -1   \n  Branches     1344    1343       -1   \n=========================================\n- Hits         6310    6309       -1   \n  Misses        283     283\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/core.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/event_listeners.js | 97.19% <100%> (-0.01%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8a43108...3aa29d1. Read the comment docs.\n. # Codecov Report\nMerging #1728 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1728   +/-\n======================================\n  Coverage    95.7%   95.7%         \n======================================\n  Files         189     189         \n  Lines        6592    6592         \n  Branches     1343    1343         \n======================================\n  Hits         6309    6309         \n  Misses        283     283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3ffe622...d3315d3. Read the comment docs.\n. # Codecov Report\nMerging #1738 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1738   +/-\n=======================================\n  Coverage   77.09%   77.09%         \n=======================================\n  Files         189      189         \n  Lines        6593     6593         \n  Branches     1343     1343         \n=======================================\n  Hits         5083     5083         \n  Misses       1510     1510\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 664143d...24e517d. Read the comment docs.\n. # Codecov Report\nMerging #1739 into master will increase coverage by 18.61%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster   #1739       +/-\n==========================================\n+ Coverage   77.09%   95.7%   +18.61%   \n==========================================\n  Files         189     189             \n  Lines        6593    6594        +1   \n  Branches     1343    1344        +1   \n==========================================\n+ Hits         5083    6311     +1228   \n+ Misses       1510     283     -1227\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.35% <\u00f8> (+24.56%) | :arrow_up: |\n| lib/s3/managed_upload.js | 93.89% <0%> (+0.38%) | :arrow_up: |\n| lib/services/s3.js | 98.13% <0%> (+0.7%) | :arrow_up: |\n| lib/model/api.js | 100% <0%> (+2.27%) | :arrow_up: |\n| lib/http.js | 100% <0%> (+4.83%) | :arrow_up: |\n| lib/node_loader.js | 87.5% <0%> (+10.71%) | :arrow_up: |\n| lib/credentials/environment_credentials.js | 91.3% <0%> (+13.04%) | :arrow_up: |\n| lib/credentials/credential_provider_chain.js | 100% <0%> (+15.62%) | :arrow_up: |\n| lib/credentials.js | 100% <0%> (+15.78%) | :arrow_up: |\n| lib/util.js | 94.33% <0%> (+16.09%) | :arrow_up: |\n| ... and 109 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 700cecd...0639c17. Read the comment docs.\n. # Codecov Report\nMerging #1743 into master will increase coverage by 18.61%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster   #1743       +/-\n==========================================\n+ Coverage   77.09%   95.7%   +18.61%   \n==========================================\n  Files         189     189             \n  Lines        6593    6593             \n  Branches     1343    1343             \n==========================================\n+ Hits         5083    6310     +1227   \n+ Misses       1510     283     -1227\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (+24.41%) | :arrow_up: |\n| lib/s3/managed_upload.js | 93.89% <0%> (+0.38%) | :arrow_up: |\n| lib/services/s3.js | 98.13% <0%> (+0.7%) | :arrow_up: |\n| lib/model/api.js | 100% <0%> (+2.27%) | :arrow_up: |\n| lib/http.js | 100% <0%> (+4.83%) | :arrow_up: |\n| lib/node_loader.js | 87.5% <0%> (+10.71%) | :arrow_up: |\n| lib/credentials/environment_credentials.js | 91.3% <0%> (+13.04%) | :arrow_up: |\n| lib/credentials/credential_provider_chain.js | 100% <0%> (+15.62%) | :arrow_up: |\n| lib/credentials.js | 100% <0%> (+15.78%) | :arrow_up: |\n| lib/util.js | 94.33% <0%> (+16.09%) | :arrow_up: |\n| ... and 109 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update afe798b...7a2bc72. Read the comment docs.\n. # Codecov Report\nMerging #1746 into master will increase coverage by 18.61%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster   #1746       +/-\n==========================================\n+ Coverage   77.09%   95.7%   +18.61%   \n==========================================\n  Files         189     189             \n  Lines        6593    6593             \n  Branches     1343    1343             \n==========================================\n+ Hits         5083    6310     +1227   \n+ Misses       1510     283     -1227\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 93.89% <0%> (+0.38%) | :arrow_up: |\n| lib/services/s3.js | 98.13% <0%> (+0.7%) | :arrow_up: |\n| lib/model/api.js | 100% <0%> (+2.27%) | :arrow_up: |\n| lib/http.js | 100% <0%> (+4.83%) | :arrow_up: |\n| lib/node_loader.js | 87.5% <0%> (+10.71%) | :arrow_up: |\n| lib/credentials/environment_credentials.js | 91.3% <0%> (+13.04%) | :arrow_up: |\n| lib/credentials/credential_provider_chain.js | 100% <0%> (+15.62%) | :arrow_up: |\n| lib/credentials.js | 100% <0%> (+15.78%) | :arrow_up: |\n| lib/util.js | 94.33% <0%> (+16.09%) | :arrow_up: |\n| lib/api_loader.js | 100% <0%> (+16.66%) | :arrow_up: |\n| ... and 108 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d38bcfd...265fc13. Read the comment docs.\n. # Codecov Report\nMerging #1754 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1754   +/-\n======================================\n  Coverage    95.7%   95.7%         \n======================================\n  Files         189     189         \n  Lines        6593    6593         \n  Branches     1343    1343         \n======================================\n  Hits         6310    6310         \n  Misses        283     283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 98b5e28...2c7a8f9. Read the comment docs.\n. # Codecov Report\nMerging #1762 into master will decrease coverage by 0.72%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1762      +/-\n==========================================\n- Coverage    95.7%   94.98%   -0.73%   \n==========================================\n  Files         189      189            \n  Lines        6593     6596       +3   \n  Branches     1343     1345       +2   \n==========================================\n- Hits         6310     6265      -45   \n- Misses        283      331      +48\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 77.73% <100%> (-16.16%) | :arrow_down: |\n| lib/util.js | 93.19% <0%> (-1.14%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2751999...f8dbd52. Read the comment docs.\n. # Codecov Report\nMerging #1765 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1765   +/-\n======================================\n  Coverage    95.7%   95.7%         \n======================================\n  Files         189     189         \n  Lines        6593    6593         \n  Branches     1343    1343         \n======================================\n  Hits         6310    6310         \n  Misses        283     283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ebcb767...a0ecf35. Read the comment docs.\n. # Codecov Report\nMerging #1767 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1767   +/-\n======================================\n  Coverage    95.7%   95.7%         \n======================================\n  Files         189     189         \n  Lines        6593    6593         \n  Branches     1343    1343         \n======================================\n  Hits         6310    6310         \n  Misses        283     283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e778e17...14e7742. Read the comment docs.\n. # Codecov Report\nMerging #1773 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1773   +/-\n======================================\n  Coverage    95.7%   95.7%         \n======================================\n  Files         189     189         \n  Lines        6593    6593         \n  Branches     1343    1343         \n======================================\n  Hits         6310    6310         \n  Misses        283     283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e778e17...8ad6b68. Read the comment docs.\n. # Codecov Report\nMerging #1775 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1775   +/-\n======================================\n  Coverage    95.7%   95.7%         \n======================================\n  Files         189     189         \n  Lines        6593    6593         \n  Branches     1343    1343         \n======================================\n  Hits         6310    6310         \n  Misses        283     283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d723282...e3487d6. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@1ecbb24). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1793   +/-\n=========================================\n  Coverage          ?   95.73%         \n=========================================\n  Files             ?      190         \n  Lines             ?     6640         \n  Branches          ?     1356         \n=========================================\n  Hits              ?     6357         \n  Misses            ?      283         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1ecbb24...2715358. Read the comment docs.\n. # Codecov Report\nMerging #1795 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1795      +/-\n==========================================\n+ Coverage   95.73%   95.74%   +<.01%   \n==========================================\n  Files         190      190            \n  Lines        6640     6649       +9   \n  Branches     1356     1362       +6   \n==========================================\n+ Hits         6357     6366       +9   \n  Misses        283      283\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/ecs_credentials.js | 94.73% <100%> (+0.7%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c108474...18526ff. Read the comment docs.\n. # Codecov Report\nMerging #1797 into master will increase coverage by 0.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1797      +/-\n==========================================\n+ Coverage   95.73%   95.75%   +0.01%   \n==========================================\n  Files         190      191       +1   \n  Lines        6640     6668      +28   \n  Branches     1356     1364       +8   \n==========================================\n+ Hits         6357     6385      +28   \n  Misses        283      283\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| clients/shield.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/protocol/rest_json.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/signers/presign.js | 100% <0%> (\u00f8) | :arrow_up: |\n| clients/opsworkscm.js | 100% <0%> (\u00f8) | :arrow_up: |\n| clients/costexplorer.js | 100% <0%> (\u00f8) | |\n| lib/credentials/ecs_credentials.js | 94.73% <0%> (+0.7%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c108474...38af2c0. Read the comment docs.\n. # Codecov Report\nMerging #1799 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1799   +/-\n=======================================\n  Coverage   95.74%   95.74%         \n=======================================\n  Files         190      190         \n  Lines        6649     6649         \n  Branches     1362     1362         \n=======================================\n  Hits         6366     6366         \n  Misses        283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3c1e832...d3349c4. Read the comment docs.\n. # Codecov Report\nMerging #1803 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1803   +/-\n=======================================\n  Coverage   95.74%   95.74%         \n=======================================\n  Files         190      190         \n  Lines        6649     6649         \n  Branches     1362     1362         \n=======================================\n  Hits         6366     6366         \n  Misses        283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2d0981a...bea13a3. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@d390fad). Click here to learn what that means.\nThe diff coverage is 95.06%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1806   +/-\n=========================================\n  Coverage          ?   95.75%         \n=========================================\n  Files             ?      192         \n  Lines             ?     6674         \n  Branches          ?     1364         \n=========================================\n  Hits              ?     6391         \n  Misses            ?      283         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/ecs_credentials.js | 100% <100%> (\u00f8) | |\n| lib/node_loader.js | 88.13% <100%> (\u00f8) | |\n| lib/credentials/remote_credentials.js | 94.73% <94.73%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d390fad...cca9f28. Read the comment docs.\n. # Codecov Report\nMerging #1807 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1807      +/-\n==========================================\n+ Coverage   95.75%   95.75%   +<.01%   \n==========================================\n  Files         191      191            \n  Lines        6663     6668       +5   \n  Branches     1362     1364       +2   \n==========================================\n+ Hits         6380     6385       +5   \n  Misses        283      283\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/protocol/rest_json.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4c8e625...29a3c87. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@d6bbbf9). Click here to learn what that means.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1808   +/-\n=========================================\n  Coverage          ?   95.75%         \n=========================================\n  Files             ?      191         \n  Lines             ?     6668         \n  Branches          ?     1364         \n=========================================\n  Hits              ?     6385         \n  Misses            ?      283         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/signers/presign.js | 100% <100%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d6bbbf9...f1516a2. Read the comment docs.\n. # Codecov Report\nMerging #1810 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1810   +/-\n=======================================\n  Coverage   95.75%   95.75%         \n=======================================\n  Files         191      191         \n  Lines        6668     6668         \n  Branches     1364     1364         \n=======================================\n  Hits         6385     6385         \n  Misses        283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 81fad87...8235ebc. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@dc6215a). Click here to learn what that means.\nThe diff coverage is 91.66%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1812   +/-\n=========================================\n  Coverage          ?   95.73%         \n=========================================\n  Files             ?      197         \n  Lines             ?     6737         \n  Branches          ?     1365         \n=========================================\n  Hits              ?     6450         \n  Misses            ?      287         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/node_loader.js | 85.24% <33.33%> (\u00f8) | |\n| lib/credentials/ecs_credentials.js | 71.42% <80%> (\u00f8) | |\n| lib/credentials/remote_credentials.js | 94.73% <94.73%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dc6215a...3e720fb. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@dc6215a). Click here to learn what that means.\nThe diff coverage is 94.93%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1813   +/-\n=========================================\n  Coverage          ?   95.79%         \n=========================================\n  Files             ?      197         \n  Lines             ?     6730         \n  Branches          ?     1364         \n=========================================\n  Hits              ?     6447         \n  Misses            ?      283         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/ecs_credentials.js | 100% <100%> (\u00f8) | |\n| lib/node_loader.js | 88.13% <100%> (\u00f8) | |\n| lib/credentials/remote_credentials.js | 94.73% <94.73%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dc6215a...5690863. Read the comment docs.\n. # Codecov Report\nMerging #1863 into master will decrease coverage by 0.06%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1863      +/-\n==========================================\n- Coverage   95.91%   95.85%   -0.07%   \n==========================================\n  Files         214      214            \n  Lines        6933     6127     -806   \n  Branches     1364        0    -1364   \n==========================================\n- Hits         6650     5873     -777   \n+ Misses        283      254      -29\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/iotdata.js | 76.92% <0%> (-4.33%) | :arrow_down: |\n| lib/config.js | 85.91% <0%> (-1.3%) | :arrow_down: |\n| lib/credentials/remote_credentials.js | 93.84% <0%> (-0.9%) | :arrow_down: |\n| lib/service.js | 95.52% <0%> (-0.87%) | :arrow_down: |\n| lib/credentials/environment_credentials.js | 90.47% <0%> (-0.83%) | :arrow_down: |\n| lib/credentials/temporary_credentials.js | 96% <0%> (-0.78%) | :arrow_down: |\n| lib/util.js | 93.61% <0%> (-0.72%) | :arrow_down: |\n| lib/model/shape.js | 94.18% <0%> (-0.69%) | :arrow_down: |\n| lib/metadata_service.js | 96% <0%> (-0.67%) | :arrow_down: |\n| lib/xml/node_parser.js | 92.77% <0%> (-0.64%) | :arrow_down: |\n| ... and 196 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 48c9f04...25bf175. Read the comment docs.\n. # Codecov Report\nMerging #1864 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1864   +/-\n=======================================\n  Coverage   95.91%   95.91%         \n=======================================\n  Files         214      214         \n  Lines        6933     6933         \n  Branches     1364     1364         \n=======================================\n  Hits         6650     6650         \n  Misses        283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 48c9f04...6489d1c. Read the comment docs.\n. # Codecov Report\nMerging #1866 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1866   +/-\n=======================================\n  Coverage   95.91%   95.91%         \n=======================================\n  Files         214      214         \n  Lines        6933     6933         \n  Branches     1364     1364         \n=======================================\n  Hits         6650     6650         \n  Misses        283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0e239c2...61a8831. Read the comment docs.\n. # Codecov Report\nMerging #1874 into master will increase coverage by 0.4%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1874     +/-\n=========================================\n+ Coverage   95.91%   96.32%   +0.4%   \n=========================================\n  Files         214      245     +31   \n  Lines        6933     8197   +1264   \n  Branches     1364     1719    +355   \n=========================================\n+ Hits         6650     7896   +1246   \n- Misses        283      301     +18\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event_listeners.js | 97.94% <\u00f8> (+0.47%) | :arrow_up: |\n| lib/protocol/rest_json.js | 96.66% <0%> (-3.34%) | :arrow_down: |\n| lib/protocol/rest_xml.js | 98.38% <0%> (-1.62%) | :arrow_down: |\n| lib/node_loader.js | 87.05% <0%> (-1.08%) | :arrow_down: |\n| lib/credentials/shared_ini_file_credentials.js | 97.93% <0%> (-0.67%) | :arrow_down: |\n| clients/mq.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/signers/v3https.js | 100% <0%> (\u00f8) | :arrow_up: |\n| clients/serverlessapplicationrepository.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/aws.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/model/resource_waiter.js | 100% <0%> (\u00f8) | :arrow_up: |\n| ... and 71 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8e2bae6...65766a9. Read the comment docs.\n. # Codecov Report\nMerging #1880 into master will increase coverage by 0.05%.\nThe diff coverage is 96.98%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1880      +/-\n==========================================\n+ Coverage   95.91%   95.97%   +0.05%   \n==========================================\n  Files         214      219       +5   \n  Lines        6933     7298     +365   \n  Branches     1364     1398      +34   \n==========================================\n+ Hits         6650     7004     +354   \n- Misses        283      294      +11\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/api_loader.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/dynamodb/set.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/browserHashUtils.js | 72.22% <72.22%> (\u00f8) | |\n| lib/browserHmac.js | 94.28% <94.28%> (\u00f8) | |\n| lib/browserSha256.js | 97.7% <97.7%> (\u00f8) | |\n| lib/browserSha1.js | 98.92% <98.92%> (\u00f8) | |\n| lib/browserMd5.js | 99.24% <99.24%> (\u00f8) | |\n| ... and 2 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8d63025...03c1019. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@77a2693). Click here to learn what that means.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1894   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      221         \n  Lines             ?     7325         \n  Branches          ?     1399         \n=========================================\n  Hits              ?     7031         \n  Misses            ?      294         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/util.js | 94.33% <\u00f8> (\u00f8) | |\n| lib/metadata_service.js | 96.96% <100%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 77a2693...1e0308b. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@2b6bcbd). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1897   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      221         \n  Lines             ?     7325         \n  Branches          ?     1399         \n=========================================\n  Hits              ?     7031         \n  Misses            ?      294         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/remote_credentials.js | 94.73% <\u00f8> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2b6bcbd...c219253. Read the comment docs.\n. # Codecov Report\nMerging #1900 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1900   +/-\n=======================================\n  Coverage   95.98%   95.98%         \n=======================================\n  Files         221      221         \n  Lines        7325     7325         \n  Branches     1399     1399         \n=======================================\n  Hits         7031     7031         \n  Misses        294      294\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1f52beb...6258f67. Read the comment docs.\n. # Codecov Report\nMerging #1905 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1905      +/-\n==========================================\n+ Coverage   95.98%   95.98%   +<.01%   \n==========================================\n  Files         221      221            \n  Lines        7325     7328       +3   \n  Branches     1399     1401       +2   \n==========================================\n+ Hits         7031     7034       +3   \n  Misses        294      294\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/shared_ini_file_credentials.js | 98.64% <100%> (+0.05%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1f52beb...bbf5b23. Read the comment docs.\n. # Codecov Report\nMerging #1923 into master will decrease coverage by 0.22%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1923      +/-\n==========================================\n- Coverage   96.85%   96.63%   -0.23%   \n==========================================\n  Files         281      255      -26   \n  Lines        8527     9029     +502   \n  Branches     1621     1947     +326   \n==========================================\n+ Hits         8259     8725     +466   \n- Misses        268      304      +36\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/shared_ini_file_credentials.js | 98.17% <100%> (+0.32%) | :arrow_up: |\n| lib/dynamodb/document_client.js | 63.56% <0%> (-30.56%) | :arrow_down: |\n| lib/credentials/remote_credentials.js | 94.73% <0%> (-2.37%) | :arrow_down: |\n| lib/model/operation.js | 81.96% <0%> (-1.11%) | :arrow_down: |\n| lib/sequential_executor.js | 99.21% <0%> (-0.79%) | :arrow_down: |\n| lib/signers/request_signer.js | 82.6% <0%> (-0.73%) | :arrow_down: |\n| lib/http/node.js | 91.39% <0%> (-0.36%) | :arrow_down: |\n| lib/credentials/temporary_credentials.js | 96.77% <0%> (-0.11%) | :arrow_down: |\n| lib/s3/managed_upload.js | 93.91% <0%> (-0.05%) | :arrow_down: |\n| lib/credentials/cognito_identity_credentials.js | 99.18% <0%> (-0.02%) | :arrow_down: |\n| ... and 99 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9403799...c8c24dd. Read the comment docs.\n. # Codecov Report\nMerging #1933 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1933      +/-\n==========================================\n+ Coverage   96.85%   96.85%   +<.01%   \n==========================================\n  Files         280      280            \n  Lines        8515     8517       +2   \n  Branches     1621     1622       +1   \n==========================================\n+ Hits         8247     8249       +2   \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 94% <100%> (+0.04%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9d1dbdf...63ef654. Read the comment docs.\n. # Codecov Report\nMerging #1956 into master will increase coverage by 0.06%.\nThe diff coverage is 98.02%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1956      +/-\n==========================================\n+ Coverage   95.97%   96.03%   +0.06%   \n==========================================\n  Files         221      224       +3   \n  Lines        7033     7144     +111   \n  Branches     1369     1398      +29   \n==========================================\n+ Hits         6750     6861     +111   \n  Misses        283      283\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/http/response-validator/attachValidators.js | 100% <100%> (\u00f8) | |\n| lib/services/s3.js | 98.21% <100%> (+0.07%) | :arrow_up: |\n| lib/request.js | 97.54% <100%> (+0.56%) | :arrow_up: |\n| lib/util.js | 94.36% <100%> (+0.03%) | :arrow_up: |\n| lib/http/node.js | 93.22% <100%> (+1.82%) | :arrow_up: |\n| lib/event_listeners.js | 97.53% <88.88%> (+0.06%) | :arrow_up: |\n| ...ib/http/response-validator/contentLengthChecker.js | 94.44% <94.44%> (\u00f8) | |\n| lib/http/response-validator/integrityChecker.js | 98.07% <98.07%> (\u00f8) | |\n| ... and 1 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4ea818e...6fa8c68. Read the comment docs.\n. # Codecov Report\nMerging #1958 into master will increase coverage by <.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1958      +/-\n==========================================\n+ Coverage   95.98%   95.98%   +<.01%   \n==========================================\n  Files         221      221            \n  Lines        7328     7329       +1   \n  Branches     1401     1401            \n==========================================\n+ Hits         7034     7035       +1   \n  Misses        294      294\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| clients/serverlessapplicationrepository.js | 100% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0fb21c0...4996384. Read the comment docs.\n. # Codecov Report\nMerging #1979 into master will increase coverage by <.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1979      +/-\n==========================================\n+ Coverage   95.98%   95.98%   +<.01%   \n==========================================\n  Files         221      221            \n  Lines        7328     7329       +1   \n  Branches     1401     1401            \n==========================================\n+ Hits         7034     7035       +1   \n  Misses        294      294\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| clients/serverlessapplicationrepository.js | 100% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a5a796c...5638735. Read the comment docs.\n. # Codecov Report\nMerging #1980 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1980   +/-\n=======================================\n  Coverage   95.98%   95.98%         \n=======================================\n  Files         221      221         \n  Lines        7329     7329         \n  Branches     1401     1401         \n=======================================\n  Hits         7035     7035         \n  Misses        294      294\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cd11ea3...bd756e3. Read the comment docs.\n. # Codecov Report\nMerging #1987 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1987   +/-\n=======================================\n  Coverage   95.98%   95.98%         \n=======================================\n  Files         221      221         \n  Lines        7329     7329         \n  Branches     1401     1401         \n=======================================\n  Hits         7035     7035         \n  Misses        294      294\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9c27fd7...644dcb4. Read the comment docs.\n. # Codecov Report\nMerging #1990 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1990      +/-\n==========================================\n+ Coverage   95.98%   95.98%   +<.01%   \n==========================================\n  Files         221      221            \n  Lines        7329     7331       +2   \n  Branches     1401     1402       +1   \n==========================================\n+ Hits         7035     7037       +2   \n  Misses        294      294\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/dynamodb/numberValue.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/dynamodb/types.js | 95.65% <100%> (\u00f8) | :arrow_up: |\n| lib/dynamodb/set.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5349bc8...5604ea9. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@d32e926). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1992   +/-\n=========================================\n  Coverage          ?   95.99%         \n=========================================\n  Files             ?      222         \n  Lines             ?     7343         \n  Branches          ?     1402         \n=========================================\n  Hits              ?     7049         \n  Misses            ?      294         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d32e926...9872b5f. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@d32e926). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #1992   +/-\n=========================================\n  Coverage          ?   95.99%         \n=========================================\n  Files             ?      222         \n  Lines             ?     7343         \n  Branches          ?     1402         \n=========================================\n  Hits              ?     7049         \n  Misses            ?      294         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d32e926...9872b5f. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@214b424). Click here to learn what that means.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2014   +/-\n=========================================\n  Coverage          ?   95.96%         \n=========================================\n  Files             ?      220         \n  Lines             ?     7021         \n  Branches          ?     1369         \n=========================================\n  Hits              ?     6738         \n  Misses            ?      283         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/shared_ini_file_credentials.js | 98.66% <100%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 214b424...e87eda3. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@214b424). Click here to learn what that means.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2014   +/-\n=========================================\n  Coverage          ?   95.96%         \n=========================================\n  Files             ?      220         \n  Lines             ?     7021         \n  Branches          ?     1369         \n=========================================\n  Hits              ?     6738         \n  Misses            ?      283         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/shared_ini_file_credentials.js | 98.66% <100%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 214b424...e87eda3. Read the comment docs.\n. # Codecov Report\nMerging #2022 into master will increase coverage by 0.73%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2022      +/-\n=========================================\n+ Coverage   95.96%   96.7%   +0.73%   \n=========================================\n  Files         220     282      +62   \n  Lines        7021    8638    +1617   \n  Branches     1369    1642     +273   \n=========================================\n+ Hits         6738    8353    +1615   \n- Misses        283     285       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| clients/clouddirectory.js | 76.47% <0%> (-23.53%) | :arrow_down: |\n| lib/protocol/rest_json.js | 94.73% <0%> (-5.27%) | :arrow_down: |\n| lib/protocol/rest_xml.js | 94.91% <0%> (-5.09%) | :arrow_down: |\n| lib/credentials.js | 96.07% <0%> (-3.93%) | :arrow_down: |\n| lib/event_listeners.js | 96.39% <0%> (-1.08%) | :arrow_down: |\n| lib/credentials/shared_ini_file_credentials.js | 97.84% <0%> (-0.82%) | :arrow_down: |\n| lib/param_validator.js | 96.52% <0%> (-0.58%) | :arrow_down: |\n| lib/api_loader.js | 100% <0%> (\u00f8) | :arrow_up: |\n| clients/acmpca.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/model/collection.js | 100% <0%> (\u00f8) | :arrow_up: |\n| ... and 120 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6863343...37e1c06. Read the comment docs.\n. # Codecov Report\nMerging #2022 into master will increase coverage by 0.73%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2022      +/-\n=========================================\n+ Coverage   95.96%   96.7%   +0.73%   \n=========================================\n  Files         220     282      +62   \n  Lines        7021    8638    +1617   \n  Branches     1369    1642     +273   \n=========================================\n+ Hits         6738    8353    +1615   \n- Misses        283     285       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| clients/clouddirectory.js | 76.47% <0%> (-23.53%) | :arrow_down: |\n| lib/protocol/rest_json.js | 94.73% <0%> (-5.27%) | :arrow_down: |\n| lib/protocol/rest_xml.js | 94.91% <0%> (-5.09%) | :arrow_down: |\n| lib/credentials.js | 96.07% <0%> (-3.93%) | :arrow_down: |\n| lib/event_listeners.js | 96.39% <0%> (-1.08%) | :arrow_down: |\n| lib/credentials/shared_ini_file_credentials.js | 97.84% <0%> (-0.82%) | :arrow_down: |\n| lib/param_validator.js | 96.52% <0%> (-0.58%) | :arrow_down: |\n| lib/api_loader.js | 100% <0%> (\u00f8) | :arrow_up: |\n| clients/acmpca.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/model/collection.js | 100% <0%> (\u00f8) | :arrow_up: |\n| ... and 120 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6863343...37e1c06. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@3e9b8ee). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2026   +/-\n=========================================\n  Coverage          ?   95.97%         \n=========================================\n  Files             ?      221         \n  Lines             ?     7033         \n  Branches          ?     1369         \n=========================================\n  Hits              ?     6750         \n  Misses            ?      283         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3e9b8ee...73484d1. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@3e9b8ee). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2026   +/-\n=========================================\n  Coverage          ?   95.97%         \n=========================================\n  Files             ?      221         \n  Lines             ?     7033         \n  Branches          ?     1369         \n=========================================\n  Hits              ?     6750         \n  Misses            ?      283         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3e9b8ee...73484d1. Read the comment docs.\n. # Codecov Report\nMerging #2035 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2035   +/-\n=======================================\n  Coverage   95.97%   95.97%         \n=======================================\n  Files         221      221         \n  Lines        7033     7033         \n  Branches     1369     1369         \n=======================================\n  Hits         6750     6750         \n  Misses        283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4ea818e...1d995b0. Read the comment docs.\n. # Codecov Report\nMerging #2035 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2035   +/-\n=======================================\n  Coverage   95.97%   95.97%         \n=======================================\n  Files         221      221         \n  Lines        7033     7033         \n  Branches     1369     1369         \n=======================================\n  Hits         6750     6750         \n  Misses        283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4ea818e...1d995b0. Read the comment docs.\n. # Codecov Report\nMerging #2045 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2045      +/-\n=========================================\n+ Coverage   95.97%     96%   +0.02%   \n=========================================\n  Files         221     225       +4   \n  Lines        7033    7080      +47   \n  Branches     1369    1372       +3   \n=========================================\n+ Hits         6750    6797      +47   \n  Misses        283     283\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/xml/escape-element.js | 100% <100%> (\u00f8) | |\n| lib/xml/xml-text.js | 100% <100%> (\u00f8) | |\n| lib/xml/builder.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/xml/escape-attribute.js | 100% <100%> (\u00f8) | |\n| lib/xml/xml-node.js | 100% <100%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2b84fb5...c61523a. Read the comment docs.\n. # Codecov Report\nMerging #2045 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2045      +/-\n=========================================\n+ Coverage   95.97%     96%   +0.02%   \n=========================================\n  Files         221     225       +4   \n  Lines        7033    7080      +47   \n  Branches     1369    1372       +3   \n=========================================\n+ Hits         6750    6797      +47   \n  Misses        283     283\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/xml/escape-element.js | 100% <100%> (\u00f8) | |\n| lib/xml/xml-text.js | 100% <100%> (\u00f8) | |\n| lib/xml/builder.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/xml/escape-attribute.js | 100% <100%> (\u00f8) | |\n| lib/xml/xml-node.js | 100% <100%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2b84fb5...c61523a. Read the comment docs.\n. # Codecov Report\nMerging #2050 into master will decrease coverage by 0.02%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2050      +/-\n==========================================\n- Coverage      96%   95.97%   -0.03%   \n==========================================\n  Files         225      221       -4   \n  Lines        7080     7033      -47   \n  Branches     1372     1369       -3   \n==========================================\n- Hits         6797     6750      -47   \n  Misses        283      283\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/model/api.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/signers/v4_credentials.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/aws.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/service.js | 96.38% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/signers/presign.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/metadata_service.js | 96.96% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/dynamodb/types.js | 95.65% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/cloudfront/signer.js | 78.08% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/model/resource_waiter.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/protocol/rest_json.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| ... and 36 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update eef6e5c...e735d35. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@c4c3286). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2051   +/-\n=========================================\n  Coverage          ?   96.01%         \n=========================================\n  Files             ?      227         \n  Lines             ?     7103         \n  Branches          ?     1372         \n=========================================\n  Hits              ?     6820         \n  Misses            ?      283         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c4c3286...271723d. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (wip-s3-select@e2723e1). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@               Coverage Diff                @@\nwip-s3-select    #2054   +/-\n================================================\n  Coverage                 ?   96.01%         \n================================================\n  Files                    ?      227         \n  Lines                    ?     7103         \n  Branches                 ?     1372         \n================================================\n  Hits                     ?     6820         \n  Misses                   ?      283         \n  Partials                 ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e2723e1...35183f1. Read the comment docs.\n. # Codecov Report\nMerging #2056 into wip-s3-select will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@              Coverage Diff               @@\nwip-s3-select    #2056   +/-\n==============================================\n  Coverage          96.01%   96.01%         \n==============================================\n  Files                227      227         \n  Lines               7103     7103         \n  Branches            1372     1372         \n==============================================\n  Hits                6820     6820         \n  Misses               283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b702e4a...3eb917e. Read the comment docs.\n. # Codecov Report\nMerging #2056 into wip-s3-select will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@              Coverage Diff               @@\nwip-s3-select    #2056   +/-\n==============================================\n  Coverage          96.01%   96.01%         \n==============================================\n  Files                227      227         \n  Lines               7103     7103         \n  Branches            1372     1372         \n==============================================\n  Hits                6820     6820         \n  Misses               283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b702e4a...3eb917e. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@43f996d). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2061   +/-\n=========================================\n  Coverage          ?   96.01%         \n=========================================\n  Files             ?      227         \n  Lines             ?     7103         \n  Branches          ?     1372         \n=========================================\n  Hits              ?     6820         \n  Misses            ?      283         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 43f996d...2ad6727. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@43f996d). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2061   +/-\n=========================================\n  Coverage          ?   96.01%         \n=========================================\n  Files             ?      227         \n  Lines             ?     7103         \n  Branches          ?     1372         \n=========================================\n  Hits              ?     6820         \n  Misses            ?      283         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 43f996d...2ad6727. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@9c32604). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2066   +/-\n=========================================\n  Coverage          ?   96.02%         \n=========================================\n  Files             ?      228         \n  Lines             ?     7115         \n  Branches          ?     1372         \n=========================================\n  Hits              ?     6832         \n  Misses            ?      283         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9c32604...6ce1781. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@9c32604). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2066   +/-\n=========================================\n  Coverage          ?   96.02%         \n=========================================\n  Files             ?      228         \n  Lines             ?     7115         \n  Branches          ?     1372         \n=========================================\n  Hits              ?     6832         \n  Misses            ?      283         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9c32604...6ce1781. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@83094bb). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2067   +/-\n=========================================\n  Coverage          ?   96.02%         \n=========================================\n  Files             ?      229         \n  Lines             ?     7128         \n  Branches          ?     1372         \n=========================================\n  Hits              ?     6845         \n  Misses            ?      283         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 83094bb...59c33e2. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@83094bb). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2067   +/-\n=========================================\n  Coverage          ?   96.02%         \n=========================================\n  Files             ?      229         \n  Lines             ?     7128         \n  Branches          ?     1372         \n=========================================\n  Hits              ?     6845         \n  Misses            ?      283         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 83094bb...59c33e2. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (wip-s3-select@7ef247f). Click here to learn what that means.\nThe diff coverage is 93.27%.\n\n\n```diff\n@@               Coverage Diff                @@\nwip-s3-select    #2074   +/-\n================================================\n  Coverage                 ?   95.98%         \n================================================\n  Files                    ?      239         \n  Lines                    ?     7513         \n  Branches                 ?     1442         \n================================================\n  Hits                     ?     7211         \n  Misses                   ?      302         \n  Partials                 ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event-stream/to-buffer.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/int64.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/buffered-create-event-stream.js | 100% <100%> (\u00f8) | |\n| lib/model/shape.js | 95.34% <100%> (\u00f8) | |\n| lib/event-stream/parse-message.js | 97.05% <100%> (\u00f8) | |\n| lib/util.js | 94.34% <100%> (\u00f8) | |\n| .../event-stream/event-message-unmarshaller-stream.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/event-message-chunker-stream.js | 97.82% <100%> (\u00f8) | |\n| lib/event-stream/streaming-create-event-stream.js | 100% <100%> (\u00f8) | |\n| lib/node_loader.js | 88.52% <100%> (\u00f8) | |\n| ... and 7 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ef247f...d060db2. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (wip-s3-select@7ef247f). Click here to learn what that means.\nThe diff coverage is 93.27%.\n\n\n```diff\n@@               Coverage Diff                @@\nwip-s3-select    #2074   +/-\n================================================\n  Coverage                 ?   95.98%         \n================================================\n  Files                    ?      239         \n  Lines                    ?     7513         \n  Branches                 ?     1442         \n================================================\n  Hits                     ?     7211         \n  Misses                   ?      302         \n  Partials                 ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event-stream/to-buffer.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/int64.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/buffered-create-event-stream.js | 100% <100%> (\u00f8) | |\n| lib/model/shape.js | 95.34% <100%> (\u00f8) | |\n| lib/event-stream/parse-message.js | 97.05% <100%> (\u00f8) | |\n| lib/util.js | 94.34% <100%> (\u00f8) | |\n| .../event-stream/event-message-unmarshaller-stream.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/event-message-chunker-stream.js | 97.82% <100%> (\u00f8) | |\n| lib/event-stream/streaming-create-event-stream.js | 100% <100%> (\u00f8) | |\n| lib/node_loader.js | 88.52% <100%> (\u00f8) | |\n| ... and 7 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ef247f...d060db2. Read the comment docs.\n. # Codecov Report\nMerging #2079 into wip-s3-select will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@              Coverage Diff               @@\nwip-s3-select    #2079   +/-\n==============================================\n  Coverage          96.01%   96.01%         \n==============================================\n  Files                227      227         \n  Lines               7103     7103         \n  Branches            1372     1372         \n==============================================\n  Hits                6820     6820         \n  Misses               283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ef247f...04c7166. Read the comment docs.\n. # Codecov Report\nMerging #2079 into wip-s3-select will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@              Coverage Diff               @@\nwip-s3-select    #2079   +/-\n==============================================\n  Coverage          96.01%   96.01%         \n==============================================\n  Files                227      227         \n  Lines               7103     7103         \n  Branches            1372     1372         \n==============================================\n  Hits                6820     6820         \n  Misses               283      283\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ef247f...04c7166. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (wip-s3-select@7ef247f). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@               Coverage Diff                @@\nwip-s3-select    #2080   +/-\n================================================\n  Coverage                 ?   95.98%         \n================================================\n  Files                    ?      239         \n  Lines                    ?     7513         \n  Branches                 ?     1442         \n================================================\n  Hits                     ?     7211         \n  Misses                   ?      302         \n  Partials                 ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ef247f...752b172. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (wip-s3-select@7ef247f). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@               Coverage Diff                @@\nwip-s3-select    #2080   +/-\n================================================\n  Coverage                 ?   95.98%         \n================================================\n  Files                    ?      239         \n  Lines                    ?     7513         \n  Branches                 ?     1442         \n================================================\n  Hits                     ?     7211         \n  Misses                   ?      302         \n  Partials                 ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ef247f...752b172. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@0a7112b). Click here to learn what that means.\nThe diff coverage is 95.42%.\n\n\n```diff\n@@           Coverage Diff            @@\nmaster   #2085   +/-\n========================================\n  Coverage          ?     96%         \n========================================\n  Files             ?     243         \n  Lines             ?    7562         \n  Branches          ?    1442         \n========================================\n  Hits              ?    7260         \n  Misses            ?     302         \n  Partials          ?       0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event-stream/split-message.js | 100% <100%> (\u00f8) | |\n| .../event-stream/event-message-unmarshaller-stream.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/streaming-create-event-stream.js | 100% <100%> (\u00f8) | |\n| lib/node_loader.js | 88.52% <100%> (\u00f8) | |\n| lib/event-stream/to-buffer.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/buffered-create-event-stream.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/event-message-chunker.js | 100% <100%> (\u00f8) | |\n| lib/model/shape.js | 95.34% <100%> (\u00f8) | |\n| lib/event-stream/int64.js | 100% <100%> (\u00f8) | |\n| lib/util.js | 94.34% <100%> (\u00f8) | |\n| ... and 9 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0a7112b...90853a1. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@0a7112b). Click here to learn what that means.\nThe diff coverage is 95.42%.\n\n\n```diff\n@@           Coverage Diff            @@\nmaster   #2085   +/-\n========================================\n  Coverage          ?     96%         \n========================================\n  Files             ?     243         \n  Lines             ?    7562         \n  Branches          ?    1442         \n========================================\n  Hits              ?    7260         \n  Misses            ?     302         \n  Partials          ?       0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event-stream/split-message.js | 100% <100%> (\u00f8) | |\n| .../event-stream/event-message-unmarshaller-stream.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/streaming-create-event-stream.js | 100% <100%> (\u00f8) | |\n| lib/node_loader.js | 88.52% <100%> (\u00f8) | |\n| lib/event-stream/to-buffer.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/buffered-create-event-stream.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/event-message-chunker.js | 100% <100%> (\u00f8) | |\n| lib/model/shape.js | 95.34% <100%> (\u00f8) | |\n| lib/event-stream/int64.js | 100% <100%> (\u00f8) | |\n| lib/util.js | 94.34% <100%> (\u00f8) | |\n| ... and 9 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0a7112b...90853a1. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@9c15bf9). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff            @@\nmaster   #2086   +/-\n========================================\n  Coverage          ?     96%         \n========================================\n  Files             ?     243         \n  Lines             ?    7562         \n  Branches          ?    1442         \n========================================\n  Hits              ?    7260         \n  Misses            ?     302         \n  Partials          ?       0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9c15bf9...97ce5e9. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@9c15bf9). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff            @@\nmaster   #2086   +/-\n========================================\n  Coverage          ?     96%         \n========================================\n  Files             ?     243         \n  Lines             ?    7562         \n  Branches          ?    1442         \n========================================\n  Hits              ?    7260         \n  Misses            ?     302         \n  Partials          ?       0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9c15bf9...97ce5e9. Read the comment docs.\n. # Codecov Report\nMerging #2089 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2089      +/-\n==========================================\n+ Coverage      96%   96.03%   +0.02%   \n==========================================\n  Files         243      243            \n  Lines        7562     7565       +3   \n  Branches     1442     1442            \n==========================================\n+ Hits         7260     7265       +5   \n+ Misses        302      300       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event_listeners.js | 96.36% <100%> (+0.03%) | :arrow_up: |\n| lib/service.js | 97.18% <0%> (+0.8%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3cedc60...50338ac. Read the comment docs.\n. # Codecov Report\nMerging #2089 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2089      +/-\n==========================================\n+ Coverage      96%   96.03%   +0.02%   \n==========================================\n  Files         243      243            \n  Lines        7562     7565       +3   \n  Branches     1442     1442            \n==========================================\n+ Hits         7260     7265       +5   \n+ Misses        302      300       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event_listeners.js | 96.36% <100%> (+0.03%) | :arrow_up: |\n| lib/service.js | 97.18% <0%> (+0.8%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3cedc60...50338ac. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@baff9c7). Click here to learn what that means.\nThe diff coverage is 96.15%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2090   +/-\n=========================================\n  Coverage          ?   96.02%         \n=========================================\n  Files             ?      243         \n  Lines             ?     7574         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7273         \n  Misses            ?      301         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event-stream/build-message.js | 98.88% <\u00f8> (\u00f8) | |\n| lib/event-stream/streaming-create-event-stream.js | 100% <100%> (\u00f8) | |\n| .../event-stream/event-message-unmarshaller-stream.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/parse-event.js | 94.28% <100%> (\u00f8) | |\n| lib/event-stream/event-message-chunker-stream.js | 95.91% <93.75%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update baff9c7...10d611f. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@baff9c7). Click here to learn what that means.\nThe diff coverage is 96.15%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2090   +/-\n=========================================\n  Coverage          ?   96.02%         \n=========================================\n  Files             ?      243         \n  Lines             ?     7574         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7273         \n  Misses            ?      301         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event-stream/build-message.js | 98.88% <\u00f8> (\u00f8) | |\n| lib/event-stream/streaming-create-event-stream.js | 100% <100%> (\u00f8) | |\n| .../event-stream/event-message-unmarshaller-stream.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/parse-event.js | 94.28% <100%> (\u00f8) | |\n| lib/event-stream/event-message-chunker-stream.js | 95.91% <93.75%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update baff9c7...10d611f. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@81babe8). Click here to learn what that means.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2096   +/-\n=========================================\n  Coverage          ?   96.02%         \n=========================================\n  Files             ?      243         \n  Lines             ?     7575         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7274         \n  Misses            ?      301         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event-stream/split-message.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/int64.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/to-buffer.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/build-message.js | 98.9% <100%> (\u00f8) | |\n| lib/event-stream/alloc-buffer.js | 87.5% <100%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 81babe8...1fa0d0d. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@81babe8). Click here to learn what that means.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2096   +/-\n=========================================\n  Coverage          ?   96.02%         \n=========================================\n  Files             ?      243         \n  Lines             ?     7575         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7274         \n  Misses            ?      301         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event-stream/split-message.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/int64.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/to-buffer.js | 100% <100%> (\u00f8) | |\n| lib/event-stream/build-message.js | 98.9% <100%> (\u00f8) | |\n| lib/event-stream/alloc-buffer.js | 87.5% <100%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 81babe8...1fa0d0d. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@81babe8). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2097   +/-\n=========================================\n  Coverage          ?   95.99%         \n=========================================\n  Files             ?      245         \n  Lines             ?     7634         \n  Branches          ?     1450         \n=========================================\n  Hits              ?     7328         \n  Misses            ?      306         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 81babe8...5f1cc9e. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@81babe8). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2097   +/-\n=========================================\n  Coverage          ?   95.99%         \n=========================================\n  Files             ?      245         \n  Lines             ?     7634         \n  Branches          ?     1450         \n=========================================\n  Hits              ?     7328         \n  Misses            ?      306         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 81babe8...5f1cc9e. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@81babe8). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2098   +/-\n=========================================\n  Coverage          ?   96.02%         \n=========================================\n  Files             ?      243         \n  Lines             ?     7574         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7273         \n  Misses            ?      301         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 81babe8...7f329bf. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@81babe8). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2098   +/-\n=========================================\n  Coverage          ?   96.02%         \n=========================================\n  Files             ?      243         \n  Lines             ?     7574         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7273         \n  Misses            ?      301         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 81babe8...7f329bf. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@e5e5b28). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2116   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      244         \n  Lines             ?     7592         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7287         \n  Misses            ?      305         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e5e5b28...beb83c6. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@e5e5b28). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2116   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      244         \n  Lines             ?     7592         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7287         \n  Misses            ?      305         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e5e5b28...beb83c6. Read the comment docs.\n. # Codecov Report\nMerging #2124 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2124   +/-\n=======================================\n  Coverage   95.98%   95.98%         \n=======================================\n  Files         244      244         \n  Lines        7592     7592         \n  Branches     1443     1443         \n=======================================\n  Hits         7287     7287         \n  Misses        305      305\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b8a4ee7...f066c1e. Read the comment docs.\n. # Codecov Report\nMerging #2124 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2124   +/-\n=======================================\n  Coverage   95.98%   95.98%         \n=======================================\n  Files         244      244         \n  Lines        7592     7592         \n  Branches     1443     1443         \n=======================================\n  Hits         7287     7287         \n  Misses        305      305\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b8a4ee7...f066c1e. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@f5a9d42). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2125   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      244         \n  Lines             ?     7593         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7288         \n  Misses            ?      305         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f5a9d42...b8cbf16. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@f5a9d42). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2125   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      244         \n  Lines             ?     7593         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7288         \n  Misses            ?      305         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f5a9d42...b8cbf16. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@a6b3ff2). Click here to learn what that means.\nThe diff coverage is 96.77%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2126   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      245         \n  Lines             ?     7630         \n  Branches          ?     1448         \n=========================================\n  Hits              ?     7324         \n  Misses            ?      306         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/shared_ini_file_credentials.js | 98% <96.77%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a6b3ff2...4f772c8. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@a6b3ff2). Click here to learn what that means.\nThe diff coverage is 96.77%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2126   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      245         \n  Lines             ?     7630         \n  Branches          ?     1448         \n=========================================\n  Hits              ?     7324         \n  Misses            ?      306         \n  Partials          ?        0\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/shared_ini_file_credentials.js | 98% <96.77%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a6b3ff2...4f772c8. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@279db76). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2137   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      245         \n  Lines             ?     7605         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7300         \n  Misses            ?      305         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 279db76...8f58915. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@279db76). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2137   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      245         \n  Lines             ?     7605         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7300         \n  Misses            ?      305         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 279db76...8f58915. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@279db76). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2138   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      245         \n  Lines             ?     7605         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7300         \n  Misses            ?      305         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 279db76...35beac7. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@279db76). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2138   +/-\n=========================================\n  Coverage          ?   95.98%         \n=========================================\n  Files             ?      245         \n  Lines             ?     7605         \n  Branches          ?     1443         \n=========================================\n  Hits              ?     7300         \n  Misses            ?      305         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 279db76...35beac7. Read the comment docs.\n. # Codecov Report\nMerging #2140 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2140   +/-\n=======================================\n  Coverage   95.98%   95.98%         \n=======================================\n  Files         245      245         \n  Lines        7605     7605         \n  Branches     1443     1443         \n=======================================\n  Hits         7300     7300         \n  Misses        305      305\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d6573a8...c63b521. Read the comment docs.\n. # Codecov Report\nMerging #2140 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2140   +/-\n=======================================\n  Coverage   95.98%   95.98%         \n=======================================\n  Files         245      245         \n  Lines        7605     7605         \n  Branches     1443     1443         \n=======================================\n  Hits         7300     7300         \n  Misses        305      305\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d6573a8...c63b521. Read the comment docs.\n. # Codecov Report\nMerging #2142 into master will increase coverage by 0.1%.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff            @@\nmaster   #2142     +/-\n========================================\n+ Coverage   95.99%   96.1%   +0.1%   \n========================================\n  Files         245     250      +5   \n  Lines        7644    7852    +208   \n  Branches     1452    1506     +54   \n========================================\n+ Hits         7338    7546    +208   \n  Misses        306     306\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/publisher/configuration.js | 100% <100%> (\u00f8) | |\n| lib/core.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/node_loader.js | 88.88% <100%> (+0.36%) | :arrow_up: |\n| lib/request.js | 96.99% <100%> (+0.01%) | :arrow_up: |\n| lib/model/api.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/publisher/index.js | 100% <100%> (\u00f8) | |\n| lib/sequential_executor.js | 98.8% <100%> (+0.02%) | :arrow_up: |\n| lib/publisher/string-to-buffer.js | 100% <100%> (\u00f8) | |\n| lib/realclock/nodeClock.js | 100% <100%> (\u00f8) | |\n| ... and 6 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6777045...816c85a. Read the comment docs.\n. # Codecov Report\nMerging #2143 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2143   +/-\n=======================================\n  Coverage   95.98%   95.98%         \n=======================================\n  Files         245      245         \n  Lines        7605     7605         \n  Branches     1443     1443         \n=======================================\n  Hits         7300     7300         \n  Misses        305      305\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3601f8b...37b4679. Read the comment docs.\n. # Codecov Report\nMerging #2144 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2144   +/-\n=======================================\n  Coverage   95.98%   95.98%         \n=======================================\n  Files         245      245         \n  Lines        7630     7630         \n  Branches     1448     1448         \n=======================================\n  Hits         7324     7324         \n  Misses        306      306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4869c87...1471769. Read the comment docs.\n. # Codecov Report\nMerging #2148 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2148      +/-\n==========================================\n+ Coverage   95.98%   95.99%   +<.01%   \n==========================================\n  Files         245      245            \n  Lines        7630     7633       +3   \n  Branches     1448     1450       +2   \n==========================================\n+ Hits         7324     7327       +3   \n  Misses        306      306\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/model/shape.js | 95.41% <100%> (+0.06%) | :arrow_up: |\n| lib/protocol/rest.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 316d6b0...98b1d82. Read the comment docs.\n. # Codecov Report\nMerging #2155 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2155   +/-\n=======================================\n  Coverage   95.98%   95.98%         \n=======================================\n  Files         245      245         \n  Lines        7630     7630         \n  Branches     1448     1448         \n=======================================\n  Hits         7324     7324         \n  Misses        306      306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f16d340...45b46fd. Read the comment docs.\n. # Codecov Report\nMerging #2172 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2172      +/-\n==========================================\n+ Coverage   95.99%   95.99%   +<.01%   \n==========================================\n  Files         245      245            \n  Lines        7634     7635       +1   \n  Branches     1450     1450            \n==========================================\n+ Hits         7328     7329       +1   \n  Misses        306      306\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/util.js | 94.35% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ebf03c7...444d7c0. Read the comment docs.\n. # Codecov Report\nMerging #2175 into master will increase coverage by 0.02%.\nThe diff coverage is 98.18%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2175      +/-\n==========================================\n+ Coverage   96.32%   96.34%   +0.02%   \n==========================================\n  Files         275      276       +1   \n  Lines        8407     8462      +55   \n  Branches     1599     1614      +15   \n==========================================\n+ Hits         8098     8153      +55   \n  Misses        309      309\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/chainable_temporary_credentials.js | 100% <100%> (\u00f8) | |\n| lib/credentials/environment_credentials.js | 91.3% <100%> (\u00f8) | :arrow_up: |\n| lib/util.js | 94.34% <100%> (+0.01%) | :arrow_up: |\n| lib/node_loader.js | 89.55% <100%> (+0.15%) | :arrow_up: |\n| lib/credentials/cognito_identity_credentials.js | 99.2% <100%> (+0.01%) | :arrow_up: |\n| lib/credentials/file_system_credentials.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/credentials/remote_credentials.js | 97.1% <100%> (+2.36%) | :arrow_up: |\n| lib/credentials/ec2_metadata_credentials.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/credentials/temporary_credentials.js | 96.87% <100%> (+0.1%) | :arrow_up: |\n| lib/credentials/web_identity_credentials.js | 100% <100%> (\u00f8) | :arrow_up: |\n| ... and 4 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b13fb02...a9300d8. Read the comment docs.\n. # Codecov Report\nMerging #2176 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2176   +/-\n=======================================\n  Coverage   95.99%   95.99%         \n=======================================\n  Files         245      245         \n  Lines        7635     7635         \n  Branches     1450     1450         \n=======================================\n  Hits         7329     7329         \n  Misses        306      306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5aa0d4d...be126a8. Read the comment docs.\n. # Codecov Report\nMerging #2189 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2189   +/-\n=======================================\n  Coverage   95.99%   95.99%         \n=======================================\n  Files         245      245         \n  Lines        7635     7635         \n  Branches     1450     1450         \n=======================================\n  Hits         7329     7329         \n  Misses        306      306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0e4934a...ac1f681. Read the comment docs.\n. # Codecov Report\nMerging #2191 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2191   +/-\n=======================================\n  Coverage   95.99%   95.99%         \n=======================================\n  Files         245      245         \n  Lines        7635     7635         \n  Branches     1450     1450         \n=======================================\n  Hits         7329     7329         \n  Misses        306      306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c323426...ef20526. Read the comment docs.\n. # Codecov Report\nMerging #2198 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2198      +/-\n==========================================\n+ Coverage   95.99%   95.99%   +<.01%   \n==========================================\n  Files         245      245            \n  Lines        7635     7636       +1   \n  Branches     1450     1450            \n==========================================\n+ Hits         7329     7330       +1   \n  Misses        306      306\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/shared_ini_file_credentials.js | 98.01% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 11da426...ddfd8de. Read the comment docs.\n. # Codecov Report\nMerging #2199 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2199      +/-\n==========================================\n+ Coverage   95.99%   95.99%   +<.01%   \n==========================================\n  Files         245      245            \n  Lines        7641     7644       +3   \n  Branches     1450     1452       +2   \n==========================================\n+ Hits         7335     7338       +3   \n  Misses        306      306\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.14% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f7728f4...62b1594. Read the comment docs.\n. # Codecov Report\nMerging #2201 into master will decrease coverage by 0.07%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2201      +/-\n==========================================\n- Coverage   95.99%   95.91%   -0.08%   \n==========================================\n  Files         245      245            \n  Lines        7641     7642       +1   \n  Branches     1450     1451       +1   \n==========================================\n- Hits         7335     7330       -5   \n- Misses        306      312       +6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/param_validator.js | 92.8% <100%> (-4.3%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 462e8d5...2bf30be. Read the comment docs.\n. # Codecov Report\nMerging #2206 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2206      +/-\n==========================================\n+ Coverage   95.99%   95.99%   +<.01%   \n==========================================\n  Files         245      245            \n  Lines        7641     7642       +1   \n  Branches     1450     1450            \n==========================================\n+ Hits         7335     7336       +1   \n  Misses        306      306\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event_listeners.js | 96.37% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 462e8d5...6b25199. Read the comment docs.\n. # Codecov Report\nMerging #2207 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2207   +/-\n=======================================\n  Coverage   95.99%   95.99%         \n=======================================\n  Files         245      245         \n  Lines        7641     7641         \n  Branches     1450     1450         \n=======================================\n  Hits         7335     7335         \n  Misses        306      306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 462e8d5...7cb4df5. Read the comment docs.\n. # Codecov Report\nMerging #2209 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2209   +/-\n=======================================\n  Coverage   95.99%   95.99%         \n=======================================\n  Files         245      245         \n  Lines        7641     7641         \n  Branches     1450     1450         \n=======================================\n  Hits         7335     7335         \n  Misses        306      306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7924728...a718183. Read the comment docs.\n. # Codecov Report\nMerging #2214 into master will decrease coverage by 64.87%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster    #2214       +/-\n===========================================\n- Coverage   95.99%   31.12%   -64.88%   \n===========================================\n  Files         245      242        -3   \n  Lines        7644     7534      -110   \n  Branches     1452     1441       -11   \n===========================================\n- Hits         7338     2345     -4993   \n- Misses        306     5189     +4883\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/sqs.js | 3.38% <0%> (-96.62%) | :arrow_down: |\n| lib/services/s3.js | 1.62% <0%> (-96.52%) | :arrow_down: |\n| lib/param_validator.js | 1.44% <0%> (-95.66%) | :arrow_down: |\n| lib/signers/v4.js | 5.5% <0%> (-94.5%) | :arrow_down: |\n| lib/signers/s3.js | 6.06% <0%> (-93.94%) | :arrow_down: |\n| lib/services/cloudsearchdomain.js | 6.66% <0%> (-93.34%) | :arrow_down: |\n| lib/services/glacier.js | 4.54% <0%> (-93.19%) | :arrow_down: |\n| lib/services/ec2.js | 6.89% <0%> (-93.11%) | :arrow_down: |\n| lib/polly/presigner.js | 6.97% <0%> (-93.03%) | :arrow_down: |\n| lib/credentials/remote_credentials.js | 3.94% <0%> (-90.79%) | :arrow_down: |\n| ... and 227 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bbbbf8e...4608d7c. Read the comment docs.\n. # Codecov Report\nMerging #2215 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2215   +/-\n=======================================\n  Coverage   95.99%   95.99%         \n=======================================\n  Files         245      245         \n  Lines        7644     7644         \n  Branches     1452     1452         \n=======================================\n  Hits         7338     7338         \n  Misses        306      306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4cf44bb...e4504d4. Read the comment docs.\n. # Codecov Report\nMerging #2216 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2216   +/-\n=======================================\n  Coverage   95.99%   95.99%         \n=======================================\n  Files         245      245         \n  Lines        7644     7644         \n  Branches     1452     1452         \n=======================================\n  Hits         7338     7338         \n  Misses        306      306\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/service.js | 97.18% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 303da61...d6926a3. Read the comment docs.\n. # Codecov Report\nMerging #2218 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2218   +/-\n=======================================\n  Coverage   95.99%   95.99%         \n=======================================\n  Files         245      245         \n  Lines        7644     7644         \n  Branches     1452     1452         \n=======================================\n  Hits         7338     7338         \n  Misses        306      306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 303da61...547145c. Read the comment docs.\n. # Codecov Report\nMerging #2223 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2223   +/-\n=======================================\n  Coverage   95.99%   95.99%         \n=======================================\n  Files         245      245         \n  Lines        7644     7644         \n  Branches     1452     1452         \n=======================================\n  Hits         7338     7338         \n  Misses        306      306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 53916e8...55ea3fa. Read the comment docs.\n. # Codecov Report\nMerging #2225 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #2225   +/-\n======================================\n  Coverage      96%     96%         \n======================================\n  Files         246     246         \n  Lines        7657    7657         \n  Branches     1452    1452         \n======================================\n  Hits         7351    7351         \n  Misses        306     306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7a4e198...7add095. Read the comment docs.\n. # Codecov Report\nMerging #2240 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #2240   +/-\n======================================\n  Coverage    96.1%   96.1%         \n======================================\n  Files         250     250         \n  Lines        7852    7852         \n  Branches     1506    1506         \n======================================\n  Hits         7546    7546         \n  Misses        306     306\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e0ffda5...ab96af1. Read the comment docs.\n. # Codecov Report\nMerging #2246 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2246      +/-\n=========================================\n+ Coverage    96.1%   96.1%   +<.01%   \n=========================================\n  Files         250     250            \n  Lines        7852    7853       +1   \n  Branches     1506    1507       +1   \n=========================================\n+ Hits         7546    7547       +1   \n  Misses        306     306\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/service.js | 98.01% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0b5e95c...ce4d9ee. Read the comment docs.\n. # Codecov Report\nMerging #2253 into master will increase coverage by 0.06%.\nThe diff coverage is 98.48%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2253      +/-\n==========================================\n+ Coverage   96.11%   96.17%   +0.06%   \n==========================================\n  Files         253      254       +1   \n  Lines        7901     8086     +185   \n  Branches     1515     1579      +64   \n==========================================\n+ Hits         7594     7777     +183   \n- Misses        307      309       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/util.js | 94.33% <\u00f8> (-0.03%) | :arrow_down: |\n| lib/event_listeners.js | 96.39% <100%> (+0.03%) | :arrow_up: |\n| lib/node_loader.js | 89.39% <100%> (+0.5%) | :arrow_up: |\n| lib/model/api.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/credentials/shared_ini_file_credentials.js | 98% <100%> (\u00f8) | :arrow_up: |\n| lib/http.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/model/operation.js | 82.81% <100%> (+0.84%) | :arrow_up: |\n| lib/publisher/configuration.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/core.js | 100% <100%> (\u00f8) | :arrow_up: |\n| ... and 6 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 70cbaa7...525d087. Read the comment docs.\n. # Codecov Report\nMerging #2270 into master will increase coverage by 0.01%.\nThe diff coverage is 97.87%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2270      +/-\n==========================================\n+ Coverage   96.85%   96.87%   +0.01%   \n==========================================\n  Files         281      282       +1   \n  Lines        8527     8599      +72   \n  Branches     1621     1635      +14   \n==========================================\n+ Hits         8259     8330      +71   \n- Misses        268      269       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/shared_ini_file_credentials.js | 98.78% <100%> (+0.93%) | :arrow_up: |\n| lib/credentials/sts_credentials_cache.js | 97.59% <97.59%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8a82f6b...1af3262. Read the comment docs.\n. # Codecov Report\nMerging #2271 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2271   +/-\n=======================================\n  Coverage   96.09%   96.09%         \n=======================================\n  Files         250      250         \n  Lines        7854     7854         \n  Branches     1508     1508         \n=======================================\n  Hits         7547     7547         \n  Misses        307      307\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/ec2_metadata_credentials.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ddc67e6...e4d0926. Read the comment docs.\n. # Codecov Report\nMerging #2279 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2279   +/-\n=======================================\n  Coverage   96.09%   96.09%         \n=======================================\n  Files         250      250         \n  Lines        7854     7854         \n  Branches     1508     1508         \n=======================================\n  Hits         7547     7547         \n  Misses        307      307\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cabf1fa...d72c3c1. Read the comment docs.\n. # Codecov Report\nMerging #2283 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2283      +/-\n==========================================\n+ Coverage   96.09%   96.09%   +<.01%   \n==========================================\n  Files         250      251       +1   \n  Lines        7860     7863       +3   \n  Branches     1510     1508       -2   \n==========================================\n+ Hits         7553     7556       +3   \n  Misses        307      307\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/publisher/configuration.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/shared-ini/ini-loader.js | 100% <100%> (\u00f8) | |\n| lib/node_loader.js | 88.88% <100%> (\u00f8) | :arrow_up: |\n| lib/credentials/shared_ini_file_credentials.js | 97.97% <100%> (-0.03%) | :arrow_down: |\n| lib/shared-ini/index.js | 100% <100%> (\u00f8) | |\n| lib/sequential_executor.js | 98.8% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8878346...5d1956d. Read the comment docs.\n. # Codecov Report\nMerging #2284 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2284      +/-\n==========================================\n+ Coverage   96.09%   96.09%   +<.01%   \n==========================================\n  Files         250      250            \n  Lines        7854     7859       +5   \n  Branches     1508     1510       +2   \n==========================================\n+ Hits         7547     7552       +5   \n  Misses        307      307\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/param_validator.js | 96.52% <100%> (+0.12%) | :arrow_up: |\n| lib/services/s3.js | 98.14% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6e7dfeb...d5624fe. Read the comment docs.\n. # Codecov Report\nMerging #2286 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2286   +/-\n=======================================\n  Coverage   96.09%   96.09%         \n=======================================\n  Files         250      250         \n  Lines        7859     7859         \n  Branches     1510     1510         \n=======================================\n  Hits         7552     7552         \n  Misses        307      307\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0049882...9085618. Read the comment docs.\n. # Codecov Report\nMerging #2293 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2293   +/-\n=======================================\n  Coverage   96.09%   96.09%         \n=======================================\n  Files         250      250         \n  Lines        7859     7859         \n  Branches     1510     1510         \n=======================================\n  Hits         7552     7552         \n  Misses        307      307\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d5ea34e...b6bbff5. Read the comment docs.\n. # Codecov Report\nMerging #2295 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2295   +/-\n=======================================\n  Coverage   96.09%   96.09%         \n=======================================\n  Files         250      250         \n  Lines        7859     7859         \n  Branches     1510     1510         \n=======================================\n  Hits         7552     7552         \n  Misses        307      307\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d5ea34e...2241baa. Read the comment docs.\n. # Codecov Report\nMerging #2328 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #2328   +/-\n======================================\n  Coverage    96.1%   96.1%         \n======================================\n  Files         252     252         \n  Lines        7887    7887         \n  Branches     1514    1514         \n======================================\n  Hits         7580    7580         \n  Misses        307     307\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ef650d4...bcb60d6. Read the comment docs.\n. # Codecov Report\nMerging #2330 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2330      +/-\n=========================================\n+ Coverage    96.1%   96.1%   +<.01%   \n=========================================\n  Files         252     252            \n  Lines        7887    7888       +1   \n  Branches     1514    1515       +1   \n=========================================\n+ Hits         7580    7581       +1   \n  Misses        307     307\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 93.91% <100%> (+0.02%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ef650d4...b813800. Read the comment docs.\n. # Codecov Report\nMerging #2336 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2336   +/-\n=======================================\n  Coverage   96.85%   96.85%         \n=======================================\n  Files         280      280         \n  Lines        8515     8515         \n  Branches     1621     1621         \n=======================================\n  Hits         8247     8247         \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/event-stream/parse-message.js | 97.05% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/event-stream/alloc-buffer.js | 87.5% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/event-stream/streaming-create-event-stream.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/event-stream/parse-event.js | 94.28% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/event-stream/to-buffer.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/shared-ini/index.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/event-stream/event-message-chunker.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/event-stream/event-message-chunker-stream.js | 95.91% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/rds/signer.js | 98.11% <\u00f8> (\u00f8) | :arrow_up: |\n| .../event-stream/event-message-unmarshaller-stream.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| ... and 30 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 74e2746...4477075. Read the comment docs.\n. # Codecov Report\nMerging #2339 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #2339   +/-\n======================================\n  Coverage    96.1%   96.1%         \n======================================\n  Files         252     252         \n  Lines        7888    7888         \n  Branches     1515    1515         \n======================================\n  Hits         7581    7581         \n  Misses        307     307\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d36a581...f316795. Read the comment docs.\n. # Codecov Report\nMerging #2342 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2342   +/-\n=======================================\n  Coverage   96.11%   96.11%         \n=======================================\n  Files         253      253         \n  Lines        7901     7901         \n  Branches     1515     1515         \n=======================================\n  Hits         7594     7594         \n  Misses        307      307\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 66e7431...13b0dd3. Read the comment docs.\n. # Codecov Report\nMerging #2348 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2348   +/-\n=======================================\n  Coverage   96.17%   96.17%         \n=======================================\n  Files         254      254         \n  Lines        8086     8086         \n  Branches     1579     1579         \n=======================================\n  Hits         7777     7777         \n  Misses        309      309\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/discover_endpoint.js | 98.2% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 53490b4...fe0753e. Read the comment docs.\n. # Codecov Report\nMerging #2365 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2365   +/-\n=======================================\n  Coverage   96.25%   96.25%         \n=======================================\n  Files         262      262         \n  Lines        8249     8249         \n  Branches     1598     1598         \n=======================================\n  Hits         7940     7940         \n  Misses        309      309\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b03e56e...81a43bc. Read the comment docs.\n. # Codecov Report\nMerging #2366 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2366   +/-\n=======================================\n  Coverage   96.21%   96.21%         \n=======================================\n  Files         259      259         \n  Lines        8171     8171         \n  Branches     1587     1587         \n=======================================\n  Hits         7862     7862         \n  Misses        309      309\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3a8901f...5595889. Read the comment docs.\n. # Codecov Report\nMerging #2369 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2369      +/-\n==========================================\n+ Coverage   96.23%   96.25%   +0.02%   \n==========================================\n  Files         261      262       +1   \n  Lines        8201     8249      +48   \n  Branches     1587     1598      +11   \n==========================================\n+ Hits         7892     7940      +48   \n  Misses        309      309\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/model/operation.js | 83.07% <100%> (+0.26%) | :arrow_up: |\n| lib/protocol/json.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/protocol/rest.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/protocol/helpers.js | 100% <100%> (\u00f8) | |\n| lib/protocol/query.js | 100% <100%> (\u00f8) | :arrow_up: |\n| lib/model/shape.js | 95.45% <100%> (+0.02%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a61fc56...704230e. Read the comment docs.\n. # Codecov Report\nMerging #2378 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2378      +/-\n==========================================\n+ Coverage   96.27%   96.27%   +<.01%   \n==========================================\n  Files         266      266            \n  Lines        8297     8298       +1   \n  Branches     1598     1598            \n==========================================\n+ Hits         7988     7989       +1   \n  Misses        309      309\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| clients/mediaconvert.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 64a02e0...a433c59. Read the comment docs.\n. # Codecov Report\nMerging #2387 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2387   +/-\n=======================================\n  Coverage   96.32%   96.32%         \n=======================================\n  Files         275      275         \n  Lines        8407     8407         \n  Branches     1599     1599         \n=======================================\n  Hits         8098     8098         \n  Misses        309      309\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.14% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d9f9728...33e029e. Read the comment docs.\n. # Codecov Report\nMerging #2394 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2394   +/-\n=======================================\n  Coverage   96.32%   96.32%         \n=======================================\n  Files         275      275         \n  Lines        8407     8407         \n  Branches     1599     1599         \n=======================================\n  Hits         8098     8098         \n  Misses        309      309\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 61f8913...49c74f8. Read the comment docs.\n. # Codecov Report\nMerging #2405 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2405   +/-\n=======================================\n  Coverage   96.32%   96.32%         \n=======================================\n  Files         275      275         \n  Lines        8407     8407         \n  Branches     1599     1599         \n=======================================\n  Hits         8098     8098         \n  Misses        309      309\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.14% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a472063...05537c6. Read the comment docs.\n. # Codecov Report\nMerging #2407 into master will increase coverage by 0.47%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2407      +/-\n=========================================\n+ Coverage   96.32%   96.8%   +0.47%   \n=========================================\n  Files         275     275            \n  Lines        8407    8381      -26   \n  Branches     1599    1592       -7   \n=========================================\n+ Hits         8098    8113      +15   \n+ Misses        309     268      -41\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/dynamodb/document_client.js | 94.11% <100%> (+30.83%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update db34481...3b9d16c. Read the comment docs.\n. # Codecov Report\nMerging #2409 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2409   +/-\n=======================================\n  Coverage   96.32%   96.32%         \n=======================================\n  Files         275      275         \n  Lines        8407     8407         \n  Branches     1599     1599         \n=======================================\n  Hits         8098     8098         \n  Misses        309      309\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/config.js | 87.2% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b13fb02...dd62e44. Read the comment docs.\n. # Codecov Report\nMerging #2410 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2410      +/-\n=========================================\n+ Coverage    96.8%   96.8%   +<.01%   \n=========================================\n  Files         275     275            \n  Lines        8381    8383       +2   \n  Branches     1592    1592            \n=========================================\n+ Hits         8113    8115       +2   \n  Misses        268     268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 93.96% <100%> (+0.04%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e9bdef5...43b1ad8. Read the comment docs.\n. # Codecov Report\nMerging #2420 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2420      +/-\n==========================================\n+ Coverage   96.82%   96.82%   +<.01%   \n==========================================\n  Files         276      276            \n  Lines        8434     8435       +1   \n  Branches     1607     1607            \n==========================================\n+ Hits         8166     8167       +1   \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.14% <100%> (\u00f8) | :arrow_up: |\n| lib/credentials/chainable_temporary_credentials.js | 100% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4e074eb...933ab8e. Read the comment docs.\n. # Codecov Report\nMerging #2422 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2422   +/-\n=======================================\n  Coverage   96.82%   96.82%         \n=======================================\n  Files         276      276         \n  Lines        8434     8434         \n  Branches     1607     1607         \n=======================================\n  Hits         8166     8166         \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/chainable_temporary_credentials.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 68ad0d0...9e75edb. Read the comment docs.\n. # Codecov Report\nMerging #2427 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2427   +/-\n=======================================\n  Coverage   96.82%   96.82%         \n=======================================\n  Files         276      276         \n  Lines        8434     8434         \n  Branches     1607     1607         \n=======================================\n  Hits         8166     8166         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1e75eb6...f130543. Read the comment docs.\n. # Codecov Report\nMerging #2429 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2429   +/-\n=======================================\n  Coverage   96.82%   96.82%         \n=======================================\n  Files         276      276         \n  Lines        8434     8434         \n  Branches     1607     1607         \n=======================================\n  Hits         8166     8166         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 11b7544...5f4a8ab. Read the comment docs.\n. # Codecov Report\nMerging #2443 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2443   +/-\n=======================================\n  Coverage   96.83%   96.83%         \n=======================================\n  Files         278      278         \n  Lines        8461     8461         \n  Branches     1607     1607         \n=======================================\n  Hits         8193     8193         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1987862...adb9c32. Read the comment docs.\n. # Codecov Report\nMerging #2444 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2444      +/-\n==========================================\n+ Coverage   96.83%   96.83%   +<.01%   \n==========================================\n  Files         278      278            \n  Lines        8461     8465       +4   \n  Branches     1607     1608       +1   \n==========================================\n+ Hits         8193     8197       +4   \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/ec2_metadata_credentials.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 36a025e...781787c. Read the comment docs.\n. # Codecov Report\nMerging #2445 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2445   +/-\n=======================================\n  Coverage   96.83%   96.83%         \n=======================================\n  Files         278      278         \n  Lines        8461     8461         \n  Branches     1607     1607         \n=======================================\n  Hits         8193     8193         \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.14% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1987862...7d8593a. Read the comment docs.\n. # Codecov Report\nMerging #2446 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2446   +/-\n=======================================\n  Coverage   96.83%   96.83%         \n=======================================\n  Files         278      278         \n  Lines        8461     8461         \n  Branches     1607     1607         \n=======================================\n  Hits         8193     8193         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1987862...a8075de. Read the comment docs.\n. # Codecov Report\nMerging #2448 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2448      +/-\n==========================================\n+ Coverage   96.83%   96.84%   +<.01%   \n==========================================\n  Files         278      278            \n  Lines        8472     8486      +14   \n  Branches     1609     1620      +11   \n==========================================\n+ Hits         8204     8218      +14   \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/publisher/index.js | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| lib/service.js | 98.12% <100%> (+0.07%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8db24fb...faf870d. Read the comment docs.\n. # Codecov Report\nMerging #2451 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2451      +/-\n==========================================\n+ Coverage   96.83%   96.83%   +<.01%   \n==========================================\n  Files         278      278            \n  Lines        8465     8471       +6   \n  Branches     1608     1609       +1   \n==========================================\n+ Hits         8197     8203       +6   \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/credential_provider_chain.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2a872ad...2936f17. Read the comment docs.\n. # Codecov Report\nMerging #2452 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2452   +/-\n=======================================\n  Coverage   96.83%   96.83%         \n=======================================\n  Files         278      278         \n  Lines        8465     8465         \n  Branches     1608     1608         \n=======================================\n  Hits         8197     8197         \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/service.js | 98.05% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2a872ad...5e09d40. Read the comment docs.\n. # Codecov Report\nMerging #2453 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2453      +/-\n==========================================\n+ Coverage   96.83%   96.84%   +<.01%   \n==========================================\n  Files         278      278            \n  Lines        8472     8487      +15   \n  Branches     1609     1620      +11   \n==========================================\n+ Hits         8204     8219      +15   \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/discover_endpoint.js | 98.22% <100%> (+0.02%) | :arrow_up: |\n| lib/publisher/index.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/credentials/credential_provider_chain.js | 100% <0%> (\u00f8) | :arrow_up: |\n| lib/service.js | 98.12% <0%> (+0.07%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8db24fb...8cf2c8c. Read the comment docs.\n. # Codecov Report\nMerging #2456 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2456      +/-\n==========================================\n- Coverage   96.83%   96.83%   -0.01%   \n==========================================\n  Files         278      278            \n  Lines        8472     8471       -1   \n  Branches     1609     1609            \n==========================================\n- Hits         8204     8203       -1   \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/credential_provider_chain.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8db24fb...8491495. Read the comment docs.\n. # Codecov Report\nMerging #2463 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2463   +/-\n=======================================\n  Coverage   96.83%   96.83%         \n=======================================\n  Files         278      278         \n  Lines        8471     8471         \n  Branches     1609     1609         \n=======================================\n  Hits         8203     8203         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9a9d7e8...331871f. Read the comment docs.\n. # Codecov Report\nMerging #2465 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2465   +/-\n=======================================\n  Coverage   96.84%   96.84%         \n=======================================\n  Files         278      278         \n  Lines        8485     8485         \n  Branches     1620     1620         \n=======================================\n  Hits         8217     8217         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9aa6b03...e9c5fdd. Read the comment docs.\n. # Codecov Report\nMerging #2473 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2473   +/-\n=======================================\n  Coverage   96.84%   96.84%         \n=======================================\n  Files         278      278         \n  Lines        8485     8485         \n  Branches     1620     1620         \n=======================================\n  Hits         8217     8217         \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.14% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c211d41...9c90af6. Read the comment docs.\n. # Codecov Report\nMerging #2474 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2474   +/-\n=======================================\n  Coverage   96.84%   96.84%         \n=======================================\n  Files         278      278         \n  Lines        8485     8485         \n  Branches     1620     1620         \n=======================================\n  Hits         8217     8217         \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/shared-ini/ini-loader.js | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5390b54...07af026. Read the comment docs.\n. # Codecov Report\nMerging #2477 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2477   +/-\n=======================================\n  Coverage   96.84%   96.84%         \n=======================================\n  Files         278      278         \n  Lines        8485     8485         \n  Branches     1620     1620         \n=======================================\n  Hits         8217     8217         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b4b9b2a...15edc6b. Read the comment docs.\n. # Codecov Report\nMerging #2480 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2480      +/-\n==========================================\n+ Coverage   96.84%   96.84%   +<.01%   \n==========================================\n  Files         279      279            \n  Lines        8500     8503       +3   \n  Branches     1620     1621       +1   \n==========================================\n+ Hits         8232     8235       +3   \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/service.js | 98.13% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3e10a5e...4d1ff0a. Read the comment docs.\n. # Codecov Report\nMerging #2481 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2481   +/-\n=======================================\n  Coverage   96.84%   96.84%         \n=======================================\n  Files         279      279         \n  Lines        8498     8498         \n  Branches     1620     1620         \n=======================================\n  Hits         8230     8230         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ff134c0...3ffa984. Read the comment docs.\n. # Codecov Report\nMerging #2492 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2492   +/-\n=======================================\n  Coverage   96.85%   96.85%         \n=======================================\n  Files         280      280         \n  Lines        8515     8515         \n  Branches     1621     1621         \n=======================================\n  Hits         8247     8247         \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 93.96% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 74e2746...0383a03. Read the comment docs.\n. # Codecov Report\nMerging #2494 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2494   +/-\n=======================================\n  Coverage   96.85%   96.85%         \n=======================================\n  Files         280      280         \n  Lines        8515     8515         \n  Branches     1621     1621         \n=======================================\n  Hits         8247     8247         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9d1dbdf...4cd9ee7. Read the comment docs.\n. # Codecov Report\nMerging #2541 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2541   +/-\n=======================================\n  Coverage   96.85%   96.85%         \n=======================================\n  Files         281      281         \n  Lines        8527     8527         \n  Branches     1621     1621         \n=======================================\n  Hits         8259     8259         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4eda88e...ffaaae3. Read the comment docs.\n. # Codecov Report\nMerging #2542 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2542   +/-\n=======================================\n  Coverage   96.85%   96.85%         \n=======================================\n  Files         281      281         \n  Lines        8527     8527         \n  Branches     1621     1621         \n=======================================\n  Hits         8259     8259         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4eda88e...f1f228f. Read the comment docs.\n. # Codecov Report\nMerging #2544 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2544   +/-\n=======================================\n  Coverage   96.85%   96.85%         \n=======================================\n  Files         281      281         \n  Lines        8527     8527         \n  Branches     1621     1621         \n=======================================\n  Hits         8259     8259         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a0e2c0...b3c3949. Read the comment docs.\n. # Codecov Report\nMerging #2549 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2549   +/-\n=======================================\n  Coverage   96.85%   96.85%         \n=======================================\n  Files         281      281         \n  Lines        8527     8527         \n  Branches     1621     1621         \n=======================================\n  Hits         8259     8259         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fb9cb46...854307c. Read the comment docs.\n. # Codecov Report\nMerging #2554 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2554   +/-\n=======================================\n  Coverage   96.85%   96.85%         \n=======================================\n  Files         281      281         \n  Lines        8527     8527         \n  Branches     1621     1621         \n=======================================\n  Hits         8259     8259         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cb451b3...0b4018a. Read the comment docs.\n. # Codecov Report\nMerging #2559 into master will decrease coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2559      +/-\n==========================================\n- Coverage   96.85%   96.82%   -0.03%   \n==========================================\n  Files         281      282       +1   \n  Lines        8527     8577      +50   \n  Branches     1621     1631      +10   \n==========================================\n+ Hits         8259     8305      +46   \n- Misses        268      272       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/credentials/shared_ini_file_credentials.js | 98.36% <100%> (+0.51%) | :arrow_up: |\n| clients/appmesh.js | 76.47% <0%> (-23.53%) | :arrow_down: |\n| clients/textract.js | 100% <0%> (\u00f8) | |\n| lib/s3/managed_upload.js | 94.05% <0%> (+0.08%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d0cb104...e350dd2. Read the comment docs.\n. # Codecov Report\nMerging #2562 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2562   +/-\n=======================================\n  Coverage   96.85%   96.85%         \n=======================================\n  Files         281      281         \n  Lines        8527     8527         \n  Branches     1621     1621         \n=======================================\n  Hits         8259     8259         \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/protocol/rest_json.js | 94.73% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b7a39ad...0dc188e. Read the comment docs.\n. # Codecov Report\nMerging #2564 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2564   +/-\n=======================================\n  Coverage   96.85%   96.85%         \n=======================================\n  Files         281      281         \n  Lines        8527     8527         \n  Branches     1621     1621         \n=======================================\n  Hits         8259     8259         \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/protocol/rest_json.js | 94.73% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 701aff0...c93b60b. Read the comment docs.\n. # Codecov Report\nMerging #2566 into master will decrease coverage by 0.04%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2566      +/-\n==========================================\n- Coverage   96.85%   96.81%   -0.05%   \n==========================================\n  Files         281      282       +1   \n  Lines        8527     8544      +17   \n  Branches     1621     1621            \n==========================================\n+ Hits         8259     8272      +13   \n- Misses        268      272       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| clients/appmesh.js | 76.47% <0%> (-23.53%) | :arrow_down: |\n| clients/textract.js | 100% <0%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ab6cdc4...185397f. Read the comment docs.\n. # Codecov Report\nMerging #2567 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2567      +/-\n==========================================\n+ Coverage   96.86%   96.86%   +<.01%   \n==========================================\n  Files         282      282            \n  Lines        8539     8543       +4   \n  Branches     1621     1622       +1   \n==========================================\n+ Hits         8271     8275       +4   \n  Misses        268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/s3/managed_upload.js | 94.05% <100%> (+0.08%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fb46b60...f9daddc. Read the comment docs.\n. # Codecov Report\nMerging #2568 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2568   +/-\n=======================================\n  Coverage   96.86%   96.86%         \n=======================================\n  Files         282      282         \n  Lines        8539     8539         \n  Branches     1621     1621         \n=======================================\n  Hits         8271     8271         \n  Misses        268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1435e3b...76ce73a. Read the comment docs.\n. # Codecov Report\nMerging #2574 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2574      +/-\n==========================================\n+ Coverage   96.81%   96.81%   +<.01%   \n==========================================\n  Files         282      282            \n  Lines        8544     8546       +2   \n  Branches     1621     1622       +1   \n==========================================\n+ Hits         8272     8274       +2   \n  Misses        272      272\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/protocol/rest_json.js | 94.91% <100%> (+0.17%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8156f09...f7e71bc. Read the comment docs.\n. # Codecov Report\nMerging #2578 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2578   +/-\n=======================================\n  Coverage   96.81%   96.81%         \n=======================================\n  Files         282      282         \n  Lines        8548     8548         \n  Branches     1622     1622         \n=======================================\n  Hits         8276     8276         \n  Misses        272      272\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3aaa2e2...5fbe306. Read the comment docs.\n. # Codecov Report\nMerging #2580 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2580   +/-\n=======================================\n  Coverage   96.81%   96.81%         \n=======================================\n  Files         282      282         \n  Lines        8548     8548         \n  Branches     1622     1622         \n=======================================\n  Hits         8276     8276         \n  Misses        272      272\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6eb50b2...c652be3. Read the comment docs.\n. # Codecov Report\nMerging #2583 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2583   +/-\n=======================================\n  Coverage   96.81%   96.81%         \n=======================================\n  Files         282      282         \n  Lines        8548     8548         \n  Branches     1622     1622         \n=======================================\n  Hits         8276     8276         \n  Misses        272      272\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.14% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6eb50b2...b1e8688. Read the comment docs.\n. # Codecov Report\nMerging #2583 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2583   +/-\n=======================================\n  Coverage   96.81%   96.81%         \n=======================================\n  Files         282      282         \n  Lines        8548     8548         \n  Branches     1622     1622         \n=======================================\n  Hits         8276     8276         \n  Misses        272      272\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| lib/services/s3.js | 98.14% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6eb50b2...b1e8688. Read the comment docs.\n. # Codecov Report\nMerging #2584 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2584   +/-\n=======================================\n  Coverage   96.81%   96.81%         \n=======================================\n  Files         282      282         \n  Lines        8550     8550         \n  Branches     1623     1623         \n=======================================\n  Hits         8278     8278         \n  Misses        272      272\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3899a45...d1d7680. Read the comment docs.\n. # Codecov Report\nMerging #2586 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2586   +/-\n=======================================\n  Coverage   96.81%   96.81%         \n=======================================\n  Files         282      282         \n  Lines        8550     8550         \n  Branches     1623     1623         \n=======================================\n  Hits         8278     8278         \n  Misses        272      272\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3899a45...03235c9. Read the comment docs.\n. # Codecov Report\nMerging #2587 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2587   +/-\n=======================================\n  Coverage   96.81%   96.81%         \n=======================================\n  Files         282      282         \n  Lines        8550     8550         \n  Branches     1623     1623         \n=======================================\n  Hits         8278     8278         \n  Misses        272      272\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d8ff25e...8919c18. Read the comment docs.\n. \n",
    "et304383": "Am I to understand that the SDK currently doesn't support ~/.aws at all?  Or just doesn't support using assumed roles through the ~/.aws entries, which has only recently been added to PHP and worked a while in Python?. ",
    "dotchev": "Why not load ~/.aws/confi by default as AWS CLI does?. ",
    "lucleray": "Hi,\nThis page is still confusing : https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/setting-region.html#setting-region-config-file.\n\"The SDK for JavaScript automatically searches for a config file when it loads.\"\nI understand here that I need to set AWS_SDK_LOAD_CONFIG to something truthy to get that behavior. Is this documented somewhere ?. Thanks!. ",
    "villasv": "@chrisradek any update on the version bump? Having this inconsistency for over a year is enough justification for a major release IMHO. ",
    "PersianIronwood": "It would have been nice if you could add an example of using this feature. I am having issues of using assume role with JS. . ",
    "chandwki": "This seems to be missing support for the mfa_serial option.. This seems to be missing support for the mfa_serial option.. ",
    "vitalis": "\ud83d\udc4d  looks great!. ",
    "bobmccracken": "Have you guys had any internal discussion about replacing https://github.com/awslabs/aws-sdk-react-native with this? The same question has been raised over there, but it seems like no one is watching that repo anymore.. ",
    "MattyK14": "Any update on the react native support?. ",
    "sridharrajagopal": "@chrisradek \nI'm running phantomjs 2.1.1\nCan you tell me where I can find 2.1.14?\nI get mine from https://bitbucket.org/ariya/phantomjs/downloads/?tab=downloads and only see 2.1.1 there. \nThanks,\nSridhar. @chrisradek \nI'm running phantomjs 2.1.1\nCan you tell me where I can find 2.1.14?\nI get mine from https://bitbucket.org/ariya/phantomjs/downloads/?tab=downloads and only see 2.1.1 there. \nThanks,\nSridhar. @chrisradek \nHere is such a call that phantomjs complains about - \n        // s3.upload({\n        //     Key: \"file.png\",\n        //     Body: buf,\n        //     ACL: 'public-read',\n        //     ContentType: 'image/png'\n        // }, function(err, data) {\n        //     if (err) {\n        //         return alert('There was an error uploading your chart: ', err.message); \n        //     }\n        //     alert('Successfully uploaded chart.');\n        // });\n\nThe other one was a dynamodb query\nBoth of these work in a \"regular\" browser. \n. @chrisradek \nHere is such a call that phantomjs complains about - \n        // s3.upload({\n        //     Key: \"file.png\",\n        //     Body: buf,\n        //     ACL: 'public-read',\n        //     ContentType: 'image/png'\n        // }, function(err, data) {\n        //     if (err) {\n        //         return alert('There was an error uploading your chart: ', err.message); \n        //     }\n        //     alert('Successfully uploaded chart.');\n        // });\n\nThe other one was a dynamodb query\nBoth of these work in a \"regular\" browser. \n. Btw, I checked that 2.1.14 from npm is indeed version 2.1.1 that I got directly from the phantomjs github. \n. Btw, I checked that 2.1.14 from npm is indeed version 2.1.1 that I got directly from the phantomjs github. \n. Any updates or feedback on this issue?\nMuch appreciated!\nThanks,\nSridhar. Any updates or feedback on this issue?\nMuch appreciated!\nThanks,\nSridhar. Any updates?\nThanks,\nSridhar. Any updates?\nThanks,\nSridhar. Any updates?\nThanks,\nSridhar. Any updates?\nThanks,\nSridhar. Yes, this is still an outstanding issue for me, though I haven't tried it again since I reported my issue above as it didn't work. \nI can look into alternatives for PhantomJS as you suggest. \n. Yes, this is still an outstanding issue for me, though I haven't tried it again since I reported my issue above as it didn't work. \nI can look into alternatives for PhantomJS as you suggest. \n. ",
    "bla0815": "Hi i had the same error message using the AWS browser SDK in  Apache Cordova, whenever i tried to invoke aws-lambda functions. Apparently it was due my restrictive Content-Security-Policy (albeit allowing for the aws-sdk), since i was able to get it running by using the wildcard (for development). \nI have yet to figure out, which domain is specifically needed for my lambda-invocations, so feedback is appreciated, but i wanted to get this out, because i never would have guessed the cause from the error message alone and maybe it's also the cause for OP's problem.. ",
    "1c7": "No updates?\nThanks \nCheng. No updates?\nThanks \nCheng. ",
    "harishreddy-m": "I tried posting on forum.I got error like this.\nYour account is not ready for posting messages yet. Please try again later. \nCan someone post it on behalf of me.\nI am able to run my task without hostPort in portMappings.App is running at dynamically assigned 32774 port.\nWhen I try to create a service behind Application load balancer - I am getting following error.\nInvalidParameterException : A host port mapping for the container port 80 is required.\nHere is my service creation definition\n var params = {\n      desiredCount: 1,\n      cluster:\"my-first-cluster\",\n      loadBalancers: [\n        {\n          containerName: name,\n          containerPort: 80,\n          loadBalancerName: \"harish-alb-ecs\"\n        }\n      ],\n      role: \"ecsInstanceRole\",\n      serviceName: name+\"-alb\",\n      taskDefinition: taskDef\n    };\nThen I tried to manually create the service from aws console GUI - Same task is running and ec2 instance is registered as listener to ALB automatically.But the health check fails- I tried increasing timeout,interval of health check without any success.\nIs Application Load balancer stable?. ",
    "maxcountryman": "@jeskew if I allow the CRC check I get a CRC error and data is null.\nNote that it works when called outside of setInterval.\nNode v7.4.0.. ```\nconsole.log(err, data) -->\n{ CRC32CheckFailed: CRC32 integrity check failed\n    at Request.checkCrc32 (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/services/dynamodb.js:23:35)\n    at Request.callListeners (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/request.js:673:14)\n    at Request.transition (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request. (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/request.js:675:12)\n    at Request.callListeners (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/sequential_executor.js:115:18)\n    at Request.emit (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/request.js:673:14)\n    at Request.transition (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request. (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/request.js:675:12)\n    at Request.callListeners (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/sequential_executor.js:115:18)\n    at callNextListener (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/sequential_executor.js:95:12)\n    at IncomingMessage.onEnd (/Users/max/Documents/projects/prism/node_modules/aws-sdk/lib/event_listeners.js:233:11)\n    at emitNone (events.js:91:20)\n    at IncomingMessage.emit (events.js:185:7)\n    at endReadableNT (_stream_readable.js:974:12)\n    at _combinedTickCallback (internal/process/next_tick.js:74:11)\n    at process._tickDomainCallback (internal/process/next_tick.js:122:9)\n  message: 'CRC32 integrity check failed',\n  code: 'CRC32CheckFailed',\n  retryable: true,\n  time: 2017-03-13T21:26:37.497Z,\n  statusCode: 200 } null\n```. No I see issues in both cases. If The CRC check is disabled I see null data and no error when called from the setInterval callback. If enabled and when called from the setInterval I see the above error. Note that in both cases when setActiceHosts is called in line it works consistently. The only place it seems to fail is inside of setInterval. . I raised the allowed reads to 100 per minute and still see the same issue.. Sure. I'll do that in a moment. I can tell you the table has 26 entries, each containing a small JSON blob of three or four keys. . @jeskew I modified the code in this way:\njs\nfunction getActiveHosts(cb) {\n  const TableName = 'Hosts';\n  const params = { TableName };\n  docClient.scan(params, function (err, data) {\n    console.log(this.httpResponse.body.toString());\n    if (data && data.Items) {\n      cb(data.Items);\n    } else {\n      cb([]);\n    }\n  });\n}\nHowever I don't see anything in the console, it's logging what appears to be a line break.. (Note that the CRC32 check is still failing. If I log the err I see the same error message.). @jeskew Not a problem. Here's the output from this.httpReponse.headers and this.httpResponse.statusCode respectively:\n```\n{ server: 'Server',\n  date: 'Tue, 14 Mar 2017 19:12:29 GMT',\n  'content-type': 'application/x-amz-json-1.0',\n  'content-length': '11581',\n  connection: 'keep-alive',\n  'x-amzn-requestid': 'LVDC9787R33D0IE9RGF0S0014BVV4KQNSO5AEMVJF66Q9ASUAAJG',\n  'x-amz-crc32': '1885504171' }\n200\n```. The issue surfaced through a third-party library but I believe is due to the fact the SDK makes assumptions about the configuration of the underlying Node HTTP machinery.\nI have isolated the issue to a specific release over at the Raven repo. This only happens when the auto breadcrumbs feature is enabled, FYI.\nYou and your team may be in a better position to understand how this affects the SDK and the assumptions it makes. If this is indeed a configuration issue I would suggest at the very least documenting this constraint and if possible enforcing those configuration parameters within the SDK itself.\nI would consider this an SDK bug.. That's also the change I highlighted as the likely cause.\nFrom the AWS SDK side, please enforce this configuration! Again: this is an assumption on this library's part and needs to be documented and if possible enforced within the library itself: Fail to run if not set properly or at the very least display a prominent warning.\nThe latest Raven release seems to work.. At a minimum this is a documentation bug. Thanks for looking into this.. ",
    "tomowens3": "I should have held on submitting the ticket until I had repro steps with a test table and shareable code.\nAlso, it appears I was looking into the wrong code path. Will continue to dig deeper.. I should have held on submitting the ticket until I had repro steps with a test table and shareable code.\nAlso, it appears I was looking into the wrong code path. Will continue to dig deeper.. ",
    "gitisz": "I haven't had a chance... but I believe it will solve my issue.  Thanks for pointing me to this :). I haven't had a chance... but I believe it will solve my issue.  Thanks for pointing me to this :). ",
    "palakons": "looks like this is the issue... \nas my npm install aws-sdk@2.1.39 for me\nin package.json include...\n\n\"dependencies\": {\n    \"aws-sdk\": \"latest\",\n\nI tried uploaded to AWS Beanstalk, still get same error though\npls bear with me, I'm really new to node.js . looks like this is the issue... \nas my npm install aws-sdk@2.1.39 for me\nin package.json include...\n\n\"dependencies\": {\n    \"aws-sdk\": \"latest\",\n\nI tried uploaded to AWS Beanstalk, still get same error though\npls bear with me, I'm really new to node.js . I managed to have aws-sdk 2.28.0\nrekognition worked now, thanks :). I managed to have aws-sdk 2.28.0\nrekognition worked now, thanks :). ",
    "damonmaria": "@chrisradek Although an extra request it is a multipart upload so is already multiple requests. When uploading a large file you're not expecting millisecond latency. . Just checking. I gather from the commits that this will apply to S3.upload() as well as ManagedUpload. If so, what is happening with S3.upload({ Tagging: '...' })? Will that still exist as it is now (therefore not work if the upload switches to multipart)? . Just checking. I gather from the commits that this will apply to S3.upload() as well as ManagedUpload. If so, what is happening with S3.upload({ Tagging: '...' })? Will that still exist as it is now (therefore not work if the upload switches to multipart)? . ",
    "gabegorelick": "Is this documented anywhere? Judging by the linked PR, callers should be using this like so\njs\ns3.upload(\n    {\n        Bucket: 'BUCKET',\n        Key: 'KEY',\n        Body: Buffer.from('some data')\n    },\n    {\n        tags: [\n            {\n                Key: 'key',\n                Value: 'value',\n            },\n            {\n                Key: 'otherKey',\n                Value: 'otherValue',\n            },\n        ],\n    }\n);\ninstead of passing a Tagging string option in the first argument to upload. However, this is not documented in the JS SDK docs.\n. Is this documented anywhere? Judging by the linked PR, callers should be using this like so\njs\ns3.upload(\n    {\n        Bucket: 'BUCKET',\n        Key: 'KEY',\n        Body: Buffer.from('some data')\n    },\n    {\n        tags: [\n            {\n                Key: 'key',\n                Value: 'value',\n            },\n            {\n                Key: 'otherKey',\n                Value: 'otherValue',\n            },\n        ],\n    }\n);\ninstead of passing a Tagging string option in the first argument to upload. However, this is not documented in the JS SDK docs.\n. ",
    "JordanSinko": "@chrisradek I am using the version that's included in the lambda execution environment, which appears to be 2.22.0. It appears to be an issue with the CLI as well.. @chrisradek \nMore info.. When I view the network traffic on the AWS console, it also returns an empty object for\nhttps://console.aws.amazon.com/apigateway/backend/domainnames/{{domainName}}/basepathmappings?limit=1000\nIt appears to be a broader issue than the SDK/CLI.. @chrisradek \nRegion: us-east-1\nSeems to be recent, within the last couple of weeks. Coincidentally, around the time the AWS console UI changed for the Custom Domain Names view. . @chrisradek \nMore info.. I added a base path to the domain having issues, and then immediately removed it. Once removed, and when I call GetBasePathMappings on it, I get the expected response of\n{\n    items: []\n}\nIt appears the issue of it not returning the right response happens when the domain name is first created and no mappings have been added.. Closing. https://forums.aws.amazon.com/thread.jspa?threadID=251826&tstart=0. ",
    "dcollinsf5": "@chrisradek I was just working on hacking this to allow this functionality, so this is perfectly timed!  How long does it generally take for a merge like this to make it's way to the NPM registry? Excuse my ignorance, this is the first time I've needed something that was just merged.. OK, thanks!. ",
    "treyrich": "@chrisradek thanks for the quick reply!\nUnfortunately, this was something missed by our tests, and our package.json was configured incorrectly and this library has been updating freely, so this has possibly been broken over the course of a couple of different versions (hate it when that happens!). So I can't track down the exact version that this started occurring.\nHere's the request headers:\nContent-Type:application/x-amz-json-1.1\nOrigin:http://localhost:5555\nReferer:http://localhost:5555/profile\nUser-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36\nX-Amz-Content-Sha256:801f1cb7f717cb7778c7ec6d80721e3db93fd9b343007100a1fdaef4b6ec2238\nX-Amz-Target:AWSCognitoIdentityService.GetCredentialsForIdentity\nX-Amz-User-Agent:aws-sdk-js/2.30.0 callback\nI just tested by setting the region directly, and have the same result, but I would have expected that since the request is being made to https://cognito-identity.us-east-1.amazonaws.com/ and we're using the us-east-1 region.. Yes, there is a request ID. Here's the response headers:\naccess-control-allow-origin:*\naccess-control-expose-headers:x-amzn-RequestId,x-amzn-ErrorType,x-amzn-ErrorMessage,Date\nconnection:keep-alive\ncontent-length:113\ndate:Wed, 22 Mar 2017 17:29:42 GMT\nx-amzn-requestid:233609a1-0f25-11e7-b000-e71ef302849e\nBased on tests today, this seems to happen 100% of the time. But I only caught the error today, so I can't say much beyond that.. No problem. I just tried it, and it's not working on that version either. Same error.\nI know FOR SURE we have been functional since 2.7.x though. So maybe this isn't an issue with the SDK? Is it possible a server-side change on AWS caused an issue with the compatibility of this SDK? My mind keeps going back to the fact that I seem to be the only one having this issue (based on the lack of bug reports), which means it probably is project specific. But the relevant code in the project hasn't changed for close to a year.. Fair enough. I posted this thread https://forums.aws.amazon.com/thread.jspa?threadID=252029. Hey @jeskew thanks for the response. I was actually wondering about the lack of activity on the forum post I made.\nSo, interestingly enough we have recently introduced a Service Worker into our project, it hadn't occurred to me that it could be connected though since all other services/APIs we're using work perfectly and the service worker is only caching specific resources.\nHave you guys tried running the API along with a service worker? It would be interesting to know whether this is something specific to our implementation of a service worker, or if this is a global problem with service workers.\n(To confirm it's definitely related to service workers I've verified by testing in Safari which doesn't support service workers and everything seems to function properly.)\n. @jeskew sorry, I may not have made this clear. We aren't using the SDK in a service worker, but we're using it alongside a service worker. When you have a service worker installed all network traffic flows through the worker (basically as an in-browser proxy). See https://github.com/w3c/ServiceWorker/blob/master/explainer.md#network-intercepting\nSo although we aren't trying to make use of the SDK within the service worker, we are using the service worker within our application which causes all network traffic to pass through. I expect that as ServiceWorkers get more and more prevalent (which Google is pushing hard for with Chrome) this will become more pronounced of an issue if the SDK has some incompatibility with this paradigm though.\nI think there is some potential that our specific implementation of a service worker could be causing this incompatibility, but we're using other 3rd party APIs (LinkedIn, FB, Twilio, etc) without any issue so I guess the question is whether any testing has been done in a web app with a service worker? If not I think someone (probably me) needs to build a barebones app with the SDK and a skeleton service worker to determine whether this is a broad issue, or specific to this particular case.. That would be awesome. As far as I'm aware we don't have anything in place to resolve the bare domain to /index.html, but we're making some use of https://github.com/angular/mobile-toolkit which could be doing some magic that I'm unaware of.\nI'm happy to help with the test case if you want/need. Just let me know.\nIt'll probably be easier for everyone if this is something specific to our implementation. But based on all the other 3rd party SDKs we're using successfully, I'm feeling like it's probably a 50/50 chance.. Interesting. Ok, I'll look into that. Obviously if that routing section of the manifest results in other domains being routed that's a fairly large issue on their end.\nLet me look into that before you go any further with this in case it's a bug on Google's end, I truly appreciate all the help so far! I'll update once I find out if that is indeed what's causing this issue.. Ok. I've confirmed, this is indeed a bug with the Angular Mobile Toolkit.\nI have opened this issue here: https://github.com/angular/mobile-toolkit/issues/135\nThanks for all the help here, sorry for the false alarm, this is obviously a very frightening bug to have in the Angular code.. ",
    "henrysachs": "has anybody a solution for this? or do we still live with this bug and go on? . ",
    "Archetype90": "@jeskew Thanks so much for the reply. In regards to Mocha testing, thank you for the answer - I will test that out now. I am relatively new to those testing frameworks and passed a Mocha problem to the SDK.\nNow Lambda is a little more curious for me. No, I did not wrap it in a handler function, but I am using Apex to deploy, which sets the handler in code (or at least, that is my understanding - again, new to Apex). Now, my Lambda function will execute, and return the correct response (using it for an Alexa skill). But, it does not ever hit that callback (giving me \"success\" or \"err\" to the console). \nI will see the Lambda function say \"Entering test...\" and \"After publish...\", but as I said, no callback. Thsi is the one that really confuses me, as the Lambda function is clearly executing properly, but the publish will not work. Which is odd, as my understanding is it is a simple HTTP POST to the endpoint that you specify (in this case, the AWS IoT endpoint). . @jeskew Ok, I just tested it as an asynchronous function and it worked like a charm. Thank you so much for answer what was clearly not an AWS-SDK problem \ud83d\udc4d . \nYour async comments made me think that may be where my problem lies with Lambda, and as it turns out, it was. It seems like the whole key was for me to have my code like this:\n```\n    var iotdata = new awsIot.IotData(config.iotDevice);\nvar params = {\n  topic: 'DeviceNumber',\n  payload: 'blah',\n  qos: 0\n};\n\nreturn iotdata.publish(params, function(err, data){\n  if(err) {\n      console.log(err);\n  }\n  else {\n      console.log(\"success!\");\n  }\n})\n\n```\nThe \"return\" being the key. Ill be perfectly honest - I am not entirely sure why that is necessary. I attempted to make the code synchronous, as follows:\n```\nvar iotdata = new awsIot.IotData(config.iotDevice);\nvar params = {\n  topic: 'DeviceNumber',\n  payload: 'blah',\n  qos: 0\n};\nrequest = iotdata.publish(params);\nrequest.send();\n```\nBut that was not OK either. Anyway, it is working all across my code, and I did not need to change any of my packages or architecture. All it took was a good nights sleep and a great comment from you! Thanks a ton! . ",
    "uopeydel": "ok can u modified my code to enable to upload image?\nin sample code it will be return link\n (1) https://mybucketfolder.s3-ap-southeast-1.amazonaws.com/pic03.jpg \ni think it should be\n (2) https://s3-ap-southeast-1.amazonaws.com/mybucketfolder/pic03.jpg \nto can upload bcause when i upload by manual i have link  (2) but when use that code it will return  err (1). ",
    "iwllyu": "oh sorry this is totally on the wrong forum - this should be opened at aws-sdk-mock\napologies. ",
    "jmparsons": "Hi @jeskew, \nThe only part that stands out is the persistence layer. Inside of the cognito sdk is the StorageHelper class: \nhttps://github.com/aws/amazon-cognito-identity-js/blob/master/src/StorageHelper.js\nIt does mimic every method already available in react-native using AsyncStorage here:\nhttps://github.com/facebook/react-native/blob/master/Libraries/Storage/AsyncStorage.js\nWith the differences being the AsyncStorage calls are asynchronous using promises. The cognito library uses the values mostly in one class CognitoUser for storing sessions and tokens. \nThe integration was very straightforward. I copied the src folder, then changed all of the imports to point to the react-native version of aws-sdk-js:\n```javascript\n// AuthenticationHelper.js, CognitoAccessToken.js, CognitoToken.js, CognitUser.js\nimport { util } from 'aws-sdk/global';\n// to this:\nimport { util } from 'aws-sdk/dist/aws-sdk-react-native';\n// CognitoUserPool.js\nimport CognitoIdentityServiceProvider from 'aws-sdk/clients/cognitoidentityserviceprovider';\n// to this:\nimport { CognitoIdentityServiceProvider } from 'aws-sdk/dist/aws-sdk-react-native';\n```\nThen created a file to load AWS and map the classes onto it using lodash:\n```javascript\n// AWS.js\nimport AWS, { CognitoIdentityServiceProvider } from 'aws-sdk/dist/aws-sdk-react-native';\nimport _ from 'lodash';\nimport * as enhancements from '../AWSCognito/src';\n_.assign(CognitoIdentityServiceProvider, enhancements);\nexport default AWS;\n```\n. @rbbruce I ended up writing a bridge module to compute modpow. It was taking 6 seconds on the simulator and 20+ seconds on the device before. I used https://github.com/kirsteins/JKBigInteger/ the same library that is used in the aws IOS sdk. Java does modpow natively. Now modpow is instant. \nI was planning on releasing it, but I ended up changing the source code quite a bit from the cognito identity library - converted it to use promises. If I have time I will recreate it as a standalone, but that may not be for awhile. . @rbbruce Even if it's merged in, the modpow method won't change and really is just too slow in javascript. The cognito library is mostly getters and setters for data modeling, the majority of the code is in CognitoUser and AuthenticationHelper, both of which rely on modpow to generate their auth values. I'm going to make my setup a library probably within the next month to separate it out of my project.. @jeskew @rbbruce I've open sourced my project. \nhttps://github.com/AirLabsTeam/react-native-aws-cognito-js\nhttps://github.com/aws/amazon-cognito-identity-js/issues/327#issuecomment-297166971. Wow it finally got merged. Thanks @jeskew . ",
    "rbbruce": "Howdy,\nWe have recently stumbled across this issue when exploring Cognito for an app we are building for react native. Can you give any background as to why this is an issue? Additionally, can you give any status of a fix for this issue?\nThanks,\nRB Bruce. Howdy,\nWe have recently stumbled across this issue when exploring Cognito for an app we are building for react native. Can you give any background as to why this is an issue? Additionally, can you give any status of a fix for this issue?\nThanks,\nRB Bruce. @jmparsons Thanks for the quick response. We have not gotten that far with the testing. Will keep the delay in mind. That could make things a bit more interesting.\nWhat about having to bring source code in from amazon-cognito-identity-js into the react-native branch? Is that something that is in queue to be fixed? This method seems to be the cleanest implementation for using Cognito with react native. Just concerned about using this method in production. \nThanks,\nRB Bruce. @jmparsons Thanks for the quick response. We have not gotten that far with the testing. Will keep the delay in mind. That could make things a bit more interesting.\nWhat about having to bring source code in from amazon-cognito-identity-js into the react-native branch? Is that something that is in queue to be fixed? This method seems to be the cleanest implementation for using Cognito with react native. Just concerned about using this method in production. \nThanks,\nRB Bruce. @jmparsons Thank you! Will check it out!. @jmparsons Thank you! Will check it out!. ",
    "rahuljaguste": "thank you @jmparsons . You just helped me save 10 secs of boot time for my app. \ud83d\udc4d . thank you @jmparsons . You just helped me save 10 secs of boot time for my app. \ud83d\udc4d . ",
    "cvrajeesh": "@chrisradek  shouldn't we close the stream on error in other stream?. ",
    "antonsamper": "@kaihendry You're mixing promises and streams. Why dont you just return a promise using the SDK using the promise() method?\njavascript\ns3.getObject(params).promise().then(...).catch(...). Hi @jeskew,\nThat would be great! In hindsight adding a new dBInstanceModifying state would be cleaner and more robust as relying on arbitrary timings is not good practice.. hi @jeskew, do you know when this new waiter might be added?. ",
    "KidA001": "For anyone using async await:\n```javascript\nasync function getS3File(filename) {\n  const params = {\n    Bucket: 'some-bucket-name',\n    Key: filename,\n    Body: fileIO,\n  };\nconst response = await s3.getObject(params, (err) => {\n    if (err) {\n      // handle errors\n    }\n  });\nreturn response.Body.toString(); // your file as a string\n}\n```\nNote you can return response.Body to work with the buffer directly.\nIf you want the function to return a promise just do this instead:\n```javascript\nfunction getS3File(filename) {\n  const params = {\n    Bucket: 'some-bucket-name',\n    Key: filename,\n    Body: fileIO,\n  };\nreturn s3.getObject(params, (err) => {\n    if (err) {\n      // handle errors\n    }\n  }).promise();\n}\n```. ",
    "elasticsteve": "@antonsamper But what if I want to have a promise for a stream? I'm really struggling to figure this out!\nAny idea?\nRight now I have:\ns3.getObject(params).createReadStream().pipe(file);\nHow would I be able to catch any errors or know that the stream/write has completed?\nThanks!\n. @chrisradek  Thanks and sorry, I deleted my question just before you posted the reply, because all of a sudden I understood why things were that way.. @chrisradek Sorry for being a pain, but how do you close the S3 stream?\nWould this work (I don't think s3 has a destroy() method):\nfunction downloadFile(key,destPath) {\n    return new Promise((resolve, reject)=>{\n        const params = {\n            Bucket    : '/xxxx',\n            Key    : key\n        };\n        const s3Stream = s3.getObject(params).createReadStream();\n        const fileStream = fs.createWriteStream(destPath);\n        s3Stream.on('error', ()=>{\n            fileStream.destroy();\n            reject();\n        });\n        fileStream.on('error', ()=>{\n            s3Stream.destroy();\n            reject();\n        });\n        fileStream.on('close', resolve);\n        s3Stream.pipe(fileStream);\n    });\n}\n. ",
    "deejbee": "@KidA001 I'm confused with the example you gave:\n```\nconst response = await s3.getObject(params, (err) => {\n    if (err) {\n      // handle errors\n    }\n  });\nreturn response.Body.toString(); // your file as a string\n```\nsince the aws docs seem to suggest it returns an AWS.Request object which must be set up to handle the relevant events.\nDoes the example you mentioned refer to a very recent version of the api or is there something i'm not understanding?\n. ",
    "DrMiaow": "The issue is the mixing of Promise and stream semantics. A Promise must resolve() or reject(reason). A stream can fail at any point for any number of reasons, even if a stream is intially returned.\nIf you have a Promise that returns a stream, you still need to monitor it for errors outside the context of the promise.\nSo, my approach to resolving this is ...\nInside the promise...\ntry {\n  resolve(s3.getObject(params).createReadStream())\n} catch (err) {\n  reject(new Error(err.message))\n}\nWhen invoking the promise\n....then((stream) => {\n  stream.on('error', ()=>{\n    // Handle error            \n  })\n  // Do stuff   \n}\n. ",
    "a1anw2": "Is there anything special you need to do to your Lambda function to make this example at the top of the thread work?   There is nothing I can do to make this work.\nIt works beautifully when i run it locally via the serverless framework.\nNo matter what I do, this will simply not download a single byte when run as a Lambda function.\n```\nfunction downloadImage (s3Input) {\n  return new Promise((resolve, reject) => {\n    const destPath = \"/tmp/tabbedFile.csv\";\nconst s3Stream = S3.getObject(s3Input).createReadStream();\nconst fileStream = fs.createWriteStream(destPath);\ns3Stream.on(\"error\", reject);\nfileStream.on(\"error\", reject);\nfileStream.on(\"close\", () => { resolve(destPath);});\ns3Stream.pipe(fileStream);\n\n});\n}\n```\nI have attempted async/wait .. as well as .then().   So i guess if someone can give me a complete example of a Lambda of this running, that would help a lot!  thank you. ",
    "skipjack": "I stumbled on this thread as I was surprised no official promise support exists yet. However, in my case I didn't need a stream or to download the file locally. So here's slightly simpler option for those who just want a s3 getObject they can await:\njs\n/**\n * Get an object from a s3 bucket\n * \n * @param  {string} key - Object location in the bucket\n * @return {object}     - A promise containing the response\n */\nconst getObject = key => {\n    return new Promise((resolve, reject) => {\n        s3.getObject({\n            Bucket: process.env.BUCKET_NAME, // Assuming this is an environment variable...\n            Key: key\n        }, (err, data) => {\n            if ( err ) reject(err)\n            else resolve(data)\n        })\n    })\n}\nAnd a small usage example:\n``` js\nasync () => {\n    try {\n        // You'd probably replace 'someImage' with a variable/parameter\n        const response = await getObject('someImage.jpg')\n} catch (err) {\n    console.error(err)\n}\n\n}\n```\nHoping this package adds first-class support for promises soon though!. ",
    "greg-benner-klick-sensei": "I think it does offer promise support @skipjack as per https://github.com/aws/aws-sdk-js/issues/1436#issuecomment-375101246\nYou just add .promise to the s3. call\nSo you can just do \nawait s3.getObject({\n            Bucket: process.env.BUCKET_NAME, // Assuming this is an environment variable...\n            Key: key\n    }).promise(). actually seems to be better your way, the promise method they provide seems flaky. ",
    "DaveCollinsJr": "Anyone still looking at this... With Nodejs 8.10, this is a simple working version I've used to get an S3 object to the Lambda's /tmp directory:\nNodejs\n  const getObject = (handle) => {\n    return new Promise((resolve, reject) => {\n      s3.getObject(handle, (err, data) => {\n        if (err) reject(err)\n        else resolve(data.Body)\n      })\n    })\n  };\nThen:\nNodejs\nvar handle = {Bucket: root_bucket, Key: source_file};\nconst file_data = await getObject(handle);\n// file_data is the actual file data... You can fs.writeFile that to your /tmp directory\n. ",
    "rajeshdavidbabu": "\n@kaihendry You're mixing promises and streams. Why dont you just return a promise using the SDK using the promise() method?\njs\ns3.getObject(params).promise().then(...).catch(...)\n\nHow will you createAStream and close one in this approach ??. > @kaihendry You're mixing promises and streams. Why dont you just return a promise using the SDK using the promise() method?\n\njs\ns3.getObject(params).promise().then(...).catch(...)\n\nHow will you createAStream and close one in this approach ??. ",
    "johncmunson": "@KidA001 you forgot to chain .promise() onto s3.getObject. See below for the correction.\n\nFor anyone using async await:\n```js\nasync function getS3File(filename) {\n  const params = {\n    Bucket: 'some-bucket-name',\n    Key: filename,\n    Body: fileIO,\n  };\nconst response = await s3.getObject(params, (err) => {\n    if (err) {\n      // handle errors\n    }\n  }).promise();\nreturn response.Body.toString(); // your file as a string\n}\n```\n\n. ",
    "Deamoner": "Yep that works, wow spent so much time making my own presigned url function lol thanks . ",
    "kevincollins7": "Sure! From my package.json:\n\"aws-sdk\": \"^2.36.0\",\nI also tried 2.20.0 which is what I had originally. When it wasn't working I figured I'd update.\nI actually have tried just hardcoding my creds in as opposed to using the env variables and got the same result. I've tested both on Ubuntu and Windows. I've verified the variables were populated and could even console.log(ses) just to see what was there.\nI get the error when calling ses.SendEmail - which was working before.\nThe temporary credentials thing is a new one! I'm not entirely sure how to check? I've tried both with the credentials I had before and with brand new ones created on the SES console clicking the \"Create SMTP Credentials\" button.. @chrisradek I went ahead and created a key/pass for my user and tried it out and it worked! So strange I thought that I was using the SMTP creds before though. So I guess the right path to take is to create a new user as sort of a service account in our AWS and then use that user to send the emails.\nNot sure why I didn't think of that before. Thank you!. ",
    "parthrhce": "Great Help. . ",
    "kpotehin": "Still the same, @tierfire thank you for posting the correct way to pass AssumeRolePolicyDocument.. ",
    "charly3pins": "Thank you @chrisradek for your answer!. Thank you @chrisradek for your answer!. ",
    "jdilkes": "aws-sdk-2.36.0.min.js\nI can not see any other error in the console; the error message received is a pop-up on the screen.. The regions are the same.\nThe error is intermittent, and if I subsequently refresh the page it then loads correctly.\nI don't know how to get the full error, can you advise me how to do that?\n\nOn 7 Apr 2017, at 01:05, Christopher Radek notifications@github.com wrote:\nThere's no way to view the error in the debugger?\nIt's possible that the region you're using with your credentials isn't the same as the one your identity pool id is located in. If that's the case, you can pass the correct region in when constructing your CognitoIdentityCredentials https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityCredentials.html#constructor-property.\nIf you can get the full error, we might be able to help more with troubleshooting your issue.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/aws/aws-sdk-js/issues/1450#issuecomment-292369255, or mute the thread https://github.com/notifications/unsubscribe-auth/AAjr3PGs2VROWj8fE9kYaCTV7W8KIPjWks5rtX29gaJpZM4M1Rdp.\n\n\n. ",
    "ox42": "@jeskew If I want to implement that by myself for my needs, by editing ('m guessing) AWS.S3.ManagedUpload, I would probably just need to save the required data to local storage, and then fill information such as \"parts\", \"completeInfo\", and other such properties in ManagedUpload, right? Or is it a much bigger tasks and I'm missing something?\n(I'll be validating the file size, name, etc  myself, so I don't expect issues for me there. Because we probably don't have to support as many browsers as you guys do).. ",
    "SanderElias": "@jeskew I'm running into this also, did you do the testing as described above? \nOr perhaps there is something implemented in the main time?. @jasoneversole Can you reopen this? I stumbled on the same issue.  There is an ancient version of crypto-browserify (1.0.9) included in aws. This broke my build too.  ~Updating it to the latest version solved it for me, but I'm not entirely sure this is without consequences.~  \nUpdate: updating to the latest version breaks.\nthe curlpit is line 4 in rng.ks\nvar _global = this;\nIn strict mode the this will be undefined. \nWhen replaced by : \nvar _global = this || window || {};\nit will not error out anymore, but changing files in node_modules isn't very stable. (slight understatement there!). Sorry, I should have included that.\nI'm using V2.4.0\nI really think it's not a webpack issue. I debugged this, and it happens because this resource gets pulled in in strict mode. The same issue will start occurring once ES2015's modules become more prevalent.  The code will fail in strict mode because this is undefined instead of being the windows object.. @chrisradek found a/the solution. \nTook another look at my webpack config, as you could not reproduce, and decided to try to rename the '/node_modules/' to '${__dirname}/node_modules/'. \nThat did the trick. Somehow that triggered something in webpack.\nOn the other hand, a config like this is not that uncommon:\n```\nvar webpack = require('webpack');\nmodule.exports = {\n    entry: {\n        'main':    ${__dirname}/app/main.js,\n    },\ndevtool: \"source-map\",\n\noutput: {\n    path:     `${__dirname}/dist`,\n    filename: \"[name].bundle.js\"\n},\n\nmodule: {\n    loaders: [\n        {\n            test:    /\\.js$/,\n            use:     [\n                {\n                    loader: 'babel-loader',\n                    query:  {\n                        presets: ['es2015'],\n                    }\n                },\n            ]\n        },\n    ]\n}\n\n};\n```\nShould not cause this issue in my opinion. . ",
    "driverpt": "Do we have any feedback on this ? @jeskew . The purpose is the client uploads directly to S3 and use it as a Transient Store. After the client uploads it, the backend renames the file to it's checksum, in this case, when we're copying/renaming the object it gets a new checksum (ETag). I think it would be best if the SDK indicates in the callback if the file was uploaded via Multipart.. Hello, yes!\nDo you want me to apply your suggestion ?. ",
    "TroyWolf": "I am 58.17% through rolling my own resumable multipart upload in the browser solution. I am assuming the necessity to save not only the UploadId, but the PartNumber and ETag of all the uploaded parts. \nHas there been progress made to offer some built-in SDK help for resuming a multipart upload? I really like what @jeskew suggested:\n\n...provide a helper method that creates a state object from an upload ID (by calling ListParts and extracting the relevant data)\n\n@jeskew also pointed out the fragility of how to know the file the user selected is the exact same file they previously attempted to upload. I do plan to rely on a hash of file name, lastModified, and size. For my purposes and users, it will be a lesser-evil compromise to get the benefit of resumable file uploading.. > I have already posted the back-end code.\n\nThe front-end doesn't do anything especial, just a fetch with method PUT and passing the body binary buffer.\n\n@tomasdev , thank you very much for your back-end API example code!\nUnencumbered by facts, your back-end code suggests that for every individual part in the client, you make a call to your API to get a new signed URL. So seems you do have some relatively specific client-code beyond a standard file upload going on. As @shawnly suggested, it would be helpful to share it. \nI think what we all want, though, is a more direct support of S3 Multipart Upload in the browser using a pre-signed URL such that a single signed URL could be used for the entire upload process of a single file regardless of number of parts.\nA process that must make an API call to our own API between every chunk would surely slow the otherwise direct S3 upload way down. Kind of defeats some of the genius of \"direct upload to S3 from browser\".\nkudos also to @sandyghai for his work and sharing it with us.\n:coffee: . > That's not how multipart uploads work, you'd need authentication on each request.\nI understand that. Consider that today, the S3 Javascript SDK supports a multipart upload. It makes it very simple for the user. The user does not have to manage the individual parts--it's hidden in the SDK. But it only works with your actual credentials. The desire, therefore, is for the SDK to have a method that can do the same thing, but accept a pre-signed URL for the auth. If they wanted to, unencumbered by facts, AWS could support a signed URL that authenticates a single file ID and all it's parts.\nIn the meantime, I am going to try the approach used by you and sandyghai, but with a twist. My thought is to add a \"reqCount\" param to my custom API that is responsible for making the s3.getSignedUrl() call. I'll go into a loop and generate multiple signed URLs, adding 1 to the part number each time. This way, my API can, for example, do s3.createMultipartUpload(), and return 10 signed URLs to my client--one each for parts 1 - 10. This would cut down my API calls by a factor of 10.\nBetter yet, it would be trivial to use file.size to estimate how many parts will be needed. This would allow me to initiate the upload and return all signed URLs for all parts in a single request to my custom API.\nOf course once all parts are uploaded, I need to make an additional call to my custom API to do s3.completeMultipartUpload().\nWhat do you think of this approach?  :coffee: \n. ",
    "softprops": "thanks for replying @jeskew. I'm running using this in a google cloud function, which according to their docs should be\n\nThe Cloud Functions Node.js execution environment follows the Node \"LTS\" releases, starting with the v6 LTS release published on 2016-10-18. The current Node.js version running in Cloud Functions is Node v6.9.1.. \n",
    "villagebike": "I am having major issues with this in Lambda querying DynamoDB using a Promise. Not sure if it is helpful to you. \n2017-05-24T11:22:24.876Z    430188ef-4073-11e7-b337-efeb74a7aea9    { TypeError: Cannot read property 'push' of undefined\n    at Request.HTTP_DATA (/var/runtime/node_modules/aws-sdk/lib/event_listeners.js:321:34)\n    at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:673:14)\n    at IncomingMessage.onReadable (/var/runtime/node_modules/aws-sdk/lib/event_listeners.js:231:32)\n    at emitNone (events.js:86:13)\n    at IncomingMessage.emit (events.js:185:7)\n    at emitReadable_ (_stream_readable.js:432:10)\n    at emitReadable (_stream_readable.js:426:7)\n    at readableAddChunk (_stream_readable.js:187:13)\n  message: 'Cannot read property \\'push\\' of undefined',\n  code: 'TypeError',\n  time: 2017-05-24T11:22:24.876Z,\n  statusCode: 200,\n  retryable: false,\n  retryDelay: 87.05819427380794 }\n2017-05-24T11:22:24.877Z    430188ef-4073-11e7-b337-efeb74a7aea9    Unable to query the table. Error JSON: {\n  \"message\": \"Cannot read property 'push' of undefined\",\n  \"code\": \"TypeError\",\n  \"time\": \"2017-05-24T11:22:24.876Z\",\n  \"statusCode\": 200,\n  \"retryable\": false,\n  \"retryDelay\": 87.05819427380794\n}\n2017-05-24T11:22:24.894Z    430188ef-4073-11e7-b337-efeb74a7aea9    (node:1) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 1): TypeError: Cannot read property 'push' of undefined\n2017-05-24T11:22:24.895Z    430188ef-4073-11e7-b337-efeb74a7aea9    { TypeError: Cannot read property 'push' of undefined\n    at Request.HTTP_DATA (/var/runtime/node_modules/aws-sdk/lib/event_listeners.js:321:34)\n    at Request.callListeners (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/var/runtime/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/var/runtime/node_modules/aws-sdk/lib/request.js:673:14)\n    at IncomingMessage.onReadable (/var/runtime/node_modules/aws-sdk/lib/event_listeners.js:231:32)\n    at emitNone (events.js:86:13)\n    at IncomingMessage.emit (events.js:185:7)\n    at emitReadable_ (_stream_readable.js:432:10)\n    at emitReadable (_stream_readable.js:426:7)\n    at readableAddChunk (_stream_readable.js:187:13)\n  message: 'Cannot read property \\'push\\' of undefined',\n  code: 'TypeError',\n  time: 2017-05-24T11:22:24.876Z,\n  statusCode: 200,\n  retryable: false,\n  retryDelay: 87.05819427380794 }\n. ",
    "junketjim": "I have exactly the same as @villagebike - I'm using AWS SDK 2.67.0 and I get this issue when using a dynamodb query as a promise.  If I revert it back to a non-promise call then the function operates as per expected.  I have the issue both locally and running under AWS Lambda (node 6.11.0 locally and 6.3 on Lambda). ",
    "wshaver": "Similar issue, on listObjectsV2, the .promise() call sometimes wouldn't return at all, hanging completely. The errors seemed to not be reported to the reject/catch handler. I've converted my code to manually create a promise and use the callback function parameter for this method. If I try to pass a callback function, and call .promise() I see this error. Since removing .promise() I haven't had any issues other than lengthy code.. ",
    "xlc": "I am having similar issue with v2.190.0 on node v8.9.1. The promise API keep failing and change to callback API fixes the issue. ",
    "stevearc": "I hit this error as well, and upon digging in I found that it was because I was calling request.promise() twice. Turns out that promise() calls request.runTo, which does not play nice on multiple calls because it will cause the state machine to transition with the same state twice. If that state happens to be send, then it will hit the event listener for send twice. That will make two network requests with the same request & response object. Naturally, one will complete first and delete the buffers. When the second one completes, the buffers won't exist anymore and that's where you get the Cannot read property 'push' of undefined error.\nIt would be nice if there was some logic to prevent running the state machine twice, as this was not an easy bug to track down. Especially since I don't see anything in the docs that says \"btw if you call promise() twice you're taking a trip to crazytown\".\nI would also believe that there are cases where runTo() is called from one of the other locations and then calling promise() once is enough to hose you.. ",
    "ShawnOakley": "Thanks so much @stevearc.  This exactly describes the errors I've been seeing.  So, for instance I'm grabbing some items from s3, and using Promise.all to collect the items and do something with them in a lambda.  If I'm mapping over a collection of params items, feeding each of those into s3.getObject, then calling promise() on each of those to generate an array of promises for Promise.all, I'd run into this problem, correct?  . Thanks so much @stevearc.  This exactly describes the errors I've been seeing.  So, for instance I'm grabbing some items from s3, and using Promise.all to collect the items and do something with them in a lambda.  If I'm mapping over a collection of params items, feeding each of those into s3.getObject, then calling promise() on each of those to generate an array of promises for Promise.all, I'd run into this problem, correct?  . ",
    "jaylucas": "I am running into the same issue. Where I await on a Promise.all, curious if this is the same problem.. ",
    "mdobro": "Ran into the same issue using a promise that wrapped the promise from request.promise(). Issue was resolved by switching to callbacks. I'd recommend just using the callback and wrapping it in a promise to use this without issue until this is resolved.\nlet promise = new Promise((resolve, reject) => {\n      docClient.put(putParams, (err, data) => {\n        if (err) {\n          reject(err)\n        } else {\n          resolve(data)\n        }\n      });\n});. ",
    "MatthewGleeson": "I haven't been able to find any concise guide to setting up AWS sdk with DynamoDB and Passportjs. I have a sdk working with DynamoDB, but I'm having trouble figuring how to connect it to Passportjs(passport-local). ",
    "pjmolina": "Yes, I am using DocumentClient inside a Lambda Function. My code looks like:\n```\nlet aws = require('aws-sdk');\nlet dynamoDb = new aws.DynamoDB.DocumentClient();\nvar params = {\n        TableName: \"table1\",\n        Select: \"ALL_ATTRIBUTES\",\n        FilterExpression: \"app = :v\",\n        ExpressionAttributeValues: { \":v\": \"myApp\" }\n    };\nreturn dynamoDb.scan(params).promise();\n```\nThis code works filtering the collection as expected: Scanned 99, returned 10 items;\nBut changing line 8 to be:\nExpressionAttributeValues: { \":v\": { \"S\": \"myApp\" }  }\nScanned 99, returned 0 items.\n. Excuse me @jeskew I don't understand. If I am using DocumentClient why the enveloping is not working? . I see: double envelope. Thanks for the explanation. Much clear for me now. . ",
    "rianwouters": "Also see this AWS SDK extension: https://gist.github.com/rianwouters/17605cdb84a28ab59c89f2a46a7a36be\n. You may want to consider this extension: https://gist.github.com/rianwouters/17605cdb84a28ab59c89f2a46a7a36be\n(should make it an npm package one of these days). ",
    "ericb": "Thanks @jeskew - I've updated the docstring to mention both environments.. ",
    "freewil": "Would be nice to add this to the sdk. ",
    "attinderseesdragons": "I am running into the same issue. I can pull filtered logs via cli and also I get logs back when I provide stream names, but without stream name, there is nothing coming back. ",
    "dbburgess": "I ran into this. I'm guessing the results do contain a nextToken. That was the key in my case. I was searching with a specified startTime which was relatively recent. I have two identical environments (dev and prod). In dev, it was working fine. In production, it was returning this:\njson\n{\n  \"events\": [],\n  \"searchedLogStreams\": [],\n  \"nextToken\": \"big-long-token-was-here...\"\n}\nAs it turns out, it was related to the startTime and the amount of data (dev having a lot less). I noticed this because if I used a startTime further in the past, I'd get results. With no startTime, I'd get results even further in the past. I think it's due to the way aws searches the streams. It seems to be starting the search with the oldest stream and moving forward through time. There probably is a limit on how much data it will search before returning results. If it hasn't found any results by time it hits the limit, it will return an empty set with nextToken being a pointer to where it left off. If you search again, providing nextToken, it will continue the search there.\nDepending on the length of your log history, you may need to repeat this a number of times. After repeating this enough times, eventually, you will get the results. Of course, if your logs are rather lengthy, it probably is a much better idea to find the streams your data is most likely in, and specify those, rather than just mindlessly looping through all of them until you find the data you want.\nI hope that's helpful to someone! \ud83d\ude42 . ",
    "d10i": "@dbburgess YES! You are right! Thanks a lot, your message really helped fixing our issue :) It kind of makes sense after you pointed it out, we should have looked at the whole response...\nClosing.. ",
    "iyz1891": "Thank you for response, overlooked that. Much appreciated!. ",
    "pawansharma15": "Hi @chrisradek,\nI am calling in the following sequence AWS.config.update -> elb.describeLoadBalancers -> ec2. describeInstances. \nSo the config update is done once, right at the start and with the same config, calls to describe elb is successful but describeInstances fails, the policy attached to the user is the default AWS admin access policy via a group, also have tried directly attaching the policy to the user with same result.\nI have verified in the browsers' javascript console that the right set of credentials are passed, when I printed the whole request/response object to js console. Plus the same credentials could successfully describe the elbs.\nWhen I do not pass anythin to describe instance i.e. {} I get the same response HTTP 403.. Logs after setting AWS.config.logger = process.stdout\n[AWS elb 200 0.758s 0 retries] describeLoadBalancers({})\n[AWS ec2 403 0.515s 0 retries] describeInstances({ InstanceIds: [ 'i-049ce2e4ae3e364c3', 'i-07405b2211892801a', [length]: 2 ] }). Have anyone faced similar problem? Could anyone have a look, please?. Hi @chrisradek,\nMore information: \n- The response object does not contain the requestid. And there is no entry in AWS CloudTrail for the failed ec2 describe instances call.\n- AWS.config.systemClockOffset is 0\n- When I locally install the sdk and require the SDK like var AWS = require('aws-sdk'); instead of using the SDK Loaded by the browser <script type='text/javascript' src=\"https://sdk.amazonaws.com/js/aws-sdk-2.49.0.min.js\"></script> it worked. ",
    "ramonmulia": "I have this error too, I'm using the aws-sdk version: ^2.45.0. I used this library and solved my problem:\nhttps://www.npmjs.com/package/aws-config. ",
    "markathomas": "I'm having the same issue. ",
    "joncursi": "Same issue:\nW20170531-22:47:22.511(-4)? (STDERR) TypeError: this.extractCredentials is not a function\nW20170531-22:47:22.512(-4)? (STDERR)     at Object.Config (/Users/jcursi/Sites/joncursi/redbird-web/node_modules/aws-sdk/lib/config.js:275:20)\nW20170531-22:47:22.513(-4)? (STDERR)     at meteorInstall.imports.startup.server.accounts.js (imports/startup/server/accounts.js:86:7)\nW20170531-22:47:22.513(-4)? (STDERR)     at fileEvaluate (packages/modules-runtime.js:333:9)\nW20170531-22:47:22.514(-4)? (STDERR)     at require (packages/modules-runtime.js:228:16)\nW20170531-22:47:22.514(-4)? (STDERR)     at meteorInstall.imports.startup.server.index.js (imports/startup/server/index.js:3:1)\nW20170531-22:47:22.515(-4)? (STDERR)     at fileEvaluate (packages/modules-runtime.js:333:9)\nW20170531-22:47:22.515(-4)? (STDERR)     at require (packages/modules-runtime.js:228:16)\nW20170531-22:47:22.515(-4)? (STDERR)     at meteorInstall.server.main.js (server/main.js:3:8)\nW20170531-22:47:22.516(-4)? (STDERR)     at fileEvaluate (packages/modules-runtime.js:333:9)\nW20170531-22:47:22.516(-4)? (STDERR)     at require (packages/modules-runtime.js:228:16). I don't have the exact code since I eventually got it to work... I was trying to set a global config via AWS.config = { ... } which turned the error. I opted to set a local config on the S3 object instead:\njs\nconst AWS_S3 = new AWS.S3({\n  accessKeyId: AWS_ACCESS_ID,\n  region: 'us-east-1',\n  secretAccessKey: AWS_ACCESS_SECRET,\n});. ",
    "johnelliott": "I have the same issue, same line of code as the stack trace @joncursi mentioned.\nHere's how I'm making the call:\njs\nconst awsConfig = {\n  region: process.env.AWS_REGION,\n  accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n  secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\n  logger: console.log\n};\nconst config = AWS.Config(awsConfig);. ",
    "jarondl": "I ran into this too, and the problem is that you must run the constructor explicitly with new.\n@johnelliott , use:\nconst config = new AWS.Config(awsConfig);\n//instead of \nconst config = AWS.Config(awsConfig);\n\n. ",
    "arnold-parge": "AWS.config = {\n    region: '****,\n    accessKeyId: '****',\n    secretAccessKey: '****'\n};\nThis solved my problem!  \ud83d\udd7a. ",
    "northerncodemky": "Related to #1474  (circular dependencies). @chrisradek Thanks for fixing that one so quickly. Verified that resolves the issue we were facing and can now use 2.45.0.. ",
    "mlund01": "@andrewcharnley I recently ran into this issue, and I agree that s3.getObject({}).createReadStream() should be able to pipe errors to from getObject to createReadStream. However, I did come up with a workaround that satisfies my need to check for relevant errors (including invalid auth, invalid bucket, or resource doesn't exist) that will undoubtably come from getObject.\nHere is a my full route (using express),\n```javascript\nrouter.get('/:file', function(req, res, next) {\n    s3.listObjects({\n        Bucket: process.env.AWS_BUCKET,\n        Prefix: 'images/' + req.params.file\n    }, function(err, data) {\n        if (err) {\n            next(err);\n        } else if (data.Contents.length == 0 || data.Contents[0].Key != ('images/' + req.params.file)) {\n            next({status: 404, message: \"This image does not exist\"});\n        } \n        else {\n            var imgStream = s3.getObject({\n            Bucket: process.env.AWS_BUCKET,\n            Key: 'images/' + req.params.file\n          }).createReadStream();\n      imgStream.pipe(res);\n    }\n})\n\n})\n```\nWhat is unfortunate is that I have call to AWS twice, once to verify the object exists and no errors arise (albeit with little overhead as possible given the available endpoints in the aws-sdk), and the second to actually get the object given no errors.\nHope this helps!. ",
    "kevinbror": "@mlund01 In the code you give it is possible that between the listObjects call and getObject call the image is deleted. So while this may reduce the chance of a hanging stream, it does not solve the issue. . The docs here seem to indicate that when the callback is provided as follows, \nconst AWS = require('aws-sdk');\nconst s3 = new AWS.S3({ apiVersion: 'latest' });\nconst params = {Bucket: 'bucketName', Key: 'keyName'};\ns3.getObject(params, (err, data) => { ... });\nthat data.Body may be a readable stream. In my experiments however it always seems to be a buffer. Is there a way to configure data.Body to be a stream? Were that the case it seems that we could bypass the issue with swallowed errs. . @jeskew Thanks for clarifying. Thanks for the reply. I had an invalid signature error a few days ago and I add the full error object to my logs. I see requestId there but not the extendedRequestId. Will that not be present in some cases (like auth failure)?. It's just the sample config: \n<!-- Sample policy -->\n<CORSConfiguration>\n    <CORSRule>\n        <AllowedOrigin>*</AllowedOrigin>\n        <AllowedMethod>GET</AllowedMethod>\n        <MaxAgeSeconds>3000</MaxAgeSeconds>\n        <AllowedHeader>Authorization</AllowedHeader>\n    </CORSRule>\n</CORSConfiguration>\nBut should a signature error still have the extendedRequestId? (regardless of the cause of the signature verification failure). Great thanks!. ",
    "jfromaniello": "Glad to help! This is the best SDK I have ever used \u2764\ufe0f . Glad to help! This is the best SDK I have ever used \u2764\ufe0f . ",
    "meyerbro": "aws-sdk v2.22.0\nNode v6.10.1. aws-sdk v2.22.0\nNode v6.10.1. ",
    "sgnn7": "@chrisradek Agh! I should have read the docs better though I did expect auth problems to result in a throw of some sort which is why I thought it was an unintended bug! Thanks!. @chrisradek Agh! I should have read the docs better though I did expect auth problems to result in a throw of some sort which is why I thought it was an unintended bug! Thanks!. ",
    "gsanj": "Hi all \nI am also getting same issue very first time/load only\nhttps://s3.amazonaws.com/\nSecond time onward it is working fine like: \nhttps://bucketname.s3.amazonaws.com/flowers.png?AWSAccessKeyId=ASIAINQI3VXHA22H...\nMy code is:\nvar params = {Bucket: 'bucket name, Key: 'file path', Expires:'3600' };\nvar url = s3.getSignedUrl('getObject', params);\nLast one day I am trying to find out root cause but not get success, please provide some expert opinion  why this is happening first time only and what is the solution?\nI am using Node.JS\n. ",
    "DrLongGhost": "Thanks for this!. Thanks for this!. ",
    "matsaleh13": "Thanks for the fast reply @chrisradek!\nI added console output for the two bits of information. (FWIW I used util.inspect() instead of JSON.stringify() because IMO it's easier to read). \nAWS_SDK_LOAD_CONFIG is not set.\nHere's the top-level originalError property, with another one nested:\nshell\n  originalError:\n   { message: 'Could not load credentials from any providers',\n     code: 'CredentialsError',\n     time: 2017-04-25T21:05:42.013Z,\n     retryable: true,\n     originalError:\n      { message: 'Connection timed out after 1000ms',\n        code: 'TimeoutError',\n        time: 2017-04-25T21:05:42.013Z,\n        retryable: true } } }\nHere's the full output for completeness:\nshell\n\u03bb node aws\\loadtestctl.js status\nGetting status of LoadTester ECS tasks.\nAWS_SDK_LOAD_CONFIG = undefined\nError from listTasks(): { CredentialsError: Missing credentials in config\n    at ClientRequest.<anonymous> (C:\\Dev\\GitHub\\vcme\\cq-loadtest\\node_modules\\aws-sdk\\lib\\http\\node.js:83:34)\n    at ClientRequest.g (events.js:291:16)\n    at emitNone (events.js:86:13)\n    at ClientRequest.emit (events.js:185:7)\n    at Socket.emitTimeout (_http_client.js:620:10)\n    at Socket.g (events.js:291:16)\n    at emitNone (events.js:86:13)\n    at Socket.emit (events.js:185:7)\n    at Socket._onTimeout (net.js:339:8)\n    at ontimeout (timers.js:365:14)\n    at tryOnTimeout (timers.js:237:5)\n    at Timer.listOnTimeout (timers.js:207:5)\n  message: 'Missing credentials in config',\n  code: 'CredentialsError',\n  time: 2017-04-25T21:05:42.014Z,\n  retryable: true,\n  originalError:\n   { message: 'Could not load credentials from any providers',\n     code: 'CredentialsError',\n     time: 2017-04-25T21:05:42.013Z,\n     retryable: true,\n     originalError:\n      { message: 'Connection timed out after 1000ms',\n        code: 'TimeoutError',\n        time: 2017-04-25T21:05:42.013Z,\n        retryable: true } } }\nHope this helps, and thanks!\n. OK, tried the above, but logging AWS.config.credentials was null.\nAttempted to log what appears to be related properties:\nAWS.config.credentials: null\nAWS.config.credentialProvider: CredentialProviderChain {\n  providers: [ [Function], [Function], [Function], [Function] ] }  // via util.inspect()\nAWS.config.credentialProvider (contents): {\"providers\":[null,null,null,null]}  // via JSON.stringify()\nSeems like it can't load from any provider because it has none.\nOther info:\n FYI as I mentioned in my OP, my creds are configured via aws configure, and I've verified they are valid.\n Other node.js tools that use aws-sdk do still work (e.g. cwtail. Note: my version of cwtail uses aws-sdk@2.36.0)\n* I can switch back and forth between aws-sdk@2.43.0 and aws-sdk@2.44.0 repeatedly and recreate the working (2.43) and non-working (2.44) cases 100% of the time.\nHope this helps.\n. I just realized I neglected to mention that I'm running all this on Windows 10. Sorry for that. \nAlso, I have a HOME env var set that maps to C:\\Users\\Matthew Walker\\OneDrive\\Home (so that I can share config stuff between systems). \nFrom git bash, the $HOME folder resolves to /c/Users/Matthew Walker/OneDrive/Home (as expected).\nAlso, from git bash, the ~ resolves to the same path as $HOME (as expected).\nNow, to answer your questions above:\nLogging AWS.config.credentials.constructor: when using aws-sdk@2.43.0 shows I am indeed using SharedIniFileCredentials:\n```shell\nAWS.config.credentials.constructor: function SharedIniFileCredentials(options) {\n    AWS.Credentials.call(this);\noptions = options || {};\n\nthis.filename = options.filename;\nthis.profile = options.profile || process.env.AWS_PROFILE || 'default';\nthis.disableAssumeRole = !!options.disableAssumeRole;\nthis.get(function() {});\n\n}\n```\nIn my $HOME folder I do have an .aws folder with contents (via git bash):\nbash\n[Matthew Walker Orig@pearl:~]\n$ ls -la ~/.aws\ntotal 10K\ndrwxr-xr-x 1 Matthew Walker Orig 197609   0 Nov 30 11:13 ./\ndrwxr-xr-x 1 Matthew Walker Orig 197609   0 Apr  9 11:44 ../\n-rw-r--r-- 1 Matthew Walker Orig 197609  46 Nov 30 11:16 config\n-rw-r--r-- 1 Matthew Walker Orig 197609 119 Nov 30 11:13 credentials\nThe contents of config are:\nconf\n[default]\nregion = us-east-1\noutput = text\nThe contents of credentials are:\nconf\n[default]\naws_access_key_id = <MY_ACCESS_KEY_ID>\naws_secret_access_key = <MY_SECRET_ACCESS_KEY>\n. No, I'm not using the aws-cli through git-bash, and I don't run my node.js tool from there either. I mainly use git-bash to get better UX for navigating the file system and to access the necessary unixy tools ;).\nI have only tried Win10 bash (bash on Ubuntu on Windows) as an experiment. I don't use it for anything useful because last I heard you can't exec windows programs using it, and all my work is still Windows-based.\nNode:\nshell\n\u03bb node -v\nv6.9.1\naws-cli (in case...):\nshell\n\u03bb aws --version\naws-cli/1.11.80 Python/2.7.9 Windows/8 botocore/1.5.43\nThank you for your prompt responses and guidance! Happy to team with you on this.\nSorry I have such an awkward, hybridy environment! Um.. okay actually I'm not. I take pride in my ability to generate edge cases with simply a blink. :)\nCheers!\n. @clark-pan and @jeskew thanks for digging into this.\nI set the environment vars mentioned above in the windows shell using:\nshell\nset AWS_SDK_LOAD_CONFIG=1\nset AWS_SHARED_CREDENTIALS_FILE=%HOME%/.aws/credentials\nset AWS_CONFIG_FILE=%HOME%/.aws/config\n(Apologies to all for the bastardized blending of OS shell conventions.)\nAfter that, my node.js program worked; the CredentialError no longer occurs!\nSo, I guess the current ~/.aws/ folder and its contents were initialized via aws configure using an older version of the aws-sdk (around Nov 2016). That would have been the hard-coded implementation I guess, which would have resolved fine because of my nonstandard use of $HOME on windows.\nHowever, because of that nonstandard setup, the change in v2.44.0 to use os.homedir broke my runtime. IMO it's very reasonable to trust os.homedir to do \"the right thing\" on any platform. And, it did, since I'm really running this code on windows in a standard windows shell $HOME env var notwithstanding). \nI guess deciding how to support $HOME on windows is kind of a crap shoot. Although, it might be reasonable to assume that, if present, $HOME would have been added explicitly and should take precedence. I'd bet that whichever way you go will cause problems for someone though. \nMight be useful to present the user a message from aws configure about the nonstandard presence of $HOME on Windows and ask what they want to do, but that doesn't help after the fact, of course. \nAlso might be good to add a note about it to the docs, but they're so deep already that it's likely to be missed by the reader. :(\nFWIW, while troubleshooting this I did reinstall awscli to aws-cli/1.11.80 Python/2.7.9 Windows/8 botocore/1.5.43, and re-ran aws configure, and that didn't change anything. Perhaps that version of the CLI doesn't yet use aws-sdk@2.44.0? (edit: of course not, it's not a node.js app, doh!)\nAt any rate, thanks for the info and guidance in this! \nCheers.\n. ",
    "clark-pan": "@matsaleh13 Dug through source code and figured out the issue.\nBased on your configuration, you need to set AWS_SDK_LOAD_CONFIG=1, AWS_SHARED_CREDENTIALS_FILE=~/.aws/credentials and AWS_CONFIG_FILE=~/.aws/config.\nPrior to 2.44.0, the ~/.aws/credentials value was hardcoded into the SDK. #1391 added support for ~/.aws/config but also removed the hardcoding of these two values.\nThis isn't an edge case tbh, anybody who uses 2.44.0 will be hitting this issue. You are just early to the party :). ",
    "JarLowrey": "For me the issue was I put .aws/credentials in my project directory when it should have gone in ~/.aws/credentials. The error shows the full path of where the credentials are expected. Commenting here for people googling a similar question.. ",
    "mscottx88": "For those unfortunate souls that might find themselves in this same predicament, here is what I discovered.\nI had my credentials in the right spot ~./aws/credentials.\nI also had a profile [default] with my keys.\nBut I also had an environment variable AWS_PROFILE set to my actual profile.\nTurns out, the SharedIniFileCredentials loader will pick the stanza from the credentials file whose name matches the AWS_PROFILE environment varialble, if present.\nSo, if your AWS_PROFILE is elite.programmer but your credentials file only has a [default] stanza, copy the [default] stanza and rename it to elite.programmer.\n```\nprovided by aws configure\n[default]\naws_access_key_id = \naws_secret_access_key = \nwill be chosen through AWS_PROFILE environment variable\n[elite.programmer]\naws_access_key_id = \naws_secret_access_key = \n```. ",
    "peterdev6": "@chrisradek I am using aws-sdk-mobile-analytics-js to create a mobileAnalyticsClient and I am sending custom events like this: mobileAnalyticsClient.recordEvent('MainPageLoaded', {\n     'attribute_1': 'main',\n     'attribute_2': 'page'\n    }, {\n     'metric_1': 1\n    });\nAccording to aws android sdk, I also added \n<uses-permission android:name=\"android.permission.INTERNET\" />\n<uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\" /> to AndroidManifest.xml file.. @chrisradek Thank you so much for your help. It works perfectly now.. @spoeck I followed the installation instructions of aws-sdk-js react-native branch but I didnt install aws-sdk-mobile-analytics library via npm, instead I extracted the source files and used them locally coz I had trouble importing it and I could not prevent typescript complaining about node type errors(i'm using typescript for type checking instead of flow).\nCan you show your error messages and explain how you are using these two libraries?. @spoeck  you need to specify in the options the platform for data to display on aws dashboard and logger for logging. Here is an example:\nconst options = {\n    appId : '...',\n    platform : Platform.OS === 'ios' ? 'iPhoneOS' : 'Android',\n    logger: console\n}\nAlso you don't need to call submitEvents manually, mobileanalyticsclient is submitting events automatically.\nNote that you may need to change the imports of some of the source files for aws-sdk-mobile-analytics library. This is the structure of all the source files i have locally.\n\n. @spoeck Are you getting any errors now?. ",
    "alvelig": "@chrisradek How can adding header to AWS.Lambda can be achieved?\nI've tried:\nlambda\n  .invoke(params, callback)\n  .on('build', function(req) { \n     req.httpRequest.headers['Content-Type'] = 'application/json'; \n  })\nBut this seems to me an unstable solution.\nCould you please give me an idea?\nUPD:\nI found a workaround, but is there a better way?\n```\nimport AWS from 'aws-sdk/dist/aws-sdk-react-native';\nAWS.util.update(AWS.Lambda.prototype, {\n    setupRequestListeners: function setupRequestListeners(request) {\n        request.httpRequest.headers[\"Content-Type\"] = \"application/json\";\n        console.log(request);\n        if (request.operation === 'invoke') {\n            request.addListener('extractData', AWS.util.convertPayloadToString);\n        }\n    }\n});\n```. @chrisradek Android only. The same symptoms as stated above. \nI'm not so deep in the library. Maybe that happens because I use https://github.com/AirLabsTeam/react-native-aws-cognito-js (Advanced example from README.MD).\nIf you need, this weekend I'll try to make an example reproducing the error.. ",
    "ccmoralesj": "I've search all over the issues here and I found that it could be something with the time of the request... It seems that the kubernet server has its instances time unfolded. I don't know what else to look up.\nI read about the request time on issue 221. Kubernetes runs an internal NTP, it seems, but it didn't work, so we ran an NTP from the main server but it didn't work either. So we didn't know what to do.... But, the thing is, the problem lies there (the request time that throws the 403)? it's our suspicion.. I have to add something. Yesterday we ran just one pod and everything works pretty fine, but we scale it to two pods and we got the 403 again. We don't know why it works with  just one pod.. We load credentials from environment variables provided by our .env file, which is the same for all kubernetes pods.\nBut, as I already said, something rare is happening, because if we run the server with just one kubernete replicaset instance (pod), everything works pretty fine and we never get the 403, but if we scale it to two replicasets or more, then we get 403 again, and each replicasets take the credentials from the same static file, that is actually on each replicaset.. I contacted to our hosting provider, they fixed something (don't know what) on the main server and they said: \"the time is now sync, run your kubernete again\" ... We ran it with the four instances... IT WORKED!\nThanks a lot for the replies and the help. If something happens I will open the issue again. . ",
    "brianomchugh": "@jeskew \nI am using version 2.49.0, and am using the react-native distributable ('aws-sdk/dist/aws-sdk-react-native').\nI am experiencing this on iOS (EDIT: it seems to be working fine on Android). ",
    "jsochen": "if someone have a dome \uff0c please tell me\uff0c Grateful\uff01. thanks , I'll try again with you suggest\u3002. ",
    "humusdigital": "Hi, I'm following this posts and I have the same problem and I will like to know if you can set correctly Cognito credentials to run the app into pinpoint or what was the correct configuration or code to connect your ionic app with pinpoint to received pushnotifications. thanks.. ",
    "omkardusane": "Alternative : createPresignedPost \nexplained in\nhttp://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#createPresignedPost-property \n. ",
    "jasoneversole": "Never-mind, this seems to have been an issue with my webpack config.\nI needed to add the following:\nexclude: /node_modules/,\nThanks.\n. ",
    "timotgl": "@SanderElias thanks this helped us fix the issue. We monkey-patched rng.js with a patched version (the one line you suggested) in the postinstall script.. @SanderElias thanks this helped us fix the issue. We monkey-patched rng.js with a patched version (the one line you suggested) in the postinstall script.. ",
    "ericdcobb": "Still having this issue today, and I'm not sure why this and #1566 are closed?\nLike @timotgl we had to apply a monkey patch.\nSome notes for anyone finding themselves here having to do the same thing:\n\nin case it's not clear, the file you want to patch is in crypto-browserify\nI found patch-package very useful for this.\n\nHere is my patch:\n```\npatch-package\n--- a/node_modules/crypto-browserify/rng.js\n+++ b/node_modules/crypto-browserify/rng.js\n@@ -1,7 +1,7 @@\n // Original code adapted from Robert Kieffer.\n // details at https://github.com/broofa/node-uuid\n (function() {\n-  var _global = this;\n+  var _global = this || window || {};\nvar mathRNG, whatwgRNG;\n``. @chrisradek I was using 2.282.1, I believe, but also usingamazon-cognito-identity-js` from aws-amplify. I'll admit I'm not 100% clear on how the two projects are related, but IIUC I needed amplify because I'm doing react native, so it could be that amplify is the problem and not the aws-sdk - it's just bad googling that brought me here.\n. ",
    "matrunchyk": "I think it should be reopened. Patching files (even ad-hoc) is a good solution but would be great if you guys can fix the real problem here.. When are going to solve it?. @SanderElias why Closed?. @chrisradek this is what we use:\n\"amazon-cognito-identity-js\": \"^2.0.20\",\n\"aws-sdk\": \"^2.292.0\",\nShould we use both or aws-sdk only? We don't use React (we use Vue), and we don't use MobileHub.. ",
    "Ch3ck": "I some how cannot find the full source for this package so as to apply this patch. Another fix will be to change the crypto-browserify to amplify-crypto-browserify. Looking at the package json I couldn't find this package. ",
    "ddwp99": "Hey @chrisradek thanks for the question. I was able to resolve this issue, and posted an answer to my question originally posted here.\nAs it turns out, the script was spending most of its time re-establishing the SSL handshake on each Polly request (As confirmed by CPU profiles sampled with Chrome DevTools). To resolve this issue, ~~the module agentkeepalive~~ the build-in module https was used. \nTo use https in the context here, just do this:\nconst AWS = require('aws-sdk')\nconst httpsAgent = require('https').Agent\nconst keepAliveAgent = new httpsAgent({maxSockets:1, keepAlive:true})\nAWS.config.update({\n    httpOptions: { agent: keepAliveAgent }\n});\nIf maxSockets is not set to 1, then Polly will still attempt to establish SSL on each available socket until they are all kept alive. Once the sockets are kept alive - the latency between callbacks are reduced to ~90 ms on the RPi 3 - this is much better than the ~15000 ms that I was getting before!. Thanks Chris! I've edited the comment above to use the build-in https module instead.. ",
    "gridplus-jenkins": "Ping on this. Would be great to have the ability to specify a filename for glacier and have the client manage the process. . ",
    "shiva151": "Hello,@chrisradek \nI am using aws-sdk@2.32.0, and I am not getting that error frequently, sometimes it's working perfectly. actually am not able to figure it out, when the problem occurring, am just pushing one domain to aws server to get the nameservers of that domain, sometimes it's working perfectly and sometimes not.\nHope U will help me in this.. ",
    "dtyrrell": "@chrisradek after seeing this I updated (I'm now at v2.57.0) and I can confirm that it now works.\nI updated my work machine earlier today (before 2.57.0 was released), and still had this issue, when I'm back in tomorrow I'm curious to see what version that box is running.\nThanks again. Checked the AWS-SDK verson on mywork box, confirmed the version was below 2.55.0 as you suspected.\nAppears I failed to update it in the right way :)\nThanks again.. ",
    "adenhertog": "@chrisradek \nYep thanks, I did mean to refer to LexModelBuildingService :)\nWhen I look at https://raw.githubusercontent.com/aws/aws-sdk-js/master/dist/aws-sdk.js I still can't find that service being exported anywhere? I figured it should be somewhere like:\nmodule.exports = {\n   ...\n   LexModelBuildingService: require('./lexmodelbuildingservice'),\n   LexRuntime: require('./lexruntime'),\n   ...\n}\nWhen I do this in TS, I'm getting LexModelBuildingService is undefined:\nimport { LexModelBuildingService } from 'aws-sdk'\nconsole.log(LexModelBuildingService)\nThis is all with v2.57.0\nEdit: some more context, this is being consumed in a front-end application. Angular 2 using webpack. @chrisradek \nThat makes sense, thanks for the info :+1: . ",
    "westpole": "Thank you @jeskew for an information.. Thank you @jeskew for an information.. ",
    "rfmauri": "Any updates on when this might be available?. ",
    "iurisilvio": "I agree. Other falsy values must raise error too. Your PR works for me.. It works for me. :+1: . ",
    "cameck": "Hmm that's interesting and I get your point. \nMaybe it's not such a big deal, but I think there should be at least a little validation.\nI can literally pass the unicorn emoji (\ud83e\udd84  ) and still get a successful response.. hehe, that's true it is UTF-8. ",
    "ChristophRob": "Thats it! Thank you very much!! . @MrHubble Yes of course, i also added how i downloaded my pics (with Buffer), if that helps you, too. Well in my solution i am not using the feature that an individual user has some priviliges. I just use the IdentityPool for everyone.\nBut maybe this new Starter Kit helps you more:\nhttps://github.com/awslabs/aws-mobile-react-native-starter\nHere my code:\nconst AWS = require('aws-sdk/dist/aws-sdk-react-native');\n\nvar Buffer = require('buffer/').Buffer\nimport { Image } from 'react-native';\n\nexport const REGION = 'eu-central-1'\nexport const BUCKET_NAME = \"bucketname\"\n\nAWS.config.update({region: REGION});\nAWS.config.credentials = new AWS.CognitoIdentityCredentials({\n  IdentityPoolId: 'eu-central-1:xxxxx',\n});\n\nconst s3 = new AWS.S3({\n  apiVersion: '2006-03-01',\n  params: {Bucket: settings.BUCKET_NAME}\n});\n\nlet picParams = {\n  Bucket: settings.BUCKET_NAME,\n  Key: \"pics/1.png\"\n}\n\nexport const lambda = new AWS.Lambda({region: REGION, apiVersion: '2015-03-31'});\n\nconst getPic = (key, callback) => {\n  picParams.Key = \"pics/\" + key\n  s3.getObject(picParams, function(err, data) {\n    if (err) {\n      console.log('There was an error getting a pic: ' + err.message);\n    } else {\n      const buffer = Buffer.from(data.Body);\n      const base64ImageData = buffer.toString('base64');\n      const imgSrc = \"data:image/png;base64,\" + base64ImageData;\n      Image.getSize(imgSrc, (width, height) => {\n        callback({\n          picId: key,\n          picData: imgSrc,\n          picHeight: height,\n          picWidth: width,\n        });\n      });\n    }\n  }.bind(this))\n}\n\nexport { getPic };.\n",
    "caub": "Thanks, so I should instantiate one AWS.S3  object, or two?\nI tried multiple ways, the objects test/hello1,test/hello2,... test/hello7 exist on both bucket foo and bar, but copyObject returns either 'AccessDenied: Access Denied' or 'All access to this object has been disabled' if I just try s3Foo.copyObject({CopySource: 'test/hello5', Key: 'test/hello33'}) in the same bucket.\n```js\nvar s3Foo = new AWS.S3({\n    apiVersion: '2006-03-01',\n    params: {Bucket: 'foo'},\n    accessKeyId: '.....', \n    secretAccessKey: '.....'\n});\nvar s3Bar = new AWS.S3({\n    apiVersion: '2006-03-01',\n    params: {Bucket: 'bar'},\n    accessKeyId: '.....', \n    secretAccessKey: '........'\n});\ns3Foo.copyObject({\n    CopySource: encodeURIComponent('bar/test/hello5'), // CopySource \u2014 (String) The name of the source bucket and key name of the source object, separated by a slash (/). Must be URL-encoded.\n    Key: 'test/hello33',\n}, function(err, data) {\n    console.log(err, data);\n})\n```. Perfect, that's what I was wondering, I'll try to see if I can manage both with one account or do the streaming way\nThanks very much. ",
    "hminaeeBrunswicknews": "ooops right thanks . ",
    "darrenhgc": "@kbariotis\nThe solution I've settled on for this is to require the aws-sdk and then, using sinon, stub out the lower level makeRequest call of the aws.Service.prototype. Essentially a switch statement on the operation parameter passed to makeRequest. Then use the modified aws mock in your proxyquire to mock the aws service\n```javascript\nlet aws = require('aws-sdk')\nlet sendEmailStub = sinon.stub(aws.Sevice.prototype, 'makeRequest', (op, params, cb) => {\n  // in the case of sending emails\n  if (op == 'sendEmail') return callback(null, true)\n})\n// maybe in your beforeEach()\nlet email = proxyQuire('file', {\n  aws-sdk: aws\n})\n```\nI want to believe that there is a better solution so I'd love to hear your feedback if you try this out. . You should be able to add a WHERE clause to your rule's SQL statement that checks the metadata.desired.temp.timestamp against the metadata.reported.temp.timestamp. Keep in mind that your shadow may be structured differently than the example I provided.. ",
    "arian-kh": "Thanks Chris, appreciate it! I didn't know you can access the httpResponse from the 'this' context.. ",
    "fwhite-wsm": "This is also a problem for our team.. This is also a problem for our team.. ",
    "navaneetharaopy": "System Info : 4.9.27-14.31.amzn1.x86_64 #1 SMP Wed May 10 01:58:40 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux (Amazon AMI OS)\nRAM usage before process start:\n\nRAM usage during process \n\nThere are no multiple file download. Only single file of size 1.5GB is piped into ec2 disk from s3.\n. ",
    "benishak": "@chrisradek \nthis doesn't work put and update \nI also get same error message doing it from the AWS Console  when inserting or updating an item. @jeskew \nI'm ok with the workaround, actually the date object with __type comes from another library/api, so I cannot change it before it lands on my callback. I had to catch it, rename it and save it and when my application read it from dynamodb I must convert it back. \nI don't see any reason why this keyword can interfere with the response message of the error handling.\nsupposing that message also included key such ok or errorCode, this shouldn't mean at all that I cannot save object with these kewords, or am I wrong?\nThe error message is also misleading and doesn't tell anything about the cause\nSerializationException No assembly name on stack\nI was just lucky I was trying to save this particular single object, but if my object contained more attributes, then I will be clueless about the cause.. @jeskew \nfor some reasons while testing with DynamoDB Local, I didn't get that error and I was able to save my custom Date object with the attribute __type\n```\n@test.only 'handles updating a single object with array, object date'(done) {\n        const adapter = $;\n        const objectId = randomString(10);\n        const className = randomString(7);\n    const schema = {\n        fields: {\n            array: { type: 'Array' },\n            object: { type: 'Object' },\n            date: { type: 'Date' },\n        }\n    };\n\n\n    adapter.createObject(className, schema, { objectId: objectId })\n    .then(() => adapter._rawFind(className))\n    .then(results => {\n        expect(results.length).to.be.equal(1);\n        const update = {\n            array: [1, 2, 3],\n            object: {foo: 'bar'},\n            date: {\n                __type: 'Date',\n                iso: '2016-05-26T20:55:01.154Z',\n            },\n        };\n        const query = { objectId: objectId };\n        return adapter.findOneAndUpdate(className, schema, query, update)\n    })\n    .then(results => {\n        const mob = results;\n        expect(mob.array instanceof Array).to.be.equal(true);\n        expect(typeof mob.object).to.be.equal('object');\n        expect(mob.date.__type).to.be.equal('Date');\n        expect(mob.date.iso).to.be.equal('2016-05-26T20:55:01.154Z');\n        return adapter._rawFind(className);\n    })\n    .then(results => {\n        expect(results.length).to.be.equal(1);\n        const mob = results[0];\n        expect(mob.array instanceof Array).to.be.equal(true);\n        expect(typeof mob.object).to.be.equal('object');\n        expect(typeof mob.date).to.be.equal('object');\n        expect(mob.date.__type).to.be.equal('Date');\n        expect(mob.date.iso).to.be.equal('2016-05-26T20:55:01.154Z');\n        done();\n    })\n    .catch(error => {\n        console.log(error);\n        expect(error).to.be.undefined;\n        done();\n    });\n}\n\n``. was my mistake, everything is ok. if something like this doesn't exist, I would like to see this market as feature request. (dry-run-or something in this context)\nof course if this is even possible\n. @jeskew thats true, it was my mistake actually what I was looking for is to use the IN operator just like in MongoDB with$in` \nAnother thing why does the following query not work when applied on a set/list ?\nSuppose I have an Item with Attribute as array\n{\n   myList : [1,2,3,4]\n}\nWhen I try to find this Item using this query\nFilterExpression : \"#mylist = :list\"\nExpressionAttributeValues: { \":list\" : [3,4,1,2] }\nIn MongoDB same query will return that Item\n{ myList : [3,4,1,2] }\nor\n{ myLIst : { $in : [3,4,1,2] }\nor\n{ myLIst : { $all : [3,4,1,2] }\nbut in DynamoDB in order that this works they must be in the same order so the following will be working just fine\nFilterExpression : \"#mylist = :list\"\nExpressionAttributeValues: { \":list\" : [1,2,3,4] }\nas workaround for now, before inserting I check for possible List attributes, sort them, and query them with the a sorted array, so if my query included the list [3,4,1,2] I will make it looks like this [1,2,3,4] to match the attribute in the item.\nOr do you know a better trick ?\nIt is really werid that AWS stopped adding any new features to DynamoDB and the API still at version 2012\n. ",
    "mrtcode": "Hello @jeskew,\nIt seems that this error occurs only when enough data is sent from server. It probably happens when internal buffers are full and time out error occurs. I think you need to find the actual reason why data is written to stream even after error. There can be more problems related to this.. ",
    "dbook-kkbrandt": "Update on this: I hadn't encountered for over a week, and ran into it again last night, after killing the docker image manually (ssh + docker kill), then attempting to call ecs.runTask(). \nI'm guessing this only happens when ECS hasn't fully registered that a change has occurred, so basically if I try to do something too quickly. \nI consider it a bug that the task starts even though the SDK returns an error.\nIt would also be nice if the error messages were more fine-grained and descriptive.. @natan-hoppe \nTo be sure, does your task stay running after you get that error? Is there any more information that comes with that errorMessage?\nThe code you've posted wouldn't quite work.\nWhat you would want is something like:\n```\n...\nreturn this.ecsSdk.waitFor('tasksRunning', params).promise()\n  .then(data => {\n    console.log(data);\n    return 0;\n  })\n  .catch(e => console.error(e, e.stack));\n````. ",
    "natan-hoppe": "I'm stumbling upon this problem consistently. A bit of context, though:\nNode: 6.10.1\nSDK: 2.93.0\nFirst I call SDK's runTask with the task definition I want to run and the ECS cluster where to run it.\n```\nlet params = {\n  cluster: \"XYZ\",\n  taskDefinition: \"MyTask:1\"\n};\nlet runTaskPromise = this.ecsSdk.runTask(params).promise();\nrunTaskPromise.then(data => {\n  let params = {\n    cluster: this.cluster,\n    tasks: [ data.tasks[0].taskArn ]\n  };\nthis.ecsSdk.waitFor('tasksRunning', params).then(err => {\n      if (err) {\n        console.log(err, err.stack);\n      } else {\n        return 0;\n      }\n  });\n});\n```\nThen, when I execute the code and try to run the desired task for the first time, I get the error:\n{\n  \"errorMessage\": \"2017-08-01T16:51:53.599Z <TASK_ID> Task timed out after 3.00 seconds\"\n}\nAccording to the latest JavaScript SDK documentation, AWS.ECS: \n\nWaits for the tasksRunning state by periodically calling the underlying ECS.describeTasks() operation every 6 seconds (at most 100 times).\n\nWhich raises the following questions:\n1) Why the method request is failing in 3 seconds if the documentation says 6 seconds? \n2) If the waitFor timeout is 600 seconds, why it fails in the first request?. @dbook-kkbrandt Thanks for the feedback, I'll update the code. Now, addressing the questions...\n\nTo be sure, does your task stay running after you get that error?\n\nYes. The task stays running as intended although the method seems to fail before. I even created a Docker image using these lines just to make sure this was at the SDK level:\nFROM buildpack-deps:jessie-curl\nCMD [\"/bin/bash\", \"-c\", \"while :; do echo 'Infinite Loop - Hit CTRL+C to exit.'; sleep 1; done\"]\n\nIs there any more information that comes with that errorMessage?\n\nNo extra information. Although in a moment of clarity I realized what went wrong in my case. I was running the Node.JS code using AWS Lambda. The problem was: AWS Lambda has a default 3 second timeout, while my debugging task, the one running the Docker image mentioned above, took more than the Lambda default timeout to start. So, I got it working in my case.\nRegarding your issue specifically, this isn't a solution, but it helps to debug (aside of being a good practice with containers too):\n1) Make sure the application running in the container is logging everything to the stdout\n2) Create a Cloudwatch Log Group for the logging\n3) Update your task definition to use an IAM role with CloudWatch Logs permissions:\n\n4) Update the container definition inside the task definition you're trying to run to use awslogs and point it to the log group created in step 2:\n\nDoing this, you'll be able to see the output at the container level and pinpoint if it's the container causing problems.. ",
    "baszczewski": "In my opinion there is something wrong in promise like response. For example this methods works correctly for me when I am use callback API. My TypeScript sample:\ntypescript\necs.waitFor(\"tasksRunning\", {\n    cluster: AWS_CLUSTER,\n    tasks,\n    $waiter: {\n        delay: 1,\n        maxAttempts: 100,\n    },\n}, (err: AWS_SDK.AWSError, data: AWS_SDK.ECS.Types.DescribeTasksResponse) => {\n    if (err) {\n        reject(err.message);\n    } else {\n        resolve(data);\n    }\n});. ",
    "stevedomin": "Hi @jeskew,\nI'm providing an agent but haven't configured maxSockets. How would that help in this case?\nI've updated the strace logs, I didn't realise they were being truncated: https://gist.github.com/stevedomin/61d6964971753276d5adb4dfd0396024\nMy understanding of these logs is that a socket is being assigned by the kernel but then the actual connection takes a long time. I might be entirely wrong as my strace-fu is not fantastic :)\n. @jeskew I did, but the problem stayed. We've decided to use a different approach and are no longer affected by this issue so I'll close to keep your tracker clean. Thanks for your help.. @usamamashkoor @caleb0199 we actually got rid of the upload to S3 altogether.... We are no longer uploading files to S3 and serving them directly instead. This works well in that case but this bug could still have been problematic in other situations.. @eddjlam unfortunately this was a while ago and I can't remember. Did you figure out why that's the case?. ",
    "ashishpai2": "I am facing the same issue.  JS S3 SDK's upload function is very slow. It took me 30 minutes to upload a 3gb file. Whereas if I use S3 command line tools, it takes 3-4 minutes. . ",
    "caleb0199": "I would also appreciate seeing your new approach.\nThanks.. I would also appreciate seeing your new approach.\nThanks.. I know this was closed, but I thought I'd just tag a note here. I'm trying to add a new issue regarding this error. I'm using aws-sdk, and get this error on first upload after app (re)start. All other uploads succeed. Problem happens with aws-sdk v2.68 or higher, but does not happen prior to that version.\nI'll try to submit my issue (Submit button is not enabled for me.). I know this was closed, but I thought I'd just tag a note here. I'm trying to add a new issue regarding this error. I'm using aws-sdk, and get this error on first upload after app (re)start. All other uploads succeed. Problem happens with aws-sdk v2.68 or higher, but does not happen prior to that version.\nI'll try to submit my issue (Submit button is not enabled for me.). First, thanks so much for your help, @jeskew \nWe're using region: 'us-east-1'\nWhen I output the s3 client instance to the log immediately before the call to s3.putObject(), it shows that value.\nso, in our code, I have:\nvar s3 = new AWS.S3({params: params})\n  console.log('aws.s3:', s3)\n  s3.putObject({ContentLength: req.headers['content-length'], Body: req}, function (err) {\n    callback(err, remote_source, remote_file)\n  })\nand the output includes:\n| aws.s3: Service {\n  |   config:\n  |    Config {\n  |      credentials:\n  |       Credentials {\n  |         expired: false,\n  |         expireTime: null,\n  |         accessKeyId: removed for security\n  |         sessionToken: undefined },\n  |      credentialProvider: CredentialProviderChain { providers: [Object] },\n  |      region: 'us-east-1',\n  |      logger: null,\n  |      apiVersions: {},\n  |      apiVersion: '2016-08-09',\n  |      endpoint: 's3.amazonaws.com',\n  |      httpOptions: { timeout: 120000 },\n  |      maxRetries: undefined,\n  |      maxRedirects: 10,\n  |      paramValidation: true,\n  |      sslEnabled: true,\n  |      s3ForcePathStyle: false,\n  |      s3BucketEndpoint: false,\n  |      s3DisableBodySigning: true,\n  |      computeChecksums: true,\n  |      convertResponseTypes: true,\n  |      correctClockSkew: false,\n  |      customUserAgent: null,\n  |      dynamoDbCrc32: true,\n  |      systemClockOffset: 0,\n  |      signatureVersion: 'v2',\n  |      signatureCache: true,\n  |      retryDelayOptions: {},\n  |      useAccelerateEndpoint: false,\n  |      params: { Bucket: 'our_bucket_name', Key: 'our_file_name' } },\n  |   isGlobalEndpoint: false,\n  |   endpoint:\n  |    Endpoint {\n  |      protocol: 'https:',\n  |      host: 's3.amazonaws.com',\n  |      port: 443,\n  |      hostname: 's3.amazonaws.com',\n  |      pathname: '/',\n  |      path: '/',\n  |      href: 'https://s3.amazonaws.com/' },\n  |   _clientId: 1 \n}\nThe output of the s3 client instance is the same for the second call, except the last attribute, \"_clientId\", is 2, instead of 1.\nThanks again for your help!. First, thanks so much for your help, @jeskew \nWe're using region: 'us-east-1'\nWhen I output the s3 client instance to the log immediately before the call to s3.putObject(), it shows that value.\nso, in our code, I have:\nvar s3 = new AWS.S3({params: params})\n  console.log('aws.s3:', s3)\n  s3.putObject({ContentLength: req.headers['content-length'], Body: req}, function (err) {\n    callback(err, remote_source, remote_file)\n  })\nand the output includes:\n| aws.s3: Service {\n  |   config:\n  |    Config {\n  |      credentials:\n  |       Credentials {\n  |         expired: false,\n  |         expireTime: null,\n  |         accessKeyId: removed for security\n  |         sessionToken: undefined },\n  |      credentialProvider: CredentialProviderChain { providers: [Object] },\n  |      region: 'us-east-1',\n  |      logger: null,\n  |      apiVersions: {},\n  |      apiVersion: '2016-08-09',\n  |      endpoint: 's3.amazonaws.com',\n  |      httpOptions: { timeout: 120000 },\n  |      maxRetries: undefined,\n  |      maxRedirects: 10,\n  |      paramValidation: true,\n  |      sslEnabled: true,\n  |      s3ForcePathStyle: false,\n  |      s3BucketEndpoint: false,\n  |      s3DisableBodySigning: true,\n  |      computeChecksums: true,\n  |      convertResponseTypes: true,\n  |      correctClockSkew: false,\n  |      customUserAgent: null,\n  |      dynamoDbCrc32: true,\n  |      systemClockOffset: 0,\n  |      signatureVersion: 'v2',\n  |      signatureCache: true,\n  |      retryDelayOptions: {},\n  |      useAccelerateEndpoint: false,\n  |      params: { Bucket: 'our_bucket_name', Key: 'our_file_name' } },\n  |   isGlobalEndpoint: false,\n  |   endpoint:\n  |    Endpoint {\n  |      protocol: 'https:',\n  |      host: 's3.amazonaws.com',\n  |      port: 443,\n  |      hostname: 's3.amazonaws.com',\n  |      pathname: '/',\n  |      path: '/',\n  |      href: 'https://s3.amazonaws.com/' },\n  |   _clientId: 1 \n}\nThe output of the s3 client instance is the same for the second call, except the last attribute, \"_clientId\", is 2, instead of 1.\nThanks again for your help!. Any ideas on this one? It's still marked \"Feedback Requested\"...not sure if I should/can somehow clear that label after having responded (above) to the request.. Any ideas on this one? It's still marked \"Feedback Requested\"...not sure if I should/can somehow clear that label after having responded (above) to the request.. I've never tried that. Trying to figure out how to do that. I'm in the terminal of the container, but killing any of the app processes causes the container to stop. Still experimenting.. I've never tried that. Trying to figure out how to do that. I'm in the terminal of the container, but killing any of the app processes causes the container to stop. Still experimenting.. I haven't figured out how to do that, yet. Is that info key to diagnosing?. I haven't figured out how to do that, yet. Is that info key to diagnosing?. In order to mitigate impact to our app, I'm going to back up to v2.67.0 (which doesn't exhibit this behavior).\nAfter that, I'll try to figure out how to restart app without re-starting container. But, as a workaround for me, reverting to 2.67 is easier.\nStrange that it only impacts upload (as first operation). If I download as my first operation after restart, that works fine. And a subsequent upload also works. But, if upload is my first operation, that fails the first time, and then succeeds. \nI'd like to get to root cause on this. Seems like others are not complaining about this, so it may be something I'm doing as config...but not sure why the change in behavior between 2.67 and 2.68.. In order to mitigate impact to our app, I'm going to back up to v2.67.0 (which doesn't exhibit this behavior).\nAfter that, I'll try to figure out how to restart app without re-starting container. But, as a workaround for me, reverting to 2.67 is easier.\nStrange that it only impacts upload (as first operation). If I download as my first operation after restart, that works fine. And a subsequent upload also works. But, if upload is my first operation, that fails the first time, and then succeeds. \nI'd like to get to root cause on this. Seems like others are not complaining about this, so it may be something I'm doing as config...but not sure why the change in behavior between 2.67 and 2.68.. I'm sorry I can't be of help. I no longer work at the company where that\nproject exists, so I don't know if it's still happening with the latest\nversion of the SDK.\nOn Mon, Sep 10, 2018, 3:17 PM Chase Coalwell notifications@github.com\nwrote:\n\n@caleb0199 https://github.com/caleb0199\nAre you seeing this issue with the current version of the SDK?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1647#issuecomment-420079929,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABcdO4OIVCmDmBC3KCv2DQrjWT07ixb5ks5uZuUSgaJpZM4OjP-X\n.\n. I'm sorry I can't be of help. I no longer work at the company where that\nproject exists, so I don't know if it's still happening with the latest\nversion of the SDK.\n\nOn Mon, Sep 10, 2018, 3:17 PM Chase Coalwell notifications@github.com\nwrote:\n\n@caleb0199 https://github.com/caleb0199\nAre you seeing this issue with the current version of the SDK?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1647#issuecomment-420079929,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABcdO4OIVCmDmBC3KCv2DQrjWT07ixb5ks5uZuUSgaJpZM4OjP-X\n.\n. \n",
    "eddjlam": "@stevedomin do you remember if you were using the IAM Role on the ec2 instance to authenticate?  I ran into a similar issue and found that by setting the credentials as environment variables greatly sped up the upload process.\nstrace from slow upload . ",
    "ajmath": "+1 \nCurrently hitting issues trying to override s3.getObject.  According to the type defs there are two overloads of this method.\ngetObject(params: S3.Types.GetObjectRequest, callback?: (err: AWSError, data: S3.Types.GetObjectOutput) => void): Request<S3.Types.GetObjectOutput, AWSError>;\ngetObject(callback?: (err: AWSError, data: S3.Types.GetObjectOutput) => void): Request<S3.Types.GetObjectOutput, AWSError>;\nThe version that only has the optional callback is not valid per the API docs.\nTrying to override this like so:\ns3.getObject = (params: GetObjectRequest, callback?: (error: AWSError, output: GetObjectOutput) => void) => {\n    return {} as Request<GetObjectOutput, AWSError>;\n  };\ngives a compiler error like this:\nError:(22, 3) TS2322:Type '(params: GetObjectRequest, callback?: (error: AWSError, output: GetObjectOutput) => void) => Requ...' is not assignable to type '{ (params: GetObjectRequest, callback?: (err: AWSError, data: GetObjectOutput) => void): Request<...'.\n  Types of parameters 'params' and 'callback' are incompatible.\n    Type '(err: AWSError, data: GetObjectOutput) => void' is not assignable to type 'GetObjectRequest'.\n      Property 'Bucket' is missing in type '(err: AWSError, data: GetObjectOutput) => void'.\n. ",
    "anho": "I have additional feedback for the auto-generated type definitions. It is in the DynamoDB client which basically requires for all the requests a TableName to be present in the payload. But I'm actually setting that when I create the client. So per implementation I actually don't need it, still compilation fails with a not assignable to parameter error.\nIs it ok to just put that information here or should I create an additional issue to separately keep track of the feedback?\n. I have additional feedback for the auto-generated type definitions. It is in the DynamoDB client which basically requires for all the requests a TableName to be present in the payload. But I'm actually setting that when I create the client. So per implementation I actually don't need it, still compilation fails with a not assignable to parameter error.\nIs it ok to just put that information here or should I create an additional issue to separately keep track of the feedback?\n. ",
    "tvald": "More generally, XXXResult and resource types tend to have most fields denoted as optional, even when this makes no sense. eg:\nexport interface CreateVpcResult {\n  Vpc?: Vpc;\n}\nIf the VPC wasn't created, the API will return an error. Hence, in practice it's impossible for CreateVpcResult.Vpc to be undefined, even though the type system permits that.\nThis makes it a pain to use result properties, as each access requires validation that the \"optional\" property isn't undefined:\nif (vpcResult.Vpc === undefined || vpcResult.Vpc.VpcId === undefined)\n  throw new Error('these can't happen, but necessary to check due to incorrect declaration as optional'). I'm also seeing this problem. When submitted in sequence, the first putObject() fails and subsequent putObject()s succeed. When submitted in parallel, all putObject()s fail.\nUsing aws-sdk@2.94.0.\nI'm reading from the local file system within an upload script. The problem only occurs when an fs.ReadStream is provided as the body for putObject(). All requests succeed (both in sequence and in parallel) when file contents are read into Buffers and then provided as the body for putObject().. ",
    "alexturek": "@jeskew Any update? This is making using the AWS SDK incredibly painful. @jeskew Any update? This is making using the AWS SDK incredibly painful. ",
    "alex20465": "Same pain here, optional is like no types at all.. Same pain here, optional is like no types at all.. ",
    "exortech": "Thanks for the response. This is working now that the new release of amazon-cognito-identity-js is available.. ",
    "Gilis95": "Yes I've post to the wrong place sorry. Yes I've post to the wrong place sorry. ",
    "Christilut": "You're right, it was an error in my code. SIgning without ContentType works fine and the download is as expected now.. ",
    "FelschR": "I'm having the same issue when building an Ionic 3 app with the production flag on the default webpack configuration.. This issue is still present in the latest version 2.109.0 on npm.. ",
    "dannjoku1": "I'm experiencing the same issue while building a react.js app. Any update on a solution/fix?\n. ",
    "andersforsell": "I am still seeing this issue:\nrng.js:21 Uncaught TypeError: Cannot read property 'crypto' of undefined\n    at rng.js:21\n    at Object.<anonymous> (rng.js:31)\n    at Object../node_modules/amazon-cognito-identity-js/node_modules/crypto-browserify/rng.js (rng.js:31)\n    at __webpack_require__ (bootstrap:63)\n    at Object.<anonymous> (index.js:4)\n    at Object.<anonymous> (index.js:97)\n    at Object../node_modules/amazon-cognito-identity-js/node_modules/crypto-browserify/index.js (index.js:97)\n    at __webpack_require__ (bootstrap:63)\n    at Object../node_modules/amazon-cognito-identity-js/es/AuthenticationHelper.js (AuthenticationDetails.js:84)\n    at __webpack_require__ (bootstrap:63)\nMy dependencies in package.json:\n\"dependencies\": {\n    \"ajv-keywords\": \"^3.2.0\",\n    \"apollo-client\": \"^2.3.8\",\n    \"aws-amplify\": \"^1.0.8\",\n    \"aws-appsync\": \"^1.3.4\",\n    \"graphql\": \"^0.13.2\",\n    \"graphql-tag\": \"^2.9.2\",\n    \"regenerator-runtime\": \"^0.11.1\",\n    \"uuid\": \"^3.1.0\"\n  },\nIn package-lock.json there is a dependency to crypto-browserify:\n\"amazon-cognito-identity-js\": {\n      \"version\": \"2.0.23\",\n      \"resolved\": \"https://registry.npmjs.org/amazon-cognito-identity-js/-/amazon-cognito-identity-js-2.0.23.tgz\",\n      \"integrity\": \"sha512-TRZEphGFIPgrMNuFlAzr7ZhZfqkEMDNFXD5OI9ZL/iw+D3e2xHw0FHyaN3fWW06ZRiBvJUHAXPnOV0ON40e/ng==\",\n      \"requires\": {\n        \"buffer\": \"4.9.1\",\n        \"crypto-browserify\": \"1.0.9\",\n        \"js-cookie\": \"2.2.0\"\n      }\n    },\nPOSSIBLE WORKAROUND:  UPDATE Does not work anymore? \nManually install a newer amazon-cognito-identity:\nnpm install amazon-cognito-identity-js --save. ",
    "amangpt777": "This is happening consistently. I am sending messages to FIFO queue in a loop of three to test. The first message is successfully sent without error, the second and third message gives error as mentioned by sam-qburst. Interestingly though I have the three messages in my queue irrespective of this error!\nI created queue using : \n{\n          QueueName: 'oplog.fifo',\n          Attributes: {\n            VisibilityTimeout: '100',\n            MessageRetentionPeriod: '1000',\n            FifoQueue: 'true',\n            ContentBasedDeduplication: 'true',\n            ReceiveMessageWaitTimeSeconds: '20'\n          }\n}\nMy three messages are : \n{ MessageBody: '{\"ts\":\"6484407156664696833\",\"t\":51,\"h\":\"6425468803639300355\",\"v\":2,\"op\":\"i\",\"ns\":\"formistry-db.formtemp\",\"o\":{\"_id\":\"59fd3df4862d1b2cea9e5ffb\",\"00U\":\"141001\",\"y0k\":[\"1\"],\"y0d\":\"518122\",\"00V\":\"PB\",\"y0c\":\"01\",\"00W\":\"IND\",\"00X\":[\"T\"]}}',\n  QueueUrl: 'https://sqs.us-east-1.amazonaws.com/226488695738/oplog.fifo',\n  MessageGroupId: 'formistry_oplog' }\n{ MessageBody: '{\"ts\":\"6484407156664696834\",\"t\":51,\"h\":\"-5568902571925993521\",\"v\":2,\"op\":\"i\",\"ns\":\"formistry-db.formtemp\",\"o\":{\"_id\":\"59fd3df4862d1b2cea9e5ffd\",\"00U\":\"141001\",\"y0k\":[\"1\"],\"y0d\":\"518122\",\"00V\":\"PB\",\"y0c\":\"01\",\"00W\":\"IND\",\"00X\":[\"T\"]}}',\n  QueueUrl: 'https://sqs.us-east-1.amazonaws.com/226488695738/oplog.fifo',\n  MessageGroupId: 'formistry_oplog' }\n{ MessageBody: '{\"ts\":\"6484407156664696835\",\"t\":51,\"h\":\"-4866979313211304739\",\"v\":2,\"op\":\"i\",\"ns\":\"formistry-db.formtemp\",\"o\":{\"_id\":\"59fd3df4862d1b2cea9e5ffc\",\"00U\":\"141001\",\"y0k\":[\"1\"],\"y0d\":\"518122\",\"00V\":\"PB\",\"y0c\":\"01\",\"00W\":\"IND\",\"00X\":[\"T\"]}}',\n  QueueUrl: 'https://sqs.us-east-1.amazonaws.com/226488695738/oplog.fifo',\n  MessageGroupId: 'formistry_oplog' }\nError for second message: \nerror: InvalidChecksum: sendMessage returned an invalid MD5 response. Got \"24cd4717dcbb632e879526e264e0bbc7\", expecting \"b2acb08b14eb36364690974dcf84dde7\".InvalidChecksum: sendMessage returned an invalid MD5 response. Got \"24cd4717dcbb632e879526e264e0bbc7\", expecting \"b2acb08b14eb36364690974dcf84dde7\".\n    at features.constructor.throwInvalidChecksumError (/home/node_modules/aws-sdk/lib/services/sqs.js:94:37)\n    at Request.verifySendMessageChecksum (/home/node_modules/aws-sdk/lib/services/sqs.js:33:20)\n    at Request.callListeners (/home/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/home/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/home/node_modules/aws-sdk/lib/request.js:682:14)\n    at Request.transition (/home/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/home/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /home/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/home/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request. (/home/node_modules/aws-sdk/lib/request.js:684:12)\nError with third message: \nerror: InvalidChecksum: sendMessage returned an invalid MD5 response. Got \"0895e88eaed09dc1d9a4923a95c65d7d\", expecting \"b2acb08b14eb36364690974dcf84dde7\".InvalidChecksum: sendMessage returned an invalid MD5 response. Got \"0895e88eaed09dc1d9a4923a95c65d7d\", expecting \"b2acb08b14eb36364690974dcf84dde7\".\n    at features.constructor.throwInvalidChecksumError (/home/node_modules/aws-sdk/lib/services/sqs.js:94:37)\n    at Request.verifySendMessageChecksum (/home/node_modules/aws-sdk/lib/services/sqs.js:33:20)\n    at Request.callListeners (/home/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/home/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/home/node_modules/aws-sdk/lib/request.js:682:14)\n    at Request.transition (/home/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/home/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /home/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/home/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request. (/hom/node_modules/aws-sdk/lib/request.js:684:12)\n. ",
    "zebralight": "I am also getting this error message when sending messages using the standard, non-FIFO method with a few messages being transmitted to the queue successfully in an unpredictable manner, in the same way that @amangpt777 describes.  Resolving this quickly would be much appreciated as I'd like to transition over to sqs as my sole method of communication between my modules.. ",
    "haraldkubota": "If anyone interested: happens to me too. aws-sdk 2.162 (current as of now). NodeJS 8.9.0 as well as 8.9.1 (current as of now).\nHere my example error message:\n{ InvalidChecksum: sendMessage returned an invalid MD5 response. Got \"0d9f2fb6a7a6151e95e0b22ad6bccf44\", expecting \"d7a57fde014ff23e7d8d41b0c9b39a43\".\n    at features.constructor.throwInvalidChecksumError (/home/harald/src/aws/sqs/node_modules/aws-sdk/lib/services/sqs.js:94:37)\n    at Request.verifySendMessageChecksum (/home/harald/src/aws/sqs/node_modules/aws-sdk/lib/services/sqs.js:33:20)\n    at Request.callListeners (/home/harald/src/aws/sqs/node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n    at Request.emit (/home/harald/src/aws/sqs/node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n    at Request.emit (/home/harald/src/aws/sqs/node_modules/aws-sdk/lib/request.js:683:14)\n    at Request.transition (/home/harald/src/aws/sqs/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/home/harald/src/aws/sqs/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /home/harald/src/aws/sqs/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request.<anonymous> (/home/harald/src/aws/sqs/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request.<anonymous> (/home/harald/src/aws/sqs/node_modules/aws-sdk/lib/request.js:685:12)\n  message: 'sendMessage returned an invalid MD5 response. Got \"0d9f2fb6a7a6151e95e0b22ad6bccf44\", expecting \"d7a57fde014ff23e7d8d41b0c9b39a43\".',\n  retryable: true,\n  code: 'InvalidChecksum',\n  messageIds: [ '3e96fb76-e15c-4ee3-9b2f-518ca1adc5e2' ],\n  time: 2017-12-02T12:09:00.267Z,\n  statusCode: 200 }. ",
    "KafaltiyaMahesh": "I am facing same issue while fetching an object but upload working fine with forward slashes in bucket Name 'a/b/c'. @chrisradek  Thanks for the clarification. Only one confusion ; why are you allowing forward slashes in the bucketname while uploading file ?. @chrisradek Thanks man..! I have changed my code. Now using key prefix for nested directory structure which would be supported by you in later versions also.. ",
    "sam-jg": "@chrisradek \nI have tried with your code and got no change, works locally, but times out when run in Lambda. I did console log cognito and noticed that the accessKeyId is different when run on Lambda. My understanding is that permissions don't matter for signup. Just in case, I tried giving admin permissions to the role running the function, with no success.\nThis is in ap-southeast-2 and I am triggering with API Gateway but I can't see why either of these things would matter anyway.. @chrisradek \nFinally tracked it down to an issue with our security groups and subnet configuration. Thanks for your help and sorry to waste your time.. ",
    "flock-machine-user": "I am trying to use the s3.getObject({Bucket: 'bucket', Key:'key}).createReadStream() using Typescript is throwing an error because there is not a definition for this type getObject call.  I am getting error:\n```\nerror TS2345: Argument of type '{ Bucket: String; Key: String; }' is not assignable to parameter of type '(err: AWSError, data: GetObjectOutput) => void'.\n  Type '{ Bucket: String; Key: String; }' provides no match for the signature '(err: AWSError, data: GetObjectOutput): void'.\n12         s3.getObject(params)\n```\nWhat can i do to get this function call to compile?. ",
    "anton-107": "@flock-machine-user I don't get this specific error. But with one of other methods from aws-sdk, type casting the first parameter helped me solve the transpiler error. In your case it would look like this:\ns3.getObject({Bucket: 'bucket', Key:'key'} as GetObjectRequest).createReadStream();. @flock-machine-user I don't get this specific error. But with one of other methods from aws-sdk, type casting the first parameter helped me solve the transpiler error. In your case it would look like this:\ns3.getObject({Bucket: 'bucket', Key:'key'} as GetObjectRequest).createReadStream();. ",
    "ggutenberg": "@anton-107 's method worked for me, but I needed to specify the full path to the type.  Ex:\ns3.createBucket({ Bucket: 'bucket' } as AWS.S3.CreateBucketRequest).promise(),. ",
    "lmajano": "What was the issue @rgmembreno I am hitting the same issue. ",
    "awitherow": "I have been in touch with some dudes from Exponent. The app I am building is React Native with Create React Native App, and apparently that is the cause of the issue.\nThere is apparently a PR being worked on for this?\nThey have forked it and I have things working on this fork: https://github.com/expo/amazon-cognito-identity-js. ",
    "fifafu": "Ah sorry, that was just an example, in my actual code the filenames are correct.\nI'll provide some real example links tomorrow!. So here is an example file named \u00e4_\u00f6_\u00fc.txt (I made it publicly accessible so you can verify)\nFrom the AWS Management console I get this (working) link:\nhttps://em-sales.s3.eu-central-1.amazonaws.com/a%CC%88_o%CC%88_u%CC%88.txt (moved the bucket name to the host part for consistency)\nWith the getSignedUrl function I get this (non working) link:\nhttps://em-sales.s3.eu-central-1.amazonaws.com/%C3%A4_%C3%B6_%C3%BC.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIJH463RJWNN5KRYA%2F20170620%2Feu-central-1%2Fs3%2Faws4_request&X-Amz-Date=20170620T074748Z&X-Amz-Expires=86400&X-Amz-Signature=e25a2fcedbaeaff3915a2843433e76e22caafd725bfc154c44f2d44a0891c9d5&X-Amz-SignedHeaders=host\nconst url = this.s3.getSignedUrl('getObject', {\n      Bucket: 'em-sales',\n      Key: '\u00e4_\u00f6_\u00fc.txt',\n      Expires: 60*60*24\n    });\nIf you mouse over the both links in Google Chrome, they look exactly the same. For me this looks like a bug with S3 and not necessarily with this framework, but I'm not sure.\nFor now I'll probably try to patch the signing function to replace the not working characters, or do you have a better idea?. Thanks a lot I'll try that tomorrow!\nI think this S3 behavior is kind of strange/unexpected, especially because the S3 web uploader apparently automatically encodes filenames using these grapheme clusters.\nMaybe this would be good to add to the documentation.. @jeskew that's possible, maybe that's the default encoding for file names on macOS (where the files where created & uploaded from). Still not easy to figure that out unless you know a lot about this stuff :-). ",
    "jkrenge": "Okay thanks. Here's my actionable input for the next person searching for this issue:\nYou'll need two packages to properly encode the sender name.\nnpm i q-encoding --save\nnpm i utf8 --save\nAnd then, you can just:\n```\nvar qencode = require('q-encoding');\nvar utf8 = require('utf8');\nvar MIME = (string) => '=?UTF-8?Q?' + qencode.encode(utf8.encode(string)) + '?=';\n...\nses.sendEmail({\n  ...\n  Source: MIME(fromName) + ' <' + fromEmail + '>',\n  ...\n}, ...\n```. ",
    "WooDzu": "Note it should also be possible to avoid the extra dependencies by using base64:\njs\nconst base64ToName = Buffer.from(toName).toString('base64');\nconst finalToName = `=?UTF-8?B?${base64ToName}?= <${toEmail}>`;\nI only tested it with Destination.ToAddresses but it should be the same thing. ",
    "replicat0r": "Nodejs v7.9.0\nand Node 6.x in production. Well I was using Rxjs as my async, control mechanism, upload and download. I wanted to make sure that it wasnt that messing things up, So went to s3 docs and copy pasted the example code. as follows. \nInitially, I tried toString, JSON.stringify, and String() on the data.Body object, but it always returned emtpy, So I stringified the entire data object, thats when i found out it returned an empty buffer. \nS3.getObject({\n    Key: 'sample.json',\n    Bucket: 'TESTBUCKET'\n}, function (err, data) {\n    if (err) {\n        console.log('FILE NOT FOUND')\n\n        console.log(err, err.stack);\n    } else {\n        console.log(data)\n        console.log(data.ETag)\n        console.log ('thisis tostring',JSON.stringify(data))\n\n    } // an error occurred\n\n});. and this is my s3 upload code\n\nfunction _constructS3Object({\n    filename,\n    extension,\n    folder,\n    fileContent,\n}) {\n    const _folder = String(folder).replace(/\\/$/, \"\");\n    const _fileContent = JSON.stringify(fileContent);\n    console.log('file to be uploaded',_fileContent)\n    return {\n        Bucket: BUCKET_NAME,\n        Key: `${_folder}/${filename}.${extension}`,\n        Body: _fileContent,\n        Metadata: {}\n    };\n}\nthis basically produces a params object that gets piped into S3.upload(), _fileContent is a Json. @jeskew you are right! I have been using glimpse https://www.npmjs.com/package/@glimpse/glimpse to analyse my requests. I just took it off, everything is working as expected. Ill submit a issue to the glimpse team. This such an edge case to happen. If you didnt bring this up, I would still be banging my head against the wall. Thanks!. ",
    "alihalabyah": "@jeskew I'll provide you with the request ID once it happens again, because it's very rare to surface.. @jeskew I'll provide you with the request ID once it happens again, because it's very rare to surface.. @jeskew We have updated all of our packages recently and the version number we are using for the SDK is included above aws-sdk-js/2.80.0\nCould that be the issue?. Apparently we were using 2.58.0 before, so it might be the reason for this issue, We'll try to downgrade, test and get back to you . Yes @jeskew i confirm this just tried and got the same\nX-Amz-Target:AWSCognitoIdentityService.GetId\nX-Amz-User-Agent:aws-sdk-js/2.58.0 callback. ",
    "StevenDufresne": "This is most likely caused by using the Javascript library and generating a client secret for the app client. See answer here:\nhttps://stackoverflow.com/questions/37438879/resolve-unable-to-verify-secret-hash-for-client-in-amazon-cognito-userpools. This is most likely caused by using the Javascript library and generating a client secret for the app client. See answer here:\nhttps://stackoverflow.com/questions/37438879/resolve-unable-to-verify-secret-hash-for-client-in-amazon-cognito-userpools. ",
    "erothmayer": "@StevenDufresne @jeskew we also observed the same thing about 20 minutes ago. We are definitely not using a client secret in our app client, and are using an integration in our web app that has been working for months. It started throwing 400s on the OPTIONS call for several minutes, then returned to working as normal.\nWe'll keep an eye out for request ids to see if we can help create a repro. However, if you have any other theories (or could get the service team to take a look in their internal logs, where presumably they'll see a spike of 400s on OPTIONS requests) we'd appreciate any further attention you can spare.. @srchase Could you please double check that this is really a duplicate? It's a completely different API (S3 as opposed to CloudWatch.Metrics.PutMetricData). And I'm not necessarily looking for the SDK to internally gzip for me -- I'm happy to gzip it myself, but right now the parameter validator doesn't allow a gzip blob, even though the API claims to support it.\nWhatever gets it solved is fine at the end of the day, but these don't look like duplicates from my admittedly outside perspective.... I also just noticed that the linked issue has been open for 1.5 years with no sign of a resolution in sight. I'd really like to avoid linking the two together if a minimal correction for PutMetricData could be done more quickly (which seems possible since it may just be a validator shape definition change). thanks @srchase !. @srchase tried it out, works perfectly! Thank you so much for the rapid turnaround!. ",
    "teledemic": "In general, you can just replace awsfunction(blah, blah, callback) with awsfunction(blah, blah).promise() to return the promisified version.. ",
    "Agrumas": "@chrisradek error handling would be appreciated in that place because I spent a few hours trying to figure out what is wrong with Authentication part... The null isn't necessary in my case, just it was passed from other layer and I even wasn't aware of it. I would expect empty string, missing field, but not an error :). ",
    "Hyperblaster": "Same problem!!\n. ",
    "KurtPattyn": "@chrisradek Thx. Need a way then to blacklist the module from the scanner and to avoid the use of the AWS SDK in browser environments.. ",
    "jeanduplessis": "@chrisradek Another reason to move to a more recent version is the fact that AWS.CloudFront.Signer::getSignedCookie function depends on crypto.createSign function which isn't implemented in the 1.0.9 version of crypto-browserify. Until https://github.com/yarnpkg/rfcs/pull/68 is released the only way to get it to work seems to include a fork of this repo with a newer version of the crypto-browserify dependency which isn't ideal.. ",
    "ssshah5": "I have a similar use case where I am using a pre-signed URL to upload a large zip file from the client side without interacting with Server. Multipart upload will be ideal. I agree with @oyeanuj that presignedUrl method feels the cleanest to use to upload and a multipart support to this would be ideal.. I have a similar use case where I am using a pre-signed URL to upload a large zip file from the client side without interacting with Server. Multipart upload will be ideal. I agree with @oyeanuj that presignedUrl method feels the cleanest to use to upload and a multipart support to this would be ideal.. @jeskew - Thank you for the details and escalating this as a feature request. For our use case - The client reaches out to server to get a pre-signed URL in order to PUT objects to object-storage. The server than generates the URL (using the S3 node module) and returns it back to the client side code (bash). The client side code than tries to use this single URL to upload the entire objects. Since the size of the object can be huge, we are using HTTP streaming data mechanism (chunked transfer encoding). However, this doesn't seem to be working with S3 object storage and I get an error back (Content-Length is missing) probably because it expects that transfer should send ending bits at the end of first chunk. This scenario works fine with Swift Object Storage where a single temporary URL allows to transmit streaming data without generating temporary (pre-signed) URLs for each chunk.\nDo you think it would be possible to upload chunked data using a single pre-signed URL?\nThanks. @jeskew - Thank you for the details and escalating this as a feature request. For our use case - The client reaches out to server to get a pre-signed URL in order to PUT objects to object-storage. The server than generates the URL (using the S3 node module) and returns it back to the client side code (bash). The client side code than tries to use this single URL to upload the entire objects. Since the size of the object can be huge, we are using HTTP streaming data mechanism (chunked transfer encoding). However, this doesn't seem to be working with S3 object storage and I get an error back (Content-Length is missing) probably because it expects that transfer should send ending bits at the end of first chunk. This scenario works fine with Swift Object Storage where a single temporary URL allows to transmit streaming data without generating temporary (pre-signed) URLs for each chunk.\nDo you think it would be possible to upload chunked data using a single pre-signed URL?\nThanks. ## UPDATE\nIt worked for large files when I use \"hex\" or base64 instead of binary as follows in the above code.\n```\nvar encryptedLog = cipher.update(log, 'utf8', 'hex');\nencryptedLog += cipher.final('hex');\n```\n```\nvar encryptedLog = cipher.update(log, 'utf8', 'base64');\nencryptedLog += cipher.final('base64');\n```\nHowever, I want it to work with Binary as I need to be backward compatible with the files that are already encrypted using binary.. ## UPDATE\nIt worked for large files when I use \"hex\" or base64 instead of binary as follows in the above code.\n```\nvar encryptedLog = cipher.update(log, 'utf8', 'hex');\nencryptedLog += cipher.final('hex');\n```\n```\nvar encryptedLog = cipher.update(log, 'utf8', 'base64');\nencryptedLog += cipher.final('base64');\n```\nHowever, I want it to work with Binary as I need to be backward compatible with the files that are already encrypted using binary.. Hello @jeskew , I tried the following but having the same issue. The same code works fine against Swift Object Storage. Any Suggestions to try out the buffer differently? \nconst buf = Buffer.from(options.content);\nvar params = {Bucket: s3LogsBucketName, Key: options.container + '/' + options.name, Body: buf};. Hello @jeskew , I tried the following but having the same issue. The same code works fine against Swift Object Storage. Any Suggestions to try out the buffer differently? \nconst buf = Buffer.from(options.content);\nvar params = {Bucket: s3LogsBucketName, Key: options.container + '/' + options.name, Body: buf};. @jeskew - Yes options.content is a string. We get the data from the DB and encode it in binary using following code before calling the putObject.\nvar log = options.content;\nvar encryptedLog = cipher.update(log, 'utf8', 'binary');\nencryptedLog += cipher.final('binary');\nThe putObject() doesn't have issues if I use base64 or hex for encoding. However I see the issue with binary encoding and that too only with file size larger than 3.5 MB. . @jeskew - Yes options.content is a string. We get the data from the DB and encode it in binary using following code before calling the putObject.\nvar log = options.content;\nvar encryptedLog = cipher.update(log, 'utf8', 'binary');\nencryptedLog += cipher.final('binary');\nThe putObject() doesn't have issues if I use base64 or hex for encoding. However I see the issue with binary encoding and that too only with file size larger than 3.5 MB. . @jeskew - Thanks for sharing the code. I finally solved the issue by changing the encoding. Appreciate your time and response. Thank you.. @jeskew - Thanks for sharing the code. I finally solved the issue by changing the encoding. Appreciate your time and response. Thank you.. ",
    "raphaeljoie": "@jeskew \u2014 I used to use the block blob upload feature of Azure with signed URI. It was really convenient to upload large file from client side, directly in a storage. \nHow does it work\n1\u00b0 you generate a signed URI on server side with a write access\n2\u00b0 client split the file, attribute an uuid to every chunk, upload them using the same signed URI\n3\u00b0 client send the list of uuids and Azure re-creates the file based on the chunks sent in 2\u00b0\nIf a well understand your last post, it is not possible on Amazon ? (because every single PUT chunk request must be individually signed).\nIf it's the case, how to upload big files, from client side, directly to S3, without sending the key to the clients ?. ",
    "absoludity": "For reference of the Azure API mentioned above, a blob storage client can be created with a presigned url (shared-access signature): https://github.com/Azure/azure-storage-node/blob/master/browser/azure-storage.blob.export.js#L35\nie. and with that, you can do a normal upload which handles the chunking and sending the list of parts automatically: https://github.com/Azure/azure-storage-node/blob/master/lib/services/blob/blobservice.browser.js#L70. ",
    "ithinco": "Really hope aws-sdk-js can provide this feature.. Really hope aws-sdk-js can provide this feature.. ",
    "Plasma": "Just encountered this; surprised to see there's no way in the SDK to leverage pre-signed URLs.\nThe reason is so we can offer multi-part upload, from the web browser, but of course keep AWS keys server-side only. How else is this supposed to work? Thanks!. Just encountered this; surprised to see there's no way in the SDK to leverage pre-signed URLs.\nThe reason is so we can offer multi-part upload, from the web browser, but of course keep AWS keys server-side only. How else is this supposed to work? Thanks!. ",
    "moralesalberto": "Agreed. This would be a great feature. I was trying to find a way to do it and found this post. Seems like it is not doable today.. Agreed. This would be a great feature. I was trying to find a way to do it and found this post. Seems like it is not doable today.. ",
    "alfadaemon": "Agree, this IS a VERY needed feature. Hope we can see it available soon.. ",
    "sandyghai": "I was managed to achieve this in serverless architecture by creating a Canonical Request for each part upload using Signature Version 4. You will find the document here https://sandyghai.github.io/AWS-S3-Multipart-Upload-Using-Presigned-Url/. I was managed to achieve this in serverless architecture by creating a Canonical Request for each part upload using Signature Version 4. You will find the document here https://sandyghai.github.io/AWS-S3-Multipart-Upload-Using-Presigned-Url/. ",
    "dusty": "I was also looking for this and I ended up using STS to generate temporary security tokens for my client locked down to the particular bucket and path that I wanted to give them access to.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html\nI found this video on youtube about it.\nhttps://www.youtube.com/watch?v=4_csSXc_GNU\nPerhaps that will help someone looking at this issue.\n. I was also looking for this and I ended up using STS to generate temporary security tokens for my client locked down to the particular bucket and path that I wanted to give them access to.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html\nI found this video on youtube about it.\nhttps://www.youtube.com/watch?v=4_csSXc_GNU\nPerhaps that will help someone looking at this issue.\n. ",
    "shawnly": "\nI am on the same boat, my use case is also partial uploads for big files on a JS client side. I want people to be able to resume uploads if they lose their connection, without losing all previously uploaded chunks. And I don't want to expose any credentials (thus not using SDK on client)\nI will update this comment once I solve it.\nUPDATE: following @sandyghai guide, I was able to do it.\nThere may be syntax errors, as my backend does not use express, but I felt writing it ala express would help other devs understand it easier.\nContext: I have an API (behind auth obviously) to which users can send files, and it uploads them to S3. As I didn't want to set IAM for each user of my app, nor put the SDK in the front-end, I decided to go with a back-end authorized approach.\n```js\napp.post('/upload', (req, res) => {\n  let UploadId = req.body.UploadId;\nconst params = {\n    Bucket: 'my-bucket-name',\n    Key: req.body.filename\n  };\n// Initialize the multipart - no need to do it on the client (although you can)\n  if (req.body.part === 1) {\n    const createRequest = await s3.createMultipartUpload(params).promise();\n    UploadId = createRequest.UploadId;\n  }\n// Save createRequest.UploadId in your front-end, you will need it. \n  // Also sending the uploadPart pre-signed URL for part #1\n  res.send({\n    signedURL: s3.getSignedUrl('uploadPart', {\n      ...params,\n      Expires: 60 * 60 * 24, // this is optional, but I find 24hs very useful\n      PartNumber: req.body.part\n    }),\n    UploadId,\n    ...params\n  });\n});\napp.post('/upload-complete', (req, res) => {\n  let UploadId = req.body.UploadId;\nconst params = {\n    Bucket: 'my-bucket-name',\n    Key: req.body.filename\n  };\nconst data = await s3.completeMultipartUpload({\n    ...params,\n    MultipartUpload: {\n      Parts: req.body.parts\n    },\n    UploadId\n  }).promise();\n// data = {\n  //   Bucket: \"my-bucket-name\", \n  //   ETag: \"some-hash\", \n  //   Key: \"filename.ext\", \n  //   Location: \"https://my-bucket-name.s3.amazonaws.com/filename.ext\"\n  // }\n  res.send({\n    ...data\n  });\n});\n```\nTL;DR: it is possible, so feel free to close the ticket, IMHO.\n\nDo you have front end and back end working code for your solution?\n. > I am on the same boat, my use case is also partial uploads for big files on a JS client side. I want people to be able to resume uploads if they lose their connection, without losing all previously uploaded chunks. And I don't want to expose any credentials (thus not using SDK on client)\n\nI will update this comment once I solve it.\nUPDATE: following @sandyghai guide, I was able to do it.\nThere may be syntax errors, as my backend does not use express, but I felt writing it ala express would help other devs understand it easier.\nContext: I have an API (behind auth obviously) to which users can send files, and it uploads them to S3. As I didn't want to set IAM for each user of my app, nor put the SDK in the front-end, I decided to go with a back-end authorized approach.\n```js\napp.post('/upload', (req, res) => {\n  let UploadId = req.body.UploadId;\nconst params = {\n    Bucket: 'my-bucket-name',\n    Key: req.body.filename\n  };\n// Initialize the multipart - no need to do it on the client (although you can)\n  if (req.body.part === 1) {\n    const createRequest = await s3.createMultipartUpload(params).promise();\n    UploadId = createRequest.UploadId;\n  }\n// Save createRequest.UploadId in your front-end, you will need it. \n  // Also sending the uploadPart pre-signed URL for part #1\n  res.send({\n    signedURL: s3.getSignedUrl('uploadPart', {\n      ...params,\n      Expires: 60 * 60 * 24, // this is optional, but I find 24hs very useful\n      PartNumber: req.body.part\n    }),\n    UploadId,\n    ...params\n  });\n});\napp.post('/upload-complete', (req, res) => {\n  let UploadId = req.body.UploadId;\nconst params = {\n    Bucket: 'my-bucket-name',\n    Key: req.body.filename\n  };\nconst data = await s3.completeMultipartUpload({\n    ...params,\n    MultipartUpload: {\n      Parts: req.body.parts\n    },\n    UploadId\n  }).promise();\n// data = {\n  //   Bucket: \"my-bucket-name\", \n  //   ETag: \"some-hash\", \n  //   Key: \"filename.ext\", \n  //   Location: \"https://my-bucket-name.s3.amazonaws.com/filename.ext\"\n  // }\n  res.send({\n    ...data\n  });\n});\n```\nTL;DR: it is possible, so feel free to close the ticket, IMHO.\n\nDo you have front end and back end working code for your solution?\n. > I have already posted the back-end code.\n\nThe front-end doesn't do anything especial, just a fetch with method PUT and passing the body binary buffer.\n\n. > I have already posted the back-end code.\n\nThe front-end doesn't do anything especial, just a fetch with method PUT and passing the body binary buffer.\n\n. ",
    "linehrr": "I wonder if you just create another table with that attribute as hashKey, now you can use condition. and create a dynamo stream to update the main table whenever the helper table succeeded.. ",
    "worldgnat": "Interesting. We're using Node 6.11 and we're using Typescript, which turns async and await into generator functions. So yes, we're using polyfill.. Async/await shouldn't have anything to do with this though. If I modify the code to not use async/await, I still get the same result: \njavascript\nconst AWS = require('aws-sdk');\nconst s3 = new AWS.S3({region: 'us-west-2'});\nconst Bucket = '<bucket name>';\n        const Key = '<object key>';\n        s3.getObject({Bucket, Key}).promise().then((result: any) => {\n             console.log(result.LastModified.valueOf()); \n        });\nreturns \"Thu, 06 Jul 2017 20:00:35 GMT\". We're using version 2.9. Oh! I didn't realize that. We'll upgrade. Thanks for looking into this, and sorry for bugging you about an old version. . ",
    "mebibou": "I am using version 2.113.0 and after a few seconds I get:\n{ loaded: 7328032, total: 7328032 }\nThen I will receive the same progress every now and then, and finally display a TimeoutError: Connection timed out after 120000ms.\n$ node -v\nv7.4.0. ",
    "Shinigami92": "I would like to see also union for region.\nts\n// instead of\ntype AwsRegion = string;\n// we could use\ntype AwsRegion = 'us-east-1' | 'eu-central-1' | 'eu-west-1' | ...;\nThis could prevent errors like\nts\nnew AWS.Lambda({ region: 'us-eest-1' });. ",
    "catamphetamine": "Ok, thx for the tip, the CloudWatch Events feature seems to do the same and it seems to have the API requried.\nThe trigger interface looks like this:\n\n. ",
    "JG2203": "is this open? I am also having similar issues . ",
    "broofa": "I believe we tried attaching an error listener to the stream, but did not see an error event for the case in question (invalid s3 key).   I'm traveling today, however, so am unable to work up a test case for this.  I'll try to put something together in the next few days.. ",
    "SneakyMax": "Also happens if you call .promise(). Also happens if you call .promise(). ",
    "varenc": "@jeskew any idea where this fits on to the AWS SDK's roadmap?  As I said in another issue:\nHaving it default to false is like embedding a non-deterministic bug that'll cause problems for ~0.1% of users in everyone's AWS implementation. Any site with lots of diverse users will eventually have to deal with this likely after their implementation has gone into production. . @jeskew what about defaulting to true if \"systemClockOffset\" still has the default, 0, value?  Are there any workarounds in place that wouldn't be using systemClockOffset?  If there's a chance there are some, then yes I agree it'd need to be part of a new major version for backward compatibility.\nIn the meantime, perhaps having the JS SDK docs mention \"correctClockSkew\" anywhere at all would be helpful.  (but I'm sure changing documentation is a whole other process). ",
    "etiennemunnich": "There are 2x different approaches to try:\nAWS.S3 Object option:\n\"correctClockSkew (Boolean) \u2014 whether to apply a clock skew correction and retry requests that fail because of an skewed client clock. Defaults to false.\"\nhttps://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#constructor_details\nAWS.Config Class option:\n\"(Boolean) correctClockSkew (readwrite)\nReturns whether to apply a clock skew correction and retry requests that fail because of an skewed client clock. Defaults to false.\nReturns: (Boolean) \u2014 whether to apply a clock skew correction and retry requests that fail because of an skewed client clock. Defaults to false.\"\n http://docs.amazonaws.cn/en_us/AWSJavaScriptSDK/latest/AWS/Config.html. ",
    "bravokeyl": "Since Buffer is a global in nodejs, may I know what's the use of doing this require('buffer').Buffer? Same goes to others.. ",
    "solankipriti": "No, but if I remove lib setting it does go away.\nand after removing the setting\nimport S3 = require('aws-sdk/clients/s3');\n S3.listBuckets(params, function(err, data) {\n     if (err) console.log(err, err.stack); // an error occurred\n    else     console.log(data);    \n    });\nI am getting error TS2339: Property 'listBuckets' does not exist on type 'typeof S3'. error\n. No, but if I remove lib setting it does go away.\nand after removing the setting\nimport S3 = require('aws-sdk/clients/s3');\n S3.listBuckets(params, function(err, data) {\n     if (err) console.log(err, err.stack); // an error occurred\n    else     console.log(data);    \n    });\nI am getting error TS2339: Property 'listBuckets' does not exist on type 'typeof S3'. error\n. I have removed the lib section and successfully able to perform S3.listBuckets. . I have removed the lib section and successfully able to perform S3.listBuckets. . ",
    "adamdry": "I copied the lib section from the referenced tsconfig.json here https://github.com/aws/aws-sdk-js/blob/master/ts/tsconfig.json as per the readme.md.\nAdding that lib section broke my TS compilation. Removing it seems to work fine...\nDoes that example need updating?. I copied the lib section from the referenced tsconfig.json here https://github.com/aws/aws-sdk-js/blob/master/ts/tsconfig.json as per the readme.md.\nAdding that lib section broke my TS compilation. Removing it seems to work fine...\nDoes that example need updating?. Sure, with the issue it was:\n{\n  \"compilerOptions\": {\n    \"target\": \"es2017\",\n    \"module\": \"commonjs\",\n    \"outDir\": \"dist\",\n    \"sourceMap\": true,\n    \"lib\": [\n      \"es5\",\n      \"es2015.promise\"\n    ]\n  },\n  \"files\": [\n    \"./node_modules/@types/node/index.d.ts\"\n  ],\n  \"include\": [\n    \"src/**/*.be.ts\"\n  ],\n  \"exclude\": [\n    \"node_modules\",\n    \"src/**/*.spec.ts\"\n  ]\n}\nRemoving the lib section altogether fixed the issue and everything seems happy. You guys mentioned listBuckets which I'm not using so maybe that wouldn't work correctly... ?. Sure, with the issue it was:\n{\n  \"compilerOptions\": {\n    \"target\": \"es2017\",\n    \"module\": \"commonjs\",\n    \"outDir\": \"dist\",\n    \"sourceMap\": true,\n    \"lib\": [\n      \"es5\",\n      \"es2015.promise\"\n    ]\n  },\n  \"files\": [\n    \"./node_modules/@types/node/index.d.ts\"\n  ],\n  \"include\": [\n    \"src/**/*.be.ts\"\n  ],\n  \"exclude\": [\n    \"node_modules\",\n    \"src/**/*.spec.ts\"\n  ]\n}\nRemoving the lib section altogether fixed the issue and everything seems happy. You guys mentioned listBuckets which I'm not using so maybe that wouldn't work correctly... ?. ",
    "anurbol": "For me adding \"es5\" to libs helped. For me adding \"es5\" to libs helped. ",
    "chux0519": "the same issue. ",
    "oaleynik": "Hi @jeskew! Thank you for response. Node actually has ISO-8859-1 encoding - it is aliased to 'latin1'.\n```\n\nBuffer.from('\u00a9')\n\nBuffer.from('\u00a9', 'latin1')\n\n```\n\nI'm not sure how this can help. I will definitely try to use your solution though, but with included filename= variant which has replaced/dropped symbols which are outside of ISO-8859-1. Thanks for the explanation!. Hi @jeskew! Thank you for response. Node actually has ISO-8859-1 encoding - it is aliased to 'latin1'.\n```\n\nBuffer.from('\u00a9')\n\nBuffer.from('\u00a9', 'latin1')\n\n```\n\nI'm not sure how this can help. I will definitely try to use your solution though, but with included filename= variant which has replaced/dropped symbols which are outside of ISO-8859-1. Thanks for the explanation!. ",
    "Usamaliaquat123": "set AWS.config.correctClockSkew = true its working \u2764\ufe0f. but yes i checked my system clock it is all set according to my region. What about aws-cli get again same error... ",
    "wmagda": "Yes I'm using Cognito Identity SDK so \nCognitoUtil.getCognitoUser().getSession(...)\nis really \nAWSCognito.CognitoIdentityServiceProvider.CognitoUser(userData).getSession(...)\n. ",
    "Halimao": "@AllanFly120, Thank you very much, it works(^_^) . ",
    "mbp": "Oh! Thanks. ",
    "JoyceBabu": "Switching to FIFO queue solved the issue. ",
    "pruhstal": "@jeskew Do you know if it's possible to do it with only using boto3? . ",
    "breathe": "I don't actually need to generate buckets anywhere other than us-east-1 at the moment -- however I am adapting these requests from a different data-source which always specifies the region.  Maybe I will at some point specify a region other than us-east-1.  I can solve for now as above it just seems messy . I don't actually need to generate buckets anywhere other than us-east-1 at the moment -- however I am adapting these requests from a different data-source which always specifies the region.  Maybe I will at some point specify a region other than us-east-1.  I can solve for now as above it just seems messy . Do you not need to specify it if you want to create the bucket in a particular region?. Do you not need to specify it if you want to create the bucket in a particular region?. Hmm, yes I see now ... -- maybe relying on that behavior is the right thing for my use as well.  I'll close this.. Hmm, yes I see now ... -- maybe relying on that behavior is the right thing for my use as well.  I'll close this.. ",
    "Lanceshi2": "I am getting the error unable to get property 'Location' of undefined or null reference error in IE11. In Chrome it work well. \nThe error is on (107175, 13) in aws-sdk-2.77.0.js. I find it hard to debug this issue as I can't locate where is the exact line which has caused this. \nI have checked the file object I have passed for uploading which seems correct to me. . @jeskew I'd love to. But I am not sure what to include. I am calling S3 service to ap-southeast-2 region. \nThe code I am using is: \nif(file) {\n        var params = {\n            Key: 'content-upload-development/' + file.name,\n            ContentType: file.type,\n            Body: file\n        };\n        bucket.upload(params).on('httpUploadProgress', function(evt){\n            var percent = parseInt((evt.loaded * 100) / evt.total);\n            var signedFormData = null;\n            $(document).trigger(\"AmazonStatusUpdateEvent\", [\"Uploading\", file.name, percent, signedFormData]);\n        }).send(function(err, data) {\n            var signedFormData = {};\n            signedFormData.fileURL = data.Location;\n            signedFormData.fileSize = \"400m\";\n            $( document ).trigger( \"AmazonStatusUpdateEvent\", [ \"Completed\", file.name, 100, signedFormData ] ); \n        });\n    }\nfile is got from event.target.files. From an  tag. Is there any other information I need to provide here? . @jeskew Thank you. You are right! I am getting the error:  Object doesn't support property or method 'addEventListener'. I will investigate this one.... @jeskew Thank you for the response. I know this has been a while but we still haven't figured out which line has caused the issue. Is there a way we can find which line has caused this issue? . @jeskew And I am not quite sure what does callback supplied to the sdk mean. It doesn't seem to me that I have used an addEventListener in the callback function... . @jeskew I have put a break point into that line but it seems that it has never been reached. . ",
    "theluk": "closing in favor of #1673 . closing in favor of #1673 . ",
    "rucas": "@theluk Getting this error too on release v2.95.0 went down a version and seems to work. Would love to get this merged!. ",
    "colinhemphill": "Good idea on the util method. Thanks for this \ud83d\udc4d . Good idea on the util method. Thanks for this \ud83d\udc4d . Good call - just updated \ud83d\udc4d . ",
    "roman-io": "I can confirm that something similar is happening in SDK version 2.279.1 with the Kinesis stream putRecords function. We are using this function in conjunction with Async module. 5 seconds after receiving a valid callback, it's being called again, and the Async module throws an exception Error: Callback was already called... ",
    "KMNowak": "Thank you @AllanFly120 for very useful answer. According to it, would you please suggest best way to block/allow access to resources on the level of API Gateway with use of access token? . ",
    "ArnoTF": "I think I know where it is coming from. I will reopen this when needed.. ",
    "shinoda-akm": "Got the same problem :(. ",
    "aar6ncai": "hi guys. plz close it \ud83d\udc4d found the problem . ",
    "seaBass3": "I figured it out with the help of AWS support.  The solution is:\nMetadata: {'test':'just a test'}. \"Can you check in the networking tab that you see x-amz-meta-office in the Access-Control-Expose-Headers header, and that you also see the x-amz-meta-office header itself returned in the S3 response?\"\nI not sure what you mean here.  Where's the network tab in the s3 management console?. I am testing in the Javascript SDK but I was also looking for it in the s3 console.  Shouldn't I be able to see the new tag after I've applied it by checking the folder I just applied it to and clicking \"more\" and \"change metadata\"?  It doesn't look like it is coming down s3.headObject either.. I'll be darn, it is coming down with s3.headObject!  Just can't see it in the s3 Management Console.  Weird!. Thanks again for your help.  I was able to take your advice and figure this out.  I have everything working the way I need it.  I love the fact that I can work directly with the S3 from the browser and not have to go through a PHP server!  Have a great day!. ",
    "dkesler": "node version is 7.10.0.\nheader from the request: 'x-amz-request-id': 'F8F2960C57F5B3D7',\nI tried s3ForcePathStyle:true and that does avoid the issue.  . node version is 7.10.0.\nheader from the request: 'x-amz-request-id': 'F8F2960C57F5B3D7',\nI tried s3ForcePathStyle:true and that does avoid the issue.  . it's all alphanumeric and dashes.  . it's all alphanumeric and dashes.  . ",
    "hashans": "@chrisradek I got the endpoint from the AWS IOT console in settings screen. \nGot it working. Thank you very much. Set the Iot constructor like this.\n```\nvar iot = new AWS.Iot({\n    endpoint: \"iot.us-east-1.amazonaws.com\",\n    region: \"us-east-1\",\n    accessKeyId: \"XXXXXXXXXXXXXXXX\",\n    secretAccessKey: \"XXXXXXXXXXXXXXXX\"\n});\n```. ",
    "dumbird": "The SDK version is 2.72. Node version is 6.3.0. We only ran the program in cluster mode, and it's handling multiple requests from client at the same time, so there are multiple requests running concurrently. We've been running the same tests for some time, but this problem just showed up yesterday. One possibility is that now there are more files under the same prefix, but the prefix includes some hash, well distributed. There are only fewer than 20 files under the prefixes where upload had issue.. The SDK version is 2.72. Node version is 6.3.0. We only ran the program in cluster mode, and it's handling multiple requests from client at the same time, so there are multiple requests running concurrently. We've been running the same tests for some time, but this problem just showed up yesterday. One possibility is that now there are more files under the same prefix, but the prefix includes some hash, well distributed. There are only fewer than 20 files under the prefixes where upload had issue.. Some update: we encountered another S3 upload failure, but this time we got the callback with an internal error, even though we got the progress report multiple times. In the previous failures, we didn't get the progress report so often, and we had a timer for the uploading process to kill the job once it's beyond 30 seconds. Not sure if we will eventually get a callback if the timer didn't kick in.\nThis is the related log:\n2017-08-25 12:18:59.044 DEBUG [16292] [aws_s3] Starting s3 upload; uuid=74D09101-BC33-4AD4-BA9A-20EEC2D1FF99\n2017-08-25 12:18:59.044 DEBUG [16292] [aws_s3] region: us-west-2, bucket: qaperf-email-bucket\n2017-08-25 12:18:59.058 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:18:59.542 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:00.088 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:01.279 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:02.188 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:02.882 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:03.747 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:04.886 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:02.182 ERROR [16292] [aws_s3] Error uploading to S3 { message: 'We encountered an internal error. Please try again.',\n  code: 'InternalError',\n  region: null,\n  time: '2017-08-25T12:19:02.182Z',\n  requestId: 'C1DF69DEABA3751E',\n  extendedRequestId: 'EDsKq/IdVsw3J8K9qXK5N0/MyAtq5l8WH4XQ98fCpvePcoc8qVlCdC0Ia2nuppfyV3SPws0llDI=',\n  statusCode: 500,\n  retryable: true }. Some update: we encountered another S3 upload failure, but this time we got the callback with an internal error, even though we got the progress report multiple times. In the previous failures, we didn't get the progress report so often, and we had a timer for the uploading process to kill the job once it's beyond 30 seconds. Not sure if we will eventually get a callback if the timer didn't kick in.\nThis is the related log:\n2017-08-25 12:18:59.044 DEBUG [16292] [aws_s3] Starting s3 upload; uuid=74D09101-BC33-4AD4-BA9A-20EEC2D1FF99\n2017-08-25 12:18:59.044 DEBUG [16292] [aws_s3] region: us-west-2, bucket: qaperf-email-bucket\n2017-08-25 12:18:59.058 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:18:59.542 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:00.088 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:01.279 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:02.188 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:02.882 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:03.747 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:04.886 DEBUG [16292] [aws_s3] Uploading file... Status: { loaded: 222682,\n  total: 222682,\n  part: 1,\n  key: '74D0_2579_2799/74D09101-BC33-4AD4-BA9A-20EEC2D1FF99.1.eml' }\n--\n2017-08-25 12:19:02.182 ERROR [16292] [aws_s3] Error uploading to S3 { message: 'We encountered an internal error. Please try again.',\n  code: 'InternalError',\n  region: null,\n  time: '2017-08-25T12:19:02.182Z',\n  requestId: 'C1DF69DEABA3751E',\n  extendedRequestId: 'EDsKq/IdVsw3J8K9qXK5N0/MyAtq5l8WH4XQ98fCpvePcoc8qVlCdC0Ia2nuppfyV3SPws0llDI=',\n  statusCode: 500,\n  retryable: true }. Any update on this? We're seeing more issues like this. Is there any workaround? We're close to our release date and this would be a major bug for us. Thanks.. Any update on this? We're seeing more issues like this. Is there any workaround? We're close to our release date and this would be a major bug for us. Thanks.. No, we haven't. Because our major concern is the missing callback. If we got the callback with failure  our code will try a secondary region. But thanks for asking, we will check with S3 support.. No, we haven't. Because our major concern is the missing callback. If we got the callback with failure  our code will try a secondary region. But thanks for asking, we will check with S3 support.. Unfortunately no. With 200k files uploaded, only a few failed. It's not the file itself for sure, because we use a few templates in different size to generate testing files. And the files are evenly distributed in the process, so we don't see any pattern.. Unfortunately no. With 200k files uploaded, only a few failed. It's not the file itself for sure, because we use a few templates in different size to generate testing files. And the files are evenly distributed in the process, so we don't see any pattern.. The issue of not getting callback seemed to be caused by we didn't wait long enough (we had a 30 seconds timer), but we're still seeing occasional slow upload (about 0.5% of files uploading took more than 10 seconds, with no large files). The issue of not getting callback seemed to be caused by we didn't wait long enough (we had a 30 seconds timer), but we're still seeing occasional slow upload (about 0.5% of files uploading took more than 10 seconds, with no large files). ",
    "vipseixas": "Had the same problem at a Cordova/Ionic App in a Android 6 tablet. Using the dynamoDbCrc32: false option solved the issue.. ",
    "gbrits": "Thanks - worked for me too, when using AWS Amplify.\n```\nimport AWS from 'aws-sdk';\nAWS.config.update({\n  dynamoDbCrc32: false\n});\n```. ",
    "clarkgrg": "Thanks - worked for me using AWS-sdk with react-native. ",
    "elarbee": "Thanks @chrisradek . Worked for me with react-native same as @clarkgrg . . ",
    "jonface": "The sig v4 spec, it is possible to specify content-type. although it seems after setting signatureVersion: 'v4'  on the S3 object and setting the signed url param ContentType, it is possible to upload any content type via the signed url. It seems the ContentType is not actually part of the signature.\nI assume this is incorrect?\nIt sounds similar to the go SDK issue https://github.com/aws/aws-sdk-go/issues/503\nThanks. In https://github.com/aws/aws-sdk-js/blob/488327a234ddef160a1592e717dd1e437560119a/lib/signers/v4.js#L190\nit looks like content-type is not a signable header, but it should be from what I understand. Can someone verify this? If I've taken this off topic I'll start a new issue.\nThanks\n. Hi @AllanFly120  thanks for the reply. I'm still confused, specifically sig v4 only,\n\nIf the Content-Type header is present in the request, you must add it to the CanonicalHeaders list\n\nAll the headers in CanonicalHeaders get signed if I'm not mistaken, so content-type is optional, but it is still a signed header if available (in contrast to the SDK which ignores it).\nBut from what I can see, the AWS SDK JS removes the content-type header, even though it should be signed for v4, if present. Because of this, the final signed url, allows the user to upload any content-type as it was removed from the signature generation. \nSurely content-type just needs removing from unsignableHeaders for v4 signatures?\nFrom what I can see, the AWS SDK JS is not following the sig v4 spec.\nThanks again. ",
    "leonetosoft": "The same problem happens in my network. The same problem happens in my network. I noticed that the problem happens in my house only, would it be a problem in the modem? What could be happening.... I noticed that the problem happens in my house only, would it be a problem in the modem? What could be happening.... @jeskew @paolavness \nThanks for the answers, I set {queueSize: 1} and now I'm not having the problem.\nFor example, if I am sending a 1GB file, then I close the application, when I reopen I want to continue from where it stopped ... The s3 is able to recognize a part that was already complete and move on to next ...?\nAll the resources that I find always restart from 0. :(. @jeskew @paolavness \nThanks for the answers, I set {queueSize: 1} and now I'm not having the problem.\nFor example, if I am sending a 1GB file, then I close the application, when I reopen I want to continue from where it stopped ... The s3 is able to recognize a part that was already complete and move on to next ...?\nAll the resources that I find always restart from 0. :(. @jeskew @paolavness \nOne more question, I noticed that there is a lot of savings using the gzip stream, is there any way to send the stream through a browser upload from the generation of an upload policy? Or just using multipart upload?. @jeskew @paolavness \nOne more question, I noticed that there is a lot of savings using the gzip stream, is there any way to send the stream through a browser upload from the generation of an upload policy? Or just using multipart upload?. I just read: https://docs.aws.amazon.com/en/AmazonS3/latest/API/RESTCommonResponseHeaders.html\n\nIf an object is created by either the Multipart Upload or Part Copy operation, the ETag is not an MD5 digest, regardless of the method of encryption.\n\nHow do I check to see if my client has had problems with data transport?. I just read: https://docs.aws.amazon.com/en/AmazonS3/latest/API/RESTCommonResponseHeaders.html\n\nIf an object is created by either the Multipart Upload or Part Copy operation, the ETag is not an MD5 digest, regardless of the method of encryption.\n\nHow do I check to see if my client has had problems with data transport?. @srchase\nI am sending through MultiPart Upload, sending through the gzip stream, create the request, partition in 2MB parts, send the parts and then complete the upload.\nCode of class send multipart:\nhttps://github.com/leonetosoft/upaki-cli/blob/master/src/upaki/S3Stream.ts\nCode multipart start:\n`async MultipartUpload(localPath: string, cloudPath: string, session: S3StreamSessionDetails, config: { maxPartSize: number; concurrentParts: number }, meta = {}, lastModify = undefined): Promise {\n        let credentials = await this.MakeUpload(Util.getFileSize(localPath), cloudPath, meta, lastModify);\n    var read = fs.createReadStream(localPath);\n    var compress = zlib.createGzip();\n\n    let upStream = new S3Stream(new AWS.S3({\n        correctClockSkew: true,\n        accessKeyId: credentials.credentials.AccessKeyId,\n        secretAccessKey: credentials.credentials.SecretAccessKey,\n        sessionToken: credentials.credentials.SessionToken,\n    }), session);\n\n    let upload = upStream.Upload({\n        Bucket: credentials.bucket,\n        Key: credentials.key,\n        ServerSideEncryption: 'AES256',\n        ContentType: \"application/octet-stream\",\n        ContentEncoding: 'gzip',\n    });\n\n    // Optional configuration\n    upStream.setMaxPartSize(config.maxPartSize); // 20 MB\n    upStream.setConcurrentParts(config.concurrentParts);\n\n    upload.on('completeUpload', (details) => {\n        try {\n            this.CompleteUpload(credentials.file_id);\n            upload.emit('uploaded', { Etag: details.ETag.replace(/\"/g, ''), file_id: credentials.file_id, folder_id: credentials.folder_id });\n        } catch (error) {\n\n        }\n    });\n\n    upload.on('error', async (err) => {\n        try {\n            if (err.code === 'EXPIRED_TOKEN') {\n                let newCredentials = await this.MakeUpload(Util.getFileSize(localPath), cloudPath, meta);\n                upStream.client.config.credentials = new AWS.Credentials({\n                    accessKeyId: newCredentials.credentials.AccessKeyId,\n                    secretAccessKey: newCredentials.credentials.SecretAccessKey,\n                    sessionToken: newCredentials.credentials.SessionToken\n                });\n            }\n        } catch (error) {\n            upStream.abortUpload('Failed to upload a part to S3: ' + JSON.stringify(error));\n        }\n    });\n\n    upload.on('abort', () => {\n        upStream.Abort();\n    });\n\n    upload.on('aborted', () => {\n        try {\n            compress.unpipe(upStream.getStream());\n            read.unpipe(compress);\n            upStream.getStream().destroy();\n            compress.destroy();\n            read.destroy();\n            // read.close();\n            read = null;\n        } catch (error) {\n            console.log(error);\n        }\n    });\n\n    upload.on('pause', (pause) => {\n        // Escutar chamadas de pause\n        if (pause) {\n            upStream.pause();\n        } else {\n            upStream.resume();\n        }\n    });\n\n    read.pipe(compress).pipe(upStream.getStream());\n    return upload;\n\n}`\n\n. @srchase\nI am sending through MultiPart Upload, sending through the gzip stream, create the request, partition in 2MB parts, send the parts and then complete the upload.\nCode of class send multipart:\nhttps://github.com/leonetosoft/upaki-cli/blob/master/src/upaki/S3Stream.ts\nCode multipart start:\n`async MultipartUpload(localPath: string, cloudPath: string, session: S3StreamSessionDetails, config: { maxPartSize: number; concurrentParts: number }, meta = {}, lastModify = undefined): Promise {\n        let credentials = await this.MakeUpload(Util.getFileSize(localPath), cloudPath, meta, lastModify);\n    var read = fs.createReadStream(localPath);\n    var compress = zlib.createGzip();\n\n    let upStream = new S3Stream(new AWS.S3({\n        correctClockSkew: true,\n        accessKeyId: credentials.credentials.AccessKeyId,\n        secretAccessKey: credentials.credentials.SecretAccessKey,\n        sessionToken: credentials.credentials.SessionToken,\n    }), session);\n\n    let upload = upStream.Upload({\n        Bucket: credentials.bucket,\n        Key: credentials.key,\n        ServerSideEncryption: 'AES256',\n        ContentType: \"application/octet-stream\",\n        ContentEncoding: 'gzip',\n    });\n\n    // Optional configuration\n    upStream.setMaxPartSize(config.maxPartSize); // 20 MB\n    upStream.setConcurrentParts(config.concurrentParts);\n\n    upload.on('completeUpload', (details) => {\n        try {\n            this.CompleteUpload(credentials.file_id);\n            upload.emit('uploaded', { Etag: details.ETag.replace(/\"/g, ''), file_id: credentials.file_id, folder_id: credentials.folder_id });\n        } catch (error) {\n\n        }\n    });\n\n    upload.on('error', async (err) => {\n        try {\n            if (err.code === 'EXPIRED_TOKEN') {\n                let newCredentials = await this.MakeUpload(Util.getFileSize(localPath), cloudPath, meta);\n                upStream.client.config.credentials = new AWS.Credentials({\n                    accessKeyId: newCredentials.credentials.AccessKeyId,\n                    secretAccessKey: newCredentials.credentials.SecretAccessKey,\n                    sessionToken: newCredentials.credentials.SessionToken\n                });\n            }\n        } catch (error) {\n            upStream.abortUpload('Failed to upload a part to S3: ' + JSON.stringify(error));\n        }\n    });\n\n    upload.on('abort', () => {\n        upStream.Abort();\n    });\n\n    upload.on('aborted', () => {\n        try {\n            compress.unpipe(upStream.getStream());\n            read.unpipe(compress);\n            upStream.getStream().destroy();\n            compress.destroy();\n            read.destroy();\n            // read.close();\n            read = null;\n        } catch (error) {\n            console.log(error);\n        }\n    });\n\n    upload.on('pause', (pause) => {\n        // Escutar chamadas de pause\n        if (pause) {\n            upStream.pause();\n        } else {\n            upStream.resume();\n        }\n    });\n\n    read.pipe(compress).pipe(upStream.getStream());\n    return upload;\n\n}`\n\n. @srchase \nThanks for the answer !!\nGot it, I'll change the code so that it reports this header!\nBut I have several files already hosted without this heading ....\nWould there be any way to change the old files with the correct header?. @srchase \nThanks for the answer !!\nGot it, I'll change the code so that it reports this header!\nBut I have several files already hosted without this heading ....\nWould there be any way to change the old files with the correct header?. Thank you very much,\nI will be doing this, however I have to download several files, read their bytes, generate MD5 and then perform Copy.. Thank you very much,\nI will be doing this, however I have to download several files, read their bytes, generate MD5 and then perform Copy.. ",
    "itchingpixels": "@jeskew : I can not seem to use the existing ones because of the path we need to use to use the react-native module.\ncode:\nimport * as AWS from 'aws-sdk/dist/aws-sdk-react-native'\nresults:\napp/src/login/helpers.ts(1,22): error TS7016: Could not find a declaration file for module 'aws-sdk/dist/aws-sdk-react-native'. '/Users/mark/Dev/drops-react-native/node_modules/aws-sdk/dist/aws-sdk-react-native.js' implicitly has an 'any' type.\n  Try `npm install @types/aws-sdk/dist/aws-sdk-react-native` if it exists or add a new declaration (.d.ts) file containing `declare module 'aws-sdk/dist/aws-sdk-react-native';`. Great, thank you!\nOriginally we were using the react-native branch, with https://github.com/AirLabsTeam/react-native-aws-cognito-js.\nWe're still using react-native-aws-cognito-js, but I switched to using release 2.111.0 and to: \nimport * as AWS from 'aws-sdk'\nI needed to fix some type error, but now getting this runtime error when starting the react-native app:\n\nI cleaned everything, reinstalled all modules, etc., but nothing helped. this can be closed, I can not reproduce this error with the latest version of the lib.. ",
    "mveera21": "Hi,\nCould you please provide solution to our problem? It seems others are hitting the same problem, and it's posted on the below forum. I have lost whole day today to debug this issue.\nhttps://forums.aws.amazon.com/thread.jspa?threadID=263089&tstart=0\nThank you. RyanB@AWS said in the forum \"We identified and addressed an issue with treating request headers as case-sensitive that affected some customers' authentication using the Node.js and .NET AWS SDKs in the US West (Oregon) region. The issue was fixed and measures have been put in place to prevent this issue from reoccurring. The duration of this impact was from 9/6 15:45 PST to 9/7 9:45 PST. Thank you for bringing this issue to our attention\", so I'm closing this defect. We are evaluating AWS IOT reliability  and scalability to bring it to our product, and I hope we don't hit this kind of outage in the production. \nThank you for taking care this issue.. ",
    "syberkitten": "Having a similar yet different issue\nWhen using putObject, I don't receive back the Location property,\nonly Etag.\nWhen using upload instead, I'm getting back Location property\nbut the object itself has 0 bytes.\nIt does not matter which size I upload, it becomes 0 Bytes.\n. @hgonzalez94  It's quite mind bogging to me that Amazon\ndoes not have such basic things figured out after so many years.\nSounds like It better to look for other solutions.\nright now when i upload a file using putObject, it mostly times out\n... and bunch of tickets on this:\nhttps://github.com/aws/aws-sdk-js/issues/1235\nhttps://github.com/aws/aws-sdk-js/issues/281\n. Thanks for the lead, integrated the headObject so here is the response for upload:\n```\n  \"headData\": {\n    \"AcceptRanges\": \"bytes\",\n    \"LastModified\": \"2017-09-11T20:08:25.000Z\",\n    \"ContentLength\": 0,\n    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n    \"ContentType\": \"application/octet-stream\",\n    \"Metadata\": {}\n  }\n//and this for putobject:\n{\n  \"AcceptRanges\": \"bytes\",\n  \"LastModified\": \"2017-09-11T20:52:33.000Z\",\n  \"ContentLength\": 4473,\n  \"ETag\": \"\\\"f3e3ada967e9f95d2c9428ddb80c7ded\\\"\",\n  \"ContentType\": \"application/octet-stream\",\n  \"Metadata\": {}\n}\n```\nUsing Node Version 7.10.0\nAWS Sdk: 2.112.0\nThis is a sample of the code, I'm using TypeScript so it's a blend\nand should be easy to read, works both for putObject and upload,\njust upload returns 0 bytes as before.\n```\n  const params: PutObjectRequest = {\n    Bucket: config.S3_OTA_UPDATES_BUCKET,\n    Key: folder + '/' + payloadID,\n    Body: stream,\n    ACL: 'public-read',\n    ContentType: 'application/octet-stream',\n  }\nreturn new Promise(async (resolve, reject) => {\nlet error\nlet putResponse: UpdatePayload\n\n// Simply change upload to putObject, same signature, difference in results :(\n// [error, putResponse] = await to(s3.putObject(params).promise())\n\n[error, putResponse] = await to(s3.upload(params).promise())\n\nif (error) {\n  reject(error)\n}\n\nif (putResponse) {\n  const headData: any = await s3.headObject({\n    Bucket: config.S3_OTA_UPDATES_BUCKET,\n    Key: folder + '/' + payloadID,\n  }).promise()\n\n  const updatePayload: UpdatePayload = {\n    _id: payloadID,\n    timestamp: (new Date()).getTime(),\n    md5: hash,\n    //url: resp.Location,\n    url: createBucketResourceLocation(folder, payloadID),\n    headData: headData,\n  }\n  resolve(updatePayload)\n}\n\n})\n``` . Upgraded to Node 8.4.0 Stable, still same result.\nThe only problem with putObject for me is the timeout it seems to have \nuploading big files (larger the 1mb) on slow connections.\nGetting: \nRequestTimeout: Your socket connection to the server was not read from or written to within the timeout period. Idle connections will be closed.\nthis issue:\nhttps://github.com/aws/aws-sdk-js/issues/1633\nand this:\nhttps://github.com/aws/aws-sdk-js/issues/281\nand this:\nhttps://github.com/aws/aws-sdk-java/issues/1101\n. I have a version somewhere which uses multipart upload\nas an express middleware, I'll try uploading something large. :). @vnenkpet \nstarted using s3-upload-stream which use the S3 multipart upload API\nwhich does a perfect job.\nhttps://www.npmjs.com/package/s3-upload-stream\nI was able to upload files up to 1gb of size with upload times of 1.5 hours\nand it also supports resuming uploads and many more.\nno need to re-invent the wheel, we should use battle prone software\nand not shaky flacky un-tested and fragmented API calls ;)\nhere's a code sample:\n```\nimport {S3StreamUploader, S3WriteStream} from 's3-upload-stream'\nconst params: PutObjectRequest = {\n    Bucket: config.S3_OTA_UPDATES_BUCKET,\n    Key: folder + '/' + payloadID,\n    //Body: stream,\n    ACL: 'public-read',\n    ContentType: 'application/octet-stream',\n  }\nconst s3 = new AWS.S3({accessKeyId: config.S3_AK, secretAccessKey: config.S3_SK, signatureVersion: config.S3_SIGNATURE_VERSION, region: config.S3_REGION })\nconst s3Uploader: S3StreamUploader = s3Stream(s3)\nconst stream = fs.createReadStream(fileName)\nconst upload: S3WriteStream = s3Uploader.upload(params)\nstream.pipe(upload)\nupload.concurrentParts(1)\nupload.on('error', (error) => {\n  console.log(error)\n})\n\n/* Handle progress. Example details object:\n   { ETag: '\"f9ef956c83756a80ad62f54ae5e7d34b\"',\n     PartNumber: 5,\n     receivedSize: 29671068,\n     uploadedSize: 29671068 }\n*/\nupload.on('part', (details) => {\n  logger.info(details, `Uploaded part...}`)\n})\n\nupload.on('uploaded', async (details) => {\n})\n```. ",
    "hggz": "Yeah so after spending way too much time on this, (it hasn't been resolved) I'll share my findings.\n1) I am able to send the base64 encoded data to aws s3 with putobject but don't receive the location property too, only the Etag.\n2) the item I put up on s3 is corrupted because the data uploaded wasnt its binary-string form.\n3) if I DO attempt to upload the data using its binary blob form I get an error with XMLHTTPRequest.js saying that 'the request has already been sent' midway through the upload. \n4) the main reason I tried using putObject instead of the recommended upload\nmethod was because I also receive the same 'XMLHTTPRequest.js' error mid upload. Looking at the XMLHTTPRequest error, its thrown when the 'readyState' property is set to true while its trying to perform a send operation.\n5) I was getting this behavior while utilizing the Cognito credentials. \n:( my heart. Hey chris thanks for your reply.\nSo by 'corrupted' (my apologies, it was a poor choice of words) I mean I'm unable to view the images on s3 from the aws side. I do upload the base 64 string fine but Ideally I would be able to upload the binary string using the sdk so I can view it from s3, or in any other medium I download.\nI hope you can see why I would prefer to have it in its default image representable format thats generally easy to migrate among different platforms, instead of forcing any other application of mine to adhere to this standard Im setting solely from the react native. \nBecause of this, If I wanted to access my images, not only would I have to set up proper aws access credentials but I would also have to translate it from its base 64 string to its binary string, and thats just another step I don't want to have to take. \nThe method you uploaded above doesn't work for me. It times out and throws an XMLHTTPRequest error saying that 'The request has already been sent' mid upload. The putObjectRequest DOES work if I provide it the base64 data string itself. What you're doing above is taking an image object which, when it gets uploaded, has its binary string representation uploaded.\nTo anyone else interested in my work around, I chose to just post the binary string manually through a signed request I made, conforming to another web posting option aws offers.. I'm on iOS, v 0.36.0 like I posted above. When I try submitting the binary format XMLHttpRequest gives me an error mid upload saying \"The request has already been sent\". I can follow that error in the source for the request but I don't know why it happens mid way. Same behavior on android . Hey @chrisradek I'll shelve this for now, but I am unable to upgrade to that version of react native at this time. I have a work around for now, but I'll have to see about upgrading at a later time.. ",
    "shrinandhini2801": "Hi Am trying to upload an image and download that and render in  tag of react native. I have tried with the solution which @chrisradek  have suggested. But it says S3 is not a constructor so i cant create an object and assign the configurations to it . Please advice. Also i am trying with aws amplify library and below is the code ,\n```\nstoreImageInS3 = () => {\n        console.log(\"inside store image in s3\" + JSON.stringify(this.state.ImageSource))\n        Storage.put(\"test-image\", this.state.ImageSource.uri)\n            .then(result => console.log(result))\n            .catch(err => console.log(err));\n    }\ngetImageFromS3 = () => {\n    Storage.get(\"test-image\")\n        .then(result => {\n            this.setState({ ImageFroms3: { uri: result } });\n            console.log(\"ImageFroms3 state==\" + JSON.stringify(this.state.ImageFroms3))\n        })\n        .catch(err => console.log(err));\n\n\n}\n\n```\nI am able to upload , but when i download its not n a proper format . I have tried with base 64 as well as image uri . Its not working. can someone help in resolving this???. ",
    "lawrencegrey": "\nHi Am trying to upload an image and download that and render in  tag of react native. I have tried with the solution which @chrisradek have suggested. But it says S3 is not a constructor so i cant create an object and assign the configurations to it . Please advice. Also i am trying with aws amplify library and below is the code ,\nstoreImageInS3 = () => {\n      console.log(\"inside store image in s3\" + JSON.stringify(this.state.ImageSource))\n      Storage.put(\"test-image\", this.state.ImageSource.uri)\n          .then(result => console.log(result))\n          .catch(err => console.log(err));\n  }\ngetImageFromS3 = () => {\n      Storage.get(\"test-image\")\n          .then(result => {\n              this.setState({ ImageFroms3: { uri: result } });\n              console.log(\"ImageFroms3 state==\" + JSON.stringify(this.state.ImageFroms3))\n          })\n          .catch(err => console.log(err));\n}\nI am able to upload , but when i download its not n a proper format . I have tried with base 64 as well as image uri . Its not working. can someone help in resolving this???\n\nJust display the link to the image like this \n  \nvar imageFileName = \"https://s3.us-east-2.amazonaws.com/nameOfFolder/nameOfFile.ext\";\n. ",
    "vnenkpet": "I'm second this. It happens to us occassionally, I simply pass the ReadStream to putObject, response is fine, file is there at our end, I can check the size etc, but successful upload to S3 often results in zero bytes object.\nThe behaviour is also pretty inconsistent. The same file in the same requests usually fails to upload in like 30% of the time.. ",
    "nadrane": "I'm running into this issue as well. Has any headway been made towards resolving it?\nI'm using aws-sdk 2.176.0.\nI'd love to not have to switch to a 3rd party library. ",
    "notoriaga": "I was having a similar problem when passing a http.IncomingMessage stream to s3.upload. If I tried to measure the size of the file while uploading I would always get the 0 byte object. However, if I added a pause to the stream before the upload it all worked as intended. I'm not sure this is exactly your problem, but hopefully it helps. I think in general it might be a problem with adding event listeners to a stream before passing it to s3.\n```\n  let fileSize = 0;\n  req.on('data', chunk => {\n    fileSize += Buffer.byteLength(chunk);\n  });\n// Without this I get a 0 byte object\n  req.pause();\ns3.upload({ Key: s3Key, Body: req }, {}, (err, results) => {\n    ...\n  });\n```. ",
    "cpttripzz": "it would be nice to add this to the docs to save other devs from wasting hours / days \nreq.pause();\nkudos to @notoriaga for the fix. it would be nice to add this to the docs to save other devs from wasting hours / days \nreq.pause();\nkudos to @notoriaga for the fix. ",
    "ngrilly": "\nI'm second this. It happens to us occassionally, I simply pass the ReadStream to putObject, response is fine, file is there at our end, I can check the size etc, but successful upload to S3 often results in zero bytes object.\n\n@vnenkpet Have you solved the issue?\n. ",
    "TanveerDayan": "I have this same issue. Response is fine. But successful upload to s3 causes a 0 byte file. And the behaviour is very inconsistent.. ",
    "pavvell": "I use PassThrough solution that works just fine.\nconst sourceFile: ReadableStream;\nconst streamCopy = new PassThrough();\nsourceFile.pipe(streamCopy);\n\naws.getInstanse().upload({ Body: streamCopy,... });\n\n. I use PassThrough solution that works just fine.\nconst sourceFile: ReadableStream;\nconst streamCopy = new PassThrough();\nsourceFile.pipe(streamCopy);\n\naws.getInstanse().upload({ Body: streamCopy,... });\n\n. ",
    "jeffbyrnes": "Thanks for spotting that, @chrisradek; I totally missed their explanation while diving into this.\nInteresting that Bluebird.promisify doesn\u2019t log that warning, even though error.message is the same null value. \u00af\\_(\u30c4)_/\u00af\nIt feels kinda gross to set the message to an empty string when it is nonexistent (and thus, null is, to my mind, the correct value).\nI\u2019ll defer to you!. ",
    "khiem-nguyen": "@chrisradek It happened when making ajax call to kinesis...amazonaws.com\nIt's not sdk problem, it's certificate problem, I think. Old version of chrome doesn't approve ssl certificate of aws endpoint, because when I access the endpoint directly, \"not safety\" came out.\n(Android 4.2, chrome 53). ",
    "napcoder": "@chrisradek thank you for suggestion. Just created lambda and pasted your code directly on lambda web interface, using my IAM role, and replacing my connection pool id and provider name. It seems to work properly. This means the problem is somewhere in the code or in serverless framework.. Solved: the problem was due to a wrong endpoint in the AWS general config, loaded by my dynamodb data layer. ",
    "navaati": "It's huuuuuugely annoying to have to use ! absolutely everywhere !. ",
    "MatthiasKunnen": "Duplicate of #1553. ",
    "leantide": "Thank you. \nEven as a reserved word, I'm surprised I could still create using the 'state' field. The reservedness seems to be only with the update command.. ",
    "rahulitzme94": "The issue was with the fake s3 location the access to that location was denied. @manticarodrigo  the location provided in the command ie, fakes3 -r /mnt/fakes3_root -p 4567 fakeS3 does need permission for accessing the location /mnt/fakes3_root. so could you try the same command with sudo access?. ",
    "manticarodrigo": "@rahulitzme94 I thought fake s3 didn't care about permissions. I'm getting this error now and it was working fine yesterday. What should I do?. ",
    "PrimeObjects": "Hi Chrisradek,\nThank you for the help! It was actually the config ContentEncoding: 'base64' caused the problem. I simply removed it and it works fine now on Android and iOS.\nThanks\nGary. Hi Chrisradek,\nThank you for the help! It was actually the config ContentEncoding: 'base64' caused the problem. I simply removed it and it works fine now on Android and iOS.\nThanks\nGary. Hi jeskew, \nYou know what I don't know why it was there at the first place. My team is new to React Native. I believe it was added by our React Native developer (he only has his hands dirty on RN for one week) who worked on file upload function. It was escalated up to me while he found the code worked on iOS but not on Android. I knew little about React Native, I tried to play around the configs and it worked after I removed this Content-Encoding. Simply luck. It took us time to find the issue, nobody questioned about config because it worked on iOS.\nGary. Hi jeskew, \nYou know what I don't know why it was there at the first place. My team is new to React Native. I believe it was added by our React Native developer (he only has his hands dirty on RN for one week) who worked on file upload function. It was escalated up to me while he found the code worked on iOS but not on Android. I knew little about React Native, I tried to play around the configs and it worked after I removed this Content-Encoding. Simply luck. It took us time to find the issue, nobody questioned about config because it worked on iOS.\nGary. ",
    "CFCSystem": "I am using putObject. Because ?. Thanks for the attention, I'm waiting for a new version!. ",
    "shahzeb1": "Never mind, found in docs.. ",
    "mgeissen": "On this site it stands that the params are not Required.. On this site it stands that the params are not Required.. Hi @jeskew,\nI'm a noob! I checked my region settings and in the credentials.json I'm using some other region.\nSorry for the unnecessary  request and thanks for your support!\nSorry for my . Hi @jeskew,\nI'm a noob! I checked my region settings and in the credentials.json I'm using some other region.\nSorry for the unnecessary  request and thanks for your support!\nSorry for my . ",
    "alex-birch": "Hi @jeskew ,\nI think I just encountered the same issue with the util library.\nThis library is referenced in the AWS IoT documentation.\nIn order to use it I had to cast as well:\nimport * as AWSx from 'aws-sdk';\n...\nvar AWS = AWSx as any;\nvar kDate = AWS.util.crypto.hmac('AWS4' + key, date, 'buffer');\nShould the util library be included in the typings as it's referenced in the public facing docs, or am I doing something wrong?\nThanks\n  . Hi @jeskew ,\nI think I just encountered the same issue with the util library.\nThis library is referenced in the AWS IoT documentation.\nIn order to use it I had to cast as well:\nimport * as AWSx from 'aws-sdk';\n...\nvar AWS = AWSx as any;\nvar kDate = AWS.util.crypto.hmac('AWS4' + key, date, 'buffer');\nShould the util library be included in the typings as it's referenced in the public facing docs, or am I doing something wrong?\nThanks\n  . ",
    "atraver": "Similarly, when reading about how to implement alerting for ElasticSearch, both NodeHttpClient and Signers are used in the example code: https://aws.amazon.com/blogs/database/implementing-alerting-on-amazon-elasticsearch-data/\nAre these classes truly \"implementation details,\" and if so, what are the alternatives (especially since AWS's blogs/docs are suggesting their use)?. ",
    "ArnfinnA": "Hello \nSounds like good plan. I like it. Hi hi \nMvh Arnfinn Aurs\u00f8y\n\n\nmai 2018 kl. 23:54 skrev Adam Traver notifications@github.com:\n\nSimilarly, when reading about how to implement alerting for ElasticSearch, both NodeHttpClient and Signers are used in the example code: https://aws.amazon.com/blogs/database/implementing-alerting-on-amazon-elasticsearch-data/\nAre these classes truly \"implementation details,\" and if so, what are the alternatives (especially since AWS's blogs/docs are suggesting their use)?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Can everyone please try to help me keep the information from the hackers. \n\nThanks from me. \nMvh Arnfinn Aurs\u00f8y\n\n\nmai 2018 kl. 23:01 skrev Christopher Radek notifications@github.com:\n\n@Misiur\nNope, you're right, it doesn't currently support promise, but it could. Since it doesn't currently return anything, we could probably just make it return a promise directly if no callback is provided, and continue to return undefined either. Marking as a feature request, but if you feel inclined would be happy to review a PR for this!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Hello.\n\nSorry for all the waiting for you right now. But I\u2019m trying not to get stuffed with all the blame for all of the things for the shit that has been. bought at Amazon, Goggle, Apple, Adobe and Microsoft. So I\u2019m using my time to try to cancel all of my accounts that I have. Not so easy for I\u2019m in  Norway \ud83c\uddf3\ud83c\uddf4 and all of the support is in a different country. \u2639\ufe0f \nBut I have a question for all of you. That I\u2019m trying to get my Aws account  back, but I don\u2019t think \ud83e\udd14, know that Amazon has approved this? I have to be sure about that before I start up my account again. \n But it would have been nice \ud83d\udc4d to talk with you about everything actually \ud83d\ude09\ud83d\udc4c if you got time for it? Think \ud83e\udd14 about it then you can contact me.\nBest regards Arnfinn Aursoey \ud83c\uddf3\ud83c\uddf4. ",
    "adcreare": "I think its fairly clear that this isn't a \"private\" part of the SDK. \nUse of these methods ss all over the aws sample code and documentation. . ",
    "caki0915": "+1 For this one\nAWS.HttpClient is missing types as well. \nIt is used together with AWS.Signers in AWS Elastic Search Node Documentation\n. ",
    "pnkjgupta111": "Yes, the application is using Amazon Mobile Analytics JavaScript SDK.. Can you provide me the exact library's link? Thanks in advance\n. Thank you :). ",
    "ffoysal": "THanks @chrisradek . It was wrong sdk version. now working.. ",
    "uzimith": "I used aws-sdk@2.121.0.\nIn AWS Lambda with Node, I want to define Message interface by aws-sdk types.\n```\nimport { MetricName, Namespace, Statistic, Dimensions, Period, StandardUnit, EvaluationPeriods, Threshold, ComparisonOperator, TreatMissingData, EvaluateLowSampleCountPercentile } from 'aws-sdk/clients/cloudwatch';\ninterface Message {\n    AlarmName: string;\n    AlarmDescription: string | null;\n    AWSAccountId: string;\n    NewStateValue: string;\n    OldStateValue: string;\n    NewStateReason: string;\n    StateChangeTime: string;\n    Region: string;\n    Trigger: Trigger;\n}\ninterface Trigger {\n    MetricName?: MetricName;\n    Namespace?: Namespace;\n    Statistic?: Statistic;\n    StatisticType?: string;\n    Dimensions?: Dimensions;\n    Period?: Period;\n    Unit?: StandardUnit;\n    EvaluationPeriods?: EvaluationPeriods;\n    Threshold?: Threshold;\n    ComparisonOperator?: ComparisonOperator;\n    TreatMissingData?: TreatMissingData;\n    EvaluateLowSampleCountPercentile?: EvaluateLowSampleCountPercentile;\n}\nexport function handler(event: any, context: any) {\n    const message: Message = JSON.parse(event.Records[0].Sns.Message);\n    // some codes.\n}\n```\nHowever, when I trigger this lambda by SNS from CloudWatch status change action, this SNS Record is like below code.\n{\n  \"Type\": \"Notification\",\n  \"MessageId\": \"uuid\",\n  \"TopicArn\": \"arn:aws:sns:ap-northeast-1:...\",\n  \"Subject\": \"OK: \\\"...\\\" in Asia Pacific (Tokyo)\",\n  \"Message\": \"{\\\"AlarmName\\\":\\\"...\\\",\\\"AlarmDescription\\\":null,\\\"AWSAccountId\\\":\\\"...\\\",\\\"NewStateValue\\\":\\\"OK\\\",\\\"NewStateReason\\\":\\\"...\\\",\\\"StateChangeTime\\\":\\\"2017-10-02T18:50:03.901+0000\\\",\\\"Region\\\":\\\"Asia Pacific (Tokyo)\\\",\\\"OldStateValue\\\":\\\"INSUFFICIENT_DATA\\\",\\\"Trigger\\\":{\\\"MetricName\\\":\\\"HTTPCode_ELB_5XX\\\",\\\"Namespace\\\":\\\"AWS/ELB\\\",\\\"StatisticType\\\":\\\"Statistic\\\",\\\"Statistic\\\":\\\"SUM\\\",\\\"Unit\\\":null,\\\"Dimensions\\\":[{\\\"name\\\":\\\"LoadBalancerName\\\",\\\"value\\\":\\\"...\\\"}],\\\"Period\\\":000,\\\"EvaluationPeriods\\\":0,\\\"ComparisonOperator\\\":\\\"GreaterThanOrEqualToThreshold\\\",\\\"Threshold\\\":000.0,\\\"TreatMissingData\\\":\\\"\\\",\\\"EvaluateLowSampleCountPercentile\\\":\\\"\\\"}}\",\n  \"Timestamp\": \"2017-10-02T18:50:04.011Z\",\n  \"SignatureVersion\": \"1\",\n  \"Signature\": \"...\",\n  \"SigningCertUrl\": \"....pem\",\n  \"UnsubscribeUrl\": \"...\",\n  \"MessageAttributes\": {}\n}\nSo I thought dimension's actual type is interface {name: string; value: string}.. I used aws-sdk@2.121.0.\nIn AWS Lambda with Node, I want to define Message interface by aws-sdk types.\n```\nimport { MetricName, Namespace, Statistic, Dimensions, Period, StandardUnit, EvaluationPeriods, Threshold, ComparisonOperator, TreatMissingData, EvaluateLowSampleCountPercentile } from 'aws-sdk/clients/cloudwatch';\ninterface Message {\n    AlarmName: string;\n    AlarmDescription: string | null;\n    AWSAccountId: string;\n    NewStateValue: string;\n    OldStateValue: string;\n    NewStateReason: string;\n    StateChangeTime: string;\n    Region: string;\n    Trigger: Trigger;\n}\ninterface Trigger {\n    MetricName?: MetricName;\n    Namespace?: Namespace;\n    Statistic?: Statistic;\n    StatisticType?: string;\n    Dimensions?: Dimensions;\n    Period?: Period;\n    Unit?: StandardUnit;\n    EvaluationPeriods?: EvaluationPeriods;\n    Threshold?: Threshold;\n    ComparisonOperator?: ComparisonOperator;\n    TreatMissingData?: TreatMissingData;\n    EvaluateLowSampleCountPercentile?: EvaluateLowSampleCountPercentile;\n}\nexport function handler(event: any, context: any) {\n    const message: Message = JSON.parse(event.Records[0].Sns.Message);\n    // some codes.\n}\n```\nHowever, when I trigger this lambda by SNS from CloudWatch status change action, this SNS Record is like below code.\n{\n  \"Type\": \"Notification\",\n  \"MessageId\": \"uuid\",\n  \"TopicArn\": \"arn:aws:sns:ap-northeast-1:...\",\n  \"Subject\": \"OK: \\\"...\\\" in Asia Pacific (Tokyo)\",\n  \"Message\": \"{\\\"AlarmName\\\":\\\"...\\\",\\\"AlarmDescription\\\":null,\\\"AWSAccountId\\\":\\\"...\\\",\\\"NewStateValue\\\":\\\"OK\\\",\\\"NewStateReason\\\":\\\"...\\\",\\\"StateChangeTime\\\":\\\"2017-10-02T18:50:03.901+0000\\\",\\\"Region\\\":\\\"Asia Pacific (Tokyo)\\\",\\\"OldStateValue\\\":\\\"INSUFFICIENT_DATA\\\",\\\"Trigger\\\":{\\\"MetricName\\\":\\\"HTTPCode_ELB_5XX\\\",\\\"Namespace\\\":\\\"AWS/ELB\\\",\\\"StatisticType\\\":\\\"Statistic\\\",\\\"Statistic\\\":\\\"SUM\\\",\\\"Unit\\\":null,\\\"Dimensions\\\":[{\\\"name\\\":\\\"LoadBalancerName\\\",\\\"value\\\":\\\"...\\\"}],\\\"Period\\\":000,\\\"EvaluationPeriods\\\":0,\\\"ComparisonOperator\\\":\\\"GreaterThanOrEqualToThreshold\\\",\\\"Threshold\\\":000.0,\\\"TreatMissingData\\\":\\\"\\\",\\\"EvaluateLowSampleCountPercentile\\\":\\\"\\\"}}\",\n  \"Timestamp\": \"2017-10-02T18:50:04.011Z\",\n  \"SignatureVersion\": \"1\",\n  \"Signature\": \"...\",\n  \"SigningCertUrl\": \"....pem\",\n  \"UnsubscribeUrl\": \"...\",\n  \"MessageAttributes\": {}\n}\nSo I thought dimension's actual type is interface {name: string; value: string}.. ",
    "schatekar": "@chrisradek  That worked like a charm. Thanks. . ",
    "b3llash": "It is indeed a duplication issue.\nThank you very much!. ",
    "sabrehagen": "Hi @jeskew, how did you go asking around? Can we get something merged? :). What lends further credence to this is that aws-sdk is globally available in the lambda execution environment which saves packaging aws-sdk with each lambda function.\nIf this functionality were added into aws-sdk this would benefit all lambda users, and would not require them to deploy a thinly-wrapped copy of aws-sdk with each lambda (which appears to be the only option currently).. Hi @jeskew, do you have any updates on this? I am having trouble seeing any reason why this isn't a useful addition to the library. Thanks :). > I know many libraries will enable debug mode if process.env.NODE_ENV !== 'production', but I'm not sure if suddenly supporting that configuration setting and emitting a bunch of log messages would be reasonable behaviour for a library frequently used as a dependency of other libraries.\nI agree. Most developers will be operating in a process.env.NODE_ENV !== 'production' environment during their daily work, so enabling log output by default in this environment will not be desirable. It should be opt in only.\n\nWould you be open to choosing a new mechanism for enabling debug mode?\n\nWhat mechanism do you propose? I've proposed the use of an environment variable as this is de facto in the Node.js ecosystem (example 1, example 2, example 3, example 4, example 5).\n\nI would be much more open something that makes it clear that the mechanism is specific to this library\n\nBy supplying the AWS prefix to the environment variable AWS_DEBUG I believe it is clear that this debug option is only specific to the AWS libraries.. I see where you're coming from. AWSJS_DEBUG works for me.. I've updated the environment variable name. What's the ETA for getting this merged?. @jeskew Hello again, I wanted to ping you on this issue as I encounter the need for it daily, and want to see what I can do to help get it in the SDK :). Hi again @jeskew, it's been a bit over a week since I've heard from you. What would you like done to this PR to get it merged?\nI have a severe need for this feature every day, often first thing when I start work. Debugging the AWS SDK operations by having to modify the codebase every time I want to see what the SDK is doing is exceptionally time consuming and fragile. This explains my persistence with following up :). ",
    "sastrygunnu": "I`m able to generate the pre-signed URL for my bucket, i made HTTP request to get my object from the call but its returns (1000 object per call) and its returns XML data.\nMy question is that  how to make another call and find next set of objects from the same url and update my array? i m unable use s3 methods to get entire object from my bucket  something like \nbelow\n s3.listObjects(params).\n                on('success', function handlePage(r) {\n                if (r.hasNextPage()) {\n                    r.nextPage().on('success', handlePage).send();\n                    getdata(r.data.Contents)\n\n                }else{\n\ngetdata(r.data.Contents)\n}\nis there any other way around to get full object from pre-signed url using multiple http requests. \ni have to use pre-signed url to get access to entire object for given bucket.\ncan you help?\n. Yep it's working thanks you can close this now. ",
    "waelsy123": "any updates in here?. ",
    "kent-williams": "Thank you @jeskew! I knew I was probably missing something.\nAppreciate the help!. ",
    "shreya-work": "That fixed the problem. . ",
    "ashrafSeven": "@jeskew can you show me one example to upload the image.. image is coming in base64 format from fron-end so how to mention body and key. can you show me one example in node.js.. ",
    "kevinsimper": "@jeskew We could publish a new version of aws-sdk called something like minimal or bare or es6 or slim? or aws-sdk-node?. ",
    "CoreyAR": "I'm getting a timeout error when building react-native locally because of the file size. \nTimeoutError: transforming  .../node_modules/aws-sdk/dist/aws-sdk-react-native.js took longer than 601 seconds.. I tried importing with import AWS from 'aws-sdk' and still got the error. We are on react-native \n version 0.51.0. We are currently on aws-sdk version 2.224.1 and not experiencing any issues.. ",
    "Janardhanrao": "@CoreyAR \nAny Update on this issue?\nThanks in advance. ",
    "rekha110254": "@CoreyAR \nThanks for the reply it is working now . ",
    "vardhanv": "Thank you. Is the error not reported in the call back, so I can slow down my request rate?\u00a0\nSent from Yahoo Mail for iPhone\nOn Monday, October 9, 2017, 12:57 AM, Jonathan Eskew notifications@github.com wrote:\nThe SDK will retry ThrottlingException errors, but there is a maximum number of times that will be retried before the error is surfaced. By default, if a Glacier client encounters a throttling error on its 3rd retry, then the request will fail. You can adjust the number of retries that will be attempted by setting the maxRetries configuration parameter:\nconst glacier = new AWS.Glacier({\n    region: 'us-west-2',\n    maxRetries: 5\n});\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. ",
    "keviin55": "and the other recipient will never be able to delete the message too? I still don't get it. I can keep a message in the queue for more than 12 hours, but when I process it after the 12 hours, I can never delete it again?. Thank you for the explanation . ",
    "mattbryson": "This does happen to be on an older platform from a few years back (we were making an update to a legacy app).\nBut the same property name is used on all the other EB environments that we have. The newest one we have running is..\n64bit Amazon Linux 2016.09 v2.3.0 \nWhich I assume is September 2016, so only just over a year old.  Maybe this change has been made more recently than reported?  Ill spin up a 2017 and see.\n. This does happen to be on an older platform from a few years back (we were making an update to a legacy app).\nBut the same property name is used on all the other EB environments that we have. The newest one we have running is..\n64bit Amazon Linux 2016.09 v2.3.0 \nWhich I assume is September 2016, so only just over a year old.  Maybe this change has been made more recently than reported?  Ill spin up a 2017 and see.\n. I just span up a \n64bit Amazon Linux 2017.03 v2.5.0\nAnd it has the same issue..\n\nMaybe they changed from AWS_SECRET_ACCESS_KEY to AWS_SECRET_KEY 2-3 years ago :)\n. I just span up a \n64bit Amazon Linux 2017.03 v2.5.0\nAnd it has the same issue..\n\nMaybe they changed from AWS_SECRET_ACCESS_KEY to AWS_SECRET_KEY 2-3 years ago :)\n. ",
    "MatheusParanhos": "@jeskew Thanks for the reply and for clarifying that! cheers\n. ",
    "frangeris": "Hey @MatheusParanhos did you find a way? \nI'm struggle with this and the only how to it's using UI custom in app integrations.. @zhongqin0820 end up using firebase auth, easier and cleaner....  \u00af\\_(\u30c4)_/\u00af. ",
    "zhongqin0820": "how do you solve this problem finally? @MatheusParanhos @frangeris . thanks @frangeris . ",
    "sqlbot": "\nI\u2019m not sure it would be possible to access a presigned S3 URL through a CloudFront web distribution.\n\n@jeskew for what it's worth... it's technically \"possible\" but it's awkward, rarely useful, and not the intended solution.  Here's what's required:\n\nYou have to sign the request with the exact Host: header that CloudFront will use in the request it sends the bucket (testing suggests that this is always ${bucket}.s3.amazonaws.com, regardless of the bucket's region), and\nYou have to specify the object key with the path constructed the way the bucket will see it in the final request, not as it will appear in the URL sent to CloudFront (the path that S3 sees may have more components on the beginning if you use an origin path to prepend a string, or may differ if you're using Lambda@Edge to rewrite part of the path), and\nYou have to configure the distribution's Cache Behavior to \"forward all, cache based on all\" of the query string, and\nYou have to string-replace the path components in the resulting signed URL to match what CloudFront expects, if it differs from the actual object key (again, this is only in cases where you have configured CloudFront or Lambda@Edge to prepend or rewrite part of the path), and\nYou have to string-replace the hostname in the host portion (only) of the resulting signed URL with the hostname of the CloudFront distribution.  \n\nSome of these steps seem counter-intuitive, particularly the idea of modifying a pre-signed URL without invalidating it -- but it works because you're modifying the URL in such a way that it's valid after CloudFront modifies it (again) such that it will match the canonical request that was originally signed.  You're basically tweaking the signed URL so that it's for the final request, not the initial one, and then modifying the result to match what CloudFront is expecting.\nWhen you are done, you end up with a signed URL that will \"work,\" but when actually used, it results in CloudFront caching responses that it will never again actually serve from the cache... unless you're actually reusing that exact same identical signed URL, signature and all (unlikely).  This is because CloudFront (correctly) uses the entire forwarded request -- including the forwarded query string -- as the cache key used when doing a cache lookup.  So they requests will always be X-Cache: Miss from CloudFront, because CloudFront has never seen that identical request before (the signature and much of the rest of the query string differs).\nThe net result of this approach is that you end up with something very similar to S3 Transfer Acceleration -- you're transporting the request on the AWS \"edge network,\" which may speed up transfers (particularly to viewers more distant from the bucket), but not actually doing any caching.\nThis also requires that the distribution be configured without an origin access identity, and with \"Restrict Viewer Access\" set to disabled, because CloudFront isn't an active participant in the authentication -- it just passes through the requests.  Without the signature, S3 will still deny the requests if the bucket and objects aren't public.\nUsing the CloudFront signing solution is definitely the way to go, except perhaps in rare cases.. ",
    "thibaultvanluchene": "Changing property name  from AWS.config.credentials to creds fixed it:\n```javascript\n// Add the User's Id Token to the Cognito credentials login map.\n let creds = new AWS.CognitoIdentityCredentials({\n   IdentityPoolId: 'eu-central-1:831a9197-988c-4438-a426-10070c166144',\n   Logins: {\n     'cognito-idp.eu-central-1.amazonaws.com/eu-central-1_r4GImYiEJ': \n    result.getIdToken().getJwtToken()\n   }\n}, {region: 'eu-central-1'});\ncreds.refresh((data)=>{console.log(data)}); \n```\n. ",
    "mrfez": "Hi @AllanFly120 \nThanks for that - I wasn't. It's disconcerting when getSignedUrl works fine using synchronous, but createPresignedPost doesn't.. Hi @AllanFly120 \nThanks for that - I wasn't. It's disconcerting when getSignedUrl works fine using synchronous, but createPresignedPost doesn't.. ",
    "aflext": "@AllanFly120 Can i user the Prefix with regexp like file_name_with{.txt,jpg}. I specify the region to be emty but the sdk will set the region config to 'us-east-1', i think this may be a reason. ",
    "christian-bromann": "Hi @jeskew,\nthanks for your quick response. I missed that the elbv2.describeLoadBalancers params are optional filters. Thanks!. ",
    "againer": "Sure thing -- https://github.com/oohnoitz/node-gifsicle-stream/blob/master/lib/Gifsicle.js#L72.\nThis is the Stream object -- { unpipe: [Function: onunpipe],\ndrain: [Function],\nerror: [Function: onerror],\nclose: { [Function: g] listener: [Function: onclose] },\nfinish: { [Function: g] listener: [Function: onfinish] } },\n_eventsCount: 5,\n_maxListeners: undefined,\nargs: [ '-w', '-O3', '--resize-fit=nullx480' ],\nreadable: true,\nwritable: true,\nseenOutput: false,\nfinished: false }.\nfillStream should probably throw an error if read is missing from the Stream.\n. @AllanFly120 -- https://github.com/aws/aws-sdk-js/pull/1762.. *fixing tests.. ",
    "mikermcneil": "@AllanFly120 @againer Seeing this error in the Readable stream produced by the request package.  As far as I can tell, it's because request is using the .body property -- a passthrough stream is a decent workaround for this.  But it doesn't solve the big issue:\nThe main problem is that, regardless of the problematic nature of the inbound stream, since aws-sdk throws in an asynchronous callback in this case, this error crashes the process, and there's not a good solution to work around it in userland.\nSpecifically, I've seen this pop up in aws-sdk@2.265.1.  (Tried recently upgrading to 2.391.0 and hoping that prevents the crashing issue-- ~~haven't been able to determine whether or not upgrading fixes it yet, but wanted to share my findings in case it helps anyone else out~~ . EDIT: Turns out that upgrading to 2.391.0 does not solve the problem.). see also https://github.com/aws/aws-sdk-js/issues/2100. @steveLuo1 I like your PassThrough approach a lot, and I think it solves a lot of problems.  Thinking of emulating that in skipper-s3/sails-hook-uploads, only problem is I'm not sure when it was added to Node off the top of my head.  will post back if I find out. @steveLuo1 @chrisradek just verified you can use PassThrough as early as Node 8.0.0 (and probably earlier, judging by the date on this issue)\n\n. ",
    "shri3k": "@chrisradek Thank you for providing that list. :). ",
    "bennypowers": "Pardon my ignorance of the internal workings of your library, but it seems to me that you could release a minor version that doesn't break backwards, only adds an es-module version in /dist/es/ . Got it. Well I hope it comes soon \ud83d\ude09\nOn Mon, Oct 23, 2017, 8:40 PM Jonathan Eskew notifications@github.com\nwrote:\n\nThe semantics of ES modules are different enough from script-based module\nformats like CommonJS, UMD, and AMD that I don't think we could automate a\nconversion of the SDK from its current format to an ESM-based format. Since\nwe release several times a week, if the conversion cannot be automated,\nthen I'm not sure it's a feasible solution.\nThe conversion from ESM to CommonJS, however, is fairly straightforward,\nso if the SDK were written following ES module semantics, we could release\nboth versions. That, however, would require a major version bump.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1766#issuecomment-338739157,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABZgNKUmqdtvcP5FIC0aamq_pA5UUXXIks5svM-qgaJpZM4QAegK\n.\n. On that note, I'm experimenting with using that plugin to get the ball rolling, and actually successfully bundled using:\n\n```js\nimport sourcemaps from 'rollup-plugin-sourcemaps';\nimport resolve from 'rollup-plugin-node-resolve';\nimport builtins from 'rollup-plugin-node-builtins';\nimport globals from 'rollup-plugin-node-globals';\nimport commonjs from 'rollup-plugin-commonjs';\nimport json from 'rollup-plugin-json';\nexport default {\ninput: 'src/eve-redux/eve-redux-store.js',\noutput: {\n    file: 'src/bundle.js',\n    name: 'EVE',\n    format: 'iife',\n  },\nwatch: {\n    include: 'src/**',\n  },\nplugins: [\n    resolve(),\n    commonjs({\n      include: 'node_modules/**',\n      ignoreGlobal: true,\n      namedExports: {\n        'node_modules/inferno-redux/index.js': ['Provider'],\n        'node_modules/aws-sdk/global.js': ['util'],\n      },\n    }),\n    sourcemaps(),\n    json(),\n    builtins(),\n    globals(),\n  ],\n};\n``. Well, rollup doesn't give any errors, although browser complainsUncaught TypeError: Failed to resolve module specifier 'aws-sdk'`\nbundle.js:1\njs\nimport { S3 } from 'aws-sdk';. see also #1769 . By modifying the rollup.config.js slightly I've succeeded at bundling without error, however we're not out of the woods yet.\n```js\nimport sourcemaps from 'rollup-plugin-sourcemaps';\nimport resolve from 'rollup-plugin-node-resolve';\nimport builtins from 'rollup-plugin-node-builtins';\nimport globals from 'rollup-plugin-node-globals';\nimport commonjs from 'rollup-plugin-commonjs';\nimport json from 'rollup-plugin-json';\nexport default {\nexternal: ['aws-sdk'],\nglobals: {\n    'aws-sdk': 'AWS',\n  },\ninput: 'src/eve-redux/eve-redux-store.js',\noutput: {\n    file: 'src/bundle.js',\n    name: 'EVE',\n    format: 'iife',\n  },\nwatch: {\n    include: 'src/**',\n  },\nplugins: [\n    json(),\n    commonjs({\n      include: 'node_modules/**',\n      ignoreGlobal: true,\n      namedExports: {\n        'node_modules/inferno-redux/index.js': ['Provider'],\n        'node_modules/aws-sdk/global.js': ['util'],\n      },\n    }),\n    globals(),\n    builtins(),\n    resolve({\n      preferBuiltins: true,\n    }),\n    sourcemaps(),\n  ],\n};\n```\nRollup successfully bundles the SDK, however, the browser complains: \nbundle.js:2316 Uncaught TypeError: Cannot read property 'memoizedProperty' of undefined\nwhich takes us to this line: \njs\nvar memoizedProperty$1 = util_1.memoizedProperty;\nand \njs\nvar util_1 = util;\nand\njs\n/**\n * A set of utility methods for use with the AWS SDK.\n *\n * @!attribute abort\n *   Return this value from an iterator function {each} or {arrayEach}\n *   to break out of the iteration.\n *   @example Breaking out of an iterator function\n *     AWS.util.each({a: 1, b: 2, c: 3}, function(key, value) {\n *       if (key == 'b') return AWS.util.abort;\n *     });\n *   @see each\n *   @see arrayEach\n * @api private\n */\nvar util = {//...insert AWS util obj here...}. ",
    "SUpermin6u": "ran into the same error, how to resolve it?. ran into the same error, how to resolve it?. ",
    "rntgspr": "I got the same problem running on pure Node@6.10.\nI don't have sure if order of lines should be the problem in this case, but i have:\n5893: var memoizedProperty = util_1.memoizedProperty;\n20350: var util_1 = util$1;\n19431: var util$1 = { // amazon code\n. ",
    "powerful235": "Any update? Facing this issue too.. ",
    "IAmBrandonMcGregor": "Check out Amazon's documentation for using aws-iot-device-sdk-js in the browser. https://github.com/aws/aws-iot-device-sdk-js#browser\nIt's pretty crazy, but you have to manually compile the JS and include it separately via ",
    "btakita": "import AWS from 'aws-sdk' seems to be working for me. Is this the case for anybody else?. ",
    "RedHatter": "How is this still an issue? This makes the library completely unusable for anyone using rollup. I would expect a bug of this magnitude to be fixed quickly but it's been over a year and no work as been done on it.\nI'm by no means an expert with rollup but I suspect the issue arises from the multitude of circular dependencies.. ",
    "THPubs": "Just a stupid mistake. I console logged process.env.AWS_S3_REGION inside the function and it worked. But when the function initializes process.env.AWS_S3_REGION will be undefined because I only run dotenv.config() in app.js. I also need to run it in this file before everything.. ",
    "haikyuu": "I generated a presigned url and successfully uploaded a file using curl. But it does not work on react-native. I have tried using xhr and fetch and attaching an object {uri: \"file://...\"} to the body but i get the following error:\nExpected dynamic type string but had type object (constructing arguments for Networking.SendRequest at argument index 1)\n\nI even tried using react-native-fetch-blob but i get the same error (with index 3).\nUsing form-data sends the request but S3 responds with error <Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message>\nAny clues how to upload a file using a presigned url in react-native? (hi @sibelius )\n. ",
    "Sangrene": "Any news on this ?. ",
    "sibelius": "I think we can do that on rn54 using fetch with blob. ",
    "nsaboo": "Thanks @chrisradek appreciate your help on merging it, looks like travis had some transient failure with this commit checkin.\nThanks,\nNanda.. ",
    "sam0x17": "@chrisradek ok I tried it that way and now I actually get a signed URL back (before I was putting signatureVersion in the wrong place, turns out), however I am still getting SignatureDoesNotMatch when I try to PUT to the signed URL\nHere is what I am putting in to my rest client:\n\nMaybe there is some header I am supposed to be sending that I am not sending??\nI am using the latest version of the aws-sdk npm package, i.e. I ran npm install aws-sdk --save this morning.. @chrisradek here is my code:\n```javascript\nconst secureRandom = require('secure-random');\nvar AWS = require('aws-sdk');\nAWS.config.loadFromPath('./credentials.json');\nvar s3 = new AWS.S3({signatureVersion: 'v2'});\nfunction randomAlphaNumeric(size) {\n    return secureRandom(size * 2, {type: 'Buffer'})\n      .toString('base64')\n        .replace(/[\\/+=]/g, '')\n        .substring(0, size);\n}\nexports.requestUploadUrl = function requestUploadUrl(req, res) {\n  var key = randomAlphaNumeric(16);\n  s3.getSignedUrl('putObject', {Key: key, Bucket: 'bitfort', Expires: 1000000}, function (err, url) {\n    res.send(url);\n    console.log('The URL is', url);\n    if(err) throw err;\n  });\n}\n```. @chrisradek I just tried with my s3 bucket and got the same issue, so I don't think it's specific to digitalocean.\nresponse from s3:\n<Error>\n<Code>SignatureDoesNotMatch</Code>\n<Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message>\n<AWSAccessKeyId>*****************</AWSAccessKeyId>\n<StringToSign>PUT multipart/form-data; boundary=----WebKitFormBoundaryzaMQH9PWv0AwF7K5 1509876896 /blockvue.datastore/some-key</StringToSign>\n<SignatureProvided>IEyfPAkg3OXO+Wi9w63CYrJSYsc=</SignatureProvided>\n<StringToSignBytes>50 55 54 0a 0a 6d 75 6c 74 69 70 61 72 74 2f 66 6f 72 6d 2d 64 61 74 61 3b 20 62 6f 75 6e 64 61 72 79 3d 2d 2d 2d 2d 57 65 62 4b 69 74 46 6f 72 6d 42 6f 75 6e 64 61 72 79 7a 61 4d 51 48 39 50 57 76 30 41 77 46 37 4b 35 0a 31 35 30 39 38 37 36 38 39 36 0a 2f 62 6c 6f 63 6b 76 75 65 2e 64 61 74 61 73 74 6f 72 65 2f 73 6f 6d 65 2d 6b 65 79</StringToSignBytes>\n<RequestId>5B058BD7B72D1719</RequestId>\n<HostId>2UbgHN+ECXQ5nb7EVX9Ci1vhNNFOGdF7Dh5ppCBUkfrKQQuJ6BCbbI3uuCvscGibOuamyCKV8d4=</HostId>\n </Error>. ah, that is probably my problem -- thank you!. @chrisradek ok I changed it to upload just a blob. I tried \"any file data\" (no content type) and application/octet-stream, still got the same error for both:\n\n. @chrisradek I was able to get it to work by specifying ContentType: 'application/octet-stream' both in the signed url and in the PUT request. That said, I need to upload enormous files so I need chunked/multi-part. Is that going to work with a PUT request, or should I be using POST?. @chrisradek OK in that case what do I need to name the file field for the POST? will it take anything?. @chrisradek in that case what is the max size I can squeeze into a single PUT? For my application, it is OK if the data is spread out across multiple objects on the server. thanks. ",
    "GDavisSS": "I installed the aws sdk as well as the api gateway generated sdk.  So yes. I will try asking on the AWS forums, but your response makes no sense.  It makes no difference if the generated sdk uses the AWS SDK or not.  My web page for testing my api would not work if I had not installed the AWS SDK.  I would not be able to use AWSCognito for authentication or do basic front-end/aws back-end interactions because they require the AWS SDK.\nWhat's important is that others have posted issues to github.com/aws/aws-sdk-js about the exact same problem and they were helped, but that info didn't help me.  So I posted my own issue and you're telling me to go somewhere else.  Why treat me differently?. ",
    "andrew-e": "Thanks jeskew, I'm so sorry I made a mistake in my reporting... it wasn't the putObject call that was failing, it was the getObject call.  And that also seems to be failing on the AWS CLI, so maybe I'm using it incorrectly!  I think I'm using it according to the docs though.\nI'm calling it with these params\nconst getParams = {\n            Bucket: bucket,\n            Key: key,\n            SSECustomerAlgorithm: \"aws:kms\",\n            SSECustomerKey: encryptionKey\n        }\nThe error above was in fact the getObject failure, so that bit remains the same. Ah, great to know.  Thanks for your help!. ",
    "liesislukas": "Same thing while updating childDirected from false to true. I've used new bot just like described in initial post and called putBot with:\n```\nconst updatedBot = {\n  name,\n  abortStatement: currentBot.data.abortStatement,\n  checksum: currentBot.data.checksum,\n  childDirected,\n  clarificationPrompt: currentBot.data.clarificationPrompt,\n  description: currentBot.data.description,\n  idleSessionTTLInSeconds: currentBot.data.idleSessionTTLInSeconds,\n  intents: currentBot.data.intents,\n  locale: currentBot.data.locale,\n  processBehavior: currentBot.data.processBehavior,\n  voiceId: currentBot.data.voiceId,\n};\n\nLex.modelBuilding.putBot(updatedBot, (error, data) => {\n  console.log(error, data);\n});\n\n```\n```\n{ TypeError: Cannot read property 'messages' of undefined\n  at ParamValidator.validateStructure (/....../node_modules/aws-sdk/lib/param_validator.js:59:25)\n  at ParamValidator.validateMember (/....../node_modules/aws-sdk/lib/param_validator.js:88:21)\n  at ParamValidator.validateStructure (/....../node_modules/aws-sdk/lib/param_validator.js:75:14)\n  at ParamValidator.validateMember (/....../node_modules/aws-sdk/lib/param_validator.js:88:21)\n  at ParamValidator.validate (/....../node_modules/aws-sdk/lib/param_validator.js:34:10)\n  at Request.VALIDATE_PARAMETERS (/....../node_modules/aws-sdk/lib/event_listeners.js:125:42)\n  at Request.callListeners (/....../node_modules/aws-sdk/lib/sequential_executor.js:105:20)\n  at callNextListener (/....../node_modules/aws-sdk/lib/sequential_executor.js:95:12)\n  at /....../node_modules/aws-sdk/lib/event_listeners.js:85:9\n  at finish (/....../node_modules/aws-sdk/lib/config.js:320:7)\n  at /....../node_modules/aws-sdk/lib/config.js:338:9\n  at Credentials.get (/....../node_modules/aws-sdk/lib/credentials.js:126:7)\n  at getAsyncCredentials (/....../node_modules/aws-sdk/lib/config.js:332:24)\n  at Config.getCredentials (/....../node_modules/aws-sdk/lib/config.js:352:9)\n  at Request.VALIDATE_CREDENTIALS (/....../node_modules/aws-sdk/lib/event_listeners.js:80:26)\n  at Request.callListeners (/....../node_modules/aws-sdk/lib/sequential_executor.js:101:18)\n  at Request.emit (/....../node_modules/aws-sdk/lib/sequential_executor.js:77:10)\n  at Request.emit (/....../node_modules/aws-sdk/lib/request.js:683:14)\n  at Request.transition (/....../node_modules/aws-sdk/lib/request.js:22:10)\n  at AcceptorStateMachine.runTo (/....../node_modules/aws-sdk/lib/state_machine.js:14:12)\n  at Request.runTo (/....../node_modules/aws-sdk/lib/request.js:403:15)\n  at Request.send (/....../node_modules/aws-sdk/lib/request.js:367:10)\n  at features.constructor.makeRequest (/....../node_modules/aws-sdk/lib/service.js:193:27)\n  at features.constructor.svc.(anonymous function) [as putBot] (/....../node_modules/aws-sdk/lib/service.js:499:23)\n  at Object._callee$ (/....../src/api/helpers/aws/helpers/bot/updateChildDirected.js:48:33)\n  at tryCatch (/....../node_modules/regenerator-runtime/runtime.js:65:40)\n  at Generator.invoke [as _invoke] (/....../node_modules/regenerator-runtime/runtime.js:299:22)\n  at Generator.prototype.(anonymous function) [as next] (/....../node_modules/regenerator-runtime/runtime.js:117:21)\n  at step (/....../src/api/helpers/aws/helpers/bot/updateChildDirected.js:1:287)\n  at /....../src/api/helpers/aws/helpers/bot/updateChildDirected.js:1:447\n  at \n  at process._tickDomainCallback (internal/process/next_tick.js:228:7)\n  message: 'Cannot read property \\'messages\\' of undefined',\n    code: 'TypeError',\n  time: 2017-11-06T15:41:42.610Z }\n```. @AllanFly120 Thank you for quick respond. While your statement is true, data was missing but the error did not say anything about it + i was able to create bot without all data provided but when i call to update it throws TypeError. I think this issue is still valid telling about poor developer experience while using sdk because of not informative messages. And TypeError is not something i would expect from aws. Seems like a bug.\nAlso AWS documentation says different story about all this:\n\n. ",
    "AWSSteveHa": "I'm trying to figure out the Codecov report error \"No coverage uploaded for pull request base (master@c108474)\", but I have yet to figure out how to fix it. Any advice would be appreciated. Thanks. . I'm still getting errors on the last Travis test having to do with ecs_credentials.d.ts. I'm not sure what the import should look like. I've tried it a couple of ways now.. Please let me know if there are any additional changes I need to make. If possible it would be really great if this could be pulled by EOD tomorrow. Thank you.. Ah of course. I added the fix and all tests appear to be passing now. Thx. Thanks for the catch. I should have removed that when I put it in ecs_credentials.js. Added fix to PR.. ",
    "mmorearty": "Hmm, I'm not sure how I could do that. For one thing, my change is just a change to the type declarations in the .d.ts file, which is useful for those devs who are writing TypeScript code that uses AWS; but your underlying actual code is written in JavaScript, and this commit did not change that code. Also, I can't find a file named /ts/credentialprovidertest.ts or, for that matter, any TypeScript tests at all. The closest thing I can find is /test/credential_provider_chain.spec.js (JavaScript) which tests the CredentialProviderChain class. That code does already have one test that calls AWS.CredentialProviderChain() with no arguments. Not that that really matters; since that code is JavaScript, it ignores the .d.ts file.\nIn other words, I don't think it's possible write a test for this.. Hmm, I'm not sure how I could do that. For one thing, my change is just a change to the type declarations in the .d.ts file, which is useful for those devs who are writing TypeScript code that uses AWS; but your underlying actual code is written in JavaScript, and this commit did not change that code. Also, I can't find a file named /ts/credentialprovidertest.ts or, for that matter, any TypeScript tests at all. The closest thing I can find is /test/credential_provider_chain.spec.js (JavaScript) which tests the CredentialProviderChain class. That code does already have one test that calls AWS.CredentialProviderChain() with no arguments. Not that that really matters; since that code is JavaScript, it ignores the .d.ts file.\nIn other words, I don't think it's possible write a test for this.. Ah, ok \u2014 done. Let me know if that was not what you had in mind.. Ah, ok \u2014 done. Let me know if that was not what you had in mind.. ",
    "dijonkitchen": "@jeskew so it's extending the Markdown syntax? Maybe whatever is being used should be compatible with markdown or change the file type? . I'm closing this since it looks like the latest docs have already made my change. \nIn the event that there are disconnects with the API documentation, just using a plain link will likely work for anything. . ",
    "dhawalmewada": "@chrisradek \nIt's already there. \n\nI even tried removing it and keeping it in tsconfig.app.json (which I manually created) into src folder but nothing helped.\n. @chrisradek , Following are my version information, in case if you require\nvar IonicDevServerConfig={\"sendConsoleLogs\":false,\"wsPort\":53703,\"appScriptsVersion\":\"3.1.2\",\"systemInfo\":[\"Ionic Framework: 3.9.2\",\"Ionic App Scripts: 3.1.2\",\"Angular Core: 5.0.1\",\"Angular Compiler CLI: 5.0.1\",\"Node: 6.11.4\",\"OS Platform: Windows 10\"]};. ",
    "mkamrani": "same has happened to me; trying to resolve it and will report if I find a resolution.. same has happened to me; trying to resolve it and will report if I find a resolution.. ",
    "gandresr": "Same problem here! +1. ",
    "paiajay": "Same problem here :-(. ",
    "DonGlazikBro": "37second and ))). https://chromium.googlesource.com/chromium/src/+/master/CODE_OF_CONDUCT.md. ",
    "Shery11": "same happened with me. I got the issue resolved. . ",
    "peterpeterparker": "@chrisradek sure, here you go\nxxxx-xx-xx.s3.dualstack.eu-west-1.amazonaws.com. @chrisradek thank you for the confirmation, good to hear you could reproduce it!\nI checked my git history to try to remember why I set this encoding to base64 and I found a commit in June 2016 with the comment Me: Encoding for S3 ... so well to be honest, except that I feel ashamed for such a bad comment, I can't tell you why I set this encoding.\nLike you said, there are examples containing this option online, I think I just based myself on them and since I manage the images as bas64 and upload them in this format, I probably thought this was the right way to go and since it was working like a charm...\nDo you want me to try 2.151.0and remove the ContentEncoding option to see if it works too?. @chrisradek yes you are totally right, removing ContentEncoding while using 2.151.0 totally solve the issue. thx a lot!. Thx for the clear explanation. I stay at your disposal if you need a tester. Already? Sweet, so fast. Thx a lot for the quick fix and support @chrisradek . ",
    "CanGokdere": "Hello @jeskew,\nI have updated the sdk but my issue with Content Disposition still persists. I have tested by using headers and also without headers and only using pre-signed url and query parameters. If I want to include content disposition in presigned url it still causes signature mismatch.. ",
    "tomgallagher": "Many thanks. You don't by any chance know if it's easy to pipe into a putObject operation from an existing stream?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Many thanks. You don't by any chance know if it's easy to pipe into a putObject operation from an existing stream?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. ",
    "bkarv": "All good, I created a promise instead:\nlet bucket: any = this.authService.getBucket();\n          var putObjectPromise = bucket.putObject(params).promise();\n          putObjectPromise.then((data) => {\n            console.log('Success', data);\n            this.test();\n          }).catch(function(err) {\n            console.log(err);\n          });. ",
    "squarewave24": "figured it out, I added this on server cors configuration and that got rid of the error \n<AllowedHeader>*</AllowedHeader>\n. ",
    "msnikhil": "In addition to @squarewave24's answer above, to upload new object to S3, add <AllowedMethod>PUT</AllowedMethod> to the CORS configuration file.. ",
    "thevictorchang": "Oops, turns out the Windows BYOL and Linux instance prices are meant to be the same, i need to set the 'licenseModel' parameter of my filter to \"No License required\" to get the price of a Windows license-included instance (a bit misleading...). ",
    "bomb-on": "@chrisradek \nOk, I'll try to ask on forums if I don't figure it out soon :)\nI did try what you suggested and if I remember correctly, in one of the tries (cannot remember which method I used exactly) the certificate ended up marked as \"ACTIVE\"... The thing is that it seems too simple, comparing what you have to do \"manually\" to create and activate the certificate. Also, I haven't seen anywhere in documentation instruction to do so and therefore I think it might not be a correct approach...\nThank you for your help!. @chrisradek \nOk, I'll try to ask on forums if I don't figure it out soon :)\nI did try what you suggested and if I remember correctly, in one of the tries (cannot remember which method I used exactly) the certificate ended up marked as \"ACTIVE\"... The thing is that it seems too simple, comparing what you have to do \"manually\" to create and activate the certificate. Also, I haven't seen anywhere in documentation instruction to do so and therefore I think it might not be a correct approach...\nThank you for your help!. ",
    "kaushalparik27": "@chrisradek I am writing Lambda right inside inline code editor in AWS console and I have not bundled my own. It will be a great help if you can provide me hint or documentation on how can I inspect lambda object in AWS console line editor.\nThe code that I posted above, is what exactly my lambda function is.. @chrisradek thx for this help!\nI see invoke is not showing in the log (invokeAsync is the one, though). Is that expected?\n```\n2017-12-05T18:27:43.045Z    faca2b09-d9e9-11e7-9286-b53eee0eeb97    lambda.constructor exists.\nlambda.addEventSource exists.\nlambda.deleteFunction exists.\nlambda.getEventSource exists.\nlambda.getFunction exists.\nlambda.getFunctionConfiguration exists.\nlambda.invokeAsync exists.\nlambda.listEventSources exists.\nlambda.listFunctions exists.\nlambda.removeEventSource exists.\nlambda.updateFunctionConfiguration exists.\nlambda.uploadFunction exists.\nlambda.initialize exists.\nlambda.validateService exists.\nlambda.loadServiceClass exists.\nlambda.getLatestServiceClass exists.\nlambda.getLatestServiceVersion exists.\nlambda.makeRequest exists.\nlambda.makeUnauthenticatedRequest exists.\nlambda.waitFor exists.\nlambda.addAllRequestListeners exists.\nlambda.setupRequestListeners exists.\nlambda.getSignerClass exists.\nlambda.serviceInterface exists.\nlambda.successfulResponse exists.\nlambda.numRetries exists.\nlambda.retryDelays exists.\nlambda.retryableError exists.\nlambda.networkingError exists.\nlambda.expiredCredentialsError exists.\nlambda.throttledError exists.\nlambda.endpointFromTemplate exists.\nlambda.setEndpoint exists.\nlambda.paginationConfig exists.\n\uf13f\n18:27:43\n2017-12-05T18:27:43.084Z    faca2b09-d9e9-11e7-9286-b53eee0eeb97    invoke exists: false\n. @chrisradek thanks for right direction! I believe it has something to do with API version. I deleted Lambda and recreated it again and now I see invoke method working fine!\nCould this be because of one fact that initially before starting on inline editor, I uploaded the lambda function using Visual Studio Toolkit (by creating a AWS Lambda node.js project in Visual Studio)?. Yes, I was using VSTS Tools for publishing initially for same lambda function. I confirm this was due to old API npm installed in my AWS node.js project. I did two tests:\n1. Created a new lambda function in inline editor and I found invoke function as log result:\n2017-12-06T06:31:36.431Z 19aab9b1-da4f-11e7-8a77-69dbeb7399b9 lambda.constructor exists.\nlambda.addPermission exists.\nlambda.createAlias exists.\nlambda.createEventSourceMapping exists.\nlambda.createFunction exists.\nlambda.deleteAlias exists.\nlambda.deleteEventSourceMapping exists.\nlambda.deleteFunction exists.\nlambda.getAccountSettings exists.\nlambda.getAlias exists.\nlambda.getEventSourceMapping exists.\nlambda.getFunction exists.\nlambda.getFunctionConfiguration exists.\nlambda.getPolicy exists.\nlambda.invoke exists.\nlambda.invokeAsync exists.\nlambda.listAliases exists.\nlambda.listEventSourceMappings exists.\nlambda.listFunctions exists.\nlambda.listTags exists.\nlambda.listVersionsByFunction exists.\nlambda.publishVersion exists.\nlambda.removePermission exists.\nlambda.tagResource exists.\nlambda.untagResource exists.\nlambda.updateAlias exists.\nlambda.updateEventSourceMapping exists.\nlambda.updateFunctionCode exists.\nlambda.updateFunctionConfiguration exists.\nlambda.setupRequestListeners exists.\nlambda.initialize exists.\nlambda.validateService exists.\nlambda.loadServiceClass exists.\nlambda.getLatestServiceClass exists.\nlambda.getLatestServiceVersion exists.\nlambda.customizeRequests exists.\nlambda.makeRequest exists.\nlambda.makeUnauthenticatedRequest exists.\nlambda.waitFor exists.\nlambda.addAllRequestListeners exists.\nlambda.getSignerClass exists.\nlambda.serviceInterface exists.\nlambda.successfulResponse exists.\nlambda.numRetries exists.\nlambda.retryDelays exists.\nlambda.retryableError exists.\nlambda.networkingError exists.\nlambda.timeoutError exists.\nlambda.expiredCredentialsError exists.\nlambda.clockSkewError exists.\nlambda.getSkewCorrectedDate exists.\nlambda.applyClockOffset exists.\nlambda.isClockSkewed exists.\nlambda.throttledError exists.\nlambda.endpointFromTemplate exists.\nlambda.setEndpoint exists.\nlambda.paginationConfig exists.\n2017-12-06T06:31:36.487Z 19aab9b1-da4f-11e7-8a77-69dbeb7399b9 invoke exists: true\n```\n2. Created a new lambda function in Visual Studio thru AWS VSTS Tools/Templates and published function thru it, and I found the same issue I was facing. Ultimately when referring npm dialog window in solution explorer; it has complained me of old api being used for aws-sdk. When I updated aws-sdk from there and again published, the error went away.\nThanks for all help!. ",
    "eMahtab": "It was due to aws js sdk version I was using, changed it to https://sdk.amazonaws.com/js/aws-sdk-2.2.32.min.js , that solved the problem. ",
    "IvanAlegre": "Yes, it's larger than 5mb.\nI've just updated the code. Hope this works. Thank you so much!. After a long period without happening, it has happened again:\n[AWS s3 undefined 0.01s 0 retries] uploadPart({ Body: <Buffer 00 00 00 18 66 74 79 70 6d 70 34 32 00 00 00 00 69 73 6f 6d 6d 70 34 32 00 00 20 98 6d 6f 6f 76 00 00 00 6c 6d 76 68 64 00 00 00 00 d5 f7 fb 30 d5 f7 ... >,\n  ContentLength: 5242880,\n  PartNumber: 1,\n  Bucket: BUCKET_NAME,\n  Key: '5a2e6dee2288f600405c0c35',\n  UploadId: undefined })\n[AWS s3 undefined 0.002s 0 retries] uploadPart({ Body: <Buffer 06 67 48 2a 7c a8 96 7b 93 ca 93 8d b3 2b 89 05 98 ed ab 8e df 4d 83 26 97 f4 7c ff 25 17 4e 68 08 00 b8 44 1c fb df 95 28 8d b9 46 ff 9d df 02 c1 80 ... >,\n  ContentLength: 406557,\n  PartNumber: 2,\n  Bucket: BUCKET_NAME,\n  Key: '5a2e6dee2288f600405c0c35',\n  UploadId: undefined })\n[AWS s3 200 0.177s 0 retries] createMultipartUpload({ Bucket: BUCKET_NAME, Key: '5a2e6dee2288f600405c0c35' })\n2017-12-11T11:37:18.352Z ERROR  {\n  \"message\": \"Missing required key 'UploadId' in params\",\n  \"code\": \"MissingRequiredParameter\",\n  \"time\": \"2017-12-11T11:37:18.347Z\"\n}\nIt's really strange because once it happens for the first time, continues happening on every upload (bigger than 5MB, the smaller files does work). But if I restart the service it works for a period of time.\nI tried recreating the S3 Client every upload, but no luck again.\nThanks a lot. Hi! It seems that is uploading the parts before getting an UploadId. Which could be the reason for that?\nThanks!. Nope thanks.\nIt dissapeared without doing anything as it appeared without any change.. ",
    "zacharynevin": "Thanks @chrisradek!\nI remember reading somewhere in the documentation that the endpoint parameter in AWS.Config is of the form https://<service_name>.<region>.amazonaws.com. If the authorizer can't deal with endpoints from MediaStore (e.g. https://i5domqqdltrwci.data.mediastore.us-east-1.amazonaws.com), is there another query or header property I can add to the request to force it?. Alright I can do that. Thanks @chrisradek!. ",
    "nexeh": "@jeskew I have this same issue. Let me see if i can get you some information. @jeskew Like the original poster i am adding cognito to an existing angular application. The key piece here is that it is NOT angular 2+ or using webpack. these were not popular when this application was created. So i have coped some piece of information from my bower.json and package json for you\n\"angular\": \"~1.5.3\",\n\"aws-sdk\": \"^2.251.1\",\n\"amazon-cognito-identity-js\": \"^1.31.0\",\ngulp\": \"~3.9.0\",\n    \"gulp-autoprefixer\": \"~3.0.2\",\n    \"gulp-angular-templatecache\": \"~1.8.0\",\n    \"del\": \"~2.0.2\",\n    \"lodash\": \"~3.10.1\",\n    \"gulp-cssnano\": \"~2.1.1\",\n    \"gulp-filter\": \"~3.0.1\",\n    \"gulp-flatten\": \"~0.2.0\",\n    \"gulp-eslint\": \"~1.0.0\",\n    \"eslint-plugin-angular\": \"~0.12.0\",\n    \"gulp-load-plugins\": \"~0.10.0\",\n    \"gulp-size\": \"~2.0.0\",\n    \"gulp-uglify\": \"~1.4.1\",\n    \"gulp-useref\": \"~3.0.3\",\n    \"gulp-util\": \"~3.0.6\",\n    \"gulp-ng-annotate\": \"~1.1.0\",\n    \"gulp-replace\": \"~0.5.4\",\n    \"gulp-rename\": \"~1.2.2\",\n    \"gulp-rev\": \"~6.0.1\",\n    \"gulp-rev-replace\": \"~0.4.2\",\n    \"gulp-htmlmin\": \"~1.3.0\",\n    \"gulp-inject\": \"~3.0.0\",\n    \"gulp-protractor\": \"~2.1.0\",\n    \"gulp-sourcemaps\": \"~1.6.0\",\n    \"gulp-ruby-sass\": \"~0.7.1\",\n    \"gulp-angular-filesort\": \"~1.1.1\",\nI have been stuck on this issue for a few hours. It works fine when im running locally with just source files. Once i concatenate and minify it doesn't work any more. im by trying to get around it \n@rainnaren Did you ever make any progress on this issue? . ",
    "abjsaha": "Yes it does, strange. ",
    "AJB99": "A Solution\nAfter many hours of wrestling with this I figured out that the issue was with the Condition block of the IAM Policy that I was sending through as the Policy param of my AWS.STS.getFederationToken() request. Specifically, AWS.S3.upload() only sends an x-amz-acl header for the first PUT request, which is the call to S3.initiateMultipartUpoad.\nThe x-amz-acl header is not included in the subsequent PUT requests for the actual parts of the upload.\nI had the following condition on my IAM Policy, which I was using to ensure that any uploads must have an ACL of 'private':\nCondition : {\n    StringEquals : {\n        's3:x-amz-acl' : ['private']\n    }\n}\n\nSo the initial PUT request to S3.initiateMultipartUpload was fine, but the subsequent PUTs failed because they didn't have the x-amz-acl header.\nThe solution was to edit the policy I was attaching to the temporary user and move the s3:PutObject permission into its own statement, and then adjust the condition to apply only if the targeted value exists. The final policy looks like so:\nvar policy = {\n    Version : '2012-10-17',\n    Statement : [\n        {\n            Effect : 'Allow',\n            Action : [\n                's3:PutObject'\n            ],\n            Resource : [\n                'arn:aws:s3:::' + bucket + '/' + account._id + '/files/' + file.name\n            ],\n            Condition : {\n                StringEqualsIfExists : {\n                    's3:x-amz-acl' : ['private']\n                }\n            }\n        },\n        {\n            Effect : 'Allow',\n            Action : [\n                's3:AbortMultipartUpload'\n            ],\n            Resource : [\n                'arn:aws:s3:::' + bucket + '/' + account._id + '/files/' + file.name\n            ]\n        }\n    ]\n};\n\nHopefully that'll help someone else from wasting three days on this.. ",
    "thieman": "In addition to size on disk, this issue also contributes significantly to TypeScript compile times, as you're forced to load type definitions for the entire project (in the neighborhood of 200K lines!) on every compile even if you are only using a specific module.. ",
    "blablabla156": "I'm getting the else response. Let me send you a pic. Here it's (https://imgur.com/Lu8EPCq). I don't know if it is encrypted, I mean like, I downloaded the .pem file and thats it. . Sorry for the delayed answer. I had to take an emergency travel of 20 days and I'm back at home today. Really sorry for this, I'm going to answer tomorrow or in a couple of hours.\n  . Okay, got this after changing \n\nconsole.log(\"Could not get the encrypted password\"); to console.log(err);\n\nhttps://imgur.com/nJLqJpe. ",
    "Bang-Kwangmin": "@AllanFly120 Thank you for your response.\nBut, I think this issue is not related with async call.\nIt's NOT about the case that updateAutoScalingGroup() and describeAutoScalingGroups() are called continuously.\nEven if wait few minutes after updating launch config(A->B) on AWS web console, describeAutoScalingGroups() api returns always launch_config A.\nYou can test like below senario :\n1. test-ASG has a instance created by launch_config A.\n2. describeAutoScalingGroups() return launch_config A.\n3. change launch_config from A to B via updateAutoScalingGroup() or AWS web console.\n4. AWS web console shows launch_config B. !!!\n5. describeAutoScalingGroups() still returns launch_config A .\n   Even if trying many times, it's not changes.\n\ncreate 2 or more instance created by launch config B in test-ASG. now test-ASG has 2 B, 1 A.\nAWS web console show launch config B. \nFianally, describeAutoScalingGroups() also return launch config B.\n\n. In same situation, boto3 describe_auto_scaling_groups api return B.\nonly JS api returns the launch config name of the most-used launch config for instances in ASG.. ",
    "kevindavee": "@chrisradek the problem is I can't find listUsers action under any amazon cognito userpools permissions\n\n. ",
    "PedrooBritoo": "I have the same problem. . ",
    "aventurella": "Just so I could test, I granted all permissions. When I manually add \n\"cognito-idp:ListUser\"\nOR\n\"cognito-idp:ListUsers\"\nAs the Action to allow in the policy, it tells me I'm crazy. I can only seem to use \"cognito-idp:*\" to grant ListUsers\nEven when I do this:\n\nIt won't let me ListUsers. I have to \"cognito-idp:*\". ",
    "chrispaynter": "I'm also having this problem. I'm using the wildcard for now, but this really needs to be fixed.. I'm also having this problem. I'm using the wildcard for now, but this really needs to be fixed.. ",
    "jessedoyle": "\nI'm also having this problem. I'm using the wildcard for now, but this really needs to be fixed.\n\nI just wanted to add that we were experiencing the same problem.\nWe had to add the following permissions to successfully list users:\n\ncognito-idp:ListUsers\ncognito-idp:ListUsersInGroups\n\nJust trying to help out anyone who may be encountering the same issue!. ",
    "polovi": "@chrisradek \nSeems that AWSXray.captureAWSClient(client.service) works as workaround\nBut there is problem if i use  typescript that AWS.DynamoDB.DocumentClient.service is internal and not defined in d.ts file so I am not able to compile it. @chrisradek \nSeems that AWSXray.captureAWSClient(client.service) works as workaround\nBut there is problem if i use  typescript that AWS.DynamoDB.DocumentClient.service is internal and not defined in d.ts file so I am not able to compile it. ",
    "leonardovillela": "Any progress?\n. ",
    "Tehnix": "@polovi a workaround for your TypeScript issue is,\ntypescript\n...\nAWSXray.captureAWSClient((client as any).service);\nwhich casts the client to any, just so you can access service on it.. ",
    "slawiko": "@chrisradek ok, here is my use case:\nI would like to embed kibana (ES) dashboards in my web-application. I need authorize request on kibana for that. So I have created route /kibana and proxy, which just authorizes request:\n```ts\n(proxyReq: ClientRequest, req: IncomingMessage, res: ServerResponse, options: ServerOptions): void => {\n  const endpoint = new Endpoint(kibanaHost);\n  // Here I'm instantiating HttpRequest and filling it with authorization headers etc.\n  const request = new HttpRequest(endpoint, AWS_REGION);\n  request.method = req.method;\n  request.path = req.url;\n  if (Buffer.isBuffer((req).body)) {\n    request.body = (req).body;\n  }\n  request.headers.Host = kibanaHost;\nconst sign = new SignerV4(request, 'es');\n  sign.addAuthorization(credentials, new Date());\nproxyReq.setHeader('Host', request.headers.Host);\n  proxyReq.setHeader('X-Amz-Date', request.headers['X-Amz-Date']);\n  proxyReq.setHeader('Authorization', request.headers.Authorization);\n}\n```\nMaybe this way isn't nice, could you suggest something else?\n  . ",
    "eyalcohen7": "I was unable to reproduce this on Safari or Firefox - it seems that both send the request with the correct Range...however both browsers don't have the If-None-Match header (or other cache related headers).\n. No, I wasn't able to solve this issue. I ended up requesting the first chunk (100K for example) and if I get a SignatureError message I request it again, but without the range header, so the second request works).\nPlease note that I am not monitoring the code, so I don't know if this error still happens.\nThanks. ",
    "kt3k": "Hello @jeskew,\nThanks for the feedback.\nA major difference between istanbul (v0.4.x) and nyc is the support of es6 (and above). Old istanbul does not instrument the code correctly when we use es6. Perhaps that doesn't affects this project immediately, but in future when we dropped the support of <= 0.12.x the use of istanbul will block us using es6.\n\nI'm not sure I see enough value in upgrading this build-time only dependency to justify auditing the differences\n\nHow do you audit the dependencies? Isn't the scanning tool like nsp enough for this project?. Hello @jeskew,\nThanks for the feedback.\nA major difference between istanbul (v0.4.x) and nyc is the support of es6 (and above). Old istanbul does not instrument the code correctly when we use es6. Perhaps that doesn't affects this project immediately, but in future when we dropped the support of <= 0.12.x the use of istanbul will block us using es6.\n\nI'm not sure I see enough value in upgrading this build-time only dependency to justify auditing the differences\n\nHow do you audit the dependencies? Isn't the scanning tool like nsp enough for this project?. ",
    "Y-LyN-10": "@jeskew, I understand the attitude of \"why changing x if it works\", but with the time it really get's harder to maintain the codebase with outdated dependencies. \nCurrently \"npm audit\" on the master branch reports 18 vulnerabilities (some of them critical) and the only fix for them is to make an update with breaking-changes. Also, there are multiple deprecation warnings from node, running the latest LTS version. Sure, these vulnerabilities are only for the dev dependencies, which doesn't matter for the package users, but I think that we shouldn't hesitate some changes :) \nPerhaps, some of the open PRs already fixed that, I don't know, but I support the idea of switching to nyc . ",
    "jimmywarting": "I think you should stop support old platforms and remove old legacy code.\nv6 will drop soon in april so maybe just support node >= 8 for the next major version change.. ",
    "roberthelmick08": "Hi @AllanFly120, please see the error message below:\nError uploading data to S3 bucket: TimeoutError: Connection timed out after 120000ms\nC:\\Users\\426782\\documents\\workspace-sts\\lf-rti-file-copier-sql\\node_modules\\aws-sdk-proxy\\node_modules\\aws-sdk\\lib\\request.js:31\n            throw err;\n            ^\n\nTimeoutError: Connection timed out after 120000ms\n    at ClientRequest.<anonymous> (C:\\Users\\426782\\documents\\workspace-sts\\lf-rti-file-copier-sql\\node_modules\\aws-sdk-proxy\\node_modules\\aws-sdk\\lib\\http\\node.js:83:34)\n    at ClientRequest.g (events.js:292:16)\n    at emitNone (events.js:86:13)\n    at ClientRequest.emit (events.js:185:7)\n    at Socket.emitTimeout (_http_client.js:630:10)\n    at Socket.g (events.js:292:16)\n    at emitNone (events.js:86:13)\n    at Socket.emit (events.js:185:7)\n    at Socket._onTimeout (net.js:338:8)\n    at ontimeout (timers.js:386:11)\n    at tryOnTimeout (timers.js:250:5)\n    at Timer.listOnTimeout (timers.js:214:5)\n\nnpm ERR! Windows_NT 10.0.14393\nnpm ERR! argv \"C:\\\\Program Files\\\\nodejs\\\\node.exe\" \"C:\\\\Program Files\\\\nodejs\\\\node_modules\\\\npm\\\\bin\\\\npm-cli.js\" \"run\" \"test\" \"65056\"\nnpm ERR! node v6.11.4\nnpm ERR! npm  v3.10.10\nnpm ERR! code ELIFECYCLE\nnpm ERR! lf-rti-file-copier@1.0.0 test: `serverless invoke local -f dataRefresh -s dev -d  \"65056\"`\nnpm ERR! Exit status 1\nnpm ERR!\nnpm ERR! Failed at the lf-rti-file-copier@1.0.0 test script 'serverless invoke local -f dataRefresh -s dev -d  \"65056\"'.\nnpm ERR! Make sure you have the latest version of node.js and npm installed.\nnpm ERR! If you do, this is most likely a problem with the lf-rti-file-copier package,\nnpm ERR! not with npm itself.\nnpm ERR! Tell the author that this fails on your system:\nnpm ERR!     serverless invoke local -f dataRefresh -s dev -d  \"65056\"\nnpm ERR! You can get information on how to open an issue for this project with:\nnpm ERR!     npm bugs lf-rti-file-copier\nnpm ERR! Or if that isn't available, you can get their info via:\nnpm ERR!     npm owner ls lf-rti-file-copier\nnpm ERR! There is likely additional logging output above.`.\n",
    "mszu": "As per this comment on one of the related questions linked to the SO post above, the commit where this behavior was introduced is 0638060402931f572f9f75d77a12bd6c529b0424. ",
    "eivbsmed": "Looks like it has improved has stopped for now. But looked like it was the aws-sdk and the server not agreeing on the credential signing.. ",
    "iamNoah1": "Hi @jeskew \nthank you so much for the info. I couldn't find that solution anywhere on the interwebs.\nCheers \nNoah . Hi @jeskew \nthank you so much for the info. I couldn't find that solution anywhere on the interwebs.\nCheers \nNoah . ",
    "amitdhawan": "Hey @AllanFly120,\nI did use promise for the getObject function but still get the crash. Also using writeFileSync will not work for large files to be written.\nAnd i tried using callback for getObject in which case I cannot use createReadStream which is the preferred way to read and write large files.\nPlease suggest.  . @AllanFly120  Any updates on this?. ",
    "odykyi": "@lsegal @konklone @corymsmith @ericb @ColmHally Please . @jeskew , thanks, issue fixed . @lsegal  @konklone @corymsmith @ericb @jeskew  Please review\nThanks!. ",
    "gerhardberger": "The presigned url looks like this (the request body has a json DynamoDB query):\n```\nPOST https://dynamodb.eu-central-1.amazonaws.com\nContent-Type:application/x-amz-json-1.0\nX-Amz-Target:DynamoDB_20120810.Query\nX-Amz-Date:20180113T154227Z\nX-Amz-Expires:3600\nX-Amz-Security-Token: XXXXXXXX\nAuthorization:AWS4-HMAC-SHA256 Credential=XXXXXX/20180113/eu-central-1/dynamodb/aws4_request, SignedHeaders=host;x-amz-target, Signature=XXXXX\n```\nHowever making this request results in InvalidSignatureException.\nI took a look at Cognito and it looks quite fitting for my use case! But it is not yet clear to me, that whether is it a good practice to commit an IdentityPoolId in your code. Am I correct that this would be the only thing needed to access DynamoDB from a client application?\n. ",
    "brettneese": "For future reference, I was able to do:\n```\nvar object = {hello: \"world\"}\nvar params = {\n  FunctionName: 'hello-world' / required /,\n  Payload: JSON.stringify(object)' / Strings will be Base-64 encoded on your behalf /\n};\n````\nHowever, I still contend that this means the docs are wrong, because they suggest you can send a string. Technically that's correct, but the string must be JSON.. Hey @jeskew, yeah, that makes sense. I read this thing many times and did not see that, and it seems there are a few others on the internets with this problem without a solution as well (hopefully they will Google and find this answer). The message coming back from Lambda is also less than helpful here, so that might be a good solution.\nThanks for your response!. ",
    "ER1011": "same problem here. Added that line of code and can't build for release on Android anymore. @non-standard and @AllanFly120, we were able to solve this issue. Although the symptom was a memory issue, that does not seem to be the actual root cause. It appears that the AWS file we are importing is already transpiled and can be left out when doing the babel transformation. This can be achieved by adding the following to the package.json of our react native project:\n\"babel\": {\n    \"ignore\": [\"node_modules/aws-sdk/dist/aws-sdk-react-native.js\"]\n  },\nWithout the above, it seems that the packager goes into a loop and no matter how much memory you give node, it's never enough. We got a hint that this could be the issue looking at this post. There are other ways to ignore said file, including adding the above code to the package.json of the aws-sdk itself. @AllanFly120 feel free to address it the way you deem more appropriate. At a minimum it'd be great if a note could be added to the readme for those that use the aws-sdk in a react-native project.. ",
    "truongluong1314520": "Hi @ER1011 , I have the same issue like you. When I see your code, I feel like my life is saved. However, I add your code to my package.json in react native app but it is not working. It looks like the babel do not ignore is. Do you have any way to ignore a file in babel.\nHere is my package.json:\n{\n  \"babel\": {\n    \"ignore\": [\n      \"node_modules/aws-sdk/dist/aws-sdk-react-native.js\"\n    ]\n  },\n  \"name\": \"IncreaseHeapSize\",\n  \"version\": \"0.0.1\",\n  \"private\": true,\n  \"scripts\": {\n    \"start\": \"node node_modules/react-native/local-cli/cli.js start\",\n    \"test\": \"jest\"\n  },\n  \"dependencies\": {\n    \"aws-sdk\": \"2.369.0\",\n    \"react\": \"16.6.0-alpha.8af6728\",\n    \"react-native\": \"0.57.4\"\n  },\n  \"devDependencies\": {\n    \"babel-jest\": \"23.6.0\",\n    \"jest\": \"23.6.0\",\n    \"metro-react-native-babel-preset\": \"0.50.0\",\n    \"react-test-renderer\": \"16.6.0-alpha.8af6728\"\n  },\n  \"jest\": {\n    \"preset\": \"react-native\"\n  }\n}\n\nI try this but fail too: \nnpx babel node_modules --out-dir lib --ignore \"node_modules/aws-sdk/dist/aws-sdk-react-native.js\"\n\nIf you have any suggestion, please let me know. I am willing to hear from you. Hi everyone , I am able to run debug by changing the package.json to:\n\"scripts\": {\n    ...\n  \"start-max\": \"node --max-old-space-size=8192 node_modules/react-native/local-cli/cli.js start\",\n    ...\n},\n\nIn terminal run: \"npm run start-max\" to start the node server with 8,192 MB.\nThen run your project as normal: \"react-native run-android\".\nEnjoy!\n. Hi @Blutude , run \"start-max\" is only for debug mode. If you want to release, we have to config more. I write a post on Medium about it. Hope it can help you.\nReact native max-old-space-size\nSorry for replying you late. Hi @Blutude , I guess it is because of the version. I fix the issue with: \n1/ React native: \"0.57.4\"\n2/ Xcode version: \"10.1\"\nCan you please let me know your version? . hey @Blutude , how is everything?. Hi @Blutude , you can try this one:\nexport NODE_ARGS=\"--max-old-space-size=8192\"\nexport NODE_BINARY=\"node\"\n../node_modules/react-native/scripts/react-native-xcode.sh\n\nMaybe my previous solution is out of date \ud83d\ude1e \n. Hi @srchase , thank you for your quick reply. I looked at the guide before. It uses aws-amplify to do stuffs. However, aws-amplify does not support form-data for uploading large files. We have to load everything to memory then give the data to Storage. If the file is too large, then OOM appears.\nRequest form-data in aws-amplify\nThat is why I have to use the aws-sdk-react-native to handle multipart upload and get stuck at the Credentials things.\nFortunately, I am able to solve it this morning. I will write down step by step to help the other people if they get the same problem:\n1/  Use the aws-amplify for logging.\n2/ Use the Auth. currentCredentials() and past the returned credentials to Auth.essentialCredentials(credentials). Create IAM ad-hoc\n3/ Auth.essentialCredentials(credentials) will give you the accessKeyId, secretKey, sessionToken, and expiration. You can use them for the Credentials of aws-sdk.\nconst AWS = require('aws-sdk/dist/aws-sdk-react-native');\n\nconst s3 = new AWS.S3({\n    region: region,\n    accessKeyId: accessKeyId,\n    secretAccessKey: secretKey,\n    sessionToken: sessionToken,\n});\n\nNow, everything is setup and you can createMultipartUpload by using this code:\nlet params = {\n    Bucket: region,\n    Key: \"iPhoneX.mp4\",\n};\n\ns3.createMultipartUpload(params, (err, data) => {\n\n    console.log('error', err);\n    console.log('data', data);\n\n    if (typeof data !== 'undefined' && data !== null) {\n\n        this._uploadId = data.UploadId;\n    }\n});\n\nI am checking if the aws-amplify and aws-sdk are conflict with each other? I will give the result ASAP.. I checked, aws-amplify work with aws-sdk no conflict. The thing is that sessionToken returned from AWS contain a lot of special character. That is why when we put it as a param of url, the sessionToken is INVALID.\nFixed: \nthis._preSignedUrl = json.url + '/' + json.fields.key + \"?\" +\n    \"X-Amz-Expires=86400\" +\n    \"&X-Amz-Algorithm=\" + json.fields[\"X-Amz-Algorithm\"] +\n    \"&X-Amz-Credential=\" + json.fields[\"X-Amz-Credential\"] +\n    \"&X-Amz-Date=\" + json.fields[\"X-Amz-Date\"] +\n    \"&X-Amz-Security-Token=\" + encodeURIComponent(json.fields[\"X-Amz-Security-Token\"]) +\n    \"&X-Amz-SignedHeaders=content-type;host;x-amz-acl\" +\n    \"&X-Amz-Signature=\" + json.fields[\"X-Amz-Signature\"];. Hi @srchase , thank you for your quick reply. Sorry I am not able to reply you soon because I got a fever few days ago.\n\nHere is my code to get the pre-signed url: \ncreatePreSignedPost = (key, success, fail) => {\n\n    let params = {\n        Bucket: awsConfig.Storage.bucket,\n        Fields: {\n            key: key,\n        },\n        Expires: 86400,\n    };\n    this._s3.createPresignedPost(params, function (err, data) {\n\n        if (err) {\n\n            fail(err);\n\n        } else {\n\n            success(data);\n        }\n    });\n};\n\nHope this can help you to find out why there is a [SignatureDoesNotMatch] error.\nThank you again @srchase .. Hi @srchase , it is strange, I am able to get signed url by using this function:\ngetSignedUrl\nHowever, when I come to this function:\ncreatePresignedPost\nFrom what I know the error [SignatureDoesNotMatch] is come from X-Amz-SignedHeaders not match xhr. setRequestHeader().\nsignature doesn\u2019t match the request headers\nCan you please tell me the difference between getSignedUrl and createPresignedPost. From the document, I see:\n\nNote: Not all operation parameters are supported when using pre-signed URLs. Certain parameters, such as SSECustomerKey, ACL, Expires, ContentLength, or Tagging must be provided as headers when sending a request. If you are using pre-signed URLs to upload from a browser and need to use these fields, see createPresignedPost()\n\nHowever, I am able to set the ALC when using getSignedUrl, which make me confuse.\nThank you in advance!. Hi @srchase , I will use the getSignedUrl for now and research more about createPresignedPost.\nThank you for both your kindness and your time. . Here is the stack-trace:\ntransform[stdout]:\ntransform[stdout]: <--- Last few GCs --->\ntransform[stdout]:\ntransform[stdout]: [10452:0x104800000]    79749 ms: Mark-sweep 1267.8 (1449.6) -> 1267.8 (1415.6) MB, 3994.2 / 0.0 ms  (average mu = 0.090, current mu = 0.000) last resort GC in old space requested\ntransform[stdout]: [10452:0x104800000]    83505 ms: Mark-sweep 1267.8 (1415.6) -> 1267.8 (1410.6) MB, 3755.6 / 0.0 ms  (average mu = 0.048, current mu = 0.000) last resort GC in old space requested\ntransform[stdout]:\ntransform[stdout]:\ntransform[stdout]: <--- JS stacktrace --->\ntransform[stdout]:\ntransform[stdout]: ==== JS stack trace =========================================\ntransform[stdout]:\ntransform[stdout]:     0: ExitFrame [pc: 0x3750e1f5c0f8]\ntransform[stdout]:     1: StubFrame [pc: 0x3750e248c015]\ntransform[stdout]: Security context: 0x04318491e6e1 <JSObject>\ntransform[stdout]:     2: DoJoin(aka DoJoin) [0x43184905e89] [native array.js:1] [bytecode=0x4315e940699 offset=212](this=0x043188e826f1 <undefined>,l=0x04318ec15a61 <JSArray[1548785]>,m=1548785,A=0x043188e828c9 <true>,w=0x043188e829f1 <String[0]: >,v=0x043188e829a1 <false>)\ntransform[stdout]:     3: Join(aka Join) [0x43184905ed9] [native array.js:1] [byteco...\ntransform[stdout]:\ntransform[stderr]: FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory\ntransform[stderr]:  1: 0x10003ae75 node::Abort() [/usr/local/bin/node]\ntransform[stderr]:  2: 0x10003b07f node::OnFatalError(char const*, char const*) [/usr/local/bin/node]\ntransform[stderr]:  3: 0x1001a7ae5 v8::internal::V8::FatalProcessOutOfMemory(v8::internal::Isolate*, char const*, bool) [/usr/local/bin/node]\ntransform[stderr]:  4: 0x100572ef2 v8::internal::Heap::FatalProcessOutOfMemory(char const*) [/usr/local/bin/node]\ntransform[stderr]:  5: 0x10057c3f4 v8::internal::Heap::AllocateRawWithRetryOrFail(int, v8::internal::AllocationSpace, v8::internal::AllocationAlignment) [/usr/local/bin/node]\ntransform[stderr]:  6: 0x10054e1e4 v8::internal::Factory::NewRawTwoByteString(int, v8::internal::PretenureFlag) [/usr/local/bin/node]\ntransform[stderr]:  7: 0x100826c6d v8::internal::Runtime_StringBuilderConcat(int, v8::internal::Object**, v8::internal::Isolate*) [/usr/local/bin/node]\ntransform[stderr]:  8: 0x3750e1f5c0f8\ntransform[stderr]:  9: 0x3750e248c015\ntransform[stderr]: 10: 0x3750e1f118d5 . Hi aws-sdk team, it is me again. I am here to give more information about the issue.\n\nWhen I install the newest version of aws-sdk, this issue appear. Version of new aws-sdk: \"2.361.0\".\nWhen I install the old version of aws-sdk, this issue disappear. Version of old aws-sdk: \"2.356.0\".\nI hope this can help both of us to figure it out the issue and how to solve it.\nThank you in advance!. thanks @srchase for re-open this issue, do you have any suggestion for me? Currently, aws-sdk: \"2.356.0\" is failed to build release on Android too.. Hi @srchase , I am able to run debug by changing the package.json to:\n\"scripts\": {\n    ...\n  \"start-max\": \"node --max-old-space-size=8192 node_modules/react-native/local-cli/cli.js start\",\n    ...\n},\n\nIn terminal run: \"npm run start-max\" to start the node server with 8,192 MB.\nThen run your project as normal: \"react-native run-android\".\nEnjoy!\n. @ricbermo I added the same thing to babelrc too, but it is not working for me.\nI am using react native version 0.57.8. ",
    "Blutude": "I am able to run my simulator when running that \"start-max\" script, but when I try to upload my app to testflight, I keep getting the out of memory error. --max-old-space-size=8192 does not help.\nI put the babel \"ignore\" command on both my app's package.json and the aws-sdk package.json but that does not help. Did anyone of you figure it out?. I had actually seen that post and did try that. But the Xcode build still fails with JavaScript heap out of memory.\nI can't use this package because of this. Instead I am using react-native-aws3 which works well, but it does not implement deleteObject (I can't delete files from s3), which I could do with aws-sdk. Hi @truongluong1314520, sorry for the late reply. I still have errors. My Xcode version is 10.1, React native is 0.58.6.\nIn build phases --> Bundle React Native code and images when I replace node by 'node\u200a --max_old_space_size=8192', it gives me the following error: \"Can't find 'node\u200a--max_old_space_size=8192' binary to build React Native bundle\"\nInstead, I try it without the quotes and archive the project but I get \"FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory\".\n. I am able to run my simulator only if I run that \"start-max\" script beforehand, but when I try to upload my app to the app store, I keep getting the out of memory error. --max-old-space-size=8192 does not fix it in this situation. It seems like when I archive my project from xcode, it does not use the node server I created with \"start-max\".\nI also tried adding the babel \"ignore\" command on both my app's package.json and the aws-sdk package.json but that does not help either. Did anyone of you figure it out?. I get a \"FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory\" instead of \"FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory\" if that changes anything.\nHere is my stack-trace:\ntransform[stdout]: \ntransform[stdout]: <--- Last few GCs --->\ntransform[stdout]: \ntransform[stdout]: [38882:0x104800000]    34501 ms: Scavenge 1258.5 (1435.5) -> 1250.1 (1440.5) MB, 18.6 / 0.0 ms  (average mu = 0.203, current mu = 0.122) allocation failure \ntransform[stdout]: [38882:0x104800000]    37155 ms: Mark-sweep 1262.1 (1440.5) -> 1246.2 (1438.0) MB, 2596.1 / 0.0 ms  (average mu = 0.144, current mu = 0.082) allocation failure scavenge might not succeed\ntransform[stdout]: [38882:0x104800000]    37239 ms: Scavenge 1258.1 (1438.0) -> 1249.8 (1442.0) MB, 15.3 / 0.0 ms  (average mu = 0.144, current mu = 0.082) allocation failure \ntransform[stdout]: \ntransform[stdout]: \ntransform[stdout]: <--- JS stacktrace --->\ntransform[stdout]: \ntransform[stdout]: ==== JS stack trace =========================================\ntransform[stdout]: \ntransform[stdout]:     0: ExitFrame [pc: 0x2cd5fb5cfb7d]\ntransform[stdout]: Security context: 0x20d93e71d921 <JSObject>\ntransform[stdout]:     1: /* anonymous */ [0x20d99457d7e9] [/Users/ronyazrak/Documents/McGill/Winter18/Spap/code/PROJ_SPAP_FRONT/Spap/node_modules/@babel/traverse/lib/index.js:~87] [pc=0x2cd5fbbae5ef](this=0x20d9f43e36a1 <JSFunction traverse (sfi = 0x20d959560b49)>,0x20d993337b39 <Object map = 0x20d9932a9261>,0x20d9ece4d0b1 <Object map = 0x20d9a92681e1>,0x20d936835749 <Scope m...\ntransform[stdout]: \ntransform[stderr]: FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory\ntransform[stderr]:  1: 0x100063a65 node::Abort() [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]:  2: 0x100064104 node::errors::TryCatchScope::~TryCatchScope() [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]:  3: 0x10019d9a7 v8::Utils::ReportOOMFailure(v8::internal::Isolate*, char const*, bool) [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]:  4: 0x10019d944 v8::internal::V8::FatalProcessOutOfMemory(v8::internal::Isolate*, char const*, bool) [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]:  5: 0x1005a2122 v8::internal::Heap::FatalProcessOutOfMemory(char const*) [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]:  6: 0x1005a4653 v8::internal::Heap::CheckIneffectiveMarkCompact(unsigned long, double) [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]:  7: 0x1005a0b88 v8::internal::Heap::PerformGarbageCollection(v8::internal::GarbageCollector, v8::GCCallbackFlags) [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]:  8: 0x10059ed45 v8::internal::Heap::CollectGarbage(v8::internal::AllocationSpace, v8::internal::GarbageCollectionReason, v8::GCCallbackFlags) [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]:  9: 0x1005ab5ec v8::internal::Heap::AllocateRawWithLightRetry(int, v8::internal::AllocationSpace, v8::internal::AllocationAlignment) [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]: 10: 0x1005ab66f v8::internal::Heap::AllocateRawWithRetryOrFail(int, v8::internal::AllocationSpace, v8::internal::AllocationAlignment) [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]: 11: 0x10057a864 v8::internal::Factory::NewFillerObject(int, bool, v8::internal::AllocationSpace) [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]: 12: 0x10082cd64 v8::internal::Runtime_AllocateInNewSpace(int, v8::internal::Object**, v8::internal::Isolate*) [/usr/local/Cellar/node/9.6.1/bin/node]\ntransform[stderr]: 13: 0x2cd5fb5cfb7d \ntransform[stderr]: 14: 0x2cd5fbbae5ef \ntransform[stderr]: 15: 0x2cd5fbb2ef9a \ntransform[stderr]: 16: 0x2cd5fbbae453. @ricbermo With this change, running the ios simulator works without needing to run the start-max script before hand. But Xcode --> Archive does not work, I get \"FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory\".\nIf I comment out const AWS = require('aws-sdk/dist/aws-sdk-react-native');\nThen Xcode --> Archive works with no problem.. ",
    "climberwoodi": "I have a similar problem. I get the error \"Missing credentials in config\"when trying to access s3. However, the problem only occurs if I want to access s3 from my Windows 7 machine. Everything works as usual from the ECS. A few days ago everything was working with the same configuration.\nAWS.config.credentials = new AWS.EC2MetadataCredentials({\n        httpOptions: {timeout: 10000}\n    });\n   AWS.config.region = 'eu-central-1';\nOS: Windows 7\nNodeJs: 8.9.4. The problem is fixed. Sorry, that was a configuration error I caused. . ",
    "hoodsy": "I am not.. could it be related to me specifying the contentLength?. I am not.. could it be related to me specifying the contentLength?. ",
    "jmhmd": "@srchase \nI have just started getting this error sporadically as well- specifically, I'm uploading streamed gzipped files from an electron app to s3, using upload(). On a user's Windows machine in the UK, he gets the BadDigest error for some files (seems like smaller ones), but I cannot reproduce the errors on my test machines using the same files. Trying various combinations of including or excluding ContentLength and ContentType have not fixed the errors. I'm not including ContentMD5 at all.\nSDK version: \"aws-sdk\": \"^2.404.0\"\nRelevant code:\njs\n // upload instance to s3\n          const remoteFolder = `${remoteStudy.shortid}_${series.seriesInstanceUID}`;\n          const filename = path.basename(compressedInstancePath);\n          // Don't use path.join to build s3 key, because it will use system specific path\n          // separators, i.e. backslashes on windows which we don't want.\n          const fileKey = `${prefix}/${remoteFolder}/${filename}`;\n          const uploadParams = {\n            Bucket: s3Bucket,\n            Body: fs.createReadStream(compressedInstancePath),\n            Key: fileKey,\n          };\n          await s3.upload(uploadParams).promise();\n          instanceObj.url = `https://${path.join('s3.amazonaws.com', s3Bucket, fileKey)}`;. ",
    "aaronallan": "@chrisradek thank you for the clarification and quick response. v2.177.0 works as expected. I'll keep an eye out for the next release. Thanks again. . ",
    "dennisAWS": "This can be closed. It's not SDK issue. Pinpoint does not \"currently\" support Email through direct message. Email is fully supported by campaigns if you need to send email through Pinpoint.. ",
    "erwineverts": "Thanks for your reaction @jeskew!\nThe problem I was having was that with the Limit, the limit is first taken into account and then the filters/expressions. In that case if I would search for example status equals open with a limit of 5 and the fist 4 items have a status closed, I get only 1 result. \nIs there a workaround on this issue?. ",
    "napikle": "Any ways to over come this issue of first limiting then adding filter ?. ",
    "gohackfelipe": "Yeahh I am using the 2.141.0. So we found the problem... . Yeahh I am using the 2.141.0. So we found the problem... . But there's a issue with npm repository beacuse when i use the command npm install aws-sdk it consider as the 2.159.0 as last version .. . But there's a issue with npm repository beacuse when i use the command npm install aws-sdk it consider as the 2.159.0 as last version .. . ",
    "muthumani-prabhu": "Anyone with this issue, please follow this forum link https://forums.aws.amazon.com/thread.jspa?messageID=817558&tstart=0. You can definitely make it work :-). Closing the ticket. ",
    "danielrambo": "example code:\nimport amazon = require(\"aws-sdk\")\nI'm using a version 2.6.2. My tsconfig.json:\n{\n  \"compilerOptions\": {\n    \"target\": \"es5\",\n    \"module\": \"commonjs\",\n    \"moduleResolution\": \"node\",\n    \"sourceMap\": false,\n\"rootDir\":\"src\",\n\"emitDecoratorMetadata\": true,\n\"experimentalDecorators\": true,\n\"removeComments\": false,\n\"noImplicitAny\": true\n\n},\n  \"exclude\": [\n    \"node_modules\"\n  ]\n}\nYes, I had tried to change the import syntax.. ",
    "thecodejunkie": "I figured it out, I was missing the FifoQueue attribute. I'll post the solution here in-case someone else ends up here thanks to a search\nconst params = {\n    QueueName: 'mytest.fifo',\n    Attributes: {\n        \"FifoQueue\": \"true\"\n    }\n};. ",
    "ratkorle": "Can anyone help me of how I push messages or queues to FIFO using SDK. I have this but it is not working:\nasync function sendJob(job) {`\n    `console.log('new AWS.SQS');`\n  `  let params = {`\n       ` DelaySeconds: 10,`\n      `  MessageAttributes: {`\n          `  \"Title\": {`\n              `  DataType: \"String\",`\n           `     StringValue: \"job\"`\n `           },`\n    `        \"Author\": {`\n              `  DataType: \"String\",`\n               ` StringValue: \"Subscription Job\"`\n         `   },`\n      `      \"WeeksOn\": {`\n         `       DataType: \"Number\",`\n     `           StringValue: \"6\"`\n    ```\n        }\n        },\n```\n       ` MessageBody: job,`\n   `     QueueUrl: \"https://sqs.eu-west-1.amazonaws.com/978485304731/scraper-jobs-queue.fifo\"`\n  `  };`\n`    console.log('params defined');`\n`    console.log(params);`\n   ` await sqs.sendMessage(params, function(err, data) {`\n       ` console.log('sending message');`\n     `   if (err) {`\n          `  console.log(\"Error\", err);`\n        `} else {`\n           ` console.log(\"Success\", data.MessageId);`\n    `    }`\n  `  });`\n`}\nI get timed out when it gets to sas.sendMessage()\nlet sqs = new AWS.SQS({apiVersion: '2012-11-05'});\n. ",
    "akdor1154": "yep, no luck with that sorry.. Hi @jeskew, sorry to nag but is there someone I can ping to review this?. Awesome, thanks for looking into this.  By static you mean keys saved into the credentials file? (Not env). ",
    "MeLlamoPablo": "Alright so I'm very dumb. Turns out that my IAM user was missing the DescribeImages policy. However the error message is deceitful, I would suggest to change it.. ",
    "itayod": "On Wed, 24 Jan 2018 at 22:27 Jonathan Eskew notifications@github.com\nwrote:\n\ngetSignedUrl does not call an API but instead generates a presigned URL\nusing the credentials with which the client was constructed. Is S3Service\nan instance of AWS.S3?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1893#issuecomment-360262690,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGaIf5qeYN7YWASfyymJAin_EXL4MtFyks5tN5IogaJpZM4RrP5x\n.\nThanks for the reply,\n\nso the error will allways apear in the generated url? Because it\u2019s a little\nconfusing when you look at the function signature.\n. i see the xml error when i copy paste the url into the browser...\n[image: Inline image 2]\nbut how can i get this error in my code?\nis it possible to pass it into the callback function?\nthanks.\nOn Sun, Jan 28, 2018 at 8:46 PM, Jonathan Eskew notifications@github.com\nwrote:\n\nThe error will not appear in the generated URL but will be present as the\nfirst parameter to the callback.\nWhat I meant by my question is, where are you seeing an XML error? The\nonly errors that should be encountered during signing are errors\nencountered while refreshing credentials or performing the hashes required\nfor a presigned URL. Calling getSignedUrl should not result in any\nnetwork traffic.\nCould you post the error you're seeing (redact any sensitive information)\nand a snippet where S3Service is being instantiated?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/1893#issuecomment-361085593,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGaIf1Z4Y-X54Rzkar0Bj3bdcir0nyhhks5tPMCLgaJpZM4RrP5x\n.\n. \n",
    "zverev": "These requests appear if I disconnect after at least one file is uploaded. Seems that they are sent in series immediately one after another and there is no any timeout between them.\nIf I break the connection at the beginning of download, I see only failing file upload requests.\nWhy is it trying to determine the region? I've specified it in options. And why it returns 403 after connection is established again? There is no such problem if I am always online.. EvaporateJS solved my problem.\nBut I still have no idea, what is the nature of these requests.. ",
    "andregagne": "I am also seeing this problem with the newest version of the SDK (built about a month ago from here: https://sdk.amazonaws.com/builder/js/)\nI have a UI that does a multi-part upload to s3 and if you disconnect/block the domain in the chrome developer tools part way through the upload I see the same problem as above.\nAny suggestions on how to prevent this?. ",
    "ryno1234": "I'm also seeing this if I attempt to abort() the multipart upload. I'm receiving a 403 forbidden as well.. ",
    "Voltmod": "It's still not resolved. Any s3 requests in offline causes an uncontrolled new requests spam {Bucket}?max-keys=0. ",
    "dcchristopher": "npm install --save-dev @types/node\nIt's really compile-time issues that you're trying to address .... @srchase\nNo question, I just posted a possible solution to a problem others might face.. ",
    "nimeshabuddhika": "It was my bad. I used wrong region. Thanks for your help.\nhttps://docs.aws.amazon.com/general/latest/gr/rande.html. ",
    "riteshapatel": "@AllanFly120 Thank you for your response. First link in the comments above returns Git 404. Any idea? I just need the listing of instance types. No other details, if there is a way to accomplish that via api or curl would be great. \nThx\nRitesh. @AllanFly120 Thank you for your response. First link in the comments above returns Git 404. Any idea? I just need the listing of instance types. No other details, if there is a way to accomplish that via api or curl would be great. \nThx\nRitesh. Ah never mind. Got it. Had to send a Port as well when deregistering the Ip with the target.. https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/MediaConvert.html\nLook for NameModifier and then $Name or $Time$.. https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/MediaConvert.html\nLook for NameModifier and then $Name or $Time$.. ",
    "tomgoren": "@riteshapatel I added an example to the original thread: https://github.com/aws/aws-cli/issues/1279#issuecomment-415458307\nCopying here in case someone lands on this page.\n$ aws pricing get-attribute-values --service-code AmazonEC2 --attribute-name instanceType\n{\n    \"AttributeValues\": [\n        {\n            \"Value\": \"c1.medium\"\n        },\n        {\n            \"Value\": \"c1.xlarge\"\n        },\n        {\n            \"Value\": \"c3.2xlarge\"\n        },\n...\n        {\n            \"Value\": \"z1d.xlarge\"\n        },\n        {\n            \"Value\": \"z1d\"\n        }\n    ]\n}. ",
    "niftynei": "Sweet. For what it's worth this was pretty much the only change I had to make to get it working for S3. . ",
    "sk16": "@AllanFly120  Can you tell me how to build aws-sdk for node/browser/service worker env. I have created a file service_worker.js under path lib/http , but not able to build for service worker env.. ",
    "taylor-a-barnette": "@jeskew Good advice - done. :) FWIW, here's the code (FYI the code that executes the image processing and storage in a new folder was being run in a loop, which I removed for some testing purposes):\n```\n// dependencies\nvar async = require('async');\nvar path = require('path');\nvar AWS = require('aws-sdk');\nvar gm = require('gm')\n            .subClass({ imageMagick: true }); // Enable ImageMagick integration.\nvar util = require('util');\nvar max_width  = 20;\nvar max_height = 20;\n// get reference to S3 client\nvar s3 = new AWS.S3();\nexports.handler = function(event, context) {\n  // Read options from the event.\n  console.log(\"Reading options from event:\\n\", util.inspect(event, {depth: 5}));\n  var srcBucket = event.Records[0].s3.bucket.name;\n  // Object key may have spaces or unicode non-ASCII characters.\n    var srcKey    =\n    decodeURIComponent(event.Records[0].s3.object.key.replace(/+/g, \" \"));\n  var dstBucket = srcBucket;\n  var dstKey    = srcKey.replace('originals', 'assets');\n  var extension = path.extname(dstKey);\n  var filename  = path.basename(dstKey, extension);\n  var directory = path.dirname(dstKey);\n  // dstKey = directory + '/' + filename + extension;\n  // var sub_folders = ['original', 'large', 'medium', 'thumb']\n//  LARGE image from S3, transform, and upload to a different S3 bucket.\n\n//  in the loop, \"large\" would be replaced with an element from sub_folders (above)\n    dstKey = directory + '/' + 'large' + '/' + filename + extension;\n    max_width  = 600;\n    max_height = 600;\n    async.waterfall([\n      function download(next) {\n        // Download the image from S3 into a buffer.\n        s3.getObject({\n            Bucket: srcBucket,\n            Key: srcKey\n          },\n          next);\n        },\n      function transform(response, next) {\n        gm(response.Body).size(function(err, size) {\n          // Infer the scaling factor to avoid stretching the image unnaturally.\n          var scalingFactor = Math.min(\n            max_width / size.width,\n            max_height / size.height\n          );\n          var width  = scalingFactor * size.width;\n          var height = scalingFactor * size.height;\n          // var height = scalingFactor * size.height;\n          // Transform the image buffer in memory.\n          this.resize(width, height)\n            .toBuffer(null, function(err, buffer) {\n              if (err) {\n                next(err);\n              } else {\n                next(null, response.ContentType, buffer);\n              }\n            });\n        });\n      },\n      function upload(contentType, data, next) {\n        // Stream the transformed image to a different S3 bucket.\n        s3.putObject({\n            Bucket: dstBucket,\n            Key: dstKey,\n            Body: data,\n            ContentType: contentType\n          },\n          next);\n        }\n      ], function (err) {\n        if (err) {\n          console.error(\n            'Unable to resize ' + srcBucket + '/' + srcKey +\n            ' and upload to ' + dstBucket + '/' + dstKey +\n            ' due to an error: ' + err\n          );\n        } else {\n          console.log(\n            'Successfully resized ' + srcBucket + '/' + srcKey +\n            ' and uploaded to ' + dstBucket + '/' + dstKey\n          );\n        }\n    context.done();\n  }\n);\n\n```. Yes, all good to close this. I figured it out, but it was so long ago I don't remember the solution. If anyone sees this and has the same issues, drop a note here or ping me and I'll dig up how I solved it. Thanks for your help, everyone!. ",
    "SheriefBadran": "@jeskew when I tried what you suggest I get the following error message:\nReturnValues can only be ALL_OLD or NONE. ",
    "PrasaViatick": "getting same error. Is there any solution available?. ",
    "azarboon": "\n@jeskew when I tried what you suggest I get the following error message:\nReturnValues can only be ALL_OLD or NONE\n\nSame issue. This is strange and confusing that Dynamodb PUT operation doesn't return any value. . ",
    "normanhuang": "I'm actually still having an issue with the DocumentClient Put method. With the ReturnValues set to ALL_OLD or NONE, the object returned is still empty.. > @normanhuang\n\nCan you provide an example of your code?\n\n@srchase Yes, see below:\n```\nvar dynamodb = new AWS.DynamoDB.DocumentClient();\nlet params = {\n    TableName: 'Campaigns',\n    Item: {\n        CampaignId: uuid.v4(),\n        name: 'Fubar'\n    },\n   ReturnValues: 'ALL_OLD'\n};\ndynamodb.put(params, (err, data) => {\n    if (err) {\n        console.error('Error: ', JSON.stringify(err, null, 2));\n    } else {\n        console.log('Successfully retrieved item:', JSON.stringify(data));\n    }\n});\n```\n. Apologies. I figured it out. I was adding new items, for which the PutItem method and, by extension, the DocumentClient Put method returns empty because there were no older items to return. I tried replacing an existing item and the old item was returned.. ",
    "onurburak9": "It says inside the documentation , The ReturnValues parameter is used by several DynamoDB operations; however, PutItem does not recognize any values other than NONE or ALL_OLD.. ",
    "jferrer21": "@AllanFly120\nThank you.  It worked.. ",
    "rizowski": "@jeskew Thanks for the quick response. The biggest thing I would like to get out of what I am proposing is being able to reduce code deployed to services other than front end. Although, by its very nature, implementing the lerna pattern could also produce smaller bundle sizes for webpack. \ud83d\ude04 . @jeskew Thanks for the quick response. The biggest thing I would like to get out of what I am proposing is being able to reduce code deployed to services other than front end. Although, by its very nature, implementing the lerna pattern could also produce smaller bundle sizes for webpack. \ud83d\ude04 . ",
    "dncrews": "+1. I've actually been bumping up against the Lambda total file size limit for months now. This would be significant.. ",
    "tusbar": "I would definitely be happy to see smaller and independently versioned packages, so I don\u2019t have to update aws-sdk when a client I\u2019m not using gets updated. \nLerna (or equivalent) seems like a good solution to this, so I could just yarn add @aws/s3, which would depend on something like @aws/core, just like https://github.com/Turfjs/turf. . ",
    "spilliton": "+1 also running into this.  \nAs an alternative solution, what if aws-sdk was available on all lambdas by default?  Seems like such a common dependency it would be nice if it were just available for you (and didn't count toward total size) similar to the base node stuff you can import.. +1 also running into this.  \nAs an alternative solution, what if aws-sdk was available on all lambdas by default?  Seems like such a common dependency it would be nice if it were just available for you (and didn't count toward total size) similar to the base node stuff you can import.. @srchase awesome I had no idea!  Thanks!. @srchase awesome I had no idea!  Thanks!. ",
    "chaitanya11": "Hi @rafaeljcuevas aws-sdk-js library generates signature with acceskeyId, and some headers (which doesn't include x-amz-date value), here is the reference code from aws-sdk/signers/s3.js \n(pls view the comments in stringToSign method.)\n```\naddAuthorization: function addAuthorization(credentials, date) {\n    console.log('signers/s3.js - addAuthorization');\n    console.log('after ',this.request.headers['presigned-expires']);\n    if (!this.request.headers['presigned-expires']) {\n      this.request.headers['X-Amz-Date'] = AWS.util.date.rfc822(date);\n    }\nif (credentials.sessionToken) {\n  // presigned URLs require this header to be lowercased\n  this.request.headers['x-amz-security-token'] = credentials.sessionToken;\n}\n// console.log(credentials);\nconsole.log(this.stringToSign());\nvar signature = this.sign(credentials.secretAccessKey, this.stringToSign()); (// x-amz-date is not included in signature calculation).\nvar auth = 'AWS ' + credentials.accessKeyId + ':' + signature;\n\nthis.request.headers['Authorization'] = auth;\n\n},\nstringToSign: function stringToSign() {\n    var r = this.request;\nvar parts = [];\nparts.push(r.method);\nparts.push(r.headers['Content-MD5'] || '');\nparts.push(r.headers['Content-Type'] || '');\n\n// This is the \"Date\" header, but we use X-Amz-Date.\n// The S3 signing mechanism requires us to pass an empty\n// string for this Date header regardless.\nparts.push(r.headers['presigned-expires'] || '');\n\nvar headers = this.canonicalizedAmzHeaders();\nif (headers) parts.push(headers);\nparts.push(this.canonicalizedResource());\n\nreturn parts.join('\\n');\n\n},\n```\n(i added some console logs just for debugging purpose)\nso, if you are sending x-amz-date header the calculated signature(calculated with accesskey and some headers other than x-amz-date) mismatches.\nBut, when you come to java library, implementation was different and it contains X-Amz-Expires,X-Amz-Date in its signature, here is the reference code from java library.\n```\npackage com.amazonaws.services.s3;\n.....\n.....\npublic URL generatePresignedUrl(GeneratePresignedUrlRequest req) {\n        rejectNull(req,\n            \"The request parameter must be specified when generating a pre-signed URL\");\n        req.rejectIllegalArguments();\n    final String bucketName = req.getBucketName();\n    final String key = req.getKey();\n\n    if (req.getExpiration() == null) {\n        req.setExpiration(\n                new Date(System.currentTimeMillis() + 1000 * 60 * 15));\n    }\n\n    HttpMethodName httpMethod = HttpMethodName.valueOf(req.getMethod().toString());\n\n    // If the key starts with a slash character itself, the following method\n    // will actually add another slash before the resource path to prevent\n    // the HttpClient mistakenly treating the slash as a path delimiter.\n    // For presigned request, we need to remember to remove this extra slash\n    // before generating the URL.\n    Request<GeneratePresignedUrlRequest> request = createRequest(\n            bucketName, key, req, httpMethod);\n\n    addParameterIfNotNull(request, \"versionId\", req.getVersionId());\n\n    if (req.isZeroByteContent())\n        request.setContent(new ByteArrayInputStream(new byte[0]));\n\n    for (Entry<String, String> entry : req.getRequestParameters().entrySet()) {\n        request.addParameter(entry.getKey(), entry.getValue());\n    }\n\n    addHeaderIfNotNull(request, Headers.CONTENT_TYPE, req.getContentType());\n    addHeaderIfNotNull(request, Headers.CONTENT_MD5, req.getContentMd5());\n\n    // SSE-C\n    populateSSE_C(request, req.getSSECustomerKey());\n    // SSE\n    addHeaderIfNotNull(request, Headers.SERVER_SIDE_ENCRYPTION,\n            req.getSSEAlgorithm());\n    // SSE-KMS\n    addHeaderIfNotNull(request,\n            Headers.SERVER_SIDE_ENCRYPTION_AWS_KMS_KEYID, req.getKmsCmkId());\n\n    // Custom headers that open up the possibility of supporting unexpected\n    // cases.\n    Map<String, String> customHeaders = req.getCustomRequestHeaders();\n    if (customHeaders != null) {\n        for (Map.Entry<String, String> e: customHeaders.entrySet()) {\n            request.addHeader(e.getKey(), e.getValue());\n        }\n    }\n\n    addResponseHeaderParameters(request, req.getResponseHeaders());\n\n    Signer signer = createSigner(request, bucketName, key);\n\n    if (signer instanceof Presigner) {\n        // If we have a signer which knows how to presign requests,\n        // delegate directly to it.\n        ((Presigner) signer).presignRequest(\n                request,\n                CredentialUtils.getCredentialsProvider(request.getOriginalRequest(), awsCredentialsProvider).getCredentials(),\n                req.getExpiration()\n        );\n    } else {\n        // Otherwise use the default presigning method, which is hardcoded\n        // to use QueryStringSigner.\n        presignRequest(\n            request,\n            req.getMethod(),\n            bucketName,\n            key,\n            req.getExpiration(),\n            null\n        );\n    }\n\n    // Remove the leading slash (if any) in the resource-path\n    return ServiceUtils.convertRequestToUrl(request, true, false);\n}\n\npackage com.amazonaws.auth;\n.....\n.....\npublic void presignRequest(SignableRequest<?> request, AWSCredentials credentials,\n            Date userSpecifiedExpirationDate) {\n    // anonymous credentials, don't sign\n    if (isAnonymous(credentials)) {\n        return;\n    }\n\n    long expirationInSeconds = generateExpirationDate(userSpecifiedExpirationDate);\n\n    addHostHeader(request);\n\n    AWSCredentials sanitizedCredentials = sanitizeCredentials(credentials);\n    if (sanitizedCredentials instanceof AWSSessionCredentials) {\n        // For SigV4 pre-signing URL, we need to add \"X-Amz-Security-Token\"\n        // as a query string parameter, before constructing the canonical\n        // request.\n        request.addParameter(X_AMZ_SECURITY_TOKEN,\n                ((AWSSessionCredentials) sanitizedCredentials)\n                        .getSessionToken());\n    }\n\n    final AWS4SignerRequestParams signerRequestParams = new AWS4SignerRequestParams(\n            request, overriddenDate, regionName, serviceName,\n            AWS4_SIGNING_ALGORITHM, endpointPrefix);\n\n    // Add the important parameters for v4 signing\n    final String timeStamp = signerRequestParams.getFormattedSigningDateTime();\n\n    addPreSignInformationToRequest(request, sanitizedCredentials,\n            signerRequestParams, timeStamp, expirationInSeconds);\n\n    final String contentSha256 = calculateContentHashPresign(request);\n\n    final String canonicalRequest = createCanonicalRequest(request,\n            contentSha256);\n\n    final String stringToSign = createStringToSign(canonicalRequest,\n            signerRequestParams);\n\n    final byte[] signingKey = deriveSigningKey(sanitizedCredentials,\n            signerRequestParams);\n\n    final byte[] signature = computeSignature(stringToSign, signingKey,\n            signerRequestParams);\n\n    request.addParameter(X_AMZ_SIGNATURE, BinaryUtils.toHex(signature));\n}\nprivate void addPreSignInformationToRequest(SignableRequest<?> request,\n        AWSCredentials credentials, AWS4SignerRequestParams signerParams,\n        String timeStamp, long expirationInSeconds) {\n\n    String signingCredentials = credentials.getAWSAccessKeyId() + \"/\"\n            + signerParams.getScope();\n\n    request.addParameter(X_AMZ_ALGORITHM, AWS4_SIGNING_ALGORITHM);\n    request.addParameter(X_AMZ_DATE, timeStamp); (// included in signature calculation.)\n    request.addParameter(X_AMZ_SIGNED_HEADER,\n            getSignedHeadersString(request));\n    request.addParameter(X_AMZ_EXPIRES,\n            Long.toString(expirationInSeconds));\n    request.addParameter(X_AMZ_CREDENTIAL, signingCredentials);\n}\n\n```\nHope this helps you.. ",
    "rafaeljcuevas": "@AllanFly120 I updated the code to ensure that the response does not occur before the file finishes uploading to S3. I'm still seeing the same result. Based on the feedback from @chaitanya11, do you think there could be a bug with the implementation? \nIf having the x-amz-date header will always lead to a mismatch signature, then how are we supposed to send authenticated requests?. So I'm arriving at the conclusion that this is not a supported use case at this point. I am moving forward with a functional change in our approach. Instead of attempting to redirect the client directly, we will simply return the url so they can use as needed - most likely by following it. The approach overall is well documented here: https://sookocheff.com/post/api/uploading-large-payloads-through-api-gateway/. ",
    "KMiso": "I already granted iam:PassRole permisson for unauthorized cognito federated identity user.\nI get the same error even when I granted every AWS permission.\nI am using unauthenticated user since it is the process when cognito user clicks verification button( cognito user's status is unconfirmed until they click the button)\nWhen user verified their account, I would like to create lambda function for them.\nand it has been possible for unauthenticated user to use s3 and dynamodb services\nwith permission given.\nOnly creating lambda function is giving me trouble...\nIs there anything I am missing?. ",
    "vinay20045": "@chrisradek Thanks for the suggestion. I've implemented an alternate for now, but, I will try this later and let you know.\nNow coming to the actual issue, are you saying that the API sends malformed XML at times? However, in my tests, the API was sending back the correct XML but the parser was not available.\nThough I'm not in a position to comment how to handle this as I do not fully understand the flow, in my view there are a couple of problems here...\n1. The error message thrown says \"Parse error in document\" event though the Parser itself was not available and the XML itself might have been correct. This must be enhanced.\n\nWhen the execution enters this code block, the assumption is that the request went through therefore \"retryable\" must be false while still throwing an error. This way the user can catch it and handle however it is appropriate. For example, look up the message execution status again using the message id etc.\n\nPlease let me know if my understanding is wrong anywhere.\n. ",
    "glb": "@chrisradek any updates? This would be super-helpful resolving the referenced serverless deploy issues!. ",
    "bweigel": "Is there any effort going on towards this feature? I'd like to give it a try and would be more than happy for any hints and things to watch out for. I assume the shared_ini_file_credentials.js would be the place to start and implement this? . @hojatbay every once in a while I have some time and look at it a little. Have something running locally, but nothing in the extend of a PR so far. . ",
    "mashwin": "No. The same credentials were used. But not able to figure out what has changed. Do we need to pass security token to the sdk ?. ",
    "miles-po": "I just changed my call to (new Date()).toISOString()\nPassing in a Date object is missing the point. The docs specifically mention 'expressed as the number of milliseconds since Jan 1, 1970 00:00:00 UTC'. This describes Date.now(). Either the API needs to be fixed to follow the documentation or the docs need to reflect the actual usage.. Thank you. Much appreciated.. ",
    "SeanSnyders": "Also stumbled on this problem. Docs/API still seems to be incorrect.. ",
    "dheffx": "I am having trouble understanding why my request failed the second time around. The step \"npm run coverage\" passes for me on repeated attempts, and passed during my first pull, but now fails after I only made fixes for eslint.\nI poked around on other user's pull request failures and saw a few that had what appeared to be the same failure. I am hoping that means its an issue with the CI or it is easy to identify the problem.\nedit: seems to have reran automatically and worked. idk.. Thanks for looking at my request and giving this feedback! I've made changes and added tests based on your response. I believe the patch should address both cases.\nAlso, a side question, is it intentional that CredentialProviderChain in node_loader simply checks if an error wasn't thrown, and moves on? It threw me off for a bit, since it effectively hid the error messages thrown by SharedIniFileCredentials.\n.resolve(function(err, creds) {\n      if (!err) credentials = creds;\n    });\nhttps://github.com/dheffx/aws-sdk-js/blob/501a7fe0e1286ac4f8e74d4e395c68e822438bef/lib/node_loader.js#L66. I pushed the changes you requested, and moved the tests to credentials.spec.js from credentials_provider_chain.spec.js since provider chain ended up not changing.\nThe CI failed but it appears to be a common timeout issue and seemingly unrelated to my change, unless I am mistaken.. @jeskew Hey, just curious if you had a chance to look at this again, and if it looked all good? Thanks.. I am not sure that is true given that I added an additional instance to the defaultProvider list, so the instance without passing useCredentialProcess will execute first, and if the credentials do not exist in the ini then that instance will error out, and go to the instance in the chain with it set to true.\nThe example you provided for should be handled in one of the test cases I added to the credential_provider_chain.spec.js\nAWS.CredentialProviderChain.defaultProviders = [\n  function () { return new AWS.EnvironmentCredentials('AWS'); },\n  function () { return new AWS.EnvironmentCredentials('AMAZON'); },\n  function () { return new AWS.SharedIniFileCredentials(); },\n  function () { return new AWS.SharedIniFileCredentials({ useCredentialProcess: true }); },\n  function () {\n    if (AWS.ECSCredentials.prototype.isConfiguredForEcsCredentials()) {\n      return new AWS.ECSCredentials();\n    }\n    return new AWS.EC2MetadataCredentials();\n  }\n];\nThat said, I will test it out tonight and make the adjustments you mentioned if it is necessary.. Fair enough. \nI had tried to mimic the example you had cited from boto where the \"SharedCredentialProvider\" followed by \"ProcessProvider\" in the providers list. I thought that's what you were originally asking for, but if preferred I can switch it to the simpler implementation you mentioned.. ",
    "lorengordon": "Any updates on supporting this feature? Would be very handy.. Awesome to see progress on this! FWIW, there is a PR on botocore to at least print stderr, so that users have some method to see and respond to things like a prompt for MFA... The implementation in the AWS SDK for GO does this already.. I can respect that. Perhaps my javascript naivete is showing, but is the stderr being returned to the calling process? That's the problem in botocore... stderr is swallowed, not returned. So the credential_process can't decide what to do with it; it's just gone.. > To support interactive CLI, maybe child_process.spawn() makes more sense here.\nYeah, that would definitely be necessary.\nThe primary use case I'm aware of for credential_process is to retrieve a temporary credential from a federated identity provider (ADFS, Okta, etc, maybe AWS SSO one day \ud83e\udd1e). The identity provider commonly requires username/password/MFA, and so the process may prompt for all these things.\nThe credential_process of course outputs the credential in the required JSON schema on stdout, and nothing else can be on stdout or the credential process spec is violated. That leaves stderr as the only method for interacting with the user.\n. ",
    "thebenwaters": "Python, Ruby, and Java support this, why can't javascript (since a high percentage of users are using javascript).... ",
    "kevinsperrine": "@dudunato I just hacked around it. https://github.com/aws/aws-amplify/issues/281#issuecomment-370049435 \u00af_(\u30c4)/\u00af. @dudunato I just hacked around it. https://github.com/aws/aws-amplify/issues/281#issuecomment-370049435 \u00af_(\u30c4)/\u00af. ",
    "ethagnawl": "Thanks for the prompt, insightful response, @jeskew!. ",
    "Ashirogi-Muto": "My AWS SDK version is 2.24.0. I'll update the SDK. Thanks for pointing it out.. My AWS SDK version is 2.24.0. I'll update the SDK. Thanks for pointing it out.. ",
    "manjufy": "@jeskew Thanks you for the response.\nThe athena output location (i.e. S3) returns the following  csv string format\n\"id\",\"country\",\"title\",\"count\",\"date\"\n\"111\",\"ga\",\"Test\",\"1\",\"2018-02-12\"\n\"111\",\"ga\",\"Test\",\"2\",\"2018-02-12\"\n\"11\",\"ga\",\"Test\",\"3\",\"2018-02-14\"\n\"11\",\"ga\",\"Test\",\"1\",\"2018-02-13\"\nWhen I try to receive it as a createReadStream(): ReadStream, I would not get any stream\nHere is my sample code\nconst stream = s3.getObject(params).createReadStream()\nstream.pipe(process.stdout)\nSince its PassThrough as instance of ReadStream, wouldn't display the results when I try it process.stdout\n. @jeskew OK, sorry probably I would have missed out some pointers here.\nTo your question whether i'm using any third-party libraries ? No.\nHere is my scenario:\n1. When I query athena, Athena execution query gives me a path to CSV something looks like this\ns3://bucketname/results.csv\n2. When I use AWS SDK to read the csv, I do the following\n```\n      const results = execution.ResultConfiguration.OutputLocation.split('/')\n      const s3 = new AWS.S3({ region: config.region })\n      const params = {\n        Bucket: results[2],\n        Key: results[3]\n      }\nconst readableStream = await s3.getObject(params).createReadStream()\n``\nI want to pipe thisreadableStreamto anotherTransform` stream, where I do some conditional checking.\nThe problem I have is, \nconst readableStream = await s3.getObject(params).createReadStream()\nThis one actually doesn't return the stream\nHowever, when I tried to \nconst bufferdata = await s3.getObject(params).promise()\n      const result = s3Buffer.Body.toString()\nThe CSV results are returned at once as a Buffer.\nHowever, I'm trying to create a Readable stream on that csv returns, parse line by line and pipe it to another Transformer\nNote.\nThis is what Athena csv results looks like\n\"id\",\"country\",\"title\",\"count\",\"date\"\n\"111\",\"ga\",\"Test\",\"1\",\"2018-02-12\"\n\"111\",\"ga\",\"Test\",\"2\",\"2018-02-12\"\n\"11\",\"ga\",\"Test\",\"3\",\"2018-02-14\"\n\"11\",\"ga\",\"Test\",\"1\",\"2018-02-13\"\nI want this to be a stream where I can read line by line, But not a Buffer.\nIs there a way around to read CSV results line by line from the SDK, instead of returning it as  Buffer. @AllanFly120 That was helpful thanks a lot \ud83d\udc4d . ",
    "rguiliani": "Thank you for the link. I was going off of this: https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/S3.html#getBucketLocation-property, which doesn't mention that nuance. It doesn't include 'us-east-1' in the list of possible values though, so I guess that should have been a clue.. ",
    "3DEsprit": "Thanks for this.\nEven though us-east-1 is not listed, neither is us-east-2 yet it returns the correct region when using this method. . ",
    "fcheong3b": "I'm having the same issue and I had set the correctClockSkew = true using the AWS. ",
    "RoboSparrow": "I suppose this is the case with a number of sdk Methods. It would be helpful if CORS limitations could be included in the SDK docs.. There might be valid reasons not to enable CORS for some operations. It would be good to have these labelled clearly in \"Method Summary\" section for each service class and the method section itself.\nYes, CORS limits are mentioned int the installation section. But hey, most of us read that section only once. The problem is that we usually just jump right to the method \n(where it is not mentioned), and start coding... ",
    "Private-SO": "+1 for this feature request. ",
    "adwerrd": "+1 for this feature request. ",
    "haraldrudell": "I want aws-sdk to be better and more useful to me and other CloudFormation users\nCircular import references is not good software engineering principles and to me, it is an embarrassment to see it from Amazon Web Services in its repository knowingly since at least October 2017.\nCommonJS is a disorganized standard that prominent people has tried to replace since 2013. As late as 2015, Node.js did not support circular references surfaced by breaking https for everyone building Node.js from source.\nThe right thing to do is ECMAScript modules with .mjs Michael Jackson extension and the --native-modules flag for Node.js. This has been Elvis since v8.5\nRollup is the bundler that matters for the last 12 months and into the future unless you need a development server, and is crucial to the server-side infrastructure management that AWS does for a living. I wrote the es2049package package exactly for this reason and tried to make @Facebook browser people use it for React. They were at the time thinking there was something good about Node.js 6.\nAWS seems to be using some sort of generator or systematic structure. To fix those circular references should not be more than a day for a skilled engineer prior to testing.\nStep 1 is to remove circular references, it\u2019s about a week and bundling with Rollup will list exactly where they are\nStep 2 is to rewrite to ES.Next ECMAScript 2017+ which is a major undertaking but worth it since both maintainability and performance is 10x better. For example, I refactored a stream-thing to async iterators and it immediately got 100x faster. This rewrite is not urgent but a fun exercise and making of history and greatness. Here\u2019s what it looks like when I do it: https://github.com/haraldrudell/ECMAScript2049/blob/master/workspace/packages/allspawn/src/SpawnAsync.mjs\nPlease make aws-sdk great again\u2026. I only use aws-sdk in two executables:\nyarn the cloud creator that integrates configuring the AWS account and deploying to it with the build scripts-command in package.json making it easy as create-react-app. An app distribution can then deploy itself into the AWS accounts of customers\nstack enforcer that handles all those esoteric AWS troubles surfacing in managing an AWS account with nothing else than CloudFormation yaml files under version control\nAt the present time, aws-sdk is an unbundled external dependency so that Node.js deals with the circularities. The impact is that executable deploy is a bit more involved and lacks tree-shaking. ",
    "leoafarias": "Hi guys, I am currently using Rollup, and I am getting circular dependencies warnings up the wazoo. It seems that this is causing issues in certain deployment situations\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/json/builder.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/json/builder.js -> commonjs-proxy:/Users/leofarias/Documents/Projects/neardb/node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/json/parser.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/protocol/helpers.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/protocol/helpers.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/protocol/helpers.js -> commonjs-proxy:/Users/leofarias/Documents/Projects/neardb/node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/query.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/query.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/query.js -> node_modules/aws-sdk/lib/query/query_param_serializer.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/query.js -> node_modules/aws-sdk/lib/model/shape.js -> node_modules/aws-sdk/lib/model/collection.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/query.js -> node_modules/aws-sdk/lib/model/shape.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/rest.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/rest_json.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/rest_xml.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/rest_xml.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/xml/builder.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/model/api.js -> node_modules/aws-sdk/lib/model/operation.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/model/api.js -> node_modules/aws-sdk/lib/model/paginator.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/model/api.js -> node_modules/aws-sdk/lib/model/resource_waiter.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/model/api.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/sequential_executor.js-> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/service.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/service.js -> node_modules/aws-sdk/lib/region_config.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/config.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/config.js -> node_modules/aws-sdk/lib/credentials.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/config.js -> node_modules/aws-sdk/lib/credentials/credential_provider_chain.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/http.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/event_listeners.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/event_listeners.js -> node_modules/aws-sdk/lib/discover_endpoint.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/event_listeners.js -> node_modules/aws-sdk/lib/discover_endpoint.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/request.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/response.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/resource_waiter.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/v2.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/v3.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/v3https.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/v4.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/v4.js -> node_modules/aws-sdk/lib/signers/v4_credentials.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/s3.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/presign.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/param_validator.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/duplex.js -> node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/readable.js -> node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/duplex.js\n(!) Circular dependency: node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/duplex.js -> node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/writable.js -> node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/duplex.js\n(!) Circular dependency: node_modules/aws-sdk/lib/node_loader.js -> node_modules/aws-sdk/lib/credentials/temporary_credentials.js -> node_modules/aws-sdk/clients/sts.js -> node_modules/aws-sdk/lib/node_loader.js\n(!) Circular dependency: node_modules/aws-sdk/lib/node_loader.js -> node_modules/aws-sdk/lib/credentials/temporary_credentials.js -> node_modules/aws-sdk/clients/sts.js -> commonjs-proxy:/Users/leofarias/Documents/Projects/neardb/node_modules/aws-sdk/lib/node_loader.js -> node_modules/aws-sdk/lib/node_loader.js\n(!) Circular dependency: node_modules/aws-sdk/lib/node_loader.js -> node_modules/aws-sdk/lib/credentials/cognito_identity_credentials.js -> node_modules/aws-sdk/clients/cognitoidentity.js -> node_modules/aws-sdk/lib/node_loader.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLElement.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLElement.js -> commonjs-proxy:/Users/leofarias/Documents/Projects/neardb/node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLCData.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLComment.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDeclaration.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDocType.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDocType.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDTDAttList.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDocType.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDTDEntity.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDocType.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDTDElement.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDocType.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDTDNotation.js-> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLRaw.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLText.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLProcessingInstruction.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js. Hi guys, I am currently using Rollup, and I am getting circular dependencies warnings up the wazoo. It seems that this is causing issues in certain deployment situations\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/json/builder.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/json/builder.js -> commonjs-proxy:/Users/leofarias/Documents/Projects/neardb/node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/json/parser.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/protocol/helpers.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/protocol/helpers.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/json.js -> node_modules/aws-sdk/lib/protocol/helpers.js -> commonjs-proxy:/Users/leofarias/Documents/Projects/neardb/node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/query.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/query.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/query.js -> node_modules/aws-sdk/lib/query/query_param_serializer.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/query.js -> node_modules/aws-sdk/lib/model/shape.js -> node_modules/aws-sdk/lib/model/collection.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/query.js -> node_modules/aws-sdk/lib/model/shape.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/rest.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/rest_json.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/rest_xml.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/protocol/rest_xml.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/xml/builder.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/model/api.js -> node_modules/aws-sdk/lib/model/operation.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/model/api.js -> node_modules/aws-sdk/lib/model/paginator.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/model/api.js -> node_modules/aws-sdk/lib/model/resource_waiter.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/model/api.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/sequential_executor.js-> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/service.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/service.js -> node_modules/aws-sdk/lib/region_config.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/config.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/config.js -> node_modules/aws-sdk/lib/credentials.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/config.js -> node_modules/aws-sdk/lib/credentials/credential_provider_chain.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/http.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/event_listeners.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/event_listeners.js -> node_modules/aws-sdk/lib/discover_endpoint.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/util.js -> node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/event_listeners.js -> node_modules/aws-sdk/lib/discover_endpoint.js -> node_modules/aws-sdk/lib/util.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/request.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/response.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/resource_waiter.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/v2.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/v3.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/v3https.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/v4.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/v4.js -> node_modules/aws-sdk/lib/signers/v4_credentials.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/s3.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/signers/request_signer.js -> node_modules/aws-sdk/lib/signers/presign.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/aws-sdk/lib/core.js -> node_modules/aws-sdk/lib/param_validator.js -> node_modules/aws-sdk/lib/core.js\n(!) Circular dependency: node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/duplex.js -> node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/readable.js -> node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/duplex.js\n(!) Circular dependency: node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/duplex.js -> node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/writable.js -> node_modules/rollup-plugin-node-builtins/src/es6/readable-stream/duplex.js\n(!) Circular dependency: node_modules/aws-sdk/lib/node_loader.js -> node_modules/aws-sdk/lib/credentials/temporary_credentials.js -> node_modules/aws-sdk/clients/sts.js -> node_modules/aws-sdk/lib/node_loader.js\n(!) Circular dependency: node_modules/aws-sdk/lib/node_loader.js -> node_modules/aws-sdk/lib/credentials/temporary_credentials.js -> node_modules/aws-sdk/clients/sts.js -> commonjs-proxy:/Users/leofarias/Documents/Projects/neardb/node_modules/aws-sdk/lib/node_loader.js -> node_modules/aws-sdk/lib/node_loader.js\n(!) Circular dependency: node_modules/aws-sdk/lib/node_loader.js -> node_modules/aws-sdk/lib/credentials/cognito_identity_credentials.js -> node_modules/aws-sdk/clients/cognitoidentity.js -> node_modules/aws-sdk/lib/node_loader.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLElement.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLElement.js -> commonjs-proxy:/Users/leofarias/Documents/Projects/neardb/node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLCData.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLComment.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDeclaration.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDocType.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDocType.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDTDAttList.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDocType.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDTDEntity.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDocType.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDTDElement.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDocType.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLDTDNotation.js-> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLRaw.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLText.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js\n(!) Circular dependency: node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLProcessingInstruction.js -> node_modules/xml2js/node_modules/xmlbuilder/lib/XMLNode.js. Sorry for the verbose log. I am currently importing the following way\nimport S3 from 'aws-sdk/clients/s3'\nI tried importing as described on the documentation for typescript.\nimport S3 = require('aws-sdk/clients/s3');\nBut I get an error about import assignment not being able to be used for ECMAscript modules\n. Sorry for the verbose log. I am currently importing the following way\nimport S3 from 'aws-sdk/clients/s3'\nI tried importing as described on the documentation for typescript.\nimport S3 = require('aws-sdk/clients/s3');\nBut I get an error about import assignment not being able to be used for ECMAscript modules\n. ",
    "sebastienfi": "Hi @AllanFly120 \nThe credentials.get seems like the way to go in order to exchange the OIDC JWT for AWS JWT to assume a IAM role, and it seems to work. Well at least, it sends back some values !\nThis is the console log you're asking for.\n\nMaybe this as to do with the fact that AWS.config.credentials.identityId is empty ? But it seems kinda logic to me, because the IdP is delegated to a third party, therefore the user doesn't have an ID for AWS.\nOr maybe it's the fact that the request is not signed ? X-Amz-Content-Sha256:UNSIGNED-PAYLOAD. Also in docs it's completely unclear if I should call AWS.config.credentials = new AWS.WebIdentityCredentials ** therefore obtaining identity directly from the identity provider then exchanging my OIDC JWT  and then access S3, or if I should call AWS.config.credentials = new AWS.CognitoIdentityCredentials therefore obtaining identity from the User Pool in which the OpenID IdP is configured.\nAlso please note that I have no User Pool but only Identity Pool (aka. Federated Identities) because my users are stored on a third-party software which handles the sign-in into their account, and sends back a RS256 JWT Token.\nUser Pool vs. Identity Pool\nUser Pool\n\nSay you were creating a new web or mobile app and you were thinking about how to handle user registration, authentication, and account recovery. This is where Cognito User Pools would come in. Cognito User Pool handles all of this and as a developer you just need to use the SDK to retrieve user related information.\n\nIdentity Pool\n\nCognito Identity Pool (or Cognito Federated Identities) on the other hand is a way to authorize your users to use the various AWS services. Say you wanted to allow a user to have access to your S3 bucket so that they could upload a file; you could specify that while creating an Identity Pool. And to create these levels of access, the Identity Pool has its own concept of an identity (or user). The source of these identities (or users) could be a Cognito User Pool or even Facebook or Google.\n\nWhich architecture should I use ?\nGet a JWT Token from AWS in exchange of an OIDC JWT Token\njs\nAWS.config.credentials = new AWS.WebIdentityCredentials({\n  RoleArn: 'arn:aws:iam::<id>:role/<role ID>',\n  WebIdentityToken: localStorage.getItem('oidc.idToken'), // JWT token from identity provider\n  RoleSessionName: localStorage.getItem('email')\n});\nAWS.config.credentials.get(function (err, data) {\n  var accessKeyId = AWS.config.credentials.accessKeyId; // Value is obtained\n  var secretAccessKey = AWS.config.credentials.secretAccessKey; // Value is obtained\n  var sessionToken = AWS.config.credentials.sessionToken; // Value is obtained\n  var identityId = AWS.config.credentials.identityId; // Value is NULL\n});\nvar s3 = new AWS.S3()\ns3.listObjects() // 403 Forbidden\nGet an identity with Cognito (but my users are on Google-Apps GSuite (Entreprise Account)\njs\nAWS.config.credentials = new AWS.CognitoIdentityCredentials({\n  RoleSessionName: localStorage.getItem('email'),\n  IdentityPoolId: 'us-east-1:<federated-identity-pool-id>',\n  Logins: {\n    'auth.example.com/': localStorage.getItem('oidc.idToken')\n  }\n});\nAWS.config.credentials.get(function (err, data) {\n  // AWS.config.credentials.get Error: Invalid login token. Issuer doesn't match providerName\n  // My IdP name is `auth.example.com/` while my JWT Issuer (`:iss`) is `https://auth.example.com/`\n  // I don't have hand on this, as the token is generated by Auth0.\n});\n** : current code provided. Solved this using AWS.CognitoIdentityCredentials with appropriate parameters.. ",
    "TrejGun": "Hi Allan\n\nI don't think this is an issue with SDK, it more of a style inconsistency\n\nYes you are right\n\nIs there any documentation mentioning using promise on s3.geSignedUrl\n\nit says nothing about it, so I spent some time thinking what i'm doing wrong\n\nis used to 'promisify' the operation that makes http request\n\nactually i can't say from the method name if it does request or not\nso please add some notes in docs or add promise method to getSignedUrl. Hi Allan\n\nI don't think this is an issue with SDK, it more of a style inconsistency\n\nYes you are right\n\nIs there any documentation mentioning using promise on s3.geSignedUrl\n\nit says nothing about it, so I spent some time thinking what i'm doing wrong\n\nis used to 'promisify' the operation that makes http request\n\nactually i can't say from the method name if it does request or not\nso please add some notes in docs or add promise method to getSignedUrl. guys its already one year passed since this issue was raised. is it so difficult to add one line to docs?. guys its already one year passed since this issue was raised. is it so difficult to add one line to docs?. ",
    "achelkia": "Thanks for the answer.\nHere is the package.json : \n\"dependencies\": {\n    \"@angular/animations\": \"^5.2.1\",\n    \"@angular/cdk\": \"^5.1.0\",\n    \"@angular/common\": \"^5.2.1\",\n...\n    \"aws-sdk\": \"^2.140.0\",\n  },\n  \"devDependencies\": {\n    \"@angular/cli\": \"^1.6.5\",\n    \"@angular/compiler-cli\": \"^5.2.1\",\n    \"@angular/language-service\": \"^5.2.1\",\n    \"@types/aws-sdk\": \"^2.7.0\",\n    \"express-http-proxy\": \"^1.0.7\",\n    \"typescript\": \"2.4.2\"\n  }\nthe tsconfig.app.json:\n{\n  \"extends\": \"../tsconfig.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"../out-tsc/app\",\n    \"baseUrl\": \"./\",\n    \"module\": \"es2015\",\n    \"types\": [\"node\"]\n  },\n  \"exclude\": [\n    \"test.ts\",\n    \"**/*.spec.ts\"\n  ]\n}\nand the tsconfig.server.json:\n{\n  \"extends\": \"../tsconfig.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"../out-tsc/app\",\n    \"baseUrl\": \"./\",\n    \"module\": \"commonjs\",\n    \"types\": []\n  },\n  \"exclude\": [\n    \"test.ts\",\n    \"**/*.spec.ts\"\n  ],\n  \"angularCompilerOptions\": {\n    \"entryModule\": \"app/app.server.module#AppServerModule\"\n  }\n}\nI already tried moving  @types/aws-sdk from the dev-dependenciesto the dependencies but it didn't change anything.. Oh sorry about that : \n\"dependencies\": {\n    \"@angular/animations\": \"^5.2.1\",\n    \"@angular/cdk\": \"^5.1.0\",\n    \"@angular/common\": \"^5.2.1\",\n    \"@angular/compiler\": \"^5.2.1\",\n    \"@angular/core\": \"^5.2.1\",\n    \"@angular/forms\": \"^5.2.1\",\n    \"@angular/http\": \"^5.2.1\",\n    \"@angular/material\": \"^5.1.0\",\n    \"@angular/platform-browser\": \"^5.2.1\",\n    \"@angular/platform-browser-dynamic\": \"^5.2.1\",\n    \"@angular/platform-server\": \"^5.2.1\",\n    \"@angular/router\": \"^5.2.1\",\n    \"@nguniversal/module-map-ngfactory-loader\": \"^5.0.0-beta.5\",\n    \"@ngx-translate/core\": \"^9.1.1\",\n    \"@ngx-translate/http-loader\": \"^2.0.1\",\n    \"@types/fabric\": \"^1.5.27\",\n    \"@types/node\": \"^8.0.50\",\n    \"angular2-toaster\": \"^4.0.1\",\n    \"aws-sdk\": \"^2.140.0\",\n    \"bugsnag-js\": \"^4.0.3\",\n    \"core-js\": \"^2.5.3\",\n    \"fabric\": \"2.0.0-rc.4\",\n    \"file-loader\": \"^1.1.6\",\n    \"font-awesome\": \"^4.7.0\",\n    \"hammerjs\": \"^2.0.8\",\n    \"intl\": \"^1.2.5\",\n    \"ngx-carousel\": \"^1.3.5\",\n    \"ngx-facebook\": \"^2.4.0\",\n    \"ngx-infinite-scroll\": \"^0.6.1\",\n    \"ngx-webstorage\": \"^2.0.1\",\n    \"rxjs\": \"^5.5.6\",\n    \"ts-loader\": \"^3.5.0\",\n    \"web-animations-js\": \"^2.3.1\",\n    \"webpack\": \"^3.10.0\",\n    \"zone.js\": \"^0.8.20\"\n  }\nWe do have @types/node in our dependencies. I didn't put the whole thing for readability. \nAs I was saying, when we build using ng build --prod everything runs fine.. Hi @AllanFly120, thanks for the answer. \nThat was it, removing \"types\": [] or adding node did it for me.\nThanks again.. ",
    "leonard333": "I have the same problem but with angular 6, and even with these changes that comment can not solve the problem, I hope you can help me thanks!\n\n. ",
    "carlara75": "Hi, just upgraded to angular 6 yesterday and I having the same issue when trying to upload an object to S3. Browsing for answer I got to a post, which somehow I managed to lose, that gave me some ideas. The issue seems to be the SDK is not recognising it is running in a browser.\nI just edited, for testing purpose only, node_module/aws-sdk/lib/utils.js (line 40) to\nisNode: function isNode() { return false },\ninstead\nisNode: function isNode() { return !util.isBrowser(); },\nand I have my application running again. \nI assume something changed in the guts of angular or nodes, in my case running v6.0.3 and v8.11.3 respectively, which makes aws-sdk to behave differently.  . Hi, just upgraded to angular 6 yesterday and I having the same issue when trying to upload an object to S3. Browsing for answer I got to a post, which somehow I managed to lose, that gave me some ideas. The issue seems to be the SDK is not recognising it is running in a browser.\nI just edited, for testing purpose only, node_module/aws-sdk/lib/utils.js (line 40) to\nisNode: function isNode() { return false },\ninstead\nisNode: function isNode() { return !util.isBrowser(); },\nand I have my application running again. \nI assume something changed in the guts of angular or nodes, in my case running v6.0.3 and v8.11.3 respectively, which makes aws-sdk to behave differently.  . Just an update, I upgraded dependencies to Angular 6.0.9 and aws-sdk 2.276.1 and problem still persists.. Just an update, I upgraded dependencies to Angular 6.0.9 and aws-sdk 2.276.1 and problem still persists.. Hi Karthi,\nIf you read my post, more or less I got to the same conclusion. \nI am not actively chasing this issue because I have an stack of things to do and the hack I documented above works for the time being, however I would be nice to have the official fix.. Hi Karthi,\nIf you read my post, more or less I got to the same conclusion. \nI am not actively chasing this issue because I have an stack of things to do and the hack I documented above works for the time being, however I would be nice to have the official fix.. Hi Karthi, \nAs I said at the end of the original post I was not sure if the problem was AWS-SDK, Angular or Nodes. This is my first project with Angular 5/6 and the AWS-SDK and things like this are challenging when learning.\nJust to complete your comment above, the script below is added when migrating Angular from 5 to 6 on ${PROJECT-ROOT}/src/index.html:\n<script>\nvar global = global || window;\n  var Buffer = Buffer || [];\n  var process = process || {\n      env: { DEBUG: undefined },\n      version: []\n  };\n</script>\nAs you said before, the issue disappear when removing var process = { ...  } from this script:\n<script>\nvar global = global || window;\nvar Buffer = Buffer || [];\n</script>\nMany thanks for this.\nHopefully this does not break something else down the line.\nCheers,\nCarlos\n. Hi Karthi, \nAs I said at the end of the original post I was not sure if the problem was AWS-SDK, Angular or Nodes. This is my first project with Angular 5/6 and the AWS-SDK and things like this are challenging when learning.\nJust to complete your comment above, the script below is added when migrating Angular from 5 to 6 on ${PROJECT-ROOT}/src/index.html:\n<script>\nvar global = global || window;\n  var Buffer = Buffer || [];\n  var process = process || {\n      env: { DEBUG: undefined },\n      version: []\n  };\n</script>\nAs you said before, the issue disappear when removing var process = { ...  } from this script:\n<script>\nvar global = global || window;\nvar Buffer = Buffer || [];\n</script>\nMany thanks for this.\nHopefully this does not break something else down the line.\nCheers,\nCarlos\n. Karthi found out the issue is not in the SDK but in the migration process from Angular 5 to 6.\nClosing the issue.. Karthi found out the issue is not in the SDK but in the migration process from Angular 5 to 6.\nClosing the issue.. Hi DeveloperAsela,\nCareful with this change, this was just a hack to make things work when I upgrade my project from Angular 5 to 6 meanwhile investigating. See the comment above and you will find maybe your problem is related to the variable process being included at ${PROJECT-ROOT}/src/index.html as one of the upgrade guides points out.\nYour scripts in index.html should look like below (without any variable process defined or overwritten):\n<script>\nvar global = global || window;\nvar Buffer = Buffer || [];\n</script>\nCheers\nCarlos. Hi DeveloperAsela,\nCareful with this change, this was just a hack to make things work when I upgrade my project from Angular 5 to 6 meanwhile investigating. See the comment above and you will find maybe your problem is related to the variable process being included at ${PROJECT-ROOT}/src/index.html as one of the upgrade guides points out.\nYour scripts in index.html should look like below (without any variable process defined or overwritten):\n<script>\nvar global = global || window;\nvar Buffer = Buffer || [];\n</script>\nCheers\nCarlos. > Hello @carlara75 I have a problem with this change.\n\nI'm using Algolia Search for Angular 6 and they say:\n\nIf you are using Angular v6 you will need an extra step, polyfill process.env by adding in your src/polyfill.ts:\n(window as any).process = {env: {}};\n\nI need add env.process to run my project with Algolia.\nWhat could I do to solve my problem?\nhttps://community.algolia.com/angular-instantsearch/guides/migration-guide.html\n\nSorry for the delayed response. I am far from being an Angular expert, however I would like to help if I can. Note I am not familiar with Algolia at all and without a source code to look at and your error stack trace I am blind.\nMy suggestions: \n open a proper question/bug at the Algolia support page.\n open a question in stackoverflow or similiar community pages with a sample code, your package.json,  and the error stack trace and share with me the link.\nThis post was opened for an issue I found when upgraded from Angular 5 to 6 and the AWS-SDK.. > Hello @carlara75 I have a problem with this change.\n\nI'm using Algolia Search for Angular 6 and they say:\n\nIf you are using Angular v6 you will need an extra step, polyfill process.env by adding in your src/polyfill.ts:\n(window as any).process = {env: {}};\n\nI need add env.process to run my project with Algolia.\nWhat could I do to solve my problem?\nhttps://community.algolia.com/angular-instantsearch/guides/migration-guide.html\n\nSorry for the delayed response. I am far from being an Angular expert, however I would like to help if I can. Note I am not familiar with Algolia at all and without a source code to look at and your error stack trace I am blind.\nMy suggestions: \n open a proper question/bug at the Algolia support page.\n open a question in stackoverflow or similiar community pages with a sample code, your package.json,  and the error stack trace and share with me the link.\nThis post was opened for an issue I found when upgraded from Angular 5 to 6 and the AWS-SDK.. ",
    "chandra-kantha": "Adding \"types\":[\"node\"] to tsconfig.app.json file worked for me.\n. ",
    "SammySrivastava": "Angular 6,\nFirst I got the error while compiling : \nERROR in node_modules/aws-sdk/clients/acm.d.ts(133,37): error TS2304: Cannot find name 'Buffer'\nSo as suggested in this post:\nI added \"types\": [\"node\"] to tsconfig.app.json\nProject Compiled successfully.\nBut then it did open on browser.\nGot the error in console: \nUncaught ReferenceError: global is not defined\nSo as suggested in other issue on https://github.com/aws-amplify/amplify-js/issues/678\nadded following line \n// Add global to window, assigning the value of window itself.\n(window as any).global = window;\ngot my app working. ",
    "Banzyme": "\nAdding \"types\":[\"node\"] to tsconfig.app.json file worked for me.\n\nThis worked for me on IONIC 4.5.0 & Angular 7.1.3. Alternatively you can remove types from the same location.. ",
    "VivithaAlamur": "We are also facing the same issue  while building the library in angular7.\n our package.json file inside library::\n{\n  \"name\": \"common-registration\",\n  \"version\": \"0.0.1-5\",\n  \"peerDependencies\": {\n    \"@angular/common\": \"^7.2.0\",\n    \"@angular/core\": \"^7.2.0\",\n    \"@lodash.merge\": \"^4.6.1\",\n    \"@angular/http\": \"^7.2.0\",\n    \"@angular/router\": \"^7.2.0\",\n    \"rxjs\": \"^6.3.3\",\n    \"@angular/forms\": \"^7.2.0\",\n    \"@angular/material\": \"^7.1.1\",\n    \"ngx-toastr\": \"^9.1.1\",\n    \"@angular/cdk\": \"^7.1.1\",\n    \"ngx-auto-unsubscribe\": \"^2.4.1\",\n    \"aws-sdk\": \"^2.270.1\",\n    \"ngx-webstorage\": \"^3.0.0-beta.14\",\n    \"@ng-select/ng-select\": \"^2.14.0\",\n    \"@ngx-translate/core\": \"^10.0.2\",\n    \"@ngx-translate/http-loader\": \"^3.0.1\",\n    \"@types/node\": \"^8.10.26\"\n  }\n}\npackage.json for app\n{\n  \"name\": \"registration\",\n  \"version\": \"0.0.0\",\n  \"scripts\": {\n    \"ng\": \"ng\",\n    \"start\": \"ng serve\",\n    \"build\": \"ng build\",\n    \"test\": \"ng test\",\n    \"lint\": \"ng lint\",\n    \"e2e\": \"ng e2e\",\n    \"prebuild:library:dev\": \"cross-env LIB_ENV=dev node projects/common-registration/src/lib/environment-switcher.js\",\n    \"build:library:dev\": \"ng build common-registration --prod\",\n    \"prebuild:library:qa\": \"cross-env LIB_ENV=qa node projects/common-registration/src/lib/environment-switcher.js\",\n    \"build:library:qa\": \"ng build common-registration --prod\",\n    \"prebuild:library:prod\": \"cross-env LIB_ENV=prod node projects/common-registration/src/lib/environment-switcher.js\",\n    \"build:library:prod\": \"ng build common-registration --prod\"\n  },\n  \"private\": true,\n  \"dependencies\": {\n    \"@angular/animations\": \"~7.2.0\",\n    \"@angular/cdk\": \"^7.1.1\",\n    \"@angular/common\": \"^7.2.0\",\n    \"@angular/compiler\": \"~7.2.0\",\n    \"@angular/core\": \"^7.2.0\",\n    \"@angular/forms\": \"^7.2.0\",\n    \"@angular/http\": \"^7.2.0\",\n    \"@angular/material\": \"^7.1.1\",\n    \"@angular/platform-browser\": \"~7.2.0\",\n    \"@angular/platform-browser-dynamic\": \"~7.2.0\",\n    \"@angular/router\": \"^7.2.0\",\n    \"@ng-select/ng-select\": \"^2.14.0\",\n    \"@ngx-translate/core\": \"^10.0.2\",\n    \"@ngx-translate/http-loader\": \"^3.0.1\",\n    \"aws-sdk\": \"^2.270.1\",\n    \"core-js\": \"^2.5.4\",\n    \"lodash.merge\": \"^4.6.1\",\n    \"ngx-auto-unsubscribe\": \"^2.4.1\",\n    \"ngx-toastr\": \"^9.1.1\",\n    \"ngx-webstorage\": \"^3.0.0-beta.14\",\n    \"rxjs\": \"^6.3.3\",\n    \"tslib\": \"^1.9.0\",\n    \"zone.js\": \"~0.8.26\"\n  },\n  \"devDependencies\": {\n    \"@angular-devkit/build-angular\": \"~0.12.0\",\n    \"@angular-devkit/build-ng-packagr\": \"~0.12.0\",\n    \"@angular/cli\": \"~7.2.3\",\n    \"@angular/compiler-cli\": \"~7.2.0\",\n    \"@angular/language-service\": \"~7.2.0\",\n    \"@types/jasmine\": \"~2.8.8\",\n    \"@types/jasminewd2\": \"~2.0.3\",\n    \"@types/node\": \"^8.10.39\",\n    \"codelyzer\": \"~4.5.0\",\n    \"jasmine-core\": \"~2.99.1\",\n    \"jasmine-spec-reporter\": \"~4.2.1\",\n    \"karma\": \"~3.1.1\",\n    \"karma-chrome-launcher\": \"~2.2.0\",\n    \"karma-coverage-istanbul-reporter\": \"~2.0.1\",\n    \"karma-jasmine\": \"~1.1.2\",\n    \"karma-jasmine-html-reporter\": \"^0.2.2\",\n    \"ng-packagr\": \"^4.2.0\",\n    \"protractor\": \"~5.4.0\",\n    \"ts-node\": \"~7.0.0\",\n    \"tsickle\": \">=0.34.0\",\n    \"tslib\": \"^1.9.0\",\n    \"tslint\": \"~5.11.0\",\n    \"typescript\": \"~3.2.2\"\n  }\n}\ntsconfig.app.json\n{\n  \"extends\": \"../tsconfig.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"../out-tsc/app\",\n    \"types\": [\"node\"]\n  },\n  \"exclude\": [\n    \"test.ts\",\n    \"*/.spec.ts\"\n  ]\n}\ntsconfig.json\n{\n  \"compileOnSave\": false,\n  \"compilerOptions\": {\n    \"baseUrl\": \"./\",\n    \"outDir\": \"./dist/out-tsc\",\n    \"sourceMap\": true,\n    \"declaration\": false,\n    \"module\": \"es2015\",\n    \"moduleResolution\": \"node\",\n    \"emitDecoratorMetadata\": true,\n    \"experimentalDecorators\": true,\n    \"importHelpers\": true,\n    \"target\": \"es5\",\n    \"typeRoots\": [\n      \"node_modules/@types\"\n    ],\n    \"types\": [ \"node\" ],\n    \"lib\": [\n      \"es2018\",\n      \"dom\"\n    ],\n    \"paths\": {\n      \"common-registration\": [\n        \"dist/common-registration\"\n      ],\n      \"common-registration/\": [\n        \"dist/common-registration/\"\n      ]\n    }\n  }\n}\nFacing build error\nBUILD ERROR\nnode_modules/aws-sdk/lib/http_response.d.ts(1,25): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/lib/http_response.d.ts(14,18): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/acm.d.ts(132,37): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/acm.d.ts(134,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/acm.d.ts(468,32): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/acm.d.ts(470,32): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/apigateway.d.ts(1146,23): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/clouddirectory.d.ts(1573,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/cloudsearchdomain.d.ts(7,24): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/clients/cloudsearchdomain.d.ts(42,23): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/cloudtrail.d.ts(141,28): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/codecommit.d.ts(634,29): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/codecommit.d.ts(1601,22): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/cognitoidentityserviceprovider.d.ts(2581,31): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/directconnect.d.ts(992,28): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/dms.d.ts(448,35): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/lib/dynamodb/document_client.d.ts(2,25): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/lib/dynamodb/document_client.d.ts(93,30): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/lib/dynamodb/document_client.d.ts(274,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field\nin your tsconfig.\nnode_modules/aws-sdk/clients/dynamodb.d.ts(498,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/dynamodbstreams.d.ts(92,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/ec2.d.ts(3074,23): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/ecr.d.ts(767,31): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/firehose.d.ts(182,22): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/lib/services/glacier.d.ts(10,28): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/glacier.d.ts(7,24): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/clients/glacier.d.ts(1313,24): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iam.d.ts(1120,32): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iam.d.ts(3118,35): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iot.d.ts(4487,27): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iotdata.d.ts(73,30): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iotdata.d.ts(74,25): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/kinesis.d.ts(237,22): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/kms.d.ts(328,32): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/kms.d.ts(962,31): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/lambda.d.ts(7,24): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/clients/lambda.d.ts(372,23): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/lambda.d.ts(373,28): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/lexruntime.d.ts(7,24): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/clients/lexruntime.d.ts(35,28): error TS2580: Cannot find name 'Buffer'. Do you need to\ninstall type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/polly.d.ts(8,24): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/clients/polly.d.ts(70,29): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/rekognition.d.ts(1135,27): error TS2580: Cannot find name 'Buffer'. Do you need\nto install type definitions for node? Try npm i @types/node and then add node to the types field in your\ntsconfig.\nnode_modules/aws-sdk/clients/ses.d.ts(1513,32): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/sns.d.ts(275,24): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/sqs.d.ts(216,24): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/ssm.d.ts(4954,48): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/support.d.ts(336,22): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/waf.d.ts(628,39): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/wafregional.d.ts(672,39): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/lexmodelbuildingservice.d.ts(306,23): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/mobile.d.ts(106,26): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/mediastoredata.d.ts(7,24): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/clients/mediastoredata.d.ts(199,29): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/appsync.d.ts(248,23): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/kinesisvideoarchivedmedia.d.ts(7,24): error TS2307: Cannot find module 'stream'.node_modules/aws-sdk/clients/kinesisvideoarchivedmedia.d.ts(121,25): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/kinesisvideomedia.d.ts(7,24): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/clients/kinesisvideomedia.d.ts(53,25): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/sagemakerruntime.d.ts(24,26): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/acmpca.d.ts(283,37): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/acmpca.d.ts(285,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/acmpca.d.ts(353,25): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/secretsmanager.d.ts(627,34): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iotanalytics.d.ts(1007,32): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/lib/config.d.ts(1,34): error TS2307: Cannot find module 'http'.\nnode_modules/aws-sdk/lib/config.d.ts(2,35): error TS2307: Cannot find module 'https'.\nnode_modules/aws-sdk/lib/request.d.ts(1,25): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/lib/request.d.ts(132,45): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/lib/event-stream/event-stream.d.ts(1,55): error TS2503: Cannot find namespace 'NodeJS'.\nnode_modules/aws-sdk/clients/s3.d.ts(11,24): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/clients/s3.d.ts(787,22): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/s3.d.ts(1151,42): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/s3.d.ts(3347,15): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/s3.d.ts(3553,32): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nError: node_modules/aws-sdk/lib/http_response.d.ts(1,25): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/lib/http_response.d.ts(14,18): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/acm.d.ts(132,37): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/acm.d.ts(134,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/acm.d.ts(468,32): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/acm.d.ts(470,32): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/apigateway.d.ts(1146,23): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/clouddirectory.d.ts(1573,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/cloudsearchdomain.d.ts(7,24): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/clients/cloudsearchdomain.d.ts(42,23): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/cloudtrail.d.ts(141,28): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/codecommit.d.ts(634,29): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/codecommit.d.ts(1601,22): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/cognitoidentityserviceprovider.d.ts(2581,31): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/directconnect.d.ts(992,28): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/dms.d.ts(448,35): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/lib/dynamodb/document_client.d.ts(2,25): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/lib/dynamodb/document_client.d.ts(93,30): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/lib/dynamodb/document_client.d.ts(274,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field\nin your tsconfig.\nnode_modules/aws-sdk/clients/dynamodb.d.ts(498,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/dynamodbstreams.d.ts(92,38): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/ec2.d.ts(3074,23): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/ecr.d.ts(767,31): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.node_modules/aws-sdk/clients/firehose.d.ts(182,22): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/lib/services/glacier.d.ts(10,28): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/glacier.d.ts(7,24): error TS2307: Cannot find module 'stream'.\nnode_modules/aws-sdk/clients/glacier.d.ts(1313,24): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iam.d.ts(1120,32): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iam.d.ts(3118,35): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iot.d.ts(4487,27): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iotdata.d.ts(73,30): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\nnode_modules/aws-sdk/clients/iotdata.d.ts(74,25): error TS2580: Cannot find name 'Buffer'. Do you need to install type definitions for node? Try npm i @types/node and then add node to the types field in your tsconfig.\n. ",
    "xlukaszgrabowskix": "I'm using AWS.NodeHttpClient().\n(new AWS.NodeHttpClient()).handleRequest {} doesn't return Authorization error when AWS Elasticsearch cluster access is denied. Other errors are handled without issue.\nThanks. I'm using AWS.NodeHttpClient().\n(new AWS.NodeHttpClient()).handleRequest {} doesn't return Authorization error when AWS Elasticsearch cluster access is denied. Other errors are handled without issue.\nThanks. ",
    "efortes": "@jeskew seemed like an internal problem. The AWS_CONFIG_FILE env was there and pointing to the wrong path. I will close this ticket. Thank you for your reply.. ",
    "chmontgomery": "sorry, just realized this needs to be a stringified json object, not a real object \ud83d\ude22 . sorry, just realized this needs to be a stringified json object, not a real object \ud83d\ude22 . ",
    "RobertLowe": "I was just digging for these!\nIt doesn't look like too much effort to implement either:\nSomething like:\nhttps://github.com/aws/aws-sdk-js/blob/master/clients/ec2.js#L13\nhttps://github.com/aws/aws-sdk-js/blob/master/apis/ec2-2016-11-15.waiters2.json\nInto:\nhttps://github.com/aws/aws-sdk-js/blob/master/clients/medialive.js#L11\nI looked quickly, maybe I can monkey it in to my app. I might report back later. \n\nAll in all it seems like the Media Services are still half-beta. My larger concern is some the latency I've seen spinning up . @zshenker I'm not sure, I didn't really bother digging in, I just wrote something quick and dirty like this:\nlet state = \"IDLE\";\n\nnew Promise((resolve, reject)=>{\n  // poll every 3 seconds for idle\n  let statusCheck = setInterval(() => {\n\n    medialive.describeChannel({ ChannelId: ChannelId }, (error, data) => {\n\n      if(error){\n        console.log(\"waitForChannelState error:\", error);\n        reject(new Error(\"Something went wrong\"));\n      } else {\n\n        if(data.State == state){\n          clearInterval(statusCheck);\n          clearTimeout(tooLong);\n          resolve(data);\n        }\n\n      }\n\n    });\n\n  }, 3000);\n\n  // took long, stop\n  let tooLong = setTimeout(() => {\n    clearInterval(statusCheck);\n    console.log(\"waitForChannelState error: took too long!\");\n    reject(new Error(\"Somehing went wrong\"));\n  }, 30000); // it's slow\n\n}).then((channel)=>{\n  console.log(\"Got new state:\", channel.State)\n}).catch((error)=>{\n  console.log(\"Bad stuff:\", error)\n});\n\nWorks enough for me, but uh I quickly ported/ripped that out of my app, no promises it isn't typo free.. ",
    "zshenker": "@RobertLowe My understanding is that those JSON files in the apis/ directory cannot be modified directly in this project?. @RobertLowe My understanding is that those JSON files in the apis/ directory cannot be modified directly in this project?. ",
    "vivekkr12": "Closing this as it needs to be posted in aws-java-sdk. ",
    "duxing": "hi @chrisradek \nI have done that already. see the part I posted about my .bash_profile file.\nI've also verified this at runtime by logging process.env and verified it was set to 1 as an available env variable to the process. hi @chrisradek \nI have done that already. see the part I posted about my .bash_profile file.\nI've also verified this at runtime by logging process.env and verified it was set to 1 as an available env variable to the process. thanks for the quick response.\nI just checked that and got the same error message.\nI did see aws.config.credentials being forced to SharedIniFileCredentials and the disableAssumeRole was false in this case.\nso I guess it's safe to assume this flag is not the cause of the issue.\nI'm not familiar with the source code, but setting up a small project should be trivial (all information needed has been listed). do you think you can take some time to debug this when available?. thanks for the quick response.\nI just checked that and got the same error message.\nI did see aws.config.credentials being forced to SharedIniFileCredentials and the disableAssumeRole was false in this case.\nso I guess it's safe to assume this flag is not the cause of the issue.\nI'm not familiar with the source code, but setting up a small project should be trivial (all information needed has been listed). do you think you can take some time to debug this when available?. still no luck :(\nwith my set up it only works when I have an access-key-based default profile under ~/.aws/credentials\ndo you know why the setting for telling aws-sdk to read from ~/.aws/credentials wasn't working as expected?. still no luck :(\nwith my set up it only works when I have an access-key-based default profile under ~/.aws/credentials\ndo you know why the setting for telling aws-sdk to read from ~/.aws/credentials wasn't working as expected?. I did another test:\n - move the credentials from ~/.aws/credentials to ~/.aws/config and name the profile as default\n - commented out the few AWS environment variable related lines in ~/.bash_profile and source it again.\nrunning my sample code and I got successful response.\nseems like the sdk is smart enough to figure out the credentials even when they're located in the wrong location and the environment variables are off.\ncan you point me to the place in source code where credentials are loaded from disk, as well as the part for picking which file to load from? I feel debugging this is the best approach . I did another test:\n - move the credentials from ~/.aws/credentials to ~/.aws/config and name the profile as default\n - commented out the few AWS environment variable related lines in ~/.bash_profile and source it again.\nrunning my sample code and I got successful response.\nseems like the sdk is smart enough to figure out the credentials even when they're located in the wrong location and the environment variables are off.\ncan you point me to the place in source code where credentials are loaded from disk, as well as the part for picking which file to load from? I feel debugging this is the best approach . @chrisradek sry I was afk for the past 30mins. thanks for digging into this.\nI didn't even notice the profile default thing while writing this issue down. \nafter correcting that to default, the configuration I had started working. \nthanks!. @chrisradek sry I was afk for the past 30mins. thanks for digging into this.\nI didn't even notice the profile default thing while writing this issue down. \nafter correcting that to default, the configuration I had started working. \nthanks!. ",
    "bahtou": "Having a similar issue as @duxing \nversion: aws-sdk@2..263.1\nOS: macOS Sierra 10.13.4\nSample code:\n```\nvar AWS = require('aws-sdk');\nAWS.config.update({\n  region: 'us-east-1'\n});\nvar sts = new AWS.STS();\nsts.assumeRole({\n  RoleArn: \n  RoleSessionName: \n}, function(err, data) {\n  if (err) { // an error occurred\n    console.log('Cannot assume role');\n    console.log(err, err.stack);\n  } else { // successful response\n    AWS.config.update({\n      accessKeyId: data.Credentials.AccessKeyId,\n      secretAccessKey: data.Credentials.SecretAccessKey,\n      sessionToken: data.Credentials.SessionToken\n    });\n  }\n});\n```\nI followed the steps outlined in this thread and still unable to successfully assumeRole.\n.aws/config\n[default]\nregion=us-east-1\nrole_arn=<arn-role>\nsource_profile=default\naws_access_key_id=<access-key>\naws_secret_access_key=<access-secret>\nI commented out everything in the credentials file and also in the .bash_profile. \nSharedIniFileCredentials {\n  expired: false,\n  expireTime: null,\n  accessKeyId: undefined,\n  sessionToken: undefined,\n  filename: undefined,\n  profile: 'default',\n  disableAssumeRole: false,\n  preferStaticCredentials: false }. @shyamkumar123 \ndid you go through the process outlined above?\nIf yes, make sure the AWS is configured right to accept the role. I ran into this issue and after weeks it turned out that AWS wasn't configured right to accept the role.  FYI. I changed my test code to \n```\nasync function connAWS() {\n  var sts = new AWS.STS({ region: 'us-east-1' });\nconst params = {\n    RoleArn: '',\n    RoleSessionName: 'awssdk'\n  };\n  let assumeRole;\ntry {\n    assumeRole1 = await sts.assumeRole(params).promise();\n  } catch (err) {\n    console.log('Cannot assume role');\n    console.log(err, err.stack);\n    return;\n  }\nconst accessparams = {\n    accessKeyId: assumeRoleStep1.Credentials.AccessKeyId,\n    secretAccessKey: assumeRoleStep1.Credentials.SecretAccessKey,\n    sessionToken: assumeRoleStep1.Credentials.SessionToken,\n  };\nconst s3 = await new AWS.S3(accessparams);\n  console.log('s3', s3);\n}\nconnAWS();\n```\nDevops had the assumeRole misconfigured. The above works.\nclosing. @shivarajbakale whats your setup look like?. @rhaegar453 I am currently using the above configuration on my machine without issues. I would suggest you make sure your ~/.aws/config & ~/.aws/credentials have the necessary creds. Also, double check the configuration in the AWS console to make sure assumeRole is configured correctly (as that was the ONLY issue preventing the above to work. Once the assumeRole was configured correctly the issue went away.. ",
    "shyamkumar123": "hi CredentialsError: Missing credentials in config\n    at ClientRequest. (C:\\Users\\Zinios Dev\\Downloads\\image-rekognition\\node_modules\\aws-sdk\\lib\\http\\node.js:83:34)    at Object.onceWrapper (events.js:313:30)    at emitNone (events.js:106:13)\n    at ClientRequest.emit (events.js:208:7)\n    at Socket.emitTimeout (_http_client.js:706:34)\n    at Object.onceWrapper (events.js:313:30)\n    at emitNone (events.js:106:13)\n    at Socket.emit (events.js:208:7)\n    at Socket._onTimeout (net.js:410:8)\n    at ontimeout (timers.js:498:11)\n  message: 'Missing credentials in config',\n  code: 'CredentialsError',\n  time: 2018-08-06T18:05:45.323Z,\n  retryable: true,\n  originalError:\n   { message: 'Could not load credentials from any providers',\n     code: 'CredentialsError',\n     time: 2018-08-06T18:05:45.323Z,\n     retryable: true,\n     originalError:\n      { message: 'Connection timed out after 1000ms',\n        code: 'TimeoutError',\n        time: 2018-08-06T18:05:45.323Z,\n        retryable: true } } } 'CredentialsError: Missing credentials in config\\n    at ClientRequest. (C:\\Users\\Zinios Dev\\Downloads\\image-rekognition\\node_modules\\aws-sdk\\lib\\http\\node.js:83:34)\\n    at Object.onceWrapper (events.js:313:30)\\n    at emitNone (events.js:106:13)\\n    at ClientRequest.emit (events.js:208:7)\\n    at Socket.emitTimeout (_http_client.js:706:34)\\n    at Object.onceWrapper (events.js:313:30)\\n    at emitNone (events.js:106:13)\\n    at Socket.emit (events.js:208:7)\\n    at Socket._onTimeout (net.js:410:8)\\n    at ontimeout (timers.js:498:11)'\n{ CredentialsError: Missing credentials in config\n    at ClientRequest. (C:\\Users\\Zinios Dev\\Downloads\\image-rekognition\\node_modules\\aws-sdk\\lib\\http\\node.js:83:34)\n    at Object.onceWrapper (events.js:313:30)\n    at emitNone (events.js:106:13)\n    at ClientRequest.emit (events.js:208:7)\n    at Socket.emitTimeout (_http_client.js:706:34)\n    at Object.onceWrapper (events.js:313:30)\n    at emitNone (events.js:106:13)\n    at Socket.emit (events.js:208:7)\n    at Socket._onTimeout (net.js:410:8)\n    at ontimeout (timers.js:498:11)\n  message: 'Missing credentials in config',\n  code: 'CredentialsError',\n  time: 2018-08-06T18:05:45.332Z,\n  retryable: true,\n  originalError:\n   { message: 'Could not load credentials from any providers',\n     code: 'CredentialsError',\n     time: 2018-08-06T18:05:45.332Z,\n     retryable: true,\n     originalError:\n      { message: 'Missing credentials in config',\n        code: 'CredentialsError',\n        time: 2018-08-06T18:05:45.323Z,\n        retryable: true,\n        originalError: [Object] } } } 'CredentialsError: Missing credentials in config\\n    at ClientRequest.\n(C:\\Users\\Zinios Dev\\Downloads\\image-rekognition\\node_modules\\aws-sdk\\lib\\http\\node.js:83:34)\\n    at Object.onceWrapper (events.js:313:30)\\n    at emitNone (events.js:106:13)\\n    at ClientRequest.emit (events.js:208:7)\\n    at Socket.emitTimeout (_http_client.js:706:34)\\n    at Object.onceWrapper (events.js:313:30)\\n    at emitNone (events.js:106:13)\\n    at Socket.emit (events.js:208:7)\\n    at Socket._onTimeout (net.js:410:8)\\n    at ontimeout (timers.js:498:11)'\nhow to solve above issue ,please help me. ",
    "chemitaxis": "Same error here... All working fine during months, today Sentry has logged this error in my EC2 instance... \n// timeout support\n    stream.setTimeout(httpOptions.timeout || 0, function() {\n      if (stream.didCallback) return; stream.didCallback = true;\n      var msg = 'Connection timed out after ' + httpOptions.timeout + 'ms';\n      errCallback(AWS.util.error(new Error(msg), {code: 'TimeoutError'}));\n      stream.abort();\n    });. ",
    "ted-intradenver": "Making sure the AWS CLI is running and the default user is configured helps. (note: how to Mac OSX brew install here: https://www.code2bits.com/how-to-install-awscli-on-macos-using-homebrew/) Then (https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html). . ",
    "ronayadid": "Thanks @jeskew \nSo the way S3 can automatically delete object is by using a bucket lifecycle rule?. ",
    "Setup007": "The data is e.g. this pdf \nGrimm__Maerchen.pdf\nBut it also happens with other files.\nThe file object put into the body has some Backbone.js collections attached to it, which themselves hold multiple Backbone Collections - while Backbone Models in those Backbone Collections can hold more Backbone Collections and so on. Is that the issue? Why is it not occuring when using the AWS.S3.ManagedUpload() ?\n. ",
    "esbenvb": "I've made it work like this:\n```js\n// updateQueues.js\nimport AWS from 'aws-sdk'\n// exported for test mocks\nexport const sqs = new AWS.SQS()\nexport const main = async event => {\n  // ...\n  sqs.deleteQueue({ aaa: 1234 })\n// updateQueues.test.js\nimport { main as updateQueues, sqs } from './updateQueues'\nconst addEvent = require('../../mocks/tableAddEvent.json')\ndescribe('updateQueues', () => {\n  it('deletes queue for removed object', async () => {\n    sqs.deleteQueue = jest.fn()\n    await updateQueues(addEvent)\n    expect(sqs.deleteQueue).toHaveBeenCalledWith({ aaa: 1234 })\n  })\n})\n. ",
    "fcarrilLB": "@chrisradek\nThat's exactly what I was looking for, thank you very much!. ",
    "rbirkby": "Is this browser-based? If so, StepFunctions does not support CORS and therefore isn't included in the default build:\nhttps://github.com/aws/aws-sdk-js/blob/master/SERVICES.md. Is this browser-based? If so, StepFunctions does not support CORS and therefore isn't included in the default build:\nhttps://github.com/aws/aws-sdk-js/blob/master/SERVICES.md. ",
    "tnyanhongo": "@AllanFly120 Here is the link to the tut Im following \nhttps://aws.amazon.com/blogs/big-data/a-zero-administration-amazon-redshift-database-loader/\nAs far as the code I believe here are lines you need.\nFirst two are 440&441\n\nstatements.map(function (item) { \nif (item.Resource ===   functionArn && item.Condition.StringEquals['AWS:SourceAccount'] ===   sourceAccount) {\nfoundMatch = true; \u00a0\n} \nNote In the documentation this is the Getting Started: Entering the Configuration--.run the setup.js script by entering node setup.js in Git Bash \nThe code isnt mine and I have the common.js file installed locally when I installed AWS SDK for JavaScript .\nLet me know if u need more information and thanks for looking at this!. ",
    "JimtotheB": "@AllanFly120 I started to type this up and stopped to investigate before hitting save. I think this may have to do with how Firefox is dealing with a 200 and an empty response body.\nThis is the actual error that I get in the web console, its only when I click into it that I see the <Code>SignatureDoesNotMatch</Code> Error, which on further inspection is a GET request, not the original post.\nXML Parsing Error: no root element found\nLocation: https://s3.amazonaws.com/snip.snip.snip/c0b1ce4d-da18-4841-8aac-d00bdad74c13?Content-Type=image%2Fjpeg&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=SNIP%2F20180316%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20180316T030128Z&X-Amz-Expires=3600&X-Amz-Signature=snipe&X-Amz-SignedHeaders=host\n  Line Number 1, Column 1:\n. ",
    "zimny": "+1. +1. ",
    "abajowski": "+1. ",
    "gbegher": "+1 -- related: https://github.com/feross/ieee754/issues/17. +1 -- related: https://github.com/feross/ieee754/issues/17. ",
    "devTalha": "I am facing this issue explicitly with 'aws-sdk'. When i add it in node modules i get the exception but when i remove it's all fine.\n. @zimny , @abajowski , @gbegher  at the moment following patch fixed my issue:\nI added following lines to buildspec.yml after 'npm install'\n\nls $CODEBUILD_SRC_DIR\nfind $CODEBUILD_SRC_DIR/node_modules -mtime +10950 -exec touch {} \\;\n\nBut i want aws to fix to this issue. I am really disappointed aws-sdk is not working with aws.. . @chrisradek mentioned patch fixes the issue. But again i want aws to fix their issue as it spoiled my weekend roundly 48 hours. so anyone else doesn't have issue and aws-sdk works with aws pipeline. And yeah you got it right it happens when i add aws-sdk to my dependencies.\nThanks for concern.. ",
    "dkrosso": "+1. +1. ",
    "TonyFNZ": "I have upgraded to v2.211.1 and done a clean install, but I still get this issue when running aws cloudformation deploy ....\nSimilar to @devTalha, the following command fixes things for me:\nfind ./node_modules -mtime +10950 -exec touch {} \\;\nNote, prior to running the above command, the following files appear to have very old modified dates:\n$ find ./node_modules -mtime +10950\n./node_modules/hosted-git-info/CHANGELOG.md\n./node_modules/hosted-git-info/git-host-info.js\n./node_modules/hosted-git-info/git-host.js\n./node_modules/hosted-git-info/index.js\n./node_modules/hosted-git-info/LICENSE\n./node_modules/hosted-git-info/README.md\n./node_modules/lru-cache/index.js\n./node_modules/lru-cache/LICENSE\n./node_modules/lru-cache/README.md\n./node_modules/regexpp/index.d.ts\n./node_modules/regexpp/index.js\n./node_modules/regexpp/index.js.map\n./node_modules/regexpp/LICENSE\n./node_modules/regexpp/README.md. ",
    "JimTheMan": "wow, thanks @devTalha for starting this thread and everyone else for helping to solve it. I too just came in one morning to my projects that were using CodePipeline / CodeBuild and was in a panic because all of my projects were failing on the build stage with the exact same error (same one that op describes in his Logs but with my resource name), even ones whose source code I hadn't touched in months!\nAnyway, after adding the command from @TonyFNZ after the npm install command my buildspec.yml my build stages passed again, and it was smooth sailing.\n```\nversion: 0.2\nphases:\n  install:\n    commands:\n      # Install dependencies needed for running tests\n      - npm install\n  # Prevent files from having a timestamp before 1980\n  - find ./node_modules -mtime +10950 -exec touch {} \\;\n\n  # Upgrade AWS CLI to the latest version\n  - pip install --upgrade awscli\n\npre_build:\n    commands:\n      # Discover and run unit tests in the 'tests' directory\n      - npm test\n  build:\n    commands:\n      # Use AWS SAM to package the application using AWS CloudFormation\n      - aws cloudformation package --template template.yml --s3-bucket $S3_BUCKET --output-template template-export.yml\nartifacts:\n  type: zip\n  files:\n    - template-export.yml\n```\nCase closed.\n. ",
    "amirws": "The problem is the version project and the version cloudformation nodejs.. ",
    "bencmbrook": "@AllanFly120 the latest, assuming this works:\njs\nconst acm = new AWS.ACM({ apiVersion: 'latest' });. @AllanFly120 the latest, assuming this works:\njs\nconst acm = new AWS.ACM({ apiVersion: 'latest' });. Got it. NPM package was in fact out of date. Thanks. Got it. NPM package was in fact out of date. Thanks. ",
    "emainier": "I have the same issue on a table, it always returns a LastEvaluatedKey outside of the range of the query:\n{ Items: [],\n  Count: 0,\n  ScannedCount: 7880,\n    LastEvaluatedKey:\n     { id: '0d12c551-1942-11e8-957b-b5f8e93ff7ae',\n       date: 1511578408599 } } { TableName: 'log-db-dev',\n    IndexName: 'date-index',\n    ProjectionExpression: '#date, email',\n    FilterExpression: '#date between :startTime and :endTime',\n    ExpressionAttributeNames: { '#date': 'date' },\n    ExpressionAttributeValues: { ':startTime': 1523102683059, ':endTime': 1523189083058 } }\n\nThis issue appeared suddenly a few days ago.. I have the same issue on a table, it always returns a LastEvaluatedKey outside of the range of the query:\n{ Items: [],\n  Count: 0,\n  ScannedCount: 7880,\n    LastEvaluatedKey:\n     { id: '0d12c551-1942-11e8-957b-b5f8e93ff7ae',\n       date: 1511578408599 } } { TableName: 'log-db-dev',\n    IndexName: 'date-index',\n    ProjectionExpression: '#date, email',\n    FilterExpression: '#date between :startTime and :endTime',\n    ExpressionAttributeNames: { '#date': 'date' },\n    ExpressionAttributeValues: { ':startTime': 1523102683059, ':endTime': 1523189083058 } }\n\nThis issue appeared suddenly a few days ago.. @CKitisak \nNot really solved but I got rid of it by doing a similar request and made sure it wasn't interrupted.\nIt seems to come back every time a request is interrupted and keeps the value of the LastEvaluatedKey.\n. @CKitisak \nNot really solved but I got rid of it by doing a similar request and made sure it wasn't interrupted.\nIt seems to come back every time a request is interrupted and keeps the value of the LastEvaluatedKey.\n. ",
    "CKitisak": "I have this issue as well. How you guy solve it ?\nMy aws-sdk is 2.245.1.. ",
    "rlisnoff": "We're having the same issue on our end, when using DynamoDB Local our queries return with a LastEvaluatedKey when the Limit passed in happened to equal the exact amount of entries that matched the query. For example, I query DDB local with a limit of 1, and I happen to know that query only matches a single entry, yet the query returns with a non-empty LastEvaluatedKey, which signals that there's more data left to be retrieved even though I know there is not.\nUsing the same exact setup & code, when run against AWS-hosted DynamoDB we get an empty object as we expect via the documentation. The issue here seems to be an inconsistency between the local program & your hosted solution.. ",
    "praveshkhatana": "@RaphaelRosa did you find any solution?\n. @chrisradek is there any workaround for this. \ni am getting an error at this line after using a webpack. https://github.com/aws/aws-sdk-js/blob/c4452adb0fe7cbabfec1379da17d1a316408ff6b/lib/util.js#L704\n. ",
    "mazerab": "I worked around this issue by reverting my code back to a S3 presigned PUT URL and using XmlHttpRequest to upload the file.. Please close and thank you for following up.. ",
    "nickmacia": "You have a syntactical error.\nform.append('Content-Type': 'image/jpeg')\nought to be\nform.append('Content-Type', 'image/jpeg'). ",
    "gischer": "@AllanFly120 Thank you for your interest.  I have resolved this issue. Ultimately what happened is I was working with two different access keys and got them mixed up:  access_key_id and secret_access_key from the other.  Which quite clearly should lead to the error shown.\nThis happened despite my checking that it wasn't happening.  So I was apparently experiencing an hallucination.  At least it was temporary.\nI suspect that the issue that started me down this path was that I was trying to set an ACL with the call to putObject, but the policy didn't grant permission to set ACLs.\nIt's all working now.. @AllanFly120 Thank you for your interest.  I have resolved this issue. Ultimately what happened is I was working with two different access keys and got them mixed up:  access_key_id and secret_access_key from the other.  Which quite clearly should lead to the error shown.\nThis happened despite my checking that it wasn't happening.  So I was apparently experiencing an hallucination.  At least it was temporary.\nI suspect that the issue that started me down this path was that I was trying to set an ACL with the call to putObject, but the policy didn't grant permission to set ACLs.\nIt's all working now.. ",
    "paolavness": "@leonetosoft I have found on slow internet connections mulitpart uploads fail unless limited to 1 part...  Using Chrome's network throttling, on Slow 3G connections I limit the simultaneous parts to 1. \nIn my code I detect upload speed and then adjust the number of parts according to the user's speed. Where the speed is super slow, only one part at a time.\nAnother issue you may encounter - If the upload lasts longer than an hour you will need to refresh credentials.. @leonetosoft I have not looked into gzipping the upload yet but will be. I'd love to hear if you find anything about this. . ",
    "lajpatshah": "Hi @AllanFly120 \nThanks for guidance.\nI tried again my original code & now I am receiving the attributes. I did not changed my code yet why I couldn't receive the attributes earlier is not clear to me.\nOne thing is I had created Queue on the day of testing the code itself. Could be that attributes are not given immediately? Just wondering.\nThanks. ",
    "xinghul": "@saachinsiva This is not related to aws-sdk, you need to mark authenticateUser as async:\njavascript\npublic async authenticateUser(user:User): User {\n  ...\n}. ",
    "kennu": "Thanks @chrisradek, I would not have guessed to use AWS_SDK_LOAD_CONFIG. It's nice to know this now, but I think it's also something AWS could make a little clearer in documentation. Especially since people might create their ~/.aws/ files based on AWS CLI documentation and just expect them to work with AWS SDK later on.. ",
    "fawad1985": "@kennu @chrisradek I tried multiple times with those types of accounts and it didn't work. So I presume that Serverless doesn't work with assume roles especially with MFA.\nA quick workaround is to use assumed roles https://github.com/remind101/assume-role. I was then able to use sls commands normally.\n. ",
    "mdaum": "I tried to create a simple reproduction of this issue without actually running out of lambda-local/SAM-Local or actually hitting S3. I transcribed managedUpload's fillStream routine but no dice in reproducing on box. It seems to only arise during a synchronous uploading managedUpload run, something I was not able to effectively simulate.. ",
    "murphman300": "Nevermind, second error was another package. Thanks for the quick fix on the first issue!. Apologies for jumping back in, the fix only seems to work if i npm uninstall/install manually post fetch from my app's repo. If you notice in the first copy-paste in the code in my post, i call npm install on npm start (this is an auto-deploy app on ec2 from a custom ami).\nThis is first for the first launch start, however, my git hook which re-bases the app's directory on repo updates seems to reset the uninstall/install fix, meaning i still have to manually uninstall/install the aws-sdk package each time. The error i get is still the same, any clue as to why this might be happening? It wasn't doing this yesterday\nEdit: \naws-sdk version: 2.221.1\n. It seems that the file ./connect in the clients directory isn't present when running npm install inside a clean directory (before uninstall/install or running npm install through npm start), and after the previous approaches\n. It looks like i had corrupted git history on github and my app's clone action would fetch that version of my node_modules folder. Seems to have fixed it! Thanks for the patience. ",
    "mt-ronkorving": "@AllanFly120 Thanks for the feedback. Do you know by any chance if there's a machine-parseable file with the information from that table?. @AllanFly120 Sure \ud83d\udc4d \nGiven that you closed this issue, does that mean the documentation issue has been resolved?. ",
    "JorisAndrade": "Definitely interested by this feature too ! :) . ",
    "imjustd": "Is this done? I see in the changelog https://github.com/aws/aws-sdk-js/blob/master/CHANGELOG.md#22341 this feature: S3: Added BytesReturned details for Progress and Stats Events for Amazon S3 Select. ",
    "Rinoir": "Is there any information, when this feature will be implemented?. ",
    "joshrussell": "+1. I have a use case for S3 Select now. Is this released yet?. Thanks for the update @chrisradek. Any thoughts on when the final release will happen?. ",
    "kymc": "\nI can respond back here once the branch is useable so you can try it out before its final release.\n\n@chrisradek is anything being built regarding response stream parsing usable yet? \nLooking for some tooling here to parse a response stream returned by the HTTP API (https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectSELECTContent.html) with nodejs before s3 select is officially supported by the SDK. ",
    "alexvpickering": "This doesn't seem to be available on lambda environment version yet (listed as AWS SDK \u2013 AWS SDK for JavaScript version 2.233.1).. This doesn't seem to be available on lambda environment version yet (listed as AWS SDK \u2013 AWS SDK for JavaScript version 2.233.1).. ",
    "jewelsjacobs": "Being able to get this supported in lambda would be great.. ",
    "willvincent": "@chrisradek \n\nPlease let us know if something isn't clear in the docs. This is a new pattern where the JSON output contains a stream that emits the events from S3.\n\nBasically nothing is clear. your examples are fairly worthless, most of the available (even required) properties are so poorly documented that one must guess about what they mean. This is a problem that plagues aws documentation across the board.\nYou have good tools, but the instructions on their use tend toward being so obtuse, incomplete and uninformative that it's hard to get excited about using your tools.\nPlease run this up the flagpole to whomever needs to see it and make generation of GOOD documentation a priority.. ",
    "robermar2": "I'd like to second the lack of documentation on this.  \nThe examples in the documentation don't work.\nNor is their a clear explanation on how to actually get this new pattern working with async/promises, if it even can.. I'd like to second the lack of documentation on this.  \nThe examples in the documentation don't work.\nNor is their a clear explanation on how to actually get this new pattern working with async/promises, if it even can.. @srchase I was using lambda with nodejs 8.10 runtime.\nSo I was using the EventStream example and was confused on the async logic.  I needed to wrap the selectObjectContent call (and thus also handling of the event stream) in a promise as my call was in an async function.. @srchase I was using lambda with nodejs 8.10 runtime.\nSo I was using the EventStream example and was confused on the async logic.  I needed to wrap the selectObjectContent call (and thus also handling of the event stream) in a promise as my call was in an async function.. @srchase It was a long day of troubleshooting. I copied in the wrong attempt.\nThe EventStream example did work once I wrapped it all in a promise as I need to work with async functions.\n. @srchase It was a long day of troubleshooting. I copied in the wrong attempt.\nThe EventStream example did work once I wrapped it all in a promise as I need to work with async functions.\n. ",
    "garygrubb": "Hi @AllanFly120 I was removing 'X-Amz-User-Agent' as I was getting a CORS error. I managed to get around that by adding it to the allowed headers in the OPTIONS method. Now the problem is chrome is not allowing me to set the 'host' header. It gives error 'Refused to set unsafe header \"host\"'. API gateway requests are not working without the host header.. I was not able to get AWS.Signers.V4 to work. From what I read, this API is not public yet.\nI was able to get my APP working using a custom client to sign the request. It is available for download Here. ",
    "cekvenich": "I tried a browser builder https://sdk.amazonaws.com/builder/js/\nand selected only S3.\n384 kB. \n. Things that run in browser | mobile browser need to be managed.  . ",
    "taju20910": "It was my network issue. \nApologies for late updation.. ",
    "yuyuma": "Thanks for your response.  I kept digging and turns out the issue I encountered was not due to the trigger association but due to the fact that lambda needed additional privileges to be invoked by Cognito.  https://forums.aws.amazon.com/thread.jspa?messageID=748566 has more details in case anyone else needs it.. Thanks for your response.  I kept digging and turns out the issue I encountered was not due to the trigger association but due to the fact that lambda needed additional privileges to be invoked by Cognito.  https://forums.aws.amazon.com/thread.jspa?messageID=748566 has more details in case anyone else needs it.. ",
    "jaggu07": "Hi @AllanFly120 \nThanks for your response... It worked for me. ",
    "davidgatti": "Hi @chrisradek,\nAbout your question:\n\nafter 5 min I decided to stop \nI did try your suggestion and it worked, and it is actually what I was looking for.\n\nAdditional question to you:\n\nHow come createReadStream() information is missing from the documentation? At least I did not see any indication that I can create a Read Stream. After reading the documentation I assumed that you will call the callback at every frame. \nEventually I'm going to stream the video to the browser but just for now, how can I save the video to disk? If I make a file called movie.mkv I just see the initial key frame, and done. Despite the file being 100mb. This project is long gone. I don't even have access to that code anymore.. \n",
    "mim-Armand": "Not using any library for that, I simply add ( the undocumented ) .promise() at the end of the method call, as it does promisify most of the other methods I've worked with, it does with this one too, the only difference is that it returns undefined when the promise is resolved.\nSo sts.assumeRole(params).promise() returns the correct response for you? If so, please let me know and I'll do more investigation to find out what was the cause of the issue. thanks. the printout was undefined when stringified, but I think found the problem, adding .promise() doesn't return a normal promise, in contrary to the promises standard api which can have more than one .then() and executes them one after the other, AWS Node SDK, executes the second then() as well but it doesn't pass the resolved value to thens after the first one.\nI had one then() in my class and one in my test, I was logging the results in my test which was printing undefined. however, logging the results in the class itself works, and if I remove the then() from the class, the test will start working.\nSo I will close this issue as it's kind of solved, I appreciate your help @AllanFly120 . ",
    "aaronpeterson": "I didn't know promise() was required.  Thank you.. ",
    "aviggiano": "@chrisradek You are right, I was looking at the wrong documentation.\nFrom the stackoverflow question and AWS forum, it seems this is a common mistake :thinking: \nI'll close the issue. Thanks!. ",
    "yohei1126": "It works with boto3.\nimport boto3\nclient = boto3.client('s3')\n3_object_exists_waiter = s3.get_waiter('object_exists')\nobject_exists_waiter.wait(Bucket='rfid-test', Key='index.js'). Java SDK seems to be OK\n```\nimport com.amazonaws.services.s3.AmazonS3ClientBuilder;\nimport com.amazonaws.services.s3.model.GetObjectMetadataRequest;\nimport com.amazonaws.waiters.Waiter;\nimport com.amazonaws.waiters.WaiterParameters;\npublic class Main {\n    public static void main(String[] args) {\n        Waiter waiter = AmazonS3ClientBuilder.defaultClient().waiters().objectExists();\n        GetObjectMetadataRequest request = new GetObjectMetadataRequest(\"rfid-test\", \"index.js\");\n        waiter.run(new WaiterParameters<>(request));\n    }\n}\n. This issue can be fixed by specifying region.\n    const s3 = new S3({apiVersion: \"2006-03-01\", region: \"ap-northeast-1\"});\n```. ",
    "firohuber": "ReadStream\u00a0{_readableState: ReadableState, readable: true, _events: {\u2026}, _eventsCount: 1, _maxListeners: undefined,\u00a0\u2026}\nThat's what I get when I print it. I went to this section of the aws\n```\nif (AWS.util.isNode()) { // special check for buffer/stream in Node.js\n      var Stream = AWS.util.stream.Stream;\n      if (AWS.util.Buffer.isBuffer(value) || value instanceof Stream) return;\n}\nvar types = ['Buffer', 'Stream', 'File', 'Blob', 'ArrayBuffer', 'DataView'];\n  if (value) {\n    for (var i = 0; i < types.length; i++) {\n      if (AWS.util.isType(value, types[i])) return;\n      if (AWS.util.typeName(value.constructor) === types[i]) return;\n    }\n  }\n``\nBecause I'm working in nwjs it return false onif (AWS.util.isNode())` and the typeof fileData is returning readstream. I am using a bundler and when we excluded it out of the bundle it does work well but we will prefer to bundle it all together is there a way to solve it?. ",
    "daryllstrauss": "I'm having the same problem with an electron app, where I have node enabled in the worker. The check is deciding that we're isBrowser instead of isNode. We're actually both!\nI tried to force it as @AllanFly120 suggested, but that changes the error:\ncore.js:1449 ERROR TypeError: Cannot read property 'Stream' of undefined\n    at ManagedUpload.send (managed_upload.js:179)\n    at features.constructor.upload (s3.js:1067)\n    at Village.performUpload (village.ts:82)\n    at SafeSubscriber.eval [as _next] (worker.component.ts:117)\n    at SafeSubscriber.__tryOrUnsub (Subscriber.js:243)\n    at SafeSubscriber.next (Subscriber.js:190)\n    at Subscriber._next (Subscriber.js:131)\n    at Subscriber.next (Subscriber.js:95)\n    at TakeSubscriber._next (take.js:83)\n    at TakeSubscriber.Subscriber.next (Subscriber.js:95). \nI was doing that originally. I was including the code in the main thread, and then using remote to send it messages.\nThat sort of worked. It definitely identified the ReadStream correctly, but I wasn't getting all the callbacks I should from the SDK. I'd call it periodically and I get them in batches rather than having each one run to completion before the next call. It was weird enough I had no idea how to diagnose it.\nThey seem to discourage doing much in the main thread. Having that thread be fast is critical to a responsive UI. The recommended way to do multithreading is to launch a new window without showing it and run the code there. \nForcing the AWS SDK to node mode seemed like a good solution. I get the rest of my web environment for other backend HTTP commands, Observables, etc that I'd have to replicate. I do a parallel multipart get using a node module and it works great. It's just the upload that's giving me trouble.\n. \nI went down that path as well, using webpack in fact. The problem is that these are really large files (multi GB). I don't know how to get a stream without using fs. As far as I can tell XMLHTTPRequest only hands me the complete file in memory. That's also why I'm doing all the multipart stuff. \n. My work around was to create a class that duck-type's the file object the uploader wanted for it's web interface. It really only needed slice() and size in the end.\nI'd still like to see the native node interface work. The Slice problem is that once we fooled it to know it was in node, the node_loader.js was doing require(...) not window.require(...) which is what electron wants. \nThe detector should be smarter (understand node/electron/etc) and/or allow you to specify the environment.. ",
    "elgalu": "Awesome! thanks. Awesome! thanks. ",
    "ArjunMani": "Thanks for the response.\nThen what is the best way to upload multiple files inside a directory?\nThanks in advance.. Ohh Greatt!!!\nSame way is there any to download a directory from browser??\nThanks in advance.. ",
    "dougwit": "@AllanFly120 yes. All I'm doing with the response object is printing it.. @srchase\nHey guys. My apologies for the late response. \nI just traced the call and found the offending line of code: https://github.com/aws/aws-sdk-js/blob/ef10482481ea827f88c4f3cd8311024fb75f32e6/lib/s3/managed_upload.js#L664\nI'm not sure why that's there, or why others aren't able to reproduce the issue.. ",
    "pgrzesik": "Hello @AllanFly120,\nI'm uploading from Node, what's more interesting, S3 Object in bucket has the correct Content-Type and Content-Encoding Metadata, however after downloading, it's the same as I showed in my first message. I tried downloading it via cURL, Node, from Browser - the same for all cases.. Hey @AllanFly120, sorry for delayed response. I tried getting it this way and while provided snipped doesn't yield any results, I managed to get them and headers look like in S3 buckets. . Thanks for your engagement @AllanFly120, it looks like it's not a matter of AWS SDK so I'm closing this issue.\nCheers. ",
    "Baraksi": "Let me elaborate.\nMy case is a file being uploaded to S3 bucket using a 3rd party service (probably using put object or multipart upload).\nThe uploaded file name was: \"VJ9B4951 - Copy - Copy (2) - Copy.jpg\" - which yield the same file name in the S3 bucket (as expected).\nIn order to get notified about the new item, I use a Lambda function with S3 trigger on ObjectCreated event.\nWhen the event occur, the key, as appear in the event data is \"VJ9B4951+-+Copy+-+Copy+%282%29+-+Copy.jpg\". Which is different than the file name as appear in the S3 bucket.\nI have no problem with the representation, but I would expect that using the same representation, given by the trigger data, for invoking SDK methods on the same key will run smoothly, But instead, using the same input key in order to getObject afterwards is raising the error of \"The specified key does not exist\".\nThat's why I called the behavior inconsistent.. ",
    "dennererthal01": "Thanks @AllanFly120, I'll close this issue as it's not related to the sdk itself.. ",
    "ArunPrasathSubramani": "Resolved the issue with a Lambda function in AWS, calling the same through AWS- API Gateway.. ",
    "ashemedai": "@chrisradek No worries! Thanks for merging.. ",
    "nakulp007": "Having same error. I have working project. However, if I delete the node_modules folder and run npm install again. After that launching the application, I receive the mentioned error. \nLaunching using backup version of node_module folder still works. But transporting to different environment this way is not ideal. \nEdit:\nFor me issue resolved by adding \"amazon-cognito-identity-js\": \"2.0.3\" dependency in package.json file.. @chrisradek my node_modules had amazon-cognito-identity-js version 2.0.1, however it wasn't explicitly mentioned in our package.json in our app backup copy. \nWe resolved the problem by adding amazon-cognito-identity-js\": \"2.0.3\" to package.json.. @klaytaybai thanks for your response. I was able to reproduce it at will the day I posted the question. But for some reason, it is working as expected now. No change to deployed function. Maybe some temporary problem on the database side. Thanks.\nThis can be closed.. @klaytaybai thanks for your response. I was able to reproduce it at will the day I posted the question. But for some reason, it is working as expected now. No change to deployed function. Maybe some temporary problem on the database side. Thanks.\nThis can be closed.. ",
    "keithdarragh": "@chrisradek \nThis is how we call the compareFaces operation: \nconst request = {\n      SimilarityThreshold: 50,\n      SourceImage: {\n        Bytes: getBinary(body.sourceImage),\n      },\n      TargetImage: {\n        Bytes: getBinary(body.targetImage),\n      },\n};\nconst response = await rekognition.compareFaces(request).promise();\nWhere body.sourceImage is a base64 encoded string. It is sent as base64-encoded image bytes.\nAny ideas? . We solved the issue. Our image was getting captured with incorrect orientation values. Stripping the image exif data solved the problem. . ",
    "harishankards": "Hi @AllanFly120, \nHave you had a chance to look into this? . Hey @chrisradek ,\nForgot to mention that I got it fixed by using the same method which you referenced above.\nThanks. ",
    "Luckstai": "Thank you very much, but I managed to solve the problem yes, see you later. ",
    "myst729": "Weird. I don't understand why the CI fails. The PR is branched from the master HEAD. \ud83d\ude23 . @chrisradek \nThe script is maintained by my colleague. It wraps XMLHttpRequest. Later then I've found the root cause and sent a PR to fix it (already merged).\n. ",
    "SamWSoftware": "This code works fine when you change to node 6.10 using callbacks. I had to promisify the .getObject to make it work with async . ",
    "Vadorequest": "I have a similar error, migrating an AWS Lambda (that is runnin Next.js powered website) from 6 to 8, and it's failing with the same error and nothing else in the logs. No way to know what's causing this.\n```\n2018-06-19 20:01:50.691 (+02:00)        d6dadfff-73ea-11e8-8235-83b997ef5a85    2018-06-19 18:01:50 req from http://staging.hep.loan-advisor.studylink.fr/\nUnable to stringify response body as json: Converting circular structure to JSON: TypeError\nUnable to stringify response body as json: Converting circular structure to JSON: TypeError\n2018-06-19 20:01:50.912 (+02:00)        d7041355-73ea-11e8-804b-f5627adbf026    2018-06-19 18:01:50 req from http://staging.hep.loan-advisor.studylink.fr/\n2018-06-19 20:01:50.952 (+02:00)        d70c292b-73ea-11e8-860b-e33ea957669f    2018-06-19 18:01:50 req from http://staging.hep.loan-advisor.studylink.fr/\nUnable to stringify response body as json: Converting circular structure to JSON: TypeError\n2018-06-19 20:01:51.232 (+02:00)        d73e0f5e-73ea-11e8-8a97-49f136cb780a    2018-06-19 18:01:51 req from http://staging.hep.loan-advisor.studylink.fr/\nUnable to stringify response body as json: Converting circular structure to JSON: TypeError\n2018-06-19 20:01:51.150 (+02:00)        d7400ac6-73ea-11e8-b19b-8dd89a20a874    2018-06-19 18:01:51 req from http://staging.hep.loan-advisor.studylink.fr/\nEND RequestId: d45bd68f-73ea-11e8-96ae-97c810255855\nREPORT RequestId: d45bd68f-73ea-11e8-96ae-97c810255855  Duration: 6003.99 ms    Billed Duration: 6000 ms        Memory Size: 128 MB     Max Memory Used: 48 MB  \n2018-06-19T18:01:51.757Z d45bd68f-73ea-11e8-96ae-97c810255855 Task timed out after 6.00 seconds\nTypeError\nEND RequestId: d4992e1a-73ea-11e8-80ae-bfc369787d66\nREPORT RequestId: d4992e1a-73ea-11e8-80ae-bfc369787d66  Duration: 6005.77 ms    Billed Duration: 6000 ms        Memory Size: 128 MB     Max Memory Used: 43 MB  \n2018-06-19T18:01:52.130Z d4992e1a-73ea-11e8-80ae-bfc369787d66 Task timed out after 6.01 seconds\nTypeError\nSTART RequestId: d87a3803-73ea-11e8-b8dd-d3ff652bc1cc Version: $LATEST\nUnable to stringify response body as json: Converting circular structure to JSON: TypeError\n2018-06-19 20:01:53.033 (+02:00)        d87a3803-73ea-11e8-b8dd-d3ff652bc1cc    2018-06-19 18:01:53 req from http://staging.hep.loan-advisor.studylink.fr/\nSTART RequestId: d9622768-73ea-11e8-99f4-4d4070fb60ab Version: $LATEST\nUnable to stringify response body as json: Converting circular structure to JSON: TypeError\n2018-06-19 20:01:54.736 (+02:00)        d9622768-73ea-11e8-99f4-4d4070fb60ab    2018-06-19 18:01:54 req from http://staging.hep.loan-advisor.studylink.fr/\nEND RequestId: d635515f-73ea-11e8-931c-792352da3d2a\nREPORT RequestId: d635515f-73ea-11e8-931c-792352da3d2a  Duration: 6003.08 ms    Billed Duration: 6000 ms        Memory Size: 128 MB     Max Memory Used: 48 MB  \n2018-06-19T18:01:54.829Z d635515f-73ea-11e8-931c-792352da3d2a Task timed out after 6.00 seconds\nTypeError\nSTART RequestId: da461f49-73ea-11e8-931c-792352da3d2a Version: $LATEST\nEND RequestId: d6dadfff-73ea-11e8-8235-83b997ef5a85\nREPORT RequestId: d6dadfff-73ea-11e8-8235-83b997ef5a85  Duration: 6006.63 ms    Billed Duration: 6000 ms        Memory Size: 128 MB     Max Memory Used: 41 MB  \n2018-06-19T18:01:55.918Z d6dadfff-73ea-11e8-8235-83b997ef5a85 Task timed out after 6.01 seconds\nTypeError\nUnable to stringify response body as json: Converting circular structure to JSON: TypeError\n```\nLogs are a mess, I took the first and last reference of d6dadfff-73ea-11e8-8235-83b997ef5a85 and everything in between.\nWorks fine when deploying using node6. If you have any idea how to get better logs it'd be great, just saying! :). What I'm calling is full-featured website powered by https://github.com/zeit/next.js, therefore I can't really share any relevant code, it's way too complex.\nAlso, as far as I know, it's not using aws-sdk, but since it's a framework I cannot be sure either. (there is no db connection/fetching for sure, but the sdk could be used to do other things)\nI mean, the issue could be related to aws-sdk, or could not be, I don't know. And the lack of stacktrace sure doesn't help.\nHere is something that looks like what I've got, it was a POC of serverless + next.js frameworks, to power next.js on AWS lambda: https://github.com/Vadorequest/serverless-with-next. ",
    "taylor-zr": "Internal customer here. The code doesn't have to be complex to reproduce:\n```\nvar aws = require('aws-sdk')\nvar s3 = new aws.S3();\nvar params = {Bucket: 'dms-taylorzr', Key: 'testlocal'};\nexports.handler = async (event) => {\n    const response = await s3.getObject(params, function(err, data){\n        if(err) {\n            console.log(err)\n        } else {\n            console.log(data);\n        }\n    });\nreturn response.Body.toString();\n\n};\n```\nHaven't tested it locally, but I am getting this occurring 100% of the time from a Lambda invocation.. Thanks! Still very new to node, I appreciate the help.. ",
    "xtianus79": "@AllanFly120 yes it wa a problem with the headers.  The issue is and is probably causing some poor soul's frustration regarding a code base out there that is creating a signature in the manner we are for an S3 bucket... Along with the fact that IOS has now updated their internal browser to not accept Date as an awsheader or aparently a signature authorization we had been using. . any help with this?. ",
    "g-cassie": "Hi @AllanFly120,\nThanks, this kind of addresses the issue. Basically I now need to do this:\nAWS.config.credentials = new AWS.TemporaryCredentials({\n   RoleArn: 'arn:aws:iam::<my-account>:role/my-role'\n  }, new AWS.EC2MetadataCredentials());\nThis means I have to patch the third party library I'm working with because it assumes you can do all the config in ~/.aws/credentials.  Not really ideal but will work for now.\nI think the SDK should have parity with the cli so that if you can perform an action in the cli with a given a profile, it will also work in the SDK.  At a minimum the SDK should throw an error notifying the user that the credentials_source property is not supported as that took a considerable amount of time to get to the bottom of.. ",
    "angrychewie": "A string array of Developer Provider Names is returned. For instance, if angrychewie authenticates via a Federated Identity Pool (not User Pool), that accepted authentication request (with the developer identifier of 'angrychewie') gets 'linked' to the corresponding custom developer provider name that was specified within the request. So in the Cognito console, if you check the identity browser for angrychewie, you'll see 'angrychewie' as the Developer Identifier as well as a field called Linked Login which is just the Developer Provider Name they made the request to (say, login.company.product).\nCalling describeIdentity() then just returns a string array of the Developer Provider Names, which in all likelihood could just be 1 thing (login.company.product) such as in my case. \nThis is what the documentation states regarding the part of the Data returned:\nLogins \u2014 (Array)\nA set of optional name-value pairs that map provider names to provider tokens\nRe-reading this in hindsight, I suppose the method is 'working as intended'.\nIn the end, though, what really matters to me is having a way to get that Developer Identifier/username back out since my goal is to send it along in the context to other services who will be able to make use of the username, but have no clue about random cognito identities. You need to provide a required logins mapping to make the initial getOpenIdTokenForDeveloperIdentity() call (developer provider name -> developer identifier), so why is there not a way to grab it back out? You can search the Identity Browser in the console using a Developer Identifier, too, and the identifier is clearly shown within a specific identity's details so it's a significant field that's stored somewhere.\nI just see no clear way of accessing it and the desribeIdentity() method seems like the most ideal place to have that information be returned (especially when it talks about pairing and mapping). Instead of returning just half of the original map that I initially give it in getOpenIdTokenForDeveloperIdentity() as a string array, I would really like it to return the entire mapping of developer provider name -> developer identifier pairs. \nTLDR: DeveloperIdentifier seems to have been forgotten about even though it's a part of the required mapping parameter in getOpenIdTokenForDeveloperIdentity() and it's a field I could really really use. describeIdentity() already returns half of that original mapping, so why not have it return the useful other half, too?. ",
    "filipre": "For anyone having the same issue: It seems like Istio 0.8 introduced a new resource type ServiceEntry that can enable https for defined hostnames.. ",
    "nicolaerusan": "I should add that when I use the CLI command to access the same log group with this command I do get back the correct list of logs:\naws logs filter-log-events --log-group-name {logGroupName}. @AllanFly120 Thanks for the response. From the testing we've done, we've always seen consistent logs being returned from the CLI. We've tested it on a few different machines and then used the API call and did not receive the same list of logs back. @AllanFly120 We're still hitting this issue. Anything you suggest we can try to do to resolve this? Happy to hop on a quick call to walk someone through the issue and see if we can get to a resolution.. @chrisradek @AdityaManohar any thoughts on what might be going on here? appreciate any help you can give / alternative resolutions. We may drop the SDK and try to find some other solution if we can't consistently get the latest logs from that call :/. if there's an API we can hit directly instead of using the SDK, we could try that out. @srchase we ended up finding a workaround. We call:\ncloudWatchLogs.describeLogStreams with orderBy LastEventTime & then do the filterLogEvents call based on that. Seems to resolve it.. ",
    "decompil3d": "@chrisradek Thank you, that did the trick. I do wish that this had been made more clear in the assumeRole SDK docs though.. ",
    "jjosef": "Wow, thank you @chrisradek that fixed it. I thought I had tried this, but apparently I had configured it wrong.\njs\nstream = await sftp.get(`${config.sftp.environment}/${file.name}`, false, null). Wow, thank you @chrisradek that fixed it. I thought I had tried this, but apparently I had configured it wrong.\njs\nstream = await sftp.get(`${config.sftp.environment}/${file.name}`, false, null). ",
    "nitrocode": "It looks like it would work because then it would load it in chunks. I did get that to work before leaving the project however I still had memory issues when trying to initiate the upload even though it split the file into chunks and utilized etag automatically.... @chrisradek sorry, I mistyped. I was using s3.upload with a queueSize: 10 and partSize of 10 MB or partSize: 10 * 1024 * 1024. I updated https://github.com/aws/aws-sdk-js/issues/2083.. I don't believe so. Changed companies and haven't used this since. ",
    "yousharizvi": "Any catch on this issue? I'm having exactly same scenario and facing the same issue, that is on some streams I receive timeout and eventually the download breaks.\n. I found a workaround by instantiating a new instance of s3 for every read request i.e., avoiding singleton for s3 instance. Basically it was choking the s3 instance on my end. Try doing aws.s3() before every getObject() call i.e., aws.s3().getObject(params).createReadStream(). Hope that'll help.. @srchase , any feedback on the PR?. ",
    "geekguy": "Facing a similar problem. Is there any update on this?. ",
    "JarrodAMcEvers": "+1. ",
    "JeremiasEh": "+1. Didn't worked for me\n\nI found a workaround by instantiating a new instance of s3 for every read request i.e., avoiding singleton for s3 instance. Basically it was choking the s3 instance on my end. Try doing aws.s3() before every getObject() call i.e., aws.s3().getObject(params).createReadStream(). Hope that'll help.\n\nDidn't worked for me. The same timeout error appears :/. ",
    "kononenko-a-m": "@AllanFly120 does someone really works on it?. ",
    "ptcc": "I have the same issue: timeout after 120s even if the stream finished successfully. @jeffbski this is not about the timeout being insufficient. I get this error even when the request is quickly handled. ",
    "jeffbski": "I believe this is the httpTimeout which defaults to 120000ms\nIncrease the httpTimeout by passing in options to whatever AWS API you are using, so if using S3\njs\nconst awsOptions = {\n  region: 'us-east-1',\n  httpOptions: {\n    timeout: 900000 // 15 minutes\n  }\n};\nconst s3 = new AWS.S3(awsOptions);\nUse whatever timeout makes sense for your situation.. Also I found that if it is timing out for your from invoking it via cli locally, you should set the env variable AWS_CLIENT_TIMEOUT to override the default 120000ms timeout it uses.\nbash\nexport AWS_CLIENT_TIMEOUT=900000\nsls invoke -f helllo\n. Well different issue than what I was experiencing then. Mine went away with\nthese changes.\nOn Tue, Feb 26, 2019 at 6:11 AM Tiago Coelho notifications@github.com\nwrote:\n\n@jeffbski https://github.com/jeffbski this is not about the timeout\nbeing insufficient. I get this error even when the request is quickly\nhandled\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/2087#issuecomment-467414603,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAWOSqLSyV_0E_UvpCv1TvyWZ75HmWuks5vRSRUgaJpZM4UmWi3\n.\n-- \nJeff Barczewski\nFounder of CodeWinds\nhttp://codewinds.com/\nLive and Online developer training\n. \n",
    "kristoft": "+1\nThe same for me. \nDoes anybody know if that\u2019s happening using some language, say java?. ",
    "alexandruluca": "Any follow up on this? I'm in the exact same situation as the op. @bebu259 Did you manage to find a solution to your problem?. ",
    "bebu259": "@alexandruluca I found a solution by creating the streams in a lazy way using PassThrough streams.\nThe idea is to create each single stream when a first listener registers on that stream. So in practice there are only a few streams existing at the same time, not hundreds of streams.\nThe code to solved my initial question looks like this:\n```JS\nimport { PassThrough } from 'stream';\n// ...\nconst getStream = (path) => {\n  let streamCreated = false;\nconst passThroughStream = new PassThrough();\npassThroughStream.on('newListener', event => {\n    if (!streamCreated && event === 'data') {\n      logEvents && console.log(\\n\u2b50  create stream for path ${path});\n  const s3Stream = Storage.getFileStream(path);  // <<== stream is created here\n\n  s3Stream\n    .on('error', err => passThroughStream.emit('error', err))\n    .on('finish', () => logEvents && console.log(`\u2705  finish stream for path ${path}`))\n    .on('close', () => logEvents && console.log(`\u274c  stream close\\n`))\n    .pipe(passThroughStream);\n\n  streamCreated = true;\n}\n\n});\n  return passThroughStream;\n};\n// ...\nfor (const entry of content) {\n  archive.append(getStream(entry.path), { name: entry.nameInArchive });\n}\n```\n. ",
    "christopher-dG": "Oh my, I thought I opened this in mapboxes' repo. A bit too late for a Monday mistake...\nThanks for the heads up, I'll go raise this over there!\n. ",
    "kadishmal": "From the XHR docs, it is saying that a request will be terminated in the specified timeout milliseconds.\nThe way I understand it is that even though the connection was successful and the request went through, and even the response is coming through but is taking longer than the timeout, the browser will kill the request. This is not what the SDK doc says:\n\ntimeout [Integer] \u2014 Sets the socket to timeout after timeout milliseconds of inactivity on the socket. Defaults to two minutes (120000).\n\nThe SDK doc mentioned that the timeout is for inactivity of the socket, and not about the request timeout. So, the SDK docs need to be fixed.\nI am also looking for a way to set the connection timeout for an XHR. Please let me know if anybody knows.. From the XHR docs, it is saying that a request will be terminated in the specified timeout milliseconds.\nThe way I understand it is that even though the connection was successful and the request went through, and even the response is coming through but is taking longer than the timeout, the browser will kill the request. This is not what the SDK doc says:\n\ntimeout [Integer] \u2014 Sets the socket to timeout after timeout milliseconds of inactivity on the socket. Defaults to two minutes (120000).\n\nThe SDK doc mentioned that the timeout is for inactivity of the socket, and not about the request timeout. So, the SDK docs need to be fixed.\nI am also looking for a way to set the connection timeout for an XHR. Please let me know if anybody knows.. ",
    "ada1013": "Thanks chrisradek,\nBut if I removed the attribute value S, the error message shows:\n\nUnable to scan the table. Error JSON:\n{\n    \"message\": \"There were 4 validation errors:\\n InvalidParameterType: Expected params.ExpressionAttributeValues[':id'] to be a structure\\n UnexpectedParameter: Unexpected key '0' found in params.ExpressionAttributeValues[':id']\\n UnexpectedParameter: Unexpected key '1' found in params.ExpressionAttributeValues[':id']\\n UnexpectedParameter: Unexpected key '2' found in params.ExpressionAttributeValues[':id']\",\n    \"code\": \"MultipleValidationErrors\",\n    \"time\": \"2018-06-20T07:33:47.185Z\",\n    \"errors\": [\n        {\n            \"message\": \"Expected params.ExpressionAttributeValues[':id'] to be a structure\",\n            \"code\": \"InvalidParameterType\",\n            \"time\": \"2018-06-20T07:33:47.184Z\"\n        },\n        {\n            \"message\": \"Unexpected key '0' found in params.ExpressionAttributeValues[':id']\",\n            \"code\": \"UnexpectedParameter\",\n            \"time\": \"2018-06-20T07:33:47.184Z\"\n        },\n        {\n            \"message\": \"Unexpected key '1' found in params.ExpressionAttributeValues[':id']\",\n            \"code\": \"UnexpectedParameter\",\n            \"time\": \"2018-06-20T07:33:47.184Z\"\n        },\n        {\n            \"message\": \"Unexpected key '2' found in params.ExpressionAttributeValues[':id']\",\n            \"code\": \"UnexpectedParameter\",\n            \"time\": \"2018-06-20T07:33:47.184Z\"\n        }\n    ],\n    \"originalError\": {\n        \"message\": \"shape.toType is not a function\",\n        \"code\": \"TypeError\",\n        \"time\": \"2018-06-20T07:33:47.124Z\"\n    }\n}\n\nHere is the stack trace content:\n\nError\nat Object. (/var/task/iot-intents.js:8:13)\nat Module._compile (module.js:570:32)\nat Object.Module._extensions..js (module.js:579:10)\nat Module.load (module.js:487:32)\nat tryModuleLoad (module.js:446:12)\nat Function.Module._load (module.js:438:3)\nat Module.require (module.js:497:17)\nat require (internal/module.js:20:19)\nat Object. (/var/task/alexa-skills-index.js:4:18)\nat Module._compile (module.js:570:32)\n\nThis issue really make me confuse, thanks~. ",
    "roccomuso": "Thanks for the reply.\nI'm trying to do it in JS with ffmpeg.\nThat cpp sdk and container are too big for my purpose.\nI was able to do it in JS, but now i'm facing a different issue.\n1) I'm not sure if the mkv im sending is considered valid since i can't see it on the kinesis-video video preview page. I'm getting the RECEIVED ack though.\nI'm using aws4 to generate my signatures.. I've used aws4 to generate the signature, and axios to put the video stream (from fluent-ffmpeg) to kinesis.. ",
    "cwardcode": "@AllanFly120 Cognito added the feature to their service a couple weeks ago, I was just wondering if the SDK support for the feature was on the aws-sdk-js roadmap. I thought that the SDK team would handle the integration, but if it's the service team I can definitely create a topic in the forum!  I just want to make sure I'm posting in the correct place.. ",
    "mcarriere": "@AllanFly120 in my case I am targeting es5 so this part of the documentation was useful :). ",
    "steveLuo1": "I am using  \"aws-sdk\": \"2.224.1\". Hi Chris,\nThanks a lot for quick and great reply!\nRegards,\nSteve\nOn Tue, Jun 19, 2018 at 4:29 PM, Christopher Radek <notifications@github.com\n\nwrote:\n@steveLuo1 https://github.com/steveLuo1\nThe S3 managed upload (s3.upload) requires a readable stream for the Body\nfield as input (when the input is a stream). Internally, the SDK will call\nread\nhttps://nodejs.org/dist/latest-v10.x/docs/api/stream.html#stream_readable_read_size\non the the stream that's passed in so it can control how quickly to read\ndata. It needs to do this so that it can control how many uploadPart\nrequests to make at a time (this enables controlling the number of parallel\nrequests to have at a time as well as retrying individual parts without\nstoring the entire object in memory).\nCalling request().get() doesn't provide a true readable stream. Instead,\nthe object returned inherits from the Nodejs Stream\nhttps://github.com/request/request/blob/536f0e76b249e4545c3ba2ac75e643146ebf3824/request.js#L130\nclass, and then implements methods like write\nhttps://github.com/request/request/blob/536f0e76b249e4545c3ba2ac75e643146ebf3824/request.js#L1487.\nWhile it supports the common flowing mode\nhttps://nodejs.org/dist/latest-v10.x/docs/api/stream.html#stream_two_modes\noperation with pipe and listening for data events, it does not support\npaused more with read() since read is not implemented by request.\nYou actually figured out a work-around by piping the stream from request\ninto a PassThrough stream. Since PassThrough streams extend Readable\nstreams, they have the read method defined and can operate fully in\npaused mode.\nHope my response wasn't too long-winded!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/2100#issuecomment-398534493,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AmhfGumdSU4L0MIYijGUwGVQnt5wKcChks5t-V8SgaJpZM4Uth4Z\n.\n. Hi Chris,\n\nDo you know any better ways,. or right ways to do this?\nThanks a lot.\nSteve\nOn Tue, Jun 19, 2018 at 5:18 PM, Steve Luo steve@havenlife.com wrote:\n\nHi Chris,\nThanks a lot for quick and great reply!\nRegards,\nSteve\nOn Tue, Jun 19, 2018 at 4:29 PM, Christopher Radek \nnotifications@github.com wrote:\n\n@steveLuo1 https://github.com/steveLuo1\nThe S3 managed upload (s3.upload) requires a readable stream for the Body\nfield as input (when the input is a stream). Internally, the SDK will call\nread\nhttps://nodejs.org/dist/latest-v10.x/docs/api/stream.html#stream_readable_read_size\non the the stream that's passed in so it can control how quickly to read\ndata. It needs to do this so that it can control how many uploadPart\nrequests to make at a time (this enables controlling the number of parallel\nrequests to have at a time as well as retrying individual parts without\nstoring the entire object in memory).\nCalling request().get() doesn't provide a true readable stream. Instead,\nthe object returned inherits from the Nodejs Stream\nhttps://github.com/request/request/blob/536f0e76b249e4545c3ba2ac75e643146ebf3824/request.js#L130\nclass, and then implements methods like write\nhttps://github.com/request/request/blob/536f0e76b249e4545c3ba2ac75e643146ebf3824/request.js#L1487.\nWhile it supports the common flowing mode\nhttps://nodejs.org/dist/latest-v10.x/docs/api/stream.html#stream_two_modes\noperation with pipe and listening for data events, it does not support\npaused more with read() since read is not implemented by request.\nYou actually figured out a work-around by piping the stream from request\ninto a PassThrough stream. Since PassThrough streams extend Readable\nstreams, they have the read method defined and can operate fully in\npaused mode.\nHope my response wasn't too long-winded!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/2100#issuecomment-398534493,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AmhfGumdSU4L0MIYijGUwGVQnt5wKcChks5t-V8SgaJpZM4Uth4Z\n.\n\n\n. \n",
    "serverlesspolska": "Oh My God!!! I thought I was going crazy... \nHow... why?.... I lost 3 hours on this. \nthanks @chrisradek for this information. ",
    "Nop0x": "@chrisradek Thanks for updating me on that issue!\nAnother question concerning this error: Is there any way to lower the required delay between requests? After I get the error and remove the Intent from my bot (via the API as well), I have to wait approx. 50+ seconds before I go ahead and retry the intent deletion.\nI would suggest removing or lowering the delay after a successful request.. ",
    "lucas-rudd": "Thank you. I'll check out the Lambda forum. Please update this if the service team responds. It would give us the ability to make our lambdas more generic to accept data from multiple places such as SNS or SQS, etc.. ",
    "GlauberF": "now gave the following error\n\nmy user created, direct by SMTP settings\nhttps://console.aws.amazon.com/ses/home?region=us-east-1#smtp-settings:. It worked. ",
    "equinox7": "Thank you for you response @AllanFly120\nNot resuming a stream on the SDK's side is fine for me as I can .resume() it myself. So I will get an end event eventually. Not sure though if the stream is even being paused in my case.. Hi @AllanFly120, is there any progress regarding the investigation?. ",
    "ttulka": "It works well when it's called asynchronously. Thank you very much!\nRegarding the synchronous variant, it'd be nice to get an error instead of the constant string https://s3.eu-central-1.amazonaws.com/. ",
    "ccorcos": "@chrisradek your answer was very helpful, but it took me a while to find it! It would be nice if the SDK just threw an error explaining this reasoning. We just moved over to using AWS Secrets Manager and it was a hard bug to find because it was so unreliable (only when a server starts up).. @chrisradek your answer was very helpful, but it took me a while to find it! It would be nice if the SDK just threw an error explaining this reasoning. We just moved over to using AWS Secrets Manager and it was a hard bug to find because it was so unreliable (only when a server starts up).. ",
    "korya": "Thanks!\nThe builder is great, but the generated minimized file containing only S3 functionality is still 250KB. So I will probably wait for the official solution and stick to EvaporateJS for now.. ",
    "lenin-jaganathan": "No the file size compresses by some 2-3%.I don't why it is compressing\nExample : If my original file was 89kb the file in s3 was only 86kb. data: file.\nHere file is an file standard file object. . ",
    "adewergifosse": "@AllanFly120  Ok thank you for your quick answer ! If you want, you can close the issue.. ",
    "vivex": "Never-mind, maybe its something not related to aws-sdk, got resolved by doing npm cache clean --force\n. ",
    "Karthi-SRV": "Hi, \nI am to getting the same error . I am using Angular 6.0.8, node 8.11.2 andaws-sdk 2.279.1 and problem still persists.. Hi, I have debugged the aws-sdk and came to solution that in angular 6 global is not declared by default, so to declared the global we used below code\n```\n  var global = global || window;\n  var Buffer = Buffer || [];\n  var process = process || {\n      env: { DEBUG: undefined },\n      version: []\n  };\n```\nAfter some research I found the key point where system goes wrong /node_module/aws-sdk/lib/utils.js\nisNode: function isNode() { return !util.isBrowser(); },\nisBrowser: function isBrowser() { return process && process.browser; },\nSo, in isBrowser function process.browser is undefined so it return false. \nBy default in browser_loader.js , if process is undefined they will set as browser true.\nif (typeof process === 'undefined') {\n  process = {\n    browser: true\n  };\n}\nSo, to declare some of global function which taken out in Angular 6, we should not declare process value by own. \nsimply isBrowser() function ... which doesn't come from Angular returns the unexpected value ... \"{\"env\":{},\"version\":[]}\" instead {\"title\":\"browser\",\"browser\":true,\"env\":{},\"argv\":[],\"version\":\"\",\"versions\":{}} so SDK can't correctly evaluate in which environment it's running. \"browser\":true is missing.. Hi carlara75 ,\nI am concluded that there is no issue in the AWS SDK in front-end, it may because of the global variable you have added for Angular 6. \n```\nvar global = global || window;\n  var Buffer = Buffer || [];\n  var process = process || {\n      env: { DEBUG: undefined },\n      version: []\n  };\n```\nThe issue is that we have declared process variable.. ",
    "DeveloperAsela": "I just edited, for testing purpose only, line 40 to\nisNode: function isNode() { return false },\ninstead\nisNode: function isNode() { return !util.isBrowser(); },\n@carlara75  Thank you.this is worked for me. hi carlara75,\n<script>\nvar global = global || window;\nvar Buffer = Buffer || [];\n</script>i change like this in index.html and it works.Thank you @carlara75.:-). ",
    "Bodeclas": "Hello @carlara75 I have a problem with this change. \nI'm using Algolia Search for Angular 6 and they say: \n\nIf you are using Angular v6 you will need an extra step, polyfill process.env by adding in your src/polyfill.ts:\n(window as any).process = {env: {}};\n\nI need add env.process to run my project with Algolia.\nWhat could I do to solve my problem?\nhttps://community.algolia.com/angular-instantsearch/guides/migration-guide.html. ",
    "jpb": "I'm not sure that the related changes have caused the test failure, but rather surfaced a broken test:\nhttps://github.com/aws/aws-sdk-js/blob/a6b3ff2cac81edc6fb479b37cecfa619ac273c68/test/credentials.spec.js#L450-L459\nI can't find where the error message Credentials not set in source_profile ... using profile ... would be generated.. Thanks @chrisradek! \nI dug a bit more while I was adding some additional tests and discovered that the error message generated (on master and this branch) didn't match what the test was expecting. It appears that the exception handling in SharedIniFileCredentials catches the exception thrown by assert, which masked the error. I'm still not 100% sure why the test behaves differently with these changes, but I have fixed the error message.. @chrisradek I believe I've addressed your comments (including adding the additional test case).. @chrisradek would you be able to review this PR (or be able to suggest another person)?. Thanks @AllanFly120 \u2013\u00a0I've rebased this branch with master.. These properties now get assigned in STSCredentialsCache via credentialsFrom.. ",
    "donglaizhang": "Thank you for the quick response.. ",
    "Sparticuz": "What was the problem/solution @kscherr?. ",
    "leonfs": "Yes, what was the solution @kscherr ? I'm seeing the same behaviour and it does not match the documentation. I would expect to see a null response instead of an {} (empty) object.. ",
    "ynnr85": "Same here. I need ResourceRecord in client. I am getting the same response. ResourceRecord is missing. Any idea? . That's correct Daniel, in an straight process a delay is necessary to get DNS configuration values.\nThanks. ",
    "danielbdias": "Hi, @AllanFly120 I'll try do this! For now, I discovered a workaround to my problem.\nI'm calling acm.describeCertificate just after acm.requestCertificate, but it seems that ACM doesn't generated the ResourceRecord yet, returning no ResourceRecord after the call.\nThinking about this, I added a \"wait\" command before calling acm.describeCertificate for the first time in a code very similar to this:\n```\nconst wait = async function (timeInSeconds) {\n  await new Promise(resolve => setTimeout(resolve, timeInSeconds * 1000))\n}\nasync createCertificate(...) {\n  //...\nconst { CertificateArn } = await acm.requestCertificate(params).promise()\nawait wait(10)\nconst certificateOptions = await acm.describeCertificate({ CertificateArn }).promise()\n  // now it returns ResourceRecord !\n//...\n}\n```\nAnd now I can capture the ResouceRecord to validate the certificate with DNS.\nThe entire process of creating and validating a certificate took about 1 min to me (due the waiting in the validation process), so 10 seconds waiting was not a problem is my case.\nThanks!. Hi @srchase ,\nOk! This issue is solved.\nThanks!. ",
    "robertsj-vmware": "Thanks @AllanFly120!!. ",
    "ihd2911": "\nimport aws from 'aws-sdk';\n\naws.config.accessKeyId = process.env.REACT_APP_AWS_ACCESS_KEY;\naws.config.secretAccessKey = process.env.REACT_APP_AWS_SECRET_ACCESS_KEY;\naws.config.region = process.env.REACT_APP_REGION;\n\nlet params = {\n    UserPoolId: process.env.REACT_APP_USERPOOLID,\n    Username: user.cognitoClientId,\n};\n\nlet cognitoidentityserviceprovider = new aws.CognitoIdentityServiceProvider();\n\ncognitoidentityserviceprovider.adminDeleteUser(params, function (err, data)\n\nAll the admin services can be used like this. AWS documentation is hard to understand.\nThis example for admin should be put somewhere as it can come handy.\nNo need to add following, as it is done for frontend only.\n aws.config.credentials = new aws.CognitoIdentityCredentials({\n   IdentityPoolId: process.env.REACT_APP_IDENTITYPOOLID,\n   AllowUnauthenticatedIdentities: true,\n   AccountId: 'string',\n   RoleArn: 'arn:aws:iam::string:role/Cognito_WebReactAuth_Role',\n});.\n",
    "rhaegar453": "The error persists @bahtou . The error persists @bahtou . @bahtou  I did change it to your code. I got the following error\nCannot assume role\n{ Error: connect EHOSTUNREACH 169.254.169.254:80\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1191:14)\n  message: 'Missing credentials in config',\n  errno: 'EHOSTUNREACH',\n  code: 'CredentialsError',\n  syscall: 'connect',\n  address: '169.254.169.254',\n  port: 80,\n  time: 2019-01-21T16:23:51.812Z,\n  originalError:\n   { message: 'Could not load credentials from any providers',\n     errno: 'EHOSTUNREACH',\n     code: 'CredentialsError',\n     syscall: 'connect',\n     address: '169.254.169.254',\n     port: 80,\n     time: 2019-01-21T16:23:51.812Z,\n     originalError:\n      { errno: 'EHOSTUNREACH',\n        code: 'EHOSTUNREACH',\n        syscall: 'connect',\n        address: '169.254.169.254',\n        port: 80,\n        message: 'connect EHOSTUNREACH 169.254.169.254:80' } } } 'Error: connect EHOSTUNREACH 169.254.169.254:80\\n    at TCPConnectWrap.afterConnect [asoncomplete] (net.js:1191:14)'. @bahtou  I did change it to your code. I got the following error\nCannot assume role\n{ Error: connect EHOSTUNREACH 169.254.169.254:80\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1191:14)\n  message: 'Missing credentials in config',\n  errno: 'EHOSTUNREACH',\n  code: 'CredentialsError',\n  syscall: 'connect',\n  address: '169.254.169.254',\n  port: 80,\n  time: 2019-01-21T16:23:51.812Z,\n  originalError:\n   { message: 'Could not load credentials from any providers',\n     errno: 'EHOSTUNREACH',\n     code: 'CredentialsError',\n     syscall: 'connect',\n     address: '169.254.169.254',\n     port: 80,\n     time: 2019-01-21T16:23:51.812Z,\n     originalError:\n      { errno: 'EHOSTUNREACH',\n        code: 'EHOSTUNREACH',\n        syscall: 'connect',\n        address: '169.254.169.254',\n        port: 80,\n        message: 'connect EHOSTUNREACH 169.254.169.254:80' } } } 'Error: connect EHOSTUNREACH 169.254.169.254:80\\n    at TCPConnectWrap.afterConnect [asoncomplete] (net.js:1191:14)'. My setup is that I have installed the aws-sdk using npm and ran the code you have posted above with my role arn. My setup is that I have installed the aws-sdk using npm and ran the code you have posted above with my role arn. var AWS=require('aws-sdk');\nasync function connAWS() {\n    var sts = new AWS.STS({ region: 'ap-south-1' });\nconst params = {\n  RoleArn: 'my role',\n  RoleSessionName: 'awssdk'\n};\nlet assumeRole;\n\ntry {\n  assumeRole1 = await sts.assumeRole(params).promise();\n} catch (err) {\n  console.log('Cannot assume role');\n  console.log(err, err.stack);\n  return;\n}\n\nconst accessparams = {\n  accessKeyId: assumeRoleStep1.Credentials.AccessKeyId,\n  secretAccessKey: assumeRoleStep1.Credentials.SecretAccessKey,\n  sessionToken: assumeRoleStep1.Credentials.SessionToken,\n};\n\nconst s3 = await new AWS.S3(accessparams);\nconsole.log('s3', s3);\n\n}\nconnAWS();. var AWS=require('aws-sdk');\nasync function connAWS() {\n    var sts = new AWS.STS({ region: 'ap-south-1' });\nconst params = {\n  RoleArn: 'my role',\n  RoleSessionName: 'awssdk'\n};\nlet assumeRole;\n\ntry {\n  assumeRole1 = await sts.assumeRole(params).promise();\n} catch (err) {\n  console.log('Cannot assume role');\n  console.log(err, err.stack);\n  return;\n}\n\nconst accessparams = {\n  accessKeyId: assumeRoleStep1.Credentials.AccessKeyId,\n  secretAccessKey: assumeRoleStep1.Credentials.SecretAccessKey,\n  sessionToken: assumeRoleStep1.Credentials.SessionToken,\n};\n\nconst s3 = await new AWS.S3(accessparams);\nconsole.log('s3', s3);\n\n}\nconnAWS();. Cannot assume role\n{ AccessDenied: Access denied\n    at Request.extractError (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/protocol/query.js:50:29)\n    at Request.callListeners (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:106:20)\n    at Request.emit (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\n    at Request.emit (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:683:14)\n    at Request.transition (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request. (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:685:12)\n    at Request.callListeners (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:116:18)\n  message: 'Access denied',\n  code: 'AccessDenied',\n  time: 2019-01-19T19:06:57.121Z,\n  requestId: '6406fec1-1c1d-11e9-b9d0-79ec2d277109',\n  statusCode: 403,\n  retryable: false,\n  retryDelay: 7.110264642110686 } 'AccessDenied: Access denied\\n    at Request.extractError (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/protocol/query.js:50:29)\\n    at Request.callListeners (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:106:20)\\n    at Request.emit (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\\nat Request.emit (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:683:14)\\n    at Request.transition (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:22:10)\\n    at AcceptorStateMachine.runTo (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/state_machine.js:14:12)\\n    at /home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/state_machine.js:26:10\\n    at Request. (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:38:9)\\n    at Request. (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:685:12)\\n    at Request.callListeners (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:116:18)'. Cannot assume role\n{ AccessDenied: Access denied\n    at Request.extractError (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/protocol/query.js:50:29)\n    at Request.callListeners (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:106:20)\n    at Request.emit (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\n    at Request.emit (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:683:14)\n    at Request.transition (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/state_machine.js:26:10\n    at Request. (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:38:9)\n    at Request. (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:685:12)\n    at Request.callListeners (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:116:18)\n  message: 'Access denied',\n  code: 'AccessDenied',\n  time: 2019-01-19T19:06:57.121Z,\n  requestId: '6406fec1-1c1d-11e9-b9d0-79ec2d277109',\n  statusCode: 403,\n  retryable: false,\n  retryDelay: 7.110264642110686 } 'AccessDenied: Access denied\\n    at Request.extractError (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/protocol/query.js:50:29)\\n    at Request.callListeners (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:106:20)\\n    at Request.emit (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\\nat Request.emit (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:683:14)\\n    at Request.transition (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:22:10)\\n    at AcceptorStateMachine.runTo (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/state_machine.js:14:12)\\n    at /home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/state_machine.js:26:10\\n    at Request. (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:38:9)\\n    at Request. (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/request.js:685:12)\\n    at Request.callListeners (/home/shivarajapple/projects/competition/projectS/node_modules/aws-sdk/lib/sequential_executor.js:116:18)'. @srchase  No , I didnt make any changes to assumeRole variables. Just installed the AWS-sdk for node and use the above code. @srchase  No , I didnt make any changes to assumeRole variables. Just installed the AWS-sdk for node and use the above code. Please do have a look at this error```\n{ Error: connect EHOSTUNREACH 169.254.169.254:80\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1191:14)\n  message: 'Missing credentials in config',\n  errno: 'EHOSTUNREACH',\n  code: 'CredentialsError',\n  syscall: 'connect',\n  address: '169.254.169.254',\n  port: 80,\n  time: 2019-01-24T05:54:02.155Z,\n  originalError:\n   { message: 'Could not load credentials from any providers',\n     errno: 'EHOSTUNREACH',\n     code: 'CredentialsError',\n     syscall: 'connect',\n     address: '169.254.169.254',\n     port: 80,\n     time: 2019-01-24T05:54:02.155Z,\n     originalError:\n      { errno: 'EHOSTUNREACH',\n        code: 'EHOSTUNREACH',\n        syscall: 'connect',\n        address: '169.254.169.254',\n        port: 80,\n        message: 'connect EHOSTUNREACH 169.254.169.254:80' } } }\n. Please do have a look at this error\n{ Error: connect EHOSTUNREACH 169.254.169.254:80\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1191:14)\n  message: 'Missing credentials in config',\n  errno: 'EHOSTUNREACH',\n  code: 'CredentialsError',\n  syscall: 'connect',\n  address: '169.254.169.254',\n  port: 80,\n  time: 2019-01-24T05:54:02.155Z,\n  originalError:\n   { message: 'Could not load credentials from any providers',\n     errno: 'EHOSTUNREACH',\n     code: 'CredentialsError',\n     syscall: 'connect',\n     address: '169.254.169.254',\n     port: 80,\n     time: 2019-01-24T05:54:02.155Z,\n     originalError:\n      { errno: 'EHOSTUNREACH',\n        code: 'EHOSTUNREACH',\n        syscall: 'connect',\n        address: '169.254.169.254',\n        port: 80,\n        message: 'connect EHOSTUNREACH 169.254.169.254:80' } } }\n```. I did try loading credentials with a JSON file. But I got access denied when assuming the role. The credentials were of an IAM user with S3 bucket permissions only though.. I did try loading credentials with a JSON file. But I got access denied when assuming the role. The credentials were of an IAM user with S3 bucket permissions only though.. ",
    "craigbrett17": "Got this too after resolving #1271.. It seems slightly related to the aforementioned bug.\nTemporary hack around seems to be (from StackOverflow) to add something like this to your polyfils.ts\n// aws-sdk requires global to exist\n(window as any).global = window;\nhttps://stackoverflow.com/questions/50264344/aws-sdk-crash-after-updating-from-angular5-to-angular6\nPerhaps they changed their polyfills or similar in Angular6 to not include global for you. If indeed you are using Angular 6 as I am.. ",
    "afaneh262": "@jayeshsheta in my case i had to add: \n// aws-sdk requires global to exist\n(window as any).global = window;\nto /src/polyfills.ts\nand \n\"types\": [\"node\"]\nto compilerOptions block in /src/tsconfig.app.json. ",
    "nodren": "Problem was found back in v2.187.0, updated to latest v2.275.1 with no affect.  Tried this before posting the issue here. Problem was found back in v2.187.0, updated to latest v2.275.1 with no affect.  Tried this before posting the issue here. ",
    "maitien2004": "@AllanFly120 \nI try to call updateJobExecution API  but job status not change.\nHere is my code.\n\nconst iotJobsDataPlane = new AWS.IoTJobsDataPlane(\n    { \n      endpoint: 'xxxx.jobs.iot.us-west-2.amazonaws.com' \n    }\n  );\nvar updateJobExecutionParams = {\n    thingName: 'thingId',\n    jobId: 'jodId',\n    status: 'SUCCEEDED'\n  };\niotJobsDataPlane.updateJobExecution(updateJobExecutionParams, function (err, data) {\n    if (err) {\n      console.log(err);\n    } else {\n      console.log(data);\n    }\n  });\n\nHere is output when execute above functions\n\n{}\n. Hi AllanFly120,\n\nI got the endpoint from describeEndpoint API with iot:Jobs endpoint type.\nI try remove endpoint in constructor but i have a difference problem.(Access Denied: Invalid enpoint. Use describe enpoint api to get vaild endpoint)\nPlease let me know if anything incorrect.\nThanks,\nTi\u1ebfn Mai\nFrom: AllanFly120\nSent: Saturday, July 28, 01:04\nSubject: Re: [aws/aws-sdk-js] ResourceNotFoundException on IoTJobsDataPlane updateJobExecution (#2146)\nTo: aws/aws-sdk-js\nCc: Mai Tien, Mention\nHi @maitien2004https://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fmaitien2004&data=02%7C01%7C%7Cec187019fe824e13bfcb08d5f3eb5d71%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636683114570335069&sdata=p5ZVyTppVImgRmiiw43EYtT%2FcttGKk9k8H2I0nHnDzY%3D&reserved=0\nWhere are you getting the endpoint from? Endpoint from the iot jobs data modelhttps://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Faws%2Faws-sdk-js%2Fblob%2Fbb421b7553b0e8a79299475a26e8c65350e642a1%2Fapis%2Fiot-jobs-data-2017-09-29.normal.json%23L5&data=02%7C01%7C%7Cec187019fe824e13bfcb08d5f3eb5d71%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636683114570491332&sdata=wFJhW8OvQet6zAloqsNijmAQuuUXoAe%2BVxwCojONIq0%3D&reserved=0 should be automatically set to something like data.jobs.iot.{region}.amazonaws.com Then you don't need to set the endpoint.\nWhat makes the issue a little weird is that the service returns a successful response instead of a resourceNotFound exception. Can you first try to remove the endpoint setting in constructing the client to see if that solve the problem?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Faws%2Faws-sdk-js%2Fissues%2F2146%23issuecomment-408495704&data=02%7C01%7C%7Cec187019fe824e13bfcb08d5f3eb5d71%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636683114570491332&sdata=R3f8e%2FMYUH%2BTWowHieHIw9qwnGzR8QOGibfnFE11NIE%3D&reserved=0, or mute the threadhttps://nam03.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAC49SmiF9ixzRBy3bpT_1Vwp14nADpVMks5uK1YcgaJpZM4VUGCm&data=02%7C01%7C%7Cec187019fe824e13bfcb08d5f3eb5d71%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636683114570491332&sdata=Q7cZbhChrQu108jxdxj3JZ%2BkpyUhz85eb1mUIizsIdU%3D&reserved=0.\n. ",
    "nsraptor": "I am facing the same issue while trying to update the  job status .. var iotjdp =  new AWS.IoTJobsDataPlane({\n\"accessKeyId\":\"**\",\n \"secretAccessKey\":\"**\", \n\"region\":\"*\"\n});\n     var params = {\n      jobId: job_id, / required /\n      status: new_Status, / required */\n      thingName: thing_Name,\n      stepTimeoutInMinutes: 0\n    };\n    iotjdp.updateJobExecution(params, function(err, data) {\n      if (err) {\n         reject(err, err.stack); // an error occurred\n      }\n      else {\n         console.log(\" status updated!!!!\")\n         resolve(\"JOB STATUS DATA : \"+JSON.stringify(data));           // successful response\n  }  . Appreciate your help :)\n\n. Appreciate your help :)\n. ",
    "cronvel": "@AllanFly120 Thanks for the reply.\nIt returns an error 500, with a valid XML, telling \"Unknown error\". It works when I craft the request all by myself, and I don't know what headers are sent by the SDK, or if the request is chunked, etc...\nIf there is a simple way to debug the SDK request, I would appreciate it! ;). ",
    "naveenp77": "Thanks,\nWhat is the best practice to update multiple items without scan or query? # # @AllanFly120 . ",
    "sashakorman": "Hey @AllanFly120. Exactly, we are hitting 500MB part size. Currently the incoming stream is always fs stream and the size of the stream is known. Generally speaking the incoming stream can be any stream, for example, streaming from Azure blob to S3 object.. Hey @AllanFly120. Exactly, we are hitting 500MB part size. Currently the incoming stream is always fs stream and the size of the stream is known. Generally speaking the incoming stream can be any stream, for example, streaming from Azure blob to S3 object.. @AllanFly120 make sense. We can ensure the input stream is a fs stream.. @AllanFly120 make sense. We can ensure the input stream is a fs stream.. We just tried to update to the latest SDK version (2.353.0) from 2.336.0\nIn our production environment we have a process that rolling AWS credentials, i.e. generates access/secret/token and stores it under profile name in shared credentials file.\nIt does not work with the latest SDK (ExpiredToken is received) since the file is loaded only once in the singleton IniLoader. \nIt works as expected using 2.336.0 version.\nHow credentials rolling should be handled now? \nShould we call clearCachedFiles function explicitly?\n. We just tried to update to the latest SDK version (2.353.0) from 2.336.0\nIn our production environment we have a process that rolling AWS credentials, i.e. generates access/secret/token and stores it under profile name in shared credentials file.\nIt does not work with the latest SDK (ExpiredToken is received) since the file is loaded only once in the singleton IniLoader. \nIt works as expected using 2.336.0 version.\nHow credentials rolling should be handled now? \nShould we call clearCachedFiles function explicitly?\n. @srchase Sure, will do. Thanks\nDo you need to assign the issue to me and create a branch for the fix?\n. @srchase Sure, will do. Thanks\nDo you need to assign the issue to me and create a branch for the fix?\n. @AllanFly120 You are right, the root cause is not clear.\nYes, we using read stream. The parameters are the same.\nThe interesting part is that this part is retried twice, both with RequestTimeout error from S3 (\"Your socket connection to the server was not read or written to within the timeout period\"). \n. @AllanFly120 You are right, the root cause is not clear.\nYes, we using read stream. The parameters are the same.\nThe interesting part is that this part is retried twice, both with RequestTimeout error from S3 (\"Your socket connection to the server was not read or written to within the timeout period\"). \n. ",
    "JerryKurata": "AllanFly120.  Thanks for looking into this so quickly!. AllanFly120.  Thanks for looking into this so quickly!. ",
    "ANTGOMEZ": "Yes, that worked. Thank you!. Thanks. This gets me undefined still:\n Here it is simplified.\n\n function getP(){\n    var params = {\n        Name: '/redshift/cluster/cluster-name-test/endpoint2', \n        WithDecryption: true\n    };\n\n    var request = ssm.getParameter(params);\n\n    request.on('success', function(response) {\n      return response.data; // or return in your function\n    });\n\n    request.on('error', function(err) {\n        throw err;\n      });\n\n    request.send();    \n}\n\nvar resp = getP();\n\nconsole.log(resp);\n. Yes the data is there inside the function, but I need to be able to return it so I can use it outside.. It's so much simpler, here it is:\n\n async function getP()\n {\n    var params = {\n        Name: '/redshift/cluster/cluster-name-test/endpoint2', \n        WithDecryption: true\n    };\nvar request = await ssm.getParameter(params).promise();\n\nreturn request.Parameter.Value;\n\n}\nasync function getParam()\n{\n    var resp = await getP();\nconsole.log(resp);\n\n}\ngetParam();\n.     var params = {\n        ClusterIdentifier: 'my-cluster',\n        SnapshotIdentifier: 'my-snap',\n       ManualSnapshotRetentionPeriod : 90\n    };\n    const redshift = new aws.Redshift();\n\n    var snap = await redshift.createClusterSnapshot(params).promise(); //fails here with error. But if this is the case then it should not fail when run on AWS Lambda, yet it does.\n\n\"error\": \"UnexpectedParameter\",\n  \"cause\": {\n    \"errorMessage\": \"Unexpected key 'ManualSnapshotRetentionPeriod' found in params\",\n    \"errorType\": \"UnexpectedParameter\",\n    \"stackTrace\": ....",
    "RLThomaz": "I'm closing this ticket. The method is asynchronous, so the last bit of code should be inside of the inner callback.. Hello @srchase,\nThanks for your help.\nSure, the actual template that is failing to create changeset is pretty simple. Of course, I removed all sensitive data - company name, userId, etc - so this won't work if you try to deploy it, but this is just a sample.\njson\n{\n  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n  \"Resources\": {\n    \"LambdaDeployment\": {\n      \"DependsOn\": [],\n      \"Type\": \"AWS::Lambda::Function\",\n      \"Properties\": {\n        \"FunctionName\": \"My-Macro-Lambdas\",\n        \"Code\": {\n          \"S3Bucket\": {\n            \"Fn::Sub\": \"mybucket.${AWS::Region}.cloud\"\n          },\n          \"S3Key\": \"macro/api.zip\"\n        },\n        \"Description\": \"Macro to deploy lambdas, stages, versions and alias.\",\n        \"Environment\": {\n          \"Variables\": {\n            \"SomeVariables\": \"{\\\"AWSTemplateFormatVersion\\\":\\\"2010-09-09\\\",\\\"Parameters\\\":{\\\"Stage\\\":{\\\"Type\\\":\\\"String\\\",\\\"Default\\\":\\\"dev\\\"}},\\\"Conditions\\\":{\\\"NotDev\\\":{\\\"Fn::Not\\\":[{\\\"Fn::Equals\\\":[\\\"dev\\\",{\\\"Ref\\\":\\\"Stage\\\"}]}]}},\\\"Resources\\\":{},\\\"Outputs\\\":{}}\"\n          }\n        },\n        \"Handler\": \"api.handler\",\n        \"Role\": \"arn:aws:iam::SomeUserId:role/Role-Lambda\",\n        \"Timeout\": 300,\n        \"Runtime\": \"nodejs8.10\"\n      }\n    },\n    \"DeployLambdas\": {\n      \"Type\": \"AWS::CloudFormation::Macro\",\n      \"Properties\": {\n        \"FunctionName\": {\n          \"Fn::GetAtt\": [\n            \"LambdaDeployment\",\n            \"Arn\"\n          ]\n        },\n        \"Name\": \"DeployLambdas\"\n      }\n    }\n  },\n  \"Parameters\": {\n    \"Product\": {\n      \"Type\": \"String\",\n      \"Description\": \"Product name.\"\n    }\n  }\n}\nSo I can create a stack using the template by either using the CFN Console or using my AWS-SDK-JS script. The script is also pretty simple, here is a snipt to create the stack and changeset (for update) if the stack already exists. At first the script tries to deploy (create) a stack. If the stack already exists it tries creating a changeset. So the second method creates the changeset and describes it, there it checks the changeset status, and for this specific template (without any changes) it will be FAILED right away.\n```javascript\nconst cf = new AWS.CloudFormation();\nasync function deployStack(stackname, params) {\n    try {\n        const stack = await cf.describeStacks().promise()\n        if (JSON.stringify(stack).match(stackname) === null) {\n            console.log(\"Creating \" + stackname + \" ...\")\n            await cf.createStack(params).promise()\n            const createComplete = await cf.waitFor(\"stackCreateComplete\", { StackName: stackname }).promise()\n            console.log(\"CREATE_COMPLETE - \" + stackname)\n            return createComplete\n        } else {\n            params.ChangeSetName = params.StackName + \"-CS\"\n            await deployChangeSet(stackname, params)\n        }\n    } catch (e) {\n        console.log(e, e.stack)\n    }\n}\nasync function deployChangeSet(stackname, params) {\n    try {\n        console.log(\"Computing changes for \" + stackname)\n        const stack = await cf.describeStacks().promise()\n        if (JSON.stringify(stack).match(stackname) === null)\n            params.ChangeSetType = \"CREATE\"\n    await cf.createChangeSet(params).promise()\n    const changeSetStatus = await cf.describeChangeSet({ ChangeSetName: params.ChangeSetName, StackName: params.StackName }).promise()\n    if (changeSetStatus.Status === \"FAILED\") {\n        await cf.deleteChangeSet({ChangeSetName: params.ChangeSetName, StackName: params.StackName }).promise()\n        console.log(\"FAILED - \" + stackname + \"-CS\")\n        return changeSetStatus\n    }\n    await cf.waitFor(\"changeSetCreateComplete\", { ChangeSetName: params.ChangeSetName, StackName: params.StackName }).promise()\n    console.log(\"Executing \" + stackname + \"-CS\")\n    await cf.executeChangeSet({ ChangeSetName: params.ChangeSetName, StackName: params.StackName }).promise()\n\n    if (JSON.stringify(stack).match(stackname) === null) {\n        console.log(\"Creating \" + stackname + \" ...\")\n        const createComplete = await cf.waitFor(\"stackCreateComplete\", { StackName: stackname }).promise()\n        console.log(\"CREATE_COMPLETE - \" + stackname)\n        return createComplete\n    } else {\n        console.log(\"Updating \" + stackname + \" ...\")\n        const updateComplete = await cf.waitFor(\"stackUpdateComplete\", { StackName: stackname }).promise()\n        console.log(\"UPDATE_COMPLETE - \" + stackname)\n        return updateComplete\n    }\n} catch (e) {\n    console.log(e, e.stack)\n}\n\n}\nasync function cfdeploy() {\n    console.log(\"Starting CloudFormation deployment...\")\ntry {\n    const macros = await deployStack(\"Mercury-Macros\", {\n        StackName: \"My-Macros\",\n        Parameters: [{ ParameterKey: \"Product\", ParameterValue: \"ProductName\" }],\n        TemplateURL: \"https://s3.amazonaws.com/mybucket.cloudformation/my.template.macro\"\n    })\n    console.log(macros)\n} catch (e) {\n    console.log(e, e.stack)\n}\n\n}\ncfdeploy()\n```. @srchase \nThank you for your response. I understand that the console (web version) has greater abstraction - which makes sense since it is a graphical interface, and also for the CLI to reduce verbosity. \nI am already catching failed changesets and cleaning them afterward. I'll introduce versioning to my lambda CFN resource by activating S3 bucket versioning, that way whenever there is an updated lambda version the changeset shall not fail.\nI'll raise the question with the team to understand their decision on failing the changeset creation, instead of only warning that no update is required, when there are no changes on the template. After all, when no changes are present it still creates the changeset on the console version and updates the stacks.\nThank you again. ",
    "stripathix": "I an using AWS cognito no external provider.\n`\nAWS.config.region = AccountConst.awsCognito.region;\n    AWS.config.correctClockSkew = true;\n\n    AWS.config.credentials = new AWS.CognitoIdentityCredentials({IdentityPoolId: identityPoolId});\n\n    var userPool = new AmazonCognitoIdentity.CognitoUserPool(poolData);`\n\n`if (cognitoUser !== null) {\n        cognitoUser.getSession(function (err, oldSession) {\n            if (!err) {\n                var refreshToken = oldSession.getRefreshToken();\n                console.log(\"Refreshing session\");\n                cognitoUser.refreshSession(refreshToken, function (err, refreshedSession) {\n                    if (!err) {\n                        console.log(\"Session refreshed successfully!\");\n                        var logins = {};\n                        logins[cognitoEndpoint + \"/\" + poolData.UserPoolId] = refreshedSession.getIdToken().getJwtToken();\n                        AWS.config.credentials = new AWS.CognitoIdentityCredentials({\n                            IdentityPoolId: identityPoolId,\n                            Logins: logins\n                        });\n                        // Make the call to obtain credentials\n                        AWS.config.credentials.get(function () {\n                            console.log(\"Success credentials\");\n                            refreshingToken = false;\n                            client = new AWS.CognitoSyncManager();\n                            if (client) {\n                                client.openOrCreateDataset(bnDataSetName, function (err, dataset) {\n                                    if (!err) {\n                                        bnUserPreferencesDataset = dataset;\n                                    } else {\n                                        errorLogService(err);\n                                    }\n                                });\n                            }\n                        });\n                        deferred.resolve(true);\n                    } else {\n                        err.message = \"ResumeAppplicationCognito: Not able to refresh session: User \" + cognitoUser.username + \":\" + err.message;\n                        err.emailTried = cognitoUser.username;\n                        if (!err.code.match(new RegExp(AppConst.exceptionsCodes.UserNotFoundException)) && !err.message.match(new RegExp(AppConst.exceptionsCodes.incorrectPassword))) {\n                            errorLogService(err);\n                        }\n                        deferred.resolve(false);\n                    }\n                });\n            }\n        });\n    }``. It's intermittent so I would say it happens after some time using app.\n\nI have a aws configuration that refresh token expiry after 3650 days. As CognitoIdentityCredentials does not manage refreshing token, so what should I use to make sure that AWS sdk handle refreshing of auth token by itself.. ",
    "kishoredonepudi": "@chrisradek \nI am using the following way to call unsubscribe a subscription.\nconst aws = require('aws-sdk');\nconst bluebird = require('bluebird');\naws.config.setPromisesDependency(bluebird);\naws.config.apiVersions = {\n        sns: '2010-03-31'\n    };\nconst sns =new aws.SNS();\nvar unsubscribe = async (subscriptionArn) => {\n        if (typeof subscriptionArn !== 'undefined' && subscriptionArn) {\n            var params = {\n                SubscriptionArn: subscriptionArn  / required /\n            };\n            return await sns.unsubscribe(params).promise().then((res) => {\n                console.log(\"response for unsubscribe:\", res);\n                return res;\n            }).catch((error) => {\n                console.log(\"Error unsubscribing subscription:\", error, params);\n                return Promise.reject({\n                    errorType: \"ERROR_UNSUBSCRIBING_SUBSCRIPTION\",\n                    error: error,\n                    input: subscriptionArn,\n                    params: params\n                });\n            });\n        } else {\n            return Promise.reject({ error: \"Require subscriptionArn\" });\n        }\n    };\n. @chrisradek \nI didn't used listSubscriptions since there are more than 1000 subscriptions. After looking at the error I logged into console and verified it is there and also verified with aws cli command.. @chrisradek \nYes I was able to unsubscribe with same code by requesting again . The problem I have is why it is not unsubscribing on first request every time. This is not happening for all requests.\n. ",
    "mclaborn": "Setting Metadata. I did find and follow the Tags example and it works as written. I did see Metadata mentioned on the s3.upload function but not on the AWS.S3.ManagedUpload object. Both places need to discuss the header names and the fact that the SDK adds the necessary prefix for you.  . ",
    "zhiwei-nu": "@chrisradek I see, thanks for the info! Could be worth having a warning for this scenario :). ",
    "sul4bh": "Awesome. Thanks for the quick fix.. Awesome. Thanks for the quick fix.. ",
    "mohammad1990": "@chrisradek  , \nI solve the problem by adding config the Aws on windows.\n1- I  install the Amazon CLI for windows.\n2- then I write \"aws configure\" on a command line, after that, I put my secret key... \nso everything thing works correctly. :). Hello, I know where is the problem but I don't know why that happened,\nWhen I  use pre_sign URL without login by passport everything going correct but when I login in using passport JWT I get that error so can you please help me.\nNote: the Passport works correctly without error.. My request URL is \ud83d\udc4d \n\nhttps://test.s3.amazonaws.com/2f327da0-9fe7-11e8-b094-ef4582a079a8.jpeg?Content-Type=image%2Fjpeg&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIFZ55WBR5RKAPT5A%2F20180814%2Feu-west-1%2Fs3%2Faws4_request&X-Amz-Date=20180814T172631Z&X-Amz-Expires=600000&X-Amz-Signature=330143d61f9bbfcf56cff60b0ece899d18f3e06ac9f84f8038c73ed9b7bbe5e5&X-Amz-SignedHeaders=host%3Bx-amz-acl&x-amz-acl=public-read. This the action that I send \ud83d\udc4d \nI think PreSign URL send also the passport auth with the PreSign URL if that, how I can send  PreSign URL without that ArgumentValue Thanks.\nconst options = {\n        headers: {\n          'Content-Type': file.type\n        }\n      };\n  axios.put(uploadURL, file, options).catch(err =>{\n        console.log(err);\n      });\n{data: \"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\u21b5\", status: 400, statusText: \"Bad Request\", headers: {\u2026}, config: {\u2026},\u00a0\u2026}config: {adapter: \u0192, transformRequest: {\u2026}, transformResponse: {\u2026}, timeout: 0, xsrfCookieName: \"XSRF-TOKEN\",\u00a0\u2026}data: \"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\u21b5InvalidArgumentOnly one auth mechanism allowed; only the X-Amz-Algorithm query parameter, Signature query string parameter or the Authorization header should be specifiedAuthorizationBearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjViNmMzMDkxNzVhZDA3MDQ2ODQ4NTlkZCIsInVzZXJuYW1lIjoibW9oYW1tYWQgdG9maSIsImF2YXRhciI6Ii8vd3d3LmdyYXZhdGFyLmNvbS9hdmF0YXIvOThiYzE0MTIxNWE1MjQ0ZjJhNDhkMzNiMTQ1NmM1NmQ_cz0yMDAmcj1wZyZkPW1tIiwiaWF0IjoxNTM0MzE4OTc2LCJleHAiOjE1MzQzMjI1NzZ9.0nKbw9Fd_xhzre1980snJTVOc0RvoF3vyrK25JKouSc49F5F0EAB1171FE1GQ0XFcIz+xX1wQTmfqyweLpUOCutvngKauKOVGgUMbLbJdo1VxVTpWA9u5wW614ekt2IPuUoYsk=\"headers: {content-type: \"application/xml\"}request: XMLHttpRequest\u00a0{onreadystatechange: \u0192, readyState: 4, timeout: 0, withCredentials: false, upload: XMLHttpRequestUpload,\u00a0\u2026}status: 400statusText: \"Bad Request\"proto: Object\n. \n",
    "isaacrdz": "Could you show me your post or put request  please. ",
    "coolrz": "it is not regarding AWS API issue, sorry for misunderstanding. ",
    "rix0rrr": "@chrisradek, that is a solution, but that requires us to reorder imports (nonstandard) and keep them that way (requires constant human attention & effort). It works as a workaround, but I would hope the SDK would have a way of picking a profile from code. That's why I'm reporting this.\n@AllanFly120, not ideal, but given the design of the SDK API I understand. It would make me feel better about the rest of our code, at least.. To be clear, this is what my copy of credential_provider_chain.d.ts looks like:\n```ts\nimport {Credentials} from '../credentials';\nimport {AWSError} from '../error';\nexport class CredentialProviderChain extends Credentials {\n    /\n     * Creates a new CredentialProviderChain with a default set of providers specified by defaultProviders.\n     */\n    constructor(providers?: provider[])\n    /\n     * Resolves the provider chain by searching for the first set of credentials in providers.\n     /\n    resolve(callback:(err: AWSError, credentials: Credentials) => void): CredentialProviderChain;\n    /\n     * Return a Promise on resolve() function\n     */\n    resolvePromise(): Promise;\n    /\n     * Returns a list of credentials objects or functions that return credentials objects. If the provider is a function, the function will be executed lazily when the provider needs to be checked for valid credentials. By default, this object will be set to the defaultProviders.\n     /\n    providers: Credentials[]|provider[];\nstatic defaultProviders: provider[]\n\n}\ntype provider = () => Credentials;\n```. Yes, I was thinking the same.\nAre you writing the type definition files by hand, by the way?. You should come to TypeScript land. The water's nice here! :stuck_out_tongue_winking_eye: . Ping?. Okay I have no idea how that STS client gets its region (given that there's no region argument to it), but it seems to work, so I've got nothing. Sorry for bothering you.. ",
    "kylefarris": "tl;dr\nSeems like the SDK is completely ignoring the profile option.\nFull\nIf this helps at all... I'm also unable to load any credentials profile other than the default one. My credentials file (located in ~/.aws/credentials) is like this:\n```ini\n[default]\naws_access_key_id=REDACTED\naws_secret_access_key=VERYREDACTED\n[other_profile]\naws_access_key_id=ALSOREDACTED\naws_secret_access_key=ALSOVERYREDACTED\n```\nIn my very simple script, I do the following to set my profile and region:\njavascript\nconst AWS = require('aws-sdk');\nconst credentials = AWS.config.credentials = new AWS.SharedIniFileCredentials({profile: 'other_profile'});\nAWS.config.update({region:'us-west-2'});\nIf I spit out the the credentials object (console.log(\"Credentials: \", credentials)), I get the following:\njavascript\nCredentials SharedIniFileCredentials {\n  expired: false,\n  expireTime: null,\n  accessKeyId: undefined,\n  sessionToken: undefined,\n  refreshCallbacks: [],\n  filename: undefined,\n  profile: 'other_profile',\n  disableAssumeRole: false,\n  preferStaticCredentials: false,\n  tokenCodeFn: null }\nWhich, while it seems to get the profile part correct, all the pertinent information is undefined. So, being the forever-optimist that I am, I'm thinking maybe it's just not visible here or something...?\nSo, on the very next lines I initiate an instance of CognitoIdentityServiceProvider and spit out the credentials config to see what's going on...\njavascript\nconst congnito_isp = new AWS.CognitoIdentityServiceProvider();\n// SharedIniFileCredentials is the third option in the providers array...\nconsole.log(\"Credentials\", cognito_isp.config.credentialProvider.providers[2]());\nI get the following output:\njavascript\nSharedIniFileCredentials {\n  expired: false,\n  expireTime: null,\n  accessKeyId: 'REDACTED',  // aws_access_key_id from [default]\n  sessionToken: undefined,\n  refreshCallbacks: [],\n  filename: undefined,\n  profile: 'default', // WHY!?!?\n  disableAssumeRole: false,\n  preferStaticCredentials: false,\n  tokenCodeFn: null }\nI've also tried passing the credentials info directly with the same effect\njavascript\nconst congnito_isp = new AWS.CognitoIdentityServiceProvider({credentials});\nI've also tried running this simple script by specifying the AWS_PROFILE (and of course commenting-out the AWS.config.credentials bit of code in the app) when running the script to no avail:\nbash\nAWS_PROFILE=other_profile node index.js\nJust trying to give some more data points on this one. Would love to have some answers on this.\nThanks!. Okay, so, after all that...\n\nI'm an idiot and actually had the [other_profile] items set like: aws_access_key_id:REDACTED (notice the colon and not an = sign).\nOnce I fixed that, spitting out both objects yielded the correct info.\nI also made a mistake in that if I were to pass that credentials variable to the CognitoIdentityServiceProvider constructor, it would have needed to be with the credentialProvider not the credentials key.\n\nSo, yeah, ignore what I said above. I'm a dumbass.. ",
    "mclark-newvistas": "Yep, that does it. Thank you. Where did I miss this documented?. Fair enough, I look forward to the next major version bump, and the doc update :).. ",
    "seanirby": "Thanks for taking a look!. ",
    "entmike": "This is happening in an AWS Lambda function for me, as well..... This is happening in an AWS Lambda function for me, as well..... ",
    "d1sco": "Just to help anyone else stuck in this situation. There is nothing wrong, if Authorizer tests are responding with 200 status code and showing the proper after body parse everything is working fine. Principal and Content will be removed and forwarded over proxy/mapping template automatically.\nInvoking the lambda over testing showed the full result, invoking the Authorizer over API Gateway > Testing shows the response after body parse.\nIf you getting successful body parse over Authorizer testing you are good. I was acting paranoid, my API Gateway Authorizer is working great and allowing me to Authenticate and Authorizer my users no issues!\nHope this helps someone else. ",
    "dpmallinger": "@chrisradek \nI updated the pull request to just be the fix for this.options\nif I don't have any env vars set and I call\nconst profileCredentials = new AWS.SharedIniFileCredentials({filename:'/foo/bar/baz'});\nthe filename is lost in loadRoleProfile().  I feel that fix is still valid and needed.\n. @srchase \nHello,\nI believe I had a situation where I passed an aws profile as a parameter in options that needed to then be passed into the AWS.SharedIniFileCredentials instantiation ( line 233 of the same file ).    \nvar sourceCredentials = new AWS.SharedIniFileCredentials(\nAWS.util.merge(this.options || {}, {\nprofile: sourceProfileName,\npreferStaticCredentials: true\n})\n);\nIts been a while and the exact use case is a bit fuzzy.\n. ",
    "olahivepriyanka": "@srchase \nIm sorry for the late response , my question is how to generate a SignedURL with extension?\nnow my signed url is coming like this https://scgallery.s3.ap-south-1.amazonaws.com/artistgallery/1531994728558.mp3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=F20180815%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20180815T083839Z&X-Amz-Expires=900&X-Amz-Signature=67b4c6ac7ceb5f414e912b2010e8419707d5371b83fdc62d6&X-Amz-SignedHeaders=host\ni want something like this\nhttps://scgallery.s3.ap-south-1.amazonaws.com/artistgallery/1531994728558.mp3?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=F20180815%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20180815T083839Z&X-Amz-Expires=900&X-Amz-Signature=67b4c6ac7ceb5f414e912b2010e8419707d5371b83fdc62d6&X-Amz-SignedHeaders=host.mp3\nAt the end i need extension of the file. ",
    "MainAero": "Decrease of coverage is caused by missing tests for payloads that aren't Blob. . @chrisradek \nDouble checked the code of param_validator.js and it seems disabling the parameter validation has no effect on the validatePayload method.\nFrom my understanding global variables aren't mangled.\nOther option is to disable mangling on react native production build as mentioned on stackoverflow.com\nUpdated PR with check if Blob exists.. @chrisradek I did both. But currently my testing environment isn't reliable. Let me test it again.. @chrisradek  I can now confirm disabling the parameter validation fixes the issue. There is no clean way to disabling param validation thru aws-amplify. Did it inside the node_modules/aws-amplify folder of my project because aws-amplify has a dependency to aws-sdk.\nI see two ways of fixing this issue:\n1. Merge PR and let aws-amplify maintainers bump aws-sdk version\n2. I can open a issue/pr at aws-amplify to get the possibility to disabling parameter validation thru aws-amplify / disabling it by default.\nNot fixing this issue let RN Expo projects stuck on file upload. There is a workaround for images (base64) but no efficient one for videos, audio records etc without detaching the expo project.. I'm patching the aws-sdk during ci build.\nFor everyone else here is my patch file created for aws-amplify:\n```patch\n--- a/node_modules/aws-amplify/node_modules/aws-sdk/dist/aws-sdk-core-react-native.js\n+++ b/node_modules/aws-amplify/node_modules/aws-sdk/dist/aws-sdk-core-react-native.js\n@@ -16892,6 +16892,7 @@\n          var Stream = AWS.util.stream.Stream;\n          if (AWS.util.Buffer.isBuffer(value) || value instanceof Stream) return;\n        }\n+       if (Blob && value instanceof Blob) return;\n    var types = ['Buffer', 'Stream', 'File', 'Blob', 'ArrayBuffer', 'DataView'];\n    if (value) {\n\n--- a/node_modules/aws-amplify/node_modules/aws-sdk/dist/aws-sdk-react-native.js\n+++ b/node_modules/aws-amplify/node_modules/aws-sdk/dist/aws-sdk-react-native.js\n@@ -17194,6 +17194,7 @@\n              var Stream = AWS.util.stream.Stream;\n              if (AWS.util.Buffer.isBuffer(value) || value instanceof Stream) return;\n            }\n+           if (Blob && value instanceof Blob) return;\n        var types = ['Buffer', 'Stream', 'File', 'Blob', 'ArrayBuffer', 'DataView'];\n        if (value) {\n\n--- a/node_modules/aws-amplify/node_modules/aws-sdk/lib/param_validator.js\n+++ b/node_modules/aws-amplify/node_modules/aws-sdk/lib/param_validator.js\n@@ -243,6 +243,7 @@\n       var Stream = AWS.util.stream.Stream;\n       if (AWS.util.Buffer.isBuffer(value) || value instanceof Stream) return;\n     }\n+    if (Blob && value instanceof Blob) return;\n var types = ['Buffer', 'Stream', 'File', 'Blob', 'ArrayBuffer', 'DataView'];\n if (value) {\n\n```\napply for instance with: patch -p1 < patches/0001-Fixes-payload-validation-for-react-native-if-payload.patch. @chrisradek Any news?. Replacment merged.. ",
    "ballenwillis": "I'm really hoping this gets merged in. We were foolish for a deadline that MUST be met by Saturday at 6pm and it relies on this to work. For now, we're going to go into node_modules to fix. I'm not comfortable with this, but it's more comfortable than my app continuing to crash in production.. Just commenting here again that we've had this running in our codebases for almost a week now. It's been working great and I'm extremely grateful that this PR was made just in time. We pushed our update 2 hours before the launch event (our deadline).. ",
    "tallpants": "@chrisradek any chance of getting this merged soon? It's a pretty big blocker since it essentially prevents Amplify's storage module from being used in React Native in production.. @chrisradek any chance of getting this merged soon? It's a pretty big blocker since it essentially prevents Amplify's storage module from being used in React Native in production.. ",
    "kidcosmic": "ReadableStream is actually supported by the JavascriptCore, ie, Safari. We're actually shaping another stream into ReadableStream via the ReadableStream constructor:\nfunction convertStreamToReadableStream(_in) {\n  return new ReadableStream({\n    start(controller) {\n      _in.open()\n      _in.onData((chunk) => {\n        controller.enqueue(chunk)\n      })\n      _in.onError((error) => {\n        controller.error(error)\n      })\n      _in.onEnd(() => {\n        controller.close()\n      })\n    }\n  })\n}\nDoesn't even look like even fetch accepts streams as body arguments, which kinda makes this whole discussion moot, lmao.\nI'll look at File/Blob but I can't seem to get blobs outside of blobbing a response body. Their constructors are out of the question because of loading into mem. Hmmm... Will most likely have to native this. That's pretty clever actually. Sounds like it would work to me. 5MB in memory much better than 20\nSo per the blob suggestion, the pattern works in React Native. XMLHttpRequest accepts a plain js object with {uri, optional field, optional name}, and XHR does the dirty work under the hood. Note that both XHR.send and FormData.add accept this. For example:\nhttps://github.com/facebook/react-native/issues/15724\nThere is no explicit formalization of it, just that it works.\nPossible solution would be in ManagedUpload\nif isReactNative() acceptUri \nlike \nif isNode() acceptStream, but that's a whole do we support X debate. \nWould love to use Storage with amplifyjs, but it looks we'd probably have to drop down to aws-sdk-js and run with getSignedUrl if we want XHR.sendUri.\nCould also take this to Amplify team and have them implement getSignedUrl in the case of {uri} since they seem to explicitly support react native.\nReadableStream definitely on your turf though... ",
    "Grimml": "Hi,\nI am using it in a LambdaFunction with Node.js 8.10. i tried specifying the api version but it did not fix anything.. I just ran it now on Lambda again (the exact same code as yesterday) and it worked fine, I also got \n\nInvalidParameterType: Expected params.DefaultActions[0].RedirectConfig.Port to be a string\n\nwhich I quickly changed. \nI guess this is resolved, though I still wonder why it didn't work yesterday.. ",
    "bright-future": "\"aws-sdk\": \"^2.286.2\",\n\"sinon\": \"^6.1.4\"\nI don't know if its AWS or sinon . Because as mentioned above its working in old style of code i.e dynamodb.getItem(params,callback) in node node is been stubbed by sinon if i stub the \"make request\" method in  AWS.Service.prototype .  but \"make request\" is not invoked when i use dynamodb.getItem(params).promise(). So im forced to change my production code just for testing purpose. Can u show me how use it in dynamodb by stubbing  'makeRequest' of Aws Service it will be great. As the given example doesn't overrides it. It will be great if you share a workable code  of dynamodb stubbing. ",
    "spouzols": "@chrisradek \nThank you. We checked our use case with fix version 2.297.0 packaged in lambda and it's working.. ",
    "dawsbot": "Looks like tests fail on node 9 and 10. This aligns with my experience locally that this module does not work with node 9 and 10. There are binary C dependencies which fail in these newer versions of node.\nPerhaps this PR can stay open as a \"bar to reach\" or version support? Let me know. Looks like tests fail on node 9 and 10. This aligns with my experience locally that this module does not work with node 9 and 10. There are binary C dependencies which fail in these newer versions of node.\nPerhaps this PR can stay open as a \"bar to reach\" or version support? Let me know. Hi @AllanFly120, thank you for the rapid message. I've found that installing the module is blocked entirely on node 9 and 10. I added this test as a benchmark so that I could check back when that's working.\nWith your encouragement to contribute though, I'd be happy. After examining the official node docs for dispose, I'm finding that their recommendation is to \"Domain.dispose() has been removed. Recover from failed I/O actions explicitly via error event handlers set on the domain instead.\"\nI'm not sure what that means, any other advice?. Hi @AllanFly120, thank you for the rapid message. I've found that installing the module is blocked entirely on node 9 and 10. I added this test as a benchmark so that I could check back when that's working.\nWith your encouragement to contribute though, I'd be happy. After examining the official node docs for dispose, I'm finding that their recommendation is to \"Domain.dispose() has been removed. Recover from failed I/O actions explicitly via error event handlers set on the domain instead.\"\nI'm not sure what that means, any other advice?. ",
    "Ferrari": "Sorry, there has some private information some I could not list the setting at here, but our key & bucket just a simple string and did not have any special characters inside. All we're just calling a simple upload function like following. (Credential assign by env)\nThe issue happened because we use ^ on package.json so aws-sdk upgrade automatically. We test our credential with AWS cli and it work fine. So we guess might be package issue so we downgrade the package version and problem solved.\n```\nconst s3 = new AWS.S3()\ns3\n  .upload({\n    Body: FILE_BUFFER,\n    Bucket: BUCKET_NAME,\n    Key: KEY,\n    ACL: 'public-read'\n  })\n  .send(callback)\n```. Sorry, there has some private information some I could not list the setting at here, but our key & bucket just a simple string and did not have any special characters inside. All we're just calling a simple upload function like following. (Credential assign by env)\nThe issue happened because we use ^ on package.json so aws-sdk upgrade automatically. We test our credential with AWS cli and it work fine. So we guess might be package issue so we downgrade the package version and problem solved.\n```\nconst s3 = new AWS.S3()\ns3\n  .upload({\n    Body: FILE_BUFFER,\n    Bucket: BUCKET_NAME,\n    Key: KEY,\n    ACL: 'public-read'\n  })\n  .send(callback)\n```. ",
    "belmer": "Hi @srchase,\nI'm having the same exact issue as the above mentioned. The version I've used is \"2.233.1\". Upgrading to \"v2.276.1\" or the latest does not help, Even downgrading does not help too.. Im still trying to figure out why. . ",
    "danielopatich": "@chrisradek sorry about that. Meant to create the PR for my fork of aws-sdk-js.. ",
    "acheronfail": "Anything holding this back folks? :) . ",
    "matwerber1": "Hi, \nWhen I don't specify filters, both spot and scheduled instances are shown. The issue is that the instanceLifecycle key is not present in the JSON response for scheduled instances. The documentation and API guidance suggests that the instanceLifecycle key should be present with a value of \"scheduled\", rather than not being present. \nThis NodeJS Lambda demonstrates this:\n```\nvar AWS = require('aws-sdk');\nvar ec2 = new AWS.EC2();\nexports.handler = async (event, context) => {\ntry {\n    var ec2_data = await ec2.describeInstances().promise();\nfor (const r of ec2_data.Reservations) {\n  for (const i of r.Instances) {\n    let instanceLifecycle;\n\n    if (\"InstanceLifecycle\" in i) { \n      instanceLifecycle = i.InstanceLifecycle  \n    } else {\n      instanceLifecycle = '<not present>';\n    }\n    console.log(i.InstanceId + \" lifecycle is: \" + instanceLifecycle);\n  }\n}\n\n}\n  catch (err) {\n    console.log('>>>>>>ERROR>>>>>>>\\n' + err);\n  }\n};\n```\nHere is the response:\nFunction Logs:\n2018-08-28 04:14:05.020 i-0cf89faa939d3b343 lifecycle is: <not present>\n2018-08-28 04:14:05.021 i-0935a66cc64e3b7ad lifecycle is: <not present>\n2018-08-28 04:14:05.022 i-0a09edbdfb958036b lifecycle is: <not present>\n2018-08-28 04:14:05.023 i-02c54b27658bbb7df lifecycle is: <not present>\n2018-08-28 04:14:05.023 i-01a46092c2a621ae3 lifecycle is: spot\n2018-08-28 04:14:05.023 i-01417ba491e0f5a6d lifecycle is: spot\n2018-08-28 04:14:05.024 i-0692ff4de87185f69 lifecycle is: <not present>\n2018-08-28 04:14:05.025 i-030e06b1a18a30f60 lifecycle is: <not present>\n2018-08-28 04:14:05.025 i-049d45e1532a1d2ef lifecycle is: <not present>\n2018-08-28 04:14:05.026 i-049f03f1414aaebdb lifecycle is: <not present>\nWhile I could use describeInstances() to get all instances and then loop through the list to find scheduled instances (i.e. where instanceLifecycle not in object response), it would be preferable to simply specify a filter of instanceLifecycle=\"scheduled\" to avoid the need for the loop. This can be done for spot instances, but as best I can tell, not for scheduled instances. . > UPDATE: Since the describeInstance call doesn't return any lifeCycle configuration for scheduled instance, how did you confirmed these instances are actually in the scheduled state?\nYour question just made me realize that I was wrong to open this as an issue. I was incorrectly assuming that \"scheduled\" was the same as \"on-demand\" instances. In looking through the console & docs, I was clearly missing the fact that scheduled instances are not the same as on-demand. My goal from the start was to query for on-demand vs. spot, since I want to decide between either terminating instances (spot) vs. stopping instances (on-demand). \nThe instances in my prior comment that had the value of \"\" for lifecycle were actually on-demand instances. I don't have any scheduled instances in my account. \nSorry for confusion and thank you for your help. I think this can be closed.. ",
    "bittlingmayer": "@AllanFly120 Currently https://www.npmjs.com/package/aws-sdk depends on an old version of uuid.\nThat old version of uuid has a bug in its ESLint config.  The exact bug is the line with \"installedESLint\": true, which should never have existed.\nSo when linting aws-sdk or a package that depends on aws-sdk, developers will hit the error.\nSo to repro, assuming eslint is installed: eslint --fix * --parser-options='ecmaVersion:2018' from the top-level directory.\nYou will hit an error caused by the offending line in /node_modules/aws-sdk/node_modules/uuid/.eslintrc.json.\nTo fix it locally you can:  \nremove that line from uuid/.eslintrc.json (essentially patch in their fix in a local hacky way)\nOR change aws-sdk to depend on a version of uuid with the fix (like my PR)\nOR add **/node_modules/** to your own .eslintignore in the top-level dir (generally a good idea anyway, but sort of just masks the problem, and not ideal that aws-sdk is the cause of such surprises)\nMaybe there is a clever way for aws-sdk to stop transitive linting past itself, but I am not sure if .eslintignore can be used like that.. @AllanFly120 Seems like this could be merged.. ",
    "SimonSchick": "To be fair https://github.com/kelektiv/node-uuid/blob/master/CHANGELOG.md there are no breaking changes, shouldn't be an issue to update to latest.. As suggested in #2341 you might want to use the files property in package.json, this should prevent you from ever shipping unwanted files again :). ",
    "sdrioux": "Aha, thank you!  That makes more sense.  I was using the definitions in a lambda that was reading off the stream.  It seems a little odd that when a lambda reads off the stream, the eventSourceArn is present, however, when reading the stream from the CLI it is missing.. ",
    "mvidalis": "@chrisradek \nin one of my routes files, I instantiate the s3 object as follows:\nvar s3 = new aws.S3();\n. I've been using the sdk for just over a month now. I just ran an npm install to install all my node packages and now I'm seeing this error.\nIt was working perfectly fine before the npm-install!\n. If i run npm -v aws-sdk, it shows I'm running v5.6.0. It is a node project.\nI'm not entirely sure of which version I was running previously, however I installed it around the beginning of July this year.\nDo you think it could be an error on my side? \n. Sorry, my mistake!\nIt shows I am running v2.304.0. @chrisradek \nMy package.json shows v2.304.0\nI ran that code in a new project and it prints out the buckets in my s3 repository.. Should I uninstall the package and reinstall it?\n. ",
    "ztj1993": "@srchase Nginx is required to forward all, including authentication, list, and details. The front page and interface must be downloaded at the same IP address.\nThe authentication of AWS S3 is too complicated.\nAfter nginx forwarding, authentication fails.\nBecause of the hostname changed by the agent, the signature failure is caused.\nI've seen some nginx AWS auth modules, all for single accounts, and I need multi-account authentication. Nginx only needs to be responsible for URI forwarding function (it may require nginx to set some header)(https://github.com/anomalizer/ngx_aws_auth).\n\nI mainly do a front-end file management function (I want to implement a front-end file management function (note that AWS configuration is not possible).).\n\nThe above content comes from software translation.. @srchase Nginx is required to forward all, including authentication, list, and details. The front page and interface must be downloaded at the same IP address.\nThe authentication of AWS S3 is too complicated.\nAfter nginx forwarding, authentication fails.\nBecause of the hostname changed by the agent, the signature failure is caused.\nI've seen some nginx AWS auth modules, all for single accounts, and I need multi-account authentication. Nginx only needs to be responsible for URI forwarding function (it may require nginx to set some header)(https://github.com/anomalizer/ngx_aws_auth).\n\nI mainly do a front-end file management function (I want to implement a front-end file management function (note that AWS configuration is not possible).).\n\nThe above content comes from software translation.. @srchase yes, I need to do all the operations through aws-sdk-js, just like aws-sdk-php. But it does not pass any back-end program, only through API. \nPerhaps the headers were involved in the signature calculation and the server authentication failed. I need to do more research on the AWS signature.\n. @srchase yes, I need to do all the operations through aws-sdk-js, just like aws-sdk-php. But it does not pass any back-end program, only through API. \nPerhaps the headers were involved in the signature calculation and the server authentication failed. I need to do more research on the AWS signature.\n. ",
    "jasonfutch": "We had same issue.. the root cause of our issue has to do with dynasty package not locking into one version ^ of aws-sdk.\nAlso AWS-SDK team should of never changed a callback to a promise in a minor release as this is a breaking change.. It happened Wednesday about 4pm EST, it was with release 3.305.0\n\nOn Aug 31, 2018, at 7:40 PM, Mike Flores notifications@github.com wrote:\n@jasonfutch when did that change occur, do you know?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "felipefcm": "From http://bluebirdjs.com/docs/api/promise.promisifyall.html:\n\nIf a method name already has an \"Async\"-suffix, it will be duplicated. E.g. getAsync's promisified name is getAsyncAsync.\n\nApparently Bluebird changed this behaviour, and throws an exception now.. ",
    "nicogreenarry": "~Though, it looks like the promise() method doesn't exist on .upload, per this comment?~\nNever mind, s3.upload(params).promise() worked exactly as I'd hope!. ",
    "ronjouch": "@chrisradek regarding https://github.com/aws/aws-sdk-js/issues/2229#issuecomment-417386675 :\n\nIt shouldn't be necessary to promisify the SDK. If promises are available in your environment, any operation can return a promise by calling the promise() method on the operation's return value:\n```js\n// with callbacks\nsqs.listQueues(function(err, data) {});\n// using .promise\nsqs.listQueues().promise();\n```\n\nI initially liked that path, but after moving a chunk of existing code to .promise(), I'm going to stay with Bluebird,\n\nEither using a custom suffix other than Async...\n... or replacing promisifyAll with specific promisify calls (thus avoiding collisions when/if AWS introduces new foo/fooAsync APIs like done in https://github.com/aws/aws-sdk-js/commit/fe88308a8699b39aef06492c898b265a3a24251f with on/onAsync).\n\nBecause all things considered, promisifying callback-based APIs is a pattern I'm okay with, and AWS promise() sounds neat, but makes mocking a bit more painful and a lot uglier:\n```js\n// stubbing a promisified DynamoDB.DocumentClient\nsinon.stub(dynamo.documentClient, 'putAsync')\n     .resolves({ Attributes: response });\n// stubbing a DynamoDB.DocumentClient using promise()\nsinon.stub(dynamo.documentClient, 'put')\n     .returns({ promise: () => (new Promise(resolve => (resolve({ Attributes: response })))) });\n```. ",
    "mflores-verys": "@jasonfutch when did that change occur, do you know? . ",
    "ebrearley": "Thanks @jasonfutch. Moving back and pegging aws-sdk-js to version 2.304.0 is the perfect workaround for now.. ",
    "Palisand": "What's wrong with using the latest version of the sdk and a different suffix?\nbluebird.promisifyAll(AWSSQS, {suffix: 'Promise'})\n. What's wrong with using the latest version of the sdk and a different suffix?\nbluebird.promisifyAll(AWSSQS, {suffix: 'Promise'})\n. ",
    "ankon": "\nWhat's wrong with using the latest version of the sdk and a different suffix?\n\nThis would require existing code (using ...Async until now) to be modified, which clearly should not be the intent of a patch semver release.. ",
    "nghiadhd": "We can change suffix while doing promisifyAll\nPromise.promisifyAll(\n    Object target,\n    [Object {\n        suffix: String=\"Async\",\n        multiArgs: boolean=false,\n        filter: boolean function(String name, function func, Object target, boolean passesDefaultFilter),\n        promisifier: function(function originalFunction, function defaultPromisifier)\n    } options]\n) -> Object\nhttp://bluebirdjs.com/docs/api/promise.promisifyall.html\nI changed to 'Async2' and it works well. ",
    "marmuel": "I use the latest version. I\u00b4m using it in my typescript app and found out, that importing the sdk by \nimport * as AWS from 'aws-sdk';\ndeclare var AWS:any;\ndo not work for Comprehend. That is really strange because it works for other constructors/services like Polly. \nI import the now by: \nimport '../../../node_modules/aws-sdk/dist/aws-sdk';\nand it works!. ",
    "davidadas": "@srchase I'm using version 2.282.1.  And yes, we did try with the AWS CLI and it created the URL correctly. . @srchase I'm using version 2.282.1.  And yes, we did try with the AWS CLI and it created the URL correctly. . @srchase sure will try!  Thanks!. @srchase sure will try!  Thanks!. @srchase apologies for the delay.  I tried again and got the same error :( . . @srchase apologies for the delay.  I tried again and got the same error :( . . @srchase do you have the option to \"Restrict User Access\" enabled in  Behavior?\n\n?\nI am still getting \"403: MalformedUrl\" though.. @srchase do you have the option to \"Restrict User Access\" enabled in  Behavior?\n\n?\nI am still getting \"403: MalformedUrl\" though.. Okay, that fixed it @srchase!  Thanks a bunch!  Closing out this issue.. Okay, that fixed it @srchase!  Thanks a bunch!  Closing out this issue.. ",
    "emptyseth": "@srchase \naws-sdk: 2.309.0\nnode: 10.9.0\nI can successfully upload any file with s3cmd.. @srchase \nEverything works with AWS CLI. So Im also trying to find error in other parts of application.. ",
    "elliotthill": "Also when I do require the aws-sdk I build from source, I get this error:\nReferenceError: AWS is not defined\n    at Object.<anonymous> (/my/path/aws-sdk.js:12302:1)\nVery frustrating. I'm only concerned with node back end I'm doing nothing within the browser.\nI've built the new source with the above instructions but still getting:\nTypeError: AWS.Rekognition is not a constructor\nCan you spell out exactly what I need to do to get this to work please. I've tried overwriting the aws-sdk with aws-sdk-full within /dist and still the above error.. That code returns:\nTypeError: AWS.Rekognition is not a constructor\nMy aws-sdk was installed with npm install and I've run the above instructions to build all services into aws-sdk-full.\nInterestingly my aws-sdk-full has no references at all within it to 'rekognition'. Its obviously not packing that into it.. I uninstalled and reinstalled. Its working now.\nInteresting the version now is 2.309.0 but before it was 2.6.7 WTF?. How did I end up with version 2.6.7 when that doesn't even exist?\nhttps://www.npmjs.com/package/aws-sdk. ",
    "univerze": "Hi @srchase \nI have double checked the documentation and now it's clear why the API can't return an async error message. It works as expected.\nThanks!. ",
    "zxlin": "No, I don't have any other creds configured, e.g. aws configure, the code is verbatim, except the keys obviously\n\nFrom: Chase Coalwell notifications@github.com\nSent: Friday, September 7, 2018 1:17:05 PM\nTo: aws/aws-sdk-js\nCc: Zhi Xiang Lin; Author\nSubject: Re: [aws/aws-sdk-js] S3 constructor ignores accessKeyId and secretAccessKey in options hash (#2241)\nHow are you populating the accessKeyId and secretAccessKey?\nIf those are hardcoded strings (as you acknowledge is not recommended), this should be working.\nAre you doing anything else that would be trying to resolve credentials elsewhere in your code?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/aws/aws-sdk-js/issues/2241#issuecomment-419507420, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AFMgGTLE-1DPyY7mFHozDojLtODwW3iLks5uYqoRgaJpZM4Wd2Wo.\n. ",
    "abidingotter": "Hi Chris,\nThanks for your response.\nYes, I'm using webpack.\nInterestingly this works, as in, it doesn't throw an immediate exception: (thanks!)\nimport { AWSError, CognitoIdentityCredentials, config } from \"aws-sdk\";\nimport * as MediaConvert from \"aws-sdk/clients/mediaconvert\";\n\u2026\nconst endpointPromise = new MediaConvert({apiVersion: '2017-08-29'}).describeEndpoints(params).promise();\nand this doesn't: (yet using AWS.S3 works fine)\nimport * as AWS from \"aws-sdk\";\n\u2026\nconst endpointPromise = new AWS.MediaConvert({apiVersion: '2017-08-29'}).describeEndpoints(params).promise();\nAlthough I realised MediaConvert doesn't support CORS anyway, so using it in the browser is kind of fruitless :)\n. Hi Chris,\nThanks for your response.\nYes, I'm using webpack.\nInterestingly this works, as in, it doesn't throw an immediate exception: (thanks!)\nimport { AWSError, CognitoIdentityCredentials, config } from \"aws-sdk\";\nimport * as MediaConvert from \"aws-sdk/clients/mediaconvert\";\n\u2026\nconst endpointPromise = new MediaConvert({apiVersion: '2017-08-29'}).describeEndpoints(params).promise();\nand this doesn't: (yet using AWS.S3 works fine)\nimport * as AWS from \"aws-sdk\";\n\u2026\nconst endpointPromise = new AWS.MediaConvert({apiVersion: '2017-08-29'}).describeEndpoints(params).promise();\nAlthough I realised MediaConvert doesn't support CORS anyway, so using it in the browser is kind of fruitless :)\n. ",
    "hanginwithdaddo": "update: problem does not appear using typescript@3.0.3-insiders.20180829. I did not have time yesterday to look into this further yesterday, but will do so today. I believe all of the necessary dependencies are installed correctly (if they weren't upgrading to typescript@3.0.3-insiders.20180829 wouldn't have worked either). . @types/node is installed:\n$ ls -d node_modules/@types/node\nnode_modules/@types/node\n$ grep version node_modules/@types/node/package.json\n    \"version\": \"10.1.0\",\nHowever, I've run into what could be another compiler bug that is precluding me from continuing to test this issue. Let me see if I can produce a smaller example of this new issue. If it turns out to be a compiler bug and not my code, I'll submit a new issue and link this ticket to the new one.\n. Based on one of the other problems I was running into, this appears to be a problem in compatibility between typescript and tslint versions. Using typescript 3.0.3 requires running tslint 5.11.0. I had tslint 5.10.0 installed. Closing this ticket. Unfortunately, with the error that was reported,  there was no context indicating that this could have been a problem with tslint.. My apologies. Thanks for closing.. ",
    "OriginUnknown": "Dude! You are a legend! I spent a day on this and it drove me to near insanity! Massive thanks again :). ",
    "pabloDon": "It works... looks like base64 is encoded wrongly, who knows. Thanks!. ",
    "sriramHaven": "@srchase  the command that I have run is for mocha unit test. It will be npm run coverage:ci\n\"coverage:ci\": \"nyc --nycrc-path ./.nycrc-ci npm run test\",\n    \"test\": \"mocha --require source-map-support/register ./dist-server/test/unit/**/*.js --recursive\",\nis there a particular section of package.json , you would like to see ?\n. @srchase this error comes out when we include aws-sdk in our .ts files, which is imported when the unit test runs. \n. @srchase its also worth to note that , this works totally fine with Node 8.11.4 . @chrisradek These are the following steps i did\nAdd the following in one of the controllers that will be imported while running the mocha tests. \nThese are from .ts format which gets transpiled to .js before running tests.. i believe it should happen if you do it in .js\nimport * as AWS from \"aws-sdk\";\nin the constructor or a init() of controller.ts\nlet a = new AWS.S3();\n        console.log(a);\nthe command I have run is\nnpm run tsc && npm run test\nDefinition from package.json for them\n\"tsc\" : tsc\n\"test\": \"mocha --require source-map-support/register ./dist-server/test/unit/**/*.js --recursive\",\nVersions of mocha and source-map-support\n\"mocha\": \"5.2.0\",\n   \"source-map-support\": \"0.5.3\",\n. ",
    "fyn-dev": "@srchase \nPlease provide example code how to achieve same result using aws-sdk? I can't find any example in entire internet.. ",
    "PinnacleOne": "@srchase but anyone tested this signature generation with wss uri?\n\nwss://endpoint_name.iot.region.amazonaws.com/mqtt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX&X-Amz-Date=20180920T092557Z&X-Amz-Expires=172800&X-Amz-SignedHeaders=host&X-Amz-Signature=e4f6d25ecca833052b7858c26539e524080f99389cc88dbcb6d5c35c36ca895a\n\nWhy X-Amz-Expires doesn't work? Is problem with this third party library or just AWS service related to wss and iot has issue? Anyone can help investigate this please? . ",
    "armorgreg": "@srchase I did another git clone of my repo and it seems to be working now. Not sure what happened there. Thanks for the response. . ",
    "rajeshbala01": "Error: Cannot find module 'jmespath'\nError: Cannot find module 'jmespath'\n    at Function.Module._resolveFilename (internal/modules/cjs/loader.js:581:15)\n    at Function.Module._load (internal/modules/cjs/loader.js:507:25)\n    at Module.require (internal/modules/cjs/loader.js:637:17)\n    at require (internal/modules/cjs/helpers.js:22:18)\n    at Object. (C:\\Program Files\\Git\\aws-sdk-js-master\\lib\\request.js:5:16)\n    at Module._compile (internal/modules/cjs/loader.js:689:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:700:10)\n    at Module.load (internal/modules/cjs/loader.js:599:32)\n    at tryModuleLoad (internal/modules/cjs/loader.js:538:12)\n    at Function.Module._load (internal/modules/cjs/loader.js:530:3)\nI am still not able to run the command which build aws-sdk.js file. ",
    "Dom-HMH-MTL": "Sure, here is the set of Typescript functions I rely on. The one directly called is configureAWS() with the corresponding parameters (region, endpoint, and credentials). The role has the format arn:aws:iam::<accountId>:role/<customName>.\n``` typescript\nprivate async configureAWS(config: ServiceConfigurationOptions, applicationRole: string): Promise {\n    if (applicationRole) {\n        if (!(await this.isRoleAssumed(config, applicationRole))) {\n            await this.assumeRole(config, applicationRole);\n        }\n    }\n    AWSConfig.update(config);\n}\nprivate async isRoleAssumed(config: ServiceConfigurationOptions, applicationRole: string): Promise {\n    return this.getSTS()\n        .getCallerIdentity({})\n        .promise()\n        .then(\n            (data: any): boolean => {\n                // tslint:disable-next-line: no-console\n                console.log('STS.getCallerIdentity():\\n', data);\n                return data.Arn === applicationRole;\n            }\n        )\n        .catch(\n            (error: any): boolean => {\n                // tslint:disable-next-line: no-console\n                console.log('STS.getCallerIdentity() Error!\\n', error);\n                if (error.code === 404) {\n                    return true; // FIXME: understand why the sts role is returned with an error 404 when the doc (https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/STS.html#getSessionToken-property) says it should be successfully returned.\n                }\n                throw error;\n            }\n        );\n}\nprivate async assumeRole(config: ServiceConfigurationOptions, applicationRole: string): Promise {\n    return this.getSTS()\n        .assumeRole({ RoleArn: applicationRole, RoleSessionName: 'CrossAccountCredentials' })\n        .promise();\n}\nprivate getSTS(): STS {\n    return new STS();\n}\n```. Re: how are you applying the credentials...\nAs you can see in the code above, I don't do anything other than calling assumeRole(). I did not find a documentation explaining their usage per-se... If I don't call assumeRole(), the access to the DynamoDB instance on the other account fails. When I call it the first time, the call is successful and I can access the DynamoDB on another account. It returns the subsequent error 404 after that initial call. I does not seems to have an effect (the call to the remote DynamoDB are fine, AFAIK) but I don't like to have let the misunderstood error incorrectly handled... That's why I try to rely on getCallerIdentity() first, but it has the same behaviour.\nMaybe the effect of the role endorsement is managed by the environment my application is deployed on... Going to follow-up with our tech team here.. Yes, I do see the 404 error MalformedQueryString.\nI don't see the query as it is built internally in the aws-sdk library...\nI'm going to give a try tracking the emitted HTTP request. Any direction will be appreciated.. I've tried to get more information at the request level. So far the main difference I can see is located in the endpoint which is switched from sts.amazon.com to dynamodb.us-east-1.amazonaws.com when it's always the same sequence of new STS().getCallerIdentity()...\nAny idea on this URL switch? A cache incorrectly updated? An unexpected interaction in a cache induced by sequences of new STS(), new DynamoDB(config), new DocumentClient(), etc.?\nNote that the URL change explains the 404-Not found response produced inside the API...\nThanks, Dom\nAt the request level, the header list:\njavascript\n{\n        protocol: 'https:',\n        host: 'dynamodb.us-east-1.amazonaws.com',\n        port: 443,\n        hostname: 'dynamodb.us-east-1.amazonaws.com',\n        pathname: '/',\n        path: '/',\n        href: 'https://dynamodb.us-east-1.amazonaws.com/',\n        constructor: [Function] \n}\nversus\njavascript\n{\n        protocol: 'https:',\n        host: 'sts.amazonaws.com',\n        port: 443,\n        hostname: 'sts.amazonaws.com',\n        pathname: '/',\n        path: '/',\n        href: 'https://sts.amazonaws.com/',\n        constructor: [Function] \n}\nIn the response body with status 200-OK:\nxml\n<GetCallerIdentityResponse xmlns=\"https://sts.amazonaws.com/doc/2011-06-15/\">\n  <GetCallerIdentityResult>\n    <Arn>arn:aws:sts::711638685743:assumed-role/brnpb-mesos-agent-nonprod-standard/i-0223bd038256285e7</Arn>\n    <UserId>AROAJPRFIDDJGV2KE6ZFW:i-0223bd038256285e7</UserId>\n    <Account>711638685743</Account>\n  </GetCallerIdentityResult>\n  <ResponseMetadata>\n    <RequestId>f37649cb-c037-11e8-97de-e7f795926c0f</RequestId>\n  </ResponseMetadata>\n</GetCallerIdentityResponse>\nversus the response body with the status 404-Not Found:\nhtml\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n<head>\n  <title>Page Not Found</title>\n</head>\n<body>Page Not Found</body>\n</html>. it appears that relying on a instanceSTS initialized only once (instead of calling new STS() every time, as illustrated in my comment above) fixed the issue...\nNow, there is no more endpoint mixed-up, and the role can be applied many times. For all subsequent call, the STS.getCallerIdentity() always returns the same payload (that is with a non matching Arn value) so the code is always triggering STS.assumeRole(). It seems that the applied role information is lost after the interaction with the DynamoDB instance belonging to another AWS account, but I don't know enough the intrinsics of the library to get a clear reason...\nI'm going to close this issue but if someone can confirm the impact of the new STS() every time ;). ",
    "travisbt": "Still debugging, appears as though going from the send to validationResponse states in the ASM, is failing, and failing in a place where the error is not bubbled up. \nStill looking (as best I can).  Any insight into the issue from the AWS team would be \ud83d\udc4d \nThanks guys!. ",
    "jhappoldt": "@travisbt can you give any guidance on how you fixed the issue?  We're seeing the same behavior locally and have not been able to identify a workaround.\nThanks!. ",
    "jstarry": "@travisbt @jhappoldt  Not sure if related, but I have been trying to track down a similar issue. In our case we sometimes see SecretsManager fail silently but sometimes will succeed just fine. I added some extra options to the SecretsManager client like so:\nhttpOptions: {\n        connectTimeout: 5000\n      },\n      logger: global.console,\n      maxRetries: 3,\nWith these options, it was clear that it was failing because it was unable to establish a connection. Still trying to figure out why that's happening ;) . ",
    "Wolfie010": "Are you running this in a VPC? If so you might need to set an endpoint up, I ran into this issue when trying to access SecretsManager from inside of a VPC. (Sorry if this is too short of a message) . ",
    "jadbox": "SDK version was 2.316 I think. The commands I was using are listed here (with the permissions set the same as also prescribed in the gist):\nhttps://gist.github.com/pveller/fc7660bdfaf19eed4b29b2e9415c3917. @srchase I'm not able to test this anymore as I've had to opt for an alternative solution, but may be able to circle back to this in a few weeks. You're free to close this if you feel strongly that #2329 should fix the issue. As I mentioned on that ticket, I would highly recommend making the CORS header work without OPTION negotiation as it makes debugging non-transparent, and it's rather unorthodox to how CORs typically works in my experience (decoupled from authorization).. @srchase I would highly recommend making the CORS header work without OPTION negotiation. It masks the actual problem and makes it a headache to debug if there's like a call parameter is misspelled or missing.. ",
    "SnooHD": "Hello, thanks for your reply.\nThe SDK im using is: 2012-08-10.\nThe code im using is split in multiple function that return the query based on a simple input..\nIf you make an API request like this: {test: 'hi'} it will make a getItem request.\nIf you make an API request like this: [{test: 'hi'}, {test: 'h2'}] it will make a batchGetItem request.\nThe query that it returns is correct, as it also works with another table im using.\nThe final query it returns would be something like this:\n{\n   query:{\n      RequestItems: {\n         paths: {\n            Keys: [{\n               test: 'hi'\n            },{\n               test: 'hi1'\n            }]\n         }\n      }\n   },\n   type: 'batchGet'\n}\nThe final code that runs to fetch the actual response:\n```\nexport const runQuery = async (queryData) => {\n    let data;\n    try{\n      data = await dbqueryData.type.promise();\n    }catch(error){\n      throw Error(error);\n    }\nlet response;\ntry{\n  response = await getters.getResponse(queryData.type, data, queryData.query);\n}catch(error){\n  throw Error(error);\n}\n\nreturn response;\n\n}\n```\ngetResponse is where i check the response that is returned, if it contains the key that contains id's for another request, i simply trigger the function again to build the query and then run it.. When i run a getItem call it works as expected.\nWhen i run the batchGetItem with multiple keys i get the empty response.\nAs for running specific calls;\n- I tried running a batchQuery with only 1 item, and this also did not work.\n- A batchQuery on another table works as expected..\n- getItem calls do return the items as they should\n- Query calls work.\n- Scan also as expected.\nIf there are any other calls you want me to try, please let me know.\nRight now i changed my code to fetch all item's i need with multiple getItem calls, which i then merge manually.\nBut this is of course not ideal.. I found a stackoverflow question about this as well.\nI am afraid it was not resolved, but at least it shows im not the only one.\nhttps://stackoverflow.com/questions/46913376/why-is-batchget-not-showing-any-records-in-the-response\n@rupali317 did you manage to find a solution for this issue? Any insight?. @srchase this is the table that works: (tags)\n\nThis is the table that does NOT work: (paths)\n\n. @srchase AWS CLI only returns the LAST item in the request.\nso the result is not the same, but also wrong.\nTable:\n\nQuery:\n{\n  \"paths\": {\n    \"Keys\":[{\n      \"_id\": {\"S\":\"4ec43e9c9a95abcfd928f166f61a623d016c5251\"},\n      \"_id\": {\"S\":\"c40f9cccb8625c42f0732cea777237b091db2864\"}\n      }]\n  }\n}\nResult:\n\n. @srchase I just noticed that my query in the previous post was wrong! (I only had 1 object inside the array).\nIt was supposed to be like this: \n{\n  \"paths\": {\n    \"Keys\":[\n      {\"_id\": {\"S\":\"4ec43e9c9a95abcfd928f166f61a623d016c5251\"}},\n      {\"_id\": {\"S\":\"c40f9cccb8625c42f0732cea777237b091db2864\"}}\n      ]\n  }\n}\nNow the CLI DOES return the correct result!. @srchase This is for S3 right?\nI'm not sure if i'm following the magic relation with DynamoDB here.. @srchase I tried running this directly:\n```\n  let test;\n  try{\n    test = await db.batchGet({\"RequestItems\":{\n      \"paths\": {\n        \"Keys\":[\n          {\"_id\": \"4ec43e9c9a95abcfd928f166f61a623d016c5251\"},\n          {\"_id\": \"c40f9cccb8625c42f0732cea777237b091db2864\"}\n        ]\n      }\n    }}).promise();\n  }catch(error){\n    console.log(error);\n  }\nconsole.log(test);\n```\nAnd the response is correct!?\nSo i changed my code back to the way it was originally.. and all works as expected now..\nThis seems very weird to me.. Any clue whats going on here?\nAt least i'm happy to say that its working correctly now!. ",
    "jhohlfeld-otto": "bash\n% grep version node_modules/aws-sdk/package.json \n  \"version\": \"2.307.0\"\nI was using 2.307.0. I'll try aws-sdk@2.321.0. Sorry, I thought I was using the most recent version.. Thanks a lot, bumping the version solved it \ud83d\ude44\ud83e\udd26\u200d\u2642\ufe0f\ud83d\ude05. ",
    "againksy": "@srchase  res is usual express res,  function(req, res, next). ",
    "holmberd": "You can set the Content-Disposition header on your response before piping it.\nres.header('Content-Disposition', 'attachment; filename=\"' + key'\"');. ",
    "rkulla": "Meant this for the aws-sdk-mock github repo. Sorry.. ",
    "blakewilson": "Awesome work! Thanks for the quick response.. ",
    "dppower": "@srchase \nThanks, I was thinking it might be to do with the version lambda uses. Calling the API directly like the code I show above works, so I'll keep using that for now.. ",
    "jpike88": "\n. VSCode. ",
    "ChenFeldman": "Hey @srchase \nThank you for trying to help.\nI tried running you code with lambda but now it is also not putting an empty object after using putObject.\nI am trying to figure out what am I missing here?\n. @srchase Will try it now and get back to you here. So after some more tries I was able to solve the issue.\nHere is part of the code:\n```\nasync function upload(fileStream, fileName, bucketName) {\n      let params = {\n        Body: fileStream,\n        Key: fileName,\n        Bucket: bucketName\n      };\n      await s3.client.putObject(params).promise();\n    }\nmodule.exports.handler = async(event, context) => {\n      try {\n        let s3UploadParams = {\n          uri: imageDownloadUrl,\n          encoding: null\n        };\n        let imageFileStream = await request(s3UploadParams);\n        await upload(imageFileStream, s3FileName, bucketName);\n      } catch (err) {\n        context.fail(null, 'Error trying to upload to aws' + err);\n      }\n    }\n```\nInstead of using 'request' lib I am using 'request-promise-native' to get the stream from the url. (thanks for that @srchase )\nAnd also using the .promise() of aws-sdk library to make it fully work.. ",
    "AntonSmatanik": "That is nice, but is there some example how to set device with secrets and certificates? Need to use it with broker.. ",
    "jaisrael1": "All 3 environment variables are being properly loaded in the script. \nWhen I type 'aws configure list' in cmd, it prints:\nName                    Value             Type                  Location\n      ----                    -----             ----                   --------\n   profile               MFAProfile      manual             --profile\naccess_key           [MYKEY]          shared-credentials-file\nsecret_key           [MYSECRET]     shared-credentials-file\n    region                us-east-1      config-file  \\Users\\Jake\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\jaisrael\\.aws\\config\nSo that path is the location of the config file that the region is being loaded from. It is stored in the AWS_CONFIG_FILE environment variable.\nI found a workaround, I can manually set 'AWS.config.credentials = new AWS.SharedIniFileCredentials({ filename: process.env.AWS_SHARED_CREDENTIALS_FILE});'\nAs long as I put  'const S3 = new AWS.S3()' after the above line, everything works.\nSo because I can load the creds from the shared credentials file manually and everything appears to work, it looks like it's just not doing this automatically. . That fixed the problem, many thanks!. ",
    "DoodahProductions": "@AllanFly120  It's not a package dependency of jmespath. When running npm install, the jmespath dependency create an index.html file that will require jquery.. @AllanFly120  It's not a package dependency of jmespath. When running npm install, the jmespath dependency create an index.html file that will require jquery.. \n. \n. @srchase \nHello,\nThanks for your response. I created a issue report on jmespath repos as you suggested, but the repo seems unmaintained (3 years without updates).\nIs there a chance that you could fix it on your side?\nThanks.. @srchase \nHello,\nThanks for your response. I created a issue report on jmespath repos as you suggested, but the repo seems unmaintained (3 years without updates).\nIs there a chance that you could fix it on your side?\nThanks.. Hello @srchase \nI mean by replacing that unmaintained dependency by another for exemple.\nThanks for contacting the maintainer of jmespath. :)\nHave a nice day.\n. Hello @srchase \nI mean by replacing that unmaintained dependency by another for exemple.\nThanks for contacting the maintainer of jmespath. :)\nHave a nice day.\n. ",
    "ztzven": "Currently using typescript v3.1.0.. types seemed to worked with v2.282.1 however a similar definition using SQS seems to be broken using v2.282.1.\nCould the issue be local?. Currently using typescript v3.1.0.. types seemed to worked with v2.282.1 however a similar definition using SQS seems to be broken using v2.282.1.\nCould the issue be local?. Ahh found it, typescript version wasn't the issue.\nThe module that Notification resided in specified sdk v2.298.0, yet the project used v2.330.0.\nThis seemed to cause type incompatibilities.\nShould be okay to mark as closed.. Ahh found it, typescript version wasn't the issue.\nThe module that Notification resided in specified sdk v2.298.0, yet the project used v2.330.0.\nThis seemed to cause type incompatibilities.\nShould be okay to mark as closed.. ",
    "hemanth-sp": "i am getting file uploaded successfully with 200 good request and body is null, is content type really matter here ? file size is 8.0 bytes in s3. curl working fine.. do you have any tutorials on angular v2,v4.v5.v6 how to do in presingedurl file upload. ok i will ask rest of the question to stack overflow it seems to difficult for me to implement. thank you for your huge help..... my file reference was wrong. it should be File[0] like this my problem is solved. ",
    "rameshSolutionFuse": "Same issue i'm facing, any help?. ",
    "gatsbyz": "This is a question.. @srchase Hi. If I do node index.js (the webpack output file), I get an error saying that aws-sdk doesn't exist. Since lambda has aws-sdk built in, I assumed if I upload the zipped file to lambda, it would understand it. However, I'm getting an error in Lambda CloudWatch console saying Unable to import module 'index'. @srchase I am zipping the contents of the directory. When I zip the directory, I get an error on the AWS  Console UI. \n\n. \n This is what it looks like now. When I include new webpack.IgnorePlugin(/aws-sdk/) in webpack configuration, it fails. (When I take out this config, it works) Below is the CloudWatch logs. \n\n. Logs when I upload the zip with aws-sdk in it. \n\n. @srchase Sorry for the confusion. When I ignore aws-sdk, it is failing. I assume it should work since lambda has this built in. I'm trying to reduce package size.. @srchase Yeah I just wanted to show you that zipping the directory and uploading gives me a different error than what I'm getting now. And yes, the directory IS the same in the case of having and not having aws-sdk included.. @srchase I guess my final question is that, do I need some annotation or configuration to tell lambda to include aws-sdk from it's built-in?. Thank you so much. You're the best. It's funny to know that it was a built-in attribute all along. Doi.. Try it like \n```\n      if (environment.AWS_ACCESS_KEY_ID && environment.AWS_SECRET_ACCESS_KEY) {   \n         awsConfigurationParmeters.httpOptions = customHttpOptions;\n         aws.config.update({  \n            credentials: new aws.Credentials(\n               environment.AWS_ACCESS_KEY_ID, \n               environment.AWS_SECRET_ACCESS_KEY) \n         });\n      }\n      const awsConfig = new aws.Config();\n      awsConfig.update(awsConfigurationParmeters);\n```. I had to remove env credential configuration - sdk handles this by default and its use breaks lambda usage. Hi @srchase. The reason I was avoiding this option was that it causes a race condition with aws usage. . Thanks so much for that reference! \naws.config = new aws.Config(awsConfigurationParmeters);\nreturn aws;\nThis is working for now.. ",
    "bamapookie": "Noooooooo!  Intermittent problem!  I tried it again in another environment with the latest version, 2.334.0.  It succeeded once and failed once.  (I am updating a number of sibling stacks, and it stops when one fails.)  I then rolled back to 2.293.0 and tried again.  Failed.  Then I rolled back to 2.292.0.  Succeeded twice, then failed.  All of these sibling stacks are essentially the same.  All the CF stacks complete just fine, ending up in UPDATE_COMPLETE status with no errors, but the javascript seems to think that they are failing.. Here's a little serverless handler I threw together to test this:\n```\n'use strict';\nconst AWS = require('aws-sdk'),\n  CF = new AWS.CloudFormation(),\n  UUID = require('uuid/v4');\nmodule.exports.createStack = async (event) => {\nlet stackName = TempStack-${UUID()};\n  console.log(Stack name: ${stackName});\n  await CF.createStack({\n    StackName: stackName,\n    DisableRollback: false,\n    EnableTerminationProtection: false,\n    TemplateBody: AWSTemplateFormatVersion: '2010-09-09'\nResources:\n  SampleResource:\n    Type: AWS::CloudFormation::WaitConditionHandle\n    Metadata:\n      Comment: Dummy resource\n      StackTime: ${new Date().getMilliseconds()}}).promise()\n    .then(() => console.log('Stack creation started.'))\n    .then(CF.waitFor('stackCreateComplete', {StackName: stackName}).promise())\n    // .then(() => console.log('Stack creation completed.'))\n    // .then(updateStack)\n    // .then(() => console.log('First stack update completed.'))\n    // .then(updateStack)\n    // .then(() => console.log('Second stack update completed.'))\n    // .then(updateStack)\n    // .then(() => console.log('Final stack update completed.'))\n  ;\n  return {\n    statusCode: 200,\n    body: JSON.stringify({\n      message: 'Stack deploy succeeded!',\n      input: event\n    })\n  };\n};\nasync function updateStack(stackName) {\n  return CF.updateStack({\n    StackName: stackName,\n    TemplateBody: AWSTemplateFormatVersion: '2010-09-09'\nResources:\n  SampleResource:\n    Type: AWS::CloudFormation::WaitConditionHandle\n    Metadata:\n      Comment: Dummy resource\n      StackTime: ${new Date().getMilliseconds()}\n  }).promise()\n    .then(CF.waitFor('stackUpdateComplete', {StackName: stackName}).promise())\n}\n``. I can reproduce it pretty reliably with that code.  It always creates the stack.  If I uncomment the waitfor's, they always fail.. I left out some() => `'s in front of the waitFor calls.  Now I can't get it to fail, even though I haven't changed my actual code that was failing.. That was just another way of making the call a properly chained promise.\nIt was different from the way I have it implemented in the private code\nthat was failing.  Either way, I can't get the old code to fail now.\nOn Wed, Oct 17, 2018 at 1:54 PM Chase Coalwell notifications@github.com\nwrote:\n\nDoes the change I suggested also work for you?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/2301#issuecomment-430726111,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AB1fPt_nijQAauVyNxV09ccC-DDrUa5Yks5ul27fgaJpZM4XdBHA\n.\n. \n",
    "neoacevedo": "mmm, too strange. Just removing the x-amz-date field from formData did the trick:\njs\n        var fileKey = document.getElementById('key').value;\n        var form = new FormData();\n        form.append(\"key\", fileKey);\n        form.append('acl', document.getElementById('acl').value);\n        form.append(\"x-amz-algorithm\", document.getElementById('algorithm').value);\n        form.append(\"x-amz-date\", document.getElementById('date').value);\n        form.append(\"x-amz-credential\", document.getElementById('credentials').value);\n        //form.append(\"x-amz-date\", document.getElementById('date').value); <-- commented it and worked.\n        form.append(\"policy\", document.getElementById('policy').value);\n        form.append(\"x-amz-signature\", document.getElementById('signature').value);\n        form.append(\"success_action_redirect\", \"\");\n        form.append(\"file\", file);\nNo, I'm not using the SDK at all, I have followed the AWS guide https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-HTTPPOSTConstructPolicy.html \nI will close this issue.. ",
    "m-athar": "Hi @srchase  ,\nAppreciate your response. \nI'm using node js.  Also the version of aws-sdk is: ^2.88.0. \nMost of the time the code is working perfectly fine but sometimes it starts giving this error, which i have mentioned above. . I'm using 10 seconds of time out. Let me remove the content-types , run it and test it again. Will update on this thread.  . @srchase  Hey, \nI removed the content type and it gave like the previous mention error 1 time out of like 60 times. Which is some progress I guess,  but I need to specify the content type so Images on the AWS is properly saved along with specific content type. Is there any other way around this error??  . I increased the expiration time to 1000 seconds but still getting the same 403 and 404 error as described above. . ",
    "ajhstn": "Thanks I believe I have the same issue. ",
    "woodpav": "\"@aws-amplify/api\": \"^1.0.18\",\n\"@aws-amplify/auth\": \"^1.2.4\",\n\"@aws-amplify/core\": \"^1.0.13\",\n\"@babel/runtime\": \"^7.1.2\",\n\"amazon-cognito-identity-js\": \"^3.0.3\",\n\"aws-sdk\": \"^2.198.0\",\n\"babel-plugin-transform-remove-console\": \"^6.9.4\",\n\"buffer\": \"^5.2.1\",\n\"extensible-runtime\": \"^4.1.0\",\n\"react\": \"^16.7.0-alpha.0\",\n\"react-native\": \"^0.57.4\",\n\n// dev\n\"@babel/plugin-external-helpers\": \"^7.0.0\",\n\"@babel/plugin-proposal-class-properties\": \"^7.1.0\",\n\"@babel/plugin-transform-runtime\": \"^7.1.0\",\n\"babel-core\": \"7.0.0-bridge.0\",\n\"babel-jest\": \"^23.6.0\",\n\"babel-preset-flow\": \"^6.23.0\",\n\"metro-react-native-babel-preset\": \"^0.48.2\",\n\"schedule\": \"0.4.0\",\n\"xmlhttprequest\": \"^1.8.0\"\n. For the record, I am no longer getting this error. I'm not sure what changed \ud83d\ude05.\n",
    "EliSadaka": "I'm also experiencing this error after updating my React Native project from 0.55 to 0.57. If I comment out all the references to the aws-sdk package from my project then it runs fine.\nHere are my dependencies:\n\"dependencies\": {\n    \"@babel/runtime\": \"^7.3.1\",\n    \"@feathersjs/authentication-client\": \"^1.0.8\",\n    \"@feathersjs/feathers\": \"^3.2.3\",\n    \"@feathersjs/socketio-client\": \"^1.1.5\",\n    \"autolinker\": \"^1.7.1\",\n    \"aws-sdk\": \"^2.392.0\",\n    \"lodash\": \"^4.17.11\",\n    \"mobx\": \"^5.9.0\",\n    \"mobx-react\": \"^5.4.3\",\n    \"moment\": \"^2.22.2\",\n    \"prop-types\": \"^15.6.2\",\n    \"react\": \"16.6.3\",\n    \"react-native\": \"0.57.8\",\n    \"react-native-animatable\": \"^1.3.1\",\n    \"react-native-background-upload\": \"^4.4.0\",\n    \"react-native-firebase\": \"^5.2.1\",\n    \"react-native-gesture-handler\": \"^1.0.10\",\n    \"react-native-keyboard-input\": \"^5.3.1\",\n    \"react-native-linear-gradient\": \"^2.5.3\",\n    \"react-native-navigation\": \"^2.8.0\",\n    \"react-native-tab-view\": \"^1.3.1\",\n    \"react-native-text-size\": \"^3.0.0\",\n    \"react-native-video\": \"^4.3.1\",\n    \"recyclerlistview\": \"^2.0.0-beta.1\",\n    \"rn-fetch-blob\": \"^0.10.15\",\n    \"shortid\": \"^2.2.14\",\n    \"socket.io-client\": \"2.1.1\"\n  },\n  \"devDependencies\": {\n    \"@babel/core\": \"^7.2.2\",\n    \"@babel/plugin-proposal-decorators\": \"^7.3.0\",\n    \"@babel/plugin-transform-runtime\": \"^7.2.0\",\n    \"babel-eslint\": \"^10.0.1\",\n    \"babel-jest\": \"23.6.0\",\n    \"babel-plugin-transform-remove-console\": \"^6.9.4\",\n    \"eslint\": \"^5.12.1\",\n    \"eslint-plugin-react\": \"^7.12.4\",\n    \"jest\": \"23.2.0\",\n    \"metro-react-native-babel-preset\": \"^0.51.1\",\n    \"react-test-renderer\": \"16.6.3\"\n  },\nAnd here is my .babelrc:\n{\n    \"presets\": [\"module:metro-react-native-babel-preset\"],\n    \"ignore\": [\"node_modules/aws-sdk/dist/aws-sdk-react-native.js\"],\n    \"env\": {\n        \"production\": {\n            \"plugins\": [\n                [\n                    \"@babel/plugin-proposal-decorators\",\n                    {\n                        \"legacy\": true\n                    }\n                ],\n                \"@babel/plugin-transform-runtime\",\n                \"transform-remove-console\"\n            ]\n        },\n        \"development\": {\n            \"plugins\": [\n                [\n                    \"@babel/plugin-proposal-decorators\",\n                    {\n                        \"legacy\": true\n                    }\n                ],\n                \"@babel/plugin-transform-runtime\"\n            ]\n        }\n    }\n}\nEDIT: Changing all appearances of Buffer.TYPED_ARRAY_SUPPORT = global.TYPED_ARRAY_SUPPORT !== undefined\n      ? global.TYPED_ARRAY_SUPPORT\n      : typedArraySupport() to Buffer.TYPED_ARRAY_SUPPORT = typedArraySupport() in node_modules/aws-sdk/dist/aws-sdk-react-native.js does resolve the error, but I'm not sure what other unintentional side-effects it might have.. ",
    "sebasmurphy": "Will do. Thanks for the heads up @AllanFly120 . ",
    "Rishi74744": "@srchase I tried removing endpoint still got the same error.. @srchase I got this working by adding sessionToken while creating iot object.\nlet iot = new Iot( {\n            endpoint: 'iot.ap-south-1.amazonaws.com',\n            accessKeyId: credentials.accessKeyId,\n            secretAccessKey: credentials.secretAccessKey,\n            sessionToken: credentials.sessionToken\n} );. ",
    "juanlet": "The problem was really silly. In the bucket I had face1.jpg and a face.png and I was calling both with jpg like  \n```\n    var params = {\n      SimilarityThreshold: 90, \n      SourceImage: {\n      S3Object: {\n        Bucket: \"reconfaces\", \n        Name: \"face1.jpg\"\n      }\n      }, \n      TargetImage: {\n      S3Object: {\n        Bucket: \"reconfaces\", \n        Name: \"face2.jpg\"   }\n      }\n    };\n```\nafter correcting the face2.jpg with face2.png I got the proper response:\n ```\n\nvar params = {\n          SimilarityThreshold: 90, \n          SourceImage: {\n          S3Object: {\n            Bucket: \"reconfaces\", \n            Name: \"face1.jpg\"\n          }\n          }, \n          TargetImage: {\n          S3Object: {\n            Bucket: \"reconfaces\", \n            Name: \"face2.png\"   }\n          }\n        };\n```\nResponse:\n{\n    \"SourceImageFace\": {\n        \"BoundingBox\": {\n            \"Width\": 0.48317307233810425,\n            \"Height\": 0.6442307829856873,\n            \"Left\": 0.2584134638309479,\n            \"Top\": 0.18910256028175354\n        },\n        \"Confidence\": 99.9949722290039\n    },\n    \"FaceMatches\": [\n        {\n            \"Similarity\": 98,\n            \"Face\": {\n                \"BoundingBox\": {\n                    \"Width\": 0.2640642821788788,\n                    \"Height\": 0.2769230902194977,\n                    \"Left\": 0.16237494349479675,\n                    \"Top\": 0.29230770468711853\n                },\n                \"Confidence\": 99.84500122070312,\n                \"Landmarks\": [\n                    {\n                        \"Type\": \"eyeLeft\",\n                        \"X\": 0.2420874983072281,\n                        \"Y\": 0.3932344913482666\n                    },\n                    {\n                        \"Type\": \"eyeRight\",\n                        \"X\": 0.3406614661216736,\n                        \"Y\": 0.3887109160423279\n                    },\n                    {\n                        \"Type\": \"nose\",\n                        \"X\": 0.3142981231212616,\n                        \"Y\": 0.4448704421520233\n                    },\n                    {\n                        \"Type\": \"mouthLeft\",\n                        \"X\": 0.2604469358921051,\n                        \"Y\": 0.5106690526008606\n                    },\n                    {\n                        \"Type\": \"mouthRight\",\n                        \"X\": 0.3448459208011627,\n                        \"Y\": 0.5012921690940857\n                    }\n                ],\n                \"Pose\": {\n                    \"Roll\": -3.831692695617676,\n                    \"Yaw\": 14.12887954711914,\n                    \"Pitch\": 3.8166630268096924\n                },\n                \"Quality\": {\n                    \"Brightness\": 82.77690887451172,\n                    \"Sharpness\": 89.91268920898438\n                }\n            }\n        }\n    ],\n    \"UnmatchedFaces\": [],\n    \"SourceImageOrientationCorrection\": \"ROTATE_0\",\n    \"TargetImageOrientationCorrection\": \"ROTATE_0\"\n}\n\n```\nSo to wrap up, CHECK THAT THE NAMES OF THE FILES MATCHES THE ONES IN THE BUCKET YOU ARE TARGETING. I guess I have to get some sleep xD. ",
    "stevewillard": "Ok that makes sense. I'll see if I can switch to an HTTPS proxy. Thanks!. ",
    "riwu": "@srchase sorry didn't have time to respond, can you please reopen this?\nAdding either \naws.config.update({\n  credentials: new aws.Credentials(process.env.S3_ACCESS_KEY_ID, process.env.S3_ACCESS_KEY_SECRET),\n});\nor\naws.config.loadFromPath('./config.json');\nor setting AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables fix the problem.\nNot sure why this alone does not work:\nnew aws.S3({\n  accessKeyId: process.env.S3_ACCESS_KEY_ID,\n  secretAcessKey: process.env.S3_ACCESS_KEY_SECRET,\n});. Figured out the problem...had a typo secretAcessKey instead of secretAccessKey.. ",
    "Qix-": "\nwill take this as a feature request\n\nPlease file it internally as a bug, not a feature request.. > will take this as a feature request\nPlease file it internally as a bug, not a feature request.. ",
    "gsuess": "@AllanFly120 \nI agree, it's a bug and not a feature.. ",
    "chrisfowler": "ARNs for APIGatway may not have account ID specified: https://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html\nSo that was not the root fo my issue. My Role Policy was incorrect. Updated to remove the account id from resource ARN and all worked.\n. ",
    "rohit1018": "@srchase \nlet params = {\nBucket: s3environment.Bucket,\nPrefix: 'folderName/'+ siteList.name\n};\ns3.listObjects(params).promise();. No,\ngoogle chrome 70.0.3538.77 \nI am able to successfully use listObjects with version 2.349.0 with Node and the browser.\nAngular v7.0.3 and typescrpit 3.1.6\n. v2.352.0 !! added new feature !! this is breaking s3 listObject function and or more\n\"feature: Endpoint Discovery: Some services provide endpoint discovery operations (e.g. 'DescribeEndpoints()'). This SDK feature, if turned on, will automatically request endpoints from services if needed and cache the endpoints returned.\"\nhttps://github.com/aws/aws-sdk-js/blob/master/CHANGELOG.md. ",
    "Temkit": "Same issue ! i can confirm that downgrading to \"aws-sdk\": \"2.351.0\", make it works fine.\n. Angular v7.0.3 and typescrpit 3.1.6. listobject not working when aws-sdk versions are v2.352.0 and 2.353.0, working with v2.351.0. i think that the issue is  not related to the S3 fonctions but more with aws sdk, i see the same issue with dynamodb on stackoverflow. ",
    "portacha": "Thank you very much in advance!. ",
    "dominiceden": "@portacha You can still do this though:\nimport TranscribeService from \"aws-sdk/clients/transcribeservice\";\nSee https://github.com/aws/aws-sdk-js#in-nodejs-1. @srchase Great, thanks for the quick response! I'll keep my eyes on this one then.. ",
    "SodaGremlin": "Is there any plan to include CORS support and allow this to be used? Thanks, Gregg. ",
    "bennettbuchanan": "Great, thanks for the quick reply @srchase.. ",
    "Goal54": "@srchase Yes it's : \nconst getFinalFolderForS3 = async (idSociety, folder, filename) => {\n  const keyFile = uuid(filename, uuidv4());\n  let tmpKey;\n  if (idSociety) {\n    tmpKey =${idSociety}/${folder}/${keyFile};\n  } else {\n    tmpKey =${folder}/${keyFile};\n  }\n  return tmpKey.toString();\n};. @srchase if I replace with a string it doesn't work as expected too. Type of (tmpKey) return me \"string\" as expected. This issue is very strange because I can upload with success my file. I create something like that:\nAWS.config = new AWS.Config();\nAWS.config.accessKeyId = awsClementine.accessKeyId;\nAWS.config.secretAccessKey = awsClementine.secretAccessKey;\nconst s3 = new AWS.S3({\n  apiVersion: awsClementine.apiVersion,\n  params: {\n    Bucket: clementineBucketName,\n  },\n});`\nI put all in my config.json. ",
    "pacopicon": "@srchase \nVery good question.  The problem ultimately lay in the fact that my employer has several aws credentials.  I tried headObject with all of them until and that is how I found the right set of keys.\nThanks for your help! . ",
    "koladilip": "```\nconst cloudWatchEvents = new AWS.CloudWatchEvents();\ntry {\n    await cloudWatchEvents.putTargets({\n        Rule: cliArgs.ruleName,\n        Targets: [{\n            Id: cliArgs.targetId,\n            Arn: cliArgs.ssmAutomationDocument,\n            RoleArn: cliArgs.roleArn\n        }]\n    }).promise();\n} catch (error) {\n    throw error;\n}\n\n```. @srchase How do I contact AWS service team? via support?\nIs this package officially supported by AWS right?. @srchase Ok got it. Thanks.. ",
    "tambry": "Wouldn't this break some existing projects using the incorrectly cased variant, due to the \"removal\" of the typings for it? That said, all of those projects would have been using the setting with no effect, so the only thing that'd break are builds.\nThere are a few incorrect variants in config.ts too.. Wouldn't this break some existing projects using the incorrectly cased variant, due to the \"removal\" of the typings for it? That said, all of those projects would have been using the setting with no effect, so the only thing that'd break are builds.\nThere are a few incorrect variants in config.ts too.. @AllanFly120 That'd indeed work around that problem. Though then you'd probably also want to add a deprecation warning, which also mentions that it actually had no effect, for the incorrectly cased one and remove it in the next major version.. @AllanFly120 That'd indeed work around that problem. Though then you'd probably also want to add a deprecation warning, which also mentions that it actually had no effect, for the incorrectly cased one and remove it in the next major version.. I'd like to add that in my opinion it would also be a good idea to change useDualstack to true by default in the next major version.. I'd like to add that in my opinion it would also be a good idea to change useDualstack to true by default in the next major version.. Wouldn't it still be a good idea to add something like this here:\nif (service.config.useDualStack) {\n  console.warn('useDualStack is deprecated and due to wrong casing has had no effect since its introduction. Use useDualstack instead.');\n}. Wouldn't it still be a good idea to add something like this here:\nif (service.config.useDualStack) {\n  console.warn('useDualStack is deprecated and due to wrong casing has had no effect since its introduction. Use useDualstack instead.');\n}. ",
    "simonmit": "Thanks for the fast reply.\nThe behaviour of the DocumentClient is correct.\nThe issue is about the type definitions: There are type definitions with overlapping types (which differ slightly).\nI found out that the the issue is in how VS Code lets me navigate to the respective type definition files. \nI was able to convince VS Code to use the correct type:\n```\n// import { GetItemOutput } from 'aws-sdk/clients/dynamodb';\nimport { DocumentClient } from 'aws-sdk/clients/dynamodb';\nconst resp: DocumentClient.GetItemOutput = ...\n```\nSo this is not an issue of the type definitions. \nIt was my fault to rely on VS Code to point me to the correct type definition. I will close the issue.. Thanks for the fast reply.\nThe behaviour of the DocumentClient is correct.\nThe issue is about the type definitions: There are type definitions with overlapping types (which differ slightly).\nI found out that the the issue is in how VS Code lets me navigate to the respective type definition files. \nI was able to convince VS Code to use the correct type:\n```\n// import { GetItemOutput } from 'aws-sdk/clients/dynamodb';\nimport { DocumentClient } from 'aws-sdk/clients/dynamodb';\nconst resp: DocumentClient.GetItemOutput = ...\n```\nSo this is not an issue of the type definitions. \nIt was my fault to rely on VS Code to point me to the correct type definition. I will close the issue.. ",
    "marshalld139": "The SDK is being called per the docs:\n```\nconst params = {\n    DistributionConfig : newConfig.DistributionConfig,\n    Id: id,\n    IfMatch: newConfig.ETag\n  };\ncloudfront.updateDistribution(params, function(err, data) {\n  if (err) console.log(err, err.stack);\n  else     console.log(data);\n});\n```\nEssentially I have pulled down the Distribution config using getDistributionConfig and modified the value of Origins:\n```\n{ Quantity: 1,\n  Items: \n   [ { Id: 'id',\n       DomainName: ,\n       OriginPath: '',\n       CustomHeaders: [Object],\n       S3OriginConfig: [Object] } ]\n }\n```\nThis is done as CloudFront requires a full config to be provided if any change is made to any of the config.\nBut when I tried to push up the new value I get that error.\nThe CLI command:\naws cloudfront update-distribution --id <id> --distribution-config file://cloudfront_config.json --if-match <ETag>\nThis gets the same error as the SDK.. @srchase \nThanks for the response.\nI did try and create a CustomOrigin in the past and it still did not work, but I will have a closer look today and try it out again. Thanks.. ",
    "awood45": "LGTM. ",
    "ricbermo": "hey @truongluong1314520  did you manage to solve this?\nI added this to my babelrc and I got it working.\n\"ignore\": [\n  \"node_modules/aws-sdk/dist/aws-sdk-react-native.js\"\n]. @Blutude I'm using this version: 2.228.1, also can u try this?\nopen node_modules/aws-sdk/dist/aws-sdk-react-native.js.\nLook for TYPED_ARRAY_SUPPORT. Replace this\nBuffer.TYPED_ARRAY_SUPPORT = global.TYPED_ARRAY_SUPPORT !== undefined\n      ? global.TYPED_ARRAY_SUPPORT\n      : typedArraySupport()\nwith this:\nBuffer.TYPED_ARRAY_SUPPORT = typedArraySupport(). @truongluong1314520 sorry for the late reply. can u try that ^^ as well?. ",
    "jrounsav": "Applying the ignore opens up another error for me with the latest sdk\nundefined is not an object (evaluating 'global.TYPED_ARRAY_SUPPORT'). ",
    "Gregjarvez": "@jstewmon using exclude ihttps://babeljs.io/docs/en/options#ignore worked for me. See note under ignore . @jstewmon sorry no. \nI meant @jrounsav. ",
    "oliviertassinari": "We use the same client. So if I understanding it correctly, it's not the default expected behavior? I can share the source code. . SqsConsumer.js\n```js\nimport AWS from 'aws-sdk'\nimport config from 'config'\nimport log from 'modules/scripts/log'\nimport crashReporter from 'modules/crashReporter/common'\nimport { addTeardown, removeTeardown, isShuttingDown } from 'modules/process/handleKillSignals'\nimport Queue from 'modules/async/Queue'\nexport default class SqsConsumer {\n  constructor(options) {\n    const QueueUrl = options.queueUrl || config.get(sqs.${options.topic})\n    if (!QueueUrl) {\n      throw new Error(The following config is missing: sqs.${options.topic})\n    }\nthis.name = `sqsConsumer[${options.topic}]`\nthis.sqs = new AWS.SQS({\n  apiVersion: '2012-11-05',\n  params: {\n    MaxNumberOfMessages: 10,\n    QueueUrl,\n    WaitTimeSeconds: 5, // Long polling\n  },\n  region: config.get('aws.region'),\n})\nthis.queue = new Queue(\n  async ({ message, options: { handler } }) => {\n    try {\n      await handler(JSON.parse(message.Body))\n      await this.sqs.deleteMessage({ ReceiptHandle: message.ReceiptHandle }).promise()\n    } catch (err) {\n      if (isShuttingDown() && err.signal === 'SIGINT') {\n        return\n      }\n\n      crashReporter.captureException(err, {\n        extra: {\n          message,\n        },\n        tags: {\n          scope: this.name,\n        },\n      })\n    }\n  },\n  { concurrency: options.concurrency }\n)\naddTeardown((this.tearDown = { callback: this.destroy, nice: 3 }))\n\n}\ndestroy = async () => {\n    log.info({ name: this.name, msg: 'closing' })\n    removeTeardown(this.tearDown)\n    await this.queue.destroy()\n    log.info({ name: this.name, msg: 'closed' })\n  }\nconsume = async options => {\n    const { Messages: messages = [] } = await this.sqs.receiveMessage().promise()\nif (isShuttingDown()) {\n  return\n}\n\nthis.queue.push(messages.map(message => ({ message, options })))\n\nawait this.queue.waitUntil({ predicate: 'almost-empty' })\n\n}\nconsumeAll = async options => {\n    log.info({ name: this.name, msg: 'consumeAll' })\n    await new Promise(async (resolve, reject) => {\n      try {\n        while (!isShuttingDown()) {\n          // eslint-disable-next-line no-await-in-loop\n          await this.consume(options)\n        }\n        resolve()\n      } catch (err) {\n        reject(err)\n      }\n    })\n  }\n}\n```. @srchase We will try that out next Monday :). Should it be https or it's not important? (for us, it's not sensitive). No luck:\n\nNetworkingError: Protocol \"https:\" not supported. Expected \"http:\"\n  at new ClientRequest (_http_client.js:118:11)\n  at Object.request (https.js:280:10)\n  at features.constructor.handleRequest (/onepixel/app/node_modules/aws-sdk/lib/http/node.js:42:23)\n  at executeSend (/onepixel/app/node_modules/aws-sdk/lib/event_listeners.js:333:29)\n  at Request.SEND (/onepixel/app/node_modules/aws-sdk/lib/event_listeners.js:347:9)\n  at Request.callListeners (/onepixel/app/node_modules/aws-sdk/lib/sequential_executor.js:102:18)\n  at Request.emit (/onepixel/app/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\n  at Request.emit (/onepixel/app/node_modules/aws-sdk/lib/request.js:683:14)\n  at Request.transition (/onepixel/app/node_modules/aws-sdk/lib/request.js:22:10)\n  at AcceptorStateMachine.runTo (/onepixel/app/node_modules/aws-sdk/lib/state_machine.js:14:12)\n\nUsing an https agent doesn't help either.. @srchase Yes, thanks. I have tried with an https agent after seeing the http network error of node. However, the flame graph is the same.. @srchase We are using:\n- Node v10.12.0\n- aws-sdk v2.335.0\n- The node process is hosted by a Fargate on a Docker.. @srchase Yes, I can test it again Tuesday :).. ",
    "tverney": "@srchase Great workaround! Just fixed for me! . ",
    "hagiosofori": "@srchase \nthe query I'm running\n```AWS.config.update({\n    region: env.AWS_REGION,\n    credentials: new AWS.Credentials(\n        env.AWS_ACCESS_KEY_ID,\n        env.AWS_SECRET_ACCESS_KEY,\n        env.AWS_SESSION_TOKEN\n    ),\n})\nexports.handler = function(event, context, callback) {\n    //eslint-disable-line\n    const requestBody = {\n        query: ListSources,\n        operationName: \"ListSources\",\n    }\n    console.log(posting ${JSON.stringify(requestBody, null, 2)})\n    console.log('check')\n    const uri = URL.parse(config.graphqlEndpoint)\n    const httpRequest = new AWS.HttpRequest(uri.href, env.REGION)\n    httpRequest.headers.host = uri.host\n    httpRequest.headers[\"Content-Type\"] = \"application/json\"\n    httpRequest.method = \"GET\"\n    httpRequest.body = JSON.stringify(requestBody)\nconsole.log(`http request: ${JSON.stringify(httpRequest)}`)\n\nAWS.config.credentials.get(err => {\n    const signer = new AWS.Signers.V4(httpRequest, \"appsync\", true)\n\n    console.log(`signer: ${JSON.stringify(signer)}`)\n    const date = AWS.util.date.getDate()\n    const isodate = date.toISOString()\n\n    console.log(`credentials: ${JSON.stringify(AWS.config.credentials)}`)\n    console.log(typeof isodate)\n\n    signer.addAuthorization(AWS.config.credentials, AWS.util.date.getDate())\n    console.log(`not signer.addauthentication`)\n    const options = {\n        method: httpRequest.method,\n        body: httpRequest.body,\n        headers: httpRequest.headers,\n    }\n\n    fetch(uri.href, options)\n        .then(res => res.json())\n        .then(json => {\n            console.log(`JSON Response = ${JSON.stringify(json, null, 2)}`)\n            callback(null, event)\n        })\n})\n\ncallback(null, event)\n//fetch the appropriate source\u00e4, with the source fields\n\n}\nactual query\nListSources = `query ListSources{\n    listSources{\n        items{\n            id\n            name\n            url\n            isActive\n            fields{\n                items{\n                name\n                id\n                selector\n                isActive\n                }\n            }\n        }\n    }\n}\nQuery version\n2.361.0\nI pulled it from npm, for use in the application. I've made no modifications to it\nThank you... . @srchase \nthe query I'm running\n```AWS.config.update({\n    region: env.AWS_REGION,\n    credentials: new AWS.Credentials(\n        env.AWS_ACCESS_KEY_ID,\n        env.AWS_SECRET_ACCESS_KEY,\n        env.AWS_SESSION_TOKEN\n    ),\n})\nexports.handler = function(event, context, callback) {\n    //eslint-disable-line\n    const requestBody = {\n        query: ListSources,\n        operationName: \"ListSources\",\n    }\n    console.log(posting ${JSON.stringify(requestBody, null, 2)})\n    console.log('check')\n    const uri = URL.parse(config.graphqlEndpoint)\n    const httpRequest = new AWS.HttpRequest(uri.href, env.REGION)\n    httpRequest.headers.host = uri.host\n    httpRequest.headers[\"Content-Type\"] = \"application/json\"\n    httpRequest.method = \"GET\"\n    httpRequest.body = JSON.stringify(requestBody)\nconsole.log(`http request: ${JSON.stringify(httpRequest)}`)\n\nAWS.config.credentials.get(err => {\n    const signer = new AWS.Signers.V4(httpRequest, \"appsync\", true)\n\n    console.log(`signer: ${JSON.stringify(signer)}`)\n    const date = AWS.util.date.getDate()\n    const isodate = date.toISOString()\n\n    console.log(`credentials: ${JSON.stringify(AWS.config.credentials)}`)\n    console.log(typeof isodate)\n\n    signer.addAuthorization(AWS.config.credentials, AWS.util.date.getDate())\n    console.log(`not signer.addauthentication`)\n    const options = {\n        method: httpRequest.method,\n        body: httpRequest.body,\n        headers: httpRequest.headers,\n    }\n\n    fetch(uri.href, options)\n        .then(res => res.json())\n        .then(json => {\n            console.log(`JSON Response = ${JSON.stringify(json, null, 2)}`)\n            callback(null, event)\n        })\n})\n\ncallback(null, event)\n//fetch the appropriate source\u00e4, with the source fields\n\n}\nactual query\nListSources = `query ListSources{\n    listSources{\n        items{\n            id\n            name\n            url\n            isActive\n            fields{\n                items{\n                name\n                id\n                selector\n                isActive\n                }\n            }\n        }\n    }\n}\nQuery version\n2.361.0\nI pulled it from npm, for use in the application. I've made no modifications to it\nThank you... . lambda runtime\nNode 8.10. lambda runtime\nNode 8.10. thanks @srchase ... will check it out. thanks @srchase ... will check it out. ",
    "BronxBombers": "I am encountering the same issue in an almost identical use case, when removing the signer.addAuthorization the call is successfully gets a not authenticated response, so the request is being sent correctly. \n. ",
    "flyandi": "Same problem here - almost same code for the signing part. Also the signing part is used in tons of blogs, etc. So did anyone actually manage to use it?\n. Same problem here - almost same code for the signing part. Also the signing part is used in tons of blogs, etc. So did anyone actually manage to use it?\n. ",
    "nireak": "In case it helps somebody...I had the same problem and the origin of it was that the region environment variable was empty ( I was typing env.REGION when mine was defined in lowercase: env.region). ",
    "asyba": "No, it should use the default one.. The default from AWS.RDSDataService() class that are in the sdk, don't know what is , I guess is us-east-1.\nThe documentation doesn't say that region is a required field to use this service.. The default from AWS.RDSDataService() class that are in the sdk, don't know what is , I guess is us-east-1.\nThe documentation doesn't say that region is a required field to use this service.. I guess the class is looking the \"AWS_REGION\" env variable if I didn't add one in the creation of RDSDataService()\n. I guess the class is looking the \"AWS_REGION\" env variable if I didn't add one in the creation of RDSDataService()\n. Okey now I got this error:\nBadRequestException: ERROR: invalid cluster id: arn:aws:rds:us-east-1:xxxxx:db:xxxxxtest\n    at Object.extractError (/var/task/node_modules/aws-sdk/lib/protocol/json.js:48:27)\n    at Request.extractError (/var/task/node_modules/aws-sdk/lib/protocol/rest_json.js:52:8)\n    at Request.callListeners (/var/task/node_modules/aws-sdk/lib/sequential_executor.js:106:20)\n    at Request.emit (/var/task/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\n    at Request.emit (/var/task/node_modules/aws-sdk/lib/request.js:683:14)\n    at Request.transition (/var/task/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/var/task/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /var/task/node_modules/aws-sdk/lib/state_machine.js:26:10. Okey now I got this error:\nBadRequestException: ERROR: invalid cluster id: arn:aws:rds:us-east-1:xxxxx:db:xxxxxtest\n    at Object.extractError (/var/task/node_modules/aws-sdk/lib/protocol/json.js:48:27)\n    at Request.extractError (/var/task/node_modules/aws-sdk/lib/protocol/rest_json.js:52:8)\n    at Request.callListeners (/var/task/node_modules/aws-sdk/lib/sequential_executor.js:106:20)\n    at Request.emit (/var/task/node_modules/aws-sdk/lib/sequential_executor.js:78:10)\n    at Request.emit (/var/task/node_modules/aws-sdk/lib/request.js:683:14)\n    at Request.transition (/var/task/node_modules/aws-sdk/lib/request.js:22:10)\n    at AcceptorStateMachine.runTo (/var/task/node_modules/aws-sdk/lib/state_machine.js:14:12)\n    at /var/task/node_modules/aws-sdk/lib/state_machine.js:26:10. Second: can you create a feature to develop to use by default \"AWS_REGION\", if region not present? \nIn Java SDK we have that feature.. Second: can you create a feature to develop to use by default \"AWS_REGION\", if region not present? \nIn Java SDK we have that feature.. is a Single instance (mysql).\nand my secret manager where created with  the template \"Credentials for RDS database\".\n. is a Single instance (mysql).\nand my secret manager where created with  the template \"Credentials for RDS database\".\n. so is a bug like the other, ok. so is a bug like the other, ok. But in Java SDK I can do this, without setting the region:\nAmazonS3ClientBuilder.defaultClient()\n\nUsing \"Use the default credential provider chain (recommended)\"\nhttps://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html\nhttps://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/regions/DefaultAwsRegionProviderChain.html\n\n. But in Java SDK I can do this, without setting the region:\nAmazonS3ClientBuilder.defaultClient()\n\nUsing \"Use the default credential provider chain (recommended)\"\nhttps://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html\nhttps://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/regions/DefaultAwsRegionProviderChain.html\n\n. ",
    "svenmilewski": "I am using a cluster and maybe I am having the same issue. Setting the region manually didn't change anything. I am still getting \n\"errorMessage\": \"AWS.RDSDataService is not a constructor\" \nwhen using  \nnew AWS.RDSDataService({apiVersion: '2018-08-01', region: 'us-east-1'});\nin my AWS Lambda function.\nAny ideas?\n// EDIT:\nWhen using the AWS-SDK via npm install it seems to work. Looks like the region's (us-east-1) isn't that up-to-date or am I going wrong?. @srchase \nDidn't know that page. Thanks. :-)  Is there atypical cycle where the SDK is updated in the Lambda Environment?\n. ",
    "edzillion": "ok thanks I have posted it there FWIW: https://forums.aws.amazon.com/thread.jspa?threadID=294130. ",
    "Yijx": "\n@Yijx\nAre you wanting to pause the upload?\nAbort() will clean up the multipart upload, which is why it cannot be restarted with send().\n\nI use\nleavePartsOnError: true\nthen when I abort() uploading, no error appear.If I use send() aging upload will restart from break point...\nIs it a right way to stop uploading? or Is there any other way to stop the breakpoint sequel?\nThank you O(\u2229_\u2229)O~~. @AllanFly120 \nThanks for helping me!\nI will try to use basic api to refactoring my code. \n. ",
    "govindrai": "@srchase Thanks for the quick reply! Yes, I am specifically referring to the Lambda API, but I have gotten stuck like this, with other APIs as well, for example the same thing happened with SQS's getQueueUrl which behaves the same way. \nYou're right the API reference does say so! Good to see it documented, but I never looked at this docs.. or even knew to look there.  Why isn't this information in the JS AWS SDK docs \ud83d\ude22 .. When I got the \"ResourceNotFoundException\" error, I started looking everywhere, thinking maybe my lambda doesn't exist, maybe my cloudwatch rule doesn't exist, etc and then finally (after much too long imo) realizing that if a policy doesn't exist an error will be returned. \nSo yea, I'm wondering why that isn't documented on the SDK page.. it seems wrong to have to look in two areas... \nAnother pain point is that when an error is returned, the error doesn't include information about which api call the error is coming from. This forces the developer to go through the logs and see where the code is breaking (sometimes it's easy to recognize if the error includes a certain keyword, but in this example, the resource you requested does not exist could not be more vague. If it could include the API method, say a property as such attached on the error: api: getPolicy, that would also be a GREAT improvement to the api.\nThank you for listening to my woes haha \ud83d\ude03 . @srchase Thanks for the quick reply! Yes, I am specifically referring to the Lambda API, but I have gotten stuck like this, with other APIs as well, for example the same thing happened with SQS's getQueueUrl which behaves the same way. \nYou're right the API reference does say so! Good to see it documented, but I never looked at this docs.. or even knew to look there.  Why isn't this information in the JS AWS SDK docs \ud83d\ude22 .. When I got the \"ResourceNotFoundException\" error, I started looking everywhere, thinking maybe my lambda doesn't exist, maybe my cloudwatch rule doesn't exist, etc and then finally (after much too long imo) realizing that if a policy doesn't exist an error will be returned. \nSo yea, I'm wondering why that isn't documented on the SDK page.. it seems wrong to have to look in two areas... \nAnother pain point is that when an error is returned, the error doesn't include information about which api call the error is coming from. This forces the developer to go through the logs and see where the code is breaking (sometimes it's easy to recognize if the error includes a certain keyword, but in this example, the resource you requested does not exist could not be more vague. If it could include the API method, say a property as such attached on the error: api: getPolicy, that would also be a GREAT improvement to the api.\nThank you for listening to my woes haha \ud83d\ude03 . Thanks for listening. Will do, and sounds good to me. Appreciate your help!. ",
    "viamuli": "Ahhh, perfect! Should have been looking at this method before. Thanks, it is exactly what we need.. ",
    "marceloneias": "Thanks for your help, I was making a little mistake, your example helped me and it worked perfectly. Thank you!!!. ",
    "koneru9999": "@anwarhamr , I'm curious to know .. how did you solve this issue ?\nI'm facing a similar issue but with SendMessageBatch. ",
    "anwarhamr": "function acknowledgeMessages(QueueUrl,messages) {\n  //not sure why i have 'del' below, nothing in the docs mention it.  Delete this line if we see it's working.\n  // var entries = _.map(messages,(m)=>({Id:'del'+m.MessageId,ReceiptHandle:m.ReceiptHandle})),\n    var entries = _.map(_.uniqWith(messages,d=>d.MessageId),(m)=>({Id:m.MessageId,ReceiptHandle:m.ReceiptHandle})),\n      params = {\n        QueueUrl,\n        Entries: entries,\n      };\n    // debug('acknowledgeMessages params', params);\n    return sqs.deleteMessageBatch(params)\n      .promise()\n      .then(result => {\n        // debug('acknowledgeMessages complete for params', params,result);\n        return {result,params};\n      });\n  }\nWhere the messages parameter is the .Messages value of the object that receiveMessage passes us.\n@koneru9999 I hope this helps.. My above example is how I got it to work.. ",
    "jmm": "Hi @srchase, thanks for your reply.\nYou're able to execute the operation with the alias only in Qualifier?\n\nDid you verify that the user has permissions to invoke that Lambda function?\n\nLet me make sure I understand the question: permissions to invoke the unqualified function, or the particular alias? I'm able to invoke the alias by appending :{aliasName} to the FunctionName option as illustrated above.\nThe permissions policy I configured looks like:\n{\n    \"Sid\": \"VisualEditor1\",\n    \"Effect\": \"Allow\",\n    \"Action\": \"lambda:InvokeFunction\",\n    \"Resource\": [\n        \"arn:aws:lambda:...:...:function:someFunction:*\"\n    ]\n}\nThe permissions part was pretty confusing, as for example I saw comments online where people seemed to be saying they couldn't configure permissions per-alias, and in my experience it was required to configure them per-alias or with a wildcard as I have here (though the policy editor acted like the wildcard is incorrect, yet that's what works).. Hi @srchase, thanks for your reply.\nYou're able to execute the operation with the alias only in Qualifier?\n\nDid you verify that the user has permissions to invoke that Lambda function?\n\nLet me make sure I understand the question: permissions to invoke the unqualified function, or the particular alias? I'm able to invoke the alias by appending :{aliasName} to the FunctionName option as illustrated above.\nThe permissions policy I configured looks like:\n{\n    \"Sid\": \"VisualEditor1\",\n    \"Effect\": \"Allow\",\n    \"Action\": \"lambda:InvokeFunction\",\n    \"Resource\": [\n        \"arn:aws:lambda:...:...:function:someFunction:*\"\n    ]\n}\nThe permissions part was pretty confusing, as for example I saw comments online where people seemed to be saying they couldn't configure permissions per-alias, and in my experience it was required to configure them per-alias or with a wildcard as I have here (though the policy editor acted like the wildcard is incorrect, yet that's what works).. Thanks for your reply.\n\nGetting this working is a bit tricky\n\nAgreed!\n\nWhen using the fully qualified resource in your policy, you cannot use the qualifier and must invoke that function with the matching fully qualified ARN:\n\nThough I can't use Qualifier, it doesn't seem to be necessary to use the full ARN, rather I can do this:\njs\nconst invokeConfig = {\n  FunctionName: \"someFunction:someAlias\",\n};\nIn any case, for my money it's really counterintuitive that although the function name and alias names are configured in the lambda, and there is a FunctionName parameter and a separate Qualifier parameter that's advertised as specifying the alias, the permissions policy dictates how the alias must be referenced from the invoke call -- as part of FunctionName in this case.\nThanks for the link. It's hard to know exactly how that relates to making these invoke calls without knowing what happens under the hood (I haven't looked). And this part makes it sound like it would not be possible to invoke an alias at all using the policy:\n\nIf you use an unqualified function name (such as helloworld), the permission is valid only for invoking the helloworld function using the unqualified function ARN (using any other ARNs will result in a permission error).\n\nWhereas you're saying that's a prerequisite to using Qualifier to specify the alias (and I see that in my testing). I interpret that to mean permission has been granted only for the unqualified function and no aliases, so that description seems problematic in its own right. I guess the invoke operation using Qualifier does something other than append the alias to the ARN, but it's certainly not clear from that description that it'll be possible to invoke aliases somehow using that policy.\nI think the most sensible thing would be if this could be implemented so that alias or version can consistently be specified with Qualifier. If that's not possible, I'd suggest the following documentation changes:\n\nInsert an explanation or link to an explanation of how configuration of permission policy affects applicability of Qualifier.\n\nIt shouldn't make this blanket statement that's not always true:\n\nTo invoke a published version, use the Qualifier parameter to specify a version or alias.\n\n\nIn the name formats examples for FunctionName, include the alias versions, e.g.:\nFunction name - MyFunction.\nFunction and alias name - MyFunction:MyAlias.\n\n\n\nAnd an explanation of when they're applicable.\n\n\nIn the Qualifier documentation, explain when it is / isn't applicable.\n\n\nImproving that part in the Versioning, Aliases, and Resource Policies documentation would also make sense.\n\n\nIn the meantime, thanks for the help understanding what's happening!\n. Thanks for your reply.\n\nGetting this working is a bit tricky\n\nAgreed!\n\nWhen using the fully qualified resource in your policy, you cannot use the qualifier and must invoke that function with the matching fully qualified ARN:\n\nThough I can't use Qualifier, it doesn't seem to be necessary to use the full ARN, rather I can do this:\njs\nconst invokeConfig = {\n  FunctionName: \"someFunction:someAlias\",\n};\nIn any case, for my money it's really counterintuitive that although the function name and alias names are configured in the lambda, and there is a FunctionName parameter and a separate Qualifier parameter that's advertised as specifying the alias, the permissions policy dictates how the alias must be referenced from the invoke call -- as part of FunctionName in this case.\nThanks for the link. It's hard to know exactly how that relates to making these invoke calls without knowing what happens under the hood (I haven't looked). And this part makes it sound like it would not be possible to invoke an alias at all using the policy:\n\nIf you use an unqualified function name (such as helloworld), the permission is valid only for invoking the helloworld function using the unqualified function ARN (using any other ARNs will result in a permission error).\n\nWhereas you're saying that's a prerequisite to using Qualifier to specify the alias (and I see that in my testing). I interpret that to mean permission has been granted only for the unqualified function and no aliases, so that description seems problematic in its own right. I guess the invoke operation using Qualifier does something other than append the alias to the ARN, but it's certainly not clear from that description that it'll be possible to invoke aliases somehow using that policy.\nI think the most sensible thing would be if this could be implemented so that alias or version can consistently be specified with Qualifier. If that's not possible, I'd suggest the following documentation changes:\n\nInsert an explanation or link to an explanation of how configuration of permission policy affects applicability of Qualifier.\n\nIt shouldn't make this blanket statement that's not always true:\n\nTo invoke a published version, use the Qualifier parameter to specify a version or alias.\n\n\nIn the name formats examples for FunctionName, include the alias versions, e.g.:\nFunction name - MyFunction.\nFunction and alias name - MyFunction:MyAlias.\n\n\n\nAnd an explanation of when they're applicable.\n\n\nIn the Qualifier documentation, explain when it is / isn't applicable.\n\n\nImproving that part in the Versioning, Aliases, and Resource Policies documentation would also make sense.\n\n\nIn the meantime, thanks for the help understanding what's happening!\n. Ok thanks. Does that team maintain this documentation?. Ok thanks. Does that team maintain this documentation?. Thanks! With this ticket being closed, is there anywhere it'll be tracked?. Thanks! With this ticket being closed, is there anywhere it'll be tracked?. Thanks!. Thanks!. ",
    "naripok": "related stack overflow thread. related stack overflow thread. ",
    "evgenykireev": "As I temporary solution, I use some private methods of AWS.DynamoDB.DocumentClient. I know it's a bad practice, but works fine for me.\n1. I collect all transaction items in an array:\n....\ntransactions.push({ Put: params });\n....\ntransactions.push({ Delete: params })\nwhere params are AWS.DynamoDB.DocumentClient params.\n\nI map these transactions to AWS.DynamoDB array\n```\nconst operationsMapping = {\n  Put: 'putItem',\n  Delete: 'deleteItem',\n  Update: 'updateItem',\n};\n\nconst dynamoClient = new Dynamo.DocumentClient({ service: dynamoInstance });\n// Danger zone - using private methods!\nconst translator = dynamoClient.getTranslator();\nconst transactionItems = this.transaction.map((transaction) => {\n  const operation = Object.keys(transaction)[0];\n  const params = Object.values(transaction)[0];\n  const operationType = operationsMapping[operation];\nconst request = dynamoClient.serviceoperationType;\n  const inputShape = request.service.api.operations[operationType].input;\n  const translated = translator.translateInput(params, inputShape);\nreturn {\n    [operation]: translated\n  }\n});\n```\n\nI execute all as a transaction:\nconst dynamo = new Dynamo();\nconst result = await dynamo.transactWriteItems({ TransactItems: transactionItems }).promise();. \n",
    "varnitgoyal95": "thankyou\n. ",
    "rajdas98": "SOLUTION\nUpdate the aws-sdk package (>2.365)\nuse-\nvar dynamodb = new aws.DynamoDB();\ninstead of\nvar dynamodb = new aws.DynamoDB.DocumentClient();\n. ",
    "MartinMasek": "Does anyone know when this will be supported in lambdas? I've tried to use it but it seems the package used by Amazon is older than the newest one (as described here https://stackoverflow.com/questions/53902670/dynamodb-transactwriteitems-is-not-a-function-error-on-lambda-but-not-when-using) \nQuite ironic.. @srchase \nThat's something I try to avoid. The whole reason why I started with lambdas was to get rid of taking care of versions and dependencies myself. However, you have a good point that my approach may introduce some problems later.\nI'd be helpful to get roadmap info from Amazon (so I would know whether it's worth waiting) but I was not able to find such thing.. ",
    "mrapczynski": "@AndrewBarba @srchase I just spent an hour this morning debugging this too.\nTo the extent that I can tell, from 2.366.0 and up, all.js refers to the licensemanager module, but it was not delivered in the file tree as part of the NPM package.\nSee screenshot below. Compare file tree on the left (from 2.368.0) to the module imports on the right.\n. @AndrewBarba Pin a slightly older revision until this is fixed by AWS: npm install --save-exact aws-sdk@2.365.0\n(unless you wanted the newest stuff \ud83d\ude2c ). @srchase \nWhen I run npm view aws-sdk@2.368.0, I get the info printed below which shows the tarball in the registry. And no, I am not using Serverless.\nJust for fun, I just downloaded the tarball into a temp folder, and surprise the license manager module is in there. Then I tried an npm rm... and reinstalled 2.368.0 from scratch, and now license manager is also there in the tree. Seems like we need to do some kind of clean up on our node_modules to get the install to be successful.\ndist:\n   { integrity: 'sha512-pRq4mn6MGthemTcDETBpQ0accSn/VpZEHzjloFmj2KpEavS8FEwhLvatgFqu8zlDunEfARRyL3Gz3FONO8ZFVA==',\n     shasum: '5855201dca9740f253e468f7ce8caa03e5eb696b',\n     tarball: 'https://registry.npmjs.org/aws-sdk/-/aws-sdk-2.368.0.tgz',\n     fileCount: 1089,\n     unpackedSize: 34682284,\n     'npm-signature': '-----BEGIN PGP SIGNATURE-----\\r\\nVersion: OpenPGP.js v3.0.4\\r\\nComment: https://openpgpjs.org\\r\\n\\r\\nwsFcBAEBCAAQBQJcAIM9CRA9TVsSAnZWagAAeDEP/R3RYgWoVNFUvlsDRlqh\\n2zU8Lh/4hXZpq00W7ul16AMEC7HF34hWMnhhk/EhE++FPv6EthmYJzrLkIXD\\njR609rgMbEurwHRj2iVIUT12tFTx/PBRi1sN0cYWmE7eGrhwfw/lZvgzZpdm\\nnoXZA9nFIqNYZULq9N9orG2BQ3RmYsGQERjSHDr6y28PXZtXoR5W4YrbGesi\\nJ2nCqCdUw45bLs4mBYk/41HjhMu9c6yQ0xma8CIfQ5l+zYg36XV0jpUpJFp4\\ntMS/xY4knKgAh5/7QyWEJdni27yt8Kpzit9IJ5BGTMN8WBFjNyppHigxQs6Z\\ny+8apLkh+J8QtiGNXaRf5gG590DOjGcXMafPLbYJgEPmpRk8DdG6587VwTNe\\n6x05yXbvchNbkG/0QhMhzgL74QOZuCzOmjlfMb5aDZrksaJSLU4zgUwYclOH\\nbxtV9IWNYvj4etgaFJr72yJV8ezqNlGbc2w54eTCVKuNbkOdWIkBknhHrth2\\nUkmMInEMCem2AIU9lagiq7XIPjf69GW3YmraVSk9XKixP4Qp9kmcx968SGJ6\\ndIX6CZiqFX8nWtFNje84OlHRNoRonacf6ShcaFH/gqB6PZuzsG4d+7DtctQ8\\nyKc2LlOAVyxdiHjyFgTo1x5xEuFjl4iQ5F1E03Kd1QFL79D8yyaBZNrHy3RR\\nEw9+\\r\\n=VDbu\\r\\n-----END PGP SIGNATURE-----\\r\\n' } }. ",
    "uptownaravi": "okay will post it there, thanks.. ",
    "abbottdev": "@srchase Thanks for getting back to me.\nSo, I'm trying to use create-react-app, which I then added typescript to. I understand CRA has an opinionated webpack implementation under the hood. (I'm sure I won't be the only person that tries this though... maybe one of the first? \ud83d\ude04 ) \nI've created a dummy project for you which reproduces the behaviour in the attached zip. (Note I didn't include node_modules so you'll need to npm install too, then npm start ).\nIf you want to get the full webpack definition, you can run npm run eject in the root of that folder and that will create the webpack.config.js if you wish to inspect the specifics of what it's doing under the hood.\nWhat webpack requirements are there (If any)? \naws-comprehend-medical-ts-react.zip\nThanks \ud83d\udc4d . @srchase \nNo worries - Published here: https://github.com/abbottdev/aws-comprehend-medical-ts \nIs there anything more you need from me to help debug? \ud83d\ude03 . Just a bit of further info - I may be completely off with this but if I try to consume this API using vanilla nodejs in an AWS lambda, I receive a similar response. So this may not be a typescript specific issue - but I figure this info may be helpful for you @srchase \nHere's the lambda I'm attempting to use:\n```\nlet AWS = require('aws-sdk');\nexports.handler = (event, context, callback) => {\n    var cp = new AWS.ComprehendMedical({});\nvar params = {\n  Text: 'he has diabetes' \n};\n\nreturn cp.detectPHI(params, function(err, data) {\n  if (err) callback(err); // an error occurred\n  else  {\n      console.log(data);           // successful response\n\n    // TODO implement\n    const response = {\n        statusCode: 200,\n        body: JSON.stringify(data),\n    };\n\n    callback(response);\n  }\n});\n\n};\n```\nAnd the error stack I receive if I test this in the lambda console:\nFunction Logs:\nSTART RequestId: a287c941-fbaa-11e8-bef7-4bea21fa5549 Version: $LATEST\n2018-12-09T12:04:52.793Z    a287c941-fbaa-11e8-bef7-4bea21fa5549    TypeError: AWS.ComprehendMedical is not a constructor\n    at exports.handler (/var/task/index.js:4:14)\nEND RequestId: a287c941-fbaa-11e8-bef7-4bea21fa5549\nREPORT RequestId: a287c941-fbaa-11e8-bef7-4bea21fa5549  Duration: 50.57 ms  Billed Duration: 100 ms     Memory Size: 128 MB Max Memory Used: 26 MB  \nRequestId: a287c941-fbaa-11e8-bef7-4bea21fa5549 Process exited before completing request. @srchase  - FYI - The above import  @wedwin53 is using does allow us to consume the SDK, but I don't believe it's the intended way - As using the above import doesn't have any TS definition files, so we get no intellisense via the .d.ts files already in the sdk.\n. Sweet, thanks @srchase . ",
    "wedwin53": "I solve this problem doing this ->\nimporting from the path:\nimport AWS from 'aws-sdk/dist/aws-sdk-react-native';. Thanks, \nI'm Using Reactjs\n\"aws-sdk\": \"2.368.0\",\n\"react\": \"16.6.0\"\n`getTextAnalysisCM(textToAnalyze){\nAWS.config.region = 'us-west-2'; \nAWS.config.credentials = new AWS.CognitoIdentityCredentials({\n  IdentityPoolId: 'IDENTITY-POOL-ID'\n});\n\n\nlet entAnalysisCM = new Promise(function (successCallback, errorCallback) {\n  var params = {\n       Text: textToAnalyze\n     }\n  var comprehendMed = new AWS.ComprehendMedical();\n\n  comprehendMed.detectEntities(params, function (error, data) {\n      if (error) {\n          errorCallback(error)\n      } else {\n          console.log('comprehendMedical: ' + JSON.stringify(data))\n          successCallback(data)\n      }\n  }); // End detectEntities\n\n}); // End promise entAnalysis\n\nreturn entAnalysisCM\n}`\nWhen giving me error this option, also test placing the parameters in the constructor and also gives me error. I include the SDK like that:\nimport AWS from 'aws-sdk/dist/aws-sdk-react-native';. Normal react app but importing as the documentation says does not work for Comprehen and ComprehenMedical.. ",
    "JilanBasha": "Looks like this is npm related issue, solution can be found in this stackoverflow question. ",
    "nehagupta-itbhu": "@srchase \nIf I call collectMessages in a for loop of 20 iterations, the time taken is same as calling async.parallel for 20 tasks, which is 3 secs. I want the entire operation to be finished within a second.. ",
    "mfaizan1": "Silly mistake i had set wrong region. ",
    "knalx": "I reproduced this error many times. Always JSON parsing error. But now it breaks at the different place.\n{\"executionArn\":\"arn:aws:states:eu-west-1:******:execution:Subscriptio{\"executions\":[{\nMy code just reading all running execution. I tried to call this endpoint few times,\n- ok\n- broken\n- ok\n- ok\n- broken\nSeems suspicious that it's always brakes at the symbol number 17268.. The length of the raw response string from your API. May be you have some network level balancers that messed up. \nLooks like a chunks are missing or mixed up.. ",
    "Strydom": "@srchase \nI'm using await but have tested with .then().catch() and neither work. Wow okay so it does throw it eventually, I didn't expect it to take longer than 10 seconds?\nIf you use a callback it fails almost immediately, why does it take so long when using a promise?. ",
    "jschimmoeller": "+1 ... also looking for support in js sdk; i have a current web app that I would love to port to aws. . ",
    "kidplug": "Is any work being done on this? I'd love to get a look at it. thanks.. ",
    "taikim8484": "Looking forward too. Is there another way to use this feature for now? Thank you.. ",
    "OvidijusParsiunas": "Would enforcing the use of StartStreamTranscription not be applicable via the use of existing web-socket libraries? I am wondering what is the particular need for the use of HTTP2 in this regard. Thankyou.. ",
    "orgads": "Is there any package that supports this service on node.js?. ",
    "gberdzenishvili": "+1. +1. ",
    "Le0Michine": "just faced that issue, looks like version 2.296.0 was fine and the bug was introduced in v2.297.0. ",
    "batrudinych": "Hello @srchase \nI noticed that the metadata may be provided during upload or changed using copyObject operation. That's the reason I've made this request.\nI am able to change existing object's metadata in S3 Management Console, but not able to do it through aws-sdk, which seems weird.\nIs there any change the ability to modify existing object's metadata could be added? Would be very handy to have this in order to be able to control caching, etc. from the code omitting unnecessary copy operation. @srchase \nWill raise a question with them and post a link to this thread, thanks.\nHere is the link: https://forums.aws.amazon.com/thread.jspa?threadID=295289. ",
    "lenawal": "@srchase \nYes, it does meet all the prerequisites. \nAlso, I am able to launch an instance with hibernation enabled using the console, but when trying to launch an instance with the same configuration (the same encrypted ami and instance type c5.large) using the javascript sdk I get the error.. @srchase Sorry for the late response. This issue is still not resolved for me. I am using the latest version of the SDK. I am calling this code from a lambda function, so I assume the latest sdk version is beeing used. \nAny help is appreciated.\n. This resolved the issue, thank you!. ",
    "brettswift": "I am switching to use credentials per service object, and have this working now. \nWhat is strange is that the AWS.config.update  is counter-intuitive.  It seems to work globally, and even though I'm creating the codepipeline object off of the AWS object, it doesn't update the credentials.\nrequiring more specific packages than just 'aws-sdk' seems to be a better pattern anyways. \nThanks. . ",
    "j": "Going over it again seems that they are correct.  Closing.\n. ",
    "byF": "@srchase Nope, I have not raised it with them yet. This is the relevant documentation:https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-using-tags.html; there is nothing apparent to me (my IAM user is enrolled into new ARN format for ECS)\nAnyhow, where should I move this issue?\n. Nope, but I've just did the same config through the web console and it fails as well.\nThanks, I'm going to ask on that forum.. For anyone who might be looking for this - I contacted AWS Premium Support, this is an internal AWS bug which they are aware of.. ",
    "Gr1mR3aver": "So what is the bug?  I'm also hitting this issue - do you have a URL or incident# for reference?. So what is the bug?  I'm also hitting this issue - do you have a URL or incident# for reference?. ",
    "j1mb0jay": "Suffering same issue. ",
    "cleverguy25": "Suffering the same issue. ",
    "bentito": "Same here, is there a workaround?. Same here, is there a workaround?. ",
    "rfpludwick": "I'm experiencing the same issue, but not via the SDK - I'm getting the problem via the console. Additionally, @srchase, I cannot post the question in said forum because their forum is not allowing me to do so.. Thanks @srchase . ",
    "lucasltv": "Same issue here too. ",
    "axiomoixa": "Same issue as well. ",
    "mirhec": "I could solve the problem by opting out from the new ARN and resource id format:\n\nClicking on \"Configure ECS ARN setting\" and disabling all checkboxes solved the problem for me.. ",
    "tabascoterrier": "I'm also opted into the new ARN and resource format.  I was able to work round the issue in the GUI by unselecting \"Enable ECS managed tags\". ",
    "marcpalm": "https://github.com/aws/aws-sdk-js/pull/2429. Because I tried with AttributeValue type objects and the API failed with an primary keys are not there-type error. Any is always the quickfix;) But I think that is really what belongs there imo, see correct docs example. The other docs simply don't work or represent an early stage of your sdk with somebody forgetting to update typings accordingly.. https://github.com/aws/aws-sdk-js/issues/2428. ",
    "sm2017": "@AllanFly120 I tried using callback style , it called and the console log is\nbash\nerror:  { ETag: '\"b7876ff6d244133c31fb35a24f243157\"',\n  Location:\n   'http://localhost:1000/test/bb7a994c-90d4-4f6d-8ae9-3b039b89cb01.jpg',\n  key: 'bb7a994c-90d4-4f6d-8ae9-3b039b89cb01.jpg',\n  Key: 'bb7a994c-90d4-4f6d-8ae9-3b039b89cb01.jpg',\n  Bucket: 'test' }\nBut my emitted error is different , I emit a custom Error object like MyCustomError that extended Error , So the logged err is different . Yes , I'm using node v10.13.0\nSDK version : 2.375.0. @srchase , I do very similar as you do, If your code works my code must works too\nCurrently my emitted error is not thrown by await s3.upload(params).promise() and this line await for ever\nSo I use\njs\nawait Promise.all([\n    s3.upload(params).promise(),\n   myPromise\n])\nmyPromise rejected on stream error and resolved on stream end , So the await Promise.all is not await for ever because when we have an emitted error myPromise is rejected\n\nPerhaps something else is going on outside the SDK?\n\nWhat can happens out side of SDK ?. ",
    "prismhr-alexandraz": "Hello!\nCan you specify the trouble you ran into? which operation are you\ncalling? The SDK version?\n\n\nI am trying to send SMS via Node js SNS SDK 2.375.0. Tried to set custom\nSenderId via SDK as 'DefaultSenderId' and ''. Code snippet is attached.\nAnd when I receive to Russian phone numbers sms I see that senderId is\ndefault ('NOTICE)', not my custom.\nMessageId is 600ad28a-a545-5f44-9eba-861ac800faa0\n\n\nThe completely another issue is that since 17.12.2018 I can't send sms\nat all , but no error appears. Via AWS console\nI see hover  window that request was successful, but actually SMS wasn't\ndelivered.  - image.png attached\nRequest id is b46afb76-ed83-53cd-a5e8-afe42cb05f62.\n\n\nOn Mon, Dec 17, 2018 at 6:28 PM AllanFly120 notifications@github.com\nwrote:\n\nHi @prismhr-alexandraz https://github.com/prismhr-alexandraz\nCan you specify the trouble you ran into? which operation are you calling?\nThe SDK version?\nThis seems to be an issue on service side, SDK doesn't do any\ncustomization for SMS service. But I can possibly bring this to the service\nteam.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/2431#issuecomment-448038451,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AfbfBH1DPoeNFiMqQu6CXzVdMIrohpelks5u6Ci2gaJpZM4ZVSjj\n.\n\n\n-- \nCONFIDENTIALITY NOTICE: The material contained in or accompanying this \nelectronic transmission contains confidential information which is the \nproperty of the sender and is legally privileged. The information is \nintended only for the individual or entity named above. If you are not the \nintended recipient, you are hereby notified that disclosing, copying, \ndistributing or taking any action in reliance on the contents of this email \nis strictly prohibited. If you have received this information in error, \nplease notify us immediately.\nvar smsAttributes = {\n  attributes: {\n    'DefaultSMSType': requestParams.smsType,\n    'DefaultSenderID': requestParams.from\n  }\n};\n\nvar setSMSTypePromise = aws.setSMSAttributes(smsAttributes).promise();\n\n// Handle promise's fulfilled/rejected states\nsetSMSTypePromise.then(\n  function (data) {\n    Logger.log(data, 'info');\n  }).catch(\n    function (err) {\n      Logger.log('Invalid sms settings: ' + err, 'error');\n      Logger.log(err.stack, 'error');\n    });\n\nvar params = {\n  Message: requestParams.message,\n  PhoneNumber: requestParams.to,\n  MessageAttributes: {\n    'AWS.SNS.SMS.SenderID': {\n      'DataType': 'String',\n      'StringValue': requestParams.from,\n    },\n  },\n};\n\nvar publishTextPromise = aws.publish(params).promise();\n\n. Hello , thank you, I looked inside documentation provided above ans\nunderstood.\nOn Thu, Dec 27, 2018 at 6:28 PM AllanFly120 notifications@github.com\nwrote:\n\nHey @prismhr-alexandraz https://github.com/prismhr-alexandraz\nAccording to SNS docuemnt\nhttps://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-countries.html,\nyou need to request for a special sender id from AWS support. And here\nhttps://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-awssupport-sender-id.html\nis how you can request one. I hope this is helpful.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/2431#issuecomment-450252490,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AfbfBI7rSb8Ujdq2nBOLBaacQm6UWLGuks5u9VeWgaJpZM4ZVSjj\n.\n\n\n-- \nCONFIDENTIALITY NOTICE: The material contained in or accompanying this \nelectronic transmission contains confidential information which is the \nproperty of the sender and is legally privileged. The information is \nintended only for the individual or entity named above. If you are not the \nintended recipient, you are hereby notified that disclosing, copying, \ndistributing or taking any action in reliance on the contents of this email \nis strictly prohibited. If you have received this information in error, \nplease notify us immediately.\n. Hello , thank you, I looked inside documentation provided above ans\nunderstood.\nOn Thu, Dec 27, 2018 at 6:28 PM AllanFly120 notifications@github.com\nwrote:\n\nHey @prismhr-alexandraz https://github.com/prismhr-alexandraz\nAccording to SNS docuemnt\nhttps://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-countries.html,\nyou need to request for a special sender id from AWS support. And here\nhttps://docs.aws.amazon.com/pinpoint/latest/userguide/channels-sms-awssupport-sender-id.html\nis how you can request one. I hope this is helpful.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/2431#issuecomment-450252490,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AfbfBI7rSb8Ujdq2nBOLBaacQm6UWLGuks5u9VeWgaJpZM4ZVSjj\n.\n\n\n-- \nCONFIDENTIALITY NOTICE: The material contained in or accompanying this \nelectronic transmission contains confidential information which is the \nproperty of the sender and is legally privileged. The information is \nintended only for the individual or entity named above. If you are not the \nintended recipient, you are hereby notified that disclosing, copying, \ndistributing or taking any action in reliance on the contents of this email \nis strictly prohibited. If you have received this information in error, \nplease notify us immediately.\n. ",
    "sargun": "Correct, and in addition, and we could check that the credentials.expired != false before making the request, so we don't make an errant request which will fail no matter what.. ",
    "huynhtuanh": "@srchase \nThank you so much for your support!\nAnyway, I still had some problems about this one:\nI am using this service (KinesisVideoMedia) to get fragment metadata from our stream, but it's not working as document (https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/KinesisVideoMedia.html) \nHere is my code (sorry I don't know how to insert space at begin of lines):\n\nvar options = {\n                    accessKeyId: $('#accessKeyId').val(),\n                    secretAccessKey: $('#secretAccessKey').val(),\n                    sessionToken: $('#sessionToken').val() || undefined,\n                    region: $('#region').val(),\n                    endpoint: undefined\n                }\nvar kinesisVideo = new AWS.KinesisVideo(options);\nkinesisvideomedia = new AWS.KinesisVideoMedia(options);\nkinesisVideo.getDataEndpoint({\n                    StreamName: streamName,\n                    APIName: \"GET_MEDIA_FOR_FRAGMENT_LIST\" \n                }, function (err, response) {\n                    if (err) { return console.error(err); }\n                    kinesisvideomedia.endpoint = new AWS.Endpoint(response.DataEndpoint);\n                    var mopts = {\n                        StartSelector: {\n                            StartSelectorType: 'FRAGMENT_NUMBER',\n                            AfterFragmentNumber: '91343852333181439800515257700576805632773130907'\n                        },\n                        StreamName: streamName\n                    };\n                    kinesisvideomedia.getMedia(mopts, function (error, vmresp) {\n                        if (error) {\n                            console.log(error);\n                        }\n                        console.log(vmresp);\n                    });\n                });\n\nAfter using kinesisVideo.getDataEndpoint to get DataEndpoint, we use it to call getMedia but it always return: {\"Output\":{\"__type\":\"com.amazon.coral.service#UnknownOperationException\"},\"Version\":\"1.0\"}\nI had tried to change some parameters as 'APIName' of kinesisVideo.getDataEndpoint or StartSelector of kinesisvideomedia.getMedia, but it was not working too.\nI don't know what wrong I was doing :). If you have any ideas, please let me know. \nThanks so much for any help :). @srchase \nThank you so much. ",
    "NamelessHH": "We're using aws-sdk v2.0.23. I don't know if you guys fixed it on your end or not but 5 min ago I update the sdk from v2.0.23 to v2.1.0 and it seemingly fixed the issue.  We'll do some further testing on this and report back to this.. We've made changes in our qa environment and updated to v2.1.0 and it seemly like its happening less often but it is certainly still happening.. @srchase \nWe've checked our nightly tests and it seems to be fixed.  If there is any further visibility on this issue we'd love to hear about it because the aws status page never even mentioned it.. ",
    "richardsengers": "Sorry for the late reply ... holidays etc ;-)\nAnyways, I've compared the headers and couldn't find anything weird except for a remote ip chnage\nI've changed any private data with an X\nWithout --source-map and a successfull login\n```\nOPTIONS\nRequest URL: https://cognito-idp.us-east-1.amazonaws.com/\nRequest Method: OPTIONS\nStatus Code: 200 \nRemote Address: 34.195.235.147:443\nReferrer Policy: no-referrer-when-downgrade\naccess-control-allow-headers: content-type,x-amz-target,x-amz-user-agent\naccess-control-allow-methods: POST\naccess-control-allow-origin: *\naccess-control-expose-headers: x-amzn-RequestId,x-amzn-ErrorType,x-amzn-ErrorMessage,Date\naccess-control-max-age: 172800\ncontent-length: 0\ndate: Thu, 27 Dec 2018 09:00:44 GMT\nstatus: 200\nx-amzn-requestid: X\nProvisional headers are shown\nAccess-Control-Request-Headers: content-type,x-amz-target,x-amz-user-agent\nAccess-Control-Request-Method: POST\nOrigin: https://app.stage.slashlead.com\nReferer: https://app.stage.slashlead.com/auth/login\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\nPOST\nRequest URL: https://cognito-idp.us-east-1.amazonaws.com/\nRequest Method: POST\nStatus Code: 200 \nRemote Address: 34.195.235.147:443\nReferrer Policy: no-referrer-when-downgrade\naccess-control-allow-origin: *\naccess-control-expose-headers: x-amzn-RequestId,x-amzn-ErrorType,x-amzn-ErrorMessage,Date\ncontent-length: 4251\ncontent-type: application/x-amz-json-1.1\ndate: Thu, 27 Dec 2018 09:00:44 GMT\nstatus: 200\nx-amzn-requestid: e4ae75c8-09b5-11e9-aa3e-b9dcdb3443fe\nProvisional headers are shown\nContent-Type: application/x-amz-json-1.1\nOrigin: https://app.stage.slashlead.com\nReferer: https://app.stage.slashlead.com/auth/login\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\nX-Amz-Target: AWSCognitoIdentityProviderService.RespondToAuthChallenge\nX-Amz-User-Agent: aws-amplify/0.1.x js\n{ChallengeName: \"PASSWORD_VERIFIER\", ClientId: \"X\",\u2026}\nChallengeName: \"PASSWORD_VERIFIER\"\nChallengeResponses: {USERNAME: \"X\",\u2026}\nPASSWORD_CLAIM_SECRET_BLOCK: \"X\"\nPASSWORD_CLAIM_SIGNATURE: \"X\"\nTIMESTAMP: \"Thu Dec 27 09:11:52 UTC 2018\"\nUSERNAME: \"X\"\nClientId: \"X\"\n```\nWith --source-map enabled and no login\n```\nOPTIONS\nRequest URL: https://cognito-idp.us-east-1.amazonaws.com/\nRequest Method: OPTIONS\nStatus Code: 200 \nRemote Address: 52.45.149.48:443\nReferrer Policy: no-referrer-when-downgrade\naccess-control-allow-headers: content-type,x-amz-target,x-amz-user-agent\naccess-control-allow-methods: POST\naccess-control-allow-origin: *\naccess-control-expose-headers: x-amzn-RequestId,x-amzn-ErrorType,x-amzn-ErrorMessage,Date\naccess-control-max-age: 172800\ncontent-length: 0\ndate: Thu, 27 Dec 2018 09:11:57 GMT\nstatus: 200\nx-amzn-requestid: X\nProvisional headers are shown\nAccess-Control-Request-Headers: content-type,x-amz-target,x-amz-user-agent\nAccess-Control-Request-Method: POST\nOrigin: https://app.stage.slashlead.com\nReferer: https://app.stage.slashlead.com/auth/login\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\nPOST\nRequest URL: https://cognito-idp.us-east-1.amazonaws.com/\nRequest Method: POST\nStatus Code: 400 \nRemote Address: 52.45.149.48:443\nReferrer Policy: no-referrer-when-downgrade\naccess-control-allow-origin: *\naccess-control-expose-headers: x-amzn-RequestId,x-amzn-ErrorType,x-amzn-ErrorMessage,Date\ncontent-length: 79\ncontent-type: application/x-amz-json-1.1\ndate: Thu, 27 Dec 2018 09:11:57 GMT\nstatus: 400\nx-amzn-errormessage: Incorrect username or password.\nx-amzn-errortype: NotAuthorizedException:\nx-amzn-requestid: X\nProvisional headers are shown\nContent-Type: application/x-amz-json-1.1\nOrigin: https://app.stage.slashlead.com\nReferer: https://app.stage.slashlead.com/auth/login\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\nX-Amz-Target: AWSCognitoIdentityProviderService.RespondToAuthChallenge\nX-Amz-User-Agent: aws-amplify/0.1.x js\n{ChallengeName: \"PASSWORD_VERIFIER\", ClientId: \"X\",\u2026}\nChallengeName: \"PASSWORD_VERIFIER\"\nChallengeResponses: {USERNAME: \"X\",\u2026}\nPASSWORD_CLAIM_SECRET_BLOCK: \"X\"\nPASSWORD_CLAIM_SIGNATURE: \"X\"\nTIMESTAMP: \"Thu Dec 27 09:11:52 UTC 2018\"\nUSERNAME: \"X\"\nClientId: \"X\"\n```. Hi @AllanFly120 \nBuilding a new ng app will take me a bit more time, for now ... this is where the error takes place\nI hope this is enough info ...?\n```\npublic login(username: string, password: string) {\nreturn new Observable(observer => {\n\n  const authenticationData = {\n    Username: username,\n    Password: password,\n  };\n\n  const authenticationDetails = new AuthenticationDetails(authenticationData);\n\n  const userData = {\n    Username: username,\n    Pool: this.getUserPool()\n  };\n\n  const cognitoUser = new CognitoUser(userData);\n  const that = this;\n  cognitoUser.authenticateUser(authenticationDetails, {\n\n    onSuccess: function (result) {\n\n      const creds = that.buildCognitoCreds(result.getIdToken().getJwtToken());\n\n      AWS.config.credentials = creds;\n\n      const sts = new STS();\n      sts.getCallerIdentity(function (err, data) {\n        observer.next(result);\n        observer.complete();\n      });\n\n    },\n    onFailure: function (err) {\n      observer.error(err);\n      observer.complete();\n    },\n  });\n});\n\n}\n```\ngetUserPool() {\n    if (environment.cognito_idp_endpoint) {\n      AuthService.POOL_DATA.endpoint = environment.cognito_idp_endpoint;\n    }\n    return new CognitoUserPool(AuthService.POOL_DATA);\n  }\nI will get back in the new year about the new ng app. I'm sorry I haven't.\nI'm a frontend developer and I'm having trouble setting up a new environment without sharing our company aws account settings :-(. ",
    "thegnuu": "@srchase I run into the same issue today and was searching for a solution for about two hours.\nI would be able to create a new app and provide you some example code if that might be helpful for you to resolve this issue. Might take me a day or two to extract the needed code out of my application though.\nWould be great to have the source-maps for error-tracking reasons in our use case.... ",
    "pripatel93": "@srchase Running into the same issue. For me, the request seems to hang after \"PASSWORD_VERIFIER\" is requested.\nWorks perfectly in development build but fails using the production flag. I will continue to debug the issue to provide more information. ",
    "holtc": "@srchase yep!. @srchase yep!. ",
    "andrewryan1906": "```\nlet bucket = util.s3.resolveBucketName(config.s3.bucket_name);\n    let timelineRetrievalParams = {Bucket: bucket, Key: timelineKey};\nconsole.log(timelineRetrievalParams);\nconst timelineDownloadUri = s3Client.getSignedUrl('getObject', timelineRetrievalParams);\n\n```. So...\nAm I correct in assuming the reason why I've never observed this behavior before is that this is the first time in my code that I happen to NOT precede this call with an async s3Client operation?\nAlso, I'm just realizing per #1008 that there's no promise syntax on this method, so I have to use a callback, instead of the async/await..... The call is as follows:\n```\n  do {\n                    await sleep(1000);\n                executionDescription = await stepFunctions.describeExecution({executionArn: executionInfo.executionArn}).promise();\n                console.log('Received status: ' + JSON.stringify(executionDescription.status));\n\n            } while (executionDescription.status === 'RUNNING');\n\n```\nI don't know if it's the SDK or the API - the only way I interact with the API is through the SDK, and since it's intermittent, I can't try to replicate once it's observed. I do know, however, that the step function IS in fact returning an output.. Chase,\nI don't know if it's the SDK or not. But I'm running this through the Test apparatus provided by the AWS Console... so it feels like it's a problem in the transformation engine being used to modify lambda responses.\nHard to give code, since this happens deep inside of multiple layers of Node code... but this should be pretty easy to reproduce if you create a lambda that throws an error with newlines...\nOn 1/2/2019 11:30:00 AM, Chase Coalwell notifications@github.com wrote:\n@andrewryan1906 [https://github.com/andrewryan1906]\nDo you believe this an issue with the SDK? Can you provide a sample of your code?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub [https://github.com/aws/aws-sdk-js/issues/2466#issuecomment-450911186], or mute the thread [https://github.com/notifications/unsubscribe-auth/Aj7R4upwZmYfpAB9TmXKV9rhRPRkPACxks5u_N6IgaJpZM4ZmHIG].. ",
    "theneverstill": "\ud83d\udc4dThanks, I'll bundle it with my files.. ",
    "reefqi037": "I got an empty array in the Items response of getApis request.\njavascript\n{ Items: [] } \nI used the example PetStore API with regional endpoint.\nThe SDK version is 2.382.0.. I see.\nThe one that I should use is getRestApi right?\nJavascript\nvar params = {\n  restApiId: 'STRING_VALUE' /* required */\n};\napigateway.getRestApi(params, function(err, data) {\n  if (err) console.log(err, err.stack); // an error occurred\n  else     console.log(data);           // successful response\n});\nSorry for my misunderstanding and thank you for  your guidance!. ",
    "WilliamEddy": "Yes, I get the same behavior when running from the CLI\nnull. I have also posted a message on the forums. I was not sure if it was a code issue causing the strings in question to somehow not be sent to the service correctly.. ",
    "makoit": "\nCan you confirm the instance is in a running state when the subsequent call to get the public DNS name is made?\n\nThis is what I want to do with return await this.ec2.waitFor(\"instanceRunning\", waitParams). I want use this func to wait until the instance is running and then return the instanceId to a other service which get's the DNS name. But the wait function does not wait until the state of the instance is running. This is exactly the problem.\n\nWhich version of the SDK are you using?\n\nversion:  apiVersion: \"2016-11-15\"\n\nWhat state is given for the instance returned on return await this.ec2.waitFor(\"instanceRunning\", waitParams)?\n\nResponse from return await this.ec2.waitFor(\"instanceRunning\", waitParams)\nRequest {\n  domain: null,\n  service:\n   Service {\n     config:\n      Config {\n        credentials: [Object],\n        credentialProvider: [Object],\n        region: 'eu-central-1',\n        logger: null,\n        apiVersions: {},\n        apiVersion: '2016-11-15',\n        endpoint: 'ec2.eu-central-1.amazonaws.com',\n        httpOptions: [Object],\n        maxRetries: undefined,\n        maxRedirects: 10,\n        paramValidation: true,\n        sslEnabled: true,\n        s3ForcePathStyle: false,\n        s3BucketEndpoint: false,\n        s3DisableBodySigning: true,\n        computeChecksums: true,\n        convertResponseTypes: true,\n        correctClockSkew: false,\n        customUserAgent: null,\n        dynamoDbCrc32: true,\n        systemClockOffset: 0,\n        signatureVersion: 'v4',\n        signatureCache: true,\n        retryDelayOptions: {},\n        useAccelerateEndpoint: false,\n        clientSideMonitoring: false,\n        endpointDiscoveryEnabled: false,\n        endpointCacheSize: 1000 },\n     isGlobalEndpoint: false,\n     endpoint:\n      Endpoint {\n        protocol: 'https:',\n        host: 'ec2.eu-central-1.amazonaws.com',\n        port: 443,\n        hostname: 'ec2.eu-central-1.amazonaws.com',\n        pathname: '/',\n        path: '/',\n        href: 'https://ec2.eu-central-1.amazonaws.com/' },\n     _events: { apiCallAttempt: [Array], apiCall: [Array] },\n     MONITOR_EVENTS_BUBBLE: [Function: EVENTS_BUBBLE],\n     CALL_EVENTS_BUBBLE: [Function: CALL_EVENTS_BUBBLE],\n     _clientId: 1 },\n  operation: 'describeInstances',\n  params: { InstanceIds: [ 'i-020d69988e19b03f9' ] },\n  httpRequest:\n   HttpRequest {\n     method: 'POST',\n     path: '/',\n     headers: { 'User-Agent': 'aws-sdk-nodejs/2.353.0 win32/v8.12.0' },\n     body: '',\n     endpoint:\n      Endpoint {\n        protocol: 'https:',\n        host: 'ec2.eu-central-1.amazonaws.com',\n        port: 443,\n        hostname: 'ec2.eu-central-1.amazonaws.com',\n        pathname: '/',\n        path: '/',\n        href: 'https://ec2.eu-central-1.amazonaws.com/',\n        constructor: [Object] },\n     region: 'eu-central-1',\n     _userAgent: 'aws-sdk-nodejs/2.353.0 win32/v8.12.0' },\n  startTime: 2019-01-04T16:03:05.202Z,\n  response:\n   Response {\n     request: [Circular],\n     data: null,\n     error: null,\n     retryCount: 0,\n     redirectCount: 0,\n     httpResponse:\n      HttpResponse {\n        statusCode: undefined,\n        headers: {},\n        body: undefined,\n        streaming: false,\n        stream: null },\n     maxRetries: 40,\n     maxRedirects: 10 },\n  _asm:\n   AcceptorStateMachine {\n     currentState: 'validate',\n     states:\n      { validate: [Object],\n        build: [Object],\n        afterBuild: [Object],\n        sign: [Object],\n        retry: [Object],\n        afterRetry: [Object],\n        send: [Object],\n        validateResponse: [Object],\n        extractError: [Object],\n        extractData: [Object],\n        restart: [Object],\n        success: [Object],\n        error: [Object],\n        complete: [Object] } },\n  _haltHandlersOnError: false,\n  _events:\n   { validate:\n      [ [Function],\n        [Object],\n        [Function: VALIDATE_REGION],\n        [Function: BUILD_IDEMPOTENCY_TOKENS],\n        [Function: VALIDATE_PARAMETERS] ],\n     afterBuild:\n      [ [Object],\n        [Function: SET_CONTENT_LENGTH],\n        [Function: SET_HTTP_HOST] ],\n     restart: [ [Function: RESTART] ],\n     sign: [ [Function], [Object], [Object] ],\n     validateResponse: [ [Function: VALIDATE_RESPONSE], [Function] ],\n     send: [ [Object] ],\n     httpHeaders: [ [Function: HTTP_HEADERS] ],\n     httpData: [ [Function: HTTP_DATA] ],\n     httpDone: [ [Function: HTTP_DONE] ],\n     retry:\n      [ [Function: FINALIZE_ERROR],\n        [Function: INVALIDATE_CREDENTIALS],\n        [Function: EXPIRED_SIGNATURE],\n        [Function: CLOCK_SKEWED],\n        [Function: REDIRECT],\n        [Function: RETRY_CHECK],\n        [Function: API_CALL_ATTEMPT_RETRY],\n        [Function] ],\n     afterRetry: [ [Object] ],\n     build: [ [Function: buildRequest] ],\n     extractData:\n      [ [Function: extractData],\n        [Function: extractRequestId],\n        [Function: CHECK_ACCEPTORS] ],\n     extractError:\n      [ [Function: extractRequestId],\n        [Function: extractError],\n        [Function: CHECK_ACCEPTORS] ],\n     httpError: [ [Function: ENOTFOUND_ERROR] ],\n     success: [ [Function: API_CALL_ATTEMPT] ],\n     complete: [ [Function: API_CALL] ] },\n  emit: [Function: emit],\n  API_CALL_ATTEMPT: [Function: API_CALL_ATTEMPT],\n  API_CALL_ATTEMPT_RETRY: [Function: API_CALL_ATTEMPT_RETRY],\n  API_CALL: [Function: API_CALL],\n  _waiter:\n   constructor {\n     service:\n      Service {\n        config: [Object],\n        isGlobalEndpoint: false,\n        endpoint: [Object],\n        _events: [Object],\n        MONITOR_EVENTS_BUBBLE: [Function: EVENTS_BUBBLE],\n        CALL_EVENTS_BUBBLE: [Function: CALL_EVENTS_BUBBLE],\n        _clientId: 1 },\n     state: 'instanceRunning',\n     config:\n      { name: 'InstanceRunning',\n        operation: 'describeInstances',\n        delay: 15,\n        maxAttempts: 40,\n        acceptors: [Array] } } }. > The waitFor should be used with a .promise(). The request (for use with a callback) is being returned.\nAhh you're right! I changed my code to: return await this.ec2.waitFor(\"instanceRunning\", waitParams).promise(); . Now it's running correctly. The func waits until the instance is started.\nThank you very much for your competent and quick help!\n. ",
    "mungojam": "Ok, I did expect it might need a change under the covers but I'm surprised that authorising with AWS auth involves API gateway as it feels more fundamental than that.\nNow raised here. ",
    "Exitialis1": "Ah, that reference is clearer, I was going from here: https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/CognitoIdentityServiceProvider.html#adminUpdateUserAttributes-property \nWhere I was getting confused because the first line says that the return will be null where the next line says there will be data returned from the request. \nNow that I have confirmation that what I am seeing is intended behaviour it isn't so bad. But I would argue that when using the SDK directly I cannot see the HTTP 200 response so I have nothing explicit to say the operation worked. Normally in Javascript I associate a blank response with something went so badly wrong you didn't even catch the error. \nJust getting the attributes changed back would be really useful, it is explicit vs an explicit error message and is what I need to return through my GraphQL API anyway.. ",
    "SeongwoonHong": "@srchase Thank you for your response... but I've tried to add Content-Encoding header on axios.put\nbut it's still giving me the same error...\n```\nconst sendPresignedUrlWithFile = async (url: string, file) => {\n  try {\n    const result = await axios.put(url, file, {\n      headers: {\n        'Content-Type': 'image/jpeg',\n        'Content-Encoding': 'base64',\n         // even tried this\n        // ConentEncoding: 'base64'\n      }\n    });\n    return result;\n  }\ncatch (e) {\n    console.log(e.message);\n    throw new Error(e);\n  }\n}\nthank you. @srchase yes! Thank you!  without body, it is working.. it is saved on s3 bucket but..  the thing is the image file is broken..  like this!\n<img width=\"672\" alt=\"screen shot 2019-01-11 at 9 00 34 pm\" src=\"https://user-images.githubusercontent.com/19713918/51067846-fd1ff400-15e3-11e9-9a33-16a95c11dda1.png\">\n. i've tried multiple images and all of them are broken.. I'm assuming the way i save (or send) base64 is wrong..??. oh I got it fixed..\nconst sendPresignedUrlWithFile = async (url: string, file) => {\n  try {\n    const type = file.split(';')[0].split('/')[1]\n    const buffer = Buffer.from(file.replace(/^data:image\\/\\w+;base64,/, \"\"),'base64');\n    const result = await axios.put(url, buffer, {\n      headers: {\n        'Content-Type': 'image/jpeg',\n        'Content-Encoding': 'base64'\n      },\n    });\n    return result;\n  }...\nI really appreciate it!. @srchase \nit's in try catch and doesn't throw any error. @srchase \nI have no access to add code to test headObject for now but this works. (when i get signedUrl from s3)\nconst presignedUrl = await s3.getSignedUrl('putObject', {\n      Bucket,\n      Key: key,\n      ContentEncoding: 'base64',\n      ContentType: image/${type},\n    });\n. @srchase \nheadObject returns this\nRequest {\n  domain: null,\n  service:\n   Service {\n     config:\n      Config {\n        credentials:\n         Credentials {\n           expired: false,\n           expireTime: null,\n           refreshCallbacks: [],\n           accessKeyId: 'something',\n           sessionToken: undefined },\n        credentialProvider:\n         CredentialProviderChain {\n           providers: [ [Function], [Function], [Function], [Function] ] },\n        region: 'ca-central-1',\n        logger: null,\n        apiVersions: {},\n        apiVersion: null,\n        endpoint: 's3.ca-central-1.amazonaws.com',\n        httpOptions: { timeout: 120000 },\n        maxRetries: undefined,\n        maxRedirects: 10,\n        paramValidation: true,\n        sslEnabled: true,\n        s3ForcePathStyle: false,\n        s3BucketEndpoint: false,\n        s3DisableBodySigning: true,\n        computeChecksums: true,\n        convertResponseTypes: true,\n        correctClockSkew: false,\n        customUserAgent: null,\n        dynamoDbCrc32: true,\n        systemClockOffset: 0,\n        signatureVersion: 'v4',\n        signatureCache: true,\n        retryDelayOptions: {},\n        useAccelerateEndpoint: false,\n        clientSideMonitoring: false,\n        endpointDiscoveryEnabled: false,\n        endpointCacheSize: 1000,\n        hostPrefixEnabled: true,\n        accessKeyId: 'something',\n        secretAccessKey: 'something' },\n     isGlobalEndpoint: false,\n     endpoint:\n      Endpoint {\n        protocol: 'https:',\n        host: 's3.ca-central-1.amazonaws.com',\n        port: 443,\n        hostname: 's3.ca-central-1.amazonaws.com',\n        pathname: '/',\n        path: '/',\n        href: 'https://s3.ca-central-1.amazonaws.com/' },\n     _events:\n      { apiCallAttempt: [ [Function: EVENTS_BUBBLE] ],\n        apiCall: [ [Function: CALL_EVENTS_BUBBLE] ] },\n     MONITOR_EVENTS_BUBBLE: [Function: EVENTS_BUBBLE],\n     CALL_EVENTS_BUBBLE: [Function: CALL_EVENTS_BUBBLE],\n     _clientId: 1 },\n  operation: 'headObject',\n  params:\n   { Bucket: 'jmt-bucket9551',\n     Key:\n      'vukexuwav@321-email.com/fb5e3590-2101-11e9-8a4e-d3824ac500e4.jpeg' },\n  httpRequest:\n   HttpRequest {\n     method: 'POST',\n     path: '/',\n     headers: { 'User-Agent': 'aws-sdk-nodejs/2.375.0 darwin/v11.2.0' },\n     body: '',\n     endpoint:\n      Endpoint {\n        protocol: 'https:',\n        host: 's3.ca-central-1.amazonaws.com',\n        port: 443,\n        hostname: 's3.ca-central-1.amazonaws.com',\n        pathname: '/',\n        path: '/',\n        href: 'https://s3.ca-central-1.amazonaws.com/',\n        constructor: { [Function: Endpoint] super: [Function: Object] } },\n     region: 'ca-central-1',\n     _userAgent: 'aws-sdk-nodejs/2.375.0 darwin/v11.2.0' },\n  startTime: 2019-01-31T23:17:27.175Z,\n  response:\n   Response {\n     request: [Circular],\n     data: null,\n     error: null,\n     retryCount: 0,\n     redirectCount: 0,\n     httpResponse:\n      HttpResponse {\n        statusCode: undefined,\n        headers: {},\n        body: undefined,\n        streaming: false,\n        stream: null },\n     maxRetries: 3,\n     maxRedirects: 10 },\n  _asm:\n   AcceptorStateMachine {\n     currentState: 'validate',\n     states:\n      { validate:\n         { accept: 'build', fail: 'error', fn: [Function: transition] },\n        build:\n         { accept: 'afterBuild',\n           fail: 'restart',\n           fn: [Function: transition] },\n        afterBuild:\n         { accept: 'sign', fail: 'restart', fn: [Function: transition] },\n        sign:\n         { accept: 'send', fail: 'retry', fn: [Function: transition] },\n        retry:\n         { accept: 'afterRetry',\n           fail: 'afterRetry',\n           fn: [Function: transition] },\n        afterRetry:\n         { accept: 'sign', fail: 'error', fn: [Function: transition] },\n        send:\n         { accept: 'validateResponse',\n           fail: 'retry',\n           fn: [Function: transition] },\n        validateResponse:\n         { accept: 'extractData',\n           fail: 'extractError',\n           fn: [Function: transition] },\n        extractError:\n         { accept: 'extractData',\n           fail: 'retry',\n           fn: [Function: transition] },\n        extractData:\n         { accept: 'success', fail: 'retry', fn: [Function: transition] },\n        restart:\n         { accept: 'build', fail: 'error', fn: [Function: transition] },\n        success:\n         { accept: 'complete',\n           fail: 'complete',\n           fn: [Function: transition] },\n        error:\n         { accept: 'complete',\n           fail: 'complete',\n           fn: [Function: transition] },\n        complete: { accept: null, fail: null, fn: [Function: transition] } } },\n  _haltHandlersOnError: false,\n  _events:\n   { validate:\n      [ [Function],\n        [Function: validateBucketName],\n        { [Function: VALIDATE_CREDENTIALS] _isAsync: true },\n        [Function: BUILD_IDEMPOTENCY_TOKENS],\n        [Function: VALIDATE_PARAMETERS],\n        [Function: validateScheme],\n        [Function: validateBucketEndpoint],\n        [Function: correctBucketRegionFromCache] ],\n     afterBuild:\n      [ [Function: SET_CONTENT_LENGTH],\n        [Function: SET_HTTP_HOST],\n        [Function: addExpect100Continue],\n        [Function: disableBodySigning] ],\n     restart: [ [Function: RESTART] ],\n     sign:\n      [ [Function],\n        { [Function: discoverEndpoint] _isAsync: true },\n        { [Function: SIGN] _isAsync: true } ],\n     validateResponse: [ [Function: VALIDATE_RESPONSE], [Function] ],\n     send: [ { [Function: SEND] _isAsync: true } ],\n     httpHeaders: [ [Function: HTTP_HEADERS] ],\n     httpData: [ [Function: HTTP_DATA] ],\n     httpDone: [ [Function: HTTP_DONE] ],\n     retry:\n      [ [Function: FINALIZE_ERROR],\n        [Function: INVALIDATE_CREDENTIALS],\n        [Function: EXPIRED_SIGNATURE],\n        [Function: CLOCK_SKEWED],\n        [Function: REDIRECT],\n        [Function: RETRY_CHECK],\n        [Function: API_CALL_ATTEMPT_RETRY] ],\n     afterRetry: [ { [Function: RESET_RETRY_STATE] _isAsync: true } ],\n     build:\n      [ [Function: buildRequest],\n        [Function: addContentType],\n        [Function: populateURI],\n        [Function: computeContentMd5],\n        [Function: computeSseCustomerKeyMd5] ],\n     extractData:\n      [ [Function: extractData],\n        [Function: extractRequestId],\n        [Function: extractData],\n        [Function: hoistPayloadMember] ],\n     extractError:\n      [ [Function: extractError],\n        [Function: extractRequestId],\n        [Function: extractError],\n        { [Function: requestBucketRegion] _isAsync: true } ],\n     httpError: [ [Function: ENOTFOUND_ERROR] ],\n     beforePresign: [ [Function: prepareSignedUrl] ],\n     success: [ [Function: API_CALL_ATTEMPT] ],\n     complete: [ [Function: API_CALL] ] },\n  emit: [Function: emit],\n  API_CALL_ATTEMPT: [Function: API_CALL_ATTEMPT],\n  API_CALL_ATTEMPT_RETRY: [Function: API_CALL_ATTEMPT_RETRY],\n  API_CALL: [Function: API_CALL] }\n```. @AllanFly120 Thank you! I completely didn't know about this.. Thank you I'll close this ticket. ",
    "bugb": "@srchase thank you a lot for your effort I will try this!. @srchase Thank you a lot, it work like a charm, but I need to manual create the new policy for access Cost Explorer API, because the default Billing access policy does not provide us permission to call to Cost explorer API, it like this:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"aws-portal:*Billing\",\n                \"awsbillingconsole:*Billing\",\n                \"aws-portal:*Usage\",\n                \"awsbillingconsole:*Usage\",\n                \"aws-portal:*PaymentMethods\",\n                \"awsbillingconsole:*PaymentMethods\",\n                \"budgets:ViewBudget\",\n                \"budgets:ModifyBudget\",\n                \"cur:*\",\n                \"ce:*\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\nThe line that I added is  \"ce:*\".\nBy the way, I found it is very hard to debug lambda function because the environment for Lambda and environment in my computer is too different, could you please suggest me any approach to deal with it?\n. @srchase Thank you a lot!. ",
    "amitguptagwl": "Thanks @srchase \nHere is the tabular representation.\nfile size |  fxp 3.0 parser (rps) | xml2js 0.4.19 (rps)\n-- |  -- | --\n1.5k  | 14032.09323 | 4615.930805\n1.5m  | 13.23366098 | 5.90682005\n13m  | 1.135582008 | -1\n1.3k with CDATA  | 43160.52342 | 8398.556349\n1.3m with CDATA  | 52.68877009 | 7.966000795\n1.6k with cdata,prolog,doctype  | 41433.98547 | 7872.399268\n98m  | 0.2600104004 | -1\nAs you can notice, in case of large file xml2js gives an error.. ",
    "ricardolpd": "@srchase,\nSorry forgot to put those in:\nThe version of sdk in my machine is: \"2.387.0\"\nI am on a MAC 10.14.2 (18C54) using node 9.2.0 for the debug session.\nthe code used is here:\nconst conversationUnmarshalled = JSON.parse(item);\n      console.log(item);\n      const options = {\n        convertEmptyValues: true,\n        wrapNumbers: false,\n      };\n      const conversation = converter.unmarshall(conversationUnmarshalled, options);\n      conversations.push(conversation);\nThe following example uses the marshalling example from the SDK page, and the same happens this last bit was run in the REPL.it website.\n```\nconst converter = require('aws-sdk').DynamoDB.Converter;\nconst item = {\n  string: {'s': 'foo'},\n  list: { 'l': [{ 's': 'fizz'}, { 's': 'buzz'}, { 's': 'pop'}]},\n  map: {\n    'm': {\n      nestedMap: {\n        'm': {\n          key: {'s': 'value'}\n        }\n      }\n    }\n  },\n  number: { 'n': '123'},\n  nullValue: { 'nULLValue': true},\n  boolValue: { 'nULLValue': true}\n};\nconst conversation = converter.unmarshall(item);\nconsole.log(JSON.stringify(conversation));\n``\nthe output is:{}`\nAlso, while i cannot find an exhaustive list of data types for javascript in the documentation i managed to find this page for the java tools:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBMapper.DataTypes.html\nin here i can see that the types are as per the sdk however when exporting a dynamodb table using aws dataPipeline:\nS -> s\nL -> l\nM -> m\nN -> n\nNULL -> nULLValue\nBOOL -> bOOL\nMy tables do not use BS, NS, B types, so i don't know what these types would have.\n. @srchase What i meant is that if i use the unmarshall example using the lowercases like i get when i export dynamodb records, it wont work. it was meant to be a more generic example.\n. @srchase i created the pipeline through the console using the templates (as an initial measure)\nthe json generated is the following:\n{\n  \"objects\": [\n    {\n      \"bootstrapAction\": \"s3://#{myDDBRegion}.elasticmapreduce/bootstrap-actions/configure-hadoop, --yarn-key-value,yarn.nodemanager.resource.memory-mb=11520,--yarn-key-value,yarn.scheduler.maximum-allocation-mb=11520,--yarn-key-value,yarn.scheduler.minimum-allocation-mb=1440,--yarn-key-value,yarn.app.mapreduce.am.resource.mb=2880,--mapred-key-value,mapreduce.map.memory.mb=5760,--mapred-key-value,mapreduce.map.java.opts=-Xmx4608M,--mapred-key-value,mapreduce.reduce.memory.mb=2880,--mapred-key-value,mapreduce.reduce.java.opts=-Xmx2304m,--mapred-key-value,mapreduce.map.speculative=false\",\n      \"name\": \"EmrClusterForBackup\",\n      \"coreInstanceType\": \"m1.large\",\n      \"coreInstanceCount\": \"1\",\n      \"amiVersion\": \"3.9.0\",\n      \"id\": \"EmrClusterForBackup\",\n      \"masterInstanceType\": \"m1.large\",\n      \"region\": \"#{myDDBRegion}\",\n      \"type\": \"EmrCluster\",\n      \"terminateAfter\": \"10 Minutes\"\n    },\n    {\n      \"failureAndRerunMode\": \"CASCADE\",\n      \"resourceRole\": \"DataPipelineDefaultResourceRole\",\n      \"role\": \"DataPipelineDefaultRole\",\n      \"pipelineLogUri\": \"s3://somebucket/\",\n      \"scheduleType\": \"ONDEMAND\",\n      \"name\": \"Default\",\n      \"id\": \"Default\"\n    },\n    {\n      \"output\": {\n        \"ref\": \"S3BackupLocation\"\n      },\n      \"input\": {\n        \"ref\": \"DDBSourceTable\"\n      },\n      \"maximumRetries\": \"2\",\n      \"name\": \"TableBackupActivity\",\n      \"step\": \"s3://dynamodb-emr-#{myDDBRegion}/emr-ddb-storage-handler/2.1.0/emr-ddb-2.1.0.jar,org.apache.hadoop.dynamodb.tools.DynamoDbExport,#{output.directoryPath},#{input.tableName},#{input.readThroughputPercent}\",\n      \"id\": \"TableBackupActivity\",\n      \"runsOn\": {\n        \"ref\": \"EmrClusterForBackup\"\n      },\n      \"type\": \"EmrActivity\",\n      \"resizeClusterBeforeRunning\": \"true\"\n    },\n    {\n      \"readThroughputPercent\": \"#{myDDBReadThroughputRatio}\",\n      \"name\": \"DDBSourceTable\",\n      \"id\": \"DDBSourceTable\",\n      \"type\": \"DynamoDBDataNode\",\n      \"tableName\": \"#{myDDBTableName}\"\n    },\n    {\n      \"directoryPath\": \"#{myOutputS3Loc}/#{format(@scheduledStartTime, 'YYYY-MM-dd-HH-mm-ss')}\",\n      \"name\": \"S3BackupLocation\",\n      \"id\": \"S3BackupLocation\",\n      \"type\": \"S3DataNode\"\n    }\n  ],\n  \"parameters\": [\n    {\n      \"description\": \"Output S3 folder\",\n      \"id\": \"myOutputS3Loc\",\n      \"type\": \"AWS::S3::ObjectKey\"\n    },\n    {\n      \"description\": \"Source DynamoDB table name\",\n      \"id\": \"myDDBTableName\",\n      \"type\": \"String\"\n    },\n    {\n      \"default\": \"0.25\",\n      \"watermark\": \"Enter value between 0.1-1.0\",\n      \"description\": \"DynamoDB read throughput ratio\",\n      \"id\": \"myDDBReadThroughputRatio\",\n      \"type\": \"Double\"\n    },\n    {\n      \"default\": \"us-east-1\",\n      \"watermark\": \"us-east-1\",\n      \"description\": \"Region of the DynamoDB table\",\n      \"id\": \"myDDBRegion\",\n      \"type\": \"String\"\n    }\n  ],\n  \"values\": {\n    \"myDDBRegion\": \"eu-west-1\",\n    \"myDDBTableName\": \"Conversation\",\n    \"myDDBReadThroughputRatio\": \"0.25\",\n    \"myOutputS3Loc\": \"s3://somebucket/\"\n  }\n}\n. Thanks for checking this for me, so are you saying that the data pipeline doesnt use any of the DynamoDB sdks?\nIts not just the lowercase that is the issue, the values are considerably different for null values, as i described above.\nWill the issue be tracked here, also which api have an update? so i can track the issue, and remove the conversion code.\n. ",
    "CarsonMcKinstry": "Cool cool!\nWhich branch/package will the s3 functions be under? I can't seem to figure out which one.. ",
    "yigaldviri": "Is there any estimation regarding when V3 will be in production?. Is there any estimation regarding when V3 will be in production?. ",
    "Clete2": "@srchase absolutely. I\u2019ll give you one first thing Monday morning when I boot up my laptop. . @srchase absolutely. I\u2019ll give you one first thing Monday morning when I boot up my laptop. . I have a Lambda which consumes an event stream from Pinpoint. I will eventually use it for SMS, e-mail, and push. For now I am just using SMS. I need to consume those events emitted from Pinpoint and I want to use TypeScript.\nHere's an example SMS event that I receive from Pinpoint into my Kinesis stream:\njson\n{\n    \"event_type\": \"_SMS.SUCCESS\",\n    \"event_timestamp\": 1547839009132,\n    \"arrival_timestamp\": 1547839008948,\n    \"event_version\": \"3.1\",\n    \"application\": {\n        \"app_id\": \"<snip>\",\n        \"sdk\": {}\n    },\n    \"client\": {\n        \"client_id\": \"<snip>\"\n    },\n    \"device\": {\n        \"platform\": {}\n    },\n    \"session\": {},\n    \"attributes\": {\n        \"sender_request_id\": \"<snip UUID>\",\n        \"record_status\": \"DELIVERED\",\n        \"iso_country_code\": \"US\",\n        \"number_of_message_parts\": \"1\",\n        \"message_id\": \"<snip>\",\n        \"message_type\": \"Transactional\"\n    },\n    \"metrics\": {\n        \"price_in_millicents_usd\": 645\n    },\n    \"awsAccountId\": \"<snip>\"\n}\nIf you look in the docs that I linked, you can see discrepancies between this event and the event that I've pasted above.\nSomething like this:\ntypescript\nexport interface PinpointSMSEvent {\n  event_type: '_SMS.SEND' | '_SMS.SUCCESS' | '_SMS.FAIL' | '_SMS.OPTOUT';\n  arrival_timestamp: number;\n  event_timestamp: number;\n  event_version: string;\n  application: { app_id: string; sdk: object };\n  client: { client_id: string };\n  device: { platform: object };\n  session: object;\n  attributes: { [key: string]: any };\n  metrics: { [key: string]: any };\n  awsAccountId: string;\n}. I have a Lambda which consumes an event stream from Pinpoint. I will eventually use it for SMS, e-mail, and push. For now I am just using SMS. I need to consume those events emitted from Pinpoint and I want to use TypeScript.\nHere's an example SMS event that I receive from Pinpoint into my Kinesis stream:\njson\n{\n    \"event_type\": \"_SMS.SUCCESS\",\n    \"event_timestamp\": 1547839009132,\n    \"arrival_timestamp\": 1547839008948,\n    \"event_version\": \"3.1\",\n    \"application\": {\n        \"app_id\": \"<snip>\",\n        \"sdk\": {}\n    },\n    \"client\": {\n        \"client_id\": \"<snip>\"\n    },\n    \"device\": {\n        \"platform\": {}\n    },\n    \"session\": {},\n    \"attributes\": {\n        \"sender_request_id\": \"<snip UUID>\",\n        \"record_status\": \"DELIVERED\",\n        \"iso_country_code\": \"US\",\n        \"number_of_message_parts\": \"1\",\n        \"message_id\": \"<snip>\",\n        \"message_type\": \"Transactional\"\n    },\n    \"metrics\": {\n        \"price_in_millicents_usd\": 645\n    },\n    \"awsAccountId\": \"<snip>\"\n}\nIf you look in the docs that I linked, you can see discrepancies between this event and the event that I've pasted above.\nSomething like this:\ntypescript\nexport interface PinpointSMSEvent {\n  event_type: '_SMS.SEND' | '_SMS.SUCCESS' | '_SMS.FAIL' | '_SMS.OPTOUT';\n  arrival_timestamp: number;\n  event_timestamp: number;\n  event_version: string;\n  application: { app_id: string; sdk: object };\n  client: { client_id: string };\n  device: { platform: object };\n  session: object;\n  attributes: { [key: string]: any };\n  metrics: { [key: string]: any };\n  awsAccountId: string;\n}. @AllanFly120 No, it is not an API endpoint. It is a Kinesis stream event that Pinpoint generates for analytical purposes.\nAre you thinking it belongs in @types/aws-lambda?. @AllanFly120 No, it is not an API endpoint. It is a Kinesis stream event that Pinpoint generates for analytical purposes.\nAre you thinking it belongs in @types/aws-lambda?. @AllanFly120 Yes the event is from the Lambda Kinesis trigger which is generated from Pinpoint. You can find more info on this page about Pinpoint Events via Kinesis.\nIf only API calls are in scope for the SDK I am thinking it should be added to @types/aws-lambda within DefinitelyTyped. They maintain types for all the lambda kinesis triggers -- at least as far as I can tell. Do you know anything about that project or whether it falls in their scope instead?. @AllanFly120 Yes the event is from the Lambda Kinesis trigger which is generated from Pinpoint. You can find more info on this page about Pinpoint Events via Kinesis.\nIf only API calls are in scope for the SDK I am thinking it should be added to @types/aws-lambda within DefinitelyTyped. They maintain types for all the lambda kinesis triggers -- at least as far as I can tell. Do you know anything about that project or whether it falls in their scope instead?. ",
    "thenickdude": "I think the confusion arose because in the single-part upload case, the tags are included in an HTTP header in a PUT call, where they must be URL-escaped, but in the multi-part case the tags are included in a call to PUT object tagging where they appear in an XML body and must not be URL-escaped.. How does this \"require further discussion\"? In 2.378 it was possible to upload files with tags containing spaces, in both single and multi-part mode, because both of them used appropriate escaping. In 2.379, that became impossible.. ",
    "VTLee": "I was incorrect on my SDK version, but looks like it still falls under #2433 -- We were running 2.377 on the container. Will try latest.. I was incorrect on my SDK version, but looks like it still falls under #2433 -- We were running 2.377 on the container. Will try latest.. ",
    "Salinn": "@srchase it looks like it is on our artifactory as other teams are having issues with this library where sometimes it installs and sometimes it throws the error above. I will close this as it is more likely than not to be an issue with artifactory. . ",
    "sajithvim": "@srchase Thanks for your quick response. My lambda functions are running in the Sydney region. But I am setting the region forcibly due to the SDK limitation mentioned above.. ",
    "tan31989": "@srchase can this not be updated in the documentation?\nCan this be raised as a PR?. I thought the documentation has definitive one-liner example for those filters. So thought if raised a PR which would be showcasing how to use the filters.\nHow do you think that example bit would be for the PR?. Great, @srchase . \nThanks for the update. I will try to come up with a PR for this. . Hi,\nI see the failure is for node version 0.10.x\nIn this case, since according to Node, the maintenance end date was 2016-10-31, does it make sense to run the tests for the older version?\nRegards,\nNagaraj. @AllanFly120 I will verify for the existing solution and get it fixed.. @AllanFly120 I have downgraded the version but made sure the high vulnerability is removed. Let me know if something else is needed here.. ",
    "mblag": "thanks for the reply, the issue turned out to be pre-deployed older version of aws-sdk along the docker image that somehow messed up the setup. making sure the modules are the current version solves the issue.. ",
    "nickyang07": "@srchase , thanks! It will be really helpful if AWS can keep dynamodb-local updated as much as possible. . ",
    "Bunnoo": "I use node 8.11.3 and aws-sdk 2.395.0\nThe code that uses the SDK is as follows:\njavascript\nvar AWS = require('aws-sdk');\nAWS.config.update({region: 'eu-west-3'});\nThe AWS object is then used to define a callback function (as data of a Vue component), that is passed as a property to a child component, which is my contact form:\n```javascript\ndata: function () {\n    return {\n      submitHandler: function (form) {\n        var params = {\n          Destination: { \n            ToAddresses: ['me@example.com']\n          },\n          Message: {\n            Body: {\n              Html: {\n               Charset: \"UTF-8\",\n               Data: \"HTML_FORMAT_BODY\"\n              },\n              Text: {\n               Charset: \"UTF-8\",\n               Data: \"TEXT_FORMAT_BODY\"\n              }\n             },\n             Subject: {\n              Charset: 'UTF-8',\n              Data: 'Contact form'\n             }\n            },\n          Source: 'me@example.com'\n        };\n    // Create the promise and SES service object\n    var sendPromise = new AWS.SES().sendEmail(params).promise();\n\n    // Handle promise's fulfilled/rejected states\n    sendPromise.then(\n      function(data) {\n        console.log(data.MessageId);\n      }).catch(\n        function(err) {\n        console.error(err, err.stack);\n      });\n  }\n}\n\n}\n```. ",
    "ebower12": "Never mind, found the stuff about the kinesis-settings options. It appears kinesis was just never added to the list of possible engine types in any of the documentation.. ",
    "CooperWolfe": "I get the same behavior for both, yes. Same error message, just replace us-east-1 with those regions.. I get the same behavior for both, yes. Same error message, just replace us-east-1 with those regions.. Setting the region in the client directly gave me the same results. Here is the result of console.log-ing the SES client:\n{\n    \"config\": {\n        \"credentials\": {\n            \"expired\": false,\n            \"expireTime\": null,\n            \"refreshCallbacks\": [],\n            \"accessKeyId\": <accessKeyId>\n        },\n        \"credentialProvider\": {\n            \"providers\": [\n                null,\n                null,\n                null,\n                null\n            ],\n            \"resolveCallbacks\": []\n        },\n        \"region\": \"us-east-1\",\n        \"logger\": null,\n        \"apiVersions\": {},\n        \"apiVersion\": null,\n        \"endpoint\": \"email.us-east-1.amazonaws.com\",\n        \"httpOptions\": {\n            \"timeout\": 120000\n        },\n        \"maxRedirects\": 10,\n        \"paramValidation\": true,\n        \"sslEnabled\": true,\n        \"s3ForcePathStyle\": false,\n        \"s3BucketEndpoint\": false,\n        \"s3DisableBodySigning\": true,\n        \"computeChecksums\": true,\n        \"convertResponseTypes\": true,\n        \"correctClockSkew\": false,\n        \"customUserAgent\": null,\n        \"dynamoDbCrc32\": true,\n        \"systemClockOffset\": 0,\n        \"signatureVersion\": \"v4\",\n        \"signatureCache\": true,\n        \"retryDelayOptions\": {},\n        \"useAccelerateEndpoint\": false,\n        \"clientSideMonitoring\": false,\n        \"endpointDiscoveryEnabled\": false,\n        \"endpointCacheSize\": 1000,\n        \"hostPrefixEnabled\": true\n    },\n    \"isGlobalEndpoint\": false,\n    \"endpoint\": {\n        \"protocol\": \"https:\",\n        \"host\": \"email.us-east-1.amazonaws.com\",\n        \"port\": 443,\n        \"hostname\": \"email.us-east-1.amazonaws.com\",\n        \"pathname\": \"/\",\n        \"path\": \"/\",\n        \"href\": \"https://email.us-east-1.amazonaws.com/\"\n    },\n    \"_events\": {\n        \"apiCallAttempt\": [\n            null\n        ],\n        \"apiCall\": [\n            null\n        ]\n    },\n    \"_clientId\": 1\n}. Setting the region in the client directly gave me the same results. Here is the result of console.log-ing the SES client:\n{\n    \"config\": {\n        \"credentials\": {\n            \"expired\": false,\n            \"expireTime\": null,\n            \"refreshCallbacks\": [],\n            \"accessKeyId\": <accessKeyId>\n        },\n        \"credentialProvider\": {\n            \"providers\": [\n                null,\n                null,\n                null,\n                null\n            ],\n            \"resolveCallbacks\": []\n        },\n        \"region\": \"us-east-1\",\n        \"logger\": null,\n        \"apiVersions\": {},\n        \"apiVersion\": null,\n        \"endpoint\": \"email.us-east-1.amazonaws.com\",\n        \"httpOptions\": {\n            \"timeout\": 120000\n        },\n        \"maxRedirects\": 10,\n        \"paramValidation\": true,\n        \"sslEnabled\": true,\n        \"s3ForcePathStyle\": false,\n        \"s3BucketEndpoint\": false,\n        \"s3DisableBodySigning\": true,\n        \"computeChecksums\": true,\n        \"convertResponseTypes\": true,\n        \"correctClockSkew\": false,\n        \"customUserAgent\": null,\n        \"dynamoDbCrc32\": true,\n        \"systemClockOffset\": 0,\n        \"signatureVersion\": \"v4\",\n        \"signatureCache\": true,\n        \"retryDelayOptions\": {},\n        \"useAccelerateEndpoint\": false,\n        \"clientSideMonitoring\": false,\n        \"endpointDiscoveryEnabled\": false,\n        \"endpointCacheSize\": 1000,\n        \"hostPrefixEnabled\": true\n    },\n    \"isGlobalEndpoint\": false,\n    \"endpoint\": {\n        \"protocol\": \"https:\",\n        \"host\": \"email.us-east-1.amazonaws.com\",\n        \"port\": 443,\n        \"hostname\": \"email.us-east-1.amazonaws.com\",\n        \"pathname\": \"/\",\n        \"path\": \"/\",\n        \"href\": \"https://email.us-east-1.amazonaws.com/\"\n    },\n    \"_events\": {\n        \"apiCallAttempt\": [\n            null\n        ],\n        \"apiCall\": [\n            null\n        ]\n    },\n    \"_clientId\": 1\n}. That was it, I was using remote hosting and requested access to the domain. Thank you very much.. That was it, I was using remote hosting and requested access to the domain. Thank you very much.. ",
    "dannyskim": "@srchase seeing as how I can replicate this issue in a brand new RN project via react-native init ... and simply adding the import statement, my project is being currently held up from being updated and deployed live to app stores.\nDo you or another dev member have a hard suggestion on which RN version I should utilize for the time being where this issue is not present?. @srchase \nIt appears that my project exhibits the same behavior. If Metro Bundler is left alone after attempting a build, about 3 - 5 minutes after my project red screens, I see Metro spit out the following:\ntransform[stdout]: \ntransform[stdout]: <--- Last few GCs --->\ntransform[stdout]: \ntransform[stdout]: [88606:0x103000000]   220175 ms: Mark-sweep 1250.3 (1470.4) -> 1250.2 (1470.4) MB, 2329.9 / 0.0 ms  allocation failure GC in old space requested\ntransform[stdout]: [88606:0x103000000]   222217 ms: Mark-sweep 1250.2 (1470.4) -> 1250.2 (1431.4) MB, 2041.9 / 0.0 ms  last resort GC in old space requested\ntransform[stdout]: [88606:0x103000000]   224452 ms: Mark-sweep 1250.2 (1431.4) -> 1250.2 (1426.9) MB, 2235.4 / 0.0 ms  last resort GC in old space requested\ntransform[stdout]: \ntransform[stdout]: \ntransform[stdout]: <--- JS stacktrace --->\ntransform[stdout]: \ntransform[stdout]: ==== JS stack trace =========================================\ntransform[stdout]: \ntransform[stdout]: Security context: 0x3605f07a5879 <JSObject>\ntransform[stdout]:     1: append [<PROJECT_PATH>/node_modules/@babel/generator/lib/buffer.js:~74] [pc=0x2755ec55b7cb](this=0x3605fdbe7071 <Buffer map = 0x360508150f91>,str=0x3605f07837e9 <String[7]: refresh>)\ntransform[stdout]:     2: Identifier [<PROJECT_PATH>/node_modules/@babel/generator/lib/generators/types.js:~46] [pc=0x275...\ntransform[stdout]: \ntransform[stderr]: FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory\ntransform[stderr]:  1: node::Abort() [/usr/local/bin/node]\ntransform[stderr]:  2: node::FatalException(v8::Isolate*, v8::Local<v8::Value>, v8::Local<v8::Message>) [/usr/local/bin/node]\ntransform[stderr]:  3: \ntransform[stderr]: v8::internal::V8::FatalProcessOutOfMemory(char const*, bool) [/usr/local/bin/node]\ntransform[stderr]:  4: \ntransform[stderr]: v8::internal::Factory::NewUninitializedFixedArray(int) [/usr/local/bin/node]\ntransform[stderr]:  5: \ntransform[stderr]: v8::internal::(anonymous namespace)::ElementsAccessorBase<v8::internal::(anonymous namespace)::FastPackedObjectElementsAccessor, v8::internal::(anonymous namespace)::ElementsKindTraits<(v8::internal::ElementsKind)2> >::GrowCapacity(v8::internal::Handle<v8::internal::JSObject>, unsigned int) [/usr/local/bin/node]\ntransform[stderr]:  6: \ntransform[stderr]: v8::internal::Runtime_GrowArrayElements(int, v8::internal::Object**, v8::internal::Isolate*) [/usr/local/bin/node]\ntransform[stderr]:  7: 0x2755ec4042fd\ntransform[stderr]:  8: 0x2755ec55b7cb. @srchase \nAs advised I ran export NODE_OPTIONS=\"--max-old-space-size=4096\" (ran with 2GB and 4GB). Still exhibits similar behavior to my previous post.\nAfter running the above command in bash, I verified by running export and made sure that my env variable was set, and it was both times.. @srchase \nI've reverted to utilizing v2.318.0 with RN v0.56.1 and the errors have gone away. I will keep you updated as I am now going through the process of eliminating errors and dealing with deprecations from updating other libraries associated with the project.\nI will be available to assist you in any way that I can while you are still investigating the original issue if you should so need.. ",
    "kevinresol": "I can confirm this is still happening with 2.404.0 (RN 0.58.3). Is there a rough ETA? or how long on average for past issues like this get fixed? \nI need to decide whether wait for the fix or go for some alternatives.\nThanks.. I'm still getting \n{\"Output\":{\"__type\":\"com.amazon.coral.service#UnknownOperationException\",\"message\":null},\"Version\":\"1.0\"}. Interestingly it is working now (or somewhat randomly). I assume the deployment was in progress back then. Closing now.. ",
    "jacobbodkin": "@srchase, working with @dannyskim I was able to get the project building with the lastest AWS-SDK and RN 0.58.4 running in debug locally. However when I go to produce an IPA or APK react native hangs during the bundling phases, \"bundleJSandAssets\" or run custom shell scripts (Bundle RN Code and Assets) respectively.\n~~Do you know if devs using this library through AWS-Amplify are having this same issue? I did not see aws-sdk as a dependency and if switching over will get us moving faster we will do it.~~\nEdit: dug into aws-amplify-react-native more and found the usage of aws-sdk, seems like we're stuck...\nLet me know what I can provide you that could help get to the bottom of this issue.\nEdit 2: @srchase our company pays for Amazon support, any suggestions on where to start raising this issue louder with that to get this as a higher ticket?. ",
    "abhilashkgowda": "Add Max-old-space-size at react-native-xcode.sh\n[ -z \"$NODE_BINARY\" ] && export NODE_BINARY=\"node\"\n[ -z \"$NODE_ARGS\" ] && export NODE_ARGS=\"--max-old-space-size=4096\"\n[ -z \"$CLI_PATH\" ] && export CLI_PATH=\"$REACT_NATIVE_DIR/cli.js\"\n[ -z \"$BUNDLE_COMMAND\" ] && BUNDLE_COMMAND=\"bundle\". ",
    "shlomisas": "Weird. 2.233.1 installed by default today. I'll recheck it abd reply\nOn Wed, Jan 30, 2019, 20:56 Chase Coalwell <notifications@github.com wrote:\n\n@shlomisas https://github.com/shlomisas\nThanks for opening this issue.\n2.233.1 is an older version. Did you mean you upgraded to 2.286.2?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/aws/aws-sdk-js/issues/2507#issuecomment-459065056,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAzQXjcjyDPbt7E2nvycEvqHM4o018ezks5vIer4gaJpZM4aay1E\n.\n. Checked again, with version 2.233.1 I'm getting undefined but with a newer version or major 2 (2.286.2 or 2.395.0) it works fine.\n\nClosing the bug.. ",
    "piyumi25": "Thank you. But I tried this way by putting ContentType:image/png\nbut is s3 it shows as image/png; charset=UTF-8. what should be the reason for that?. ",
    "klaytaybai": "Hi @adanilev,\nYou are correct that the RequestId is not present in the SendMessageResult TypeScript definition, but you can still access it when using promises.  Here is an example how:\n```\nimport SQS = require('aws-sdk/clients/sqs');\nconst sqsClient = new SQS({region: 'us-west-2'});\nvar params = {\n    MessageBody: 'sqs request id', / required /\n    QueueUrl: 'https://sqs.us-west-2.amazonaws.com/xxx/xxx', / required /\n    DelaySeconds: 0\n  };\nasync function sendMessageAsync(params: any) :Promise {\n    try {\n        const sendMessageResponse = await sqsClient.sendMessage(params).promise();\n        console.log(sendMessageResponse.$response.requestId);\n    } catch(err) {\n        console.log(err);\n    }\n}\nsendMessageAsync(params);\n```. @nakulp007, I wasn't able to reproduce this with 2.286.2 or 2.395.0.  I'm not sure why you are seeing this, but I would try updating your SDK since it is quite outdated.  Please let me know if that fixes the issue for you.  I would also be interested to know if you have had this occur multiple times.. Hi @viper25, I was not able to reproduce this.  Transfer is a fairly new addition, so it is likely that your AWS package might be out of date.  Can you please check and let me know what version you are using?. That is interesting.  It\u2019s good to be aware of that.  Thanks for sharing!\nI\u2019ll close this out now.. Hi @VivithaAlamur, It looks like those errors are all asking the same thing.  Have you tried running npm i @types/node and adding node to the types field in your tsconfig?  If you have, I'll work on reproducing and resolving this.. @VivithaAlamur, one thought is that your @types/node is two major versions behind.  The current version is 10.12.21.  Using the caret range won't update you to major versions.  If you are able to do so, could you please try updating to the newest version and see if that fixes your problem?. Hi @jono99, can you please provide an example of where and how you used putObjectTagging in the context of the above code?  This function must use an object that already exists within S3.. @jono99, the most likely explanation is that the version of the SDK you are using is very old.  I wasn't able to reproduce this.  Can you run npm list aws-sdk and let us know the value?. Also, are you running this in Lambda?. Hi @GeekStocks, thanks for being involved in this discussion.  The Lambda team is aware of the issue, and they are hoping to find a good long term solution.  In the meantime, they do recommend your draconian measure of including the current SDK in your builds.  We'll have some more discussion about whether the JavaScript SDK team can help out with Lambda Layers releases and other solutions.. Does it work if you use region instead of subregion and use 'eu-west-1' instead of 'eu-west-v1'?  Those are the proper keys and values.. @worc, thank you for identifying this.  I was able to reproduce the problem.. Hi @dacacioa, I'm a little confused.  If you are able to connect with the proxy set, why do you think it is being ignored?. Hi @chris-pardy, I'm having some trouble reproducing this.  Which version of the SDK are you using?. Hi @postama, thank you for catching this.  We appreciate the offer of a pull request, but we do need the service team to make the change.  I'll ask them to take care of it.\nWe prefer that this issues list is scoped to the responsibilities of the JavaScript SDK, so I'm going to close out this issue.. Hi @cchakkaravarthi, do you have a specific error message that you are getting?\nAlso, to make sure this issue is in the right place, can you please verify that you are using the AWS SDK for JavaScript and not the AWS SDK for .NET in your project?. Hi @TDA, I'm going to close out this issue as a duplicate of your CLI issue https://github.com/aws/aws-cli/issues/3926 and https://github.com/aws/aws-cli/issues/3866.  When the service is fixed up, the JS SDK should be fine.. Hi @Jordan-McMillan101, thank you for the update.  I'm glad you got it working.  Have you tried to get it to work with Lambda and the JS SDK?  @justnance, will probably be following up with you in your CLI issue.  She is trying to work with Premium Support to make this a better experience for you and others.  . Hi @ngoc233, you will need to use the code in the aws/aws-iot-device-sdk-js repo.  The README of that repository contains examples that will show you how to subscribe.. @cocobiz, are these long running queries eventually succeeding, or do they result in an error?. ",
    "adanilev": "Thanks for the quick response @klaytaybai. The only reason I noticed this is we have test data files that we use for unit and stub tests. We specify the type for each piece of test data. In this case, I had to define as any rather than SendMessageResponse since I want to use a real response.. ",
    "viper25": "\nHi @viper25, I was not able to reproduce this. Transfer is a fairly new addition, so it is likely that your AWS package might be out of date. Can you please check and let me know what version you are using?\n\nI created a brand new nodeJS app at the time of writing this post. And I got the latest node packages via\njs\nnpm install aws-sdk\nso I assume it's pulling the latest. Or no?. > \n\nHi @viper25, I was not able to reproduce this. Transfer is a fairly new addition, so it is likely that your AWS package might be out of date. Can you please check and let me know what version you are using?\n\nI created a brand new nodeJS app at the time of writing this post. And I got the latest node packages via\njs\nnpm install aws-sdk\nso I assume it's pulling the latest. Or no?. I'm sure it's the aws-sdk version I'm using. It's shown as \ndos\nC:\\AWS_nodeJs> npm list aws-sdk\nC:\\AWS_nodeJs\n`-- aws-sdk@2.361.0\nI got the above by running npm install aws-sdk and that's what I got. Noob Q: How do I force npm to get the latest aws-sdk? I tried:\n```dos\nC:\\AWS_nodeJs>npm install aws-sdk@2.396.0\nnpm WARN registry Unexpected warning for https://registry.npmjs.org/: Miscellaneous Warning EPROTO: request to https://registry.npmjs.org/aws-sdk failed, reason: write EPROTO 101057795:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:openssl\\ssl\\s23_clnt.c:827:\\n\nnpm WARN registry Using stale package data from https://registry.npmjs.org/ due to a request error during revalidation.\nnpm ERR! code ETARGET\nnpm ERR! notarget No matching version found for aws-sdk@2.396.0\nnpm ERR! notarget In most cases you or one of your dependencies are requesting\nnpm ERR! notarget a package version that doesn't exist.\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     C:\\Users\\user1\\AppData\\Roaming\\npm-cache_logs\\2019-02-06T02_35_01_611Z-debug.log\n```\nThe package-lock.json in my root shows:\njson\n{\n  \"requires\": true,\n  \"lockfileVersion\": 1,\n  \"dependencies\": {\n    \"aws-sdk\": {\n      \"version\": \"2.361.0\",\n      \"resolved\": \"https://registry.npmjs.org/aws-sdk/-/aws-sdk-2.361.0.tgz\",\n      \"integrity\": \"sha512-dS6R7Ox5IKe1fpNAymaQSumLri6Eq7Gl4qgY4++cbARhMnqCes8f2jpoctlgJB+oZKiZITj2G9bHh5OIKRMlzA==\",\n      \"requires\": {\n        \"buffer\": \"4.9.1\",\n...... I'm sure it's the aws-sdk version I'm using. It's shown as \ndos\nC:\\AWS_nodeJs> npm list aws-sdk\nC:\\AWS_nodeJs\n`-- aws-sdk@2.361.0\nI got the above by running npm install aws-sdk and that's what I got. Noob Q: How do I force npm to get the latest aws-sdk? I tried:\n```dos\nC:\\AWS_nodeJs>npm install aws-sdk@2.396.0\nnpm WARN registry Unexpected warning for https://registry.npmjs.org/: Miscellaneous Warning EPROTO: request to https://registry.npmjs.org/aws-sdk failed, reason: write EPROTO 101057795:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:openssl\\ssl\\s23_clnt.c:827:\\n\nnpm WARN registry Using stale package data from https://registry.npmjs.org/ due to a request error during revalidation.\nnpm ERR! code ETARGET\nnpm ERR! notarget No matching version found for aws-sdk@2.396.0\nnpm ERR! notarget In most cases you or one of your dependencies are requesting\nnpm ERR! notarget a package version that doesn't exist.\nnpm ERR! A complete log of this run can be found in:\nnpm ERR!     C:\\Users\\user1\\AppData\\Roaming\\npm-cache_logs\\2019-02-06T02_35_01_611Z-debug.log\n```\nThe package-lock.json in my root shows:\njson\n{\n  \"requires\": true,\n  \"lockfileVersion\": 1,\n  \"dependencies\": {\n    \"aws-sdk\": {\n      \"version\": \"2.361.0\",\n      \"resolved\": \"https://registry.npmjs.org/aws-sdk/-/aws-sdk-2.361.0.tgz\",\n      \"integrity\": \"sha512-dS6R7Ox5IKe1fpNAymaQSumLri6Eq7Gl4qgY4++cbARhMnqCes8f2jpoctlgJB+oZKiZITj2G9bHh5OIKRMlzA==\",\n      \"requires\": {\n        \"buffer\": \"4.9.1\",\n...... OK, think I figured it out. It seems to a corporate policy issue. I didn't set npm proxy and ran:\nbat\nnpm install aws-sdk\nand I get version 2.361.0. But when i set npm proxy:\nbat\nnpm config set proxy http://proxy.company.com:8080\nnpm config set https-proxy http://proxy.company.com:8080\nand THEN run npm install aws-sdk I get the latest version of the SDK + aws-sdk@2.397.0\nStrange.. . OK, think I figured it out. It seems to a corporate policy issue. I didn't set npm proxy and ran:\nbat\nnpm install aws-sdk\nand I get version 2.361.0. But when i set npm proxy:\nbat\nnpm config set proxy http://proxy.company.com:8080\nnpm config set https-proxy http://proxy.company.com:8080\nand THEN run npm install aws-sdk I get the latest version of the SDK + aws-sdk@2.397.0\nStrange.. . ",
    "drexler": "@viper25 the constructor takes an argument, optionally. I'm using version 2.387.0 of the aws-sdk https://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/Transfer.html#constructor-property\nTest file:\n```\nconst AWS = require('aws-sdk')\nconst transfer = new AWS.Transfer({\n    apiVersion: '2018-11-05',\n    region: us-east-1\n});\nconst params = {\n    ServerId: 'hlaoslloesses' / required /\n};\ntransfer.describeServer(params, (err, data) => {\n    if (err) console.log(err, err.stack); // an error occurred\n    else     console.log(data);           // successful response\n});\n``. @chris-pardy  i see what you're trying to do. Interesting. Butv2.403.0still ships with a definition file expecting this interface as a parameter for.get` calls on the documentClient:\nexport interface GetItemInput {\n    TableName: TableName;\n    Key: Key;\n    AttributesToGet?: AttributeNameList;\n    ConsistentRead?: ConsistentRead;\n    ReturnConsumedCapacity?: ReturnConsumedCapacity;\n    ProjectionExpression?: ProjectionExpression;\n    ExpressionAttributeNames?: ExpressionAttributeNameMap;\n  }\nYou're better off just explicitly referencing the table name in the parameters for each individual call - a bit ugly but necessary for now till its changed.. ",
    "hughjdavey": "I am also having this issue on Angular 7, with latest (11.9.4) version of @types/node and the \"types\": [\"node\"] declaration in tsconfig.json. In case it helps anyone, what finally worked was putting \"types\": [\"node\"] in src/tsconfig.app.json and not tsconfig.json. ",
    "jono99": "Hi @klaytaybai I can't right now, will try tomorrow, but to give an idea, it was in-between the else after putObject just before Reset Progress Bar. Will try clarify tomorrow.. ```\nbucket.putObject(params, function(err, data) {\n        if (err) {\n            console.error(err.message, err.code);\n            return false;\n        } else {\n            console.log('File Uploaded Successfully', 'Done');\n                    **bucket.putObjectTagging(tags, function(err, data) {  }**\n\n        // Reset Progress Bar\n        $timeout(function() {\n            $scope.uploadProgress = 0;\n        }, 500);\n    }\n})\n\n```\nReturns: TypeError: bucket.putObjectTagging is not a function\n. ```\nbucket.putObject(params, function(err, data) {\n        if (err) {\n            console.error(err.message, err.code);\n            return false;\n        } else {\n            console.log('File Uploaded Successfully', 'Done');\n                    **bucket.putObjectTagging(tags, function(err, data) {  }**\n\n        // Reset Progress Bar\n        $timeout(function() {\n            $scope.uploadProgress = 0;\n        }, 500);\n    }\n})\n\n```\nReturns: TypeError: bucket.putObjectTagging is not a function\n. @klaytaybai apologies for not replying to this.\nBit late now, but output was aws-sdk@2.395.0\nAnd no, this isn't running in Lambda.\nI sort of worked out another solution, so this isn't too important anymore.. @klaytaybai apologies for not replying to this.\nBit late now, but output was aws-sdk@2.395.0\nAnd no, this isn't running in Lambda.\nI sort of worked out another solution, so this isn't too important anymore.. ",
    "eduard-malakhov": "I think I was wrong, my issue does not seem to be caused by promise(). Closing for now.. ",
    "GeekStocks": "Probably everyone reading this has cut a software release they wish they could have back; its the business. I certainly am not advocating for every release to make it to an officially supported Lambda Layer. I am making these two points:\n\n110 versions behind is not acceptable;\nlagging 3+ months behind the release of major new enhancements is not acceptable.\n\nSo for this to work, the JavaScript SDK team would have to decide upon some measure of a release's \"maturity\" to be a Lambda Layer candidate. And maturity isn't a driving factor when re:Invent rolls around with a shiny new toy. semver isn't going to help you much here \u2014 and, thank you for that btw ;-)\nUnless there are considerations I am not aware of, this would seem to be a win-win solution, would it not? Adding a few MB's to every build is wasteful.. > We also want to make customers avoid extra configurations.\nThat's a noble goal, and one I share with you. The way that goal gets achieved best is for the Lambda team to become serious about delivering an up-to-date experience. There is zero extra configuration when the SDK is kept current-ish in Lambda. We'll call this \"Plan A\", and I am heartened to hear that your team is speaking with the Lambda team.\nThis issue was opened to propose a \"Plan B\", because \"Plan A\" hasn't been working since 2017 (and prior, too). In any proposed \"Plan B\", there is going to be extra configuration involved somewhere. Having a current-ish SDK as a public Lambda Layer at least keeps the \"extra configuration\" from adding megabytes to EVERY build. \nI'm super stoked that you are talking to the Lambda team about \"Plan A\". An SDK refresh in Lambda timed to support newly announced capabilities would be a much more customer friendly modus operandi.. ",
    "santiq": "You are right @klaytaybai thank you.. ",
    "dacacioa": "Hi @klaytaybai,\nLet my explain myself better. With a proxy configuration setted, in a closed network (where I can only access to internet through a proxy server) it doesn't work. But the same configuration (with proxy) in a open network (with direct access without proxy) it works correctly.. Hi @srchase ,\nUnfortunately I don't have access to proxy logs. It's a corporate proxy and I don't have access.\nSetting sslEnabled: false doesn't make a difference.\nConfig {\n  credentials:\n   EnvironmentCredentials {\n     expired: false,\n     expireTime: null,\n     refreshCallbacks: [],\n     accessKeyId: 'AKIAJEDXHNVXQOOG3WMA',\n     sessionToken: undefined,\n     envPrefix: 'AWS' },\n  credentialProvider:\n   CredentialProviderChain {\n     providers: [ [Function], [Function], [Function], [Function] ],\n     resolveCallbacks: [] },\n  region: 'eu-west-1',\n  logger: null,\n  apiVersions: {},\n  apiVersion: null,\n  endpoint: undefined,\n  httpOptions:\n   { timeout: 120000,\n     agent:\n      ProxyAgent {\n        domain: null,\n        _events: {},\n        _eventsCount: 0,\n        _maxListeners: undefined,\n        _promisifiedCallback: false,\n        callback: [Function: connect],\n        timeout: null,\n        options: undefined,\n        proxy: [Object],\n        proxyUri: 'http://<mi.proxy.com:8080>',\n        proxyFn: [Function: httpOrHttpsProxy] } },\n  maxRetries: undefined,\n  maxRedirects: 10,\n  paramValidation: true,\n  sslEnabled: false,\n  s3ForcePathStyle: false,\n  s3BucketEndpoint: false,\n  s3DisableBodySigning: true,\n  computeChecksums: true,\n  convertResponseTypes: true,\n  correctClockSkew: false,\n  customUserAgent: null,\n  dynamoDbCrc32: true,\n  systemClockOffset: 0,\n  signatureVersion: null,\n  signatureCache: true,\n  retryDelayOptions: {},\n  useAccelerateEndpoint: false,\n  clientSideMonitoring: false,\n  endpointDiscoveryEnabled: false,\n  endpointCacheSize: 1000,\n  hostPrefixEnabled: true }\nError:\n{ Error: connect ETIMEDOUT 52.94.220.213:443\n    at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1191:14)\n  message: 'connect ETIMEDOUT 52.94.220.213:443',\n  errno: 'ETIMEDOUT',\n  code: 'NetworkingError',\n  syscall: 'connect',\n  address: '52.94.220.213',\n  port: 443,\n  region: 'eu-west-1',\n  hostname: 'email.eu-west-1.amazonaws.com',\n  retryable: true,\n  time: 2019-02-21T18:10:36.823Z }\nIf I try to access throught my proxy with curl it works fine:\n```\n$ curl -v https://email.eu-west-1.amazonaws.com/\n STATE: INIT => CONNECT handle 0x600057a60; line 1404 (connection #-5000)\n Added connection 0. The cache now contains 1 members\n   Trying *...\n TCP_NODELAY set\n STATE: CONNECT => WAITCONNECT handle 0x600057a60; line 1456 (connection #0)\n Connected to  port 8080 (#0)\n STATE: WAITCONNECT => WAITPROXYCONNECT handle 0x600057a60; line 1573 (connection #0)\n Marked for [keep alive]: HTTP default\n allocate connect buffer!\n* Establish HTTP proxy tunnel to email.eu-west-1.amazonaws.com:443\n\nCONNECT email.eu-west-1.amazonaws.com:443 HTTP/1.1\nHost: email.eu-west-1.amazonaws.com:443\nUser-Agent: curl/7.59.0\nProxy-Connection: Keep-Alive\n< HTTP/1.1 200 Connection established\n< Date: Thu, 21 Feb 2019 18:16:09 GMT\n< Via: HTTP/1.1 \n< Connection: keep-alive\n< Proxy-Connection: keep-alive\n<\n Proxy replied 200 to CONNECT request\n CONNECT phase completed!\n ALPN, offering h2\n ALPN, offering http/1.1\n Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH\n successfully set certificate verify locations:\n  CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n TLSv1.2 (OUT), TLS header, Certificate Status (22):\n TLSv1.2 (OUT), TLS handshake, Client hello (1):\n STATE: WAITPROXYCONNECT => SENDPROTOCONNECT handle 0x600057a60; line 1552 (connection #0)\n CONNECT phase completed!\n CONNECT phase completed!\n STATE: SENDPROTOCONNECT => PROTOCONNECT handle 0x600057a60; line 1587 (connection #0)\n TLSv1.2 (IN), TLS handshake, Server hello (2):\n TLSv1.2 (IN), TLS handshake, Certificate (11):\n TLSv1.2 (IN), TLS handshake, Server key exchange (12):\n TLSv1.2 (IN), TLS handshake, Server finished (14):\n TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\n TLSv1.2 (OUT), TLS change cipher, Client hello (1):\n TLSv1.2 (OUT), TLS handshake, Finished (20):\n TLSv1.2 (IN), TLS change cipher, Client hello (1):\n TLSv1.2 (IN), TLS handshake, Finished (20):\n SSL connection using TLSv1.2 / ECDHE-RSA-AES128-SHA\n ALPN, server did not agree to a protocol\n Server certificate:\n  subject: CN=email.eu-west-1.amazonaws.com\n  start date: Dec  4 00:00:00 2018 GMT\n  expire date: Nov 23 12:00:00 2019 GMT\n  subjectAltName: host \"email.eu-west-1.amazonaws.com\" matched cert's \"email.eu-west-1.amazonaws.com\"\n  issuer: C=US; O=Amazon; OU=Server CA 1B; CN=Amazon\n  SSL certificate verify ok.\n STATE: PROTOCONNECT => DO handle 0x600057a60; line 1608 (connection #0)\nGET / HTTP/1.1\nHost: email.eu-west-1.amazonaws.com\nUser-Agent: curl/7.59.0\nAccept: /*\n\nSTATE: DO => DO_DONE handle 0x600057a60; line 1670 (connection #0)\nSTATE: DO_DONE => WAITPERFORM handle 0x600057a60; line 1795 (connection #0)\nSTATE: WAITPERFORM => PERFORM handle 0x600057a60; line 1811 (connection #0)\nHTTP 1.1 or later with persistent connection, pipelining supported\n< HTTP/1.1 404 Not Found\n< x-amzn-RequestId: c3479c74-3604-11e9-a517-819ec05a1f62\n< Content-Length: 29\n< Date: Thu, 21 Feb 2019 18:16:08 GMT\n<\n\nSTATE: PERFORM => DONE handle 0x600057a60; line 1980 (connection #0)\nmulti_done\nConnection #0 to host ** left intact\n\n\n```. @srchase \nSDK version 2.401.0. ",
    "chris-pardy": "@klaytaybai 2.149.0 let me upgrade to the latest and see if it goes away. To be clear the error occurs during type checking not during runtime.\nJust confirmed with the latest version of the AWS SDK that this is still happening:\n\"aws-sdk\": \"^2.403.0\"\n\"typescript\": \"^3.3.3\". @drexler hence why I filed this bug, Typescript is preventing me from using documented behavior of the API and it's possible to re-write the typescript types to allow it (using conditional types). I've already implemented it locally and would be happy to do so here as well but I'm not sure what the guideline for contributing is.. @AllanFly120 I can open a PR to improve the typing to handle this but I'll have to upgrade the typescript version to 2.8 (conditional type support) if you're comfortable with that I'd start with a PR to update one of the clients (Probably Dynamo) and if it looks good I could automate the updates to the other clients.. @AllanFly120 see PR #2541 - This handles the case of parameters bound in the constructor. I've got examples of that in the ts/dynamodb.ts file. The types are pretty obtuse but it does the job.. @AllanFly120 Thanks for the review. As far as I can play around with the type definitions a bit more to see if I can get something that would work better for Intellisense but I'm not sure it's ever going to read cleanly with the combination of mapped types / exclude. I feel like passing this off to the VS code team to let them know that Intellisense kinda ends up garbled with these advanced features may be the better move then bending over backwards to create types that it works with.. @AllanFly120 thanks for all the feedback been a bit busy with life/work this past week but I think I can get to all these comments tomorrow. Since the Document client is less auto-generated I think I can make the Intelli-sense better. I'll take a look at the ManagedUpload too. \nIs there any concern with this causing the minimum required version of Typescript to bump from 2.0.8 to 2.8.4? It means anyone downstream with a dependency on a fixed version of Typescript is going to have build issues. I think introducing this as a point release \"should\" be ok, but I don't know your policy on breaking changes.. @AllanFly120 I was trying to fix-up the S3.ManagedUpload types but it looks like there's a larger issue, currently there are 2 operations on S3 buckets which take the parameter named Tagging but their types are different. This is obviously an issue for typescript when typing the params option in the constructor since the input type needs to be both a string and a TagSet, however digging a little deeper it looks like this will likely also cause issues if you were to provide this parameter to a constructor as it would cause one of the validations to fail.\nRoot issue aside I can resolve the type conflicts by basically ignoring the types of the params I think this will also make the rest of the intellisense more readable, but will mean less type safety.. I should probably verify this in the code but my assumption is that if you bind say the TableName parameter in the DynamoDB constructor and then pass it as the service to the DynamoDB.DocumentClient constructor you should be able to omit the TableName parameter. This generic parameter here helps with Type Inference, so that you don't need to actually specify the type of Params anywhere....\nbasically this:\nconst client = new DynamoDB.DocumentClient({\n  service: new DynamoDB({\n   params: {\n     TableName: 'MyTable'\n    }\n  })\n});\n// can omit TableName\nclient.get({Key: {id: '1234'}}, console.log)\nI added a case to the Dynamo typescript file (the one that validates the types) to show that this was working but now that I type this I think I want to also validate the case that I can pass both a service with parameters and a additional parameters and all of them should be omitted from the method calls.. ",
    "teroxik": "Where do you run the code? EC2 instance, Fargate instance, lambda?  169.254.169.254:80 is a metadata endpoint for the EC2 instance. I have some problems with running nodejs app on fargate using aws sdk javascript SDK as well. So it probably has something to do with the AWS credentials configuration.. Where do you run the code? EC2 instance, Fargate instance, lambda?  169.254.169.254:80 is a metadata endpoint for the EC2 instance. I have some problems with running nodejs app on fargate using aws sdk javascript SDK as well. So it probably has something to do with the AWS credentials configuration.. Don't want to hijack your issue, but have a similar problem. Running a nodejs 10.15 app, tried version 8 as well. Looking at the code in the credentials chain:\nAWS.CredentialProviderChain.defaultProviders = [\n  function () { return new AWS.EnvironmentCredentials('AWS'); },\n  function () { return new AWS.EnvironmentCredentials('AMAZON'); },\n  function () { return new AWS.SharedIniFileCredentials(); },\n  function () {\n    if (AWS.ECSCredentials.prototype.isConfiguredForEcsCredentials()) {\n      return new AWS.ECSCredentials();\n    }\n    return new AWS.EC2MetadataCredentials();\n  }\n];\nthe last EC2MetadataCredentilas() makes the call to that endpoint, which is not available. In my fargate process I have listed the process.env variables and there isn't the full or relative url variable set as described in here remote_credentials. Any advice on that? \n@LovikaJ would recommend you to do the same - to get the output.\nsdk version: 2.401 - latest. Don't want to hijack your issue, but have a similar problem. Running a nodejs 10.15 app, tried version 8 as well. Looking at the code in the credentials chain:\nAWS.CredentialProviderChain.defaultProviders = [\n  function () { return new AWS.EnvironmentCredentials('AWS'); },\n  function () { return new AWS.EnvironmentCredentials('AMAZON'); },\n  function () { return new AWS.SharedIniFileCredentials(); },\n  function () {\n    if (AWS.ECSCredentials.prototype.isConfiguredForEcsCredentials()) {\n      return new AWS.ECSCredentials();\n    }\n    return new AWS.EC2MetadataCredentials();\n  }\n];\nthe last EC2MetadataCredentilas() makes the call to that endpoint, which is not available. In my fargate process I have listed the process.env variables and there isn't the full or relative url variable set as described in here remote_credentials. Any advice on that? \n@LovikaJ would recommend you to do the same - to get the output.\nsdk version: 2.401 - latest. @LovikaJ Thank you, that's what I have done already as a workaround. But I want to use the instance profile without using an explicit credentials as that works correctly with Java SDK for example. And I would like to understand where the problem is. If this is an expected behaviour. For example in lambdas the SDK is set up correctly without the need for using ENV credentials. That makes me wonder if there is something wrong with the fargate runtime or my setup.. @LovikaJ Thank you, that's what I have done already as a workaround. But I want to use the instance profile without using an explicit credentials as that works correctly with Java SDK for example. And I would like to understand where the problem is. If this is an expected behaviour. For example in lambdas the SDK is set up correctly without the need for using ENV credentials. That makes me wonder if there is something wrong with the fargate runtime or my setup.. My stack trace is actually slightly different, but could be just different version of node:\n{ Error: connect EINVAL 169.254.169.254:80 - Local (0.0.0.0:0) \nat internalConnect (net.js:964:16)\nat defaultTriggerAsyncIdScope (internal/async_hooks.js:281:19)\nat net.js:1062:9\nat _combinedTickCallback (internal/process/next_tick.js:132:7)\nat process._tickDomainCallback (internal/process/next_tick.js:219:9)\nmessage: 'Missing credentials in config',\nerrno: 'EINVAL',\ncode: 'CredentialsError',\nsyscall: 'connect',\naddress: '169.254.169.254',\nport: 80,\ntime: 2019-02-13T12:46:53.403Z,\nI will reach out to the forum, thanks for help. . My stack trace is actually slightly different, but could be just different version of node:\n{ Error: connect EINVAL 169.254.169.254:80 - Local (0.0.0.0:0) \nat internalConnect (net.js:964:16)\nat defaultTriggerAsyncIdScope (internal/async_hooks.js:281:19)\nat net.js:1062:9\nat _combinedTickCallback (internal/process/next_tick.js:132:7)\nat process._tickDomainCallback (internal/process/next_tick.js:219:9)\nmessage: 'Missing credentials in config',\nerrno: 'EINVAL',\ncode: 'CredentialsError',\nsyscall: 'connect',\naddress: '169.254.169.254',\nport: 80,\ntime: 2019-02-13T12:46:53.403Z,\nI will reach out to the forum, thanks for help. . Sorry for inactivity, on hols this week. Opened an issue in the AWS Forum as suggested.\nAWS Forum. Sorry for inactivity, on hols this week. Opened an issue in the AWS Forum as suggested.\nAWS Forum. I'm OK if you close it. I will come back to it and update it here, reopen eventually.. I'm OK if you close it. I will come back to it and update it here, reopen eventually.. ",
    "LovikaJ": "I am running it using npm test in lambdas and trying to connect to SES and S3. I checked my AWS credentials multiple times. the same configurations were working fine till 2-3 days ago.. @teroxik Hi, I was able to resolve the issue by setting the aws credentials in environment variables and by updating the aws-sdkversion, although had to keep it > than 2.305.0 as I get async-suffix error in the upper versions. Although good thing is issue has been resolved.\nYou could try to set the AWS_ACCESS_KEY_IDand AWS_SECRET_ACCESS_KEYin process.env . ",
    "cchakkaravarthi": "Hi @klaytaybai,\nI'm using AWS SDK in JavaScript while I got the error.\n\nIf I use AWS SDK in .NET while I got the error is bellow ,\nInner Exception 1:\nCOMException: The text associated with this error code could not be found.\nThe server name or address could not be resolved\n. Hi @srchase ,\nEnvironment :\nTools:\nVisual Studio Code Version: 1.25.1 ,\nMicrosoft Visual Studio Professional 2017 \nVersion 15.2 (26430.6) Release\nVisualStudio.15.Release/15.2.0+26430.6\nMicrosoft .NET Framework\nVersion 4.7.03190.\n\"react\": \"16.3.1\",\n \"react-native\": \"^0.55.4\",\"react-native-windows\": \"0.55.0-rc.0\",\n \"rnpm-plugin-windows\": \"^0.2.8\"\n\"@types/aws-sdk\": \"^2.7.0\",\n  \"aws-sdk\": \"^2.401.0\",\nExample Of Code :\nJavaScript  Code : \n    AWS.config.update({ accessKeyId: config.accessKeyId, secretAccessKey: config.secretAccessKey }); \n    AWS.config.region = config.region;\n\n    var bucket = new AWS.S3({params: {Bucket: config.bucket}});\n    bucket.getObject({Key: config.key},function(err:any,file:any){\n    debugger;\n    const data = file.Body;\n    });\n\nC# Code : \n            References : AWSSDK.S3 3.3.31.19\n        var regionIdentifier = RegionEndpoint.GetBySystemName(region);\n        using (var client = new AmazonS3Client(storageKeyId, storageSecret, regionIdentifier))\n        {\n            var response = await client.GetObjectAsync(AwsBucketName, AwsBucketFileName);\n            var reader =response.ResponseStream))\n        }\n\n. Hi @srchase ,\nI am facing this issue in every S3 methods\nplease refer this \nvar params = {\n  Bucket: \"examplebucket\", \n  Key: \"HappyFace.jpg\"\n };\n s3.getObject(params, function(err, data) {\n   if (err) console.log(err, err.stack); // an error occurred\n   else     console.log(data);           // successful response\n});\nIn AWS.HttpRequest(endpoint, region)\nheader : { X-Amz-User-Agent: \"aws-sdk-js-react-native/2.401.0\" }\n. ",
    "Jordan-McMillan101": "I ended up getting this to work successfully -- This is why it was failing:\nI was originally making the cli call with the following request object as this is what is including in the documentation:\n```\naws pinpoint update-apns-sandbox-channel --application-id [physicalID]  --cli-input-json file:///path-to-requestObject.json\n```\n```\n{\n    \"APNSSandboxChannelRequest\": {\n        \"BundleId\": \"com.bundleId.value\", \n        \"Certificate\":\"P12_FILE_PATH_OR_CERT_AS_STRING\", \n        \"DefaultAuthenticationMethod\": \"CERTIFICATE\", \n        \"Enabled\": true, \n        \"PrivateKey\":\"PRIVATEKEY_FILE_PATH_OR_AS_STRING\",\n        \"TeamId\": \"\",\n        \"TokenKey\": \"\",\n        \"TokenKeyId\": \"\"\n    }, \n    \"ApplicationId\": \"Pinpoint_PhysicalId\"\n}\n```\nAfter playing around with it some more I got it to work by removing BundleId, TeamId, TokenKey, and TokenKeyId. I believe these fields are needed when using a p8 certificate.\n```\n{\n    \"APNSSandboxChannelRequest\": {\n        \"Certificate\":\"P12_FILE_PATH_OR_CERT_AS_STRING\", \n        \"DefaultAuthenticationMethod\": \"CERTIFICATE\", \n        \"Enabled\": true, \n        \"PrivateKey\":\"PRIVATEKEY_FILE_PATH_OR_AS_STRING\"    \n    }, \n    \"ApplicationId\": \"Pinpoint_PhysicalId\"\n}\n```\nIt would be very useful to add better error messages or more information in the documentation. . @klaytaybai thanks for the reply, yes I was able to get it working with my lambda-backed custom resource (pinpoint app).\nAre you able to confirm if the fields I removed are for different apns certificate types? In my case I have a .p12 for apns_sandbox and a .p12 for apns. I see pinpoint and mobileHub support .p8 as well as a universal certificate, so I think a little more explanation in the documentation would really help.\n. ",
    "coreyjv": "Similarly I think an API like getConnectionStatus would be nice to have as well. \nIs it possible to contribute to the SDK to provide these abilities?. @rodolphito \u2014\nWe force close connections by issuing a HTTP DELETE request on /@connections/:id. \nThe only place I\u2019ve seen this documented is here:\nhttps://youtu.be/3SCdzzD0PdQ?t=687. ",
    "rodolphito": "This is important. Currently, it is not possible to \"kick\" a connection, you just have to send them a disconnect message, and they need to choose to accept it. Clients could theoretically be modified to never close the connection, and incur large bills on the AWS Account owner.... This is important. Currently, it is not possible to \"kick\" a connection, you just have to send them a disconnect message, and they need to choose to accept it. Clients could theoretically be modified to never close the connection, and incur large bills on the AWS Account owner.... Thank you, looks great!. Thank you, looks great!. ",
    "renatoargh": "@srchase I tried version 2.278.1 but I have just noticed that I actually have this (note the tabs and line breaks):\njs\n\"ConditionExpression\": \"\\tsize(aggregators) = :one AND \\n\\t contains(aggregators, :aggregator)\\n\"\nI will get home soon and try removing these additional characters and let you know.\nIt used to work but because I had \"aws-sdk\": \"^2.278.1\", on my package.json I can't be 100% sure what version this worked and when stopped working.\nWere you able to reproduce?. Yes, I can confirm that I stop seeing the problem as long as I remove the new lines and tabs!. Everything worked as expected now.\nI think that this should be fixed but I agree that it is very low priority. But honestly, I spent an hour until I fugured out that line breaks and tabs were the breaking problem.\nMy code was like this:\njs\n      const params = {\n        'Key': ddt.wrap({ topicArn }),\n        'ExpressionAttributeValues': ddt.wrap({\n          ':one': 1,\n          ':aggregator': aggregator\n        }),\n        'ConditionExpression': `\n            size(aggregators) = :one AND \n            contains(aggregators, :aggregator)\n        `,\n        'TableName': SUBSCRIPTIONS_TABLE,\n        'ReturnValues': 'ALL_OLD'\n      }\nI don't think that whitespaces and tabs should be a problem to the sdk, but these are only my 2 cents. Fell free to close you think this is the right flow. \nThank you a lot for your time, and thanks for the great work! All the best\nPS.: The working code now looks like this:\njs\n      const params = {\n        'Key': ddt.wrap({ topicArn }),\n        'ExpressionAttributeValues': ddt.wrap({\n          ':one': 1,\n          ':aggregator': aggregator\n        }),\n        'ConditionExpression': 'size(aggregators) = :one AND contains(aggregators, :aggregator)',\n        'TableName': SUBSCRIPTIONS_TABLE,\n        'ReturnValues': 'ALL_OLD'\n      }. ",
    "santriseus": "@AllanFly120 Thanks for your reply. I tried to inject the logger inside the ses config, but there is no any useful output from it during the hanging.\nWhen we try to run it at our local machines, which utilize the developer credentials set via the cli (accessKeyId+secretAccessKey) everything works fine and we receive the following message from the sdk internal logging: \n\"[AWS ses 200 0.655s 0 retries] sendEmail({ Destination: { ToAddresses: ...\"\nBut when we run the same code at the EC2 instance with policies set via the IAM Role we don`t have any output from sdk internal logging. It just hangs: no callback is called. No information/errors in log. We also tried to use sendEmail(...).promise() but the situation is the same: promise never becomes resolved.\nIs it possible to make internal sdk logging more verbose, to understand what cause the hanging?\nFor now we rolled back to version \"2.358.0\" and it works fine. Callback is called and we could see in internal sdk log: \"AWS ses 200 0.147s 0 retries] sendEmail({ Destination: { ToAddresses: ... }) \"\n . ",
    "BenjD90": "@mreinstein I've updated the project, and removed promise-pool-executor dependency. I've replaced it with a simple Promise.all on 100. So we upload many files at the same time, to simulate multiple concurrent calls to our api at the same time.\nThe result didn't change :cry: \nHere the part that has changed :\n```js\nawait Promise.all(\n      Array(100)\n          .fill(0)\n          .map(async (v, index) => {\n            const fileStream = fs.createReadStream('media-sample.jpg');\n        let uploadResult;\n        if (storageType === 'fs') uploadResult = await saveToFileSystem(fileStream);\n        else if (storageType === 's3') uploadResult = await saveToS3(fileStream);\n        else throw new Error('unknown-storage-type');\n\n        // printRAMUsage();\n        // console.log(`DONE ${index} size: ${uploadResult.size} hash : ${uploadResult.hash}`);\n      })\n\n);\n``\n. @mreinstein I've updated the project, and removedpromise-pool-executor` dependency. I've replaced it with a simple Promise.all on 100. So we upload many files at the same time, to simulate multiple concurrent calls to our api at the same time.\nThe result didn't change :cry: \nHere the part that has changed :\n```js\nawait Promise.all(\n      Array(100)\n          .fill(0)\n          .map(async (v, index) => {\n            const fileStream = fs.createReadStream('media-sample.jpg');\n        let uploadResult;\n        if (storageType === 'fs') uploadResult = await saveToFileSystem(fileStream);\n        else if (storageType === 's3') uploadResult = await saveToS3(fileStream);\n        else throw new Error('unknown-storage-type');\n\n        // printRAMUsage();\n        // console.log(`DONE ${index} size: ${uploadResult.size} hash : ${uploadResult.hash}`);\n      })\n\n);\n```\n. Hello @AllanFly120,\nThank you for investigating the issue !\nThe purpose of the launch concurrently 100 s3.upload is to simulate 100 calls to my API that upload files to S3.\nThe first part of the reproduce project show how it reacts for the same task but saving to disk, with a slow disk writing NoeJS slows down the reading stream and do not use a lot of memory (backpreasure solution in NodeJS).\n1 - Do you know if I can avoid multi-part upload and upload my file in one part with a stream ? I'm thinking of something like this with request library :\njs\nfs.createReadStream('file.json').pipe(request.put('http://mysite.com/obj.json'))\n2 - If it is not possible, why there is a minimum of 5 MB for the multi-part buffer ?\n3 - If I count, I upload 100 files each use a 5 MB buffer in memory, so I should use 200 MB (node idle) + 500 MB = 700 MB which not what I measure :disappointed: \n. Hello @AllanFly120,\nThank you for investigating the issue !\nThe purpose of the launch concurrently 100 s3.upload is to simulate 100 calls to my API that upload files to S3.\nThe first part of the reproduce project show how it reacts for the same task but saving to disk, with a slow disk writing NoeJS slows down the reading stream and do not use a lot of memory (backpreasure solution in NodeJS).\n1 - Do you know if I can avoid multi-part upload and upload my file in one part with a stream ? I'm thinking of something like this with request library :\njs\nfs.createReadStream('file.json').pipe(request.put('http://mysite.com/obj.json'))\n2 - If it is not possible, why there is a minimum of 5 MB for the multi-part buffer ?\n3 - If I count, I upload 100 files each use a 5 MB buffer in memory, so I should use 200 MB (node idle) + 500 MB = 700 MB which not what I measure :disappointed: \n. ",
    "victorboissiere": "I have the same issue! Is it related to the way AWS sdk handles streams ?. ",
    "laynemoseley": "Thank you!\nAny idea on timeframe?. perfect. Thank you for the help! . thank you! . ",
    "tijanirf": "I tried to create MediaConvert job in lambda function, but i still having same problem like @laynemoseley. The error is still the same (Unexpected key 'Rotate' found in params.Settings.Inputs[0].VideoSelector) even though i specified the api version using apiVersion AWS.config.apiVersion = {mediaconvert: '2017-08-29'}\nIs it because i specify wrong api version? Any hint would helping me a lot, i tried to solve this error for 2 days but still no result. Thanks. ",
    "akefirad": "I'm sorry guys, it was permission entry in the manifest file.. ",
    "marcus-hiles": "@JeremyCraigMartinez  Yeah, that will be helpful. Great @coxom . Great @klaytaybai \ud83d\udc4d . Awesome find and fix @tan31989 . ",
    "coxom": "@AllanFly120 thank you for the help.\nI have added the keepAlive through the agent property in httpOptions like suggested here: \nhttps://github.com/aws/aws-sdk-js/issues/671 hope it solves the issue.\nWill give feedback if it gets solved.. Hi,\nUnfortunately this hasn't solved the issue. We continue to have random timeouts.. ",
    "iamzee": "i dont know how to generate without express.\nbut with express i do get a presigned url.\nwhen i open that url in the browser it shows signature does not match\n. on client side, i will use PUT with pre signed url\n. ",
    "stayingcool": "Any feedback please?. Thank you @srchase I removed AttributesToGet completely and I see all the attributes are returned. This is for good for now.. ",
    "bodyslam": "@AllanFly120 thank you so much, that tidbit really helped. I'm currently seeing 'Access Denied'.\nI think the issue is that I'm generating the tokens as my own account; rather than as the 'readOnly' role.\nI'm using the following bash script to setup the env vars:\n```\nawsTempToken()\n{\n  if [ ! -f ~/.aws/credentials ]; then\n    echo \"AWS credentials not configured!\"\n    return 1\n  fi  \nAWS_TEMP_CREDS=aws sts get-session-token --serial-number someARN  --token-code \"$1\" | jq -c '.Credentials'\n  export AWS_ACCESS_KEY_ID=echo $AWS_TEMP_CREDS | jq -r '.AccessKeyId'\n  export AWS_SECRET_ACCESS_KEY=echo $AWS_TEMP_CREDS | jq -r '.SecretAccessKey'\n  export AWS_SESSION_TOKEN=echo $AWS_TEMP_CREDS | jq -r '.SessionToken'\n}\n```\nI'm thinking that I should be able to get the tokens under the new role by:\n1. Authenticate as myself\n2. Switch to the readonly role\n3. Get the temp creds as the switched role\nWould you know if it's thats possible ? Something like the following (it doesnt work):\naws sts assume-role --role-arn someArnRole | aws sts get-session-token --serial-number someArnToken --token-code \"$1\"\n. Hey @AllanFly120 \nSo the issue I'd like to avoid right now is to provide a MFA token every time   'AWS.STS.assumeRole()'  is called. My thinking was that if save the tokens (generated via the aforementioned method) that I could just inject them for future use until the token TTL expires.  \nSo what I did next is my own bone-headed move; I tried to assumeRole with the store creds. I now realize that the creds were for the role I orignally wanted.  Therefore I didn't have to call assumeRole the 2nd time just to get the creds I already had! Oy I need more coffee. \nThanks again for your help!. Hey @AllanFly120 \nOne quick, related question I cant seem to fire off tokenCodeFn Lambda\n```\n  public static async getLocalCreds(): Promise {\n    console.log('--> start getLocal <---')\n    AwsS3Tool.instance = new AWS.S3({\n      region: 'us-east-2',\n      credentials: new AWS.SharedIniFileCredentials({\n        profile: 'readonly',\n        tokenCodeFn: async (mfaSerial, cb) => {\n          console.log('---> Inner fn call <---');\n          cb(null, await AwsS3Tool.askForToken(mfaSerial));\n        }\n      })\n    })\n  }\npublic static askForToken(serial?: string): Promise {\n    return new Promise((resolve) => {\n      const rl = readline.createInterface({\n        input: process.stdin,\n        output: process.stdout,\n      });\n  rl.question(`Please enter your MFA token (${serial}):`, token => {\n    rl.close();\n    return resolve(token)\n  })\n})\n\n}\n```\nThe inner function never gets called; instead I'm seeing:\n$ generate-swagger\n$ node ./dist/index.js\n--> start getLocal <---\n2019-03-12T21:37:55.181Z [-] fatal [0] 'The following error occurred when starting the service: Missing credentials in config [\"{ CredentialsError: Missing credentials in config\\\\n    at SharedIniFileCredentials.loadRoleProfile\nAny clues what I'm doing wrong?\n. @allanFly120 Thanks again for your help!. Ok the issue here is 2 fold: \n\n\nI needed to provide a callback to capture the error\npublic static async getLocalCreds(profile: string = AwsS3Tool.DEFAULT_PROFILE): Promise<void> {\n    const creds: SharedIniFileCredentials = await new AWS.SharedIniFileCredentials({\n      tokenCodeFn: AwsS3Tool.tokenCodeFn,\n      profile,\n      disableAssumeRole: false,\n      callback: (err) => {\n        // This will provide useful error messages!!\n        console.log(`SharedIniFileCreds Error: ${err}`)\n      }\n    });\n    AWS.config.credentials = creds;\n  }\n\n\nI'm using typescript which the type definition wasn't updated; I've submitted a PR here\nhttps://github.com/aws/aws-sdk-js/pull/2578\n\n\nHope this helps someone\n. ",
    "cerber717": "wow, how was this missed! \ud83d\udd14 . wow, how was this missed! \ud83d\udd14 . ",
    "iffyio": "Hi @AllanFly120 Thanks for looking into this :)\nI did start reading from the response stream since that is what triggers the sleep (it has a destination piped in) - it's the processing of the first chunk that takes time.\nI figured as much that the content length mismatch was between what was supposed to be read vs actual since their bytecounts matched - what I find surprising is that the request is considered successful even though the data was not read in its entirety from the server.\nI think what's confusing would be why end is being triggered? This should only be done internally when all data has been received from the server right?. Hey @AllanFly120 Could you confirm whether this issue is a bug?. ",
    "greg-peters": "The goal is to query all expired accounts in the future that have expired globally. It just seems like begins_with doesn't work on put operations.\n```\nconst db = new AWS.DynamoDB.DocumentClient({apiVersion: '2012-08-10'});\n...\nvar expirationDate =  new Date();\nexpirationDate.setDate(new Date().getDate() + 30); //30 day trial on new accounts\nvar account = {\n    PK: data.domain,\n    SK: 'account_'+expirationDate.getTime(),\n    plan_type: 'Trial'\n};\n\ndb.put({\n        TableName: process.env.TABLE_NAME,\n        ConditionExpression: \"attribute_not_exists(PK) AND (NOT begins_with(SK,'account_'))\",\n        Item: account\n    },function(err,data) {\n        callback(null,done(err,data));\n    });\n\n```. I haven't tried this on the CLI, however, this still may be an issue. I redesigned my architecture to work around this perceived issue.. ",
    "sawaikar-gauri": "@srchase  Sorry, missed to include the link\nhttps://github.com/sawaikar-gauri/html-to-pdf-serverless\nAlso, this was the solution proposed to use :\nhttps://stackoverflow.com/questions/55141434/issue-writing-non-ascii-content-to-s3?noredirect=1#comment97090136_55141434\n. No, Can i get an reference of s3 upload  with string containing Chinese or Korean text ?\nOr Can we get upload_file python equivalent in the javascript sdk ? Wherein i can directly upload a local file to s3?\nMy query is regarding the encoding, and how to fix it.\nThanks for response.. M not sure that the issue is in the SDK.\nThe function that i am using for upload works fine for local file and not s3 . So I am not sure, what is going wrong\nThe upload i am using is with streams which fails\nI will try the example. ",
    "cocobiz": "@srchase @klaytaybai Thanks for your replay.\nThese long queries have no issue and they are very simple (just query by index or primary key), because we can get the response immediately when retry the same query in local env.\n@klaytaybai these long running queries eventually succeeding from our log and has no errors.\nTheses cases occurred in our Production Environment, and we can not debug it because these kind of problems are not reproducible.\nThere is a suspicion that it is the Production Environment which has a large concurrency, we have done some researches:\n- increase the value of  UV_THREADPOOL_SIZE, but seem to has no effect.\n- we guess if the event loop was blocked when long query occurred? but we found that there are lots of process logs when the long query occurred, in other words the event loop was not blocked by any other operations when the long query occurred.\n- and we also did some profiling for the app, but get nothing useful info.\nFinally, we have no idea about how to fix this issue.. ",
    "demobox": "This unfortunately is broken on Node 4.4.5 :-( Buffer.from is a function but fails (because it is actually Uint8Array.from)\n```\nroot@45d8cedbf66f:/app# node --version\nv4.4.5\nroot@45d8cedbf66f:/app# node\n\nBuffer.from === Uint8Array.from\ntrue\ntypeof Buffer.from\n'function'\nBuffer.from(\"foo\")\nTypeError: this is not a typed array.\n    at Function.from (native)\n...\n```. \n",
    "mpareja": "Based on the usage of connectTimeoutId in the stream error handler below, do you think it would be clearer if this declaration were moved outside of the if (httpOptions.connectTimeout) block? I don't think it will have a functional difference given var is function scoped - just clarity.\n. ",
    "jasdel": "this will only replace the first instance of the hostLabel. While odd, a hostPrefix template could have multiple instances of the same hostLabel.\nIf you add the global (g) flag to this regex i think it will solve the issue.. ",
    "cjyclaire": "Just echoing diff at line 309, is statusCode && check also needed here?. "
}